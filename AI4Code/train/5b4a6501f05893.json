{"cell_type":{"d26ad95d":"code","17a2e160":"code","ad28256a":"code","cb5008dc":"code","243638cb":"code","399f79b3":"code","3ad390f4":"code","e69cf647":"code","b046c5f2":"code","fb19d8da":"code","51c701b5":"code","4ac937c5":"code","266bce4c":"code","ff588edf":"code","03228688":"markdown","8687dde6":"markdown","b8357597":"markdown","50a69feb":"markdown","9ad2f97e":"markdown","bc8f6df9":"markdown","7db51bdb":"markdown","ea149603":"markdown","96503600":"markdown","517edba0":"markdown","899bf5eb":"markdown","81931421":"markdown","0bc4ab3c":"markdown","44dcdef1":"markdown"},"source":{"d26ad95d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17a2e160":"# our basic libraries\nimport torch\nimport torchvision\n\n# data loading and transforming\nfrom torchvision.datasets import FashionMNIST\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# The output of torchvision datasets are PILImage images of range [0, 1]. \n# We transform them to Tensors for input into a CNN\n\n## Define a transform to read the data in as a tensor\ndata_transform=transforms.ToTensor()\n\n# choose the training and test datasets\ntrain_data=FashionMNIST(root=\".\/data\",train=True,download=True,transform=data_transform)\ntest_data=FashionMNIST(root=\".\/data\",train=False,download=True,transform=data_transform)\n\n# Print out some stats about the training data and Test data\nprint(\"Train Data, Number of Images : \",len(train_data))\nprint(\"Test Data, Number of Images : \",len(test_data))","ad28256a":"# prepare data loaders, set the batch_size\n## TODO: you can try changing the batch_size to be larger or smaller\n## when you get to training your network, see how batch_size affects the loss\nbatch_size=20\n\ntrain_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\ntest_loader=DataLoader(test_data,batch_size=batch_size,shuffle=True)\n\n# specify the image classes\nclasses = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","cb5008dc":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Obtain one batch of Training Images\ndataiter=iter(train_loader)\nimages,labels=dataiter.next()\nimages=images.numpy()\n\n# plot the images in the batch, along with the corresponding labels\nfig=plt.figure(figsize=(25,4))\nfor idx in np.arange(batch_size):\n    ax=fig.add_subplot(2,batch_size\/2,idx+1,xticks=[],yticks=[])\n    ax.imshow(np.squeeze(images[idx]),cmap='gray')\n    ax.set_title(classes[labels[idx]])\n","243638cb":"# select an image by index\nidx = 2\nimg = np.squeeze(images[idx])\n\n# display the pixel values in that image\nfig = plt.figure(figsize = (12,12)) \nax = fig.add_subplot(111)\nax.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max()\/2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y],2) if img[x][y] !=0 else 0\n        ax.annotate(str(val), xy=(y,x),\n                    horizontalalignment='center',\n                    verticalalignment='center',\n                    color='white' if img[x][y]<thresh else 'black')","399f79b3":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # 1 input image channel (grayscale), 10 output channels\/feature maps\n        # 3x3 square convolution kernel\n        ## output size = (W-F)\/S +1 = (28-3)\/1 +1 = 26\n        # the output Tensor for one image, will have the dimensions: (10, 26, 26)\n        # after one pool layer, this becomes (10, 13, 13)\n        self.conv1 = nn.Conv2d(1, 10, 3)\n        \n        # maxpool layer\n        # pool with kernel_size=2, stride=2\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # second conv layer: 10 inputs, 20 outputs, 3x3 conv\n        ## output size = (W-F)\/S +1 = (13-3)\/1 +1 = 11\n        # the output tensor will have dimensions: (20, 11, 11)\n        # after another pool layer this becomes (20, 5, 5); 5.5 is rounded down\n        self.conv2 = nn.Conv2d(10, 20, 3)\n        \n        # 20 outputs * the 5*5 filtered\/pooled map size\n        # 10 output channels (for the 10 classes)\n        self.fc1 = nn.Linear(20*5*5, 10)\n        \n\n    # define the feedforward behavior\n    def forward(self, x):\n        # two conv\/relu + pool layers\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n\n        # prep for linear layer\n        # flatten the inputs into a vector\n        x = x.view(x.size(0), -1)\n        \n        # one linear layer\n        x = self.fc1(x)\n        # a softmax layer to convert the 10 outputs into a distribution of class scores\n        x = F.log_softmax(x, dim=1)\n        \n        # final output\n        return x\n\n# instantiate and print your Net\nnet = Net()\nprint(net)","3ad390f4":"import torch.optim as optim\n\n## TODO: specify loss function \n# cross entropy loss combines softmax and nn.NLLLoss() in one single class.\ncriterion = nn.NLLLoss()\n\n## TODO: specify optimizer \n# stochastic gradient descent with a small learning rate\noptimizer = optim.SGD(net.parameters(), lr=0.001)","e69cf647":"# Calculate accuracy before training\ncorrect = 0\ntotal = 0\n\n# Iterate through test dataset\nfor images, labels in test_loader:\n\n    # forward pass to get outputs\n    # the outputs are a series of class scores\n    outputs = net(images)\n\n    # get the predicted class from the maximum value in the output-list of class scores\n    _, predicted = torch.max(outputs.data, 1)\n\n    # count up total number of correct labels\n    # for which the predicted and true labels are equal\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\n# calculate the accuracy\n# to convert `correct` from a Tensor into a scalar, use .item()\naccuracy = 100.0 * correct.item() \/ total\n\n# print it out!\nprint('Accuracy before training: ', accuracy)","b046c5f2":"def train(n_epochs):\n    \n    loss_over_time = [] # to track the loss as the network trains\n    \n    for epoch in range(n_epochs):  # loop over the dataset multiple times\n        \n        running_loss = 0.0\n        \n        for batch_i, data in enumerate(train_loader):\n            # get the input images and their corresponding labels\n            inputs, labels = data\n\n            # zero the parameter (weight) gradients\n            optimizer.zero_grad()\n\n            # forward pass to get outputs\n            outputs = net(inputs)\n\n            # calculate the loss\n            loss = criterion(outputs, labels)\n\n            # backward pass to calculate the parameter gradients\n            loss.backward()\n\n            # update the parameters\n            optimizer.step()\n\n            # print loss statistics\n            # to convert loss into a scalar and add it to running_loss, we use .item()\n            running_loss += loss.item()\n            \n            if batch_i % 1000 == 999:    # print every 1000 batches\n                avg_loss = running_loss\/1000\n                # record and print the avg loss over the 1000 batches\n                loss_over_time.append(avg_loss)\n                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n                running_loss = 0.0\n\n    print('Finished Training')\n    return loss_over_time\n","fb19d8da":"# define the number of epochs to train for\nn_epochs = 30 # start small to see if your model works, initially\n\n# call train and record the loss over time\ntraining_loss = train(n_epochs)","51c701b5":"# visualize the loss as the network trained\nplt.plot(training_loss)\nplt.xlabel('1000\\'s of batches')\nplt.ylabel('Loss')\nplt.ylim(0,2.5) # Consistent Scale\nplt.show()","4ac937c5":"# initialize tensor and lists to monitor test loss and accuracy\ntest_loss=torch.zeros(1)\nclass_correct=list(0. for i in range(10))\nclass_total=list(0. for i in range(10))\n\n\n# set the module to evaluation mode\nnet.eval()\n\nfor batch_i,data in enumerate(test_loader):\n    # get the input Images and their corresponding labels\n    inputs,labels=data\n    \n    # forward pass to get Outputs\n    outputs=net(inputs)\n    \n    # Calculate the loss\n    loss=criterion(outputs,labels)\n    \n    \n    # update average test loss \n    test_loss = test_loss + ((torch.ones(1) \/ (batch_i + 1)) * (loss.data - test_loss))\n    \n    # get the predicted class from the maximum value in the output-list of class scores\n    _,predicted=torch.max(outputs.data,1)\n    \n    # compare predictions to true label\n    # this creates a `correct` Tensor that holds the number of correctly classified images in a batch\n    correct=np.squeeze(predicted.eq(labels.data.view_as(predicted)))\n    \n    # calculate test accuracy for *each* object class\n    # we get the scalar value of correct items for a class, by calling `correct[i].item()`\n    for i in range(batch_size):\n        label = labels.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n        \nprint('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\n        \nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","266bce4c":"# obtain one batch of test images\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n# get predictions\npreds = np.squeeze(net(images).data.max(1, keepdim=True)[1].numpy())\nimages = images.numpy()\n\n# plot the images in the batch, along with predicted and true labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(batch_size):\n    ax = fig.add_subplot(2, batch_size\/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))","ff588edf":"# Saving the model\nmodel_dir = 'saved_models\/'\nmodel_name = 'fashion_net_simple.pt'\n\n# after training, save your model parameters in the dir 'saved_models'\n# when you're ready, un-comment the line below\ntorch.save(net.state_dict(), model_name)","03228688":"### Specify the loss function and optimizer\n\nLearn more about [loss functions](http:\/\/pytorch.org\/docs\/master\/nn.html#loss-functions) and [optimizers](http:\/\/pytorch.org\/docs\/master\/optim.html) in the online documentation.\n\nNote that for a classification problem like this, one typically uses cross entropy loss, which can be defined in code like: `criterion = nn.CrossEntropyLoss()`; cross entropy loss combines `softmax` and `NLL loss` so, alternatively (as in this example), you may see NLL Loss being used when the output of our Net is a distribution of class scores. \n\nPyTorch also includes some standard stochastic optimizers like stochastic gradient descent and Adam. You're encouraged to try different optimizers and see how your model responds to these choices as it trains.\n","8687dde6":"**Answer**: This model performs well on everything but shirts and pullovers (0% accuracy); it looks like this incorrectly classifies most of those as a coat which has a similar overall shape. Because it performs well on everything but hese two classes, I suspect this model is overfitting ceratin classes at the cost of generalization. I suspect that this accuracy could be improved by adding some dropout layers to aoid overfitting.","b8357597":"### Define the network architecture\n\nThe various layers that make up any neural network are documented, [here](http:\/\/pytorch.org\/docs\/master\/nn.html). For a convolutional neural network, we'll use a simple series of layers:\n* Convolutional layers\n* Maxpooling layers\n* Fully-connected (linear) layers\n\nYou are also encouraged to look at adding [dropout layers](http:\/\/pytorch.org\/docs\/stable\/nn.html#dropout) to avoid overfitting this data.\n\n---\n\nTo define a neural network in PyTorch, you define the layers of a model in the function `__init__` and define the feedforward behavior of a network that employs those initialized layers in the function `forward`, which takes in an input image tensor, `x`. The structure of this Net class is shown below .\n\nNote: During training, PyTorch will be able to perform backpropagation by keeping track of the network's feedforward behavior and using autograd to calculate the update to the weights in the network.\n\n#### Define the Layers in ` __init__`\nAs a reminder, a conv\/pool layer may be defined like this (in `__init__`):\n```\n# 1 input image channel (for grayscale images), 32 output channels\/feature maps, 3x3 square convolution kernel\nself.conv1 = nn.Conv2d(1, 32, 3)\n\n# maxpool that uses a square window of kernel_size=2, stride=2\nself.pool = nn.MaxPool2d(2, 2)      \n```\n\n#### Refer to Layers in `forward`\nThen referred to in the `forward` function like this, in which the conv1 layer has a ReLu activation applied to it before maxpooling is applied:\n```\nx = self.pool(F.relu(self.conv1(x)))\n```\n\nYou must place any layers with trainable weights, such as convolutional layers, in the `__init__` function and refer to them in the `forward` function; any layers or functions that always behave in the same way, such as a pre-defined activation function, may appear *only* in the `forward` function. In practice, you'll often see conv\/pool layers defined in `__init__` and activations defined in `forward`.\n\n#### Convolutional layer\nThe  convolution layer  it takes in a 1 channel (grayscale) image and outputs 10 feature maps as output, after convolving the image with 3x3 filters.\n\n#### Flattening\n\nRecall that to move from the output of a convolutional\/pooling layer to a linear layer, you must first flatten your extracted features into a vector. If you've used the deep learning library, Keras, you may have seen this done by `Flatten()`, and in PyTorch you can flatten an input `x` with `x = x.view(x.size(0), -1)`.\n\n### TODO: Define the rest of the layers\n\nIt will be up to you to define the other layers in this network; we have some recommendations, but you may change the architecture and parameters as you see fit.\n\nRecommendations\/tips:\n* Use at least two convolutional layers\n* Your output must be a linear layer with 10 outputs (for the 10 classes of clothing)\n* Use a dropout layer to avoid overfitting\n\n### A note on output size\n\nFor any convolutional layer, the output feature maps will have the specified depth (a depth of 10 for 10 filters in a convolutional layer) and the dimensions of the produced feature maps (width\/height) can be computed as the _input image_ width\/height, W, minus the filter size, F, divided by the stride, S, all + 1. The equation looks like: `output_dim = (W-F)\/S + 1`, for an assumed padding size of 0. You can find a derivation of this formula, [here](http:\/\/cs231n.github.io\/convolutional-networks\/#conv).\n\nFor a pool layer with a size 2 and stride 2, the output dimension will be reduced by a factor of 2. Read the comments in the code below to see the output size for each layer.","50a69feb":"### Test the Trained Network\n\nOnce you are satisfied with how the loss of your model has decreased, there is one last step: test!\n\nYou must test your trained model on a previously unseen dataset to see if it generalizes well and can accurately classify this new dataset. For FashionMNIST, which contains many pre-processed training images, a good model should reach **greater than 85% accuracy** on this test dataset. If you are not reaching this value, try training for a larger number of epochs, tweaking your hyperparameters, or adding\/subtracting layers from your CNN.","9ad2f97e":"### Visualize some training data\n\nThis cell iterates over the training dataset, loading a random batch of image\/label data, using `dataiter.next()`. It then plots the batch of images and labels in a `2 x batch_size\/2` grid.","bc8f6df9":"### View an image in more detail\n\nEach image in this dataset is a `28x28` pixel, normalized, grayscale image.\n\n#### A note on normalization\n\nNormalization ensures that, as we go through a feedforward and then backpropagation step in training our CNN, that each image feature will fall within a similar range of values and not overly activate any particular layer in our network. During the feedfoward step, a network takes in an input image and multiplies each input pixel by some convolutional filter weights (and adds biases!), then it applies some activation and pooling functions. Without normalization, it's much more likely that the calculated gradients in the backpropagaton step will be quite large and cause our loss to increase instead of converge.\n","7db51bdb":"#### Data iteration and batching\n\nNext, we'll use ``torch.utils.data.DataLoader`` , which is an iterator that allows us to batch and shuffle the data.\n\nIn the next cell, we shuffle the data and load in image\/label data in batches of size 20.","ea149603":"### Visualize sample test results\n\nFormat: predicted class (true class)","96503600":"# Load and Visualize FashionMNIST\n---\nIn this notebook, we load and look at images from the [Fashion-MNIST database](https:\/\/github.com\/zalandoresearch\/fashion-mnist).\n\nThe first step in any classification problem is to look at the dataset you are working with. This will give you some details about the format of images and labels, as well as some insight into how you might approach defining a network to recognize patterns in such an image set.\n\nPyTorch has some built-in datasets that you can use, and FashionMNIST is one of them; it has already been dowloaded into the `data\/` directory in this notebook, so all we have to do is load these images using the FashionMNIST dataset class *and* load the data in batches with a `DataLoader`.","517edba0":"## Visualizing the loss\n\nA good indication of how much your network is learning as it trains is the loss over time. In this example, we printed and recorded the average loss for each 1000 batches and for each epoch. Let's plot it and see how the loss decreases (or doesn't) over time.\n\nIn this case, you can see that it takes a little bit for a big initial loss decrease, and the loss is flattening out over time.","899bf5eb":"### Question: What are some weaknesses of your model? (And how might you improve these in future iterations.)","81931421":"### Load the [data](http:\/\/pytorch.org\/docs\/master\/torchvision\/datasets.html)\n\n#### Dataset class and Tensors\n\n``torch.utils.data.Dataset`` is an abstract class representing a\ndataset. The FashionMNIST class is an extension of this Dataset class and it allows us to 1. load batches of image\/label data, and 2. uniformly apply transformations to our data, such as turning all our images into Tensor's for training a neural network. *Tensors are similar to numpy arrays, but can also be used on a GPU to accelerate computing.*\n\nLet's see how to construct a training dataset.\n","0bc4ab3c":"### Train the Network\n\nBelow, we've defined a `train` function that takes in a number of epochs to train for. \n* The number of epochs is how many times a network will cycle through the entire training dataset. \n* Inside the epoch loop, we loop over the training dataset in batches; recording the loss every 1000 batches.\n\nHere are the steps that this training function performs as it iterates over the training dataset:\n\n1. Zero's the gradients to prepare for a forward pass\n2. Passes the input through the network (forward pass)\n3. Computes the loss (how far is the predicted classes are from the correct labels)\n4. Propagates gradients back into the network\u2019s parameters (backward pass)\n5. Updates the weights (parameter update)\n6. Prints out the calculated loss\n\n","44dcdef1":"### A note on accuracy\n\nIt's interesting to look at the accuracy of your network **before and after** training. This way you can really see that your network has learned something. In the next cell, let's see what the accuracy of an untrained network is (we expect it to be around 10% which is the same accuracy as just guessing for all 10 classes)."}}