{"cell_type":{"da1e60f9":"code","fd931269":"code","f13e5ce0":"code","dfe67446":"code","e4e51bfd":"code","1cf195b8":"code","c3070a90":"code","39f3ef14":"code","15fcd97c":"code","6b94fd1d":"code","a649dab6":"code","29719e24":"code","dd7b7254":"code","0c907cb4":"code","e24bd49d":"code","f2396451":"code","5a11391c":"code","542ca0c8":"code","7b83ec55":"code","f71fed93":"code","0fab3e30":"code","041229ce":"code","8b6bfc6e":"code","72628b7c":"code","0be9f34c":"code","18fc7428":"code","04ce2b8a":"code","530bd81c":"code","70bfbe9c":"code","8fba9764":"code","d298841e":"code","4c24cc52":"code","51af432d":"code","a3d76bcf":"code","36f89a01":"code","4dcce4f7":"code","28154b13":"code","c1d3b779":"code","582148bb":"code","d0ab1fb3":"code","c497b2f8":"code","7e15a697":"code","d5b78506":"code","af1b42e6":"code","83d10e49":"code","10c2e15e":"code","4dfd2074":"code","fd967315":"code","b7d4b58a":"markdown","3257510b":"markdown","769588e6":"markdown","b639c77c":"markdown","9c931471":"markdown","f001c3ac":"markdown","20568cbe":"markdown","7cb1701e":"markdown","5bdc9916":"markdown","bd426f07":"markdown","f2808f36":"markdown","55038d75":"markdown","ec090bff":"markdown","8a05d630":"markdown","365ee192":"markdown","16535612":"markdown","0e5709b8":"markdown","911ee406":"markdown","38126b10":"markdown","51bf76e0":"markdown","554c62f2":"markdown","9c853ee1":"markdown","2222d3f6":"markdown","29d21c58":"markdown","e2c6e79a":"markdown","9918c65b":"markdown","ecd7f793":"markdown","972a1b90":"markdown","ecb85226":"markdown","b5d64476":"markdown","ad4662e3":"markdown","b8aee3a8":"markdown","084dffdb":"markdown"},"source":{"da1e60f9":"import numpy as np\nimport pandas as pd\nimport csv\nfrom collections import defaultdict","fd931269":"SALES = \"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\"\nPRICES = \"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\"\nCALENDAR = \"..\/input\/m5-forecasting-accuracy\/calendar.csv\"\n\n# SALES = \"..\/data\/raw\/sales_train_validation.csv\"\n# PRICES = \"..\/data\/raw\/sell_prices.csv\"\n# CALENDAR = \"..\/data\/raw\/calendar.csv\"\n\nNUM_SERIES = 30490\nNUM_TRAINING = 1913\nNUM_TEST = NUM_TRAINING + 2 * 28","f13e5ce0":"series_ids = np.empty(NUM_SERIES, dtype=object)\nitem_ids = np.empty(NUM_SERIES, dtype=object)\ndept_ids = np.empty(NUM_SERIES, dtype=object)\ncat_ids = np.empty(NUM_SERIES, dtype=object)\nstore_ids = np.empty(NUM_SERIES, dtype=object)\nstate_ids = np.empty(NUM_SERIES, dtype=object)","dfe67446":"qties = np.zeros((NUM_TRAINING, NUM_SERIES), dtype=float)\nsell_prices = np.zeros((NUM_TEST, NUM_SERIES), dtype=float)","e4e51bfd":"%%time\nid_idx = {}\nwith open(SALES, \"r\", newline='') as f:\n    is_header = True\n    i = 0\n    for row in csv.reader(f):\n        if is_header:\n            is_header = False\n            continue\n        series_id, item_id, dept_id, cat_id, store_id, state_id = row[0:6]\n        # Remove '_validation\/_evaluation' at end by regenerating series_id\n        series_id = f\"{item_id}_{store_id}\"\n\n        qty = np.array(row[6:], dtype=float)\n\n        series_ids[i] = series_id\n\n        item_ids[i] = item_id\n        dept_ids[i] = dept_id\n        cat_ids[i] = cat_id\n        store_ids[i] = store_id\n        state_ids[i] = state_id\n\n        qties[:, i] = qty\n\n        id_idx[series_id] = i\n\n        i += 1","1cf195b8":"%%time\nwm_yr_wk_idx = defaultdict(list)  # map wmyrwk to d:s\nwith open(CALENDAR, \"r\", newline='') as f:\n    for row in csv.DictReader(f):\n        d = int(row['d'][2:])\n        wm_yr_wk_idx[row['wm_yr_wk']].append(d)\n        # TODO: Import the rest of the data","c3070a90":"%%time\nwith open(PRICES, \"r\", newline='') as f:\n    is_header = True\n    for row in csv.reader(f):\n        if is_header:\n            is_header = False\n            continue\n        store_id, item_id, wm_yr_wk, sell_price = row\n        series_id = f\"{item_id}_{store_id}\"\n        series_idx = id_idx[series_id]\n        for d in wm_yr_wk_idx[wm_yr_wk]:\n            sell_prices[d - 1, series_idx] = float(sell_price)","39f3ef14":"qty_ts = pd.DataFrame(qties,\n                      index=range(1, NUM_TRAINING + 1),\n                      columns=[state_ids, store_ids,\n                               cat_ids, dept_ids, item_ids])\n\nqty_ts.index.names = ['d']\nqty_ts.columns.names = ['state_id', 'store_id',\n                        'cat_id', 'dept_id', 'item_id']\n\nprice_ts = pd.DataFrame(sell_prices,\n                        index=range(1, NUM_TEST + 1),\n                        columns=[state_ids, store_ids,\n                                 cat_ids, dept_ids, item_ids])\nprice_ts.index.names = ['d']\nprice_ts.columns.names = ['state_id', 'store_id',\n                          'cat_id', 'dept_id', 'item_id']","15fcd97c":"qty_ts","6b94fd1d":"price_ts","a649dab6":"LEVELS = {\n    1: [],\n    2: ['state_id'],\n    3: ['store_id'],\n    4: ['cat_id'],\n    5: ['dept_id'],\n    6: ['state_id', 'cat_id'],\n    7: ['state_id', 'dept_id'],\n    8: ['store_id', 'cat_id'],\n    9: ['store_id', 'dept_id'],\n    10: ['item_id'],\n    11: ['state_id', 'item_id'],\n    12: ['item_id', 'store_id']\n}","29719e24":"COARSER = {\n    'state_id': [],\n    'store_id': ['state_id'],\n    'cat_id': [],\n    'dept_id': ['cat_id'],\n    'item_id': ['cat_id', 'dept_id']\n}","dd7b7254":"def aggregate_all_levels(df):\n    levels = []\n    for i in range(1, max(LEVELS.keys()) + 1):\n        level = aggregate_groupings(df, i, *LEVELS[i])\n        levels.append(level)\n    return pd.concat(levels, axis=1)\n\ndef aggregate_groupings(df, level_id, grouping_a=None, grouping_b=None):\n    \"\"\"Aggregate time series by summing over optional levels\n\n    New columns are named according to the m5 competition.\n\n    :param df: Time series as columns\n    :param level_id: Numeric ID of level\n    :param grouping_a: Grouping to aggregate over, if any\n    :param grouping_b: Additional grouping to aggregate over, if any\n    :return: Aggregated DataFrame with columns as series id:s\n    \"\"\"\n    if grouping_a is None and grouping_b is None:\n        new_df = df.sum(axis=1).to_frame()\n    elif grouping_b is None:\n        new_df = df.groupby(COARSER[grouping_a] + [grouping_a], axis=1).sum()\n    else:\n        assert grouping_a is not None\n        new_df = df.groupby(COARSER[grouping_a] + COARSER[grouping_b] +\n                            [grouping_a, grouping_b], axis=1).sum()\n\n    new_df.columns = _restore_columns(df.columns, new_df.columns, level_id,\n                                      grouping_a, grouping_b)\n    return new_df","0c907cb4":"def _restore_columns(original_index, new_index, level_id, grouping_a, grouping_b):\n    original_df = original_index.to_frame()\n    new_df = new_index.to_frame()\n    for column in original_df.columns:\n        if column not in new_df.columns:\n            new_df[column] = None\n\n    # Set up `level` column\n    new_df['level'] = level_id\n\n    # Set up `id` column\n    if grouping_a is None and grouping_b is None:\n        new_df['id'] = 'Total_X'\n    elif grouping_b is None:\n        new_df['id'] = new_df[grouping_a] + '_X'\n    else:\n        assert grouping_a is not None\n        new_df['id'] = new_df[grouping_a] + '_' + new_df[grouping_b]\n\n    new_index = pd.MultiIndex.from_frame(new_df)\n    # Remove \"unnamed\" level if no grouping\n    if grouping_a is None and grouping_b is None:\n        new_index = new_index.droplevel(0)\n    new_levels = ['level'] + original_index.names + ['id']\n    return new_index.reorder_levels(new_levels)","e24bd49d":"aggregate_all_levels(qty_ts)","f2396451":"def calculate_weights(totals):\n    \"\"\"Calculate weights from total sales.\n\n    Uses all data in the dataframe so remember to calculate total sales\n    (quantity times sell price) and .\n\n    :param totals: Total sales\n    :return: Series of weights with (level, *_id, id:) as multi-index\n    \"\"\"\n    summed = aggregate_all_levels(totals).sum()\n    \n    return summed \/ summed.groupby(level='level').sum()","5a11391c":"final_month_totals = (qty_ts.loc[NUM_TRAINING - 28 + 1:NUM_TRAINING + 1] *\n                      price_ts.loc[NUM_TRAINING - 28 + 1:NUM_TRAINING + 1])\n\nweights = calculate_weights(final_month_totals)","542ca0c8":"def cumulative_scales(history, f):\n    \"\"\"Calculate column-wise cumulative scales.\n    \n    :param history: Values (in day-order)\n    :param f: Function to apply to differeces, eg., square for RMSSE, abs for SPL\"\"\"\n    # Number of values after the first non-zero\n    ns = (history.cumsum() > 0).cumsum().shift(1, fill_value=0)\n    scales = f(history - history.shift(1)).cumsum() \/ ns\n    \n    # Fill parts where no sales with \u221e (effectively ignore series there)\n    return scales.fillna(np.inf)\n\n\ndef cumulative_squared_scales(history):\n    \"\"\"Calculate column-wise cumulative scales for RMSSE (squared).\"\"\"\n    return cumulative_scales(history, np.square)","7b83ec55":"def calculate_scales(history):\n    \"\"\"Calculate scales using all of history.\"\"\"\n    return cumulative_squared_scales(history).iloc[-1]","f71fed93":"def evaluate_rmsse(actual_full, forecast_full, history_full):\n    scale = calculate_scales(history_full)\n\n    rmsse = ((actual_full - forecast_full).pow(2).mean() \/ scale) \\\n        .pow(1 \/ 2)\n    return rmsse\n\ndef evaluate_all_rmsse(actual, forecast, history):\n    \"\"\"Evaluate per-series RMSSE after aggregation\"\"\"\n    actual_full = aggregate_all_levels(actual)\n    forecast_full = aggregate_all_levels(forecast)\n    history_full = aggregate_all_levels(history)\n\n    return evaluate_rmsse(actual_full, forecast_full, history_full)\n\ndef evaluate_rmsse_wrmsse_per_level(actual, forecast, history, weights):\n    \"\"\"Aggregate series and return per-level RMSSE\"\"\"\n    rmsse = evaluate_all_rmsse(actual, forecast, history)\n    # Average per-series RMSSE over levels\n    return rmsse.mean(level='level'), (weights * rmsse).sum(level='level')","0fab3e30":"final_month = qty_ts.loc[NUM_TRAINING - 28 + 1:NUM_TRAINING + 1]\nfinal_month_noise = np.clip(final_month + np.random.normal(loc=0.0, scale=0.5, size=(28, 30490)), 0, None)","041229ce":"noise_rmsse, noise_wrmsse = evaluate_rmsse_wrmsse_per_level(final_month, final_month_noise, \n                                 qty_ts.loc[:NUM_TRAINING - 28 + 1], weights)","8b6bfc6e":"noise_rmsse","72628b7c":"noise_rmsse.mean()","0be9f34c":"noise_wrmsse","18fc7428":"noise_wrmsse.mean()","04ce2b8a":"qty_train = qty_ts.loc[:NUM_TRAINING - 28 + 1]\nqty_test = qty_ts.loc[NUM_TRAINING - 28 + 1:NUM_TRAINING + 1]\n\ndef evaluate_model(model):\n    model.fit(None, qty_train)\n    qty_pred = model.forecast(None, 28)\n    _, wrmsses = evaluate_rmsse_wrmsse_per_level(qty_test, qty_pred, qty_train, weights)\n    return wrmsses.mean()","530bd81c":"class SeasonalNaive(object):\n    def __init__(self, period):\n        self.period = period\n\n    def fit(self, features, target):\n        self.history = target.iloc[-self.period:]\n        self.d = self.history.index[-1]\n\n        return self\n\n    def forecast(self, features, h):\n        \"\"\"Forecast the next h days\"\"\"\n        fs = []\n        for i in range(h):\n            self.d += 1\n            assert self.history.index[0] + self.period == self.d\n            f = self.history.iloc[0:1]\n            f.index = [self.d]\n            fs.append(f)\n            self.history = self.history.iloc[1:].append(f)\n        return pd.concat(fs)","70bfbe9c":"evaluate_model(SeasonalNaive(7))","8fba9764":"evaluate_model(SeasonalNaive(28))","d298841e":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm","4c24cc52":"ACTIVATION = {\n    'relu': F.relu,\n    'tanh': F.tanh,\n    'sigmoid': F.sigmoid,\n    'linear': lambda x: x\n}\n\nACTIVATION_FUNCTIONS = list(ACTIVATION.keys())","51af432d":"def hidden_init(layer):\n    fan_in = layer.weight.data.size()[0]\n    lim = 1. \/ np.sqrt(fan_in)\n    return (-lim, lim)\n\n\nclass Network(nn.Module):\n    def __init__(self, lookback, layer_1_size, layer_1_activation, layer_2_size,\n                 layer_2_activation):\n        \"\"\"Initialize parameters and build model.\"\"\"\n        super().__init__()\n        self.fc1 = nn.Linear(lookback, layer_1_size)\n        self.d1 = nn.Dropout()\n        self.f1 = ACTIVATION[layer_1_activation]\n\n        self.fc2 = nn.Linear(layer_1_size, layer_2_size)\n        self.f2 = ACTIVATION[layer_2_activation]\n        self.d2 = nn.Dropout()\n\n        self.fc3 = nn.Linear(layer_2_size, FORECAST_DAYS)\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        \"\"\"Initializes the weights with random values\"\"\"\n        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n\n    def forward(self, qties):\n        x = self.fc1(qties)\n        x = self.d1(x)\n        x = self.f1(x)\n\n        x = self.fc2(x)\n        x = self.d2(x)\n        x = self.f2(x)\n\n        x = self.fc3(x)\n\n        return x","a3d76bcf":"def rmsse_loss(input, target, scales):\n    return (((input - target)**2 \/ scales).sum() \/ input.data.nelement()).sqrt()\n\nFORECAST_DAYS = 28\n\nclass NeuralNet(object):\n    def __init__(self, lookback,\n                 layer_1_size, layer_1_activation,\n                 layer_2_size, layer_2_activation,\n                 batches, shuffle,\n                 epochs,\n                 device):\n        self.device = device\n\n        self.lookback = lookback\n        self.layer_1_size = layer_1_size\n        self.layer_1_activation = layer_1_activation\n        self.layer_2_size = layer_2_size\n        self.layer_2_activation = layer_2_activation\n        self.batches = batches\n        self.shuffle = shuffle\n\n        self.epochs = epochs\n\n    def fit(self, features, target):\n        \"\"\"Attempts to predict the last 28 days\"\"\"\n        y = (target.iloc[-(FORECAST_DAYS + self.batches):].values\n             .transpose())\n        X = (target.iloc[-(FORECAST_DAYS + self.lookback\n                           + self.batches):-FORECAST_DAYS]\n             .values.transpose())\n\n        y = torch.from_numpy(y).float().to(self.device)\n        X = torch.from_numpy(X).float().to(self.device)\n\n        # Calculate scales (remember to avoid leaks from the future!)\n        scales = cumulative_squared_scales(target) \\\n                     .values[\n                 -(FORECAST_DAYS + self.batches):-(FORECAST_DAYS - 1)]\n        scales = scales.transpose()\n        scales = torch.from_numpy(scales).float().to(self.device)\n\n\n        net = Network(self.lookback,\n                      self.layer_1_size,\n                      self.layer_1_activation,\n                      self.layer_2_size,\n                      self.layer_2_activation).to(self.device)\n        self.net = net\n\n        optimizer = optim.Adam(net.parameters())\n\n        for epoch in tqdm(range(self.epochs)):\n            running_loss = 0.0\n\n            batch_idxs = np.arange(self.batches + 1)\n            if self.shuffle:\n                np.random.shuffle(batch_idxs)\n            for i in batch_idxs:\n                optimizer.zero_grad()\n\n                X_run = X[:, i:(i + self.lookback)]\n                y_run = y[:, i:(i + FORECAST_DAYS)]\n                scales_run = scales[:, i:(i + 1)]\n\n\n                forecast = net(X_run)\n\n                loss = rmsse_loss(forecast, y_run, scales_run)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n\n            mean_loss = running_loss \/ self.batches\n            # print(f\"Epoch {epoch + 1}: Loss {mean_loss:.2f}\")\n\n        # Store history\n        self.history = target.iloc[-self.lookback:]\n        self.d = self.history.index[-1]\n\n        return self\n\n    def forecast(self, features, h):\n        # For now, only handle full period\n        assert h == FORECAST_DAYS\n\n        assert h <= FORECAST_DAYS\n\n        with torch.no_grad():\n            X = self.history.values.transpose()\n            X = torch.from_numpy(X).float().to(self.device)\n            forecast = self.net(X).cpu().numpy()\n\n            forecast = forecast.transpose()\n            self.d += 1\n\n            # TODO: Update self.d properly\n\n            forecast = pd.DataFrame(forecast,\n                                    index=range(self.d, self.d + h),\n                                    columns=self.history.columns)\n            # Remove any negative values\n            forecast = forecast.clip(lower=0)\n\n            # TODO: Truncate to h days only and store into history\n            self.d += 1 + h\n            self.history = forecast\n            return forecast","36f89a01":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","4dcce4f7":"nnet = NeuralNet(lookback=140, \n                 layer_1_size=512, layer_1_activation='relu',\n                 layer_2_size=256, layer_2_activation='relu',\n                 batches=7, shuffle=True, epochs=64, \n                 device=device)","28154b13":"evaluate_model(nnet)","c1d3b779":"from functools import reduce\nimport operator\n\nclass Ensemble(object):\n    def __init__(self, models):\n        self.models = models\n    \n    def fit(self, features, target):\n        for model in self.models:\n            model.fit(features, target)\n    \n    def forecast(self, features, h):\n        return reduce(operator.add, \n                      [model.forecast(features, h) for model in self.models]) \/ len(self.models)","582148bb":"naive_ensemble = Ensemble([SeasonalNaive(7), SeasonalNaive(28)])","d0ab1fb3":"evaluate_model(naive_ensemble)","c497b2f8":"large_ensemble = Ensemble([naive_ensemble, nnet])","7e15a697":"evaluate_model(large_ensemble)","d5b78506":"huge_ensemble =  Ensemble([\n    SeasonalNaive(7), \n    SeasonalNaive(14), \n    SeasonalNaive(21), \n    SeasonalNaive(28),\n    SeasonalNaive(56),\n    NeuralNet(lookback=140,\n              layer_1_size=512, layer_1_activation='relu',\n              layer_2_size=256, layer_2_activation='relu',\n              batches=7, shuffle=True, epochs=64, \n              device=device),\n    NeuralNet(lookback=365, \n              layer_1_size=1024, layer_1_activation='relu',\n              layer_2_size=512, layer_2_activation='relu',\n              batches=140, shuffle=True, epochs=64, \n              device=device)])","af1b42e6":"evaluate_model(huge_ensemble)","83d10e49":"%%time\nhuge_ensemble.fit(None, qty_ts)\nqty_pred = huge_ensemble.forecast(None, 28)","10c2e15e":"def convert_to_submission(forecast):\n    \"\"\"Convert level 12-predictions to submssion\"\"\"\n    df = aggregate_all_levels(qty_pred)\\\n        .transpose()\\\n        .reset_index(level=['level', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id'],\n                    drop=True)\n    df.columns = [f\"F{i}\" for i in range(1, 29)]\n    validation = df\n    evaluation = df.copy()\n    \n    validation.index += \"_validation\"\n    evaluation.index += \"_evaluation\"\n    \n    return pd.concat([validation, evaluation])","4dfd2074":"submission = convert_to_submission(qty_pred)","fd967315":"# You can't submit zip-files directly from notebooks, otherwise one could use this instead:\n# submission.to_csv(\"submission.zip\")\nsubmission.to_csv(\"submission.csv\")","b7d4b58a":"## Submission","3257510b":"We can also include our neural net in the mix.  Note that by reusing the previous ensemble, we're essentially given the seasonal na\u00efve predictors individual weights of 0.25, and the neural net 0.5.  We could try to optimize the weights, but we probably want a better validation scheme for that.","769588e6":"### Building DataFrame\n\nWe'll store the dataset in two dataframes:\n\n- **`qty_ts`:** sales data.\n- **`price_ts`:** prices.","b639c77c":"## Benchmarks\n\nWe'll evaluate the model using the final month of the training data as our validation set.  For better CV, \nwe should really split the data into more pieces, but then we have to take into account the way different\nitems are introduced in different stores at different times.  For the final month, we know that all items\nhave been available for at least a couple of months.","9c931471":"And if we look at the data, we see how the series are organized into columns:","f001c3ac":"### Importing and reshaping sales data\n\nEach row in the sales data consists of six columns for an id of the series together with the five levels item, department, category, store, and, state.","20568cbe":"Let's use the huge ensemble above and create a submission using it.","7cb1701e":"The model has a couple of parameters: \n\n- the network architecture is described by the size and activation functions of the layers, \n- the number of previous steps the model uses in order to forecast the future is given by `lookback`, and,\n- the model trains on all series for a series of `batches`, each offset by one day.\n\n(Handling scales and making sure all tensors have the correct size is kind of tricky.)","5bdc9916":"It seems likely that an ensemble of seasonal na\u00efve predictors would perform better than a single and it turns out to be quite an improvement!:","bd426f07":"For fun, we can take the sales data for the final month and add some noise and see how large RMSSE and WRMSSE that gives us. (This could be useful in order to get an idea of how good our predictions are.)","f2808f36":"### Ensemble\n\nAnother fun experiment we can make is to create simple ensemble that take the mean of all forecasts by some models.","55038d75":"A quick peek at the aggregated sales data:","ec090bff":"In this competition, our models are evaluated on 12 different levels defined by combinations of the groupings of the series.  \n\nIt is important that we can aggregate our time series, eg., calculate the total sales in each state, so that\nwe can evaluate a model's per-store item sales data forecasts on every level.\n\nThe levels used in the competition are:","8a05d630":"### Scales\n\nFor the Root Mean Squared Scale Error metric used in the competion, we need to compute scales using time series data up to the forecast.  \n\n> **NB.** We have to be careful not to use scale values using data from after the forecasting has begun since that would leak information from the future.  For the same reason, we can't use the weights we calculated above during training since they are based on the last period.\n\nFor each day, we calculate the scales of all series upto that day.  Scales are essentially defined by the mean squared difference between consecutive days (the Scaled Pinball Loss used in the companion to this competition uses absolute differences instead of squared).","365ee192":"I have run this ensemble a number of times, and sometimes it performs a lot better than the na\u00efve ensemble, and other times it performs worse.  Even though our validation isn't really good enough, it seems as if we could squeeze out some extra performance using a larger ensemble. \n\nJust for fun, let's try a much larger one:","16535612":"### Neural Net\n\nAs a hopefully more interesting example of how we might use the data organized in this form, let's try a simple 2-layer neural network using PyTorch.\n\nWe'll implement a custom RMSSE loss and try to make sure we move all of our data into the GPU before training.\n\nLet's assume that the relationship between past and future values are the same for all series, so that we can just bundle up all of the series and train the model on all at once.  The model will forecast all 28 days at once.\n\n> Since this is just meant as a simple example, not much thought has been put into the model, and no effort has been\n> made to make results reproducible.","0e5709b8":"### Importing calendar data\n\nThe calendar data has information about which day of the week a given day is, if there are any special events, and most importantly for this notebook, which week (`wm_yr_wk`) the day is in.  We'll need this to get the prices of items, which in turn is necessary in order to calculate the weights we need for estimating our scores.","911ee406":"A small complication is that Pandas doesn't align during column-wise concatenation, ie., if two dataframes have some different column levels, `pd.concat` does not match levels that are the same between the frames.\n\nThe easiest solution is to add back the levels we lost after grouping for now.","38126b10":"### Importing price data\n\nThe price data describes the weekly prices for each item in every store.","51bf76e0":"Pandas views all column levels as independent, but here they are not; all series with the same `dept_id` belong to the same `cat_id`, for example.  When grouping our columns, we'll also keep any coarser groupings.","554c62f2":"## Create dataset\n\nUsing Pandas directly to read the data and reshape it appears to be a bit slow and uses a significant amount of memory.  Instead we'll read the data line by line and store it in NumPy arrays (but we'll try and keep the rest of the code in the notebook nicely vectorized and high-level =).","9c853ee1":"## Aggregation","2222d3f6":"(We can compare the weights with [the validation weights in the M5 repo](https:\/\/raw.githubusercontent.com\/Mcompetitions\/M5-methods\/master\/validation\/weights_validation.csv) to check that the weights have been calculated correctly.)","29d21c58":"### RMSSE and WRMSSE\n\nThe metric in this competition sort of compares the models performance to a naive model that always predicts that the next day will be the same as the current day:\n\n$$\n\\mathrm{RMSSE} = \\sqrt{\n\\frac{1}{h} \\frac{\\sum_{t = n + 1}^{n + h}(Y_t - \\hat{Y}_t)^2}{\\frac{1}{n - 1}\\sum_{t = 2}^{n}(Y_t - Y_{t - 1})^2}\n}.\n$$\n\n$Y_t$ is the actual value at $t$, $\\hat{Y}_t$ the forecasted value, $n$ the number of values, and, $h$ the forecasting horizon.","e2c6e79a":"### Weights\n\nThe scoring takes into account the final month's total sales and weights the series on every level accordingly.","9918c65b":"The data for the competition consists primarily of 30490 time series of sales data for 3049 items sold in 10 different stores in 3 states.  The items are classified as being in one of 3 categories that are further subdivided into a total of 7 departments.\n\nThe representation we'll look at in this notebook is representing each individual time series as a column in a data frame indexed by the day (`d`).\n\nFor the individual (level 12 series), we'll index the series in the columns by `(state_id, store_id, cat_id, dept_id, item_id)`.","ecd7f793":"> I'm assuming some familiarity with the competition and its data.  If you're just starting out, I'd recommend\n> [Heads or Tails'](https:\/\/www.kaggle.com\/headsortails) excellent [EDA kernel](https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda).","972a1b90":"There is some seasonality in the data, for example, weekly and 4-weekly.  Lets try and take advantage of it.","ecb85226":"## Evaluation","b5d64476":"### Seasonal Na\u00efve\n\nLet's start with a simple model which repeats the last `period` observations. ","ad4662e3":"# Efficient Handling of Hierarchical Time Series using Pandas Multi-Indices\n\n- Using Pandas multi-level indexing for convenient manipulation of hierarchical time-series.\n- Fast and memory-efficent loading of raw dataset (uses 1.5\u20132 GB of RAM during processing).\n- Calculation of RMSSE and WRMSSE.\n- Seasonal na\u00efve and simple neural net (using PyTorch) benchmarks.\n\nI haven't had much use of Pandas' multi-level indexing in the past, but I figured it might be worth giving them a shot for this competition.  Turns out they're pretty handy for hierarchical time series!  \n\n(I saw that someone in the competition has written a package for HTS which I intend to take a look at some time, any\nother pointers are also appreciated!)","b8aee3a8":"> **NB.** I'm writing this notebook when the public leaderboard is based on the actual final month (strictly speakin, the final 28 day period) of the training data, therefore the weights are actually calculated using the month before that.  A bit confusing, I know.","084dffdb":"Let's try some random values for the parameters and see what kind of performance we get:"}}