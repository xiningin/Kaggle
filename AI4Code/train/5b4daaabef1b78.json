{"cell_type":{"e112ccdc":"code","599c454d":"code","25903ae6":"code","3fa684eb":"code","f78ba971":"code","2699182a":"code","72ba2b67":"code","5a694525":"code","d864704c":"code","96516e5e":"code","c6d2a711":"code","188dc6ff":"code","e47489b1":"code","b88e5144":"code","208f137c":"code","611a58ec":"code","bbe1f43f":"code","0d96f850":"markdown"},"source":{"e112ccdc":"# https:\/\/www.kaggle.com\/qgh1223\/inceptionv3-siamesenet\nfrom keras.applications.vgg16 import VGG16\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nimport random\n# Read or generate p2h, a dictionary of image name to image id (picture to hash)\nimport pickle\nimport platform\nimport random\n# Determine the size of each image\nfrom os.path import isfile","599c454d":"from keras.models import Model,Sequential\nfrom keras.layers import *\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.mobilenetv2 import MobileNetV2\n","25903ae6":"IMG_ROW=IMG_COL=144\nBASE_DIR='..\/input\/humpback-whale-identification\/train\/'\nlabelpath='..\/input\/humpback-whale-identification\/train.csv'\ntraindata=pd.read_csv(labelpath)","3fa684eb":"# https:\/\/www.kaggle.com\/khaled34\/siamese-0-822\nTRAIN_DF = '..\/input\/humpback-whale-identification\/train.csv'\nSUB_Df = '..\/input\/humpback-whale-identification\/sample_submission.csv'\nTRAIN = '..\/input\/humpback-whale-identification\/train\/'\nTEST = '..\/input\/humpback-whale-identification\/test\/'\nP2H = '..\/input\/humpbackwhaleidentificationmetadata\/p2h.pickle'\nP2SIZE = '..\/input\/humpbackwhaleidentificationmetadata\/p2size.pickle'\nBB_DF = \"..\/input\/humpbackwhaleidentificationmetadata\/bounding_boxes.csv\"\ntagged = dict([(p, w) for _, p, w in pd.read_csv(TRAIN_DF).to_records()])","f78ba971":"if isfile(P2SIZE):\n    print(\"P2SIZE exists.\")\n    with open(P2SIZE, 'rb') as f:\n        p2size = pickle.load(f)\nelse:\n    p2size = {}\n    for p in tqdm(join):\n        size = pil_image.open(expand_path(p)).size\n        p2size[p] = size\n\n","2699182a":"\ndef match(h1, h2):\n    for p1 in h2ps[h1]:\n        for p2 in h2ps[h2]:\n            i1 = pil_image.open(expand_path(p1))\n            i2 = pil_image.open(expand_path(p2))\n            if i1.mode != i2.mode or i1.size != i2.size: return False\n            a1 = np.array(i1)\n            a1 = a1 - a1.mean()\n            a1 = a1 \/ sqrt((a1 ** 2).mean())\n            a2 = np.array(i2)\n            a2 = a2 - a2.mean()\n            a2 = a2 \/ sqrt((a2 ** 2).mean())\n            a = ((a1 - a2) ** 2).mean()\n            if a > 0.1: return False\n    return True\nif isfile(P2H):\n    print(\"P2H exists.\")\n    with open(P2H, 'rb') as f:\n        p2h = pickle.load(f)\n# For each image id, determine the list of pictures\nh2ps = {}\nfor p, h in p2h.items():\n    if h not in h2ps: h2ps[h] = []\n    if p not in h2ps[h]: h2ps[h].append(p)\n#len(p2h), list(p2h.items())[:5]\nprint(len(p2h))\nprint(len(h2ps))\n\n","72ba2b67":"def build_siamese_model( img_shape,lr,branch_model,activation='sigmoid'):\n\n    optim  = Adam(lr=lr)\n    \n    \n    \n    mid        = 32\n    xa_inp     = Input(shape=branch_model.output_shape[1:])\n    xb_inp     = Input(shape=branch_model.output_shape[1:])\n    x1         = Lambda(lambda x : x[0]*x[1])([xa_inp, xb_inp])\n    x2         = Lambda(lambda x : x[0] + x[1])([xa_inp, xb_inp])\n    x3         = Lambda(lambda x : K.abs(x[0] - x[1]))([xa_inp, xb_inp])\n    x4         = Lambda(lambda x : K.square(x))(x3)\n    x          = Concatenate()([x1, x2, x3, x4])\n    x          = Reshape((4, branch_model.output_shape[1], 1), name='reshape1')(x)\n\n    # Per feature NN with shared weight is implemented using CONV2D with appropriate stride.\n    x          = Conv2D(mid, (4, 1), activation='relu', padding='valid')(x)\n    x          = Reshape((branch_model.output_shape[1], mid, 1))(x)\n    x          = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n    x          = Flatten(name='flatten')(x)\n    x          = Dense(1, use_bias=True, activation=activation, name='weighted-average')(x)\n    head_model = Model([xa_inp, xb_inp], x, name='head')\n    img_a      = Input(shape=img_shape)\n    img_b      = Input(shape=img_shape)\n    xa         = branch_model(img_a)\n    xb         = branch_model(img_b)\n    x          = head_model([xa, xb])\n    model      = Model([img_a, img_b], x)\n    model.compile(optim, loss='binary_crossentropy', metrics=[ 'accuracy'])\n    return model,head_model","5a694525":"def kind_list(imgdata):\n    kindlist=imgdata.groupby('Id').size()\n    return kindlist.index\n\ndef fetch_all_kind_list(imgdata):\n    kindlist=kind_list(imgdata)\n    kindimgpathlist=[]\n    for kind in kindlist:\n        kindimgpathlist.append(list(imgdata['Image'][imgdata['Id']==kind]))\n    return kindimgpathlist,kindlist\n\ndef fetch_kind_list_split(kindimgpathlist,split_size=0.8):\n    trainkindimgpathlist=[]\n    validkindimgpathlist=[]\n    for pathlist in kindimgpathlist:\n        if(len(pathlist)<=3):\n            trainkindimgpathlist.append(pathlist)\n            validkindimgpathlist.append(pathlist)\n        else:\n            trainkindimgpathlist.append(pathlist[:int(len(pathlist)*split_size)])\n            validkindimgpathlist.append(pathlist[int(len(pathlist)*split_size):])\n    return trainkindimgpathlist,validkindimgpathlist","d864704c":"def imgarr(imgpath):\n    img=cv2.imread(imgpath)\n    return img","96516e5e":"def siamese_img_gen(BASE_DIR,IMG_ROW,IMG_COL,kindimgpathlist,\n                    contrast_times=5,batch_size=50):\n    while(True):\n        imglist1=[]\n        imglist2=[]\n        labellist=[]\n        for i in range(batch_size):\n            for j in range(contrast_times):\n                rndid=random.randint(0,len(kindimgpathlist)-1)\n                if(i%2==0):\n                    #print(len(kindimgpathlist[rndid]))\n                    pair=np.random.randint(0,len(kindimgpathlist[rndid]),2)\n                    imgpath1=kindimgpathlist[rndid][pair[0]]\n                    imgpath2=kindimgpathlist[rndid][pair[1]]\n                    labellist.append(1)\n                else:\n                    rndid1=random.randint(0,len(kindimgpathlist[rndid])-1)\n                    imgpath1=kindimgpathlist[rndid][rndid1]\n                    index1=random.choice([num for num in range(len(kindimgpathlist)) if num not in [rndid]])\n                    rndid2=random.randint(0,len(kindimgpathlist[index1])-1)\n                    imgpath2=kindimgpathlist[index1][rndid2]\n                    labellist.append(0)\n                img1=imgarr(BASE_DIR+imgpath1)\n                img2=imgarr(BASE_DIR+imgpath2)\n                img1=cv2.resize(img1,(IMG_ROW,IMG_COL))\n                img2=cv2.resize(img2,(IMG_ROW,IMG_COL))\n                imglist1.append(img1)\n                imglist2.append(img2)\n        yield ([np.asarray(imglist1),np.asarray(imglist2)],np.asarray(labellist))","c6d2a711":"img_shape=(IMG_ROW,IMG_COL,3)\nmodelfn=InceptionV3(weights=None,\n                   input_shape=img_shape,\n                   classes=300)","188dc6ff":"model,head_model = build_siamese_model(img_shape,64e-5,modelfn)\nmodel.summary()\nmodel.summary()\nmodel.compile(optimizer=Adam(0.001),metrics=['accuracy'],\n              loss=['binary_crossentropy'])\ncallbacks=[\n    ReduceLROnPlateau(monitor='val_loss',patience=5,min_lr=1e-9,verbose=1,mode='min'),\n    ModelCheckpoint('siamese.h5',monitor='val_loss',save_best_only=True,verbose=1)\n]","e47489b1":"kindimgpathlist,kindlist=fetch_all_kind_list(traindata)\ntrainkindimgpathlist,validkindimgpathlist=fetch_kind_list_split(kindimgpathlist)","b88e5144":"history=model.fit_generator(siamese_img_gen(BASE_DIR,IMG_ROW,IMG_COL,\n                                            trainkindimgpathlist,batch_size=30),\n                            steps_per_epoch=2,\n                            epochs=1,\n                            validation_data=siamese_img_gen(BASE_DIR,IMG_ROW,IMG_COL,\n                                                            validkindimgpathlist,contrast_times=10,batch_size=5),\n                            validation_steps=2,\n                            callbacks=callbacks)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','valid'])\nplt.show()","208f137c":"# https:\/\/www.kaggle.com\/khaled34\/siamese-0-822\ndef prepare_submission(threshold, filename):\n    \"\"\"\n    Generate a Kaggle submission file.\n    @param threshold the score given to 'new_whale'\n    @param filename the submission file name\n    \"\"\"\n    vtop = 0\n    vhigh = 0\n    pos = [0, 0, 0, 0, 0, 0]\n    with open(filename, 'wt', newline='\\n') as f:\n        f.write('Image,Id\\n')\n        for i, p in enumerate(tqdm(submit)):\n            t = []\n            s = set()\n            a = score[i, :]\n            for j in list(reversed(np.argsort(a))):\n                h = known[j]\n                if a[j] < threshold and new_whale not in s:\n                    pos[len(t)] += 1\n                    s.add(new_whale)\n                    t.append(new_whale)\n                    if len(t) == 5: break;\n                for w in h2ws[h]:\n                    assert w != new_whale\n                    if w not in s:\n                        if a[j] > 1.0:\n                            vtop += 1\n                        elif a[j] >= threshold:\n                            vhigh += 1\n                        s.add(w)\n                        t.append(w)\n                        if len(t) == 5: break;\n                if len(t) == 5: break;\n            if new_whale not in s: pos[5] += 1\n            assert len(t) == 5 and len(s) == 5\n            f.write(p + ',' + ' '.join(t[:5]) + '\\n')\n    return vtop, vhigh, pos","611a58ec":"# https:\/\/www.kaggle.com\/khaled34\/siamese-0-822\n# Find elements from training sets not 'new_whale'\ntic = time.time()\nnew_whale = 'new_whale'\nh2ws = {}\nfor p, w in tagged.items():\n    if w != new_whale:  # Use only identified whales\n        h = p2h[p]\n        if h not in h2ws: h2ws[h] = []\n        if w not in h2ws[h]: h2ws[h].append(w)\nknown = sorted(list(h2ws.keys()))\n\n# Dictionary of picture indices\nh2i = {}\nfor i, h in enumerate(known): h2i[h] = i\n\n# Evaluate the model.\nfknown = model.predict_generator(siamese_img_gen(BASE_DIR,IMG_ROW,IMG_COL,\n                                            trainkindimgpathlist,batch_size=30),                                                                                    \n                            steps=1,\n                            callbacks=callbacks,\n                            verbose=0)\n\"\"\"\nfsubmit = branch_model.predict_generator(FeatureGen(submit), max_queue_size=20, workers=10, verbose=0)\nscore = head_model.predict_generator(ScoreGen(fknown, fsubmit), max_queue_size=20, workers=10, verbose=0)\nscore = score_reshape(score, fknown, fsubmit)\n\"\"\"\n\n# Generate the subsmission file.\nprepare_submission(0.99, 'submission.csv')\ntoc = time.time()\nprint(\"Submission time: \", (toc - tic) \/ 60.)","bbe1f43f":"modelfn.save('mobile_encoder.h5')","0d96f850":"Duplicate image identification\nThis part was from the original kernel, seems like in the playground competition dulicated images was a real issue. I don't know the case about this one but I took one for the team and generated the results anyway. I'm such a nice chap.\n"}}