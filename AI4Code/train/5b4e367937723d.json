{"cell_type":{"f516ffcc":"code","0f71c8f0":"code","1d9554aa":"code","e3f8d5e5":"code","550e0144":"code","8ddcda54":"code","bd01747a":"code","8222667a":"code","92b14a88":"code","13d5e7a8":"code","327383fb":"code","ed639c74":"code","9e1f74bb":"code","098e75b0":"code","fc2c2204":"code","16813cf0":"code","70c84c49":"code","af3b888f":"code","9ba4cdf9":"code","6d5502e0":"code","9575825f":"code","cb02dc4d":"code","07c0c4b7":"code","adc56d7e":"code","6e0bb180":"code","a2964428":"code","bd2e52eb":"code","d51e63e3":"code","840ef576":"code","7cced4c5":"code","1c61cb58":"code","ff4adefc":"code","856748ee":"code","15a7c8a5":"code","9278acdd":"code","99160589":"code","8c8975e7":"code","8d53af33":"markdown","26fbe073":"markdown","ab75d2bb":"markdown","8882673b":"markdown","6a029d19":"markdown","92e33171":"markdown","8f2225f2":"markdown","292f5998":"markdown","0b828e00":"markdown"},"source":{"f516ffcc":"## Basics libraries\nimport numpy as np\nimport pandas as pd\n# pd.set_option('display.max_colwidth', -1)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n## Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nimport pyLDAvis\nimport pyLDAvis.gensim\n\nimport networkx as nx\n\n## Web Scrapping libraries\nfrom requests import get\nfrom bs4 import BeautifulSoup\n\nimport time\nfrom time import sleep\nfrom IPython.core.display import clear_output\nfrom random import randint\nfrom warnings import warn\n\n## Data Cleaning libraries\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport re\nimport string\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\n\nimport gensim\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim.corpora import Dictionary\n\n## Machine Learning libraries\nfrom gensim.models import LdaModel, CoherenceModel\nfrom scipy.spatial import distance","0f71c8f0":"## Copying the url of the most popular series from IMDb\nurl = \"https:\/\/www.imdb.com\/chart\/tvmeter\"\n\n## Making a request to the web site with and alert in case of failure\nresponse = get(url)\nif response.status_code != 200:\n    warn('Request for series information: Status code: {0}'.format(response.status_code))  \n\n## Parsing the html and filtering the content\nhtml_soup = BeautifulSoup(response.text, 'html.parser')\nmovie_container = html_soup.find_all(\"td\", class_=(\"titleColumn\",\"ratingColumn imdbRating\"))\n\n## Generating empty list for information filling\nurls = []\nnames = []\nyears_begins = []\npopular_rankings = []\nimdb_ratings = []\npopularities_trends = []\n\nfor i in range(0,len(movie_container)):\n    if i%2==0:\n        print(\"Loop de la serie n\u00famero\", i)\n        clear_output(wait=True)\n        \n        ## Url title\n        url_titulo = str(movie_container[i].a[\"href\"][7:-1])\n        urls.append(url_titulo)\n        \n        ## Name \n        name = str(movie_container[i].a.text).lower()\n        names.append(name)\n        \n        ## Release year\n        if len(movie_container[i].span.text[1:-1]) == 4:\n            year_begin = str(movie_container[i].span.text[1:-1])\n            years_begins.append(year_begin)\n        else:\n            year_begin = np.nan\n            years_begins.append(year_begin)\n            \n        ## Popularity ranking\n        popular_ranking = int(movie_container[i].div.text.split(\"\\n\")[0])\n        popular_rankings.append(popular_ranking)\n        \n        ## Popularity trend\n        neg = str(movie_container[i].find(\"span\", class_=\"global-sprite titlemeter down\")) \n        pos = str(movie_container[i].find(\"span\", class_=\"global-sprite titlemeter up\")) \n        tendency = neg + pos\n\n        if \"down\" in tendency:\n            temp=\"-\"\n        elif \"up\" in tendency: \n            temp=\"+\"\n        else:\n            temp=\"\"\n        if \"no change\" in movie_container[i].div.text:\n            trend = movie_container[i].div.text.replace(\"no change\", \"0\").split(\"(\")[1].split(\")\")[0]\n        else:\n            trend = movie_container[i].div.text.replace(\",\", \"\").split(\"\\n\\n\")[1].split(\")\")[0]\n            \n        popularity_trend = int(temp + trend)\n        popularities_trends.append(popularity_trend)\n                              \n    if i%2!=0 and len(str(movie_container[i].text)) != 1:\n        print(\"Loop de la serie n\u00famero\", i)\n        clear_output(wait=True)\n\n        ## IMDb rating\n        imdb_rating = float(str(movie_container[i].text[1:4]).replace(\",\",\".\"))\n        imdb_ratings.append(imdb_rating)\n           \n    elif i%2!=0 and len(str(movie_container[i].text)) == 1:\n        print(\"Loop de la serie n\u00famero\", i)\n        clear_output(wait=True)\n\n        ## Filling with NaN\n        imdb_rating = np.nan\n        imdb_ratings.append(imdb_rating)\n\nprint(\"Bucle finalizado\")","1d9554aa":"## Creating a dataframe with the scrapped data\ndata_series = pd.DataFrame({'urls' : urls,\n                            'names' : names,\n                            'years_begins' : years_begins,\n                            'popular_rankings' : popular_rankings,\n                            'popularities_trends' : popularities_trends,\n                            'imdb_ratings' : imdb_ratings}, )\nprint(data_series.info())\n\ndata_series.sample(5)","e3f8d5e5":"## Generating empty list for information filling\nusers_names = []\nusers_ratings = []\ncomments_dates = []\ncomments_titles = []\nusers_texts = []\nurls_comments = []\n\nrequests=0\nstart_time_title = time.time()\n\n## Making a loop\nfor url_title in urls:\n    url = \"https:\/\/www.imdb.com\/title\/\"+url_title+\"\/reviews\"\n    \n    ## Making a request to the web site with and alert in case of failure\n    response = get(url)\n    if response.status_code != 200:\n        warn('Request for new serie: {0}; Status code: {1}'.format(requests, response.status_code))    \n    \n    ## Stopping the loop for a few seconds and controling the frequency of requests\n    requests += 1\n    sleep(randint(1,3))\n    elapsed_time = time.time() - start_time_title\n    print('Request for new serie: {0}; Frequency: {1:.4} requests\/s'.format(requests, requests\/elapsed_time))\n    clear_output(wait=True)    \n    \n    ## Parsing the html and filtering the content\n    html_soup = BeautifulSoup(response.text, 'html.parser')\n    comment_container = html_soup.find_all(\"div\", class_=(\"header\", \"review-container\", \"load-more-data\"))\n\n    ## Obtanining the total number of comments\n    number_comments = int(str(comment_container[0].div.text).replace(\",\",\"\").replace(\"Reviews\",\"\"))\n   \n    ## Initializing the parameters of the loop\n    n1=1\n    n2=0\n    requests_ajax=0\n    start_time_comment = time.time()\n    length = len(comment_container)-2\n    flag = True\n    \n    ## Obtaining the URL-AJAX\n    data_ajaxurl = comment_container[length][\"data-ajaxurl\"]\n\n    ## Creagint a nested loop\n    while (n2 < number_comments and flag):        \n        for i in range(n1, length):\n            n2+=1\n            ## User rating\n            if comment_container[i].find(\"span\", class_=\"rating-other-user-rating\") is not None:\n                user_rating = int(comment_container[i].find(\"span\", class_=\"rating-other-user-rating\").span.text)\n                users_ratings.append(user_rating)\n\n                ## User name\n                user_name = comment_container[i].find(\"span\", class_=\"display-name-link\").text\n                users_names.append(user_name)\n\n                ## Comment date\n                comment_date = comment_container[i].find(\"span\", class_=\"review-date\").text\n                comments_dates.append(comment_date)\n\n                ## Comment title\n                comment_title = comment_container[i].find(class_=\"title\").text[1:-1]\n                comments_titles.append(comment_title)\n\n                ## User comment\n                user_text = comment_container[i].find(class_=\"text show-more__control\").text\n                users_texts.append(user_text)\n\n                ## URL title\n                urls_comments.append(url_title)\n        \n        ## URL-AJAX key\n        if \"data-key\" in str(comment_container[length]):\n            data_key= comment_container[length][\"data-key\"]\n            \n            ## Generating a new URL with more comments\n            new_url = \"http:\/\/www.imdb.com\/{0}?paginationKey={1}\".format(data_ajaxurl, data_key)\n        \n            ## Making a request to the web site with and alert in case of failure\n            response = get(new_url)\n            if response.status_code != 200:\n                warn('Request for more comments: {0}.{1}; Status code: {2}'.format(requests, requests_ajax, response.status_code))    \n        \n            ## Stopping the loop for a few seconds and controling the frequency of requests\n            requests_ajax += 1\n            sleep(randint(1,3))\n            elapsed_time = time.time() - start_time_comment\n            print('Request for comments: {0}.{1}; Frequency: {2:.4} requests\/s'.format(requests, requests_ajax, requests_ajax\/elapsed_time))\n            clear_output(wait=True)\n\n            ## Parsing the html and filtering the content\n            html_soup = BeautifulSoup(response.text, 'html.parser')\n            comment_container = html_soup.find_all(\"div\", class_=(\"review-container\", \"load-more-data\"))\n\n            ## Updating the parameters of the loop\n            length = len(comment_container)-1\n            n1=0   \n\n        else:\n            flag = False\n\nprint(\"Bucle finalizado\", \"\\n\")","550e0144":"## Creating a dataframe with the scrapped data\ndata_comments = pd.DataFrame({'urls' : urls_comments,\n                              'users_names' : users_names,\n                              'users_ratings' : users_ratings,\n                              'comments_dates' : comments_dates,\n                              'comments_titles' : comments_titles,\n                              'users_texts' : users_texts})\n\n## Obatining some information from the variables and showing a sample\nprint(data_comments.info())\ndata_comments.sample(5)","8ddcda54":"## Saving the data sets\npath = \"..\/input\/\"\n\nprint(\"Tama\u00f1o del set de series:\", data_series.shape)\nprint(\"Tama\u00f1o del set de comentarios:\", data_comments.shape)\n\ndata_series.to_csv(path + \"data_series.csv\", header=True, sep=\";\", index=False)\ndata_comments.to_csv(path + \"data_comments.csv\", header=True, sep=\"|\", index=False)\n\nprint(\"\\nSets de datos guardados\")","bd01747a":"## Loading the data sets\npath = \"..\/input\/\"\n\ndata_series = pd.read_csv(path + \"data_series.csv\", header=0, sep=\";\", dtype={\"years_begins\":str})\ndata_comments = pd.read_csv(path + \"data_comments.csv\", header=0, sep=\"|\", dtype={\"users_ratings\":float})\n\nprint(\"Sets de datos cargados\")","8222667a":"## Joining the data from the series with the comments of the users by the url name\ndata = data_series.merge(data_comments, how='left', on=\"urls\")\n\n## Obtaining the number of comments per serie\ncomments_per_serie = data.groupby([\"names\"])[\"users_texts\"].agg(\"count\").reset_index()\ncomments_per_serie.columns = [\"names\",\"total_comments\"]\n\n## Joining the preview data set with the comments count by the serie name\ndata = data.merge(comments_per_serie, how='left', on=\"names\")\n\n## Transforming the type of the data\ndata.comments_dates = pd.to_datetime(data.comments_dates)\ndata.total_comments = data.total_comments.astype(\"int64\")\ndata[\"short_dates\"] = data.comments_dates.astype(str).apply(lambda x: x[:][0:4])\n\n## Obatining some information from the variables and showing a sample\nprint(data.info())\ndata.sample(3)","92b14a88":"## Obtaining the sum of the NaN values\nmiss_values = data.isnull().sum()\n\n## Calculating the percentaje of NaN values and rounding the result\nmiss_values_percent = (miss_values*100\/len(data)).round(2)\n\n## Joining both results in the same table\nmiss_values_table = pd.concat([miss_values,miss_values_percent], axis=1)\n\n## Renaming the columns and filtering the rows with non-zero values for a proper visualization\nmiss_values_table = miss_values_table.rename(columns={0:\"Total de NaN\", 1:\"% de NaN\"})\nmiss_values_table[miss_values_table.loc[:,\"Total de NaN\"] != 0]","13d5e7a8":"## Dropping rows with NaN values\ndata = data.dropna(axis=0)\n\n## Counting for possible duplicates and erasing them\nduplicated_rows = data.duplicated().sum()   \nif (duplicated_rows > 0):\n    data = data.drop_duplicates().reset_index(drop=True)\n    print(\"N\u00famero de filas duplicadas eliminadas:\", duplicated_rows)\nelse:\n    print(\"No se han encontrado filas duplicadas\")","327383fb":"## Obtaining a descriptive analisis of the cuantitative variables and rounding the results\ndata.describe().round(2)","ed639c74":"## Generating a correlation table to observ the interaction between variables\ncorr= data.corr().round(2)\n\n## Visualizing these correlations with a color map\nfig, ax = plt.subplots(figsize=(15,7))\nax=sns.heatmap(corr, \n               ax=ax,           \n               cmap=\"coolwarm\", \n               annot=True, \n               fmt='.2f',       \n               annot_kws={\"size\": 14},\n               linewidths=3)\n\nax.set_title(\"Mapa de calor de correlaciones entre variables\", fontsize=18, fontweight=\"bold\")\n\nplt.show()","9e1f74bb":"## Generating a pairplot graph to observ the correlation of the numeric variables\npp = sns.pairplot((data),\n              kind=\"scatter\",\n              diag_kind=\"kde\",\n              height=2.5,\n              markers=\"o\",\n              vars=[\"popular_rankings\", \"popularities_trends\", \"imdb_ratings\", \"total_comments\", \"users_ratings\",])\n\nfig = pp.fig \nfig.subplots_adjust(top=0.95, wspace=0.05)\nfig.suptitle(\"Pairplot de correlaciones entre variables\", fontsize=18, fontweight=\"bold\")\n\nplt.show()","098e75b0":"## Generating a frequency graph\nfig, ax = plt.subplots(figsize=(15,5))\nax=sns.distplot(data.years_begins.astype(int),\n             bins=max(data.years_begins.astype(int))-min(data.years_begins.astype(int))+1,\n             hist=True,\n             hist_kws={\"color\": \"g\", \"alpha\": 0.5},\n             kde=True,          \n             kde_kws={\"color\": \"k\", \"lw\": 2})\n\nax.set_title(\"Ditribuci\u00f3n de a\u00f1os de estreno de series\", fontsize=18, fontweight=\"bold\")\nax.set_xlabel(\" \")\nax.set_ylabel(\"Frecuencia de a\u00f1os de estreno\", fontsize=14)\n\n## Generating a boxplot graph\nfig, ax = plt.subplots(figsize=(15,2.5))\nax=sns.boxplot(data=data.years_begins.astype(int),\n               color=\"g\",\n               width=0.5,\n               linewidth=2,\n               orient=\"h\")\n\nax.set_xlabel(\"A\u00f1o de estreno\", fontsize=14)\nax.set_ylabel(\" \", fontsize=14)\n\nplt.show()","fc2c2204":"## Generating a frequency graph\nfig, ax = plt.subplots(figsize=(15,5))\nax=sns.distplot(data.short_dates.astype(int),\n             bins=max(data.short_dates.astype(int))-min(data.short_dates.astype(int))+1,\n             hist=True,\n             hist_kws={\"color\": \"r\", \"alpha\": 0.5},\n             kde=True,          \n             kde_kws={\"color\": \"k\", \"lw\": 2})\n\nax.set_title(\"Ditribuci\u00f3n de comentarios a lo largo del tiempo\", fontsize=18, fontweight=\"bold\")\nax.set_xlabel(\" \")\nax.set_ylabel(\"Frecuencia de comentarios\", fontsize=14)\n\n## Generating a boxplot graph\nfig, ax = plt.subplots(figsize=(15,2.5))\nax=sns.boxplot(data=data.short_dates.astype(int),\n               color=\"r\",\n               width=0.5,\n               linewidth=2,\n               orient=\"h\")\n\nax.set_xlabel(\"A\u00f1o del comentario\", fontsize=14)\nax.set_ylabel(\" \", fontsize=14)\n\nplt.show()","16813cf0":"## Filtering the rows with comments\ncomments_per_serie= comments_per_serie[comments_per_serie.total_comments > 0]\n\n## Sorting the data by the number of comments\ntop10 = comments_per_serie.sort_values(by=[\"total_comments\"], ascending=False).reset_index(drop=True).head(10)\nbottom10 = comments_per_serie.sort_values(by=[\"total_comments\"], ascending=False).reset_index(drop=True).tail(10)\n\n## Generating a subplot with some parameters\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\nexplode= [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]\n\n## Generating a pie chart with the top10 series with more comments\nax1.set_title(\"Series m\u00e1s comentadas\", fontsize=18, fontweight=\"bold\")\nax1.pie(top10[\"total_comments\"],\n       labels=top10.names,\n       explode=explode,\n       autopct=\"%.2f%%\",\n       startangle=90)\n\n## Generating a pie chart with the top10 series with less comments\nax2.set_title(\"Series menos comentadas\", fontsize=18, fontweight=\"bold\")\nax2.pie(bottom10[\"total_comments\"],\n       labels=bottom10.names,\n       explode=explode,\n       autopct=\"%.2f%%\",\n       startangle=90)\n\nplt.show()","70c84c49":"## Generating a list with the top10 series and obtaining their stadistics\nnames_list = top10.names.tolist()\ntop10_data = data_series[data_series[\"names\"].isin(names_list)]\nprint(top10_data.describe().round(2), \"\\n\")\n\n## Visualizing the data distribution of the numerical variables\nfig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,5))\n\nax1.violinplot(top10_data.imdb_ratings,vert=False, showmeans=True)\nax1.set_title(\"IMDb Ratings\", fontsize=18, fontweight=\"bold\")\n\nax2.violinplot(top10_data.popularities_trends, vert=False, showmeans=True)\nax2.set_title(\"Popularities Trends\", fontsize=18, fontweight=\"bold\")\n\nax3.violinplot(top10_data.popular_rankings, vert=False, showmeans=True)\nax3.set_title(\"Popular Rankings\", fontsize=18, fontweight=\"bold\")\n\nplt.show()","af3b888f":"## Generating a list with the bottom10 series and obtaining their stadistics\nnames_list = bottom10.names.tolist()\nbottom10_data = data_series[data_series[\"names\"].isin(names_list)]\nprint(bottom10_data.describe().round(2), \"\\n\")\n\n## Visualizing the data distribution of the numerical variables\nfig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,5))\n\nax1.violinplot(bottom10_data.imdb_ratings,vert=False, showmeans=True)\nax1.set_title(\"IMDb Ratings\", fontsize=18, fontweight=\"bold\")\n\nax2.violinplot(bottom10_data.popularities_trends, vert=False, showmeans=True)\nax2.set_title(\"Popularities Trends\", fontsize=18, fontweight=\"bold\")\n\nax3.violinplot(bottom10_data.popular_rankings, vert=False, showmeans=True)\nax3.set_title(\"Popular Rankings\", fontsize=18, fontweight=\"bold\")\n\nplt.show()","9ba4cdf9":"## Selecting the variables with natural language\ntext = data.loc[:,(\"names\", \"comments_titles\", \"users_texts\")]\n\n## Joining the comments title with the comments texts of each user\ntext[\"all_text\"] = np.NAN\ntext.all_text = text.comments_titles + \". \" + text.users_texts\n\n## Generating a string like a document with each text of each serie\nseries_texts = []\nn=0\nfor i in text.names.unique():\n    text_union = text[text.names==i].all_text.tolist()\n    text_union = \" \".join(text_union)\n    series_texts.append(i + \". \" + text_union)\n    \n## Creating a dataframe with all the natural language joined on one variable\ndata_NLP = pd.DataFrame({\"names\":text.names.unique().tolist(),\n                         \"series_texts\":series_texts})\n\nprint(\"Tama\u00f1o del DataFrame:\", data_NLP.shape)","6d5502e0":"## Generating a object class to clean the data\nwarnings.filterwarnings('ignore')\n\nclass CleanText(BaseEstimator, TransformerMixin):\n   \n    def remove_urls(self, input_text):\n        return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', input_text)\n    \n    def remove_emoji(self, input_text):\n        return input_text.encode('ascii', 'ignore').decode('ascii')\n  \n    def remove_punctuation(self, input_text):\n        punct = string.punctuation\n        trantab = str.maketrans(punct, len(punct)*' ') \n        return input_text.translate(trantab)\n\n    def remove_digits(self, input_text):\n        return re.sub(r'\\d+', '', input_text)\n\n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords(self, input_text):\n        words = input_text.split() \n        clean_words = [word for word in words if (word not in STOPWORDS and word not in STOP_WORDS) and len(word) > 3] \n        return \" \".join(clean_words) \n      \n    def lemmatize_all(self, input_text):\n        wn = WordNetLemmatizer()\n        words = input_text.split() \n        clean_words = []\n        for word, tag in pos_tag(words):\n            if tag.startswith(\"NN\"):\n                clean_words.append(wn.lemmatize(word, pos='n'))\n            elif tag.startswith('VB'):\n                clean_words.append(wn.lemmatize(word, pos='v'))\n            elif tag.startswith('JJ'):\n                clean_words.append(wn.lemmatize(word, pos='a'))\n            elif tag.startswith('ADV'):\n                clean_words.append(wn.lemmatize(word, pos='r'))               \n            else:\n                clean_words.append(word)\n        return \" \".join(clean_words)\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        clean_X = X.apply(self.remove_urls).apply(self.remove_emoji).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.lemmatize_all)\n        return clean_X","9575825f":"## Generating a CleanText object to clean the text\nct = CleanText()\ndata_NLP.series_texts = ct.fit_transform(data_NLP.series_texts)\n\n## Tokenining the text of each serie\ntokenize = [word_tokenize(text) for text in data_NLP.series_texts]\n\n## Generating a dictionary with the tokens\ndictionary = Dictionary(tokenize)\nprint(\"Tama\u00f1o del diccionario original:\", len(dictionary))\n\n## Filtering the extreme values of the dictionary to keep the more representative tokens\ndictionary.filter_extremes(no_below=10, no_above=0.90)\nprint(\"Tama\u00f1o del diccionario filtrado:\", len(dictionary))\n\n## Generating a frequencies bag of words\nbow_corpus = [dictionary.doc2bow(doc) for doc in tokenize]","cb02dc4d":"## Optimizing the coherence of the model\nwarnings.filterwarnings('ignore')\n\ncoherence_list = []\nmodel_list = []\ntopics_range = range(2,6,1)\nfor num_topics in topics_range:\n    lda_model = LdaModel(corpus=bow_corpus,\n                         id2word=dictionary,\n                         num_topics=num_topics,\n                         chunksize=5,\n                         passes=20,\n                         alpha=\"auto\",\n                         eta=\"auto\",\n                         per_word_topics=True)\n    model_list.append(lda_model)\n    \n    coherence_model = CoherenceModel(model=lda_model, texts=tokenize, dictionary=dictionary, coherence='c_v')\n    coherence_list.append(coherence_model.get_coherence())\n    \nfig, ax = plt.subplots(figsize=(15,5))\n\nax = sns.lineplot(x=topics_range, y=coherence_list, color=\"darkorange\", linewidth=2.5)\n\nax.set_title(\"N\u00famero optimo de Topics\", fontsize=18)\nax.set_xlabel(\"N\u00famero de Topics\", fontsize=14)\nax.set_ylabel(\"Coherencia\", fontsize=14)\n\nplt.show()","07c0c4b7":"## Obtaining the more representative words of each topic by chossing the model with the best coherence\nlda_model_opt = model_list[np.argmax(coherence_list)]\n\nfor idx, topic in lda_model_opt.print_topics(-1):\n    print('\\nTopic: {} \\nWord: {}'.format(idx, topic))","adc56d7e":"warnings.filterwarnings('ignore')\n\n## Visualizing the obtained topics\nvis = pyLDAvis.gensim.prepare(lda_model_opt, bow_corpus, dictionary, mds='mmds')\npyLDAvis.display(vis)","6e0bb180":"## Obtaining the topic of each document\ndoc_topic = []\nfor i in range(0, len(bow_corpus)):\n    topic = pd.DataFrame(lda_model_opt[bow_corpus[i]][0]).sort_values(by=1, ascending=False).values[0][0].astype(int)\n    doc_topic.append(topic)\n\n## Obtaning the number of the document\nnum_doc = [\"Doc \" + str(i+1) for i in range(len(bow_corpus))]\n\n## Generating a dataframe with the data\ndata_doc_topic = pd.DataFrame({\"Documento\":num_doc, \"Topics\":doc_topic})\n\n## Obtaining a count of the number of documents per topic \ndoc_topic_count = data_doc_topic.groupby(\"Topics\")[\"Documento\"].aggregate(\"count\")\nprint(pd.DataFrame(doc_topic_count))\n\n## Creating a threshold to avoid a huge standard desviation of the topics per document\nthreshold = 10\nout_thrs = doc_topic_count[doc_topic_count < threshold].index.tolist()\n\nprint(\"\\nEstablecemos un umbral de {threshold} documentos que afecta a los topics: {out_thrs}\".format(threshold=threshold, out_thrs=out_thrs))\n\n## Grouping the series under the threshold with the topic that has the less series\nindex_list = []\nfor j in out_thrs:\n    ind = data_doc_topic[data_doc_topic[\"Topics\"] == j].index.tolist()\n    [index_list.append(k) for k in ind]\n    \n    new_topic_list = doc_topic_count.sort_values(ascending=True).index.tolist()\n    for l in new_topic_list:\n        if l not in out_thrs:\n            new_topic = l\n            break\n        else:\n            continue\n    \n    print(\"El nuevo topic asignado es:\", new_topic)\n    data_doc_topic.loc[index_list, \"Topics\"] = new_topic\n    \n## Obtaining again a count of the number of documents per topic \ndoc_topic_count = data_doc_topic.groupby(\"Topics\")[\"Documento\"].aggregate(\"count\")\n        \n## Visualizating the distribution of topics along all the series  \nfig, ax = plt.subplots(figsize=(15,5))\nax = plt.bar(doc_topic_count.index.astype(str), doc_topic_count, linewidth=2, width=1, edgecolor=\"black\", color=sns.color_palette(\"bright\"))\n\nplt.title(\"Distribuci\u00f3n de documentos por topic\", fontsize=18, fontweight=\"bold\")\nplt.xlabel(\"Topic\", fontsize=14)\nplt.ylabel(\"N\u00famero de documentos\", fontsize=14)\n\nplt.show()","a2964428":"## Joining the data of the series with the obtained topic for each serie\ndata_series_topics = pd.concat([data_series, data_doc_topic.Topics.astype(str)], axis=1)\n\n## Obtaining the sum of the NaN values\nmiss_values = data_series_topics.isnull().sum()\n\n## Obtaining the percentaje of NaN values and rounding the result\nmiss_values_percent = (miss_values*100\/len(data_series_topics)).round(2)\n\n## Joining both results on the same table\nmiss_values_table = pd.concat([miss_values,miss_values_percent], axis=1)\n\n## Renaming the columns and filtering those rows with comments for a proper visualization\nmiss_values_table = miss_values_table.rename(columns={0:\"Total de NaN\", 1:\"% de NaN\"})\nmiss_values_table[miss_values_table.loc[:,\"Total de NaN\"] != 0]","bd2e52eb":"## Dropping all rows with NaN values\ndata_series_topics = data_series_topics.dropna(axis=0)\n\n## Counting for posible duplicates and erasing them\nduplicated_rows = data_series_topics.duplicated().sum()   \nif (duplicated_rows > 0):\n    data_series_topics = data_series_topics.drop_duplicates().reset_index(drop=True)\n    print(\"N\u00famero de filas duplicadas eliminadas:\", duplicated_rows)\nelse:\n    print(\"No se han encontrado filas duplicadas\")\n\n## Joining the data of the series with the comments by the url name\ndata_total = data_series_topics.loc[:,[\"urls\",\"Topics\"]].merge(data, how='left', on=\"urls\")","d51e63e3":"## Saving the data sets\npath = \"..\/input\/\"\n\nprint(\"Tama\u00f1o del set de series con topics:\", data_series_topics.shape)\nprint(\"Tama\u00f1o del set total:\", data_total.shape)\n\ndata_series_topics.to_csv(path + \"data_series_topics.csv\", header=True, sep=\";\", index=False)\ndata_total.to_csv(path + \"data_total.csv\", header=True, sep=\"|\", index=False)\n\nprint(\"\\nSets de datos guardados\")","840ef576":"## Loading the data sets\npath = \"..\/input\/\"\n\ndata_series_topics = pd.read_csv(path + \"data_series_topics.csv\", header=0, sep=\";\", dtype={\"years_begins\":str})\ndata_total = pd.read_csv(path + \"data_total.csv\", header=0, sep=\"|\", dtype={\"user_ratings\": float})\n\nprint(\"Sets de datos cargados\")","7cced4c5":"## Creating a function to obtain the interaction item-user matrices by topics\ndef get_interaction_matrix(data, topic):\n    if topic==\"all\":\n        interaction_matrix = data.pivot_table(index=[\"users_names\"], columns=[\"names\"], values=\"users_ratings\")\n    else:\n        interaction_matrix = data[data[\"Topics\"]==topic].pivot_table(index=[\"users_names\"], columns=[\"names\"], values=\"users_ratings\")\n    \n    return interaction_matrix","1c61cb58":"## Obtaining the interaction matrix without filtering the topic\ninteraction_matrix = get_interaction_matrix(data_total, \"all\")\nprint(\"Tama\u00f1o de la matriz de interacciones:\", interaction_matrix.shape)\n\n## Calculating how sparse the interaction matrix is\nsuma = []\n[suma.append(i) for i in (interaction_matrix > 0).sum()]\nsparsicity = (sum(suma)\/interaction_matrix.size)*100\n\nprint(\"Sparsicity de la matriz de interacciones: {:.5}%\".format(sparsicity))","ff4adefc":"## ## Creating a function to obtain the similarity item-item matrices by topics\ndef get_similarity_matrix(interaction_matrix):\n    \n    ## Generating a matrix of zeros\n    cos_distance = np.zeros((interaction_matrix.shape[1], interaction_matrix.shape[1]))\n\n    ## Itering over the interaction matrix to obtaing the cosine distance dismissing NaN values\n    for i in range(0, interaction_matrix.shape[1]):\n        u = interaction_matrix[interaction_matrix.columns[i]]\n        for j in range(0, interaction_matrix.shape[1]):\n            v = interaction_matrix[interaction_matrix.columns[j]]\n            ind = []\n            for k in range(0, len(u)):\n                if (np.isnan(u[k]) or np.isnan(v[k])) == False:\n                    ind.append(k)\n                else:\n                    continue\n            cos_distance[i,j] = 1-distance.cosine(u[ind], v[ind]).round(4)\n\n    similarity_matrix = pd.DataFrame(cos_distance, columns=interaction_matrix.columns, index= interaction_matrix.columns)\n    similarity_matrix = similarity_matrix.replace(np.NaN, \"0\").astype(\"float64\")\n    \n    return similarity_matrix","856748ee":"## Obtaining the similarity matrix without filtering by topics\nsimilarity_matrix = get_similarity_matrix(interaction_matrix)\nprint(\"Tama\u00f1o de la matriz de similitudes:\", similarity_matrix.shape)\n\n## Calculating how sparse the similarity matrix is\nsuma = []\n[suma.append(i) for i in (similarity_matrix > 0).sum()]\nsparsicity = (sum(suma)\/similarity_matrix.size)*100\n\nprint(\"Sparsicity de la matriz de similitudes: {:.5}%\".format(sparsicity))","15a7c8a5":"## Obtaining a list with the unique topics\ntopic_list = data_total.Topics.unique().astype(\"int64\").tolist()\n\n## Appliying the preview functions we can obtaing the simlarity and interaction matrices per topic\nfor i in topic_list:\n    globals()['interaction_matrix_'+ str(i)] = get_interaction_matrix(data_total, i)\n    globals()['similarity_matrix_'+ str(i)] = get_similarity_matrix(eval('interaction_matrix_%d'% (i)))","9278acdd":"## Generating a graph visualization with the differents recommendations\nplt.figure(figsize=(15,20))\n\n## Indicating the path to save each graph\npath = \"..\/input\/\"\n\nn=0\nfor i in topic_list:\n    n+=1\n    ## Generating a graph using the dataframe of similarities\n    G = nx.from_pandas_adjacency(eval('similarity_matrix_%d'% (i)))\n\n    ## Selecting a design to visualize the graph\n    pos = nx.circular_layout(G)\n\n    ## Creating a dictionary with the names of each serie to use labels on the edges of each node\n    labels = eval('similarity_matrix_%d'% (i)).columns.values\n    G = nx.relabel_nodes(G, dict(zip(range(len(labels)), labels)))\n\n    ## Applying the maximum spanning tree algorithm\n    G = nx.maximum_spanning_tree(G, algorithm='kruskal', weight=\"weight\", ignore_nan=False)\n\n    ## Obtaining the similarities of each edge\n    edge_labels = nx.get_edge_attributes(G, \"weight\")\n\n    values = []\n    [values.append(round(j,4)) for j in list(edge_labels.values())]\n    \n    edge_labels = dict(zip(edge_labels.keys(), values))\n\n    ## Showing the final graph    \n    plt.subplot(int(np.ceil(len(topic_list)\/2)*100+20+n))\n    nx.draw(G,\n            pos,\n            with_labels=True,\n            font_size=15,\n            font_weight='bold',\n            node_color='skyblue',\n            node_size=800,\n            edge_color='grey',\n            linewidths=2)\n\n    nx.draw_networkx_edge_labels(G,\n                                 pos,\n                                 edge_labels=edge_labels,\n                                 font_color='darkblue',\n                                 font_size=13,\n                                 font_weight='bold')\n    \n    ## Saving the obtaing graph\n    nx.write_edgelist(G, path=path+\"grafo_topic_\"+str(i), delimiter=\";\")","99160589":"## Generating a function to obtain the final recommendation that the user will see\ndef Recomendation(serie):\n    \n    data_series_topics = pd.read_csv(path + \"data_series_topics.csv\", header=0, sep=\";\", dtype={\"years_begins\":str})\n\n    topic = int(data_series_topics[data_series_topics.names==serie].Topics)\n\n    G = nx.read_edgelist(path=path+\"grafo_topic_\"+str(topic), delimiter=\";\")\n\n    EG = nx.ego_graph(G, n=serie, radius=2, center=True, undirected=True, distance=\"weight\")\n    ego_node=nx.ego_graph(G, n=serie, radius=0)\n\n    fig, ax = plt.subplots(figsize=(15,10))\n    pos = nx.spring_layout(EG)\n\n    ax = nx.draw(EG,\n                 pos,\n                 with_labels=True,\n                 font_size=15,\n                 font_weight='bold',\n                 node_color='lime',\n                 node_size=800,\n                 edge_color='grey',\n                 linewidths=2)\n\n    ax = nx.draw_networkx_nodes(ego_node, pos=pos, radius=0, node_size=900, node_color='red')\n    \n    plt.show()","8c8975e7":"## Using the function to show a example\nRecomendation(\"perdidos\")","8d53af33":"## Exploratory Data Analysis (EDA)<a name=\"id3\"><\/a>\n\n---\n\nThe variables that are included in the previous scrapping technique are:\n\n1. **urls**: Code identifier of the series in the url\n2. **names**: Name of the series\n3. **years_begins**: Year in which the series began\n4. **popular_rankings**: Popularity ranking\n5. **popularities_trends**: Popularity trend\n6. **imdb_ratings**: Average rating of IMDb\n7. **users_names**: User's name\n8. **users_ratings**: User's score\n9. **comments_dates**: Comment date\n10. **comments_titles**: Title of the comment\n11. **users_texts**: Comment text\n\n---","26fbe073":"## Algorithm Latent Dirichlet Allocation (LDA)<a name=\"id5\"><\/a>","ab75d2bb":"## Conclusion<a name=\"id8\"><\/a>\n\nThroughout this Jupyter Notebook we have observed the different steps to be followed in order to build a Similarity Recommendation System through natural language processing and the use of collaborative filtering techniques, starting with the obtaining of the data and ending with a graph visualization of the recommendation that the user will see. In this case we have used the top 100 most popular series of the IMDb page, however this recommendation structure can be applied to many other areas with the potential of being able to make a recommendation to a user who does not want to interact with you explicitly . Thanks to this type of generative algorithms in conjunction with the prevailing techniques in the market, such as collaborative filtering, we can open a new business area or anchor point where the user can be captured, having an impact on the future benefit of the company.","8882673b":"# Similarity Recommendation System (SRS)\n\n---\n\n### **Index**\n\n1. [Loading libraries](# id1)\n2. [IMDb Web Scrapping](# id2)\n3. [Exploratory Data Analysis (EDA)](# id3)\n4. [Natural Language Processing (NLP)](# id4)\n5. [Algorithm Latent Dirichlet Algorithm (LDA)](# id5)\n6. [Collaborative Filtering](# id6)\n7. [Visualization and Result](# id7)\n8. [Conclusion](# id8)\n\n---","6a029d19":"## Natural Language Processing (NLP)<a name=\"id4\"><\/a>","92e33171":"## Visualization and Result<a name=\"id7\"><\/a>","8f2225f2":"## IMDb Web Scrapping<a name=\"id2\"><\/a>","292f5998":"## Loading libraries<a name=\"id1\"><\/a>","0b828e00":"## Collaborative Filtering<a name=\"id6\"><\/a>"}}