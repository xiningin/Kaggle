{"cell_type":{"1524f168":"code","85b12d5b":"code","8fe636e2":"code","014d932a":"code","b3a5b07a":"code","b7ec533d":"code","1d242581":"code","09699a7f":"code","b02c8dee":"code","ee11c07f":"code","052037c8":"code","8e5bf606":"code","01afcd70":"code","2724295d":"code","d785b0f3":"code","e038352c":"code","44854819":"code","b6d1a65a":"code","d3fd90b9":"code","e415a2d6":"markdown","c4a3d2b4":"markdown","96b5c5a2":"markdown","bf3f37e5":"markdown","c6027561":"markdown"},"source":{"1524f168":"! pip install monai","85b12d5b":"import gc\nimport glob\nimport math\nimport os\nimport random\nimport time\nfrom pathlib import Path\n\nimport feather\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom monai.metrics import ROCAUCMetric\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.core.memory import ModelSummary\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import (GroupKFold, KFold, StratifiedKFold,\n                                     train_test_split)\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils import data","8fe636e2":"class Config:\n    competition = \"TPS_202111\"\n    seed = 42\n    n_folds = 10\n    batch_size = 1024\n    epochs = 125\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    es_patience = 20\n    lr_patience = 7\n    lr = 0.01","014d932a":"data_dir = Path('..\/input\/tabular-playground-series-nov-2021')","b3a5b07a":"%%time\n# Loading files in feather format\ntrain_df = feather.read_dataframe('..\/input\/tpsnov21\/train.feather')\ntest_df = feather.read_dataframe('..\/input\/tpsnov21\/test.feather')\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\nprint(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","b7ec533d":"# features = [col for col in train_df.columns if col not in ('id', 'target')]\nfeatures = ['f1', 'f10', 'f11', 'f14', 'f15', 'f16', 'f17', 'f2', 'f20', 'f21', 'f22', 'f24', 'f25', 'f26', 'f27', 'f28', 'f3', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f4', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f5', 'f50', 'f51', 'f53', 'f54', 'f55', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f64', 'f66', 'f67', 'f70', 'f71', 'f76', 'f77', 'f8', 'f80', 'f81', 'f82', 'f83', 'f87', 'f89', 'f9', 'f90', 'f91', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98']","1d242581":"class TPSDataset(data.Dataset):\n    def __init__(self, X, y=None):\n        super(TPSDataset).__init__()\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return {\n                'X' : torch.tensor(self.X.values[idx], dtype=torch.float),\n                'y' : torch.tensor(self.y.values[idx], dtype=torch.float)\n            }\n        else:\n            return {\n                'X' : torch.tensor(self.X.values[idx], dtype=torch.float),\n            }","09699a7f":"scaler = StandardScaler()\n\ntrain_df[features] = scaler.fit_transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","b02c8dee":"y_train = train_df.target\n\nX_test = test_df.drop(columns=[\"id\"], axis=1)\nX_train = train_df.drop(columns=[\"id\", \"target\"], axis=1)","ee11c07f":"# remove the unimportant features\nX_train = X_train[features]\nX_test = X_test[features]","052037c8":"train_dataset = TPSDataset(X_train, y_train)\ntest_dataset = TPSDataset(X_test)","8e5bf606":"test_loader = data.DataLoader(test_dataset, batch_size = 1024)","01afcd70":"def initialize_weights(model):\n    if isinstance(model, nn.Linear):\n#         nn.init.normal_(model.weight.data)\n#         nn.init.xavier_uniform_(model.weight.data)\n        nn.init.kaiming_uniform_(model.weight.data, nonlinearity=\"relu\")\n        nn.init.constant_(model.bias.data, 0)\n    elif isinstance(model, nn.Conv2d):\n        nn.init.kaiming_uniform_(model.weight.data, nonlinearity=\"relu\")\n        if model.bias is not None:\n            nn.init.constant_(model.bias.data, 0)","2724295d":"class Model(pl.LightningModule):\n    def __init__(self, in_features, activation=F.relu, lr=Config.lr):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(in_features, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.fc3 = nn.Linear(64, 32)\n        self.bn3 = nn.BatchNorm1d(32)\n        self.fc4 = nn.Linear(32, 1)\n        self.flatten = nn.Flatten()\n        self.activation = activation\n        self.roc_auc_metric = ROCAUCMetric()\n        self.lr = lr\n        \n    def forward(self, x):\n#         print(\"forward\")\n        x = self.flatten(x)\n        x = self.bn1(self.activation(self.fc1(x)))\n        x = self.bn2(self.activation(self.fc2(x)))\n        x = self.bn3(self.activation(self.fc3(x)))\n        x = torch.sigmoid(self.fc4(x))\n        \n        return torch.squeeze(x, dim=1)        \n    \n    def training_step(self, batch, batch_idx):\n#         print(\"training_step\")\n        X, y = batch[\"X\"], batch[\"y\"]\n        _y_pred = self(X)#.squeeze(1)\n        loss = F.binary_cross_entropy(_y_pred, y)\n        self.log(\"loss\", loss)\n        \n        return {\"loss\": loss}\n        \n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch[\"X\"], batch[\"y\"]\n        _y_pred = self(X)#.squeeze(1)\n        self.roc_auc_metric(_y_pred, y)\n        \n    \n    def validation_epoch_end(self, training_step_outputs):\n        roc_auc = self.roc_auc_metric.aggregate()\n        self.roc_auc_metric.reset()\n        self.log(\"roc_auc\", roc_auc)\n        \n    def predict_step(self, X, batch_idx, loader_idx=None):\n        X = X[\"X\"]\n        return self(X)\n    \n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer","d785b0f3":"model = Model(in_features=len(features), activation=nn.SiLU())\nprint(ModelSummary(model))","e038352c":"%%time\n\npl.utilities.seed.seed_everything(Config.seed)\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nhistories = []\n\nkf = StratifiedKFold(n_splits=Config.n_folds, random_state=Config.seed, shuffle=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X_train, y = y_train)):\n    print(10*\"=\", f\"Fold={fold+1}\/{Config.n_folds}\", 10*\"=\")\n    start_time = time.time()\n\n    train_subset = data.Subset(train_dataset, train_idx)\n    valid_subset = data.Subset(train_dataset, valid_idx)\n    train_loader = data.DataLoader(train_subset, batch_size = Config.batch_size, shuffle=True)\n    valid_loader = data.DataLoader(valid_subset, batch_size = Config.batch_size)\n        \n    model = Model(in_features=len(features), activation=nn.SiLU()).to(Config.device)\n#     model.apply(initialize_weights)\n    \n    cp_callback = pl.callbacks.ModelCheckpoint(dirpath=Path.cwd() \/ \"models\", \n#                                                filename=f\".\/models\/model_{fold}_{roc_auc:.3}\", \n                                               filename=f\"model_{fold}\", \n                                               monitor=\"roc_auc\", \n                                               mode=\"max\", \n                                               save_weights_only=True)\n    logger = TensorBoardLogger(save_dir=Path.cwd(), version=fold, name=\"lightning_logs\")\n    es_callback = EarlyStopping(monitor=\"loss\", min_delta=0.0, patience=Config.es_patience, verbose=True, mode=\"min\")\n    \n    trainer = pl.Trainer(\n        fast_dev_run=False,\n        max_epochs=Config.epochs,\n        gpus=1,\n        precision=32,\n        limit_train_batches=1.0,\n        num_sanity_val_steps=0,\n        val_check_interval=1.0,\n        callbacks=[cp_callback, es_callback],\n        logger=logger\n    )\n    \n    trainer.fit(model, train_loader, valid_loader)\n    \n    del model, trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    trainer = pl.Trainer(gpus=1)\n    \n    model = Model(in_features=len(features), activation=nn.SiLU()).to(Config.device)\n    model.load_state_dict(torch.load(glob.glob(f\".\/models\/model_{fold}*.ckpt\")[0])[\"state_dict\"])    \n\n    valid_preds = trainer.predict(model, valid_loader)\n    valid_preds = torch.cat(valid_preds).cpu().numpy().flatten()\n    test_preds = trainer.predict(model, test_loader)\n    test_preds = torch.cat(test_preds).cpu().numpy().flatten()\n    \n#     _y = [data[\"y\"] for data in valid_loader]\n    _valid_true = []\n    for batch in valid_loader:\n        for x in batch[\"y\"].numpy():\n            _valid_true.append(x)\n    \n    _valid_true = np.array(_valid_true)\n    \n    auc = roc_auc_score(_valid_true, valid_preds)\n    scores.append(auc)\n    \n    final_valid_predictions.update(dict(zip(valid_idx, valid_preds)))    \n    final_test_predictions.append(test_preds)\n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, auc: {auc:.8f}, Run Time: {run_time:.2f}\")","44854819":"# glob.glob(f\".\/models\/model_{fold}*.ckpt\")[0]","b6d1a65a":"print(f\"Scores -> corrected: {np.mean(scores)-np.std(scores):.8f}, mean: {np.mean(scores):.8f}, std: {np.std(scores):.8f}\")","d3fd90b9":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"test_pred_2.csv\",index=None)\nsample_submission.to_csv(\"submission.csv\",index=None)\nsample_submission","e415a2d6":"Adapting [my PyTorch Notebook](https:\/\/www.kaggle.com\/yusufmuhammedraji\/pytorch-cv-earlystopping-lrscheduler) to PyTorch Lightning\n\nUsing Bizen's [notebook](https:\/\/www.kaggle.com\/hiro5299834\/tps-nov-2021-pytorch-lightning) as a source.","c4a3d2b4":"# PyTorch Lightning Module","96b5c5a2":"# Training with Cross Validation","bf3f37e5":"# References\n\n- https:\/\/www.kaggle.com\/yusufmuhammedraji\/pytorch-cv-earlystopping-lrscheduler\n- https:\/\/www.kaggle.com\/hiro5299834\/tps-nov-2021-pytorch-lightning","c6027561":"# Version\n\n- V2: First successful run\n- V1: Draft"}}