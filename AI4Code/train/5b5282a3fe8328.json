{"cell_type":{"772279b1":"code","790f4dc6":"code","184f397e":"code","0c91f7d7":"code","012f081b":"code","5dca270b":"code","7fc3aa7d":"code","e223c46d":"code","4db03912":"code","98210bcd":"code","d6cae8f3":"code","51881630":"code","4b099fa2":"code","207703a2":"code","5a8eb5f9":"code","5c89c37e":"code","55ac4736":"code","54c7aece":"code","04454cd1":"code","933f9a8e":"code","14ba3c21":"code","2bedb029":"code","0cc294b4":"code","d16c1fa6":"code","cd2e4e98":"code","29a54cbc":"code","7b97647d":"code","e52debdf":"code","de97e43f":"code","b802d144":"code","98da6e9d":"markdown","926ddb64":"markdown","27d32721":"markdown","76408900":"markdown","383690c1":"markdown","a9f08762":"markdown","e0f72ec5":"markdown","ca7d9988":"markdown","1f0ef035":"markdown","9f10bc32":"markdown","80750967":"markdown","bfa98a3e":"markdown","9ccff7bd":"markdown","1ab62a2f":"markdown","5f4f9da3":"markdown","a3bd50d2":"markdown","70b66625":"markdown","dd16812a":"markdown","726f7d8b":"markdown","8613c0c7":"markdown","047ca1d4":"markdown","ce00e061":"markdown","71b1b819":"markdown","4b4cf723":"markdown","f23eef7a":"markdown"},"source":{"772279b1":"# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport time\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics.pairwise import cosine_similarity as cos_sim\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.sparse import csr_matrix\nfrom pandas.api.types import CategoricalDtype","790f4dc6":"# data loading\nratings = pd.read_csv('..\/input\/movielens-20m-dataset\/rating.csv', parse_dates=[3])\nmovies = pd.read_csv('..\/input\/movielens-20m-dataset\/movie.csv')\ntags = pd.read_csv('..\/input\/movielens-20m-dataset\/tag.csv', parse_dates=[3])\nrelevances = pd.read_csv('..\/input\/movielens-20m-dataset\/genome_scores.csv')\ntagIDs = pd.read_csv('..\/input\/movielens-20m-dataset\/genome_tags.csv')\n\nratings100k = pd.read_csv('..\/input\/movielens-latest-small\/ratings.csv')\ntags100k = pd.read_csv('..\/input\/movielens-latest-small\/tags.csv')\nratings100k.timestamp = pd.to_datetime(ratings100k.timestamp, unit='s')\ntags100k.timestamp = pd.to_datetime(tags100k.timestamp, unit='s')\n\nmovies100k = pd.read_csv('..\/input\/movielens-latest-small\/movies.csv')\nmovies = pd.merge(movies, movies100k, how='outer')\nmovies = movies[~movies.duplicated(subset='movieId')]","184f397e":"import os\nprint(os.listdir('..\/input\/movielens-20m-dataset'))","0c91f7d7":"print('Dataset contains {:,} ratings from {:,} distinct users applied to {:,} movies.'\\\n      .format(len(ratings), ratings['userId'].nunique(), ratings['movieId'].nunique()))\nprint('Dataset contains data since {} until {}.'\\\n      .format(ratings['timestamp'].min().date(), ratings['timestamp'].max().date()))","012f081b":"user_ratings = ratings.groupby(by='userId')\nd = user_ratings['rating'].count()\nlimit = 500\nplt.hist(d[d<=limit], bins='fd')\nplt.xlabel('number of rated movies')\nplt.ylabel('number of users')\nprint(f'Only users with less than {limit} ratings are displayed ({len(user_ratings) - len(d[d<=limit]):,} users omitted).')\nplt.show()","5dca270b":"users_average = ratings.groupby('userId')['rating'].mean()\nitems_average = ratings.groupby('movieId')['rating'].mean()\nplt.hist([users_average, items_average], histtype='step', density=True)\nplt.xlabel('average rating for a movie \/ by a user')\nplt.ylabel('number of movies \/ users')\nplt.legend(['average rating given by a user', 'average rating of a movie'], loc=2)\nplt.show()","7fc3aa7d":"genres = Counter()\nfor g in movies['genres']:\n    genres.update(g.split('|'))\nprint('List of 10 most common genres: ', genres.most_common(10))","e223c46d":"movie_ratings = ratings.groupby(by='movieId')\nmost_rated = movie_ratings['rating'].count().sort_values(ascending=False).head(10)\ntop_rated = movie_ratings['rating'].mean().where(movie_ratings['rating'].count() > 20).sort_values(ascending=False).head(10)\nprint(pd.merge(pd.DataFrame(most_rated), movies, on='movieId')[['title','rating']].rename(index=lambda x: x+1, columns={'rating': 'n. of ratings'}),'\\n')\nprint(pd.merge(pd.DataFrame(top_rated), movies, on='movieId')[['title','rating']].rename(index=lambda x: x+1, columns={'rating': 'average rating'}))","4db03912":"print(f\"Tags were given by {tags['userId'].nunique():,} users to {tags['movieId'].nunique():,} movies.\")\nprint(f\"Together, {tags['tag'].nunique():,} unique tags were given in a period since {tags['timestamp'].min().date()} until {tags['timestamp'].max().date()}.\")","98210bcd":"tag_tags = tags.groupby(by='tag')\ntag_tags['movieId'].count().sort_values(ascending=False).head(10).rename('n. of movies')","d6cae8f3":"relevances.sort_values(by='relevance').head(20).merge(movies, on='movieId', how='left').merge(tagIDs, on='tagId', how='left').rename(index=lambda x: x+1)[['tag', 'title', 'relevance']]","51881630":"class Sampler():\n    def sample_full(self):\n        return ratings\n    def sample_latest(self):\n        return ratings100k\n    ratings_rated = None\n    def sample_rated(self):\n        if self.ratings_rated is None:\n            most_rated = ratings.groupby(by='movieId').count().sort_values(by='userId').tail(500).index\n            self.ratings_rated = ratings[ratings.movieId.isin(most_rated)]\n        return self.ratings_rated\n    ratings_active = None\n    def sample_active(self):\n        if self.ratings_active is None:\n            most_active = ratings.groupby(by='userId').count().sort_values(by='movieId').tail(1000).index\n            self.ratings_active = ratings[ratings.userId.isin(most_active)]\n        return self.ratings_active\n    def sample_random(self, sample_size=100000):\n        return ratings.sample(sample_size)\n    samplers = {'full': sample_full, 'official_small': sample_latest, 'most_rated': sample_rated, \\\n                'most_active': sample_active, 'random': sample_random}\n\nsampler = Sampler()","4b099fa2":"print(f\"'rated' dataset contains {len(sampler.samplers['most_rated'](sampler)):,} ratings.\")\nprint(f\"'active' dataset contains {len(sampler.samplers['most_active'](sampler)):,} ratings.\")","207703a2":"class Divider():\n    @staticmethod\n    def divide_time(data):\n        div_time = data['timestamp'].quantile(0.8)\n        train = data[data['timestamp'] <= div_time]\n        test = data[data['timestamp'] > div_time].copy()\n        return train, test\n    @staticmethod\n    def divide_users(data):\n        u = np.random.choice(data.userId.unique(), int(data.userId.nunique()*0.2))\n        train = data[~data.userId.isin(u)]\n        test = data[data.userId.isin(u)].copy()\n        return train, test\n    @staticmethod\n    def divide_ratings(data):\n        rank = data.groupby('userId').timestamp.rank(method='first', ascending=False)\n        train = data[rank > 10]\n        test = data[rank <= 10].copy()\n        return train, test\n    dividers = {'time': divide_time.__func__, 'users': divide_users.__func__, 'last_ratings': divide_ratings.__func__}","5a8eb5f9":"print('Division by time.')\nfor s in sampler.samplers:\n    print(f'Sampling type: {s}')\n    data = sampler.samplers[s](sampler)\n    train, test = Divider.dividers['time'](data)\n    new_users = set(test['userId']) - set(train['userId'])\n    new_items = set(test['movieId']) - set(train['movieId'])\n    print('Test subset is {:,} ratings long. It contains {} days of data.\\n\\\nTest subset contains {:,} new users who created {:.4} % of all test ratings.\\n\\\nTest subset contains {:,} new movies that correspond to {:.4} % of all test ratings.'\\\n          .format(len(test), (test['timestamp'].max() - test['timestamp'].min()).days, \\\n                  len(new_users), len(test[test['userId'].isin(new_users)])\/len(test)*100,\\\n                  len(new_items), len(test[test['movieId'].isin(new_items)])\/len(test)*100))","5c89c37e":"test_set_sizes = pd.DataFrame(index=sampler.samplers, columns=Divider.dividers)\nfor s in sampler.samplers:\n    for d in Divider.dividers:\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        test_set_sizes.at[s,d] = len(test)\/len(data)*100\ntest_set_sizes","55ac4736":"r = (4.5 * np.random.random_sample((len(test),))) + 0.5\ndef RMSE(true, predicted):\n    return np.sqrt(MSE(true, predicted))\ndef RMSE1(true, predicted):\n    return np.sqrt(((true - predicted) ** 2).mean())\nstart = time.time()\nrmse = RMSE1(test['rating'], r)\nprint(f'took {time.time()-start} seconds for numpy. RMSE = {rmse}')\nstart = time.time()\nrmse = RMSE(test['rating'], r)\nprint(f'took {time.time()-start} seconds for sklearn. RMSE = {rmse}')","54c7aece":"methods = pd.MultiIndex.from_product([sampler.samplers, Divider.dividers], names=['sampling methods', 'division methods'])\nmodels = ['random', 'user_avg', 'item_avg', 'item_CF', 'item_CF_lenskit']\nresults = pd.DataFrame(index=models, columns=methods)\nresults","04454cd1":"start = time.time()\nfor s in sampler.samplers:\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        r = (4.5 * np.random.random_sample((len(test),))) + 0.5\n        rmse = RMSE(test['rating'], r)\n        results.loc['random',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","933f9a8e":"results","14ba3c21":"start = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':  # for performance reasons when submitting\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        global_average = train['rating'].mean()\n        users_average = train.groupby(by='userId')['rating'].mean()\n        test['predicted'] = np.repeat(global_average, len(test))  # global average fallback\n        users = np.intersect1d(users_average.index, test.userId.unique(), assume_unique=True)  # will be empty for users divider\n        c = 0\n        p = 0\n        step = len(users)\/10\n        print('[__________]')\n        for u in users:\n            c += 1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(10-p) + ']')\n            test.loc[test['userId'] == u,'predicted'] = users_average[u]\n        rmse = RMSE(test['rating'], test['predicted'])\n        results.loc['user_avg',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","2bedb029":"results","0cc294b4":"start = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        global_average = train['rating'].mean()\n        genres_average = train.merge(movies[['movieId', 'genres']]).groupby('genres').rating.mean()\n        items_average = train.groupby(by='movieId')['rating'].mean()\n        test['predicted'] = np.repeat(global_average, len(test))  # second fallback for unknown genres\n        test_items = test.movieId.unique()\n        c = 0\n        p = 0\n        step = len(test_items)\/10\n        print('[__________]')\n        for i in np.intersect1d(test_items, items_average.index, assume_unique=True):  # predict item average\n            c+=1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(10-p) + ']')\n            test.loc[test['movieId'] == i,'predicted'] = items_average[i]\n        for i in np.setdiff1d(test_items, items_average.index, assume_unique=True):  # predict genre average for new items\n            c+=1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(10-p) + ']')\n            g = movies.loc[movies.movieId == i, 'genres']\n            if g.empty:  # unknown genre\n                continue\n            try:\n                a = genres_average[g.values[0]]\n            except KeyError:  # not a perfect genres match\n                a = genres_average.filter(like=g.values[0])  # try any more specific genres\n                if a.empty:\n                    a = global_average\n                    for j in g.values[0].split('|'):  # try subgenres\n                        try:\n                            a = (a + genres_average[j]) \/ 2\n                        except KeyError:\n                            continue\n                else:\n                    a = a.mean()\n            test.loc[test['movieId'] == i, 'predicted'] = a\n        rmse = RMSE(test['rating'], test['predicted'])\n        results.loc['item_avg',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","d16c1fa6":"results","cd2e4e98":"for s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        new_items = set(test['movieId']) - set(train['movieId'])\n        print(\"Number of new items:{}\\nData from test set created by new items: {:.4} %\".format(len(new_items), len(test[test['movieId'].isin(new_items)])\/len(test)*100))","29a54cbc":"def itemCFpred(user, item, user_c, item_c, item_user, matrix_full):\n    i = np.argwhere(item_c.categories == item)[0][0]\n    u = np.argwhere(user_c.categories == user)[0][0]\n    rated = matrix_full.loc[:,matrix_full.loc[user,:].notna()].columns\n    irated = np.argwhere(item_c.categories.isin(rated)).flatten()\n    a = cos_sim(item_user[i,:],item_user[irated,:])[0]\n    k = min(len(rated), K)\n    ind = np.argpartition(a, -k)[-k:]\n    similarities = a[ind]\n    s = similarities.sum()\n    r = np.multiply(item_user[irated, u][ind].todense(), similarities.reshape(k,1)).sum()\n    return r\/s","7b97647d":"K = 50\n\nstart = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        global_average = train['rating'].mean()\n        users_average = train.groupby(by='userId')['rating'].mean()\n        items_average = train.groupby(by='movieId')['rating'].mean()\n        genres_average = train.merge(movies[['movieId', 'genres']]).groupby('genres').rating.mean()\n        # matrix_full = train.pivot(index='userId', columns='movieId', values='rating')  # too large\n        user_c = CategoricalDtype(sorted(train['userId'].unique()), ordered=True)\n        item_c = CategoricalDtype(sorted(train['movieId'].unique()), ordered=True)\n\n        row = train['userId'].astype(user_c).cat.codes\n        col = train['movieId'].astype(item_c).cat.codes\n        user_item = csr_matrix((train[\"rating\"], (row, col)), \\\n                                   shape=(user_c.categories.size, item_c.categories.size))\n        item_user = csr_matrix((train[\"rating\"], (col, row)), \\\n                                   shape=(item_c.categories.size, user_c.categories.size))\n        matrix_full = pd.SparseDataFrame(user_item, index=user_c.categories, columns=item_c.categories)\n        c = 0\n        p = 0\n        step = len(test)\/50\n        print('[__________________________________________________]')\n        for row in test.itertuples():\n            c += 1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(50-p) + ']')\n            if row.movieId not in matrix_full.columns:  # unseen item\n                try:\n                    ua = users_average.at[row.userId]\n                    test.at[row.Index, 'predicted'] = ua\n                except KeyError:  # and unseen user\n                    g = movies.loc[movies.movieId == i, 'genres']\n                    if g.empty:  # unknown genre\n                        test.at[row.Index, 'predicted'] = global_average                        \n                    try:\n                        a = genres_average[g.values[0]]\n                    except KeyError:  # not a perfect genres match\n                        a = genres_average.filter(like=g.values[0])  # try any more specific genres\n                        if a.empty:\n                            a = global_average\n                            for j in g.values[0].split('|'):  # try subgenres\n                                try:\n                                    a = (a + genres_average[j]) \/ 2\n                                except KeyError:\n                                    continue\n                        else:\n                            a = a.mean()\n                    test.at[row.Index, 'predicted'] = a\n            elif row.userId not in matrix_full.index:  #seen item but unseen user\n                test.at[row.Index, 'predicted'] = items_average[row.movieId]\n            else:  # seen both user and item; use CF\n                test.loc[row.Index, 'predicted'] = itemCFpred(row.userId, row.movieId, user_c, item_c, item_user, matrix_full)\n        test.predicted.fillna(global_average, inplace=True)  # to be fail-proof\n        rmse = RMSE(test['rating'], test['predicted'])\n        results.loc['item_CF',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","e52debdf":"results","de97e43f":"from lenskit import batch\nfrom lenskit.algorithms import item_knn as knn\n\nalgo = knn.ItemItem(K)\n\nstart = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        train.rename(columns={'userId': 'user', 'movieId': 'item'}, inplace=True)\n        test.rename(columns={'userId': 'user', 'movieId': 'item'}, inplace=True)\n        global_average = train['rating'].mean()\n        print('training')\n        model = algo.train(train)\n        print('recommending')\n        recs = batch.predict(algo, test[['user', 'item']], model)\n        res = pd.merge(recs, test, how='left', on=('user', 'item'))\n        res.prediction.fillna(global_average, inplace=True)\n        rmse = RMSE(test['rating'], res['prediction'])\n        results.loc['item_CF_lenskit',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","b802d144":"results","98da6e9d":"![](https:\/\/is.muni.cz\/www\/bielovico\/pv254\/final_table1.PNG)\n![item_average](https:\/\/is.muni.cz\/www\/bielovico\/pv254\/itemAVG.PNG)\n![](https:\/\/is.muni.cz\/www\/bielovico\/pv254\/resources.PNG)","926ddb64":"Highest improvement was achieved with `users` division method. It is expected thanks to the fact that users division contains radically smaller portion of test data created by new items (see list below). On the other hand with `last_ratings` division we completely ignore user's preference and even though there are even less new items, score is worse than with `user_avg` method.","27d32721":"### Random rating\nSimplest model, predict random rating from [0.5, 5) interval.","76408900":"### User average\n\nPredict average rating per user. Predict overall average rating for new users.","383690c1":"### Movies\nMovies are described in `movie.csv` table. It contains `movieId`, `title` and `genres` columns. One movie can contain several genres that are delimited by `|`.","a9f08762":"## Dataset sampling\nI will use subsampling for performance reasons as well as train\/test division for evaluation.\n\n### Subsampling\n#### MovieLens 20M `full`\nNo subsampling\n#### MovieLens Latest Small 100k `official_small`\nOfficial release of subsampled current MovieLens database with 100,000 ratings.\n#### 500 Most rated movies `most_rated`\nMovieLens 20M subsampled for 500 most rated movies.\n#### 1K Most active users `most_active`\nMovieLens 20M subsampled for 1,000 most active users.\n#### Random subsample 100K `random`\nMovieLens 20M randomly subsampled, not stratified. Will resample every time it is demanded.\n\nSampling methods are stored  in `Sampler` class with `sampler` instance. ","e0f72ec5":"### Tags\nTags given by users are stored in `tag.csv`. 4 columns are: `userId`,  `movieId`,  `tag` and  `timestamp`. ","ca7d9988":"Best performance was achieved with `last_ratings` method even though lenskit doesn't use users average. Performance with fallbacks is better on both `time` and `users` methods but lacks with `last_ratings` method. As seen with (almost) full table below, overall best performance was achieved when original dataset was subsampled only for most active users and put their last ratings into test set.","1f0ef035":"## Introduction\n\nMovielens 20M is a\n> stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags [1].\n\nIt was last updated on 10\/2016. It was created and maintained by GroupLens, a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities [2]. Dataset was obtained from an existing and active system [movielens.org](https:\/\/movielens.org\/), that uses item-item collaborative filtering techniques to recommend movies based on past user's ratings [3].\n\n[1] [grouplens.org\/datasets\/movielens\/20m](http:\/\/https:\/\/grouplens.org\/datasets\/movielens\/20m\/)\n[2] [grouplens.org\/about\/what-is-grouplens](https:\/\/grouplens.org\/about\/what-is-grouplens\/)\n[3] [movielens.org\/info\/about](https:\/\/movielens.org\/info\/about\/)","9f10bc32":"### Collaborative filtering\nFirst create user-item matrix. This is very sparse matrix therefore I use pandas.SparseDataFrame data structure created from scipy.sparse.csr_matrix (see e.g. [Stack Overflow thread](https:\/\/stackoverflow.com\/questions\/31661604\/efficiently-create-sparse-pivot-tables-in-pandas) and [scipy docs](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix))\n\n#### Item-based CF\nUsing basic prediction furmula from slides \n![alt text](https:\/\/is.muni.cz\/www\/bielovico\/pv254\/itemCF.PNG \"Item-based Collaborative Filtering prediction\")\n\nI set `K=50` as a size of the neighborhood that should be checked and use cosine similarity measure.\n\nFirst fallback for unseen item is users' average. For unseen user, genre average is used. Last fallback is general average.","80750967":"## Recommendation models\nAll results (RMSE scores) will be stored in `results` DataFrame with sampling and division methods as index and recommendation models as columns.","bfa98a3e":"## Possible future work:\n* More exploratory analysis\n* Latent factors\n* Make use of tags\n* Cross-validation\n* Top n recommendation (nDCG metric)","9ccff7bd":"### Item average\n\nPredict average rating per item. Predict genre average rating for new items. Predict overall average rating for unknown genres.","1ab62a2f":"We can see improvement in both `time` and `last_ratings` division methods. No improvement was achieved with `users` method but this is dependant on random user selection. Cross-fold validation could help. ","5f4f9da3":"User added tags were evaluated for relevance by other users. Relevance of a tag is in `genome_scores.csv` table with columns: `movieId`, `tagId` and `relevance`. `tagId` is mapped to a `tag` in `genome_tags.csv` table.","a3bd50d2":"### Ratings\nRatings from users are stored in `rating.csv`.  It contains 4 columns: `userId`, `movieId`, `rating` and `timestamp`. ","70b66625":"Here is a distribution of number of ratings given by each user. Histogram is truncated for users that rated less than 500 movies. ","dd16812a":"## Dataset description\n\nDataset contains six data tables.","726f7d8b":"### Train\/Test division\n#### Time-based division `time`\nDivide dataset by a timestamp with 80\/20 train\/test ratio\n#### Last 10 ratings `last_ratings`\nTest set contains last 10 ratings for each user.\n#### User-based division `users`\nTest set contains randomly selected 20% of users.\n\nDivision methods are stored in `Divider` class as static methods.","8613c0c7":"## Evaluation methods\n#### Timing\nI will use Python's built-in time.time() method on start and end of every execution to track time elapsed when executing.\n#### Metric\nI will use RMSE metric to evaluate models. RMSE = numpy.sqrt(sklearn.metrics.mean_squared_error(true, predicted)). \nScikit-learn implementation is about 2 times faster than simple np.sqrt(((true - predicted) ** 2).mean()) for my test data, even though times are small anyway.\n","047ca1d4":"This is a list of the most rated and top rated movies (among movies with at least 20 ratings):","ce00e061":" 20 least relevant movie-tag pairs are:","71b1b819":"\n# Movielens 20M dataset offline analysis\n\n###### PV254 Recommender systems\n\nThis is a notebook for a project of a PV254 course on Masaryk University Brno, autumn 2018\n\n###### Oliver Velich, 409776","4b4cf723":"Best score was achieved with `last_ratings` division methods, which is expeceted due to the fact that we have all users from train set in test set.","f23eef7a":"#### lenskit\nlenskit is a python package created by the same group that created MovieLens. It consist of a few algorithms so far, started about half a year ago but is actively developed. [docs](https:\/\/lkpy.lenskit.org\/en\/latest\/) and [GitHub](https:\/\/github.com\/lenskit\/lkpy)"}}