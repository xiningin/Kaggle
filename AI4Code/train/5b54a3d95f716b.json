{"cell_type":{"bf0043e1":"code","edc63ef7":"code","0acce2b2":"code","057490e1":"code","392dbc87":"code","36d5e7ac":"code","e645e76f":"code","34072e8d":"code","3fd428db":"code","efee6f5c":"code","a26c9c2e":"code","986405b9":"code","c00bae54":"code","fcd02bb2":"code","1e23f311":"code","5ebfbd8b":"code","401f95b8":"code","4ba1f0e3":"code","8da86442":"code","234158ab":"code","4a815609":"code","ef54804a":"code","8a92c4cd":"code","12e9f25f":"code","63ce80cc":"code","70a03eb4":"code","343ce783":"code","080449cf":"code","c3791686":"code","fda2ec45":"code","b455d42b":"code","1588fce1":"code","539cd976":"code","d5773b36":"code","cc3c21ce":"code","0bd0c782":"code","40132b1b":"markdown","d8948713":"markdown","3bcac475":"markdown","ed218655":"markdown","38f54a75":"markdown","1464423d":"markdown"},"source":{"bf0043e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","edc63ef7":"import spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing, svm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport nltk\nnlp = spacy.load('en_core_web_sm')","0acce2b2":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_df.head()","057490e1":"train_df.isna().sum()","392dbc87":"keywords_dis = train_df[train_df['target'] == 1].keyword.value_counts().sort_values(ascending=False)\nkeywords_dis","36d5e7ac":"keywords_nondis = train_df[train_df['target'] == 0].keyword.value_counts().sort_values(ascending=False)\nkeywords_nondis ","e645e76f":"sns.barplot(y = (keywords_dis \/ keywords_nondis).index, x = (keywords_dis - keywords_nondis))\nsns.set(rc={'figure.figsize':(40,120)})","34072e8d":"train_df.keyword.value_counts().sort_values(ascending=False)","3fd428db":"sns.barplot(y = keywords_dis.index[:40], x = keywords_dis[:40])\nsns.set(rc={'figure.figsize':(20, 10)})","efee6f5c":"sns.barplot(y = keywords_nondis.index[:40], x = keywords_nondis[:40])\nsns.set(rc={'figure.figsize':(20,10)})","a26c9c2e":"train_df['text'] = train_df['text'].str.lower()","986405b9":"train_df['text'] = train_df['text'].str.replace('[^\\w\\s]','')\ntrain_df['text'].head()","c00bae54":"#Importing stopwords from nltk library\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\n# Function to remove the stopwords\ndef stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n# Applying the stopwords to 'text_punct' and store into 'text_stop'\ntrain_df[\"text\"] = train_df[\"text\"].apply(stopwords)","fcd02bb2":"# Checking the first 10 most frequent words\nfrom collections import Counter\ncnt = Counter()\nfor text in train_df[\"text\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\n# Removing the frequent words\nfreq = set([w for (w, wc) in cnt.most_common(10)])\n# function to remove the frequent words\ndef freqwords(text):\n    return \" \".join([word for word in str(text).split() if word not \nin freq])\n# Passing the function freqwords\ntrain_df[\"text\"] = train_df[\"text\"].apply(freqwords)","1e23f311":"# Function for url's\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\n#Passing the function to 'text_rare'\ntrain_df['text'] = train_df['text'].apply(remove_urls)","5ebfbd8b":"from bs4 import BeautifulSoup\n#Function for removing html\ndef html(text):\n    return BeautifulSoup(text, \"lxml\").text\n# Passing the function to 'text_rare'\ntrain_df['text'] = train_df['text'].apply(html)","401f95b8":"train_df['lemmatized'] = train_df[\"text\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)  if not w.is_stop]))","4ba1f0e3":"train_df['text'].to_numpy()","8da86442":"from collections import Counter\ncnt_1 = Counter()\ncnt_2 = Counter()\nfor i, text in enumerate(train_df[\"lemmatized\"].values):\n    if train_df['target'][i] == 1:\n        for word in text.split():\n            cnt_1[word] += 1\n    else:\n        for word in text.split():\n            cnt_2[word] += 1","234158ab":"word_vecs = []\n\nfor i, text in enumerate(train_df[\"lemmatized\"].values):\n    sum_pos_1 = 0\n    sum_neg_1 = 0\n\n    for word in text.split():\n        sum_pos_1 += cnt_1[word]\n        sum_neg_1 += cnt_2[word]\n    word_vecs.append([1, sum_pos_1,sum_neg_1])","4a815609":"train_word_vecs = np.array(word_vecs)","ef54804a":"pd.DataFrame(word_vecs, columns=['sentiment', 'pos', 'neg']).values","8a92c4cd":"#tfidvectorizer = TfidfVectorizer(stop_words= 'english', max_df=0.8, ngram_range=(1,3))\n#train_vectors = tfidvectorizer.fit_transform(train_df['lemmatized'])","12e9f25f":"test_df['text'] = test_df['text'].str.lower()\n\ntest_df['text'] = test_df['text'].str.replace('[^\\w\\s]','')\n\ntest_df[\"text\"] = test_df[\"text\"].apply(stopwords)\n\n# Checking the first 10 most frequent words\nfrom collections import Counter\ncnt = Counter()\nfor text in test_df[\"text\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\n# Removing the frequent words\nfreq = set([w for (w, wc) in cnt.most_common(10)])\n# function to remove the frequent words\ndef freqwords(text):\n    return \" \".join([word for word in str(text).split() if word not \nin freq])\n# Passing the function freqwords\ntest_df[\"text\"] = test_df[\"text\"].apply(freqwords)\ntest_df[\"text\"].head()\n\ntest_df['text'] = test_df['text'].apply(remove_urls)\n\ntest_df['text'] = test_df['text'].apply(html)\n\n\n\ntest_df['lemmatized'] = test_df[\"text\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)  if not w.is_stop]))","63ce80cc":"train_df.tail()","70a03eb4":"word_vecs_test = []\n\nfor i, text in enumerate(test_df[\"lemmatized\"].values):\n    sum_pos_1_test = 0\n    sum_neg_1_test = 0\n\n    for word in text.split():\n            sum_pos_1_test += cnt_1[word]\n            sum_neg_1_test += cnt_2[word]\n    word_vecs_test.append([1, sum_pos_1_test,sum_neg_1_test])\n","343ce783":"test_word_vecs = np.array(word_vecs_test)\ntest_word_vecs","080449cf":"def loss(features, target, alpha=1e-5, epochs=1000):\n    losses = []\n    thetas = []\n    n = features.shape[0]\n    m = features.shape[1]\n    theta = np.zeros((m,1))\n    xT = np.transpose(features)\n    for epoch in range(epochs):\n        sum_loss = 0\n        h = 1 \/ (1 + np.exp(-np.dot(features, theta)))\n        h = np.reshape(h, (-1,))\n        L = -(np.dot((np.transpose(target)), np.log(h)) + np.dot((1 - np.transpose(target)), np.log(1 - h)))\/n\n        theta = (theta + np.reshape(((alpha * (np.dot(xT, (target - h)))) \/ n), (-1,1)))\n        losses.append(L)\n        thetas.append(theta)\n        \n    plt.plot(range(epochs), losses)\n    coeffs = thetas[-1]\n    return print(coeffs)","c3791686":"train_word_vecs","fda2ec45":"model = loss(train_word_vecs, train_df['target'], alpha=1e-5, epochs=1000)\n#from sklearn.naive_bayes import MultinomialNB\n#model = MultinomialNB(alpha=0.01)","b455d42b":"coeffs = np.array([[-0.00058711],\n [ 0.01651384],\n [-0.01408895]])","1588fce1":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","539cd976":"submission['target'] = np.dot(test_word_vecs, coeffs)\nfor i,x in enumerate(submission['target']):\n    if submission['target'][i] > 0.5:\n        submission['target'][i] = 1\n    else:\n        submission['target'][i] = 0\nsubmission['target'] = submission['target'].astype(int)","d5773b36":"submission.head()","cc3c21ce":"submission.to_csv('submission.csv', index=False)","0bd0c782":"test_df.head()","40132b1b":"## despite many missing values, location can be useful for future versions. Tweets from the same location aboout the same keywords can be used","d8948713":"## now do same for test data","3bcac475":"## First let's make some eda","ed218655":"## preprocess the text data","38f54a75":"## most used keywords are different for disaster and non disaster samples which will help algorithm to differentiate","1464423d":"##  barplot for all keywords was too big and words couldn't be differentiated so I limited only top 40"}}