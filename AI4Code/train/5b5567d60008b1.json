{"cell_type":{"c0812d98":"code","e555a315":"code","1d134bfb":"code","b9fb89a6":"code","3685e50a":"code","14ec3d1f":"code","ba41cfd4":"code","854b55c0":"code","990afeee":"code","04d7a59f":"code","7b6094a0":"code","4aae8f64":"code","ed985f01":"code","8d78776d":"code","21171c54":"code","080ecf59":"code","15a7cef9":"code","23ed0a2b":"code","eddfd6f9":"code","da118e2b":"code","3a623b84":"code","6af1d2aa":"code","dde87773":"code","9510dee4":"code","e858635e":"code","a579ab02":"code","cb5e9b08":"code","81f8c2f0":"code","fae4a6ac":"code","b77c65a3":"code","9c96864a":"code","a2f74a06":"code","f69cfb11":"code","6ca71887":"code","5611014b":"code","c57b5c1b":"code","f311628f":"code","b3c63834":"code","28ad3f1b":"code","7a2e3af2":"code","647617a8":"code","f2539c56":"code","ac30caba":"code","3ef48802":"code","da0a91a6":"code","c8b93fb6":"code","6fdeb835":"code","71e83f80":"code","afa2c475":"code","59df88e5":"code","f77d5f54":"code","e81f8d0e":"code","ddbe2e60":"code","c1064301":"code","a28c8bbd":"code","fd250a4c":"code","d5846873":"code","b858c4f2":"code","d0e6ec6e":"code","302c2bec":"code","7bd964aa":"code","860d6622":"code","0ec03351":"code","8d2f732b":"code","5272e1ec":"code","07c072f4":"code","f6d6949e":"code","dfb66338":"code","ac26a170":"code","eaf1bc80":"code","49c5967c":"code","85531d94":"code","5bd22c10":"code","d5378993":"code","fdf50e31":"code","0f87cd2e":"code","cc7f22f5":"code","44ba5e0d":"code","d72b43e8":"code","5dffb878":"code","daa703f2":"code","d8b7bcfd":"code","534b0280":"code","b0d89d31":"code","5b4a7949":"code","d67c11e5":"code","315e7141":"code","3f32ad6c":"code","b1d0fb8a":"code","406b1197":"code","3202cdb0":"code","2fa1b359":"code","7da48849":"code","5935a11d":"code","89457409":"code","97695295":"code","bf1ef204":"code","7b2cec90":"code","c5a44f96":"code","03ce886d":"code","cf0639b1":"code","c0ea520c":"code","55aa1d4f":"markdown","41a38ee9":"markdown","781d664d":"markdown","7398ada0":"markdown","329738f8":"markdown","8f159b37":"markdown","4cca5351":"markdown","aef50752":"markdown","836c0823":"markdown","3083c7f4":"markdown","c9a368e2":"markdown","e3e25242":"markdown","cf8d78c1":"markdown","f8e70155":"markdown","81793be0":"markdown","e9280706":"markdown","eee706eb":"markdown","3a48e5e2":"markdown","bb88b52a":"markdown","93a80054":"markdown","29993b15":"markdown","78954465":"markdown","93067119":"markdown","4b2ff121":"markdown","fa59a3d8":"markdown","b8a3bf92":"markdown","801e7e9b":"markdown","2e430878":"markdown","a80af645":"markdown"},"source":{"c0812d98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e555a315":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nmpl.rcParams['figure.dpi'] = 150","1d134bfb":"df_train = pd.read_csv('..\/input\/titanic\/train.csv', sep = ',')\ndf_train.head()","b9fb89a6":"df_train.shape","3685e50a":"df_train.columns.tolist()","14ec3d1f":"total = df_train.isnull().sum().sort_values(ascending = False)\npercent = (df_train.isnull().sum())\/ (df_train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1,\n                        keys = ['Total', 'Percent'])\nmissing_data.head(5)","ba41cfd4":"df_train[['Cabin', 'Embarked']] = df_train[['Cabin', 'Embarked']].fillna('Unknown')","854b55c0":"total = df_train.isnull().sum().sort_values(ascending = False)\npercent = (df_train.isnull().sum())\/ (df_train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1,\n                        keys = ['Total', 'Percent'])\nmissing_data.head(5)","990afeee":"age_mask = df_train['Age'].isnull()","04d7a59f":"df_train.loc[~age_mask, 'Age']","7b6094a0":"from scipy.stats import norm","4aae8f64":"sns.distplot(df_train.loc[~age_mask, 'Age'], fit=norm);","ed985f01":"print(\"Skewness: %f\" % df_train.loc[~age_mask, 'Age'].skew())\nprint(\"Kurtosis: %f\" % df_train.loc[~age_mask, 'Age'].kurt())","8d78776d":"from scipy.stats import shapiro","21171c54":"import scipy.stats as stats","080ecf59":"res = stats.probplot(df_train.loc[~age_mask, 'Age'], plot=plt)","15a7cef9":"stat, p = shapiro(df_train.loc[~age_mask, 'Age'])\nprint('Shapiro_statistic: ' + str(stat))\nprint('Shapiro_pvalue: ' + str(p))","23ed0a2b":"df_train.loc[~age_mask, 'Age'].describe()","eddfd6f9":"df_train.loc[~age_mask, 'Age'].median()","da118e2b":"df_train.loc[~age_mask, 'Age'].mode()","3a623b84":"median = []\nmedian_boot = []\nfor i in range(2500):\n    sample = np.random.choice(df_train.loc[~age_mask, 'Age'], replace = True, size = 40)\n    med = np.median(sample)\n    median_boot.append(med)\nmedian_boot = np.array(median_boot)","6af1d2aa":"sns.distplot(median_boot, fit=norm);","dde87773":"res = stats.probplot(median_boot, plot=plt)","9510dee4":"replace_index = df_train.loc[age_mask, 'Age'].index.tolist()\nmu, sigma = np.mean(median_boot), np.std(median_boot)\nprint('mu: ' + str(mu))\nprint('sigma: ', str(sigma))","e858635e":"for i in range(len(replace_index)):\n    df_train['Age'][replace_index[i]] = np.random.normal(mu, sigma)\n\ntotal = df_train.isnull().sum().sort_values(ascending = False)\npercent = (df_train.isnull().sum())\/ (df_train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1,\n                        keys = ['Total', 'Percent'])\nmissing_data.head(5)","a579ab02":"df_train.loc[age_mask, 'Age']","cb5e9b08":"name = df_train['Name'].str.split(',', expand = True)\ndf_train['Title'] = name[1].str.split('.', expand = True)[0]\ndf_train['Title'].value_counts()","81f8c2f0":"df_train['Title'] = df_train['Title'].replace(['Dr', 'Rev', 'Major', 'Col', 'Mlle', \n                              'Jonkheer', 'Don', 'Capt', 'the Countess', 'Mme'], 'Misc', regex = True)\ndf_train['Title'] = df_train['Title'].replace(['Ms', 'Lady'], 'Miss', regex = True)\ndf_train['Title'] = df_train['Title'].replace('Sir', 'Mr', regex = True)\ndf_train['Title'].value_counts()","fae4a6ac":"unknown_mask = df_train[df_train['Cabin'] == 'Unknown']['Cabin'].index\ndf_train.loc[unknown_mask, 'Cabin'] = df_train.loc[unknown_mask, 'Cabin'].replace('Unknown', 0)\nknown_mask = df_train[df_train['Cabin'] != 0]['Cabin'].index\ndf_train.loc[known_mask, 'Cabin'] = 1\ndf_train['Cabin'].value_counts()","b77c65a3":"df_train.head()","9c96864a":"df_train['SibSp'].value_counts()","a2f74a06":"df_train['Parch'].value_counts()","f69cfb11":"df_train['FamilyMembers'] = df_train['SibSp'] + df_train['Parch']\ndf_train['FamilyMembers'].value_counts()","6ca71887":"df_train['Embarked'].value_counts()","5611014b":"df_train['Embarked'] = df_train['Embarked'].replace('Unknown', 'S', regex = True)\ndf_train['Embarked'].value_counts()","c57b5c1b":"f, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x = 'Survived', y = 'Age', data = df_train)\nplt.title('Age Boxplot')","f311628f":"f, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x = 'Survived', y = 'Fare', data = df_train)\nplt.title('Fare Boxplot')","b3c63834":"df_train.head()","28ad3f1b":"pd.pivot_table(df_train,values='Survived',columns='Pclass').plot.bar()","7a2e3af2":"pd.pivot_table(df_train,values='Survived',columns='Sex').plot.bar()","647617a8":"pd.pivot_table(df_train,values='Survived',columns='SibSp').plot.bar()","f2539c56":"pd.pivot_table(df_train,values='Survived',columns='Parch').plot.bar()","ac30caba":"pd.pivot_table(df_train,values='Survived',columns='Cabin').plot.bar()","3ef48802":"pd.pivot_table(df_train,values='Survived',columns='Embarked').plot.bar()","da0a91a6":"pd.pivot_table(df_train,values='Survived',columns='FamilyMembers').plot.bar()","c8b93fb6":"pd.pivot_table(df_train,values='Survived',columns='Title').plot.bar()","6fdeb835":"df_train['Title'].value_counts()","71e83f80":"df_train['Sex'] = df_train['Sex'].replace(['female', 'male'], [1, 0], regex = True)\ndf_train['Embarked'] = df_train['Embarked'].replace(['C', 'S', 'Q'], [1, 2, 3], regex = True)\ndf_train['Title'] = df_train['Title'].replace(['Miss', 'Mrs', 'Mr','Master', 'Misc'], [1, 2, 3, 4, 5], regex = True)","afa2c475":"df_train['Sex'].value_counts()","59df88e5":"df_train['Embarked'].value_counts()","f77d5f54":"df_train['Title'].value_counts()","e81f8d0e":"df_train.head()","ddbe2e60":"df_train = df_train.drop(['Name', 'Ticket'], axis = 1)","c1064301":"Survived = pd.Series(df_train['Survived'])","a28c8bbd":"df_train = df_train.drop('Survived', axis = 1)","fd250a4c":"df_train['Survived'] = Survived","d5846873":"df_train.head()","b858c4f2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","d0e6ec6e":"log_model_ridge = LogisticRegression(solver = 'liblinear')\nlog_model_lasso = LogisticRegression(solver = 'liblinear', penalty = 'l1')\nlog_model_elastic = LogisticRegression(solver ='saga', penalty = 'elasticnet', l1_ratio = .5, max_iter = 10000000)","302c2bec":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve\nk_folds_df_train = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 1)","7bd964aa":"def kfold_roc_auc_scores(k_folds, model, X, y):\n    \n    #import roc auc score\n    from sklearn.metrics import roc_auc_score\n    \n    #create list to store our scores\n    train_roc_auc_score = []\n    test_roc_auc_score = []\n\n    #loop through each fold to get auc\n    for train_index, test_index in k_folds.split(X, y):\n        X_cv_train, X_cv_test = X.iloc[train_index], X.iloc[test_index]\n        y_cv_train, y_cv_test = y.iloc[train_index], y.iloc[test_index]\n    \n        # obtain predict probabilities \n        model.fit(X_cv_train, y_cv_train)\n        y_cv_train_predict_proba = model.predict_proba(X_cv_train)\n        y_cv_test_predict_proba = model.predict_proba(X_cv_test)\n    \n        #get roc_auc_score and add it to list\n        train_roc_auc_score.append(roc_auc_score(y_cv_train, y_cv_train_predict_proba[:,1]))\n        test_roc_auc_score.append(roc_auc_score(y_cv_test, y_cv_test_predict_proba[:,1]))\n\n    return train_roc_auc_score, test_roc_auc_score","860d6622":"train_roc_auc_log_ridge, test_roc_auc_log_ridge = kfold_roc_auc_scores(k_folds_df_train, log_model_ridge, df_train.iloc[:, :-1], df_train.iloc[:, -1])\ntrain_roc_auc_log_lasso, test_roc_auc_log_lasso = kfold_roc_auc_scores(k_folds_df_train, log_model_lasso, df_train.iloc[:, :-1], df_train.iloc[:, -1])\ntrain_roc_auc_log_elastic, test_roc_auc_log_elastic = kfold_roc_auc_scores(k_folds_df_train, log_model_elastic, df_train.iloc[:, :-1], df_train.iloc[:, -1])","0ec03351":"np.mean(train_roc_auc_log_ridge)","8d2f732b":"train_auc_means = [np.mean(train_roc_auc_log_ridge), np.mean(train_roc_auc_log_lasso), np.mean(train_roc_auc_log_elastic)]\ntest_auc_means = [np.mean(test_roc_auc_log_ridge), np.mean(test_roc_auc_log_lasso), np.mean(test_roc_auc_log_elastic)]","5272e1ec":"print(train_auc_means)\nprint(test_auc_means)","07c072f4":"barWidth = 0.25\n \n# set height of bar\nbars1 = [train_auc_means[0], test_auc_means[0]]\nbars2 = [train_auc_means[1], test_auc_means[1]]\nbars3 = [train_auc_means[2], test_auc_means[2]]\n \n# Set position of bar on X axis\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \n# Make the plot\nplt.bar(r1, bars1, color='red', width=barWidth, edgecolor = 'white', label = 'Log Reg Ridge Model')\nplt.bar(r2, bars2, color='green', width=barWidth, edgecolor = 'white', label = 'Log Reg Lasso Model')\nplt.bar(r3, bars3, color='blue', width=barWidth, edgecolor = 'white', label = 'Log Reg Elastic Model')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('data', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(bars1))], ['Training', 'Testing'])\nplt.ylim(0, 1.3)\n\n# Create legend & Show graphic\nplt.title('Titanic Logistic Regression ROC AUC Score\\n')\nplt.legend(loc = 'best')\nplt.show()","f6d6949e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","dfb66338":"X_train, X_val, y_train, y_val = train_test_split(df_train.iloc[:, :-1], df_train.iloc[:, -1], \n                                                  test_size = .3, random_state = 100)","ac26a170":"log_model_lasso.fit(X_train, y_train)\ny_pred = log_model_lasso.predict(X_val)\nconfusion_log = confusion_matrix(y_val, y_pred)","eaf1bc80":"accuracy = (138 + 77) \/ (138 + 77 + 32 + 21)\nsensitivity = 138 \/ (138 + 32)\nspecificity = 77 \/ (77 + 21)\nprint('accuracy: ' + str(accuracy) + '\\nsensitivity: ' + str(sensitivity) +'\\nspecificity: ' + str(specificity))","49c5967c":"from sklearn.metrics import roc_curve\nlog_model_lasso_pred_proba = log_model_lasso.predict_proba(X_val)\nfpr, tpr, thresholds = roc_curve(y_val, log_model_lasso_pred_proba[:, -1])","85531d94":"plt.plot(fpr, tpr, '*-')\nplt.plot([0,1], [0,1], 'r--')\nplt.title('Titanic Classification ROC Curve\\n')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(['Logisitic Regression', 'Random Chance'])","5bd22c10":"df_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test.head()","d5378993":"df_test.shape","fdf50e31":"total = df_test.isnull().sum().sort_values(ascending = False)\npercent = (df_test.isnull().sum())\/ (df_test.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1,\n                        keys = ['Total', 'Percent'])\nmissing_data.head(5)","0f87cd2e":"name_test = df_test['Name'].str.split(',', expand = True)\nname_test = name_test[1].str.split('.', expand = True)","cc7f22f5":"df_test['Title'] = name_test[0]\ndf_test['Title'].value_counts()","44ba5e0d":"df_test['Title'] = df_test['Title'].replace(['Col', 'Dona', 'Rev', 'Dr'], 'Misc', regex = True)\ndf_test['Title'] = df_test['Title'].replace('Ms', 'Miss', regex = True)\ndf_test['Title'].value_counts()","d72b43e8":"df_test.columns.tolist()","5dffb878":"df_test['FamilyMembers'] = df_test['SibSp'] + df_test['Parch']","daa703f2":"sns.distplot(df_test['Age'], fit=norm);","d8b7bcfd":"age_test_mask = df_test['Age'].isnull()\nmedian_test = []\nmedian_test_boot = []\nfor i in range(2500):\n    sample_test = np.random.choice(df_test.loc[~age_test_mask, 'Age'], replace = True, size = 40)\n    med_test = np.median(sample_test)\n    median_test_boot.append(med_test)\nmedian_boot = np.array(median_test_boot)","534b0280":"replace_test_index = df_test.loc[age_test_mask, 'Age'].index.tolist()\nmu_test, sigma_test = np.mean(median_test_boot), np.std(median_test_boot)\nprint('mu: ' + str(mu_test))\nprint('sigma: ', str(sigma_test))","b0d89d31":"for i in range(len(replace_test_index)):\n    df_test['Age'][replace_test_index[i]] = np.random.normal(mu_test, sigma_test)\n\ntotal = df_test.isnull().sum().sort_values(ascending = False)\npercent = (df_test.isnull().sum())\/ (df_test.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1,\n                        keys = ['Total', 'Percent'])\nmissing_data.head(5)","5b4a7949":"df_test['Cabin'] = df_test['Cabin'].fillna('Unknown')","d67c11e5":"df_test['Cabin'].value_counts()","315e7141":"unknown_test_mask = df_test[df_test['Cabin'] == 'Unknown']['Cabin'].index\ndf_test.loc[unknown_test_mask, 'Cabin'] = df_test.loc[unknown_test_mask, 'Cabin'].replace('Unknown', 0)\nknown_test_mask = df_test[df_test['Cabin'] != 0]['Cabin'].index\ndf_test.loc[known_test_mask, 'Cabin'] = 1\ndf_test['Cabin'].value_counts()","3f32ad6c":"print(df_train.columns.tolist())\nprint(df_test.columns.tolist())","b1d0fb8a":"df_test = df_test.drop(['Name', 'Ticket'], axis = 1)","406b1197":"df_test['Embarked'].value_counts()","3202cdb0":"df_test['Sex'] = df_test['Sex'].replace(['female', 'male'], [1, 0], regex = True) \ndf_test['Embarked'] = df_test['Embarked'].replace(['C', 'S', 'Q'], [1, 2, 3], regex = True)\ndf_test['Title'] = df_test['Title'].replace(['Miss', 'Mrs', 'Mr','Master', 'Misc'], [1, 2, 3, 4, 5], regex = True)","2fa1b359":"total = df_test.isnull().sum().sort_values(ascending = False)\npercent = (df_test.isnull().sum())\/ (df_test.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1,\n                        keys = ['Total', 'Percent'])\nmissing_data.head(5)","7da48849":"sns.distplot(df_test['Fare'], fit = norm)","5935a11d":"df_test['Fare'] = df_test['Fare'].fillna(np.mean(df_test['Fare']))","89457409":"print(df_train.columns.tolist())\nprint(df_test.columns.tolist())","97695295":"df_test.head()","bf1ef204":"log_model_lasso.fit(df_train.iloc[:, :-1], df_train.iloc[:, -1])","7b2cec90":"df_test_pred = log_model_lasso.predict(df_test)","c5a44f96":"PassengerId = pd.Series(df_test['PassengerId'])\nSurvived = pd.Series(df_test_pred)","03ce886d":"predictions = pd.concat([PassengerId, Survived], axis = 1,keys = ['PassengerId', 'Survived'])\npredictions.head()","cf0639b1":"predictions.to_csv('submission_titanic_log.csv')","c0ea520c":"print('All Done')","55aa1d4f":"# We used a bootstrap method to get a condfidence interval for our parameter estimate of the median. Now we will use this to fill our missing values","41a38ee9":"# We are trying to predict who survived based on these factors","781d664d":"# We should reduce the amount of classes","7398ada0":"# We are corectly predicting 80 percent our True Values<br\/> We are correct predicting 81 pecent of our survivials<br\/> We are correctly predicting 78.5 percent of our deaths","329738f8":"# Our cabin variable has too many categories to be usefull, lets just change it to with cabin and without","8f159b37":"# Let's check if its coming from a normal distribution","4cca5351":"# We can combine the sibsp and parch into one family members columns","aef50752":"# Our pvalue is less that 0.05 so we reject the null hypothesis that our data is coming from a normal distribution. Since there is strong evidence that our data is non normal and right skewed. We will use a boot strap method with the median to replace our missing values. We do this because our Median lies between the mode and mean and better reflects the central tendency of our data","836c0823":"# Lastly, lets take a look at the confusion matrix of the Lasso Model","3083c7f4":"# Our testing data is ready for predictions","c9a368e2":"# Random median imputation of median  value","e3e25242":"# Create grouped bar chart to compare models","cf8d78c1":"# Lets also get the describive statisitics","f8e70155":"# I do not think the names will be very usuful to us, but I will keep the title of the name just incase.","81793be0":"# We will Split the data into training and validation data using StratifiedKFold","e9280706":"# Now lets clean the testing data and submit our predictions using our model","eee706eb":"# As the ROC Curve shows our model is better than random chance at predicting the passengers that survived","3a48e5e2":"# Now lets optimize factors relate to work with predicition algorithms","bb88b52a":"# I want to try a bootstrap method to replace the age variables, but lets take a look at the histogram first to get the distribution of the data","93a80054":"# As the graph shows the ElasticNet Logistic Regression should not be used for this data since it performs the worst on this data. We will go for the Lasso Model since it seems to perform slightly better with the data. Also, it will set the coefficients of variables not useful us to zero","29993b15":"# Data Visualizations","78954465":"# Drop columns we do not need","93067119":"# First lets fill our missing values","4b2ff121":"# The boxplots show that survivals came from low and high fair as well as a variety of ages.","fa59a3d8":"# Our data is slightly skewed.","b8a3bf92":"# Lets create an ROC Curve for our predictions","801e7e9b":"# Now we will impliment our Logistic Regression Model to the data","2e430878":"# Now we have replaced our missing values","a80af645":"# Now we need to change the sex and embarked categories to ordinal categories"}}