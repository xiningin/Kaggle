{"cell_type":{"730d9fce":"code","f839e9a1":"code","a2e1f161":"code","9fd5b189":"code","e9e1f115":"code","b4ae4dff":"code","14784ec6":"code","d287f2a1":"code","688e0f2e":"code","bd3077d1":"code","af38a912":"code","6bb40e56":"code","036e40b0":"code","38d151b4":"code","3cf58f08":"code","5b7a990e":"code","b830385d":"code","7ceff7a9":"code","598641a4":"code","08066f3b":"code","f00a53a8":"code","3c0ad6a1":"code","7d0489ad":"code","e64c0c44":"code","7800cc68":"code","cd1541fe":"code","afeae42a":"code","e4f0e9a0":"code","869d02b8":"code","ef6face3":"code","737b0b87":"code","ca0dab08":"code","8f8d5cd7":"code","e63e6d5f":"code","af135c06":"markdown","bdcde442":"markdown","ff379c81":"markdown","94033cea":"markdown","9d082936":"markdown","d0a4f8e8":"markdown","12cfcf97":"markdown","4442794e":"markdown","56339889":"markdown","ee449b1e":"markdown","b32c656f":"markdown","8fc1c2ff":"markdown","9450dfdb":"markdown","5753ca51":"markdown","7b79e370":"markdown","f450f932":"markdown","e504d7e4":"markdown","4d7e2c96":"markdown","cb9c3804":"markdown","dceea58b":"markdown","bf14919b":"markdown","0d953694":"markdown","a71cd7d5":"markdown","5a0b5734":"markdown"},"source":{"730d9fce":"import os\nimport json\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.io.json import json_normalize\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation, Dropout\nfrom keras import optimizers\n\n%matplotlib inline\npd.options.display.max_columns = 999","f839e9a1":"def add_time_features(df):\n    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n    df['year'] = df['date'].apply(lambda x: x.year)\n    df['month'] = df['date'].apply(lambda x: x.month)\n    df['day'] = df['date'].apply(lambda x: x.day)\n    df['weekday'] = df['date'].apply(lambda x: x.weekday())\n    \n    return df\n\n\ndef plot_metrics(loss, val_loss):\n    fig, (ax1) = plt.subplots(1, 1, sharex='col', figsize=(20,7))\n    ax1.plot(loss, label='Train loss')\n    ax1.plot(val_loss, label='Validation loss')\n    ax1.legend(loc='best')\n    ax1.set_title('Loss')\n    plt.xlabel('Epochs')","a2e1f161":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\n    df = pd.read_csv(csv_path, dtype={'fullVisitorId': 'str'}, nrows=nrows)\n\n    for column in JSON_COLUMNS:\n        df = df.join(pd.DataFrame(df.pop(column).apply(pd.io.json.loads).values.tolist(), index=df.index))\n\n    return df","9fd5b189":"train = load_df(\"..\/input\/train.csv\")\ntest = load_df(\"..\/input\/test.csv\")","e9e1f115":"train.head()","b4ae4dff":"print('TRAIN SET')\nprint('Rows: %s' % train.shape[0])\nprint('Columns: %s' % train.shape[1])\nprint('Features: %s' % train.columns.values)\nprint()\nprint('TEST SET')\nprint('Rows: %s' % test.shape[0])\nprint('Columns: %s' % test.shape[1])\nprint('Features: %s' % test.columns.values)","14784ec6":"train = add_time_features(train)\ntest = add_time_features(test)\n# Convert target feature to 'float' type.\ntrain[\"transactionRevenue\"] = train[\"transactionRevenue\"].astype('float')\ntrain['hits'] = train['hits'].astype(float)\ntest['hits'] = test['hits'].astype(float)\ntrain['pageviews'] = train['pageviews'].astype(float)\ntest['pageviews'] = test['pageviews'].astype(float)","d287f2a1":"# Train\ngp_fullVisitorId_train = train.groupby(['fullVisitorId']).agg('sum')\ngp_fullVisitorId_train['fullVisitorId'] = gp_fullVisitorId_train.index\ngp_fullVisitorId_train['mean_hits_per_day'] = gp_fullVisitorId_train.groupby(['day'])['hits'].transform('mean')\ngp_fullVisitorId_train['mean_pageviews_per_day'] = gp_fullVisitorId_train.groupby(['day'])['pageviews'].transform('mean')\ngp_fullVisitorId_train['sum_hits_per_day'] = gp_fullVisitorId_train.groupby(['day'])['hits'].transform('sum')\ngp_fullVisitorId_train['sum_pageviews_per_day'] = gp_fullVisitorId_train.groupby(['day'])['pageviews'].transform('sum')\ngp_fullVisitorId_train = gp_fullVisitorId_train[['fullVisitorId', 'mean_hits_per_day', 'mean_pageviews_per_day', 'sum_hits_per_day', 'sum_pageviews_per_day']]\ntrain = train.join(gp_fullVisitorId_train, on='fullVisitorId', how='inner', rsuffix='_')\ntrain.drop(['fullVisitorId_'], axis=1, inplace=True)\n\n# Test\ngp_fullVisitorId_test = test.groupby(['fullVisitorId']).agg('sum')\ngp_fullVisitorId_test['fullVisitorId'] = gp_fullVisitorId_test.index\ngp_fullVisitorId_test['mean_hits_per_day'] = gp_fullVisitorId_test.groupby(['day'])['hits'].transform('mean')\ngp_fullVisitorId_test['mean_pageviews_per_day'] = gp_fullVisitorId_test.groupby(['day'])['pageviews'].transform('mean')\ngp_fullVisitorId_test['sum_hits_per_day'] = gp_fullVisitorId_test.groupby(['day'])['hits'].transform('sum')\ngp_fullVisitorId_test['sum_pageviews_per_day'] = gp_fullVisitorId_test.groupby(['day'])['pageviews'].transform('sum')\ngp_fullVisitorId_test = gp_fullVisitorId_test[['fullVisitorId', 'mean_hits_per_day', 'mean_pageviews_per_day', 'sum_hits_per_day', 'sum_pageviews_per_day']]\ntest = test.join(gp_fullVisitorId_test, on='fullVisitorId', how='inner', rsuffix='_')\ntest.drop(['fullVisitorId_'], axis=1, inplace=True)","688e0f2e":"time_agg = train.groupby('date')['transactionRevenue'].agg(['count', 'sum'])\nyear_agg = train.groupby('year')['transactionRevenue'].agg(['sum'])\nmonth_agg = train.groupby('month')['transactionRevenue'].agg(['sum'])\nday_agg = train.groupby('day')['transactionRevenue'].agg(['sum'])\nweekday_agg = train.groupby('weekday')['transactionRevenue'].agg(['count','sum'])","bd3077d1":"plt.figure(figsize=(20,7))\nplt.ticklabel_format(axis='y', style='plain')\nplt.ylabel('Sum transactionRevenue', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.scatter(time_agg.index.values, time_agg['sum'])\nplt.show()","af38a912":"plt.figure(figsize=(20,7))\nplt.ticklabel_format(axis='y', style='plain')\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.scatter(time_agg.index.values, time_agg['count'])\nplt.show()","6bb40e56":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(20,7))\nax1.scatter(year_agg.index.values, year_agg['sum'])\nax1.locator_params(nbins=2)\nax1.ticklabel_format(axis='y', style='plain')\nax1.set_xlabel('Year', fontsize=12)\n\nax2.scatter(month_agg.index.values, month_agg['sum'])\nax2.locator_params(nbins=12)\nax2.ticklabel_format(axis='y', style='plain')\nax2.set_xlabel('Month', fontsize=12)\n\nax3.scatter(day_agg.index.values, day_agg['sum'])\nax3.locator_params(nbins=10)\nax3.ticklabel_format(axis='y', style='plain')\nax3.set_xlabel('Day', fontsize=12)\n\nax4.scatter(weekday_agg.index.values, weekday_agg['sum'])\nax4.locator_params(nbins=7)\nax4.ticklabel_format(axis='y', style='plain')\nax4.set_xlabel('Weekday', fontsize=12)\n\nplt.tight_layout()\nplt.show()","036e40b0":"# Drop stange 'dict' column\ntrain = train.drop(['adwordsClickInfo'], axis=1)\ntest = test.drop(['adwordsClickInfo'], axis=1)\n# Drop column that exists only in train data\ntrain = train.drop(['campaignCode'], axis=1)\n# Input missing transactionRevenue values\ntrain[\"transactionRevenue\"].fillna(0, inplace=True)\n\ntest_ids = test[\"fullVisitorId\"].values","38d151b4":"# Unwanted columns\nunwanted_columns = ['fullVisitorId', 'sessionId', 'visitId', 'visitStartTime', \n                    'browser', 'browserSize', 'browserVersion', 'flashVersion', \n                    'mobileDeviceInfo', 'mobileDeviceMarketingName', 'mobileDeviceModel', \n                    'mobileInputSelector', 'operatingSystemVersion', 'screenColors', \n                    'metro','networkDomain', 'networkLocation', 'adContent', 'campaign', \n                    'isTrueDirect', 'keyword', 'referralPath', 'source', 'operatingSystem', 'day']\n\ntrain = train.drop(unwanted_columns, axis=1)\ntest = test.drop(unwanted_columns, axis=1)\n# Constant columns\nconstant_columns = [c for c in train.columns if train[c].nunique()<=1]\nprint('Columns with constant values: ', constant_columns)\ntrain = train.drop(constant_columns, axis=1)\ntest = test.drop(constant_columns, axis=1)\n# Columns with more than 50% null data\nhigh_null_columns = [c for c in train.columns if train[c].count()<=len(train) * 0.5]\nprint('Columns more than 50% null values: ', high_null_columns)\ntrain = train.drop(high_null_columns, axis=1)\ntest = test.drop(high_null_columns, axis=1)","3cf58f08":"print('TRAIN SET')\nprint('Rows: %s' % train.shape[0])\nprint('Columns: %s' % train.shape[1])\nprint('Features: %s' % train.columns.values)\nprint()\nprint('TEST SET')\nprint('Rows: %s' % test.shape[0])\nprint('Columns: %s' % test.shape[1])\nprint('Features: %s' % test.columns.values)","5b7a990e":"train.head()","b830385d":"categorical_features = ['isMobile', 'month', 'weekday']\ntrain = pd.get_dummies(train, columns=categorical_features)\ntest = pd.get_dummies(test, columns=categorical_features)","7ceff7a9":"# align both data sets (by outer join), to make they have the same amount of features,\n# this is required because of the mismatched categorical values in train and test sets\ntrain, test = train.align(test, join='outer', axis=1)\n\n# replace the nan values added by align for 0\ntrain.replace(to_replace=np.nan, value=0, inplace=True)\ntest.replace(to_replace=np.nan, value=0, inplace=True)","598641a4":"X_train = train[train['date']<=datetime.date(2017, 5, 31)]\nX_val = train[train['date']>datetime.date(2017, 5, 31)]","08066f3b":"# Get labels\nY_train = X_train['transactionRevenue'].values\nY_val = X_val['transactionRevenue'].values\nX_train = X_train.drop(['transactionRevenue'], axis=1)\nX_val = X_val.drop(['transactionRevenue'], axis=1)\ntest = test.drop(['transactionRevenue'], axis=1)\n# Log transform the labels\nY_train = np.log1p(Y_train)\nY_val = np.log1p(Y_val)","f00a53a8":"reduce_features = ['city', 'medium', 'channelGrouping', 'region', \n                   'subContinent', 'country', 'continent', 'deviceCategory', \n                   'year', 'date']\nX_train = X_train.drop(reduce_features, axis=1)\nX_val = X_val.drop(reduce_features, axis=1)\ntest = test.drop(reduce_features, axis=1)","3c0ad6a1":"X_train.head()","7d0489ad":"normalized_features = ['visitNumber', 'hits', 'pageviews', \n                       'mean_hits_per_day', 'mean_pageviews_per_day', \n                       'sum_hits_per_day', 'sum_pageviews_per_day']\n\n# Normalize using Min-Max scaling\nscaler = preprocessing.MinMaxScaler()\nX_train[normalized_features] = scaler.fit_transform(X_train[normalized_features])\nX_val[normalized_features] = scaler.transform(X_val[normalized_features])\ntest[normalized_features] = scaler.transform(test[normalized_features])","e64c0c44":"X_train.head()","7800cc68":"BATCH_SIZE = 64\nEPOCHS = 100\nLEARNING_RATE = 0.0003","cd1541fe":"model = Sequential()\nmodel.add(Dense(256, kernel_initializer='glorot_normal', activation='relu', input_dim=X_train.shape[1]))\nmodel.add(Dense(128, kernel_initializer='glorot_normal', activation='relu'))\nmodel.add(Dense(1))","afeae42a":"adam = optimizers.adam(lr=LEARNING_RATE)\nmodel.compile(loss='mse', optimizer=adam)","e4f0e9a0":"print('Dataset size: %s' % X_train.shape[0])\nprint('Epochs: %s' % EPOCHS)\nprint('Learning rate: %s' % LEARNING_RATE)\nprint('Batch size: %s' % BATCH_SIZE)\nprint('Input dimension: %s' % X_train.shape[1])\nprint('Features used: %s' % X_train.columns.values)","869d02b8":"model.summary()","ef6face3":"history = model.fit(x=X_train.values, y=Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, \n                    verbose=1, validation_data=(X_val.values, Y_val))","737b0b87":"val_predictions = model.predict(X_val)\nmse = mean_squared_error(val_predictions, Y_val)\nrmse = np.sqrt(mean_squared_error(val_predictions, Y_val))\n\nprint('Model validation metrics')\nprint('MSE: %.2f' % mse)\nprint('RMSE: %.2f' % rmse)","ca0dab08":"plot_metrics(history.history['loss'], history.history['val_loss'])","8f8d5cd7":"predictions = model.predict(test)\n\nsubmission = pd.DataFrame({\"fullVisitorId\":test_ids})\npredictions[predictions<0] = 0\nsubmission[\"PredictedLogRevenue\"] = predictions\nsubmission = submission.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsubmission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"]\nsubmission.to_csv(\"submission.csv\", index=False)","e63e6d5f":"submission.head(10)","af135c06":"### Drop unwanted columns","bdcde442":"### The let's do some cleaning","ff379c81":"### Exploratory data analysis","94033cea":"Again we had higher frequency at a similar time period.","9d082936":"Function to load and convert files borrowed from this [kernel](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook), thanks!","d0a4f8e8":"#### Let's take a look at other time features.","12cfcf97":"### Auxiliar functions","4442794e":"### Dependencies","56339889":"Seems we had more transactions on late 2016 and early 2017, date features seems to be a good addition to our model.","ee449b1e":"### This is our new data with some cleaning and engineering.","b32c656f":"#### And here count of our target feature \"transactionRevenue\".","8fc1c2ff":"## Deep Learning with Keras - Google Analytics Customer Revenue Prediction\n* Note: this is just a starting point, there's a lot of work to be done.\n* A begginer version with [LGBM](https:\/\/www.kaggle.com\/dimitreoliveira\/lgbm-google-store-revenue-prediction)","9450dfdb":"#### Here is sum of our tagert feature \"transactionRevenue\" through the time.","5753ca51":"### One-hot encode categorical data","7b79e370":"### Agregated features","f450f932":"### Model metrics and plot","e504d7e4":"#### Let's take a look at our target value through the time.","4d7e2c96":"### This is how our data looks like","cb9c3804":"### Split data in train and validation by date\n* This time based split will result in approximated 85% train and 15% validation.","dceea58b":"### Feature engineering","bf14919b":"### About the engineered time features\n* Year: It seem transactions had a large increase from 2016 to 2017\n* Month: Lager transaction on december seems ok, but about months but im not sure why high values on april and august (maybe because of easter (april) or Tax-free weekend, back-to-school season(august)?)\n* Day: Here it seems that not really important is going on, seems this features can be discarded.\n* Weekday: Something strange is going on here, seems that weekends have less transactions?","0d953694":"### About the train data","a71cd7d5":"### Normalize the data","5a0b5734":"### Model\n* Now let's try some deep learning to model our data."}}