{"cell_type":{"16cc5552":"code","d48d6462":"code","231e9932":"code","ac6420ed":"code","2616bf5d":"code","504a41b9":"code","37bdb0e8":"code","305976aa":"code","1d8ff3e9":"code","ace9eb32":"code","c09c3d88":"code","a4b410c5":"code","66a62c4c":"code","13bd20f1":"code","b0e11457":"code","a091a7f6":"code","4f50f247":"code","20e80644":"code","da485503":"code","c8803660":"markdown","262717a8":"markdown","4e8a52e3":"markdown","81533ad4":"markdown","2cd04a0b":"markdown","c3197c20":"markdown","ea9e331d":"markdown","20704c85":"markdown","1272708a":"markdown","8560e462":"markdown","f6df452c":"markdown","5f23adb1":"markdown","659c87c3":"markdown","4fd4c755":"markdown","362ba0a1":"markdown"},"source":{"16cc5552":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport os\nimport cv2 as cv\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_files\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder as ohe\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.manifold import TSNE\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom keras.layers import Dense,BatchNormalization,Dropout,Conv2D,MaxPooling2D,Flatten,GlobalMaxPool2D\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential","d48d6462":"%cd \/kaggle\/input\/english-alphabets\/english_alphabets\n","231e9932":"alphabets = os.listdir('\/kaggle\/input\/english-alphabets\/english_alphabets')\nmain_list = []\nfor i in range(len(alphabets)):\n    sub_list=[]\n    os.chdir('\/kaggle\/input\/english-alphabets\/english_alphabets\/{}'.format(alphabets[i]))\n    imgs = os.listdir()\n    for j in imgs:\n        img = cv.imread(j,-1).flatten()\n        sub_list.append([img,alphabets[i]])\n    main_list.append(sub_list)","ac6420ed":"df_main_list = []\nfor i in range(26):\n    df = pd.DataFrame(main_list[i])\n    df_main_list.append(df)\ndf = pd.concat(df_main_list,axis=0,ignore_index=True)\ndf = pd.concat([df[0].apply(pd.Series),df[1]],axis=1,ignore_index=True) # Explode the lists\n# shuffle all the  rows\ndf = df.sample(frac=1).reset_index(drop=True)","2616bf5d":"df.head()","504a41b9":"Y = df.pop(1156)\nx_train,x_test,y_train,y_test = train_test_split(df,Y,test_size=0.3,stratify=Y)","37bdb0e8":" def show(image):\n    \"\"\"\n    Pass the index number of the row\/datapoint to view its plot\n    \"\"\"\n    a = x_train.iloc[image].values.reshape(34,34)\n    plt.imshow(a)\n    plt.show()\nshow(120)","305976aa":"# Get basic info about the dataframe\ndf.info()","1d8ff3e9":"# number of datapoints available for each class\nY.value_counts()","ace9eb32":"# Check whether there are any null\/missing values.\nprint(f\"The number of Null values in the dataframe : {df.isna().values.flatten().sum()}\")","c09c3d88":"X = StandardScaler().fit_transform(df)\npca = PCA(n_components=2)\nvisual_data = pca.fit_transform(X)\nplt.figure(figsize=(10,10))\nsns.scatterplot(visual_data[:,0],visual_data[:,1],hue=Y)\nplt.title(\"Scatterplot after PCA\")\nplt.show()","a4b410c5":"tsne = TSNE(n_components=2,perplexity=5,verbose=1)\nvisual_data = tsne.fit_transform(X)\nplt.figure(figsize=(10,10))\nsns.scatterplot(visual_data[:,0],visual_data[:,1],hue=Y)\nplt.title(\"Scatterplot after TSNE\")\nplt.show()","66a62c4c":"svm_pipe = make_pipeline(StandardScaler(),SVC())\nsvm_pipe.fit(x_train,y_train)\ny_pred = svm_pipe.predict(x_test)\nprint(f\"ACCURACY : {accuracy_score(y_test,y_pred)}\")","13bd20f1":"lr_pipe = make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000))\nlr_pipe.fit(x_train,y_train)\ny_pred = lr_pipe.predict(x_test)\nprint(f\"ACCURACY : {accuracy_score(y_test,y_pred)}\")","b0e11457":"xgb_pipe = make_pipeline(StandardScaler(),xgb.XGBClassifier())\nxgb_pipe.fit(x_train,y_train)\ny_pred = xgb_pipe.predict(x_test)\nprint(f\"ACCURACY : {accuracy_score(y_test,y_pred)}\")","a091a7f6":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)\nmodel = Sequential()\nmodel.add(Dense(1024,input_shape=(1156,),activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(26,activation='softmax'))\n                \nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\ny_train = pd.get_dummies(y_train)\ny_test = pd.get_dummies(y_test)\n\nhistory = model.fit(x_train,y_train,epochs=25,batch_size=512,validation_split=0.1)\n\n","4f50f247":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","20e80644":"model.evaluate(x_test,y_test,batch_size=512,verbose=1)","da485503":"train_data_dir = '\/kaggle\/input\/english-alphabets\/english_alphabets\/'\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(34,34),\n    class_mode='categorical',\n    color_mode='grayscale',\n    subset='training')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir, # same directory as training data\n    target_size=(34,34),\n    class_mode='categorical',\n    color_mode='grayscale',\n    subset='validation')\n\n# Define Model\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(34,34,1),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(32, (3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# model.add(Conv2D(64, (3, 3),activation='relu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(GlobalMaxPool2D())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(26,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nhistory = model.fit_generator(\n    train_generator,\n    validation_data = validation_generator, \n    epochs = 15)\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","c8803660":"> Dosen't give us much to interpret","262717a8":"# Handwritten Character Recognition (College Project)","4e8a52e3":"### Change DIR to the data folder. This dataset has been prepared by me and my teammates.","81533ad4":"### Now, Lets read all the images and store them as a big list.\n    Directory Structure:\n    \n    [English_alphabets]\n                |\n                |____[A]\n                |    |\n                |    |__1765.jpg\n                |    |__1764.jpg\n                |\n                |\n                |____[B]\n                |\n                |\n                |____[Z]","2cd04a0b":"## Import Required Packages","c3197c20":"## CNN","ea9e331d":"### Visualization using PCA and T-SNE","20704c85":"### Let's plot some alphabets to get the general idea.","1272708a":"### DataFrame creation for easy processing and modelling.","8560e462":"## SVM","f6df452c":"# Exploratory Data Analysis","5f23adb1":"> This plot gives us a little clearer picture of the dataset, This is a lot better than PCA.","659c87c3":"## Logistic regression","4fd4c755":"# Modeling using various ML techniques","362ba0a1":"### Split Train and test sets."}}