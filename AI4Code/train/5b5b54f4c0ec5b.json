{"cell_type":{"5da74f58":"code","664cf2ac":"code","67ffe89e":"code","cb115471":"code","3207c7ea":"code","ff317bd9":"code","49b4725a":"code","6daf8006":"code","b7fbd94f":"code","44c6e554":"code","d8caa1f4":"code","13238344":"code","0a05293c":"code","92228e52":"code","45774dad":"code","d4ae1460":"code","9a477723":"code","683f68d9":"code","947116d6":"code","05c19dcc":"code","9ec34249":"code","29c4d392":"code","01c7b595":"code","12716a39":"code","d157d578":"code","d6106651":"code","45fcf90b":"code","c02a5821":"code","0da10b71":"code","bad5139e":"code","c870375b":"code","50270e06":"code","e9346fb9":"code","20660828":"code","4965efac":"code","69ebe43a":"code","f3937aef":"code","42876888":"code","2f808b97":"code","f612ff2e":"markdown","ab2107d6":"markdown","2b96daea":"markdown","eb8ad76f":"markdown","a82b0315":"markdown","b64e29ed":"markdown","b14cdc87":"markdown","36dbd8b3":"markdown","874e430d":"markdown","89287099":"markdown","772978f8":"markdown","da16cb81":"markdown","14a47897":"markdown","b5d91342":"markdown","2f870012":"markdown","6c08290a":"markdown","e139ce57":"markdown","4aedc774":"markdown","41110a3a":"markdown","d207fd19":"markdown","3a5516b9":"markdown","37a1a0cd":"markdown","8c2f0bfa":"markdown","14a1dff8":"markdown","2cd0347f":"markdown","cb249d87":"markdown","a4942e45":"markdown","14668287":"markdown","63e6c73c":"markdown","78b35c57":"markdown","81513369":"markdown","bda11b54":"markdown","379e3763":"markdown","6c9bb852":"markdown","d41918a0":"markdown","eb40e050":"markdown","89bfd768":"markdown","976230c6":"markdown","299d81d6":"markdown","989420bc":"markdown","7f5f2b0c":"markdown","3b491ec5":"markdown","f24188b9":"markdown","34261ba0":"markdown"},"source":{"5da74f58":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nimport graphviz\nimport seaborn as sns\nimport os\nimport plotly_express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')","664cf2ac":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","67ffe89e":"df = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndf","cb115471":"df.info()","3207c7ea":"df.describe()","ff317bd9":"print(\"Total number of missing values\")\nprint(30 * \"-\")\nprint(df.isna().sum())\nprint(30 * \"-\")\nprint(\"Total missing values are:\", df.isna().sum().sum())\nprint(30 * \"-\")","49b4725a":"df.drop(columns = ['Id'], axis = 1, inplace = True)\ndf","6daf8006":"print(\"The different Species in the dataset are:\", df['Species'].unique())\nprint(\"The total number of unique species are:\", df['Species'].nunique())","b7fbd94f":"count_list = [(df.Species == 'Iris-setosa').sum(), (df.Species == 'Iris-versicolor').sum(), (df.Species == 'Iris-virginica').sum()]\nlabel_list = list(df['Species'].unique())\nplt.figure(figsize = (10, 7))\nplt.pie(count_list, labels = label_list, autopct = \"%.2f %%\", startangle = 90, explode = (0.1, 0.1, 0.0), textprops = {'fontsize': 12})\nplt.title('Distribution of Classes', fontsize = 20)\nplt.show()","44c6e554":"print(\"The distribution of the classes is:\\n\", df['Species'].value_counts(), sep = \"\")","d8caa1f4":"fig = plt.figure(figsize = (15, 10))\n\nax1 = fig.add_subplot(2, 2, 1)\nax2 = fig.add_subplot(2, 2, 2)\nax3 = fig.add_subplot(2, 2, 3)\nax4 = fig.add_subplot(2, 2, 4)\n\nax1.plot((df[df['Species'] == 'Iris-setosa'])['SepalLengthCm'], np.zeros_like((df[df['Species'] == 'Iris-setosa'])['SepalLengthCm']), 'ro')\nax1.plot((df[df['Species'] == 'Iris-virginica'])['SepalLengthCm'], np.zeros_like((df[df['Species'] == 'Iris-virginica'])['SepalLengthCm']), 'go')\nax1.plot((df[df['Species'] == 'Iris-versicolor'])['SepalLengthCm'], np.zeros_like((df[df['Species'] == 'Iris-versicolor'])['SepalLengthCm']), 'bo')\nax1.set_title('Analysis of Sepal Length', size = 20)\nax1.get_yaxis().set_visible(False)\n\nax2.plot((df[df['Species'] == 'Iris-setosa'])['SepalWidthCm'], np.zeros_like((df[df['Species'] == 'Iris-setosa'])['SepalWidthCm']), 'ro')\nax2.plot((df[df['Species'] == 'Iris-virginica'])['SepalWidthCm'], np.zeros_like((df[df['Species'] == 'Iris-virginica'])['SepalWidthCm']), 'go')\nax2.plot((df[df['Species'] == 'Iris-versicolor'])['SepalWidthCm'], np.zeros_like((df[df['Species'] == 'Iris-versicolor'])['SepalWidthCm']), 'bo')\nax2.set_title('Analysis of Sepal Width', size = 20)\nax2.get_yaxis().set_visible(False)\n\nax3.plot((df[df['Species'] == 'Iris-setosa'])['PetalLengthCm'], np.zeros_like((df[df['Species'] == 'Iris-setosa'])['PetalLengthCm']), 'ro')\nax3.plot((df[df['Species'] == 'Iris-virginica'])['PetalLengthCm'], np.zeros_like((df[df['Species'] == 'Iris-virginica'])['PetalLengthCm']), 'go')\nax3.plot((df[df['Species'] == 'Iris-versicolor'])['PetalLengthCm'], np.zeros_like((df[df['Species'] == 'Iris-versicolor'])['PetalLengthCm']), 'bo')\nax3.set_title('Analysis of Petal Length', size = 20)\nax3.get_yaxis().set_visible(False)\n\nax4.plot((df[df['Species'] == 'Iris-setosa'])['PetalWidthCm'], np.zeros_like((df[df['Species'] == 'Iris-setosa'])['PetalWidthCm']), 'ro')\nax4.plot((df[df['Species'] == 'Iris-virginica'])['PetalWidthCm'], np.zeros_like((df[df['Species'] == 'Iris-virginica'])['PetalWidthCm']), 'go')\nax4.plot((df[df['Species'] == 'Iris-versicolor'])['PetalWidthCm'], np.zeros_like((df[df['Species'] == 'Iris-versicolor'])['PetalWidthCm']), 'bo')\nax4.set_title('Analysis of Petal Width', size = 20)\nax4.get_yaxis().set_visible(False)\n\nplt.subplots_adjust(left = 0.1,\n                    bottom = 0.1, \n                    right = 0.9, \n                    top = 0.9, \n                    wspace = 0.4, \n                    hspace = 0.4)\nplt.show()","13238344":"plt.figure(figsize=(15, 20))\n\ndef create_boxplot(feature):\n    sns.boxplot(data = df, x = 'Species', y = feature)\n    if(feature == 'SepalLengthCm'):\n        feature = 'Sepal Length'\n    if(feature == 'SepalWidthCm'):\n        feature = 'Sepal Width'\n    if(feature == 'PetalLengthCm'):\n        feature = 'Petal Length'\n    if(feature == 'PetalWidthCm'):\n        feature = 'Petal Width'\n    plt.title('Boxplot for ' + feature, fontsize = 20)\n    plt.xlabel('Species', fontsize = 15)\n    plt.ylabel(feature, fontsize = 15)\n      \nplt.subplot(221)\ncreate_boxplot('SepalLengthCm')\n  \nplt.subplot(222)\ncreate_boxplot('SepalWidthCm')\n  \nplt.subplot(223)\ncreate_boxplot('PetalLengthCm')\n  \nplt.subplot(224)\ncreate_boxplot('PetalWidthCm')\n  \nplt.show()","0a05293c":"plt.figure(figsize=(15, 20))\n\ndef create_violinplot(feature):\n    sns.violinplot(data = df, x = 'Species', y = feature)\n    if(feature == 'SepalLengthCm'):\n        feature = 'Sepal Length'\n    if(feature == 'SepalWidthCm'):\n        feature = 'Sepal Width'\n    if(feature == 'PetalLengthCm'):\n        feature = 'Petal Length'\n    if(feature == 'PetalWidthCm'):\n        feature = 'Petal Width'\n    plt.title('Violinplot for ' + feature, fontsize = 20)\n    plt.xlabel('Species', fontsize = 15)\n    plt.ylabel(feature, fontsize = 15)\n      \nplt.subplot(221)\ncreate_violinplot('SepalLengthCm')\n  \nplt.subplot(222)\ncreate_violinplot('SepalWidthCm')\n  \nplt.subplot(223)\ncreate_violinplot('PetalLengthCm')\n  \nplt.subplot(224)\ncreate_violinplot('PetalWidthCm')\n  \nplt.show()","92228e52":"plt.figure(figsize = (10, 7))\nsns.scatterplot(x = 'PetalLengthCm', y = 'PetalWidthCm', hue = 'Species', data = df, s = 65)\nplt.legend(bbox_to_anchor = (1, 1), loc = 2, fontsize = 15)\nplt.title('Petal Length VS Petal Width', fontsize = 20)\nplt.xlabel('Petal Length', fontsize = 15)\nplt.ylabel('Petal Width', fontsize = 15)\nplt.show()","45774dad":"plt.figure(figsize = (10, 7))\nsns.scatterplot(x = 'SepalLengthCm', y = 'SepalWidthCm', hue = 'Species', data = df, s = 65)\nplt.legend(bbox_to_anchor = (1, 1), loc = 2, fontsize = 15)\nplt.title('Sepal Length VS Sepal Width', fontsize = 20)\nplt.xlabel('Sepal Length', fontsize = 15)\nplt.ylabel('Sepal Width', fontsize = 15)\nplt.show()","d4ae1460":"sns.pairplot(df, hue = 'Species', height = 5)\nplt.show()","9a477723":"fig = plt.figure(figsize=(20, 20))\n\ndef create_3d(feature1, feature2, feature3):\n    \n    ax.scatter3D((df[df['Species'] == 'Iris-setosa'])[feature1], \n                 (df[df['Species'] == 'Iris-versicolor'])[feature1], \n                 (df[df['Species'] == 'Iris-virginica'])[feature1], \n                 color = 'r')\n    \n    ax.scatter3D((df[df['Species'] == 'Iris-setosa'])[feature2], \n                 (df[df['Species'] == 'Iris-versicolor'])[feature2], \n                 (df[df['Species'] == 'Iris-virginica'])[feature2], \n                 color = 'g')\n    \n    ax.scatter3D((df[df['Species'] == 'Iris-setosa'])[feature3], \n                 (df[df['Species'] == 'Iris-versicolor'])[feature3], \n                 (df[df['Species'] == 'Iris-virginica'])[feature3],\n                 color = 'b')\n    \n    if(feature1 == 'SepalLengthCm'):\n        feature1 = 'Sepal Length'\n    if(feature1 == 'SepalWidthCm'):\n        feature1 = 'Sepal Width'\n    if(feature1 == 'PetalLengthCm'):\n        feature1 = 'Petal Length'\n    if(feature1 == 'PetalWidthCm'):\n        feature1 = 'Petal Width'\n    if(feature2 == 'SepalLengthCm'):\n        feature2 = 'Sepal Length'\n    if(feature2 == 'SepalWidthCm'):\n        feature2 = 'Sepal Width'\n    if(feature2 == 'PetalLengthCm'):\n        feature2 = 'Petal Length'\n    if(feature2 == 'PetalWidthCm'):\n        feature2 = 'Petal Width'\n    if(feature3 == 'SepalLengthCm'):\n        feature3 = 'Sepal Length'\n    if(feature3 == 'SepalWidthCm'):\n        feature3 = 'Sepal Width'\n    if(feature3 == 'PetalLengthCm'):\n        feature3 = 'Petal Length'\n    if(feature3 == 'PetalWidthCm'):\n        feature3 = 'Petal Width'\n        \n    plt.title(feature1 + ' VS ' + feature2 + ' VS ' + feature3, fontweight = 'bold', size = 20)\n    ax.set_xlabel(feature1, fontweight = 'bold', size = 15)\n    ax.set_ylabel(feature2, fontweight = 'bold', size = 15)\n    ax.set_zlabel(feature3, fontweight = 'bold', size = 15)\n    \nax = fig.add_subplot(2, 2, 1, projection='3d')\ncreate_3d('SepalWidthCm', 'SepalLengthCm', 'PetalWidthCm')\n\nax = fig.add_subplot(2, 2, 2, projection='3d')\ncreate_3d('SepalWidthCm', 'SepalLengthCm', 'PetalLengthCm')\n\nax = fig.add_subplot(2, 2, 3, projection='3d')\ncreate_3d('PetalWidthCm', 'PetalLengthCm', 'SepalWidthCm')\n\nax = fig.add_subplot(2, 2, 4, projection='3d')\ncreate_3d('PetalWidthCm', 'PetalLengthCm', 'SepalLengthCm')\n\nfig.tight_layout()\nplt.show()","683f68d9":"plt.figure(figsize = (15, 10))\nsns.heatmap(df.corr(), cmap = 'Blues', square = True, annot = True)\nplt.title(\"Visualizing Correlations\", size = 20)\nplt.show()","947116d6":"X = df.drop(['Species'], axis = 1)\ny = df['Species']\nprint(X, \"\\n\\n\", y)","05c19dcc":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","9ec34249":"label_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)","29c4d392":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 0)","01c7b595":"def generate_results(model, predictions, name):\n    cl_rep = classification_report(y_test, predictions)\n    print(\"\\nThe classification report for \" + name + \" is:\", cl_rep, sep = \"\\n\")\n    cm_model = confusion_matrix(y_test, predictions)\n    plt.figure(figsize = (8, 6))\n    sns.heatmap(cm_model, annot = True, cmap = 'Blues', annot_kws = {'size': 15}, square = True)\n    plt.title('Confusion Matrix for ' + name, size = 15)\n    plt.xticks(size = 15)\n    plt.yticks(size = 15)\n    plt.show()    ","12716a39":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nacc_lr = lr.score(X_test, y_test)\nprint(\"The accuracy for Logistic Regression is:\", acc_lr * 100, \"%\")\ngenerate_results(lr, y_pred, 'Logistic Regression')","d157d578":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = knn.score(X_test, y_test)\nprint(\"The accuracy for KNN is:\", acc_knn * 100, \"%\")","d6106651":"store_acc = []\nneighbors = [i for i in range(1, 11)]\nfor i in range(len(neighbors)):\n    knn_improved = KNeighborsClassifier(n_neighbors = neighbors[i])\n    knn_improved.fit(X_train, y_train)\n    y_pred = knn_improved.predict(X_test)\n    acc_knn_for_diff_values = round((knn_improved.score(X_test, y_test)) * 100, 2)\n    store_acc.append(acc_knn_for_diff_values)\nplt.figure(figsize = (10, 7))\nplt.plot(neighbors, store_acc, color = 'blue', marker = 'o')\nplt.title('N Neighbors VS Score', fontsize = 20)\nplt.xlabel('N Neighbors', fontsize = 15)\nplt.ylabel('Score', fontsize = 15)\nplt.grid(True)\nplt.show()","45fcf90b":"knn_improved = KNeighborsClassifier(n_neighbors = 7)\nknn_improved.fit(X_train, y_train)\ny_pred = knn_improved.predict(X_test)\nacc_knn_imp = knn_improved.score(X_test, y_test)\nprint(\"The accuracy for KNN is:\", acc_knn_imp * 100, \"%\")\ngenerate_results(knn_improved, y_pred, 'KNN (n_neighbors = 7)')","c02a5821":"rnc = RadiusNeighborsClassifier()\nrnc.fit(X_train, y_train)\ny_pred = rnc.predict(X_test)\nacc_rnc = rnc.score(X_test, y_test)\nprint('The accuracy for Radius Neighbors Classifier is:', acc_rnc * 100, \"%\")\ngenerate_results(rnc, y_pred, 'Radius Neighbors Classifier')","0da10b71":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\nacc_gnb = gnb.score(X_test, y_test)\nprint('The accuracy for Gaussian Naive Bayes is:', acc_gnb * 100, '%')\ngenerate_results(gnb, y_pred, 'Gaussian Naive Bayes')","bad5139e":"svc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = svc.score(X_test, y_test)\nprint('The accuracy for SVC is:', acc_svc * 100, '%')\ngenerate_results(svc, y_pred, 'Support Vector Classifier')","c870375b":"qda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train, y_train)\ny_pred = qda.predict(X_test)\nacc_qda = qda.score(X_test, y_test)\nprint('The accuracy for Quadratic Discriminant Analysis is:', acc_qda * 100, '%')\ngenerate_results(qda, y_pred, 'QDA')","50270e06":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_pred = dtc.predict(X_test)\nacc_dtc = dtc.score(X_test, y_test)\nprint('The accuracy of the Decision Tree Classifier is:', acc_dtc * 100, '%')\ngenerate_results(dtc, y_pred, 'Decision Tree Classifier')","e9346fb9":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nacc_rfc = rfc.score(X_test, y_test)\nprint('The accuracy of the Random Forest Classifier is:', acc_rfc * 100, '%')\ngenerate_results(rfc, y_pred, 'Random Forest Classifier')","20660828":"abc = AdaBoostClassifier()\nabc.fit(X_train, y_train)\ny_pred = abc.predict(X_test)\nacc_abc = abc.score(X_test, y_test)\nprint('The accuracy for Ada Boost Classifier is:', acc_abc * 100, '%')\ngenerate_results(abc, y_pred, 'Ada Boost Classifier')","4965efac":"etc = ExtraTreesClassifier(random_state = 0)\netc.fit(X_train, y_train)\ny_pred = etc.predict(X_test)\nacc_etc = etc.score(X_test, y_test)\nprint('The accuracy for Etra Trees Classifier is:', acc_etc * 100, '%')\ngenerate_results(etc, y_pred, 'Extra Tress Classifier')","69ebe43a":"bc = BaggingClassifier()\nbc.fit(X_train, y_train)\ny_pred = bc.predict(X_test)\nacc_bc = bc.score(X_test, y_test)\nprint('The accuracy for Bagging Classifier is:', acc_bc * 100, '%')\ngenerate_results(bc, y_pred, 'Bagging Classifier')","f3937aef":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ny_pred = gbc.predict(X_test)\nacc_gbc = gbc.score(X_test, y_test)\nprint('The accuracy for the Gradient Boosting Classifier is:', acc_gbc * 100, '%')\ngenerate_results(gbc, y_pred, 'Gradient Boosting Classifier')","42876888":"xgbc = XGBClassifier(n_jobs = -1, silent = True, verbosity = 0)\nxgbc.fit(X_train, y_train)\ny_pred = xgbc.predict(X_test)\nacc_xgbc = xgbc.score(X_test, y_test)\nprint('The accuracy for XGB Classifier is:', acc_xgbc * 100, '%')\ngenerate_results(xgbc, y_pred, 'XGB Classifier')","2f808b97":"data = {'Logistic Regression': acc_lr * 100, \n        'KNN (default parameters)': acc_knn * 100, \n        'KNN (n_neighbors = 7)': acc_knn_imp * 100, \n        'Radius Neighbors Classifier': acc_rnc * 100, \n        'Gaussian Naive Bayes': acc_gnb * 100,\n        'Support Vector Classifier': acc_svc * 100, \n        'Quadratic Discriminant Analysis': acc_qda * 100, \n        'Decision Tree Classifier': acc_dtc * 100, \n        'Random Forest Classifier': acc_rfc * 100,\n        'Ada Boost Classifier': acc_abc * 100, \n        'Extra Trees Classifier': acc_etc * 100,\n        'Bagging Classifier': acc_bc * 100, \n        'Gradient Boosting Classifier': acc_gbc * 100,\n        'XGBoost Classifier': acc_xgbc * 100}\ndata = dict(sorted(data.items(), key = lambda x: x[1], reverse = True))\nmodels = list(data.keys())\nscore = list(data.values())\nfig = plt.figure(figsize = (15, 10))\nsns.barplot(x = score, y = models)\nplt.xlabel(\"Models Used\", size = 20)\nplt.xticks(size = 12)\nplt.ylabel(\"Score\", size = 20)\nplt.yticks(size = 12)\nplt.title(\"Score for Different Models\", size = 25)\nplt.show()","f612ff2e":"# KNN\nKNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label.","ab2107d6":"**Observations**:\nIn terms of the median the observations from the violin-plot are similar to that of the box-plot.\n - `SepalLengthCm`: There is a high probability that Iris-setosa will have a sepal length of 5 cm.\n - `SepalWidthCm`: For Iris-versicolor and Iris-virginica there is a high probability that they will have approximately the same sepal width. Thus identification of the species based on this feature only might not yield good results.\n - `PetalLengthCm`: Most of petal lengths for Iris-setosa are about 1.5 cm.\n - `PetalWidthCm`: There is a high probability that the petal width for Iris-setosa species would be approximately 0.25 cm. The violin plots for Iris-versicolor and Iris-virginica are not as broad as that of Iris-setosa.","2b96daea":"# Logistic Regression\nLogistic regression, despite its name, is a classification model rather than regression model. It is a process of modeling the probability of a discrete outcome given input variables. ","eb8ad76f":"A violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data. It is used to visualize the distribution of numerical data. Unlike a box plot that can only show summary statistics, violin plots depict summary statistics and the density of each variable. On each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value, the skinnier sections represent a lower probability. **To learn more about violinplots you can visit this link: https:\/\/mode.com\/blog\/violin-plot-examples\/**\n<span style=\"width:100%;text-align: center;\"><img align = left src=\"https:\/\/images.ctfassets.net\/fi0zmnwlsnja\/sdfgtcRp16wTNOcRceGQm\/5bfcb73d2261d49ff20dd7857e0152b1\/Screen_Shot_2019-03-01_at_11.36.10_AM.png\" style=\"height:550px;\"><\/span>","a82b0315":"The classes are equally balanced. Now, let us look at the count of each flower type in our dataset.","b64e29ed":"# Gaussian Naive Bayes\nThis is a variant of Naive Bayes which supports continuous values and has an assumption that each class is normally distributed.","b14cdc87":"# XGBoost Classifier\nXGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. XGBoost provides a highly efficient implementation of the stochastic gradient boosting algorithm and access to a suite of model hyperparameters designed to provide control over the model training process.","36dbd8b3":"# Bagging Classifier\nBagging classifier is an ensemble technique that fits base classifiers each on random subsets of the original dataset and then aggregates their individual predictions to form a final prediction.","874e430d":"<h3>Reading the input file<\/h3>","89287099":"# Checking Correlations","772978f8":"We can see that the petal length and petal width is the least for Iris-setosa. The petal width and length for Iris-versicolor lies in an intermediate range, between that of setosa and virginica. Iris-virginica has the largest petal length and width. A few outliers exist in the case of both versicolor and virginica.","da16cb81":"<h3>Taking a look at the input files<\/h3>","14a47897":"# Support Vector Classifier\nIt is a supervised classification algorithm. The idea of SVM is simple. It creates a line or a hyperplane which separates the data into classes.","b5d91342":"# Ada Boost Classifier\nIt combines multiple classifiers to increase the accuracy of classifiers. AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier.","2f870012":"# 3D Visualization\n3D scatter plots are used to plot data points on three axes in the attempt to show the relationship between three variables. Each row in the data table is represented by a marker whose position depends on its values in the columns set on the X, Y, and Z axes. **To learn about plotting 3D scatter plots using matplot you can visit this link: https:\/\/pythonguides.com\/matplotlib-3d-scatter\/**","6c08290a":"# Radius Neighbors Classifier\nRadius Neighbors Classifier is a classification machine learning algorithm. It is an extension to the k-nearest neighbors algorithm that makes predictions using all examples in the radius of a new example rather than the k-closest neighbors. As such, the radius-based approach to selecting neighbors is more appropriate for sparse data, preventing examples that are far away in the feature space from contributing to a prediction.","e139ce57":"# Decision Tree Classifier\nA decision tree is a graphical representation of all possible solutions to a decision based on certain conditions. On each step or node of a decision tree, used for classification, we try to form a condition on the features to separate all the labels or classes contained in the dataset to the fullest purity.","4aedc774":"<h3>Looking at the statistics of our dataset<\/h3>","41110a3a":"# About the Dataset\nDescription of the data given:\n- **Id**: Unique number for each row\n- **SepalLengthCm**: Length of the sepal (in cm)\n- **SepalWidthCm**: Width of the sepal (in cm)\n- **PetalLengthCm**: Length of the petal (in cm)\n- **PetalWidthCm**: Width of the petal (in cm)\n- **Species**: Name of the species","d207fd19":"# Importing the Necessary Libraries","3a5516b9":"# Final Results","37a1a0cd":"# Exploratory Data Analysis\nWe should try to visualize the distribution of the `Species` in our dataset to see if the classes are balanced.","8c2f0bfa":"# Encoding the Categories for the Label","14a1dff8":"Taking a look at the different types of `Species` and counting the total number of unique values so that we can look the class distribution of our dataset.","2cd0347f":"# Classification Report\n > A classification report is a performance evaluation metric in machine learning. It is used to show the precision, recall, F1 Score, and support of your trained    classification model. Some of the common terms associated with a classification report are as follows:\n> - **Precision**: Precision is defined as the ratio of true positives to the sum of true and false positives.\n> - **Recall**: Recall is defined as the ratio of true positives to the sum of true positives and false negatives.\n> - **F1 Score**: The F1 Score is the weighted harmonic mean of precision and recall. The closer the value of the F1 score is to 1.0, the better the expected performance of the model is.\n> - **Support**: Support is the number of actual occurrences of the classes in the dataset. It doesn\u2019t vary between models, it just diagnoses the performance evaluation process. \n\n**Credits for the ML algorithms**: https:\/\/www.kaggle.com\/marcovasquez\/top-machine-learning-algorithms-beginner#Machine-Learning-with-Scikit-Learn--","cb249d87":"# Quadratic Discriminant Analysis\nQDA is a variant of LDA (Linear Discriminant Analysis) in which an individual covariance matrix is estimated for every class of observations. QDA is particularly useful if there is prior knowledge that individual classes exhibit distinct covariances.","a4942e45":"# Bivariate Analysis\nBivariate analysis is a form of statistical analysis. It involves the analysis of two variables for the purpose of determining the relationship between them.","14668287":"# Extra Trees Classifier\nThis is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a \u201cforest\u201d to output it's classification result.","63e6c73c":"<h3>Lets check for any missing values in the dataset<\/h3>","78b35c57":"Removing the `Id` column from our dataset as it is not needed to peform EDA (Exploratory Data Analysis).","81513369":"We can observe that Iris-setosa has a relatively lower sepal length (as compared to versicolor or virginica). Iris-setosa has a large sepal width. It is hard to distinguish between versicolor and virginica based on sepal length and width. ","bda11b54":"# Random Forest Classifier\nThe term \u201cRandom Forest Classifier\u201d refers to the classification algorithm made up of several decision trees. The algorithm uses randomness to build each individual tree to promote uncorrelated forests, which then uses the forest's predictive powers to make accurate decisions.","379e3763":"**Observations**:\n - `SepalLengthCm`: The median value for sepal length is the least for Iris-setosa and the most for Iris-virginica.\n - `SepalWidthCm`: The median value for sepal width is the least for Iris-versicolor and the largest for Iris-setosa. Based on the sepal width of the flowers it might be tough to differentiate between the 3 species.\n - `PetalLengthCm`: Visually it is very evident that Iris-setosa has the least petal length. Even though there are a few outliers all of them are less than 2 cm. Iris-versicolor has the second largest median petal length, whereas Iris-virginica has the largest median petal length.\n - `PetalWidthCm`: Similar to the petal lengths of the species, we can see that Iris-setosa has the lowest median petal width, whereas Iris-virginica has the largest median petal width.","6c9bb852":"# Scaling the Data","d41918a0":"# Univariate Analysis\nUnivariate analysis is the simplest form of analyzing data. We only take a single variable into consideration and try to find relations based on that. ","eb40e050":"# KNN Model Performance\nOn the default KNN model we are getting a score of 96.67%. The default value for `n_neighbors` is 5 in KNN. Lets look at the score for different values of `n_neighbors` and select the value that gives us the best results.","89bfd768":"<h3>Lets take a look at the information in our dataset<\/h3>","976230c6":"Boxplots are a standardized way of displaying the distribution of data.\n**To know more about boxplots you can visit this link: https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51**\n<div style=\"width:100%;text-align: center;\"> <img align = left src=\"https:\/\/datavizcatalogue.com\/methods\/images\/anatomy\/box_plot.png\" style=\"height:600px;\"> <\/div>","299d81d6":"# Splitting the Data","989420bc":"# <center>Iris Species Classification<\/center>\n<div style=\"width:100%;text-align: center;\"> <img align = middle src=\"https:\/\/hellor.netlify.app\/slides\/iris_flower_dataset.png\" style=\"height:300px;\"> <\/div>","7f5f2b0c":"**Observations**:\n - `SepalLengthCm`: Iris-setosa has the smallest sepal length. We can see an outlier (red dot) in the graph. Iris-versicolor has a sepal length of about 5 cm to 7 cm. Iris-virginica has the largest sepal length (above 7 cm).\n - `SepalWidthCm`: Iris-versicolor has the smallest sepal width. The distinction between setosa and virginica is not so prominent in the range of 3.5 cm to 4 cm. However for a sepal width equal to or greater than 4 cm all the flowers belong to the Iris-setosa species.\n - `PetalLengthCm`: Iris-setosa has the smallest petal length. The length of the petals do not exceed 2 cm. For Iris-versicolor the petal length is in the range of 3 cm to 5 cm. Iris-virginica has the largest petal length (5 cm or greater).\n - `PetalWidthCm`: Iris-setosa has the smallest petal width. Iris-versicolor has a petal length from 1 cm to slightly less than 2 cm. Iris-virginica has a petal width that is approximately greater than 1.8 cm.","3b491ec5":"From the graph above we can clearly observe that for all values of `n_neighbors` above 6, the accuracy is a 100%. Lets select the smallest value of `n_neighbors` that gives us the highest accuracy which is 7.","f24188b9":"# Gradient Boosting Classifier\nGradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model.","34261ba0":"# Multivariate Analysis\nIt refers to an analysis involving multiple dependent variables resulting in one outcome. Creating different graphs for all the features to perform bivariate analysis would be extremely tedious. Seaborn provides a convenient way to perform multivariate analysis using the `pairplot` function."}}