{"cell_type":{"1c88389b":"code","d443be3b":"code","d9f4d367":"code","89f4937e":"code","b3946375":"code","ca4b500f":"code","88819887":"code","5ab626f2":"code","7358dd0a":"code","9e778373":"code","1c5e24e3":"code","106beb61":"code","2e7d0faa":"code","abacce63":"code","87a9c75c":"markdown"},"source":{"1c88389b":"#coding: utf-8","d443be3b":"# constants\nTILE_WIDTH = 256\nTILE_HEIGHT = 256","d9f4d367":"import os\nimg_dir = '\/kaggle\/input\/prostate-cancer-grade-assessment\/test_images'\n\nis_test_phase = os.path.exists(img_dir)\n\nif is_test_phase:\n    img_name_list = sorted(os.listdir(img_dir))\nelse:\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u304c\u898b\u3048\u306a\u3044 (=submit\u524d) \u5834\u5408\u306f\u8a13\u7df4\u753b\u50cf\u3092\u3044\u304f\u3064\u304b\u53d6\u3063\u3066\u304d\u3066\u4f7f\u3046\n    img_dir = os.path.join(os.path.dirname(img_dir), 'train_images')\n    img_name_list = os.listdir(img_dir)[:5]\n    \nimg_path_list = [os.path.join(img_dir, s) for s in img_name_list]","89f4937e":"if not is_test_phase:\n    print('\\n'.join(img_path_list))","b3946375":"import time","ca4b500f":"import numpy as np\nimport cv2\n\ndef fill_black_holes(img):\n    _img = img.copy()\n    contour, _ = cv2.findContours(_img, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n    for cnt in contour:\n        cv2.drawContours(_img, [cnt], 0, 255, -1)\n    return _img\n    \ndef fill_white_holes(img):\n    return 255 - fill_black_holes(255 - img)\n\ndef crop_tiles(original_img, tile_width, tile_height, is_test_phase=True):\n    # \u8272\u3005\u8003\u3048\u305f\u306e\u3067\u3059\u304c\uff0c\u30de\u30b9\u30af\u306e\u4f5c\u6210\u306f\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u5909\u63db\u3057\u3066\u5927\u6d25\u6cd5\u30672\u5024\u5316\u3057\u3066\u7a74\u3092\u57cb\u3081\u308b\uff0c\u3068\u3044\u3046\u65b9\u6cd5\u304c\u3044\u3044\u6c17\u304c\u3057\u3066\u3044\u307e\u3059\uff0e\n    \n    gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)\n    thresh_val, thresh_img = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)\n    mask = fill_white_holes(thresh_img)\n    \n    target_region_points = np.where(mask == 0) # \u6ce8: \u3053\u3053\u3067\u306f\u80cc\u666f\u304c 255, \u524d\u666f\u304c 0 \u306b\u306a\u3063\u3066\u3044\u308b (\u524d\u666f\u306e\u307b\u3046\u304c\u6697\u3044\u306e\u3067)\n    upperleft_y, upperleft_x = (target_region_points[i].min() for i in range(2))\n    lowerright_y, lowerright_x = (target_region_points[i].max() for i in range(2))\n    \n    img = original_img[upperleft_y : lowerright_y + 1, upperleft_x : lowerright_x + 1]\n    mask = mask[upperleft_y : lowerright_y + 1, upperleft_x : lowerright_x + 1]\n    \n    h, w = img.shape[:2]\n    pad_x = (tile_width - w % tile_width) % tile_width\n    pad_y = (tile_height - h % tile_height) % tile_height\n    \n    img = np.pad(img, [[0, pad_y], [0, pad_x], [0, 0]], 'constant', constant_values=255)\n    mask = np.pad(mask, [[0, pad_y], [0, pad_x]], 'constant', constant_values=0)\n    \n    num_tiles_y = img.shape[0] \/\/ tile_height\n    num_tiles_x = img.shape[1] \/\/ tile_width\n    tile_list = [None] * (num_tiles_y * num_tiles_x)\n    \n    # \u4e88\u6e2c\u306e\u7d50\u679c\u3067\u3042\u308b2\u6b21\u5143\u914d\u5217 (\u7e26 num_tiles_y x \u6a2a num_tiles_x) \u3092\uff0c\u5168\u884c\u3092\u6a2a\u4e26\u3073\u306b\u3057\u30661\u6b21\u5143\u306b\u3057\u305f\u3082\u306e\n    if not is_test_phase:\n        prediction_list = [-1] * (num_tiles_y * num_tiles_x)\n    \n    tile_id = 0\n    for tile_id_y in range(num_tiles_y):\n        upperleft_y = tile_id_y * tile_height\n\n        for tile_id_x in range(num_tiles_x):\n            upperleft_x = tile_id_x * tile_width\n            \n            tile_img = img[upperleft_y : upperleft_y + tile_height, upperleft_x : upperleft_x + tile_width]\n            tile_mask = mask[upperleft_y : upperleft_y + tile_height, upperleft_x : upperleft_x + tile_width]\n            \n            if np.count_nonzero(tile_mask) > int(tile_mask.size * 0.5):\n                # 5\u5272\u4ee5\u4e0a\u80cc\u666f\u306e\u30bf\u30a4\u30eb\u306f\u7121\u8996\n                if not is_test_phase:\n                    prediction_list[tile_id_y * num_tiles_x + tile_id_x] = 0\n            else:\n                tile_list[tile_id] = tile_img\n                tile_id += 1 \n    \n    if not is_test_phase:\n        return np.array(tile_list[:tile_id]), prediction_list, (num_tiles_y, num_tiles_x)\n    return np.array(tile_list[:tile_id])","88819887":"import openslide\n\ndef load_img(tiff_path, slide_level=1):\n    slide = openslide.OpenSlide(tiff_path)\n    img = np.array(slide.read_region((0, 0), slide_level, slide.level_dimensions[slide_level]))\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    return img","5ab626f2":"import torch","7358dd0a":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') ","9e778373":"from torch import nn\n\nimport sys\nsys.path.append('\/kaggle\/input\/efficientnetpytorch\/')\nfrom efficientnet_pytorch.model import EfficientNet\n\nclass Enet(nn.Module):\n    def __init__(self, model_name, num_classes, use_pretrained_models):\n        super(Enet, self).__init__()\n        self.enet = EfficientNet.from_pretrained(model_name) if use_pretrained_models else EfficientNet.from_name(model_name)\n        # self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n        self.enet._fc = nn.Linear(self.enet._fc.in_features, num_classes)\n\n    def forward(self, x):\n        return self.enet(x)\n\n    def load_parameters(self, parameters_path):\n        self.load_state_dict(torch.load(parameters_path, map_location=device))","1c5e24e3":"model = Enet('efficientnet-b0', 5, False)\nmodel.load_parameters('..\/input\/saved-parameters\/net_epoch_0009.pth')\nmodel.to(device)\nmodel.eval()","106beb61":"def gleason_map_to_isup(gmap):\n    '''\n        \u5f15\u6570 gmap: 2\u6b21\u5143\u306e numpy \u914d\u5217 (ndarray)\uff0e\u5404\u8981\u7d20\u306f\u9818\u57df\u3054\u3068\u306e Gleason \u30b9\u30b3\u30a2 (3-5) \u307e\u305f\u306f 0 (\u80cc\u666f)\uff0c1 (\u9055\u3046\u7d44\u7e54), 2 (\u826f\u6027\u7d44\u7e54)\uff0e\n        \u51fa\u529b isup_grade: ISUP \u30b0\u30ec\u30fc\u30c9 (\u4ee5\u4e0b\u306e\u30b5\u30f3\u30d7\u30eb\u3067\u306f\u9069\u5f53\u306b 0 \u306b\u3057\u3066\u3044\u308b)\n        \n        \u51e6\u7406\u5185\u5bb9: \n        (1) gmap \u306e\u8981\u7d20\u306e\u3046\u3061\uff0c\u3082\u3063\u3068\u3082\u591a\u304f\u542b\u307e\u308c\u3066\u3044\u308b\u8981\u7d20 (a) \u30682\u756a\u3081\u306b\u591a\u304f\u542b\u307e\u308c\u3066\u3044\u308b\u8981\u7d20 (b) \u3092\u8abf\u3079\u308b\uff0e\n        (2) a \u3068 b \u306e\u5024\u306b\u5fdc\u3058\u3066\uff0cISUP \u30b0\u30ec\u30fc\u30c9\u3092\u8a08\u7b97\u3057\u3066\u51fa\u529b\u3059\u308b\uff0e\n            - a + b == 6 \u306a\u3089\u3070 1\n            - a == 3, b == 4 \u306a\u3089\u3070 2\n            - a == 4, b == 3 \u306a\u3089\u3070 3\n            - a + b == 8 \u306a\u3089\u3070 4\n            - a + b == 9 \u307e\u305f\u306f 10 \u306a\u3089\u3070 5\n            - \u305d\u308c\u4ee5\u5916\u306f 0\n        \n        \u203b(1) \u3067\uff0cgmap \u306e\u8981\u7d20\u304c\u4e00\u7a2e\u985e\u3057\u304b\u306a\u3051\u308c\u3070\uff0ca \u3068 b \u306f\u540c\u3058\u5024\u3068\u3059\u308b\uff0e\n        \u203b(1) \u3067\uff0c\u3082\u3063\u3068\u3082\u591a\u304f\u542b\u307e\u308c\u3066\u3044\u308b\u8981\u7d20\u304c 2 \u7a2e\u985e\u4ee5\u4e0a\u3042\u308b\u5834\u5408\u306f\uff0c\u5024\u304c\u5927\u304d\u3044\u3082\u306e\u304b\u3089 2 \u3064\u9078\u3093\u3067 a, b \u3068\u3059\u308b\uff0e\n        \u203b(1) \u3067\u306f 0 (\u80cc\u666f)\uff0c1 (\u9055\u3046\u7d44\u7e54)\uff0c2 (\u826f\u6027\u7d44\u7e54) \u306f\u7121\u8996\u3059\u308b\uff0egmap \u306b 0, 1, 2 \u4ee5\u5916\u306e\u8981\u7d20\u304c\u542b\u307e\u308c\u306a\u3051\u308c\u3070\uff0c(2) \u306b\u95a2\u4fc2\u306a\u304f 0 \u3092\u8fd4\u3059\uff0e\n\n    '''\n    \n\n    X = np.count_nonzero(gmap == 3)\n    Y = np.count_nonzero(gmap == 4)\n    Z = np.count_nonzero(gmap == 5)\n\n    if X == 0 and Y == 0 and Z == 0:\n        isup_grade = 0\n\n    elif Y == 0 and Z == 0:\n        isup_grade = 1\n\n    elif X > Y > Z:\n        isup_grade = 2\n\n    elif Y >= X > Z:\n        isup_grade = 3\n\n    elif (Z >= Y and X > Y) or (X == 0 and Z == 0):\n        isup_grade = 4\n\n    elif (Z >= X and Y >= X) or (X == 0 and Y == 0):\n        isup_grade = 5\n    \n    else:\n        isup_grade = 0\n    \n\n    \n    return isup_grade","2e7d0faa":"img_ID_list = [] #keep the img_IDs\nisup_G_list = [] #keep the isup_grades\nID_append = img_ID_list.append\nisup_append = isup_G_list.append\n# because List => pd.frame is faster\nwith torch.no_grad():\n    for img_no, img_path in enumerate(img_path_list, start=1):\n        img_ID = os.path.basename(img_path).split('.')[0] #The ID of the image added to submission.csv\n        if not is_test_phase:\n            print('Testing {} ... ({} \/ {})'.format(os.path.basename(img_path), img_no, len(img_path_list)))\n            loop_start_time = time.time()\n\n        img = load_img(img_path)\n        _cropped_tiles = crop_tiles(img, tile_width=TILE_WIDTH, tile_height=TILE_HEIGHT, is_test_phase=is_test_phase)\n        if is_test_phase:\n            tiles = _cropped_tiles\n        else:\n            tiles, pred_list, img_shape = _cropped_tiles\n        \n        if len(tiles) == 0:\n            isup_grade = 0\n        else:\n            tiles = tiles.astype(np.float) \/ 255.0\n            tiles = ((tiles - [0.485, 0.456, 0.406]) \/ [0.229, 0.224, 0.225]).transpose([0, 3, 1, 2])\n            tiles_tensor = torch.from_numpy(tiles)\n            tiles_tensor = tiles_tensor.float().to(device)\n\n            if tiles_tensor.shape[0] > 256:\n                # \u5927\u304d\u3059\u304e\u308b\u3068\u30e1\u30e2\u30ea\u306b\u4e57\u3089\u306a\u304f\u306a\u3063\u305f\u308a\u3057\u305d\u3046\u306a\u306e\u3067\u5206\u5272\n                num_split = tiles_tensor.shape[0] \/\/ 256 + 1\n                tensor_list = [None] * num_split\n\n                for _i in range(num_split):\n                    tensor_list[_i] = model(tiles_tensor[_i * 256 : min((_i + 1) * 256, tiles_tensor.shape[0])])\n\n                output_tensor = torch.cat(tensor_list)\n            else:\n                output_tensor = model(tiles_tensor)\n\n            output_prob = nn.Softmax(dim=1)(model(tiles_tensor)).cpu()\n            _, classes = torch.max(output_prob, dim=1)\n            classes.add(1)\n\n            if is_test_phase:\n                # \u5b9f\u306f2\u6b21\u5143\u914d\u5217\u306b\u3059\u308b\u5fc5\u8981\u306f\u306a\u304f\u3066\uff0c\u80cc\u666f\u306e\u3068\u3053\u308d\u3082\u542b\u3081\u308b\u5fc5\u8981\u306f\u306a\u3044 (\u7121\u8996\u3055\u308c\u308b) \u306e\u3067\uff0c\u30c6\u30b9\u30c8\u306e\u3068\u304d\u306f\u6642\u9593\u77ed\u7e2e\u306e\u305f\u3081\u306b\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u3092\u305d\u306e\u307e\u307e\u5165\u308c\u308b\n                isup_grade = gleason_map_to_isup(classes.numpy())\n            else:\n                # \u30c7\u30d0\u30c3\u30b0\u7528\u306b\u6574\u5f62\n                _i = 0\n                for i, pred in enumerate(pred_list):\n                    if pred != 0:\n                        pred_list[i] = classes[_i].item()\n                        _i += 1\n\n                gleason_map = np.reshape(pred_list, img_shape)\n                isup_grade = gleason_map_to_isup(gleason_map)\n        \n        ID_append(img_ID) #add ID to the list\n        isup_append(isup_grade) # add isup grade to the list\n        print('image_id: {}'.format(img_ID))\n        print('ISUP grade: {}'.format(isup_grade))\n            \n\n        if not is_test_phase:\n            print('Done. Elapsed time: {:.2f}[s]\\n'.format(time.time() - loop_start_time))","abacce63":"import pandas as pd\nsubmission_data = pd.DataFrame(\n                    data={'image_id': img_ID_list, 'isup_grade': isup_G_list},\n                    columns=['image_id', 'isup_grade']\n                )\n\nsubmission_data.to_csv('submission.csv', index=False)\n# if is_test_phase:\n#     submission_data.to_csv('submission.csv', index=False)\n# else:\n#     print('submission_data\\n {}'.format(submission_data.head()))","87a9c75c":"converting two colums `img_ID_list` and `isup_G_list` to the pandas frame, next making the submission data"}}