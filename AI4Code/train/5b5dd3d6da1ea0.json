{"cell_type":{"130ab358":"code","fe6ddd0f":"code","7ff98188":"code","9eca4454":"code","77519c5e":"code","bedb1bc9":"code","011f78de":"code","a3eed935":"code","096291fd":"code","a8fcb7d8":"code","c6b1ad9c":"code","ad87e69f":"code","f6d2ac67":"code","0ce653a0":"code","21914257":"code","39885b44":"code","0772d2a4":"code","63085190":"code","9571c082":"code","f7de684b":"code","1402b484":"code","02a5ac59":"code","bcd758b0":"code","88d82eec":"code","f8bc4ae0":"code","6b90a27d":"code","99390899":"code","977747e8":"code","92f9b7b6":"markdown","54a8e156":"markdown","bce5acbb":"markdown","240be33d":"markdown","fcc7eab3":"markdown","20ee73c0":"markdown","d198c34f":"markdown","a60a5fca":"markdown","e883f74a":"markdown","132715da":"markdown","5aecf0ae":"markdown","53a2eb4b":"markdown","4d2462e1":"markdown","5e24311a":"markdown","79d2b90c":"markdown","00a29e17":"markdown"},"source":{"130ab358":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nfrom sklearn import preprocessing\nimport time\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fe6ddd0f":"df = pd.read_csv('\/kaggle\/input\/forest-cover-type-dataset\/covtype.csv')\npd.options.display.max_columns = None\ndf.head()","7ff98188":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","9eca4454":"sns.set(style=\"ticks\")\nf = sns.countplot(x=\"Cover_Type\", data=df, palette=\"bwr\")\nplt.show()","77519c5e":"df['Cover_Type'].value_counts()","bedb1bc9":"X = df.drop(['Cover_Type'], axis = 1)\nY = df['Cover_Type']","011f78de":"X.shape","a3eed935":"figure(num=None, figsize=(15, 20), dpi=80, facecolor='w', edgecolor='k')\n\ncorr= X.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)","096291fd":"from sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier","a8fcb7d8":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX2 = StandardScaler().fit_transform(X)\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X2, Y, test_size = 0.30, random_state = 101)","c6b1ad9c":"start = time.process_time()\ntrainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\nprint(time.process_time() - start)\npredictionsvm = trainedsvm.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))","ad87e69f":"start = time.process_time()\ntrainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\nprint(time.process_time() - start)\npredictionstree = trainedtree.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionstree))\nprint(classification_report(Y_Test,predictionstree))","f6d2ac67":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\ndata = export_graphviz(trainedtree,out_file=None,feature_names= X.columns,\n                       class_names=['1', '2', '3', '4', '5', '6', '7'],  \n                       filled=True, rounded=True,  \n                       max_depth=2,\n                       special_characters=True)\ngraph = graphviz.Source(data)\ngraph","0ce653a0":"start = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","21914257":"figure(num=None, figsize=(16, 18), dpi=80, facecolor='w', edgecolor='k')\n\nfeat_importances = pd.Series(trainedforest.feature_importances_, index= X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","39885b44":"X_Reduced = df[['Elevation','Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Hydrology']]\nX_Reduced = StandardScaler().fit_transform(X_Reduced)\nX_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(X_Reduced, Y, test_size = 0.30, random_state = 101)","0772d2a4":"start = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train2,Y_Train2)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_Test2)\nprint(confusion_matrix(Y_Test2,predictionforest))\nprint(classification_report(Y_Test2,predictionforest))","63085190":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel = ExtraTreesClassifier()\nstart = time.process_time()\nmodel = model.fit(X_Train,Y_Train)\nmodel = SelectFromModel(model, prefit=True)\nprint(time.process_time() - start)\nSelected_X = model.transform(X_Train)\nSelected_X.shape","9571c082":"start = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(Selected_X, Y_Train)\nprint(time.process_time() - start)\nSelected_X_Test = model.transform(X_Test)\npredictionforest = trainedforest.predict(Selected_X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","f7de684b":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\nimportances = trainedforest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in trainedforest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(Selected_X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(Selected_X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(Selected_X.shape[1]), indices)\nplt.xlim([-1, Selected_X.shape[1]])\nplt.show()","1402b484":"Numeric_df = pd.DataFrame(X)\nNumeric_df['Y'] = Y\nNumeric_df.head()","02a5ac59":"figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n\ncorr= Numeric_df.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)\n\n# Selecting only correlated features\ncorr_y = abs(corr[\"Y\"])\nhighest_corr = corr_y[corr_y >0.25]\nhighest_corr.sort_values(ascending=True)","bcd758b0":"X_Reduced2 = X[['Elevation','Wilderness_Area4']]\nX_Reduced2 = StandardScaler().fit_transform(X_Reduced2)\nX_Train3, X_Test3, Y_Train3, Y_Test3 = train_test_split(X_Reduced2, Y, test_size = 0.30, random_state = 101)","88d82eec":"start = time.process_time()\ntrainedsvm = svm.LinearSVC().fit(X_Train3, Y_Train3)\nprint(time.process_time() - start)\npredictionsvm = trainedsvm.predict(X_Test3)\nprint(confusion_matrix(Y_Test3,predictionsvm))\nprint(classification_report(Y_Test3,predictionsvm))","f8bc4ae0":"min_max_scaler = preprocessing.MinMaxScaler()\nScaled_X = min_max_scaler.fit_transform(X2)","6b90a27d":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX_new = SelectKBest(chi2, k=2).fit_transform(Scaled_X, Y)\nX_Train3, X_Test3, Y_Train3, Y_Test3 = train_test_split(X_new, Y, test_size = 0.30, random_state = 101)\nstart = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train3,Y_Train3)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_Test3)\nprint(confusion_matrix(Y_Test3,predictionforest))\nprint(classification_report(Y_Test3,predictionforest))","99390899":"from sklearn.linear_model import LassoCV\n\nregr = LassoCV(cv=5, random_state=101)\nregr.fit(X_Train,Y_Train)\nprint(\"LassoCV Best Alpha Scored: \", regr.alpha_)\nprint(\"LassoCV Model Accuracy: \", regr.score(X_Test, Y_Test))\nmodel_coef = pd.Series(regr.coef_, index = list(X.columns[:-1]))\nprint(\"Variables Eliminated: \", str(sum(model_coef == 0)))\nprint(\"Variables Kept: \", str(sum(model_coef != 0))) ","977747e8":"figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n\ntop_coef = model_coef.sort_values()\ntop_coef[top_coef != 0].plot(kind = \"barh\")\nplt.title(\"Most Important Features Identified using Lasso (!0)\")","92f9b7b6":"### Feature Selection","54a8e156":"#### Lasso Regression","bce5acbb":"## Preprocessing","240be33d":"Univariate Feature Selection is a statistical method used to select the features which have the strongest relationship with our corrispondent labels. Using the SelectKBest method we can decide which metrics to use to evaluate our features and the number of K best features we want to keep. Different types of scoring functions are available depending on our needs:\n\n- Classification: chi2, f_classif, mutual_info_classif\n- Regression: f_regression, mutual_info_regression\n\nIn this example, we will be using chi2. Chi-squared (Chi2) can take as input just non-negative values, therefore, first of all we scale our input data in a range between 0 and 1.","fcc7eab3":"When applying regularization to a Machine Learning model, we add a penalty to the model parameters so that to avoid that our model tries to resemble too closely our input data. In this way, we can make our model less complex and we can avoid overfitting (making learn to our model not just the key data characheteristics but also it's intrinsic noise).\n\nOne of the possible Regularization Methods is Lasso (L1) Regrssion. When using Lasso Regression, the coefficients of the inputs features gets shrinken if they are not positively contributing towards our Machine Learning model training. In this way, some of the features might get automatically discarded assigning them coefficients equal to zero.","20ee73c0":"Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importaqnce of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it's predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model (our confuse it to make a wrong decision!).","d198c34f":"In this notebook, I will make use of the Forest Cover dataset to try to predict the different Cover Types given the previded features. I will successivelly try differerent feature elimination techniques to see how this can affect training times and overall model accuracy. <br>\n\nReducing the number of features in a dataset, can lead to:\n\n- Accuracy improvements\n- Overfitting risk reduction\n- Speed up in training\n- Improved Data Visualization","a60a5fca":"# Forest Cover Type","e883f74a":"#### Univariate Feature Selection","132715da":"## Machine Learning","5aecf0ae":"There are many different methods which can be applied for Feature Selection. Some of the most important ones are:\n\n- Filter Method = filtering our dataset and taking only a subset of it containg all the relevant features (eg. correlation matrix using Pearson Correlation)\n- Wrapper Method = follows the same objective of the FIlter Method but uses a Machine Learning model as it's evaluation criteria (eg. Forward\/Backward\/Bidirectional\/Recursive Feature Elimination). We feed some features to our Machine Learning model, evaluate their performance and then decide if add or remove feature to increase accuracy. As a result, this mothod can be more accurate than filtering, is more computationally expensive.\n- Embedded Method = like the FIlter Method also the Embedded Method makes use of a Machine Learning model. The difference between the two different methods is that the Embedded Method examines the different training iterations of our ML model and then ranks the importance of each feature based on how much each of the features contributed to the ML model training (eg. LASSO Regularization).","53a2eb4b":"Using Seaborn, we can now plot the Pearson correlation heatmap of our dataset. Inspecting this plot, we can then be able to see the correlation of our independent variables (X) with our label (Y). Finally, we can then select just the features which are most correlated with Y and train\/test an SVM model to test the results of this approach.\n\nUsing Pearson correlation our returned coefficient values will vary between -1 and 1:\n\n- If the correlation between two features is 0 this means that changing any of these two features will not affect the other.\n- If the correlation between two features is greater than 0 this means that increating the values in one feature will make increase also the values in the other feature (the closer the correlation coefficient is to 1 and the stronger is going to be this bond between the two different features).\n- If the correlation between two features is less than 0 this means that increating the values in one feature will make decrease the values in the other feature (the closer the correlation coefficient is to -1 and the stronger is going to be this relationship between the two different features).\n\nAnother possible aspect to control in this analysis would be to check if the selected variables are highly correlated each other. If they are, we would then need to keep just one of the correlated ones and drop the others.","4d2462e1":"SelectFromModel is another Scikit-learn method which can be used for Feature Selection. This method can be used with all the different types of Scikit-learn models (after fitting) which have a coef_ or featureimportances attribute. Compared to RFE, SelectFromModel is a less robust solution. In fact, SelectFromModel just removes less important features based on a calculated threshold (no optimization iteration process involved).\n\nIn order to test SelectFromModel efficacy, I decided to use an ExtraTreesClassifier in this example. ExtraTreesClassifier (Extremely Randomized Trees) is tree based ensamble classifier which can yield less variance compared to Random Forest methods (reducing therefore the risk of overfitting). The main difference between Random Forest and Extremely Randomized Trees is that in Extremely Randomized Trees nodes are sampled without replacement.","5e24311a":"#### Feature Importance","79d2b90c":"#### SelectFromModel: Meta-transformer for selecting features based on importance weights.","00a29e17":"#### Correlation Matrix Analysis"}}