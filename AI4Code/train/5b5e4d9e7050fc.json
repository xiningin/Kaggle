{"cell_type":{"44289dce":"code","7aa33ed7":"code","48edda08":"code","2211c6b7":"code","d3e623d0":"code","7b3c8ef4":"code","26e5ff88":"code","bea8c3b2":"code","1a56769f":"code","f7049994":"code","3864d835":"code","38c7e349":"code","757a7aea":"code","132cfe9d":"code","774904bf":"code","169f9ae4":"code","1d8ed87f":"code","2bc60e43":"code","6e96d8b0":"markdown","6bdb7704":"markdown","761a090d":"markdown","a19f5227":"markdown","25f8b792":"markdown","39086d2d":"markdown","8a6fc78d":"markdown","f244b1ad":"markdown","d0b4b09f":"markdown","b5467eb3":"markdown","063e54a2":"markdown"},"source":{"44289dce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7aa33ed7":"train_data = pd.read_csv(r\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.pop('Cabin') #removing cabin column\ntrain_data.pop('Ticket')\n\n\n#checking for missing data\nmissing_data = pd.concat([train_data.isnull().sum()], axis=1, keys=['train_data'])\nmissing_data[missing_data.sum(axis=1) > 0]\n\n#train_data.shape\n\n#filling missing data\n#fill in missing embarked with most frequent value in dataset \ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\n\ntrain_data.head()","48edda08":"#train_data_clean = pd.DataFrame({'PassengerId': train_data.PassengerId, 'Survived': train_data.Survived, 'Pclass': train_data.Pclass, 'Name': train_data.Name, 'Sex': train_data.Sex, 'Age': train_data.Age, 'SibSp': train_data.SibSp, 'Parch': train_data.Parch, 'Fare': train_data.Fare, 'Embarked': train_data.Embarked})\n#train_data_clean.to_csv(r'Downloads\\train_dat.csv', index = False)\n#print(\"file saved successfully\")","2211c6b7":"test_data = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')\ntest_data.pop('Cabin')\ntest_data.pop('Ticket')\n\n\n#checking missing data\nmissing_data = pd.concat([test_data.isnull().sum()], axis=1, keys=['test_data'])\nmissing_data[missing_data.sum(axis=1) > 0]\n\ntest_data.shape\n\n#filling missing values for age with mean age value\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\ntest_data['Embarked'] = test_data['Embarked'].fillna(test_data['Embarked'].mode()[0]) # mode for most often\n\ntest_data.head()","d3e623d0":"#test_data_clean = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Pclass': test_data.Pclass, 'Name': test_data.Name, 'Sex': test_data.Sex, 'Age': test_data.Age, 'SibSp': test_data.SibSp, 'Parch': test_data.Parch, 'Fare': test_data.Fare, 'Embarked': test_data.Embarked})\n#test_data_clean.to_csv(r'Downloads\\test_data1_cleaned.csv', index = False)\n#print(\"file saved successfully\")","7b3c8ef4":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nprop_women = sum(women) \/ len(women)\nprint(\"% of women who survived Titanic Crash:\", prop_women)","26e5ff88":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nprop_men = sum(men) \/ len(men)\nprint(\"% of men who survived Titanic Crash:\", prop_men)","bea8c3b2":"#Performing one hot encoding of categorical features \n# getting Dummies from all other categorical vars\ntrain_data['Pclass'] = train_data['Pclass'].apply(str)\n\nfor col in train_data.dtypes[train_data.dtypes == 'object'].index:\n    for_dummy = train_data.pop(col)\n    train_data = pd.concat([train_data, pd.get_dummies(for_dummy, prefix=col)], axis=1)\n    \ntrain_data.head()","1a56769f":"labels = train_data.pop('Survived')","f7049994":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.25)","3864d835":"from sklearn.ensemble import RandomForestClassifier\nfrom warnings import simplefilter #runs slower but doesnt iterate through all the lines of future warnings\nsimplefilter(action='ignore', category=FutureWarning) #update scikit learn version to take this snippet away \nfrom math import sqrt\n\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\nmax_depth=7, max_features='auto', max_leaf_nodes=None,\nmin_impurity_split=1e-07, min_samples_leaf=1,\nmin_samples_split=0.3, min_weight_fraction_leaf=0.0,\nn_estimators=10, n_jobs=-1, oob_score=False, random_state=0,\nverbose=0, warm_start=True)\ny_pred = rf.predict(x_test)\nprint(y_pred)\n\nrf_output = pd.DataFrame({'PassengerId': test_set.PassengerId, 'Survived': predictions})\n#model_output.head(10)\n\nimport os\nos.chdir(r'\/kaggle\/working')\n\nmodel_output.to_csv('submiss.csv', index=False)\n#print(\"save success\")\n\nfrom IPython.display import FileLink\nFileLink(r'submiss.csv')","38c7e349":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","757a7aea":"n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\ntrain_results = []\ntest_results = []\nfor estimator in n_estimators:\n    rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n    rf.fit(x_train, y_train)\n    train_pred = rf.predict(x_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = rf.predict(x_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_estimators, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(n_estimators, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('n_estimators')\nplt.show()","132cfe9d":"from warnings import simplefilter #runs slower but doesnt iterate through all the lines of future warnings\nsimplefilter(action='ignore', category=FutureWarning) #update scikit learn version to take this snippet away \n\nmax_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\nfor max_depth in max_depths:\n    rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1)\n    rf.fit(x_train, y_train)\n    train_pred = rf.predict(x_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = rf.predict(x_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Tree depth')\nplt.show()","774904bf":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True) #10% to 100% varying of the sample\ntrain_results = []\ntest_results = []\nfor min_samples_split in min_samples_splits:\n    rf = RandomForestClassifier(min_samples_split=min_samples_split)\n    rf.fit(x_train, y_train)\n    train_pred = rf.predict(x_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = rf.predict(x_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_splits, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(min_samples_splits, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('min samples split')\nplt.show()","169f9ae4":"max_features = list(range(1,train_data.shape[1]))\ntrain_results = []\ntest_results = []\nfor max_feature in max_features:\n    rf = RandomForestClassifier(max_features=max_feature)\n    rf.fit(x_train, y_train)\n    train_pred = rf.predict(x_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = rf.predict(x_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_features, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(max_features, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('max features')\nplt.show()","1d8ed87f":"train_set = pd.read_csv('\/kaggle\/input\/train-clean\/train_data1_cleaned.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/test-clean\/test_data1_cleaned.csv')","2bc60e43":"from sklearn.ensemble import RandomForestClassifier\n\nresponse = train_set[\"Survived\"]\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_set[features])\nX_test = pd.get_dummies(test_set[features])\n                    \nmodel = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\nmax_depth=7, max_features=3, max_leaf_nodes=None,\nmin_impurity_split=1e-07, min_samples_leaf=1,\nmin_samples_split=0.3, min_weight_fraction_leaf=0.0,\nn_estimators=27, n_jobs=-1, oob_score=False, random_state=0,\nverbose=0, warm_start=True)\nmodel.fit(X, response)\npredictions = model.predict(X_test)\n\n\nmodel_output = pd.DataFrame({'PassengerId': test_set.PassengerId, 'Survived': predictions})\n#model_output.head(10)\n\nimport os\nos.chdir(r'\/kaggle\/working')\n\nmodel_output.to_csv('subTra.csv', index=False)\n#print(\"save success\")\n\nfrom IPython.display import FileLink\nFileLink(r'subTra.csv')","6e96d8b0":"## Tuning max_depths for random forest\n- max_depth represents the depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information about the data. \n- We fit each decision tree with depths ranging from 1 to 32 and plot the training and test errors.","6bdb7704":"### Building a random forest model to see if predictions for survival can be improved based on more column data\n* Performing one hot encoding and using pd.get_dummies() ","761a090d":"# Begin cleaning data to feed into random forest model","a19f5227":"# Calculating percentage of survival rates based on gender","25f8b792":"### Saving cleaned test_data df to new csv file ","39086d2d":"## min_samples_split\n- min_samples_split represents the minimum number of samples required to split an internal node. \n- This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, each tree in the forest becomes more constrained as it has to consider more samples at each node. \n- Here we will vary the parameter from 10% to 100% of the samples","8a6fc78d":"- We see that our model overfits for large depth values. The trees perfectly predicts all of the train data, however, it fails to generalize the findings for new data","f244b1ad":"### Saving cleaned train_data df to new csv file ","d0b4b09f":"# max_features\n- max_features represents the number of features to consider when looking for the best split.","b5467eb3":"# Above shows the parameter tuning of random forest model","063e54a2":"## Parameter tuning random forest (n_estimators, max_depths, min_samples_split)\n- n_estimators represents the number of trees in the forest. \n- Usually the higher the number of trees the better to learn the data. \n- However, adding a lot of trees can slow down the training process considerably, therefore we do a parameter search to find the sweet spot."}}