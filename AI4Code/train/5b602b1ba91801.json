{"cell_type":{"dd405607":"code","30824cf1":"code","d787a00e":"code","63fb4853":"markdown"},"source":{"dd405607":"import pandas as pd\nimport numpy as np\nimport functools\nimport re\nfrom IPython.core.display import display, HTML\n### BERT QA\nimport torch\n!pip install -q transformers --upgrade\nfrom transformers import *\nmodelqa = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","30824cf1":"print ('packages loaded')\n# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file\n#usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file']\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\nprint ('ALL CORD19 articles',df.shape)\n#fill na fields\ndf=df.fillna('no data provided')\n#drop duplicate titles\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#keep only 2020 dated papers\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint ('Keep only COVID-19 related articles',df.shape)\n\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport math\n\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\n\nfor index, row in df.iterrows():\n    #print (row['pdf_json_files'])\n    if 'no data provided' not in row['pdf_json_files'] and os.path.exists('\/kaggle\/input\/CORD-19-research-challenge\/'+row['pdf_json_files'])==True:\n        with open('\/kaggle\/input\/CORD-19-research-challenge\/'+row['pdf_json_files']) as json_file:\n            #print ('in loop')\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            #print (body)\n            body=body.replace(\"\\n\", \" \")\n            text=row['abstract']+' '+body.lower()\n            df.loc[index, 'abstract'] =text\n\ndf=df.drop(['pdf_json_files'], axis=1)\ndf=df.drop(['sha'], axis=1)\ndf.head()","d787a00e":"def extract_daysclear(text,word):\n    text=text.replace('covid-19','')\n    extract=''\n    res=''\n    if word in text:\n        res = [i.start() for i in re.finditer(word, text)]\n    for result in res:\n        extracted=text[result-100:result+150]\n        if 'days' in extracted and 'virus' in extracted or 'days' in extracted and 'viral' in extracted or 'days' in extracted and 'positive' in extracted or 'days' in extracted and 'negative' in extracted:\n            extract=extract+' '+extracted\n    #print (extract)\n    return extract\n\ndef extract_design(text):\n    words=['prospective cohort','retrospective cohort', 'systematic review',' meta ',' search ','case control','case series,','time series','cross-sectional','observational cohort', 'retrospective clinical','virological analysis','prevalence study','literature']\n    study_types=['prospective cohort','retrospective cohort','systematic review','meta-analysis','literature search','case control','case series','time series analysis','cross sectional','observational cohort study', 'retrospective clinical studies','virological analysis','prevalence study','literature search']\n    extract=''\n    res=''\n    for word in words:\n        if word in text:\n            res = [i.start() for i in re.finditer(word, text)]\n        for result in res:\n            extracted=text[result-30:result+30]\n            extract=extract+' '+extracted\n    i=0\n    study=''\n    for word in words:\n        if word in extract:\n            study=study_types[i]\n        #print (extract)\n        i=i+1\n    return study\n\n# BERT pretrained question answering module\ndef answer_question(question,text,model,tokenizer):\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n    input_ids = tokenizer.encode(input_text)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    # show qeustion and text\n    #tokenizer.decode(input_ids)\n    answer=answer.replace(' ##','')\n    #print (answer)\n    return answer\n\ndef process_topic(focus):\n    df1 = df[df['abstract'].str.contains(focus)]\n    df1 = df1[df1['abstract'].str.contains('methods')]\n    df1 = df1[df1['abstract'].str.contains('days')]\n    #print (focus,'focused articles',df1.shape)\n    df_results = pd.DataFrame(columns=['date','study','link','study type','material','method','Days After Onset\/Admission (+) Covid-19 Presence (maximum unless otherwise stated)','Conclusion','Measure of Evidence','added on','cord_uid'])\n    for index, row in df1.iterrows():\n        \n        ### get days 'til clear\n        raw_days=extract_daysclear(row['abstract'],focus)\n    \n        if raw_days !='':\n            ### get study design\n            study_type=''\n            study_type=extract_design(row['abstract'])\n            \n            days_q='how many days or weeks after onset does the virus persist in '+focus+'?'\n            days=answer_question(days_q,raw_days,modelqa,tokenizer)\n    \n            ### get sample size\n            sample_q='how many patients cases studies were included collected or enrolled'\n            sample=row['abstract'][0:1000]\n            sample_size=answer_question(sample_q,sample,modelqa,tokenizer)\n            if '[SEP]' in sample_size or '[CLS]' in sample_size:\n                sample_size='-'\n                \n            link=row['doi']\n            linka='https:\/\/doi.org\/'+link\n            to_append = [row['publish_time'],row['title'],linka,study_type,focus,'RT-PCR',days,raw_days,sample_size,'-',row['cord_uid']]\n            df_length = len(df_results)\n            df_results.loc[df_length] = to_append\n\n    df_results=df_results.sort_values(by=['date'], ascending=False)\n    file='viral_shedding_in_'+focus+'.csv'\n    df_results.to_csv(file,index=False)\n    df_table_show=HTML(df_results.to_html(escape=False,index=False))\n    display(HTML('<h1>'+focus+'<\/h1>'))\n    display(df_table_show)\n########### main program\n\nkeywords=['stool','sputum','blood',' urine ']\nfor focus in keywords:\n    process_topic(focus)","63fb4853":"# Round 2 - working example material studies\n\nCurrently it uses several custom extractor functions and uses BERT QA to find specific data for a few of the columns because it is easier than creating rule based systems for every textual possiblity.\n\nYou will notice in a couple of the columns, it will throw the kitchen sink at the column as suggested in the task instructions- \"It may be advantageous to extract larger excerpts instead of specific values\".\n"}}