{"cell_type":{"d92d3d2a":"code","3b14c054":"code","29e49fb9":"code","1d839313":"code","cf3fe569":"code","a2b7b8d7":"code","df52ebe3":"code","abde8c4f":"code","c991eb1d":"code","17c27acd":"code","7157c160":"code","8dce0876":"code","d188555d":"code","ea541146":"code","84c41089":"code","1cfc233d":"code","22b397a1":"code","07864119":"code","f70c406e":"code","f65003ec":"code","08b6ccda":"code","22f95c31":"code","2d164e0e":"code","457be57b":"code","8fcf71de":"code","12294d79":"markdown","e8444a31":"markdown","2f7967b4":"markdown","ce9013a4":"markdown","94361ce0":"markdown","204bfa48":"markdown","f7b005c1":"markdown","67e5542d":"markdown","c81078e9":"markdown","1b3508f0":"markdown","64352276":"markdown","25ec1605":"markdown","99fce965":"markdown"},"source":{"d92d3d2a":"import numpy as np\nimport pandas as pd\n\n#Visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')","3b14c054":"data=pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata.head() #to display the first five rows","29e49fb9":"data.info()","1d839313":"data.describe().transpose()","cf3fe569":"data['thal'].unique","a2b7b8d7":"sns.countplot(x='thal',data=data)","df52ebe3":"data.head()","abde8c4f":"#sex,restecg,exang,slope,ca,thal\ndata.columns\n","c991eb1d":"#to check to age groups in our data set\nsns.distplot(data['age'],kde=False,bins=20)\n","17c27acd":"data.isnull().sum()  #no missing data","7157c160":"data.columns","8dce0876":"x=data.drop('target',axis=1).values  #all columns except target\ny=data['target'].values ","d188555d":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5) #80% is training data","ea541146":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() #create an instance of the function\nx_train_scaled = scaler.fit_transform(x_train) #fit and tranform training data\nx_train = pd.DataFrame(x_train_scaled)\nx_test_scaled = scaler.transform(x_test) #only tranform test data\nx_test = pd.DataFrame(x_test_scaled)","84c41089":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout","1cfc233d":"model=Sequential()\nx_train.shape #13 features","22b397a1":"from tensorflow.keras.callbacks import EarlyStopping","07864119":"# taking 13 neurons as we have 13 features\nmodel.add(Dense(13,activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(13,activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(13,activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(13,activation='relu'))\nmodel.add(Dropout(0.2))\n\n\nmodel.add(Dense(1,activation='sigmoid')) #output is binary values so using sigmoid function\nmodel.compile(optimizer='adam',loss='binary_crossentropy') #use binary_crossentropy as output is binary values\n","f70c406e":"##model.fit(x=x_train,y=y_train,validation_data=(x_test,y_test),batch_size=120,epochs=250)\nearly_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=30)","f65003ec":"model.fit(x=x_train,y=y_train,epochs=50,validation_data=(x_test,y_test),callbacks=[early_stop])\n#validation data is used to check for overfitting","08b6ccda":"losses=pd.DataFrame(model.history.history)\nlosses #we get loss and validation loss","22f95c31":"losses.plot()\n#if validation loss increases then data is overfitting","2d164e0e":"predictions= model.predict_classes(x_test)","457be57b":"import math\nfrom sklearn import metrics\n\n#metrics to find accuracy of continous variables\nprint('Mean Abs value:' ,metrics.mean_absolute_error(y_test,predictions))\nprint('Mean squared value:',metrics.mean_squared_error(y_test,predictions))\nprint('root mean squared error value:',math.sqrt(metrics.mean_squared_error(y_test,predictions)))\nprint('explained_variance_score:',metrics.explained_variance_score(y_test,predictions)) #1 is good value\n","8fcf71de":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test,predictions))","12294d79":"## 8. Train the neural network","e8444a31":"## 9. Find the losses","2f7967b4":"## 2. Read Input data","ce9013a4":"## 10. Predict using test data","94361ce0":"## 1. Import the required libraries","204bfa48":"### Explanation of the columns \n#### 1. age\n#### 2. sex\n#### 3. chest pain type (4 values)\n#### 4. resting blood pressure\n#### 5. serum cholestoral in mg\/dl\n#### 6. fasting blood sugar > 120 mg\/dl\n#### 7. resting electrocardiographic results (values 0,1,2)\n#### 8. maximum heart rate achieved\n#### 9. exercise induced angina\n#### 10. oldpeak = ST depression induced by exercise relative to rest\n#### 11. the slope of the peak exercise ST segment\n#### 12. number of major vessels (0-3) colored by flourosopy\n#### 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","f7b005c1":"## 4. Check for missing values","67e5542d":"## 6. Standardize Data","c81078e9":"# HEART DISEASE PREDICTION USING KERAS - DEEP LEARNING ","1b3508f0":"## 7. Create your neural network","64352276":"## 3. Understand your data","25ec1605":"## 5. Split your dataset into training and testing data","99fce965":"## 11. Validate the predicted output with the actual value"}}