{"cell_type":{"23bdde28":"code","8bbb39a7":"code","92834849":"code","8c2e1de7":"code","12e41939":"code","3d067581":"code","ce44e142":"code","d521e317":"code","e76f1248":"code","544fd5dd":"code","6f3f8b7d":"code","7fb49c1a":"code","865122bd":"code","78183577":"code","0059824f":"code","02b74f40":"code","87ce5cfb":"code","67867ce7":"code","c17ee227":"code","edb3a630":"code","243a34aa":"code","93f44d17":"code","3770ea7a":"code","1cfc772b":"code","34838471":"code","6f1d1915":"code","cf2cbcb1":"code","89f9d147":"code","f81c8289":"code","e7ba0eed":"code","f622d7c2":"code","64be8651":"code","cdfe6c9d":"code","321e27ee":"code","503031bb":"code","bf463beb":"markdown","3b223556":"markdown","b2fc3a75":"markdown","b26fdfb5":"markdown","6f4b3ab2":"markdown","c5a9bb48":"markdown","f076e65f":"markdown","2c75e2cd":"markdown"},"source":{"23bdde28":"import numpy as np \nimport pandas as pd \nimport joblib\nimport re\nimport os\nimport sys\nimport glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport spacy\nfrom spacy import displacy\nfrom nltk.corpus import stopwords\nnlp = spacy.load(\"en_core_web_sm\")\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer","8bbb39a7":"data_dir = \"\/kaggle\/input\/donald-trumps-rallies\/\"\ntext_files = glob.glob(f\"{data_dir}*.txt\")\nprint(f\"Total Number of Documents : {len(text_files)}\")","92834849":"uscitymap = pd.read_csv(\"\/kaggle\/input\/uscity2statemap\/uscities_map.csv\")\nuscitymap.head(2)","8c2e1de7":"text_place_date_ls = []\nfor i,val in enumerate(text_files):\n    with open(text_files[i]) as f:\n        speech_file = f.read()\n        key = val.split(\"\/\")[-1].split(\".\")[0]\n        try:\n            text_place_date_ls.append([key,key[:-10],pd.to_datetime(key[-10:],format=\"%b%d_%Y\"),speech_file])\n        except:\n            text_place_date_ls.append([key,key[:-9],pd.to_datetime(key[-9:],format=\"%b%d_%Y\"),speech_file])\n\ndf = pd.DataFrame(text_place_date_ls,columns = ['filename','place','date','speech_text'])        \n","12e41939":"## correcting the name of place as for two part name it is without space between two parts but the second part start with capital letter\ndf['correct_place'] = df.place.apply(lambda x : re.findall('[A-Z][^A-Z]*',x))\ndf['locate_'] = df['correct_place'].apply(lambda x: sum([1 for i in x if i.find(\"-\")>=0]))\ndf['correct_place'] = df.apply(lambda x : \"\".join(x['correct_place']) if x['locate_']>0 else \" \".join(x['correct_place']),axis=1)\ndf.drop([\"locate_\"],axis=1,inplace=True)\n\ndf = pd.merge(df,uscitymap[['city','state_name']].drop_duplicates()\n              ,right_on='city',left_on='correct_place',how='left')\n\ndf['number_lines'] = df.speech_text.apply(lambda x: len(x.split('.'))) ## assuming sentence ends at '.'\ndf['place_name_count'] = df.apply(lambda x : len([m.start() for m in re.finditer(x['correct_place'],x['speech_text'])]),axis=1) \ndf['state_name_count'] = df.apply(lambda x : len([m.start() for m in re.finditer(str(x['state_name']),x['speech_text'])]),axis=1) \ndf['self_name_count'] = df.apply(lambda x : len([m.start() for m in re.finditer('Donald Trump',x['speech_text'])]),axis=1) \n\ndf['words_ls'] = df.speech_text.apply(lambda x: x.split(\" \"))\ndf['total_words'] = df.words_ls.apply(lambda x: sum([1 for i in x if i.isalpha()]))\ndf['total_words_remove_stopwords'] = df.words_ls.apply(lambda x: sum([1 for i in x if i not in stopwords.words('english')]))\n\ndf['date'] = pd.to_datetime(df['date'])\ndf['day'] = df['date'].dt.day\ndf['weekday'] = df['date'].dt.weekday\ndf['month'] = df['date'].dt.month","3d067581":"df_dict = df[['filename','speech_text']].to_dict(orient='records')\nfor sub_dict in df_dict:\n    ent_ls = nlp(sub_dict['speech_text'])\n    ent_ls = [[ent.text,ent.label_] for ent in ent_ls.ents]\n    sub_dict['named_entity_dict'] = ent_ls","ce44e142":"all_entity_df = pd.DataFrame()\nfor i in range(len(df_dict)):\n    entity_type_df = pd.DataFrame(df_dict[i]['named_entity_dict'],columns=['value','label']).reset_index().groupby(['label'],as_index=False).value.count()\n    entity_type_df['filename'] = df_dict[i]['filename']\n    all_entity_df = all_entity_df.append(entity_type_df)\n\nall_entity_val_df = pd.DataFrame()\nfor i in range(len(df_dict)):\n    entity_val_type_df = pd.DataFrame(df_dict[i]['named_entity_dict'],columns=['value','label']).reset_index().groupby(['value','label'],as_index=False).count()\n    entity_val_type_df['filename'] = df_dict[i]['filename']\n    all_entity_val_df = all_entity_val_df.append(entity_val_type_df)\n\n","d521e317":"speech_text_ls = df[['filename','speech_text']].drop_duplicates()['speech_text'].values\n\ntfidf = TfidfVectorizer(stop_words=stopwords.words('english'))\ntfidf_vec = tfidf.fit_transform(speech_text_ls)\n\ntfidf_df = pd.DataFrame(tfidf_vec.todense())\ntfidf_df.columns = tfidf.get_feature_names()\ntfidf_df['filename'] = list(df[['filename']].drop_duplicates().filename)","e76f1248":"joblib.dump({\"df_dict\":df_dict,'df':df,\n             'all_entity_df':all_entity_df,'all_entity_val_df':all_entity_val_df,\n             'tfidf_df':tfidf_df},\"data.pkl\")\n# data_saved = joblib.load(\"\/kaggle\/input\/processed-data\/data.pkl\")\n# df_dict = data_saved['df_dict']\n# df = data_saved['df']\n\n","544fd5dd":"data_saved = joblib.load(\"\/kaggle\/input\/processed\/data-2.pkl\")\ntfidf_df = data_saved['tfidf_df']\ndf = data_saved['df']\nall_entity_df = data_saved['all_entity_df']\nall_entity_val_df = data_saved['all_entity_val_df']\ndf_dict = data_saved['df_dict']\nall_entity_val_df_agg = all_entity_val_df.groupby(['value','label'],as_index=False)['index'].sum()","6f3f8b7d":"tfidf_df.drop(columns=['filename']).iloc[0].sort_values(ascending=False).head(20).plot(kind='bar')","7fb49c1a":"plt.bar('month','date',data  = df.groupby(['month'],as_index=False).date.count())","865122bd":"plt.bar('weekday','date',data  = df.groupby(['weekday'],as_index=False).date.count())##0 is monday","78183577":"all_entity_df.groupby(['label'],as_index=False).value.sum().plot(x='label',y='value',kind='bar')","0059824f":"plt.figure(figsize=(10,15))\nplt.subplots_adjust(wspace=0.8,hspace=1.5)\ncnt = 1\nfor label in all_entity_val_df_agg.label.unique():\n    plt.subplot(6,3,cnt)\n    cnt+=1\n    plt.barh('value','index',\n             data = all_entity_val_df_agg[all_entity_val_df_agg.label==label].sort_values(['index'],ascending=False).head(5))\n    plt.xticks(rotation=90)\n    plt.title(label)","02b74f40":"sns.distplot(df['total_words'])","87ce5cfb":"sns.distplot(df['self_name_count'])","67867ce7":"sns.distplot(df.number_lines)","c17ee227":"## example of named entity tagging\ndoc = nlp(df_dict[0]['speech_text'][:1000])\ndisplacy.render(doc, style=\"ent\",jupyter=True)","edb3a630":"from spacy.lang.en import English\nimport string\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora\nimport gensim\nimport pyLDAvis.gensim","243a34aa":"punctuations= string.punctuation\nparser = English()\nall_stopwords = nlp.Defaults.stop_words","93f44d17":"external_stopwords = ['people','know','going','say','great','right','want','like','get','good','come','guy','think','thank','...','years','time',\n                     'tell','look','american','country','lot','new','state','way','go','ok','sir','let','actually','america','little','okay','happen',\n                      'try','remember','hear','best','thing','numbers','pay','beautiful','take'\n                     ]","3770ea7a":"def tokenize(txt):\n    tokens = parser(txt)\n    lda_tokens = []\n    for token in tokens :\n        if token.orth_.isspace(): continue\n        elif token.lower_ in all_stopwords : continue\n        elif token.lower_ in punctuations : continue\n        else :\n            lda_tokens.append(token.lower_)\n    return lda_tokens\n\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None: return word\n    else : return lemma\n\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)\n\n\ndef prepare_data_for_lda(txt):\n    tokens = tokenize(txt)\n    tokens = [get_lemma(word) for word in tokens]\n    tokens = [word for word in tokens if word not in external_stopwords]\n    return tokens","1cfc772b":"data_saved = joblib.load(\"\/kaggle\/input\/processed\/data-2.pkl\")\ndf = data_saved['df']\n\ntext_data = [prepare_data_for_lda(txt) for txt in df[['speech_text','filename']].drop_duplicates().speech_text]\n\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(txt) for txt in text_data]\n\nnum_topics = 5\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,num_topics=num_topics,id2word=dictionary,passes=200)","34838471":"topics = lda_model.print_topics(num_words=20)\nplt.figure(figsize=(25,4))\nplt.subplots_adjust(wspace=0.5,hspace=0.9)\nfor idx in range(len(topics)):\n    plt.subplot(1,num_topics,idx+1)\n    split_topics = pd.DataFrame([[float(i.split(\"*\")[0]),i.split(\"*\")[1]] for i in topics[idx][1].split(\"+\")],columns=['weight','word'])\n    plt.barh(\"word\",'weight',data=split_topics)\n    plt.xticks(rotation=90)\n    plt.title(f\"Topic Number {idx}\")","6f1d1915":"lda_display = pyLDAvis.gensim.prepare(lda_model,corpus,dictionary,sort_topics=False)\npyLDAvis.display(lda_display)","cf2cbcb1":"# !pip install umap-learn\n# !pip install sentence-transformers","89f9d147":"from sentence_transformers import SentenceTransformer\nimport umap\nfrom sklearn.cluster import DBSCAN,KMeans\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nwarnings.filterwarnings(\"ignore\")","f81c8289":"data = df[['speech_text','filename']].drop_duplicates().speech_text.values\n\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\nembeddings = model.encode(data, show_progress_bar=True)","e7ba0eed":"## creating clusters\ncluster_bert  =  KMeans(n_clusters = 4 ).fit(embeddings)\n\n## visualization\numap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\nresult = pd.DataFrame(umap_data, columns=['x', 'y'])\nresult['labels'] = cluster_bert.labels_\nplt.scatter(\"x\",\"y\",c='labels',data = result)\n","f622d7c2":"docs_df = pd.DataFrame(data,columns =['doc'])\ndocs_df['labels'] = cluster_bert.labels_\ndocs_df['row_num'] = range(len(docs_df))\n\ndocs_per_id = docs_df.groupby(['labels'],as_index=False).agg({'doc':\" \".join})","64be8651":"\ndef c_tf_idf(documents, m,external_stopwords, ngram_range=(1, 1)):\n    count = CountVectorizer(ngram_range=ngram_range, stop_words=list(all_stopwords)+external_stopwords).fit(documents)\n    t = count.transform(documents).toarray()\n    w = t.sum(axis=1)\n    tf = np.divide(t.T, w)\n    sum_t = t.sum(axis=0)\n    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n    tf_idf = np.multiply(tf, idf)\n\n    return tf_idf, count\n  \n","cdfe6c9d":"external_stopwords = external_stopwords + ['said','done','ve','got']","321e27ee":"tf_idf, count = c_tf_idf(docs_per_id.doc.values,len(data),external_stopwords)\nn_words=20\ntf_idf_transposed = tf_idf.T\nwords = count.get_feature_names()\nlabels = docs_per_id.labels.values\n\nindices = np.argsort(tf_idf_transposed,axis=1)[:,:n_words]\n\ntop_n_words = {i:[(words[j],tf_idf_transposed[i,j]) for j in indices[i]] for i in labels}\n\ntopic_sizes = (docs_df.groupby(['labels'],as_index=False).doc.count()\n               .rename(columns={\"doc\":'size'})\n               .sort_values(by=['size'],ascending=False))\n\n\n","503031bb":"plt.figure(figsize=(15,10))\nfor i in range(len(top_n_words)):\n    plt.subplot(2,3,i+1)\n    sns.barplot(\"tfidf\",\"word\",data=pd.DataFrame(top_n_words[i],columns =['word','tfidf']))","bf463beb":"### Meaning of clusters - topic extraction","3b223556":"### generating document embeddings using BERT","b2fc3a75":"This notebook is work in progress, will be updating the notebook with better visualizations and topic modelling","b26fdfb5":"### clustering and basic visualization","6f4b3ab2":"## Topic Modelling ","c5a9bb48":"# Exploratory Analysis of basis text data stats","f076e65f":"## LDA Topic Modelling","2c75e2cd":"## Topic Modelling using BERT"}}