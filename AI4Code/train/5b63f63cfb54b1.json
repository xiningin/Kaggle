{"cell_type":{"91e035c7":"code","d4b5f316":"code","c510fdfc":"code","693e6842":"code","c13c890d":"code","1e33c704":"code","2b7d2f27":"code","c57298c0":"code","62f153af":"code","d949f128":"code","e0d4d137":"code","3c9a16cf":"code","bed000ff":"code","796acd8a":"code","231f751b":"code","6c0fda8c":"code","135fdc11":"code","5f7eb9c3":"code","086a7562":"code","ae036e68":"code","7c13e948":"code","a335439b":"code","49330ce0":"markdown","989a778c":"markdown","a7307ce5":"markdown","bdc49b85":"markdown"},"source":{"91e035c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4b5f316":"from __future__ import print_function, division\nfrom builtins import range, input","c510fdfc":"from keras.layers import Input, Lambda, Dense, Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator","693e6842":"from sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom glob import glob","c13c890d":"IMAGE_SIZE = [100, 100]","1e33c704":"epochs = 3\nbatch_size = 32","2b7d2f27":"train_path = '..\/input\/fruits\/fruits-360\/Training'\nvalid_path = '..\/input\/fruits\/fruits-360\/Test'","c57298c0":"# Useful to get number of files\nimage_files = glob(train_path + '\/*\/*.jpg')\nvalid_image_files = glob(valid_path + '\/*\/*.jpg')\n# Useful to get number of classes\nfolders = glob(train_path + '\/*')","62f153af":"plt.imshow(image.load_img(np.random.choice(image_files)))\nplt.show()","d949f128":"vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)","e0d4d137":"# Don't train existing weights:\nfor layer in vgg.layers:\n    layer.trainable = False","3c9a16cf":"# New layers:\nx = Flatten()(vgg.output)\nx = Dense(1000, activation='relu')(x)\nprediction = Dense(len(folders), activation='softmax')(x)","bed000ff":"# Create Model:\nmodel = Model(inputs=vgg.input, outputs=prediction)","796acd8a":"model.summary()","231f751b":"model.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","6c0fda8c":"gen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)","135fdc11":"# Test image generator\ntest_gen = gen.flow_from_directory(valid_path, target_size=IMAGE_SIZE)\nprint(test_gen.class_indices)\nlabels = [None] * len(test_gen.class_indices)\nfor k, v in test_gen.class_indices.items():\n    labels[v] = k\n    \nfor x, y in test_gen:\n    print(\"min:\", x[0].min(), \"max:\", x[0].max())\n    plt.title(labels[np.argmax(y[0])])\n    plt.imshow(x[0])\n    plt.show()\n    break","5f7eb9c3":"# create generators\ntrain_generator = gen.flow_from_directory(\n    train_path,\n    target_size=IMAGE_SIZE,\n    shuffle=True,\n    batch_size=batch_size\n)\n\nvalid_generator = gen.flow_from_directory(\n    valid_path,\n    target_size=IMAGE_SIZE,\n    shuffle=True,\n    batch_size=batch_size\n)","086a7562":"# Fitting model:\nr = model.fit_generator(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=epochs,\n    steps_per_epoch=len(image_files) \/\/ batch_size,\n    validation_steps=len(valid_image_files) \/\/ batch_size\n)","ae036e68":"# def get_confusion_matrix(data_path, N):\n#     print(\"Generating confusion matrix\", N)\n#     predictions = []\n#     targets = []\n#     i = 0\n#     for x, y in gen.flow_from_directory(data_path, target_size=IMAGE_SIZE, shuffle=False,batch_size=batch_size * 2):\n#         i += 1\n#         if i % 50 == 0:\n#             print(i)\n#         p = model.predict(x)\n#         p = np.argmax(p, axis=1)\n#         y = np.argmax(y, axis=1)\n#         predictions = np.concatenate((predictions, p))\n#         target = np.concatenate((targets, y))\n#         if len(targets) >= N:\n#             break\n            \n#     cm = confusion_matrix(targets, predictions)\n#     return cm\n        ","7c13e948":"# cm = get_confusion_matrix(train_path, len(image_files))\n# print(cm)\n# valid_cm = get_confusion_matrix(valid_path, len(valid_image_files))\n# print(valid_cm)","a335439b":"plt.plot(r.history['loss'], label='train loss')\nplt.plot(r.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\n\nplt.plot(r.history['accuracy'], label='train acc')\nplt.plot(r.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()","49330ce0":"Plot an image:","989a778c":"Add preprocessing layer to front of VGG","a7307ce5":"Training config:","bdc49b85":"Resize all images to this:"}}