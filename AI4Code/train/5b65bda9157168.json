{"cell_type":{"f92a7f92":"code","36264b1c":"code","3d177256":"code","8d6c75a6":"code","d3144bef":"code","7c4d7c29":"code","01f421ed":"code","f9aa78a8":"code","ce9ce64f":"code","5d37fe52":"code","3ccf6c7b":"code","ef5ae6a6":"code","0ed38fb1":"code","ae79c185":"code","4ecdc86a":"code","df3dd477":"code","3c221bd1":"code","d4d9bfbe":"code","8a88ff15":"code","c1d78b19":"code","6d389098":"code","488cf8af":"code","04f06256":"code","2b8d62ad":"code","2028716e":"code","592602e2":"code","c306cab1":"code","f9eb13f7":"markdown","22693111":"markdown","4996a5ef":"markdown","23b403c7":"markdown","5ded03f2":"markdown","c613068d":"markdown","d472fa2d":"markdown","deb0d910":"markdown","05068c21":"markdown","0927f100":"markdown","da5b6cf1":"markdown","57ad1bd1":"markdown","05e2499b":"markdown","5807b3fb":"markdown","6cd1db59":"markdown","ad0b1d70":"markdown","f7551c17":"markdown","f20d32b5":"markdown"},"source":{"f92a7f92":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.express as px \nfrom glob import glob \nimport os \nimport time \nfrom IPython.display import display \nimport gc \nfrom wordcloud import WordCloud \nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\nfrom tqdm import tqdm \nimport scipy as sp \n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans \nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport torch \nimport torch.nn as nn \nfrom torch.utils.data import DataLoader, Dataset ","36264b1c":"%time \n\ndistrics = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\", usecols=[\"district_id\", \"state\", \"locale\"])\nproduct = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\", usecols=[\"LP ID\", \"Primary Essential Function\"])\ndistrics = districs.rename(columns={\"district_id\": \"id\"})\nproduct = product.rename(columns={\"LP ID\": \"lp id\"})\nengagement = pd.DataFrame()\n\ncount = 0 \nfor i, f in enumerate(glob(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/*.csv\")):\n    df = pd.read_csv(f)\n    df[\"id\"] = int(f.split(\"\/\")[-1].split(\".\")[0])\n    engagement = pd.concat([engagement, df])\n    count += 1 \n    if count == 100:\n        break","3d177256":"display(districs.isnull().sum().to_frame())\ndisplay(product.isnull().sum().to_frame())\ndisplay(engagement.isnull().sum().to_frame())","8d6c75a6":"%%time\n\ndf = pd.merge(districs, engagement, how=\"right\", left_on=\"id\", right_on=\"id\")\ndf = pd.merge(df, product, how=\"left\", left_on=\"lp_id\", right_on=\"lp id\")\ndf.drop([\"lp id\", \"id\", \"lp_id\"], axis=1, inplace=True)\n\ndel districs, product, engagement\ngc.collect()\ndf.head()","d3144bef":"print(df.shape)","7c4d7c29":"%%time \n\n'''\n\u6642\u7cfb\u5217\u306e\u578b\u5909\u63db\n\u4f11\u65e5\u3068\u30b3\u30ed\u30ca\u611f\u67d3\u958b\u59cb\u65e5\u306e\u30d5\u30e9\u30b0\n\nmain\/sub \u4f5c\u6210\n\n'''\n\ndef split_essential_main(x):\n    if type(x) != list:\n        return \"missing\"\n    else:\n        return x[0]\n    \ndef split_essential_sub(x):\n    if type(x) != list or len(x) == 1:\n        return \"missing\"\n    else:\n        return x[1]\n        \n# datetime \ndf[\"time\"] = pd.to_datetime(df.time)\ndf[\"week\"] = df.time.dt.dayofweek \ndf[\"holiday\"] = df.week.apply(lambda x: 1 if x in [5, 6] else 0)\nd = pd.date_range(start=\"2020-01-01\", end=\"2020-01-19\")\ndf[\"is_pandemic\"] = df.time.apply(lambda x: 0 if x in d else 1)\ndf.drop(\"week\", axis=1, inplace=True)\n\n# primary essential functions \ndf[\"Primary Essential Function\"] = df[\"Primary Essential Function\"].fillna(\"missing\")\ndf[\"split\"] = df[\"Primary Essential Function\"].apply(lambda x: x.split(\"-\"))\ndf[\"main\"] = df.split.apply(split_essential_main)\ndf[\"sub\"] = df.split.apply(split_essential_sub)\ndf.drop(\"split\", axis=1, inplace=True)\n\ngc.collect()\ndf.head()","01f421ed":"fig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.ravel()\n\nmain = df.main.value_counts()\nax[0].pie(x=main.values, labels=main.index)\nax[0].set_title(\"main counts\")\n\nsub = df[\"sub\"].value_counts().to_frame()\nsub.plot(kind=\"bar\", ax=ax[1])\nax[1].set_title(\"sub counts\")\n\ndel sub, main \ngc.collect()\n\nplt.show()","f9aa78a8":"def main_plot(df, n=10):\n    main = df.loc[df.main != \"missing\", \"main\"].unique()\n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    ax = axes.ravel()\n    \n    for i, m in enumerate(main):\n        x = df.loc[df.main == m, \"sub\"].value_counts().to_frame().sort_values(\"sub\", ascending=False)[:n]\n        x.plot(kind=\"bar\", ax=ax[i])\n        ax[i].set_title(f\"main={m}\")\n    del main \n    plt.suptitle(\"main vs sub counts.\", fontsize=16)\n    plt.tight_layout()\n\nmain_plot(df)","ce9ce64f":"fig, axes = plt.subplots(1, 3, figsize=(22, 6))\nax = axes.ravel()\n\nlocal = df.locale.value_counts()\nax[0].pie(x=local.values, labels=local.index, )\nax[0].set_title(\"locale value counts.\")\n\nstate = df.state.value_counts().to_frame()\nstate.sort_values(\"state\", ascending=False)[:10].plot(kind=\"bar\", ax=ax[1])\nax[1].set_title(\"Top 10k statement counts.\")\n\nstate.sort_values(\"state\", ascending=False)[-10:].plot(kind=\"bar\", ax=ax[2])\nax[2].set_title(\"Under 10k statement counts.\")\n\ndel state, local \ngc.collect()\nplt.tight_layout()","5d37fe52":"%%time \n\ndef show_cloud(df):\n    local = df.loc[df.locale != \"missing\", \"locale\"].unique()\n    \n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    ax = axes.ravel()\n    for i, l in enumerate(local):\n        x = df.loc[df.locale == l, [\"state\"]]\n        if len(x) == 0: continue\n        word = WordCloud(width=1500, height=1100, background_color=\"white\", max_words=10).generate(\" \".join(x[\"state\"]))\n        ax[i].imshow(word)\n        ax[i].set_title(l)\n        ax[i].set_xticks([])\n        ax[i].set_yticks([])\n    plt.tight_layout()\n    \n    \ndf[\"locale\"] = df.locale.fillna(\"missing\")\ndf[\"state\"] = df.state.fillna(\"missing\")\nshow_cloud(df)\ngc.collect()","3ccf6c7b":"def transition_all(df):\n    time = df.groupby(\"time\").mean().loc[:, [\"pct_access\", \"engagement_index\"]]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(22, 6))\n    ax = axes.ravel()\n    time.drop(\"engagement_index\", axis=1).plot(ax=ax[0])\n    ax[0].set_title(\"pct_access\")\n    time.drop(\"pct_access\", axis=1).plot(ax=ax[1])\n    ax[1].set_title(\"engagement_index\")\n    plt.show()\n    gc.collect()\n    \n    \ndef transition_locale(df, is_access=True):\n    local = df.locale.unique()\n    fig, axes = plt.subplots(1, 2, figsize=(22, 6))\n    ax = axes.ravel()\n    for l in local:\n        x = df.loc[df.locale == l, [\"time\", \"pct_access\"]]\n        y = df.loc[df.locale == l, [\"time\", \"engagement_index\"]]\n        x.groupby(\"time\").mean().plot(ax=ax[0])\n        y.groupby(\"time\").mean().plot(ax=ax[1])\n    ax[0].legend(local)\n    ax[1].legend(local)\n    ax[0].set_title(\"access\")\n    ax[1].set_title(\"engagement\")\n    plt.show()\n    gc.collect()\n        \n    \ndef transition_locale_access_trand(df):\n    local = df.locale.unique()\n    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n    ax = axes.ravel()\n    for i, l in enumerate(local):\n        x = df.loc[df.locale == l, [\"time\", \"pct_access\"]]\n        x[\"rolling_7\"] = x.groupby(\"time\")[\"pct_access\"].rolling(window=90).mean().reset_index(drop=True)\n        x.groupby(\"time\").mean().plot(ax=ax[i])\n        ax[i].set_title(f\"locale {l}\")\n        del x \n    plt.suptitle(\"locale classies pct_access trainsitin trends.\",fontsize=18)\n    plt.tight_layout()\n    gc.collect()\n    \n    \ndef transition_locale_engage_trand(df):\n    local = df.locale.unique()\n    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n    ax = axes.ravel()\n    for i, l in enumerate(local):\n        x = df.loc[df.locale == l, [\"time\", \"engagement_index\"]]\n        x[\"rolling_7\"] = x.groupby(\"time\")[\"engagement_index\"].rolling(window=90).mean().reset_index(drop=True)\n        x.groupby(\"time\").mean().plot(ax=ax[i])\n        ax[i].set_title(f\"locale {l}\")\n        del x\n    plt.suptitle(\"locale classies engagement_index trainsitin trends.\", fontsize=18)\n    plt.tight_layout()\n    gc.collect()\n    ","ef5ae6a6":"transition_all(df)","0ed38fb1":"transition_locale(df)","ae79c185":"fig, axes = plt.subplots(1, 2, figsize=(22, 12))\nax = axes.ravel()\n\ndf[\"month\"] = df.time.dt.month\nsns.violinplot(data=df, x=\"month\", y=\"engagement_index\", hue=\"holiday\", ax=ax[0])\nax[0].set_title(\"engagement\")\nsns.violinplot(data=df, x=\"month\", y=\"pct_access\", hue=\"holiday\", ax=ax[1])\nax[1].set_title(\"pct_access\")\n\ndf.drop(\"month\", axis=1, inplace=True)\ngc.collect()\nplt.show()","4ecdc86a":"transition_locale_access_trand(df)","df3dd477":"transition_locale_engage_trand(df)","3c221bd1":"fig, axes = plt.subplots(2, 2, figsize=(22, 10))\nax = axes.ravel()\n\nsns.barplot(data=df, x=\"holiday\", y=\"pct_access\", ax=ax[0])\nax[0].set_title(\"is holiday use access rate\")\ngc.collect()\n\nsns.barplot(data=df, x=\"holiday\", y=\"engagement_index\", ax=ax[1])\nax[1].set_title(\"is holiday use engagement_index rate\")\ngc.collect()\n\nsns.barplot(data=df, x=\"is_pandemic\", y=\"pct_access\", ax=ax[2])\nax[2].set_title(\"before aftere covid19 access rate\")\ngc.collect()\n\nsns.barplot(data=df, x=\"is_pandemic\", y=\"engagement_index\", ax=ax[3])\nax[2].set_title(\"before aftere covid19 engagement rate\")\ngc.collect()\n\nplt.tight_layout()","d4d9bfbe":"%%time\n\n'''\ncolumns: state \nindex: state \n\nused cosine calculate.\n'''\n\nstate_sub_count_df = pd.crosstab(df.state, df[\"sub\"])\ns = MinMaxScaler(feature_range=(0.0, 1.0))\ns_df = s.fit_transform(state_sub_count_df)\n\ndf_sparse = sp.sparse.csr_matrix(s_df)\ndf_sparse = cosine_similarity(df_sparse)\ndf_sparse = pd.DataFrame(df_sparse, columns=state_sub_count_df.index, index=state_sub_count_df.index)\ndel state_sub_count_df, s_df\ngc.collect()\ndf_sparse.head()","8a88ff15":"us_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\n\n\ndef find_similar_state(state_name: str, n: int=10):\n    x = df_sparse[[state_name]].sort_values(state_name, ascending=False)[1:n+1]\n    x.columns = [\"similar\"]\n    return x \n\n\ndef show_area(similar_state: pd.DataFrame, state_name: str):\n    similar_state[\"state\"] = similar_state.index\n    similar_state = similar_state.reset_index(drop=True)\n    similar_state[\"state_abbver\"] = similar_state.state.replace(us_state_abbrev)\n    \n    fig = go.Figure()\n    layout = dict(\n        title_text = f\"Search for similar ({state_name}) state top 10k\",\n        geo_scope='usa',\n    )\n\n    fig.add_trace(\n        go.Choropleth(\n            locations=similar_state.state_abbver,\n            zmax=1,\n            z = similar_state.similar,\n            locationmode = 'USA-states', \n            marker_line_color='white',\n            geo='geo',\n            colorscale=px.colors.sequential.Teal, \n        )\n    )\n\n    fig.update_layout(layout)   \n    fig.show()\n    \n    \ndef show_count_bar(similar_state: pd.DataFrame):\n    state = similar_state.index.to_list()\n    \n    fig, axes = plt.subplots(1, 2, figsize=(22, 6))\n    ax = axes.ravel()\n    main = df.loc[df.state.isin(state), [\"main\"]].value_counts()\n    ax[0].pie(x=main.values)\n    ax[0].legend(main.index)\n    sub = df.loc[df.state.isin(state), [\"sub\"]].value_counts().to_frame().sort_values(\"sub\", ascending=False)[:5].sort_values(\"sub\", ascending=True)\n    sub.plot(kind=\"barh\", ax=ax[1])\n        \n    ax[0].set_title(\"similar for main rate.\")\n    ax[1].set_title(\"similar for sub counts.\")\n    plt.tight_layout()\n    del main, sub \n    gc.collect()\n    \n    \ndef show_transition(similar_state: pd.DataFrame, state_name):\n#     similar_state = find_similar_state(state_name)\n    state = similar_state.index.to_list()[:5]\n    \n    fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n    ax = axes.ravel()\n    \n    x = df.loc[df.state == state_name, [\"time\", \"engagement_index\"]]\n    x.groupby(\"time\").mean().plot(ax=ax[0])\n    ax[0].set_title(\"current state transition.\")\n    \n    for i, s in enumerate(state):\n        x = df.loc[df.state == s, [\"time\", \"engagement_index\"]]\n        x = x.groupby(\"time\").mean()\n        x.plot(ax=ax[i+1])\n        ax[i+1].set_title(f\"similaer state is {s}.\")\n        \n    plt.tight_layout()\n    del x \n    gc.collect()\n    \n    \ndef search_for_similar_plot(state_name: str, n: int=10):\n    similar_df = find_similar_state(state_name, n)\n    show_area(similar_df, state_name)\n    show_count_bar(similar_df)\n    show_transition(similar_df, state_name)\n    display(similar_df)\n    gc.collect()\n    \n    ","c1d78b19":"%%time \n\nsearch_for_similar_plot(\"Wisconsin\")","6d389098":"search_for_similar_plot('North Carolina')","488cf8af":"%%time \n\nx = df.groupby(\"time\")[\"engagement_index\", \"pct_access\"].sum()\nx = x.rename(columns={\"engagement_index\": \"engagement_index_lag_1\", \"pct_access\": \"pct_access_lag_1\"})\nx[\"engagement_index\"] = x.engagement_index_lag_1.shift(-1)\n\nfor col in [\"engagement_index_lag_1\", \"pct_access_lag_1\"]:\n    x[col.split(\"_\")[0]+\"_lag_2\"] = x[col].shift(1).fillna(0)\n    x[col.split(\"_\")[0]+\"_lag_3\"] = x[col].shift(2).fillna(0)\n    x[col.split(\"_\")[0]+\"_lag_30\"] = x[col].shift(30).fillna(0)\n    x[col.split(\"_\")[0]+\"_rolling7\"] = x[col].rolling(window=7).mean().fillna(0).reset_index(drop=True)\n    x[col.split(\"_\")[0]+\"_rolling30\"] = x[col].rolling(window=30).mean().fillna(0).reset_index(drop=True)\nx = x.fillna(0)\nx.head()","04f06256":"class DigitalDataset(Dataset):\n    def __init__(self, df):\n        n_span = 30 \n        train = df.iloc[:(-1)*(n_span+1), :]\n        val = df.iloc[(-1)*(n_span+1): -1, :]\n        test = df.iloc[(-1)*n_span:, :]\n        self.train = []\n        self.val = []\n        self.test = []\n        \n        x_train, x_val, x_test = train.drop(\"engagement_index\", axis=1), val.drop(\"engagement_index\", axis=1), test.drop(\"engagement_index\", axis=1)\n        y_train, y_val = train[[\"engagement_index\"]], val[[\"engagement_index\"]]\n        \n        x_train, x_val, x_test = self._scaler(x_train, x_val, x_test)\n        \n        for i in range(x_train.shape[0]-n_span):\n            input_data = {}\n            inputs = x_train[i:i+n_span]\n            inputs = torch.FloatTensor(inputs)\n            target = y_train.iloc[i+n_span]\n            target = torch.tensor(target, dtype=torch.float)\n            \n            input_data[\"inputs\"] = inputs \n            input_data[\"target\"] = target \n            self.train.append(input_data)\n            \n        for i in range(n_span):\n            input_data = {}\n            inputs_tr = x_train[(-1)*n_span+i:, :]\n            inputs_va = x_val[:i, :]\n            inputs = np.concatenate([inputs_tr, inputs_va])\n            inputs = torch.FloatTensor(inputs)\n            target = y_val.iloc[i]\n            target = torch.tensor(target, dtype=torch.float)\n            \n            input_data[\"inputs\"] = inputs \n            input_data[\"target\"] = target \n            self.val.append(input_data)\n            \n        input_data = {\"inputs\": torch.FloatTensor(x_test)}\n        self.test.append(input_data)\n        \n    def _scaler(self, tr, va, te):\n        rs = RobustScaler()\n        return rs.fit_transform(tr), rs.transform(va), rs.transform(te)\n    \n\nparams = {\n    \"hidden_dim\": 128, \n    \"input_size\": 12, \n}\n\n\nconfig = {\n    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\", \n    \"batch_size\": 12, \n    \"epoch\": 1000, \n    \"lr\": 0.001\n    \n}\n    \nclass DigitalModel(nn.Module):\n    def __init__(self, input_size=params[\"input_size\"], hidden_dim=params[\"hidden_dim\"]):\n        super(DigitalModel, self).__init__()\n        self.hidden_dim = hidden_dim \n        self.lstm = nn.LSTM(input_size, hidden_dim, batch_first=True)\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim\/\/2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim\/\/2, 1)\n        )\n        \n    def forward(self, x):\n        x, _ = self.lstm(x)\n        x = x[:, -1, :].view(-1, self.hidden_dim)\n        x = self.fc(x)\n        return x \n    ","2b8d62ad":"a = DigitalDataset(x)\nprint(f\"input shape: {a.train[0]['inputs'].size()}\")\nprint(f\"target shape: {a.val[0]['target'].size()}\")\n\nnet = DigitalModel()\na = torch.rand(2, 30, 12)\ny = net(a)\nprint(f\"output shape: {y.size()}\")","2028716e":"def train_fn(dl, model, criterion, optimizer, is_train=True):\n    total_loss = []\n    if is_train:\n        model.train()\n    else:\n        model.eval()\n        \n    for d in tqdm(dl):\n        x = d[\"inputs\"].to(config[\"device\"])\n        t = d[\"target\"].to(config[\"device\"])\n        \n        if is_train:\n            y = model(x)\n            loss = criterion(y.view(-1), t.view(-1))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n        else:\n            with torch.no_grad():\n                y = model(x)\n                loss = criterion(y.view(-1), t.view(-1))\n                \n        total_loss.append(loss.item())\n        del x, t\n    total_loss = np.array(total_loss)\n    return np.mean(total_loss)\n\ndef val_fn(dl, model):\n    with torch.no_grad():\n        pred = []\n        for d in tqdm(dl):\n            x = d[\"inputs\"].to(config[\"device\"])\n\n            y = model(x)\n            y = y.squeeze().detach().cpu().numpy()\n            for yy in y:\n                pred.append(yy)\n            del x\n    return pred\n\ndef test_fn(dl, model):\n    with torch.no_grad():\n        pred = []\n        for d in tqdm(dl):\n            x = d[\"inputs\"].to(config[\"device\"])\n\n            y = model(x)\n            y = y.squeeze().detach().cpu().numpy()\n            pred.append(y.squeeze())\n    return pred\n\n\ndef mae(pred, corr):\n    return np.mean(np.abs(pred - corr))\n","592602e2":"def fit(train_dl, val_dl, debug=True):\n    model = DigitalModel()\n    criterion = nn.L1Loss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n    \n    best_model, best_loss = None, np.inf \n    \n    for e in range(1 if debug else config[\"epoch\"]):\n        ts = time.time()\n        loss_tr = train_fn(train_dl, model, criterion, optimizer)\n        val_tr = train_fn(val_dl, model, criterion, None, False)        \n        \n        if best_loss > val_tr:\n            best_model = model \n            best_loss = val_tr\n        now = time.time()\n        print(f\"epoch: {e+1} | tr loss: {loss_tr:.3f} | va loss: {val_tr:.3f} | dilation {now-ts}s | \")\n    print(f\"best val loss: {best_loss:.3f}\")\n    gc.collect()\n    return best_model \n\ndef predict(dl, model, is_test=False):\n    if is_test:\n        p = test_fn(dl, model)\n    else:\n        p = val_fn(dl, model)\n    return p \n","c306cab1":"def main(df, debug):\n    data = DigitalDataset(df)\n    train, val, test = data.train, data.val, data.test \n    \n    train_dl = DataLoader(train, \n                         batch_size=config[\"batch_size\"],\n                         shuffle=False, drop_last=False)\n    val_dl = DataLoader(val, \n                         batch_size=config[\"batch_size\"], \n                         shuffle=False, drop_last=False)\n    test_dl = DataLoader(test, \n                         batch_size=1,\n                         shuffle=False, drop_last=False)\n    model = fit(train_dl, val_dl, debug)\n    predv = predict(val_dl, model, False)\n    predt = predict(test_dl, model, True)\n    \n    print(\"===================================================================================\")\n    print(f\"validation mae: {mae(predv, df.iloc[-31: -1, :]['engagement_index'].values.ravel())}\")\n    print(\"===================================================================================\")\n\n    print(f\"Expected to be {predt[0]} tomorrow \")\n    print(\"===================================================================================\")\n\n    \nif __name__ == \"__main__\":\n    main(x, False)","f9eb13f7":"### locale and statement value counts.","22693111":"# Value counts EDA.","4996a5ef":"On holidays and weekdays, weekdays are clearly larger. Surprisingly, there is less access before the pandemic due to the distribution before and after the corona.  This can be seen from the time series graph above, due to the fact that it was originally large in the city category. ","23b403c7":"### main and sub value counts.","5ded03f2":"### all locale and engagement and access","c613068d":"# Search for same state by sub \nThe similarity is calculated by putting together the count tables from the states and the subs used based on them. That is, you can split states that belong to similar classification categories. It also searches for aggregates from states that belong to similar categories. ","d472fa2d":"### holiday and covid19 before after pandemic ","deb0d910":"The transition of group distribution by states with similar subs cannot be said to be very similar.\nRather, it seems to depend on the locale. ","05068c21":"### How well developed cities can be categorized by a particular locale ","0927f100":"# Transition EDA ","da5b6cf1":"# Select Use columns ","57ad1bd1":"# ","05e2499b":"# Predict tomorrow engagement_index\tby LSTM model.","5807b3fb":"### Locale vs engagement and poc_access transition.","6cd1db59":"### Locale dupicated ","ad0b1d70":"## merge dataframe ","f7551c17":"### month vs engagement by holiday.","f20d32b5":"### Sub count in a specific main "}}