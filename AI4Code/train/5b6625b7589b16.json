{"cell_type":{"63ca1121":"code","cdd7b3d3":"code","9418cdb8":"code","2ba738fd":"code","b2254263":"code","d002ff94":"code","6675ba54":"code","19dc1ca7":"code","6e1e9486":"code","4c36332b":"code","fdcd5378":"code","f44a2012":"markdown","49522ac5":"markdown","9551b340":"markdown","5bdf7db1":"markdown","feaaf36c":"markdown","a37b79a3":"markdown","27ac56a8":"markdown","a5a747a8":"markdown","13e6e267":"markdown","9041172b":"markdown","bae6311d":"markdown"},"source":{"63ca1121":"import os\nimport time\nimport pickle\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n# from sklearn.metrics import log_loss, roc_auc_score this is way to track down if the model is overfitting","cdd7b3d3":"#DATA_PATH = '..\/input\/jane-street-market-prediction\/' no need to import the data because we're going to only load pytorch and tensorflow models.\nCACHE_PATH = '..\/input\/mlp012003weights'\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n    # with gzip.open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n    # with gzip.open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\nf_mean = np.load(f'{CACHE_PATH}\/f_mean_online.npy')","9418cdb8":"# list of the features\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\n# list of all the features\nall_feat_cols = [col for col in feat_cols]\n\n# add two more features to the feature list\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n# resp 1,2,3,4\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']","2ba738fd":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F","b2254263":"##### Model&Data fnc\nclass Model(nn.Module):\n    \n    def __init__(self):\n        \n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        \n        # Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, \n        # which helps prevent overfitting. Inputs not set to 0 are scaled up by 1\/(1 - rate) such that the sum over all inputs is unchanged.\n        # https:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/\n        \n        # Dropout is a regularization technique. You should use it only to reduce variance (validation performance vs training performance).\n        # It is not intended to reduce the bias, and you should not use it in this way.\n        # Dropout works by probabilistically removing, or \u201cdropping out,\u201d inputs to a layer, \n        # which may be input variables in the data sample or activations from a previous layer.\n        # With dropout (dropout rate less than some small value), the accuracy will gradually increase and loss will gradually decrease first\n        # When you increase dropout beyond a certain threshold, it results in the model not being able to fit properly.\n        # https:\/\/stackoverflow.com\/questions\/59044351\/can-dropout-increases-training-data-performance\n        \n        # Dropout prevents overfitting due to a layer's \"over-reliance\" on a few of its inputs. Because these inputs aren't always present \n        # during training (i.e. they are dropped at random), the layer learns to use all of its inputs, improving generalization.\n        # https:\/\/stats.stackexchange.com\/questions\/374742\/does-dropout-regularization-prevent-overfitting-due-to-too-many-iterations\n        \n        # The default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, \n        # where 1.0 means no dropout, and 0.0 means no outputs from the layer. \n        # A good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8.\n        # https:\/\/machinelearningmastery.com\/dropout-for-regularizing-deep-neural-networks\/\n        \n        self.dropout0 = nn.Dropout(0.8) # 0.2\n\n        dropout_rate = 0.5 # 0.2\n        hidden_size = 256\n        \n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        \n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x","d002ff94":"# Since we're going to onyl load the model, then use it for inference, then it's better to use cpu\n# Otherwise, you want to retrain the model, then enable the GPU for faster calculations\n\nif torch.cuda.is_available():\n    print('using device: cuda')\n    device = torch.device(\"cuda:0\")\nelse:\n    print('using device: cpu')\n    device = torch.device('cpu')","6675ba54":"NFOLDS = 5\n\nmodel_list = []\ntmp = np.zeros(len(feat_cols))\nfor _fold in range(NFOLDS):\n    torch.cuda.empty_cache()\n    model = Model()\n    model.to(device)\n    model_weights = f\"{CACHE_PATH}\/online_model{_fold}.pth\"\n    #model.load_state_dict(torch.load(model_weights))\n    model.load_state_dict(torch.load(model_weights, map_location=device))\n    model.eval()\n    model_list.append(model)","19dc1ca7":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa","6e1e9486":"SEED = 1111\n\nnp.random.seed(SEED)\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 300\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(len(feat_cols), 5, hidden_units, dropout_rates, label_smoothing, learning_rate)","4c36332b":"# Fit the model and save it with \n#clf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n#clf.save(f'model.h5')\n\n# Load the Fitted model\n# !ls ..\/input\/jane-street-with-keras-nn-overfit\/\nclf.load_weights('..\/input\/jane-street-with-keras-nn-overfit\/model.h5')\n\n# If you have several models, the you can store into a list\n#tf_models = [clf]","fdcd5378":"th = 0.5 # 0.5 0.501 0.0502 0.053 did not have an effect\nimport janestreet\njanestreet.competition.make_env.__called__ = False\n\nenv = janestreet.make_env()\nenv_iter = env.iter_test()\n\n#test_df has one single row of data with all the feautures\n# pred_dfd has 1 or 0 which is an action that comes from test_df\n\nfor (test_df, pred_df) in tqdm(env_iter):\n\n    # Data Processing\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, feat_cols].values\n        \n        # if there is only one value eqauls nan, then x_tt.sum will return nan, then np.isnan will be true, and enters the loop\n        if np.isnan(x_tt.sum()):\n            # Replace NaN with zero and infinity with large finite numbers (default behaviour)\n            x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean # the score goew down if we take the if statement\n\n    \n        cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n        cross_1_2 = x_tt[:, 1] \/ (x_tt[:, 2] + 1e-5)\n        feature_inp = np.concatenate((x_tt, np.array(cross_41_42_43).reshape(x_tt.shape[0], 1), np.array(cross_1_2).reshape(x_tt.shape[0], 1),), axis=1)\n\n        \n        # PytTorch prediction \n        torch_pred = np.zeros((1, len(target_cols)))\n        for model in model_list:\n            torch_pred += model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() \/ NFOLDS\n        torch_pred = np.median(torch_pred)\n\n        \n        # TensofFlow prediction\n        # I foyu have several TF models then use\n        #tf_pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in tf_models],axis=0))\n        # If you have only one model then use\n        tf_pred = np.median(clf(x_tt))\n\n        \n        # PyTorch and TensorFlow Average prediction\n        pred = torch_pred * 0.4 + tf_pred * 0.6\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n        \n    else:\n        pred_df.action = 0\n        \n    env.predict(pred_df)","f44a2012":"### Feature Engineering","49522ac5":"### PyTorch Load Model weights","9551b340":"# TensorFlow\n### Libs","5bdf7db1":"### PyTorch Model\n\nI have used this course before, to be able to understand bacis about about Pytorch:<br\/>\nhttps:\/\/www.udacity.com\/course\/deep-learning-pytorch--ud188","feaaf36c":"### TensorFlow Load model weights","a37b79a3":"### PyTorch on CPU or GPU ","27ac56a8":"# Data Save\/Load","a5a747a8":"### TensorFlow model\n\nTo understand the following code, refer to my detailed notebook: <br\/>\n[Explanation of the model](https:\/\/www.kaggle.com\/mouafekmk\/simple-mlp)\n","13e6e267":"# PyTorch\nI have used this course before, to be able to understand bacis about about Pytorch:<br\/>\nhttps:\/\/www.udacity.com\/course\/deep-learning-pytorch--ud188\n\n### PyTorch Libs","9041172b":"# Libs & Config","bae6311d":"\n\n# Inference"}}