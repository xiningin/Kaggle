{"cell_type":{"976178c6":"code","9c622cc1":"code","6133774d":"code","cd3bb0e4":"code","1a7014a8":"code","12d55c6b":"code","884f6fe8":"code","2e96dc69":"code","fec8a7e9":"code","afba08c8":"code","448814b1":"code","e52d1f5d":"code","ef9a6d22":"code","c44270ac":"code","256811d7":"code","a26916a5":"code","78d01907":"code","142fa087":"code","86074bf3":"code","b8116c54":"code","52e8a424":"code","166b78c4":"code","6a5c8a7e":"markdown","8f4ab914":"markdown","6bd3c131":"markdown","a4a76a09":"markdown","ffd6f6dd":"markdown","49c73c9a":"markdown","37d923e3":"markdown","1528f4cb":"markdown","6a4a96d2":"markdown","a898ed79":"markdown","1a0af317":"markdown","e5ba8dbf":"markdown"},"source":{"976178c6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df =  pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","9c622cc1":"train_df.head()","6133774d":"train_df.info()","cd3bb0e4":"test_df.isnull().sum()","1a7014a8":"sns.countplot(\"Embarked\", data=train_df)","12d55c6b":"train_df[\"Embarked\"].fillna('S', inplace=True)","884f6fe8":"sns.pairplot(data=train_df, hue='Survived', height=1.5, diag_kind=None)","2e96dc69":"# Higher fares correlate to a higher survival rate.\nsns.scatterplot(x=train_df[\"PassengerId\"], y=train_df[\"Fare\"], hue=train_df[\"Survived\"])","fec8a7e9":"# While the age itself doesn't show a exceedingly strong correlation to survival, it seem that children and minors have higher survival rates.\nsns.scatterplot(x=train_df[\"PassengerId\"], y=train_df[\"Age\"], hue=train_df[\"Survived\"])","afba08c8":"train_df[\"Deck\"] = train_df[\"Cabin\"].str[0]\ntrain_df[\"Deck\"].fillna(\"Z\", inplace=True) # Fills the missing values with a non-existent Deck","448814b1":"train_df[\"Age\"].fillna(train_df[train_df[\"Age\"] >= 18][\"Age\"].mean(), inplace=True)\n\nprint(\"Average non-minor age is\", train_df[train_df[\"Age\"] >= 18][\"Age\"].mean())","e52d1f5d":"def data_treatment(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Treats a DataFrame as needed for our models\"\"\"\n    \n    df[\"Deck\"] = df[\"Cabin\"].str[0]\n    df[\"Deck\"].fillna(\"Z\", inplace=True) # Fill the missing values with a non-existent Deck\n    \n    df[\"Age\"].fillna(33.58, inplace=True)\n    \n    df[\"Minor\"] = df[\"Age\"] < 18\n    \n    df[\"FamilyNum\"] = df['SibSp'] + df['Parch'] # Amount of family members on board\n    \n    df[\"Alone\"] = df[\"FamilyNum\"] == 0\n    \n    df.drop([\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\"], axis=1, inplace=True, errors=\"ignore\")\n    \n    df = pd.get_dummies(df, columns=[\"Sex\", \"Embarked\", \"Deck\"], prefix=[\"Sex\", \"Embarked\", \"Deck\"])\n    \n    return df","ef9a6d22":"treated_train_df = data_treatment(train_df.copy())\ntreated_test_df = data_treatment(test_df.copy())\ntreated_test_df[\"Deck_T\"] = 0 # Ops, it seems the test data has no passengers in deck T. We must create the missing columns","c44270ac":"treated_train_df.head()","256811d7":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","a26916a5":"# We split our data into training and validation datasets. As is usual (and a partially arbitrary), 20% of the data is used for validation.\nX = treated_train_df.drop(columns=[\"Survived\"])\nY = treated_train_df[\"Survived\"]\n\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)","78d01907":"x_train.head()","142fa087":"models = {\n    \"Logistic Regression\": LogisticRegression(random_state=1, solver='liblinear'),\n    \"Support Vector Machine\": SVC(random_state=1),\n    \"AdaBoost\": AdaBoostClassifier(random_state=1),\n    \"Decision Tree\": DecisionTreeClassifier(random_state=1), \n    \"Random Forest\": RandomForestClassifier(random_state=1),\n    \"LGBM\": LGBMClassifier(random_state=1, is_unbalance=True), \n    \"CatBoost\": CatBoostClassifier(random_state=1, verbose=0)\n}\n\npredictions = {}\n\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    predictions[name] = model.predict(x_val)","86074bf3":"metrics.roc_auc_score(predictions[\"CatBoost\"], y_val)","b8116c54":"score_df = pd.DataFrame(columns=[\"Model\", \"Acc\", \"Precision\", \"Recall\", \"F-Score\", \"AUC\"])\n\nfor name, pred in predictions.items():\n    model_pred_df = pd.DataFrame({\n        \"Model\": name,\n        \"Acc\": metrics.accuracy_score(pred, y_val),\n        \"Precision\": metrics.precision_score(pred, y_val),\n        \"Recall\": metrics.recall_score(pred, y_val),\n        \"F-Score\": metrics.f1_score(pred, y_val),\n        \"AUC\": metrics.roc_auc_score(pred, y_val)\n    }, index=[0])\n    score_df = pd.concat([score_df, model_pred_df])\n\nscore_df.head()","52e8a424":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 10))\n\nax[0, 0].set_title(\"Acc\")\nax[0, 0].barh(score_df[\"Model\"], score_df[\"Acc\"])\n\nax[0, 2].set_title(\"Precision\")\nax[0, 2].barh(score_df[\"Model\"], score_df[\"Precision\"])\n\nax[1, 0].set_title(\"Recall\")\nax[1, 0].barh(score_df[\"Model\"], score_df[\"Recall\"])\n\nax[1, 2].set_title(\"F-Score\")\nax[1, 2].barh(score_df[\"Model\"], score_df[\"F-Score\"])\n\nax[2, 0].set_title(\"AUC\")\nax[2, 0].barh(score_df[\"Model\"], score_df[\"AUC\"])\n\nax[0, 1].set_visible(False)\nax[1, 1].set_visible(False)\nax[2, 1].set_visible(False)\nax[2, 2].set_visible(False)","166b78c4":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"], \"Survived\": models[\"LGBM\"].predict(treated_test_df)})\nsubmission.to_csv(\"submission.csv\", index=False)","6a5c8a7e":"# Data Treatment and Feature Engineering\n\n## Missing `Cabin` Values\n\nThe current Cabin data is too detailed for ease of use or treatment. Let's surrogate it with just the information of which Deck each Cabin belongs to. It should be enough information to account if the passenger could get to lifeboats in time while allowing us to work with it.","8f4ab914":"Right off the bat, we can notice some NaN values in the **\"Cabin\"** column, and with `info()` we can verify that the **\"Age\"** column also has quite a few missing values, and **\"Embarked\"** has two.\n\nSince **\"Embarked\"** has only a couple of missing values, let's fill them with the most common value and move on.","6bd3c131":"From the graphs above, we already can gather some insights!","a4a76a09":"With a `pairplot` we can try to visualize general relationships between data, and then explore them further to confirm our suspicions.","ffd6f6dd":"### Before dealing with the other missing values, let's try to understand the data better.\n\nLet's create a couple useful columns that may help our predictions","49c73c9a":"# Conclusion\n\nNaturally there is a lot of room for improvement on this approach. I aimed on creating an easy-to-follow notebook that deals with some basic problems of null value treatment, EDA and Model Training.\n\nHope this can help in some way, and feel free to comment your recommendations on how I could improve.\n\nGood luck!","37d923e3":"# Exploratory Data Analysis","1528f4cb":"## Missing `Age` Values\n\nLet's not *rock the boat* too much and just use the average age of all the non-minor passengers to fill the missing values. I'm ignoring the minors because I'm **assuming** (probably incorrectly) that a minor would be more likely to be accounted for, and therefore missing age data is represented of mostly adults.","6a4a96d2":"# Model Selection and Training\n\nTime to train models.\n\nIn real situations, we should maybe put more tought into what model we are using and why, but for sake of simplicity we will just test a bunch of models and see what returns better results. Yay!\n\n---\n### Importing a bunch of models and auxiliary functions","a898ed79":"### Treating all data\n\nWe can't forget to also treat our test data in the same way we do with the training data. For this, let's create a function that deals with a generic DataFrame as we did above, creates a few other columns we may find useful and deletes the ones we won't use.","1a0af317":"Hello! This is my first notebook on Kaggle, so any recommendations on how to improve are welcome. Thank you!\n\n## Data Extraction","e5ba8dbf":"Most models had similar results (with the exception of the SVM, that was considerably worse off). LGBM seems to be the best overall, so let's submit it's predictions!"}}