{"cell_type":{"83e9d1af":"code","8bc32cb3":"code","cd733933":"code","1e36ccc0":"code","17d5414a":"code","b549375f":"code","9c626097":"code","551b9ce3":"code","b83a3c46":"code","68066d19":"code","ef2feb07":"code","f87b590e":"code","69ba6a65":"code","42810ab0":"code","c07ba711":"code","286c5a43":"code","38978176":"code","377fd083":"code","cb921a95":"code","0e43271c":"code","b7bca2b9":"code","deb41e9f":"code","a05e40cd":"code","af85665c":"code","1a82a845":"code","7b5cf055":"code","ead572a2":"code","d6dfca0b":"code","9a795396":"code","63ee79d3":"code","58946d5b":"code","2c95746c":"markdown","ec8085df":"markdown","6e3c179a":"markdown","eefe133a":"markdown","00eca75c":"markdown","520c85dc":"markdown","d3e7d831":"markdown","ede29d8a":"markdown","2309d773":"markdown","a278a5b1":"markdown","82b2d3cf":"markdown","8e3b28b6":"markdown","76c383ce":"markdown","81c7c09a":"markdown","acdcd6b0":"markdown","8587e754":"markdown","711ef791":"markdown","41f4c1f0":"markdown","fdf839ee":"markdown","093a4801":"markdown","fca355c7":"markdown","738e609b":"markdown","c04a7fdf":"markdown","bc3223f1":"markdown","2ce69dc2":"markdown","9752ad5c":"markdown","aa7b39e4":"markdown","4da816bd":"markdown","a875ca4c":"markdown","29032b15":"markdown","2c792aa1":"markdown","389767e2":"markdown","73727f28":"markdown","1214dae2":"markdown","904376a6":"markdown","849cbf4d":"markdown"},"source":{"83e9d1af":"!pip install -q efficientnet >> \/dev\/null\n!pip install -q imagesize\n!pip install -qU wandb\n!pip install -q vit-keras","8bc32cb3":"import pandas as pd, numpy as np, random,os, shutil\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nimport sklearn\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nimport imagesize\nimport wandb\nimport yaml\n\nfrom vit_keras import vit\nfrom IPython import display as ipd\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.metrics import roc_auc_score","cd733933":"print('np:', np.__version__)\nprint('pd:', pd.__version__)\nprint('sklearn:', sklearn.__version__)\nprint('tf:',tf.__version__)\nprint('tfa:', tfa.__version__)\nprint('w&b:', wandb.__version__)","1e36ccc0":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"WANDB\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    anonymous = \"must\"\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","17d5414a":"class CFG:\n    wandb         = True\n    competition   = 'petfinder' \n    _wandb_kernel = 'awsaf49'\n    debug         = False\n    exp_name      ='vit_b16+cls-aug1' # name of the experiment, folds will be grouped using 'exp_name'\n    \n    # USE verbose=0 for silent, vebose=1 for interactive, verbose=2 for commit\n    verbose      = 1 if debug else 0\n    display_plot = True\n\n    device = \"TPU\" #or \"GPU\"\n\n    model_name = 'vit_b16' # 'vit_b32'\n\n    # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\n    seed = 42\n\n    # NUMBER OF FOLDS. USE 2, 5, 10\n    folds = 5\n    \n    # FOLDS TO TRAIN\n    selected_folds = [0, 1, 2, 3, 4]\n\n    # IMAGE SIZE\n    img_size = [512, 512]\n\n    # BATCH SIZE AND EPOCHS\n    batch_size  = 32\n    epochs      = 5\n\n    # LOSS\n    loss      = 'BCE'\n    optimizer = 'Adam'\n\n    # CFG.augmentATION\n    augment   = True\n    transform = True\n\n    # TRANSFORMATION\n    fill_mode = 'reflect'\n    rot    = 10.0\n    shr    = 5.0\n    hzoom  = 30.0\n    wzoom  = 30.0\n    hshift = 30.0\n    wshift = 30.0\n\n    # FLIP\n    hflip = True\n    vflip = True\n\n    # CLIP [0, 1]\n    clip = False\n\n    # LEARNING RATE SCHEDULER\n    scheduler   = 'exp' # Cosine\n\n    # Dropout\n    drop_prob   = 0.75\n    drop_cnt    = 10\n    drop_size   = 0.05\n\n    #bri, contrast\n    sat  = [0.7, 1.3]\n    cont = [0.8, 1.2]\n    bri  =  0.15\n    hue  = 0.05\n\n    # TEST TIME CFG.augmentATION STEPS\n    tta = 1\n    \n    tab_cols    = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n                   'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n    target_col  = ['Pawpularity']","b549375f":"def seeding(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n#     os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    tf.random.set_seed(SEED)\n    print('seeding done!!!')\nseeding(CFG.seed)","9c626097":"if CFG.device == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        CFG.device = \"GPU\"\n\nif CFG.device != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif CFG.device == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","551b9ce3":"BASE_PATH = '\/kaggle\/input\/petfinder-pawpularity-score'\nGCS_PATH  = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')","b83a3c46":"def get_imgsize(row):\n    width, height = imagesize.get(row['image_path'].replace(GCS_PATH, BASE_PATH))\n    row['width']  = width\n    row['height'] = height\n    return row","68066d19":"# Train Data\ndf = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ndf['image_path'] = GCS_PATH + '\/train\/' + df.Id + '.jpg'\ntqdm.pandas(desc='train')\ndf = df.progress_apply(get_imgsize, axis=1)\ndisplay(df.head(2))\n\n# Test Data\ntest_df  = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\ntest_df['image_path'] = GCS_PATH + '\/test\/' + test_df.Id + '.jpg'\ntqdm.pandas(desc='test')\ntest_df = test_df.progress_apply(get_imgsize, axis=1)\n\ndisplay(test_df.head(2))","ef2feb07":"print('train_files:',df.shape[0])\nprint('test_files:',test_df.shape[0])","f87b590e":"from pandas_profiling import ProfileReport\ntrain_profile = ProfileReport(df, title=\"Train Data\")\ntest_profile  = ProfileReport(test_df, title=\"Test Data\")","69ba6a65":"display(train_profile)","42810ab0":"display(test_profile)","c07ba711":"num_bins = int(np.floor(1 + np.log2(len(df))))\ndf[\"bins\"] = pd.cut(df[CFG.target_col].values.reshape(-1), bins=num_bins, labels=False)\n\nskf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df, df[\"bins\"])):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.groupby(['fold', \"bins\"]).size())","286c5a43":"def get_mat(shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    #rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n#     c1   = tf.math.cos(rotation)\n#     s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n#     rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n#                                    -s1,  c1,   zero, \n#                                    zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                               zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n\n    return  K.dot(shear_matrix,K.dot(zoom_matrix, shift_matrix)) #K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))                  \n\ndef transform(image, DIM=CFG.img_size):#[rot,shr,h_zoom,w_zoom,h_shift,w_shift]):\n    if DIM[0]!=DIM[1]:\n        pad = (DIM[0]-DIM[1])\/\/2\n        image = tf.pad(image, [[0, 0], [pad, pad+1],[0, 0]])\n        \n    NEW_DIM = DIM[0]\n    \n    rot = CFG.rot * tf.random.normal([1], dtype='float32')\n    shr = CFG.shr * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ CFG.hzoom\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ CFG.wzoom\n    h_shift = CFG.hshift * tf.random.normal([1], dtype='float32') \n    w_shift = CFG.wshift * tf.random.normal([1], dtype='float32') \n    \n    transformation_matrix=tf.linalg.inv(get_mat(shr,h_zoom,w_zoom,h_shift,w_shift))\n    \n    flat_tensor=tfa.image.transform_ops.matrices_to_flat_transforms(transformation_matrix)\n    \n    image=tfa.image.transform(image,flat_tensor, fill_mode=CFG.fill_mode)\n    \n    rotation = math.pi * rot \/ 180.\n    \n    image=tfa.image.rotate(image,-rotation, fill_mode=CFG.fill_mode)\n    \n    if DIM[0]!=DIM[1]:\n        image=tf.reshape(image, [NEW_DIM, NEW_DIM,3])\n        image = image[:, pad:DIM[1]+pad,:]\n    image = tf.reshape(image, [*DIM, 3])    \n    return image\n\ndef dropout(image,DIM=CFG.img_size, PROBABILITY = 0.6, CT = 5, SZ = 0.1):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    \n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n    if (P==0)|(CT==0)|(SZ==0): \n        return image\n    \n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM[1]),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM[0]),tf.int32)\n        # COMPUTE SQUARE \n        WIDTH = tf.cast( SZ*min(DIM),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH\/\/2)\n        yb = tf.math.minimum(DIM[0],y+WIDTH\/\/2)\n        xa = tf.math.maximum(0,x-WIDTH\/\/2)\n        xb = tf.math.minimum(DIM[1],x+WIDTH\/\/2)\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa,3], dtype = image.dtype) \n        three = image[ya:yb,xb:DIM[1],:]\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM[0],:,:]],axis=0)\n        image = tf.reshape(image,[*DIM,3])\n\n#     image = tf.reshape(image,[*DIM,3])\n    return image","38978176":"def build_decoder(with_labels=True, target_size=CFG.img_size, ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.image.resize(img, target_size)\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.reshape(img, [*target_size, 3])\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), tf.cast(label, tf.float32)\/100.0\n    \n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True, dim=CFG.img_size):\n    def augment(img, dim=dim):\n        img = transform(img,DIM=dim) if CFG.transform else img\n        img = tf.image.random_flip_left_right(img) if CFG.hflip else img\n        img = tf.image.random_flip_up_down(img) if CFG.vflip else img\n        img = tf.image.random_hue(img, CFG.hue)\n        img = tf.image.random_saturation(img, CFG.sat[0], CFG.sat[1])\n        img = tf.image.random_contrast(img, CFG.cont[0], CFG.cont[1])\n        img = tf.image.random_brightness(img, CFG.bri)\n        img = dropout(img, DIM=dim, PROBABILITY = CFG.drop_prob, CT = CFG.drop_cnt, SZ = CFG.drop_size)\n        img = tf.clip_by_value(img, 0, 1)  if CFG.clip else img         \n        img = tf.reshape(img, [*dim, 3])\n        return img\n    \n    def augment_with_labels(img, label):    \n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, batch_size=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\", drop_remainder=False):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    \n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    ds = ds.cache(cache_dir) if cache else ds\n    ds = ds.repeat() if repeat else ds\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(AUTO)\n    return ds","377fd083":"def display_batch(batch, size=2):\n    imgs, tars = batch\n    plt.figure(figsize=(size*5, 5))\n    for img_idx in range(size):\n        plt.subplot(1, size, img_idx+1)\n        plt.title(f'{CFG.target_col[0]}: {tars[img_idx].numpy()[0]}', fontsize=15)\n        plt.imshow(imgs[img_idx,:, :, :])\n        plt.xticks([])\n        plt.yticks([])\n    plt.tight_layout()\n    plt.show() ","cb921a95":"fold = 0\nfold_df = df.query('fold==@fold')[:1000]\npaths  = fold_df.image_path.tolist()\nlabels = fold_df[CFG.target_col].values\nds = build_dataset(paths, labels, cache=False, batch_size=CFG.batch_size*REPLICAS,\n                   repeat=True, shuffle=True, augment=True)\nds = ds.unbatch().batch(20)\nbatch = next(iter(ds))\ndisplay_batch(batch, 5);","0e43271c":"def RMSE(y_true, y_pred, denormalize=True):\n    if denormalize:\n        # denormalizing\n        y_true = y_true*100.0\n        y_pred = y_pred*100.0\n    # rmse\n    loss = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(tf.subtract(y_true, y_pred))))\n    return loss\nRMSE.__name__='rmse'","b7bca2b9":"import efficientnet.tfkeras as efn\nfrom vit_keras import vit, utils, visualize, layers\n\nname2effnet = {\n    'efficientnet_b0': efn.EfficientNetB0,\n    'efficientnet_b1': efn.EfficientNetB1,\n    'efficientnet_b2': efn.EfficientNetB2,\n    'efficientnet_b3': efn.EfficientNetB3,\n    'efficientnet_b4': efn.EfficientNetB4,\n    'efficientnet_b5': efn.EfficientNetB5,\n    'efficientnet_b6': efn.EfficientNetB6,\n    'efficientnet_b7': efn.EfficientNetB7,\n}\n\ndef build_model(model_name=CFG.model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):       \n#     base = name2effnet[model_name](input_shape=(DIM, DIM, 3),\n#                                   include_top=include_top,\n#                                    weights='imagenet',\n#                                   )\n    base = getattr(vit, model_name)(image_size=(DIM, DIM),\n                   include_top=False, \n                   pretrained_top=False,\n                   pretrained=True, \n                   weights='imagenet21k+imagenet2012')\n    inp = base.inputs\n    out = base.output\n#     out = tf.keras.layers.GlobalAveragePooling2D()(out)\n    out = tf.keras.layers.Dense(64,activation='selu')(out)\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n    model = tf.keras.Model(inputs=inp, outputs=out)\n    if compile_model:\n        #optimizer\n        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n        #loss\n        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n        #metric\n        rmse = RMSE\n        model.compile(optimizer=opt,\n                      loss=loss,\n                      metrics=[rmse])\n    return model","deb41e9f":"tmp = build_model(CFG.model_name, DIM=CFG.img_size[0], compile_model=True)","a05e40cd":"def get_lr_callback(batch_size=8, plot=False):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        elif CFG.scheduler=='exp':\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        elif CFG.scheduler=='cosine':\n            decay_total_epochs = CFG.epochs - lr_ramp_ep - lr_sus_ep + 3\n            decay_epoch_index = epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index \/ decay_total_epochs\n            cosine_decay = 0.5 * (1 + math.cos(phase))\n            lr = (lr_max - lr_min) * cosine_decay + lr_min\n        return lr\n    if plot:\n        plt.figure(figsize=(10,5))\n        plt.plot(np.arange(CFG.epochs), [lrfn(epoch) for epoch in np.arange(CFG.epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('learnig rate')\n        plt.title('Learning Rate Scheduler')\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\n_=get_lr_callback(CFG.batch_size, plot=True )","af85665c":"import matplotlib.cm as cm, cv2\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import array_to_img\n\ndef attention_map(model, image):\n    \"\"\"Get an attention map for an image and model using the technique\n    described in Appendix D.7 in the paper (unofficial).\n    Args:\n        model: A ViT model\n        image: An image for which we will compute the attention map.\n    \"\"\"\n    size = model.input_shape[1]\n    grid_size = int(np.sqrt(model.layers[5].output_shape[0][-2] - 1))\n\n    # Prepare the input\n    X = cv2.resize(image, (size, size))[np.newaxis, :]  # type: ignore\n\n    # Get the attention weights from each transformer.\n    outputs = [\n        l.output[1] for l in model.layers if isinstance(l, layers.TransformerBlock)\n    ]\n    weights = np.array(\n        tf.keras.models.Model(inputs=model.inputs, outputs=outputs).predict(X)\n    )\n    num_layers = weights.shape[0]\n    num_heads = weights.shape[2]\n    reshaped = weights.reshape(\n        (num_layers, num_heads, grid_size ** 2 + 1, grid_size ** 2 + 1)\n    )\n\n    # From Appendix D.6 in the paper ...\n    # Average the attention weights across all heads.\n    reshaped = reshaped.mean(axis=1)\n\n    # From Section 3 in https:\/\/arxiv.org\/pdf\/2005.00928.pdf ...\n    # To account for residual connections, we add an identity matrix to the\n    # attention matrix and re-normalize the weights.\n    reshaped = reshaped + np.eye(reshaped.shape[1])\n    reshaped = reshaped \/ reshaped.sum(axis=(1, 2))[:, np.newaxis, np.newaxis]\n\n    # Recursively multiply the weight matrices\n    v = reshaped[-1]\n    for n in range(1, len(reshaped)):\n        v = np.matmul(v, reshaped[-1 - n])\n\n    # Attention from the output token to the input space.\n    mask = v[0, 1:].reshape(grid_size, grid_size)\n    mask = mask \/ mask.max()\n    mask = np.uint8(255 * mask)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors  = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[mask]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = cv2.resize(jet_heatmap, dsize=(image.shape[1], image.shape[0]))\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.5 + image\n    superimposed_img = array_to_img(superimposed_img)\n    return superimposed_img","1a82a845":"if CFG.wandb:\n    def wandb_init(fold):\n        config = {k:v for k,v in dict(vars(CFG)).items() if '__' not in k}\n        config.update({\"fold\":int(fold)})\n        yaml.dump(config, open(f'\/kaggle\/working\/config fold-{fold}.yaml', 'w'),)\n        config = yaml.load(open(f'\/kaggle\/working\/config fold-{fold}.yaml', 'r'), Loader=yaml.FullLoader)\n        run    = wandb.init(project=\"petfinder-public\",\n                   name=f\"fold-{fold}|dim-{CFG.img_size[0]}|model-{CFG.model_name}\",\n                   config=config,\n                   anonymous=anonymous,\n                   group=CFG.exp_name\n                        )\n        return run\n    \ndef log_wandb(fold):\n    \"log best result and grad-cam for error analysis\"\n    \n    valid_df = df.loc[df.fold==fold].copy()\n    if CFG.debug:\n        valid_df = valid_df.iloc[:1000]\n    valid_df['pred'] = oof_pred[fold].reshape(-1)\n    valid_df['diff'] =  abs(valid_df.Pawpularity - valid_df.pred)\n    valid_df    = valid_df[valid_df.fold == fold].reset_index(drop=True)\n    vali_df     = valid_df.sort_values(by='diff', ascending=False)\n    \n    noimg_cols  = ['Id', 'fold', 'Subject Focus','Eyes','Face','Near','Action','Accessory','Group',\n                    'Collage','Human','Occlusion','Info','Blur',\n                   'Pawpularity', 'pred', 'diff']\n    # select top and worst 10 cases\n    gradcam_df  = pd.concat((valid_df.head(10), valid_df.tail(10)), axis=0)\n    gradcam_ds  = build_dataset(gradcam_df.image_path, labels=None, cache=False, batch_size=1,\n                   repeat=False, shuffle=False, augment=False)\n    data = []\n    for idx, img in enumerate(gradcam_ds):\n#         gradcam = get_gradcam(img, model)\n        gradcam = array_to_img(attention_map(model=model, image=img.numpy()[0]))\n        row = gradcam_df[noimg_cols].iloc[idx].tolist()\n        data+=[[*row, wandb.Image(img.numpy()[0]), wandb.Image(gradcam)]]\n    wandb_table = wandb.Table(data=data, columns=[*noimg_cols,'image', 'gradcam'])\n    wandb.log({'best_rmse':oof_val[-1], \n               'best_rmse_tta':rmse,\n               'best_epoch':np.argmin(history.history['val_rmse']),\n               'viz_table':wandb_table})","7b5cf055":"oof_pred = []; oof_tar = []; oof_val = []; oof_ids = []; oof_folds = []\npreds = np.zeros((test_df.shape[0],1))\n\nfor fold in np.arange(CFG.folds):\n    if fold not in CFG.selected_folds:\n        continue\n    if CFG.wandb:\n        run = wandb_init(fold)\n        WandbCallback = wandb.keras.WandbCallback(save_model=False)\n    if CFG.device=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n            \n    # TRAIN AND VALID DATAFRAME\n    train_df = df.query(\"fold!=@fold\")\n    valid_df = df.query(\"fold==@fold\")\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    train_paths = train_df.image_path.values; train_labels = train_df[CFG.target_col].values.astype(np.float32)\n    valid_paths = valid_df.image_path.values; valid_labels = valid_df[CFG.target_col].values.astype(np.float32)\n    test_paths  = test_df.image_path.values\n    \n    # SHUFFLE IMAGE AND LABELS\n    index = np.arange(len(train_paths))\n    np.random.shuffle(index)\n    train_paths  = train_paths[index]\n    train_labels = train_labels[index]\n    \n    if CFG.debug:\n        train_paths = train_paths[:2000]; train_labels = train_labels[:2000]\n        valid_paths = valid_paths[:1000]; valid_labels = valid_labels[:1000]\n    \n    print('#'*25); print('#### FOLD',fold)\n    print('#### IMAGE_SIZE: (%i, %i) | MODEL_NAME: %s | BATCH_SIZE: %i'%\n          (CFG.img_size[0],CFG.img_size[1],CFG.model_name,CFG.batch_size*REPLICAS))\n    train_images = len(train_paths)\n    val_images   = len(valid_paths)\n    if CFG.wandb:\n        wandb.log({'num_train':train_images,\n                   'num_valid':val_images})\n    print('#### NUM_TRAIN %i | NUM_VALID: %i'%(train_images, val_images))\n    \n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():\n        model = build_model(CFG.model_name, DIM=CFG.img_size[0], compile_model=True)\n\n    # DATASET\n    train_ds = build_dataset(train_paths, train_labels, cache=True, batch_size=CFG.batch_size*REPLICAS,\n                   repeat=True, shuffle=True, augment=CFG.augment)\n    val_ds   = build_dataset(valid_paths, valid_labels, cache=True, batch_size=CFG.batch_size*REPLICAS,\n                   repeat=False, shuffle=False, augment=False)\n    \n    print('#'*25)   \n    # SAVE BEST MODEL EACH FOLD\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_rmse', verbose=CFG.verbose, save_best_only=True,\n        save_weights_only=False, mode='min', save_freq='epoch')\n    callbacks = [sv,get_lr_callback(CFG.batch_size)]\n    if CFG.wandb:\n        callbacks.append(WandbCallback)\n    # TRAIN\n    print('Training...')\n    history = model.fit(\n        train_ds, \n        epochs=CFG.epochs if not CFG.debug else 2, \n        callbacks = callbacks, \n        steps_per_epoch=len(train_paths)\/CFG.batch_size\/\/REPLICAS,\n        validation_data=val_ds, \n        #class_weight = {0:1,1:2},\n        verbose=CFG.verbose\n    )\n    \n    # Loading best model for inference\n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)  \n    \n    # PREDICT OOF USING TTA\n    print('Predicting OOF with TTA...')\n    ds_valid = build_dataset(valid_paths, labels=None, cache=False, batch_size=CFG.batch_size*REPLICAS*2,\n                   repeat=True, shuffle=False, augment=True if CFG.tta>1 else False)\n    ct_valid = len(valid_paths); STEPS = CFG.tta * ct_valid\/CFG.batch_size\/2\/REPLICAS\n    pred = model.predict(ds_valid,steps=STEPS,verbose=CFG.verbose)[:CFG.tta*ct_valid,] \n    oof_pred.append(np.mean(pred.reshape((ct_valid,-1,CFG.tta),order='F'),axis=-1)*100.0 )                 \n    \n    # GET OOF TARGETS AND idS\n    oof_tar.append(valid_df[CFG.target_col].values[:(1000 if CFG.debug else len(valid_df))])\n    oof_folds.append(np.ones_like(oof_tar[-1],dtype='int8')*fold )\n    oof_ids.append(valid_df.Id.values)\n    \n    # PREDICT TEST USING TTA\n    print('Predicting Test with TTA...')\n    ds_test = build_dataset(test_paths, labels=None, cache=True, \n                    batch_size=(CFG.batch_size*2 if len(test_df)>8 else 1)*REPLICAS,\n                   repeat=True, shuffle=False, augment=True if CFG.tta>1 else False)\n    ct_test = len(test_paths); STEPS = 1 if len(test_df)<=8 else (CFG.tta * ct_test\/CFG.batch_size\/2\/REPLICAS)\n    pred = model.predict(ds_test,steps=STEPS,verbose=CFG.verbose)[:CFG.tta*ct_test,] \n    preds[:ct_test, :] += np.mean(pred.reshape((ct_test,-1,CFG.tta),order='F'),axis=-1) \/ CFG.folds*100 # not meaningful for DIBUG = True\n    \n    # REPORT RESULTS\n    y_true =oof_tar[-1]; y_pred = oof_pred[-1]\n    rmse   =RMSE(y_true.astype(np.float32),y_pred, denormalize=False).numpy()\n    oof_val.append(np.min( history.history['val_rmse'] ))\n    print('#### FOLD %i OOF RMSE without TTA = %.3f, with TTA = %.3f'%(fold,oof_val[-1],rmse))\n    \n    if CFG.wandb:\n        log_wandb(fold) # log result to wandb\n        wandb.run.finish() # finish the run\n        display(ipd.IFrame(run.url, width=1080, height=720)) # show wandb dashboard","ead572a2":"# COMPUTE OVERALL OOF RMSE\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\nids = np.concatenate(oof_ids); folds = np.concatenate(oof_folds)\nrmse = RMSE(true.astype(np.float32),oof, denormalize=False)\nprint('Overall OOF RMSE with TTA = %.3f'%rmse)","d6dfca0b":"# SAVE OOF TO DISK\ncolumns = ['Id', 'fold', 'true', 'pred']\ndf_oof = pd.DataFrame(np.concatenate([ids[:,None], folds[:, 0:1], true, oof], axis=1), columns=columns)\ndf_oof.to_csv('oof.csv',index=False)\ndf_oof.head()","9a795396":"import seaborn as sns\nsns.set(style='dark')\n\nplt.figure(figsize=(10*2,6))\n\nplt.subplot(1, 2, 1)\nsns.kdeplot(x=train_df[CFG.target_col[0]], color='b',shade=True);\nsns.kdeplot(x=df_oof.pred.values, color='r',shade=True);\nplt.grid('ON')\nplt.xlabel(CFG.target_col[0]);plt.ylabel('freq');plt.title('KDE')\nplt.legend(['train', 'oof'])\n\nplt.subplot(1, 2, 2)\nsns.histplot(x=train_df[CFG.target_col[0]], color='b');\nsns.histplot(x=df_oof.pred.values, color='r');\nplt.grid('ON')\nplt.xlabel(CFG.target_col[0]);plt.ylabel('freq');plt.title('Histogram')\nplt.legend(['train', 'oof'])\n\nplt.tight_layout()\nplt.show()","63ee79d3":"pred_df = pd.DataFrame({'Id':test_df.Id,\n                        'Pawpularity':preds.reshape(-1)})\nsub_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ndel sub_df['Pawpularity']\nsub_df = sub_df.merge(pred_df, on='Id', how='left')\nsub_df.to_csv('submission.csv',index=False)\nsub_df.head(2)","58946d5b":"!rm -r \/kaggle\/working\/wandb","2c95746c":"## Visualization\n* Check if augmentation is working properly or not.","ec8085df":"# **Wandb** Logger\nLog:\n* Best Score\n* Attention MAP","6e3c179a":"# Configuration","eefe133a":"# Idea:\n* Use **Classification** instead of **Regression**.\n    * Normalize `Pawpularity` from `[0,100]` range to `[0,1]`.\n    * Use `BinaryCrossentropy` loss intead of `RootMeanSquaredError`\n    * Calculate competition metric after **denormalizing** the prediction\n* Use **Transformer** instead of **CNN**\n* Use only **Image** Feature.\n* **Wandb** is integrated hence we can use this notebook to track which experiemnt is peforming better and also do error analysis using **Attenion-MAP** at the end.\n","00eca75c":"# GCS Path for TPU\n* TPU requires **GCS** path. Luckily Kaggle Provides that for us :)","520c85dc":"# TPU Configs","d3e7d831":"# Version Check","ede29d8a":"# Data Split\n* Data is splited using **Pawpularity** distrubtion.\n","2309d773":"# Remove Files","a278a5b1":"# [PetFinder.my - Pawpularity Contest](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score)\n> Predict the popularity of shelter pet photos\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25383\/logos\/header.png)","82b2d3cf":"## Train","8e3b28b6":"# Light EDA\nIf you're too lazy to do **EDA** by yourself. Then definitely this library is for you. You can use **Pandas-Profiling** to do bunch of **EDA** with vey few lines of code. \ud83d\ude09","76c383ce":"# Submission\nThis notebook can't be used for submission as this noteboook uses **TPU** which requires `internet access`.","81c7c09a":"# Calculate OOF Score","acdcd6b0":"# Metric\nMetric for this competition is **RMSE: Root Mean Squared Error**\n$$\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\nwhere $\\hat{y}_i$ is the **predicted** value and $y_i$ is the **original** value for each instance $i$.\n* We have to denormalize our prediction to convert `[0-1]` range to `[0-100]` range\n* Then we have to use **RMSE**","8587e754":"# Install Libraries","711ef791":"# Reference:\n1. [ViT - Pytorch xla (TPU) for leaf disease](https:\/\/www.kaggle.com\/mobassir\/vit-pytorch-xla-tpu-for-leaf-disease)\n2. [baseline solution [LB17.91201]](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/275094)\n3. [RANZCR: EfficientNet TPU Training](https:\/\/www.kaggle.com\/xhlulu\/ranzcr-efficientnet-tpu-training)\n4. [Triple Stratified KFold with TFRecords](https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords)","41f4c1f0":"# Content:\n* Install Libraries.\n* Import Libraries.\n* Libraries Version Check\n* Wandb\n* Configuration.\n* Set Seed for Reproducibility.\n* TPU Configs.\n* GCS Path for TPU.\n* Meta Data.\n* Train-Test Distrubution\n* EDA\n    * Train.\n    * Test.\n* Data Split.\n* Data Augmentation.\n* Data Pipeline.\n* Visualization.\n* Loss Function.\n* Build Model.\n* Learning-Rate Scheduler.\n* Grad-CAM Helper\n* Wandb Logger\n* Train Model\n* Calculate OOF Scorej\n* Pawpularity Distrubtion of Train & OOF\n\n\n\n","fdf839ee":"# Learning-Rate Scheduler","093a4801":"# Wandb","fca355c7":"# Train Model\n* Cross-Validation: 5 fold\n* **WandB** dashboard is shown end of the each fold. So we don't need to plot anything. We can select best model from here.","738e609b":"# Set Seed for Reproducibility","c04a7fdf":"# Build Model: \n## **Vision Transformer (ViT)**\n\n[**How it works?**](https:\/\/analyticsindiamag.com\/hands-on-vision-transformers-with-pytorch\/)\n\nPaper: [\u201cAn Image is Worth 16\u00d716 Words: Transformers for Image Recognition at Scale\u201d](https:\/\/arxiv.org\/abs\/2010.11929)\n\n* ViT breaks an input image of 16\u00d716 to a  sequence of patches, just like a series of word embeddings generated by an NLP Transformers. \n* Each patch gets flattened into a single vector in a series of interconnected channels of all pixels in a patch, then projects it to desired input dimension.\n* Because transformers operate in self-attention mode, and they do not necessarily depend on the structure of the input elements, which in turns helps the architecture to learn and relate sparsely-distributed information more efficiently.\n* The relationship between the patches in an image is not known and thus allows it to learn more relevant features from the training data and encode in positional embedding in ViT. \n\n<img src=\"https:\/\/lh5.googleusercontent.com\/xk2mQd9H3w4uS482ursxhbDZhr7UXxJ9RMZ7VVjErBMuhbsB1QfSary9pOWU4P5EeZHmB05R8KalB5GXx__eCiN2AQ5qRhXY4vHwYe2zoFqIO0XkpHHXIE8VP99lpcgW5HtjPKKx\" width=\"800\">\n","bc3223f1":"## Train-Test Ditribution","2ce69dc2":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=\"400\" alt=\"Weights & Biases\" \/>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Some of the cool features of **W&B**:<\/span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Track, compare, and visualize ML experiments<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Get live metrics, terminal logs, and system stats streamed to the centralized dashboard.<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Explain how your model works, show graphs of how model versions improved, discuss bugs, and demonstrate progress towards milestones.<br><\/span>","9752ad5c":"# Attention Map\n* Attention map generated form **ViT** can  help you to interpret the result.\n\n<img src=\"https:\/\/user-images.githubusercontent.com\/6073256\/101206904-2a338f00-36b3-11eb-8920-f617abab1604.png\" width=\"800\">","aa7b39e4":"# Import Libraries","4da816bd":"# Pawpularity Distribution of OOF & Train \nCheck **Pawpularity** distribution of `train` and `oof`. ","a875ca4c":"# Overview:\n\n## Augmentations:\n* Random - Horizontal Flip\n* Random - Brightness, Contrast, Hue, Saturation\n* Coarse Dropout\n\n<img src=\"https:\/\/i.ibb.co\/XC4dBJs\/results-39-0.png\" alt=\"results-39-0\" border=\"0\">\n\n## WandB Integration:\n* You can track your training using **wandb**\n* It's very easy to compare model's performance using **wandb**.\n\n<img src=\"https:\/\/i.ibb.co\/KKxF8Fj\/wandb-result.png\" alt=\"wandb-result\" border=\"0\">\n\n## Attention-MAP:\n* You can use **Attention-MAP** to interpret the results\n\n<img src=\"https:\/\/i.ibb.co\/hCPQWsD\/cvt.gif\" alt=\"cvt\" border=\"0\">\n\n## Train Vs OOF Distribution:\n\n<img src=\"https:\/\/i.ibb.co\/Yy4NXvW\/results-58-0.png\" alt=\"results-58-0\" border=\"0\">","29032b15":"# Meta Data","2c792aa1":"# Data Augmentation\nUsed simple augmentations, some of them may hurt the model.\n* RandomFlip (Left-Right)\n* No Rotation\n* RandomBrightness\n* RndomContrast\n* Shear\n* Zoom\n* Coarsee Dropout\/Cutout","389767e2":"# Notebooks:\n* Only Image:\n    * CNN:\n        * train: [[TF] PetFinder: Image [TPU][Train] \ud83d\udc36](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-image-tpu-train)\n        * infer: [[TF] PetFinder: Image [TPU][Infer] \ud83d\udc36](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-image-tpu-infer)\n    * Transformer + Classification:\n        * train: [[TF] PetFinder: ViT+Cls [TPU][Train] \ud83d\ude3a](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-vit-cls-tpu-train\/edit)\n* Image+Tabular:\n    * CNN:\n        * train: [[TF] PetFinder: Image+Tabular [TPU][Train] \ud83d\udc36](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-image-tabular-tpu-train)\n","73727f28":"## Test","1214dae2":"## Model Check","904376a6":"# Loss Function\nLoss Function for this notebook is **BCE: Binary Crossentropy** as the task is converted form **regression** to **classification**\n<!-- $$\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$ -->\n$$\\textrm{BCE}  = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\cdot log(\\hat{y}_i) + (1 - y_i) \\cdot log(1 - \\hat{y}_i)$$\n\nwhere $\\hat{y}_i$ is the **predicted** value and $y_i$ is the **original** value for each instance $i$.\n\n* `tf.keras.losses.BinaryCrossentropy`","849cbf4d":"## Data Pipeline\n* Reads the raw file and then decodes it to tf.Tensor\n* Resizes the image in desired size\n* Chages the datatype to **float32**\n* Caches the Data for boosting up the speed.\n* Uses Augmentations to reduce overfitting and make model more robust.\n* Finally, splits the data into batches.\n"}}