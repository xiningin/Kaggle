{"cell_type":{"fdf66ad6":"code","ca27fcfa":"code","9a2c2ce9":"code","57f2a2af":"code","ee1bc6e1":"code","ff0d28da":"code","a02ba86e":"code","bde92913":"code","9fa67757":"code","7d10aa71":"code","efb59e73":"code","ce3ca7fc":"code","bd4ded60":"code","222f1df0":"code","3b009bae":"code","ad796aff":"code","9f0c6192":"code","3c260ceb":"code","fb6edea7":"code","4bbd4caf":"code","f5bc44cc":"markdown","869b6ff4":"markdown","8acfb0ae":"markdown"},"source":{"fdf66ad6":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectFromModel\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np \nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca27fcfa":"# Pull data into dataframes\nsubmission = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")","9a2c2ce9":"# First look at training data\ntrain.head()","57f2a2af":"# First look at testing data\ntest.head()","ee1bc6e1":"# Save 'id' for submission and drop it from test data\ntest_id = test[\"id\"]\ntest.drop(['id'],inplace=True, axis = 1)\ntest.head()","ff0d28da":"# Prepare training data prior to encoding\ny = train['target']\nX = train.drop(['target', 'id'], axis = 1)\nX.head()","a02ba86e":"# What are all of the catagorical columns?\ncat_columns = [cols for cols in train.columns if train[cols].dtype == 'object']\nprint(cat_columns)","bde92913":"# Encode catagorical columns\nencoder = LabelEncoder()\nfor col in cat_columns:\n    X[col] = pd.DataFrame(encoder.fit_transform(X[col]))\n    test[col] = pd.DataFrame(encoder.fit_transform(test[col]))   \n","9fa67757":"# Check data to ensure correct encoding\nX.head()","7d10aa71":"# Create train and test datasets from training data for model validation\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,random_state=0)","efb59e73":"# Create, train, and test XGB model\nmodel = XGBClassifier(n_estimators=500,scale_pos_weight=2,random_state=1,colsample_bytree=0.5)\nmodel.fit(Xtrain,ytrain)\npred = model.predict_proba(Xtest)[:, 1]\nscore = roc_auc_score(ytest, pred)\n\nprint(\"score: \", score)","ce3ca7fc":"# Reset test and other variables to use for logistic regression\ntest = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\ny2 = train['target']\nX2 = train.drop(['target', 'id'], axis = 1)\ntest.drop(['id'],inplace=True, axis = 1)\ntest.head()","bd4ded60":"# Create dummies\ntraintest = pd.concat([X2, test])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, sparse=True)","222f1df0":"# What do dummies look like?\nX2 = dummies.iloc[:X2.shape[0], :]\ntest = dummies.iloc[X2.shape[0]:, :]\n\nprint(X2.shape)\nprint(test.shape)","3b009bae":"# Speed up model\nX2 = X2.sparse.to_coo().tocsr()\ntest = test.sparse.to_coo().tocsr()","ad796aff":"# Create training and test data sets\nXtrain,Xtest,ytrain,ytest = train_test_split(X2,y,random_state=0)","9f0c6192":"# Create, train, and test Logistic Regression model\nmodel = LogisticRegression(solver='lbfgs', C = 0.1, max_iter=1000)\nmodel.fit(Xtrain,ytrain)\npred = model.predict_proba(Xtest)[:, 1]\nscore = roc_auc_score(ytest, pred)\n\nprint(\"score: \", score)","3c260ceb":"submission[\"id\"] = test_id\nsubmission[\"target\"] = model.predict_proba(test)","fb6edea7":"submission.head()","4bbd4caf":"submission.to_csv(\"submission.csv\", index=False)","f5bc44cc":"I wanted to compare model performance between logistic regression and Xgboost. The logistic regression model uses dummy variables for all catagorical data, whereas the XGB model uses a simple encoder for all catagorical columns. \n\nOverall, I saw a better performance with the Logistic Regression model compared to the XGB","869b6ff4":"# Can we improve our performance with creating dummy variables and using logistic regression?","8acfb0ae":"**We see a better performance with the logistic regression model + dummy variables, compared to XGB and encoding**"}}