{"cell_type":{"cd8b0e69":"code","1be5de55":"code","d524c857":"code","81195966":"code","3a35a865":"code","79183de0":"code","4dedd14a":"code","9521a566":"code","7bbcca65":"code","d8c210a0":"code","8dbe2d0e":"code","f59480e0":"code","c90e7ac8":"code","993c6217":"code","2aeb78f3":"code","3bd7ace3":"code","031d4025":"code","24ab2431":"code","9bd19434":"code","dfeca8a5":"code","9270a20e":"code","7cdfd48f":"code","bfcc3cae":"code","a417bf55":"code","150f6916":"code","aa6712eb":"code","adb23111":"code","18d34fcc":"code","ed1e4fe5":"code","c0450a5d":"code","2014b7d8":"code","a3574523":"code","4170cbe6":"code","8a88c6ca":"markdown","04e433cc":"markdown","3231e3fa":"markdown","8a7ffa62":"markdown","3c7e6f97":"markdown","bfc9f336":"markdown","60297e87":"markdown","fc60d50b":"markdown","55e33b9e":"markdown","df410805":"markdown","0ba7b927":"markdown","faa11510":"markdown","6962c049":"markdown","5d72f350":"markdown","52e91a54":"markdown","1e9e71a7":"markdown","408b1dfb":"markdown","82a951fb":"markdown","a7c489d8":"markdown","36fe0bed":"markdown","8d0c28c3":"markdown","8bae11d2":"markdown","3de97444":"markdown","e19ee9f5":"markdown"},"source":{"cd8b0e69":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","1be5de55":"with open(\"..\/input\/housing.csv\",\"r\") as f:\n    data=f.readlines()\n\nhousing_data=[]\nfor line in data:\n    samples=[np.float32(x) for x in line.split()]\n    housing_data.append(samples)\n\nhousing_data=np.asarray(housing_data)\nboston=pd.DataFrame(housing_data,columns=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LTSTAT\",\"MEDV\"])\nprint(boston.head())","d524c857":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(boston,figsize=(16,16))\nplt.show()","81195966":"import seaborn as sns\ncor=boston.corr()\nfig=plt.figure(figsize=(12,12))\nfig=sns.heatmap(cor,annot=True)\nplt.show()\n","3a35a865":"X1=boston.loc[:,[\"RM\",\"PTRATIO\",\"LTSTAT\",\"INDUS\",\"NOX\",\"TAX\"]]\n\ny=boston[\"MEDV\"]","79183de0":"print(\"X\\n\",X1.head(),\"\\n\\nY\\n\",y.head())","4dedd14a":"print(X1.describe())","9521a566":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV\n","7bbcca65":"seed=35\nkfold=KFold(n_splits=10,random_state=seed)\nscoring=\"r2\"","d8c210a0":"x_train,x_test,y_train,y_test=train_test_split(X1,y,test_size=.3,random_state=seed)","8dbe2d0e":"models=[]\nmodels.append([\"LR\",LinearRegression()])\nmodels.append([\"ENet\",ElasticNet()])\n\nmodels.append([\"SVR\",SVR(gamma=\"scale\")])\nmodels.append([\"KNR\",KNeighborsRegressor()])\nmodels.append([\"GPR\",GaussianProcessRegressor(normalize_y=True)])\nmodels.append([\"CART\",DecisionTreeRegressor()])","f59480e0":"param_grids=[]\nLR_param_grid={}\nparam_grids.append(LR_param_grid)\nENet_param_grid={}\nENet_param_grid[\"alpha\"]=[.001,.01,.1,.3,.5]\nENet_param_grid[\"l1_ratio\"]=[0,.2,.4,.5,.7,1]\nparam_grids.append(ENet_param_grid)\nsvr_param_grid={}\nsvr_param_grid[\"kernel\"]=[\"poly\",\"linear\",\"rbf\"]\nsvr_param_grid[\"degree\"]=[1,2,3,4]\nsvr_param_grid[\"C\"]=[.001,0.1,.3,.5,1,2,3]\nparam_grids.append(svr_param_grid)\nknr_param_grid={}\nknr_param_grid[\"n_neighbors\"]=[3,5,7,11]\nknr_param_grid[\"weights\"]=[\"uniform\",\"distance\"]\nparam_grids.append(knr_param_grid)\ngpr_param_grid={}\nparam_grids.append(gpr_param_grid)\ncart_param_grid={}\ncart_param_grid[\"max_depth\"]=[1,2,3,4]\nparam_grids.append(cart_param_grid)","c90e7ac8":"results=[['LR', {}, 0.6743397010174477], ['ENet', {'alpha': 0.1, 'l1_ratio': 0.7}, 0.6758633980611956], ['SVR', {'C': 0.3, 'degree': 1, 'kernel': 'linear'}, 0.6646899021487152], ['KNR', {'n_neighbors': 3, 'weights': 'distance'}, 0.7612886361653691], ['GPR', {}, -13.56991654534516], ['CART', {'max_depth': 3}, 0.7346860566317643]]","993c6217":"print(results)","2aeb78f3":"test_scores=[]\nfor model,result in zip(models,results):\n    clf=model[1]\n    clf.set_params(**result[1])\n    clf.fit(x_train,y_train)\n    score=clf.score(x_test,y_test)\n    test_scores.append([model[0],result[1],score])\n\n","3bd7ace3":"for model,param,score in test_scores:\n    print(\"%s : %0.4f\"%(model,score))","031d4025":"from sklearn.ensemble import BaggingRegressor\nbgr_param_dict={\"n_estimators\":list(np.arange(1,100,5)),\"max_samples\":list(np.linspace(.1,1,10)),\"random_state\":[seed]}\nknn=KNeighborsRegressor(n_neighbors=3,weights=\"distance\")\nbg=BaggingRegressor(base_estimator=knn)","24ab2431":"bagging_best_params = {'max_samples': 1.0, 'n_estimators': 76, 'random_state': 35}\nbagging_best_score=0.764816","9bd19434":"print(\"best training score : %0.6f\\nbest params : %r\"%(bagging_best_score,bagging_best_params))\n","dfeca8a5":"bg.set_params(**bagging_best_params)\nbg.fit(x_train,y_train)\nprint(\"Bagging Test score : %0.6f\"%bg.score(x_test,y_test))","9270a20e":"knn.fit(x_train,y_train)\nprint(knn.score(x_test,y_test))","7cdfd48f":"from sklearn.ensemble import RandomForestRegressor\n","bfcc3cae":"params_dict={}\nparams_dict[\"n_estimators\"]=list(np.arange(1,100,5))\nparams_dict[\"max_depth\"]=[None,2,3,4,5,6,7,8,9,10]\nparams_dict[\"max_features\"]=[.2,.6,.8,1.0]\nparams_dict[\"bootstrap\"]=[True]\nparams_dict[\"random_state\"]=[seed]\n","a417bf55":"#print(gcv.best_params_)\nrfc_best_params={'bootstrap': True,\n 'max_depth': 9,\n 'max_features': 0.2,\n 'n_estimators': 96,\n 'random_state': 35}\nprint(rfc_best_params)","150f6916":"#print(gcv.best_score_)\nrfc_best_score=0.8607639890718424\nprint(rfc_best_score)","aa6712eb":"rfc=RandomForestRegressor()\nrfc.set_params(**rfc_best_params)\nrfc.fit(x_train,y_train)","adb23111":"from sklearn.ensemble import ExtraTreesRegressor","18d34fcc":"#print(gcv.best_params_)\net_best_params={'bootstrap': True,\n 'max_depth': None,\n 'max_features': 0.6,\n 'n_estimators': 71,\n 'random_state': 35}\nprint(et_best_params)","ed1e4fe5":"#print(gcv.best_score_)\net_best_score=0.870845214813861\nprint(et_best_score)","c0450a5d":"et=ExtraTreesRegressor()\net.set_params(**et_best_params)\net.fit(x_train,y_train)","2014b7d8":"print(et.score(x_test,y_test))","a3574523":"evaluated_models=['LR', 'ENet', 'SVR', 'KNR', 'GPR', 'CART', 'BAGGING', 'RForest','ETrees']\nevaluated_test_scores= [0.6077, 0.6012,0.587,0.6843,-0.0222,0.6223,0.6673,0.8172,0.8490]   ","4170cbe6":"plt.figure(figsize=(8,6))\nplt.plot(evaluated_models,evaluated_test_scores,marker=\"o\",linestyle=\"--\",color=\"r\")\nplt.ylim(-0.5,1)\nplt.title(\"Model Evaluation Plot\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"R-Score\")\nplt.show()","8a88c6ca":"### Running the below cell takes some time and kaggle kernel, while commiting usually stops. \nSo i have stored the results i got seperately and have used in the next cell\n\n### for all the cells running GridSearch, I have done the same. Stored the results i got seperately and commented out the cells running GridSearch\n\n### you can try running these seperately, if you want","04e433cc":"### We haven't done data cleaning for this dataset. \n### Data cleaning and scaling will improve the model performance.\n### I will cover the impacts of that in another kernel","3231e3fa":"'''\ngcv=GridSearchCV(estimator=RandomForestRegressor(bootstrap=True,random_state=seed),param_grid=params_dict,cv=kfold,scoring=scoring,iid=False)\ngcv.fit(x_train,y_train)\n'''","8a7ffa62":"## Ensemble Methods\n\nWe will now use the family of ensemble methods\n\nThere exists mainly two classes of ensemble methods\n#### Averaging  and Boosting \n\n- I am only covering Averaging class of ensemble methods for now. Boosting methods may be added later.\n","3c7e6f97":"In averaging methods, we will be trying Bagging Regressor with KNR, RandomForest and ExtraTrees","bfc9f336":"\nWe can see how these estimators performs on the test set.\n\n","60297e87":"'''\ngcv=GridSearchCV(estimator=ExtraTreesRegressor(),param_grid=params_dict,cv=kfold,scoring=scoring,iid=False)\ngcv.fit(x_train,y_train)\n'''","fc60d50b":"### When we take a threshold of |0.4| we get 6 important features\n\n- seperating dataset to features and target sets","55e33b9e":"Now we will use gridsearch to get the best parameters","df410805":"'''\ngcv=GridSearchCV(estimator=bg,param_grid=bgr_param_dict,iid=False,cv=kfold,scoring=scoring)\n\ngcv.fit(x_train,y_train)\n'''","0ba7b927":"#### This is the best score we got from the models we used.","faa11510":"### KNR Ensemble vs single estimator","6962c049":"We will use GridSearchCV to find the best parametets","5d72f350":"#### Here BaggingRegressor performed worse than base estimator.\n#### We can explore the reasons later.\n#### Feel free to comment the answers","52e91a54":"now we take some regressors and fit and test the  model.\n\nwe are going to use LR,lasso,elasticnet,svr,knr,gaussian,decisiontree\n\nWe will cover ensemble methods seperately.\n\nwe are not removing outliers for now. we will check the efficiency by removing outliers later.","1e9e71a7":"Data description\nThe Boston data frame has 506 rows and 14 columns.\n\nThis data frame contains the following columns:\n\n#### crim\nper capita crime rate by town.\n\n#### zn\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\n#### indus\nproportion of non-retail business acres per town.\n\n#### chas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n#### nox\nnitrogen oxides concentration (parts per 10 million).\n\n#### rm\naverage number of rooms per dwelling.\n\n#### age\nproportion of owner-occupied units built prior to 1940.\n\n#### dis\nweighted mean of distances to five Boston employment centres.\n\n#### rad\nindex of accessibility to radial highways.\n\n#### tax\nfull-value property-tax rate per $10,000.\n\n#### ptratio\npupil-teacher ratio by town.\n\n#### black\n1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n\n#### lstat\nlower status of the population (percent).\n\n#### medv\nmedian value of owner-occupied homes in $1000s.","408b1dfb":"splitting the dataset to train and validation sets","82a951fb":"### Scatter Plot ","a7c489d8":"### Random Forests","36fe0bed":"setting the seed so that we will get same splits for every case.\n\nPerfomance of each models can be easily compared then.","8d0c28c3":"### Correlation Matrix","8bae11d2":"### KNearestRegressor has performed the best on test set.\n#### Now we can explore Ensemble Methods","3de97444":"### Extra Trees","e19ee9f5":"'''\nresults=[]\nfor model,params in zip(models,param_grids):\n    gcv=GridSearchCV(estimator=model[1],param_grid=params,cv=kfold,scoring=scoring,iid=False)\n    gcv.fit(x_train,y_train)\n    results.append([model[0],gcv.best_params_,gcv.best_score_])\n''' "}}