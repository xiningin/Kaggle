{"cell_type":{"ad85641f":"code","9727c59f":"code","060f65d4":"code","4d59378c":"code","e16cd6b5":"code","5dcae155":"code","c2727618":"code","d032330f":"code","71692db1":"code","e55495e1":"code","bae17f66":"code","6c3d9e83":"code","bc9f7c3a":"code","f01c23e6":"code","b1aff4d0":"code","372bf5c1":"code","8ab5218b":"code","748e8d50":"code","f92e2bee":"code","584064ec":"code","7166977d":"code","4ee735fb":"code","beeea5e9":"code","c3e6d5ab":"code","21b6489c":"code","180350fd":"code","c2057727":"code","60a7f80c":"code","603e5df8":"code","122db118":"code","4cf13ea1":"code","4b1b4407":"code","3974f482":"code","1141dff9":"code","abe31785":"code","8e580e3c":"code","3a3064f9":"code","ced86718":"code","1fa2bf32":"code","76b57253":"code","3d5e3b47":"code","fab419c6":"code","5939ae15":"code","69967158":"code","5da31a68":"code","31750822":"code","1c28c9ac":"code","3be4ef67":"code","1acf608e":"code","44cbd1cc":"code","6c74ec1e":"code","650b7067":"code","795d4575":"code","aaf6c0e6":"code","966d6074":"code","7ee2c9fa":"code","1cb31840":"code","33b99ea6":"code","f95a1e32":"code","49f55bd4":"code","2747cce5":"code","f9d1ef6b":"code","0b1516c8":"code","2c3d7e47":"code","47506156":"code","5fe24c16":"code","faa22c53":"code","5a0e6e61":"code","4833be31":"code","99e580d7":"code","c531dd32":"code","1586b2c3":"code","d087c467":"code","46aa77c7":"code","bee03ac8":"code","5fcbb8c4":"code","1182e0ab":"code","45f3aade":"code","a4a3ac6b":"code","048fae2e":"code","564b9365":"code","b33e93e5":"code","4c386abd":"code","a567d60c":"code","1bb89df9":"code","e5ceb16d":"code","9e4bcd36":"code","a4ee737c":"code","de468f34":"code","ad26f31a":"code","f8094005":"code","e3c6b4a8":"code","7ccaf7a1":"code","85d1d678":"code","8488d06f":"code","8daf08d4":"code","c1e6ecb6":"code","f0f68d9d":"code","5ecfafbe":"code","f99bab21":"code","b01483cc":"code","551a633a":"code","3c2b6800":"code","339b350a":"code","dc7cde97":"code","c9db6a80":"code","59e8e9f0":"code","98a9c3fe":"code","0b244df4":"code","4bee2362":"code","5bb6c942":"code","05bf79c5":"code","1e24e5b9":"code","e0bc8f12":"code","04f826c0":"code","045eb6b6":"code","584183a1":"code","9daa60a7":"code","fa318f80":"code","edfd3a67":"code","4deed5b2":"code","9b1fcf7f":"code","80908bab":"code","367460d3":"code","d95ddbd3":"code","1be1d779":"markdown","9e9940bc":"markdown","fbc13443":"markdown","28674877":"markdown","f7a80d33":"markdown","6a569e4e":"markdown","23c901cc":"markdown","b60fc14b":"markdown","c3d7d50f":"markdown","625d7dd6":"markdown","b35c5dc6":"markdown","d5a124ee":"markdown","1db91fd8":"markdown","a819734a":"markdown","ac97846d":"markdown","5db6a1cc":"markdown","548c4c35":"markdown","41c366be":"markdown","4cc08858":"markdown","22312a1a":"markdown","a6773809":"markdown","106184d3":"markdown","73e565dc":"markdown","d3f83a02":"markdown","dc95d0d5":"markdown","207b5080":"markdown","ff65885b":"markdown","f1b40bab":"markdown","39562f6b":"markdown","12f7ffb8":"markdown","1c1708be":"markdown","8abe8412":"markdown","347b4233":"markdown","849de3b5":"markdown","4eb09501":"markdown","27c2c168":"markdown","73936672":"markdown","07a34779":"markdown","8724c793":"markdown","b21f6db5":"markdown","96e1a188":"markdown","cbf8542f":"markdown","e14bcc8e":"markdown","02026bd8":"markdown","4e13bc7b":"markdown","3172e783":"markdown","50293237":"markdown","92a48a1e":"markdown","a856ba77":"markdown","bc73f044":"markdown","69550dec":"markdown","1c656056":"markdown","23d758e7":"markdown","0f323bd0":"markdown","d093efb1":"markdown","fca6ee33":"markdown","208540c1":"markdown","73fa7f94":"markdown","239c4d45":"markdown","86f0e5e0":"markdown","83d54b5b":"markdown","2d57d4e3":"markdown","7209b5f0":"markdown","d868199b":"markdown","59120f6d":"markdown","90c90caf":"markdown","0da605b5":"markdown","51647adb":"markdown","0755c217":"markdown","dba49fb8":"markdown","6466f06e":"markdown","9651dc19":"markdown","5619aa71":"markdown","8c2cc721":"markdown","392714fb":"markdown","ce15d72f":"markdown","dea9ac23":"markdown","1f694b38":"markdown","3a2e12b9":"markdown","b4e49cff":"markdown","a94e9ae5":"markdown"},"source":{"ad85641f":"# Importing some important libraries.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport json\nimport datetime as dt\nimport warnings\nimport re\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom IPython.display import display\nfrom warnings import filterwarnings\nwarnings.filterwarnings('ignore',category=DeprecationWarning)\nsns.set_theme(style=\"darkgrid\")","9727c59f":"# Loading twitter archive csv file.\ntwitter = pd.read_csv('..\/input\/twitter-archive\/twitter_archive.csv')","060f65d4":"twitter","4d59378c":"# Using requests module to get the content of the url.\nfolder = 'predictions'\nif not os.path.exists(folder):\n    os.makedirs(folder)\nurl = 'https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/599fd2ad_image-predictions\/image-predictions.tsv'\nget_data = requests.get(url)\nget_data.status_code","e16cd6b5":"# Downloading the data programmatically.\nwith open(os.path.join(folder,url.split('\/')[-1]),'wb') as file:\n    file.write(get_data.content)","5dcae155":"# Loading image prediction tsv file.\npredictions = pd.read_csv('..\/input\/predictions\/image-predictions.tsv',sep='\\s+')\npredictions.head()","c2727618":"\"\"\"consumer_key = \nconsumer_secret = \naccess_token = \naccess_secret = \n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n\napi = tweepy.API(auth,wait_on_rate_limit=True)\"\"\"","d032330f":"# Collecting tweets of the users using their unique tweet_id.\ntweet_ids = twitter['tweet_id'].unique()","71692db1":"tweet_ids","e55495e1":"# Getting data from twitter using twitter API and saving it in an outfile with every new data on new line.\n\"\"\"count = 0\nfails_dict = {}\nstart = timer()\n# Save each tweet's returned JSON as a new line in a .txt file\nwith open('tweet_json.txt', 'w') as outfile:\n    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n    for tweet_id in tweet_ids:\n        count += 1\n        print(str(count) + \": \" + str(tweet_id))\n        try:\n            tweet = api.get_status(tweet_id, tweet_mode='extended')\n            print(\"Success\")\n            json.dump(tweet._json, outfile)\n            outfile.write('\\n')\n        # This will select all twitter ids with no tweets.\n        except tweepy.TweepError as e:\n            print(\"Fail\")\n            fails_dict[tweet_id] = e\n            pass\nend = timer()\nprint(end - start)\nprint(fails_dict)\"\"\"","bae17f66":"# Opening tweet json text file and appending each line to a list.\n\"\"\"data = []\nwith open('tweet_json.txt') as f:    \n        for line in f:         \n             data.append(json.loads(line))\ntweets = pd.DataFrame(data)\ntweets.to_csv(r'tweets_json.csv',index=False,header=True)\"\"\"","6c3d9e83":"# Converting list into dataframe.\ntweets = pd.read_csv('..\/input\/tweets-json\/tweets_json.csv')","bc9f7c3a":"# Selecting only 6 features from tweets dataframe for our data cleaning, analysis and visualization part.\ntweet_api = tweets[['id','retweet_count','favorite_count','display_text_range','lang','created_at']]","f01c23e6":"# Assessing twitter dataset visually.\n# Displayinh first five rows.\ntwitter.head()","b1aff4d0":"# Asscessing twitter dataset visually.\n# Displaying last five rows.\ntwitter.tail()","372bf5c1":"# Asscessing twitter dataset visually.\n# Displaying random 20 rows.\ntwitter.sample(20)","8ab5218b":"# Asscessing predictions dataset visually.\n# Displaying first fifty rows\npredictions.head(50)","748e8d50":"# Asscessing predictions dataset visually.\n# Displaying last five rows.\npredictions.tail()","f92e2bee":"# Asscessing predictions dataset visually.\n# Displaying 10 random rows.\npredictions.sample(10)","584064ec":"# Asscessing tweet_api dataset visually.\n# Displaying first five rows\ntweet_api.head()","7166977d":"# Asscessing tweet_api dataset visually.\n# Displaying last five rows.\ntweet_api.tail()","4ee735fb":"# Asscessing tweet_api dataset visually.\n# displaying 10 random rows.\ntweet_api.sample(10)","beeea5e9":"# Asscessing twitter dataset programatically.\n# Displaying number of rows and columns with their datatypes.\ntwitter.info()","c3e6d5ab":"# Asscessing twitter dataset programatically.\n# Displaying mean, standard deviation, min ,max, count for all numerical features.\ntwitter.describe()","21b6489c":"# Asscessing twitter dataset programatically.\n# Displaying count of duplicates tweet_id.\ntwitter.duplicated().sum()","180350fd":"# Asscessing twitter dataset programatically.\n# Displaying count of null values in columns.\ntwitter.isnull().sum()","c2057727":"# Asscessing predictions dataset Programmatically.\n# Displaying number of entries and columns with their datatypes.\npredictions.info()","60a7f80c":"# Asscessing predictions dataset programmatically.\n# Displaying mean, standard deviation, min ,max, count for all numerical features.\npredictions.describe()","603e5df8":"# Asscessing predictions dataset programmatically.\n# Displaying count of duplicates tweet_id.\npredictions.tweet_id.duplicated().sum()","122db118":"# Asscessing predictions dataset programmatically.\n# Displaying number of null values in column.\npredictions.isnull().sum()","4cf13ea1":"# Asscessing tweet_api dataset programmatically.\n# # Displaying number of entries and columns with their datatypes.\ntweet_api.info()","4b1b4407":"# Asscessing tweet_api dataset programmatically.\n# Displaying mean, standard deviation, min ,max, count for all numerical features.\ntweet_api.describe()","3974f482":"# Asscessing tweet_api dataset programmatically.\n# Displaying count of duplicates in id.\ntweet_api.id.duplicated().sum()","1141dff9":"# Asscessing tweet_api dataset programmatically.\n# Displaying number of null values in column.\ntweet_api.isnull().sum()","abe31785":"# Making a copy of all three original datasets in order to do data cleaning on copied datasets and original datasets remain untouched.\ntwitter_c = twitter.copy()\npredictions_c = predictions.copy()\ntweets_c = tweet_api.copy()","8e580e3c":"twitter_c.head()","3a3064f9":"# Replacing 'None' with empty string in 4 dog stage features.\ntwitter_c['doggo'] = twitter_c.doggo.replace('None','')\ntwitter_c['floofer'] = twitter_c.floofer.replace('None','')\ntwitter_c['pupper'] = twitter_c.pupper.replace('None','')\ntwitter_c['puppo'] = twitter_c.puppo.replace('None','')\n# Concatenating data from each dog stage features together and adding it to new column 'dog_stage'\ntwitter_c['dog_state'] = twitter_c.doggo+twitter_c.floofer+twitter_c.pupper+twitter_c.puppo","ced86718":"twitter_c['dog_state'].unique()","1fa2bf32":"# Separating two separate strings with ',' assigning it back to its place using .loc\ntwitter_c.loc[twitter_c.dog_state == 'doggofloofer', 'dog_state'] = 'doggo,floofer'\ntwitter_c.loc[twitter_c.dog_state == 'doggopuppo', 'dog_state'] = 'doggo,puppo'\ntwitter_c.loc[twitter_c.dog_state == 'doggopupper', 'dog_state'] = 'doggo,pupper'","76b57253":"# Replacing empty strings with 'None'\ntwitter_c['dog_state'] = twitter_c.dog_state.replace('','None')","3d5e3b47":"# Dropping features 'doggo','floofer','pupper','puppo'.\ntwitter_clean = twitter_c.drop(['doggo','floofer','pupper','puppo'],axis=1)","fab419c6":"twitter_clean","5939ae15":"twitter_clean['dog_state'].unique()","69967158":"twitter_clean['dog_state'][190:201]","5da31a68":"twitter_clean[['source']][:5]","31750822":"# Splitting the source feature into two parts and then selecting the desired text like 'twitter for iphone' by applying lambda function.\ntwitter_clean['source'] = twitter_clean['source'].apply(lambda x: x.split('>',1)[1][:-4])","1c28c9ac":"# Testing\ntwitter_clean[['source']].head()","3be4ef67":"twitter_clean[['text']][:5]","1acf608e":"def text(data,feature):\n    a = []\n    url = data[feature].str.extract('((?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-&?=%.]+)')\n    for text in data[feature]:\n        sen = re.sub(r\"http\\S+\", \"\", text)\n        new_sen = re.sub(r\"[0-9]\",\"\",sen)\n        new_sen = new_sen.replace(' \/','').replace(' \\n','')\n        a.append(new_sen)\n    data['Link'] = url\n    data['Text'] = a\n    return data","44cbd1cc":"twitter_clean = text(twitter_clean,'text')","6c74ec1e":"twitter_clean = twitter_clean.drop('text',axis=1)","650b7067":"twitter_clean.columns","795d4575":"twitter_clean.Link[:5]","aaf6c0e6":"twitter_clean.Text[:5]","966d6074":"tweets_c.head()","7ee2c9fa":"# Making a new column in tweet_api and appending splitted data in it.\ntweets_c['day'] = tweets_c.created_at.apply(lambda x: x[:3])\ntweets_c['month'] = tweets_c.created_at.apply(lambda y: y[4:7])\ntweets_c['date'] = tweets_c.created_at.apply(lambda z: z[8:10])\ntweets_c['year'] = tweets_c.created_at.apply(lambda w: w[-4:])","1cb31840":"# Removing 'created_at' feature.\ntweets_c = tweets_c.drop('created_at',axis=1)","33b99ea6":"tweets_c.head()","f95a1e32":"tweets_c.columns","49f55bd4":"#Renaming 'id' to 'tweet_id' in tweet_api dataframe.\ntweets_c.rename(columns={'id':'tweet_id'},inplace=True)","2747cce5":"# Selecting the common feature on which merging will be done.\nall_columns = pd.Series(list(twitter_clean) + list(tweets_c)+list(predictions_c))\n#all_columns[all_columns.duplicated()]\nall_columns[all_columns.duplicated()]","f9d1ef6b":"# Merging two datasets 'twitter_clean' and 'tweets_c' on tweet_id.\ntwitter_merge = pd.merge(twitter_clean,tweets_c,on=['tweet_id'],how='inner')","0b1516c8":"# Making one master dataset by merging all three tables twitter_merge(formed by merging two tables) and predictions_c\nmaster_dataset = pd.merge(twitter_merge,predictions_c,on='tweet_id',how='inner')","2c3d7e47":"predictions_c.isnull().sum()","47506156":"master_dataset","5fe24c16":"master_dataset.shape","faa22c53":"master_dataset.tweet_id.duplicated().sum()","5a0e6e61":"# Checking null values in all features.\nmaster_dataset.isnull().sum()","4833be31":"master_dataset[['in_reply_to_status_id','in_reply_to_user_id']]","99e580d7":"# Removing features.\nmaster_dataset = master_dataset.drop(['in_reply_to_status_id','in_reply_to_user_id'],axis=1)","c531dd32":"# Testing\nmaster_dataset.columns,master_dataset.shape","1586b2c3":"master_dataset[master_dataset['retweeted_status_id'].notnull()]","d087c467":"# Selecting rows with retweeted status id data.\nk = (master_dataset['retweeted_status_id'].notnull())|(master_dataset['retweeted_status_user_id'].notnull())\n# Removing retweeted data.\nmaster_dataset = master_dataset.loc[-k,:]","46aa77c7":"# Testing\nmaster_dataset[['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp']].notnull().sum()\n#twitter_merge.shape","bee03ac8":"master_dataset[['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp']]","5fcbb8c4":"# Dropping features.\nmaster_dataset = master_dataset.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'],axis=1)","1182e0ab":"# Testing\nmaster_dataset.info()","45f3aade":"# Converting object datatype to datetime datatype using pandas to_datetime method.\nmaster_dataset.loc[:,'timestamp'] = pd.to_datetime(master_dataset['timestamp'])","a4a3ac6b":"# Testing\nmaster_dataset['timestamp'].dtype","048fae2e":"master_dataset.info()","564b9365":"master_dataset[['p1','p2','p3']]","b33e93e5":"# Running a loop through p1,p2, and p3 features and converting first character from lowercase to uppercase.\nfor f in ['p1','p2','p3']:\n    master_dataset[f] = master_dataset[f].apply(lambda x:x[0].upper() + x[1:])","4c386abd":"master_dataset[['p1','p2','p3']].head()","a567d60c":"master_dataset[['display_text_range']]","1bb89df9":"# Selecting the desired value from the list by applying lambda function.\nmaster_dataset['display_text_range'] = twitter_merge['display_text_range'].apply(lambda x: int(x[4:6]) if (len(x)==7) else int(x[4:7]))","e5ceb16d":"master_dataset[['display_text_range']]","9e4bcd36":"master_dataset['name']","a4ee737c":"# Replacing invalid names with 'Null'.\nl = ['a','an','this','not','the','none','None','by','officially','just','so']\nfor name in twitter_merge.name:\n    if name in l:\n        master_dataset['name'] = master_dataset.name.replace(name,'Null')","de468f34":"# Testing\nmaster_dataset['name']","ad26f31a":"master_dataset[['date','year']].info()","f8094005":"# Converting datatype to int.\nmaster_dataset.loc[:,['date','year']]= master_dataset.loc[:,['date','year']].astype('int')","e3c6b4a8":"master_dataset[['date','year']].info()","7ccaf7a1":"master_dataset[['p1_dog','p2_dog','p3_dog']]","85d1d678":"# Resetting index of master_dataset.\nmaster_dataset.reset_index(drop=True,inplace=True)","8488d06f":"# Filtering tweets with no predictions for dog as 'TRUE' in top three predictions.\npre=[]\nfor i in range(master_dataset.shape[0]):\n    if (master_dataset['p1_dog'][i]== False) & (master_dataset['p2_dog'][i]==False) & (master_dataset['p3_dog'][i]==False):\n        r = master_dataset.loc[i,:]\n        pre.append(r)\nno_dog = pd.DataFrame(pre)","8daf08d4":"# Removing no_dog data from master_dataset dataframe.\nmaster_dataset  = master_dataset.drop(index=no_dog.index,axis=1)","c1e6ecb6":"master_dataset[['p1_dog','p2_dog','p3_dog']]","f0f68d9d":"master_dataset['img_num'] = master_dataset['img_num'].astype('int')","5ecfafbe":"master_dataset[['img_num']].info()","f99bab21":"for p in ['p1_conf','p2_conf','p3_conf']:\n    master_dataset[p] = round(master_dataset[p],3)","b01483cc":"master_dataset[['p1_conf','p2_conf','p3_conf']]","551a633a":"master_dataset[['p1_conf','p2_conf','p3_conf']].info()","3c2b6800":"master_dataset.reset_index(drop=True,inplace=True)","339b350a":"#Plotting barplot using seaborn countplot.\nplt.figure(figsize=[14,8])\nsns.countplot(x='year',data=master_dataset,color='#09aed5',order=master_dataset.year.value_counts().index)\n# Adding counts on top of each bar\nfor i in range (master_dataset.year.value_counts().shape[0]):\n    count = master_dataset.year.value_counts().values[i]\n    plt.text(i, count+30, count, ha = 'center', va='top')\n    \nplt.title('Tweets count for every year',fontdict={'size':20})\nplt.xlabel('Year',fontdict={'size':15})\nplt.ylabel('Tweets',fontdict={'size':15});","dc7cde97":"#Plotting barplot using seaborn countplot.\nc = sns.color_palette(\"husl\", 9)[7]\nplt.figure(figsize=[14,8])\nfiltered = master_dataset[master_dataset['year']==2016]\nsns.countplot(x='month',data=filtered,color=c,order=filtered.month.value_counts().index)\n# Adding counts on top of each bar\nfor i in range (filtered.month.value_counts().shape[0]):\n    count = filtered.month.value_counts().values[i]\n    plt.text(i, count+3, count, ha = 'center', va='top')\n    \nplt.title('Tweets count for months in year 2016',fontdict={'size':20})\nplt.xlabel('Months',fontdict={'size':15})\nplt.ylabel('Tweets',fontdict={'size':15});\n#plt.xticks(rotation=25);","c9db6a80":"#Plotting barplot using seaborn countplot.\nc = sns.color_palette(\"husl\", 9)[6]\nplt.figure(figsize=[14,8])\nfiltered = master_dataset[master_dataset['year']==2016]\nsns.countplot(x='day',data=filtered,color=c,order=filtered.day.value_counts().index)\n# Adding counts on top of each bar\nfor i in range (filtered.day.value_counts().shape[0]):\n    count = filtered.day.value_counts().values[i]\n    plt.text(i, count+3, count, ha = 'center', va='top')\n    \nplt.title('Total tweets count for days in year 2016',fontdict={'size':20})\nplt.xlabel('Days in 2016',fontdict={'size':15})\nplt.ylabel('Tweets',fontdict={'size':15});","59e8e9f0":"#Plotting barplot using seaborn countplot.\nc = sns.color_palette(\"husl\", 9)[8]\nplt.figure(figsize=[14,8])\nfiltered2 = filtered[filtered['month']=='Jan']\nsns.countplot(x='date',data=filtered2,color=c,order=filtered2.date.value_counts().index)\n# Adding counts on top of each bar\nfor i in range (filtered2.date.value_counts().shape[0]):\n    count = filtered2.date.value_counts().values[i]\n    plt.text(i, count+0.2, count, ha = 'center', va='top')\n    \nplt.title('Tweets count for every date in January, 2016',fontdict={'size':20})\nplt.xlabel('Dates in Jan 2016',fontdict={'size':15})\nplt.ylabel('Tweets',fontdict={'size':15})\n#Plotting barplot using seaborn countplot.\nc = sns.color_palette(\"husl\", 9)[3]\nplt.figure(figsize=[14,8])\nsns.countplot(x='day',data=filtered2,color=c,order=filtered2.day.value_counts().index)\n# Adding counts on top of each bar\nfor i in range (filtered2.day.value_counts().shape[0]):\n    count = filtered2.day.value_counts().values[i]\n    plt.text(i, count+1, count, ha = 'center', va='top')\n    \nplt.title('Tweets count for days in January, 2016',fontdict={'size':20})\nplt.xlabel('Days in Jan 2016',fontdict={'size':15})\nplt.ylabel('Tweets',fontdict={'size':15});\n#plt.xticks(rotation=25);;\n#plt.xticks(rotation=25);","98a9c3fe":"#Plotting a stacked bar chart\nplt.figure(figsize=[18,7])\nc = sns.color_palette(\"husl\", 9)[6]\nsns.countplot(x='month',data=master_dataset,hue='year',palette='Dark2',order=master_dataset.month.value_counts().index)\nplt.title('Tweets count in months for year 2015,2016,2017',fontdict={'size':20})\nplt.xlabel('Months',fontdict={'size':15})\nplt.ylabel('Tweets',fontdict={'size':15});\n","0b244df4":"#Plotting a stacked bar chart\nplt.figure(figsize=[18,7])\nc = sns.color_palette(\"husl\", 9)[6]\nsns.countplot(x='day',data=master_dataset,hue='year',palette='coolwarm_r',order=master_dataset.day.value_counts().index)\nplt.title('Total number of tweets made in a day (2015,2016,2017)',fontdict={'size':20})\nplt.xlabel('Days',fontdict={'size':15})\nplt.ylabel('Tweets',fontdict={'size':15});\n;","4bee2362":"# Line plot\nplt.figure(figsize=[18,8])\nc = sns.color_palette(\"husl\", 9)[3]\nsns.lineplot(x='month',y='favorite_count',hue='year',data=master_dataset,palette='cool',ci=20,dashes=True,markers='o')\nplt.title('Favorite_count vs Month in 2015,2016, and 2017',fontdict={'size':20})\nplt.xlabel('Months',fontdict={'size':15})\nplt.ylabel('Number of Favorite_count',fontdict={'size':15});","5bb6c942":"master_dataset.describe()","05bf79c5":"# Making a interval of 6 numbers for every display text range. for eg, if display text range in 36 than this will fall in the interval of [34-40] \nbins_edges = np.arange(28,148+1,6)\nbins_center = bins_edges[:-1]+3\n# Cut the bin values into discrete intervals. Returns a Series object.\ndispl_binned = pd.cut(master_dataset['display_text_range'], bins_edges, include_lowest = True)","1e24e5b9":"#Mapping each display text range to its appropriate interval using groupby function.\nds = master_dataset['display_text_range'].groupby(displ_binned).count()","e0bc8f12":"ds.values","04f826c0":"# Plotting a bar chart with total counts of text range within a interval\nplt.figure(figsize=[18,7])\nc = sns.color_palette(\"husl\", 9)[8]\nlabels = ['31-37','37-42','42-47','47-52','52-57','57-62','62-67','67-72','72-77','77-82','82-87','87-92','92-97','97-102','102-107','107-112','112-117','117-122','122-127','127-132']\nsns.barplot(x=ds.index,y=ds.values,color=c)\nfor i in range(ds.values.shape[0]):\n    count = ds.values[i]\n    plt.text(i, count+20, count, ha = 'center', va='top')\nplt.xlabel('Display Text range',fontdict={'size':15})\nplt.ylabel('Total count of characters in text within a range',fontdict={'size':15})\nplt.title('Number of characters used in a text within a particular range',fontdict={'size':20})\nplt.xticks(rotation=25);","045eb6b6":"#Filtering predictions_dog dataframe for p1_conf > 0.9 and p1_conf <0.2.\np1_coeff_1 = master_dataset[master_dataset['p1_conf']>=0.9]\np1_coeff_2 = master_dataset[master_dataset['p1_conf']<=0.2]","584183a1":"#Plotting barplot using seaborn countplot.\nplt.figure(figsize=[14,8])\nsns.countplot(y='p1',data=p1_coeff_1,color='#09aed5',order=p1_coeff_1.p1.value_counts().index[:10])\n# Adding counts on top of each bar\nfor i in range(10):\n    count = p1_coeff_1.p1.value_counts().values[i]\n    plt.text(count+0.35, i, count, ha = 'center', va='top')\n    \nplt.title('Top 10 dog breeds with prediction 1 (p1) conf. greater than 0.9 by neural network.',fontdict={'size':20})\nplt.ylabel('Dog breed (top to bottom)',fontdict={'size':15})\nplt.xlabel('Count',fontdict={'size':15})\n\nplt.figure(figsize=[14,8])\nsns.countplot(y='p1',data=p1_coeff_2,color=c,order=p1_coeff_2.p1.value_counts().index[:10])\n# Adding counts on top of each bar\nfor i in range(10):\n    count = p1_coeff_2.p1.value_counts().values[i]\n    plt.text(count+0.1, i, count, ha = 'center', va='top')\n    \nplt.title('Top 10 dog breeds with prediction 1 (p1) conf. less than 0.2 by neural network.',fontdict={'size':20})\nplt.ylabel('Dog breed (top to bottom)',fontdict={'size':15})\nplt.xlabel('Count',fontdict={'size':15});","9daa60a7":"##Filtering predictions_dog dataframe for p1_conf <=0.1.\np1_coeff_3 = master_dataset[master_dataset['p1_conf']<=0.1]","fa318f80":"# Gathering urls of dogs images with predictions less than 0.1\nurls=p1_coeff_3.jpg_url","edfd3a67":"# Forming in array inorder to process the urls.\nimg = np.array(urls[:10])","4deed5b2":"# Using requests library to obtain image data from the given url and using PIL Image and BytesIo to read and show the content of the data collected.\nim=[]\nfor i in range(img.shape[0]):\n    url = img[i]\n    r = requests.get(url)\n    #plt.title('Golden Retriever p1_coeff >0.9')\n    im.append(Image.open(BytesIO(r.content)))\n    ","9b1fcf7f":"#displaying images.\nfor i in im:\n    display(i)\n    ","80908bab":"# Selecting data with prediction greater than 0.9 and prediction for dog as True.\ntrue_dogs =(master_dataset['p1_conf']>=0.9)&(master_dataset['p1_dog']==True)\ntrue_df = master_dataset.loc[true_dogs,:]","367460d3":"# # Gathering urls of dogs images with predictions greater than 0.9\nurls=true_df.jpg_url\n# Taking only 10 image urls.\nimg2= np.array(urls[:10])\n# Using requests library to obtain image data from the given url and using PIL Image and BytesIo to read and show the content of the data collected.\nim2=[]\nfor i in range(img2.shape[0]):\n    url = img2[i]\n    r = requests.get(url)\n    #plt.title('Golden Retriever p1_coeff >0.9')\n    im2.append(Image.open(BytesIO(r.content)))\n","d95ddbd3":"# Displaying image.\nfor i in im2:\n    display(i)","1be1d779":"<a id='clean'><\/a>\n# Clean","9e9940bc":"## Test","fbc13443":"## Test","28674877":"## Code","f7a80d33":"We can see that in 2016, month 'January'(141) had the highest number of tweets made whereas lowest tweets were made in 'november'(45). Now, lets find out the date on which maximum tweets were made in 'January' 2016.","6a569e4e":"# 3- For which dog breed was p1_conf greater than 0.9 and less than 0.2?","23c901cc":"## Test","b60fc14b":"## Test","c3d7d50f":"Golden retriever is the dog breed with p1_ conf greater than 0.9 and have 36 counts which is the highest. Also, for p1_conf less than 0.2, 'Italian greyhound' and 'Chihuahua' are the dog breeds with maximum counts i.e 4. Moreover, we can see that gloden retriever has a p1 coeff. less than 0.2. Let's find out why neural network predicts dog breeds with different p1_conf. ","625d7dd6":"## Define\n3- __text__ feature in twitter contains non descriptive URLs and also ratings at the end of each text. It should be split into text,rating and url. But since ratings are already present, I will remove ratings from 'Text' and also drop the feature after splitting url and text.","b35c5dc6":"<a id='qi'><\/a>\n# Quality Issue.","d5a124ee":"## Define\n\n### Validity\n6- Datatype of __display_text_range__ is an object datatype which should be converted into integer datatype either by removing brackets and 0 (which will be the same for every range) or by just using the index of second value in the list in twitter_merge dataframe.","1db91fd8":"## Define\n\n### Accuracy\n9- Remove tweets which do not have ratings for dog. I will take only those tweets for which neural network has predicted at least one of the top three predictions as dog 'TRUE'. If none of the predictions for dog are 'TRUE' than I will discard those tweets from the dataset since their is very low chance that the dog will be present in the image.","a819734a":"## Test","ac97846d":"## Test","5db6a1cc":"## Test","548c4c35":"## Code","41c366be":"## Define\n\n### Validity\n7- Column __name__ contain invalid data which should be removed. But removing this data will also remove some important features data hence it will be better to convert all invalid names to NULL in master_dataset dataframe.","4cc08858":"## Code","22312a1a":"## Code","a6773809":"2- __Programmatic assessment__","106184d3":"For year 2015, number of tweets made were highest in december and lowest in november. Also, from the graph we can see that November and December were the only months in all three years in which the total number of tweets made were largest despite the fact that in 2015 people tweeted only in 'nov','dec'. Moreover, for year 2017 maximum count of tweets were made in both January and February.","73e565dc":"## Code","d3f83a02":"## Year vs Month and Favorite count.","dc95d0d5":"## Test","207b5080":"## Code","ff65885b":"## Code","f1b40bab":"<a id='assess'><\/a>\n# Assess.\nIn data accessing process I assessed data for two issues:<br>\n__1- Tidiness Issue- This issue was related to the proper arrangement of data in the table for easier analysis. The requirements were as follows:__<br>\n\u2022 Every variable forms a column.<br>\n\u2022 Every observation forms a row.<br>\n\u2022 Every observational unit forms a table.<br><br>\n__2- Quality issue- This issue was related to the content for example incorrect data types, missing data, invalid data, inaccurate data, inconsistent data. I used mainly four data quality dimensions to access this\nissue. These were as follows:__<br>\n\u2022 Completeness : This dimension is related with missing data. issue.<br>\n\u2022 Validity: This dimension is related with invalid data. For Example- negative weights, invalid datatypes, etc..<br>\n\u2022 Accuracy: In this dimension, although data is valid but issue is with its accuracy. For example- A person with weight 80 Kg and height 56 cm. Although, height 56 cm is valid, it is inaccurate to associate this height with weight 80 kg.<br>\n\u2022 Consistency: Finally, this dimension is related with the consistency in variables in a column. For example a state having its full names as well as its abbreviations(California- CA) in the same column.<br>\n  \n__I assessed quality and tidiness issues using two processes:__<br>\n\u2022 Visual Assessment- Used external software like MS-Excel. Apart from this, I used pandas functions like head(), tail(), sample().<br>\n\u2022 Programmatic Assessment- Using pandas functions like info(), describe(), isnull(), duplicated().\n","39562f6b":"## Code","12f7ffb8":"## Define\n\n### Validity\n8- Converting datatype of __date__ and __year__ from object to int in twitter_merge.","1c1708be":"## Tweets vs Month and days for year 2015, 2016 and 2017","8abe8412":"Above 10 images have p1_conf greater than 0.9 and they have been labeled as True for dog category. So, the main reason according to me after analyzing both images one with p1_conf <0.1 and another with p1_conf >0.9 is that the images with p1_conf less than 0.1 have a lot of noise in the background or dog is not properly visible. Whereas, images with p1_conf greater than 0.9 have dogs clearly visible and also noise is very low. So, I think we have to make our neural network algorithm more robust by adding some more layers or using some dropouts or regularization or normalization techniques.","347b4233":"## Tweets vs Day (2016)","849de3b5":"For month 'august' favorite count was maximum in 2017 around 30000. Whereas in 2016, favourite count in month 'Aug' was somewhere arounf 8000. Lowest number of favorite count was in month 'Nov' for 2015.","4eb09501":"## Tweets vs date (in january 2016)","27c2c168":"> Number of tweets made in 2016 were maximum. I will try to find the month (in 2016) in which  maximum tweets were made.","73936672":"## Define\n\n### Completeness.\n1- Dropping the features which do not contain much data for eg. __in_reply_to_status_id__, __in_reply_to_user_id__.","07a34779":"#### Line  shows how to use keys and tokens to connect to the API which were obtained after making a twitter developer account.\n\n#### Function of the code in line  is to query the API for 'tweet_id' and write json data into 'tweet_json.txt' file and save it into the directory.\n\n#### Line is for reading 'tweet_json.txt' file and converting it into a dataframe.\n\n#### In line , I have saved the dataframe into 'tweets.csv' file.\n\n#### I have not shown the running process of the above mentioned lines because of the security resons.<br><br>\n\n___Note- Do not share your API credentials with anyone.___","8724c793":"<a id='ti'><\/a>\n## Tidiness issue.","b21f6db5":"## Define\n4- Split day,month,date and year from __created_at__ feature and make separate columns for each in tweet_api. Drop created_at feature after splitting.","96e1a188":"## Table of Contents\n- [Data Gathering](#gather)\n- [Data Assessing](#assess)\n- [Data Cleaning](#clean)\n> - [Tidiness Issues](#ti)\n> - [Quality Issues](#qi)\n- [Analysis and Visualization](#analysisndvis)","cbf8542f":"Although all images are of dogs, the main reason behind predicting p1 wrong could be the noise in these images. For eg, one image have bananas,apples on the dog so neural network has predicted this image as egyptian cat. In one image dog is wearing a shark costume so that's why it could be the reason why neural netwrok was not able to make right prediction for dog breed. Let's look at the images with p1_conf greater than 0.9. ","e14bcc8e":"## Define\n5- Combining __twitter_clean__ and __tweets_c__ and __predictions_c__ data frames on __tweet_id__ and forming one table master_dataset.","02026bd8":"Out of all the tweets made in 2015 and 2016, 'Monday' has the longest bars for both 2015 and 2016. Whereas in 2017, maximum tweets were made on 'friday'.","4e13bc7b":"## Code","3172e783":"From the above dataframe we can see that neural network has predicted invalid objects (p1) like ox, bath_towel, teddy etc. in place of dogs. Also, some predictions are True for dogs but their p1_conf is very low. So, let's see why this has happened.","50293237":"## Test","92a48a1e":"Tweets made in january were highest (8) on 5th of Jan. Also, maximum tweets (26) were made on 'Friday' in january 2016. ","a856ba77":"## Tweets vs Month ( in 2016)","bc73f044":"## Test","69550dec":"__Project Motivation__<br>\nDon't include retweets.<br>\nInclude tweets with images.<br>\nExclude tweets with rating not for dog.<br>\nYou do not need to gather the tweets beyond August 1st, 2017<br>\nFrom tweets df, select atleast tweet id,retweets and favourites features.<br>\nTweets can have upto 4 images.<br>\nimg_num in prediction df corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n__assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.__","1c656056":"![alt text](https:\/\/imagesvc.meredithcorp.io\/v3\/jumpstartpure\/image?url=https:\/\/cf-images.us-east-1.prod.boltdns.net\/v1\/static\/6157254766001\/8a7a5cae-813b-4772-b47b-1c33cb4a83f7\/ecc29185-8ae0-4444-b792-53357486cd5a\/1280x720\/match\/image.jpg&w=1280&h=720&q=90&c=cc)\n# Wrangle and Analyze data\n\nIn this project my objective was to gather, access, clean, and analyze data. The data analysis and visualization part was done on the cleaned data obtained after performing data wrangling in which I removed some data quality and data tidiness issues using three datasets which were based on tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. The final cleaned dataset included one table master_dataset which was used for the data analysis and visualization part.","23d758e7":"## Define\n\n2- Splitting __source__ column from twitter dataframe and selecting only text instead of url and '<a','\/a'","0f323bd0":"'Monday' is the day on which maximum tweets were made with counts 139 in year 2016. Whereas 'Sunday' is the day on which minimum tweets were made with the counts 104.","d093efb1":"## Define\n\n### Accuracy\n2- Removing the data from master_dataset dataframe which include retweets in the form of __retweeted_status_id__ or __retweeted_status_user_id__ or __retweeted_status_timestamp__.","fca6ee33":"## Define\n\n### Completeness.\n3- Dropping the features which do not contain any data for eg. __retweeted_status_id__, __retweeted_status_user_id__ and __retweeted_status_timestamp__.","208540c1":"## Code","73fa7f94":"# 4-What could be the reason behind predicting dog breeds with low and high p1 conf.?","239c4d45":"## Define\n\n### Consistency\n\n11- Their are inconsistencies in p1_conf, p2_conf and p3_conf values. Some values are upto 6 decimal places whereas some are greater than 6 and less than 6. Take values upto 3 decimal places only.","86f0e5e0":"## Define\n\n### Validity\n10-Change the datatype of feature 'img_num' from float to int.\n\n## Code","83d54b5b":"## Define\n\n### Validity.\n4- __Timestamp__ is an object datatype instead of datetime datatype in master_dataset dataframe. Let's convert it into datetime datatype.","2d57d4e3":"<a id='analysisndvis'><\/a>\n# Analysis and Visualization.\n\n- In which year, month, day and date tweets were the highest and the lowest?\n- What is the counts of display_text_range within a particular range?\n- For which dog breed was p1_conf greater than 0.9 and less than 0.2?\n- What is the reason behind predicting dog breeds with different p1_conf? ","7209b5f0":"Maximum count of characters in tweets are within a range of 136 - 142 with total count of 367.","d868199b":"<a id='gather'><\/a>\n# Gather\n__1- DATA GATHERING PROCESS.__<br>\n\n__STEP-1__ I set up my twitter application by making a twitter developer account in order to generate the Consumer API keys, and the Access Token and Access Token Secret. I used \u2018Tweepy\u2019 (An easy-to-use Python library for accessing the Twitter API) to query twitter API for each tweet's JSON data and store each tweet's entire set of JSON data in a file\ncalled tweet_json.txt file. I wrote each tweet's JSON data to its own line. Then I read this .txt file line by line into a pandas DataFrame with \u2018tweet_id\u2019, \u2018retweet count\u2019, \u2018favourite count\u2019, \u2018display text range\u2019, \u2018lang\u2019\nand \u2018created_at\u2019 variables.<br><br>\n__STEP-2__ Downloaded the twitter_archive_enhanced.csv manually from resource section. This file contained ratings, text, dog type, urls and many more variables.<br>\n    \n__STEP-3__ Downloaded image_predictions.tsv, a tab separated value file (tsv), and accessed it using pandas read_csv with sep=\u2018\\t\u2019. This file contained a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n","59120f6d":"## Test","90c90caf":"## Code","0da605b5":"## Test","51647adb":"## Code","0755c217":"# 2- What is the average count of display_text_range within a particular interval?","dba49fb8":"## Visual Assessment and Programmatic Assessment<br><br>\n\n1- __Visual Assessment__","6466f06e":"## Code","9651dc19":"## Quality issues\n\n> __twitter__\n>> - Dropping the features which contain lots of missing data like 'in_reply_to_status_id','in_reply_to_user_id',<br>\n>> - 'Timestamp' is an object datatype instead of datetime datatype.<br>\n>> - Removing the rows which include retweets in the form of 'retweeted_status_id','retweeted_status_user_id',\t'retweeted_status_timestamp'. And than dropping these features as they contain missing data as well.<br>\n>> - 'Name' column contain invalid data which should be removed. But removing this data will also remove some important features data hence it will be better to convert all invalid names to NULL.<br>\n>> - Removing 'retweeted_status_id','retweeted_status_user_id',\t'retweeted_status_timestamp' variables because they do not contain data.<br>\n\n> __predictions__\n>> - Lowercase and uppercase 'p_1','p_2', and 'p_3' variables. Convert all of them to uppercase.<br>\n>> - Remove tweets which do not have ratings for dog. I will take only those tweets for which neural network has predicted at least one of the top three predictions as dog 'TRUE'. If none of the predictions for dog is 'TRUE' than I will discard those tweets from the dataset since their is very low chance that the dog will be present in the image.\n>> - Their are inconsistencies in p1_conf, p2_conf and p3_conf values. Some values are upto 6 decimal place whereas some are greater than 6. Take values upto 3 decimal place.\n>> - Change the datatype of feature 'img_num' from float to int.\n\n> __tweet_api__\n>> - Converting datatype of 'date' and 'year' from object to int in twitter_merge.<br>\n>> - Datatype of 'Display_text_range' is object which should be converted into int by removing brackets and 0 which will be the same for every range.<br>\n\n\n\n\n\n## Tidiness issues\n\n> __twitter__\n>> - 'Text' contains non descriptive URLs and also ratings at the end of each text. It should be split into text,rating and url. But since ratings are already present, I will remove ratings from 'Text'.<br>\n>> - Making one column for features 'doggo','floofer','pupper','puppo' because column headers are values, not variable names.<br>\n>> - Splitting source column and selecting only text instead of url and '<a','\/a'.<br>\n\n>__tweet_api__\n>> - Split day,month,date and year from created_at feature and make separate columns for each.<br>\n> - Merging 'twitter_clean', 'predictions_c', 'tweets_c' on tweet_id.\n","5619aa71":"# 1- In which year, month, day and date tweets were the highest and the lowest?\n\n## Tweets vs Year.","8c2cc721":"## Test","392714fb":"## Test","ce15d72f":"## Define\n\n1- Making one single column 'dog_state' for features __doggo__, __floofer__, __pupper__, __puppo__  because column headers are values, not variable names.","dea9ac23":"## Test","1f694b38":"## Code","3a2e12b9":"## Define\n\n### Consistency\n5- Lowercase and uppercase __p1__, __p2__, and __p3__ variables. Convert first character of all to uppercase in master_dataset dataframe.","b4e49cff":"## Code","a94e9ae5":"## Test"}}