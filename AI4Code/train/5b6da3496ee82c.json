{"cell_type":{"5a9b26c2":"code","bbf89fca":"code","781224df":"code","39c08b63":"code","c0b1a698":"code","61540917":"markdown"},"source":{"5a9b26c2":"import pandas as pd\nfrom pathlib import Path\n\npath = Path('rossmann')\ntrain_df = pd.read_pickle('..\/input\/rossman-fastai-sample\/train_clean').drop(['index', 'Date'], axis='columns')\ntest_df = pd.read_pickle('..\/input\/rossman-fastai-sample\/test_clean')","bbf89fca":"train_df.head()","781224df":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nimport numpy as np\n\n\ncat_vars = [\n    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n]\ncont_vars = [\n    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n]\ntarget_var= 'Sales'\n\n\nclass ColumnFilter:\n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        return X.loc[:, cat_vars + cont_vars]\n        \n\nclass GroupLabelEncoder:\n    def __init__(self):\n        self.labeller = LabelEncoder()\n    \n    def fit(self, X, y):\n        self.encoders = {col: None for col in X.columns if col in cat_vars}\n        for col in self.encoders:\n            self.encoders[col] = LabelEncoder().fit(\n                X[col].fillna(value='N\/A').values\n            )\n        return self\n    \n    def transform(self, X):\n        X_out = []\n        categorical_part = np.hstack([\n            self.encoders[col].transform(X[col].fillna(value='N\/A').values)[:, np.newaxis]\n            for col in cat_vars\n        ])\n        return pd.DataFrame(categorical_part, columns=cat_vars)\n\n\nclass GroupNullImputer:\n    def fit(self, X, y):\n        return self\n        \n    def transform(self, X):\n        return X.loc[:, cont_vars].fillna(0)\n\n\nclass Preprocessor:\n    def __init__(self):\n        self.cf = ColumnFilter()\n        self.gne = GroupNullImputer()\n        \n    def fit(self, X, y=None):\n        self.gle = GroupLabelEncoder().fit(X, y=None)\n        return self\n    \n    def transform(self, X):\n        X_out = self.cf.transform(X)\n        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n        return X_out\n\n\nX_train_sample = Preprocessor().fit(train_df).transform(train_df.loc[:999])\ny_train_sample = train_df[target_var].loc[:999]","39c08b63":"import torch\nfrom torch import nn\nimport torch.utils.data\n# ^ https:\/\/discuss.pytorch.org\/t\/attributeerror-module-torch-utils-has-no-attribute-data\/1666\n\n\nclass FeedforwardTabularModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.batch_size = 512\n        self.base_lr, self.max_lr = 0.001, 0.003\n        self.n_epochs = 5\n        self.cat_vars_embedding_vector_lengths = [\n            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n            (3, 3), (8, 4), (8, 4)\n        ]\n        self.loss_fn = torch.nn.MSELoss()\n        self.score_fn = torch.nn.MSELoss()\n        \n        # Layer 1: embeddings.\n        self.embeddings = []\n        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n            emb = nn.Embedding(in_size, out_size)\n            self.embeddings.append(emb)\n            setattr(self, f'emb_{i}', emb)\n\n        # Layer 1: dropout.\n        self.embedding_dropout = nn.Dropout(0.04)\n        \n        # Layer 1: batch normalization (of the continuous variables).\n        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n        \n        # Layers 2 through 9: sequential feedforward model.\n        self.seq_model = nn.Sequential(*[\n            nn.Linear(in_features=215, out_features=1000, bias=True),\n            nn.ReLU(),\n            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n            nn.Dropout(p=0.001),\n            nn.Linear(in_features=1000, out_features=500, bias=True),\n            nn.ReLU(),\n            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n            nn.Dropout(p=0.01),\n            nn.Linear(in_features=500, out_features=1, bias=True)\n        ])\n    \n    \n    def forward(self, x):\n        # Layer 1: embeddings.\n        inp_offset = 0\n        embedding_subvectors = []\n        for emb in self.embeddings:\n            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n            out = emb(inp)\n            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n            embedding_subvectors.append(out)\n            inp_offset += 1\n        out_cat = torch.cat(embedding_subvectors)\n        out_cat = out_cat.view(out_cat.shape[::-1])\n        \n        # Layer 1: dropout.\n        out_cat = self.embedding_dropout(out_cat)\n        \n        # Layer 1: batch normalization (of the continuous variables).\n        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n        \n        out = torch.cat((out_cat, out_cont), dim=1)\n        \n        # Layers 2 through 9: sequential feedforward model.\n        out = self.seq_model(out)\n            \n        return out\n        \n        \n    def fit(self, X, y):\n        self.train()\n        \n        # TODO: set a random seed to invoke determinism.\n        # cf. https:\/\/github.com\/pytorch\/pytorch\/issues\/11278\n\n        X = torch.tensor(X, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.float32)\n        \n        # The build of PyTorch on Kaggle has a blog that prevents us from using\n        # CyclicLR with ADAM. Cf. GH#19003.\n        # optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n        # scheduler = torch.optim.lr_scheduler.CyclicLR(\n        #     optimizer, base_lr=base_lr, max_lr=max_lr,\n        #     step_size_up=300, step_size_down=300,\n        #     mode='exp_range', gamma=0.99994\n        # )\n        optimizer = torch.optim.Adam(self.parameters(), lr=(self.base_lr + self.max_lr) \/ 2)\n        batches = torch.utils.data.DataLoader(\n            torch.utils.data.TensorDataset(X, y),\n            batch_size=self.batch_size, shuffle=True\n        )\n        \n        for epoch in range(self.n_epochs):\n            for i, (X_batch, y_batch) in enumerate(batches):\n                X_batch = X_batch.cuda()\n                y_batch = y_batch.cuda()\n                \n                y_pred = model(X_batch).squeeze()\n                # scheduler.batch_step()  # Disabled due to a bug, see above.\n                loss = self.loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            print(\n                f\"Epoch {epoch + 1}\/{self.n_epochs}, Loss {loss.detach().cpu().numpy()}\"\n            )\n    \n    \n    def predict(self, X):\n        self.eval()\n        with torch.no_grad():\n            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n        return y_pred.squeeze()\n    \n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        y = torch.tensor(y, dtype=torch.float32).cuda()\n        return self.score_fn(y, y_pred)","c0b1a698":"model = FeedforwardTabularModel()\nmodel.cuda()\nmodel.fit(X_train_sample.values, y_train_sample.values)","61540917":"## Replication a linear fast.ai model, part 2\n\nIn this notebook we move the model onto GPU."}}