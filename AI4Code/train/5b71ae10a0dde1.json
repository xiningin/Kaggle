{"cell_type":{"af8f9f3b":"code","6966651b":"code","1d6aebf5":"code","ceaa20d5":"code","e6500a0b":"code","84ee2e36":"code","d8b7861c":"code","eb8f6a21":"code","bf908c03":"code","c4b0c4b7":"code","6a34e15d":"code","38d63a51":"code","266e4121":"code","72122c8d":"code","64768225":"code","a40be700":"code","2540a0d0":"code","b7e417d7":"code","2346c09b":"code","884326f7":"code","323e1bf1":"code","46fbb7b1":"code","1c5cac43":"code","b5fc8c58":"markdown","2f32eae8":"markdown","f3dd3a02":"markdown","9491f660":"markdown","e2f20ded":"markdown","cf13c040":"markdown","3bbb3744":"markdown","03a36eae":"markdown","cc37c4e2":"markdown","8059bf86":"markdown","6b49630d":"markdown","7e487f46":"markdown","a83f6a35":"markdown","554da5a9":"markdown","f44b0c61":"markdown","50492671":"markdown","f2c32659":"markdown","b200f3ae":"markdown","a7cbed04":"markdown","6cb4e709":"markdown","27a43eb1":"markdown","13a88c5a":"markdown","ce12598b":"markdown","f38a6b02":"markdown","1490cfef":"markdown"},"source":{"af8f9f3b":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","6966651b":"cyber = pd.read_csv(\"..\/input\/cyberpunk-2077\/comments.csv\", lineterminator='\\n')","1d6aebf5":"cyber= cyber[[\"text\", \"likeCount\"]]\ncyber.head()","ceaa20d5":"len(cyber[cyber.likeCount < 0])","e6500a0b":"cyber[cyber.text.isnull()]","84ee2e36":"cyber= cyber[~cyber.text.isnull()]","d8b7861c":"print(round(len(cyber[cyber.likeCount == 0]) \/ len(cyber.likeCount) * 100, 2), \"%\")","eb8f6a21":"cyber[\"is_liked\"]= \"\"\ncyber.loc[cyber.likeCount == 0, \"is_liked\"] = 0\ncyber.loc[cyber.likeCount > 0, \"is_liked\"] = 1","bf908c03":"len(cyber[cyber.is_liked== 0])","c4b0c4b7":"len(cyber[cyber.is_liked== 1])","6a34e15d":"df= cyber.filter([\"is_liked\"]).groupby([\"is_liked\"], as_index=False).size().sort_values(['size'], ascending= False)[:7].sort_values(\"size\", ascending=False)\n\nfig, ax =plt.subplots(figsize=(11,9))\nplot = sns.barplot(x=\"is_liked\", y=str(\"size\"), data=df, label=\"Total\", color= \"darkred\")\nplot.set_title(\"Is_liked\", fontsize=20, fontweight=\"bold\")\nplot.tick_params(axis='x', labelsize=14)\nplot.tick_params(axis='y', labelsize=14)","38d63a51":"cyber.describe().apply(lambda s: s.apply('{0:.2f}'.format))","266e4121":"fig, ax =plt.subplots(figsize=(11,9))\nplot = sns.kdeplot(np.log2(cyber.likeCount[cyber.likeCount != 0]), shade=True, color=\"darkblue\")\nplot.tick_params(axis='x', labelsize=14)\nplot.tick_params(axis='y', labelsize=9)\nplot.set_title(\"likeCount\", fontsize=20, fontweight=\"bold\")\nplot.axvline(np.log2(cyber.likeCount[cyber.likeCount != 0]).mean(), color='r', linestyle='--')\nplt.legend({'likeCount': np.log2(cyber.likeCount[cyber.likeCount != 0]), 'Mean': np.log2(cyber.likeCount[cyber.likeCount != 0]).mean()}, fontsize=18)\nplt.show()","72122c8d":"nltk.download('vader_lexicon')\nanalyser = SentimentIntensityAnalyzer()","64768225":"def print_sentiment_scores(sentence):\n    snt = analyser.polarity_scores(sentence)  \n    print(\"{:-<40} {}\".format(sentence, str(snt)))","a40be700":"i=0 \ncompval= []\n\nwhile (i<len(cyber)):\n    k = analyser.polarity_scores(cyber.iloc[i]['text'])\n    compval.append(k['compound'])\n    i = i+1\n\ncompval = np.array(compval)\ncyber['vader_score'] = compval","2540a0d0":"i = 0\npredicted_value = [] \n\nwhile(i<len(cyber)):\n    if ((cyber.iloc[i]['vader_score'] >= 0)):\n        predicted_value.append(1)\n        i = i+1\n    elif ((cyber.iloc[i]['vader_score'] <= 0)):\n        predicted_value.append(0)\n        i = i+1\ncyber['pred_sentiment'] = predicted_value","b7e417d7":"didntmakeit = cyber[cyber['is_liked'] != cyber['pred_sentiment']]","2346c09b":"madeit = cyber[cyber['is_liked'] == cyber['pred_sentiment']]","884326f7":"len(madeit)\/len(cyber)","323e1bf1":"df= cyber.filter(['pred_sentiment']).groupby(['pred_sentiment'], as_index=False).size().sort_values(['pred_sentiment'], ascending= False)\nplot = sns.barplot(x=\"pred_sentiment\", y=str(\"size\"), data= df,\n            label=\"Total\")\nplot.tick_params(axis='x', labelsize=9)\nplot.tick_params(axis='y', labelsize=14)","46fbb7b1":"df = madeit[madeit['pred_sentiment']== 0] \n\nwords = ' '.join(df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nstopwords = set(STOPWORDS)\n\n# stopwords are common words repeated in both classes which are going to be omitted\n\nstopwords.add(\"Cyberpunk\")\nstopwords.add(\"game\")\nstopwords.add(\"Projekt\")\nstopwords.add(\"CDPR\")\nstopwords.add(\"will\")\nstopwords.add(\"Keanu\")\nstopwords.add(\"CD\")\nstopwords.add(\"look\")\nstopwords.add(\"Reeve\")\nstopwords.add(\"Reeves\")\nstopwords.add(\"Red\")\n\nwordcloud = WordCloud(stopwords=stopwords,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","1c5cac43":"df = madeit[madeit['pred_sentiment']== 1] \n\nwords = ' '.join(df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nstopwords = set(STOPWORDS)\n\nstopwords.add(\"Cyberpunk\")\nstopwords.add(\"game\")\nstopwords.add(\"Projekt\")\nstopwords.add(\"CDPR\")\nstopwords.add(\"will\")\nstopwords.add(\"Keanu\")\nstopwords.add(\"CD\")\nstopwords.add(\"look\")\nstopwords.add(\"Reeve\")\nstopwords.add(\"Reeves\")\nstopwords.add(\"Red\")\n\nwordcloud = WordCloud(stopwords=stopwords,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","b5fc8c58":"## - Liked:","2f32eae8":"## Two rows in the \"text\" feature are empty, hence, these are going to be removed","f3dd3a02":"## It seems the comments from the \"not likes\" group most of the times contain negative words, such as \"shit\" or \"fuck\", this is probably the reason no one liked these kind of comments. I guess most of the people mentioned bad things about the game, or they did not like the content of the video in general\n## On the other hand, in comments with likes, there are not words with negative connotation. The most common words have positive connotation, such as \"omg\" or \"love\", probably the people were speaking well about the game, and it is important to know that the data was extracted when the game was not release yet, and it is well known that there was a lot of \"hype\" for this game","9491f660":"## 3.2) likeCount:","e2f20ded":"## Below one can find a density plot of the same feature after applying a log transformation (0 values are not represented)","cf13c040":"## There are not negative numbers","3bbb3744":"## Below one can find the main statistics of the numercial feature \"likeCount\"","03a36eae":"## 3.1) is_liked:","cc37c4e2":"## Below one can find a simple bar plot of the created feature \"is_liked\", which is compossed by two classes, as mentioned before the classes are unbalanced","8059bf86":"## In this step the feature \"is_liked\" is going to be created.","6b49630d":"## - Not liked:","7e487f46":"# 4) Sentiment analysis:","a83f6a35":"# 1) Data cleaning","554da5a9":"## \"vader_lexicon\" from the package nltk is going to be used: more details can be found: https:\/\/www.nltk.org\/_modules\/nltk\/sentiment\/vader.html","f44b0c61":"## Only 34.55% of the data was predicted correctly","50492671":"# 3) Data exploration:","f2c32659":"# 2) Feature enginering:","b200f3ae":"## The created column is based on \"likeCount\" and it splits the data in two groups, more details can be found below:\n### - \"Not liked\" - Comments without likes, number of likes equal to 0 - Value: 0\n### - \"Liked\" - Comments with likes, number of likes greater than 0 - Value: 1","a7cbed04":"## Some of he most common words in \"not liked\" group:\n### - shit\n### - fuck","6cb4e709":"## It seems the data is concentrated around low number of likes (close to zero). This shows that it is quite challenging to have high amount of likes in the comments, and most of the comments did not receive any like","27a43eb1":"# 5) Results and conclusions:","13a88c5a":"## Some of the most common words in \"liked\" group:\n### - omg\n### - love","ce12598b":"## It seems the data is really imbalanced, there are many more comments without likes than with likes","f38a6b02":"## As it can be seen, there are a lot of comments without any like, more than 74% of the comments. This could lead to obtaining imbalance data when performing the division of groups","1490cfef":"## Firstly, the data is going to be filtered. The selected columns are the following:\n- \"text\": This column contains the comments published on the different videos, hence, there are different string values\n- \"likeCount\": It represents the number of likes each comment received, hence, there are integer values"}}