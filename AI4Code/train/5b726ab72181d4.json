{"cell_type":{"1be5b276":"code","62fee632":"code","5349f8c3":"code","71504626":"code","13d9a3a8":"code","4d05fe3a":"code","d427b5e1":"code","020bbbd1":"code","2f2cc71a":"code","19852a9e":"code","897a5ffd":"code","613ca0f8":"code","e23ed46e":"code","671c343b":"code","3aa3502e":"code","c9293016":"code","48bcc387":"code","53ae06ef":"code","147d1bfe":"code","6c519de6":"code","a8b091ff":"code","e7a67e3e":"code","d986aec6":"code","b8e0972f":"code","2efc058c":"code","a6c4ddfa":"markdown","55362bcf":"markdown","7d93e4be":"markdown","6583a2d0":"markdown","d06325c8":"markdown","d8d837de":"markdown","5b207a04":"markdown","593a6fb1":"markdown","d6cd22ec":"markdown","09776b83":"markdown","92b493ee":"markdown","317d99b2":"markdown","f6b5066e":"markdown"},"source":{"1be5b276":"#Load starting packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","62fee632":"#Load data and save response as target\ntrain_raw = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv', index_col= 'Id')\ntest_all = pd.read_csv('..\/input\/forest-cover-type-prediction\/test.csv', index_col='Id')\n\ntarget = train_raw['Cover_Type']","5349f8c3":"train_raw.describe()","71504626":"train_raw.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)\ntest_all.drop(['Soil_Type7', 'Soil_Type15'],axis =1, inplace = True)","13d9a3a8":"train_raw.info()","4d05fe3a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# set the figure size\nplt.figure(figsize=(10,8))\ny_target = np.array(target)\ndata = y_target\/len(y_target)*100\nchart = sns.countplot(data,\n    palette=('Set3')\n)\nchart.set_xticklabels([\"Spruce\/Fir (1)\",\"Lodgepole Pine (2)\",\"Ponderosa Pine (3)\",\"Cottonwood\/Willow (4)\",\"Aspen (5)\",\n                       \"Douglas-fir (6)\",\"Krummholtz (7)\"],rotation=45)","d427b5e1":"corrMatrix = train_raw.corr()\n\ncorrAbs = corrMatrix.abs().unstack()\ncorrSorted = corrAbs.sort_values(ascending = False).drop_duplicates()\n\ncorrSorted.head(50)","020bbbd1":"train_all = train_raw.copy()\ntrain_all.drop(['Cover_Type'], axis=1, inplace=True)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(train_all, target, \n                                                                train_size=0.9, test_size=0.1, random_state=5, stratify = target)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state = 20, n_jobs = -1, n_estimators = 100, bootstrap = True, max_depth = 50,\n                            max_features = 0.5)\nrfc.fit(X_train, Y_train)\n\npred_valid_rf = rfc.predict(X_valid)\n\nfrom sklearn import metrics\nprint(metrics.accuracy_score(Y_valid, pred_valid_rf))","2f2cc71a":"pred_test_rf = rfc.predict(test_all)","19852a9e":"#output = pd.DataFrame({'Id': test_all.index,\n#                       'Cover_Type': pred_test_rf})\n#output.head()\n#output.to_csv('submission.csv', index=False)","897a5ffd":"importance = rfc.feature_importances_\n\nfrom matplotlib import pyplot\npyplot.bar([x for x in range(len(importance))], importance)\npyplot.show()","613ca0f8":"train_all.columns","e23ed46e":"from scipy import stats\nfrom scipy.cluster import hierarchy as hc\n\ncorr = np.round(stats.spearmanr(train_all).correlation, 2)\nplt.figure(figsize=(20,20))\n\nhc.dendrogram(hc.linkage(hc.distance.squareform(1-corr), \n                         method='average'), \n              labels=train_all.columns, orientation='left', \n              leaf_font_size=14)\nplt.show()","671c343b":"from matplotlib import pyplot\n# elevation buckets for every 250 feet\n\ncolors = train_raw['Cover_Type']\npyplot.scatter(train_raw['Elevation'], train_raw['Horizontal_Distance_To_Roadways'], c=colors, cmap = 'Paired')","3aa3502e":"pyplot.scatter(train_raw['Elevation'], train_raw['Horizontal_Distance_To_Fire_Points'], c=colors, cmap = 'Paired')","c9293016":"pyplot.scatter(train_raw['Horizontal_Distance_To_Roadways'], train_raw['Horizontal_Distance_To_Fire_Points'], c=colors, cmap = 'Paired')","48bcc387":"pyplot.scatter((train_raw['Hillshade_3pm'] + train_raw['Hillshade_Noon'] + train_raw['Hillshade_9am']), (train_raw['Elevation']), c=colors, cmap = 'Paired')","53ae06ef":"pyplot.scatter(train_raw['Vertical_Distance_To_Hydrology'], train_raw['Horizontal_Distance_To_Hydrology'], c=colors, cmap = 'Paired')","147d1bfe":"import math\ncols = list(train_all.columns)\n\nfor data in [train_raw, test_all]:\n    data['Hillshade'] = data['Hillshade_9am'] + data['Hillshade_3pm'] + data['Hillshade_Noon']\n    data['binned_elev'] = [math.floor(v\/50.0) for v in data['Elevation']]\n    data['Elevation_Fire_Points'] = data['Elevation']+data['Horizontal_Distance_To_Fire_Points']\n    data['Road_Fire'] = data['Horizontal_Distance_To_Roadways'] + data['Horizontal_Distance_To_Fire_Points']\n    data['Road-Fire'] = data['Horizontal_Distance_To_Roadways'] - data['Horizontal_Distance_To_Fire_Points']\n    data['Ele_Road_Fire_Hydro'] = data['Elevation'] + data['Horizontal_Distance_To_Roadways']  + data['Horizontal_Distance_To_Fire_Points'] + data['Horizontal_Distance_To_Hydrology']\n    data['Ele-Road'] = data['Elevation'] + data['Horizontal_Distance_To_Roadways']\n    data['Ele_Road'] = data['Elevation'] - data['Horizontal_Distance_To_Roadways']\n    data['Ele-Fire'] = data['Elevation'] + data['Horizontal_Distance_To_Fire_Points']\n    data['Ele_Fire'] = data['Elevation'] - data['Horizontal_Distance_To_Fire_Points']\n    data['Ele_Hillshade'] = data['Elevation'] - data['Hillshade']\n    data['Ele-Hillshade'] = data['Elevation'] + data['Hillshade']\n    #None elevation combos:\n    data['Soil_W1'] = data['Soil_Type29'] + data['Wilderness_Area1']\n    data['Soil_W4'] = data['Wilderness_Area4'] + data['Soil_Type3']\n    data['Hydrology_Total'] = abs(data[\"Horizontal_Distance_To_Hydrology\"])+abs(data['Vertical_Distance_To_Hydrology'])\n    #Summary metrics\n    data[\"mean\"] = data[cols].mean(axis=1)\n    data[\"min\"] = data[cols].min(axis=1)\n    data[\"max\"] = data[cols].max(axis=1)\n    data[\"std\"] = data[cols].std(axis=1)","6c519de6":"train_all = train_raw.copy()\ntrain_all.drop(['Cover_Type'], axis=1, inplace=True)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(train_all, target, \n                                                                train_size=0.9, test_size=0.1, random_state=5, stratify = target)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc2 = RandomForestClassifier(random_state = 20, n_jobs = -1, n_estimators = 100, bootstrap = True, max_depth = 50,\n                            max_features = 0.5)\nrfc2.fit(X_train, Y_train)\n\npred_valid_rf2 = rfc2.predict(X_valid)\n\nfrom sklearn import metrics\nprint(metrics.accuracy_score(Y_valid, pred_valid_rf2))\n","a8b091ff":"from catboost import CatBoostClassifier\ncbc = CatBoostClassifier(random_state = 20, iterations = 3000, learning_rate = 0.03,od_wait = 1000,\n                         depth = 7, l2_leaf_reg = 3, eval_metric = 'Accuracy', verbose = 1000)\ncbc.fit(X_train, Y_train)\n\npred_valid_cbc = cbc.predict(X_valid)\nprint(metrics.accuracy_score(Y_valid, pred_valid_cbc))","e7a67e3e":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier(random_state = 20, n_jobs = -1, max_features = 'auto')\n\netc.fit(X_train, Y_train)\npred_valid_etc = etc.predict(X_valid)\npred_test_etc = etc.predict(test_all)\n\nprint(metrics.accuracy_score(Y_valid, pred_valid_etc))","d986aec6":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state=20, learning_rate = 0.4, objective ='multi:softprob', num_class = 7, eval_metric= 'merror',\n                   verbose = False)\n\nxgb.fit(X_train, Y_train)\npred_valid_xgb = xgb.predict(X_valid)\nprint(metrics.accuracy_score(Y_valid, pred_valid_xgb))","b8e0972f":"import lightgbm as lgb\n\nlb = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42, application = 'multiclass')\n\nlb.fit(X_train, Y_train)\n\npred_valid_lb = lb.predict(X_valid)\nprint(metrics.accuracy_score(Y_valid, pred_valid_lb))","2efc058c":"output = pd.DataFrame({'Id': test_all.index,\n                       'Cover_Type': pred_test_etc})\noutput.head()\noutput.to_csv('submission.csv', index=False)","a6c4ddfa":"**Forest Covertype Prediction Strategy**\n\n1. Perform brief exploratory analysis of all predictor variables, look for missing data, distribution of response, correlations. \n2. Fit a baseline random forest model based on the raw data. Analysis goal will be to better this model. \n3. Measure feature importances based on baseline model. Further examine data correlations and relationships between predictors. Use this information to perform basic feature engineering such as predictor transformations, combinations, and the creation of additional predictors. \n4. Split training data into a training and validation set. Fit multiple multivariate classification models based on modified data, evaluate model performance and tune parameters based on results from validation data. \n5. Select the best fitting model and use it predict on test data. \n","55362bcf":"Plot the distribution of cover types in the training data.","7d93e4be":"Again, we see strong relationships between Hillshade predictors, distance to Hydrology Predictors, Elevation\/Distance to roadways and fire points, and certain Wilderness areas with some soil types. \n\nScatter plots will be used to investigate some of these relationships and determine if variable transformation or combination would be appropriate.","6583a2d0":"Before doing anything else, split the training data into training and test, and evaluate performance of the untouched data. ","d06325c8":"Elevation has the largest share of variable importance, and the majority of the Soil Type variables contribute little to the model. \n\nNext, let's take another look a different look at variable correlations. ","d8d837de":"It looks like there is a perfectly even distribution of cover_types. I wonder if this distribution is maintained in the test data? \n\nNext, look at variable correlations. ","5b207a04":"After loading data and dropping the response from the training data, split the training data into training and validation sets (I use a 90\/10 split). Make sure to stratify the target incase the distribution of the response is skewed. ","593a6fb1":"Some high correlations between Hillshade variables, distance to hydrology. Makes sense since these variables seem interrelated. ","d6cd22ec":"More notes on variables. Elevation shows the most distinct differences between covertype groups, indicating that a binned elevation predictor may be useful. Also since the test data is much larger than the training data, and performs differently, I'm not going to remove any predictors. Instead I'm going to focus on creating new ones that highlight similarities in the data. I also want to combine elevation with other predictors.\n\nNote: I did not add all these new predictors at once. Instead I added a few at a time and then checked the results on the validation data. For most of the models, and especially the extra trees model, accuracy kept increasing as predictors were added. ","09776b83":"The raw data already performs extremely well on the validation data, with an accuracy of 89%. This is much higher than the scores people are reporting for test data accuracy, which leads me to suspect that the training data may not be representative of the test data (at least in covertype distribution). We also know that the test data is much larger than the training data. \n\nNext step: establish baseline performance on test data: ","92b493ee":"Soil_Type7 and Soil_Type15 both are not present from the training data, so they are dropped from both training and test. The good news is that there appear to be no missing values. ","317d99b2":"Try combining classifiers","f6b5066e":"Test data performs much worse than training data, with results showing 74.7% accuracy. Given the 14% difference in accuracy between the test data and validation data, will improving the training model (and validation score) actually lead to improvements in the test score? We'll find out. \n\nNext step: evaluate feature importance. "}}