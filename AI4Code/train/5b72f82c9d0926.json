{"cell_type":{"d298bde7":"code","1561501b":"code","990fb493":"code","5d40a7e1":"code","9871acfe":"code","b95dee90":"code","1a27bcc4":"code","cf48d23e":"code","0da793b0":"code","fe892358":"code","109ff273":"code","e471805c":"code","54dc9a18":"code","552cc7c5":"code","8c7f135b":"code","5155d446":"code","83c37ccc":"code","43b4e460":"code","66e70309":"code","58e3267b":"code","a671cd63":"code","c33d515d":"code","80553248":"code","b3c75fce":"code","1e3fcbb2":"code","de1faf8f":"code","07a7b2b7":"markdown","4f78925c":"markdown","99be1856":"markdown"},"source":{"d298bde7":"import os\nimport string\nimport numpy as np\nimport pandas as pd\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Model, Sequential, model_from_json\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.layers import Input, Dense, Dropout, Flatten, Lambda, Embedding\nfrom keras.initializers import RandomNormal, Constant\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras import regularizers\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimport matplotlib.pyplot as plt\n\nimport itertools\nimport pickle\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nimport seaborn as sns\n\nimport json\n\nimport warnings\n\nnp.random.seed(123)  # for reproducibility\n%matplotlib inline\n\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.set(font_scale=1)","1561501b":"def get_all_types_properties(all_data):\n    # Get all types of properties\n    all_types_properties = all_data['List of All Property Use Types at Property'].unique()\n    all_types_properties = [np.chararray.split(item, ',').tolist() for item in all_types_properties]\n    all_types_properties = list(itertools.chain.from_iterable(all_types_properties))\n    all_types_properties = list(set(all_types_properties))\n    all_types_properties = [item.lstrip() for item in all_types_properties]\n    all_types_properties = list(set(all_types_properties))\n    all_types_properties.remove('etc.)')\n\n    # Dataframe with all types of properties\n    data_types = np.zeros([all_data.shape[0], len(all_types_properties)], dtype = int)\n    data_types = pd.DataFrame(data = data_types, columns=all_types_properties)\n\n    properties = all_data['List of All Property Use Types at Property']\n\n    for item in data_types.columns:\n        data_types[item] = [1 if i in item else 0 for i in properties]\n\n    return data_types","990fb493":"def clean_data(use_text_columns = True):\n    cols_num = [\"2nd Largest Property Use - Gross Floor Area (ft\u00b2)\", \"3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)\",\n                \"DOF Gross Floor Area\", \"Diesel #2 Use (kBtu)\", \"Direct GHG Emissions (Metric Tons CO2e)\",\n                \"District Steam Use (kBtu)\", \"ENERGY STAR Score\", \"Electricity Use - Grid Purchase (kBtu)\", \n                \"Fuel Oil #1 Use (kBtu)\", \"Fuel Oil #2 Use (kBtu)\", \"Fuel Oil #4 Use (kBtu)\", \"Fuel Oil #5 & 6 Use (kBtu)\",\n                \"Indirect GHG Emissions (Metric Tons CO2e)\", \"Largest Property Use Type - Gross Floor Area (ft\u00b2)\",\n                \"Natural Gas Use (kBtu)\", \"Number of Buildings - Self-reported\", \"Occupancy\", \"Property GFA - Self-Reported (ft\u00b2)\",\n                \"Site EUI (kBtu\/ft\u00b2)\", \"Source EUI (kBtu\/ft\u00b2)\", \"Total GHG Emissions (Metric Tons CO2e)\",\n                \"Water Intensity (All Water Sources) (gal\/ft\u00b2)\", \"Water Use (All Water Sources) (kgal)\",\n                \"Weather Normalized Site EUI (kBtu\/ft\u00b2)\", \"Weather Normalized Site Electricity (kWh)\",\n                \"Weather Normalized Site Electricity Intensity (kWh\/ft\u00b2)\", \"Weather Normalized Site Natural Gas Intensity (therms\/ft\u00b2)\",\n                \"Weather Normalized Site Natural Gas Use (therms)\", \"Weather Normalized Source EUI (kBtu\/ft\u00b2)\", \"Year Built\"]\n\n    cols_text = [\"2nd Largest Property Use Type\", \"3rd Largest Property Use Type\", \"Largest Property Use Type\",\n                 \"Metered Areas  (Water)\", \"Metered Areas (Energy)\", \"Primary Property Type - Self Selected\"]   \n    \n    df_train = pd.read_csv('..\/input\/dataset_treino.csv')\n    len_train_data = len(df_train)\n    df_test = pd.read_csv('..\/input\/dataset_teste.csv')\n    df_test['ENERGY STAR Score'] = np.nan\n    \n    all_data = pd.concat([df_train, df_test], ignore_index=True)\n\n    data_numeric = all_data[cols_num]\n    # Replace values Not Available to zero and convert type from object to numeric\n    data_numeric = data_numeric.replace('Not Available', 0).apply(pd.to_numeric)\n    \n    # Fill NAN values to median\n    for col in cols_num:\n        data_numeric[col].fillna(data_numeric[col].median(), inplace=True)\n\n    if (use_text_columns):\n        data_text = all_data[cols_text]\n        data_types = get_all_types_properties(all_data)\n\n        # Transform text into numbers\n        label_encoder = LabelEncoder()\n        for column in data_text.columns:\n            data_text[column] = label_encoder.fit_transform(data_text[column])\n\n        complete_data = pd.concat([data_numeric, data_types, data_text], axis = 1)\n\n        df_train = complete_data.iloc[:len_train_data,:]\n        df_test = complete_data.iloc[len_train_data:,:]\n    else:\n        df_train = data_numeric.iloc[:len_train_data,:]\n        df_test = data_numeric.iloc[len_train_data:,:]\n\n    return df_train, df_test","5d40a7e1":"train, test = clean_data(use_text_columns = True)\ndata = pd.concat([train, test], ignore_index=True)\n\n# Info method provides information about dataset like \n# total values in each column, null\/not null, datatype, memory occupied etc\ndata.info()","9871acfe":"# How many columns with different datatypes are there?\ndata.get_dtype_counts()","b95dee90":"## Describe gives statistical information about numerical columns in the dataset\ndata.describe()","1a27bcc4":"data.head()","cf48d23e":"corr=data.corr()[\"ENERGY STAR Score\"]\ncorr[np.argsort(corr, axis=0)[::-1]]","0da793b0":"#plotting correlations\nnum_feat=data.columns[data.dtypes!=object]\nnum_feat=num_feat[1:-1] \nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(data[col].values, data['ENERGY STAR Score'].values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,15))\nrects = ax.barh(ind, np.array(values), color='red')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients w.r.t ENERGY STAR Score\")","fe892358":"correlations=data.drop(['ENERGY STAR Score'], axis=1).corr()\nattrs = correlations.iloc[:-6,-7:] # all except target\n\ncorrelations","109ff273":"correlations=data.drop(['ENERGY STAR Score'], axis=1).corr() # all except target\nattrs = correlations \n\nthreshold = 0.5\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), \n        columns=['Attribute Pair', 'Correlation'])\n\n    # sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['Correlation']).argsort()[::-1]]\n\nunique_important_corrs","e471805c":"corrMatrix=data[[\"ENERGY STAR Score\", \"2nd Largest Property Use - Gross Floor Area (ft\u00b2)\",\"3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)\",\n                \"DOF Gross Floor Area\",\"Diesel #2 Use (kBtu)\",\"Direct GHG Emissions (Metric Tons CO2e)\",\n                \"District Steam Use (kBtu)\", \"Electricity Use - Grid Purchase (kBtu)\",\"Fuel Oil #1 Use (kBtu)\",\"Fuel Oil #2 Use (kBtu)\",\n                \"Fuel Oil #4 Use (kBtu)\",\"Fuel Oil #5 & 6 Use (kBtu)\",\"Indirect GHG Emissions (Metric Tons CO2e)\",\n                \"Largest Property Use Type - Gross Floor Area (ft\u00b2)\",\"Natural Gas Use (kBtu)\",\n                \"Number of Buildings - Self-reported\",\"Occupancy\",\"Property GFA - Self-Reported (ft\u00b2)\",\n                \"Site EUI (kBtu\/ft\u00b2)\",\"Source EUI (kBtu\/ft\u00b2)\",\"Total GHG Emissions (Metric Tons CO2e)\",\n                \"Water Intensity (All Water Sources) (gal\/ft\u00b2)\",\"Water Use (All Water Sources) (kgal)\",\n                \"Weather Normalized Site EUI (kBtu\/ft\u00b2)\",\"Weather Normalized Site Electricity (kWh)\",\n                \"Weather Normalized Site Electricity Intensity (kWh\/ft\u00b2)\",\"Weather Normalized Site Natural Gas Intensity (therms\/ft\u00b2)\",\n                \"Weather Normalized Site Natural Gas Use (therms)\",\"Weather Normalized Source EUI (kBtu\/ft\u00b2)\", \"Year Built\"]].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(30, 30))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features')","54dc9a18":"sns.set(rc={'figure.figsize':(9,7)})\n_ = sns.distplot(data['ENERGY STAR Score'])","552cc7c5":"data['ENERGY STAR Score'] = data['ENERGY STAR Score'] \/ 100\n_ = sns.distplot(data['ENERGY STAR Score'])","8c7f135b":"fig, ax = plt.subplots()\nax.scatter(x = data['Site EUI (kBtu\/ft\u00b2)'], y = data['ENERGY STAR Score'])\nplt.ylabel('ENERGY STAR Score', fontsize=13)\nplt.xlabel('Site EUI (kBtu\/ft\u00b2)', fontsize=13)\nplt.show()","5155d446":"fig, ax = plt.subplots()\nax.scatter(x = data['Electricity Use - Grid Purchase (kBtu)'], y = data['ENERGY STAR Score'])\nplt.ylabel('ENERGY STAR Score', fontsize=13)\nplt.xlabel('Electricity Use - Grid Purchase (kBtu)', fontsize=13)\nplt.show()","83c37ccc":"train, test = clean_data(use_text_columns = True)","43b4e460":"def load_train_data():\n    X_train = train.drop([\"ENERGY STAR Score\"], axis=1) # Features\n    y_train = train[\"ENERGY STAR Score\"] \/ 100 # Targets\n    X_train = StandardScaler().fit_transform(X_train)\n\n    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n\n    return (X_train, y_train), (X_test, y_test)","66e70309":"def load_test_data():      \n    X_test = test.drop([\"ENERGY STAR Score\"], axis=1) # Features\n    X_test = StandardScaler().fit_transform(X_test)\n\n    return X_test","58e3267b":"# Show info of model\ndef show_info(model, X, y, log, weights = None):\n    warnings.filterwarnings('ignore')\n    if (log != None):\n        # summarize history for loss\n        plt.figure(figsize=(15,10))\n        plt.plot(log.history['loss'])\n        plt.plot(log.history['val_loss'])\n        plt.title('Model Loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()\n\n    if (weights != None):\n        model.load_weights(weights)\n\n    score = model.evaluate(X, y, verbose=0)\n    print('Mean Absolute Error {:2.4f}'.format(score * 100))","a671cd63":"def create_model():\n    initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)\n\n    model = Sequential()\n    model.add(Dense(128, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer=initializer, kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(Dropout(0.4))\n    model.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer=initializer, kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(Dense(1, activation=\"linear\", kernel_initializer=initializer))\n    adam = Adam(lr=1e-4, decay=1e-4 \/ 50)\n\n    # Compile model\n    model.compile(loss=\"mean_absolute_error\", optimizer=adam)\n\n    return model","c33d515d":"batch_size = 10\nnb_epoch = 300\n\nprint('Loading data...')\n(X_train, y_train), (X_test, y_test) = load_train_data()\n\nprint('Build model...')\nmodel = create_model()\nmodel.summary()","80553248":"print('Fit model...')\nfilepath=\"weights_energy.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='min')\ncallbacks_list = [checkpoint]\n\nlog = model.fit(X_train, y_train,\n          validation_split=0.2, batch_size=batch_size, epochs=nb_epoch, shuffle=True, callbacks=callbacks_list)","b3c75fce":"show_info(model, X_test, y_test, log, weights='weights_energy.best.hdf5')","1e3fcbb2":"test_data = load_test_data()\n\ndf_teste = pd.read_csv('..\/input\/dataset_teste.csv')","de1faf8f":"predict = model.predict(test_data)\npredict = predict * 100\npredict = predict.astype(int)\n\nsubmission = pd.DataFrame()\nsubmission['Property Id'] = df_teste[\"Property Id\"]\nsubmission['score'] = predict\n\nsubmission['score'] = submission['score'].apply(lambda x: 100 if x > 100 else x)\nsubmission['score'] = submission['score'].apply(lambda x: 0 if x < 0 else x)\n\nsubmission.to_csv('submission.csv', index=False)","07a7b2b7":"## Exploratory Analisys","4f78925c":"## Correlation in Data","99be1856":"## Heatmap"}}