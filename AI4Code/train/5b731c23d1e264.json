{"cell_type":{"62cdb002":"code","82a7285b":"code","1728743b":"code","1e2f2d57":"code","f53563ec":"code","d62b36c8":"code","0d247259":"code","0adacbe5":"code","7e983b6f":"code","69106534":"code","808a235e":"code","0ec81b6d":"code","e0f5b2f4":"code","9631adee":"code","cc107fe5":"code","3c39eb64":"code","77af7e75":"code","02bbae34":"code","084d7552":"code","da14052d":"markdown","3b3837f9":"markdown","1db98e43":"markdown","1bd55ace":"markdown","aad00e0f":"markdown","1104ca85":"markdown","ff35cf14":"markdown"},"source":{"62cdb002":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82a7285b":"import re\nimport string","1728743b":"temp_df=pd.read_csv('\/kaggle\/input\/indonesiandata1\/data-indonesian-tweets.csv')","1e2f2d57":"temp_df","f53563ec":"temp_df.emotion.value_counts()","d62b36c8":"temp_df.drop(list(temp_df.loc[temp_df['emotion']=='no_emotion'].index),inplace=True)\nprint(temp_df.emotion.value_counts())","0d247259":"from nltk.corpus import stopwords\nimport nltk\nstopwords.fileids()","0adacbe5":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('indonesian')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","7e983b6f":"temp_df['content']=temp_df['content'].apply(str).apply(lambda x: text_preprocessing(x))\n","69106534":"def rep(text):\n    grp = text.group(0)\n    if len(grp) > 1:\n        return grp[0:1] # can change the value here on repetition\n    return grp\n   \ndef unique_char(rep,sentence):\n    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n    return convert\n\ntemp_df['content']=temp_df['content'].apply(lambda x : unique_char(rep,x))","808a235e":"temp_df.drop(['Unnamed: 0'],axis=1,inplace=True)\ntemp_df.head()","0ec81b6d":"df=pd.DataFrame(columns=['content','emotion'])\nfor i in  temp_df.emotion.unique():\n    emotion_c=temp_df.loc[temp_df['emotion']==str(i)][:90]\n    df=df.append(emotion_c)\ndf=df.sample(frac=1)\ndf","e0f5b2f4":"temp_df=pd.DataFrame()\nls=[]\ndef token_after_clean(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    tokenized_text = tokenizer.tokenize(text)\n    ls.append(tokenized_text)\n    return tokenized_text\n\n\ndf['content']=df['content'].apply(str).apply(lambda x: token_after_clean(x))\n\n\ndf","9631adee":"from gensim.models import FastText\nmodel = FastText(size=150, window=3, min_count=1)\nmodel.build_vocab(sentences=df.content)\nmodel.train(sentences=df.content, total_examples=len(df.content), epochs=50)  # train","cc107fe5":"model.wv['ketua'] #ketua in english is chairman","3c39eb64":"def l2_norm(x):\n    return np.sqrt(np.sum(x**2))\n\ndef div_norm(x):\n    norm_value = l2_norm(x)\n    if norm_value > 0:\n        return x * ( 1.0 \/ norm_value)\n    else:\n        return x\n    \nfinal_vector=[]\ndef sentence_builder(ls):\n    for i in ls:\n\n        fast_sentence=0\n        for j in i:\n            v1=model.wv[str(j)]\n            fast_sentence+=div_norm(v1)\n        sentence_vector=(fast_sentence)\/len(i)\n        final_vector.append(sentence_vector)\n    return final_vector\n\nk=sentence_builder(ls) # ls is the lsit we developed earlier        ","77af7e75":"from sklearn.cluster import KMeans\ntrue_k = 8\nk_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nk_model.fit(k)","02bbae34":"order_centroids = k_model.cluster_centers_.argsort()[:, ::-1]\nterms=list(model.wv.vocab.keys())","084d7552":"for i in range(true_k):\n    print('\\n')\n    print(\"Cluster %d:\" % i)\n    for ind in order_centroids[i, :10]:\n        print(\"%s\"% terms[ind])","da14052d":"## fasttext way of building sentence vector\n\nrefered [here](https:\/\/stackoverflow.com\/questions\/54181163\/fasttext-embeddings-sentence-vectors)","3b3837f9":"## Next Step :-\n1. convert to english top n words of each cluster to understand what each cluster is pointing to for help we can compare our translations to visualisation of top n grams of each emotion\n    done in visualtion kernel.\n2. Suppose Cluster 8 represents **Disgust** now after testing we can compare how many sentences are correctly classified.\n\n3. We can use **Accuracy**as metric because here we have balanced Dataset.","1db98e43":"## to  get centroids of each cluster","1bd55ace":"# Remove repititive characters","aad00e0f":"# Selecting 90 rows from each emotion\n## i.e k=90\n","1104ca85":"Example of word vector developed after training","ff35cf14":"# clustering"}}