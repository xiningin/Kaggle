{"cell_type":{"10b7891e":"code","f7e2d348":"code","30e1ab4c":"code","b8bbcc16":"code","09f09f75":"code","bfb053fa":"code","4b703b22":"code","79211058":"code","1eec8771":"code","a3901de5":"code","665b218a":"code","01aae2ea":"code","146ebd53":"code","6beadc8c":"code","d1593886":"code","1eef52d7":"code","81261d3b":"code","7982052d":"code","b9e16bee":"code","dcbdce6f":"code","5b0ba55f":"code","5b1947e2":"code","7610e8eb":"code","343b3424":"code","ebca746e":"code","a3b39db8":"code","fe51d5d7":"markdown","e3c5f1b6":"markdown","21b1605b":"markdown","dec2e8fd":"markdown","312ee224":"markdown","495c8a50":"markdown","5e0da76e":"markdown"},"source":{"10b7891e":"from time import time\nimport psutil\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import RobustScaler\n\nimport tensorflow.keras as ks\nfrom tensorflow.keras import optimizers\nimport tensorflow as tf\n\n!pip install plot-keras-history\nfrom plot_keras_history import plot_history\n\nfrom scipy.special import erfinv\n\nimport gc\ngc.collect()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f7e2d348":"class NBConfig:\n    \n    general = {\n        \"seed\": 2022,\n        \"folds\": 7,\n        \"rounding\": False\n    }\n    opt = {\n      \"leRa\": 0.0025,\n      \"dec\": 0.000,\n      \"clip\": 100\n    }\n    nn = {\n      \"eps\": 400,\n      \"bs\": 512+128+64\n    }\n    compiler = {\n      \"loss\": \"mae\",\n      \"metric\": \"mae\"\n    }\n\nprint(\"Learningrate ok: \" + str(NBConfig.opt[\"leRa\"] - NBConfig.opt[\"dec\"] * NBConfig.nn[\"eps\"] >= 0))","30e1ab4c":"trainpath = \"..\/input\/ventilator-pressure-prediction\/train.csv\"\ntestpath = \"..\/input\/ventilator-pressure-prediction\/test.csv\"\nsamsubpath = \"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\"\ntrain, test, samSub = pd.read_csv(trainpath, index_col=\"id\"), pd.read_csv(testpath, index_col=\"id\"), pd.read_csv(samsubpath)","b8bbcc16":"train[\"timeDiff\"] = train[\"time_step\"].groupby(train[\"breath_id\"]).diff(1).fillna(0)\ntest[\"timeDiff\"] = test[\"time_step\"].groupby(test[\"breath_id\"]).diff(1).fillna(0)","09f09f75":"%%time\ntrain[\"maxu_in\"] = train[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").transform(\"max\")[\"u_in\"]\ntest[\"maxu_in\"] = test[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").transform(\"max\")[\"u_in\"]\ntrain[\"meanu_in\"] = train[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").transform(\"mean\")[\"u_in\"]\ntest[\"meanu_in\"] = test[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").transform(\"mean\")[\"u_in\"]\n\ngc.collect() ","bfb053fa":"%%time\ntrain[\"R\"] = train[\"R\"].astype(\"str\")\ntrain[\"C\"] = train[\"C\"].astype(\"str\")\ntrain[\"R__C\"] = train[\"R\"].astype(\"str\") + '__' + train[\"C\"].astype(\"str\")\ntrain = pd.get_dummies(train)\ngc.collect() \n\ntest[\"R\"] = test[\"R\"].astype(\"str\")\ntest[\"C\"] = test[\"C\"].astype(\"str\")\ntest[\"R__C\"] = test[\"R\"].astype(\"str\") + '__' + test[\"C\"].astype(\"str\")\ntest = pd.get_dummies(test)\ngc.collect() ","4b703b22":"def ByBreath(method: str, DF, lags=None, center=False, fillNas=0):\n    \n    start = time()\n\n    output = pd.DataFrame()\n    if center == True:\n        c = \"c\"\n    else:\n        c = \"\"\n    \n    if method == \"mean\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\", \"u_out\"]].groupby(\"breath_id\").rolling(window=l, center=center).mean().fillna(fillNas)\n            output[[\"{0}mu_in_l{1}\".format(c, l), \"{0}mu_out_l{1}\".format(c, l)]] = agg[[\"u_in\", \"u_out\"]]\n            gc.collect()\n            \n    elif method == \"max\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(window=l, center=center).max().fillna(fillNas)  \n            output[[\"{0}mxu_in_l{1}\".format(c, l)]] = agg[[\"u_in\"]]\n            gc.collect()\n            \n    elif method == \"min\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(window=l, center=center).min().fillna(fillNas)  \n            output[[\"{0}miu_in_l{1}\".format(c, l)]] = agg[[\"u_in\"]]\n            gc.collect()\n            \n    elif method == \"std\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(window=l, center=center).std().fillna(fillNas)  \n            output[\"{0}su_in_l{1}\".format(c, l)] = agg[\"u_in\"]\n            gc.collect()\n            \n    elif method == \"shift\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\", \"u_out\"]].groupby(\"breath_id\").shift(l).fillna(fillNas)  \n            output[[\"sftu_in_l{0}\".format(l), \"sftu_out_l{0}\".format(l)]] = agg[[\"u_in\", \"u_out\"]]\n            gc.collect()     \n        \n    elif method == \"diff\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").diff(l).fillna(fillNas)  \n            output[\"du_in_l{0}\".format(l)] = agg[\"u_in\"]\n            gc.collect()  \n            \n    elif method == \"log\":\n        output[\"lgu_in\"] = np.log1p(DF[\"u_in\"].values)\n        gc.collect()  \n        \n    elif method == \"cumsum\":\n            agg = \\\n            DF[[\"breath_id\", \"u_in\", \"u_out\"]].groupby(\"breath_id\").cumsum() \n            output[[\"csu_in\", \"csu_out\"]] = agg[[\"u_in\", \"u_out\"]]\n            gc.collect()   \n            \n    elif method == \"area\":\n            agg = \\\n            DF[[\"time_step\", \"u_in\", \"breath_id\"]]\n            agg[\"area\"] = agg[\"time_step\"] * agg[\"u_in\"]\n            output[\"area\"] = agg.groupby(\"breath_id\")[\"area\"].cumsum()\n            gc.collect()   \n            \n    elif method == \"centering\":\n            agg = \\\n            DF[[\"u_in\", \"breath_id\"]].groupby(\"breath_id\").transform('mean')#does not aggregate like just mean()\n            output[\"cenu_in\"] = DF[\"u_in\"] - agg[\"u_in\"]\n            gc.collect()  \n    end = time()\n    print(c + method + \" created in \" + str(round(end - start)) + \" seconds.\" + \"RAM usage: \" + str(psutil.virtual_memory()[2]) + \"%\")\n    return output","79211058":"def assignment(DF, mDF):\n    DF = DF.copy()\n    colNames = mDF.columns\n    for n in colNames:\n        DF[\"{0}\".format(n)] = mDF[\"{0}\".format(n)].values\n    gc.collect()\n    return DF","1eec8771":"train = assignment(train, ByBreath(\"area\", train))\n#train = assignment(train, ByBreath(\"mean\", train, lags=[6,9]))\ntrain = assignment(train, ByBreath(\"mean\", train, center=True, lags=[9]))\n#train = assignment(train, ByBreath(\"max\", train, lags=[9]))\n#train = assignment(train, ByBreath(\"min\", train, lags=[9]))\ntrain = assignment(train, ByBreath(\"diff\", train, lags=[1,2,3,4]))\ntrain = assignment(train, ByBreath(\"log\", train))\n#train = assignment(train, ByBreath(\"std\", train, lags=[6]))\ntrain = assignment(train, ByBreath(\"shift\", train, lags=[-3,-2,-1,1,2,3,4]))\ntrain = assignment(train, ByBreath(\"cumsum\", train))\ntrain = assignment(train, ByBreath(\"centering\", train))\n\ntest = assignment(test, ByBreath(\"area\", test))\n#test = assignment(test, ByBreath(\"mean\", test, lags=[6,9]))\ntest = assignment(test, ByBreath(\"mean\", test, center=True, lags=[9]))\n#test = assignment(test, ByBreath(\"max\", test, lags=[9]))\n#test = assignment(test, ByBreath(\"min\", test, lags=[9]))\ntest = assignment(test, ByBreath(\"diff\", test, lags=[1,2,3,4]))\ntest = assignment(test, ByBreath(\"log\", test))\n#test = assignment(test, ByBreath(\"std\", test, lags=[6]))\ntest = assignment(test, ByBreath(\"shift\", test, lags=[-3,-2,-1,1,2,3,4]))\ntest = assignment(test, ByBreath(\"cumsum\", test))\ntest = assignment(test, ByBreath(\"centering\", test))","a3901de5":"print(\"train shape is: \" + str(train.shape))\nprint(\"test shape is: \" + str(test.shape))\ntrain.head()","665b218a":"train.reset_index(drop=True, inplace=True)\ntarget = train.pressure\nuniTarg = np.array(sorted(target.unique()))\nnames = [c for c in train.columns if c not in [\"breath_id\", \"u_out\", \"pressure\", \"R\", \"C\", \"sftu_out_l-1\", \"sftu_out_l-2\", \"sftu_out_l-3\", \"sftu_out_l-4\"]]\ntrain.head()","01aae2ea":"RS = RobustScaler()\nRS.fit(pd.concat([train[names], test[names]]))\ngc.collect()\ntrain = RS.transform(train[names])                               \ntest = RS.transform(test[names])\ndel(RS)\ngc.collect()\nprint('RAM memory used after scaling:', psutil.virtual_memory()[2], \"%\")","146ebd53":"def rounder(df, rl):\n    for i in rl:\n        df[i] = np.round(df[i] ,decimals=5)\n    return df","6beadc8c":"if NBConfig.general[\"rounding\"] == True:\n    roundings = [i for i, n in enumerate(names) if n not in ['u_out','R_20','R_5','R_50','C_10','C_20','C_50','R__C_20__10','R__C_20__20','R__C_20__50','R__C_50__10','R__C_50__20','R__C_50__50','R__C_5__10','R__C_5__20','R__C_5__50']]\n    train=rounder(train, roundings)\n    test=rounder(test, roundings)\n    gc.collect()\n    print('RAM memory used after rounding:', psutil.virtual_memory()[2], \"%\")","d1593886":"test = test.reshape((int(test.shape[0]\/80), 80, -1))# samples, timesteps, features\ngc.collect()\n\ntarget = target.to_numpy()\ntarget = target.reshape((int(target.shape[0]\/80), 80, 1))# samples, timesteps, features\ngc.collect()\n\ntrain = train.reshape((int(train.shape[0]\/80), 80, -1))# samples, timesteps, features\ngc.collect()\n\nprint('RAM memory used after reshaping:', psutil.virtual_memory()[2], \"%\")\ntrain.shape","1eef52d7":"print(tf.version.VERSION)\ntry: # detect TPU\n    tpu = None\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError: # detect GPU(s) and enable mixed precision\n    strategy = tf.distribute.MirroredStrategy() # works on GPU and multi-GPU\n    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    tf.config.optimizer.set_jit(True) # XLA compilation\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n    print('Mixed precision enabled')\ngc.collect()\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","81261d3b":"def LSTM_Model(strategy, dfForShape, seeds, optimizer, loss, metric, bs, cellState = False):\n    with strategy.scope():\n        np.random.seed(seeds)\n        tf.random.set_seed(seeds)\n\n        if cellState == True:\n            INPUT = ks.layers.Input(batch_input_shape=(bs, dfForShape.shape[1], dfForShape.shape[2]), name=\"input\")\n        else:\n            INPUT = ks.layers.Input(shape=(dfForShape.shape[1], dfForShape.shape[2]), name=\"input\")\n\n        L = ks.layers.Bidirectional( \n                ks.layers.LSTM(\n                    500, \n                    #kernel_initializer='LecunUniform',\n                    activation = \"tanh\", \n                    return_sequences = True,\n                    stateful=cellState,\n                    name=\"L1\"\n                )\n            )(INPUT)\n\n        L = ks.layers.Bidirectional( \n                ks.layers.LSTM(\n                    250, \n                    #kernel_initializer='LecunUniform',\n                    activation = \"tanh\", \n                    return_sequences = True, \n                    stateful=cellState,\n                    name=\"L2\"\n                )\n            )(L)\n\n        L3 = ks.layers.Bidirectional( \n                ks.layers.LSTM(\n                    125, \n                    #kernel_initializer='LecunUniform',\n                    activation = \"tanh\", \n                    return_sequences = True, \n                    stateful=cellState,\n                    name=\"L3\"\n                )\n            )(L)  \n\n        D = ks.layers.Dense(\n                125*2, \n                activation = \"selu\",\n                kernel_initializer='LecunUniform',\n                name=\"dense1\"\n            )(L3)\n\n        A = ks.layers.Average()([D, L3])\n\n        OUT = ks.layers.Dense(\n                1, \n                activation = \"linear\",\n                #kernel_initializer='LecunUniform',\n                name=\"L_out\"\n            )(A)\n\n        m = ks.Model(inputs=INPUT, outputs=OUT)\n        \n        m.compile(\n            optimizer = optimizer, \n            loss = loss, \n            metrics=metric\n        )\n\n        m.summary()\n        gc.collect()\n        print('RAM memory used after the configuration of LSTM:', psutil.virtual_memory()[2], \"%\")\n\n    return m","7982052d":"def lr_schaker(epoch, lr):\n    if epoch == 25:\n        lr = lr*1.05\n    elif epoch == 50:\n        lr = lr*1.04\n    elif epoch == 75:\n        lr = lr*1.03\n    elif epoch == 100:\n        lr = lr*1.02\n    elif epoch == 130:\n        lr = lr*1.02\n    elif epoch == 150:\n        lr = lr*1.02\n    elif epoch == 200:\n        lr = lr*1.03\n    elif epoch == 250:\n        lr = lr*1.04\n    elif epoch == 300:\n        lr = lr*1.05\n    elif epoch == 350:\n        lr = lr*1.06\n    return lr","b9e16bee":"kf = KFold(\n    n_splits=NBConfig.general[\"folds\"], \n    random_state=NBConfig.general[\"seed\"], \n    shuffle=True\n)","dcbdce6f":"int(train.shape[0]*4\/5\/80)","5b0ba55f":"trainPahse = True\npreds = []\n\nfor k, (train_index, test_index) in enumerate(kf.split(train)):\n    \n    print(\"Fold: \" + str(k+1))\n    X, x = train[train_index], train[test_index]\n    Y, y = target[train_index], target[test_index]  \n    print(\"Train data has \" + str(X.shape[0]) + \" observations\" + \\\n          \"\\n\" + \"Test data has \" + str(x.shape[0]) + \" observations\")\n    print('RAM memory used after setting train and test:', psutil.virtual_memory()[2], \"%\")\n    \n#model\n    if trainPahse == True:\n        ks.backend.clear_session()\n        \n        checkpoint = f\"folds_{k}.hdf5\"\n        \n        sv = ks.callbacks.ModelCheckpoint(\n            checkpoint, \n            monitor='val_loss', \n            verbose=0, \n            save_best_only=True,\n            save_weights_only=False, \n            mode='min', \n            save_freq='epoch',\n            options=None\n        )\n        \n        lrReducer = ks.callbacks.ReduceLROnPlateau(    \n            monitor=\"val_loss\",\n            factor=0.6,\n            patience=15,\n            verbose=1,\n            mode=\"min\"\n        ) \n        \n        stop = ks.callbacks.EarlyStopping(\n            monitor='val_loss',\n            mode='min', \n            patience=100, \n            verbose=1,\n            restore_best_weights=True\n        )\n        \n        optimizer = ks.optimizers.Adam(\n            lr=NBConfig.opt[\"leRa\"], \n            decay=NBConfig.opt[\"dec\"], \n            clipvalue=NBConfig.opt[\"clip\"]\n        )\n\n        model = LSTM_Model(\n            strategy, \n            test, \n            NBConfig.general[\"seed\"], \n            optimizer, \n            NBConfig.compiler[\"loss\"], \n            NBConfig.compiler[\"metric\"], \n            NBConfig.nn[\"bs\"]\n        )\n        \n        #TRAINING\n        history = model.fit(\n            x=X, \n            y=Y,\n            validation_data=(x, y),\n            epochs = NBConfig.nn[\"eps\"], \n            batch_size = NBConfig.nn[\"bs\"], \n            shuffle = False,\n            callbacks=[sv, stop, lrReducer, ks.callbacks.LearningRateScheduler(lr_schaker, verbose=0)],\n            verbose=1\n        )\n        \n        plot_history(history)\n        plt.show()\n        \n        model = ks.models.load_model('.\/'+ checkpoint)\n        print(checkpoint + \" successfully loaded.\")\n        \n        prediction = np.reshape(model.predict(x=test, batch_size = NBConfig.nn[\"bs\"]), samSub.shape[0]) \n\n        preds.append(prediction)\n        \n        gc.collect()","5b1947e2":"samSub.pressure = np.median(np.array(preds), axis=0)","7610e8eb":"samSub.describe()","343b3424":"samSub[[\"id\", \"pressure\"]].to_csv(\"sampleSubmission.csv\", index=False)","ebca746e":"%%time\nsamSub[\"pressure\"] = samSub.pressure.map(lambda x: uniTarg[np.argmin(((uniTarg - x)**2))])","a3b39db8":"samSub[[\"id\", \"pressure\"]].to_csv(\"sampleSubmissionPP.csv\", index=False)\nsamSub.head()","fe51d5d7":"# ML Approach\n\nNormalize:","e3c5f1b6":"**Normalize**","21b1605b":"# LSTM","dec2e8fd":"**Reshape data:**","312ee224":"## Submission","495c8a50":"Detect hardware, return appropriate distribution strategy","5e0da76e":"**Rounder**"}}