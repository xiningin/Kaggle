{"cell_type":{"70b5a387":"code","f5f9905f":"code","0eccd07d":"code","30d5f6a7":"code","9c8d2086":"code","20a4bb51":"code","cf6f795f":"code","bacaf89e":"code","6b4a9af1":"code","01ee94eb":"code","2e26a714":"code","4331d1a8":"code","9ad8d2b8":"code","1a423c23":"code","e97ec08c":"code","ca6a3197":"code","803141ed":"code","7cdb54ec":"code","ee3e8d67":"code","f2243560":"code","b5e8fe5d":"code","076175d8":"code","507dd8d3":"code","3ed60d4d":"code","861d945c":"code","64ec4dca":"code","c7cd59c5":"code","10c11e34":"code","80cfae1d":"code","d0aaec1b":"code","a4486157":"code","80c6ae1b":"code","c2fcf4f3":"code","df5ddd3d":"code","7ce7a440":"code","e6018559":"code","5ab4b34e":"code","7537069e":"code","486640ee":"code","b213a274":"code","e9d97592":"code","6e93a684":"code","072f9004":"code","3278d351":"code","fe3869e1":"code","a09ef771":"code","ac3c169e":"code","31d02c85":"code","bc130ed6":"code","8d099416":"code","bd134464":"code","2c313502":"code","d00fe5f6":"code","abc89ae8":"code","ec7139a4":"code","04561e0f":"code","816b362a":"code","7e6dae1c":"markdown","999e2d15":"markdown","3f03f813":"markdown","9e6e7d26":"markdown","4109fbf6":"markdown","df4f29de":"markdown","e136259b":"markdown","430f7fed":"markdown","ac4eddb3":"markdown","f9d85121":"markdown","86832fe7":"markdown","24542477":"markdown","4aa2a2e8":"markdown","f5a52a75":"markdown","e5ebf5fa":"markdown","ad02a8d4":"markdown","afc81c1c":"markdown","0429906f":"markdown","93eb6813":"markdown","c754f024":"markdown","325525e0":"markdown","2b3c29d5":"markdown","0106afb0":"markdown","dfd8c68c":"markdown","41b9470a":"markdown","0f0e6268":"markdown","ba7ce481":"markdown","2deaed80":"markdown","13709c0c":"markdown","ac0b7a89":"markdown","e80c1729":"markdown","73b87442":"markdown","ba77b7da":"markdown","d3469aff":"markdown","4fa3b87f":"markdown","19679cad":"markdown","bfc40f2a":"markdown","1f16a7a2":"markdown","2bf44c7e":"markdown","f896adb8":"markdown","718b37ab":"markdown","1371cb15":"markdown","543368eb":"markdown","fcadd447":"markdown","90080c68":"markdown","f27e19e6":"markdown","896ef906":"markdown","18043f60":"markdown","22453ef7":"markdown","2877304d":"markdown","3a066267":"markdown","e607475b":"markdown","59bb7e5b":"markdown","65fba5e9":"markdown","3d636844":"markdown","999cf8c9":"markdown","701562a1":"markdown","78414358":"markdown"},"source":{"70b5a387":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import VotingClassifier\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f5f9905f":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\ndata = pd.concat([train, test])\n%matplotlib inline\ndata.columns","0eccd07d":"train.info()","30d5f6a7":"train.describe()","9c8d2086":"train.describe().columns","20a4bb51":"varNum = train[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nvarCat = train[[\"Survived\", \"Pclass\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]]","cf6f795f":"for i in varNum:\n    plt.hist(varNum[i])\n    plt.title(i)\n    plt.show()","bacaf89e":"print(varNum.corr())\nsns.heatmap(varNum.corr())","6b4a9af1":"pd.pivot_table(train, index = \"Survived\", values = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])","01ee94eb":"for i in varCat.columns:\n    sns.barplot(varCat[i].value_counts().index,varCat[i].value_counts()).set_title(i)\n    plt.show()","2e26a714":"print(pd.pivot_table(train, index = 'Survived', columns = 'Pclass',\n                     values = 'Ticket', aggfunc = 'count'))\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Sex',\n                     values = 'Ticket', aggfunc = 'count'))\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Embarked',\n                     values = 'Ticket', aggfunc = 'count'))","4331d1a8":"train['cabin_multiple'] = train.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntrain['cabin_multiple'].value_counts()","9ad8d2b8":"print(pd.pivot_table(train, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket', aggfunc = 'count'))","1a423c23":"train['cabin_adv'] = train.Cabin.apply(lambda x: str(x)[0])\nprint(train.cabin_adv.value_counts())\nprint(pd.pivot_table(train, index = 'Survived', columns = 'cabin_adv', values = 'Ticket', aggfunc = 'count'))","e97ec08c":"train['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain['ticket_letter'] = train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() \n                                            if len(x.split(' ')[:-1]) > 0 else 0)\ntrain['numeric_ticket'].value_counts()","ca6a3197":"pd.set_option('max_rows', None)\ntrain['ticket_letter'].value_counts()","803141ed":"pd.pivot_table(train, index = 'Survived', columns = 'numeric_ticket', \n               values = 'Ticket', aggfunc = 'count')","7cdb54ec":"pd.pivot_table(train, index = 'Survived', columns = 'ticket_letter', \n               values = 'Ticket', aggfunc = 'count')","ee3e8d67":"train.Name.head(20)","f2243560":"train['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntrain['name_title'].value_counts()","b5e8fe5d":"data['cabin_multiple'] = data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ndata['cabin_adv'] = data.Cabin.apply(lambda x: str(x)[0])\ndata['numeric_ticket'] = data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ndata['ticket_letters'] = data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)\ndata['name_title'] = data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","076175d8":"data.Age = data.Age.fillna(train.Age.mean())\ndata.Fare = data.Fare.fillna(train.Fare.median())","507dd8d3":"data.dropna(subset=['Embarked'], inplace = True)","3ed60d4d":"data['norm_sibsp'] = np.log(data.SibSp + 1)\ndata['norm_sibsp'] .hist(color = 'green')","861d945c":"data['norm_fare'] = np.log(data.Fare + 1)\ndata['norm_fare'] .hist(color = 'red')","64ec4dca":"data_dummies = pd.get_dummies(data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'norm_fare',\n                               'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket',\n                               'name_title', 'train_test']])","c7cd59c5":"X_train = data_dummies[data_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = data_dummies[data_dummies.train_test == 0].drop(['train_test'], axis =1)\ny_train = data[data.train_test == 1].Survived\ny_train.shape","10c11e34":"scale = StandardScaler()\ndata_dummies_scaled = data_dummies.copy()\ndata_dummies_scaled[['Age','SibSp',\n                     'Parch','norm_fare']] = scale.fit_transform(data_dummies_scaled[['Age',\n                                                                                      'SibSp',\n                                                                                     'Parch',\n                                                                                     'norm_fare']])\ndata_dummies_scaled.head()","80cfae1d":"X_train_scaled = data_dummies_scaled[data_dummies_scaled.train_test == 1].drop(['train_test'],\n                                                                               axis =1)\nX_test_scaled = data_dummies_scaled[data_dummies_scaled.train_test == 0].drop(['train_test'],\n                                                                              axis =1)\ny_train = data[data.train_test==1].Survived","d0aaec1b":"gnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","a4486157":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr, X_train, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","80c6ae1b":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr, X_train_scaled, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","c2fcf4f3":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt, X_train, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","df5ddd3d":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt, X_train_scaled, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","7ce7a440":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","e6018559":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train_scaled, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","5ab4b34e":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf, X_train, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","7537069e":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf, X_train_scaled, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","486640ee":"svc = SVC(probability = True)\ncv = cross_val_score(svc, X_train_scaled, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","b213a274":"xgb = XGBClassifier(random_state =1, use_label_encoder = False, eval_metric = 'error')\ncv = cross_val_score(xgb, X_train_scaled, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","e9d97592":"voting_clf = VotingClassifier(estimators = [('lr',lr), ('knn',knn), \n                                            ('rf',rf), ('gnb',gnb),\n                                            ('svc',svc), ('xgb',xgb)], voting = 'soft') ","6e93a684":"cv = cross_val_score(voting_clf, X_train_scaled, y_train, cv = 5)\nprint(\"Media: \", cv.mean(), \"\\nValores:\", cv)","072f9004":"voting_clf.fit(X_train_scaled, y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data = basic_submission)\nbase_submission.to_csv('submissionVcSoftBase.csv', index = False)","3278d351":"def clf_performance(classifier, model_name):\n    print(model_name)\n    print('Mejor Puntuaci\u00f3n: ' + str(classifier.best_score_))\n    print('Mejores Par\u00e1metros: ' + str(classifier.best_params_))","fe3869e1":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr, 'Logistic Regression')","a09ef771":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3, 5, 7, 9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree', 'kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled, y_train)\nclf_performance(best_clf_knn, 'KNN')","ac3c169e":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'],\n                                  'gamma': [.1, .5, 1],\n                                  'C': [.1, 1, 10]},\n                                 {'kernel': ['linear'], \n                                  'C': [.1, 1, 10]},\n                                 {'kernel': ['poly'], \n                                  'degree' : [2, 3, 4], \n                                  'C': [.1, 1, 10]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled, y_train)\nclf_performance(best_clf_svc, 'SVC')","31d02c85":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400, 450, 500],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [2, 3],\n                                  'min_samples_split': [2, 3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled, y_train)\nclf_performance(best_clf_rf, 'Random Forest')","bc130ed6":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","8d099416":"xgb = XGBClassifier(random_state = 1, use_label_encoder = False)\n\nparam_grid = {\n    'n_estimators': [450, 500],\n    'colsample_bytree': [0.8],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5],\n    'subsample': [0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5, 1],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scaled, y_train)\nclf_performance(best_clf_xgb,'XGB')","bd134464":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\nxgb_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('submissionXgb.csv', index = False)","2c313502":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),\n                                                 ('rf',best_rf),\n                                                 ('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),\n                                                 ('rf',best_rf),\n                                                 ('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),\n                                                ('rf',best_rf),\n                                                ('svc',best_svc), \n                                                ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),\n                                                ('rf',best_rf),\n                                                ('svc',best_svc),\n                                                ('xgb', best_xgb),\n                                                ('lr', best_lr)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())","d00fe5f6":"params = {'weights' : [[1, 1, 1], [1, 2, 1], [1, 1, 2],\n                       [2, 1, 1], [2, 2, 1], [1, 2, 2], [2, 1, 2]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params,\n                           cv = 5, verbose = True, n_jobs = -1)\n\nbest_clf_weight = vote_weight.fit(X_train_scaled, y_train)\nclf_performance(best_clf_weight, 'VC Weights')\nvoting_clf_sub = best_clf_weight.best_estimator_.predict(X_test_scaled)","abc89ae8":"voting_clf_hard.fit(X_train_scaled, y_train)\nvoting_clf_soft.fit(X_train_scaled, y_train)\nvoting_clf_all.fit(X_train_scaled, y_train)\nvoting_clf_xgb.fit(X_train_scaled, y_train)\n\nbest_rf.fit(X_train_scaled, y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\ny_hat_rf = best_rf.predict(X_test_scaled).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test_scaled).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test_scaled).astype(int)","ec7139a4":"final_data = {'PassengerId': test.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data = final_data_4)\n\nfinal_data_5 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': test.PassengerId,\n                   'Survived_vc_hard': y_hat_vc_hard,\n                   'Survived_rf': y_hat_rf,\n                   'Survived_vc_soft' : y_hat_vc_soft, \n                   'Survived_vc_all' : y_hat_vc_all,\n                   'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data = final_data_comp)","04561e0f":"comparison['difference_rf_vc_hard'] = comparison.apply(\n    lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(\n    lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(\n    lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)\ncomparison.difference_hard_all.value_counts()","816b362a":"submission.to_csv('submissionRF.csv', index = False) \nsubmission_2.to_csv('submissionVcHard.csv',index = False) \nsubmission_3.to_csv('submissionVcSoft.csv', index = False)\nsubmission_4.to_csv('submissionVcAll.csv', index = False)\nsubmission_5.to_csv('submissionVcXgb2.csv', index = False)","7e6dae1c":"La mayor parte de pasajeros presentan un valor nulo, como ya se hab\u00eda visto. No obstante, este an\u00e1lisis s\u00ed que ha aportado cierta luz, porque se puede comprobar que aquellos pasajeros de los que se tiene registrada la cabina tienen una tasa de supervivencia mayor que aquellos que presentan valores perdidos. De esta forma, esta variable que contiene la letra de la cabina es aquella que se utilizar\u00e1 para entrenar los modelos (sigue teniendo el mismo problema que antes, a saber, que hay letras con muy pocos registros y ser\u00eda conveniente agrupar dichos niveles). \n\nEl siguiente paso es estudiar la variable que se remite al ticket. Primeramente se eval\u00faa si el ticket consta o no de un n\u00famero. ","999e2d15":"## Divisi\u00f3n del Marco de Datos\n\nSe realiza de nuevo la divisi\u00f3n del marco de datos en los subconjutnos de entrenamiento y prueba. ","3f03f813":"# Importaci\u00f3n del Marco de Datos\n\nEn primer lugar es preciso cargar las librer\u00edas necesarias para la lectura de datos. ","9e6e7d26":"Se parte de la regresi\u00f3n log\u00edstica. Se eval\u00faa un total de 40 modelos candidatos, de los cuales el ganador es el mostrado a continuaci\u00f3n. ","4109fbf6":"# Proyecto Titanic\n\nEn este cuaderno se va a ejecutar un an\u00e1lisis exploratorio y predictivo sobre el proyecto Titanic en Kaggle en castellano. El objetivo del estudio es determinar con la mayor certeza posible si un sujeto sobrevivir\u00eda al hundimiento del Titanic. Este cuaderno est\u00e1 basado en el [tutorial de Ken Jee](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example).\n\n## \u00cdndice\n\n1. Importaci\u00f3n del Marco de Datos\n2. Limpieza del Marco de Datos\n3. Exploraci\u00f3n del Marco de Datos\n4. Selecci\u00f3n de Variables. \n5. Preprocesamiento para el Modelado. \n6. Construcci\u00f3n de un Modelo B\u00e1sico.\n7. Tuneado del Modelo. \n8. Ensemble para el Modelado. \n9. Resultados. ","df4f29de":"Se pueden separar las columnas num\u00e9ricas de las categ\u00f3ricas de una forma muy simple. Para ello, se miran las columnas primero.","e136259b":"Como se puede comprobar la mayor\u00eda de sujetos no presenta m\u00faltiples cabinas. Se mira la tasa de supervivencia en funci\u00f3n de los niveles definidos para esta nueva variable. ","430f7fed":"Se efect\u00faa la validaci\u00f3n cruzada con el conjunto de entrenamiento estandarizado. ","ac4eddb3":"Se aplica el mismo modelo sobre el marco de datos estandarizado. En este caso no se observa diferencia alguna. ","f9d85121":"Se hace lo mismo para las cadenas de caracteres con un fin puramente experimental. Al ser tan pocos valores para la mayor\u00eda de las categor\u00edas apenas se puede deducir nada. ","86832fe7":"Se guarda el resultado obtenido y se establece como la primera entrega para la competici\u00f3n. ","24542477":"## Xtreme Gradient Boosting","4aa2a2e8":"## M\u00e9todo de los k vecinos m\u00e1s cercanos (kNN)","f5a52a75":"Se contempla correlaci\u00f3n positiva relativamente fuerte entre la variable que se remite al n\u00famero de hermanos y de parejas, lo cual sugiere que las familias suelen viajar juntas. Por otra parte, la edad y las variables que se remiten a los hermanos y parejas tienen una correlaci\u00f3n negativa muy fuerte. A la hora de elaborar los modelos esta correlaci\u00f3n es muy importante. Por ejemplo, en modelos de regresi\u00f3n interesa evitar la multicolinearidad, que se ofrece cuando dos variables se encuentran altamente correladas. \n\nSiguientemente, se va a ver c\u00f3mo la tasa de supervivencia var\u00eda seg\u00fan estos grupos, tal que: ","e5ebf5fa":"Al ejecutar el algoritmo sobre los datos escalados se ofrece una ligera mejora de la actuaci\u00f3n del modelo. ","ad02a8d4":"Se tiene que en las variables *Age* y *Cabin* se tienen valores perdidos. Para profundizar un pco m\u00e1s y comprender mejor estas variables, se recurre al m\u00e9todo `describe()`, que ofrece las medidas de tendencia central de los marcos de datos.  ","afc81c1c":"Tras esto se realiza la dumificaci\u00f3n de las variables. ","0429906f":"Ahora se mira la variable que se remite al nombre con el que se registra la persona. Como se puede observar, hay diferentes tratamientos en funci\u00f3n del apelativo con el que se escribe el nombre y esto puede servir como un indicador. ","93eb6813":"El pr\u00f3ximo algoritmo sobre el que realizar la b\u00fasqueda de hiperpar\u00e1metros es el clasificador de soporte de vectores. ","c754f024":"# Exploraci\u00f3n del Marco de Datos\n\nAhora se hace una exploraci\u00f3n superficial del marco de datos. Como punto de partida, se observan los tipos de las variables y se realiza el conteo de los valores perdidos con la funci\u00f3n `info()`.\n\n","325525e0":"# Preprocesado del Marco de Datos para el Modelo\n\nEn este apartado se van a efectuar las siguientes acciones: \n\n1. Prescindir de los valores nulos para la variable *Embarked*.\n2. Incluir solamente las variables m\u00e1s relevantes en el conjunto final. \n3. Hacer la dumificaci\u00f3n sobre todo el marco de datos (tambi\u00e9n se podr\u00eda hacer con un codificador one-hot). \n4. Efectuar la imputaci\u00f3n por el valor de la media (o de la mediana) para las variables *fare* y *age*.\n5. Normalizar la variable *fare* para dar una forma que se asemeje a una distribuci\u00f3n normal. \n6. Escalar todas las variables con un escalador est\u00e1ndar. \n\nEl primer paso es establecer para el marco de datos completo todas las variables categ\u00f3ricas que se hab\u00edan creado para los subconjutnos de entrenamiento y prueba. ","2b3c29d5":"Fin del tutorial. Espero que te haya servido de ayuda. Pasa un buen d\u00eda!","0106afb0":"## Clasificador de Soporte de Vcetores (VSC)","dfd8c68c":"## An\u00e1lisis de las Variables Cuantitativas","41b9470a":"A continuaci\u00f3n, se establece que se puedan ver todos los registros del marco de datos para comprobar los diferentes valores de cadenas de caracteres que toma la variable que se remite al ticket. ","0f0e6268":"Se efect\u00faa la misma divisi\u00f3n del marco de datos, pero ahora para el conjutno de datos estandarizado. ","ba7ce481":"Una vez se tienen las librer\u00edas y las rutas de los archivos a importar, se recurre a la funci\u00f3n `read_csv()` para cargar los datos. Los dos marcos se unen y se crea una variable binaria que identifique si el sujeto procede del marco de entrenamiento o si por el contrario lo hace del marco de prueba. ","2deaed80":"No parece que ninguna de estas categor\u00edas pueda reflejar una convenci\u00f3n que sea de utilidad, pues de hecho la mayr parte de tickets no contiene letra, por lo que se opta por elegir la variable m\u00e1s simple que se remite a la presencia o ausencia de caracteres num\u00e9ricos. Sobre esta decisi\u00f3n, se contempla la diferencia en la ratio de supervivencia en funci\u00f3n de que el ticket sea o no num\u00e9rico. No parece haber una diferencia significativa entre las ratios de supervivencia de los dos grupos de tickets. ","13709c0c":"## \u00c1rboles de Decisi\u00f3n","ac0b7a89":"# Entrega de los Resultados\n\nSe transforma la salida de las predicciones en marcos de datos para poder realizar la entrega. ","e80c1729":"A continuaci\u00f3n, se separan las variables en dos marcos, uno para num\u00e9ricas y otro para categ\u00f3ricas, con el fin de que el estudio se vea facilitado. ","73b87442":"# B\u00fasqueda de Hiperpar\u00e1metros\n\nPor \u00faltimo se realiza la b\u00fasqueda de hiperpar\u00e1metros para cada uno de los modelos de la secci\u00f3n previa. Antes de ello, se dise\u00f1a la siguiente funci\u00f3n propia que sirva para mostrar la evaluaci\u00f3n de dicha b\u00fasqueda. ","ba77b7da":"Se ejecuta el mismo algoritmo sobre el conjunto estandarizado. El resultado es pr\u00e1cticamente id\u00e9ntico, si bien se puede observar un ligero empeoramiento de unas tres mil\u00e9simas aproximadamente. ","d3469aff":"Nuevamente, se aplica el mismo algoritmo sobre el conjunto de datos estanddarizados. El resultado permanece pr\u00e1cticamente igual. ","4fa3b87f":"Se eliminan entonces los registros donde *embarked* presente valor nulo. Solamente ser\u00e1n dos registros para el subconjunto de entrenamiento y ninguno para el de prueba. ","19679cad":"# Creaci\u00f3n de los Modelos\n\nSe parte de la creaci\u00f3n de diferentes modelos con los par\u00e1metros ofrecidos por defecto. Se utilizar\u00e1 una validaci\u00f3n cruzada de cinco pliegues, que consiste en dividir el marco de entrenamiento de forma aleatoria en subconjuntos de tal forma que uno de ellos haga las veces de marco de testeo mientras que el resto servir\u00e1n para el entrenamiento durante varios ciclos de modo que todos los subconjuntos definidos adquieran el rol de marco de prueba. De este modo se permite que el entrenamiento se acerque m\u00e1s a lo que ser\u00eda el mundo real y se evite el sobreajuste. \n\nAdem\u00e1s, mediante el subconjunto de validaci\u00f3n se podr\u00e1 monitorizar c\u00f3mo mejora con el tuneado cada uno de los modelos. Ahora bien, que un modelo act\u00fae mejor sobre el conjunto de validaci\u00f3n no implica necesariamente que lo vaya a hacer sobre el conjunto de prueba. Se utilizar\u00e1n los siguientes algoritmos.\n\n## Naive Bayes\n\nEs un modelo que se suele usar como base para tareas de clasificaci\u00f3n junto con el **random forest**. ","bfc40f2a":"## Estandarizaci\u00f3n del Marco de Datos\n\nSe realiza una transformaci\u00f3n del marco de datos ya que para algunos de los algoritmos es necesario que se ofrezca la estandarizaci\u00f3n de las variables. ","1f16a7a2":"Se efect\u00faa la estandarizaci\u00f3n logar\u00edtmica de la variable *sibsp*.","2bf44c7e":"Se estudian las diferencias entre las salidas de las diferentes predicciones. ","f896adb8":"Como se puede observar, la edad sigue una distribuci\u00f3n aproximadamente normal, mientras que el resto de variables no cumplen esta distribuci\u00f3n; los valores bajos priman. Por ello, para el desarrollo de algunos modelos puede ser interesante llevar a cabo la normalizaci\u00f3n en el caso de que haya mucha diferencia en los valores, como es el caos de la variable *Fare*.\n\nAhora se pueden estudiar las correlaciones entre las variables, para cuya visualizaci\u00f3n se puede recurri a la librer\u00eda **seaborn**. ","718b37ab":"Se separan estos apelativos y se efect\u00faa el conteo de los mismos. Por lo que se puede ver, podr\u00eda mejorarse tambi\u00e9n esta variable reduciendo el n\u00famero de categor\u00edas mediante una agregaci\u00f3n. ","1371cb15":"Se mira la distribuci\u00f3n de las variables cuantitativas por medio de sus respectivos histogramas.  ","543368eb":"As\u00ed pues, sobre los resultados de las votaciones se realizan las predicciones.","fcadd447":"## Clasificador de Votaci\u00f3n por Mayor\u00eda (Soft Voting)\n\nEste sistema clasificador toma todos los resultados obtenidos para cada uno de los modelos como entrada y para el caso que nos ocupa efect\u00faa una evaluaci\u00f3n para ver si finalmente el sujeto sobrevivi\u00f3 o no al hundimiento del Titanic. Si se elige como sistema de votaci\u00f3n *hard*, cada clasificador obtendr\u00e1 un voto (s\u00ed\/no) y el resultado final para cada pasajero del Titanic ser\u00eda por votaci\u00f3n popular. Para emplear este sistema de votaci\u00f3n, por lo general se deseara tener n\u00fameros impares. Por el contrario, un sistema de votaci\u00f3n *soft* promediar\u00e1 la confianza de cada uno de los modelos. Si la confianza promedio es superior al 50% se tomar\u00e1 como 1. Se recomienda su uso para un conjunto de clasificadores bien calibrados.","90080c68":"## Regresi\u00f3n Log\u00edstica","f27e19e6":"Tras ello se realiza la imputaci\u00f3n de valores nulos para las variables continuas. La imputaci\u00f3n se efect\u00faa con el valor de la media del subconjunto de entrenamiento para la variable *age* y con la mediana para la variable *Fare* al no presentar una distribuci\u00f3n normal. ","896ef906":"Adem\u00e1s, en un sistema de clasificaci\u00f3n por votaci\u00f3n suave se pueden aplicar pesos de tal modo que unos modelos tengan m\u00e1s fuerza que el resto a la hora de efectuar la votaci\u00f3n. Esto arrojara unos resultados diferentes seg\u00fan c\u00f3mo se efect\u00fae esta asignaci\u00f3n de pesos. ","18043f60":"Por \u00faltimo se aplica la b\u00fasqueda de hiperpar\u00e1metros sobre el *Xgradient boosting*. ","22453ef7":"Hay algunos niveles con muy pocos sujetos, lo cual podr\u00eda suponer un problema para algunos algoritmos (convendr\u00eda agrupar los los niveles distintos de cero en un solo nivel para superar un umbral del cinco por ciento de las observaciones).\n\nEl siguiente paso es crear categor\u00edas en virtud de la letra de la cabina. En este caso se tratar\u00e1n los valores nulos como una categor\u00eda, que se designar\u00e1 como **n**. Se mira tambi\u00e9n la tasa de supervivencia sobre estos niveles. ","2877304d":"Se pasan los marcos de datos a formato csv para realizar la entrega y que puedan ser evaluados en la competici\u00f3n. ","3a066267":"Con estas tablas de doble entrada se puede comprobar que los pasajeros de primera clase son los que mayor tasa de supervivencia presentan frente al resto de clases; las mujeres tienden a sobrevivir m\u00e1s que los hombres, y en cuanto al puerto de embarque, parece que la procedencia de Southampton disminuye la tasa de supervivencia en relaci\u00f3n a las otras dos regiones (lo cual podr\u00eda estar relacionado tambi\u00e9n con el poder adquisitivo).\n\n# Ingenier\u00eda de Variables\n\nPrimeramente se ha de simplificar el n\u00famero de categor\u00edas que presenta la variable *cabin*, puesto que al tener tan pocos registros, un n\u00famero tan alto de categor\u00edas no nos aporta informaci\u00f3n. Si se mira esta variable se podr\u00e1 observar que lo que sucede es que se agrupan por letra y n\u00famero el tipo de la cabina, de modo que una forma de solventar el problema es separar el n\u00famero de la letra y efectuar los an\u00e1lisis o bien por el n\u00famero o bien por la letra. Se define sobre esta base una nueva variable denominada *cabin_multiple*. ","e607475b":"Se sigue con el modelo de los k vecinos m\u00e1s cercanos. En este caso se estudia un total de 48 modelos. ","59bb7e5b":"La edad no parece influir tanto en la supervivencia, al igual que no lo hacen la variable referentes a la presencia de hermanos (aunque s\u00ed existe una ligera diferencia en este caso). Por otra parte, s\u00ed que se aprecia una notable diferencia si se mira la variable que se remite al precio del billete, de modo que cuanto m\u00e1s alto es el precio mayor probabilidad de supervivencia parece ofrecerse. Tambi\u00e9n parece existir una notable diferencia si se tiene padres a bordo, lo cual tiene sentido, ya que es de esperar que antepusieran la vida de sus hijos. \n\n## An\u00e1lisis de las Variables Cualitativas\n\nEvaluadas las variables num\u00e9ricas, se pasa a estudiar las categ\u00f3ricas. Primero, se obtendr\u00e1n los diagramas de barras de \u00e9stas.  ","65fba5e9":"Se toman los modelos ganadores encontrados para cada una de las b\u00fasquedas de hiperpar\u00e1metros realizadas y se vuelve a realizar una votaci\u00f3n suave como en la secci\u00f3n previa. ","3d636844":"El siguiente clasificador sobre el que se realiza el tuneado es el *random forest*.","999cf8c9":"Se realiza la misma transformaci\u00f3n para la variable *Fare*.","701562a1":"## Random Forest","78414358":"Por los diagramas de barras se tiene que el n\u00famero de supervivientes es inferior al de muertes, hay mayor n\u00famero de varones que de mujeres, el grueso de viajeros son de tercera clase y que la mayor\u00eda de pasajeros proceden del puerto de Southampton. Adem\u00e1s, las variables sobre el ticket y la cabina tienen un gran n\u00famero de categor\u00edas, lo cual podr\u00eda ser un problema en un futuro. \n\nAhora se pasa a efectuar un an\u00e1lisis parecido al que se hizo con las variables cuantitativas en relaci\u00f3n a la variable diana sobre la supervivencia comparando para cada una de las categor\u00edas de cada variable cualitativa el n\u00famero de supervivientes y fallecidos. "}}