{"cell_type":{"7e43ef6d":"code","f391c0c6":"code","2ff2b896":"code","7f6c2ca3":"code","e6935915":"code","c1afbd7c":"code","6b9663b2":"code","81cdce8c":"code","33aec4ea":"code","c7a894ec":"code","3e130ab8":"code","bf7f68d4":"code","0c8d3fc3":"code","5cbdde52":"code","80158c1b":"code","8dc9c1ae":"code","6746a6cd":"code","9a2e4eb5":"code","1464a322":"code","d7805824":"code","04937067":"code","e0007c2c":"code","fc299ccc":"code","eff08cf3":"code","d95fc745":"code","740ce734":"code","778a68f2":"code","d69fea5f":"code","ddcdf1fb":"code","6b05aac3":"code","ab7c5878":"markdown","e1ac24af":"markdown","8cb6f41a":"markdown","d0b638d7":"markdown","55bee499":"markdown","5b33b264":"markdown","e214c0ff":"markdown","5912fc7c":"markdown","d7b39f58":"markdown","12d485e7":"markdown","406c4f1d":"markdown","122aa102":"markdown","1a4e6f60":"markdown","f40d4e44":"markdown","7fcb92f3":"markdown","fc609859":"markdown","54dcc009":"markdown","f581a848":"markdown","f9264279":"markdown","51f3f7ee":"markdown","5dd1dd37":"markdown","87934c77":"markdown","abfb80fb":"markdown","3c18ba10":"markdown","cc4692e7":"markdown","457d879c":"markdown","86a10548":"markdown","122b06de":"markdown","0e9e3ba7":"markdown","f0d0d8f5":"markdown","a852aa59":"markdown","5ed23975":"markdown","23e2f41e":"markdown","f34e1608":"markdown","2a07aea1":"markdown","2fc12088":"markdown","08544c73":"markdown"},"source":{"7e43ef6d":"# Importing basic modules\n\nimport os, sys\nimport numpy as np\nimport pandas as pd\nimport warnings, random\nwarnings.filterwarnings('ignore')\nprint(os.listdir('..\/input\/fashionmnist'))","f391c0c6":"# Reading trainset and testset\n\ntrain = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ntrain.shape, test.shape","2ff2b896":"import torch\nfrom tensorflow import random as rd\n\nseed = 123456789\nrd.set_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","7f6c2ca3":"# Importing Keras modules\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout, Conv2D, MaxPooling2D, SpatialDropout2D, Flatten, Reshape\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.utils import to_categorical","e6935915":"train.head(1)","c1afbd7c":"test.head(1)","6b9663b2":"train_label = train.label\ntrain.drop(columns = 'label', inplace = True)\n\ntest_label = test.label\ntest.drop(columns = 'label', inplace = True)","81cdce8c":"train_label_keras = to_categorical(train_label, num_classes=10)\ntest_label_keras = to_categorical(test_label, num_classes=10)","33aec4ea":"train = train\/255.0\ntest = test\/255.0","c7a894ec":"import tensorflow as tf\nseed = 2010\ntf.random.set_seed(seed)\nnp.random.seed(seed)","3e130ab8":"# Model architecture\n\nmodel_mlp = Sequential()\nmodel_mlp.add(Dense(784, activation='relu', use_bias=True))\nmodel_mlp.add(Dropout(0.3))\nmodel_mlp.add(BatchNormalization())\nmodel_mlp.add(Dense(256, activation='relu', use_bias=True))\nmodel_mlp.add(Dropout(0.4))\nmodel_mlp.add(BatchNormalization(momentum=0.4))\nmodel_mlp.add(Dense(128, activation='relu', use_bias=True))\nmodel_mlp.add(Dropout(0.3))\nmodel_mlp.add(BatchNormalization())\nmodel_mlp.add(Dense(64, activation='relu', use_bias=True))\nmodel_mlp.add(Dropout(0.4))\nmodel_mlp.add(BatchNormalization(momentum=0.4))\nmodel_mlp.add(Dense(10, activation='softmax', use_bias=True))\nmodel_mlp.compile(optimizer='Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'] )\ncheck = ModelCheckpoint('weight.best.hdf5', monitor ='val_loss', save_best_only=True)","bf7f68d4":"# Also using learning Rate reduction when we are moving towards optimized weights and bias\nlr = ReduceLROnPlateau(monitor='val_loss', patience=7, min_lr=0.0001)\n#es = EarlyStopping(monitor='val_loss', patience=5)\nmodel_mlp.fit(x = train.values, y = train_label_keras, epochs=15, batch_size=64, validation_split=0.20, callbacks=[lr,check])","0c8d3fc3":"# Model architecture\n\nmodel_cnn = Sequential()\nmodel_cnn.add(Reshape((28,28,-1), input_shape = (784,)))\nmodel_cnn.add(Conv2D(32, kernel_size=(3,3), strides = (1,1), padding = 'same' ))\nmodel_cnn.add(Conv2D(64, kernel_size=(3,3), strides = (1,1), padding = 'same' ))\nmodel_cnn.add(BatchNormalization())\nmodel_cnn.add(MaxPooling2D(pool_size =(2,2), strides = (2,2)))\nmodel_cnn.add(Dropout(0.3))\n\nmodel_cnn.add(Conv2D(128, kernel_size=(3,3), strides =(1,1), padding = 'same' ))\nmodel_cnn.add(Conv2D(256, kernel_size=(3,3), strides =(2,2), padding = 'same' ))\nmodel_cnn.add(BatchNormalization())\nmodel_cnn.add(MaxPooling2D(pool_size =(2,2), strides =(2,2)))\nmodel_cnn.add(Dropout(0.3))\n\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(256, activation='relu', use_bias=True))\nmodel_cnn.add(Dropout(0.3))\n\nmodel_cnn.add(Dense(64, activation='relu', use_bias=True))\nmodel_cnn.add(Dropout(0.3))\nmodel_cnn.add(BatchNormalization())\n\nmodel_cnn.add(Dense(10, activation='softmax', use_bias=True))\nmodel_cnn.compile(optimizer='Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'] )\ncheck = ModelCheckpoint('weight_cnn.best.hdf5', monitor ='val_loss', save_best_only=True)\n\n# Also using learning Rate reduction when we are moving towards optimized weights and bias\nlr = ReduceLROnPlateau(monitor='val_loss', patience=7)\n#es = EarlyStopping(monitor='val_loss', patience=5)\nmodel_cnn.fit(x = train.values, y = train_label_keras, epochs=15, batch_size=64, validation_split=0.20, callbacks=[lr,check])","5cbdde52":"import matplotlib.pyplot as plt\n\nfig,ax = plt.subplots(1,2, figsize = (20,7))\nplt.subplot(1,2,1)\nplt.plot([a for a in range(1,16)], model_mlp.history.history['val_loss'])\nplt.plot([a for a in range(1,16)], model_mlp.history.history['loss'])\nplt.legend(['validation_loss', 'training_loss'])\n\nplt.subplot(1,2,2)\nplt.plot([a for a in range(1,16)], model_mlp.history.history['val_accuracy'])\nplt.plot([a for a in range(1,16)], model_mlp.history.history['accuracy'])\nplt.legend(['validation_accuracy', 'training_accuracy'])","80158c1b":"import matplotlib.pyplot as plt\n\nfig,ax = plt.subplots(1,2, figsize = (20,7))\nplt.subplot(1,2,1)\nplt.plot([a for a in range(1,16)], model_cnn.history.history['val_loss'])\nplt.plot([a for a in range(1,16)], model_cnn.history.history['loss'])\nplt.legend(['validation_loss', 'training_loss'])\n\nplt.subplot(1,2,2)\nplt.plot([a for a in range(1,16)], model_cnn.history.history['val_accuracy'])\nplt.plot([a for a in range(1,16)], model_cnn.history.history['accuracy'])\nplt.legend(['validation_accuracy', 'training_accuracy'])","8dc9c1ae":"print(\"MLP Performace\")\nmodel_mlp.load_weights('weight.best.hdf5')\nscore_keras = model_mlp.evaluate(x= test, y = test_label_keras, batch_size=256)\nprint('Keras Accuracy on testset is :  ', score_keras[1])\nprint('Keras Loss on testset is     :  ', score_keras[0])","6746a6cd":"print(\"CNN Performace\")\nmodel_cnn.load_weights('weight_cnn.best.hdf5')\nscore_keras = model_cnn.evaluate(x= test, y = test_label_keras, batch_size=256)\nprint('Keras Accuracy on testset is :  ', score_keras[1])\nprint('Keras Loss on testset is     :  ', score_keras[0])","9a2e4eb5":"import torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import datasets, transforms\nimport random","1464a322":"# Custom Dataset\n\nclass MyData(Dataset):\n  def __init__(self, data, label, transform = None):\n    self.data = data\n    self.label = label\n  \n  def __len__(self):\n    return(len(self.data))\n\n  def __getitem__(self,idx):\n    self.x = self.data.loc[idx,:].values\n    self.y = self.label.loc[idx]\n    return(self.x, self.y)","d7805824":"# Dividing data into train and valid set\n\nindices = len(train)\nindices = [a for a in range(indices)]\nsplit = 0.20\nsplit = int(np.floor(split*len(train)))\nrandom.shuffle(indices)\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)","04937067":"# Creating DataLoaders\n\ntraining = MyData(train, train_label, transforms.ToTensor())\ntraining_loader = DataLoader(training, batch_size=64, sampler=train_sampler)\n\nvalid_loader = DataLoader(training, batch_size=64, sampler=valid_sampler)\n\ntesting = MyData(test, test_label, transforms.ToTensor())\ntesting_loader = DataLoader(testing, batch_size=64)","e0007c2c":"pymodel_mlp = nn.Sequential(nn.Linear(784,784),\n                        nn.ReLU(),\n                        nn.Dropout(p=0.3),\n                        nn.BatchNorm1d(784),\n                        nn.Linear(784,256),\n                        nn.ReLU(),\n                        nn.Dropout(p=0.4),\n                        nn.BatchNorm1d(256),\n                        nn.Linear(256,128),\n                        nn.ReLU(),\n                        nn.Dropout(p=0.3),\n                        nn.BatchNorm1d(128),\n                        nn.Linear(128,64),\n                        nn.ReLU(),\n                        nn.Dropout(p=0.4),\n                        nn.BatchNorm1d(64),\n                        nn.Linear(64,10),\n                        nn.LogSoftmax(dim = 1),)\n\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(pymodel_mlp.parameters(), lr = 0.008)","fc299ccc":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npymodel_mlp = pymodel_mlp.to(device)\nepoch = 15\nvalid_score = np.inf\ntorch_valid_loss = []\ntorch_valid_acc  = []\n\nfor e in range(epoch):\n  training_loss = 0\n  valid_loss  = 0\n\n  running_loss = 0\n  pymodel_mlp.train()\n  for image, label in training_loader :\n    optimizer.zero_grad()\n    image = image.to(device)\n    label = label.to(device)\n    out = pymodel_mlp(image.float())\n    loss = criterion(out,label)\n    running_loss += loss.item()\n    loss.backward()\n    optimizer.step()\n  \n  with torch.no_grad():\n    running_valid = 0\n    acc_valid = 0\n    pymodel_mlp.eval()\n    for image, label in valid_loader:\n      image = image.to(device)\n      label = label.to(device)\n      out = pymodel_mlp(image.float())\n      loss = criterion(out, label)\n      top_p, top_class = torch.exp(out).topk(1, dim = 1)\n      equal = top_class == label.view(top_class.shape)\n      accuracy = torch.mean(equal.type(torch.FloatTensor))\n      running_valid += loss.item()\n      acc_valid += accuracy.item()\n    \n    if running_valid < valid_score :\n      torch.save(pymodel_mlp.state_dict(), 'checkpoint.pth')\n      print('\\nError changes from {} to {}'.format(valid_score\/len(valid_loader), running_valid\/len(valid_loader)))\n      valid_score = running_valid\n      print('Saved model\\n')\n        \n  training_loss = running_loss\/len(training_loader)\n  valid_loss    = running_valid\/len(valid_loader)\n    \n  torch_valid_loss.append(valid_loss)\n  torch_valid_acc.append(acc_valid\/len(valid_loader))\n  print('Epoch : %s\\nTraining_error : %s\\nValid_error    : %s\\nAccuracy_valid : %s\\n------------------------------' \n          %(e+1, training_loss, valid_loss, acc_valid\/len(valid_loader)))","eff08cf3":"from torch.nn import functional as f\nclass net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d_32 = nn.Conv2d(1,32,3,padding=1)\n        self.conv2d_64 = nn.Conv2d(32,64,3,padding=1)\n        self.max2d     = nn.MaxPool2d(2,2)\n        self.conv2d_128 = nn.Conv2d(64,128,3,padding=1)\n        self.conv2d_256 = nn.Conv2d(128,256,3, stride = 2,padding=1)\n        self.linear1    = nn.Linear(3*3*256, 256)\n        self.linear2    = nn.Linear(256,64)\n        self.linear3    = nn.Linear(64,10)\n        self.batch2d1     = nn.BatchNorm2d(64)\n        self.batch2d2    = nn.BatchNorm2d(256)\n        self.batch1d     = nn.BatchNorm1d(64)\n        self.drop      = nn.Dropout(p=0.3)\n        self.flat      = nn.Flatten()\n    \n    def forward(self,x):\n        x = x.view(-1,1,28,28)\n        x = f.relu(self.conv2d_32(x))\n        x = f.relu(self.conv2d_64(x))\n        x = self.batch2d1(x)\n        x = f.relu(self.max2d(x))\n        x = self.drop(x)\n        \n        x = f.relu(self.conv2d_128(x))\n        x = f.relu(self.conv2d_256(x))\n        x = self.batch2d2(x)\n        x = f.relu(self.max2d(x))\n        x = self.drop(x)\n        \n        x = self.flat(x)\n        x = f.relu(self.linear1(x))\n        x = self.drop(x)\n        x = f.relu(self.linear2(x))\n        x = self.drop(x)\n        x = self.batch1d(x)\n        x = f.log_softmax(self.linear3(x), dim=1)\n        return(x)\n\npymodel_cnn = net()\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(pymodel_cnn.parameters(), lr = 0.008)","d95fc745":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npymodel_cnn = pymodel_cnn.to(device)\nepoch = 15\nvalid_score = np.inf\ntorch_valid_loss_cnn = []\ntorch_valid_acc_cnn  = []\n\nfor e in range(epoch):\n  training_loss = 0\n  valid_loss  = 0\n\n  running_loss = 0\n  pymodel_cnn.train()\n  for image, label in training_loader :\n    optimizer.zero_grad()\n    image = image.to(device)\n    label = label.to(device)\n    out = pymodel_cnn(image.float())\n    loss = criterion(out,label)\n    running_loss += loss.item()\n    loss.backward()\n    optimizer.step()\n  \n  with torch.no_grad():\n    running_valid = 0\n    acc_valid = 0\n    pymodel_cnn.eval()\n    for image, label in valid_loader:\n      image = image.to(device)\n      label = label.to(device)\n      out = pymodel_cnn(image.float())\n      loss = criterion(out, label)\n      top_p, top_class = torch.exp(out).topk(1, dim = 1)\n      equal = top_class == label.view(top_class.shape)\n      accuracy = torch.mean(equal.type(torch.FloatTensor))\n      running_valid += loss.item()\n      acc_valid += accuracy.item()\n    \n    if running_valid < valid_score :\n      torch.save(pymodel_cnn.state_dict(), 'checkpoint_cnn.pth')\n      print('\\nError changes from {} to {}'.format(valid_score\/len(valid_loader), running_valid\/len(valid_loader)))\n      valid_score = running_valid\n      print('Saved model\\n')\n        \n  training_loss = running_loss\/len(training_loader)\n  valid_loss    = running_valid\/len(valid_loader)\n    \n  torch_valid_loss_cnn.append(valid_loss)\n  torch_valid_acc_cnn.append(acc_valid\/len(valid_loader))\n  print('Epoch : %s\\nTraining_error : %s\\nValid_error    : %s\\nAccuracy_valid : %s\\n------------------------------' \n          %(e+1, training_loss, valid_loss, acc_valid\/len(valid_loader)))","740ce734":"state = torch.load('checkpoint.pth')\npymodel_mlp.load_state_dict(state)","778a68f2":"torch_test_loss  = []\ntorch_test_acc   = []\n\nfor e in range(1):\n    testing_loss = 0\n    running_test = 0\n    acc_test = 0\n    pymodel_mlp.eval()\n    for image, label in testing_loader:\n      image = image.to(device)\n      label = label.to(device)\n      out = pymodel_mlp(image.float())\n      loss = criterion(out, label)\n      top_p, top_class = torch.exp(out).topk(1, dim = 1)\n      equal = top_class == label.view(top_class.shape)\n      accuracy = torch.mean(equal.type(torch.FloatTensor))\n      running_test += loss.item()\n      acc_test += accuracy.item()\n    testing_loss = running_test\/len(testing_loader)\n    torch_test_loss.append(testing_loss)\n    torch_test_acc.append(acc_test\/len(testing_loader))\n    print('Epoch : %s\\nTesting_error : %s\\nAccuracy_test : %s\\n----------------' %(e+1, testing_loss, acc_test\/len(testing_loader)))","d69fea5f":"state = torch.load('checkpoint_cnn.pth')\npymodel_cnn.load_state_dict(state)\n\ntorch_test_loss_cnn  = []\ntorch_test_acc_cnn   = []\n\nfor e in range(1):\n    testing_loss = 0\n    running_test = 0\n    acc_test = 0\n    pymodel_cnn.eval()\n    with torch.no_grad():\n        for image, label in testing_loader:\n          image = image.to(device)\n          label = label.to(device)\n          out = pymodel_cnn(image.float())\n          loss = criterion(out, label)\n          top_p, top_class = torch.exp(out).topk(1, dim = 1)\n          equal = top_class == label.view(top_class.shape)\n          accuracy = torch.mean(equal.type(torch.FloatTensor))\n          running_test += loss.item()\n          acc_test += accuracy.item()\n    testing_loss = running_test\/len(testing_loader)\n    torch_test_loss_cnn.append(testing_loss)\n    torch_test_acc_cnn.append(acc_test\/len(testing_loader))\n    print('Epoch : %s\\nTesting_error : %s\\nAccuracy_test : %s\\n----------------' %(e+1, testing_loss, acc_test\/len(testing_loader)))","ddcdf1fb":"import matplotlib.pyplot as plt\n\nfig,ax = plt.subplots(2,1, figsize = (18,12))\nplt.subplot(2,1,1)\nplt.title('Keras v\/s Pytorch (MLP) Loss', fontsize = 20)\nplt.plot([a for a in range(1,16)], torch_valid_loss)\nplt.plot([a for a in range(1,16)], model_mlp.history.history['val_loss'])\nplt.legend(['Pytorch Validation_loss', 'Keras Validation_loss'])\n\nplt.subplot(2,1,2)\nplt.title('Keras v\/s Pytorch (MLP) Accuracy', fontsize = 20)\nplt.plot([a for a in range(1,16)], torch_valid_acc)\nplt.plot([a for a in range(1,16)], model_mlp.history.history['accuracy'])\nplt.legend(['Pytorch Validation_Accuracy', 'Keras Validation_Accuracy'])","6b05aac3":"import matplotlib.pyplot as plt\n\nfig,ax = plt.subplots(2,1, figsize = (18,12))\nplt.subplot(2,1,1)\nplt.title('Keras v\/s Pytorch (CNN) Loss' , fontsize = 20)\nplt.plot([a for a in range(1,16)], torch_valid_loss_cnn)\nplt.plot([a for a in range(1,16)], model_cnn.history.history['val_loss'])\nplt.legend(['Pytorch Validation_loss', 'Keras Validation_loss'])\n\nplt.subplot(2,1,2)\nplt.title('Keras v\/s Pytorch (CNN) Accuracy', fontsize = 20)\nplt.plot([a for a in range(1,16)], torch_valid_acc_cnn)\nplt.plot([a for a in range(1,16)], model_cnn.history.history['accuracy'])\nplt.legend(['Pytorch Validation_Accuracy', 'Keras Validation_Accuracy'])","ab7c5878":"### On Keras we get an Accuracy of 73-79% wheras on CNN keras we got accuracy of 74-82%.Now let's check Pytorch","e1ac24af":"Let's compare Validation set loss and Accuracy between Pytorch and Keras","8cb6f41a":"**Keras expects the labels to be Hot-One-Encoded form** Therefore we need to convert label into Hot-One-Encoding","d0b638d7":"### => MLP on Pytorch","55bee499":"Also convert it to binary from greyscale image","5b33b264":"Loading best pytorch model on MLP and testing it on testset","e214c0ff":"### Pytorch model out performed Keras.","5912fc7c":"We are ready to create DataLoader to for trainset, validset, testset","d7b39f58":"#### Since we have imported trainset as well as testset. Now next step is to transform data so that it will we can feed this data to Keras Model","12d485e7":"### Feel free to give suggestion\/feedback. Upvote it if you like this","406c4f1d":"### => CNN on Pytorch","122aa102":"We will now build network architecture and train it on trainset. Continuously we will be monitoring on validation data. ","1a4e6f60":"#### Here we got an accuracy of 87 on testset.","f40d4e44":"### 2) Network architecture training - MLP","7fcb92f3":"### => Multi-layer perceptron","fc609859":"To save model best weights and bias we are going to use **ModelCheckpoint from keras callback module**. Furthermore we will utilise this checkpoint to load best parameters for training set and use it to check model performance on testset","54dcc009":"Let's try on Trained CNN pytorch model","f581a848":"### 3) Evaluating model performance and testing","f9264279":"### Objective : \nMain obejctive behind this notebook is to give an idea on building model with Keras and Pytorch. \n\nStarting from **Data Preparation, Creating custom DataLoaders, Model building, Validation of Model and Model comparison.**\n\nI am trying to keep it as **simple** as i can so that newbie can also understand the workflow.\n\nIf you learn anything useful from this notebook then **Give Upvote :)","51f3f7ee":"Now we are ready to build an architecture. We are going to build a 4-Hidden layer network with dropout.","5dd1dd37":"### => CNN On Keras","87934c77":"# In Depth Keras v\/s Pytorch Approach & Comparison","abfb80fb":"* ## Contents of the Notebook:\n\n### Part1: Keras:\n#### 1) Data Preparation\n\n#### 2) Network architecture training (MLP + CNN) \n\n#### 3) Evaluating model performance and testing\n\n### Part2: Pytorch:\n#### 1) Data Preparation\n\n#### 2) Creating custom DataLoaders\n\n#### 3) Build Pytoch model (MLP + CNN) and validating model performance \n\n### Part3: Comparison between Keras and Pytorch\n#### 1) Compare performace of Keras and Pytorch (MLP)\n\n#### 2) Compare performace of Keras and Pytorch (CNN)","3c18ba10":"#### First we need to make Dataloader to load testset and trainset. With dataloader we will train our model and validate it's performance on testset","cc4692e7":"### ALSO pytorch model is performing better than Keras model. Let's get into more detailed analysis ","457d879c":"Checking performance on **CNN** below","86a10548":"#### Now evaluate performace on testset using model best weights and bias parameters","122b06de":"### 2) Network architecture training","0e9e3ba7":"## Part1: Keras:\n### 1) Data Preparation\n","f0d0d8f5":"## Part2: Pytorch:\n### 1) Data Preparation","a852aa59":"### Pytorch CNN is performing better than Pytorch MLP model","5ed23975":"Let's divide dataset into train + label variables. ","23e2f41e":"### Accuracy on testset better in Pytorch also we are mostly getting near ablut 80%-90% accuarcy with Pytorch model. Whereas on Keras we are getting accuracy of 70-80%.  ","f34e1608":"### 3) Evaluating Pytorch model performance and testing it on testset","2a07aea1":"## Part3: Comparison between Keras and Pytorch\n### 1) Compare performace of Keras and Pytorch","2fc12088":"We will try to built a Convolutional network model and then we will compare different models","08544c73":"let's check model performace and test it on testset for **MLP**"}}