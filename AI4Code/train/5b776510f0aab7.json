{"cell_type":{"001e3ceb":"code","fe8a2df2":"code","c3410839":"code","ef091797":"code","6e9411ee":"code","48851a88":"code","33cb80c4":"code","17fc8494":"code","b5dcf9e0":"code","0a4c7460":"code","a4f5be04":"code","dca83f7a":"code","d7457c86":"code","f109d8c2":"code","51ee2ab5":"code","8fe89252":"code","b9b30426":"code","eafeec4b":"code","a792acc6":"code","18ef2877":"markdown"},"source":{"001e3ceb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fe8a2df2":"import re\nimport nltk\nnltk.download(\"stopwords\")\nnltk.download('punkt')\nfrom nltk import word_tokenize,sent_tokenize\nnltk.download('wordnet')\nimport nltk as nlp","c3410839":"df = pd.read_csv('..\/input\/trump-tweets\/trumptweets.csv',encoding='utf8')\ndf.head()","ef091797":"cult_list=[]\n\nfor cult in df.content:\n    cult=re.sub(\"[^a-zA-z]\",\" \",cult)\n    cult=cult.lower()\n    cult=nltk.word_tokenize(cult)\n    lemma=nlp.WordNetLemmatizer()\n    cult=[lemma.lemmatize(word) for word in cult]\n    cult=\" \".join(cult)\n    cult_list.append(cult)","6e9411ee":"from sklearn.feature_extraction.text import CountVectorizer\n\nmax_features=800\ncount_vectorizer=CountVectorizer(max_features=max_features,stop_words=\"english\")\nsparce_matrix=count_vectorizer.fit_transform(cult_list).toarray()","48851a88":"sparce_matrix.shape ","33cb80c4":"sparce_matrix","17fc8494":"print(\"Top {} the most used words: {}\".format(max_features,count_vectorizer.get_feature_names()))","b5dcf9e0":"data = pd.DataFrame(count_vectorizer.get_feature_names(),columns=[\"Words\"])\ndata[0:10]","0a4c7460":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\n\nplt.subplots(figsize=(10,10))\nwordcloud=WordCloud(background_color=\"black\",width=1024,height=768).generate(\" \".join(data.Words[0:100]))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","a4f5be04":"X=sparce_matrix[0:20000]\ny0=df.favorites[0:20000]\n\ny1=[]\nfor item in y0:\n    y1+=[int(np.log1p(item))]\ny=pd.Series(y1)\n\nprint(X.shape)\nprint(y.shape)","dca83f7a":"print(y.value_counts())","d7457c86":"from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,log_loss,precision_score\nfrom sklearn.metrics import roc_auc_score,roc_curve\n\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f109d8c2":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","51ee2ab5":"lgbm_model = LGBMClassifier()\nlgbm_model.fit(X_train,y_train)\ny_pred = lgbm_model.predict(X_test)","8fe89252":"print(\"Accuracy:\",accuracy_score(y_test,y_pred))","b9b30426":"df1=df[['favorites','content']].copy()\ndf1","eafeec4b":"df2=df1.sort_values(by=['favorites'],ascending=False).reset_index()\ndf2","a792acc6":"for i in range(8):\n    print('['+str(i+1)+'] '+df2['content'][i])","18ef2877":"NLTK is a leading platform for building Python programs to work with human language data.\nhttps:\/\/www.nltk.org\/"}}