{"cell_type":{"fbf014e2":"code","a549b066":"code","ee24b14a":"code","d993ea69":"code","e8cb0b29":"code","e8b99665":"code","1966927d":"code","630632fc":"code","9b7a7ab4":"code","1c9969d1":"code","78e2b255":"code","29bf6a81":"code","892c0c39":"code","22fc772a":"code","49656906":"code","e84147fd":"code","5bc2b622":"code","8f34906d":"code","04f22fe6":"code","a19c7e7d":"code","a5b5c2eb":"code","b5c2d7af":"code","da3bc6f1":"code","769d6cb3":"markdown","2b46a912":"markdown","88e5fa0d":"markdown","f7b1c8b2":"markdown","10e93c8e":"markdown","d14238e8":"markdown","bf91f5ca":"markdown","f7b0f00f":"markdown","c68fb912":"markdown","92ea5bc5":"markdown","748046ab":"markdown","73f9f670":"markdown","a53d84a9":"markdown","5682e3f2":"markdown","c8e15a56":"markdown","7b91c804":"markdown","0268c450":"markdown","3ed671a8":"markdown","75d32012":"markdown","b8164c6a":"markdown","a235c245":"markdown","501bc393":"markdown","ef6a476f":"markdown","ee7d5486":"markdown","3a6a87d6":"markdown","0bfa5d4f":"markdown"},"source":{"fbf014e2":"import numpy as np\nRANDOM_SEED = 4\nnp.random.seed(RANDOM_SEED) # set random seed for reproducability","a549b066":"import pandas as pd\ndf_neut = pd.read_csv(\"..\/input\/wikipedia-promotional-articles\/good.csv\")\ndf_prom = pd.read_csv(\"..\/input\/wikipedia-promotional-articles\/promotional.csv\")","ee24b14a":"df_prom = df_prom.drop(df_prom.columns[1:], axis=1)\ndf_neut = df_neut.drop(df_neut.columns[1:], axis=1)","d993ea69":"df_neut.head()","e8cb0b29":"df_prom.head()","e8b99665":"df_neut.insert(1, 'label', 0) # neutral labels\ndf_prom.insert(1, 'label', 1) # promotional labels","1966927d":"df_prom.head()","630632fc":"df_neut.head()","9b7a7ab4":"df = pd.concat((df_neut, df_prom), ignore_index=True, axis=0) # merge dataframes","1c9969d1":"df.head()","78e2b255":"df = df.reindex(np.random.permutation(df.index))\ndf.head()","29bf6a81":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)","892c0c39":"from keras.preprocessing.text import Tokenizer\n\n# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n\ntext_data = [str(txt) for txt in df_train['text'].values] # convert text data to strings\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True) # create tokenizer object\ntokenizer.fit_on_texts(text_data) # make dictionary\n\nx_train = tokenizer.texts_to_sequences(text_data) # vectorize dataset","22fc772a":"from keras.preprocessing import sequence\n\n# Max number of words in each sequence\nMAX_SEQUENCE_LENGTH = 400\n\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)","49656906":"y_train = df_train['label'].values","e84147fd":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\n\nmodel = Sequential()","5bc2b622":"EMBEDDING_DIM = 100\nmodel.add(Embedding(MAX_NB_WORDS+1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))","8f34906d":"model.add(LSTM(80))","04f22fe6":"model.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","a19c7e7d":"model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])","a5b5c2eb":"EPOCHS = 2\nBATCH_SIZE = 64\n\nhistory = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.15)","b5c2d7af":"x_test = np.array(tokenizer.texts_to_sequences([str(txt) for txt in df_test['text'].values]))\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n\ny_test = df_test['label'].values","da3bc6f1":"scores = model.evaluate(x_test, y_test, batch_size=128)\nprint(\"The model has a test loss of %.2f and a test accuracy of %.1f%%\" % (scores[0], scores[1]*100))","769d6cb3":"Getting the test labels as well","2b46a912":"Adding the LSTM layer to the model, we need to set the number of units. This number corresponds to the dimensionality of the output space and thus also the dimensionality of the cell state, the hidden state, and the neural network gates.","88e5fa0d":"We create an embedding layer by specifying the input size of the mapping (number of words in vocabulary + 1), output size (length of vector representation), and input length (amount of words per sequence).\n\nWe add 1 to the vocab size because 0 is a reserved value that we use for padding.","f7b1c8b2":"![Embedding Layer](https:\/\/miro.medium.com\/max\/1596\/1*1hPDk0gPyIBg0D5SzY5t7Q.png)\n\nSource: https:\/\/towardsdatascience.com\/what-the-heck-is-word-embedding-b30f67f01c81","10e93c8e":"In this notebook, we're going to make a simple model to classify an article as promotional or not promotional. This notebook serves as a basic beginner's guide to LSTMs and text processing.","d14238e8":"**An issue with word representation:**\n\nRecall that when we vectorized our data, we converted each word into indices. This allows words to be processed, but it creates the issue that the values lose their meaning. In image data, the magnitude of a datapoint corresponds the the brightness of a pixel, so a larger value will correspond to a brighter pixel. However, the magnitude of datapoints in a vectorized dataset don't have that meaning -- a point with value 5000 doesn't necessarily have more meaning than a point with the value 0. If we represent the text as categorical data with one-hot arrays, we get the issue of sparcity: with a vocabulary of 50000 words, where only 400 words are used per document, this method would be extremely inefficient.\n\nThis is why we use an embedding layer. An embedding layer is simply a matrix where each row corresponds to a representation of a word in the vocabulary. Each representation is a 1D vector of real numbers that is learned when training. Now, we can process words with vectors of managable, fixed sizes and, because these representations are learned, they often are semantically reasonable as well.","bf91f5ca":"# Testing","f7b0f00f":"# Import Data","c68fb912":"Converting the test set using the previously trained tokenizer","92ea5bc5":"# Training","748046ab":"In both datasets, we only need the text.","73f9f670":"This model alone obviously isn't ideal: a couple ideas for improvement are to increase the number of epochs, change the intial learning rate (by setting the parameter \"learning_rate\" in Adam()), or change the number of units in the LSTM.","a53d84a9":"We don't need to train for too many epochs as this model will overfit fairly easily.","5682e3f2":"Embedding layers aren't limited to word representations either, they can be useful in many types of data, especially when it is sparse.","c8e15a56":"Evaluating the model","7b91c804":"Since we're doing binary classification, we're using [binary cross entropy loss](https:\/\/towardsdatascience.com\/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a). The Adam optimizer is used to train to model.","0268c450":"**LSTM Layer**\n\nInitially, Recurrent Neural Networks (RNNs) allowed for neural networks to effectively process time series data by taking in past outputs as input:\n\n![rnn](http:\/\/www.easy-tensorflow.com\/images\/NN\/01.png)\n\nSource: http:\/\/www.easy-tensorflow.com\/tf-tutorials\/recurrent-neural-networks\/vanilla-rnn-for-classification\n\nHowever, this type of network had a very short memory. It wasn't possible for the network to take into account context information from very far in the past, something very important for processing documents. LSTMs offered a solution through a structure that consists of a cell state, hidden state, and multiple gates:\n\n![lstm](https:\/\/www.researchgate.net\/publication\/324600237\/figure\/fig3\/AS:616974623178753@1524109621725\/Long-Short-term-Memory-Neural-Network.png)\n\nSource: https:\/\/www.researchgate.net\/figure\/Long-Short-term-Memory-Neural-Network_fig3_324600237\n\nThe hidden state (\\\\(h_{t-1}\\\\)) of an LSTM consists of the output from the previous time step.\nThe cell state (\\\\(C_{t}\\\\)) passes through each time step, and its alteration is decided by a series of gates. The gates are essentially one layer neural networks that use the current input along with the hidden state to decide what values within the cell state to forget and what new information to add. Another gate also decides what information to output.\n\nThis is an extreme oversimplification and I would recommend anyone interested to check out this [great article](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/).","3ed671a8":"Our data is structured where each row corresponds to the text of one article.","75d32012":"Currently our data has all neutral articles first and all promotional articles second. The promotional articles are alphabetically sorted as well. Because of this, we're going to randomize the order.","b8164c6a":"When we process text data, we need to vectorize it, that is, convert it to data that can be interpreted by the model. To do this, we're using the Tokenizer class in Keras.\n\nTokenizer preprocesses the words, removing symbols and making the text lowercass, then adds all unique words to a dictionary. When we then vectorize text, that text gets converted to a sequence of indices, corresponding to a word's position in the dictionary.","a235c245":"We need to then convert our data to a fixed shape. The following Keras function will pad each document with a default value of 0 if a given sequence is smaller than the target sequence length and truncate if it is larger.","501bc393":"It will be easier to use we combine both into one dataframe. When we do this, we need to add labels corresponding to promotional or neutral as well. Here, I'm using promotional = 1 and neutral = 0.","ef6a476f":"To complete the classifier, we can then add two dense layers, the last one being a sigmoid output, producing a range between 0 and 1, corresponding to not promotional and promotional respectively.\nThe dropout layer randomly sets a proportion of its input units to zero. This is added to prevent overfitting as it reduces the neural network's dependence on certain features.","ee7d5486":"# Creating the Model","3a6a87d6":"Prior to training, we need to split the data into training\/testing sets as well.","0bfa5d4f":"# Text Processing"}}