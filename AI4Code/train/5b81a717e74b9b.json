{"cell_type":{"1298ea54":"code","4af03bb4":"code","955d0f3f":"code","192b2601":"code","4f5f5f87":"code","74d9000b":"code","a6f3ad38":"code","b9074abf":"code","e14f2ccc":"code","a3f2c99d":"code","32bf8232":"code","580e440b":"code","fbe23298":"code","f6aaab5c":"code","ef78a705":"code","a50d77c4":"code","22055787":"code","1dfb7138":"code","255246d3":"code","adcc66a6":"code","557e041e":"code","8faefe1b":"code","4138761f":"code","8e73e432":"code","eae0275e":"code","966454f2":"code","944375e3":"code","32b13e9d":"code","50097c87":"code","ae7fd012":"code","f6627b7e":"code","456c5344":"code","254d040b":"code","cf9b1fdc":"code","7b345d17":"code","5f1dce91":"code","4d0b4164":"code","598f9c5a":"code","fc996e86":"code","09ecb92d":"code","c634b3e8":"code","f9bded87":"code","1a208e42":"code","b717d24a":"code","f13f54a0":"code","691e3fa9":"code","b6bcb075":"code","24f6ef36":"code","75790e6b":"code","1e384045":"code","b3b7911e":"code","9263bc6e":"code","fe087ab8":"code","018d8f8a":"code","2b62bfaf":"code","d2535afb":"code","e17a61c5":"code","0c657e8f":"code","908fc54f":"code","c5ecc84b":"code","457abd8e":"code","a8a3d94f":"code","3db2a5f7":"code","d91a3080":"code","8d735bb3":"code","354a7278":"code","49ce7bf8":"code","df749bdc":"code","6bc0f829":"code","95b9b78a":"code","49985047":"code","bb6668f9":"code","c32710e5":"code","a419ad96":"code","6dcc5cf7":"code","a57199c7":"code","90f974f0":"code","7c4b3d0d":"code","5bd0c3a9":"code","2a11ffa1":"code","289f54db":"code","fec4a987":"code","5c5c6d79":"code","deb342ba":"code","a6223961":"code","960ba4cd":"code","2504766b":"code","39d4fc71":"code","c8ef93f0":"code","da2dc8c1":"code","6602bdae":"code","4968e991":"code","130e41d0":"code","748f1014":"code","c263bbbc":"code","c7e36fab":"code","ba093018":"code","019331f8":"code","093f3d11":"code","e694685a":"code","e98ebe71":"code","edb9a0d6":"code","6d6016c9":"code","43a0e898":"code","16a5914f":"code","e4ecda17":"code","cc9327af":"code","877e37e4":"code","d64fd34f":"code","33ac570a":"code","5d241a94":"code","157df9dc":"code","542f87dd":"code","59a2ec9a":"markdown","49681a7c":"markdown","13b0e649":"markdown","34ccfc87":"markdown","7b433f01":"markdown","7fbe5fe8":"markdown","036f7898":"markdown","d45fabb0":"markdown","6d01a97b":"markdown","2986cdc0":"markdown","3f968821":"markdown","be1b8c54":"markdown","29108e4a":"markdown","546f1cbd":"markdown","b7923b45":"markdown"},"source":{"1298ea54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4af03bb4":"import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport catboost\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nwarnings.filterwarnings(\"ignore\")","955d0f3f":"df_sample = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","192b2601":"df_sample[0:5]","4f5f5f87":"df_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')","74d9000b":"df_train[0:5]","a6f3ad38":"df_train.shape","b9074abf":"df_shop =pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')","e14f2ccc":"df_shop[0:5]","a3f2c99d":"df_item = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')","32bf8232":"df_item[0:5]","580e440b":"df_test = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')","fbe23298":"df_test.tail()","f6aaab5c":"df_test[0:5]","ef78a705":"df_item_categories =pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')","a50d77c4":"df_item_categories[0:5]","22055787":"df_shop.isnull().values.any()","1dfb7138":"df_train = df_train.merge(df_shop,on = ['shop_id'])","255246d3":"df_train[0:5]","adcc66a6":"df_shop['shop_name'][df_shop['shop_id'] == 59]","557e041e":"df_train = df_train.merge(df_item,on = ['item_id'])","8faefe1b":"df_train[0:5]","4138761f":"df_item[df_item['item_id'] == 22154]","8e73e432":"df_train = df_train.merge(df_item_categories,on =['item_category_id'])","eae0275e":"df_train[0:5]","966454f2":"df_item_categories[df_item_categories['item_category_id'] == 37]","944375e3":"df_train.isnull().values.any()","32b13e9d":"df_train.describe()","50097c87":"df_train.date = df_train.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))","ae7fd012":"df_train['date'].describe()","f6627b7e":"df_train['date'].min().date()","456c5344":"df_train['date'].max().date()","254d040b":"# the skewed box plot shows us the presence of outliers\n%matplotlib notebook\n%matplotlib inline\nsns.boxplot(y=\"item_price\", data =df_train)\nplt.show()","cf9b1fdc":"#calculating 0-100th percentile to find a the correct percentile value for removal of outliers\nfor i in range(0,100,10):\n    var = df_train[\"item_price\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is \",var[-1])","7b345d17":"#looking further from the 99th percecntile\nfor i in range(90,100):\n    var = df_train[\"item_price\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is \",var[-1])","5f1dce91":"#calculating speed values at each percntile 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\nfor i in np.arange(0.0, 1.0, 0.1):\n    var =df_train[\"item_price\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","4d0b4164":"plt.plot(var[-5:])\nplt.show()","598f9c5a":"plt.plot(var[-2:])\nplt.show()","fc996e86":"# the skewed box plot shows us the presence of outliers\n%matplotlib notebook\n%matplotlib inline\nsns.boxplot(y=\"item_cnt_day\", data =df_train)\nplt.show()","09ecb92d":"#calculating 0-100th percentile to find a the correct percentile value for removal of outliers\nfor i in range(0,100,10):\n    var = df_train[\"item_cnt_day\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is \",var[-1])","c634b3e8":"#looking further from the 99th percecntile\nfor i in range(90,100):\n    var = df_train[\"item_cnt_day\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is \",var[-1])","f9bded87":"#calculating speed values at each percntile 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\nfor i in np.arange(0.0, 1.0, 0.1):\n    var =df_train[\"item_cnt_day\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","1a208e42":"plt.plot(var[-20:])\nplt.show()","b717d24a":"plt.plot(var[-3:])\nplt.show()","f13f54a0":"df_train = df_train[df_train['item_price']<50000]","691e3fa9":"df_train.shape","b6bcb075":"df_train = df_train[df_train['item_cnt_day']<750]","24f6ef36":"df_train = df_train[df_train['item_cnt_day']>=0]","75790e6b":"df_train.shape","1e384045":"print('Data set size before filter valid:', df_train.shape)\n# Only shops that exist in test set.\ndf_train = df_train[df_train['shop_id'].isin(df_test['shop_id'].unique())]\n# Only items that exist in test set.\ndf_train = df_train[df_train['item_id'].isin(df_test['item_id'].unique())]\nprint('Data set size after filter valid:', df_train.shape)","b3b7911e":"df_train[0:5]","9263bc6e":"# Aggregate to monthly level the sales\ndf_train_groupby= df_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    \"item_price\",\"item_cnt_day\"].agg({\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})","fe087ab8":"df_train_groupby = df_train_groupby.reset_index()","018d8f8a":"df_train_groupby[0:5]","2b62bfaf":"df_train_groupby.shape","d2535afb":"a =  list(df_train_groupby.item_cnt_day)","e17a61c5":"a","0c657e8f":"df_test[0:5]","908fc54f":"df_test.shape","c5ecc84b":"final_data = pd.merge(df_test,df_train_groupby,on = ['item_id','shop_id'],how = 'left')","457abd8e":"final_data[0:5]","a8a3d94f":"final_data.shape","3db2a5f7":"final_data.isnull().values.any()","d91a3080":"final_data.fillna(0,inplace = True)","8d735bb3":"final_data.isnull().values.any()","354a7278":"final_data.drop(['shop_id','item_id'],inplace = True, axis = 1)\n","49ce7bf8":"final_data[0:5]","df749bdc":"#We will create pivot table.\n# Rows = each shop+item code\n# Columns will be out time sequence\npivot_data = final_data.pivot_table(index='ID',values = 'item_cnt_day' ,columns='date_block_num',fill_value = 0 )","6bc0f829":"pivot_data.head()","95b9b78a":"row = 34","49985047":"pivot_data.values[0:1][0][row-3-1: row -1]","bb6668f9":"sum(a[25:])","c32710e5":"a  =(pivot_data[0:1].values)[0]","a419ad96":"def simple_average(pivot_data,row_no,col_no,window_size):\n    predicted_values = []\n    for i in range(row_no):\n        temp = pivot_data.values[i:i+1][0]\n        #print(temp[col_no-window_size-1:col_no-1])\n        predict_value = int(sum(temp[col_no-window_size-1:col_no-1])\/window_size)\n        #print(predict_value)\n        predicted_values.append(predict_value)\n        \n    return predicted_values","6dcc5cf7":"def actual_values(pivot_data):\n    actual = []\n    temp = pivot_data.shape[0]\n    for i in range(temp):\n        temp = pivot_data.values[i:i+1][0]\n        actual.extend([int(temp[-1:])])\n    return actual","a57199c7":"actual = actual_values(pivot_data)","90f974f0":"def rms_error(predicted,actual):\n    final =   list(np.array(actual) - np.array(store))\n    mse = sum([e**2 for e in final])\/len(final)\n    return mse","7c4b3d0d":"# here we have predicted the october item_cnt using the previous three months\nerror_values = []\nwindow_lenght = 6\nfor i in range(2,window_lenght):\n    store = simple_average(pivot_data,pivot_data.shape[0],pivot_data.shape[1],i)\n    error = rms_error(store,actual)\n    error_values.extend([error])","5bd0c3a9":"# here we have use window lenght from 2 to 5\nprint(error_values)","2a11ffa1":"def predict_nov_simple_average(pivot_data,row_no,col_no,window_size):\n    predicted_values = []\n    for i in range(row_no):\n        temp = pivot_data.values[i:i+1][0]\n        #print(temp[col_no-window_size:col_no])\n        predict_value = int(sum(temp[col_no-window_size:col_no])\/window_size)\n        #print(predict_value)\n        predicted_values.append(predict_value)\n        \n    return predicted_values","289f54db":"# we are getting the best result from window_size of 3 so we will use window_size of 3 to predict value for november data\npredicted_nov_savg = predict_nov_simple_average(pivot_data,pivot_data.shape[0],pivot_data.shape[1],3)","fec4a987":"len(predicted_nov_savg)","5c5c6d79":"predicted_nov_savg","deb342ba":"def weighted_average(pivot_data,row_no,col_no,window_size):\n    predicted_values = []\n    for i in range(row_no):\n        temp = pivot_data.values[i:i+1][0]\n        #print(temp[col_no-window_size-1:col_no-1])\n        temp = temp[col_no-window_size-1:col_no-1]\n        store = 0\n        for j in range(1,window_size+1):\n            store = store + j*temp[j-1]\n        predict_value = int(store\/((window_size*(window_size+1))\/2))\n        #print(predict_value)\n        predicted_values.append(predict_value)\n        \n    return predicted_values","a6223961":"# here we have predicted the october item_cnt using the previous three months\nerror_values = []\nwindow_lenght = 6\nfor i in range(2,window_lenght):\n    store = weighted_average(pivot_data,pivot_data.shape[0],pivot_data.shape[1],i)\n    error = rms_error(store,actual)\n    error_values.extend([error])","960ba4cd":"error_values","2504766b":"#here the window size of 4 gives the best result so we will use window size of 4 to predict the item_cnt for month november\ndef predict_nov_weighted_average(pivot_data,row_no,col_no,window_size):\n    predicted_values = []\n    for i in range(row_no):\n        temp = pivot_data.values[i:i+1][0]\n        temp = temp[col_no-window_size:col_no]\n        #print(temp[col_no-window_size:col_no])\n        store =0\n        for j in range(1,window_size+1):\n            store = store + j*temp[j-1]\n        predict_value = int(store\/((window_size*(window_size+1))\/2))\n        #print(predict_value)\n        predicted_values.append(predict_value)\n        \n    return predicted_values","39d4fc71":"predicted_nov_weighted_average = predict_nov_weighted_average(pivot_data,pivot_data.shape[0],pivot_data.shape[1],4)","c8ef93f0":"len(predicted_nov_weighted_average)","da2dc8c1":"predicted_nov_weighted_average","6602bdae":"def exponential_average(pivot_data,row_no,col_no,alpha):\n    predicted_values = []\n    for i in range(row_no):\n        temp = pivot_data.values[i:i+1][0]\n        #print(temp[:-1])\n        temp = temp[:-1]\n        store = 0\n        for j in range(len(temp)):\n            store = store*(1-alpha) + alpha*temp[j]\n        #print(predict_value)\n        store = int(store)\n        predicted_values.append(store)\n        \n    return predicted_values","4968e991":"# here we have predicted the october item_cnt using the previous three months\nerror_values = []\nwindow_lenght = [0.3,0.4,0.5,0.6,0.7]\nfor i in window_lenght:\n    store = exponential_average(pivot_data,pivot_data.shape[0],pivot_data.shape[1],i)\n    error = rms_error(store,actual)\n    error_values.extend([error])","130e41d0":"error_values","748f1014":"def exponential_average_nov(pivot_data,row_no,col_no,alpha):\n    predicted_values = []\n    for i in range(row_no):\n        temp = pivot_data.values[i:i+1][0]\n        #print(temp[:-1])\n        #temp = temp[:-1]\n        store = 0\n        for j in range(len(temp)):\n            store = store*(1-alpha) + alpha*temp[j]\n        #print(predict_value)\n        store = int(store)\n        predicted_values.append(store)\n        \n    return predicted_values","c263bbbc":"predicted_nov_exponential_average = exponential_average_nov(pivot_data,pivot_data.shape[0],pivot_data.shape[1],0.4)","c7e36fab":"len(predicted_nov_exponential_average)","ba093018":"# one thing to consider while autoregressive model is that the value of lag means how many previous values you are going to use\n# we can plot autocorelation to find out the value of lag.\n# here we are plotting the curve for id 0\nfig, axs = plt.subplots(2, 2)\naxs[0,0].plot(pivot_data.values[0:1][0])\naxs[0,1].plot(pivot_data.values[1:2][0])\n\naxs[1,0].plot(pivot_data.values[2:3][0])\naxs[1,1].plot(pivot_data.values[3:4][0])\n\n#plt.plot(series)\nplt.show()","019331f8":"from statsmodels.graphics.tsaplots import plot_acf\n#fig, axs = plt.subplots(2, 2)\n\nplot_acf(pivot_data.values[0:1][0],lags = 33)\nplot_acf(pivot_data.values[1:2][0],lags = 33)\nplot_acf(pivot_data.values[2:3][0],lags = 33)\nplot_acf(pivot_data.values[3:4][0],lags = 33)\n\nplt.show()","093f3d11":"def auto_regressive(pivot_data,row_no,lag):\n    predicted_values = []\n    for i in range(row_no):\n        data = pivot_data.values[i:i+1][0]\n        #print(data[:-1])\n        model = AutoReg(data[:-1],lags = lag).fit()\n        #print(model.params)\n        predict_value = model.predict(start=len(data)-1,end=len(data)-1)\n        #print(predict_value)\n        predicted_values.append(int(predict_value))  \n        \n    return predicted_values","e694685a":"'''def auto_regressive_2(pivot_data,row_no,lag):\n    predicted_values = []\n    for i in range(row_no):\n        data = pivot_data.values[i:i+1][0]\n        data = data[:-1]\n        window = len(data)\n        #print(window)\n        model = AutoReg(data[:-1],lags = lag).fit()\n        parameters = model.params\n        yhat = parameters[0]\n        for j in range(lag):\n            yhat += parameters[j+1] * data[window-j-1]\n        #print(model.params)\n        #print(yhat)\n        #predict_value = model.predict(start=len(data)-1,end=len(data)-1)\n        #print(predict_value)\n        predicted_values.append(int(yhat))  \n        \n    return predicted_values'''","e98ebe71":"'''store = auto_regressive_2(pivot_data,pivot_data.shape[0],4)'''","edb9a0d6":"'''print(rms_error(store,actual))'''","6d6016c9":"# here we have multiple time series for each id \n# we will fit model on series one by one \n# and then check oveall mean squared error\nfrom statsmodels.tsa.ar_model import AutoReg\n\nerror_values = []\nlag = [2,3,4,5]\nfor i in lag:\n    store = auto_regressive(pivot_data,pivot_data.shape[0],i)\n    error = rms_error(store,actual)\n    error_values.extend([error])\n","43a0e898":"# the result are not good while using auto regressive model\nerror_values","16a5914f":"# X we will keep all columns execpt the last one \nX_train = np.expand_dims(pivot_data.values[:,:-1],axis = 2)\n# the last column is our prediction\ny_train = pivot_data.values[:,-1:]","e4ecda17":"X_train.shape","cc9327af":"y_train.shape","877e37e4":"# for test we keep all the columns execpt the first one\nX_test = np.expand_dims(pivot_data.values[:,1:],axis = 2)","d64fd34f":"from keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout\nfrom keras.models import load_model, Model\n\n# our defining sales model \nmodel = Sequential()\nmodel.add(LSTM(units = 64,input_shape = (33,1)))\n#sales_model.add(LSTM(units = 64,activation='relu'))\nmodel.add(Dense(128))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1))\n","33ac570a":"model.summary()","5d241a94":"model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\nmodel.fit(X_train,y_train,batch_size = 300,epochs = 10)","157df9dc":"output_november_sales = model.predict(X_test)","542f87dd":"output_november_sales[0:10]","59a2ec9a":"### now using simple moving average","49681a7c":"### now using autoregressive model","13b0e649":"setting thresold 750","34ccfc87":"merging df_train and df_item_categories on item_category_id","7b433f01":"merging df_train and df_shop on shop id","7fbe5fe8":"merging df_train and df_item on item id","036f7898":"### using exponential average to predict item_cnt for november month","d45fabb0":"* so we can set the thresold of 50000","6d01a97b":"## now we have predict the sale of the item related to the id for november month","2986cdc0":"### now using lstm model for prediction","3f968821":"## data cleaning","be1b8c54":"### item_price","29108e4a":"### item_cnt_day","546f1cbd":"conclusion : the shops that are in test data are all present in train data,but train data doesnot consist of all the item that are present in the test data. So we have to fill the missing values.","b7923b45":"### using weighted average to predict prediction for novrmber month"}}