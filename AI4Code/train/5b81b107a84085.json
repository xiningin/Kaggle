{"cell_type":{"78ab9f2a":"code","42828687":"code","d7fd891a":"code","ae96a50d":"code","454e53ae":"code","ec71b202":"code","3f4fa1e2":"code","52c92d61":"code","312d549b":"code","598e7778":"code","375a64f2":"code","2074554d":"code","678d9cb8":"code","106c57ff":"code","9d15a792":"code","782dbc45":"code","8e3d6cbe":"code","4342ca28":"code","12c6ed4b":"code","75e6e3d3":"code","6bd8b4f2":"code","d40970fa":"code","f7e1f7f8":"code","349e1c2a":"code","c27fc6d8":"code","a97cdc27":"code","86e7cb5b":"code","2067dc56":"code","7c1ef2cd":"code","0c2d3873":"code","70cfb853":"markdown","ca245b47":"markdown","0292d03a":"markdown","5e79f48c":"markdown","9db79323":"markdown","2a62c8ce":"markdown","c2995442":"markdown"},"source":{"78ab9f2a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42828687":"#Manipula\u00e7\u00e3o dos dados\nimport pandas as pd\nimport numpy as np\n\n#Visualiza\u00e7\u00e3o dos dados\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d7fd891a":"#Leitura dos dados\ntrain = pd.read_csv(\"..\/input\/usp-pj02\/train_car_details.csv\")\ntest = pd.read_csv(\"..\/input\/usp-pj02\/test_car_details.csv\")","ae96a50d":"#Armazena o Id do teste em uma vari\u00e1vel auxiliar\nId = test.Id \ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#Remove o Id no treino\ntrain.drop(\"Id\", axis = 1, inplace = True) #remove coluna Id","454e53ae":"#Dimens\u00f5es dos dados de treino\ntrain.shape","ec71b202":"#Informa\u00e7\u00f5es sobre as colunas (tipos, quantidade de valores n\u00e3o nulos)\ntrain.info()","3f4fa1e2":"#Data frame com quantidade de NA por colunas (valor absoluto e porcentual)\nnulos = pd.concat([train.isna().sum(), round((train.isna().sum()\/train.shape[0])*100, 2)], axis = 1).reset_index()\nnulos.columns = [\"coluna\", \"absoluto\", \"porcentagem\"]\nnulos ","52c92d61":"#naqueles que tem milage nulo, quais outras colunas eles tem nulas\ntrain[train.mileage.isna()].isna().sum() \n\n#da pra perceber q h\u00e1 157 observa\u00e7\u00f5es com milage, engine, torque e seats vazio","312d549b":"#Vou retirar todas aquelas observa\u00e7\u00f5es que tem as 4 colunas vazias (pois \u00e9 apenas 2% dos dados).\n#OBS: Tentei arrumar eles em outro c\u00f3digo, mas o resultado no final para a predi\u00e7\u00e3o n\u00e3o melhorou.\ntrain = train[~train.mileage.isna()]","598e7778":"#Quantos valores NA sobram.\ntrain.isna().sum()","375a64f2":"#Vou a coluna torque, pois tem v\u00e1rios valores com uma estrutura completamente aleat\u00f3ria.\n#OBS: Tentei arrumar eles em outro c\u00f3digo, mas o resultado no final para a predi\u00e7\u00e3o n\u00e3o melhorou.\ntrain.drop(\"torque\", axis = 1, inplace = True)","2074554d":"def getBrand(df):\n    \"\"\"getBrand\n    \n    Objetivo: Pegar a marca do carro a partir do seu nome\n    \n    Input:\n        df - data frame com os dados que ser\u00e3o modificados\n        \n    Output:\n        df - data frame com os dados modificados (marca do carro no lugar de seu nome completo)\n    \"\"\"\n    \n    df[\"name\"] = df[\"name\"].str.split().apply(lambda x: x[0])\n    \n    return df\n\ntrain = getBrand(train)","678d9cb8":"def getMileage(df):\n    \"\"\"getMileage\n    \n    Objetivo: Pegar a efici\u00eancia do carro com rela\u00e7\u00e3o a combust\u00edvel e a unidade de medida.\n    \n    Input:\n        df - data frame com os dados que ser\u00e3o modificados\n        \n    Output:\n        df - data frame com os dados modificados \n              (coluna Mileage separada em valor e unidade da efic\u00eancia\n              com rela\u00e7\u00e3o ao uso de combust\u00edvel).\n    \"\"\"\n    \n    df[\"mileage_value\"] = pd.to_numeric(df.mileage.str.split().apply(lambda x: x[0]))\n    df[\"mileage_unit\"] = df.mileage.str.split().apply(lambda x: x[1])\n    \n    return df\n\ntrain = getMileage(train)\ntrain.drop(\"mileage\", axis = 1, inplace = True)","106c57ff":"def getEngine(df):\n    \"\"\"getEngine\n    \n    Objetivo: Pegar o \"motor\" do carro.\n    \n    Input:\n        df - data frame com os dados que ser\u00e3o modificados\n        \n    Output:\n        df - data frame com os dados modificados (valor num\u00e9rico das cilindradas que o carro tem).\n    \"\"\"\n    \n    df[\"engine\"] = pd.to_numeric(df[\"engine\"].str.split().apply(lambda x: x[0]))\n    \n    return df\n\ntrain = getEngine(train)","9d15a792":"def getPower(df):\n    \"\"\"getPower\n    \n    Objetivo: Pegar a pot\u00eancia do carro.\n    \n    Input:\n        df - data frame com os dados que ser\u00e3o modificados\n        \n    Output:\n        df - data frame com os dados modificados (valor num\u00e9rico da pot\u00eancia que o carro tem).\n    \"\"\"\n    \n    \n    df[\"power\"] = pd.to_numeric(df[\"max_power\"].str.split().apply(lambda x: x[0] if len(x) > 1 else np.nan))\n    df.drop(\"max_power\", axis = 1, inplace = True)\n    \n    return df\n\ndef cleanPower(df):\n    \"\"\"cleanPower\n    \n    Objetivo: Coloca valor para pot\u00eancia do carro que n\u00e3o tinha valor antes.\n              O valor que ser\u00e1 colocado \u00e9 a mediana dos valores de pot\u00eancia\n                dos carros da mesma marca.\n            \n    Input:\n        df - data frame com os dados que ser\u00e3o modificados\n        \n    Output:\n        df - data frame com os dados modificados (valores NA preenchidos).\n    \"\"\"\n\n    df[\"power\"] = df.groupby(\"name\").power.transform(lambda x: x.fillna(x.median()))\n        \n    return df\n    \n    \ntrain = getPower(train)\ntrain = cleanPower(train)","782dbc45":"#Aplica no teste todas as fun\u00e7\u00f5es que apliquei no treino.\ntest = getBrand(test)\n\ntest = getMileage(test)\ntest.drop(\"mileage\", axis = 1, inplace = True)\n\ntest = getEngine(test)\n\ntest = getPower(test)\ntest = cleanPower(test)\n\ntest.drop(\"torque\", axis = 1, inplace = True)","8e3d6cbe":"#Mudei pra Maruti pq s\u00e3o marcas de carros populares e s\u00f3 aparece 1 no teste,\n#  ent\u00e3o n\u00e3o deve afetar muito o modelo e o pre\u00e7o deve ser parecido.\ntest = test.replace(\"Opel\", \"Maruti\") ","4342ca28":"#Converte os valores das vari\u00e1veis categ\u00f3ricas para num\u00e9rico\n#  Exemplo para marca de carro: (Maruti -> 26, Tata -> 20, etc.) \nfrom sklearn.preprocessing import LabelEncoder\nlbl = LabelEncoder()\nfor col in train.columns.values:\n    if train.loc[:,col].dtype == \"object\":\n        lbl.fit(train.loc[:,col].astype(str))\n        train.loc[:,col] = lbl.transform(train.loc[:,col].astype(str))\n        test.loc[:,col] = lbl.transform(test.loc[:,col].astype(str))","12c6ed4b":"#Gera um mapa de correla\u00e7\u00f5es.\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1,vmin = -1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","75e6e3d3":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import r2_score","6bd8b4f2":"df = train.copy()\n\ncolumns = df.columns.values\ntarget = \"selling_price\"\ny_columns = [target]\nx_columns = [x for x in columns if x != target]\n\nX = df[x_columns]\ny = df[y_columns]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","d40970fa":"# S\u00d3 ESTOU FAZENDO A \u00c1RVORE DE DECIS\u00c3O POR CURIOSIDADE!\n# SEI QUE ESTAREI CAUSANDO OVERFIT SE ESCOLHER ELA PELO R2 NO TESTE ESTAR ALTO.\n\nmax_depth = np.arange(1, 51, 2)\ntrain_error = []\ntest_error = []\n\nfor i in max_depth:\n    model = DecisionTreeRegressor(criterion = \"mse\", splitter = \"best\", max_depth = i)\n    model.fit(X_train, y_train)\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    train_error.append(r2_score(y_train, y_pred_train))\n    test_error.append(r2_score(y_test, y_pred_test))\n    \n\nplt.figure(figsize=(6,4))\nplt.plot(max_depth, train_error, '-bo', color = \"red\", label = \"Train Error\")\nplt.plot(max_depth, test_error, '-bo', color= \"blue\", label = \"Test Error\")\nplt.xlabel('max_depth', fontsize = 15)\nplt.ylabel('R2', fontsize = 15)\nplt.xticks(max_depth)\nplt.legend()\nplt.show(True)\n\nprint(\"Test - m\u00e9dia {} ({})\".format(np.mean(test_error[3:]), np.var(test_error[3:])))","f7e1f7f8":"%%time\n\nparameters = {'max_depth':np.arange(5, 20, 2), \"n_estimators\":[2000],  \"criterion\":[\"mse\"]}\n\nrf_model = GridSearchCV(RandomForestRegressor(), parameters,\n                    cv = 10, scoring = \"r2\", n_jobs = -1, verbose = 3,\n                    refit = True)\n\nrf_model.fit(X_train, y_train.to_numpy().ravel())\ny_pred_train = rf_model.predict(X_train)\n\n\nprint(\"Melhor modelo: {}\".format(rf_model.best_estimator_))\nprint(\"Melhor score: {}\".format(rf_model.best_score_))\n","349e1c2a":"%%time\n\nparameters = {'max_depth':np.arange(2, 7, 1), 'learning_rate':[0.01],\n             \"n_estimators\":[2000], \"loss\":[\"ls\"], \"criterion\":[\"mse\"]}\n\ngb_model = GridSearchCV(GradientBoostingRegressor(), parameters,\n                    cv = 10, scoring = \"r2\", n_jobs = -1, verbose = 3,\n                    refit = True)\n\ngb_model.fit(X_train, y_train.to_numpy().ravel())\ny_pred_train = gb_model.predict(X_train)\n\n\nprint(\"Melhor modelo: {}\".format(gb_model.best_estimator_))\nprint(\"Melhor score: {}\".format(gb_model.best_score_))","c27fc6d8":"print(\"R2 Final {}\".format(r2_score(y_test, gb_model.predict(X_test))))","a97cdc27":"feature_importance = gb_model.best_estimator_.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X.columns)[sorted_idx])\nplt.title('Feature Importance (MDI)')","86e7cb5b":"#Melhores hiperpar\u00e2metros escolhidos pelo GridSearch\ngb_model.best_estimator_","2067dc56":"y_pred = gb_model.best_estimator_.predict(test)","7c1ef2cd":"predicoes = pd.DataFrame(data = {\"Id\":Id.values, \"selling_price\":y_pred})\npredicoes.to_csv(\"resposta.csv\",index = False, sep = \",\", decimal = \",\", float_format = str)","0c2d3873":"pd.read_csv(\"resposta.csv\")","70cfb853":"### Regression Tree","ca245b47":"## Modelagem","0292d03a":"### Random Forest","5e79f48c":"### Gradient Boosting","9db79323":"### Feature Importance para o Gradient Boost","2a62c8ce":"Entre o Random Forest e o Gradient Boosting, eu escolho o segundo, pelo tempo que demora pra rodar e pelo valor do R2","c2995442":"## Visualiza\u00e7\u00e3o"}}