{"cell_type":{"7bec1379":"code","23f9bec0":"code","29939e15":"code","7eb3e536":"code","968efeec":"code","8860a139":"code","f554d591":"code","0c9d90d4":"code","d3af2af1":"code","ca113577":"code","6b77c68a":"code","0bcc47c5":"code","70574558":"code","90a357ac":"code","72503de6":"code","ad0a20e3":"code","b223d10f":"code","4334aab4":"code","b586b0f9":"code","54056e9e":"code","2a979f11":"code","bcd35536":"code","cc6c2159":"code","642c1ab0":"code","a53feee7":"code","ddd87b5b":"code","44ecbf3a":"code","f298e9ec":"code","83ebc9ec":"code","97977882":"code","592762e8":"code","7168a12c":"code","1fca79a0":"code","5f3ae887":"code","017dd285":"code","d0815c08":"code","52180970":"code","b21a8a02":"code","2d68e44a":"code","88a85f81":"code","62fa7fdd":"code","6bcb39b7":"code","37320651":"markdown","f0ab32ec":"markdown","00a563c5":"markdown","3be752c9":"markdown","090eefdd":"markdown","f8959c3e":"markdown","34e24eff":"markdown","03ba1b23":"markdown","f988ae28":"markdown","355712c2":"markdown","a907db14":"markdown","16522e27":"markdown","20f67f3f":"markdown","c7b7e4ff":"markdown","5b647bb2":"markdown"},"source":{"7bec1379":"# normal libraries\nimport pandas as pd\nimport numpy as np\n\n# importing graph libraries \nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\n\n# With Gridspec you can make static dashboards\nfrom matplotlib.gridspec import GridSpec\n\n\n# Machine learning libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom scipy.stats import norm\nfrom scipy.stats import probplot\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# warning libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Maps\nimport folium\n\n# Very powerfull plugin for maps\nfrom folium.plugins import FastMarkerCluster, HeatMap\n\n\n# Deep leerning imports\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential # for creating the model\nfrom tensorflow.keras.layers import Dense # for creating the layers\n\n\n#  Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import r2_score\n\n\n\n\n\n","23f9bec0":"# importing the dataframe \ndf = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","29939e15":"df.head()","7eb3e536":"lol = df[\"condition\"].value_counts().reset_index().sort_values(by = \"condition\", ascending = False)\nvalues = lol[\"condition\"]\nindex = [\"3\",\"4\",\"5\",\"2\",\"1\"]\nfig = px.pie(lol, values=values, names=index,color_discrete_sequence=['rgb(150,0,0)','rgb(100,40,120)','rgb(200,77,50)'],hole=0.7)\nfig.update_layout(title = \" House conditions\")\nfig.show()\n\n\n","968efeec":"shape = df.shape\nprint(f\"there are {shape[0]} rows and {shape[1]} columns in the dataframe\")","8860a139":"# dropping the id column\ndf_geo = df[[\"zipcode\",\"lat\",\"long\"]]\ndf.drop([\"id\",\"zipcode\",\"lat\",\"long\",\"date\"], axis = 1 , inplace=True)","f554d591":"# checking the null values in the dataframe\n\n\nSum = df.isnull().sum()\nPercentage = (df.isnull().sum()\/df.isnull().count())\ndf_null= pd.concat([Sum,Percentage], axis =1 , keys = [\"Sum\",\"Percentage\"])\ndf_null.style.background_gradient(\"Greens\")\n","0c9d90d4":"# after cleaning \nafter_clean = df.shape\nprint(f\" After cleaning there are {after_clean[0]} rows and {after_clean[1]} columns in this dataframe\")","d3af2af1":"#check correlation in features\n#dropping the second part of the heatmap\ndrop = np.zeros_like(df.corr())\ndrop[np.triu_indices_from(drop)] = True\nsns.heatmap(df.corr(), annot = True, fmt = \".1f\", cmap = \"Blues\", linewidth = 1, mask = drop);\n\nplt.title(\"Correlation\");\nsns.set_style(\"white\")","ca113577":"# Checking the columns that have the most correlation with the column price\ndf_corr = df.corr().iloc[0,:].sort_values(ascending=False).reset_index()\ndf_corr = df_corr[df_corr[\"price\"] < 1]\nsns.barplot(x = \"index\", y = \"price\", data = df_corr);\nplt.xticks(rotation = 90)\nplt.title(\"Correlation with price\");\nplt.grid(True)\nsns.set_style(\"dark\")","6b77c68a":"# checking more correlations\nfig = plt.figure(figsize=(15,15))\nax =fig.add_subplot(2,2,1, projection = \"3d\")\nax.scatter(df[\"sqft_living\"], df[\"grade\"],df[\"price\"], color = \"red\");\nax.set_title(\"Correlation between sqft_living, grade and price \");","0bcc47c5":"log = np.log1p(df[\"price\"])\n\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1);\n\nsns.distplot(df[\"price\"], fit = norm, ax = ax1);\nsns.distplot(log, fit = norm, ax = ax2);\nprobplot(df[\"price\"], plot=ax3);\nprobplot(log, plot=ax4);","70574558":"# checking price\n\nsns.set_style(\"dark\")\n\n\nfig = plt.figure(constrained_layout= True);\n\ngs = GridSpec(3,3, fig)\n\nax = fig.add_subplot(gs[0,:])\nax2 = fig.add_subplot(gs[1,:])\nax3 = fig.add_subplot(gs[2,:])\n\ncount= df[\"price\"].count()\nsumis = df[\"price\"].sum()\nskew = round(df[\"price\"].skew(),2)\nkurt = round(df[\"price\"].kurt(),2)\n\n\nmean_bedrooms = df[\"bedrooms\"].mean()\nmean_bathrooms = df[\"bathrooms\"].mean()\nmean_floors = df[\"floors\"].mean()\n\n\ntogether = {\"name\":[\"bedrooms\",\"bathrooms\",\"floors\"],\"averange\":[mean_bedrooms,mean_bathrooms,mean_floors]}\n\n\ndf2 = pd.DataFrame(together)\n\nax.set_title(\"Figures of price\", fontsize=40, pad = 10, color='dimgrey' )\nax.text(0.25, 0.43, f' $ {sumis}', fontsize=30, color='mediumseagreen', ha='center',        \n        bbox=dict(facecolor='navy', alpha=0.1, pad=10, boxstyle='round, pad=.7'))\nax.text(0.25, 0.81, 'Total sum of price',color='darkslateblue', fontsize=20, ha='center')\n\n\nax.text(0.50, 0.43, f'{skew}', fontsize=40, color='mediumseagreen', ha='center',\n          bbox=dict(facecolor='navy', alpha=0.1, pad=10, boxstyle='round, pad=.4'))\n\nax.text(0.50, 0.81, 'The skew of price:',color='darkslateblue', fontsize=20, ha='center')\n\n\nax.text(0.75, 0.43, f'{kurt}', fontsize=40, color='mediumseagreen', ha='center',\n          bbox=dict(facecolor='navy', alpha=0.1, pad=1, boxstyle='round, pad=.4'))\nax.text(0.75, 0.81, 'The kurtosis of price:',color='darkslateblue', fontsize=20, ha='center')\n\nax2.set_title(\"Boxplot of price\", fontsize=20, pad = 10, color='dimgrey' )\n\nsns.barplot(x = df2[\"name\"], y = df2[\"averange\"], ax = ax3,  palette='YlGnBu')\nsns.boxplot(df[\"price\"],ax = ax2)\n\nax3.set_title(\"Averange of Bath, bedrooms and floors\", fontsize=20, pad = 10, color='dimgrey' )\n\nax.axis(\"off\");\n\n\n","90a357ac":"fig =  go.Figure(go.Bar(x= df2[\"name\"], y = round(df2[\"averange\"],2),\n                       marker={'color': df2['averange'],\n                              'colorscale': 'Viridis'},\n                        text=round(df2['averange'],2),\n                        textposition =  \"outside\"\n                       ))\n\n\nfig.update_layout(title_text = \"Nicer chart of the averange Bath, bedrooms and floors\")\n","72503de6":"lat = df_geo[\"lat\"] \nlong = df_geo[\"long\"]\n\ncordinates = list(zip(lat,long))\n\nmap1 = folium.Map(location=[47.5112,-122.257], zoom_start=10)\n\n\nFastMarkerCluster(data=cordinates).add_to(map1)\n\nmap1","ad0a20e3":"map2 = folium.Map(\n    location=[47.5112,-122.257], \n    zoom_start=10.0)\n\nHeatMap(name='Houses',\n    data=cordinates,\n    radius=10,\n    max_zoom=13\n).add_to(map2)\n\n\nmap2","b223d10f":"\n# A new Dataframe to track all the scores of all the models\nscores = pd.DataFrame({\"Model\":[],\n                       \"Cross_vall_score\":[], \n                       \"Mean_squared_error\":[],\n                       \"R2\":[]})","4334aab4":"df_new = df.copy()\ndf_base = df.copy()","b586b0f9":"X = df_base.drop([\"price\"], axis= 1)\ny = df_base[\"price\"]","54056e9e":" X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.25, random_state = 42)","2a979f11":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlin = LinearRegression()\nlin.fit(X_train,y_train)\npredict = lin.predict(X_test)\n","bcd35536":"\nscore_bass = round(np.sqrt(mean_squared_error(predict,y_test)),2)\nbase_r2=round(r2_score(predict,y_test),2)\nBase_cross = round(cross_val_score(lin,X_train,y_train,cv=5).mean(),2)\n\n\nscores.loc[0] = [\"Simple_linear\", Base_cross,score_bass,base_r2]\nprint(scores)","cc6c2159":"from sklearn.preprocessing import StandardScaler","642c1ab0":"X1 = df_new.drop([\"price\"], axis= 1)\ny1 = df_new[\"price\"]","a53feee7":" X_train1, X_test1, y_train1, y_test1 = train_test_split(X1,y1, test_size= 0.25, random_state = 42)","ddd87b5b":"scaler = StandardScaler()","44ecbf3a":"X_train1 = scaler.fit_transform(X_train1)\nX_test1 = scaler.transform(X_test1)","f298e9ec":"lin = LinearRegression()\nlin.fit(X_train1,y_train1)\npredict1 = lin.predict(X_test1)\ncross_vall_scaler= round(cross_val_score(lin,X_train1,y_train1,cv=5).mean(),2)\nscore_scaler =round(np.sqrt(mean_squared_error(predict1,y_test1)),2)\nr2_scaler= round(r2_score(predict1,y_test1),2)","83ebc9ec":"r = scores.shape[0]\nscores.loc[r] = [\"Simple_linear_scaler\",cross_vall_scaler,score_scaler,r2_scaler]\nprint(scores)","97977882":"from sklearn.linear_model import Lasso, Ridge","592762e8":"las = Lasso(alpha=40)\nlas.fit(X_train,y_train)\npredict3 = las.predict(X_test)\ncross_vall_lass40 = round(cross_val_score(las,X_train,y_train,cv=5).mean(),2)\nscore_lass40 = round(np.sqrt(mean_squared_error(predict3,y_test)),2)\nr2_score_lass40 = round(r2_score(predict3,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"Lasso_Regression40\",cross_vall_lass40,score_lass40,r2_score_lass40]\nprint(scores)\n","7168a12c":"las = Lasso(alpha=100)\nlas.fit(X_train,y_train)\npredict3 = las.predict(X_test)\ncross_vall_lass100 = round(cross_val_score(las,X_train,y_train,cv=5).mean(),2)\nscore_lass100 = round(np.sqrt(mean_squared_error(predict3,y_test)),2)\nr2_score_lass100 = round(r2_score(predict3,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"Lasso_Regression100\",cross_vall_lass100,score_lass100,r2_score_lass100]\nprint(scores)\n","1fca79a0":"las = Lasso(alpha=500)\nlas.fit(X_train,y_train)\npredict3 = las.predict(X_test)\ncross_vall_lass500 = round(cross_val_score(las,X_train,y_train,cv=5).mean(),2)\nscore_lass500 = round(np.sqrt(mean_squared_error(predict3,y_test)),2)\nr2_score_lass500 = round(r2_score(predict3,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"Lasso_Regression500\",cross_vall_lass500,score_lass500,r2_score_lass500]\nprint(scores)","5f3ae887":"rid = Ridge(alpha=500)\nrid.fit(X_train,y_train)\npredict4 = las.predict(X_test)\ncross_vall_rid500 = round(cross_val_score(las,X_train,y_train,cv=5).mean(),2)\nscore_rid500 = round(np.sqrt(mean_squared_error(predict4,y_test)),2)\nr2_score_rid500 = round(r2_score(predict4,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"Ridge500\",cross_vall_rid500,score_rid500,r2_score_rid500]\nprint(scores)","017dd285":"tree = DecisionTreeRegressor()\ntree.fit(X_train,y_train)\npredict5 = tree.predict(X_test)\ncross_vall_tree = round(cross_val_score(tree,X_train,y_train,cv=5).mean(),2)\nscore_tree = round(np.sqrt(mean_squared_error(predict5,y_test)),2)\nr2_score_tree = round(r2_score(predict5,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"Tree\",cross_vall_tree,score_tree,r2_score_tree]\nprint(scores)","d0815c08":"forest = RandomForestRegressor(n_estimators=100)\nforest.fit(X_train,y_train)\npredict6 = forest.predict(X_test)\ncross_vall_forest = round(cross_val_score(forest,X_train,y_train,cv=5).mean(),2)\nscore_forest = round(np.sqrt(mean_squared_error(predict6,y_test)),2)\nr2_score_forest = round(r2_score(predict6,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"forest\",cross_vall_forest,score_forest,r2_score_forest]\nprint(scores)","52180970":"xgb = XGBRegressor()\nxgb.fit(X_train,y_train)\npredict7 = xgb.predict(X_test)\ncross_vall_xgb = round(cross_val_score(xgb,X_train,y_train,cv=5).mean(),2)\nscore_xgb = round(np.sqrt(mean_squared_error(predict7,y_test)),2)\nr2_score_xgb = round(r2_score(predict7,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"xgb\",cross_vall_xgb,score_xgb,r2_score_xgb]\nprint(scores)","b21a8a02":"pol = PolynomialFeatures(degree=2)\nX_trainpol = pol.fit_transform(X_train)\nX_testpol = pol.transform(X_test)\nlin = LinearRegression()\nlin.fit(X_trainpol,y_train)\npredict8 = lin.predict(X_testpol)\ncross_vall_pol= round(cross_val_score(lin,X_trainpol,y_train,cv=5).mean(),2)\nscore_pol =round(np.sqrt(mean_squared_error(predict8,y_test1)),2)\nr2_pol= round(r2_score(predict8,y_test),2)\n\nn = scores.shape[0]\nscores.loc[n] = [\"Poly\",cross_vall_pol,score_pol,r2_pol]\nprint(scores)","2d68e44a":"plt.figure(figsize=(12,12));\nsns.barplot(x = scores[\"Model\"], y = scores[\"Cross_vall_score\"]);\nplt.xticks(rotation = 90);","88a85f81":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","62fa7fdd":"model = Sequential()\nmodel.add(Dense(200, input_dim = 15 , activation = \"relu\"))\nmodel.add(Dense(200, activation = \"relu\"))\nmodel.add(Dense(1, activation = \"relu\"))\n\nmodel.compile(optimizer= \"adam\", loss = \"mse\")","6bcb39b7":"model.fit(X_train1,y_train1, epochs = 200)","37320651":"There are at averange 3.37 bedrooms, 2.11 Bathrooms and 1.49 floors in most of the houses\n\n","f0ab32ec":"# <div align=\"center\">  **1.  Introduction** <a id=\"1\"><\/a>","00a563c5":"# <div align=\"center\">  7. Map  <a id=\"7\"><\/a>","3be752c9":"#<div align=\"center\"> **Predicting house prices with machine learning and deep learning**\n\n","090eefdd":"## Contents\n[1. Introduction](#1) \n\n[2. Importing Libraries](#2) \n\n[3. A look at the data](#3) \n\n[4. Checking Null values](#4) \n\n[5. Correlation](#5) \n\n[6. Checking price](#6) \n\n[7. Map](#7) \n\n[8.  Machine Learning](#8) \n\n[9. Deep learning ](#9) ","f8959c3e":"# <div align=\"center\">  8. Machine Learning <a id=\"8\"><\/a>","34e24eff":"# <div align=\"center\">  3. A look at the data <a id=\"3\"><\/a>","03ba1b23":"# <div align=\"center\">  **2.  Importing libraries** <a id=\"2\"><\/a>\n    ","f988ae28":"# <div align=\"center\">  6. Checking price <a id=\"6\"><\/a>","355712c2":"This map is very cool and very easy to make.\n1. First import folium and from folium.plugins import FastMarkerCluster.\n2. Make 2 variables with lat long.\n3. Zip the variables together in a list.\n4. Create a map {map1 = folium.Map(location=[47.5112,-122.257], zoom_start=10) } Check in the dataframe what the most common lat and long is.\n5. Use the plugin FastMarkerCluster and set it to the zipped list and add it to the map.\n\n\nMost houses were sold in Northgate, Ballard and Maple Leaf\n","a907db14":"# <div align=\"center\">  9. Deep learning <a id=\"\"><\/a>","16522e27":"# <div align=\"center\">  4. Checking Null values <a id=\"4\"><\/a>","20f67f3f":"# <div align=\"center\">  5. Correlation <a id=\"5\"><\/a>","c7b7e4ff":"![Abbey_House_Gardens_Malmesbury.jpg](attachment:Abbey_House_Gardens_Malmesbury.jpg)","5b647bb2":"Thank you for coming to my first kernel at Kaggle. Please if you find any errors or have advice for me, please post it in the comments. \nThe aim of this kernel is to predict house prices with machine learning methods like linear regression, SVR, Lasso regression and deep learning. \n\n\n\n\n\n # **A few assumptions of linear regression are:**\n \n\n1. Linearity\n    \n2. Constant Variance\n    \n3. Normality\n    \n4. No auto-correlation\n\n\n\n    \n# **Explanation of the different columns:**\n\n***Date***: Date when the house was sold.\n\n***Price***: Price when the house was sold.\n\n***Bedrooms***: Number of bedrooms.\n\n***Bathrooms***: Number of Bathrooms.\n\n***Sqft_living***: Square footage of appartements.\n\n***Sqft_lot***: Square footage of land space.\n\n***Floors***: Number of floor.\n\n***Waterfront***: \n\n***View***: Index from 0 - 4 how good the view is.\n\n***Condition***: Index from 1 - 5.\n\n***Grade***: Index from 1 - 13.\n\n***Sqft_above***: Square footage of upstairs.\n\n***Sqft_basement***: Square footage below ground level.\n\n***Yr_built***: When whe house was built.\n\n***Yr_renovated***: Last renovation year. \n\n***Zipcode***: The zipcode.\n\n***Lat***: The Lattitude.\n\n***Long***: The longitude.\n\n***Sqft_living15***: Square footage of interior housing living space for the nearest 15 neighbors.\n\n***Sqft_lot15***: Square footage of the land lots of the nearest 15 neighbors.\n\n\n"}}