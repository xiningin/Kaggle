{"cell_type":{"f121a304":"code","feb00bd8":"code","12633beb":"code","b68f5866":"code","8a231cea":"code","e4bf506c":"code","dcc8c716":"code","273c1c1a":"code","7831bf5b":"code","6c634b17":"code","13e81668":"code","dd05b94f":"code","0500f7b5":"code","d8766b4d":"code","a6a3529c":"code","539171c8":"code","b38bb688":"markdown","d6f6ca01":"markdown","76ffd111":"markdown","f3d5cc4a":"markdown","e5984c53":"markdown","cf6fd061":"markdown","cbcd14c1":"markdown","4a3db35b":"markdown","650bbbcc":"markdown","97c6bf91":"markdown","01c6ffd3":"markdown","b4e46226":"markdown"},"source":{"f121a304":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom scipy.ndimage.interpolation import shift\nfrom skimage.transform import rotate, AffineTransform, warp\nimport matplotlib.pyplot as plt\n\nimport random\nimport time\nimport io\nimport os\n\ndeviceCount = torch.cuda.device_count()\nprint(deviceCount)\n\ncuda0 = None\nif deviceCount > 0:\n  print(torch.cuda.get_device_name(0))\n  cuda0 = torch.device('cuda:0')","feb00bd8":"df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\n\nprint(df.shape)\ndataset_size = df.shape[0]\n\ndf.head()","12633beb":"x = df.drop('label', axis=1)\nx = x.values.reshape(dataset_size, 1, 28, 28)\n\ny = df['label'].values.reshape(dataset_size)\n\ndf = None","b68f5866":"cross_validation_ratio = 0.05\n# cross_validation_ratio = 0.2\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=cross_validation_ratio, random_state=93)\n\nx = None\ny = None","8a231cea":"start_time = time.time()\n\ndef shift_image(image, dx, dy):\n    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n    return shifted_image\n\naugmented_x = []\naugmented_y = []\n\nrandom.seed(103)\n\npercentage_processed = 0\nfor i in range(len(x_train)):\n    \n    augmented_x.append(x_train[i].astype(int))\n    augmented_y.append(y_train[i].astype(int))\n    \n    for j in range(5):\n        image = x_train[i].reshape((28, 28))\n        \n        x_shift = random.randint(-1, 1)\n        y_shift = random.randint(-1, 1)\n        rotation_deg = float(random.randint(-15, 15))\n        \n        image = rotate(image, angle=rotation_deg, cval=0, mode=\"constant\", preserve_range=True)\n        image = np.rint(image)\n        image = image.astype(int)\n        \n        image = shift_image(image, x_shift, y_shift)\n        \n#         magnification = random.uniform(0.98, 1.02)\n#         shear = random.uniform(-0.1, 0.1)\n        \n#         transformation = AffineTransform(shear=shear, scale=(magnification, magnification))\n#         image = warp(image, transformation.inverse, preserve_range=True)\n#         image = np.rint(image)\n#         image = image.astype(int)\n\n        image = x_train[i].reshape((1, 28, 28))\n\n        augmented_x.append(image)\n        augmented_y.append(y_train[i].astype(int))\n        \n    if (i+1)\/len(x_train)*100 >= percentage_processed+10:\n        print( f'{int((i+1)\/len(x_train)*100):3}% images processed')\n        percentage_processed += 10\n        \ntrain_dataset_size = len(augmented_x)\ntest_dataset_size = len(x_test)\n\nx_train = None\ny_train = None\n\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","e4bf506c":"random.seed(93)\n\nfor i in range(len(augmented_x)):\n    \n    index = random.randint(0, len(augmented_x)-1)\n    \n    tempx = augmented_x[i]\n    tempy = augmented_y[i]\n    \n    augmented_x[i] = augmented_x[index]\n    augmented_y[i] = augmented_y[index]\n    \n    augmented_x[index] = tempx\n    augmented_y[index] = tempy","dcc8c716":"augmented_x = np.array(augmented_x)\naugmented_y = np.array(augmented_y)\n\nx_train = torch.FloatTensor(augmented_x)\nx_test = torch.FloatTensor(x_test)\n\ny_train = torch.LongTensor(augmented_y)\ny_test = torch.LongTensor(y_test)\n\nif cuda0 != None:\n  x_train = x_train.cuda()\n  x_test = x_test.cuda()\n  y_train = y_train.cuda()\n  y_test = y_test.cuda()","273c1c1a":"trainingDataset = TensorDataset(x_train, y_train)\ntestDataset = TensorDataset(x_test, y_test)\n\ntrainloader = DataLoader(trainingDataset, batch_size=512, shuffle=True)\n\ntestloader = DataLoader(testDataset, batch_size=512, shuffle=False)","7831bf5b":"class ConvolutionalNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()                          \n        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Linear(5*5*32, 140)\n        self.fc2 = nn.Linear(140, 80)\n        self.fc3 = nn.Linear(80,10)  \n        self.dropout1 = nn.Dropout(p=0.5)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.bn1 = nn.BatchNorm1d(140)\n        self.bn2 = nn.BatchNorm1d(80)\n        self.conv1_bn = nn.BatchNorm2d(16)\n        self.conv2_bn = nn.BatchNorm2d(32)\n\n    def forward(self, X):\n        X = self.conv1(X)\n        X = self.conv1_bn(X)\n        X = F.relu(X)\n        X = F.max_pool2d(X, 2, 2)\n        X = self.conv2(X)\n        X = self.conv2_bn(X)\n        X = F.relu(X)\n        X = F.max_pool2d(X, 2, 2)\n        X = X.view(-1, 5*5*32)\n        X = self.fc1(X)\n        X = self.bn1(X)\n        X = F.relu(X)\n        X = self.dropout1(X)\n        X = self.fc2(X)\n        X = self.bn2(X)\n        X = F.relu(X)\n        X = self.dropout2(X)\n        X = self.fc3(X)\n        return X","6c634b17":"torch.manual_seed(103)\ntorch.cuda.manual_seed(103)\n\nmodel = ConvolutionalNetwork()\n\nif cuda0 != None:\n  model.to(cuda0)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\nprint(model)","13e81668":"import time\nstart_time = time.time()\n\nepochs = 45\ntrain_losses = []\ntest_losses = []\ntrain_correct = []\ntest_correct = []\n\nfor i in range(epochs):\n\n    model.train()\n\n    epoch_start_time = time.time()\n    trn_corr = 0\n    tst_corr = 0\n    total = 0\n    currentLoss = 0\n    \n    for currentBatch in enumerate(trainloader):\n        bno = currentBatch[0] + 1\n        x = currentBatch[1][0]\n        y = currentBatch[1][1]\n        \n        y_pred = model(x)\n        loss = criterion(y_pred, y)\n        \n        lambdaParam = torch.tensor(0.05)\n        l2_reg = torch.tensor(0.)\n        if cuda0 != None:\n          lambdaParam = lambdaParam.cuda()\n          l2_reg = l2_reg.cuda() \n\n        for param in model.parameters():\n          if cuda0 != None:\n            l2_reg += torch.norm(param).cuda()\n          else:\n            l2_reg += torch.norm(param)\n\n        loss += lambdaParam * l2_reg\n        \n        y_pred = F.log_softmax(y_pred, dim=1)\n        predicted = torch.max(y_pred.data, 1)[1]\n        batch_corr = (predicted == y).sum()\n        trn_corr += batch_corr\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        currentLoss += loss.item()\n   \n        total += len(currentBatch[1][1])\n\n        if bno%100 == 0 or bno==1:\n            printStr = f'epoch: {i+1} batch: {bno:3} loss: {loss.item():10.8f} accuracy: {trn_corr.item()\/total*100:6.3f}%'\n            print(printStr)\n            \n\n    train_losses.append(currentLoss\/bno)\n    train_correct.append(trn_corr.item())\n    \n    currentLoss = 0\n    \n    model.eval()\n    with torch.no_grad():\n        for currentBatch in enumerate(testloader):\n            bno = currentBatch[0] + 1\n            x = currentBatch[1][0]\n            y = currentBatch[1][1]\n\n            y_pred = model(x)\n\n            predicted = torch.max(y_pred.data, 1)[1] \n            tst_corr += (predicted == y).sum()\n            \n            loss = criterion(y_pred, y)\n            currentLoss += loss.item()\n            \n    test_losses.append(currentLoss\/bno)\n    test_correct.append(tst_corr.item())\n\n    print('Summary of Epoch {}:'.format(i+1))\n    print(f'Train Loss: {train_losses[i]:10.8f}  Train Accuracy: {train_correct[i]\/train_dataset_size*100:6.3f}%')\n    print(f'Test Loss: {test_losses[i]:10.8f}  Test Accuracy: {test_correct[i]\/test_dataset_size*100:6.3f}%')\n    print(f'Epoch Duration: {time.time() - epoch_start_time:.0f} seconds')\n    print('')\n\n    scheduler.step()\n    \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","dd05b94f":"x = [i+1 for i in range(len(train_losses))]\nplt.plot(x, train_losses, label='training loss')\nplt.plot(x, test_losses, label='validation loss')\nplt.title('Loss for each epoch')\nplt.legend();\nplt.show()\n\ntrain_accuracy = [i\/train_dataset_size for i in train_correct]\ntest_accuracy = [i\/test_dataset_size for i in test_correct]\n\nplt.plot(x, train_accuracy, label='training accuracy')\nplt.plot(x, test_accuracy, label='validation accuracy')\nplt.title('Accuracy for each epoch')\nplt.legend();\nplt.show()","0500f7b5":"offset = 1\n\nplt.plot(x[offset-1:], train_losses[offset-1:], label='training loss')\nplt.plot(x[offset-1:], test_losses[offset-1:], label='validation loss')\nplt.title('Loss for each epoch')\nplt.legend();\nplt.show();\n\ntrain_accuracy = [i\/train_dataset_size for i in train_correct]\ntest_accuracy = [i\/test_dataset_size for i in test_correct]\n\nplt.plot(x[offset-1:], train_accuracy[offset-1:], label='training accuracy')\nplt.plot(x[offset-1:], test_accuracy[offset-1:], label='validation accuracy')\nplt.title('Accuracy for each epoch')\nplt.legend();\nplt.show()","d8766b4d":"x_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nx_test = x_test.values.reshape(28000, 1, 28, 28)\nx_test = torch.FloatTensor(x_test)\n\nif cuda0 != None:\n    x_test = x_test.cuda()","a6a3529c":"with torch.no_grad():\n    model.eval()\n    y_pred = model(x_test)\n    predictions = torch.max(y_pred.data, 1)[1] \n    \npredictions = predictions.cpu().detach().numpy()\nids = [id+1 for id in range(len(predictions))]\noutput = pd.DataFrame({'ImageId': ids, 'Label': predictions})","539171c8":"output.to_csv('\/kaggle\/working\/my_submission.csv', index=False)","b38bb688":"# READING INPUT DATA","d6f6ca01":"# Loading and Evaluating the test data","76ffd111":"# Convolutional Neural Network Model\n### Model was initially inspired from the LeNet 5 Architecture\n* 1 x 28 x 28 input\n* Convolutional Layer with 16 filters, 3x3 window with stride 1\n* MaxPool, 2x2 window with stride 2\n* Convolutional Layer with 32 filters, 3x3 window with stride 1\n* MaxPool, 2x2 window with stride 2\n* 800x140 Fully Connected Layer\n* Dropout with p=0.5\n* 140x80 Fully Connected Layer\n* Dropout with p=0.5\n* 80x10 Fully Connected Layer\n* Softmax","f3d5cc4a":"# Training the Model\n\n* L2 regularization used with lambda = 0.05\n* Decaying Learning Rate used with gamma = 0.9","e5984c53":"# Training and Cross Validation Split\n* cross_validation_ratio of 0.05 was used during final submission\n* 0.2 can be used during hyperparameter tuning  ","cf6fd061":"# Visualizing the Model Performance\n\n* The Training Accuracy Seems to be a performance metric than the Training Loss\n* The 'offset' variable can be used to focus on a particular portion of the graph","cbcd14c1":"# Saving the results","4a3db35b":"# Data Loaders\n* A relatively large batch size of 512 is chosen after hyperparamter tuning. \n* It was probably due to batch normalization that a relatively larger batch size than usual gave better results","650bbbcc":"# Shuffling\n\n* Shuffling the augmented training dataset\n* The input features list and the target outputs list must be shuffled synchronously","97c6bf91":"# PyTorch with CUDA is used to train the CNN. \n## Other imports are required for data preprocessing, data augmentation, performance visualization, etc.\n","01c6ffd3":"# Data Augmentation\n * Random Translation within a given range, and Random Rotation within a given range is used\n * Random Shear and Random Magnification was tried out, but not to much benefit.\n * 5 augmented images produced for each original image","b4e46226":"# Data Preprocessing\n* raw input separated into input features and target outputs"}}