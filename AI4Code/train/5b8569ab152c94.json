{"cell_type":{"143608fb":"code","5838bf61":"code","c4559ad8":"code","fd7007ad":"code","a920949c":"code","66c29869":"code","f22f77ae":"code","1a19d65f":"code","70f425f1":"code","08696a42":"code","7984169c":"code","4c597ddb":"code","c45b1e7b":"code","fe9482be":"code","a4866c85":"code","fde8acb1":"code","547cbd42":"code","72950838":"code","acd7a24a":"code","5c8c43ab":"code","9ac3b4d8":"code","a65a695c":"code","0b1df383":"code","87a99bf9":"code","a25f9864":"code","2dfb1ffe":"code","6e3b91a3":"code","80b1e3e4":"code","02197de4":"code","a95e72c7":"code","ecaa9597":"code","8bc19859":"code","d2c1cf51":"code","867edf31":"code","4e6e76cc":"code","e09f9e8e":"code","6fb4e747":"code","d8ede558":"code","c032fb8b":"code","12b64c91":"code","c453a1c3":"code","2da40c20":"code","90896719":"code","083593a9":"code","97ff75ba":"code","ce7cc835":"code","0ebea24f":"code","a47c93d9":"code","08c3d545":"code","a3d12616":"code","f9167ae8":"code","1fd80917":"code","b3451af0":"code","7985602e":"code","51a05741":"code","96c15c7f":"code","8841c529":"markdown","ca20639b":"markdown","ba0ba0e4":"markdown","ee319ee1":"markdown","23f1f934":"markdown","8710cf9d":"markdown","0299c018":"markdown","9051c7d6":"markdown","3fd898f6":"markdown","33928e02":"markdown","d7998787":"markdown","7d7d2fc9":"markdown","c00ff6a6":"markdown","1fc0aae3":"markdown","804c1edc":"markdown"},"source":{"143608fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5838bf61":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nimport seaborn as sns","c4559ad8":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","fd7007ad":"df_train.shape","a920949c":"df_train.head()","66c29869":"## Remove columns with missing values more than 40% of sample size: 1406*0.4 = 562\n\ndf_null_count = pd.DataFrame(df_train.isnull().sum(),columns=['Null_values']).reset_index()\nrm_columns = list(df_null_count[df_null_count['Null_values'] >562].loc[:,'index'])\nrm_columns.append('Id')\n\ndf_train = df_train.drop(rm_columns,axis=1)","f22f77ae":"df_train.shape","1a19d65f":"## Drop the label column and extract y_train\ny_train = df_train.SalePrice.values\ndf_train = df_train.drop(['SalePrice'],axis=1)","70f425f1":"df_train.shape","08696a42":"df_train.head()","7984169c":"num_attributes = list(df_train.describe().columns)","4c597ddb":"cat_attributes = [item for item in df_train.columns if item not in num_attributes]","c45b1e7b":"## Impute NAN values in categorical columns. Create a new category 'Unknown' for NAN values.\ndef impute_nan_create_category(df, col_name):\n    df[col_name] = np.where(df[col_name].isnull(),'Unknown',df[col_name])\n       \nfor column in cat_attributes:\n    impute_nan_create_category(df_train,column)","fe9482be":"num_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler()),\n])\n\n\ncat_pipeline = Pipeline([\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\nfull_pipeline = ColumnTransformer([\n    ('num',num_pipeline, num_attributes),\n    ('cat',cat_pipeline, cat_attributes),\n])\n","a4866c85":"full_pipeline.fit(df_train)\ndf_train_prepared = full_pipeline.transform(df_train)","fde8acb1":"## After applying One-Hot-Encoding, extract the features names after encoding. Generate a full \n## column names list\/features list for the pro-processed dataset (X_train)\n\n\nfull_categories = full_pipeline.named_transformers_['cat'].named_steps['encoder'].categories_\nfull_cat_column_names =[]\nfor i,feature_name in enumerate(cat_attributes):\n    for cat_name in full_categories[i]:\n        full_cat_column_names.append(feature_name+'_'+cat_name)\n        \n   ","547cbd42":"X_train = df_train_prepared","72950838":"X_train.shape","acd7a24a":"y_train.shape","5c8c43ab":"feature_names = num_attributes + full_cat_column_names","9ac3b4d8":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Lasso","a65a695c":"lr = LinearRegression()\nlr_cv = cross_val_score(lr,X_train,y_train,cv=10)","0b1df383":"lr_cv","87a99bf9":"lr.fit(X_train,y_train)","a25f9864":"lasso = Lasso()\nlasso_cv = cross_val_score(lasso,X_train,y_train,cv=10)","2dfb1ffe":"lasso_cv","6e3b91a3":"lasso.fit(X_train,y_train)","80b1e3e4":"np.sum(lasso.coef_!= 0)","02197de4":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(max_depth=4, random_state=42)\nrf_cv = cross_val_score(rf,X_train,y_train,cv=10)","a95e72c7":"rf_cv","ecaa9597":"rf.fit(X_train,y_train)","8bc19859":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\ngb_cv = cross_val_score(gb,X_train,y_train,cv=10)","d2c1cf51":"gb_cv","867edf31":"gb.fit(X_train,y_train)","4e6e76cc":"r2_results = []\nr2_results.append(lr_cv)\nr2_results.append(lasso_cv)\nr2_results.append(rf_cv)\nr2_results.append(gb_cv)","e09f9e8e":"reg_names = ['Linear Regression','LASSO','Random Forest Regressor','Gradient Boost Regressor']","6fb4e747":"plt.boxplot(r2_results)\nplt.xticks(ticks=np.arange(5),labels=['']+reg_names,rotation=60)\nplt.ylabel('R2')\nplt.title('Cross Validation R2 Score for Each Regressor\\n')\nplt.show()","d8ede558":"def visualize_coefficients(coefficients, feature_names, n_top_features=25):\n    \"\"\"Visualize coefficients of a linear model.\n    Parameters\n    ----------\n    coefficients : nd-array, shape (n_features,)\n        Model coefficients.\n    feature_names : list or nd-array of strings, shape (n_features,)\n        Feature names for labeling the coefficients.\n    n_top_features : int, default=25\n        How many features to show. The function will show the largest (most\n        positive) and smallest (most negative)  n_top_features coefficients,\n        for a total of 2 * n_top_features coefficients.\n    \"\"\"\n\n\n    if len(coefficients) != len(feature_names):\n        raise ValueError(\"Number of coefficients {} doesn't match number of\"\n                         \"feature names {}.\".format(len(coefficients),\n                                                    len(feature_names)))\n    # get coefficients with large absolute values\n    coef = coefficients.ravel()\n    positive_coefficients = np.argsort(coef)[-n_top_features:]\n    negative_coefficients = np.argsort(coef)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients,\n                                          positive_coefficients])\n    # plot them\n    plt.figure(figsize=(15, 5))\n    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients])\n    feature_names = np.array(feature_names)\n    plt.subplots_adjust(bottom=0.3)\n    plt.xticks(np.arange(1, 1 + 2 * n_top_features),\n               feature_names[interesting_coefficients], rotation=60,\n               ha=\"right\")\n    plt.ylabel(\"Coefficient magnitude\")\n    plt.xlabel(\"Feature\")","c032fb8b":"visualize_coefficients(lr.coef_, feature_names, n_top_features=25)","12b64c91":"visualize_coefficients(lasso.coef_, feature_names, n_top_features=25)","c453a1c3":"def plot_feature_importance(regressor,regressor_name):\n    importances = pd.DataFrame(\n        {'Feature': feature_names,\n         'Importance': regressor.feature_importances_\n        })\n    \n    importances = importances.sort_values('Importance', ascending=False)[:20]\n    sns.barplot(data=importances,x='Importance',y='Feature')\n    plt.title(regressor_name)\n    plt.show()\n ","2da40c20":"plot_feature_importance(rf,'Random Forest Regressor')","90896719":"plot_feature_importance(gb,'Gradient Boosting Regresor')","083593a9":"df_test = pd.read_csv('test.csv')","97ff75ba":"test_id = df_test['Id'].values","ce7cc835":"test_id","0ebea24f":"df_test = df_test.drop(['Id'],axis=1)\ndf_test = df_test.drop(rm_columns,axis=1)","a47c93d9":"for column in cat_attributes:\n    impute_nan_create_category(df_test,column)","08c3d545":"X_test = full_pipeline.transform(df_test)","a3d12616":"X_test","f9167ae8":"y_pred = gb.predict(X_test)","1fd80917":"y_pred.shape","b3451af0":"df_pred = pd.DataFrame(np.concatenate((test_id.reshape(-1,1),y_pred.reshape(-1,1)),axis=1),columns=['Id','SalePrice'],index=None)","7985602e":"df_pred['Id'] = df_pred.Id.astype(int)","51a05741":"df_pred.info()","96c15c7f":"df_pred.to_csv('submission.csv',index=False)","8841c529":"### Random Forest Regressor","ca20639b":"## Select a model","ba0ba0e4":"## Test on the test set","ee319ee1":"### Lasso Regression","23f1f934":"## Feature Importance and Coefficients","8710cf9d":"## 1. Data Preprocessing","0299c018":"Regression Task:\n \n0. Define num_attributes and cat_attributes \n \n1. Data Preprocessing\n   \n    num_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy = 'median')),\n        ('scaler', StandardScaler()),\n    ])\n    \n       \n    cat_pipeline = Pipeline([\n        ('cat_imputer',ImputeNanCreateCategory()),\n        ('encoder', OneHotEncoder())\n    ])\n    \n    ColumnTransfer([\n        ('num',num_pipeline, num_attributes),\n        ('cat',cat_pipeline, cat_attributes),\n    ])\n \nfull_pipeline.fit(df_train)\ndf_train_prepared = full_pipeline.transform(df_train)\n\n\n2. Extract X_train, y_train\n3. Select a model, evaluate metrics Cross Validation\n    3.1 LinearRegression\n    3.2 Lasso\n    3.3 RandomForestRegressor, GradientBoostRegressor\n    3.4 Select the optimal model\n    3.5 Plot feature importance \n    \n4. Test on the test set\n    df_test_prepared = full_pipeline.transform(df_test)\n    X_test = df_test_prepared.values\n    \n    ","9051c7d6":"## 0. Define num_attributes and cat_attributes ","3fd898f6":"## Load Data","33928e02":"### Remove columns with too many missing values NAN","d7998787":"### remove some columns","7d7d2fc9":"### Linear Regression","c00ff6a6":"### Gradient Boost Regressor","1fc0aae3":"### Apply full_pipeline","804c1edc":"### impute categorical columns"}}