{"cell_type":{"e4d627b4":"code","5da53f83":"code","9c13d590":"code","f91e7d2f":"code","1f793773":"code","cb02d641":"code","b2887981":"code","bf4b168c":"code","ea943b5a":"code","bd7ebdf2":"code","1bd6ec47":"code","423dfbcd":"code","c0e7f55f":"code","7d66997a":"code","f9ed2663":"code","29cceca4":"code","6a67775b":"code","f02d5d67":"code","2fa67bc9":"code","0397f932":"code","bb3ec702":"code","086b081e":"markdown","45b99dc6":"markdown","eb7c02ba":"markdown","dcd4dd70":"markdown","de75aaff":"markdown"},"source":{"e4d627b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5da53f83":"import sys\n!cp ..\/input\/rapids\/rapids.0.13.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","9c13d590":"\nimport cudf \nimport cupy as cp\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import SVR\nfrom cuml.linear_model import Ridge, Lasso\nfrom cuml.metrics import mean_absolute_error, mean_squared_error\n","f91e7d2f":"import pandas as pd\nimport pydicom\nimport os\nimport numpy as np\n#from matplotlib import cm\nfrom matplotlib import pyplot as plt\nimport cv2\n#import seaborn as sns\nfrom tqdm import tqdm","1f793773":"train_df = pd.read_csv(\"..\/input\/osic-pulmonary-fibrosis-progression\/train.csv\")\ntrain_df.drop_duplicates(keep = False, inplace = True, subset = ['Patient', 'Weeks'])\ntest_df = pd.read_csv(\"..\/input\/osic-pulmonary-fibrosis-progression\/test.csv\")\n\n\nsub_df = pd.read_csv(\"..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv\")\nsub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub_df =  sub_df[['Patient','Weeks','Confidence','Patient_Week']]\nsub_df = sub_df.merge(test_df.drop('Weeks', axis=1), on=\"Patient\")\n","cb02d641":"train_df['WHERE'] = 'train'\ntest_df['WHERE'] = 'val'\nsub_df['WHERE'] = 'test'\n\ndf = train_df.append([test_df, sub_df])\ndf['min_week'] = df['Weeks']\ndf.loc[df.WHERE=='test','min_week'] = np.nan\ndf['min_week'] = df.groupby('Patient')['min_week'].transform('min')","b2887981":"baseline_df = df.loc[df.Weeks == df.min_week]\nbaseline_df = baseline_df[['Patient', 'FVC']].copy()\nbaseline_df.columns = ['Patient', 'min_FVC']\nbaseline_df['nb'] = 1\nbaseline_df['nb'] = baseline_df.groupby('Patient')['nb'].transform('cumsum')\nbaseline_df = baseline_df[baseline_df.nb==1]\nbaseline_df.drop('nb', axis=1, inplace=True)\n","bf4b168c":"df = df.merge(baseline_df, on='Patient', how='left')\ndf['base_week'] = df['Weeks'] - df['min_week']\n","ea943b5a":"# convert string labels to numeric labels\ncolumns = ['Sex', 'SmokingStatus']\nfeatures = []\nfor feat in columns: \n    for mode in df[feat].unique():\n        features.append(mode)\n        df[mode]  = (df[feat] == mode).astype(int)\n    \nfeatures += ['Age', 'Percent', 'min_FVC', 'base_week']\n","bd7ebdf2":"features","1bd6ec47":"# Zero-center normalization \n\nfor feat in features:\n    df[feat] = (df[feat] - np.mean(df[feat]))\/np.std(df[feat])\ndf.head()","423dfbcd":"cudf = cudf.from_pandas(df)\ntr_cudf = cudf.loc[cudf.WHERE=='train']\nval_cudf = cudf.loc[cudf.WHERE=='val']\ntest_cudf = cudf.loc[cudf.WHERE=='test']","c0e7f55f":"# evaluation metric for the comp\ndef score(y_true, y_pred):\n    C1, C2 = cp.asarray(70.0), cp.asarray(70.0)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = cp.maximum(sigma, C1)\n    delta = cp.absolute(y_true - fvc_pred)\n    delta = cp.minimum(delta, C2)\n    sq2 = cp.sqrt( 2.0)\n    metric = (delta \/ sigma_clip)*sq2 + cp.log(sigma_clip* sq2)\n    return cp.mean(metric)","7d66997a":"\nfrom sklearn.model_selection import KFold\n\nKfold = 5\nkf = KFold(n_splits=Kfold)\n\nX, y, X_test = tr_cudf[features].values, tr_cudf['FVC'].values, test_cudf[features].values","f9ed2663":"%%time\n\n\nmodel_score, model_zoo, y_preds_scores = [], [], []\nfor c1, c2, c3 in [(1, 50, 500)]:\n    \n    y_preds = cp.zeros((X.shape[0], 3))\n    y_test_preds = cp.zeros((X_test.shape[0], 3))\n    \n    model_container = []\n    SVR_kfold_ensemble,Ridge_kfold_ensemble = [], [] \n    for train_ind, val_ind in kf.split(X):\n        X_train, X_val = X[train_ind,:], X[val_ind,:]\n        y_train, y_val = y[train_ind], y[val_ind]\n        \n        model_1 = SVR(C=c1, cache_size=3000.0)\n        model_1.fit(X_train, y_train)\n        \n        model_2 = SVR(C=c2, cache_size=3000.0)\n        model_2.fit(X_train, y_train)\n        \n        model_3 = SVR(C=c3, cache_size=3000.0)\n        model_3.fit(X_train, y_train)\n        \n        \n        \n        y_preds[val_ind,0] = model_1.predict(X_val)\n        y_preds[val_ind,1] = model_2.predict(X_val)\n        y_preds[val_ind,2] = model_3.predict(X_val)\n        \n        y_test_preds[:,0] += model_1.predict(X_test) \n        y_test_preds[:,1] += model_2.predict(X_test) \n        y_test_preds[:,2] += model_3.predict(X_test) \n        \n    y_test_preds *= 1\/cp.asarray(Kfold)\n    y_preds_scores.append(y_test_preds)\n    model_score.append(score(y,y_preds))\n    model_zoo.append([model_1, model_2, model_3])","29cceca4":"model_score, y_preds_scores","6a67775b":"y_np = cp.asnumpy(y)\ny_preds_np = cp.asnumpy(y_preds)\nidxs = np.random.randint(0, y_np.shape[0], 100)\nplt.plot(y_np[idxs], label=\"ground truth\")\nplt.plot(y_preds_np[idxs, 0], alpha = 0.5, label=\"c=1\")\nplt.plot(y_preds_np[idxs, 1],  alpha = 0.5, label=\"c=5\")\nplt.plot(y_preds_np[idxs, 2],  alpha = 0.5, label=\"c=500\")\nplt.legend(loc=\"best\")\nplt.show()","f02d5d67":"test_cudf['FVC1'] = 1.0*y_test_preds[:,1]\ntest_cudf['Confidence1'] = y_test_preds[:,2] - y_test_preds[:,0]\ntest_cudf","2fa67bc9":"submission_cudf = test_cudf[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubmission_cudf.loc[~submission_cudf.FVC1.isnull()].head(10)\nsubmission_cudf[\"FVC\"] = submission_cudf[\"FVC1\"]\nsubmission_cudf[\"Confidence\"] = submission_cudf[\"Confidence1\"]","0397f932":"submission_cudf.describe().T","bb3ec702":"submission_cudf[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","086b081e":"## Convert Pandas to Cudf dataframes \n\nThis transformation will then allow us to run SVM, random forests, and other machine learning models with GPU, using the RAPIDS.AI library.","45b99dc6":"## Observing the prediction:","eb7c02ba":"## Setting up the submission file: ","dcd4dd70":"## Preprocess Data: \n\nBefore we work with cudf dataframes, we will process and curate the data through panadas with a similar taken from Ulrich GOUE. Here is a helpful notebook on preparing the tabular data and using keras NNets for regression: [Osic-Multiple-Quantile-Regression-Start](https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter). However, I will show a fast method to using support vector regressor with RAPIDS.AI (~2 mins. for notebook submission). ","de75aaff":"## Training the machine learning model with RAPIDS:"}}