{"cell_type":{"1acff12c":"code","b6d18ded":"code","1d4cfb7f":"code","20cbcd3c":"code","797d5c08":"code","8a13db53":"code","1ea7bf58":"code","ccfc9048":"code","f8cc9516":"code","d3e1c648":"code","92278e73":"markdown","d65dbf2f":"markdown","9ed99397":"markdown","c0b7a882":"markdown","5a51a939":"markdown","e58f44aa":"markdown","f2885667":"markdown","cfbe0d15":"markdown","92386dda":"markdown","b7a14c63":"markdown","df1ba9c3":"markdown","adfc381a":"markdown","ed5dbe2e":"markdown","7dc370cc":"markdown"},"source":{"1acff12c":"import tensorflow as tf \nimport numpy as np \nfrom tensorflow.keras.layers import Input, Conv2D, Dropout, MaxPool2D, Flatten, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD, Adam\nimport pickle\nimport configuration as cf\nimport Normalization as Nor","b6d18ded":"# file configuration.py represent configuration of model\nWIDTH = 32\nHEIGHT = 32\nDEPTH = 3\nDIR_TEST_IMAGE = \"dataset\/image_test\"\nDIR_TEST_LABEL = \"dataset\/label_test\"\n\nDIR_TRAIN_IMAGE = \"dataset\/DATASET_IMAGE_64_ALL\"\nDIR_TRAIN_LABEL = \"dataset\/DATASET_LABEL_64_ALL\"\n\nDIR_DATA_IMAGE = \"dataset\/DATA_IMAGE\"\nDIR_DATA_LABEL = \"dataset\/DATA_LABEL\"\n\nFILE_RESULT = \"result\/RESULTS_32_ALL\"\nFILE_JSON_SAVE = \"save_model\/model_32_ALL.json\"\nFILE_WEIGHT_SAVE = \"save_model\/model_32_ALL.h5\"","1d4cfb7f":"# file Normalization.py represent:\n# how to load data, generative data, normalize data\nimport tensorflow as tf \nimport cv2\nimport os\nimport numpy as np \nimport pickle\n# import configuration as cf","20cbcd3c":"# from an image we can create K (k = 20) image. It is created for Zoom an brightness\ndef Zoom(frame):\n    w = n_widths + int (n_widths\/3)\n    h = n_heights + int (n_heights\/3)\n    image = cv2.resize(frame,(w, h))\n    u = int (w\/2) - int (n_widths\/2)\n    v = int (h\/2) - int (n_heights\/2)\n    img_crop = image[v:n_heights+v, u:n_widths+u]\n    return img_crop\n\ndef increase_brightness(img, value):\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h, s, v = cv2.split(hsv)\n    # print (v)\n\n    lim = 255 - value\n    v[v > lim] = 255\n    v[v <= lim] += value\n\n    final_hsv = cv2.merge((h, s, v))\n    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n    return img\n\ndef generative(x_train ,y_train, image, labels):\n\tvalue = 0\n\tfor i in range(10):\n\t\t# value = 10\n\t\timages = increase_brightness(image, value)\n\t\tx_train.append(images)\n\t\ty_train.append(int(labels))\n\t\t# zoomx\n\t\timg_zoom = Zoom(images)\n\t\tx_train.append(img_zoom)\n\t\ty_train.append(int(labels))\n\t\tvalue += 5","797d5c08":"def imread_image(x_train, y_train,paths, labels, id_image):\n\timage = cv2.imread(paths, cv2.COLOR_GRAY2RGB)\n\timage = cv2.resize(image, (n_widths, n_heights))\n\tx_train.append(image)\n\ty_train.append(int(labels))\n\n\ndef read_test():\n\tdataX = pickle.load(open(cf.DIR_DATA_IMAGE, \"rb\"))\n\tdataY = pickle.load(open(cf.DIR_DATA_LABEL, \"rb\"))\n\treturn dataX, dataY\n\ndef read_train():\n\tdataX = pickle.load(open(cf.DIR_DATA_IMAGE, \"rb\"))\n\tdataY = pickle.load(open(cf.DIR_DATA_LABEL, \"rb\"))\n\tx_train = []\n\ty_train = []\n\tfor i in range(dataX.shape[0]):\n\t\tgenerative(x_train, y_train, dataX[i], dataY[i])\n\tx_train = np.asarray(x_train)\n\ty_train = np.asarray(y_train)\n\treturn x_train, y_train\n\ndef write_file_test():\n\tx_test = []\n\ty_test = []\n\tdirector_paths = os.path.abspath(dir_test)\n\tlist_image = os.listdir(director_paths)\n\tfor image in list_image:\n\t\tpaths = director_paths + '\/' + image\n\t\tid_image = image\n\t\tlabels = 100\n\t\timread_image(x_test, y_test, paths, labels, id_image)\n\tx_test = np.asarray(x_test)\n\ty_test = np.asarray(y_test)\n\tprint (x_test.shape)\n\tprint (y_test.shape)\n\tpickle.dump(x_test, open(cf.DIR_TEST_IMAGE, \"wb\"))\n\tpickle.dump(y_test, open(cf.DIR_TEST_LABEL, \"wb\"))\n\n# write_file_test()\n\ndef write_file_train():\n\tx_train = []\n\ty_train = []\n\t# count = 0\n\t# tong = 0\n\tdirector_paths = os.path.abspath(dir_train)\n\tlist_file = os.listdir(director_paths)\n\tfor file in list_file:\n\t\tfile_paths = director_paths + '\/' + file\n\t\tlist_image = os.listdir(file_paths)\n\t\tprint (\"file = \", file, \" size of file: \", len(list_image))\n\t\tfor i in range(0, len(list_image)):\n\t\t\tid_image = list_image[i]\n\t\t\tpaths = file_paths + '\/' + id_image\n\t\t\timread_image(x_train, y_train, paths, file, id_image)\n\tx_train = np.asarray(x_train)\n\ty_train = np.asarray(y_train)\n\tprint (x_train.shape)\n\tprint (y_train.shape)\n\tpickle.dump(x_train, open(cf.DIR_DATA_IMAGE, \"wb\"))\n\tpickle.dump(y_train, open(cf.DIR_DATA_LABEL, \"wb\"))\n\n# write_file_train()","8a13db53":"def normalize():\n\tdataX, y_train = read_train()\n\t# dataX = pickle.load(open(cf.DIR_TRAIN_IMAGE, \"rb\"))\n\t# dataY = pickle.load(open(cf.DIR_TEST_IMAGE, \"rb\"))\n\tdataY, y__ = read_test()\n\tdataX = dataX.astype(np.float32)\n\tdataY = dataY.astype(np.float32)\n\tprint (dataX.shape)\n\tprint (dataY.shape)\n\tprint (\"start\")\n\tn = dataX.shape[1]*dataX.shape[2]*dataX.shape[3]\n\tx_train = dataX.reshape((dataX.shape[0], dataX.shape[1]*dataX.shape[2]*dataX.shape[3]))\n\tx_test = dataY.reshape((dataY.shape[0], dataY.shape[1]*dataY.shape[2]*dataY.shape[3]))\n\tmax_x = np.max(x_train, axis = 0)\n\tmin_x = np.min(x_train, axis = 0)\n\tavagen = np.sum(x_train, axis = 0) * 1.0 \/ x_train.shape[0]\n\tprint (\"Loaded finsh\")\n\tfor i in  range(n):\n\t\tr = max_x[i] - min_x[i]\n\t\tif r==0:\n\t\t\tr = 1\n\t\t# x_train[:, i:i+1] = (x_train[: , i:i+1] - avagen[i])\/r\n\t\tx_test[:, i:i+1] = (x_test[: , i:i+1] - avagen[i])\/r\n\t# x_train = x_train.reshape((x_train.shape[0], cf.WIDTH, cf.HEIGHT, cf.DEPTH))\n\tx_test = x_test.reshape((x_test.shape[0], cf.WIDTH, cf.HEIGHT, cf.DEPTH))\n\treturn x_test","1ea7bf58":"# build model network CNN\ndef build_model_CNN():\n\tinputs = Input(shape = (cf.WIDTH, cf.HEIGHT, cf.DEPTH), name = 'input')\n\n\tlayers = inputs\n\ti = 0\n\n\tfor filters in n_neurons:\n\t\tlayers = Conv2D(filters = filters, kernel_size = (2,2), strides = (1,1), \n\t\t\t\t\t\tpadding = \"SAME\", activation = 'relu')(layers)\n\n\t\tlayers = MaxPool2D(pool_size = (2,2), strides = (2,2))(layers)\n\t\tlayers = Dropout(0.02)(layers)\n\t\ti += 1\n\tlayers = Flatten()(layers)\n\n\tlayers = Dense(units = 512, activation = 'relu')(layers)\n\tlayers = Dropout(0.02)(layers)\n\n\t# layers = Dense(units = 1024, activation = 'relu')(layers)\n\t# layers = Dropout(0.02)(layers)\n\n\toutputs = Dense(units = n_outputs, activation = 'softmax')(layers)\n\n\tmodel = Model(inputs = inputs, outputs = outputs)\n\n\tmodel.summary()\n\t# serialize model to Json\n\tmodel_json = model.to_json()\n\twith open(cf.FILE_JSON_SAVE, \"w\") as json_file:\n\t\tjson_file.write(model_json)\n\n\t# serialize weight to HDF5\n\tmodel.save_weights(cf.FILE_WEIGHT_SAVE)\n\tprint (\"saved model to disk\")\n\nbuild_model_CNN()","ccfc9048":"import tensorflow as tf \nimport numpy as np \nfrom tensorflow.keras.models import Model, model_from_json\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nimport pickle\nimport Read_Write_CSV as RW\nimport csv\nimport cv2\nimport configuration as cf\nimport Normalization as Nor\n\n# define hyper-parammeter\nlearning_rate = 0.001\nx_test, y_test = Nor.read_test()\n# print (y_test[:100])\nx_test = Nor.normalize()\n\n\ndef build_model():\n\t# load json and create model\n\tjson_file = open(cf.FILE_JSON_SAVE, \"r\")\n\tjson_file_loaded = json_file.read()\n\tjson_file.close()\n\n\t# load model\n\tmodel = model_from_json(json_file_loaded)\n\tmodel.load_weights(cf.FILE_WEIGHT_SAVE)\n\tprint (\"loaded model from disk\")\n\n\tmodel.compile(loss = 'categorical_crossentropy', \n\t\t\t\t\toptimizer = Adam(learning_rate),\n\t\t\t\t\tmetrics = ['accuracy'])\n\tprediction = model.predict(x_test)\n\tpredict = np.argmax(prediction, 1)\n\tpickle.dump(predict, open(cf.DIR_DATA_LABEL, \"wb\"))\nbuild_model()\t\n","f8cc9516":"# file CNN_Keras.py\nimport tensorflow as tf \nimport numpy as np \nfrom tensorflow.keras.layers import Input, Conv2D, Dropout, MaxPool2D, Flatten, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD, Adam\nimport pickle\nimport configuration as cf\nimport Normalization as Nor\n\n# define hyper-parammeter\nn_neurons = [128,256,512, 512, 1024]\nn_layers = len(n_neurons)\nn_outputs = 43\n\n# normalize data\ndef normalize(dataX):\n\tprint (\"start\")\n\tn = dataX.shape[1]*dataX.shape[2]*dataX.shape[3]\n\tx_train = dataX.reshape((dataX.shape[0], dataX.shape[1]*dataX.shape[2]*dataX.shape[3]))\n\tmax_x = np.max(x_train, axis = 0)\n\tmin_x = np.min(x_train, axis = 0)\n\tavagen = np.sum(x_train, axis = 0) * 1.0 \/ x_train.shape[0]\n\tprint (\"Loaded finsh\")\n\tfor i in  range(n):\n\t\tr = max_x[i] - min_x[i]\n\t\tif r==0:\n\t\t\tr = 1\n\t\tx_train[:, i:i+1] = (x_train[: , i:i+1] - avagen[i])\/r\n\tx_train = x_train.reshape((x_train.shape[0], cf.WIDTH, cf.HEIGHT, cf.DEPTH))\n\treturn x_train\n\nx_train, y_train = Nor.read_train()\nx_train = x_train.astype(np.float32)\ny_train = tf.keras.utils.to_categorical(y_train, n_outputs)\nx_train = normalize(x_train)\n\n# build model network CNN\ndef build_model_CNN():\n\tinputs = Input(shape = (cf.WIDTH, cf.HEIGHT, cf.DEPTH), name = 'input')\n\n\tlayers = inputs\n\ti = 0\n\n\tfor filters in n_neurons:\n\t\tlayers = Conv2D(filters = filters, kernel_size = (2,2), strides = (1,1), \n\t\t\t\t\t\tpadding = \"SAME\", activation = 'relu')(layers)\n\n\t\tlayers = MaxPool2D(pool_size = (2,2), strides = (2,2))(layers)\n\t\tlayers = Dropout(0.02)(layers)\n\t\ti += 1\n\tlayers = Flatten()(layers)\n\n\tlayers = Dense(units = 512, activation = 'relu')(layers)\n\tlayers = Dropout(0.02)(layers)\n\n\toutputs = Dense(units = n_outputs, activation = 'softmax')(layers)\n\n\tmodel = Model(inputs = inputs, outputs = outputs)\n\n\tmodel.summary()\n\tmodel.compile(loss = 'categorical_crossentropy', \n\t\t\t\t\toptimizer = Adam(0.001), \n\t\t\t\t\tmetrics = ['accuracy'])\n\n\tmodel.fit(x_train, y_train, batch_size = 100, epochs = 3)\n\n\t# serialize model to Json\n\tmodel_json = model.to_json()\n\twith open(cf.FILE_JSON_SAVE, \"w\") as json_file:\n\t\tjson_file.write(model_json)\n\n\t# serialize weight to HDF5\n\tmodel.save_weights(cf.FILE_WEIGHT_SAVE)\n\tprint (\"saved model to disk\")\n\nbuild_model_CNN()","d3e1c648":"import tensorflow as tf \nimport numpy as np \nfrom tensorflow.keras.models import Model, model_from_json\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nimport pickle\nimport Read_Write_CSV as RW\nimport csv\nimport cv2\nimport configuration as cf\nimport Normalization as Nor\nimport cv2\n\n# define hyper-parammeter\nlearning_rate = 0.001\n\ndef load_data():\n\tx_test = pickle.load(open(cf.DIR_TEST_IMAGE, \"rb\"))\n\ty_test_all = pickle.load(open(cf.DIR_TEST_LABEL, \"rb\"))\n\t\n\ty_test = y_test_all[:, 1:]\n\ty_id_name = y_test_all[:, :1]\n\n\tx_test = x_test.astype(np.float32)\n\n\treturn x_test, y_test, y_id_name\n\ndef build_model():\n\n\tprint (\"start\")\n\tx_test, y_test, y_id_name = load_data()\n\tid_ = y_id_name.tolist()\n\tresults = RW.read_csv()\n\n\tx_test_2, y_test_2 = Nor.read_test()\n\tx_display = x_test_2\n\tx_test_2, x_test = Nor.data_test_noisy()\n\n\t# load json and create model\n\tjson_file = open(cf.FILE_JSON_SAVE, \"r\")\n\tjson_file_loaded = json_file.read()\n\tjson_file.close()\n\n\t# load model\n\tmodel = model_from_json(json_file_loaded)\n\tmodel.load_weights(cf.FILE_WEIGHT_SAVE)\n\tprint (\"loaded model from disk\")\n\n\tmodel.compile(loss = 'categorical_crossentropy', \n\t\t\t\t\toptimizer = Adam(learning_rate),\n\t\t\t\t\tmetrics = ['accuracy'])\n\n\tpre_prob = model.predict_proba(x_test_2)\n\n\tprint (\"predict data test\")\n\tprediction = model.predict(x_test)\n\tpredict = np.argmax(prediction, 1)\n\tsorce = predict.tolist()\n\tprint (\"finsh prediction\")\n\tfor i in range(len(sorce)):\n\t\tid_name = id_[i][0]\n\t\tresults[id_name] = sorce[i]\n\t\t# print (\"id = \", id_name, \":  \", sorce[i])\n\tpickle.dump(results, open(cf.FILE_RESULT, \"wb\"))\n\tprint (\"write successful\")\nbuild_model()\t\n","92278e73":"3.1 Define The Model\n    - We using coding following Functionality. \n    - The network include 4 layers (Conv - Relu)x5 -> Dense - Dense (Output)\n    - We will save weight and We will restore this model ","d65dbf2f":"2. Prepare Dataset","9ed99397":"1. Introduce\n    We using the Tensorflow + Keras to support training and testing. This consist of 5 layers Model Convulation Neural Network. The first We can prepare dataset (It is importance). Then we will give dataset into Model an execute.\n    We using CPU(i3 Ram = 8GB) trained  with time_trained = 1hour\n    We achieved with small accuracy (76%) Beacuse data is noisy I don't remove It.","c0b7a882":"4.2 Test (Predicttion)","5a51a939":"- Restore to remove noisy data","e58f44aa":"3. Model CNN","f2885667":"- The Network and Save weight","cfbe0d15":"- Create a lot of image from an image. It make many data.","92386dda":"- Normalize data (It include data test and noisy data)","b7a14c63":"- Load dataset from the folder (folder containt image)","df1ba9c3":"**Dinh Van Hung**\n\n04\/03\/2019\n1. Introduce\n2. Prepare Data\n    1. Load Data\n    2. Generative Data\n    3. Normalize Data\n3. CNN\n    1. Define the model (Build the model)\n    2. Execute the model\n4. Evaluate The Model\n    1. Train\n    2. Test\n5. Result\n","adfc381a":"2.1 Load Dataset\n    1. File configuration.py \n        It contain name file to save dataset\n    2. Normalization.py\n        It consisit of generating data into tranform image (like Zoom. brightness, ...)","ed5dbe2e":"5. Result\n    - The model have low result (It's just 76% when We don't remove noisy data!\n    - And We had the method to reomve it. You can follow the tips.","7dc370cc":"4. Evaluate The Model\n    4.1 Training\n        - Using Keras support training and code follow functionality model"}}