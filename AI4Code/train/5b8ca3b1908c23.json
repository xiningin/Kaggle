{"cell_type":{"c1002425":"code","02234e1d":"code","ccdbff27":"code","e718cbb5":"code","a772f71b":"code","49af43b2":"code","55880f5d":"code","746776f2":"code","6d204de3":"code","2627a1d0":"code","3d922183":"code","f13ae513":"code","fa56dd83":"code","668ed572":"code","00d4cb03":"code","f69fb42e":"code","2a370d76":"code","a3e53e44":"code","2bc9efb8":"code","9132e727":"code","b498fa03":"code","7173e745":"code","9e4e511f":"code","e4c65e31":"code","b5fb1a76":"code","80e95c90":"code","c73ad028":"code","0753760c":"code","9bd46954":"code","bc5bea62":"code","5731f8db":"code","1f279d20":"code","fb23ca75":"code","e12927d0":"code","78dcdce3":"code","8cd9e7ce":"code","4fff7573":"code","719b49b6":"code","ca1b7495":"code","5572d305":"code","0e2a5c01":"code","8bd6d10b":"code","f5059275":"code","91af694d":"code","3df1fe8e":"markdown","d4ded266":"markdown","3233a755":"markdown","39b4a62a":"markdown","8677b70f":"markdown","5ae9f165":"markdown","06cedb22":"markdown","3eee2a0f":"markdown","ecd3a23f":"markdown","3062041f":"markdown","6fa21af7":"markdown","11d46bfc":"markdown","90067f25":"markdown","1bbdc4cd":"markdown","2b0c245d":"markdown","8233e6ac":"markdown","2ccc2043":"markdown","30ef1c7c":"markdown","8d0213b5":"markdown","5bd31bd8":"markdown","9be8bb32":"markdown","5527a023":"markdown","8caf0615":"markdown","794fd3bb":"markdown"},"source":{"c1002425":"\n#importing necessary Libraries \n\n#working with data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\n\n\nimport sklearn \nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom collections import defaultdict\nfrom surprise import SVD\nfrom surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split\nimport os\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","02234e1d":"Data=pd.read_csv(\"\/kaggle\/input\/amazon-product-reviews\/ratings_Electronics (1).csv\",names=['UserId', 'ProductId','Rating','timestamp'])\n","ccdbff27":"# Display the data\n\nData.head()\n","e718cbb5":"#checking datatypes of each column\nData.dtypes","a772f71b":"#shape of data \nshape_Data = Data.shape\nprint('Data set contains \"{x}\" number of rows and \"{y}\" number of columns' .format(x=shape_Data[0],y=shape_Data[1]))","49af43b2":"#null check\nsns.heatmap(Data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","55880f5d":"#Oveview of Data\nData.describe().T","746776f2":"print(\"Total data \")\nprint(\"-\"*50)\nprint(\"\\nTotal no of ratings :\",Data.shape[0])\nprint(\"Total No of Users   :\", len(np.unique(Data['UserId'])))\nprint(\"Total No of products  :\", len(np.unique(Data['ProductId'])))\n","6d204de3":"# Rating frequency\n\nsns.set(rc={'figure.figsize': (12, 6)})\nsns.set_style('whitegrid')\nax = sns.countplot(x='Rating', data=Data)\nax.set(xlabel='Rating', ylabel='Count')","2627a1d0":"# let's check what is on avarage rating of each product\nRating_prod = Data.groupby('ProductId')['Rating'].mean()\nRating_prod.head()","3d922183":"sns.distplot(Rating_prod, color=\"green\", kde=True)","f13ae513":"# let's check how many rating does a product have\n\nproduct_rating_count = Data.groupby('ProductId')['Rating'].count()\nproduct_rating_count.head()","fa56dd83":"sns.distplot(product_rating_count, color=\"red\", kde=True, bins=40)","668ed572":"#Analysis of rating given by the user \n\nno_of_rated_products_per_user = Data.groupby(by='UserId')['Rating'].count().sort_values(ascending=False)\nno_of_rated_products_per_user.head()","00d4cb03":"sns.distplot(no_of_rated_products_per_user, color=\"Orange\", kde=True, bins=40)","f69fb42e":"# checking number of users how gave 1 rating rating only.\nuser_1=no_of_rated_products_per_user[no_of_rated_products_per_user==1].count()\n#percentage of user who gave rating only one time are\nper = user_1\/no_of_rated_products_per_user.count()\nprint('Total {} percent of User have just given rating once'.format(per*100))","2a370d76":"print('\\n Number of rated product more than 50 per user : {}\\n'.format(sum(no_of_rated_products_per_user >= 50)) )","a3e53e44":"#Getting the new dataframe which contains users who has given 50 or more ratings\n\nnew_Data=Data.groupby(\"ProductId\").filter(lambda x:x['Rating'].count() >=50)","2bc9efb8":"new_Data.head()","9132e727":"new_Data.shape","b498fa03":"#percentage of data taken\nprint('we are taking {} percent of data from Raw data for analysis'.format(new_Data['UserId'].count()\/Data['UserId'].count()*100))","7173e745":"#Dropping Unwanted Columns\nnew_Data.drop('timestamp',inplace=True,axis=1)","9e4e511f":"#group by product and corresponding mean rating\nratings_mean_count = pd.DataFrame(new_Data.groupby('ProductId')['Rating'].mean())\nratings_mean_count['rating_counts'] = pd.DataFrame(new_Data.groupby('ProductId')['Rating'].count())","e4c65e31":"#let's check for highest rating count\nratings_mean_count['rating_counts'].max()","b5fb1a76":"#let's check for highest rating count\nratings_mean_count['rating_counts'].min()","80e95c90":"#checking distribution of rating_counts\nsns.distplot(ratings_mean_count['rating_counts'],kde=False, bins=40)","c73ad028":"#checking distribution of rating\nsns.distplot(ratings_mean_count['Rating'],kde=False, bins=40)","0753760c":"#Top 10 Product that would be recommended.\npopular=ratings_mean_count.sort_values(['rating_counts','Rating'], ascending=False)\npopular.head(10)","9bd46954":"#Top 30 Product that would be recommended.\npopular.head(30).plot(kind='bar')","bc5bea62":"#Reading the dataset using Surprise package for Model Based Collaborative Filtering\nreader = Reader(rating_scale=(1, 5))\ndata_reader_SVD = Dataset.load_from_df(new_Data,reader)\n#Splitting the dataset with 70% training and 30% testing using Surprise train_test_split\ntrainset_SVD, testset_SVD = train_test_split(data_reader_SVD, test_size=.30)","5731f8db":"#Data Split for Memory Based Collaborative Filtering\n# we were going out of memory problem so lets take first 10lac record to Collaborative filtering process.\n# so splitting data in diffrent part to train them saparately \n# splitting data into 5 Equal parts of 1074862 record each\nreader = Reader(rating_scale=(1, 5))\ndata_reader_1 = Dataset.load_from_df(new_Data.iloc[:1074862,0:],reader)\ndata_reader_2 = Dataset.load_from_df(new_Data.iloc[1074862:2149725,0:],reader)\ndata_reader_3 = Dataset.load_from_df(new_Data.iloc[2149725:3224586,0:],reader)\ndata_reader_4 = Dataset.load_from_df(new_Data.iloc[3224586:4299448,0:],reader)\ndata_reader_5 = Dataset.load_from_df(new_Data.iloc[4299448:,0:],reader)\n\n#Splitting the dataset with 70% training and 30% testing using Surprise train_test_split\ntrainset_1, testset_1 = train_test_split(data_reader_1, test_size=.30)\ntrainset_2, testset_2 = train_test_split(data_reader_2, test_size=.30)\ntrainset_3, testset_3 = train_test_split(data_reader_3, test_size=.30)\ntrainset_4, testset_4 = train_test_split(data_reader_4, test_size=.30)\ntrainset_5, testset_5 = train_test_split(data_reader_5, test_size=.30)\n\n#holding all training set\ntrainset=[trainset_1,trainset_2,trainset_3,trainset_4,trainset_5]\n#holding all testing set\ntestset=[testset_1,testset_2,testset_3,testset_4,testset_5]","1f279d20":"# Use user_based true\/false to switch between user-based or item-based collaborative filtering\nalgo = KNNWithMeans(k=50, sim_options={'name': 'pearson_baseline', 'user_based': False})","fb23ca75":"#fitting all training set and storing testing results\ntest=[]\nfor item in range(5):\n    algo.fit(trainset[item])\n    test.append(algo.test(testset[item]))","e12927d0":"#checking prediction\ntest[0][0:5]","78dcdce3":"algo_SVD = SVD()\nalgo_SVD.fit(trainset_SVD)","8cd9e7ce":"predictions_SVD = algo.test(testset_SVD)","4fff7573":"RMSE_SVD=accuracy.rmse(predictions_SVD, verbose=True)","719b49b6":"popular=ratings_mean_count.sort_values(['rating_counts','Rating'], ascending=False)\npopular.head(10)","ca1b7495":"# evaluating Collobarative filtering (memory based model)\nprint(\"Item-based Model : Test Set\")\nRMSE = []\nTotal_RMSE = 0\nfor i in range(5):\n    RMSE.append(accuracy.rmse(test[i], verbose=True))\n    Total_RMSE = Total_RMSE + RMSE[i]","5572d305":"#avarage RMSE\nprint ('Avarage RMSE for Memory Based Collaborative Filtering of all TEST data is = {}'.format(Total_RMSE\/5))","0e2a5c01":"# evaluating Collobarative filtering (Model based model)\nprint ('Avarage RMSE for Model Based Collaborative Filtering of all TEST data is = {}'.format(RMSE_SVD))\n","8bd6d10b":"#creating function to get top 5 Product Recommendation for each user.\ndef get_top_n(predictions, n=5):\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","f5059275":"top_n = get_top_n(predictions_SVD, n=5)","91af694d":"# Print the recommended items for first 50 user\ncount=0\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, _) in user_ratings])\n    if(count>49):\n        break\n    count=count+1","3df1fe8e":"### <font color='red'>Step 1 <\/font> Read and explore the given dataset","d4ded266":"### <font color='red'>Step 3 <\/font> Build Popularity Recommender model.","3233a755":"#### Evaluating Popularity based model","39b4a62a":"#### this shows that most user have rated just 1 item, with some outliers such as user rating more then 100 item.","8677b70f":"### <font color='red'>Step 6 <\/font> Get top - K ( K = 5) recommendations.","5ae9f165":"#### We can notice that large peak of rating \"5\", this may be because single user rating or some other kind of skewness.","06cedb22":"#### Data Understanding\n1. There is no MISSING data <br>\n2. There are 4 Attributes - *UserId*, *ProductId* are object *Rating* is Integer while *Timestamp* is float <br>\n3. Rating lies between 1-5","3eee2a0f":"#### Model-based collaborative filtering system","ecd3a23f":"#### Thus we can notice\n* For User :A359EJJXW154UD  \n* Recommendation : ['B000M2TAN4', 'B000FGI970', 'B0041OSQ9I', 'B00005LEN4', 'B000ZMCILW']\n\n* There are many Users which having less then 5 Reccomendation that occurs because those products have missing ratings via users.\n* so people having less then 5 recommendations, we will feed in product based on Popularity Based Recommendation.","3062041f":"#### The above graph gives us the most popular products (arranged in descending order) sold by the business.","6fa21af7":"* most User Rated 5","11d46bfc":"#### We can see top product i.e. B0074BW614 have rating 4.49 and number of user who gave rating to this product is 18244, which seems legit thus we can conclude we are getting expected result","90067f25":"#### Evaluating Collobarative filtering (Model based model)","1bbdc4cd":"# Import Libraries ","2b0c245d":"### <font color='red'>Step 6 <\/font>Evaluate both the models.","8233e6ac":"#### this shows that most items have around 0-100 rating, with some outliers such as product having more then 2000 rating","2ccc2043":"#### evaluating Collobarative filtering (memory based model)","30ef1c7c":"# Recommendation System Project\n### <u>Data Description and Context:<\/u>\nAmazon Reviews data (data source) The repository has several datasets. For this case study, we are using the Electronics dataset.\n### <u>Domain:<\/u>\nE-commerce \n\n### <u>Context<\/u>\nOnline E-commerce websites like Amazon, Flipkart uses different recommendation models to provide different suggestions to different users. Amazon currently uses item-to-item collaborative filtering, which scales to massive data sets and produces high-quality recommendations in real-time.\n\n### <u>Attribute Information:<\/u>\n* userId : Every user identified with a unique id\n* productId : Every product identified with a unique id\n* Rating : Rating of the corresponding product by the corresponding user\n* timestamp : Time of the rating ( ignore this column for this exercise)\n\n### <u>Objective:<\/u>\nBuild a recommendation system to recommend products to customers based on the their previous ratings for other products.\n","8d0213b5":"### <font color='red'>Step 5 <\/font>Build Collaborative Filtering model \n#### Memory Based Collaborative Filtering","5bd31bd8":"# Please upvote if you like this kernel\n","9be8bb32":"### <font color='red'>Step 2 <\/font> Take a subset of the dataset to make it less sparse\/ denser.","5527a023":"# Types of recommendations\n\nThere are mainly 6 types of the recommendations systems :-\n\n1. Popularity based systems :- It works by recommeding items viewed and purchased by most people and are rated high.It is not a personalized recommendation.\n2. Classification model based:- It works by understanding the features of the user and applying the classification algorithm to decide whether the user is     interested or not in the prodcut.\n3. Content based recommedations:- It is based on the information on the contents of the item rather than on the user opinions.The main idea is if the user likes an item then he or she will like the \"other\" similar item.\n4. Collaberative Filtering:- It is based on assumption that people like things similar to other things they like, and things that are liked by other people with similar taste. it is mainly of two types:\n a) User-User \n b) Item -Item\n \n5. Hybrid Approaches:- This system approach is to combine collaborative filtering, content-based filtering, and other approaches . \n6. Association rule mining :- Association rules capture the relationships between items based on their patterns of co-occurrence across transactions.\n\n","8caf0615":"### <font color='red'>Step 4 <\/font>Split the data randomly into train and test dataset.","794fd3bb":"* Collaborative filtering techniques aim to fill in the missing entries of a user-item association matrix.\n* We are going to use collaborative filtering approach. This is based on the idea that the best recommendations come from people who have similar tastes."}}