{"cell_type":{"9ce4297d":"code","29246c05":"code","9b372555":"code","95f5f2d2":"code","8cc57228":"code","11e8828e":"code","ae2fced7":"code","699b8765":"code","208a4bd4":"code","bec0455c":"code","d5934e97":"code","4a16ced0":"code","546b19a5":"code","3835697f":"code","4c230f32":"code","06f7ef99":"code","ce6ce106":"code","8d508f43":"code","b2660919":"code","2bc7e9c5":"code","9389186c":"code","86e29ca8":"code","7df843e5":"code","de096ab2":"code","09623edd":"code","ba92ddac":"code","ff839b82":"code","70c42920":"code","04464a1b":"code","7d9b6d05":"code","622992f0":"code","9aea5d0e":"code","74971a80":"code","228c099c":"code","65cd2974":"code","be046238":"code","18ec59e4":"code","d9ade43a":"code","1b087efb":"code","82cd057b":"code","3339725c":"markdown","afe82f49":"markdown","8d39062c":"markdown","6d8ede95":"markdown","604d7994":"markdown","675d18bf":"markdown","78b6975c":"markdown","81230a21":"markdown","919333fc":"markdown","d8b4c0d2":"markdown","e40bcb55":"markdown","863bb84a":"markdown","d90e9521":"markdown","b041a23a":"markdown","550b0841":"markdown","77e01468":"markdown","4473ec6f":"markdown","47935b76":"markdown","c93d2d9c":"markdown","f9fb5530":"markdown","71502fef":"markdown","a6feb9a4":"markdown","3a922d9f":"markdown","c3aa2f12":"markdown","702ec40d":"markdown","d7a0697c":"markdown","db0f697e":"markdown","e8a650ab":"markdown","193390b7":"markdown","e5e4b715":"markdown","1ff5a5a0":"markdown","567c36ad":"markdown","2780cb72":"markdown","3df0a26c":"markdown","abbc5892":"markdown","3d29d808":"markdown","d1267324":"markdown","9c9af416":"markdown","7a201ab8":"markdown","4c88fb2e":"markdown","c3fc6c3a":"markdown","44112971":"markdown","c3036eb8":"markdown","657db14a":"markdown","476bed35":"markdown","baf24bb4":"markdown","1d84a67b":"markdown","9adebf2c":"markdown"},"source":{"9ce4297d":"# for path related functionalities\nimport os\n# for array operations\nimport numpy as np\n# tensorflow framework\nimport tensorflow as tf\n# keras API for deep learning\nfrom tensorflow import keras\n# for image visulaizations\nimport matplotlib.pyplot as plt\n# for legends and other supporting functionalities\nimport matplotlib as mpl\n# for viewing iteration status\nfrom tqdm import tqdm","29246c05":"# a list to collect paths of 1000 images\nimage_path = []\nfor root, dirs, files in os.walk('..\/input\/people-clothing-segmentation\/png_images\/IMAGES'):\n    # iterate over 1000 images\n    for file in files:\n        # create path\n        path = os.path.join(root,file)\n        # add path to list\n        image_path.append(path)\nlen(image_path)","9b372555":"# a list to collect paths of 1000 masks\nmask_path = []\nfor root, dirs, files in os.walk('..\/input\/people-clothing-segmentation\/png_masks\/MASKS'):\n    #iterate over 1000 masks\n    for file in files:\n        # obtain the path\n        path = os.path.join(root,file)\n        # add path to the list\n        mask_path.append(path)\nlen(mask_path)","95f5f2d2":"print(image_path[:5])","8cc57228":"print(mask_path[:5])","11e8828e":"image_path.sort()\nmask_path.sort()","ae2fced7":"# create a list to store images\nimages = []\n# iterate over 1000 image paths\nfor path in tqdm(image_path):\n    # read file\n    file = tf.io.read_file(path)\n    # decode png file into a tensor\n    image = tf.image.decode_png(file, channels=3, dtype=tf.uint8)\n    # append to the list\n    images.append(image)\n\n# create a list to store masks\nmasks = []\n# iterate over 1000 mask paths\nfor path in tqdm(mask_path):\n    # read the file\n    file = tf.io.read_file(path)\n    # decode png file into a tensor\n    mask = tf.image.decode_png(file, channels=1, dtype=tf.uint8)\n    # append mask to the list\n    masks.append(mask)","699b8765":"len(images), len(masks)","208a4bd4":"plt.figure(figsize=(16,5))\nfor i in range(1,4):\n    plt.subplot(1,3,i)\n    img = images[i]\n    plt.imshow(img)\n    plt.colorbar()\n    plt.axis('off')\nplt.show()","bec0455c":"# Define a normalizer that can be applied while visualizing masks to have a consistency\n# min class value is 0\n# max class value is 58\nNORM = mpl.colors.Normalize(vmin=0, vmax=58)\n\n# plot masks\nplt.figure(figsize=(16,5))\nfor i in range(1,4):\n    plt.subplot(1,3,i)\n    img = masks[i]\n    plt.imshow(img, cmap='jet', norm=NORM)\n    plt.colorbar()\n    plt.axis('off')\nplt.show()","d5934e97":"# Use pre-trained DenseNet121 without head\nbase = keras.applications.DenseNet121(input_shape=[128,128,3], \n                                      include_top=False, \n                                      weights='imagenet')","4a16ced0":"len(base.layers)","546b19a5":"keras.utils.plot_model(base, show_shapes=True)","3835697f":"skip_names = ['conv1\/relu', # size 64*64\n             'pool2_relu',  # size 32*32\n             'pool3_relu',  # size 16*16\n             'pool4_relu',  # size 8*8\n             'relu'        # size 4*4\n             ]","4c230f32":"skip_outputs = [base.get_layer(name).output for name in skip_names]\nfor i in range(len(skip_outputs)):\n    print(skip_outputs[i])","06f7ef99":"downstack = keras.Model(inputs=base.input,\n                       outputs=skip_outputs)\n# freeze the downstack layers\ndownstack.trainable = False","ce6ce106":"!pip install -q git+https:\/\/github.com\/tensorflow\/examples.git","8d508f43":"from tensorflow_examples.models.pix2pix import pix2pix","b2660919":"# Four upstack layers for upsampling sizes \n# 4->8, 8->16, 16->32, 32->64 \nupstack = [pix2pix.upsample(512,3),\n          pix2pix.upsample(256,3),\n          pix2pix.upsample(128,3),\n          pix2pix.upsample(64,3)]","2bc7e9c5":"upstack[0].layers","9389186c":"# define the input layer\ninputs = keras.layers.Input(shape=[128,128,3])\n\n# downsample \ndown = downstack(inputs)\nout = down[-1]\n\n# prepare skip-connections\nskips = reversed(down[:-1])\n# choose the last layer at first 4 --> 8\n\n# upsample with skip-connections\nfor up, skip in zip(upstack,skips):\n    out = up(out)\n    out = keras.layers.Concatenate()([out,skip])\n    \n# define the final transpose conv layer\n# image 128 by 128 with 59 classes\nout = keras.layers.Conv2DTranspose(59, 3,\n                                  strides=2,\n                                  padding='same',\n                                  )(out)\n# complete unet model\nunet = keras.Model(inputs=inputs, outputs=out)","86e29ca8":"keras.utils.plot_model(unet, show_shapes=True)","7df843e5":"images[0].shape, masks[0].shape","de096ab2":"def resize_image(image):\n    # scale the image\n    image = tf.cast(image, tf.float32)\n    image = image\/255.0\n    # resize image\n    image = tf.image.resize(image, (128,128))\n    return image\n\ndef resize_mask(mask):\n    # resize the mask\n    mask = tf.image.resize(mask, (128,128))\n    mask = tf.cast(mask, tf.uint8)\n    return mask    ","09623edd":"X = [resize_image(i) for i in images]\ny = [resize_mask(m) for m in masks]\nlen(X), len(y)","ba92ddac":"images[0].dtype, masks[0].dtype, X[0].dtype, y[0].dtype","ff839b82":"# plot an image\nplt.imshow(X[0])\nplt.colorbar()\nplt.show()\n\n#plot a mask\nplt.imshow(y[0], cmap='jet')\nplt.colorbar()\nplt.show()","70c42920":"from sklearn.model_selection import train_test_split\n# split data into 80\/20 ratio\ntrain_X, val_X,train_y, val_y = train_test_split(X, y, test_size=0.2, \n                                                      random_state=0\n                                                     )\n# develop tf Dataset objects\ntrain_X = tf.data.Dataset.from_tensor_slices(train_X)\nval_X = tf.data.Dataset.from_tensor_slices(val_X)\n\ntrain_y = tf.data.Dataset.from_tensor_slices(train_y)\nval_y = tf.data.Dataset.from_tensor_slices(val_y)\n\n# verify the shapes and data types\ntrain_X.element_spec, train_y.element_spec, val_X.element_spec, val_y.element_spec","04464a1b":"def brightness(img, mask):\n    # adjust brightness of image\n    # don't alter in mask\n    img = tf.image.adjust_brightness(img, 0.1)\n    return img, mask\n\ndef gamma(img, mask):\n    # adjust gamma of image\n    # don't alter in mask\n    img = tf.image.adjust_gamma(img, 0.1)\n    return img, mask\n\ndef hue(img, mask):\n    # adjust hue of image\n    # don't alter in mask\n    img = tf.image.adjust_hue(img, -0.1)\n    return img, mask\n\ndef crop(img, mask):\n    # crop both image and mask identically\n    img = tf.image.central_crop(img, 0.7)\n    # resize after cropping\n    img = tf.image.resize(img, (128,128))\n    mask = tf.image.central_crop(mask, 0.7)\n    # resize afer cropping\n    mask = tf.image.resize(mask, (128,128))\n    # cast to integers as they are class numbers\n    mask = tf.cast(mask, tf.uint8)\n    return img, mask\n\ndef flip_hori(img, mask):\n    # flip both image and mask identically\n    img = tf.image.flip_left_right(img)\n    mask = tf.image.flip_left_right(mask)\n    return img, mask\n\ndef flip_vert(img, mask):\n    # flip both image and mask identically\n    img = tf.image.flip_up_down(img)\n    mask = tf.image.flip_up_down(mask)\n    return img, mask\n\ndef rotate(img, mask):\n    # rotate both image and mask identically\n    img = tf.image.rot90(img)\n    mask = tf.image.rot90(mask)\n    return img, mask","7d9b6d05":"# zip images and masks\ntrain = tf.data.Dataset.zip((train_X, train_y))\nval = tf.data.Dataset.zip((val_X, val_y))\n\n# perform augmentation on train data only\n\na = train.map(brightness)\nb = train.map(gamma)\nc = train.map(hue)\nd = train.map(crop)\ne = train.map(flip_hori)\nf = train.map(flip_vert)\ng = train.map(rotate)\n\n# concatenate every new augmented sets\ntrain = train.concatenate(a)\ntrain = train.concatenate(b)\ntrain = train.concatenate(c)\ntrain = train.concatenate(d)\ntrain = train.concatenate(e)\ntrain = train.concatenate(f)\ntrain = train.concatenate(g)","622992f0":"BATCH = 64\nAT = tf.data.AUTOTUNE\nBUFFER = 1000\n\nSTEPS_PER_EPOCH = 800\/\/BATCH\nVALIDATION_STEPS = 200\/\/BATCH","9aea5d0e":"train = train.cache().shuffle(BUFFER).batch(BATCH).repeat()\ntrain = train.prefetch(buffer_size=AT)\nval = val.batch(BATCH)","74971a80":"# infer on train dataset\nexample = next(iter(train))\npreds = unet(example[0])\n# visualize an image\nplt.imshow(example[0][60])\nplt.colorbar()\nplt.show()","228c099c":"# visualize the predicted mask\npred_mask = tf.argmax(preds, axis=-1)\npred_mask = tf.expand_dims(pred_mask, -1)\nplt.imshow(pred_mask[0], cmap='jet', norm=NORM)\nplt.colorbar()","65cd2974":"def Compile_Model():\n    unet.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            optimizer=keras.optimizers.RMSprop(lr=0.001),\n            metrics=['accuracy']) \nCompile_Model()","be046238":"hist_1 = unet.fit(train,\n               validation_data=val,\n               steps_per_epoch=STEPS_PER_EPOCH,\n               validation_steps=VALIDATION_STEPS,\n               epochs=50,\n               verbose=2)","18ec59e4":"# select a validation data batch\nimg, mask = next(iter(val))\n# make prediction\npred = unet.predict(img)\nplt.figure(figsize=(20,28))\n\nk = 0\nfor i in pred:\n    # plot the predicted mask\n    plt.subplot(4,3,1+k*3)\n    i = tf.argmax(i, axis=-1)\n    plt.imshow(i,cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Prediction')\n    \n    # plot the groundtruth mask\n    plt.subplot(4,3,2+k*3)\n    plt.imshow(mask[k], cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Ground Truth')\n    \n    # plot the actual image\n    plt.subplot(4,3,3+k*3)\n    plt.imshow(img[k])\n    plt.axis('off')\n    plt.title('Actual Image')\n    k += 1\n    if k == 4: break\nplt.suptitle('Predition After 50 Epochs (No Fine-tuning)', color='red', size=20)  \nplt.show()","d9ade43a":"downstack.trainable = True\n# compile again\nCompile_Model()\n# train from epoch 51 to 150\nhist_2 = unet.fit(train,\n               validation_data=val,\n               steps_per_epoch=STEPS_PER_EPOCH,\n               validation_steps=VALIDATION_STEPS,\n               epochs=150, initial_epoch = 50,\n               verbose = 2\n                 )","1b087efb":"# select a validation data batch\nimg, mask = next(iter(val))\n# make prediction\npred = unet.predict(img)\nplt.figure(figsize=(20,30))\n\nk = 0\nfor i in pred:\n    # plot the predicted mask\n    plt.subplot(4,3,1+k*3)\n    i = tf.argmax(i, axis=-1)\n    plt.imshow(i,cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Prediction')\n    \n    # plot the groundtruth mask\n    plt.subplot(4,3,2+k*3)\n    plt.imshow(mask[k], cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Ground Truth')\n    \n    # plot the actual image\n    plt.subplot(4,3,3+k*3)\n    plt.imshow(img[k])\n    plt.axis('off')\n    plt.title('Actual Image')\n    k += 1\n    if k == 4: break\nplt.suptitle('Predition After 150 Epochs (By Fine-tuning from 51th Epoch)', color='red', size=20)  \nplt.show()","82cd057b":"history_1 = hist_1.history\nacc=history_1['accuracy']\nval_acc = history_1['val_accuracy']\n\nhistory_2 = hist_2.history\nacc.extend(history_2['accuracy'])\nval_acc.extend(history_2['val_accuracy'])\n\nplt.plot(acc[:150], '-', label='Training')\nplt.plot(val_acc[:150], '--', label='Validation')\nplt.plot([50,50],[0.7,1.0], '--g', label='Fine-Tuning')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.ylim([0.7,1.0])\nplt.legend()\nplt.show()","3339725c":"### How many layers does this model have?","afe82f49":"# Performance Curves","8d39062c":"# Build U-Net model with skip-connections","6d8ede95":"### Build the downstack with the above layers. We use the pre-trained model as such, without any fine-tuning.","604d7994":"# Prediction","675d18bf":"# Data Augmentation ","78b6975c":"### The model is trained. Make prediction and visualize the output. Compare the results with groundtruth masks","81230a21":"### We split data into training and validation set in 80\/20 ratio. We develop TensorFlow Dataset objects for train and validation sets to ease further processing and data handling.","919333fc":"# Build Downstack with a Pre-trained CNN","d8b4c0d2":"### Fine-tune the model after 50 epochs. Unfreeze the downstack, compile the model once again. Re-train it for 100 more epochs.","e40bcb55":"### Build a U-Net model by merging downstack and upstack with skip-connections.","863bb84a":"### This dataset has 1000 images of people (one person per image). There are 1000 masks corresponding to those original images. Label images have 59 segmented classes corresponding to classes such as hair, bag, shirt, shoes, skin, sunglasses and cap.","d90e9521":"# Import necessary frameworks, libraries and modules","b041a23a":"### Let's plot our model","550b0841":"### Read and decode the images and masks; store them in separate lists","77e01468":"### Trainable part of our model is very small with fewer parameters. Hence we can train the model for more epochs quickly. However, version 8 of this notebook suggested that the model hardly learnt anything after 50 epochs (which was trained for 200 epochs). So we try train the model for 50 epochs by freezing the pre-trained model and unfreezing afterwards.","4473ec6f":"# Data Preprocessing","47935b76":"### Plot the performance curves to understand how the model learnt on the data ","c93d2d9c":"### The DenseNet121 model has 427 layers. We need to identify suitable layers whose output will be used for skip connections. Plot the entire model, along with the feature shapes.","f9fb5530":"# Training and Fine-tuning","71502fef":"### Model performance can be improved by tuning hyper-parameters and increasing the training data with few more data augmentation functions.","a6feb9a4":"### Sample few images and visualize","3a922d9f":"### Compile the model with the RMSprop optimizer, a low learning rate, the accuracy metric, and the sparse categorical cross entropy loss function","c3aa2f12":"### Obtain the outputs of these layers.","702ec40d":"# Build Upstack","d7a0697c":"### From the above plot, we select the final ReLU activation layer for each feature map size, i.e. 4, 8, 16, 32, and 64, required for skip-connections. Write down the names of the selected ReLU layers in a list.","db0f697e":"# Check for Model and Data compatibility","e8a650ab":"# Semantic Segmentation Using TensorFlow Keras\n\n### Semantic segmentation can be defined as the process of pixel-level image classification into two or more Object classes. It differs from image classification entirely, as the latter performs image-level classification. For instance, consider an image that consists mainly of a zebra, surrounded by grass fields, a tree and a flying bird. Image classification tells us that the image belongs to the \u2018zebra\u2019 class. It can not tell where the zebra is or what its size or pose is. But, semantic segmentation of that image may tell that there is a zebra, grass field, a bird and a tree in the given image (classifies parts of an image into separate classes). And it tells us which pixels in the image belong to which class!\n","193390b7":"### Resize data as the model expects. ","e5e4b715":"### Can our data fit into our model? Is the shape, batch size and everything okay? Check compatibility by inferencing with the untrained model.","1ff5a5a0":"### Fine-tuning has improved the model performance significantly!","567c36ad":"### Batch datasets as our SGD based optimizer would expect, and prefetch to have memory efficient training","2780cb72":"### Make some prediction and visualize them to evaluate the model qualitatively.","3df0a26c":"# Images are unsorted in the dataset by default. We must sort them by their file names to obtain the right image-mask pairs.","abbc5892":"### Thank you for your time!","3d29d808":"### Build the upstack using an upsampling template. Let's use pix2pix template available open-source in `tensorflow_examples` repository","d1267324":"### Find below a Typical U-Net Architecture with Skip-Connections (Source: https:\/\/lmb.informatik.uni-freiburg.de\/Publications\/2015\/RFB15a\/u-net-architecture.png)","9c9af416":"# Download and prepare data","7a201ab8":"### Perform data augmentation with original training set and concatenate with enlarged training set. Do not perform data augmentation with validation set. With 7 augmentation functions and 800 input examples, we can get 7*800 = 5600 new examples. Including original examples, we get 5600+800 = 6400 examples for training. That sounds good!","4c88fb2e":"### Here, we wish to use the functional approach of U-Net architecture, but we will have our own architecture suitable to our task. The downstack can be a pre-trained CNN, trained for image classification (e.g. MobileNetV2, ResNet, NASNet, Inception, DenseNet, or EfficientNet). It can effectively extract the features. But, we have to build our upstack to match our classes (here, 59), build skip-connections, and train it with our data. \n\n### We prefer a pre-trained DenseNet121 to be the downstack that can be obtained through transfer learning and build the upstack with pix2pix, a publicly available generative upstack template (it saves our time and code).","c3fc6c3a":"### Sample corresponding masks and visualize","44112971":"### Visualize a resized image and a resized mask","c3036eb8":"# Split Data for training and validation","657db14a":"### We have less data (just 800 examples in training set) that is not enough for deep learning. Hence we should increase the amount of training data by performing data augmentations. Define Functions for data augmentation.","476bed35":"### Build the upstack","baf24bb4":"### How many images and masks are there?","1d84a67b":"### We have to build a computer vision model that can convert an input image into a segmented image (also called masked image or label image). Building a model from scratch and training it is not a good idea as we have very limited data for training (1000 images are insufficient for 59 unbalanced classes). So we prefer a pre-trained model through transfer learning.\n\n### By understanding how semantic segmentation works, we can easily come up with an idea of how to choose our pre-trained model. One of the popular architectural approaches is FCNN (Fully Convolutional Neural Networks). In contrast to CNNs in image classification, where the decision head is made up of dense layers, an FCNN is made up of layers related to convolutional operations only. Because the final output is an image of a shape identical to the input image. \n\n### An FCNN contains two parts: an encoder and a decoder. An encoder is a downstack of convolutional neural layers that extract features from the input image. A decoder is an upstack of transpose convolutional neural layers that builds the segmented image from the extracted features. The sizes of feature maps go down while downsampling (e.g. 128, 64, 32, 16, 8, 4 \u2013 in order), and they go up while upsampling (e.g. 4, 8, 16, 32, 64, 128 \u2013 in order).\n\n### Among FCNNs, U-Net is one of the successful architectures acclaimed for its performance in Medical Image Segmentation. It encourages skip connections between a few specific-sized layers of downstack and upstack. Skip-connections yield better performance because of the truth that upstack struggles to build finer details of the image on its own during upsampling. Skip-connections bye-pass a large stack of layers to feed finer details from a downstack layer to its corresponding upstack layer.","9adebf2c":"![A typical U-Net Architecture with Skip-Connections (Source: https:\/\/lmb.informatik.uni-freiburg.de\/Publications\/2015\/RFB15a\/u-net-architecture.png)](https:\/\/lmb.informatik.uni-freiburg.de\/Publications\/2015\/RFB15a\/u-net-architecture.png)"}}