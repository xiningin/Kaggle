{"cell_type":{"0ebdc5fa":"code","29a1ec69":"code","9e0c36d7":"code","dc634a98":"code","4d011fa6":"code","a1b31a22":"code","9ed2dafd":"code","95f6e0e2":"code","ff4ccbb6":"code","8ba52c20":"code","1646ed0b":"markdown","595055cf":"markdown","d2f32cc4":"markdown","4015e869":"markdown","2726ba53":"markdown","07ab7229":"markdown","02729391":"markdown","dc2f25bd":"markdown","2102f35d":"markdown"},"source":{"0ebdc5fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\nimport keras.layers.core as core\nimport keras.layers.convolutional as conv\nimport keras.models as models\nimport keras.utils.np_utils as kutils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n","29a1ec69":"# Any results you write to the current directory are saved as output.\nimg_rows, img_cols = 28, 28\n","9e0c36d7":"def load_dataset(train_path,test_path):\n    global train,test,trainX,trainY,nb_classes\n    train = pd.read_csv(train_path).values # produces numpy array\n    test  = pd.read_csv(test_path).values # produces numpy array\n    print(\"Train Shape :\",train.shape)\n    trainX = train[:, 1:].reshape(train.shape[0], img_rows, img_cols, 1)\n    trainX = trainX.astype(float)\n    trainX \/= 255.0\n    trainY = kutils.to_categorical(train[:, 0])\n    nb_classes = trainY.shape[1]\n    print(\"TrainX Shape : \",trainX.shape)\n    print(\"Trainy shape : \",trainY.shape)\n    testX = test.reshape(test.shape[0], 28, 28, 1)\n    testX = testX.astype(float)\n    testX \/= 255.0\n    trainY = kutils.to_categorical(train[:, 0])\n    return train,test,trainX,trainY,testX,nb_classes","dc634a98":"def createModel(inp_shape,nClasses):\n    model = models.Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=inp_shape))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n \n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n \n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(nClasses, activation='softmax'))\n    \n    # Define the optimizer\n    optimizer1 = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n    \n    model.compile(optimizer=optimizer1, loss='categorical_crossentropy', metrics=['accuracy'])\n \n    return model","4d011fa6":"def submission(prediction):\n    np.savetxt('mnist-submission.csv', np.c_[range(1,len(prediction)+1),prediction], delimiter=',', header = 'ImageId,Label', comments = '', fmt='%d')","a1b31a22":"import matplotlib.pyplot as plt\ndef result_visualization(out):\n    # Loss Curves\n    plt.figure(figsize=[8,6])\n    plt.plot(out.history['loss'],'r',linewidth=3.0)\n    plt.plot(out.history['val_loss'],'b',linewidth=3.0)\n    plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n    plt.xlabel('Epochs ',fontsize=16)\n    plt.ylabel('Loss',fontsize=16)\n    plt.title('Loss Curves',fontsize=16)\n \n    # Accuracy Curves\n    plt.figure(figsize=[8,6])\n    plt.plot(out.history['acc'],'r',linewidth=3.0)\n    plt.plot(out.history['val_acc'],'b',linewidth=3.0)\n    plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n    plt.xlabel('Epochs ',fontsize=16)\n    plt.ylabel('Accuracy',fontsize=16)\n    plt.title('Accuracy Curves',fontsize=16)","9ed2dafd":"def mnist_eda(Y_train):\n    g = sns.countplot(Y_train)\n    Y_train.value_counts()","95f6e0e2":"def classification_report(X_test,test):\n    #get the predictions for the test data\n    predicted_classes = model.predict_classes(X_test)\n\n    #get the indices to be plotted\n    y_true = test.iloc[:, 0]\n    correct = np.nonzero(predicted_classes==y_true)[0]\n    incorrect = np.nonzero(predicted_classes!=y_true)[0]\n    target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n    print(classification_report(y_true, predicted_classes, target_names=target_names))\n","ff4ccbb6":"# Main\ntrain_path=\"..\/input\/train.csv\"\ntest_path=\"..\/input\/test.csv\"\n#test and testX are test dataset used for evaluation, train and trainX,trainY are training datasets\ntrain,test,trainX,trainY,testX,nb_classes=load_dataset(train_path,test_path)\n# Splitting dataset for training and testing\nX_train, X_test, y_train, y_test = train_test_split(trainX,trainY,test_size=0.1, random_state=21)\n#Model Creation\ninp_shape=(28,28,1)\nmodel=createModel(inp_shape,nb_classes)\n#Training with Image Augmentation\nimgaug=False\nbatch_size=128 #256\nnb_epochs=30\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nif imgaug==True:\n    datagen = ImageDataGenerator(\n            featurewise_center=False,  # set input mean to 0 over the dataset\n            samplewise_center=False,  # set each sample mean to 0\n            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n            samplewise_std_normalization=False,  # divide each input by its std\n            zca_whitening=False,  # apply ZCA whitening\n            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range = 0.1, # Randomly zoom image \n            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n            horizontal_flip=False,  # randomly flip images\n            vertical_flip=False)  # randomly flip images\n    datagen.fit(trainX)\n    out = model.fit_generator(datagen.flow(trainX,trainY, batch_size=batch_size),\n                              epochs = nb_epochs, validation_data = (X_test,y_test),\n                              verbose = 2, steps_per_epoch=batch_size \/\/ batch_size\n                              , callbacks=[learning_rate_reduction] )\n#Training\nelse:\n    out=model.fit(trainX, trainY, batch_size=batch_size, nb_epoch=nb_epochs, verbose=1,\n             validation_data=(X_test, y_test))\n    \n\n\n#Prediction\nyPred = model.predict_classes(testX)\nprint(\"Predictions : \",yPred)\n#Submission of results\nsubmission(yPred)\n\n#Result Visualiztion\nresult_visualization(out)\n\n","8ba52c20":"#Classification Report\n#classification_report(X_test,test)","1646ed0b":"# Main Driver","595055cf":"# Model Creation","d2f32cc4":"# Classification Report","4015e869":"# Initializations","2726ba53":"# Visualization of Results","07ab7229":"# EDA","02729391":"# Submission","dc2f25bd":"# Imports","2102f35d":"# Data Loading"}}