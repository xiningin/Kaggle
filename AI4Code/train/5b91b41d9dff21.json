{"cell_type":{"b8bf82e6":"code","1324f03f":"code","13bfd110":"code","730bc788":"code","37bb0a15":"code","54e7c498":"code","c2fdcbe2":"code","6a06661b":"code","1b9c37de":"code","09e8a0e8":"code","a7d4da3a":"code","ef03c0e0":"code","3096c93b":"code","ebbd1f68":"code","44b74f37":"code","0855a1e5":"code","337881ab":"code","deaf3e93":"code","6ff5c434":"code","cb96989c":"code","3057c80c":"code","e844de29":"code","2fc534b0":"code","0223ced6":"code","cdb880aa":"code","bbaaea2e":"code","b4ebd2da":"code","78c6eff9":"code","de8398c1":"code","ae517564":"code","11ffd68a":"markdown","64bd97b0":"markdown","fa92e7d5":"markdown","946d08ca":"markdown","f95dddb4":"markdown","ff96d596":"markdown","4eec020a":"markdown","122bef41":"markdown","6bd076c0":"markdown"},"source":{"b8bf82e6":"# before activating GPU\n# !nvidia-smi","1324f03f":"!nvidia-smi","13bfd110":"import torch\nfrom torch import nn","730bc788":"torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')","37bb0a15":"torch.cuda.device_count()","54e7c498":"def try_gpu(i=0):\n    if torch.cuda.device_count() >= i +1:\n        return torch.device(f'cuda:{i}')\n    else:\n        return torch.device('cpu')\n    ","c2fdcbe2":"def try_all_gpus():\n    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count()) ]\n    return devices if devices else torch.device('cpu')","6a06661b":"try_gpu(1), try_all_gpus()","1b9c37de":"x = torch.tensor([1,2,3])\nx.device","09e8a0e8":"x =  torch.ones(2,3, device=try_gpu())","a7d4da3a":"x","ef03c0e0":"y = torch.ones(2,3, device=try_gpu(1))","3096c93b":"y # on cpu","ebbd1f68":"z = x.cuda(0)","44b74f37":"z","0855a1e5":"x+ z","337881ab":"z.cuda(0) is z","deaf3e93":"# using gpu with neural network\nnet = nn.Sequential(nn.Linear(3,1))\nnet = net.to(device=try_gpu(0))","6ff5c434":"net(torch.randn(2,3).to(device=try_gpu(0)))","cb96989c":"net[0].weight.device","3057c80c":"# answer to question 1\n\nz = torch.randn(1000, 1000)\nx = torch.randn(1000, 1000)","e844de29":"%%time\nz * x","2fc534b0":"z = torch.randn(1000, 1000).to(device=try_gpu())\nx = torch.randn(1000, 1000).to(device=try_gpu())","0223ced6":"%%time\nz * x","cdb880aa":"%%time\nz = torch.randn(1,1)\nx = torch.randn(1,1)\nz*x","bbaaea2e":"%%time\nz = torch.randn(1,1).to(device=try_gpu())\nx = torch.randn(1,1).to(device=try_gpu())\nz*x","b4ebd2da":"# answer to question 3\nz = torch.randn(100,100).to(device=try_gpu())\nx = torch.randn(100,100).to(device=try_gpu())\n\ny = z*x\ny.shape","78c6eff9":"%%time\nfor i in range(1000):\n    z = z*x\n\ntorch.norm(z)","de8398c1":"\nz = torch.randn(100,100)\nx = torch.randn(100,100)\n\ny = z*x\ny.shape","ae517564":"%%time\nfor i in range(1000):\n    z = z*x\n\ntorch.norm(z)","11ffd68a":"People use GPUs to do machine learning because they expect them to be fast. But transferring\nvariables between devices is slow. So we want you to be 100% certain that you want to do some\u0002thing slow before we let you do it. If the deep learning framework just did the copy automatically\nwithout crashing then you might not realize that you had written some slow code.\nAlso, transferring data between devices (CPU, GPUs, and other machines) is something that is\nmuch slower than computation. It also makes parallelization a lot more difficult, since we have to\nwait for data to be sent (or rather to be received) before we can proceed with more operations. This\nis why copy operations should be taken with great care. As a rule of thumb, many small operations\nare much worse than one big operation. Moreover, several operations at a time are much better\nthan many single operations interspersed in the code unless you know what you are doing. This\nis the case since such operations can block if one device has to wait for the other before it can do\nsomething else. It is a bit like ordering your coffee in a queue rather than pre-ordering it by phone\nand finding out that it is ready when you are.\nLast, when we print tensors or convert tensors to the NumPy format, if the data is not in the main\nmemory, the framework will copy it to the main memory first, resulting in additional transmis\u0002sion overhead. Even worse, it is now subject to the dreaded global interpreter lock that makes\neverything wait for Python to complete.","64bd97b0":"By default, tensors are created on the CPU. We can query the device where the tensor is located.\n","fa92e7d5":"z and x can be computed together. since they are on the same device, subject to shape limitations.","946d08ca":"Lets define functions that allows us to run the requested code even if multiple GPUs dont exist","f95dddb4":"In PyTorch, every array has a device, we often refer it as a context. So far, by default, all variables\nand associated computation have been assigned to the CPU. Typically, other contexts might be\nvarious GPUs. Things can get even hairier when we deploy jobs across multiple servers. By as\u0002signing arrays to contexts intelligently, we can minimize the time spent transferring data between\ndevices. For example, when training neural networks on a server with a GPU, we typically prefer\nfor the model\u02bcs parameters to live on the GPU.\nNext, we need to confirm that the GPU version of PyTorch is installed. If a CPU version of PyTorch\nis already installed, we need to uninstall it first. For example, use the pip uninstall torch com\u0002mand, then install the corresponding PyTorch version according to your CUDA version. Assuming\nyou have CUDA 10.0 installed, you can install the PyTorch version that supports CUDA 10.0 via pip\ninstall torch-cu100.\n","ff96d596":"### Exercises\n\n1. Try a larger computation task, such as the multiplication of large matrices, and see the dif\u0002ference in speed between the CPU and GPU. What about a task with a small amount of cal\u0002culations?\n2. How should we read and write model parameters on the GPU?\n\n* by indexing right\n\n3. Measure the time it takes to compute 1000 matrix-matrix multiplications of 100 \u00d7 100 matri\u0002ces and log the Frobenius norm of the output matrix one result at a time vs. keeping a log on the GPU and transferring only the final result.\n\n4. Measure how much time it takes to perform two matrix-matrix multiplications on two GPUs\nat the same time vs. in sequence on one GPU. Hint: you should see almost linear scaling.\n\n* in one gpu we have seen how much time it takes, however since second GPU is not available we are letting this one go.","4eec020a":"Imagine that your variable Z already lives on your second GPU. What happens if we still call Z.\ncuda(1)? It will return Z instead of making a copy and allocating new memory","122bef41":"It is important to note that whenever we want to operate on multiple terms, they need to be on the\nsame device. For instance, if we sum two tensors, we need to make sure that both arguments live\non the same device\u2014otherwise the framework would not know where to store the result or even\nhow to decide where to perform the computation.\n\nThere are several ways to store a tensor on the GPU. For example, we can specify a storage device\nwhen creating a tensor. Next, we create the tensor variable X on the first gpu. The tensor created\non a GPU only consumes the memory of this GPU. We can use the nvidia-smi command to view\nGPU memory usage. In general, we need to make sure that we do not create data that exceed the\nGPU memory limit.","6bd076c0":"We can specify devices, such as CPUs and GPUs, for storage and calculation. By default, tensors\nare created in the main memory and then use the CPU to calculate it.\nIn PyTorch, the CPU and GPU can be indicated by torch.device('cpu') and torch.\ndevice('cuda'). It should be noted that the cpu device means all physical CPUs and memory.\nThis means that PyTorch\u02bcs calculations will try to use all CPU cores. However, a gpu device only\nrepresents one card and the corresponding memory. If there are multiple GPUs, we use torch.\ndevice(f'cuda:{i}') to represent the i\nth GPU (i starts from 0). Also, gpu:0 and gpu are equivalent"}}