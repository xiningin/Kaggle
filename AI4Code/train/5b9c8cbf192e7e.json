{"cell_type":{"b4c3b2b5":"code","8a96724f":"code","778b8a6c":"code","1a0e2ade":"code","d2682761":"code","145a65a7":"code","7911365d":"code","30743b41":"code","fea2b934":"code","b30983f5":"code","0164a2f2":"code","73e53ad2":"code","90691c50":"code","914f7648":"code","094d257c":"code","b4472d79":"code","aa7768ca":"code","8e0b9150":"code","c98cdcb2":"code","c38c5268":"code","5da87c62":"code","dbba4851":"code","e11e2d43":"code","9984f0e1":"code","30399188":"code","53559a87":"code","3d75ca09":"code","9c8a1b82":"code","ffb5872c":"code","dc2d8cea":"code","417d5090":"code","587602ec":"code","6be5d3f7":"code","2c439fd9":"code","c3ef112c":"code","02f29d61":"markdown","ae492105":"markdown","caba05f8":"markdown","b923b0a9":"markdown","5c768f3d":"markdown","d9c60603":"markdown","07152871":"markdown","d0a39666":"markdown","567f3e92":"markdown","f93ba1fc":"markdown","d3f424b8":"markdown","d50ba7d2":"markdown","e7d26f28":"markdown","0d317c86":"markdown","db7fc12b":"markdown","f53ddb7c":"markdown","c360ac5a":"markdown","b07f98c4":"markdown","9c789d8a":"markdown","f2349207":"markdown","2a675f53":"markdown","2b192d0f":"markdown","52652b6e":"markdown","590c5dc6":"markdown","844dfba0":"markdown","05b0bd12":"markdown","b91aa0f3":"markdown"},"source":{"b4c3b2b5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\ntry:\n    import theano\nexcept:\n    !pip install Theano\nimport theano\nimport keras\nimport tensorflow\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8a96724f":"# Importing the dataset\nchurn_data = pd.read_csv('..\/input\/Churn_Modelling.csv',index_col='RowNumber')","778b8a6c":"churn_data.info()","1a0e2ade":"churn_data.describe()","d2682761":"churn_data.head()","145a65a7":"# some columns are totally unproductive so let's remove them\nchurn_data.drop(['CustomerId','Surname'],axis=1,inplace=True)","7911365d":"churn_data.head()","30743b41":"# some columns have text data so let's one hot encode them\n#  for more on one hot encoding click this link below\n# https:\/\/www.kaggle.com\/shrutimechlearn\/types-of-regression-and-stats-in-depth\nGeography_dummies = pd.get_dummies(prefix='Geo',data=churn_data,columns=['Geography'])","fea2b934":"Geography_dummies.head()","b30983f5":"Gender_dummies = Geography_dummies.replace(to_replace={'Gender': {'Female': 1,'Male':0}})","0164a2f2":"Gender_dummies.head()","73e53ad2":"churn_data_encoded = Gender_dummies","90691c50":"sns.countplot(y=churn_data_encoded.Exited ,data=churn_data_encoded)\nplt.xlabel(\"Count of each Target class\")\nplt.ylabel(\"Target classes\")\nplt.show()","914f7648":"churn_data_encoded.hist(figsize=(15,12),bins = 15)\nplt.title(\"Features Distribution\")\nplt.show()","094d257c":"plt.figure(figsize=(15,15))\np=sns.heatmap(churn_data_encoded.corr(), annot=True,cmap='RdYlGn',center=0) ","b4472d79":"fig,ax = plt.subplots(nrows = 4, ncols=3, figsize=(30,30))\nrow = 0\ncol = 0\nfor i in range(len(churn_data_encoded.columns) -1):\n    if col > 2:\n        row += 1\n        col = 0\n    axes = ax[row,col]\n    sns.boxplot(x = churn_data_encoded['Exited'], y = churn_data_encoded[churn_data_encoded.columns[i]],ax = axes)\n    col += 1\nplt.tight_layout()\n# plt.title(\"Individual Features by Class\")\nplt.show()","aa7768ca":"X = churn_data_encoded.drop(['Exited'],axis=1)\ny = churn_data_encoded.Exited","8e0b9150":"X.head(10)","c98cdcb2":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)\n","c38c5268":"# Feature Scaling because yes we don't want one independent variable dominating the other and it makes computations easy\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","5da87c62":"# sequential model to initialise our ann and dense module to build the layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense","dbba4851":"classifier = Sequential()\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN | means applying SGD on the whole ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100,verbose = 0)\n\nscore, acc = classifier.evaluate(X_train, y_train,\n                            batch_size=10)\nprint('Train score:', score)\nprint('Train accuracy:', acc)\n# Part 3 - Making predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\nprint('*'*20)\nscore, acc = classifier.evaluate(X_test, y_test,\n                            batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","e11e2d43":"p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","9984f0e1":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","30399188":"from sklearn.metrics import roc_curve\ny_pred_proba = classifier.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC curve')\nplt.show()","53559a87":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","3d75ca09":"# Part 4 - Evaluating, Improving and Tuning the ANN\n\n# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\ndef build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100,verbose=0)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nmean = accuracies.mean()\nvariance = accuracies.std()","9c8a1b82":"print('Mean accuracy score of 10 different models using Kfold cross validation: {}'.format(mean))\nprint('Standard Deviation of accuracy score of 10 different models using Kfold cross validation: {}'.format(variance))","ffb5872c":"# Improving the ANN\nfrom keras.layers import Dropout\nclassifier = Sequential()\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\nclassifier.add(Dropout(rate = 0.1))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(rate = 0.1))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100,verbose = 0)\n\n# Part 3 - Making predictions and evaluating the model\n\nscore, acc = classifier.evaluate(X_train, y_train,\n                            batch_size=10)\nprint('Train score:', score)\nprint('Train accuracy:', acc)\n# Part 3 - Making predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\nprint('*'*20)\nscore, acc = classifier.evaluate(X_test, y_test,\n                            batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","dc2d8cea":"p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","417d5090":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","587602ec":"from sklearn.metrics import roc_curve\ny_pred_proba = classifier.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC curve')\nplt.show()","6be5d3f7":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","2c439fd9":"\n# Tuning the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\ndef build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier)\nparameters = {'batch_size': [25, 32],\n              'epochs': [100, 200],\n              'optimizer': ['adam', 'rmsprop']}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\ngrid_search = grid_search.fit(X_train, y_train,verbose = 0)\nbest_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_","c3ef112c":"print('Best Parameters after tuning: {}'.format(best_parameters))\nprint('Best Accuracy after tuning: {}'.format(best_accuracy))","02f29d61":"<a id=\"8\"><\/a>\n# 8. How are NNs different from classical models?\n#### To better understand artificial neural computing it is important to know first how a conventional 'serial' computer and it's software process information. A serial computer has a central processor that can address an array of memory locations where data and instructions are stored. Computations are made by the processor reading an instruction as well as any data the instruction requires from memory addresses, the instruction is then executed and the results are saved in a specified memory location as required. In a serial system (and a standard parallel one as well) the computational steps are deterministic, sequential and logical, and the state of a given variable can be tracked from one operation to another.\n\n#### In comparison, ANNs are not sequential or necessarily deterministic. There are no complex central processors, rather there are many simple ones which generally do nothing more than take the weighted sum of their inputs from other processors. ANNs do not execute programed instructions; they respond in parallel (either simulated or actual) to the pattern of inputs presented to it. There are also no separate memory addresses for storing data. Instead, information is contained in the overall activation 'state' of the network. 'Knowledge' is thus represented by the network itself, which is quite literally more than the sum of its individual components.","ae492105":"<a id=\"7\"><\/a>\n# 7. What happens without activation function?\n#### If we do not apply a Activation function then the output signal would simply be a simple linear function.A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A Neural Network without Activation function would simply be a Linear regression Model, which has limited power and does not performs good most of the times. We want our Neural Network to not just learn and compute a linear function but something more complicated than that. Also without activation function our Neural network would not be able to learn and model other complicated kinds of data such as images, videos , audio , speech etc. That is why we use Artificial Neural network techniques such as Deep learning to make sense of something complicated ,high dimensional,non-linear -big datasets, where the model has lots and lots of hidden layers in between and has a very complicated architecture which helps us to make sense and extract knowledge form such complicated big datasets.","caba05f8":"![](http:\/\/www.analyticsvidhya.com\/blog\/wp-content\/uploads\/2014\/10\/flowchart-ANN.png)","b923b0a9":"#### For a basic idea of how a deep learning neural network learns, imagine a factory line. After the raw materials (the data set) are input, they are then passed down the conveyer belt, with each subsequent stop or layer extracting a different set of high-level features. If the network is intended to recognize an object, the first layer might analyze the brightness of its pixels. The next layer could then identify any edges in the image, based on lines of similar pixels. After this, another layer may recognize textures and shapes, and so on. By the time the fourth or fifth layer is reached, the deep learning net will have created complex feature detectors. It can figure out that certain image elements (such as a pair of eyes, a nose, and a mouth) are commonly found together.\n\n#### Once this is done, the researchers who have trained the network can give labels to the output, and then use backpropagation to correct any mistakes which have been made. After a while, the network can carry out its own classification tasks without needing humans to help every time.","5c768f3d":"![](https:\/\/i.stack.imgur.com\/LgmYv.png)","d9c60603":"<a id=\"4\"><\/a>\n# 4. In what situation does the algorithm fit best?\n\n#### ANN is rarely used for predictive modelling. The reason being that Artificial Neural Networks (ANN) usually tries to over-fit the relationship. ANN is generally used in cases where what has happened in past is repeated almost exactly in same way. For example, say we are playing the game of Black Jack against a computer. An intelligent opponent based on ANN would be a very good opponent in this case (assuming they can manage to keep the computation time low). With time ANN will train itself for all possible cases of card flow. And given that we are not shuffling cards with a dealer, ANN will be able to memorize every single call. Hence, it is a kind of machine learning technique which has enormous memory. But it does not work well in case where scoring population is significantly different compared to training sample. For instance, if I plan to target customer for a campaign using their past response by an ANN. I will probably be using a wrong technique as it might have over-fitted the relationship between the response and other predictors.","07152871":"<a id=\"5\"><\/a>\n# 5. How does ANN work?\n#### It is truly said that the working of ANN takes its roots from the neural network residing in human brain. ANN operates on something referred to as Hidden State. These hidden states are similar to neurons. Each of these hidden state is a transient form which has a probabilistic behavior. A grid of such hidden state act as a bridge between the input and the output. \n#### We have an input layer which is the data we provide to the ANN. We have the hidden layers, which is where the magic happens. Lastly, we have the output layer, which is where the finished computations of the network are placed for us to use.\n\n\n\n","d0a39666":"<a id=\"2\"><\/a>\n# 2. Types of Neural Network\n\n#### There are multiple types of neural network, each of which come with their own specific use cases and levels of complexity. The most basic type of neural net is something called a feedforward neural network, in which information travels in only one direction from input to output. A more widely used type of network is the recurrent neural network, in which data can flow in multiple directions. These neural networks possess greater learning abilities and are widely employed for more complex tasks such as learning handwriting or language recognition.\n\n#### There are also convolutional neural networks, Boltzmann machine networks, Hopfield networks, and a variety of others. Picking the right network for your task depends on the data you have to train it with, and the specific application you have in mind. In some cases, it may be desirable to use multiple approaches, such as would be the case with a challenging task like voice recognition.","567f3e92":"![](https:\/\/icdn6.digitaltrends.com\/image\/artificial_neural_network_1-720x720.jpg)","f93ba1fc":"![](http:\/\/cdn-images-1.medium.com\/max\/600\/1*f0hA2R652htmc1EaDrgG8g.png)","d3f424b8":"<a id='9'><\/a>\n# 9. Implementation","d50ba7d2":"<a id='14'><\/a>\n# 14. Tuning the ANN","e7d26f28":"Big Thanks to\n* https:\/\/www.techopedia.com\/definition\/5967\/artificial-neural-network-ann\n* https:\/\/www.digitaltrends.com\/cool-tech\/what-is-an-artificial-neural-network\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2014\/10\/ann-work-simplified\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2014\/10\/introduction-neural-network-simplified\/\n* https:\/\/medium.com\/technology-invention-and-more\/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1\n* http:\/\/pages.cs.wisc.edu\/~bolo\/shipyard\/neural\/local.html\n* https:\/\/towardsdatascience.com\/activation-functions-and-its-types-which-is-better-a9a5310cc8f","0d317c86":"<a id='10'><\/a>\n# 10. Business Problem and EDA\n\n#### Our Business problem which I have chosen for this tutorial is a classification problem wherein we have a dataset in which there are details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.","db7fc12b":"<a id='12'><\/a>\n# 12. Evaluation of Multiple Training Instances","f53ddb7c":"<a id=\"3\"><\/a>\n# 3. Is there a difference between NN and ANN?\n#### Neural Network is a broad term that encompases various types of networks which were shown above. Is ANN one of the types? Well to understand this it is important to realise that neural network alone is not an algorithm but a framework which assists the algorithms to work. ANN is the most basic type of implementation of neurals. ANN was the term coined much earlier and nowadays the two terms are interchangeably used.\n","c360ac5a":"## Introduction\n\n* [What are ANNs?](#1)\n* [Types of NNs?](#2)\n* [Is there a difference between NN and ANN?](#3)\n* [In what situation does the algorithm fit best?](#4)\n* [How does ANN work?](#5)\n* [Activation Function](#6)\n* [What happens without activation function?](#7)\n* [How are NNs different from classical models?](#8)\n* [Implementation](#9)\n* [Business Problem and EDA](#10)\n* [Evaluation Metrics](#11)\n* [Evaluation of Multiple Training Instances](#12)\n* [Improving the ANN with dropout layer](#13)\n* [Tuning the ANN](#14)","b07f98c4":"#### Deviation is very low so I'd say that it is unlikely to be an overfitted model. With different training sets it got the mean with all training results is still very close to the above model.","9c789d8a":"#### Let's test 2 values of the batch size and we're gonna try 25 and 32. So why 25 and 32? Well, that's based on my experience and that's also common practice to take powers of 2. Well, you can try other values of the batch size as well.","f2349207":"<a id=\"6\"><\/a>\n# 6. Activation Function\n#### Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.\n\n#### Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.\n\n","2a675f53":"#### Initially the weights of the network can be randomly. When the input in given to the input layer the process moves forward and the hidden layer receives the input combined with the weights. This process goes on till the final layer of output is reached and result is given. When the result is out it is compared to the actual value and a back propagation algorithm comes into play to adjust the weights of the network linkages to better the result. What do the neurons in the layers then do? They are responsible for the learning individually. They consist of activation function that allows the signal to pass or not depending on which activation function is being used and what input came from the previous layer. We'll see activation functions in detail now.","2b192d0f":"<a id='13'><\/a>\n# 13. Improving ANN with Dropout layer","52652b6e":"#### Most popular types of Activation functions -\n* Sigmoid or Logistic\n* Tanh\u200a\u2014\u200aHyperbolic tangent\n* ReLu -Rectified linear units\n\n**Sigmoid Activation function**: It is a activation function of form f(x) = 1 \/ 1 + exp(-x) . Its Range is between 0 and 1. It is a S\u200a\u2014\u200ashaped curve. It is easy to understand and apply but it has major reasons which have made it fall out of popularity -\n\n* Vanishing gradient problem\n* Secondly , its output isn\u2019t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n* Sigmoids saturate and kill gradients.\n* Sigmoids have slow convergence.\n\n\n![](http:\/\/cdn-images-1.medium.com\/max\/1600\/0*WYB0K0zk1MiIB6xp.png)\n\n\n\n**Hyperbolic Tangent function- Tanh** : It\u2019s mathamatical formula is f(x) = 1\u200a\u2014\u200aexp(-2x) \/ 1 + exp(-2x). Now it\u2019s output is zero centered because its range in between -1 to 1 i.e -1 < output < 1 . Hence optimization is easier in this method hence in practice it is always preferred over Sigmoid function . But still it suffers from Vanishing gradient problem.\n\n\n![](http:\/\/cdn-images-1.medium.com\/max\/1600\/0*VHhGS4NwibecRjIa.png)\n\n**ReLu- Rectified Linear units** : It has become very popular in the past couple of years. It was recently proved that it had 6 times improvement in convergence from Tanh function. It\u2019s just R(x) = max(0,x) i.e if x < 0 , R(x) = 0 and if x >= 0 , R(x) = x. Hence as seeing the mathamatical form of this function we can see that it is very simple and efficinent . A lot of times in Machine learning and computer science we notice that most simple and consistent techniques and methods are only preferred and are best. Hence it avoids and rectifies vanishing gradient problem . Almost all deep learning Models use ReLu nowadays.\n\nBut its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n\nHence for output layers we should use a Softmax function for a Classification problem to compute the probabilites for the classes , and for a regression problem it should simply use a linear function.\n\nAnother problem with ReLu is that some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.\n\nTo fix this problem another modification was introduced called Leaky ReLu to fix the problem of dying neurons. It introduces a small slope to keep the updates alive.\n\nWe then have another variant made form both ReLu and Leaky ReLu called Maxout function .\n\n![](http:\/\/cdn-images-1.medium.com\/max\/1600\/0*qtfLu9rmtNullrVC.png)","590c5dc6":"#### A brief about the NN libraries:\n#### Theano is an open source numerical computation library based on numpy syntax. It can run not only on the CPU ( Central Processing Unit) but also the GPU (Graphical Processing Unit). GPU is a processor for graphic purposes somewhat similar to a graphic card. GPU is much more powerful in terms of efficiency etc. because it has more cores and is able to run more floating points calculations per second than the CPU. GPU is highly specialised for heavy, parallel computations which is a requirement in Neural Networks that we are about to see.\n#### How parallel computation comes into play in NNs? When we are forward propagating the different activations of neurons for the activation function or when we back propagate the error. Also calculations can be carried out faster this way. Theano was developed at the University of Montreal.\n\n#### Tensorflow is similar to Theano. It's been developed by Google.\n\n#### However the point to consider is that these two libraries are more towards the research and development side of Neural Networks. If we were to create a model from scratch and make some improvements in it, experiment or something these two would be great but right now we would be using Keras for beginning till we step up. Keras in some way wraps the two libraries for us and provides small and easy to implement modules of code.","844dfba0":"<a id='11'><\/a>\n# 11. Evaluation Metrics","05b0bd12":"#### Dropout Regularization is used to reduce overfitting if needed.\n\n#### p is the fraction of input units to drop. If suppose there are ten neurons from a layer and p is 0.1 then one of the neurons would be disabled and its output would not be sent to the further layer.\n#### It is advisable to start with p 0.1 and move to higher values when in case the overfitting problem persists. Also going over 0.5 is not advisable generally because it may cause underfitting as most of the neurons are disabled.\n","b91aa0f3":"<a id=\"1\"><\/a>\n# 1. What are ANNs?\n\n#### Artificial neural networks are one of the main tools used in machine learning. As the \u201cneural\u201d part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. ANNs have three layers that are interconnected. The first layer consists of input neurons. Those neurons send data on to the second layer, which in turn sends the output neurons to the third layer. ANNs are considered non-linear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. Note that a neuron can also be referred to as a perceptron.\n\nNote: The way layers would be created, arranged, assigned, number of neurons the layers would hold and other such questions come under architecture designing for neural networks. It is a common thing while starting out to have these queries. Architecture of NNs is an ocean frankly, one can gain knowledge of it only by exploring it, working on different problems optimising for better solutions by trial and error. There are some thumb rules that are known but since the complexity of the mechanism is such there are a very few detailed proofs of why some things work for some problems and some don't."}}