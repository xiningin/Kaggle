{"cell_type":{"edb86759":"code","5b40a6f9":"code","d2129684":"code","2428fb98":"code","a57eeb27":"code","2c8faf89":"code","4d1d85f1":"markdown","d18c24d5":"markdown","29e60dbc":"markdown","542afd20":"markdown","7faff750":"markdown","8461a214":"markdown"},"source":{"edb86759":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport joblib\nimport pickle\nimport lightgbm as lgb\nimport gc\nimport os\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom tqdm import tqdm\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom datetime import datetime, timedelta\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(seed=42)\n","5b40a6f9":"def additional_features(df):\n\n    df['month'] = df['date'].dt.month\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['hour'] = df['date'].dt.hour\n    df['day'] = df['date'].dt.day\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['week'] = df['date'].dt.week\n    df['year'] = df['date'].dt.year\n\n    df['month_dummy'] = 0\n    df.loc[df['month'] > 10, 'month_dummy'] = 1\n    df.loc[df['month'] < 3, 'month_dummy'] = 1\n\n    df['hour_dummy'] = 0\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 19), 'hour_dummy'] = 1\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 5) & (df['hour'] < 8), 'hour_dummy'] = 1\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 10) & (df['hour'] < 14), 'hour_dummy'] = 1\n\n    df['year_dummy'] = 0\n    df.loc[(df['date'] > '2017-10-01') & (df['month_dummy'] == 1), 'year_dummy'] = 1\n\n    return df\n\n\ndef get_data(ROOT='data'):\n\n    CAL_DTYPES = {\"latitude\": \"float64\", \"longitude\": \"float64\", \"type\": \"category\",\n                  \"source\": \"category\", \"station\": \"category\", 'aqi': 'float64'}\n\n    train = pd.read_csv(ROOT + \"\/pm_train.csv\", dtype=CAL_DTYPES)\n    test = pd.read_csv(ROOT + \"\/pm_test.csv\", dtype=CAL_DTYPES)\n    sub = pd.read_csv(ROOT + \"\/sample_submission.csv\")\n\n    return train, test, sub\n\ndef prepare_weather_data(ROOT='data', cat_feat=[], rolling_means=[], shifts=[],\n               diffs=[]):\n\n    cont_vars = ['precipIntensity', 'precipProbability', 'temperature', 'apparentTemperature',\n                 'dewPoint', 'humidity', 'windSpeed',\n                 'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n\n    cat_feat = ['summary', 'icon', 'month', 'dayofweek',\n                'hour', 'day', 'dayofyear', 'week', 'year',\n                'month_dummy', 'hour_dummy', 'year_dummy']\n\n    weather = pd.read_csv(ROOT + \"\/weather.csv\")\n\n    weather['date'] = pd.to_datetime(weather['date'])\n    weather = weather.sort_values(by='date')\n\n    weather_df = pd.DataFrame()\n    weather_df['date'] = pd.date_range(start=weather.iloc[0]['date'], end=weather.iloc[-1]['date'], freq='H')\n    weather_df = pd.merge(weather_df, weather.iloc[:, 1:], on=['date'], how='left')\n\n    weather_df.loc[weather_df['precipIntensity'] > 3, 'precipIntensity'] = np.nan\n    weather_df.loc[weather_df['precipIntensity']==0, 'precipIntensity']=np.nan\n    weather_df.loc[weather_df['precipProbability'] == 0, 'precipProbability'] = np.nan\n\n    del weather\n\n    weather_df = additional_features(weather_df)\n\n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        weather_df.loc[weather_df[col].dropna().index, col] = \\\n            le.fit_transform(weather_df.loc[weather_df[col].dropna().index, col])\n\n    for col in cont_vars:\n        if weather_df[col].isnull().sum(axis=0) > 0:\n            weather_df[col] = weather_df.groupby(['dayofyear', 'hour'])[col].transform(lambda x: x.fillna(x.mean()))\n            weather_df[col] = weather_df.groupby(['week', 'hour'])[col].transform(lambda x: x.fillna(x.mean()))\n            weather_df[col] = weather_df[col].fillna(weather_df[col].rolling(4, min_periods=1).mean())\n\n    for col in cat_feat:\n        if weather_df[col].isnull().sum(axis=0)>0:\n            weather_df[col] = weather_df.groupby(['week', 'hour'])[col].transform(lambda x: x.fillna(x.mode()[0]))\n\n    for col in cont_vars+[\"dayofyear\", \"hour\"]:\n        for lag in rolling_means:\n            weather_df['%s_rolling_%s_mean' % (col, lag)] = weather_df[col].rolling(lag).mean()\n\n    #print(weather_df.drop(['date']+cat_feat, axis=1).columns)\n    for col in (weather_df.drop(['date']+cat_feat, axis=1).columns):\n        for lag in shifts:\n            weather_df['%s_shift_%s' % (col, lag)] = weather_df[col].shift(lag)\n\n    return weather_df\n\n\ndef preprocess(weather_df, ROOT='data', center = [106.91787, 47.91667]):\n\n\n    cont_vars = ['temperature', 'apparentTemperature',\n                 'dewPoint', 'humidity', 'windSpeed',\n                 'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n\n    cat_feat = [\"type\", \"source\", \"station\"]\n\n    train, test, sub = get_data(ROOT=ROOT)\n\n    df = pd.concat([train, test], axis=0)\n    df['distance'] = np.sqrt((np.asarray(df['latitude']) - center[0]) ** 2 +\n                             (np.asarray(df['longitude']) - center[1]) ** 2)\n\n    cos = (np.asarray(df['latitude']) * center[0] + np.asarray(df['longitude']) * center[1]) \/ \\\n          (np.sqrt(np.asarray(df['latitude']) ** 2 + np.asarray(df['longitude']) ** 2) * np.sqrt(\n              center[0] ** 2 + center[1] ** 2))\n\n    df['angle'] = 1000 * np.arccos(cos)\n\n    df['date'] = pd.to_datetime(df['date'])\n\n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n\n    df = pd.merge(df, weather_df, on=[\"date\"], how='left')\n\n#     icols = [['type', 'station', 'month', 'hour'],\n#             ['type', 'station', 'week', 'hour'],\n#             ['type', 'station', 'dayofyear', 'hour'],\n#              ['type', 'station', 'icon', 'month', 'hour'],\n#              ['type', 'station', 'summary', 'month', 'hour']]\n\n#     temp_df = df.copy()\n#     temp_df = temp_df.loc[temp_df['date']<\"2018-10-31\"].dropna()\n#     col_fill = '_' + '_'.join(icols[0]) + '_'\n#     for col in icols:\n#         col_name = '_' + '_'.join(col) + '_'\n#         temp = temp_df[col + ['aqi']].groupby(col, as_index=False)['aqi'].mean()\n#         temp = temp.rename(columns={'aqi': 'enc%smean' % col_name})\n#         df = df.merge(temp, on=col, copy=False, how='left')\n#         df['enc%smean' % col_name] = df['enc%smean' % col_name]\n#         df['enc%smean' % col_name] = df['enc%smean' % col_name].fillna(df['enc%smean' % col_fill])\n\n    return df, sub\n","d2129684":"def normalization(df):\n    if len(df.shape)==2:\n        scaler = MinMaxScaler().fit(df.values)\n        X = scaler.transform(df.values)\n    else:\n        scaler = MinMaxScaler().fit(np.expand_dims(df.values, axis=1))\n        X = scaler.transform(np.expand_dims(df.values, axis=1))\n\n    return X, scaler\n\n\ndef make_2Dinput(dt, cont_cols, cat_cols):\n    input = {\"rnn\": dt[cont_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        input[v] = dt[[v]].to_numpy()\n    return input\n\nclass MAPLoader:\n\n    def __init__(self, X, y, shuffle=True, batch_size=1000, cat_cols=[]):\n        self.X_cont = X[\"rnn\"]\n        self.X_cat = np.concatenate([X[k] for k in cat_cols], axis=1)\n        self.y = y\n\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.n_conts = self.X_cont.shape[1]\n        self.len = self.X_cont.shape[0]\n        n_batches, remainder = divmod(self.len, self.batch_size)\n\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n        self.remainder = remainder  # for debugging\n\n        self.idxes = np.array([i for i in range(self.len)])\n\n    def __iter__(self):\n        self.i = 0\n        if self.shuffle:\n            ridxes = self.idxes\n            np.random.shuffle(ridxes)\n            self.X_cat = self.X_cat[[ridxes]]\n            self.X_cont = self.X_cont[[ridxes]]\n            if self.y is not None:\n                self.y = self.y[[ridxes]]\n\n        return self\n\n    def __next__(self):\n        if self.i >= self.len:\n            raise StopIteration\n\n        if self.y is not None:\n            y = torch.FloatTensor(self.y[self.i:self.i + self.batch_size].astype(np.float32))\n\n        else:\n            y = None\n\n        xcont = torch.FloatTensor(self.X_cont[self.i:self.i + self.batch_size])\n        xcat = torch.LongTensor(self.X_cat[self.i:self.i + self.batch_size])\n\n        batch = (xcont, xcat, y)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches\n\n\nclass RMSE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n\n    def forward(self, y_pred, y_true):\n        y_pred = y_pred.squeeze()\n        return torch.sqrt(self.mse(y_pred, y_true))","2428fb98":"def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.01)\n\n####### Simple MLP model for 2D input ############################\n\nclass MLP_MAPP(nn.Module):\n    def __init__(self, emb_dims, n_cont, hidden_dim, device=device):\n        super().__init__()\n        self.device = device\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        n_embs = sum([y for x, y in emb_dims])\n\n        n_embs = n_embs\n        n_cont = n_cont\n        inp_dim = n_cont + n_embs\n\n        self.fc0 = nn.Linear(inp_dim, hidden_dim)\n        self.drop0 = nn.Dropout(0.4)\n        self.fc1 = nn.Linear(hidden_dim, int(hidden_dim))\n        self.drop1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(int(hidden_dim), int(hidden_dim\/2))\n        self.fc3 = nn.Linear(int(hidden_dim\/2), 1)\n\n        self.fc0.apply(init_weights)\n        self.fc1.apply(init_weights)\n        self.fc2.apply(init_weights)\n\n    def encode_and_combine_data(self, cont_data, cat_data):\n        xcat = [el(cat_data[:, k]) for k, el in enumerate(self.emb_layers)]\n        xcat = torch.cat(xcat, 1)\n        x = torch.cat([xcat, cont_data], 1)\n        return x\n\n    def forward(self, cont_data, cat_data):\n\n        cont_data = cont_data.to(self.device)\n        cat_data = cat_data.to(self.device)\n        x = self.encode_and_combine_data(cont_data, cat_data)\n\n        hz = F.relu(self.fc0(x))\n        hz = self.drop0(hz)\n        hz = F.relu(self.fc1(hz))\n        hz = self.drop1(hz)\n        hz = F.relu(self.fc2(hz))\n        out = F.relu(self.fc3(hz))\n\n        return out\n","a57eeb27":"##################### Neural Network ####################################\ndef training_nn(df, sub_nn, target, cont_cols, cat_cols, MLP_model,\n                Nfolds, dt_start, dt_end, epoch=50, patience=5, MODEL_ROOT='models\/nn',\n                hidden_dim=1024):\n\n    #Drop Nan values because pytorch can't manage nan values\n    df = df.loc[df[cont_cols].dropna().index]\n\n    ## For embedding layers, categorical feautures must be integer and start from zero\n    uniques = {}\n    for i, v in enumerate((cat_cols)):\n        le = LabelEncoder()\n        le.fit(df[v])\n        df[v] = le.transform(df[v])\n        uniques[v] = (len(df[v].unique()))\n\n    if not os.path.exists(MODEL_ROOT):\n        os.makedirs(MODEL_ROOT)\n\n    ##### MinMax normalization for faster convergence\n    df[cont_cols], scaler_x = normalization(df[cont_cols])\n    y_norm, scaler_y = normalization(df[target].dropna())\n    df.loc[df[target].dropna().index, target] = y_norm\n\n    df = df.reset_index(drop=True)\n\n    test = df.loc[df['date'] > '2018-11-01']\n\n    ## Checking dimension of test set. It must be equal to submission dimension\n    print(test.shape)\n\n    fold_ = 0\n    pred_val = []\n    true_y = []\n\n    for dt0, dt1 in zip(dt_start, dt_end):\n        val_index = df.loc[(df['date'] > dt0) & (df['date'] < dt1), target].dropna().index\n        train_index = df.loc[(df['date'] < dt0) | (df['date'] > dt1), target].dropna().index\n\n        model_path = MODEL_ROOT + '\/model_nn_%s_%s.pt' % (hidden_dim, fold_)\n\n        train_set = df.loc[train_index].sample(frac=0.8)\n        val_set = df.loc[val_index]\n\n        #Make input for pytorch loader because we have categorical and continues features\n        X_train, y_train = make_2Dinput(train_set[cont_cols+cat_cols], cont_cols=cont_cols, cat_cols=cat_cols), train_set[target]\n        validx, validy = make_2Dinput(val_set[cont_cols+cat_cols], cont_cols=cont_cols, cat_cols=cat_cols), val_set[target]\n\n        #Make loader for pytorch\n        train_loader = MAPLoader(X_train, y_train.values, cat_cols=cat_cols, batch_size=1024, shuffle=True)\n        val_loader = MAPLoader(validx, validy.values, cat_cols=cat_cols, batch_size=256, shuffle=False)\n\n        if os.path.isfile(model_path):\n            model_final = torch.load(model_path)\n        else:\n\n            ## make embedding dimensions\n            # cat_feats = ['summary', 'icon', 'month', 'dayofweek', 'hour', 'day',\n            #              'dayofyear', 'week', 'year', 'type', 'source', 'station']\n            dims = [3, 3, 2, 2, 3, 4, 15, 5, 1, 1, 1, 3]\n            emb_dims = [(uniques[col], y) for col, y in zip(cat_cols, dims)]\n\n\n            n_cont = train_loader.n_conts\n\n            model = MLP_model(emb_dims=emb_dims, n_cont=n_cont, hidden_dim=hidden_dim).to(device)\n            criterion = RMSE()\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n            scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                                      max_lr=1e-2, epochs=epoch, steps_per_epoch=len(train_loader))\n\n\n            best_rmse=np.inf\n            counter=0\n            for ep in range(epoch):\n                train_loss, val_loss = 0, 0\n\n                model.train()\n                for i, (X_cont, X_cat, y) in enumerate(train_loader):\n\n                    optimizer.zero_grad()\n                    out= model(X_cont, X_cat)\n\n                    loss = criterion(out, y.to(device))\n                    loss.backward()\n                    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                    optimizer.step()\n                    scheduler.step()\n\n                    with torch.no_grad():\n                        train_loss += loss.item() \/ len(train_loader)\n\n                # Validation phase\n                phase='Valid'\n                with torch.no_grad():\n                    model.eval()\n                    y_true = []\n                    y_pred = []\n                    rloss = 0\n                    for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                        out = model(X_cont, X_cat)\n                        loss = criterion(out, y.to(device))\n                        rloss += loss.item() \/ len(val_loader)\n                        y_pred += list(out.detach().cpu().numpy().flatten())\n                        y_true += list(y.cpu().numpy())\n\n                    rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                      scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n\n                    print(f\"[{phase}] Epoch: {ep} | Tain loss: {train_loss:.4f} | Val Loss: {rloss:.4f} | RMSE: {rmse:.4f} \")\n\n                    if best_rmse > rmse:\n                        best_rmse = rmse\n                        best_model = model\n                        torch.save(best_model, model_path)\n                        counter = 0\n                    else:\n                        counter = counter + 1\n\n                    # plt.plot(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)))\n                    # plt.plot(scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1)))\n                    # plt.show()\n\n                if counter>=patience:\n                    print(\"Early stopping\")\n                    break\n\n        fold_ = fold_ + 1\n\n        model_final=torch.load(model_path)\n\n        with torch.no_grad():\n            model_final.eval()\n            y_true = []\n            y_pred = []\n            for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                out = model_final(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            y_true = scaler_y.inverse_transform(np.expand_dims(y_true, axis=1))\n            y_pred = scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))\n\n            true_y +=list(y_true[:, 0])\n            pred_val += list(y_pred[:, 0])\n\n            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n\n            print(\"*\" * 50, 'Fold %s' % fold_, \"*\" * 50)\n            print(\"CV score:\", rmse)\n            print(\"*\" * 50, 'Fold %s' % fold_, \"*\" * 50)\n\n        testx = make_2Dinput(test[cont_cols+cat_cols], cont_cols=cont_cols, cat_cols=cat_cols)\n        test_loader = MAPLoader(testx, None, cat_cols=cat_cols, batch_size=1024, shuffle=False)\n        y_pred = []\n        #sub_nn['%s_%s' %(target, fold_)]=0\n        with torch.no_grad():\n            model_final.eval()\n            for i, (X_cont, X_cat, y) in enumerate(test_loader):\n                out = model_final(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n\n        sub_nn.loc[test['ID'], target] =sub_nn.loc[test['ID'], target].values+ scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))[:, 0]\/Nfolds\n\n    return sub_nn, pred_val, true_y\n","2c8faf89":"#### Hyperparameters ############\ncat_feats = ['summary', 'icon', 'month', 'dayofweek', 'hour', 'day',\n             'dayofyear', 'week', 'year', 'type', 'source', 'station']\n\nuseless_columns = ['ID', 'date', 'latitude', 'longitude']\ntarget = 'aqi'\n\nweather = prepare_weather_data(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', \n                               cat_feat = cat_feats, rolling_means=[4, 8],\n                                            shifts=[1, 3], diffs=[1, 2])\n\ndf, sub = preprocess(weather, ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction')\n\ndt_start = ['2018-11-01', '2018-11-01', '2018-11-01', '2018-11-01', '2018-11-01']\ndt_end = ['2020-11-01', '2020-11-01', '2020-11-01', '2020-11-01', '2020-11-01']\n\nsub_nn = sub.set_index('ID', drop=True)\nsub_nn['aqi'] = 0\nfor hidden_layer in [256]:\n    cat_feats = ['summary', 'icon', 'month', 'dayofweek', 'hour', 'day',\n                 'dayofyear', 'week', 'year', 'type', 'source', 'station']\n\n    one_hot_encode = ['summary', 'icon', 'year', 'type', 'station']\n\n    for col in one_hot_encode:\n        for e in df[col].unique():\n            df['%s_%s' %(col, e)] = np.where(df[col].values==e, 1, 0)\n\n    cont_cols = list(df.drop(useless_columns+cat_feats+['aqi'], axis=1).columns)\n    print(np.asarray(cont_cols))\n\n    print(\"*\" * 25, \"hidden_layer\", hidden_layer, \"*\" * 25, )\n    sub_nn= sub.set_index('ID', drop=True)\n    sub_nn, val_pred, true_y = training_nn(df=df, sub_nn=sub_nn, target=target, MLP_model=MLP_MAPP,\n                                                cont_cols=cont_cols, cat_cols=cat_feats, Nfolds=5,\n                                                dt_start=dt_start, dt_end=dt_end,\n                                                epoch=50, patience=8, MODEL_ROOT='models\/nn',\n                                                hidden_dim=hidden_layer)\n\n    print('Neural Network CV:', np.sqrt(mean_squared_error(np.asarray(true_y), np.asarray(val_pred))))\n    sub_nn.to_csv(\"sub_nn_%s.csv\" %hidden_layer)\n","4d1d85f1":"# Import libraries","d18c24d5":"# Utils for Neural Network Model","29e60dbc":"# Data import and preprocess","542afd20":"# Model Training","7faff750":"# Neural Network Model","8461a214":"# Training phase"}}