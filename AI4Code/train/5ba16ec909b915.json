{"cell_type":{"8f04bce3":"code","a832d371":"code","6f365660":"code","2fbfe109":"code","a36f4fb6":"code","c93be9db":"code","7965297e":"code","3a6884d7":"code","3e7e34b0":"code","0f135bf0":"code","e5f25c9e":"code","87f1a39a":"code","599aa007":"code","6d350ad5":"code","ac2ca0f9":"code","7f983a69":"code","6216ee0a":"code","54dea933":"code","b5782a22":"code","e1ce52e2":"code","20a201ba":"code","251e4d43":"code","cf330a0e":"code","f1db4803":"markdown","d7a8bdcb":"markdown","f0eb6511":"markdown","f5bd393e":"markdown","7b83433e":"markdown","9d0b85fa":"markdown","67ff4f32":"markdown","f3767959":"markdown","47d00a2c":"markdown","7aeca37f":"markdown","b7791227":"markdown","569c0dab":"markdown","dd87964a":"markdown"},"source":{"8f04bce3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a832d371":"# Make sure that you have all these libaries available to run the code successfully\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport datetime as dt\nimport urllib.request, json\nimport os\nimport numpy as np\nimport tensorflow as tf # This code has been tested with TensorFlow 1.6\nfrom sklearn.preprocessing import MinMaxScaler","6f365660":"df = pd.read_csv(\"..\/input\/yahoo_dataset.csv\")\nprint(df.shape)","2fbfe109":"# Sort DataFrame by date\ndf = df.sort_values('Date')\n\n# Double check the result\ndf.head()","a36f4fb6":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),(df['Low']+df['High'])\/2.0)\nplt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Mid Price',fontsize=18)\nplt.show()","c93be9db":"# First calculate the mid prices from the highest and lowest\nhigh_prices = df.loc[:,'High'].values\nlow_prices = df.loc[:,'Low'].values\nmid_prices = (high_prices+low_prices)\/2.0","7965297e":"train_data = mid_prices[:4500]\ntest_data = mid_prices[4500:]\nprint(train_data.shape)\nprint(test_data.shape)","3a6884d7":"# Scale the data to be between 0 and 1\n# When scaling remember! You normalize both test and train data with respect to training data\n# Because you are not supposed to have access to test data\nscaler = MinMaxScaler()\ntrain_data = train_data.reshape(-1,1)\ntest_data = test_data.reshape(-1,1)","3e7e34b0":"smoothing_window_size = 1000\nfor di in range(0,4000,smoothing_window_size):\n    scaler.fit(train_data[di:di+smoothing_window_size,:])\n    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n    \n    # You normalize the last bit of remaining data\nscaler.fit(train_data[di+smoothing_window_size:,:])\ntrain_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])","0f135bf0":"# Reshape both train and test data\ntrain_data = train_data.reshape(-1)\n\n# Normalize test data\ntest_data = scaler.transform(test_data).reshape(-1)","e5f25c9e":"# Now perform exponential moving average smoothing\n# So the data will have a smoother curve than the original ragged data\nEMA = 0.0\ngamma = 0.1\nfor ti in range(4000):\n    EMA = gamma*train_data[ti] + (1-gamma)*EMA\n    train_data[ti] = EMA\n\n# Used for visualization and test purposes\nall_mid_data = np.concatenate([train_data,test_data],axis=0)","87f1a39a":"window_size = 100\nN = train_data.size\nstd_avg_predictions = []\nstd_avg_x = []\nmse_errors = []\n\nfor pred_idx in range(window_size,N):\n\n    if pred_idx >= N:\n        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n    else:\n        date = df.loc[pred_idx,'Date']\n\n    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n    std_avg_x.append(date)\n\nprint('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))","599aa007":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\nplt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Mid Price')\nplt.legend(fontsize=18)\nplt.show()","6d350ad5":"window_size = 100\nN = train_data.size\n\nrun_avg_predictions = []\nrun_avg_x = []\n\nmse_errors = []\n\nrunning_mean = 0.0\nrun_avg_predictions.append(running_mean)\n\ndecay = 0.5\n\nfor pred_idx in range(1,N):\n\n    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n    run_avg_predictions.append(running_mean)\n    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n    run_avg_x.append(date)\n\nprint('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))\n","ac2ca0f9":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\nplt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Mid Price')\nplt.legend(fontsize=18)\nplt.show()","7f983a69":"class DataGeneratorSeq(object):\n\n    def __init__(self,prices,batch_size,num_unroll):\n        self._prices = prices\n        self._prices_length = len(self._prices) - num_unroll\n        self._batch_size = batch_size\n        self._num_unroll = num_unroll\n        self._segments = self._prices_length \/\/self._batch_size\n        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n\n    def next_batch(self):\n\n        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n\n        for b in range(self._batch_size):\n            if self._cursor[b]+1>=self._prices_length:\n                #self._cursor[b] = b * self._segments\n                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n\n            batch_data[b] = self._prices[self._cursor[b]]\n            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n\n            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n\n        return batch_data,batch_labels\n\n    def unroll_batches(self):\n\n        unroll_data,unroll_labels = [],[]\n        init_data, init_label = None,None\n        for ui in range(self._num_unroll):\n\n            data, labels = self.next_batch()    \n\n            unroll_data.append(data)\n            unroll_labels.append(labels)\n\n        return unroll_data, unroll_labels\n\n    def reset_indices(self):\n        for b in range(self._batch_size):\n            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n\n\n\ndg = DataGeneratorSeq(train_data,5,5)\nu_data, u_labels = dg.unroll_batches()\n\nfor ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n    print('\\n\\nUnrolled index %d'%ui)\n    dat_ind = dat\n    lbl_ind = lbl\n    print('\\tInputs: ',dat )\n    print('\\n\\tOutput:',lbl)","6216ee0a":"D = 1 # Dimensionality of the data. Since your data is 1-D this would be 1\nnum_unrollings = 50 # Number of time steps you look into the future.\nbatch_size = 500 # Number of samples in a batch\nnum_nodes = [200,200,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\nn_layers = len(num_nodes) # number of layers\ndropout = 0.2 # dropout amount\n\ntf.reset_default_graph() # This is important in case you run this multiple times\n","54dea933":"# Input data.\ntrain_inputs, train_outputs = [],[]\n\n# You unroll the input over time defining placeholders for each time step\nfor ui in range(num_unrollings):\n    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))","b5782a22":"lstm_cells = [\n    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n                            state_is_tuple=True,\n                            initializer= tf.contrib.layers.xavier_initializer()\n                           )\n for li in range(n_layers)]\n\ndrop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n) for lstm in lstm_cells]\ndrop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\nmulti_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n\nw = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\nb = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))","e1ce52e2":"# Create cell state and hidden state variables to maintain the state of the LSTM\nc, h = [],[]\ninitial_state = []\nfor li in range(n_layers):\n    c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n    h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n    initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n\n# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n# a specific format. Read more at: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/dynamic_rnn\nall_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n\n# all_outputs is [seq_length, batch_size, num_nodes]\nall_lstm_outputs, state = tf.nn.dynamic_rnn(\n    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n    time_major = True, dtype=tf.float32)\n\nall_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n\nall_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n\nsplit_outputs = tf.split(all_outputs,num_unrollings,axis=0)","20a201ba":"# When calculating the loss you need to be careful about the exact form, because you calculate\n# loss of all the unrolled steps at the same time\n# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n\nprint('Defining training Loss')\nloss = 0.0\nwith tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n  for ui in range(num_unrollings):\n    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n\nprint('Learning rate decay operations')\nglobal_step = tf.Variable(0, trainable=False)\ninc_gstep = tf.assign(global_step,global_step + 1)\ntf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\ntf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n\nlearning_rate = tf.maximum(\n    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n    tf_min_learning_rate)\n\n# Optimizer.\nprint('TF Optimization operations')\noptimizer = tf.train.AdamOptimizer(learning_rate)\ngradients, v = zip(*optimizer.compute_gradients(loss))\ngradients, _ = tf.clip_by_global_norm(gradients, 5.0)\noptimizer = optimizer.apply_gradients(\n    zip(gradients, v))\n\nprint('\\tAll done')","251e4d43":"print('Defining prediction related TF functions')\n\nsample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n\n# Maintaining LSTM state for prediction stage\nsample_c, sample_h, initial_sample_state = [],[],[]\nfor li in range(n_layers):\n  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n\nreset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n\nsample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n                                   initial_state=tuple(initial_sample_state),\n                                   time_major = True,\n                                   dtype=tf.float32)\n\nwith tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n\nprint('\\tAll done')","cf330a0e":"epochs = 30\nvalid_summary = 1 # Interval you make test predictions\n\nn_predict_once = 50 # Number of steps you continously predict for\n\ntrain_seq_length = train_data.size # Full length of the training data\n\ntrain_mse_ot = [] # Accumulate Train losses\ntest_mse_ot = [] # Accumulate Test loss\npredictions_over_time = [] # Accumulate predictions\n\nsession = tf.InteractiveSession()\n\ntf.global_variables_initializer().run()\n\n# Used for decaying learning rate\nloss_nondecrease_count = 0\nloss_nondecrease_threshold = 2 # If the test error hasn't increased in this many steps, decrease learning rate\n\nprint('Initialized')\naverage_loss = 0\n\n# Define data generator\ndata_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings)\n\nx_axis_seq = []\n\n# Points you start your test predictions from (4500,5761)\ntest_points_seq = np.arange(4500,5500,50).tolist()\n\nfor ep in range(epochs):       \n\n    # ========================= Training =====================================\n    for step in range(train_seq_length\/\/batch_size):\n\n        u_data, u_labels = data_gen.unroll_batches()\n\n        feed_dict = {}\n        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n\n        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n\n        average_loss += l\n\n    # ============================ Validation ==============================\n    if (ep+1) % valid_summary == 0:\n\n        average_loss = average_loss\/(valid_summary*(train_seq_length\/\/batch_size))\n        \n        # The average loss\n        if (ep+1)%valid_summary==0:\n            print('Average loss at step %d: %f' % (ep+1, average_loss))\n\n        train_mse_ot.append(average_loss)\n\n        average_loss = 0 # reset loss\n\n        predictions_seq = []\n\n        mse_test_loss_seq = []\n\n      # ===================== Updating State and Making Predicitons ========================\n        for w_i in test_points_seq:\n            mse_test_loss = 0.0\n            our_predictions = []\n\n        if (ep+1)-valid_summary==0:\n          # Only calculate x_axis values in the first validation epoch\n          x_axis=[]\n\n        # Feed in the recent past behavior of stock prices\n        # to make predictions from that point onwards\n        for tr_i in range(w_i-num_unrollings+1,w_i-1):\n            current_price = all_mid_data[tr_i]\n            feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n            _ = session.run(sample_prediction,feed_dict=feed_dict)\n\n        feed_dict = {}\n\n        current_price = all_mid_data[w_i-1]\n\n        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n\n        # Make predictions for this many steps\n        # Each prediction uses previous prediciton as it's current input\n        for pred_i in range(n_predict_once):\n\n            pred = session.run(sample_prediction,feed_dict=feed_dict)\n\n            our_predictions.append(np.asscalar(pred))\n\n            feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n\n            if (ep+1)-valid_summary==0:\n            # Only calculate x_axis values in the first validation epoch\n                x_axis.append(w_i+pred_i)\n\n            mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n\n            session.run(reset_sample_states)\n\n            predictions_seq.append(np.array(our_predictions))\n\n            mse_test_loss \/= n_predict_once\n            mse_test_loss_seq.append(mse_test_loss)\n\n            if (ep+1)-valid_summary==0:\n                x_axis_seq.append(x_axis)\n\n        current_test_mse = np.mean(mse_test_loss_seq)\n\n        # Learning rate decay logic\n        if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n            loss_nondecrease_count += 1\n        else:\n            loss_nondecrease_count = 0\n\n        if loss_nondecrease_count > loss_nondecrease_threshold :\n            session.run(inc_gstep)\n            loss_nondecrease_count = 0\n            print('\\tDecreasing learning rate by 0.5')\n\n        test_mse_ot.append(current_test_mse)\n        print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n        predictions_over_time.append(predictions_seq)\n        print('\\tFinished Predictions')","f1db4803":"It seems that it is not too bad of a model for very short predictions (one day ahead). Given that stock prices don't change from 0 to 100 overnight, this behavior is sensible. Next, you will look at a fancier averaging technique known as exponential moving average.\n\n### Exponential Moving Average\nYou might have seen some articles on the internet using very complex models and predicting almost the exact behavior of the stock market. But beware! These are just optical illusions and not due to learning something useful. You will see below how you can replicate that behavior with a simple averaging method.\n\nIn the exponential moving average method, you calculate xt+1 as,\n\n$$x_{t}+1 = EMA_{t} = \u03b3 \u00d7 EMA_{t-1} + (1-\u03b3) x_{t} \\space ,where\\space EMA0 = 0$$ \nand EMA is the exponential moving average value you maintain over time.\nThe above equation basically calculates the exponential moving average from t+1 time step and uses that as the one step ahead prediction. \u03b3 decides what the contribution of the most recent prediction is to the EMA. For example, a \u03b3=0.1 gets only 10% of the current value into the EMA. Because you take only a very small fraction of the most recent, it allows to preserve much older values you saw very early in the average. See how good this looks when used to predict one-step ahead below.","d7a8bdcb":"### Loss Calculation and Optimizer\nNow, you'll calculate the loss. However, you should note that there is a unique characteristic when calculating the loss. For each batch of predictions and true outputs, you calculate the Mean Squared Error. And you sum (not average) all these mean squared losses together. Finally, you define the optimizer you're going to use to optimize the neural network. In this case, you can use Adam, which is a very recent and well-performing optimizer.","f0eb6511":"Defining Hyperparameters\nIn this section, you'll define several hyperparameters. D is the dimensionality of the input. It's straightforward, as you take the previous stock price as the input and predict the next one, which should be 1.\n\nThen you have num_unrollings, this is a hyperparameter related to the backpropagation through time (BPTT) that is used to optimize the LSTM model. This denotes how many continuous time steps you consider for a single optimization step. You can think of this as, instead of optimizing the model by looking at a single time step, you optimize the network by looking at num_unrollings time steps. The larger the better.\n\nThen you have the batch_size. Batch size is how many data samples you consider in a single time step.\n\nNext you define num_nodes which represents the number of hidden neurons in each cell. You can see that there are three layers of LSTMs in this example.","f5bd393e":"## Defining Parameters of the LSTM and Regression layer\nYou will have a three layers of LSTMs and a linear regression layer, denoted by w and b, that takes the output of the last Long Short-Term Memory cell and output the prediction for the next time step. You can use the MultiRNNCell in TensorFlow to encapsulate the three LSTMCell objects you created. Additionally, you can have the dropout implemented LSTM cells, as they improve performance and reduce overfitting.","7b83433e":"Note that you should only smooth training data.","9d0b85fa":"# LSTM","67ff4f32":"Take a look at the averaged results below. It follows the actual behavior of stock quite closely. Next, you will look at a more accurate one-step prediction method.","f3767959":"## One-Step Ahead Prediction via Averaging\nAveraging mechanisms allow you to predict (often one time step ahead) by representing the future stock price as an average of the previously observed stock prices. Doing this for more than one time step can produce quite bad results. You will look at two averaging techniques below; standard averaging and exponential moving average. You will evaluate both qualitatively (visual inspection) and quantitatively (Mean Squared Error) the results produced by the two algorithms.\n\nThe Mean Squared Error (MSE) can be calculated by taking the Squared Error between the true value at one step ahead and the predicted value and averaging it over all the predictions.\n\n### Standard Average\nYou can understand the difficulty of this problem by first trying to model this as an average calculation problem. First you will try to predict the future stock market prices (for example, xt+1) as an average of the previously observed stock market prices within a fixed size window (for example, xt-N, ..., xt) (say previous 100 days). Thereafter you will try a bit more fancier \"exponential moving average\" method and see how well that does. Then you will move on to the \"holy-grail\" of time-series prediction; Long Short-Term Memory models.\n\nFirst you will see how normal averaging works. That is you say,\n\n$$ x_{t+1} = \\frac{1}{N}\\sum_{i=t-N}^{t} x_{i}$$\n\nIn other words, you say the prediction at t+1 is the average value of all the stock prices you observed within a window of **t** to **t\u2212N**.","47d00a2c":"### Running the LSTM\nHere you will train and predict stock price movements for several epochs and see whether the predictions get better or worse over time. You follow the following procedure.\n\nDefine a test set of starting points (test_points_seq) on the time series to evaluate the model on\nFor each epoch\n1. For full sequence length of training data\n    * Unroll a set of num_unrollings batches\n    * Train the neural network with the unrolled batches\n2. Calculate the average training loss\n3.  For each starting point in the test set\n        * Update the LSTM state by iterating through the previous num_unrollings data found before the test point\n    * Make predictions for n_predict_once steps continuously, using the previous prediction as the current input\n    * Calculate the MSE loss between the n_predict_once points predicted and the true stock prices at those time stamps","7aeca37f":"So no matter how many steps you predict in to the future, you'll keep getting the same answer for all the future prediction steps.\n\nOne solution you have that will output useful information is to look at momentum-based algorithms. They make predictions based on whether the past recent values were going up or going down (not the exact values). For example, they will say the next day price is likely to be lower, if the prices have been dropping for the past days, which sounds reasonable. However, you will use a more complex model: an LSTM model.\n\nThese models have taken the realm of time series prediction by storm, because they are so good at modelling time series data. You will see if there actually are patterns hidden in the data that you can exploit.","b7791227":"### Calculating LSTM output and Feeding it to the regression layer to get final prediction\nIn this section, you first create TensorFlow variables (c and h) that will hold the cell state and the hidden state of the Long Short-Term Memory cell. Then you transform the list of train_inputs to have a shape of [num_unrollings, batch_size, D], this is needed for calculating the outputs with the tf.nn.dynamic_rnn function. You then calculate the LSTM outputs with the tf.nn.dynamic_rnn function and split the output back to a list of num_unrolling tensors. the loss between the predictions and true stock prices.","569c0dab":"### Prediction Related Calculations\nHere you define the prediction related TensorFlow operations. First, define a placeholder for feeding in the input (sample_inputs), then similar to the training stage, you define state variables for prediction (sample_c and sample_h). Finally you calculate the prediction with the tf.nn.dynamic_rnn function and then sending the output through the regression layer (w and b). You also should define the reset_sample_state operation, which resets the cell state and the hidden state. You should execute this operation at the start, every time you make a sequence of predictions.","dd87964a":"If Exponential Moving Average is this Good, Why do You Need Better Models?\nYou see that it fits a perfect line that follows the True distribution (and justified by the very low MSE). Practically speaking, you can't do much with just the stock market value of the next day. Personally what I'd like is not the exact stock market price for the next day, but would the stock market prices go up or down in the next 30 days. Try to do this, and you will expose the incapability of the EMA method.\n\nYou will now try to make predictions in windows (say you predict the next 2 days window, instead of just the next day). Then you will realize how wrong EMA can go. Here is an example:\n\n### Predict More Than One Step into the Future\n\n$$x_{t}+1 = EMA_{t} = \u03b3 \u00d7 EMA_{t-1} + (1-\u03b3) x_{t} \\space ,where\\space EMA0 = 0$$ \n\nSo you have xt+1=0.5\u00d70.5+(1\u22120.5)\u00d70.4=0.45\n\nSo xt+1=EMAt=0.45\n\nSo the next prediction xt+2 becomes,\n\nXt+2 = \u03b3 \u00d7 EMAt + (1-\u03b3)Xt+1\n\nWhich is xt+2=\u03b3\u00d7EMAt+(1\u2212\u03b3)EMAt=EMAt\n\nOr in this example, Xt+2 = Xt+1 = 0.45"}}