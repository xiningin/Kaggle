{"cell_type":{"20d627c3":"code","56ce1e08":"code","5546704e":"code","8def9cc4":"code","a28f7956":"code","411263b2":"code","27c1682f":"code","4d276ebd":"code","018be23b":"code","8d8a9a7e":"code","c758a964":"code","21cb42e2":"code","01b59499":"code","56003463":"code","fbe8dc42":"code","721ac9fd":"code","24ff277f":"code","ed801c62":"code","b816d14b":"code","b28ca502":"code","a3c3d409":"code","9e03b24e":"code","2a904d2d":"code","2f81071f":"code","da652850":"code","bdbce371":"code","b703e8c5":"markdown","693100a8":"markdown","8aae7164":"markdown","8f438bb4":"markdown","038fc0c3":"markdown","6801d76b":"markdown","919c164f":"markdown","f26692ca":"markdown","0269eb6e":"markdown","b6228ab2":"markdown","2c319593":"markdown","d6bcc7a2":"markdown","4cc13ccc":"markdown","6ce56322":"markdown","752a8c9e":"markdown","54031dea":"markdown"},"source":{"20d627c3":"import pandas as pd\nimport numpy as np\n\nfrom scipy import stats\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\n\nfrom xgboost import XGBRegressor, plot_importance\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('..\/input\/ted-talks\/ted_main.csv')\n","56ce1e08":"df.info()","5546704e":"# There are only 6 rows with missing data for speaker_occupation and I decide to drop those as it will not affect the dataset in a measurable way.\ndf.dropna(axis=0, subset=['speaker_occupation'], inplace=True)\ndf.info()","8def9cc4":"df.describe()","a28f7956":"sns.pairplot(df[['comments', 'duration', 'languages', 'num_speaker']],  size=2)\nplt.show()","411263b2":"fig, (ax1, ax2) = plt.subplots(1,2)\nsns.boxplot(y= df.views, ax=ax1)\nsns.boxplot(y=df.comments, ax=ax2)\nplt.subplots_adjust(wspace=0.50)\nplt.show()","27c1682f":"# Compute z-score for each value in a colun relative to the column mean and std and use the resulting value to filter outliers from each specified column in the dataframe.\n\ndf = df[(np.abs(stats.zscore(df[['comments', 'views']])) < 3).all(axis=1)]","4d276ebd":"df.info()","018be23b":"df.describe()","8d8a9a7e":"month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nday_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\ndf['film_date'] = pd.to_datetime(df['film_date'],unit='s')\ndf['film_month'] = df['film_date'].apply(lambda x: month_order[int(x.month) - 1])\ndf['film_day'] = df['film_date'].apply(lambda x: day_order[int(x.day) % 7 - 1])","c758a964":"fig, ax =plt.subplots(1,2, figsize=(20,10))\nsns.countplot(df['film_month'], ax=ax[0])\nsns.countplot(df['film_day'], ax=ax[1])\nfig.show()","21cb42e2":"def cal_pos_rating_ratio(ratings):\n    counter = {'Funny':0, 'Beautiful':0, 'Ingenious':0, 'Courageous':0, 'Longwinded':0, 'Confusing':0, 'Informative':0, 'Fascinating':0, 'Unconvincing':0, 'Persuasive':0, 'Jaw-dropping':0, 'OK':0, 'Obnoxious':0, 'Inspiring':0}\n    neg_descriptors = {\"Confusing\", \"Unconvincing\", \"Longwinded\", \"Obnoxious\", \"OK\"}\n    for rating_list in ratings:\n        counter[rating_list['name']] += rating_list['count']\n    neg_desc_count = sum([counter[desc] for desc in neg_descriptors])\n    total_desc_count = sum(list(counter.values()))\n    pos_desc_count = total_desc_count - neg_desc_count\n    popular_data_no_tedex_pct_positive = 100 * (pos_desc_count \/ total_desc_count)\n    return popular_data_no_tedex_pct_positive","01b59499":"df['eval_ratings'] = df['ratings'].apply(lambda x: eval(x))","56003463":"df['pos_rating_ratio'] = df.eval_ratings.apply(cal_pos_rating_ratio)","fbe8dc42":"def rating_count(ratings):\n    counter = {'Funny':0, 'Beautiful':0, 'Ingenious':0, 'Courageous':0, 'Longwinded':0, 'Confusing':0, 'Informative':0, 'Fascinating':0, 'Unconvincing':0, 'Persuasive':0, 'Jaw-dropping':0, 'OK':0, 'Obnoxious':0, 'Inspiring':0}\n    for rating_list in ratings:\n        counter[rating_list['name']] += rating_list['count']\n    return  sum(list(counter.values()))\n","721ac9fd":"df['raiting_count'] = df.eval_ratings.apply(rating_count)","24ff277f":"y = df.views\nX = df.drop(['views', 'film_date', 'published_date', 'eval_ratings'], axis=1) # drop the columns that are not needed for predictions\nX.info()","ed801c62":"\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n# Select categorical columns with relatively low cardinality\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 13 and \n                        X_train_full[cname].dtype == \"object\"] # 13 is used to fit the month encoding\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'uint64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n# One-hot encode the data\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)","b816d14b":"X_train.info()","b28ca502":"def get_mae(max_leaf_nodes, X_train, X_valid, y_train, y_valid):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(X_train, y_train)\n    preds_val = model.predict(X_valid)\n    mae = mean_absolute_error(y_valid, preds_val)\n    return(mae)\n\n\nmaes_dtr = []\n# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 25, 50, 100, 500, 5000]:\n    mae_dtr = get_mae(max_leaf_nodes, X_train, X_valid, y_train, y_valid)\n    maes_dtr.append(mae_dtr)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, mae_dtr))\n\n","a3c3d409":"def get_mae(max_leaf_nodes, X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(X_train, y_train)\n    preds_val = model.predict(X_valid)\n    mae = mean_absolute_error(y_valid, preds_val)\n    return(mae)\n\n\nmaes_rfr = []\n# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    mae_rf = get_mae(max_leaf_nodes, X_train, X_valid, y_train, y_valid)\n    maes_rfr.append(mae_rf)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, mae_rf))","9e03b24e":"\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nxgb1 = XGBRegressor(random_state=0, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)],)\nparameters = {\n              'objective':['reg:linear'],\n              'learning_rate': [.1, .05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [50, 500, 1000]}\n\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv=kf,\n                        n_jobs = 5,\n                        scoring='neg_mean_absolute_error',\n                        verbose=True)\n\nxgb_grid.fit(X_train,\n         y_train)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)","2a904d2d":"plot_importance(xgb_grid.best_estimator_)","2f81071f":"predictions_xgb = xgb_grid.predict(X_valid) \n# Calculate MAE\nmae_xgb = mean_absolute_error(predictions_xgb, y_valid)\n\nprint(\"Mean Absolute Error:\" , mae_xgb)","da652850":"y_mae = [min(maes_dtr), min(maes_rfr), mae_xgb]\nx_mae = ['Decision Tree MAE', 'Random Forest MAE', 'XGB MAE']","bdbce371":"sns.barplot(x=x_mae, y=y_mae)\nplt.ylabel(\"Mean Absolute Error\")","b703e8c5":"## Build XGBRegressor model\nI am going to use GridsearchCV and KFOLD to tune hyperparameters for the model","693100a8":"## Build base model - Decision Tree\n\n1. As a baseline model I will use simple decision tree regressor\n2. As a model evaluation metric I will use MAE - mean absolute error ","8aae7164":"## Build a better model - RandomForest.","8f438bb4":"# TEDed Talks views prediction","038fc0c3":"**Indeed the boxplots confirm that those two columns contain many outliers.**","6801d76b":"## Create train and validation data splits, keep low cardinality columns and one hot encode cathegorical features.","919c164f":"Now the standard deviation for comments and views columns is lower and mean and max values are not skewed by outliers.","f26692ca":"### Standard deviation accross some numerical columns is quite high which suggests that there are outliers.","0269eb6e":"## Prepare data for modeling - remove missing values, feature engineering, remove outliers","b6228ab2":"## I will be using ensebmle regression methods to predict the views given talk will accumulate.","2c319593":"**Looking at the distribution of talks over week days, we can see that difference is about 25% while the distribution over the months in an year is much more diverese - having less than 100 talks filmmed  in January and about 600 in February. Therefore I think that these two new features will have impact on the target feature prediction and I will use them**","d6bcc7a2":"## Detect and remove outliers from numerical columns","4cc13ccc":"## The XGB regressor model with hyper paramter tunnig has achieved the lowest mean absolute error of ~460000, which means that its predictions can be of by that much. Considering the the mean median value for TED talk views is about 1.1e+6 it is clear the the models are not perfoming very well on this dataset. More work is needed on feature engineering to produce features with higher importance and with low cardinality from the rest of the cathegorical variables - tags, ratings, speaker occupation. However we can see that the *pos_rating_ratio* and *rating_count* features are of the highest importance when making predictions.","6ce56322":"### Best performing random forest model for this task is with 50 leafs.","752a8c9e":"### Best performing decision tree model for this task is with 25 leafs.","54031dea":"### TODO:\n1. Create low cardinality feature column for speaker occupation\n2. Create feature that describes the amount of talks the speaker has given prior to the current one\n3. Create low cardinality feature column for the topic of the talk\n4. Create feature low cardinality feature column for the event the talk was given at"}}