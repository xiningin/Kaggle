{"cell_type":{"883cdf57":"code","c60fa5c6":"code","c207440a":"code","c6d63841":"code","e70aeef7":"code","2686827f":"code","35b8caf5":"code","99e50e8b":"code","2bdb88ec":"code","2bfb257c":"code","a341ba6a":"code","224d5e32":"code","a337e33d":"code","20e84d6c":"code","923ad736":"code","e1daa27b":"code","6704672b":"code","b57d2a83":"code","1072c5f8":"code","4b563ada":"code","6161eb5e":"code","149df2a5":"code","6dc67021":"code","41af19aa":"code","e0530883":"code","e8d9222d":"code","fa3e4e0e":"code","197ded3e":"code","bccf18a6":"code","a99865f3":"code","42b090f1":"code","0f02f170":"code","47b6d761":"code","b902b20e":"code","bc99032e":"code","dc5b663b":"code","8f9cced1":"code","e2e9d66f":"code","f0216352":"code","7423b5a0":"code","f5d7c365":"code","52520d69":"code","fea36f06":"code","af8cb5ca":"code","1e6fdf98":"code","a8ff5a7f":"code","53304caf":"code","431978ab":"code","c849a4ac":"code","292daf05":"code","5b59c05a":"code","b7fe87cf":"code","eca300ad":"code","e85c6456":"code","8b9b32ab":"code","b260a217":"code","1d543e8d":"code","db472b6b":"code","1d467514":"code","380387f5":"code","d8f4162e":"code","17bae382":"code","379f6ece":"code","aecc7774":"code","bd694fba":"code","c3f9add4":"code","647b0a65":"code","b2004922":"code","ccaa0e74":"code","6beea24b":"code","6284cb3d":"code","ade1fd90":"code","d9bca4d4":"code","d1252b41":"code","7f21db7a":"code","2f5b3dce":"code","f2afd23e":"code","6c4b4a9c":"code","c8dd68da":"code","942f89bd":"code","be9f1382":"code","ba011692":"code","3fde980e":"code","0fad1aa7":"code","193a70fa":"code","2abc8749":"code","0b9e71ce":"code","5e0fab25":"code","8270a825":"code","44c4e324":"markdown","9cf9fe8f":"markdown","dc3a1106":"markdown","dc93b876":"markdown","d539c5d2":"markdown","43c9ac6a":"markdown","b2fe4bb8":"markdown","a1869e62":"markdown","83ef263a":"markdown","1224d649":"markdown","5af21a9d":"markdown","ae72f4ac":"markdown","97cf1da1":"markdown","1ad4fa1d":"markdown","8dae7f6e":"markdown","3bbaf250":"markdown","ca7bc80b":"markdown","a755fca4":"markdown","420a8ca5":"markdown","ed06d7ba":"markdown","b672e815":"markdown","d2b044bd":"markdown","694f709d":"markdown","6f9e2f8d":"markdown","6243fc3b":"markdown","9499ddc2":"markdown","bc35f877":"markdown","e6eb1c3c":"markdown","5a842357":"markdown","2f42a4ae":"markdown","d803d04a":"markdown","a5e2ca81":"markdown","ee403fa5":"markdown","391c1eee":"markdown","bd970654":"markdown","dab4e32b":"markdown","654d918e":"markdown","836f94e0":"markdown","1ce9a5ad":"markdown","937c762c":"markdown","93aefdd2":"markdown","94f3e3f8":"markdown","45a84e9a":"markdown","edc232b3":"markdown","1c9b6a3b":"markdown","19f1d8e2":"markdown","7466d048":"markdown","d007d424":"markdown","6edc8fc4":"markdown","75c08e9d":"markdown","9b5f2178":"markdown","db61e8d1":"markdown","458aa2f1":"markdown","5eceea30":"markdown","87e7752e":"markdown","f4449d4c":"markdown","54c2c445":"markdown","391737a9":"markdown","39461101":"markdown","a631b7ab":"markdown","5c97625b":"markdown","388a0ae0":"markdown","17acd6dc":"markdown","814d2184":"markdown","ee2182e5":"markdown","5d427227":"markdown","8970b015":"markdown","f73a4049":"markdown","9b22869f":"markdown","b2b311c7":"markdown","8058f378":"markdown","20fb6de0":"markdown","e404b42e":"markdown","bdb0a6fb":"markdown","1978d339":"markdown","1a726bad":"markdown","5b241632":"markdown","5eb7eed7":"markdown","121738b7":"markdown","4d508b9e":"markdown","b4aa8e0a":"markdown"},"source":{"883cdf57":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_color_codes()\nsns.set(style=\"darkgrid\")\n%matplotlib inline\nfrom scipy.stats import zscore\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\t\nfrom sklearn.utils import resample\n\n#setting up for customized printing\nfrom IPython.display import Markdown, display\nfrom IPython.display import HTML\ndef printmd(string, color=None):\n    colorstr = \"<span style='color:{}'>{}<\/span>\".format(color, string)\n    display(Markdown(colorstr))\n    \n#function to display dataframes side by side    \nfrom IPython.display import display_html\ndef display_side_by_side(args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline;margin-left:50px !important;margin-right: 40px !important\"'),raw=True)","c60fa5c6":"def distplot(figRows,figCols,xSize, ySize, features, colors):\n    f, axes = plt.subplots(figRows, figCols, figsize=(xSize, ySize))\n    \n    features = np.array(features).reshape(figRows, figCols)\n    colors = np.array(colors).reshape(figRows, figCols)\n    \n    for row in range(figRows):\n        for col in range(figCols):\n            if (figRows == 1 and figCols == 1) :\n                axesplt = axes\n            elif (figRows == 1 and figCols > 1) :\n                axesplt = axes[col]\n            elif (figRows > 1 and figCols == 1) :\n                axesplt = axes[row]\n            else:\n                axesplt = axes[row][col]\n            plot = sns.distplot(bank[features[row][col]], color=colors[row][col], ax=axesplt, kde=True, hist_kws={\"edgecolor\":\"k\"})\n            plot.set_xlabel(features[row][col],fontsize=20)","c207440a":"def boxplot(figRows,figCols,xSize, ySize, features, colors, hue=None, orient='h'):\n    f, axes = plt.subplots(figRows, figCols, figsize=(xSize, ySize))\n    \n    features = np.array(features).reshape(figRows, figCols)\n    colors = np.array(colors).reshape(figRows, figCols)\n    \n    for row in range(figRows):\n        for col in range(figCols):\n            if (figRows == 1 and figCols == 1) :\n                axesplt = axes\n            elif (figRows == 1 and figCols > 1) :\n                axesplt = axes[col]\n            elif (figRows > 1 and figCols == 1) :\n                axesplt = axes[row]\n            else:\n                axesplt = axes[row][col]\n            plot = sns.boxplot(features[row][col], data= bank, color=colors[row][col], ax=axesplt, orient=orient, hue=hue)\n            plot.set_xlabel(features[row][col],fontsize=20)","c6d63841":"def countplot(figRows,figCols,xSize, ySize, features, colors=None,palette=None,hue=None, orient=None, rotation=90):\n    f, axes = plt.subplots(figRows, figCols, figsize=(xSize, ySize))\n    \n    features = np.array(features).reshape(figRows, figCols)\n    if(colors is not None):\n        colors = np.array(colors).reshape(figRows, figCols)\n    if(palette is not None):\n        palette = np.array(palette).reshape(figRows, figCols)\n    \n    for row in range(figRows):\n        for col in range(figCols):\n            if (figRows == 1 and figCols == 1) :\n                axesplt = axes\n            elif (figRows == 1 and figCols > 1) :\n                axesplt = axes[col]\n            elif (figRows > 1 and figCols == 1) :\n                axesplt = axes[row]\n            else:\n                axesplt = axes[row][col]\n                \n            if(colors is None):\n                plot = sns.countplot(features[row][col], data=bank, palette=palette[row][col], ax=axesplt, orient=orient, hue=hue)\n            elif(palette is None):\n                plot = sns.countplot(features[row][col], data=bank, color=colors[row][col], ax=axesplt, orient=orient, hue=hue)\n            plot.set_title(features[row][col],fontsize=20)\n            plot.set_xlabel(None)\n            plot.set_xticklabels(rotation=rotation, labels=plot.get_xticklabels(),fontweight='demibold',fontsize='large')\n            ","e70aeef7":"def catdist(cols):\n    dfs = []\n    for col in cols:\n        colData = pd.DataFrame(bank[col].value_counts(), columns=[col])\n        colData['%'] = round((colData[col]\/colData[col].sum())*100,2)\n        dfs.append(colData)\n    display_side_by_side(dfs)","2686827f":"bank = pd.read_csv(\"..\/input\/portuguese-banking-institution\/bank-full.csv\")\nbank.head()","35b8caf5":"print('The total number of rows :', bank.shape[0])\nprint('The total number of columns :', bank.shape[1])","99e50e8b":"bank.info()\nprint('===========================================')","2bdb88ec":"print(bank.isna().sum())\nprint('===================')\nprint(bank.isnull().sum())\nprint('===================')\nprintmd('**CONCLUSION**: As seen from the data above, we conclude there are **\"NO Missing\"** values in the data', color=\"blue\")","2bfb257c":"display(bank.describe().transpose())\nprint('==============================')\nprintmd('Total negative values under **balance**', color=\"brown\")\ndisplay(bank[bank['balance'] < 0].shape[0])\nprint('==============================')\ndisplay(bank[bank['pdays'] == -1]['pdays'].value_counts())\nprint('==============================')\ndisplay(bank[bank['previous'] == 0]['previous'].value_counts())","a341ba6a":"pandas_profiling.ProfileReport(bank)","224d5e32":"distplot(3,2, 15,15, \n         ['age', 'balance','day', 'duration', 'campaign', 'pdays'], \n         ['olive', 'indigo', 'blue', 'teal', 'brown', 'red'])\n\ndistplot(1,1, 7, 5, \n         ['previous'], \n         ['green'])","a337e33d":"pd.DataFrame.from_dict(dict(\n    {\n        'age':bank.age.skew(), \n        'balance': bank.balance.skew(), \n        'day': bank.day.skew(),\n        'duration': bank.duration.skew(),\n        'campaign': bank.campaign.skew(),\n        'pdays': bank.pdays.skew(),\n        'previous': bank.previous.skew(),        \n    }), orient='index', columns=['Skewness'])","20e84d6c":"boxplot(7,1, 20,31, \n         ['age', 'balance','day', 'duration', 'campaign', 'pdays', 'previous'], \n         ['olive', 'indigo', 'blue', 'teal', 'brown', 'red', 'cyan'])","923ad736":"def catdist(cols):\n    dfs = []\n    for col in cols:\n        colData = pd.DataFrame(bank[col].value_counts(), columns=[col])\n        colData['%'] = round((colData[col]\/colData[col].sum())*100,2)\n        dfs.append(colData)\n    display_side_by_side(dfs)\n        ","e1daa27b":"catdist(['Target', 'default', 'housing', 'loan', 'marital', 'contact', 'education', 'poutcome', 'job', 'month'])","6704672b":"countplot(2,4, 20,16, \n         ['Target', 'marital','education', 'default', 'housing', 'loan', 'contact', 'poutcome', ], \n         ['olive', 'indigo', 'blue', 'teal', 'brown', 'red', 'cyan','darkgreen'], rotation=30)\n\ncountplot(1,2, 20,5, \n         ['job', 'month'], \n         palette=['Set1_r', 'Set1'], rotation=60)","b57d2a83":"countplot(2,4, 20,15, \n         ['Target', 'marital','education', 'default', 'housing', 'loan', 'contact', 'poutcome'], \n         palette=['winter', 'Accent', 'Paired', 'Spectral', 'bone', 'cool', 'PuRd_r','inferno'], hue='Target', rotation=30)\n\ncountplot(1,2, 20,5, \n         ['job', 'month'], \n         palette=['viridis', 'Dark2_r'], hue='Target', rotation=60)","1072c5f8":"def point_bar_plot(row, col, target, figRow, figCol, palette='rocket', fontsize='large', fontweight='demibold'):\n    sns.set(style=\"whitegrid\")\n    f, axes = plt.subplots(2, 1, figsize=(figRow, figCol))\n    pplot=sns.pointplot(row,col, data=bank, ax=axes[0], linestyles=['--'])\n    pplot.set_xlabel(None)\n    pplot.set_xticklabels(labels=pplot.get_xticklabels(),fontweight=fontweight,fontsize=fontsize)    \n    bplot=sns.barplot(row,col, data=bank, hue=target, ax=axes[1],palette=palette)\n    bplot.set_xlabel(row,fontsize=20)\n    bplot.set_xticklabels(labels=bplot.get_xticklabels(),fontweight=fontweight,fontsize=fontsize)","4b563ada":"def point_box_bar_plot(row, col, target, figRow, figCol, palette='rocket', fontsize='large', fontweight='demibold'):\n    sns.set(style=\"whitegrid\")\n    f, axes = plt.subplots(3, 1, figsize=(figRow, figCol))\n    pplot=sns.pointplot(row,col, data=bank, ax=axes[0], linestyles=['--'])\n    pplot.set_xlabel(None)\n    pplot.set_xticklabels(labels=pplot.get_xticklabels(),fontweight=fontweight,fontsize=fontsize)\n    bxplot=sns.boxplot(row,col, data=bank, hue=target, ax=axes[1],palette='viridis')\n    bxplot.set_xlabel(None)\n    bxplot.set_xticklabels(labels=bxplot.get_xticklabels(),fontweight=fontweight,fontsize=fontsize)\n    bplot=sns.barplot(row,col, data=bank, hue=target, ax=axes[2],palette=palette)\n    bplot.set_xlabel(row,fontsize=20)\n    bplot.set_xticklabels(labels=bplot.get_xticklabels(),fontweight=fontweight,fontsize=fontsize)","6161eb5e":"point_box_bar_plot('job', 'age', 'Target', 20, 15)","149df2a5":"point_bar_plot('job', 'balance', 'Target', 20, 10, palette='winter')","6dc67021":"point_bar_plot('marital', 'balance', 'Target', 12, 8, palette='summer')","41af19aa":"point_bar_plot('education', 'balance', 'Target', 12, 8, palette='tab20b_r')","e0530883":"point_bar_plot('default', 'balance', 'Target', 8, 8, palette='afmhot')","e8d9222d":"point_bar_plot('housing', 'balance', 'Target', 8, 8, palette='autumn')","fa3e4e0e":"point_bar_plot('loan', 'balance', 'Target', 8, 8, palette='binary')","197ded3e":"point_bar_plot('Target', 'duration', 'Target', 10, 8, palette='cool')","bccf18a6":"point_bar_plot('contact', 'campaign', 'Target', 10, 8, palette='copper')","a99865f3":"point_bar_plot('month', 'campaign', 'Target', 20, 8, palette='Greens_r')","42b090f1":"point_bar_plot('job', 'campaign', 'Target', 20, 8, palette='YlGnBu')","0f02f170":"point_bar_plot('poutcome', 'previous', 'Target', 10, 8, palette='Paired_r')","47b6d761":"sns.pairplot(bank, hue='Target', diag_kind = 'kde', palette='rocket')","b902b20e":"bank.corr()","bc99032e":"f, axes = plt.subplots(1, 1, figsize=(12, 6))\nsns.heatmap(bank.corr().abs(), cmap='YlGnBu', annot=True, fmt=\".2f\", ax=axes, linecolor='white', linewidths=0.3, square=True)","dc5b663b":"bank.drop(bank[bank.job == 'unknown'].index, axis=0, inplace=True)\nbank.shape","8f9cced1":"bank.drop(bank[bank.education == 'unknown'].index, axis=0, inplace=True)\nbank.shape","e2e9d66f":"catdist(['Target', 'default', 'housing', 'loan', 'marital', 'contact', 'education', 'poutcome', 'job', 'month'])","f0216352":"bank.drop(['duration','default', 'day'], axis=1, inplace=True)\nbank.shape","7423b5a0":"def remove_outliers(col, data):\n    outlier_col = col + \"_outliers\"\n    data[outlier_col] = data[col]\n    data[outlier_col]= zscore(data[outlier_col])\n\n    condition = (data[outlier_col]>3) | (data[outlier_col]<-3)\n    print(data[condition].shape)\n    data.drop(data[condition].index, axis = 0, inplace = True)\n    data.drop(outlier_col, axis=1, inplace=True)","f5d7c365":"remove_outliers('balance', bank)","52520d69":"f, axes = plt.subplots(1, 1, figsize=(20, 5))\nsns.boxplot(bank['balance'], ax =axes)","fea36f06":"remove_outliers('pdays', bank)","af8cb5ca":"f, axes = plt.subplots(1, 1, figsize=(20, 5))\nsns.boxplot(bank['pdays'], ax =axes)","1e6fdf98":"remove_outliers('previous', bank)","a8ff5a7f":"f, axes = plt.subplots(1, 1, figsize=(20, 5))\nsns.boxplot(bank['previous'], ax =axes)","53304caf":"bank_enc= pd.get_dummies(bank, prefix=['job','marital','education','contact','month','poutcome'], columns=['job','marital','education','contact','month','poutcome'])","431978ab":"bank_enc.info()","c849a4ac":"bank_enc.drop(['job_student','marital_divorced', 'education_primary', 'contact_telephone', 'month_dec', 'poutcome_success'], \n                  axis=1, inplace=True)","292daf05":"from sklearn.preprocessing import LabelEncoder   # import label encoder\n\ndef lencode(col):\n    labelencoder = LabelEncoder()\n    bank_enc[col] = labelencoder.fit_transform(bank_enc[col]) # returns label encoded variable(s)\n    return bank_enc","5b59c05a":"bank_enc = lencode('housing')\nbank_enc = lencode('loan')\nbank_enc = lencode('Target')","b7fe87cf":"from sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nbank_enc[['age', 'balance', 'campaign','pdays','previous']] = std_scale.fit_transform(bank_enc[['age', 'balance', 'campaign','pdays','previous']])","eca300ad":"distplot(1,2, 12,5, \n         ['balance', 'campaign'], \n         ['indigo', 'blue'])\n\ndistplot(1,2, 12, 5, \n         ['previous', 'pdays'], \n         ['green', 'red'])","e85c6456":"bank_enc[['balance', 'campaign','pdays','previous']].skew()","8b9b32ab":"def lognew(a):    \n    a = np.array(a)\n    x = np.min(a)\n    a = a + 1 - x\n    return np.log1p(a)","b260a217":"from sklearn.preprocessing import FunctionTransformer  \nlog_transformer = FunctionTransformer(lognew)\nbank_enc[['balance', 'campaign','pdays','previous']] = log_transformer.fit_transform(bank_enc[['balance', 'campaign','pdays','previous']])","1d543e8d":"bank_enc[['balance', 'campaign','pdays','previous']].skew()","db472b6b":"distplot(1,2, 12,5, \n         ['balance', 'campaign'], \n         ['indigo', 'blue'])\n\ndistplot(1,2, 12, 5, \n         ['previous', 'pdays'], \n         ['green', 'red'])","1d467514":"def fit_n_score(model, X_train, X_test, y_train, y_test):  # take the model, and data as inputs    \n    model.fit(X_train, y_train)   # fit the model with the train data\n    \n    iterables = [[type(model).__name__], ['Training', 'Testing']]\n    \n    multiIndex = pd.MultiIndex.from_product(iterables, names=['Algorithm', 'DataSet'])\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    #get Precision Score on train and test\n    accuracy_train = round(metrics.accuracy_score(y_train, y_train_pred),3)\n    accuracy_test = round(metrics.accuracy_score(y_test, y_test_pred),3)\n    accdf = pd.DataFrame([[accuracy_train],[accuracy_test]], index=multiIndex, columns=['Accuracy'])    \n\n    #get Precision Score on train and test\n    precision_train = round(metrics.precision_score(y_train, y_train_pred),3)\n    precision_test = round(metrics.precision_score(y_test, y_test_pred),3)\n    precdf = pd.DataFrame([[precision_train],[precision_test]], index=multiIndex, columns=['Precision'])\n\n    #get Recall Score on train and test\n    recall_train = round(metrics.recall_score(y_train, y_train_pred),3)\n    recall_test = round(metrics.recall_score(y_test, y_test_pred),3)\n    recdf = pd.DataFrame([[recall_train],[recall_test]], index=multiIndex, columns=['Recall'])    \n\n    #get F1-Score on train and test\n    f1_score_train = round(metrics.f1_score(y_train, y_train_pred),3)\n    f1_score_test = round(metrics.f1_score(y_test, y_test_pred),3)\n    f1sdf = pd.DataFrame([[f1_score_train],[f1_score_test]], index=multiIndex, columns=['F1 Score'])   \n    \n    consolidatedDF= pd.concat([accdf, precdf,recdf, f1sdf], axis=1)\n    \n    confusion_matrix_test = pd.crosstab(y_test, y_test_pred, rownames=['Actual'], colnames=['Predicted'])   \n    \n    display_side_by_side([consolidatedDF, confusion_matrix_test])\n    \n    return consolidatedDF, confusion_matrix_test\n    ","380387f5":"# function for model fitting, prediction and calculating different scores\ndef Modelling_Prediction_Scores(model ,X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    #predict on train and test\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    #predict the probabilities on train and test\n    y_train_pred_proba = model.predict_proba(X_train) \n    y_test_pred_proba = model.predict_proba(X_test)\n\n    #get Accuracy Score for train and test\n    accuracy_train = metrics.accuracy_score(y_train, y_train_pred)\n    accuracy_test = metrics.accuracy_score(y_test, y_test_pred)\n    accdf = pd.DataFrame([[accuracy_train, accuracy_test, ]], columns=['Training', 'Testing'], index=['Accuracy'])    \n\n    #get Precision Score on train and test\n    precision_train = metrics.precision_score(y_train, y_train_pred)\n    precision_test = metrics.precision_score(y_test, y_test_pred)\n    precdf = pd.DataFrame([[precision_train, precision_test, ]], columns=['Training', 'Testing'], index=['Precision'])\n\n    #get Recall Score on train and test\n    recall_train = metrics.recall_score(y_train, y_train_pred)\n    recall_test = metrics.recall_score(y_test, y_test_pred)\n    recdf = pd.DataFrame([[recall_train, recall_test, ]], columns=['Training', 'Testing'], index=['Recall'])\n\n    #get F1-Score on train and test\n    f1_score_train = metrics.f1_score(y_train, y_train_pred)\n    f1_score_test = metrics.f1_score(y_test, y_test_pred)\n    f1sdf = pd.DataFrame([[f1_score_train, f1_score_test, ]], columns=['Training', 'Testing'], index=['F1 Score'])\n\n    #get Area Under the Curve (AUC) for ROC Curve on train and test\n    roc_auc_score_train = metrics.roc_auc_score(y_train, y_train_pred)\n    roc_auc_score_test = metrics.roc_auc_score(y_test, y_test_pred)\n    rocaucsdf = pd.DataFrame([[roc_auc_score_train, roc_auc_score_test, ]], columns=['Training', 'Testing'], index=['ROC AUC Score'])\n\n    #get Area Under the Curve (AUC) for Precision-Recall Curve on train and test\n    precision_train, recall_train, thresholds_train = metrics.precision_recall_curve(y_train, y_train_pred_proba[:,1])\n    precision_recall_auc_score_train = metrics.auc(recall_train, precision_train)\n    precision_test, recall_test, thresholds_test = metrics.precision_recall_curve(y_test,y_test_pred_proba[:,1])\n    precision_recall_auc_score_test = metrics.auc(recall_test, precision_test)\n    precrecaucsdf = pd.DataFrame([[precision_recall_auc_score_train, precision_recall_auc_score_test]], columns=['Training', 'Testing'], index=['Precision Recall AUC Score'])\n\n    #calculate the confusion matrix \n    #print('tn, fp, fn, tp')\n    confusion_matrix_test = pd.crosstab(y_test, y_test_pred, rownames=['Actual'], colnames=['Predicted'])\n\n    #display confusion matrix in a heatmap\n    f, axes = plt.subplots(1, 2, figsize=(20, 8))\n    hmap = sns.heatmap(confusion_matrix_test, cmap='YlGnBu', annot=True, fmt=\".0f\", ax=axes[0], )\n    hmap.set_xlabel('Predicted', fontsize=15)\n    hmap.set_ylabel('Actual', fontsize=15)\n\n    #plotting the ROC Curve and Precision-Recall Curve\n    fpr, tpr, threshold = metrics.roc_curve(y_test,y_test_pred_proba[:,1])\n    plt.plot(fpr, tpr, marker='.', label='ROC Curve')\n    plt.plot(recall_test, precision_test, marker='.', label='Precision Recall Curve')\n    plt.axes(axes[1])\n    plt.title(type(model).__name__, fontsize=15)\n    # axis labels\n    plt.xlabel('ROC Curve - False Positive Rate \\n Precision Recall Curve - Recall', fontsize=15)    \n    plt.ylabel('ROC Curve - True Positive Rate \\n Precision Recall Curve - Precision', fontsize=15)\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n\n    #concatenating all the scores and displaying as single dataframe\n    consolidatedDF= pd.concat([accdf, precdf,recdf,f1sdf, rocaucsdf, precrecaucsdf])\n\n    printmd('**Confusion Matrix**', color='brown')\n    display_side_by_side([confusion_matrix_test, consolidatedDF])\n    \n    return confusion_matrix_test, consolidatedDF","d8f4162e":"X = bank_enc.loc[:, bank_enc.columns != 'Target']\ny = bank_enc['Target']","17bae382":"printmd('**As \"Personal Loan\" attribute is imbalanced, STRATIFYING the same to maintain the same percentage of distribution**', color='brown')\nX_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size =.30, random_state=1)\n\nprintmd('**Training and Testing Set Distribution**', color='brown')\n\nprint(f'Training set has {X_train.shape[0]} rows and {X_train.shape[1]} columns')\nprint(f'Testing set has {X_test.shape[0]} rows and {X_test.shape[1]} columns')\n\nprintmd('**Original Set Target Value Distribution**', color='brown')\n\nprint(\"Original Personal Loan '1' Values    : {0} ({1:0.2f}%)\".format(len(bank_enc.loc[bank_enc['Target'] == 1]), (len(bank_enc.loc[bank_enc['Target'] == 1])\/len(bank_enc.index)) * 100))\nprint(\"Original Personal Loan '0' Values   : {0} ({1:0.2f}%)\".format(len(bank_enc.loc[bank_enc['Target'] == 0]), (len(bank_enc.loc[bank_enc['Target'] == 0])\/len(bank_enc.index)) * 100))\n\nprintmd('**Training Set Target Value Distribution**', color='brown')\n\nprint(\"Training Personal Loan '1' Values    : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])\/len(y_train)) * 100))\nprint(\"Training Personal Loan '0' Values   : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])\/len(y_train)) * 100))\n\nprintmd('**Testing Set Target Value Distribution**', color='brown')\nprint(\"Test Personal Loan '1' Values        : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])\/len(y_test)) * 100))\nprint(\"Test Personal Loan '0' Values       : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])\/len(y_test)) * 100))\n","379f6ece":"# We do upsampling only from the train dataset to preserve the sanctity of the test data\ny_train_0 = y_train[y_train == 0]\n\nextra_samples = y_train[y_train == 1].sample(10000,replace = True, random_state=1).index # Generate duplicate samples\ny_train = y_train.append(y_train.loc[extra_samples])  # use the index of the duplicate samples to append to the y_train\n\nextra_samples = X_train.loc[extra_samples]   # use the same index to generate duplicate rows in X_train\nX_train = X_train.append(extra_samples)  # append these duplicate rows to X_train","aecc7774":"printmd('**Training Set Target Value Distribution**', color='brown')\n\nprint(\"Training Personal Loan '1' Values    : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])\/len(y_train)) * 100))\nprint(\"Training Personal Loan '0' Values   : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])\/len(y_train)) * 100))\n\nprintmd('**Testing Set Target Value Distribution**', color='brown')\nprint(\"Test Personal Loan '1' Values        : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])\/len(y_test)) * 100))\nprint(\"Test Personal Loan '0' Values       : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])\/len(y_test)) * 100))","bd694fba":"#combine them back for resampling\ntrain_data = pd.concat([X_train, y_train], axis=1)\ntrain_data.shape\n\n# separate minority and majority classes\nnegative = train_data[train_data.Target==0]\npositive = train_data[train_data.Target==1]\n\ndf_majority_downsampled = resample(negative,\n replace=False, # sample without replacement\n n_samples=20000, # match number in minority class\n random_state=1) # reproducible results\n# combine minority and downsampled majority\ndownsampled = pd.concat([positive, df_majority_downsampled])\n# check new class counts\ndownsampled.Target.value_counts()\nX_train = downsampled.loc[:, downsampled.columns != 'Target']\ny_train = downsampled['Target']","c3f9add4":"printmd('**Training Set Target Value Distribution**', color='brown')\n\nprint(\"Training Personal Loan '1' Values    : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])\/len(y_train)) * 100))\nprint(\"Training Personal Loan '0' Values   : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])\/len(y_train)) * 100))\n\nprintmd('**Testing Set Target Value Distribution**', color='brown')\nprint(\"Test Personal Loan '1' Values        : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])\/len(y_test)) * 100))\nprint(\"Test Personal Loan '0' Values       : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])\/len(y_test)) * 100))","647b0a65":"logRegModel = LogisticRegression(n_jobs=-1)\ncmLR, dfLR = Modelling_Prediction_Scores(logRegModel, X_train, X_test, y_train, y_test)","b2004922":"gnb = GaussianNB()\ncmNB, dfNB = Modelling_Prediction_Scores(gnb, X_train, X_test, y_train, y_test)","ccaa0e74":"#plot the f1-scores for different values of k for a model and see which is optimal\ndef Optimal_k_Plot(model, X_train, X_test, y_train, y_test):\n    # creating odd list of K for KNN\n    myList = list(range(3,20))\n\n    # subsetting just the odd ones\n    klist = list(filter(lambda x: x % 2 != 0, myList))\n    # empty list that will hold accuracy scores\n    scores = []\n\n    # perform accuracy metrics for values from 3,5....19\n    for k in klist:        \n        model.n_neighbors = k\n        model.fit(X_train, y_train)\n        # predict the response\n        y_test_pred = knn.predict(X_test)        \n        test_score= metrics.accuracy_score(y_test, y_test_pred)\n        scores.append(test_score)\n\n    # determining best k\n    optimal_k = klist[scores.index(max(scores))]\n    print(\"The optimal number of neighbors is %d\" % optimal_k)\n\n    import matplotlib.pyplot as plt\n    # plot misclassification error vs k\n    plt.plot(klist, scores)\n    plt.xlabel('Number of Neighbors K')\n    plt.ylabel('Score')\n    plt.show()","6beea24b":"knn = KNeighborsClassifier(n_jobs=-1)\nOptimal_k_Plot(knn, X_train, X_test, y_train, y_test)","6284cb3d":"knn = KNeighborsClassifier(n_neighbors=19, n_jobs=-1)\ncmKNN, dfKNN = Modelling_Prediction_Scores(knn, X_train, X_test, y_train, y_test)","ade1fd90":"lr = LogisticRegression(C=0.1) \nlrDf, lrCm = fit_n_score(lr, X_train, X_test, y_train, y_test)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknnDf, knnCm = fit_n_score(knn, X_train, X_test, y_train, y_test)\n\nnb = GaussianNB()\nnbDf, nbCm = fit_n_score(nb, X_train, X_test, y_train, y_test)\n\nsvm = SVC(gamma='auto')\nsvmDf, svmCm = fit_n_score(svm, X_train, X_test, y_train, y_test)\n\nresult1 = pd.concat([lrDf,knnDf,nbDf,svmDf])","d9bca4d4":"lr = LogisticRegression(C=0.1, class_weight='balanced')  \nlrDf, lrCm = fit_n_score(lr, X_train, X_test, y_train, y_test)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknnDf, knnCm = fit_n_score(knn, X_train, X_test, y_train, y_test)\n\nsvm = SVC(gamma='auto', C=0.1)\nsvmDf, svmCm = fit_n_score(svm, X_train, X_test, y_train, y_test)\n\nresult2 = pd.concat([lrDf,knnDf,svmDf])","d1252b41":"from sklearn.model_selection import GridSearchCV\n\ndef find_best_model(model, parameters):\n    clf = GridSearchCV(model, parameters, scoring='accuracy')\n    clf.fit(X_train, y_train)             \n    print(clf.best_score_)\n    print(clf.best_params_)\n    print(clf.best_estimator_)\n    return clf","7f21db7a":"dTree= DecisionTreeClassifier()\ndTreeDf, dTreeCm = fit_n_score(dTree, X_train, X_test, y_train, y_test)\nresult3  = dTreeDf","2f5b3dce":"parameters = {'criterion':('gini', 'entropy'), 'max_depth':[1, 10], 'max_features':(None,'auto')}\nclf = find_best_model(dTree, parameters)","f2afd23e":"dTree2 = clf.best_estimator_\ndTreeDf, dTreeCm = fit_n_score(dTree2, X_train, X_test, y_train, y_test)\ndTreeDf\nresult4 = dTreeDf","6c4b4a9c":"bagging = BaggingClassifier()\nbaggingDf, baggingCm = fit_n_score(bagging, X_train, X_test, y_train, y_test)\nresult5  = baggingDf","c8dd68da":"bagging = BaggingClassifier(dTree, max_samples=0.1)\nbaggingDf, baggingCm = fit_n_score(bagging, X_train, X_test, y_train, y_test)\nresult5  = baggingDf","942f89bd":"parameters = {'n_estimators': [5,50]}\nclf = find_best_model(bagging, parameters)","be9f1382":"bagging2 = clf.best_estimator_\nbaggingDf, baggingCm = fit_n_score(bagging2, X_train, X_test, y_train, y_test)\nresult6  = baggingDf","ba011692":"rf = RandomForestClassifier()\nrfDf, rfgCm = fit_n_score(rf, X_train, X_test, y_train, y_test)\nresult7  = rfDf","3fde980e":"rf2 = RandomForestClassifier(n_estimators=150, max_depth=15)\nrfDf, rfCm = fit_n_score(rf2, X_train, X_test, y_train, y_test)\nresult8  = rfDf","0fad1aa7":"ab = AdaBoostClassifier()\nabDf, abCm = fit_n_score(ab, X_train, X_test, y_train, y_test)\nresult9  = abDf","193a70fa":"dTree3= DecisionTreeClassifier(max_depth=5)","2abc8749":"ab = AdaBoostClassifier(dTree3)\nabDf, abCm = fit_n_score(ab, X_train, X_test, y_train, y_test)\nresult9  = abDf","0b9e71ce":"gb = GradientBoostingClassifier()\ngbDf, gbCm = fit_n_score(gb, X_train, X_test, y_train, y_test)\nresult10 = gbDf","5e0fab25":"gb = GradientBoostingClassifier(learning_rate=0.2, max_depth=7)\ngbDf, gbCm = fit_n_score(gb, X_train, X_test, y_train, y_test)\nresult11 = gbDf","8270a825":"display_side_by_side([result8, result11])","44c4e324":"#### Observations\n1. No skewness for **day** attribute as it is close to zero.\n2. Minimal skewness for **age** attribute as it is less than 1.\n3. **balance** is highly skewed as there are outliers on both higher end and negative side.\n4. **duration**, **campaign**, **pdays** are highly skewed towards right.\n5. **previous** is very highly skewed with a value of 41.8 as there lot of \"0\", and there are few records with large values i.e there are customers who were several times before this campaign.\n","9cf9fe8f":"### Label encoding binary attributes","dc3a1106":"# Read the data as a data frame","dc93b876":"## a. Shape of the data","d539c5d2":"#### Training accuracy is lesser compared to Test accuracy. Let us try using a simple decision tree as the estimator","43c9ac6a":"### Try improving the above model as it is overfitting","b2fe4bb8":"#### Observations\nCustomers with **negative balance** have credit defaults.","a1869e62":"### Find the best model using the gridsearch","83ef263a":"### Conclusion:\n\nAll the classifiers have performed similar to the dataset. If we consider **Accuracy** as the deciding metrics, then **Ensemble Classifiers** have performed fairly well with **Random Forest** and **Gradient Boosting** coming slightly on top.\n\nHowever there are still a lot of tuning that can be done to improve the models.","1224d649":"### Dropping redundant features as the information will be obtained from other classes","5af21a9d":"### Removing Outliers","ae72f4ac":"#### Observations - Training Accuracy is improved compared to last one and Training and Testing accuracies are close.","97cf1da1":"#### Observations\nHighest number of contacts are made for **other** outcomes for the previous campaign.\nData is ditributed almost equally for **success** and **failure** outcomes for the previous campaign","1ad4fa1d":"### ADA Boosting","8dae7f6e":"## d. 5 point summary of numerical attributes","3bbaf250":"## END","ca7bc80b":"#### Observations - Training Accuracy is improved compared to last one and Training and Testing accuracies are close.","a755fca4":"## Ensemble Classifiers","420a8ca5":"#### Observations\n1. Customers who are **retired** have highest yearly balance.\n2. Customers with **management** or **self-employed** jobs also have higher yearly balance.\n3. Customers with **blue-collar**, **admin**, **services** jobs have lower yearly balance.\n4. Except customers who are **unemployed**, all others have higher term deposit subscription ratio.","ed06d7ba":"#### Observations\nAs it is obvious, **retired** customers have highest age and **students** have lowest age. However all the other jobs have a mean age of around 40.\n\n**retired** and **housemaid** are the jobs where customers with slightly higher age have availed term deposit subscription.","b672e815":"### Convert the numeric data to same scale so variables which are of different units are given equal importance","d2b044bd":"### One-hot encoding all the categorical attributes","694f709d":"## Data Preprocessing","6f9e2f8d":"### Downsampling majority class to 20000 records to improve performance","6243fc3b":"### KNN","9499ddc2":"#### Observations\n\nThe above result **result8** seems a better model so far as training and testing accuracy matches","bc35f877":"#### The above model overfits the training the set, let us try to fix this","e6eb1c3c":"### Dropping records with unknown education","5a842357":"### Split the data into training and test set in the ratio of 70:30 respectively","2f42a4ae":"### Applying log transformation to reduce the skewness","d803d04a":"### Naive Bayes","a5e2ca81":"## b. Data type of each attribute","ee403fa5":"#### Observations - Training and Testing accuracy have improved a little. ","391c1eee":"## e. Checking the presence of outliers","bd970654":"## Different values of each categorical attributes and their distributions","dab4e32b":"#### Let us try to improve the training accuracy to match the testing","654d918e":"### Upsampling minority class to add another 10000 records","836f94e0":"### Gradient Boosting","1ce9a5ad":"#### Observations\n\n1. No outliers for **day** attribute as it is close to zero.\n2. Few outliers for **age** attribute.\n3. Lot of outliers for **balance** **duration**, **campaign**, \n4. There are no min,Q1,Q2,Q3,max for **pdays** and **previous** as data is highly imbalanced with \"-1\" and \"0\" respectively. Hence many values are considered outliers.","937c762c":"#### Observations\nCustomers with no housing loans have higher yearly balance and noticeably having higher term deposit subscriptions.","93aefdd2":"### Iteration-1","94f3e3f8":"# Perform basic EDA","45a84e9a":"### Iteration-2","edc232b3":"### Dropping duration, default and day attributes\n**duration** cannot be known before a call is made. Only at the end\nof the call duration is known and as is target. So this feature is removed as recommended\n\n**default** attribute is highly skewed with 98.19% \"No\" values and hence it will no add any value in prediction\n\n**day** does not have any significant impact on the target","1c9b6a3b":"### RandomForest","19f1d8e2":"#### Observations\n\n1. All the classifiers have lower values for Precision, Recall and F1-Score. \n2. Logistic Regression have improved on Recall but a let down on Precision and hence low F1-score.\n2. KNN has overfit on training data.\n3. Support Vector Machine having a good Accuracy of 88% and better F1-score (0.42) compared to other classifiers ","7466d048":"#### Observations\n\n1. All the classifiers have lower values for Precision, Recall and F1-Score. \n2. KNN seems to perform well on training set but poorly on test sets.\n3. Support Vector Machine having a good Accuracy of 88% and better F1-score (0.436) compared to other classifiers ","d007d424":"## c. Checking the presence of missing values","6edc8fc4":"#### Observations\n\n1. **age**: It is evenly distributed with few outliers. \n2. **balance**. Evenly distributed around zero as there are many customers with negative balance. There are also few customers with very large balance.\n3. **day**: It is evenly distributed. \n4. **duration**: It is skewed towards right and there are many outliers and few with very large values.\n5. **campaign**: It is skewed towards right as there are outliers with high values.\n6. **pdays**: There are many \"-1\". There are many outliers with large values as well and hence highly skewed.\n6. **previous**: There are many \"0\" which means many customers were contacted for the first time. There are many outliers with large values as well and hence highly skewed.","75c08e9d":"## Bivariate Analysis","9b5f2178":"### Find the best model using the gridsearch","db61e8d1":"### Dropping records with unknown jobs","458aa2f1":"#### Observations\n1. **Target** attribute is highly imbalanced with large \"No\" (88.3%) and less \"Yes\" (11.7%). This means lot of customers did not subscribe to the Term deposit.\n2. **default** attribute is highly imbalanced with large \"No\" (98.2%) and less \"Yes\" (1.8%). This means only few customers have credit in default.\n3. **loan** attribute is highly imbalanced with large \"No\" (83.98%) and less \"Yes\" (16.02%). This means only few customers have personal loan.\n4. **housing** is fairly distributed with 55.58% of customers who availed housing loan and 44.42% of customers who did not.\n5. **martial** - there are several customers who married (60.19%) and fairly good amount of customer who are single (28.29%) and fewer divorcee (11.52%)\n6. **contact** - it is natural in today's world that lot of customers use \"Cellular\" (64.77%) as comminucation type with telephone being the lowest. \n7. **education** - there are lot of customers who have completed secondary (51.32%) and tertiary (29.42%) education levels with few customers with only primary education. This means we have lot of customers who educated well. \n8. **poutcome** - we have little information on the outcome of the previous marketing campaign as there are lot of unknown (81.75%) and less information on whether campaign is successful or a failure.\n9. **job** - data is faily distributed with different job types, but there are several customers with \"blue-collar\", \"management\" and 'technician' jobs\n10. **month** - There are several customers who have been last contacted in the month of May (30.45%). Least contacted months are March, September, October and December. Otherwise customers have been last contacted fairly in remaining months.\n","5eceea30":"#### we have approximately 60%-40% distribution of 0s and 1s respectively which is fairly better than 88%-12% distribution we had initially","87e7752e":"## Multivariate Analysis","f4449d4c":"#### Observations\nCustomers have higher contacts during the month of July and August and lesser contacts during January, September, October","54c2c445":"#### Observations\nHigher the duration of last contact, higher is the subscription.","391737a9":"## Standard Classifiers","39461101":"#### Observations\n\n1. Type-I Error (FP) = 1853\n2. Type-II Error (FN) = 662\n3. **Accuracy** differs significantly between traing and test sets.\n4. ROC Curve looks slightly good but Precision-Recall curve does not look good with AUC score of 0.32. \n5. Looking at the values of Precision(0.26), Recall(0.48) and AUC(0.32), this does not looks like a good model. ","a631b7ab":"#### Observations\n\n1. **Numeric Attributes**: Having int64 as dtype\n\n    a. age    \n    b. balance    \n    c. day    \n    d. duration    \n    e. campaign    \n    f. pdays    \n    g. previous\n    \n2. **Categorical Attributes**: Having object as dtype\n\n    a. job    \n    b. marital    \n    c. education    \n    d. default    \n    e. housing    \n    f. loan    \n    g. contact    \n    h. month    \n    i. poutcome    \n    j. target - (Binary)\n","5c97625b":"#### Observations\nAs per the plot above, all the attributes are scattered randomly.\nThere are no correlation between any of the numeric attributes except a partial correlation between **pdays** and **previous**","388a0ae0":"#### Observations\n1. Customers with **tertiary** education have higher yearly balance.\n2. Customers with **primary** education have higher yearly balance as compared to those with **secondary** education, which is unusual.\n2. Customers from all **education** classes have higher term deposit subscription ratio.","17acd6dc":"#### Observations\n\n**result4** is better than **result3** as have used the best model using grid search with improvements in **Accuracy (86%)**, **Precision**, **Recall**, **F1-score** values\n","814d2184":"#### Observations\n\n1. Type-I Error (FP) = 2007\n2. Type-II Error (FN) = 712\n3. **Accuracy** is almost equal between train and test sets around 78%.\n4. ROC Curve looks good with AUC of 0.67 and Precision-Recall curve does not look good with AUC score of 0.30. \n5. Looking at the values of Precision(0.26), Recall(0.52) and AUC(0.30), this does not looks like a good model. ","ee2182e5":"# Import the necessary libraries","5d427227":"#### We have reduced the overfitting to make sure Training and Testing Accuracy are close","8970b015":"## Distribution of categorical attributes with respect to Target attribute","f73a4049":"#### Observations\nCustomers with no personal loans have higher yearly balance and noticeably having higher term deposit subscriptions.","9b22869f":"### Measuring the skewness of numeric attributes","b2b311c7":"#### Observations\nCustomers with communication type as **telephone** are contacted more number of times during the current campaign, however ratio of term deposit subscription is lower. ","8058f378":"### Logistic Regression","20fb6de0":"#### Observations\n1. Customers who are **married** have highest yearly balance, followed by customers who are **single** and **divorced**.\n2. Customers from all **marital** classes have higher term deposit subscription ratio with **married** being slightly higher.","e404b42e":"### Bagging","bdb0a6fb":"## Profiling of Data","1978d339":"#### Observations\n\n1. Type-I Error (FP) = 1131\n2. Type-II Error (FN) = 710\n3. **Accuracy** is high, above 84%. \n4. Though **Accuracy** is higher, we need to look at Precision, Recall and AUC under Precision-Recall Curve since the target variable is imbalaced. \n5. ROC Curve looks slightly good but Precision-Recall curve does not look good with AUC score of 0.38. \n6. Looking at the values of Precision(0.36), Recall(0.47) and AUC(0.38), this does not looks like a good model. \n","1a726bad":"### Decision Tree","5b241632":"#### Observations\n\n1. **age**: It is evenly distributed as mean(40.9) and median(39) are close, but there are few outliers. \n2. **balance**. Evenly distributed around zero as there are many customers (3766) with negative balance. There are also customers with very high balance. Hence right skewed with mean(1362.27) greater than median(448).\n3. **day**: It is evenly distributed. \n4. **duration**: It is skewed towards right as mean(258.16) is greater than median(180) and there are many outliers and few with very large values.\n5. **campaign**: Evenly distributed when you consider quartiles Q1, Q2 and Q3. However it is skewed towards right as there are outliers with high values.\n6. **pdays**: There are many \"-1\" (#36954) as can be seen from min, Q1,Q2,Q3 values. This could be mistyped values. There are many customers who were contacted month after the last contact as can be seen from the mean (40 days) and there are many outliers with large values as well. \n6. **previous**: There are many \"0\" (#36954) as can be seen from min,Q1,Q2,Q3 values, which means many customers were contacted for the first time. There are many outliers with large values as well and hence highly skewed.","5eb7eed7":"## Distributions of Numeric Attributes","121738b7":"## Measure of skewness ","4d508b9e":"##### Observations\n\nAs per the data, customers who did not subscribe the **Term Deposit** are very high. The same is shown across different values of the categorical attributes\nThe term deposit subscription is slightly higher only when there was a successfull outcome of the previous marketing campaign. However since the total successful campaign is very less already, we can ignore this.\n","b4aa8e0a":"#### Observations\nCustomers who are **retired**, **students**, **unemployed** have been contact less number of times"}}