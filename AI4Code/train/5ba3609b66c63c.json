{"cell_type":{"9ebc8b4b":"code","7cf82ebd":"code","dc54f26c":"code","9def2fd0":"code","53c5ab93":"code","4aaa5ab4":"code","80b283dc":"code","f2b15337":"code","d39e35ca":"code","bbde7769":"code","0a124843":"code","7a4bfded":"code","ba5b23d7":"code","0bdab62e":"code","80939e98":"code","d65c181f":"code","3e00ed15":"code","6574b0d7":"code","0fa4f7c4":"code","8ad5a248":"code","70eb6844":"code","2cd3cc88":"code","b23df9dd":"code","99911b73":"code","affb4cd9":"code","8f846c1d":"code","3897b4d9":"code","ad15fa83":"code","03f43388":"code","6c6fa518":"code","fe511f48":"code","fcf4f3f1":"code","7daf783d":"code","d8e5b4b4":"code","ec416ee6":"code","1c83c052":"code","d65eab64":"code","4d11366d":"code","d9a72ecc":"code","75941924":"code","63d289cf":"code","caa5e298":"code","f279a01b":"code","4e472b8b":"code","f1797971":"markdown","fc7c56ac":"markdown","88ee7014":"markdown","4fae17ad":"markdown","9757e42f":"markdown","c5daa085":"markdown","79db2804":"markdown","364e8b38":"markdown","586a07bd":"markdown","8ef04829":"markdown","bc5524e1":"markdown","0cdf765f":"markdown","84ddd42f":"markdown","08b0997e":"markdown","50c999b6":"markdown","c07b54a2":"markdown","c8cce00b":"markdown","0130db93":"markdown","3177e28a":"markdown","1af6fec0":"markdown","dbfc5e49":"markdown","ea3c32f4":"markdown","9352a47f":"markdown","c9dc6c0d":"markdown","5338e761":"markdown","a1e38fc9":"markdown","c5af75bd":"markdown","55a0d6ae":"markdown","9d7797db":"markdown","99d0853c":"markdown","00970483":"markdown","d7b40e8b":"markdown","eab7c2fb":"markdown","8b580cfd":"markdown","308cbfd3":"markdown","0f801528":"markdown","5b0f2387":"markdown","13fb3bb5":"markdown","2e70c69b":"markdown","7faee6e0":"markdown"},"source":{"9ebc8b4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm, skew\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cf82ebd":"train = pd.read_csv(\"\/kaggle\/input\/autompg-dataset\/auto-mpg.csv\")","dc54f26c":"train.sample(5)","9def2fd0":"print(train.info())","53c5ab93":"train[train[\"horsepower\"] == \"?\"]","4aaa5ab4":"train.describe().T","80b283dc":"train[\"horsepower\"] = train[\"horsepower\"].replace(\"?\", np.NaN).astype(\"float64\")","f2b15337":"train_corr=train.corr().abs().unstack().sort_values(kind = \"quiksort\", ascending = False).reset_index()\n\ntrain_corr.rename(columns = {\"level_0\": \"Feature A\",\n                            \"level_1\": \"Feature B\",\n                            0:\"Correlation Coefs.\"}, inplace = True)\n\ntrain_corr[train_corr[\"Feature A\"] == \"horsepower\"].style.background_gradient(cmap = \"coolwarm\")","d39e35ca":"train.groupby(['displacement'], sort = False)[\"horsepower\"].apply(lambda x: x.fillna(x.mean()))\ntrain['horsepower'] = train.groupby(['cylinders'], sort=False)['horsepower'].apply(lambda x: x.fillna(x.mean()))","bbde7769":"train.isna().sum()","0a124843":"numerical_feat = train.select_dtypes(exclude = \"object\")\ncategorical_feat = train.select_dtypes(include = \"object\")\n\nprint(\"Numeric Features are   : \", *numerical_feat)\nprint(\"Categoric Features are : \", *categorical_feat)","7a4bfded":"for column in numerical_feat.columns:\n    plt.figure(figsize = (8,5))\n    sns.distplot(train[column], fit = norm)\n    plt.show()","ba5b23d7":"train[\"origin\"] = train[\"origin\"].astype(str)\ntrain[\"cylinders\"] = train[\"cylinders\"].astype(str)\n\nnumerical_feat = train.select_dtypes(exclude = \"object\")\ncategorical_feat = train.select_dtypes(include = \"object\")","0bdab62e":"def checkSkewness(df):\n  \"\"\"\n   - greater than 1 positive skewness\n   - less than 1 negative skewness\n  \n  \"\"\"\n  skewed_feat = df.apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\n  skewness = pd.DataFrame(skewed_feat, columns = [\"Skew Value\"])\n\n  return skewness.style.background_gradient(cmap='summer')","80939e98":"checkSkewness(numerical_feat)","d65c181f":"skew_feats = [\"weight\", \"displacement\",\"horsepower\"]\n\ntrain[skew_feats] = np.log1p(train[skew_feats])\n\n\n# and check again\ncheckSkewness(train[skew_feats])","3e00ed15":"categorical_feat.sample(5)","6574b0d7":"train[\"car name\"].value_counts()","0fa4f7c4":"train = train.drop(\"car name\", axis = 1)","8ad5a248":"train[\"cylinders\"].value_counts(normalize = True)","70eb6844":"plt.figure(figsize = (8,6))\nsns.countplot(data = train, x = \"cylinders\")\nplt.title(\"Cylinders\")\nplt.show()","2cd3cc88":"sns.displot(data = train, x = \"mpg\", hue = \"origin\",kind=\"kde\");\nplt.title(\"Kernel Density Estimation of MPG vs ORIGIN\")\nplt.xlabel(\"Miles Per Gallon\")\nplt.show()","b23df9dd":"fig, ax = plt.subplots(figsize=(8, 6))\n\nfig = sns.boxplot(x='origin', y=\"mpg\", data=train)\nplt.axhline(train.mpg.mean(),color='r',linestyle='dotted',linewidth=3)\n\nplt.show()","99911b73":"train_corr = train.corr()\n\nmask = np.triu(np.ones_like(train_corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(15, 6))\n\nsns.heatmap(train_corr, annot=True,fmt='.2f',mask=mask, cmap=\"coolwarm\", ax=ax);","affb4cd9":"train = pd.get_dummies(train)","8f846c1d":"train.head(5)","3897b4d9":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression,Lasso,ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb","ad15fa83":"x = train.drop('mpg', axis = 1)\ny = train[\"mpg\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .33, random_state = 42)","03f43388":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","6c6fa518":"lr_reg = LinearRegression()\n\nlr_reg.fit(X_train, y_train)\ny_predict_reg = lr_reg.predict(X_test)\n\nlr_rmse = np.sqrt(mean_squared_error(y_test, y_predict_reg))\nlr_rmse","fe511f48":"rf_reg = RandomForestRegressor().fit(X_train, y_train)\n\ny_pred = rf_reg.predict(X_test)\n\nrf_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nrf_rmse","fcf4f3f1":"lasso_reg = Lasso().fit(X_train, y_train)\n\ny_pred = lasso_reg.predict(X_test)\n\nlasso_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nlasso_rmse","7daf783d":"eNet = ElasticNet().fit(X_train, y_train)\n\ny_pred = eNet.predict(X_test)\n\neNet_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\neNet_rmse","d8e5b4b4":"lgb_reg = lgb.LGBMRegressor().fit(X_train, y_train)\n\ny_pred = lgb_reg.predict(X_test)\n\nlgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nlgb_rmse","ec416ee6":"model_names = [\"LASSO\",\"LGBM\",\"RANDOM FR\",\"LINEAR\",\"ELASTICNET\"]\n\nmodels = {\"Model\":model_names,\n          \"RMSE\":[lasso_rmse, lgb_rmse, rf_rmse,lr_rmse,eNet_rmse]}\n\nmodel_performance = pd.DataFrame(models)\n\nmodel_performance.sort_values(by = \"RMSE\",kind='quicksort', ascending=True).style.background_gradient(cmap='summer')","1c83c052":"n_estimators = [100, 200, 500, 700]\nmax_depth = np.arange(10, 100, 10)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nparams_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = RandomForestRegressor()\n\nrf_grid_search = RandomizedSearchCV(estimator = rf, param_distributions = params_grid,\n                                    n_iter = 100,scoring = \"neg_root_mean_squared_error\",cv = 5,\n                                    n_jobs = -1, verbose = 2, refit = True).fit(X_train, y_train)\n","d65eab64":"y_pred = rf_grid_search.predict(X_test)\n\nrf_tunned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(\"Tunned RF RMSE: \",rf_tunned_rmse )\nprint(\"Best Parameters : \",rf_grid_search.best_estimator_)","4d11366d":"lgbm_params = {\"learning_rate\": [0.001,0.01,0.1,1,1.5],\n               \"n_estimators\": [300,500,700,1000],\n               \"max_depth\": [3, 5, 8, 10],\n              \"num_leaves\":[32,64,128],\n              \"min_data_in_leaf\":[100,1000]}\n\n\nlgbm = lgb.LGBMRegressor()\n\nlgbm_grid_search = GridSearchCV(estimator = lgbm, param_grid = lgbm_params,\n                              scoring = \"neg_root_mean_squared_error\",cv = 5,\n                              n_jobs = -1, verbose = 2, refit = True).fit(X_train, y_train)","d9a72ecc":"y_pred = lgbm_grid_search.predict(X_test)\n\nlgbm_tunned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(\"Tunned LGBM RMSE: \",lgbm_tunned_rmse )\nprint(\"Best Parameters : \",lgbm_grid_search.best_estimator_)","75941924":"alphas = np.logspace(-4, -0.5, 30)\n\ntuned_params = {\"alpha\":alphas}\n\nlasso = Lasso()\n\ngrid_lasso = GridSearchCV(lasso, param_grid=tuned_params,\n                    cv = 5, scoring = \"neg_root_mean_squared_error\",\n                    refit = True)\ngrid_lasso.fit(X_train, y_train)","63d289cf":"y_pred = grid_lasso.predict(X_test)\n\nlasso_tunned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(\"Tunned LASSO RMSE: \",lasso_tunned_rmse )\nprint(\"Best Parameters : \",grid_lasso.best_estimator_)","caa5e298":"alphas = np.logspace(-4, -0.5, 30)\n\nparams_grid = {\"alpha\":alphas,\n              \"l1_ratio\":np.arange(0.0,1.0,0.05)}\n\neNet = ElasticNet()\n\neNet_grid = GridSearchCV(eNet, params_grid, cv = 5,\n                   scoring = \"neg_root_mean_squared_error\", refit = True)\n\neNet_grid.fit(X_train, y_train)","f279a01b":"y_pred = eNet_grid.predict(X_test)\n\nelastic_tunned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(\"Tunned LASSO RMSE: \",elastic_tunned_rmse )\nprint(\"Best Parameters : \",eNet_grid.best_estimator_)","4e472b8b":"tunned_rmses = [lasso_tunned_rmse, lgbm_tunned_rmse, rf_tunned_rmse,np.NaN,elastic_tunned_rmse]\n\nmodel_performance[\"Tunned RMSE\"] = tunned_rmses\n\nmodel_performance.sort_values(by = \"Tunned RMSE\",kind='quicksort', ascending=True).style.background_gradient(cmap='summer')\n","f1797971":"![](https:\/\/media.giphy.com\/media\/mIMsLsQTJzAn6\/giphy.gif)\n\n## **Introduction**\n\nI am trying to gain experience and improve day by day in analysis and model training. This was my aim while preparing this notebook.   \nWhat's in this notebook?\n- Explorating and visualising the data with pandas and seaborn packages\n- Building and tuning couple regression models to get some stable results with sklearn and Light GBM packages\n\n\n\n### Data Set Information:\n\nThis dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute. The original dataset is available in the file \"auto-mpg.data-original\".\n\n### Attribute Information:\n\n**mpg:** continuous\n**cylinders:** multi-valued discrete<br>\n**displacement:** continuous<br>\n**horsepower:** continuous<br>\n**weight:** continuous<br>\n**acceleration:** continuous<br>\n**model year:** multi-valued discrete<br>\n**origin:** multi-valued discrete<br>\n**car name:** string (unique for each instance)<br>\n\n\n\n### So lets start..\n","fc7c56ac":"I don't want to randomly fill in missing data in Horsepower. \nSo I will look at its correlation with other data in the data set.","88ee7014":"## **Random Forest Regressor**","4fae17ad":"## **Linear Regression**","9757e42f":"## Correlation Matrix","c5daa085":"### **Cylinders**","79db2804":"Half of our data set consists of 4-cylinder vehicles","364e8b38":"## **Scaling**","586a07bd":"# L\u0131brary Import","8ef04829":"As I mentioned above, I fixed the mismatch in the **\"horsepower\"** feature.  \n I filled the sign \"?\" with **np.NaN** and float the type.","bc5524e1":"## **Label Encoding**","0cdf765f":"## **ElasticNet**","84ddd42f":"### Let's take a look at our categorical features.","08b0997e":"## Missing Values","50c999b6":"## **LGBM**","c07b54a2":"We see that our data set consists of **398 rows and 9 columns**..\n\nIn the description of the data set, it said that the missing data is only within **hoursepower**. But when we look at the general data set, it shows **no missing data and also it shows the data type of hoursepower as object.**\n\nThe first thing that comes to my mind here is that the missing data is represented by a string \"?\"","c8cce00b":"Let's look at our five random examples","0130db93":"**ElasticNet** shows the biggest improvement after hyperparameter optimization. **LGBM** gives the worst performance after tuning. Sure, larger captive param_grid results could have been much different, but it's a very time-consuming process.  \n\nAs a result, Random Forest Regressor was the best algorithm in Tunned models, and LGBM was the best model in basic models.","3177e28a":"Yes, we handled the missing values!","1af6fec0":"- When we look at their skewness\n    - We see that **Origin** and **Cylinders** are far from normal distribution. But when we look at the data set, these features are actually categorical values. That's why I'm ignoring these two.  I will change them as categoric in a soon\n    - Weight, Displacement, Horsepower features has positive skew, we need deal with it\n    - I'm not gonna handle \"mpg\" because it is target feature\n\nNow let's change the data types of Origin and Cylinders categorically then updates the numerical_feat and categorical_feat variable with new ones","dbfc5e49":"Yes, as stated in the data set description, our missing data is 6. But we will deal with this a little later.","ea3c32f4":"# **Modelling**","9352a47f":"#### **Car Name**","c9dc6c0d":"## **Train-Test Split**","5338e761":"These are the results we got from our basic models.\n\n\nNow let's do hyperparameter optimization and see the change.","a1e38fc9":"## **LGBM**","c5af75bd":"### **Let's look at the distribution of numerical features**\n- so we can see wheather feauture are skewed or not","55a0d6ae":"## **Lasso**","9d7797db":"# **Hyperparameter Tunning**","99d0853c":"Now let's examine the skewness of these features numerically.","00970483":"## **Lasso**","d7b40e8b":"## **ElasticNet**","eab7c2fb":"Now I want to take a look at the distributions of numeric features.    \nFor this, I will divide the features into two separate groups, categoric and numeric.","8b580cfd":"# **EDA and Feature Engineering** ","308cbfd3":"They are better off now","0f801528":"As can be seen from the table, I will apply **Log Transformation** to get the \"Horsepower\", \"Displacement\", \"weight\" features from skewness.","5b0f2387":"## **Random Forest Regressor**","13fb3bb5":"I am removing this feature from the dataset because I got better results when I set up a model without it.","2e70c69b":"Thank you for taking your time and reviewing so far.Please let me know if I did some mistakes, I'm still trying to improve myself! I would be very happy if you give me upvote","7faee6e0":"There is a very high correlation between Weight and Displacement. This means that these two fefatures are almost the same.  \nWe can consider removing either of them from the model."}}