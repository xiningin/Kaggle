{"cell_type":{"2eb7e7b5":"code","c6c3cc41":"code","dd2f298a":"code","5e7d36d2":"code","e4bf7035":"code","ba9070cf":"code","b9178230":"code","f0ed6fcf":"code","b2f3161d":"code","5333123d":"code","f28ac9a2":"code","41696b12":"code","10e2e403":"code","ce78ff87":"code","d59b3ed3":"code","03b2bb3c":"code","5b911b97":"code","74b4c348":"code","3b2a6a47":"code","bd8703c6":"code","8331aee9":"code","64b44907":"code","15250103":"code","6fb8fd9b":"code","7f354cff":"code","2f89f656":"code","dbc3dbf1":"code","d59d6cbc":"code","2c027f53":"markdown","3801f5e9":"markdown","56f373e7":"markdown","7acf8954":"markdown","24a25305":"markdown","f2cf6ea7":"markdown","94f875d6":"markdown","6211024e":"markdown","9a1a89d7":"markdown","b1bf8f3f":"markdown","3f5059cb":"markdown","33872310":"markdown"},"source":{"2eb7e7b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6c3cc41":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport random\n\n# model imports\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# processing imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","dd2f298a":"# fetch the training file\nfile_path_20_percent = '..\/input\/nslkdd\/KDDTrain+_20Percent.txt'\nfile_path_full_training_set = '..\/input\/nslkdd\/KDDTrain+.txt'\nfile_path_test = '..\/input\/nslkdd\/KDDTest+.txt' \n\n#df = pd.read_csv(file_path_20_percent)\ndf = pd.read_csv(file_path_full_training_set)\ntest_df = pd.read_csv(file_path_test)","5e7d36d2":"# add the column labels\ncolumns = (['duration'\n,'protocol_type'\n,'service'\n,'flag'\n,'src_bytes'\n,'dst_bytes'\n,'land'\n,'wrong_fragment'\n,'urgent'\n,'hot'\n,'num_failed_logins'\n,'logged_in'\n,'num_compromised'\n,'root_shell'\n,'su_attempted'\n,'num_root'\n,'num_file_creations'\n,'num_shells'\n,'num_access_files'\n,'num_outbound_cmds'\n,'is_host_login'\n,'is_guest_login'\n,'count'\n,'srv_count'\n,'serror_rate'\n,'srv_serror_rate'\n,'rerror_rate'\n,'srv_rerror_rate'\n,'same_srv_rate'\n,'diff_srv_rate'\n,'srv_diff_host_rate'\n,'dst_host_count'\n,'dst_host_srv_count'\n,'dst_host_same_srv_rate'\n,'dst_host_diff_srv_rate'\n,'dst_host_same_src_port_rate'\n,'dst_host_srv_diff_host_rate'\n,'dst_host_serror_rate'\n,'dst_host_srv_serror_rate'\n,'dst_host_rerror_rate'\n,'dst_host_srv_rerror_rate'\n,'attack'\n,'level'])\n\ndf.columns = columns\ntest_df.columns = columns\n\n# sanity check\ndf.head()","e4bf7035":"# map normal to 0, all attacks to 1\nis_attack = df.attack.map(lambda a: 0 if a == 'normal' else 1)\ntest_attack = test_df.attack.map(lambda a: 0 if a == 'normal' else 1)\n\n#data_with_attack = df.join(is_attack, rsuffix='_flag')\ndf['attack_flag'] = is_attack\ntest_df['attack_flag'] = test_attack\n\n# view the result\ndf.head()","ba9070cf":"# lists to hold our attack classifications\ndos_attacks = ['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm']\nprobe_attacks = ['ipsweep','mscan','nmap','portsweep','saint','satan']\nprivilege_attacks = ['buffer_overflow','loadmdoule','perl','ps','rootkit','sqlattack','xterm']\naccess_attacks = ['ftp_write','guess_passwd','http_tunnel','imap','multihop','named','phf','sendmail','snmpgetattack','snmpguess','spy','warezclient','warezmaster','xclock','xsnoop']\n\n# we will use these for plotting below\nattack_labels = ['Normal','DoS','Probe','Privilege','Access']","b9178230":"# helper function to pass to data frame mapping\ndef map_attack(attack):\n    if attack in dos_attacks:\n        # dos_attacks map to 1\n        attack_type = 1\n    elif attack in probe_attacks:\n        # probe_attacks mapt to 2\n        attack_type = 2\n    elif attack in privilege_attacks:\n        # privilege escalation attacks map to 3\n        attack_type = 3\n    elif attack in access_attacks:\n        # remote access attacks map to 4\n        attack_type = 4\n    else:\n        # normal maps to 0\n        attack_type = 0\n        \n    return attack_type","f0ed6fcf":"# map the data and join to the data set\nattack_map = df.attack.apply(map_attack)\ndf['attack_map'] = attack_map\n\ntest_attack_map = test_df.attack.apply(map_attack)\ntest_df['attack_map'] = test_attack_map","b2f3161d":"# view the result\ndf.head()","5333123d":"# use a crosstab to get attack vs protocol\nattack_vs_protocol = pd.crosstab(df.attack, df.protocol_type)\nattack_vs_protocol","f28ac9a2":"# helper function for drawing mulitple charts.\ndef bake_pies(data_list,labels):\n    list_length = len(data_list)\n    \n    # setup for mapping colors\n    color_list = sns.color_palette()\n    color_cycle = itertools.cycle(color_list)\n    cdict = {}\n    \n    # build the subplots\n    fig, axs = plt.subplots(1, list_length,figsize=(18,10), tight_layout=False)\n    plt.subplots_adjust(wspace=1\/list_length)\n    \n    # loop through the data sets and build the charts\n    for count, data_set in enumerate(data_list): \n        \n        # update our color mapt with new values\n        for num, value in enumerate(np.unique(data_set.index)):\n            if value not in cdict:\n                cdict[value] = next(color_cycle)\n       \n        # build the wedges\n        wedges,texts = axs[count].pie(data_set,\n                           colors=[cdict[v] for v in data_set.index])\n        # build the legend\n        axs[count].legend(wedges, data_set.index,\n                           title=\"Flags\",\n                           loc=\"center left\",\n                           bbox_to_anchor=(1, 0, 0.5, 1))\n        # set the title\n        axs[count].set_title(labels[count])\n        \n    return axs   ","41696b12":"'''from mpl_toolkits.mplot3d import Axes3D\n\n# set grid style\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\n\n# axes instance\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig)\n\n# set data\ndf = pd.read_csv(\"\/kaggle\/input\/nslkdd\/KDDTrain+.txt\")\ndf.columns = columns\ndf = df.dropna()\nprint(df.shape)\ndf.head(15)\nis_attack = df.attack.map(lambda a: 0 if a == 'normal' else 1)\ndf['attack_flag'] = is_attack\nprint(\"attack_flag\", df.attack_flag)\nXY = df.drop(df.columns[[2]], axis=1)\nx = df.x\ny = df.y\nz = df.attack_flag\n\n# plot\nax.scatter(x, y, z, c=x, marker='o')\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\nplt.show()'''","10e2e403":"# get the series for each protocol\nicmp_attacks = attack_vs_protocol.icmp\ntcp_attacks = attack_vs_protocol.tcp\nudp_attacks = attack_vs_protocol.udp\n\n# create the charts\nbake_pies([icmp_attacks, tcp_attacks, udp_attacks],['icmp','tcp','udp'])\nplt.show()","ce78ff87":"# get a series with the count of each flag for attack and normal traffic\nnormal_flags = df.loc[df.attack_flag == 0].flag.value_counts()\nattack_flags = df.loc[df.attack_flag == 1].flag.value_counts()\n\n# create the charts\nflag_axs = bake_pies([normal_flags, attack_flags], ['normal','attack'])        \nplt.show()","d59b3ed3":"# get a series with the count of each service for attack and normal traffic\nnormal_services = df.loc[df.attack_flag == 0].service.value_counts()\nattack_services = df.loc[df.attack_flag == 1].service.value_counts()\n\n# create the charts\nservice_axs = bake_pies([normal_services, attack_services], ['normal','attack'])        \nplt.show()","03b2bb3c":"sns.distplot(df['level'])","5b911b97":"sns.jointplot(x='duration', y='level', data=df, kind='scatter')","74b4c348":"# get the intial set of encoded features and encode them\nfeatures_to_encode = ['protocol_type', 'service', 'flag']\nencoded = pd.get_dummies(df[features_to_encode])\ntest_encoded_base = pd.get_dummies(test_df[features_to_encode])\n\n# not all of the features are in the test set, so we need to account for diffs\ntest_index = np.arange(len(test_df.index))\ncolumn_diffs = list(set(encoded.columns.values)-set(test_encoded_base.columns.values))\n\ndiff_df = pd.DataFrame(0, index=test_index, columns=column_diffs)\n\n# we'll also need to reorder the columns to match, so let's get those\ncolumn_order = encoded.columns.to_list()\n\n# append the new columns\ntest_encoded_temp = test_encoded_base.join(diff_df)\n\n# reorder the columns\ntest_final = test_encoded_temp[column_order].fillna(0)\n\n# get numeric features, we won't worry about encoding these at this point\nnumeric_features = ['duration', 'src_bytes', 'dst_bytes']\n\n# model to fit\/test\nto_fit = encoded.join(df[numeric_features])\ntest_set = test_final.join(test_df[numeric_features])","3b2a6a47":"# create our target classifications\nbinary_y = df['attack_flag']\nmulti_y = df['attack_map']\n\ntest_binary_y = test_df['attack_flag']\ntest_multi_y = test_df['attack_map']\n\n# build the training sets\nbinary_train_X, binary_val_X, binary_train_y, binary_val_y = train_test_split(to_fit, binary_y, test_size=0.6)\nmulti_train_X, multi_val_X, multi_train_y, multi_val_y = train_test_split(to_fit, multi_y, test_size = 0.6)","bd8703c6":"#Decision Tree\nclf = DecisionTreeClassifier() #max_depth=10\nclf = clf.fit(binary_train_X, binary_train_y)\nclf_predictions = clf.predict(binary_val_X) # make prediction\ntrain_acc = clf.score(binary_train_X, binary_train_y) # mean acc on train data\nbase_clf_score = accuracy_score(clf_predictions,binary_val_y) # mean acc on test data\nprint(\"Training accuracy is:\", train_acc )\nprint(\"Testing accuracy is:\", base_clf_score)","8331aee9":"#Decision Tree Performance Metrics\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score\nTree_f1 = f1_score(binary_val_y, clf_predictions, average=\"macro\")\nTree_precision = precision_score(binary_val_y, clf_predictions, average=\"macro\")\nTree_recall = recall_score(binary_val_y, clf_predictions, average=\"macro\")\nTree_accuracy = accuracy_score(binary_val_y, clf_predictions)\nprint(\"F1 Score of Decision trees:\", Tree_f1)\nprint(\"Precision of Decision trees:\", Tree_precision)\nprint(\"Recall of Decision trees:\", Tree_recall)\nprint(\"Accuracy of Decision trees:\", Tree_accuracy)","64b44907":"# define the list of models that we want to test\nmodels = [\n    LogisticRegression(max_iter=250),\n    DecisionTreeClassifier(max_depth=10),\n]\n\n# an empty list to capture the performance of each model\nmodel_comps = []\n\n# walk through the models and populate our list\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, binary_train_X, binary_train_y, scoring='accuracy')\n    for count, accuracy in enumerate(accuracies):\n        model_comps.append((model_name, count, accuracy))","15250103":"# a box plot will do well to show us overall performance and the variation in the models.\nresult_df = pd.DataFrame(model_comps, columns=['model_name', 'count', 'accuracy'])\nresult_df.pivot(index='count',columns='model_name',values='accuracy').boxplot(rot=45)","6fb8fd9b":"# a helper function for getting some analytical data about our predictions\ndef add_predictions(data_set,predictions,y):\n    prediction_series = pd.Series(predictions, index=y.index)\n\n    # we need to add the predicted and actual outcomes to the data\n    predicted_vs_actual = data_set.assign(predicted=prediction_series)\n    original_data = predicted_vs_actual.assign(actual=y).dropna()\n    conf_matrix = confusion_matrix(original_data['actual'], \n                                   original_data['predicted'])\n    \n    # capture rows with failed predictions\n    base_errors = original_data[original_data['actual'] != original_data['predicted']]\n    \n    # drop columns with no value\n    non_zeros = base_errors.loc[:,(base_errors != 0).any(axis=0)]\n\n    # idetify the type of error\n    false_positives = non_zeros.loc[non_zeros.actual==0]\n    false_negatives = non_zeros.loc[non_zeros.actual==1]\n\n    # put everything into an object\n    prediction_data = {'data': original_data,\n                       'confusion_matrix': conf_matrix,\n                       'errors': base_errors,\n                       'non_zeros': non_zeros,\n                       'false_positives': false_positives,\n                       'false_negatives': false_negatives}\n    return prediction_data","7f354cff":"# capture our prediction data\nbinary_prediction_data = add_predictions(df,\n                                         clf_predictions,\n                                         binary_val_y)\n\n# create a heatmap of the confusion matrix\nsns.heatmap(data=binary_prediction_data['confusion_matrix'],\n            xticklabels = ['Predicted Normal','Predicted Attack'],\n            yticklabels = ['Actual Normal','Actual Attack'],\n            cmap=\"YlGnBu\",\n            fmt='d',\n            annot=True)","2f89f656":"# dataframe to store incorrect classification\nbinary_prediction_data['errors'].describe()","dbc3dbf1":"# see the standard deviation of the false positives\nbinary_prediction_data['false_positives'].std()","d59d6cbc":"# see the standard deviation of the false negatives\nbinary_prediction_data['false_negatives'].std()","2c027f53":"Data extraction","3801f5e9":"Data profiling\nFirst is a simple table of attack by protocol. In network traffic analysis protocol is a simple tool to create some initial buckets to categorize our data. 'normal' is left in the set at this point as a benchmark.","56f373e7":"Importing libraries","7acf8954":"pd.crosstab - Compute a simple cross tabulation of two (or more) factors","24a25305":"The purpose of this notebook is a basic exploration of the NSL-KDD dataset. Here are the goals of this exploration:\n\n* Gain a basic understanding of the data set\n* Look at how the data set might be used to predict network anomalies or attacks\n* Walk through some fundemental concepts of building machine learning models like Logistic Regression and Decision Tree classifier\n* Compare the performance of selected feature engineering techniques.","f2cf6ea7":"The thing to notice here is the difference in each protocol type. Our initial impression is that protocol may be useful in being able to identify the type of traffic we are observing. Let's see if flag behaves the same way.","94f875d6":"Machine Learning model fitting","6211024e":"Feature engineering - Label encoding. pd.get_dummies is a method that allows us to do a quick one hot encoding on our columns. This takes every value it finds in a single column and makes an individual column for each value, with a 0 or 1 indicating whether that column is 'hot'.\nWe're also going to throw in some basic numeric data: duration, src_bytes, dst_bytes.","9a1a89d7":"Data transformation - The first transformations that we'll want to do are around the attack field. We'll start by adding a column that encodes 'normal' values as 0 and any other value as 1. We will use this as our classifier for a simple binary model that idenfities any attack.","b1bf8f3f":"Analyzing our predictions","3f5059cb":"Wow! Look at how many services are in the attack set! Whereas a huge amount of normal traffic is http, our attack traffic is all over the place.","33872310":"References from Research Papers-\nhttp:\/\/45.113.122.54\/pdfs\/ijrscse\/v2-i3\/7.pdf - Attack cassifications for ML algos\nhttps:\/\/lupinepublishers.com\/computer-science-journal\/fulltext\/detecting-distributed-denial-of-service-ddos-attacks.ID.000110.php"}}