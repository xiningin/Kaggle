{"cell_type":{"791b6f83":"code","6f2de636":"code","71ad0738":"code","3ea7124d":"code","bbe39641":"code","7f66db5a":"code","b3fa69a5":"code","a10c7b4f":"code","63d5b435":"code","ea30a0bf":"code","ade123b9":"code","5204f92c":"code","f00a128e":"code","72cab8d3":"code","1d8e6f94":"code","5dd21181":"code","23576273":"code","01dd4f57":"code","0dcc6b10":"code","0e85a09c":"markdown","3dd56185":"markdown","f9b9378e":"markdown","a0ffcb4b":"markdown","5fb64b67":"markdown","20cfbf79":"markdown","dfc74dd7":"markdown","3b3c2423":"markdown","6b89adc9":"markdown","d3b0d0eb":"markdown","452be3df":"markdown","5ccd600c":"markdown","f2dbdaa8":"markdown","ec892317":"markdown"},"source":{"791b6f83":"!pip install langdetect\n!pip install num2words","6f2de636":"import numpy as np\nimport pandas as pd\nimport gensim\nimport nltk\nimport string\nimport spacy\nimport langdetect\nimport networkx as nx\nimport warnings\nimport math\nimport matplotlib.pyplot as plt\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n\npd.set_option('display.max_colwidth', 0)\n\nfrom spacy import displacy\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom collections import Counter\n\nfrom num2words import num2words\n\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize\nlemmatizer = WordNetLemmatizer()\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction import text\nstop = text.ENGLISH_STOP_WORDS.union([\"book\"])\n\nfrom langdetect import detect\n\nfrom gensim import models\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\n\nfrom IPython.core.display import display, HTML\n\nfrom time import time  # To time our operations\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71ad0738":"from sklearn.decomposition import PCA as sPCA\nfrom sklearn import manifold #MSD\n\ndef show_closest_2d(vecs,word1,word2,word3,word4,n):\n    tops1 = vecs.similar_by_word(word1, topn=n, restrict_vocab=None)\n    tops2 = vecs.similar_by_word(word2, topn=n, restrict_vocab=None)\n    tops3 = vecs.similar_by_word(word3, topn=n, restrict_vocab=None)\n    tops4 = vecs.similar_by_word(word4, topn=n, restrict_vocab=None)\n    \n    #display(HTML(\"<b>%d words most similar to '%s' (%s)<\/b>\" % (n,word, method)))\n   \n    items1 = [word1] + [x[0] for x in tops1]\n    items2 = [word2] + [x[0] for x in tops2]\n    items3 = [word3] + [x[0] for x in tops3]\n    items4 = [word4] + [x[0] for x in tops4]\n    \n    wvecs1 = np.array([vecs.word_vec(wd, use_norm=True) for wd in items1])\n    wvecs2 = np.array([vecs.word_vec(wd, use_norm=True) for wd in items2])\n    wvecs3 = np.array([vecs.word_vec(wd, use_norm=True) for wd in items3])\n    wvecs4 = np.array([vecs.word_vec(wd, use_norm=True) for wd in items4])\n\n    dists1 = np.zeros((len(items1), len(items1)))\n    dists2 = np.zeros((len(items2), len(items2)))\n    dists3 = np.zeros((len(items3), len(items3)))\n    dists4 = np.zeros((len(items4), len(items4)))\n    \n    for i,item1 in enumerate(items1):\n        for j,item2 in enumerate(items1):\n            dists1[i][j] = dists1[j][i] = vecs.distance(item1,item2)\n            \n    for i,item1 in enumerate(items2):\n        for j,item2 in enumerate(items2):\n            dists2[i][j] = dists2[j][i] = vecs.distance(item1,item2)\n            \n    for i,item1 in enumerate(items3):\n        for j,item2 in enumerate(items3):\n            dists3[i][j] = dists3[j][i] = vecs.distance(item1,item2)\n            \n    for i,item1 in enumerate(items4):\n        for j,item2 in enumerate(items4):\n            dists4[i][j] = dists4[j][i] = vecs.distance(item1,item2)\n        \n    mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=0, \n                       dissimilarity=\"precomputed\", n_jobs=1)\n    \n    coords1 = mds.fit(dists1).embedding_\n    coords2 = mds.fit(dists2).embedding_\n    coords3 = mds.fit(dists3).embedding_\n    coords4 = mds.fit(dists4).embedding_\n    \n    plt.figure(num=None, figsize=(8, 4), dpi=1200, facecolor='w', edgecolor='k')\n    \n    plt.subplot(221)\n    plt.tick_params(\n        axis='both',          \n        which='both',      \n        bottom=False,      \n        left=False,         \n        labelbottom=False,\n        labelleft=False)\n\n    lim1 = max([abs(x) for x in coords1[:,0] + coords1[:,1]])\n    plt.xlim([-lim1,lim1])\n    plt.ylim([-lim1,lim1])\n    \n    plt.scatter(coords1[0:,0], coords1[0:,1], color='darkgrey', s=3)\n    plt.scatter(coords1[4:5,0], coords1[4:5,1], color='darkviolet', s=3)\n    \n    for item, x, y in zip(items1[2:], coords1[2:,0], coords1[2:,1]):\n        plt.annotate(item, xy=(x, y), xytext=(-1, 1), textcoords='offset points', \n                     ha='right', va='bottom', color='grey', fontsize=6)\n\n    x0=coords1[0,0]\n    y0=coords1[0,1]\n    plt.annotate(word1, xy=(x0, y0), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='grey', fontsize=6)\n    \n    x1=coords1[1,0]\n    y1=coords1[1,1]\n    plt.annotate(items1[1] , xy=(x1, y1), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='grey', fontsize=6)\n    \n    x4=coords1[4,0]\n    y4=coords1[4,1]\n    plt.annotate(items1[4] , xy=(x4, y4), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='darkviolet', fontsize=6)\n    \n    ax = plt.gca()\n    \n    circle = plt.Circle((x0, y0), 0.1, color='black', fill=False)\n    ax.add_artist(circle)\n    \n    plt.subplot(222)\n    plt.tick_params(\n        axis='both',\n        which='both',\n        bottom=False,      \n        left=False,         \n        labelbottom=False,\n        labelleft=False)\n    \n    lim2 = max([abs(x) for x in coords2[:,0] + coords2[:,1]])\n    plt.xlim([-lim2,lim2])\n    plt.ylim([-lim2,lim2])\n    \n    plt.scatter(coords2[0:,0], coords2[0:,1], color='grey', s=3)\n    plt.scatter(coords2[5:6,0], coords2[5:6,1], color='blue', s=3)\n    plt.scatter(coords2[6:7,0], coords2[6:7,1], color='darkviolet', s=3)\n    plt.scatter(coords2[7:8,0], coords2[7:8,1], color='blue', s=3)\n    \n    for item, x, y in zip(items2[2:], coords2[2:,0], coords2[2:,1]):\n        plt.annotate(item, xy=(x, y), xytext=(-1, 1), textcoords='offset points', \n                     ha='right', va='bottom', color='grey', fontsize=6)\n\n    \n    x0=coords2[0,0]\n    y0=coords2[0,1]\n    plt.annotate(word2, xy=(x0, y0), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='grey', fontsize=6)\n    \n    x1=coords2[1,0]\n    y1=coords2[1,1]\n    plt.annotate(items2[1], xy=(x1, y1), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='grey', fontsize=6)\n    \n    x5=coords2[5,0]\n    y5=coords2[5,1]\n    plt.annotate(items2[5] , xy=(x5, y5), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='blue', fontsize=6)\n    \n    x6=coords2[6,0]\n    y6=coords2[6,1]\n    plt.annotate(items2[6] , xy=(x6, y6), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='darkviolet', fontsize=6)\n    \n    x7=coords2[7,0]\n    y7=coords2[7,1]\n    plt.annotate(items2[7] , xy=(x7, y7), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='blue', fontsize=6)\n    \n    ax = plt.gca()\n    \n    circle = plt.Circle((x0, y0), 0.1, color='black', fill=False)\n    ax.add_artist(circle)\n    \n    plt.subplot(223)\n    plt.tick_params(\n        axis='both',\n        which='both',\n        bottom=False,      \n        left=False,         \n        labelbottom=False,\n        labelleft=False)\n    \n    lim3 = max([abs(x) for x in coords3[:,0] + coords3[:,1]])\n    plt.xlim([-lim3,lim3])\n    plt.ylim([-lim3,lim3])\n    \n    plt.scatter(coords3[0:,0], coords3[0:,1], color='grey', s=3)\n    plt.scatter(coords3[0:1,0], coords3[0:1,1], color='forestgreen', s=3)\n    plt.scatter(coords3[1:2,0], coords3[1:2,1], color='blue', s=3)\n    plt.scatter(coords3[2:3,0], coords3[2:3,1], color='darkviolet', s=3)\n    plt.scatter(coords3[3:4,0], coords3[3:4,1], color='darkviolet', s=3)\n    plt.scatter(coords3[4:5,0], coords3[4:5,1], color='blue', s=3)\n    plt.scatter(coords3[5:6,0], coords3[5:6,1], color='forestgreen', s=3)\n    plt.scatter(coords3[8:9,0], coords3[8:9,1], color='darkorange', s=3)\n    plt.scatter(coords3[10:,0], coords3[10:,1], color='black', s=3)\n    \n    for item, x, y in zip(items3[0:], coords3[0:,0], coords3[0:,1]):\n        plt.annotate(item, xy=(x, y), xytext=(-1, 1), textcoords='offset points', \n                     ha='right', va='bottom', color='grey', fontsize=6)\n\n    x0=coords3[0,0]\n    y0=coords3[0,1]\n    plt.annotate(word3, xy=(x0, y0), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='forestgreen', fontsize=6)\n    \n    x1=coords3[1,0]\n    y1=coords3[1,1]\n    plt.annotate(items3[1] , xy=(x1, y1), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='blue', fontsize=6)\n    \n    x2=coords3[2,0]\n    y2=coords3[2,1]\n    plt.annotate(items3[2] , xy=(x2, y2), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='darkviolet', fontsize=6)\n    \n    x3=coords3[3,0]\n    y3=coords3[3,1]\n    plt.annotate(items3[3] , xy=(x3, y3), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='darkviolet', fontsize=6)\n    \n    x4=coords3[4,0]\n    y4=coords3[4,1]\n    plt.annotate(items3[4] , xy=(x4, y4), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='blue', fontsize=6)\n    \n    x5=coords3[5,0]\n    y5=coords3[5,1]\n    plt.annotate(items3[5] , xy=(x5, y5), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='forestgreen', fontsize=6)\n    \n    x8=coords3[8,0]\n    y8=coords3[8,1]\n    plt.annotate(items3[8] , xy=(x8, y8), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='darkorange', fontsize=6)\n    \n    x10=coords3[10,0]\n    y10=coords3[10,1]\n    plt.annotate(items3[10] , xy=(x10, y10), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='black', fontsize=6)\n    \n    ax = plt.gca()\n    circle = plt.Circle((x0, y0), 0.1, color='black', fill=False)\n    ax.add_artist(circle)\n    \n    plt.subplot(224)\n    plt.tick_params(\n        axis='both',\n        which='both',\n        bottom=False,      \n        left=False,         \n        labelbottom=False,\n        labelleft=False)\n    \n    lim4 = max([abs(x) for x in coords4[:,0] + coords4[:,1]])\n    plt.xlim([-lim4,lim4])\n    plt.ylim([-lim4,lim4])\n    \n    plt.scatter(coords4[0:,0], coords4[0:,1], color='grey', s=3)\n    plt.scatter(coords4[0:1,0], coords4[0:1,1], color='black', s=3)\n    plt.scatter(coords4[1:2,0], coords4[1:2,1], color='turquoise', s=3)\n    plt.scatter(coords4[2:3,0], coords4[2:3,1], color='black', s=3)\n    plt.scatter(coords4[3:4,0], coords4[3:4,1], color='black', s=3)\n    plt.scatter(coords4[4:5,0], coords4[4:5,1], color='black', s=3)\n    plt.scatter(coords4[5:6,0], coords4[5:6,1], color='red', s=3)\n    plt.scatter(coords4[6:7,0], coords4[6:7,1], color='forestgreen', s=3)\n    plt.scatter(coords4[7:8,0], coords4[7:8,1], color='brown', s=3)\n    plt.scatter(coords4[8:9,0], coords4[8:9,1], color='blue', s=3)\n    plt.scatter(coords4[9:10,0], coords4[9:10,1], color='darkviolet', s=3)\n    plt.scatter(coords4[10:11,0], coords4[10:11,1], color='forestgreen', s=3)\n    \n    \n    for item, x, y in zip(items4[0:], coords4[0:,0], coords4[0:,1]):\n        plt.annotate(item, xy=(x, y), xytext=(-1, 1), textcoords='offset points', \n                     ha='right', va='bottom', color='grey', fontsize=6)\n\n    x0=coords4[0,0]\n    y0=coords4[0,1]\n    plt.annotate(word4, xy=(x0, y0), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='black', fontsize=6)\n    \n    x1=coords4[1,0]\n    y1=coords4[1,1]\n    plt.annotate(items4[1] , xy=(x1, y1), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='turquoise', fontsize=6)\n    \n    x2=coords4[2,0]\n    y2=coords4[2,1]\n    plt.annotate(items4[2], xy=(x2, y2), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='black', fontsize=6)\n    \n    x3=coords4[3,0]\n    y3=coords4[3,1]\n    plt.annotate(items4[3], xy=(x3, y3), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='black', fontsize=6)\n    \n    x4=coords4[4,0]\n    y4=coords4[4,1]\n    plt.annotate(items4[4], xy=(x4, y4), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='black', fontsize=6)\n    \n    x5=coords4[5,0]\n    y5=coords4[5,1]\n    plt.annotate(items4[5], xy=(x5, y5), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='red', fontsize=6)\n    \n    x6=coords4[6,0]\n    y6=coords4[6,1]\n    plt.annotate(items4[6], xy=(x6, y6), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='forestgreen', fontsize=6)\n\n    x7=coords4[7,0]\n    y7=coords4[7,1]\n    plt.annotate(items4[7], xy=(x7, y7), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='brown', fontsize=6)\n    \n    x8=coords4[8,0]\n    y8=coords4[8,1]\n    plt.annotate(items4[8], xy=(x8, y8), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='blue', fontsize=6)\n    \n    x9=coords4[9,0]\n    y9=coords4[9,1]\n    plt.annotate(items4[9], xy=(x9, y9), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='darkviolet', fontsize=6)\n    \n    x10=coords4[10,0]\n    y10=coords4[10,1]\n    plt.annotate(items4[10], xy=(x10, y10), xytext=(-1, 1), textcoords='offset points', \n                 ha='right', va='bottom', color='forestgreen', fontsize=6)\n    \n    ax = plt.gca()\n    circle = plt.Circle((x0, y0), 0.1, color='black', fill=False)\n    ax.add_artist(circle)\n    \n    plt.subplots_adjust(wspace=0, hspace=0)\n    \n    plt.show()","3ea7124d":"# load the meta data and reduce column count\ndf=pd.read_csv('\/kaggle\/input\/eg-gensim-covid-sg-v2\/metadata_mod.csv', \n               usecols=['title','journal','abstract',\n                        'authors','doi','publish_time',\n                        'sha','full_text_file','url'],\n               encoding='utf-8', low_memory=False)\ndf.tail(2)","bbe39641":"df.describe()","7f66db5a":"def treat_na(df):\n    df = df.fillna('no data')\n    return df\n\ndef lower_case(df):\n    df['abstract_processed'] = df[\"abstract\"].str.lower()\n    return df\n\nstring_map1 = {\n    '\u03bcm': 'micromolar',\n    '%': '-percent',\n    '\u0152\u00bag' : 'microgram'\n}\n\ndef special_character_conversion(df):\n    for key, value in string_map1.items():\n        df['abstract_processed'] = df['abstract_processed'].str.replace(key, value)\n    return df\n\nstring_map2 = {\n    '\u03b1': 'alpha',\n    '\u03b2': 'beta',\n    '\u03b3': 'gamma',\n    '\u03b4': 'delta',\n    '\u03b5': 'epsilon',\n    '\u03b6': 'zeta',\n    '\u03b7': 'eta',\n    '\u03b8': 'theta',\n    '\u03b9': 'iota',\n    '\u03ba': 'kappa',\n    '\u03bb': 'lambda',\n    '\u03bc': 'mu',\n    '\u03bd': 'nu',\n    '\u03be': 'xi',\n    '\u03bf': 'omicron',\n    '\u03c0': 'pi',\n    '\u03c1': 'rho',\n    '\u03c3': 'sigma',\n    '\u03c4': 'tau',\n    '\u03c5': 'upsilon',\n    '\u03c6': 'phi',\n    '\u03c7': 'chi',\n    '\u03c8': 'psi',\n    '\u03c9': 'omega'\n}\n\ndef greek_alphabet_conversion(df):\n    for key, value in string_map2.items():\n        df['abstract_processed'] = df['abstract_processed'].str.replace(key, value)\n    return df\n\nstring_map3 = {\n    'sars-cov-two': 'sars-cov-2',\n    'sars-covtwo': 'sars-cov2',\n    'covid-nineteen': 'covid-19',\n    'covid 19': 'covid-19',\n    'two thousand and nineteen-ncov': '2019-ncov',\n    'two thousand and nineteen': '2019',\n    'hsv-one': 'hsv-1',\n    'hsv 1': 'hsv-1',\n    'hsv-two': 'hsv-2',\n    'hsv 2': 'hsv-2',\n    'hfivenone': 'h5n1',\n    'honenone': 'h1n1',\n    'hsevennnine': 'h7n9',\n    'interferon beta': 'interferon-beta',\n    'interferon gamma': 'interferon-gamma',\n    'ifn  -alpha' : 'ifn-alpha',\n    'ifnalpha': 'ifn-alpha',\n    'ifnbeta': 'ifn-beta',\n    'ifngamma': 'ifn-gamma',\n    'type i interferon': 'type-i-interferon',\n    'type i interferons': 'type-i-interferon',\n    'type i ifn': 'type-i-ifn',\n    'type ii ifn': 'type-ii-ifn',\n    'ifn type i': 'ifn-type-i',\n    'ifn type ii': 'ifn-type-ii',\n    'gamma interferon': 'gamma-interferon',\n    'interferon gamma': 'interferon-gamma',\n    'twenty-five-hydroxyvitamin d': '25-hydroxyvitamin-d',\n    'twenty-five-dihydroxyvitamin d': '25-dihydroxyvitamin-d',\n    'one,twenty-five-dihydroxyvitamin d': '25-dihydroxyvitamin-d',\n    '25-hydroxyvitamin d': '25-hydroxyvitamin-d',\n    '25-dihydroxyvitamin d': '25-dihydroxyvitamin-d',\n    '1,25-dihydroxyvitamin d': '25-dihydroxyvitamin-d',\n    'twenty-five(oh)dthree': '25-oh-d3',\n    '(twenty-five(oh)d)': '25-oh-d',\n    'twenty-five(oh)d': '25-oh-d',\n    'twenty-fiveohd': '25-oh-d',\n    '1,25 ( oh ) 2d': '25-oh-d2',\n    'twenty-fiveohd': '25-oh-d',\n    '25ohd': '25-oh-d',\n    'twenty-five ( oh ) d': '25-oh-d',\n    'twenty-five  oh  d' : '25-oh-d',\n    'angiotensin-converting enzyme': 'angiotensin-converting-enzyme',\n    'angiotensin-converting enzyme two': 'angiotensin-converting-enzyme-two',\n    'angiotensin-converting-enzyme two': 'angiotensin-converting-enzyme-two',\n    'angiotensin-converting enzyme ii': 'angiotensin-converting-enzyme-ii',\n    'angiotensin-converting-enzyme ii': 'angiotensin-converting-enzyme-ii',\n    'angiotensin-i converting enzyme' : 'angiotensin-i-converting-enzyme',\n    'angiotensin-ii converting enzyme' : 'angiotensin-ii-converting-enzyme',\n    'rna polymerase': 'rna-polymerase',\n    'rna dependent': 'rna-dependent',\n    'lopinavir-ritonavir': 'lopinavir ritonavir',\n    'gs 5734': 'gs-5734',\n    'acetwo': 'ace2',\n    'interferon-\u0153\u00b12b' : 'interferon-alpha2b',\n    '\u00ee\u00b2' : 'beta',\n    'ifn-\u0153\u00b12b' : 'interferon-alpha2b',\n    'ifn  -\u0153\u00b12b' : 'interferon-alpha2b',\n    'rifn-\u0153\u00b12a' : 'rifn-alpha2b',\n    'rifn-\u0153\u00b12b' : 'rifn-beta2b',\n    'interferon  ifn  -\u0153\u00b12b': 'interferon-alpha2b',\n    'IFN-\u0152\u00b12b' : 'IFN-a2b',\n    'TGF-\u0152\u2264' : 'TGF-beta',\n    'TNF\u0152\u00b1' : 'TNFalpha',\n    'Masson\u201a\u00c4\u00f4s' : \"Masson's\",\n    '3\u201a\u00c4\u00ec7' : '3-7',\n    '3\u201a\u00c4\u00ec14' : '3-14',\n    '6\u201a\u00c4\u00ec8' : '6-8',\n    '7\u201a\u00c4\u00ec9' : '7-9',\n    'product\u201a\u00c4\u00f4' : \"product'\",\n    'day\u201a\u00c4\u2264s' : \" day's\",\n    'its\u201a\u00c4\u2264' : \"its'\"\n}\n\ndef terms_standardisation1(df):\n    for key, value in string_map3.items():\n        df['abstract_processed'] = df['abstract_processed'].str.replace(key, value)\n    return df\n\nstring_map4 = {\n    'vitamin c': 'ascorbic-acid',\n    'vitamin d  three' : '25-hydroxyvitamin-d',\n    'vitamin d-binding': 'vitamin-d binding',\n    'vitamin d-': 'vitamin-d ',\n    'vitamin d': 'vitamin-d',\n    'vit d': 'vitamin-d',\n    '1,25-  oh   two  -vitamin-d' : '25-hydroxyvitamin-d'\n}\n\ndef terms_standardisation2(df):\n    for key, value in string_map4.items():\n        df['abstract_processed'] = df['abstract_processed'].str.replace(key, value)\n    return df\n\ndef remove_punctuation1(df):\n    symbols = \"()[\\]\" # Not included*-\/\n    for i in symbols:\n        df['abstract_processed'] = df['abstract_processed'].str.replace(i, '')\n    return df\n\ndef remove_punctuation2(df):\n    symbols = \"\/\" \n    for i in symbols:\n        df['abstract_processed'] = df['abstract_processed'].str.replace(i, ' ')\n    return df\n\n# keep only documents with relation to the subject\ndisease =['covid',\n          'covid-19',\n          '-cov2', \n          'cov2',\n          'sars-cov-2',\n          'sars-cov2',\n          'sars',\n          '2019-ncov',\n          'sars-cov']\n\npattern = '|'.join(disease)\n\ndef search_focus(df):\n    df = df[df['abstract_processed'].str.contains(pattern)]\n    return df\n\ndef remove_duplicates(df):\n    df = df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\ndef numbers_to_words(data):\n    tokens = word_tokenize(str(data))\n    new_text = \"\"\n    for w in tokens:\n        try:\n            w = num2words(int(w))\n        except:\n            a = 0\n        new_text = new_text + \" \" + w\n    return new_text\n\ndef convert_numbers(df):\n    df['abstract_processed'] = df.apply(lambda row: numbers_to_words(row['abstract_processed']),axis=1)\n    return df\n\ndef remove_non_en(df):\n    df['lang'] = df.apply(lambda row: detect(row['abstract']),axis=1)\n    df = df[df['lang'] == 'en']\n    return df","b3fa69a5":"def preprocess(data):\n    data = treat_na(data)\n    data = lower_case(data)\n    data = remove_duplicates(data)\n    data = search_focus(data)\n    data = remove_non_en(data)\n    data = special_character_conversion(data)\n    data = greek_alphabet_conversion(data)\n    data = convert_numbers(data)\n    data = terms_standardisation1(data)\n    data = remove_punctuation1(data)\n    data = terms_standardisation2(data)\n    data = remove_punctuation2(data)\n    return data\n\nt = time()\ndf=preprocess(df)\nprint('Time to preprocess everything: {} mins'.format(round((time() - t) \/ 60, 2)))","a10c7b4f":"df.describe()","63d5b435":"tmp_file = get_tmpfile('\/kaggle\/input\/eg-gensim-covid-sg-v2\/gensim-model-20200510.txt')\nmodel = KeyedVectors.load_word2vec_format(tmp_file)\nword_vectors = model","ea30a0bf":"word_vectors.similar_by_word(\"treatments\")","ade123b9":"show_closest_2d(word_vectors,'treatment','antivirals','lopinavir_ritonavir','hydroxychloroquine',10)","5204f92c":"t = time()\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('\/kaggle\/input\/eg-gensim-covid-sg-v2\/en_gensim_covid_sg_v2')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef scispacy_tokenizer(doc):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(doc)\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words and punctuation\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens\n\nprint('Time to preprocess everything: {} mins'.format(round((time() - t) \/ 60, 2)))","f00a128e":"#Calculate tfidf for all literature\nt = time()\n\n#Calculate tf\ncv = CountVectorizer(tokenizer=scispacy_tokenizer,\n                     ngram_range=(1,3),\n                     max_df=0.75,\n                     min_df=2,\n                     stop_words=stop, \n                     token_pattern='[a-z]+\\w*')\nword_count_vector = cv.fit_transform(df['abstract_processed'])\n\n#Calculate idf\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)\n\n#Calculate tfidf\ncount_vector=cv.transform(df['abstract_processed'])\ntf_idf_vector=tfidf_transformer.transform(count_vector)\n\n#Create feature names\nfeature_names = np.array(cv.get_feature_names())\n\nprint('Time to preprocess everything: {} mins'.format(round((time() - t) \/ 60, 2)))","72cab8d3":"string_map5 = {\n    'IFN-\u0152\u00b12b' : 'IFN-a2b',\n    'TGF-\u0152\u2264' : 'TGF-beta',\n    'TNF\u0152\u00b1' : 'TNFalpha',\n    'Masson\u201a\u00c4\u00f4s' : \"Masson's\",\n    '3\u201a\u00c4\u00ec7' : '3-7',\n    '3\u201a\u00c4\u00ec14' : '3-14',\n    '6\u201a\u00c4\u00ec8' : '6-8',\n    '7\u201a\u00c4\u00ec9' : '7-9',\n    'product\u201a\u00c4\u00f4' : \"product'\",\n    'day\u201a\u00c4\u2264s' : \" day's\",\n    'its\u201a\u00c4\u2264' : \"its'\",\n    '\u00d4\u00ba\u00f6' : ':',\n    '7\u201a\u00c4\u00ec10' : '7-10',\n    '\u0152\u00bag' : '\u00b5g',\n    '\u201a\u00c4\u00ec ' : '-',\n    'patients\u201a\u00c4\u00f4' : \"patients'\"\n}\n\ndef generate_sentences(text):\n    doc=sent_tokenize(text)\n    sentences = []\n    for i, token in enumerate(doc):\n        sentence=scispacy_tokenizer(token)\n        sentence=' '.join(sentence)\n        sentences.append(sentence)\n    return sentences\n\ndef build_similarity_matrix(sentences):\n    # Create an empty similarity matrix\n    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n \n    for idx1 in range(len(sentences)):\n        for idx2 in range(len(sentences)):\n            if idx1 == idx2: #ignore if both are same sentences\n                continue \n            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n    return similarity_matrix\n\ndef readability(text):\n    for key, value in string_map5.items():\n        text = text.replace(key, value)\n    return text\n\ndef sentence_similarity(text1, text2):\n    base = nlp(text1)\n    compare = nlp(text2)\n    return base.similarity(compare)\n\ndef generate_summary(file_name):\n    summarize_text = []\n    i=0\n    \n    # Step 1 - Read text, tokenize and split into sentenance\n    sentences=generate_sentences(file_name)\n    \n    # Step 2 - Generate Similary Martix across sentences\n    sentence_similarity_martix = build_similarity_matrix(sentences)\n    \n    # Step 3 - Rank sentences in similarity martix\n    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n    scores = nx.pagerank(sentence_similarity_graph)\n    \n    k = Counter(scores)\n    high = k.most_common(5) \n    high = sorted(high, reverse=False)\n    \n    # Step 4 - Sort the rank and pick top original sentences\n    original_sentences=readability(file_name)\n    original_sentences=sent_tokenize(original_sentences)\n    \n    # Step 5 - Pick top original sentences  \n    sents=len(original_sentences)\n    if sents <= 5:\n        summarize_text=original_sentences\n    elif sents > 5:\n        top_n = 5\n        for i in range(top_n):\n            summarize_text.append(\"\".join(original_sentences[high[i][0]]))\n            \n        \n    # Step 6 - Return summarized text\n    return \" \".join(summarize_text)","1d8e6f94":"def process_query(query): # Selecting relevant literature\n    \n    def get_top_tf_idf_words(tf_idf_vector, top_n=2):\n        sorted_nzs = np.argsort(tf_idf_vector.data)[:-(top_n+1):-1]\n        return feature_names[tf_idf_vector.indices[sorted_nzs]]\n    \n    disease2 =['covid',\n               'covid-19',\n               '-cov2', \n               'cov2',\n               'sars-cov-2',\n               '2019-ncov']\n    \n    pattern2 = '|'.join(disease2)\n    \n    # Calculate tfidf\n    response_vector=cv.transform([query])\n    tdtfi_response_vector=tfidf_transformer.transform(response_vector)\n    \n    # get top 15 tfidf words\n    mylist = [get_top_tf_idf_words(tf_idf_vector,15) for tf_idf_vector in tdtfi_response_vector]\n    feature_names2 = np.array(mylist)\n    feature_names2 = np.squeeze(feature_names2)\n    \n    # keep only documents with relation to the query\n    corpus_index = df.index\n    matrix = pd.DataFrame(tf_idf_vector.todense(), index=corpus_index, columns=feature_names)\n    test = pd.DataFrame(matrix, columns=feature_names2)\n    \n    test['QueryTotal']= test.sum(axis=1)\n    test = test.loc[test['QueryTotal'] > 0.4]\n    test = test.sort_values(by=['QueryTotal'],ascending=False)\n    relevant_corpus_index = test.index\n    \n  \n    # keep only documents similar to the query\n    if len(relevant_corpus_index) > 0:\n        df_similar = df.loc[relevant_corpus_index]\n        df_similar['abstract_tokenised'] = df_similar.apply(\n            lambda row: scispacy_tokenizer(row['abstract_processed']),axis=1)\n        df_similar['abstract_tokenised'] = df_similar.apply(\n            lambda row:' '.join(row['abstract_tokenised']),axis=1)\n        df_similar['query_tokenised'] = query\n        df_similar['query_tokenised'] = df_similar.apply(\n            lambda row: scispacy_tokenizer(row['query_tokenised']),axis=1)\n        df_similar['query_tokenised'] = df_similar.apply(\n            lambda row:' '.join(row['query_tokenised']),axis=1)\n        df_similar['similarity'] = df_similar.apply(\n            lambda row: sentence_similarity(row['abstract_tokenised'],row['query_tokenised']),axis=1)\n        \n        # keep only documents with relation to the COVID-19\n        df_similar = df_similar.loc[df['abstract_processed'].str.contains(pattern2)]\n        df_similar = df_similar.sort_values(by=['similarity'],ascending=False)\n        \n        if len(df_similar) > 0:\n            \n            # Pick top 15 similar abstracts to query and perform extractive summarisation\n            df_similar_condes = df_similar.loc[:,['title',\n                                                  'authors',\n                                                  'journal',\n                                                  'abstract',\n                                                  'url']].copy()\n            pd.set_option('display.max_colwidth', 0)\n            df_extract_sum=df_similar_condes[:15].copy()\n            df_extract_sum['extractive_summary_of_abstract']=df_extract_sum.apply(\n                lambda row: generate_summary(row['abstract']),axis=1)\n            df_extract_sum1 = df_extract_sum.loc[:,['title',\n                                                    'authors',\n                                                    'journal',\n                                                    'extractive_summary_of_abstract',\n                                                    'url']].copy()\n            return df_extract_sum1\n        \n        elif len(df_similar) <= 0:\n            \n            return 'No relevant artcles found.'\n    \n    elif len(relevant_corpus_index) <= 0:\n        \n        return 'No relevant artcles found.'","5dd21181":"queries1 = ['Remdesivir effective against COVID-19?',\n            'Favipiravir effective against COVID-19?',\n            'Ribavirin effective against COVID-19?',\n            'Lopinavir-ritonavir effective against COVID-19?',\n            'Arbidol effective against COVID-19?']","23576273":"import warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nt = time()\nlength = len(queries1)\ni=0\n\nfor i in range(length):\n    pd.set_option('display.max_rows', None)\n    df_summary=process_query(queries1[i])  \n    if type(df_summary) == str:\n        summary_html=df_summary\n    elif type(df_summary) != str:\n        summary_html=HTML(df_summary.to_html(escape=False,index=False))\n    display(HTML('<h3>'+'Query: '+queries1[i]+'<\/h3>'), summary_html)\n    \nprint('Time to preprocess everything: {} mins'.format(round((time() - t) \/ 60, 2)))","01dd4f57":"queries2 = ['Effectiveness of drugs being developed and tried to treat COVID-19 patients?',\n            'Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocycline that may exert effects on viral replication?',\n            'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients?',\n            'Exploration of use of best animal models and their predictive value for a human vaccine?',\n            'Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents?',\n            'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need?',\n            'Efforts targeted at a universal coronavirus vaccine?',\n            'Efforts to develop animal models and standardize challenge studies?',\n            'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers?',\n            'Approaches to evaluate risk for enhanced disease after vaccination?',\n            'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models in conjunction with therapeutics?']","0dcc6b10":"import warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nt = time()\nlength = len(queries2)\ni=0\n\nfor i in range(length):\n    pd.set_option('display.max_rows', None)\n    df_summary=process_query(queries2[i])  \n    if type(df_summary) == str:\n        summary_html=df_summary\n    elif type(df_summary) != str:\n        summary_html=HTML(df_summary.to_html(escape=False,index=False))\n    display(HTML('<h3>'+queries2[i]+'<\/h3>'), summary_html)\n    \nprint('Time to preprocess everything: {} mins'.format(round((time() - t) \/ 60, 2)))","0e85a09c":"Continuing the line of exploration used above I investigated each of the words listed above to find what other therapeutic drugs exists in the literature, only 4 of the words yielded therapeutic drugs the results of which is visualised in the figure below.\n\nThe figure was created using \"principal component analysis (PCA)\" to reduce the dimensionality of the word vectors to allow easier visualisation of the 10 most similar word vectors, a review of the figure shows that its clear that there is a great deal of different types of therapeutic drugs that is discussed in the data on treatment for COVID-19. Colour coding as below to show the different categories of therapeutic drugs:\n* Red = Medication used to treat parasite infestations.\n* Blue = Broad-spectrum antiviral medication.\n* Violet = Antiviral medication.\n* Brown = Immunosuppressive medication. \n* Black = Antimalaria medication.\n* Green = Protease inhibitor (A type of antiviral that uses a specific method that prevents viral replication).\n* Turquoise = Antibiotic.\n* Orange = Signalling proteins.","3dd56185":"## 2.2 - Preprocessing\nBefore the abstract data can be used to carry out information retrieval using the TF-IDF (Term Frequency \u2014 Inverse Document Frequency) technique, preprocessing of the abstract data was required. \n\nPreprocessing includes fairly standard techniques such as turning words to lowercase, removing punctuation, removing stop words and removing non english inputs. However exploration of the dataset showed that less common techniques such as terms standardisation, special character conversions and greek alphabet conversions were also required to allow a higher quality output to be delivered.","f9b9378e":"# 3. Conclusions\nIn this notebook, using a custom Word Vector model (created using Gensin Word2Vec) and the TD-IDF technique, we discovered literature that shows some promising therapeutic drug\/s in reducing the time to recovery of COVID-19 patients. Some of the promising therapeutic drugs in question are listed below (For the full list of queried results please refer to Section 2.6 and 2.7 of this notebook):\n* [Remdesivir (broad spectrum antiviral)](https:\/\/doi.org\/10.1101\/2020.04.15.043166)\n* [Favipiravir (antivirval)](https:\/\/doi.org\/10.1101\/2020.03.17.20037432)\n* [Lopinavir\/ritonavir (Protease inhibitor)](https:\/\/doi.org\/10.1101\/2020.04.25.20079079)  \n* [Lopinavir\/ritonavir (Protease inhibitor) & Novaferon (Protein \/ Cytokine)](https:\/\/doi.org\/10.1101\/2020.04.24.20077735)\n* [Lopinavir\/ritonavir (Protease inhibitor), Ribavirin (Antiviral) & Interferon beta-1b (Protein \/ Cytokine)](http:\/\/www.thelancet.com\/retrieve\/pii\/S0140673620310424) \n* [Arbidol (Broad-Spectrum Antiviral)](https:\/\/doi.org\/10.1101\/2020.04.11.20056523)","a0ffcb4b":"## 2.4 - TD-IDF (Term Frequency \u2014 Inverse Document Frequency) \nIn the above section we learnt that the dataset contains information on a wide range of therapeutic drugs that is being discussed in relation to treatment of COVID-19 and to allow us to query the dataset we will use the TD-IDF technique. \n\n> In information retrieval, tf\u2013idf or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n>  \n> ***[Wikipedia - https:\/\/en.wikipedia.org\/wiki\/Tf\u2013idf](https:\/\/en.wikipedia.org\/wiki\/Tf\u2013idf)***\n\nNow that we have an understanding of what is TF-IDF we will now find out the relevent documents to specific queries.","5fb64b67":"## 2.9 - Competition Queries and Relevant Articles\nThe following queries have been lifted directly from the competition itself.","20cfbf79":"Using a standard Gensim function, to find synonyms, on the word \"treatments\" we find that the 10 most similar words to be either synonyms, treatment methods or actual therapeutic drugs (***lopinavir_ritonavir, chloroquine_hydroxychloroquine***). ","dfc74dd7":"## 2.5 - Article Similarity and Relevance\nThe following function will find articles which are relevant to a specific query and then using [\"Cosine Similarity\"](https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity) rank the subject articles with the order of highest similarity. Subsequently [\"Extractive Summarisation\"](https:\/\/blog.knoldus.com\/machine-x-text-summarization-in-python\/) on the abstract of each article will be performed, this last part is mainly to reduce the length of the output file while trying to maintain the essence of the abstract. ","3b3c2423":"Exploration of the subject dataset shows that there is over fifty-nine thousand rows of data before cleaning and preprocessing.","6b89adc9":"## 2.3 - Gensim Word2Vec\nA Gensim Word2Vec (***continuous skip-gram***) word vector model (***gensim-model-20200510***) was created using the corpus of abstract data that was pre-processed above. \n\nDuring course of this project it was found that the publicly available \"off the shelf\" word vector models did not actually have word vectors to represent some of the therapeutic drugs that were mentioned in the data, hence my reason to create a domain-specific word vector model in the hopes that it produces a higher quality output.\n","d3b0d0eb":"# 4. References\nIt is worth noting that this is only my third Data Science project, having originally started out as an aerospace engineer, therefore I would welcome any constructive comments on how I can further improve in reference to this particular project. Below is some of the sites I used to help both to understand the concepts and how to practically implement each of the techniques used with construction of this project:\n* [Ultimate guide to deal with Text Data (using Python) \u2013 for Data Scientists and Engineers](https:\/\/www.analyticsvidhya.com\/blog\/2018\/02\/the-different-methods-deal-text-data-predictive-python\/) - By SHUBHAM JAIN\n* [How to Use Tfidftransformer & Tfidfvectorizer?](https:\/\/kavita-ganesan.com\/tfidftransformer-tfidfvectorizer-usage-differences\/#.XtO4eC-ZNI2) - By Kavita Ganesan \n* [TF-IDF from scratch in python on real world dataset](https:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089) - By William Scott\n* [TF IDF | TFIDF Python Example](https:\/\/towardsdatascience.com\/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76) - By Cory Maklin\n* [A Beginner\u2019s Guide to Word Embedding with Gensim Word2Vec Model](https:\/\/towardsdatascience.com\/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92) - By Zhi Li\n* [Machine X: Text Summarization in Python](https:\/\/blog.knoldus.com\/machine-x-text-summarization-in-python\/) - By Shubham Goyal","452be3df":"# ***COVID-19: NLP Review Of Available Therapeutics***\n\n**Abstract**: In this notebook, using a custom Word Vector model (created using Gensin Word2Vec) and the TD-IDF technique, we discovered literature that shows some promising therapeutic drug\/s in reducing the time to recovery of COVID-19 patients. Some of the promising therapeutic drugs in question are listed below (For the full list of queried results please refer to Section 2.6 and 2.7 of this notebook):\n* [Remdesivir (broad spectrum antiviral)](https:\/\/doi.org\/10.1101\/2020.04.15.043166)\n* [Favipiravir (antivirval)](https:\/\/doi.org\/10.1101\/2020.03.17.20037432)\n* [Lopinavir\/ritonavir (Protease inhibitor)](https:\/\/doi.org\/10.1101\/2020.04.25.20079079)  \n* [Lopinavir\/ritonavir (Protease inhibitor) & Novaferon (Protein \/ Cytokine)](https:\/\/doi.org\/10.1101\/2020.04.24.20077735)\n* [Lopinavir\/ritonavir (Protease inhibitor), Ribavirin (Antiviral) & Interferon beta-1b (Protein \/ Cytokine)](http:\/\/www.thelancet.com\/retrieve\/pii\/S0140673620310424) \n* [Arbidol (Broad-Spectrum Antiviral)](https:\/\/doi.org\/10.1101\/2020.04.11.20056523)","5ccd600c":"\n# 2. Methods \/ Analysis \n## 2.1 - Metadata Dataset & Additional Data\nThe objective of this notebook is to find out what all the related abstracts may tell me about any therapeutics that may be availble to treat COVID-19 which is caused by the virus SAR-CoV-2. \n\nThe dataset \"metadata_mod.csv\" is a copy of the \"metadata.csv\", downloaded from the Kaggle website on the 10th May 2020, with the addition of two rows of data extracted from \"The Lancet\". The reasons for the addition of these two rows of data was an attempt to build a better word vector model which is detailed in section 2.3 \"Gensim Word2Vec\" of this notebook.","f2dbdaa8":"## 2.6 - Queries and Relevant Articles\nThe following queries are written for and to answer my own questions on the effectiveness of the therapeutic drugs found in the section 2.3 of this notebook. ","ec892317":"# 1. Introduction\nSARS-CoV-2 is the pandemic of our times, as of writing (31st May 2020) some promising vaccines are already in clincal trials! However realistically there will be a minimum of at least [12](https:\/\/www.bbc.co.uk\/news\/health-51665497) months before a vaccine is available, before then effective therapeutics would be required to reduce the mortatility rate of COVID-19. In this notebook I will be using various **NLP** techniques to ascertain if effective therapeutics have been found or what therapeutics are being mentioned in the provided dataset that maybe effective at treating COVID-19.\n"}}