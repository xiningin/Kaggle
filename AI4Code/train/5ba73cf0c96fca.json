{"cell_type":{"e428ddbd":"code","8ed58f16":"code","05600887":"code","777b833a":"code","b3045ea6":"code","de2de0ff":"code","13597328":"code","45146ec3":"code","e0a91813":"code","9d16328e":"code","95d03abc":"code","dc9cac65":"code","f997e206":"code","4145bfa0":"code","018f396c":"code","67d40943":"code","84674a28":"code","50c10bb5":"code","d1199e0a":"code","96d17b2f":"code","bfa61b50":"code","7ddbeefd":"code","4f141031":"code","9971e13f":"code","78eb1ef0":"code","7a2aded4":"code","dbb528e0":"code","86cedcc0":"code","56a53a4d":"code","bcf4b728":"code","111418fc":"code","0d086028":"code","92a29af8":"code","ec372a48":"code","cca52025":"code","64778b30":"code","81cdec27":"code","aaee4224":"code","2bed9f22":"code","ee161728":"code","6987ac73":"markdown","f98a53c5":"markdown","cf8737cf":"markdown","3cad810a":"markdown","493b8a72":"markdown","c30f8c6c":"markdown","2d92ee7b":"markdown","a72a2b94":"markdown","a84eb6ab":"markdown","5bffa249":"markdown","1729e646":"markdown","0b533232":"markdown"},"source":{"e428ddbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ed58f16":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport scipy.stats as ss\nimport sklearn.preprocessing as skpe\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm\nimport sklearn.linear_model as lm\nimport sklearn.ensemble as sken\nimport matplotlib.pyplot as plt\nimport seaborn as sns","05600887":"# Data Description\n\"\"\"The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\n    CRIM - per capita crime rate by town\n    ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n    INDUS - proportion of non-retail business acres per town.\n    CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n    NOX - nitric oxides concentration (parts per 10 million)\n    RM - average number of rooms per dwelling\n    AGE - proportion of owner-occupied units built prior to 1940\n    DIS - weighted distances to five Boston employment centres\n    RAD - index of accessibility to radial highways\n    TAX - full-value property-tax rate per $10,000\n    PTRATIO - pupil-teacher ratio by town\n    B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    LSTAT - % lower status of the population\n    MEDV - Median value of owner-occupied homes in $1000's\"\"\"\n","777b833a":"path=\"..\/input\/boston-housing-dataset\/HousingData.csv\"\ndf=pd.read_csv(path)","b3045ea6":"df.shape","de2de0ff":"df.columns","13597328":"# Variable Identification\ndf.dtypes","45146ec3":"df.info()","e0a91813":"# Continuous Varaible\ndf.describe()    # CHAS seems to be a categorical variable from the description and describe method","9d16328e":"# Categorical Variable\n(df['CHAS'].value_counts()\/len(df['CHAS'])*100).plot.bar()   # Frequency Chart","95d03abc":"# Checking correlation bw different variables\nplt.figure(figsize=(18,18))\nsns.heatmap(df.corr(),vmax=.7,cbar=True,annot=True)   # From this heatmap we can conclude that INDUS, NOX AND AGE are highly correlated with DIS.Also the target variable MEDV is having a correlation score > 0.4 with following features:LSTAT, PTRATIO, RM, TAX, INDUS and NOX  ","dc9cac65":"scaler = skpe.MinMaxScaler()\ncolumn_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\nx = df.loc[:,column_sels]\ny = df['MEDV']\nx = pd.DataFrame(data=scaler.fit_transform(x), columns=column_sels)\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sels):\n    sns.scatterplot(y=y, x=x[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","f997e206":"sns.scatterplot(x='LSTAT',y='INDUS',data=df)","4145bfa0":"sns.scatterplot(x='LSTAT',y='RM',data=df)","018f396c":"sns.scatterplot(x='LSTAT',y='AGE',data=df)","67d40943":"sns.scatterplot(x='TAX',y='INDUS',data=df)","84674a28":"sns.scatterplot(x='TAX',y='NOX',data=df)","50c10bb5":"sns.scatterplot(x='TAX',y='LSTAT',data=df)","d1199e0a":"sns.scatterplot(x='NOX',y='INDUS',data=df)","96d17b2f":"# Box Plots to detect outliers\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df.items():\n    sns.boxplot(y=k, data=df, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)  # Columns like CRIM, ZN, RM and B seems to have outliers","bfa61b50":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df.drop(['CHAS'],axis=1).items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)   # We have dropped CHAS for this analysis as it is a discrete variable","7ddbeefd":"print(df['AGE'].skew())\nsns.distplot(np.log1p(df['AGE']),color='Black')\nprint(np.log1p(df['AGE']).skew())","4f141031":"print(df['B'].skew())\nsns.distplot(np.power(df['B'],3),color='Black')\nprint(np.power(df['B'],3).skew())","9971e13f":"print(df['PTRATIO'].skew())\nsns.distplot(np.sqrt(df['PTRATIO']),color='Black')\nprint(np.sqrt(df['PTRATIO']).skew())","78eb1ef0":"# Removing skewness of these features\ndf['LSTAT']=np.log1p(df['LSTAT'])\ndf['AGE']=np.log1p(df['AGE'])\ndf['PTRATIO']=np.log1p(df['PTRATIO'])\ndf['B']=np.power(df['B'],3)","7a2aded4":"df.isnull().sum()","dbb528e0":"# Replacing missing values with the Mean\navg_mean_cr = df['CRIM'].astype('float').mean(axis=0)\ndf['CRIM'].replace(np.nan,avg_mean_cr,inplace=True)\n\navg_mean_zn = df['ZN'].astype('float').mean(axis=0)\ndf['ZN'].replace(np.nan,avg_mean_zn,inplace=True)\n\navg_mean_in = df['INDUS'].astype('float').mean(axis=0)\ndf['INDUS'].replace(np.nan,avg_mean_in,inplace=True)\n\navg_mean_ch = df['CHAS'].astype('float').mean(axis=0)\ndf['CHAS'].replace(np.nan,avg_mean_ch,inplace=True)\n\navg_mean_ls = df['LSTAT'].astype('float').mean(axis=0)\ndf['LSTAT'].replace(np.nan,avg_mean_ls,inplace=True)\n\navg_mean_age = df['AGE'].astype('float').mean(axis=0)\ndf['AGE'].replace(np.nan,avg_mean_age,inplace=True)","86cedcc0":"# Segregating features and labels and dropping unnecessary features\nx=df.drop(['MEDV'],axis=1)\ny=df['MEDV']","56a53a4d":"# Scaling values using MinMaxScaler\nscaler=skpe.MinMaxScaler()\nx_scaled=scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled,columns=x.columns)\nx.head()","bcf4b728":"# Splitting dataset into train and test sets\ntrain_x,test_x,train_y,test_y=ms.train_test_split(x,y,test_size=0.2,random_state=115)\ntrain_x.shape,test_x.shape","111418fc":"lr=lm.LinearRegression(normalize=True)\nlr.fit(train_x,train_y)\nlr.coef_","0d086028":"import statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nmodel=sm.OLS(train_y,train_x)\nresult=model.fit()\nresult.summary()","92a29af8":"# Checking the probabality distribution of the two quantiles\n\n# Q-Q Normal plot\ndef resid_qq(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(test_y, y_score)\n    ## now make the residual plots\n    ss.probplot(resids, plot = plt)\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Quantiles of standard Normal distribution')\n    plt.ylabel('Quantiles of residuals')\n    \ny_score=lr.predict(test_x)\nresid_qq(test_y, y_score)         # This is perfectly fine even though it has a few outliers","ec372a48":"# A common misconception is that the features or label of a linear regression model must have Normal distributions. This is not the case! Rather, the residuals (errors) of the model should be Normally distributed\n\ndef hist_resids(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(test_y, y_score)\n    ## now make the residual plots\n    sns.distplot(resids)\n    plt.title('Histogram of residuals')\n    plt.xlabel('Residual value')\n    plt.ylabel('count')\n    \nhist_resids(test_y, y_score)        # This is almost good exvept for the reason that it is slightly right-skewed","cca52025":"# Creating a dataframe of residuals,test values and predicted values\nresiduals = pd.DataFrame({\n    'fitted values' : test_y,\n    'predicted values' : y_score\n})\n\nresiduals['residuals'] = residuals['fitted values'] - residuals['predicted values']\nresiduals.head()","64778b30":"# Residual scatterplot\n# Remember,there should be no pattern observed in this plot;the residuals should be randomly distributed across the plot\nf = range(0,102)\nk = [0 for i in range(0,102)]\nplt.scatter( f, residuals.residuals[:], label = 'residuals')\nplt.plot( f, k , color = 'red', label = 'regression line' )\nplt.xlabel('fitted points ')\nplt.ylabel('residuals')\nplt.title('Residual plot')\nplt.ylim(-4,4)\nplt.legend()      # This is good","81cdec27":"# Distribution of Error terms\nplt.hist(residuals.residuals, bins = 150)\nplt.xlabel('Error')\nplt.ylabel('Frequency')\nplt.title('Distribution of Error Terms')\nplt.show()        # It seems fine but there are a few outliers","aaee4224":"# Segregating features and labels\nx=df.drop(['MEDV'],axis=1)\ny=df['MEDV']","2bed9f22":"Coefficients = pd.DataFrame({\n    'Variable'    : x.columns ,\n    'coefficient' : lr.coef_\n})\nCoefficients.head()","ee161728":"x = range(len(train_x.columns))\ny = lr.coef_\nplt.bar( x, y )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Normalized Coefficient plot')","6987ac73":"**Variable Transformation**","f98a53c5":"**Verifying assumtions to keep in mind while building a linear regression model :-**\n\n**1. There should be a linear relationship between dependent and independent variable. If it's not you can use variable transformation **\n**2. Correlation of error terms,i.e. no particular pattern should be observed **\n**3. Constant variance of error,i.e. no particular pattern should be observed in a plot containing residuals and fitted values **\n**4. Normal Distribution of Errors (Use a Q-Q plot) **\n**5. Minimize Multi-Collinearity by eliminating on of the features which are having multi-collinearity (Use VIF technique)**","cf8737cf":"**Again seperating features and labels to check each feature's weightage**","3cad810a":"**Linear Regression**","493b8a72":"**Distribution of features**","c30f8c6c":"**Checking the type of relationship different features share with the target varaiable**","2d92ee7b":"**Model Building**","a72a2b94":"**Performance Metrics**","a84eb6ab":"**Treating Missing Values**","5bffa249":"**Univariate Analysis**","1729e646":"**Relationship with other features**","0b533232":"**Bivariate Analysis**"}}