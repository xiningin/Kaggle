{"cell_type":{"02c9e828":"code","f726c2f8":"code","b509029b":"code","2e67c27f":"code","41e02340":"code","4bc3266b":"code","17aa7aba":"code","ee8c868b":"code","2fdf039e":"code","2329a7a5":"code","a23f205f":"code","c9202e48":"code","d7966560":"code","ee3629bc":"code","384a35ef":"code","abac6585":"code","652d9fd2":"code","a80c0df4":"code","be0cfcf4":"code","4f96e153":"code","a8eab0ac":"code","44cf0f34":"code","ad8b0e8d":"code","7cd8eb01":"code","035d25df":"code","e85dec39":"code","07549cd4":"code","887493ca":"code","f644b608":"code","32f7b356":"code","28e70b51":"code","96849d17":"markdown","be6a40ec":"markdown","f43eb21c":"markdown","7c8de8b8":"markdown","8b84225a":"markdown","d656135f":"markdown","5d8108e9":"markdown","184f59b5":"markdown","769e61d2":"markdown","44c35834":"markdown","9370c9b7":"markdown","4efbe5fc":"markdown","60872423":"markdown","3acd48c1":"markdown","3f651f6c":"markdown","d0f9eb4d":"markdown","c56595c1":"markdown","1a247771":"markdown","2c807aa7":"markdown"},"source":{"02c9e828":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pickle\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nfrom textblob import TextBlob\nfrom collections import Counter\nfrom nltk.util import ngrams\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation,  TruncatedSVD \n\n#nltk.download('stopwords')\n#nltk.download('punkt')\n\n\nimport os\n#raw_data = pd.read_csv('\/kaggle\/input\/alsa\/pubmed_als_articles.csv')\n\n# we are going to read in the data and parse the date column as dates\nraw_data = pd.read_csv('\/kaggle\/input\/alsa\/pubmed_als_articles.csv', encoding='utf8', parse_dates=['publication_date'],index_col=None)\n\nraw_data.head()","f726c2f8":"raw_data.iloc[0,:]","b509029b":"raw_data.iloc[0,0]\n","2e67c27f":"abstracts = raw_data['abstract']","41e02340":"# number of transcripts you want to analyze\nfileids = range(0,51)\ndoc_sents = [TextBlob(abstracts[fileid])\n             .sentences for fileid in fileids]\n\n# let's look at a few\nprint(doc_sents[0][0:5])","4bc3266b":"doc_sents_nltk = [nltk.sent_tokenize(abstracts[fileid]) for fileid in fileids]\n\n# to print the first one\nprint('\\n-----\\n'.join(nltk.sent_tokenize(abstracts[0])))","17aa7aba":"doc_words_word_tok = [nltk.word_tokenize(abstracts[fileid]) \\\n             for fileid in fileids]\n\nprint('\\n-----\\n'.join(nltk.word_tokenize(abstracts[0][123:170])))","ee8c868b":"doc_words_punct= [nltk.wordpunct_tokenize(abstracts[fileid]) \\\n             for fileid in fileids]\n\n#to view a few\nprint('\\n-----\\n'.join(nltk.wordpunct_tokenize(abstracts[0][123:170])))","2fdf039e":"lemmizer = nltk.WordNetLemmatizer()\n\n# note that i used the results from the word_punct tokenizer, but you can use any word tokenizer method\n\nfor w in doc_words_punct[0][31:55]:\n        print(lemmizer.lemmatize(w), w)","2329a7a5":"stemmer = nltk.stem.porter.PorterStemmer()\n\nfor w in doc_words_punct[0][31:55]:\n        print(stemmer.stem(w), w)","a23f205f":"print(abstracts[19])","c9202e48":"def _removeNonAscii(s): \n        return \"\".join(i for i in s if ord(i)<128)\n\n_removeNonAscii(abstracts[19])\n\n# for more than one doc     \n#text_non_ascii = abstracts.map(lambda x: _removeNonAscii(x))","d7966560":"for w in doc_words_punct[0][33:37]:\n    print(w.lower(), w)","ee3629bc":"for fileid in fileids[0:1]: \n    for w in doc_words_punct[0][31:55]:\n        print(stemmer.stem(w.lower()),w)","384a35ef":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\nstop += ['.',\" \\'\", 'ok','okay','yeah','ya','stuff','?']\n\nclean_words = []\nfor doc in doc_words_punct:\n    for w in doc:\n        if w.lower() not in stop:\n            clean_words.append(w.lower())\nprint(clean_words[0:10])","abac6585":"unclean_words = []\nfor w in doc_words_punct[0]:\n    unclean_words.append(w.lower())\nprint(unclean_words[0:10])","652d9fd2":"def clean_text(text):\n    \n    \"\"\" \n    Takes in a corpus of documents and cleans. \n    \n    1. remove any special strings and non ascii characters\n    2. tokenize into words \n    3. lowercase and remove stop words\n    4. lemmatize and  remove stop words again\n    5. append to a list\n    \n    \n    OUT: cleaned text = a list (documents) of lists (cleaned word in each doc)\n        \"\"\"\n    \n    # choose tokenizer\n    tokenizer=nltk.wordpunct_tokenize\n    \n    # choose your stemmer\n    #stemmer = WordNetLemmatizer().lemmatize\n    stemmer = nltk.porter.PorterStemmer().stem\n    #SnowballStemmer(\"english\")\n    \n    \n    #function to remove non-ascii characters\n    def _removeNonAscii(s): \n        return \"\".join(i for i in s if ord(i)<128)\n\n\n\n    stop_w = stopwords.words('english')\n    stop_w += ['.', ',',':','...','!\"','?\"', \" ' \",\"' \",\" '\", '\"',\" - \",\"-\",\" \u2014 \",',\"','.\"','!', ';',\"\/\",\\\n             '.\\'\"','[',']',\"\u2014\",\".\\'\",'#','1','2','3','4','5','6','7','8','9',\\\n             ' oh ','la','was','wa','?','like' ,\" ' \",'I',\" ? \",\"s\", \" t \",\"ve\",\"re\",\\\n              \"(\",\")\",\").\",\"'m\",\"'s\",\"\\ 's\",\"\u2019\",\"\u2019\",\"\u2019\",\"\u2018\",\"n't\",\"...\"]\n    stop_w = set(stop_w)\n\n    cleaned_text = []\n        \n    text_non_ascii = text.map(lambda x: _removeNonAscii(x))\n    \n    for doc in text_non_ascii:\n        cleaned_words = []\n        \n        for word  in tokenizer(doc):  \n            low_word = word.lower()\n            \n            # throw out any words in stop words\n            if low_word not in stop_w:\n            \n                # get roots\n                root_word = stemmer(low_word)  \n                          \n                # keep if root is not in stopwords (yes, again)\n                if root_word not in stop_w: \n                    \n                # put into a list of words for each document, yes i lowered again, b\/c i have trust issues\n                    cleaned_words.append(root_word.lower())\n        \n        # keep corpus of cleaned words for each document    \n        cleaned_text.append(' '.join(cleaned_words))\n    \n    return cleaned_text","a80c0df4":"clean_abs = clean_text(abstracts)","be0cfcf4":"clean_abs[19]","4f96e153":"# to save our cleaned text if using your local computer, not kaggle\n#with open('cleaned_als_pubmed_abs.pkl', 'wb') as picklefile:\n#    pickle.dump(cleaned_text, picklefile)","a8eab0ac":"talks_blob = [TextBlob(abstracts[fileid]) for fileid in fileids]\n\n# pulls all the nouns and all the things that are associated with it \nprint('\\n-----\\n'.join(talks_blob[0][0:500].noun_phrases))","44cf0f34":"def n_grams_skr(data, n = 3, max_words=500):\n    \"\"\"extract ngrams and their counts, save into a dataframe. \n    INPUT: data = corpus, n=number of words to put in gram, max_words=number of most common ngrams to return\n    OUTPUT: dataframe of most common ngrams and their counts in the corpus.\"\"\"\n    \n    counter = Counter()\n    df = pd.DataFrame(columns=['n_gram','count'])\n    row = 0\n    \n    for doc in data:\n        words = TextBlob(doc).words\n        bigrams = ngrams(words, n)\n        counter += Counter(bigrams)\n\n    for phrase, count in counter.most_common(max_words):\n        df.loc[row,'n_gram'] = phrase\n        df.loc[row,'count'] = count\n        row += 1\n        \n    return df","ad8b0e8d":"# this takes about 30 min to run with half the data...and you dont' need the output, \n# its just to get an idea of what our n-grams are\ndf = n_grams_skr(clean_abs[0:100],2,500)\ndf = df.sort_values('count',ascending=False)\n\ndf1 = n_grams_skr(clean_abs[0:100],3,500)\ndf1 = df1.sort_values('count',ascending=False)\n\ndfb = pd.concat([df,df1], axis=1)\ndfb.columns=['bi-gram','bi count','trigram','tri count']\ndfb.head(20)","7cd8eb01":"# CountVectorizer is a class; so `vectorizer` below represents an instance of that object.\nc_vectorizer = CountVectorizer(ngram_range=(1,3), \n                             stop_words='english', \n                             max_df = 0.6, \n                             max_features=10000)\n\n# call `fit` to build the vocabulary\nc_vectorizer.fit(clean_abs)\n# finally, call `transform` to convert text to a bag of words\nc_data = c_vectorizer.transform(clean_abs)","035d25df":"%env JOBLIB_TEMP_FOLDER=\/tmp","e85dec39":"def topic_mod_als(vectorizer, vect_data, topics=20, iters=5, no_top_words=50):\n    \n    \"\"\" use Latent Dirichlet Allocation to get topics\"\"\"\n\n    mod = LatentDirichletAllocation(n_components=topics,\n                                    max_iter=iters,\n                                    random_state=42,\n                                    learning_method='online',\n                                    n_jobs=-1)\n    \n    mod_dat = mod.fit_transform(vect_data)\n    \n    \n    # to display a list of topic words and their scores \n    #next step is to make this into a matrix with all 5k terms and their scores from the viz?  \n    \n    def display_topics(model, feature_names, no_top_words):\n        for ix, topic in enumerate(model.components_):\n            print(\"Topic \", ix)\n            print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]) + '\\n')\n    \n    display_topics(mod, vectorizer.get_feature_names() , no_top_words)\n\n    \n    return mod, mod_dat\n","07549cd4":"mod, mod_dat = topic_mod_als(c_vectorizer, \n                            c_data, \n                            topics=20, \n                            iters=10, \n                            no_top_words=15)  ","887493ca":"import pyLDAvis, pyLDAvis.sklearn\nfrom IPython.display import display \n    \n    # Setup to run in Jupyter notebooks\npyLDAvis.enable_notebook()\n\n # Create the visualization\nvis = pyLDAvis.sklearn.prepare(mod, c_data, c_vectorizer,  sort_topics=False, mds='mmds')#, mds='tsne'\n\n# Let's view it!\ndisplay(vis)","f644b608":"raw_data.index= raw_data['publication_date']\n\nraw_data.abstract.resample('M').count().plot(style='-', figsize=(18,10))","32f7b356":"raw_data.abstract.resample('A').count().plot(style='-', figsize=(18,10))","28e70b51":"raw_data.resample('M').count().plot(style='-', figsize=(18,16))","96849d17":"## stemming\nNow we will see how stemming with the porter stemmer the tokenized words will cut off the word ending to get to the root. Now we get `recently` -> `recent`, but also `associated` -> `associ`.  \nprint out the original word next to the stemmed word to check","be6a40ec":"# N-Grams\nLet's look at some of the n-grams. This is just pairs of words (or triplets) that show up together. It will tell us something about our corpus, but also guide us in our next step of vectorization. When we vectorize, we need to choose how many n-grams (words in a row) we want to include as a single feature. For example, in this corpus, going up to three makes sense because we see alot of `amyotrophic lateral sclerosis` and we want that to be treated as one feature. \n","f43eb21c":"# put it together\nLet's stem a tokenized and lowercased text using ","7c8de8b8":"### wordpunct method (NLTK) \nThe word punct method also separates words from punctuation, but it also separated the words that had a hyphen which is something to think about with scientific literature; you may want to leave them connected if they are meant to be one word. ","8b84225a":"# lowercase\nTypically you want to lowercase your words so that `It` is the same as `it`.  The case where this isn't true, is when you are doing named entity recognition (NER), more on this later. ","d656135f":"# Dealing with word endings\n## Lemmatizer\n\nMost of the time in NLP, we want to reduce words to their root form, because the word `diagnoses` has the same meaning (*for our analytical purposes*) as the word `diagnosis` or `diagnosed`. \n\nOne method for getting the word roots is called lemmatization.  This implementation (word net lemmatizer) in nltk appears to be more conservative and also more 'correct' in that it will replace the ending with the correct letters instead of chopping it off. i.e. `organelles` -> `organelle`, `mitochondrion` -> `mitochondria`, but words like `recently` were unaffected.\n\nThus, we run this one first, and then do the stemming on that result","5d8108e9":"# Word Tokenization\nTypically, you would just go straignt to work tokenization if you are planning to do topic modeling. \nThere are MANY, MANY ways to tokenize text into words. I will just show a few, but feel free to explore the possibilities.\n","184f59b5":"# Time\nPlot the data over time. First we can just look at the number of publications (count) each month.  \nWe can see that there is an increase in publications relating to ALS over time","769e61d2":"# Topic modeling\nuse the document term matrix created with vectorization, to create a latent space and find the words that tend to ocurr together\n\nWe will use LDA Latent Dirichlet Allocation here (there are several methods, NMF, SVD)\n\nthis will reduce the data from thousands of terms (dimensions) to 20 topics.\n\nCreates a latent space that is X dimensions.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.LatentDirichletAllocation.html","44c35834":"# non-ASCII characters\nLook at what happens in this abstract, we get some non-ascii characters. We can either deal with them and try to get them to import correctly if they are important, or just remove them. In this case, I don't think they are adding meaningful content, so I will remove them. ","9370c9b7":"## word_tokenize (NLTK)","4efbe5fc":"# Sentence Tokenization\nTokenizing text into sentences is a good first step in exploration. We also use it for looking at sentiment, as you don't want to calculate sentiment of a document on a big long string of words. You would calculate it for each sentence and then go from there to aggregate the scores.  this may be useful here, but academic text tends to come out as neutral... ","60872423":"# Put it all together into one big cleaning function that will clean all documents","3acd48c1":"Plot number of articles per year","3f651f6c":"# Vectorization\nVectorization is the important step of turning our words into numbers. There are 2 common methods: count vectorizer, tf-idf. This function takes each word in each document and counts the number of times the word appears. You end up with each word (and n-gram) as your columns and each row is a document, so the data is the frequency of each word in each document. As you can imagine, there will be a large number of zeros in this matrix; we call this a sparse matrix.\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n\ntf-IDF\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer","d0f9eb4d":"# Noun Phrases\nText blob allows us to pull out interesting things like noun phrases. This could be a good way to generate keywords, or just explore your text.\n","c56595c1":"# Visualize with pyLDAvis\nthis is one of my favorite ways to initially interact with the topics (to tune hyperparams) and share \n\nworks for python (sklearn and Gensim) and R\nsharing is a simple as saving the html\nlink to a demo\n\nhttps:\/\/nbviewer.jupyter.org\/github\/bmabey\/hacker_news_topic_modelling\/blob\/master\/HN%20Topic%20Model%20Talk.ipynb","1a247771":"A second method for tokenizing into sentences","2c807aa7":"# Stopwords\nStopwords are the words that don't give us much information, (i.e., the, and, it, she, as) along with the punctuation. We want to remove these from our text, too. \n\n We can do this by importing NLTKs list of stopwords and then adding to it. "}}