{"cell_type":{"bdeba2bf":"code","29175929":"code","65235c10":"code","e39f2751":"code","73578c84":"code","40a1e7f1":"code","37bd46a5":"code","24f05dcc":"code","68a2fdc0":"code","28b51dc1":"code","fefc7237":"code","95f868da":"code","af0133c1":"code","557fc171":"code","530802fc":"code","4c82cdbb":"code","d54c58b5":"code","7bc7a3e1":"code","fa8fdca5":"code","0cc99e7a":"code","aaebcc84":"code","4508e843":"code","01ee0272":"code","a2d4a73f":"code","a4dfff93":"code","db7c97b9":"code","d8df2c7e":"code","8b9d7fc9":"code","53a02c32":"code","83cdd8a9":"code","cf5b8b31":"code","4bf869a2":"code","8d4d422c":"code","ab482f7d":"code","9b43f687":"code","a3d33fe6":"code","042f3ab0":"code","765c37a6":"code","d794937a":"code","f6ab8bcf":"code","f5ca796b":"code","6664a2bd":"code","0ae7ddf9":"code","f8693fe9":"code","58be8493":"code","3f33e739":"code","c2f0cceb":"code","4fa883be":"code","b556ee70":"code","137f6931":"code","6bcd24a2":"code","c7d2e578":"code","ba0660f5":"code","47ba2424":"code","ac3673bf":"code","6dd523ae":"code","848fe90b":"code","d7635321":"code","7da51c42":"code","62025648":"code","59f8f3e7":"code","2db14118":"code","b90af82d":"code","b6822c40":"code","1ae4ba0c":"code","d1749fbf":"code","a77b55ee":"code","5b85abc9":"code","1cc9dc86":"code","83b43f06":"code","5042e95f":"code","8e1b4642":"code","84713d47":"code","f53ba588":"code","18bfbd92":"code","7c288fb9":"code","82eb5d1b":"code","66143375":"code","197aa9a9":"code","8ba8b5ab":"code","226260cd":"code","957b2a0f":"code","44c1774c":"code","b46d7a88":"code","f2ead3e9":"code","145e8668":"code","41cf2a39":"code","42c2f746":"code","bca019cf":"code","68e7a96b":"code","4ef7625b":"code","5fafd08b":"code","f7026ee4":"code","7de32f0c":"code","fb2b64a3":"code","ea4946b2":"code","c968e6f2":"code","fb754709":"code","3aeb6b28":"code","4a179a8b":"code","8dd62ca8":"code","77041388":"code","54f3e555":"code","df863e98":"code","b8bfb293":"code","f369aba7":"code","36a6fee3":"code","52f01706":"code","2452c8cc":"code","4ff93eb8":"code","ed5c568c":"code","78337242":"code","03944145":"code","0a9c8dfd":"code","36172f50":"code","889994e4":"code","ae464ce1":"code","bbaf03b9":"code","1829dfeb":"code","fd0395d8":"code","7d4bba6a":"code","13166020":"code","521cb822":"code","bf676dd4":"code","18692a2f":"code","3c9261fe":"code","df6c53b4":"code","8fdd5a96":"code","4f574899":"code","c00f5593":"code","0fd5192d":"code","8188a21d":"code","e9d27f01":"code","38aaea21":"code","4c663647":"code","684c1b06":"code","5e4d989d":"code","3f859299":"code","efbe9eb4":"code","7d38531b":"code","096a778d":"code","6d0cb0a9":"code","a1524264":"code","36c09987":"code","aa633fbc":"code","d297c5c9":"code","c46b3497":"code","f141b895":"code","89d849dc":"code","19570791":"code","992a10f2":"code","57a34ef9":"code","a9c9e7fb":"code","8490e856":"code","ed6fec18":"code","1885859d":"code","b64ff724":"code","04504fc7":"code","642aacfd":"code","7c2b8c29":"code","13c26870":"code","33016b1a":"code","a54fdb62":"code","11377739":"code","fa9c17c0":"code","a6bd4db5":"code","d2ca5d11":"code","86dd65b9":"code","3c9d4005":"code","cbc5d61b":"code","ea248ec1":"code","10be5359":"code","e2315d05":"code","668a30e7":"code","4073f856":"code","1f266109":"code","7d8afbd8":"code","0f9659a9":"code","f5e6c124":"code","21fc1553":"code","ac0b7f6f":"code","5142046e":"code","5c7f8464":"code","a2028509":"code","e108b27b":"code","e843e06b":"code","96c84f41":"code","a528ddf6":"code","b74f02b5":"code","95b81c4d":"code","5fd61065":"code","18359c98":"code","67ec84cb":"code","e77afc70":"code","b664be68":"code","a1a8c103":"code","ac5bdb03":"code","b42f50db":"code","6e5663eb":"code","8490e437":"code","885ccbf8":"code","d97033ca":"code","482a9824":"code","a4dbc994":"code","b887c27b":"code","5595a495":"code","e6d3fb6a":"code","9fe72c8f":"code","b2ac9b74":"code","02dca761":"code","bc3236e4":"code","14fd5346":"code","cd8fb9e1":"code","5ce6080a":"code","a4816a61":"code","4e829da1":"code","11372816":"markdown","23e7705d":"markdown","adae4a4b":"markdown","0cbefbd8":"markdown","878d7d74":"markdown","13efc84f":"markdown","2f2da35e":"markdown","03870011":"markdown","2f44680c":"markdown","d5b2f7ec":"markdown","f96f798e":"markdown","cb979bff":"markdown","ad708790":"markdown","5957e19e":"markdown","fda678dc":"markdown","03d16675":"markdown","a108d4b4":"markdown","1fc0b0a7":"markdown","c671d781":"markdown","f8759c90":"markdown","2b2ee276":"markdown","220bcd7f":"markdown","a7ea1a4e":"markdown","9532b758":"markdown","01cb2aa0":"markdown","0207d5d0":"markdown","436898a9":"markdown","43d32a04":"markdown","3f69c64b":"markdown","8977b9dd":"markdown","2cd25fd2":"markdown","2cef7900":"markdown","853787ad":"markdown","acad23c5":"markdown","302a9733":"markdown","02f5386e":"markdown","f741f5ba":"markdown","443a247b":"markdown","5bba1001":"markdown","4349b2af":"markdown","8b3da06a":"markdown","29c25643":"markdown","cc20e7a5":"markdown","0fc7a32b":"markdown","b26ff8e9":"markdown","2597e14e":"markdown","44973f85":"markdown","bc743c9f":"markdown","033e13e1":"markdown","0d10710c":"markdown","408883cc":"markdown","24451683":"markdown","29310738":"markdown","99ba4868":"markdown","b409e1a6":"markdown","6571ecec":"markdown","40b6db9e":"markdown","ca7d921c":"markdown","7372594c":"markdown","9612fb4f":"markdown","4e599ca0":"markdown","a95bb74d":"markdown","90f690c7":"markdown","29e5f300":"markdown","4ea484c9":"markdown","e1440726":"markdown","8db1c5e0":"markdown","09d00018":"markdown","611973aa":"markdown","92e47996":"markdown","3b1d6456":"markdown","109aab10":"markdown","f71553e5":"markdown","1c3bf148":"markdown","e12a1b1d":"markdown","e33e6d95":"markdown","4e911638":"markdown","31b978cd":"markdown","c7eff6aa":"markdown","1b203944":"markdown","fbef3c19":"markdown","ed027e35":"markdown","4cd0e3bc":"markdown","f349dba8":"markdown","f087f229":"markdown","f51cb916":"markdown","64030dbd":"markdown","53406bbb":"markdown","b29eab0f":"markdown","27e789af":"markdown","f9b036f2":"markdown","7b4207d9":"markdown","b8f51c5e":"markdown","1150e881":"markdown","367141a9":"markdown","35288631":"markdown","96ad40db":"markdown","c0606808":"markdown","51364823":"markdown","32b75dbc":"markdown","f19512cd":"markdown","cd3df1b7":"markdown","4fa4136d":"markdown","21a7f227":"markdown","8eee5462":"markdown","da08b663":"markdown","685b6587":"markdown","301c839a":"markdown","8d36a344":"markdown","53cd4698":"markdown"},"source":{"bdeba2bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29175929":"# !pip install pyforest\n!pip install pyclustertend\n\n# 1-Import Libraies\n\nimport pandas_profiling\n# import pyforest\n\nimport ipywidgets\nfrom ipywidgets import interact\n\nimport numpy as np\nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.ticker as mticker\nimport squarify as sq\n\n# Importing plotly and cufflinks in offline mode\nimport plotly \nimport plotly.express as px\nimport cufflinks as cf\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# !pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # makes strings colored\nfrom termcolor import colored\nfrom termcolor import cprint\n\nfrom wordcloud import WordCloud\n\nimport scipy.stats as stats\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \n\nimport datetime as dt\nfrom datetime import datetime\n\nfrom pyclustertend import hopkins\n\nimport optuna\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor \nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve \nfrom sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, cross_validate\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, scale, StandardScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, PolynomialFeatures, PowerTransformer  \nfrom sklearn.svm import SVR, SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree \n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBRegressor, XGBClassifier, plot_importance\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","65235c10":"## Some Useful Functions\n\n###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending = False)\n    missing_percent = (df.isnull().sum() \/ df.isnull().count()).sort_values(ascending = False)\n    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number'] > 0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('*'*100, 'red', attrs = ['bold']),\n          colored(\"\\nInfo:\\n\", attrs = ['bold']), sep = '')\n    print(df.info(), '\\n', \n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"Number of Uniques:\\n\", attrs = ['bold']), df.nunique(),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"All Columns:\", attrs = ['bold']), list(df.columns),'\\n', \n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n\n    df.columns = df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '')  \n    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"Descriptive Statistics \\n\", attrs = ['bold']), df.describe().round(2),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n    print(colored(\"Descriptive Statistics (Categorical Columns) \\n\", attrs = ['bold']), df.describe(include = object).T,'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n    \ndef multicolinearity_control(df):\n    feature = []\n    collinear = []\n    for col in df.corr().columns:\n        for i in df.corr().index:\n            if (abs(df.corr()[col][i]) > .9 and abs(df.corr()[col][i]) < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n                                  \"red\", attrs = ['bold']), df.shape,'\\n',\n                                  colored('*'*100, 'red', attrs = ['bold']), sep = '')\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", attrs = ['bold']), sep = '')\n    print(\"There are\", df.duplicated(subset = None, keep = 'first').sum(), \"duplicated observations in the dataset.\")\n    duplicate_values = df.duplicated(subset = None, keep = 'first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep = 'first', inplace = True)\n        print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n              colored('*'*100, 'red', attrs = ['bold']), sep = '')\n#     else:\n#         print(colored(\"There are no duplicates\"),'\\n',\n#               colored('*'*100, 'red', attrs = ['bold']), sep = '')     \n        \n# def drop_columns(df, drop_columns):\n#     if drop_columns != []:\n#         df.drop(drop_columns, axis = 1, inplace = True)\n#         print(drop_columns, 'were dropped')\n#     else:\n#         print(colored('We will now check the missing values and if necessary, the related columns will be dropped!', attrs = ['bold']),'\\n',\n#               colored('*'*100, 'red', attrs = ['bold']), sep = '')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i] \/ df.shape[0]*100) > limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis = 1, inplace = True)\n            print('new shape:', df.shape)       \n    print('New shape after missing value control:', df.shape)\n    \n###############################################################################\n\n# To view summary information about the columns\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"Per_of_Nulls   : \", \"%\", round(df[col].isnull().sum() \/ df.shape[0]*100, 2))\n    print(\"Num_of_Nulls   : \", df[col].isnull().sum())\n    print(\"Num_of_Uniques : \", df[col].nunique())\n    print(\"Duplicates     : \", df.duplicated(subset = None, keep = 'first').sum())\n    print(df[col].value_counts(dropna = False))\n    \n###############################################################################\n\ndef fill_most(df, group_col, col_name):\n    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n    for group in list(df[group_col].unique()):\n        cond = df[group_col] == group\n        mode = list(df[cond][col_name].mode())\n        if mode != []:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n        else:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna = False))\n    \n###############################################################################    \n# bar grafi\u011findeki de\u011ferlerin g\u00f6sterilmesi\n# show values in bar graphic\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() \/ 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","e39f2751":"df0 = pd.read_csv('..\/input\/hr-dataset\/HR_Dataset.csv')\ndf = df0.copy()\ndf.head(3)","73578c84":"# df0 = pd.read_csv('HR_Dataset.csv')\n# df = df0.copy()\n# df.head(3) ","40a1e7f1":"df.tail(3) ","37bd46a5":"df.sample(3)","24f05dcc":"first_looking(df)\nduplicate_values(df)\nprint(colored(\"Shape:\", attrs = ['bold']), df.shape,'\\n', colored('*'*100, 'red', attrs = ['bold']))","68a2fdc0":"df.columns","28b51dc1":"df.rename({'departments_': 'department'}, axis=1, inplace=True)\ndf.head(1)","fefc7237":"# Alternative code\n# last_column = df.pop('left')\n# df.insert(9, 'left', last_column)\n# df.head(1)\n\ndf = df[['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_montly_hours', 'time_spend_company', 'work_accident',\n       'promotion_last_5years', 'department', 'salary', 'left']]\ndf.head(1)","95f868da":"cprint(\"Have a First Look to 'left' Column\",'green')\nfirst_look('left')","af0133c1":"# df['left'].value_counts().iplot(kind=\"bar\", title = '\"left\" Column Distribution')","557fc171":"fig = px.pie(df, values = df['left'].value_counts(), \n             names = (df['left'].value_counts()).index, \n             title = '\"left\" Column Distribution')\nfig.show()","530802fc":"y = df['left']\nprint(f'Percentage of left-1: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} observations for left-1)\\nPercentage of left-0: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} observations for left-0)')","4c82cdbb":"df.groupby('left').mean()","d54c58b5":"cprint('Dataset describe results according to the \"left==1\" condition','green', 'on_red')\ndf[df['left'] == 1].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')","7bc7a3e1":"cprint('Dataset describe results according to the \"left==0\" condition','green', 'on_red')\ndf[df['left'] == 0].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')","fa8fdca5":"cprint(\"Have a First Look to 'left' Column\",'green', 'on_red')\nfirst_look('satisfaction_level')","0cc99e7a":"pd.crosstab(df['satisfaction_level'], df['left']).iplot(kind='bar', title = 'satisfaction_level and left')","aaebcc84":"cprint(\"Have a First Look to 'last_evaluation' Column\",'green', 'on_red')\nfirst_look('last_evaluation')","4508e843":"pd.crosstab(df['last_evaluation'], df['left']).iplot(kind='bar', title = 'last_evaluation and left')","01ee0272":"fig = px.strip(df, x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n               title = \"'satisfaction_level' & 'last_evaluation'\")\nfig.show()","a2d4a73f":"fig = px.violin(df[['satisfaction_level', 'last_evaluation', 'left']], color = 'left', box = True, points='all',\n                title = \"'satisfaction_level' & 'last_evaluation'\")\nfig.show()","a4dfff93":"cprint(\"Have a First Look to 'number_project' Column\",'green', 'on_red')\nfirst_look('number_project')","db7c97b9":"# df['number_project'].value_counts().iplot(kind=\"bar\", title = '\"number_project\" Column Distribution')","d8df2c7e":"fig = px.pie(df, values = df['number_project'].value_counts(), \n             names = (df['number_project'].value_counts()).index, \n             title = '\"number_project\" Column Distribution')\nfig.show()","8b9d7fc9":"pd.crosstab(df['number_project'], df['left']).iplot(kind='bar', title = 'number_project and left')","53a02c32":"fig = px.strip(df[df['number_project'] == 2], x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n               title = \"'satisfaction_level' & 'last_evaluation' when 'number_project' == 2\")\nfig.show()","83cdd8a9":"fig = px.strip(df[df['number_project'] > 4], x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n               title = \"'satisfaction_level' & 'last_evaluation' when 'number_project' > 4\")\nfig.show()","cf5b8b31":"fig = px.strip(df[df['number_project'] == 7], x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n               title = \"'satisfaction_level' & 'last_evaluation' when 'number_project' == 7\")\nfig.show()","4bf869a2":"cprint(\"Have a First Look to 'average_montly_hours' Column\",'green', 'on_red')\nfirst_look('average_montly_hours')","8d4d422c":"pd.crosstab(df['average_montly_hours'], df['left']).iplot(kind='bar', title = 'average_montly_hours and left')","ab482f7d":"plt.figure(figsize = (16,6))\nsns.lineplot(data = df, x = 'average_montly_hours', y = 'number_project', hue = 'left')\nplt.title(\"'average_montly_hours' & 'number_project'\");","9b43f687":"plt.figure(figsize = (16,6))\nsns.lineplot(data = df, y = 'average_montly_hours', x = 'number_project')\nplt.title(\"'average_montly_hours' & 'number_project'\");","a3d33fe6":"cprint(\"Have a First Look to 'time_spend_company' Column\",'green', 'on_red')\nfirst_look('number_project')","042f3ab0":"# df['time_spend_company'].value_counts().iplot(kind=\"bar\", title = '\"time_spend_company\" Column Distribution')","765c37a6":"fig = px.pie(df, values = df['time_spend_company'].value_counts(), \n             names = (df['time_spend_company'].value_counts()).index, \n             title = '\"time_spend_company\" Column Distribution')\nfig.show()","d794937a":"pd.crosstab(df['time_spend_company'], df['left']).iplot(kind='bar', title = 'time_spend_company and left')","f6ab8bcf":"fig = px.strip(df[df['left'] == 1], x = 'satisfaction_level', y = 'last_evaluation', color = 'time_spend_company',\n               title =\"'satisfaction_level' & 'last_evaluation'\")\nfig.show()","f5ca796b":"fig = px.strip(df[df['left'] == 0], x = 'satisfaction_level', y = 'last_evaluation', color = 'time_spend_company')\nfig.show()","6664a2bd":"fig = px.box(df, x = 'time_spend_company', y = 'average_montly_hours', title = \"'time_spend_company' & 'average_montly_hours'\")\nfig.show()","0ae7ddf9":"fig = px.box(df, x = 'time_spend_company', y = 'number_project', title = \"'time_spend_company' & 'number_project'\")\nfig.show()","f8693fe9":"px.histogram(df, x = 'time_spend_company', color = 'number_project', title = \"'time_spend_company' & 'number_project'\")","58be8493":"cprint(\"Have a First Look to 'work_accident' Column\",'green', 'on_red')\nfirst_look('work_accident')","3f33e739":"# df['work_accident'].value_counts().iplot(kind=\"bar\", title = '\"work_accident\" Column Distribution')","c2f0cceb":"fig = px.pie(df, values = df['work_accident'].value_counts(), \n             names = (df['work_accident'].value_counts()).index, \n             title = '\"work_accident\" Column Distribution')\nfig.show()","4fa883be":"pd.crosstab(df['work_accident'], df['left']).iplot(kind='bar', title = 'work_accident and left')","b556ee70":"px.histogram(df, x = df['average_montly_hours'], color='work_accident', title = 'work_accident and average_montly_hours')","137f6931":"px.histogram(df, x = df['time_spend_company'], color='work_accident', title = 'work_accident and time_spend_company')","6bcd24a2":"cprint(\"Have a First Look to 'promotion_last_5years' Column\",'green', 'on_red')\nfirst_look('promotion_last_5years')","c7d2e578":"# df['promotion_last_5years'].value_counts().iplot(kind=\"bar\", title = '\"promotion_last_5years\" Column Distribution')","ba0660f5":"fig = px.pie(df, values = df['promotion_last_5years'].value_counts(), \n             names = (df['promotion_last_5years'].value_counts()).index, \n             title = '\"promotion_last_5years\" Column Distribution')\nfig.show()","47ba2424":"pd.crosstab(df['promotion_last_5years'], df['left']).iplot(kind='bar', title = 'promotion_last_5years and left')","ac3673bf":"fig = px.strip(df, x = 'satisfaction_level', y = 'last_evaluation', color = 'promotion_last_5years',\n               title = \"'satisfaction_level' & 'last_evaluation'\")\nfig.show()","6dd523ae":"px.histogram(df, x = df['time_spend_company'], color='promotion_last_5years', title = 'time_spend_company')","848fe90b":"px.histogram(df[df['promotion_last_5years'] == 1], x = df['time_spend_company'], title = 'promotion_last_5years')","d7635321":"cprint(\"Have a First Look to 'department' Column\",'green', 'on_red')\nfirst_look('department')","7da51c42":"# df['department'].value_counts().iplot(kind=\"bar\", title = '\"department\" Column Distribution')","62025648":"fig = px.pie(df, values = df['department'].value_counts(), \n             names = (df['department'].value_counts()).index, \n             title = '\"department\" Column Distribution')\nfig.show()","59f8f3e7":"cprint('left, not_left values and left percentage','green', 'on_red')\ndf_dep = pd.DataFrame(pd.crosstab(df['department'], df['left']))\ndf_dep.rename(columns = {0 : 'not_left', 1 : 'left'}, inplace = True)\ndf_dep = df_dep.assign(total = lambda x: (x['not_left'] + x['left']))\ndf_dep = df_dep.assign(left_percentage = lambda x: (x['left'] \/ x['total'] * 100))\ndf_dep","2db14118":"pd.crosstab(df['department'], df['left']).iplot(kind='bar', title = 'department and left')","b90af82d":"cprint(\"Have a First Look to 'salary' Column\",'green', 'on_red')\nfirst_look('salary')","b6822c40":"# df['salary'].value_counts().iplot(kind=\"bar\", title = '\"salary\" Column Distribution')","1ae4ba0c":"fig = px.pie(df, values = df['salary'].value_counts(), \n             names = (df['salary'].value_counts()).index, \n             title = '\"salary\" Column Distribution')\nfig.show()","d1749fbf":"pd.crosstab(df['salary'], df['left']).iplot(kind='bar', title = 'salary and left')","a77b55ee":"numerical= df.drop(['left'], axis = 1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint('---------------------')\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint(f'Categorical Columns: {df[categorical].columns}')\nprint('---------------------')","5b85abc9":"cprint(\"The describe values of the numerical columns\",'green', 'on_red')\ndf[numerical].describe().T.style.background_gradient(subset = ['mean','std','50%','count'], cmap = 'RdPu')","1cc9dc86":"df[numerical].iplot(kind = 'histogram', subplots = True, bins = 50, title = 'Histogram visualization of the numerical columns')","83b43f06":"for i in numerical:\n    df[i].iplot(kind = 'box', title = i, boxpoints = 'all')","5042e95f":"cprint(\"The pairplot visualization of the numerical columns\",'green', 'on_red')\nsns.pairplot(df, hue = \"left\", corner = True);","8e1b4642":"cprint(\"Heatmap of the numerical columns\",'green', 'on_red')\n\nplt.figure(figsize = (12, 8))\nsns.heatmap (df.corr(), annot = True, fmt = '.2f', vmin = -1, vmax = 1)\nplt.xticks(rotation = 45);","84713d47":"cprint(\"Multicollinearity among the features\",'green', 'on_red')\n\ndf_temp = df.corr()\n\ncount = 'Done'\nfeature =[]\ncollinear= []\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i] > .9 and df_temp[col][i] < 1) or (df_temp[col][i] < -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f'\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}')\n        else:\n            print(f'For {col} and {i}, there is NO multicollinearity problem') \n\nprint('\\033[1mThe number of strong corelated features:\\033[0m', count) ","f53ba588":"df.corr()['left'].sort_values().drop('left').iplot(kind = 'barh');","18bfbd92":"cprint('\"left\" Column Distribution','green', 'on_red')\ndf.left.value_counts()","7c288fb9":"# with plotly\n# df['left'].value_counts().iplot(kind=\"bar\", title = '\"left\" Column Distribution')","82eb5d1b":"# with seaborn\n# plt.figure(figsize = (7,5))\n# sns.countplot(data = df, x = 'left');\n# for index,value in enumerate(df.left.value_counts()):\n#     plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13);","66143375":"cprint('\"left\" Column Distribution','green', 'on_red')\nfig = plt.figure(figsize = (11,6))\nax = fig.add_axes([0,0,1,1])\nax.bar(df.left.value_counts().index, df.left.value_counts().values, color = 'green')\nplt.title('\"left\" Column Distribution')   \nplt.xlabel('left') \nplt.ylabel('Number of Employees') \nfor index,value in enumerate(df.left.value_counts()):\n    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\nplt.show()","197aa9a9":"cprint('\"number_project\" Column Distribution','green', 'on_red')\ndf.number_project.value_counts()","8ba8b5ab":"# with plotly\n# df['number_project'].value_counts().iplot(kind=\"bar\", title = '\"number_project\" Column Distribution')","226260cd":"# with seaborn\n# plt.figure(figsize = (7,5))\n# sns.countplot(data = df, x = 'number_project');\n# for index,value in enumerate(df.left.value_counts()):\n#     plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13);","957b2a0f":"cprint('\"number_project\" Column Distribution','green', 'on_red')\nfig = plt.figure(figsize = (11,6))\nax = fig.add_axes([0,0,1,1])\n# x = df.number_project.value_counts().index\n# y = df.number_project.value_counts().values\ndf.number_project.value_counts().plot(kind = \"bar\", color = \"orange\")\nplt.title('\"number_project\" Column Distribution')   \nplt.xlabel('number_project') \nplt.ylabel('Number of Employees')\nplt.xticks(rotation = 0)\nfor index,value in enumerate(df.number_project.value_counts().sort_values(ascending=False)):\n    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\nplt.show()","44c1774c":"cprint('\"time_spend_company\" Column Distribution','green', 'on_red')\ndf.time_spend_company.value_counts()","b46d7a88":"# with plotly\n# df['time_spend_company'].value_counts().iplot(kind=\"bar\", title = '\"time_spend_company\" Column Distribution')","f2ead3e9":"# with seaborn\n# plt.figure(figsize = (7,5))\n# sns.countplot(data = df, x = 'time_spend_company');\n# for index,value in enumerate(df.left.value_counts()):\n#     plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13);","145e8668":"cprint('\"time_spend_company\" Column Distribution','green', 'on_red')\nfig = plt.figure(figsize = (11,6))\nax = fig.add_axes([0,0,1,1])\ndf.time_spend_company.value_counts().plot(kind = \"bar\", color = \"pink\")\nplt.title('\"time_spend_company\" Column Distribution')   \nplt.xlabel('time_spend_company') \nplt.ylabel('Number of Employees')\nplt.xticks(rotation = 0)\nfor index,value in enumerate(df.time_spend_company.value_counts().sort_values(ascending=False)):\n    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\nplt.show()","41cf2a39":"for i in df:\n    df[i].iplot(kind = 'histogram', subplots = True, bins = 50, title = 'Subplots of Features')","42c2f746":"cprint('New df for Kmeans clustering','green', 'on_red')\ndf1 = df.drop('left', axis = 1)\ndf1.head(1)","bca019cf":"cprint('New df after getting dummied','green', 'on_red')\ndf1 = pd.get_dummies(df1, columns = ['department','salary'], drop_first = True)\ndf1.head(1)","68e7a96b":"df1.head()","4ef7625b":"cprint('Scaling','green', 'on_red')\nscaler = MinMaxScaler()\nscaler.fit(df1)\n#Store it separately for clustering\ndf1_scaled= scaler.transform(df1)","5fafd08b":"df1_scaled","f7026ee4":"cprint('Hopkins value','green', 'on_red')\nhopkins(df1_scaled,df1.shape[0])","7de32f0c":"#First : Get the Best KMeans \nks = range(1,10)\ninertias=[]\nfor k in ks :\n    # Create a KMeans clusters\n    kc = KMeans(n_clusters=k,random_state=1)\n    kc.fit(df1_scaled)\n    inertias.append(kc.inertia_)\n\n# Plot ks vs inertias\nf, ax = plt.subplots(figsize=(8, 6))\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.style.use('ggplot')\nplt.title('What is the Best Number for KMeans ?')\nplt.show()","fb2b64a3":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k = (1,10))\nvisu.fit(df1_scaled)\nvisu.show();","ea4946b2":"cprint(\"Silhouette Scores\",'green', 'on_red')\n\nssd =[]\n\nK = range(2,10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(df1_scaled)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(df1_scaled, model.labels_)}')","c968e6f2":"cprint(\"Silhouette Plot for K=3\",'green', 'on_red')\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel_3 = KMeans(n_clusters = 3, random_state = 101)\nvisualizer = SilhouetteVisualizer(model_3)\nvisualizer.fit(df1_scaled)\nvisualizer.poof();","fb754709":"cprint(\"Silhouette Plot for K=4\",'green', 'on_red')\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel_4 = KMeans(n_clusters = 4, random_state = 101)\nvisualizer = SilhouetteVisualizer(model_4)\nvisualizer.fit(df1_scaled)\nvisualizer.poof();","3aeb6b28":"cprint(\"Silhouette Plot for K=2\",'green', 'on_red')\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel_2 = KMeans(n_clusters = 2, random_state = 101)\nvisualizer = SilhouetteVisualizer(model_2)\nvisualizer.fit(df1_scaled)\nvisualizer.poof();","4a179a8b":"cprint(\"KMeans Clustering with K=2\",'green', 'on_red')\n\nk_means_model2 = KMeans(n_clusters = 2, random_state = 101)\nk_means_model2.fit_predict(df1_scaled)\nlabels = k_means_model2.labels_\nlabels","8dd62ca8":"cprint(\"Predicted clusters on our dataframe\",'green', 'on_red')\n\ndf['predicted_clusters'] = labels\ndf","77041388":"cprint('\"predicted_clusters\" value counts','green', 'on_red')\ndf['predicted_clusters'].value_counts()","54f3e555":"fig = px.pie(df, values = df['predicted_clusters'].value_counts(), \n             names = (df['predicted_clusters'].value_counts()).index, \n             title = 'Predicted_Clusters Distribution')\nfig.show()","df863e98":"fig = px.pie(df, values = df[df['left']==0]['predicted_clusters'].value_counts(), \n             names = df[df['left']==0]['predicted_clusters'].value_counts().index, \n             title = 'Predicted_Clusters & left==0 Distribution')\nfig.show()","b8bfb293":"fig = px.pie(df, values = df[df['left']==1]['predicted_clusters'].value_counts(), \n             names = df[df['left']==1]['predicted_clusters'].value_counts().index, \n             title = 'Predicted_Clusters & left==1 Distribution')\nfig.show()","f369aba7":"cprint('Mean values according to the left','green', 'on_red')\ndf.groupby('left').mean()","36a6fee3":"cprint('Mean values according to the left and predicted clusters','green', 'on_red')\ndf.groupby(['left', 'predicted_clusters']).mean()","52f01706":"pd.crosstab(df['left'], df['predicted_clusters']).iplot(kind=\"bar\", title = 'Compare (left vs predicted_clusters)',\n            xTitle = 'left & clusters', yTitle = 'counts')","2452c8cc":"cprint('Mean values according to the predicted clusters','green', 'on_red')\ndf.groupby('predicted_clusters').mean()","4ff93eb8":"cprint('Mean values according to the predicted clusters and left','green', 'on_red')\ndf.groupby(['predicted_clusters', 'left']).mean()","ed5c568c":"pd.crosstab(df['predicted_clusters'], \n            df['left']).iplot(kind=\"bar\", title = 'Compare (predicted_clusters vs left)',\n            xTitle = 'clusters & left', yTitle = 'counts')","78337242":"cprint('Mean values of the features according to the predicted clusters','green', 'on_red')\n\nsns.lineplot(data = df.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]].groupby(\"predicted_clusters\").mean().T)\nplt.xticks(rotation = 60)","03944145":"# kmeans_model = pickle.dump(k_means_model, open('kmeans_model', 'wb'))","0a9c8dfd":"cprint('New df for Classification','green', 'on_red')\ndf2 = df.drop('predicted_clusters', axis = 1)\ndf2.head(1)","36172f50":"cprint('New df after getting dummied','green', 'on_red')\ndf2 = pd.get_dummies(df2, columns = ['department','salary'], drop_first = True)\ndf2.head(1)","889994e4":"X = df2.drop('left', axis = 1)\ny = df2['left']","ae464ce1":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 101)","bbaf03b9":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","1829dfeb":"def eval(model, X_train, X_test):\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    \n    print(confusion_matrix(y_test, y_pred))\n    print(\"Test_Set\")\n    print(classification_report(y_test,y_pred))\n    print(\"Train_Set\")\n    print(classification_report(y_train,y_pred_train))\n    plot_confusion_matrix(model, X_test, y_test, cmap=\"plasma\")","fd0395d8":"def train_val(y_train, y_train_pred, y_test, y_pred):\n    \n    scores = {\"train_set\": {\"Accuracy\" : accuracy_score(y_train, y_train_pred),\n                            \"Precision\" : precision_score(y_train, y_train_pred),\n                            \"Recall\" : recall_score(y_train, y_train_pred),                          \n                            \"f1\" : f1_score(y_train, y_train_pred)},\n    \n              \"test_set\": {\"Accuracy\" : accuracy_score(y_test, y_pred),\n                           \"Precision\" : precision_score(y_test, y_pred),\n                           \"Recall\" : recall_score(y_test, y_pred),                          \n                           \"f1\" : f1_score(y_test, y_pred)}}\n    \n    return pd.DataFrame(scores)","7d4bba6a":"GB_model = GradientBoostingClassifier(random_state = 101)\nGB_model.fit(X_train, y_train)\ny_pred = GB_model.predict(X_test)\ny_train_pred = GB_model.predict(X_train)\n\nGB_model_f1 = f1_score(y_test, y_pred)\nGB_model_acc = accuracy_score(y_test, y_pred)\nGB_model_recall = recall_score(y_test, y_pred)\nGB_model_auc = roc_auc_score(y_test, y_pred)","13166020":"print(\"GB_Model\")\nprint (\"------------------\")\neval(GB_model, X_train, X_test)","521cb822":"cprint('GB_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","bf676dd4":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(GB_model)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","18692a2f":"GB_feature_imp = pd.DataFrame(index=X.columns, data = GB_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\nGB_feature_imp","3c9261fe":"fig = px.bar(GB_feature_imp.sort_values('Importance', ascending = False), x = GB_feature_imp.sort_values('Importance', \n             ascending = False).index, y = 'Importance', title = \"Feature Importance\", \n             labels = dict(x = \"Features\", y =\"Feature_Importance\"))\nfig.show()","df6c53b4":"GB_cv = GradientBoostingClassifier(random_state = 101)\nGB_cv_scores = cross_validate(GB_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nGB_cv_scores = pd.DataFrame(GB_cv_scores, index = range(1, 11))\nGB_cv_scores.mean()[2:]","8fdd5a96":"param_grid = {\"n_estimators\":[100, 200, 300],\n              \"subsample\":[0.5, 1], \n              \"max_features\" : [None, 2, 3, 4],\n              \"learning_rate\": [0.001, 0.01, 0.1], \n              'max_depth':[3, 4, 5, 6]} ","4f574899":"GB_grid = GradientBoostingClassifier(random_state = 101)\nGB_grid_model = GridSearchCV(GB_grid, param_grid, scoring = \"f1\", verbose = 2, n_jobs = -1).fit(X_train, y_train)","c00f5593":"GB_grid_model.best_estimator_","0fd5192d":"print(colored('\\033[1mBest Parameters of GridSearchCV for Gradient Boosting Model:\\033[0m', 'blue'), colored(GB_grid_model.best_params_, 'red'))","8188a21d":"GB_tuned = GradientBoostingClassifier(learning_rate = 0.01,\n                                      max_depth = 6, \n                                      n_estimators = 200,\n                                      subsample = 0.5,\n                                      random_state = 101).fit(X_train, y_train)","e9d27f01":"y_pred = GB_tuned.predict(X_test)\ny_train_pred = GB_tuned.predict(X_train)\n\nGB_tuned_f1 = f1_score(y_test, y_pred)\nGB_tuned_acc = accuracy_score(y_test, y_pred)\nGB_tuned_recall = recall_score(y_test, y_pred)\nGB_tuned_auc = roc_auc_score(y_test, y_pred)","38aaea21":"print(\"GB_tuned\")\nprint (\"------------------\")\neval(GB_tuned, X_train, X_test)","4c663647":"cprint('GB_tuned Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","684c1b06":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(GB_tuned)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","5e4d989d":"plot_roc_curve(GB_model, X_test, y_test);","3f859299":"plot_precision_recall_curve(GB_model, X_test, y_test);","efbe9eb4":"cprint('GB_tuned Predictions','green', 'on_red')\nGB_Pred = {\"Actual\": y_test, \"GB_Pred\":y_pred}\nGB_Pred = pd.DataFrame.from_dict(GB_Pred)\nGB_Pred.head()","7d38531b":"cprint('Predictions','green', 'on_red')\nModel_Preds = GB_Pred\nModel_Preds.head()","096a778d":"# gradient_boosting_classifier = pickle.dump(GB_tuned, open('gradient_boosting_model', 'wb'))","6d0cb0a9":"KNN_model = KNeighborsClassifier(n_neighbors = 5)\nKNN_model.fit(X_train, y_train)\ny_pred = KNN_model.predict(X_test)\ny_train_pred = KNN_model.predict(X_train)\n\nKNN_model_f1 = f1_score(y_test, y_pred)\nKNN_model_acc = accuracy_score(y_test, y_pred)\nKNN_model_recall = recall_score(y_test, y_pred)\nKNN_model_auc = roc_auc_score(y_test, y_pred)","a1524264":"print(\"KNN_Model\")\nprint (\"------------------\")\neval(KNN_model, X_train, X_test)","36c09987":"cprint('KNN_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","aa633fbc":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(KNN_model)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","d297c5c9":"KNN_cv = KNeighborsClassifier(n_neighbors = 5)\nKNN_cv_scores = cross_validate(KNN_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nKNN_cv_scores = pd.DataFrame(KNN_cv_scores, index = range(1, 11))\nKNN_cv_scores.mean()[2:]","c46b3497":"test_error_rates = []\n\nfor k in range(1, 30):\n    KNN = KNeighborsClassifier(n_neighbors = k)\n    KNN.fit(X_train, y_train) \n   \n    y_pred = KNN.predict(X_test)\n    \n    test_error = 1 - accuracy_score(y_test, y_pred)\n    test_error_rates.append(test_error)\n    \nprint(test_error_rates)\n\nplt.figure(figsize = (15, 8))\nplt.plot(range(1, 30), test_error_rates, color = 'blue', linestyle = '--', marker = 'o',\n         markerfacecolor = 'red', markersize = 10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K_values')\nplt.ylabel('Error Rate')\n\nplt.hlines(y = 0.04307948860478039, xmin = 0, xmax = 30, colors = 'r', linestyles = \"--\", label = \"K-Value = 2 Line\")\nplt.hlines(y = 0.04558087826570312, xmin = 0, xmax = 30, colors = 'r', linestyles = \"--\", label = \"K-Value = 4 Line\")\nplt.hlines(y = 0.055864369093941, xmin = 0, xmax = 30, colors = 'blue', linestyles = \"--\", label = \"Default K-Value = 5 Line\")\nplt.legend(prop = {\"size\":14});","f141b895":"# FIRST A QUICK COMPARISON TO OUR DEFAULT K=5\n\nknn5 = KNeighborsClassifier(n_neighbors = 5)\n\nknn5.fit(X_train,y_train)\npred = knn5.predict(X_test)\n\nprint('WITH K=5')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","89d849dc":"# NOW K=2\n\nknn2 = KNeighborsClassifier(n_neighbors = 2)\n\nknn2.fit(X_train,y_train)\npred = knn2.predict(X_test)\n\nprint('WITH K=2')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","19570791":"# NOW K=4\n\nknn4 = KNeighborsClassifier(n_neighbors = 4)\n\nknn4.fit(X_train,y_train)\npred = knn4.predict(X_test)\n\nprint('WITH K=4')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","992a10f2":"# NOW K=6\n\nknn6 = KNeighborsClassifier(n_neighbors = 6)\n\nknn6.fit(X_train,y_train)\npred = knn6.predict(X_test)\n\nprint('WITH K=6')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","57a34ef9":"# NOW K=8\n\nknn8 = KNeighborsClassifier(n_neighbors = 8)\n\nknn8.fit(X_train,y_train)\npred = knn8.predict(X_test)\n\nprint('WITH K=8')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","a9c9e7fb":"# NOW K=10\n\nknn10 = KNeighborsClassifier(n_neighbors = 10)\n\nknn10.fit(X_train,y_train)\npred = knn10.predict(X_test)\n\nprint('WITH K=10')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","8490e856":"k_values = range(1, 30)\nparam_grid = {\"n_neighbors\": k_values, \"p\": [1, 2], \"weights\": ['uniform', \"distance\"]}","ed6fec18":"KNN_grid = KNeighborsClassifier()\nKNN_grid_model = GridSearchCV(KNN_grid, param_grid, cv = 10, scoring = 'recall')\nKNN_grid_model.fit(X_train, y_train)","1885859d":"KNN_grid_model.best_estimator_","b64ff724":"print(colored('\\033[1mBest Parameters of GridSearchCV for KNN Model:\\033[0m', 'blue'), colored(KNN_grid_model.best_params_, 'red'))","04504fc7":"# NOW WITH K=3\n\nKNN_tuned3 = KNeighborsClassifier(n_neighbors = 3, p = 1, weights = 'distance')\nKNN_tuned3.fit(X_train, y_train)\ny_pred = KNN_tuned3.predict(X_test)\ny_train_pred = KNN_tuned3.predict(X_train)\n\nKNN_tuned3_f1 = f1_score(y_test, y_pred)\nKNN_tuned3_acc = accuracy_score(y_test, y_pred)\nKNN_tuned3_recall = recall_score(y_test, y_pred)\nKNN_tuned3_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"KNN_tuned (K=3)\")\nprint (\"------------------\")\neval(KNN_tuned3, X_train, X_test)\ntrain_val(y_train, y_train_pred, y_test, y_pred)","642aacfd":"# NOW WITH K=1\n\nKNN_tuned1 = KNeighborsClassifier(n_neighbors = 1, p = 1, weights = 'distance')\nKNN_tuned1.fit(X_train, y_train)\ny_pred = KNN_tuned1.predict(X_test)\ny_train_pred = KNN_tuned1.predict(X_train)\n\nKNN_tuned1_f1 = f1_score(y_test, y_pred)\nKNN_tuned1_acc = accuracy_score(y_test, y_pred)\nKNN_tuned1_recall = recall_score(y_test, y_pred)\nKNN_tuned1_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"KNN_tuned (K=1)\")\nprint (\"------------------\")\neval(KNN_tuned1, X_train, X_test)","7c2b8c29":"plot_roc_curve(KNN_model, X_test, y_test);","13c26870":"plot_precision_recall_curve(KNN_model, X_test, y_test);","33016b1a":"cprint('KNN_tuned Predictions','green', 'on_red')\nKNN_Pred = {\"Actual\": y_test, \"KNN_Pred\":y_pred}\nKNN_Pred = pd.DataFrame.from_dict(KNN_Pred)\nKNN_Pred.head()","a54fdb62":"cprint('Predictions','green', 'on_red')\nKNN_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, KNN_Pred, left_index = True, right_index = True)\nModel_Preds.head()","11377739":"# kneighbors_classifier = pickle.dump(KNN_tuned3, open('kneighbors_model', 'wb'))","fa9c17c0":"RF_model = RandomForestClassifier(class_weight = \"balanced\", random_state = 101)\nRF_model.fit(X_train, y_train)\ny_pred = RF_model.predict(X_test)\ny_train_pred = RF_model.predict(X_train)\n\nRF_model_f1 = f1_score(y_test, y_pred)\nRF_model_acc = accuracy_score(y_test, y_pred)\nRF_model_recall = recall_score(y_test, y_pred)\nRF_model_auc = roc_auc_score(y_test, y_pred)","a6bd4db5":"print(\"RF_Model\")\nprint (\"------------------\")\neval(RF_model, X_train, X_test)","d2ca5d11":"cprint('RF_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","86dd65b9":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(RF_model)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","3c9d4005":"RF_feature_imp = pd.DataFrame(index=X.columns, data = RF_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\nRF_feature_imp","cbc5d61b":"fig = px.bar(RF_feature_imp.sort_values('Importance', ascending = False), x = RF_feature_imp.sort_values('Importance', \n             ascending = False).index, y = 'Importance', title = \"Feature Importance\", \n             labels = dict(x = \"Features\", y =\"Feature_Importance\"))\nfig.show()","ea248ec1":"RF_cv = RandomForestClassifier(class_weight = \"balanced\", random_state = 101)\nRF_cv_scores = cross_validate(RF_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nRF_cv_scores = pd.DataFrame(RF_cv_scores, index = range(1, 11))\nRF_cv_scores.mean()[2:]","10be5359":"param_grid = {'n_estimators' : [50, 100, 300],\n              'max_features' : [2, 3, 4],\n              'max_depth' : [3, 5, 7, 9],\n              'min_samples_split' : [2, 5, 8]}","e2315d05":"RF_grid = RandomForestClassifier(class_weight = 'balanced', random_state = 101)\nRF_grid_model = GridSearchCV(estimator = RF_grid, \n                             param_grid = param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 2)\nRF_grid_model.fit(X_train, y_train)","668a30e7":"RF_grid_model.best_estimator_","4073f856":"print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_params_, 'red'))","1f266109":"RF_tuned = RandomForestClassifier(class_weight = 'balanced',\n                                  max_depth = 3,\n                                  max_features = 4,\n                                  n_estimators = 300,\n                                  min_samples_split = 2,\n                                  random_state = 101).fit(X_train, y_train)","7d8afbd8":"y_pred = RF_tuned.predict(X_test)\ny_train_pred = RF_tuned.predict(X_train)\n\nRF_tuned_f1 = f1_score(y_test, y_pred)\nRF_tuned_acc = accuracy_score(y_test, y_pred)\nRF_tuned_recall = recall_score(y_test, y_pred)\nRF_tuned_auc = roc_auc_score(y_test, y_pred)","0f9659a9":"print(\"RF_tuned\")\nprint (\"------------------\")\neval(RF_tuned, X_train, X_test)","f5e6c124":"cprint('RF_tuned Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","21fc1553":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(RF_tuned)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","ac0b7f6f":"plot_roc_curve(RF_model, X_test, y_test);","5142046e":"plot_precision_recall_curve(RF_model, X_test, y_test);","5c7f8464":"cprint('RF_tuned Predictions','green', 'on_red')\nRF_Pred = {\"Actual\": y_test, \"RF_Pred\":y_pred}\nRF_Pred = pd.DataFrame.from_dict(RF_Pred)\nRF_Pred.head()","a2028509":"cprint('Predictions','green', 'on_red')\nRF_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, RF_Pred, left_index = True, right_index = True)\nModel_Preds.head()","e108b27b":"# random_forest_classifier = pickle.dump(RF_tuned, open('random_forest_model', 'wb'))","e843e06b":"CB_model = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\nCB_model.fit(X_train, y_train)\ny_pred = CB_model.predict(X_test)\ny_train_pred = CB_model.predict(X_train)\n\nCB_model_f1 = f1_score(y_test, y_pred)\nCB_model_acc = accuracy_score(y_test, y_pred)\nCB_model_recall = recall_score(y_test, y_pred)\nCB_model_auc = roc_auc_score(y_test, y_pred)","96c84f41":"print(\"CB_Model\")\nprint (\"------------------\")\neval(CB_model, X_train, X_test)","a528ddf6":"cprint('CB_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","b74f02b5":"CB_cm = confusion_matrix(y_test, y_pred)\nCB_cm_df = pd.DataFrame(CB_cm)\nCB_cm_df = CB_cm_df.rename(columns={0:\"Employee_Stayed\", 1:\"Employe_Left\"}, index={0:\"Employee_Stayed\", 1:\"Employe_Left\"})\nCB_cm_df[\"Total\"] = CB_cm_df[\"Employee_Stayed\"] + CB_cm_df[\"Employe_Left\"]","95b81c4d":"fig = px.bar(CB_cm_df, x=\"Employee_Stayed\", y=\"Total\", color=\"Employe_Left\", title=\"CatBoost Confusion Matrix Distribution\")\n\nfig.update_layout(\n    xaxis_title=\"Employees-Left                    Employees_Stayed\",\n    yaxis_title=\"The Number of Employees\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","5fd61065":"CB_feature_imp = pd.DataFrame(index = X.columns, data = CB_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\nCB_feature_imp","18359c98":"fig = px.bar(CB_feature_imp.sort_values('Importance', ascending = False), x = CB_feature_imp.sort_values('Importance', \n             ascending = False).index, y = 'Importance', title = \"Feature Importance\", \n             labels = dict(x = \"Features\", y =\"Feature_Importance\"))\nfig.show()","67ec84cb":"CB_cv = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\nCB_cv_scores = cross_validate(CB_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nCB_cv_scores = pd.DataFrame(CB_cv_scores, index = range(1, 11))\n\nCB_cv_scores.mean()[2:]","e77afc70":"param_grid = {'learning_rate': [0.01, 0.03, 0.1, 0.5],\n              'depth': [4, 6, 8, 10],\n              'l2_leaf_reg': [1, 3, 5, 7, 9]}","b664be68":"CB_grid = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\nCB_grid_model = GridSearchCV(estimator = CB_grid, \n                             param_grid = param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 2)\nCB_grid_model.fit(X_train, y_train)","a1a8c103":"CB_grid_model.best_params_","ac5bdb03":"print(colored('\\033[1mBest Parameters of GridSearchCV forCatBoost Model:\\033[0m', 'blue'), colored(CB_grid_model.best_params_, 'red'))","b42f50db":"CB_tuned = CatBoostClassifier(verbose = False, \n                              scale_pos_weight = 4,\n                              depth = 4,\n                              l2_leaf_reg = 3,\n                              learning_rate = 0.01,\n                              random_state = 101).fit(X_train, y_train)      ","6e5663eb":"y_pred = CB_tuned.predict(X_test)\ny_train_pred = CB_tuned.predict(X_train)\n\nCB_tuned_f1 = f1_score(y_test, y_pred)\nCB_tuned_acc = accuracy_score(y_test, y_pred)\nCB_tuned_recall = recall_score(y_test, y_pred)\nCB_tuned_auc = roc_auc_score(y_test, y_pred)","8490e437":"print(\"CB_tuned\")\nprint (\"------------------\")\neval(CB_tuned, X_train, X_test)","885ccbf8":"cprint('CB_tuned Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","d97033ca":"CB_cm = confusion_matrix(y_test, y_pred)\nCB_cm_df = pd.DataFrame(CB_cm)\nCB_cm_df = CB_cm_df.rename(columns={0:\"Employee_Stayed\", 1:\"Employe_Left\"}, index={0:\"Employee_Stayed\", 1:\"Employe_Left\"})\nCB_cm_df[\"Total\"] = CB_cm_df[\"Employee_Stayed\"] + CB_cm_df[\"Employe_Left\"]","482a9824":"fig = px.bar(CB_cm_df, x=\"Employee_Stayed\", y=\"Total\", color=\"Employe_Left\", title=\"CatBoost Confusion Matrix Distribution\")\n\nfig.update_layout(\n    xaxis_title=\"Employees-Left                    Employees_Stayed\",\n    yaxis_title=\"The Number of Employees\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","a4dbc994":"plot_roc_curve(CB_model, X_test, y_test);","b887c27b":"plot_precision_recall_curve(CB_model, X_test, y_test);","5595a495":"cprint('CB_tuned Predictions','green', 'on_red')\nCB_Pred = {\"Actual\": y_test, \"CB_Pred\":y_pred}\nCB_Pred = pd.DataFrame.from_dict(CB_Pred)\nCB_Pred.head()","e6d3fb6a":"cprint('Predictions','green', 'on_red')\nCB_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, CB_Pred, left_index = True, right_index = True)\nModel_Preds.head()","9fe72c8f":"cprint('Random Predictions','green', 'on_red')\nModel_Preds.sample(10)","b2ac9b74":"# catboost_classifier = pickle.dump(CB_tuned, open('catboost_model', 'wb'))","02dca761":"compare = pd.DataFrame({\"Model\": [\"GB_model\", \"GB_tuned\", \"KNN_Model\", \"KNN_tuned3\", \"RF_model\", \"RF_tuned\", \"CB_model\", \n                                  \"CB_tuned\"],\n                        \n                        \"F1_Score\": [GB_model_f1, GB_tuned_f1, KNN_model_f1, KNN_tuned3_f1, RF_model_f1, RF_tuned_f1, \n                                     CB_model_f1, CB_tuned_f1],\n                                                 \n                        \"Accuracy_Score\": [GB_model_acc, GB_tuned_acc, KNN_model_acc, KNN_tuned3_acc, RF_model_acc, \n                                           RF_tuned_acc, CB_model_acc, CB_tuned_acc],\n                        \n                        \"Recall_Score\": [GB_model_recall, GB_tuned_recall, KNN_model_recall, KNN_tuned3_recall, RF_model_recall, \n                                         RF_tuned_recall, CB_model_recall, CB_tuned_recall],\n                       \n                        \"ROC_AUC_Score\": [GB_model_auc, GB_tuned_auc, KNN_model_auc, KNN_tuned3_auc, RF_model_auc, \n                                          RF_tuned_auc, CB_model_auc, CB_tuned_auc]})\n\ncompare = compare.sort_values(by=\"Recall_Score\", ascending=True)\nfig = px.bar(compare, x = \"Recall_Score\", y = \"Model\", title = \"Recall_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"F1_Score\", ascending=True)\nfig = px.bar(compare, x = \"F1_Score\", y = \"Model\", title = \"F1_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"Accuracy_Score\", ascending=True)\nfig = px.bar(compare, x = \"Accuracy_Score\", y = \"Model\", title = \"Accuracy_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"ROC_AUC_Score\", ascending=True)\nfig = px.bar(compare, x = \"ROC_AUC_Score\", y = \"Model\", title = \"ROC_AUC_Score\")\nfig.show()","bc3236e4":"cprint('Scores','green', 'on_red')\ncompare.T","14fd5346":"# gradient_boosting_classifier = pickle.dump(GB_tuned, open('gradient_boosting_model', 'wb'))","cd8fb9e1":"# kneighbors_classifier = pickle.dump(KNN_tuned3, open('kneighbors_model', 'wb'))","5ce6080a":"# random_forest_classifier = pickle.dump(RF_tuned, open('random_forest_model', 'wb'))","a4816a61":"# catboost_classifier = pickle.dump(CB_tuned, open('catboost_model', 'wb'))","4e829da1":"# col_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', \n#            'work_accident', 'promotion_last_5years', 'department_RandD', 'department_accounting', 'department_hr', \n#            'department_management', 'department_marketing', 'department_product_mng', 'department_sales', \n#            'department_support', 'department_technical', 'salary_low', 'salary_medium']\n# scaler = MinMaxScaler()\n# scaler_fitted = scaler.fit(df2[col_lst])\n# scaler_deploy = pickle.dump(scaler_fitted, open('scaler.sav', 'wb'))","11372816":"Let's have look to recall values for different K's ranging from 1 to 10","23e7705d":"***Then how is the relation between workload and time spend in the company?***\n\nThird year staff has the most workload. After that year number of participated project is decreasing stepped. It makes sense. The experienced staff becoming team leader or manager position. That's why less of them can be assigned to projects.","adae4a4b":"<a id=\"8.4.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","0cbefbd8":"<a id=\"6\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6 - DATA PRE-PROCESSING<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**- Label Encoding**<br>\n**- Scaling**  ","878d7d74":"Let's first load the required HR dataset using pandas's \"read_csv\" function.","13efc84f":"<a id=\"8\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">8 - MODEL BUILDING<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**- Split Data as Train and Test set**<br>\n**- Built Gradient Boosting Classifier, Evaluate Model Performance and Predict Test Data**<br>\n**- Built K Neighbors Classifier and Evaluate Model Performance and Predict Test Data**<br>\n**- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data**<br>\n---------","2f2da35e":"<a id=\"6.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2 - Scalling<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nSome machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Also distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n\nScaling Types:\n- Normalization: Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\n- Standardization: Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n\n[Click here for more on scaling.](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35)        ","03870011":"<a id=\"8.4.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","2f44680c":"**'satisfaction_level' Column**","d5b2f7ec":"**'time_spend_company' Column**","f96f798e":"**'work_accident' Column**","cb979bff":"<a id=\"8.2.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2.4 Gradient Boosting Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","ad708790":"***According to the basic examinations on the dataset;***\n\n- We have a classification problem.\n- We are going to make classification on the target variable \"left\".\n- And we will build a model to get the best classification on the \"left\" column.\n- Because of that we are going to look at the balance of \"left\" column.\n- The dataset has 10 columns and 11991 observations after dropping of duplicated observations.\n- 8 columns contain numerical values and 2 columns contain categorical values. \n- There seems to be no missing value. ","5957e19e":"**'last_evaluation' Column**","fda678dc":"In this project we have HR data of a company. A study is requested from us to predict which employee will churn by using this data.\n\nFirst of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, we used exploratory data analysis and data visualization techniques.\n\nThen, we performed data pre-processing operations such as ***Scaling*** and ***Label Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms.** \n\nWe used the ***K-means*** algorithm to make cluster analysis. In order to find the optimal number of clusters, we used the ***Elbow method***. Briefly, tried to predict the set to which individuals were related by using K-means and evaluate the estimation results.\n\nThen we built models to predict whether employees will churn or not. We trained our models with train set, tested the success of models with test set. \n\nIn this study, we made modelling with ***Gradient Boosting Classifier***, ***K Neighbors Classifier***, ***Random Forest Classifier*** and ***CatBoost Classifier***.\n\nWe used scikit-learn ***Confusion Metrics*** module for accuracy calculation and the ***Yellowbrick*** module for model selection and visualization.\n\nI deployed this study to the remote machine on AWS and jumped to new .py file and created my web app with Streamlit to make real life predictions.\n\nWe have come to the end of the study. I hope it was useful. I would be very happy if you send your constructive and educational comments about the kernel. \n\nPlease don't forget to upvote if you liked!","03d16675":"Welcome to \"***Employee Churn Analysis***\" study. In this study, we will be able to build our own classification models for a variety of business settings. \n\nAlso we will learn what is Employee Churn?, How it is different from customer churn, Exploratory data analysis and visualization of employee churn dataset using ***matplotlib*** and ***seaborn***, model building and evaluation using python ***scikit-learn*** package. \n\nWe will be able to implement classification techniques in Python. Using Scikit-Learn allowing us to successfully make predictions with the Random Forest, Gradient Descent Boosting , KNN and CatBoost algorithms.\n\nAt the end of the project, we will have the opportunity to deploy your model using *Streamlit*.","a108d4b4":"# WELCOME!","1fc0b0a7":"**'number_project' Column**","c671d781":"<a id=\"10.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.2 - Save and Export Variables as .pkl<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","f8759c90":"<a id=\"8.5.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.6 CatBoost Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","2b2ee276":"Let's see How it is when K=2 (According to our target variable classes)","220bcd7f":"<a id=\"4\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">4 - DATA CLEANING & EXPLORATORY DATA ANALYSIS (EDA)<p>\n\n**Exploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization.**","a7ea1a4e":"<a id=\"8.3.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.3 KNeighbors Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","9532b758":"**'department' Column**","01cb2aa0":"As can be seen on the graph, the employees are not able to make a clear assessment of the company during the first three years of their employment. This, coupled with the other factors, tends to lead to leaving the company after three years.\n\nBy the fourth year, their workload increases and their satisfaction decreases.\n\nAfter the fifth year, they make an assessment, \"they will leave or not\".\n\nIf they decide to continue in the company, they never consider leaving after the sixth year.","0207d5d0":"<a id=\"8.3.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","436898a9":"- 'work_accident' column has binary type values.\n- Left ratios are similar between those who have had a work accident and those who have not. \n- It does not appear to be a determining factor. In fact, it can be said that the left rate of those who have had a work accident is proportionally lower. ","43d32a04":"<a id=\"8.4.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.6 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","3f69c64b":"<a id=\"4.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.1 - A General Look at the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","8977b9dd":"<a id=\"1.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.3 What The Problem Is<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\nFirst of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, we must use exploratory data analysis and data visualization techniques. \n\nThen, we must perform data pre-processing operations such as ***Scaling*** and ***Label Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms. we are asked to perform ***Cluster Analysis*** based on the information you obtain during exploratory data analysis and data visualization processes. \n\nThe purpose of clustering analysis is to cluster data with similar characteristics. We are asked to use the ***K-means*** algorithm to make cluster analysis. However, you must provide the K-means algorithm with information about the number of clusters it will make predictions. Also, the data we apply to the K-means algorithm must be scaled. In order to find the optimal number of clusters, we are asked to use the ***Elbow method***. Briefly, try to predict the set to which individuals are related by using K-means and evaluate the estimation results.\n\nOnce the data is ready to be applied to the model, we must ***split the data into train and test***. Then build a model to predict whether employees will churn or not. Train our models with our train set, test the success of our model with our test set. \n\nTry to make our predictions by using the algorithms ***Gradient Boosting Classifier***, ***K Neighbors Classifier***, ***Random Forest Classifier***, and ***CatBoost Classifier. We can use the related modules of the ***scikit-learn*** library. We can use scikit-learn ***Confusion Metrics*** module for accuracy calculation. We can use the ***Yellowbrick*** module for model selection and visualization.\n\nIn the final step, we will deploy your model using Streamlit tool.","2cd25fd2":"<a id=\"5.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.1 - Employees Left<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Let's check how many employees were left?**<br>\n**Here, we can plot a bar graph using Matplotlib. The bar graph is suitable for showing discrete variable counts.**","2cef7900":"<a id=\"8.2.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","853787ad":"<a id=\"3.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">3.1 Loading & Reading the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**How to read and assign the dataset as df: You can [Visit Here](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)**","acad23c5":"<a id=\"5.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.4 - Subplots of Features<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**We can use the methods of the plotly.**","302a9733":"<a id=\"8.5.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","02f5386e":"<a id=\"8.5.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.5 CatBoost Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","f741f5ba":"<a id=\"8.3.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.5 KNeighbors Classifier GridsearchCV for Choosing Reasonable K Values<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","443a247b":"- Although it comes to mind that there should be a linear relationship between 'satisfaction_level' and 'left', it does not look like this on the graph. \n- Those with a 'satisfaction_level' value of around 0.1 are very likely to 'left'. \n- There is a significant increase in the number of those whose 'satisfaction_level' value is between 3.5 and 4.5 and 'left'. In fact, the number of left ones exceeds the notleft ones. \n- When the 'satisfaction_level' value is between 7 and 9, there is an increase in the number of those left. ","5bba1001":"<a id=\"7\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7 - CLUSTER ANALYSIS<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n- Find the optimal number of clusters (k) using the elbow method for for K-means.\n- Determine the clusters by using K-Means then Evaluate predicted results.\n---------\n\n- Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n\n    [Cluster Analysis](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis)\n\n    [Cluster Analysis2](https:\/\/realpython.com\/k-means-clustering-python\/)","4349b2af":"- It is seen that the left percentages of the salary are similar. \n- Even if it is small, there is an increase in the form of high-medium-low according to the salary status. ","8b3da06a":"<a id=\"8.2.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","29c25643":"At the graph above it is seen that the group working on two projects is working nearly 130-160 hours monthly. It can be assessed that they have only two simple projects that they don't need to work hard, so their loyalty is weak.\n\nMost of the employees are working 135-275 hours monthly. In this group usually the employees who get two or more than five projects leaving the company.\n\nWhen there is an increase on the number of projects and the average monthly working hours, there is also an increase on the number of resignings.","cc20e7a5":"# <h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">EMPLOYEE CHURN ANALYSIS<\/h1>","0fc7a32b":"<a id=\"2\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">2 - LIBRARIES NEEDED IN THE STUDY<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","b26ff8e9":"<a id=\"2.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">2.1 User Defined Functions<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**We have defined some useful user defined functions.**","2597e14e":"#### Evaluating Model Performance","44973f85":"<a id=\"8.5.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.3 Feature Importance for CatBoost Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","bc743c9f":"<a id=\"8.4.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","033e13e1":"**Here, Dataset is broken into two parts in ratio of 70:30. It means 70% data will used for model training and 30% for model testing.**","0d10710c":"- Confusion Matrix : You can use scikit-learn metrics module for accuracy calculation. A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n\n    [Confusion Matrix](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/)","408883cc":"- Normally we expect low satisfaction level for the employees who has left, so the part near to 0 on the x-axis is make sense.\n- Besides a group of employee who are not very decisive about their satisfaction level have also been left the company. This group may need extra motivation for employee loyalty. Because they are not so clear in their assesments about their future in the company.\n- Also a group of employee whose satisfaction level is above the avarage have been left the company. This does not make sense so this must be investigated deeply.\n\nThere may be some other issues:\n\na. The method of gathering this information may be wrong. So the assessment of satisfaction level and the resignings may not be directly proportional.\n\nb. The assessment may not be up to date. By the time the satisfaction level may be decreased so at the real time the satisfaction level of all resigning employees may be close to 0.\n\nc. Some of the employees may have hidden their true feelings.","24451683":"<a id=\"8.4.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.4 Random Forest Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","29310738":"<a id=\"8.5.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","99ba4868":"***Based on the examinations made above,***\n\n- There is no multicollinearity problem among the features.\n- We have weak level correlation between the numerical features and the target column.\n- Also there is weak level correlation between the columns.\n- Target variable demonstrates a slight negative correlation with the variables of \"promotion_last_5years\", \"work_accident\", \"satisfaction_level\", \n- Target variable demonstrates slight positive correlation with the variables of 'time_spend_company', 'average_montly_hours', 'number_project' and 'last_evaluation\".\n- satisfaction_level has more influence on the decision to leave the work than the other columns.","b409e1a6":"**'average_montly_hours' Column**","6571ecec":"<a id=\"8.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2 - Gradient Boosting Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","40b6db9e":"<a id=\"8.5\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5 - CatBoost Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","ca7d921c":"<a id=\"8.2.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2.3 Feature Importance for Gradient Boosting Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","7372594c":"**'salary' Column**","9612fb4f":"- Yellowbrick: Yellowbrick is a suite of visualization and diagnostic tools that will enable quicker model selection. It\u2019s a Python package that combines scikit-learn and matplotlib. Some of the more popular visualization tools include model selection, feature visualization, classification and regression visualization\n\n    [Yellowbrick](https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/yellowbrick-a-set-of-visualization-tools-to-accelerate-your-model-selection-process\/)","4e599ca0":"<a id=\"9\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">9 - THE COMPARISON OF MODELS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","a95bb74d":"![](https:\/\/blog.vantagecircle.com\/content\/images\/2020\/10\/Demographic-attrition--1---1-.png)\n\n**Image credit:** [VantageCircle](https:\/\/blog.vantagecircle.com\/employee-attrition\/)","90f690c7":"<a id=\"8.5.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","29e5f300":"<a id=\"10\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10 - MODEL DEPLOYMENT<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n- Save and Export the Model as .pkl\n- Save and Export Variables as .pkl \n---------\n\nYou cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployement helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied. Streamlit is a popular open source framework used by data scientists for model distribution.\n\nDeployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.\n\nData science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values \u200b\u200bcan be extract from its predictions.\n\nAfter doing the following operations in this notebook, jump to new .py file and create your web app with Streamlit.","4ea484c9":"If we look at the satisfaction level, evaluation score and the number of projects together;\n\nThe group of undecideds who were evaluated as 0.5 are the group who worked on only two projects. As a result, our hypothesis about this group is becoming more clear. As the employer does not assign enough projects to this group, he\/she cannot evaluate their performance and they feel worthless. Therefore, they are unsure about their future in the company. This may lead them to leave.","e1440726":"<a id=\"8.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.1 - Spliting Data as Train & Test<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","8db1c5e0":"#### The Elbow Method\n\n- \"Elbow Method\" can be used to find the optimum number of clusters in cluster analysis. The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k. If k increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.\n\n    [The Elbow Method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)\n\n    [The Elbow Method2](https:\/\/medium.com\/@mudgalvivek2911\/machine-learning-clustering-elbow-method-4e8c2b404a5d)\n\n    [KMeans](https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)\n\nLet's find out the groups of employees who left. You can observe that the most important factor for any employee to stay or leave is satisfaction and performance in the company. So let's bunch them in the group of people using cluster analysis.","09d00018":"<a id=\"8.5.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.4 CatBoost Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","611973aa":"<a id=\"5\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5 - DATA VISUALIZATION<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**- Employees Left**<br>\n**- Determine Number of Projects**<br>\n**- Determine Time Spent in Company**<br>\n**- Subplots of Features**<br>\n----------\nWe can search for answers to the following questions using data visualization methods. Based on these responses, we can develop comments about the factors that cause churn.\n\n**- How does the promotion status affect employee churn?**<br>\n**- How does years of experience affect employee churn?**<br>\n**- How does workload affect employee churn?**<br>\n**- How does the salary level affect employee churn?**<br>    ","92e47996":"<a id=\"1.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.4 Project Structure & Tasks<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n#### 1. Exploratory Data Analysis\n- Importing Modules\n- Loading Dataset\n- Data Insigts\n\n#### 2. Data Visualization\n- Employees Left\n- Determine Number of Projects\n- Determine Time Spent in Company\n- Subplots of Features\n\n#### 3. Data Pre-Processing\n- Scaling\n- Label Encoding\n\n#### 4. Cluster Analysis\n- Find the optimal number of clusters (k) using the elbow method for for K-means.\n- Determine the clusters by using K-Means then Evaluate predicted results.\n\n#### 5. Model Building\n- Split Data as Train and Test set\n- Built Gradient Boosting Classifier, Evaluate Model Performance and Predict Test Data\n- Built K Neighbors Classifier and Evaluate Model Performance and Predict Test Data\n- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data\n\n#### 6. Model Deployement\n- Save and Export the Model as .pkl\n- Save and Export Variables as .pkl ","3b1d6456":"As seen above we are getting the best results with default K (K=5)","109aab10":"The increasing of the average monthly hours according to number of projects is seen on the graph.\n\nThe rate of increase is higher between two and three projects, and after five projects. So it is clearly define the number of resignings due to the working hours.\n\n**There need to be an adjustment about the project numbers, working hours and workload. The projects must be assigned to more employees. Also, better incentives must be offered to staff who are working hard.**","f71553e5":"- 'left' column has binary type values.\n- We have an imbalanced data.\n- Almost 17% of the employees didn't continue with the company and left.\n- 1991 employees left.\n- Almost 83% of the employees continue with the company and didn't leave.\n- 10000 employees didn't leave.","1c3bf148":"- It is not observed that the departments worked alone have an effect on the left decision. \n- It is seen that the left percentages of the departments are similar. ","e12a1b1d":"<a id=\"8.4.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.3 Feature Importance for Random Forest Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","e33e6d95":"**'left' Column-Target Column**","4e911638":"<a id=\"8.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4 - Random Forest Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","31b978cd":"<a id=\"8.3.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.4 Elbow Method for Choosing Reasonable K Values<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","c7eff6aa":"<a id=\"6.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1 - Label Encoding<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column's value can be represented as low:0, medium:1, and high:2. This process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder.**","1b203944":"It becomes meaningful when the satisfaction level of employees and the evaluation of the employer shown together.\n\nAs seen in the graph; the resigning employees are grouping in three diferent clusters.\n\n1. First group has a satisfaction level of 0.4 and last evaluation of 0.5.  This group has not a clear idea about the company and the employer does not have a clear assessment about them. Other features affecting this group must to be investigated. What are the main questions of this group? Why they are confusing? What are the pros and cons of the company for these group? and so on...\n\n2. The second group has a low satisfaction even if the employer evaluated them with high degrees. Then what can be the main problem of this group?\nIntensive work with a low salary may affect this group.\nOr intensive work without promotion may cause to leave.\nOn the next steps workload and motivation factors of this group have to be investigated.\n\n3. The third group has a high satisfaction level and evaluation point as well. The density of this group is fewer than the others. The issues that triger the leave of this group need to be investigated.","fbef3c19":"According to the silhouette score, clustering according to the K=2, K=3 and K=4 are seen above.\n\n- For K=3, 0 labelled cluster is below the average silhouette score.\n- For K=4, 0 and 3 labelled clusters are below the average silhouette score.\n- For K=3 (According to Elbow) and for K=4 (According to the silhouette score) clustering is not suitable for our dataset.","ed027e35":"<a id=\"10.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.1 - Save and Export the Model as .pkl<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","4cd0e3bc":"The number of leaving employees is higher among those who have only two projects during the period. This can be summed up as: \"the employees with only two projects feel worthless or emptied\". Because most of the employees work on three or four projects.\n\nWith the 6th project, the number of resignings is getting over the number of ongoings. There are no ongoing staff members who were assigned to 7 projects.\n\nWorking on more projects may cause intensive workload,  regarding to this the satisfaction level may decrease with the insufficient motivators.","f349dba8":"<a id=\"1\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">1 - DATA<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nIn this project we have HR data of a company. A study is requested from us to predict which employee will churn by using this data.\n\nThe HR dataset has 14,999 samples with various information about the employees. In the given dataset, we have two types of employee one who stayed and another who left the company. This given dataset will be used to predict when employees are going to quit by understanding the main drivers of employee churn.\n\n**For a better understanding and more information, please refer to [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/predicting-employee-churn-python) and [Kaggle Website](https:\/\/www.kaggle.com\/c\/employee-churn-prediction\/data)**","f087f229":"As seen above, the columns in the data set do not separate from each other. All columns are intertwined with each other. \n\nAs seen above it is visually obvious that **clustering is not a good approach to our dataset**. \n\nFrom now on we are going to use **classification models to make churn predictions**. ","f51cb916":"<a id=\"1.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.1 Context<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**\"Analyze employee churn. Find out why employees are leaving the company, and learn to predict who will leave the company..\" [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/predicting-employee-churn-python)**\n\nEmployee turn-over (also known as \"employee churn\") is a costly problem for companies. The true cost of replacing an employee can often be quite large. A study by the Center for American Progress found that companies typically pay about one-fifth of an employee\u2019s salary to replace that employee, and the cost can significantly increase if executives or highest-paid employees are to be replaced. In other words, the cost of replacing employees for most employers remains significant. This is due to the amount of time spent to interview and find a replacement, sign-on bonuses, and the loss of productivity for several months while the new employee gets accustomed to the new role.\n\nIn the past, most of the focus on the \"rates\" such as attrition rate and retention rates. HR Managers compute the previous rates try to predict the future rates using data warehousing tools. These rates present the aggregate impact of churn, but this is the half picture. Another approach can be the focus on individual records in addition to aggregate.\n\nThere are lots of case studies on customer churn are available. In customer churn, you can predict who and when a customer will stop buying. Employee churn is similar to customer churn. It mainly focuses on the employee rather than the customer. Here, you can predict who, and when an employee will terminate the service. Employee churn is expensive, and incremental improvements will give significant results. It will help us in designing better retention plans and improving employee satisfaction.","64030dbd":"- 'promotion_last_5years' column has binary type values.\n- Receiving a promotion in the last 5 working years is not determinative in terms of leaving or continuing to work. ","53406bbb":"K=1 and K=3 have the same error rate. In order to reduce the complexity of the model, we can be continue with K=1 as the tuned_model. However, I will continue with tuned_model K=3 since recall values are better for K=3.","b29eab0f":"***Let's go on with the examination of numerical and categorical columns.***","27e789af":"<a id=\"5.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2 - Number of Projects<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Similarly, we can also plot a bar graph to count the number of employees deployed on how many projects?**","f9b036f2":"<a id=\"8.3.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.6 KNeighbors Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","7b4207d9":"<a id=\"11\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">11 - CONCLUSION<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","b8f51c5e":"**'promotion_last_5years' Column**","1150e881":"<a id=\"8.4.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.5 Random Forest Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","367141a9":"<a id=\"8.2.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","35288631":"<a id=\"8.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3 - KNeighbors Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","96ad40db":"- Looking at the 'time_spent_company' values, there is an increase in turnover in the 3rd working year, but this increase gradually decreases until the 6th working year. ","c0606808":"<a id=\"8.2.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2.5 Gradient Boosting Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","51364823":"- Most of the employees have been assessed above 0.4.\n- There is a local increase between 0.45-0.6 and 0.8-1 in 'last_evaluation' values, as in 'satisfaction_level' values. There is an increase in the number of people who quit their jobs in these intervals\n- Intensive work may cause the resign of high evaluated employees (second group). Because employer will be happy with performance of these staff, however it will be a burden for employee.","32b75dbc":"<a id=\"8.2.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2.6 Gradient Boosting Classifier ROC (Receiver Operating Curve) & AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","f19512cd":"- Looking at the 'average_montly_hours' values, there is a local increase in turnover in the 125-160 month working hours range and 210-290 monthly working hours. \n- Those who work more than 290 hours per month are more likely to quit their jobs than those who do not. \n- So the next question is \"The average monthly working hours are related to projects number or not?\"","cd3df1b7":"<a id=\"1.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.2 About The Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**We can describe 10 attributes (features) in detail as:**\n\n- ***satisfaction_level   :*** It is employee satisfaction point, which ranges from 0-1.\n- ***last_evaluation      :*** It is evaluated performance by the employer, which also ranges from 0-1.\n- ***number_projects      :*** How many of projects assigned to an employee?\n- ***average_monthly_hours:*** How many hours in averega an employee worked in a month?\n- ***time_spent_company   :*** time_spent_company means employee experience. The number of years spent by an employee in the company.\n- ***work_accident        :*** Whether an employee has had a work accident or not.\n- ***promotion_last_5years:*** Whether an employee has had a promotion in the last 5 years or not.\n- ***Departments          :*** Employee's working department\/division.\n- ***Salary               :*** Salary level of the employee such as low, medium and high.\n- ***left                 :*** Whether the employee has left the company or not.","4fa4136d":"<a id=\"toc\"><\/a>\n\n## <h3 style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tablist\" aria-controls=\"home\">TABLE OF CONTENTS<\/h3>\n\n* [1 - DATA](#1)\n    * [1.1 Context](#1.1)\n    * [1.2 About the Features](#1.2) \n    * [1.3 What the Problem is](#1.3) \n    * [1.4 Project Structure & Tasks](#1.4) \n* [2 - LIBRARIES NEEDED IN THE STUDY](#2)\n    * [2.1 User Defined Functions](#2.1)\n* [3 - ANALYSIS](#3)\n    * [3.1) Loading & Reading the Data](#3)\n* [4 - EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 - A General Look at the Data](#4.1)\n    * [4.2 - Examination of Features and Data Insights](#4.2)\n* [5 - DATA VISUALIZATION](#5)          \n    * [5.1 - Employees_Left](#5.1)\n    * [5.2 - Number of Projects](#5.2)\n    * [5.3 - Time Spent in the Company](#5.3)\n    * [5.4 - Subplots of features](#5.4)\n* [6 - DATA PRE-PROCESSING](#6)          \n    * [6.1 - Label Encoding](#6.1)\n    * [6.2 - Scalling](#6.2)\n* [7 - CLUSTER ANALYSIS](#7)\n* [8 - MODEL BUILDING](#8)\n    * [8.1 - Spliting Data as Train & Test](#8.1)\n    * [8.2 - Gradient Boosting Classifier](#8.3)\n        * [8.2.1 Model Building](#8.3.1)\n        * [8.2.2 Evaluating Model Performance](#8.3.2)\n        * [8.2.3 Feature Importance for Gradient Boosting Model](#8.3.3) \n        * [8.2.4 Gradient Boosting Classifier Cross Validation](#8.3.4)\n        * [8.2.5 Gradient Boosting Classifier GridSearchCV](#8.3.5)\n        * [8.2.6 Gradient Boosting Classifier ROC (Receiver Operating Curve) & AUC (Area Under Curve)](#8.3.6)            \n        * [8.2.7 Prediction](#8.3.7)        \n    * [8.3 - KNeighbors Classifier](#8.4) \n        * [8.3.1 Model Building](#8.4.1)\n        * [8.3.2 Evaluating Model Performance](#8.4.2)\n        * [8.3.3 KNeighbors Classifier Cross Validation](#8.4.3) \n        * [8.3.4 Elbow Method for Choosing Reasonable K Values](#8.4.4)\n        * [8.3.5 KNeighbors Classifier GridsearchCV for Choosing Reasonable K Values](#8.4.5)\n        * [8.3.6 KNeighbors Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#8.4.6)            \n        * [8.3.7 Prediction](#8.4.7)         \n    * [8.4 - Random Forest Classifier](#8.5)\n        * [8.4.1 Model Building](#8.5.1)\n        * [8.4.2 Evaluating Model Performance](#8.5.2)\n        * [8.4.3 Feature Importance for Random Forest Model](#8.5.3) \n        * [8.4.4 Random Forest Classifier Cross Validation](#8.5.4)\n        * [8.4.5 Random Forest Classifier GridSearchCV](#8.5.5)\n        * [8.4.6 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#8.5.6)            \n        * [8.4.7 Prediction](#8.5.7)     \n    * [8.5 - CatBoost Classifier](#8.6)\n        * [8.5.1 Model Building](#8.6.1)\n        * [8.5.2 Evaluating Model Performance](#8.6.2)\n        * [8.5.3 Feature Importance for CatBoost Model](#8.6.3) \n        * [8.5.4 CatBoost Classifier Cross Validation](#8.6.4)\n        * [8.5.5 CatBoost Classifier GridSearchCV](#8.6.5)\n        * [8.5.6 CatBoost Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#8.6.6)            \n        * [8.5.7 Prediction](#8.6.7)     \n* [9 - THE COMPARISON OF MODELS](#9)       \n* [10 - MODEL DEPLOYMENT](#10)\n    * [10.1 - Save and Export the Model as .pkl](#10.1)\n    * [10.2 - Save and Export Variables as .pkl](#10.2)\n**Note: There may be some additional sub-tasks associated with each task, you will see them in order during the course of the work.*   \n* [11 - CONCLUSION](#11)","21a7f227":"<a id=\"4.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.2 - Examination of Features and Data Insights<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**In the given dataset, we have two types of employee one who stayed and another who left the company. So, we can divide data into two groups and compare their characteristics. Here, we can find the average of both the groups using groupby() and mean() function.**   ","8eee5462":"<a id=\"3\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">3 - ANALYSIS<p>","da08b663":"The leaving employees who worked on more than four projects are the group two and three of the last_evaluation section.\n\nEspecially most of the second group of last_evaluation section are worked on seven projects and left the company. So again our hypothesis about this group is now more definite.","685b6587":"- I want to move the 'left' column, which is my target column, from where it is to the end. In this way, I will work more comfortably psychologically :)) ","301c839a":"<a id=\"8.3.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","8d36a344":"<a id=\"8.3.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","53cd4698":"<a id=\"5.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3 - Time Spent in the Company<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Similarly, we can also plot a bar graph to count the number of employees have based on how much experience?**\n"}}