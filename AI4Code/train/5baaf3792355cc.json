{"cell_type":{"d2378cad":"code","db5171ad":"code","da02fd84":"code","18e2010b":"code","4b936e38":"code","5523df70":"code","84142bc9":"code","fc7beae5":"code","9c6feda4":"code","f39c1c0e":"code","e66cf2b6":"code","db214644":"code","51deaf69":"code","ba3f2d06":"code","62b0f84b":"code","da703dce":"code","ecce0a0b":"code","a65b29d7":"code","c486cc32":"code","6e6634e1":"code","5d04f51d":"code","fd438e3c":"code","7f2f001c":"code","555afcb1":"code","ee2242f4":"code","c569b747":"code","cac081c6":"code","c4f5191c":"code","68c5c607":"code","f6a2d5b9":"code","0cfd4114":"code","ab938b0c":"code","b35f2b61":"code","38f29777":"code","abbb776f":"code","4d2d84dd":"code","ba814e8b":"code","3621c0aa":"code","d31c6184":"code","409a8d91":"code","0fdd2045":"code","7e9069d5":"code","005df765":"code","a0309704":"code","881600a8":"code","56546675":"code","a8ce5d19":"code","2ee24b56":"code","b246a4c0":"code","423c41c2":"code","e4e85cdb":"code","25972696":"code","696bb7f7":"code","dc92e29f":"code","f7de16ff":"code","0e4d5771":"code","ec7ebcc3":"code","e08126d3":"code","018abd04":"code","5c59dd53":"code","466775fb":"code","8453e1a7":"code","b7b5a0ce":"code","a40d7759":"code","635f61be":"code","ac7178b5":"code","2a6cd83f":"code","2fcfdae3":"code","dc128ebb":"code","45bc52d4":"code","9d84e5bb":"code","97153fbb":"code","2962beb7":"code","55a538ab":"code","b54c5df6":"code","c58cec5e":"markdown","4bc66d64":"markdown","a07f9b55":"markdown","a7efdaf6":"markdown"},"source":{"d2378cad":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import KFold,GroupKFold,TimeSeriesSplit,train_test_split\nfrom time import time","db5171ad":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\n\nfrom keras.layers import LSTM, Dropout","da02fd84":"try:\n    del z_test\n    del y_test\n    del pred_test\n    print('deleted')\nexcept:\n    print('go')","18e2010b":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(24)","4b936e38":"ROOT = \"..\/input\/osic-pulmonary-fibrosis-progression\"","5523df70":"## evaluation metric function\ndef laplace_log_likelihood(actual_fvc, predicted_fvc, confidence, return_values = False):\n    \"\"\"\n    Calculates the modified Laplace Log Likelihood score for this competition.\n    \"\"\"\n    sd_clipped = np.maximum(confidence, 70)\n    delta = np.minimum(np.abs(actual_fvc - predicted_fvc), 1000)\n    metric = - np.sqrt(2) * delta \/ sd_clipped - np.log(np.sqrt(2) * sd_clipped)\n\n    if return_values:\n        return metric\n    else:\n        return np.mean(metric)\n\n\n## default benchmark\n","84142bc9":"tra = pd.read_csv(f\"{ROOT}\/train.csv\")\ntra.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\nchunk = pd.read_csv(f\"{ROOT}\/test.csv\")\n\nsub = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\") # appiccica a sub le colonne di Chunk a parte week\n","fc7beae5":"tra.columns","9c6feda4":"'''\ntr['Dist'] = 0\ntr = tr.sort_values(by=['Patient', 'Weeks'])\nfor p in range(1,tr.shape[0]):\n    if tr.iloc[p, 0] == tr.iloc[p-1, 0]:\n        tr.iloc[p, -1] = tr.iloc[p, 2] - tr.iloc[p-1, 2]   \nprint(tr.iloc[:,-1].describe())\n'''","f39c1c0e":"sub.columns","e66cf2b6":"'''\nsub['Dist'] = 0\nsub = sub.sort_values(by=['Patient', 'Weeks'])\nfor p in range(1,sub.shape[0]):\n    if sub.iloc[p, 0] == sub.iloc[p-1, 0]:\n        sub.iloc[p, -1] = sub.iloc[p, 4] - sub.iloc[p-1, 4]   \nprint(sub.iloc[:,-1].describe())\n'''\n","db214644":"'''\nchunk['Dist'] = 0\n'''","51deaf69":"tr = pd.DataFrame(tra).copy()","ba3f2d06":"'''\n# devo creare le righe mancanti\n\npat = tr['Patient'][0]\nfor pat in tr['Patient'].unique():\n    k=0\n    base = tr.loc[tr.Patient == pat]\n    baseB = base.values[0]\n    baseDf = pd.DataFrame([baseB], columns = base.columns)\n    for we in sorted(sub['Weeks'].unique()):\n        a = tr.loc[tr['Patient'] == pat] #tutte le week di pat\n        b= a.loc[a['Weeks'] == we,'FVC'] #FVC di week we\n        if b.unique().size == 0:\n            if we == sub['Weeks'].max():\n                appoggio = baseDf.copy()\n                bb= a.loc[a['Weeks'] == (we-k-1),'FVC']\n                bb = bb.values[0]\n                delta = 0\n                #print('we:',we,' b:',b,' bb',bb,'k:',k,' delta:',delta)\n      \n                for j in range(1, k+1):\n                    appoggio.Weeks = (we-k+j)\n                    appoggio.FVC = bb\n                    appoggio.Percent = appoggio.FVC \/ (baseDf.FVC \/ baseDf.Percent)\n                    tr = pd.concat([tr, appoggio], ignore_index=True)\n    \n                del appoggio\n                k=0\n            elif we == sub['Weeks'].min():\n                appoggio = baseDf.copy()\n                appoggio.Weeks = we\n                tr = pd.concat([tr, appoggio], ignore_index=True)\n                del appoggio\n                k=0\n            else:\n                k = k+1\n        elif k > 0:\n            appoggio = baseDf.copy()\n            bb= a.loc[a['Weeks'] == (we-k-1),'FVC'] \n            if bb.unique().size == 0:\n                appoggio.Weeks = (we-k-1)\n                appoggio.FVC = b.values[0]\n                appoggio.Percent = appoggio.FVC \/ (baseDf.FVC \/ baseDf.Percent)\n                del bb\n                bb = a.loc[a['Weeks'] == we,'FVC']\n            \n            bb = bb.values[0]\n            b = b.values[0]\n            delta = b - bb\n            #print('we:',we,' b:',b,' bb',bb,'k:',k,' delta:',delta)\n  \n            for j in range(0, k):\n                bbb = bb + delta\/(k+1)*(j+1) #ho calcolato FVC nuova riga\n                \n                appoggio.Weeks = (we-k+j)\n                appoggio.FVC = bbb\n                appoggio.Percent = appoggio.FVC \/ (baseDf.FVC \/ baseDf.Percent)\n                \n                tr = pd.concat([tr, appoggio], ignore_index=True)\n    \n            del appoggio\n            k=0\n    del baseDf,baseB, base, a, b, bb, bbb,\n'''","62b0f84b":"tr['WHERE'] = 'train'\nchunk['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = tr.append([chunk, sub])","da703dce":"print(tr.shape, chunk.shape, sub.shape, data.shape)\nprint(tr.Patient.nunique(), chunk.Patient.nunique(), sub.Patient.nunique(), \n      data.Patient.nunique())\n#","ecce0a0b":"'''\nvoglio associare ad ogni riga:\n- aggregando per et\u00e0\/smoking status e sesso\n    - i tre percentili del valore massimo\n    - i tre percentili del valore minimo\n    - i tre percentili del rapporto tra max e min\n    f=pd.DataFrame(d.groupby(['a','b']))\n'''\ndata['min_perc'] = data.Percent\ndata.loc[data.WHERE=='test','min_perc'] = np.nan\ndata['max_perc'] = data.Percent\ndata.loc[data.WHERE=='test','max_perc'] = np.nan\n\ndata['min_perc'] = data.groupby(['Age','SmokingStatus','Sex'])['Percent'].transform('min')\ndata['max_perc'] = data.groupby(['Age','SmokingStatus','Sex'])['Percent'].transform('max')\n\ndata['rif_perc'] = data.max_perc - data.min_perc\ndata.loc[data.rif_perc > 0, 'rif_perc']= (data.loc[data.rif_perc > 0, 'Percent']- data.loc[data.rif_perc > 0, 'min_perc'])\/\\\n    (data.loc[data.rif_perc > 0, 'max_perc'] - data.loc[data.rif_perc > 0, 'min_perc'])\n\ndata = data.drop(['max_perc','min_perc'], axis=1)\n","a65b29d7":"data.info()","c486cc32":"# identifica la prima settimana per paziente\n\ndata['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test','min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')","6e6634e1":"# identifica la FVC della min_week ed elimina eventuali duplicati\n\nbase = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient','FVC']]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','min_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)","5d04f51d":"# aggiunge le colonne (identiche per paziente) min_FVC e base_week (indice della settimana dove la min_week = 0)\n\ndata = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base","fd438e3c":"FE = []\n\n# aggiunge le colonne categoriche per sex e smokingStatus utilizzando il bool sul valore assunto\n\nCOLS = ['Sex','SmokingStatus']\nfor col in COLS:\n    for mod in data[col].unique():\n        FE.append(mod)\n        data[mod] = (data[col] == mod).astype(int)\n#=================\n","7f2f001c":"'''\ndata.loc[data['SmokingStatus'] == 'Never smoked','SmokingStatus'] = 0\ndata.loc[data['SmokingStatus'] == 'Ex-smoker','SmokingStatus'] = 0.5\ndata.loc[data['SmokingStatus'] == 'Currently smokes','SmokingStatus'] = 1\n\ndata.loc[data['Sex'] == 'Male','Sex'] = 1\ndata.loc[data['Sex'] == 'Female','Sex'] = 0\n\ndata['Sex'] = data['Sex'].astype(float)\ndata['SmokingStatus'] = data['SmokingStatus'].astype(float)\n\nFE += ['Sex', 'SmokingStatus']\n'''","555afcb1":"# normalizza age\ndata['age'] = (data['Age'] - data['Age'].min() ) \/ ( data['Age'].max() - data['Age'].min() )\n\n# crea per ogni patient il normalizzato del valore di FVC alla settimana base, cio\u00e8 del punto di partenza\ndata['FVC_BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) \/ ( data['min_FVC'].max() - data['min_FVC'].min() )\n\n# crea per ogni week il normalizzato di base_week, che \u00e8 la correzione per portare a zero la min_week\ndata['week'] = (data['base_week'] - data['base_week'].min() ) \/ ( data['base_week'].max() - data['base_week'].min() )\n\n# normalizza la percent\ndata['percent'] = (data['Percent'] - data['Percent'].min() ) \/ ( data['Percent'].max() - data['Percent'].min() )\n\n'''\n# normalizza la Dist\ndata['dist'] = (data['Dist'] - data['Dist'].min() ) \/ ( data['Dist'].max() - data['Dist'].min() )\n'''\n# all'elenco delle colonne aggiunge quelle normalizzate\n'''\nFE += ['age','percent','week','FVC_BASE', 'dist','rif_perc']\n'''\nFE += ['age','percent','week','FVC_BASE']\n","ee2242f4":"data.info()","c569b747":"tr = data.loc[data.WHERE=='train']\nchunk = data.loc[data.WHERE=='val']\nsub = data.loc[data.WHERE=='test']\ndel data","cac081c6":"tr.info()","c4f5191c":"tr.shape, chunk.shape, sub.shape","68c5c607":"\nk=0\ntr['KGroup'] = 0\n\nfor pat in tr.Patient.unique():\n    k=k+1\n    tr.loc[tr.Patient == pat, 'KGroup'] = k\n        \nk=0\nchunk['KGroup'] = 0\nfor pat in chunk.Patient.unique():\n    k=k+1\n    chunk.loc[chunk.Patient == pat, 'KGroup'] = k\n        \nk=0\nsub['KGroup'] = 0\nfor pat in sub.Patient.unique():\n    k=k+1\n    sub.loc[sub.Patient == pat, 'KGroup'] = k\n\ndel k, pat","f6a2d5b9":"from tensorflow.keras import datasets, layers, models, regularizers, optimizers, callbacks\nfrom tensorflow.keras.layers import Flatten, BatchNormalization\nfrom keras.regularizers import l1_l2,l2,l1\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras import activations\nfrom keras.layers import LeakyReLU\nfrom tensorflow.keras.callbacks import History, EarlyStopping","0cfd4114":"#, kernel_regularizer=l1_l2(l1=0.0001,l2=0.001)\n#, kernel_regularizer=l2(0.01)\n\nALPHA= 0.01\nL2 = 0.001\nMOMENTUM = 0.9 #ex 0,99\nDPO = 0.3\n\nuse_metric = 1.0 #0.8\nBATCH_SIZE = 16\nHDO = 144\nEPOCHS = 2000\nOPTIMIZER= \"adamax\"\n\nqmin = 0.2 #0.25\nqmed = 0.5\nqmax = 0.8 #0.75\n\nCORRECTION = tf.constant(1, dtype='float32') # per correggere la Confidence\nGROUPS = True\nNFOLD = 5\nTESTSIZE = 0.0 #0.2\nVAL_SPLIT = 0.1 # utilizzato solo se GROUPS = False\nERR = 0.0","ab938b0c":"#tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n\ntf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')","b35f2b61":"#tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Adamax')\n\ntf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Adamax')","38f29777":"def myLoss(y_true,y_pred):\n    sigma = (y_pred[:, 2] - y_pred[:, 0])\n    delta = tf.abs(y_true[:, 0] - y_pred[:, 1])\n    errore = tf.abs(sigma) - tf.abs(delta)\n    return errore","abbb776f":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\nBEST = 0\n#=============================#\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = (y_pred[:, 2] - y_pred[:, 0])\n    #sigmaSTD = tf.math.reduce_std(sigma)\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    #sigma_clip = tf.maximum(sigmaSTD, C1)\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = -((delta \/ sigma_clip) * sq2 + tf.math.log(sigma_clip * sq2))\/BEST\n    return K.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [qmin,qmed,qmax]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    '''\n    a[[a1],...[an]],b[[b11,b12,b13],...[bn1,bn2.bn3]]\n    c = a - b = [[a1-b11,a1-b12,a1-b13],...,[an-bn1,an-bn2,an-bn3] cio\u00e8 a(1,) x b(3,) = c(3,)\n    c = q(3)*c(3,) = (3,) = [[q1*c11,q2*c12,q3*c13],...[q1*cn1,q2*cn2,q3*cn3]]\n    \n    in sostanza crea tre versioni di delta vs y_true le moltiplica per q e (1-q) e prende il max\n    '''\n\n    v = tf.maximum(q*e, (q-1)*e) \n    return K.mean(v)\n#=============================#\ndef mloss(_lambda, err):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred) + err * myLoss(y_true, y_pred)\n    return loss\n#=================\n\ndef make_model():\n    z = L.Input((len(FE),), name=\"Patient\")\n    \n    x = L.LeakyReLU(alpha=ALPHA)(z)\n    x = L.Dense(HDO, kernel_regularizer=l2(L2))(x)\n    \n    x = L.LeakyReLU(alpha=ALPHA)(x)\n    x = L.Dense(HDO, kernel_regularizer=l2(L2))(x)\n    \n    p1 = L.Dense(3, activation=\"swish\", name=\"p1\")(x)\n    \n    model = M.Model(z, p1, name=\"CNN\")\n    model.compile(loss=mloss(use_metric, ERR), optimizer= OPTIMIZER, metrics=[score]) # , metrics=[score]\n    return model\n","4d2d84dd":"net = make_model()\nprint(net.summary())\nprint(net.count_params())","ba814e8b":"print(len(tr['KGroup'].unique()))","3621c0aa":"a=tr['KGroup'].unique()\nb=int(len(tr['KGroup'].unique())*(1-TESTSIZE))","d31c6184":"train_group = np.random.choice(a, b, replace=False)\ntrain_group = list(train_group)\ntest_group = []\nfor k in range(1,len(tr['KGroup'].unique())+1):\n    if k not in train_group:\n        test_group.append(int(k))\n\nprint(len(train_group),len(test_group))\ndel a,b,k","409a8d91":"X_train = pd.DataFrame()\nY_train = pd.DataFrame()\nfor k in train_group:\n    X_train = X_train.append(tr.loc[tr.KGroup == k])\nY_train = X_train['FVC']\ndel k","0fdd2045":"X_test = pd.DataFrame()\nY_test = pd.DataFrame()\nif TESTSIZE > 0:\n    for k in test_group:\n        X_test = X_test.append(tr.loc[tr.KGroup == k])\n    Y_test = X_test['FVC']\n    ","7e9069d5":"print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)","005df765":"FE","a0309704":"z = X_train[FE].values\nz = np.reshape(z,(z.shape[0],z.shape[1],1))\n\ny = Y_train.values\n\nze = sub[FE].values\nze = np.reshape(ze,(ze.shape[0],z.shape[1],1))\n\npe = np.zeros((ze.shape[0], 3))\npred = np.zeros((z.shape[0], 3))\npred_act = np.zeros((z.shape[0], 3))\n\nif TESTSIZE > 0:\n    z_test = X_test[FE].values\n    z_test = np.reshape(z_test,(z_test.shape[0],z_test.shape[1],1))\n    \n    y_test = Y_test.values\n    \n    pred_test = np.zeros((z_test.shape[0], 3))\n    ","881600a8":"print(z.shape,'\\n',FE)","56546675":"def plot_model(model_history, epochs, metric):\n    fraz = 1.2\n    starting_point = int(epochs\/fraz)\n    if GROUPS:\n        epochs = epochs * NFOLD\n    plt.figure(figsize=(14,10))\n    plt.title(\"Metric\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Metric\")\n    plt.plot(model_history[metric][epochs - starting_point:],color='green')\n    try: plt.plot(model_history['val_' + metric][epochs - starting_point:],color='red')\n    except: no=1\n\n    plt.figure(figsize=(14,10))\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.plot(model_history['loss'][epochs - starting_point:],color='green')\n    try: plt.plot(model_history['val_loss'][epochs - starting_point:],color='red')\n    except: no=1","a8ce5d19":"def analyze_model(trax, tray, testx, testy):\n    a = net.evaluate(trax,tray, verbose=0, batch_size=BATCH_SIZE)\n    b = net.evaluate(testx,testy, verbose=0, batch_size=BATCH_SIZE)\n\n    c = tf.constant(BEST, dtype='float32')\n    \n    print(\"train:\", a*c,'\\n')\n    print(\"test:\", b*c,'\\n')","2ee24b56":"def analyze_GROUP_model(a,b,c):\n    #best = tf.constant(BEST, dtype='float32')\n    #fold =tf.constant(NFOLD, dtype='float32')\n    \n    print(\"train:\", a[0] \/ NFOLD ,a[1] * BEST \/ NFOLD,'\\n')\n    print(\"val:\", b[0] \/ NFOLD ,b[1] * BEST \/ NFOLD,'\\n')\n    print(\"test:\", c[0] \/ NFOLD ,c[1] * BEST \/ NFOLD,'\\n')","b246a4c0":"def laplace_Group_model(scope,base_x,base_y, predicted, bias):\n    actualFVC = base_y\n    predictedFVC = predicted\n    sigma_opt_actual = mean_absolute_error(actualFVC, predictedFVC[:, 1])\n    sigmaSTD = mean_squared_error(actualFVC, predictedFVC[:, 1])\n    unc_actual = (predictedFVC[:,2] - predictedFVC[:, 0]) * bias\n    sigma_mean_actual = np.mean(unc_actual)\n    best = laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = actualFVC,\\\n                                            confidence= 0)\n    print('laplace_log_likelihood su '+ scope +':\\n')\n    print(\"opt_actual\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                           confidence= sigma_opt_actual))\n    print(\"mean_squared\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                             confidence= sigmaSTD))\n    print(\"unc_actual\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                           confidence= unc_actual))\n    print(\"mean_actual\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                            confidence= sigma_mean_actual))\n    return(best)","423c41c2":"def laplace_model(scope,base_x,base_y,base_pred):\n    actualFVC = base_y\n    #predictedFVC = net.predict(base_x, batch_size=BATCH_SIZE, verbose=0) #calcolato sull'ultimo fold\n    predictedFVC = base_pred\n    sigma_opt_actual = mean_absolute_error(actualFVC, predictedFVC[:, 1])\n    sigmaSTD = mean_squared_error(actualFVC, predictedFVC[:, 1])\n    unc_actual = predictedFVC[:,2] - predictedFVC[:, 0]\n    sigma_mean_actual = np.mean(unc_actual)\n    best = laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = actualFVC,\\\n                                            confidence= 0)\n    print('laplace_log_likelihood su '+ scope +':\\n')\n    print(\"opt_actual\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                           confidence= sigma_opt_actual))\n    print(\"mean_squared\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                             confidence= sigmaSTD))\n    print(\"unc_actual\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                           confidence= unc_actual))\n    print(\"mean_actual\", laplace_log_likelihood(actual_fvc = actualFVC, predicted_fvc = predictedFVC[:,1],\\\n                                            confidence= sigma_mean_actual))\n    return(best)","e4e85cdb":"BEST = laplace_model('BEST',z,y,pred)\nstart_at = time()","25972696":"%%time\nif GROUPS:\n    history=History()\n    groups = X_train.KGroup \n\n    gkf = GroupKFold(n_splits= NFOLD)\n\n    cnt=0\n    net_train = np.zeros(2,)\n    net_val = np.zeros(2,)\n    net_test = np.zeros(2,)\n\n    start_at = time()\n    for tr_idx, val_idx in gkf.split(z, y, groups=groups): \n        cnt += 1 \n        print(f\"GROUP {cnt}\") \n        net = make_model()\n\n        start_Fold = time()\n        # addestra il modello\n        net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(z[val_idx], y[val_idx]), verbose=0, callbacks=[history])\n\n        # valutazione su train e su val\n        exec_time = time() - start_Fold\n        a = net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE)\n        b = net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE)\n        print(\"train su fold:\", a[0],a[1]*BEST)\n        print(\"val su fold:\", b[0],b[1]*BEST)\n        net_train += a\n        net_val += b\n        \n        if TESTSIZE > 0:\n            c = net.evaluate(z_test, y_test, verbose=0, batch_size=BATCH_SIZE)\n            print(\"test su fold:\", c[0],c[1]*BEST,)\n            net_test += c\n            # previsione su test\n            pred_test += net.predict(z_test, batch_size=BATCH_SIZE, verbose=0) \/ NFOLD #calcolato sul fold corrente\n        \n        # previsione su val\n        pred[val_idx] += net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0) #calcolato sul fold corrente\n\n        # previsione su sub\n        pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) \/ NFOLD #calcolato sul fold corrente\n        \n        print(\"Tempo di addestramento: %d minuti e %d secondi\" % (exec_time\/60, exec_time%60),'\\n')\n    \n    exec_time = time() - start_at\n    print(\"\\nTempo totale di addestramento: %d minuti e %d secondi\" % (exec_time\/60, exec_time%60),'\\n')\n    del a,b","696bb7f7":"if GROUPS == False:\n    print(\"Inizio addestramento: ...\\n\")\n    history = History()\n    net = make_model()\n    net.fit(z, y, batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_split=VAL_SPLIT, verbose=0, callbacks=[history])\n    exec_time = time() - start_at\n    print(\"Tempo totale di addestramento: %d minuti e %d secondi\" % (exec_time\/60, exec_time%60),'\\n')\n","dc92e29f":"if GROUPS:\n    analyze_GROUP_model(net_train, net_val, net_test)\nelse:\n    #def analyze_model(trax, tray, testx, testy):\n    if TESTSIZE > 0:\n        analyze_model(z,y,z_test, y_test)\n    else: \n        analyze_model(z,y,z, y)","f7de16ff":"# def plot_model(model_history, starting_point, metric):\nplot_model(history.history, EPOCHS, 'score' )","0e4d5771":"plt.figure(figsize=(14,10))\nplt.title(\"Metric\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Metric\")\nplt.plot(abs(y-pred[:,1]),color='red')\nplt.plot(abs(pred[:,0]-pred[:,2]),color='blue')","ec7ebcc3":"if TESTSIZE > 0:\n    plt.figure(figsize=(14,10))\n    plt.title(\"Metric\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Metric\")\n    plt.plot(abs(y_test-pred_test[:,1]),color='red')\n    plt.plot(abs(pred_test[:,0]-pred_test[:,2]),color='blue')","e08126d3":"z.shape, y.shape, pred.shape\ntry:\n    print(z.shape, y.shape, pred.shape, pred_test.shape)\nexcept:\n    print(z.shape, y.shape, pred.shape)","018abd04":"#pred_test","5c59dd53":"\nif GROUPS:\n    for BIAS in (1,0,0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2):\n        print('\\nBIAS:',BIAS,'\\n')\n        laplace_Group_model('training',z,y,pred, BIAS)\n        print('\\nx--------x\\n')\n        if TESTSIZE > 0: laplace_Group_model('test',z_test,y_test,pred_test, BIAS)\nelse:\n    laplace_model('training',z,y,pred)\n    print('\\nx--------x\\n')\n    if TESTSIZE > 0: laplace_model('test',z_test,y_test,pred_test)","466775fb":"#predxs = net.predict(z, batch_size=BATCH_SIZE, verbose=0) #calcolato sull'ultimo fold\npredxs = pred\nidxs = np.random.randint(0, y.shape[0], 100)\nidxs = np.sort(idxs)","8453e1a7":"LOW_CORRECTION = tf.constant(1.0, dtype='float32') # per correggere la Confidenc\nHIGH_CORRECTION = tf.constant(1.0, dtype='float32') \n\nplt.figure(figsize=(14,6))\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(predxs[idxs, 0] * LOW_CORRECTION, label=\"q20\")\nplt.plot(predxs[idxs, 1], label=\"q50\")\nplt.plot(predxs[idxs, 2] * HIGH_CORRECTION, label=\"q80\")\nplt.legend(loc=\"best\")\nplt.show()\n","b7b5a0ce":"if TESTSIZE > 0:\n    #predxs = net.predict(z_test, batch_size=BATCH_SIZE, verbose=0) #calcolato sull'ultimo fold\n    predxs = pred_test\n    idxs = np.random.randint(0, y_test.shape[0], 100)\n    idxs = np.sort(idxs)\n    plt.figure(figsize=(14,6))\n    plt.plot(y_test[idxs], label=\"ground truth\")\n    plt.plot(predxs[idxs, 0] * LOW_CORRECTION, label=\"q20\")\n    plt.plot(predxs[idxs, 1], label=\"q50\")\n    plt.plot(predxs[idxs, 2] * HIGH_CORRECTION, label=\"q80\")\n    plt.legend(loc=\"best\")\n    plt.show()","a40d7759":"sigma_opt = mean_absolute_error(y, pred[:, 1])\nunc = pred[:,2] - pred[:, 0]\nsigma_mean = np.mean(unc)\nprint('sigma_opt mean_absolute_error(y, pred[:, 1]):',sigma_opt, \n      '\\nsigma_mean np.mean(unc) unc = pred[:,2] - pred[:, 0]:',sigma_mean)","635f61be":"plt.hist(unc)\nplt.title(\"uncertainty in prediction\")\nplt.show()","ac7178b5":"CORRECTION = tf.constant(1.0, dtype='float32') ","2a6cd83f":"sab = sub.copy()","2fcfdae3":"\n#pe = net.predict(ze, batch_size=BATCH_SIZE, verbose=0) #calcolato sull'ultimo fold\n\nsab['FVC1'] = pe[:, 1]\nsab['Confidence1'] = (pe[:, 2] - pe[:, 0]) #* CORRECTION #devo capire perch\u00e9 utilizzare questa","dc128ebb":"CORRECTION","45bc52d4":"d21 = pe[:,2]-pe[:,1]\nd10 = pe[:,1]-pe[:,0]\nd20 = pe[:,2]-pe[:,0]\ndcorrect = (pe[:,2] - pe[:,0]) * CORRECTION\ng = {'2-1':d21,'1-0':d10,'2-0':d20,'2-0 corr':dcorrect}\ng = pd.DataFrame(data=g)\ng.describe().T","9d84e5bb":"subm = sab[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nsubm.loc[~subm.FVC1.isnull(),'Confidence'] = abs(subm.loc[~subm.FVC1.isnull(),'Confidence1'])\n\nsubm.describe().T","97153fbb":"if sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']\n\nsubm.describe().T","2962beb7":"otest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1\n","55a538ab":"subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)\nprint('Saved',GROUPS)","b54c5df6":"#import sys \n#sys.exit()","c58cec5e":"X=np.reshape(X,(X.shape[0],X.shape[1],1))","4bc66d64":"# Introduction\n\nThis notebook is forked from https:\/\/www.kaggle.com\/andypenrose\/osic-multiple-quantile-regression-starter \n","a07f9b55":"![image.png](attachment:image.png)","a7efdaf6":"trr = pd.DataFrame(tr).copy()\nppp=trr[trr.Patient == 'ID00007637202177411956430'].copy()\nppp['Old'] = True\naaa = ppp.loc[0]\naaa\nr = pd.DataFrame([aaa], columns = ppp.columns)\nr['Weeks']=0\nr['FVC']=0\nr['Percent']=0\nppp = pd.concat([ppp, r], ignore_index=True)"}}