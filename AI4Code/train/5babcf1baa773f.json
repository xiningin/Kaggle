{"cell_type":{"90dc4b3d":"code","68c96d84":"code","27dcaaee":"code","11aa00b2":"code","5ea1fb40":"code","70caae8c":"code","1cde8f01":"code","1bb3ea2e":"code","e8aa12bd":"code","9aafaec0":"code","7a87adf5":"code","a1a02fba":"code","acf39015":"code","60591e15":"code","8305ec37":"markdown","1cfbbba2":"markdown"},"source":{"90dc4b3d":"import pandas as pd\nimport numpy as np\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.model_selection import train_test_split","68c96d84":"file_path='..\/input\/used-cars-price-prediction\/train-data.csv'\ntrain=pd.read_csv(file_path,index_col=0);\ntrain.drop([\"New_Price\",\"Name\",\"Location\"],axis=1,inplace=True)\n","27dcaaee":"#data pre-processing\n\ntrain[\"Mileage\"]=train[\"Mileage\"].str.replace('kmpl','')\ntrain[\"Mileage\"]=train[\"Mileage\"].str.replace('km\/kg','')#replacing text to blank\ntrain[\"Mileage\"]=pd.to_numeric(train[\"Mileage\"],errors='coerce')#conversion to numeric from string\ntrain[\"Power\"]=train[\"Power\"].str.replace('bhp','')\ntrain[\"Power\"]=pd.to_numeric(train[\"Power\"],errors='coerce')\ntrain[\"Engine\"]=train[\"Engine\"].str.replace('CC','')\ntrain[\"Engine\"]=pd.to_numeric(train[\"Engine\"],errors='coerce')","11aa00b2":"#Missing values\ntrain=train.dropna(axis=0)# to drop the rows with missing values\n#train[\"Seats\"]=train[\"Seats\"].fillna(method='bfill',axis=0)# Fils value of preceding cell\n#print(train[\"Name\"].nunique(dropna=True))# Counting the number of unique cars in dataset\n#print(train.query('Mileage<1').count())#counts the number of rows where Mileage is 0\n#print(train.query('Engine==\"nan\"').count()) counts number of rows where specified column has nan value\ntrain[\"Mileage\"]=train[\"Mileage\"].where(train[\"Mileage\"]!=0,train[\"Mileage\"].mean())#inserts mean of mileage at all places where Mileage is 0\n","5ea1fb40":"train.columns","70caae8c":"#dividing dataset for validation\ny=train[\"Price\"];\nX=train.iloc[:,:-1];\n\n#load test data\nfile_path2='..\/input\/used-cars-price-prediction\/test-data.csv'\ntest=pd.read_csv(file_path2,index_col=0);","1cde8f01":"#Handling Categorical values\nfrom sklearn.preprocessing import OneHotEncoder\n\ns = (X.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(object_cols)\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X[object_cols]))\n#OH_cols_valid = pd.DataFrame(OH_encoder.transform(val_X[object_cols]))\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X.index\nOH_cols_train.columns=OH_encoder.get_feature_names(object_cols)#to copy the column label after encoding\n#print(OH_cols_train.head(5))\n#OH_cols_valid.index = val_X.index\n# Remove categorical columns (will replace with one-hot encoding)\nnum_train = X.drop(object_cols, axis=1)\n#num_X_valid = val_X.drop(object_cols, axis=1)\n# Add one-hot encoded columns to numerical features\nOH_train = pd.concat([num_train, OH_cols_train], axis=1)\n#print(OH_train.head(5))\n#OH_X_val = pd.concat([num_X_valid, OH_cols_valid], axis=1)\nOH_train.columns\n#x=OH_train.iloc[0,:-1]\n#x=x.values.reshape(1,-1)","1bb3ea2e":"train_X, val_X, train_y, val_y = train_test_split(OH_train, y, random_state = 0)","e8aa12bd":"correlation=OH_train.corr()\nplt.figure(figsize=(14,14))\nsns.heatmap(data=correlation,square=True,annot=True)","9aafaec0":"#Decision Tree\nfrom sklearn.tree import DecisionTreeRegressor\ndecision_tree=DecisionTreeRegressor(random_state=0)\ndecision_tree.fit(train_X,train_y)\npredict1=decision_tree.predict(val_X)\nprint(\"Mean Absolute Error:\",mean_absolute_error(val_y,predict1))\nprint(\"Mean Squared Error:\",mean_squared_error(val_y,predict1))\n#print(decision_tree.score(predict1,val_y.reshape(1,-1)[0]))\n","7a87adf5":"#Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nrandom_forest=RandomForestRegressor(random_state=1)\nrandom_forest.fit(train_X,train_y)\npredict2=random_forest.predict(val_X)\nprint(\"Mean Absolute Error:\",mean_absolute_error(val_y,predict2))\nprint(\"Mean Squared Error:\",mean_squared_error(val_y,predict2))","a1a02fba":"#XG Boost Regressor\nfrom xgboost import XGBRegressor\nmy_model = XGBRegressor()\nmy_model.fit(train_X,train_y)\npredict3 = my_model.predict(val_X)\nprint(\"Mean Absolute Error:\",mean_absolute_error(val_y,predict3))\nprint(\"Mean Squared Error:\",mean_squared_error(val_y,predict3))","acf39015":"from sklearn.linear_model import Lasso\nclf=Lasso(alpha=0.1)\nclf.fit(train_X,train_y)\npredict4=clf.predict(val_X)\nprint(\"Mean Absolute Error:\",mean_absolute_error(val_y,predict4))\nprint(\"Mean Squared Error:\",mean_squared_error(val_y,predict4))","60591e15":"import pickle \n#from sklearn.externals import joblib\nfilename = 'Price_predict.pkl'\npickle.dump(random_forest,open(filename,\"wb\"))\n","8305ec37":"I added a pickle file in this version in order to deploy it using Django.","1cfbbba2":"Random Forest works best so I will use its model"}}