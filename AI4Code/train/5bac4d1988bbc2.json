{"cell_type":{"575cd199":"code","63ec7566":"code","663adaee":"code","584f63ca":"code","4addf187":"code","1013ff0b":"code","3e449677":"code","b7804e17":"code","84eaf6c5":"code","9c2e4f09":"code","86037784":"code","461d9799":"code","f65a0cbe":"code","0666c4b5":"code","51be067c":"code","98a605cc":"code","83abcb5f":"code","1f180013":"code","24c5486d":"code","1e01e0d1":"code","9fe6fcf8":"code","9d3ca6d2":"code","502ec031":"code","ab9c7a14":"code","b95cee4e":"code","dad9cfb6":"code","7bc2e35b":"code","318fda48":"markdown","5bbdd9ec":"markdown","ca4d20a1":"markdown","d7937360":"markdown","14d4d590":"markdown","cbbc2b1b":"markdown","a3bf7e01":"markdown","990faeac":"markdown","40b76f42":"markdown","f732a26f":"markdown","baa2e5bf":"markdown","8564ea7a":"markdown","596698d0":"markdown","8882ee76":"markdown","acdc6925":"markdown","290c5e15":"markdown","99fccc56":"markdown","5df3c3ad":"markdown","8af35daa":"markdown","f18915c0":"markdown","173e706f":"markdown","b81eaefe":"markdown","afd9733e":"markdown"},"source":{"575cd199":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\n\nimport os\nimport glob\nimport pydicom\nfrom typing import Dict\nimport tqdm\n#color\nfrom colorama import Fore, Back, Style\n\nimport gc\n\n# my favourite style :-)\nplt.style.use('fivethirtyeight')","63ec7566":"train_dir = \"\/kaggle\/input\/rsna-str-pulmonary-embolism-detection\/train\/\"\ntest_dir = \"\/kaggle\/input\/rsna-str-pulmonary-embolism-detection\/test\/\"\ntrain = pd.read_csv( \"\/kaggle\/input\/rsna-str-pulmonary-embolism-detection\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/rsna-str-pulmonary-embolism-detection\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/rsna-str-pulmonary-embolism-detection\/sample_submission.csv\")","663adaee":"train['path'] = train_dir + train.StudyInstanceUID + \"\/\" + train.SeriesInstanceUID + \"\/\" + train.SOPInstanceUID + \".dcm\"","584f63ca":"print(\"Training dataset has {} rows & {} columns\".format(train.shape[0],train.shape[1]))\nprint(\"Testing  dataset has {}  rows &  {} columns\".format(test.shape[0],test.shape[1]))","4addf187":"train.head()","1013ff0b":"test.head()","3e449677":"train.isnull().sum()","b7804e17":"test.isnull().sum()","84eaf6c5":"train.info()","9c2e4f09":"for col in train.columns:\n    print(Fore.YELLOW +\"---------------------------------------------------\", Style.RESET_ALL)\n    print(Fore.BLUE + \"Unique values for the column\",col,\":\", Style.RESET_ALL)\n    print(Fore.GREEN , train[col].nunique()), Style.RESET_ALL\n    print(Fore.YELLOW +\"---------------------------------------------------\" + Style.RESET_ALL)    ","86037784":"for col in train.columns:\n    print(Fore.GREEN +\"---------------------------------------------------\", Style.RESET_ALL)\n    print(Fore.YELLOW + \"Values Distribution for the column\",col,\":\", Style.RESET_ALL)\n    print(Fore.BLUE , train[col].value_counts()), Style.RESET_ALL\n    print(Fore.GREEN +\"---------------------------------------------------\" + Style.RESET_ALL)    ","461d9799":"# function to plot countplot and piechart for different columns present in the dataset\ndef plot_graphs(cols_dict):    \n    \n    fig = plt.figure(figsize = (15,50))\n    \n    idx = 1\n    \n    for key in cols_dict:   \n        value = cols_dict[key]         \n        # countplots\n        plt.subplot(6,2,idx)\n        sns.countplot(train[key],palette=\"hls\",alpha=0.9)              \n        plt.xlabel(value)\n        plt.ylabel(\"Frequency\")\n        plt.title(value)\n        idx = idx + 1\n    \n    #plt.suptitle(\"Predictor Variables -  Distribution Graph\")\n    \n    plt.show()","f65a0cbe":"cols = {\"pe_present_on_image\":\"Pe Present On Image\",\n        \"negative_exam_for_pe\" :  \"Negative Exam For Pe\",\n        \"rv_lv_ratio_gte_1\" : \"Rv Lv Ratio Gte\",\n        \"rv_lv_ratio_lt_1\":\"Rv Lv Ratio Lt\",\n        \"leftsided_pe\" : \"Leftsided Pe\", \n        \"chronic_pe\" : \"Chronic Pe\",\n        \"true_filling_defect_not_pe\" : \"True Filling Defect Not Pe\" ,\n        \"rightsided_pe\" : \"Rightsided Pe\",\n        \"acute_and_chronic_pe\" : \"Acute And Chronic Pe\",\n        \"central_pe\":\"Central Pe\",\n        \"indeterminate\":\"Indeterminate\"}\nplot_graphs(cols)","0666c4b5":"print(\"There are {} StudyInstanceUID in the training dataset\".format(len(os.listdir(train_dir))))","51be067c":"p_sizes = []\nfor d in os.listdir(train_dir):    \n    for sub in os.listdir(train_dir + d):   \n        p_sizes.append(len(os.listdir(train_dir + d + \"\/\" + sub)))\n        print(\"StudyInstanceUID '{}' has {} SeriesInstanceUID\".format(d, len(os.listdir(train_dir + d + \"\/\" + sub))))\n        print(\"StudyInstanceUID '{}' has {} instances of the Series {}\".format(d,len(os.listdir(train_dir + d + \"\/\" + sub)),sub))","98a605cc":"# lets visualize trainig data\nfig = plt.figure(figsize = (12,8))\np = sns.color_palette()\nplt.hist(p_sizes,color = \"purple\")\nplt.ylabel('Number of StudyInstanceUID')\nplt.xlabel('Count of Series UIDs')\nplt.title('Histogram of DICOM count per StudyInstanceUID - Training Data')","83abcb5f":"# lets read the first dicom image in training dataset\ndcm = train.path.iloc[0]\nprint('Filename: {}'.format(dcm))\ndcm = pydicom.read_file(dcm)","1f180013":"print(dcm)","24c5486d":"# display the image read above\nfig = plt.figure(figsize = (20,50))\n\nimg = dcm.pixel_array\nimg[img == -2000] = 0\n\nplt.subplot(121)\nplt.axis('off')\nplt.imshow(img)\n\nplt.subplot(122)\nplt.axis('off')\nplt.imshow(-img) # Invert colors with -\nplt.show()","1e01e0d1":"# helper function\ndef dicom_to_image(filename):\n    dcm = pydicom.read_file(filename)\n    img = dcm.pixel_array\n    img[img == -2000] = 0\n    return img","9fe6fcf8":"# lets display some 20 images at random\nfiles = train.path.iloc[0:20]\n\nf, plots = plt.subplots(4, 5, sharex='col', sharey='row', figsize=(10, 8))\nfor i in range(20):\n    plots[i \/\/ 5, i % 5].axis('off')\n    plots[i \/\/ 5, i % 5].imshow(dicom_to_image(np.random.choice(files)), cmap=plt.cm.bone)","9d3ca6d2":"def extract_dicom_meta_data(filename: str) -> Dict:\n    # Load image\n    dcm = pydicom.read_file(filename)\n    img=np.array(dcm.pixel_array).flatten()\n    data = {\n        'study_instance_uid': dcm.StudyInstanceUID,\n        'series_instance_uid': dcm.SeriesInstanceUID,\n        'series_number': dcm.SeriesNumber,\n        'instance_number': dcm.InstanceNumber,\n        'specific_character_set': dcm.SpecificCharacterSet,\n        #'image_type': dcm.ImageType,\n        'sop_class_uid': dcm.SOPClassUID,\n        'sop_instance_uid': dcm.SOPInstanceUID,\n        'modality': dcm.Modality,\n        'slice_thickness': dcm.SliceThickness,\n        'kvp': dcm.KVP,\n        'gantry_detector': dcm.GantryDetectorTilt,\n        'table_height': dcm.TableHeight,\n        'rotation_direction': dcm.RotationDirection,\n        'x_ray_tube_current': dcm.XRayTubeCurrent,\n        'exposure': dcm.Exposure,\n        'convolution_kernel' : dcm.ConvolutionKernel,\n        'patient_position' : dcm.PatientPosition,\n        #'image_position_patient' : dcm.ImagePositionPatient,\n        #'image_orientation_patient': dcm.ImageOrientationPatient,\n        'frame_of_reference_uid' : dcm.FrameOfReferenceUID,\n        'samples_per_pixel' : dcm.SamplesPerPixel,\n        'photometric_interpretation' : dcm.PhotometricInterpretation,\n        'rows' : dcm.Rows,\n        'columns' : dcm.Columns,\n        'pixel_spacing' : dcm.PixelSpacing,\n        'bits_allocated' : dcm.BitsAllocated,\n        'bits_stored' : dcm.BitsStored,\n        'high_bit' : dcm.HighBit,\n        'pixel_representation': dcm.PixelRepresentation,\n        'window_center': dcm.WindowCenter,\n        'window_width': dcm.WindowWidth,\n        'rescale_intercept': dcm.RescaleIntercept,\n        'rescale_slope': dcm.RescaleSlope,\n        'pixel_data': dcm.PixelData,\n        'img_min': np.min(img),\n        'img_max': np.max(img),\n        'img_mean': np.mean(img),\n        'img_std': np.std(img)\n        }\n    return data","502ec031":"meta_data_df = extract_dicom_meta_data(train.path.iloc[0])\n\n# Convert to a pd.DataFrame from dict\nmeta_data_df = pd.DataFrame.from_dict(meta_data_df)\nmeta_data_df.shape","ab9c7a14":"feats = list(train.columns[3:5])+list(train.columns[8:12])+list(train.columns[13:17])","b95cee4e":"means = train[feats].mean().to_dict()\nmeans","dad9cfb6":"submission['label'] = 0.2799\nfor feat in means.keys():\n    submission.loc[submission.id.str.contains(feat, regex=False), 'label'] = means[feat]","7bc2e35b":"submission.to_csv('submission.csv', index = False)","318fda48":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udcca Exploratory Data Analysis \ud83d\udcca<\/center><\/h2>","5bbdd9ec":"<div class=\"alert alert-block alert-info\"> <b>\ud83d\udd11 Observations:<\/b> \n\n*     Training dataset has 7279 unique Study level UIDs\n \n*     Training dataset has 7279 unique Study level UIDs\n \n*     Training dataset has 1790594 unique SOP level UIDs\n\n*     Rest all columns are binary variables, which means they can have only 0 and 1 as possible values\n   \n<\/div>","ca4d20a1":"<div class=\"alert alert-block alert-info\"> \nNow, we have read the image in a variable called dcm, we can simply \"print\" this variable to see the information related to the image!\n <\/div>\n","d7937360":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udcc3 Training & Test Datasets \ud83d\udcc3<\/center><\/h2>\n    ","14d4d590":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83c\udf88 Explore training directory \ud83c\udf88 <\/center><\/h2>","cbbc2b1b":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udc41 No. of Unique values for each column \ud83d\udc41 <\/center><\/h2>","a3bf7e01":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>\ud83d\udcda Load Necessary Libraries \ud83d\udcda  <\/center><\/h2>\n    ","990faeac":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>\ud83d\udd0e About the Disease - Pulmonary Embolism \ud83d\udd0d <\/center><\/h2>\n    \nIf every breath is strained and painful, it could be a serious and potentially life-threatening condition. A pulmonary embolism (PE) is caused by an artery blockage in the lung. It is time consuming to confirm a PE and prone to overdiagnosis. Machine learning could help to more accurately identify PE cases, which would make management and treatment more effective for patients.\n\nCurrently, CT pulmonary angiography (CTPA), is the most common type of medical imaging to evaluate patients with suspected PE. These CT scans consist of hundreds of images that require detailed review to identify clots within the pulmonary arteries. As the use of imaging continues to grow, constraints of radiologists\u2019 time may contribute to delayed diagnosis.\n\nThe Radiological Society of North America (RSNA\u00ae) has teamed up with the Society of Thoracic Radiology (STR) to help improve the use of machine learning in the diagnosis of PE.\n\nIn this competition, you\u2019ll detect and classify PE cases. In particular, you'll use chest CTPA images (grouped together as studies) and your data science skills to enable more accurate identification of PE. If successful, you'll help reduce human delays and errors in detection and treatment.\n\nWith 60,000-100,000 PE deaths annually in the United States, it is among the most fatal cardiovascular diseases. Timely and accurate diagnosis will help these patients receive better care and may also improve outcomes.\n    \n<img src= \"https:\/\/images.medicinenet.com\/images\/article\/main_image\/pulmonary-embolism-lungs.jpg\">","40b76f42":"This gives us some idea with the sort of images we're dealing with.","f732a26f":"<div class=\"alert alert-block alert-info\"> <b>\ud83d\udd11 Observations:<\/b> \n\n*     No Null Values present in the dataset!\n   \n<\/div>","baa2e5bf":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> Add Path to the training dataset<\/center><\/h2>","8564ea7a":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udcca Data Distribution \ud83d\udcca<\/center><\/h2>","596698d0":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udcbe An Introduction to DICOM Files \ud83d\udcbe <\/center><\/h2>\n    \nDigital Imaging and Communications in Medicine (DICOM) is the standard for the communication and management of medical imaging information and related data.\n\nThese files may also include patient information to pair the image with the patient.\n\nDICOM is most commonly used for storing and transmitting medical images enabling the integration of medical imaging devices such as scanners, servers, workstations, printers, network hardware, and picture archiving and communication systems (PACS) from multiple manufacturers. \n    \nIt has been widely adopted by hospitals and is making inroads into smaller applications like dentists' and doctors' offices.\n\nMultiple .dcm files represent different slices of a single CT scan. CT scans produce 3D volumes for each scan, those volumes consist of 2D slices and each slice is a .dcm file.\n\nWe have 2 directories here, train and test, both these directories have dicom images for different-different patients.","8882ee76":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83e\udd47\ud83e\udd48 Evaluation Metrics \ud83e\udd49\ud83c\udfc6  <\/center><\/h2>\n\nEvery `study \/ exam` has a row for each label that is scored. \n\nIt is uniquely indicated by the `StudyInstanceUID`. \n\nEvery image, further, has a row for the PE Present on Image label and is uniquely indicated by the SOPInstanceUID. \n    \nPrediction file should have a number of rows equal to: (number of images) + (number of studies * number of scored labels).\n\n### Metric\ud83c\udfaf\nThe metric used in this competition is `weighted log loss`. \n\nIt is weighted to account for the relative importance of some labels. \n\nThere are `9 study-level labels` and `one image-level label`.\n\n#### Exam-level weighted log loss\nLet y_ij = 1 if label j was annotated to exam i and y_ij = 0, otherwise. Let p_ij be the predicted probability that y_ij = 1:\ni = 1, 2, \u2026, N for N exams in the test set\nj = 1, 2, \u2026, 9 labels\n\nLet w_j signify the weight for label j.\n\nThe weights are as follows:\n\n![image.png](attachment:image.png)\n\nKaggle uses a binary log loss equation for each label and then takes the mean of the log loss over all labels.\n\nThe binary weighted log loss function for label j on exam i is specified as:\nLij=\u2212wj\u2217[yij\u2217log(pij)+(1\u2212yij)\u2217log(1\u2212pij)]\n\n\n#### Image-level weighted log loss\nLet y_ik = 1 if image k in exam i was annotated as \u2018PE Present on Image\u2019; otherwise, y_ik = 0.\nLet p_ik be the predicted probability that y_ik = 1.\nw = 0.07361963\ni = 1, 2, \u2026, N exams\nk = 1, 2, \u2026, n_i, where n_i is the number of images in exam i\n\nThen, let m_i = sum_(k = 1 to n_i) y_ik be the number of positive images in exam i such that\nq_i = m_i\/n_i is the proportion of positive images in exam i\n\nAt the image level, we have a binary classification where the image is classified as PE Present on Image or not (image is negative for PE).\n\nThe image-level log loss is written as:\n\n\nThe total loss is the average of all image- and exam-level loss, divided by the sum of the weights.","acdc6925":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udcbe Let's Load some more Random DICOM Images \ud83d\udcbe <\/center><\/h2>","290c5e15":"# Baseline Model","99fccc56":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83c\udf88 Load a Random DICOM Image \ud83c\udf88 <\/center><\/h2>\n\nDICOM files can be read and processed easily with pydicom package. \n    \nDICOM files allow to store metadata along with pixel data inside them. \n    \nReading a dicom file creates a pydicom.dataset.FileDataset object. \nFileDataset object wraps dict and contains DataElement instances.","5df3c3ad":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>\ud83d\udc40 Understanding the Data \ud83d\udc40 <\/center><\/h2>\n\nWe have the `training` and `test images`, as well as `train.csv` and `test.csv`. \n\nThe images are grouped in directories by `study` and `series`. \n\nThey are in `DICOM` format, and contain additional metadata that may be relevant to the competition. \n\nEach image has a unique identifier - `SOPInstanceUID`.\n\nThe location for each image is given by: `<StudyInstanceUID>\/<SeriesInstanceUID>\/<SOPInstanceUID>.dcm`.\n\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Train.csv\ud83d\udcc4 Test.csv \ud83d\udcc4 & Sample Submission.csv \ud83d\udcdc <\/center><\/h2>\n\n* `train.csv` contains the three UIDs noted above, and a number of labels. Some are targets which require predictions, and some are informational, which will also be noted below in Data fields.\n\n* `test.csv` contains only the three UIDs.\n\n* `sample_submission.csv` contains rows for each UID+label combination that requires a prediction. Therefore it has a row for each image (for which we need to predict the existence of a pulmonary embolism within the image) and row for each study+label that requires a study-level prediction.\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udd8d What are we predicting \ud83d\udd8d <\/center><\/h2>\n    \nIn this competition we are predicting a number of labels, at both the image and study level. \n\nEach `UID+label` combination requires a prediction. So, we need to make a prediction for each image & also need to make a prediction for each study+label combination at study-level.\n<div class=\"alert alert-block alert-info\"><b>Note # 1\u270d:<\/b> Some labels are Logically Mutually Exclusive,predictions must adhere to the expected label hierarchy defined in this diagram, and the host will verify that prospective winners have not made conflicting label predictions!<\/div>\n  \n  \n  ![image.png](attachment:image.png) \n  \n  \nAbove image is a flowchart outlining the relationships between labels. \n\n<div class =\"alert alert-block alert-info\"> <b>Note # 2 \u270d:<\/b> There are four labels in the training set that are purely informational and require no predictions. They are: \n    \n* QA Contrast\n* QA Motion\n* True filling defect not PE\n* Flow artifact <\/div>\n \nAbove labels are not scored, but are meant to be used as helpers. \n\n<div class=\"alert alert-block alert-info\"><b>Note # 3 \u270d:<\/b> Acute PE is not an explicit label, but is implied by the lack of Chronic PE or Acute and Chronic PE<\/div>\n\nPredictions must adhere to the expected label hierarchy defined in this diagram, and the host will verify that prospective winners have not made conflicting label predictions, as detailed on the Prizes page.  \n\n<div class=\"alert alert-block alert-info\"> <b>Note # 4 \u270d:<\/b> Submission kernels will NOT have access to train images, so we need to build our models elsewhere and incorporate them into our submissions<\/div>\n","8af35daa":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udc40 Null Values Check \ud83d\udc40<\/center><\/h2>","f18915c0":"#  Extracting DIOCOM files information in a dataframe ","173e706f":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:brown; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> \ud83d\udcc3 Let's take a look at the datasets \ud83d\udcc3 <\/center><\/h2>","b81eaefe":"<div class=\"alert alert-block alert-info\"> \n\nThere are two things here that I think are significant, slice location and the 'Pixel Data'.\n\nWe can retrieve a image as a numpy array by calling dcm.pixel_array, and we can then replace the -2000s, which are essentially NAs, with 0s.\n    \n<\/div>","afd9733e":"<div class=\"alert alert-block alert-info\"> \n<b>\ud83d\udd11 Observations:  \n    \nData set is highly imbalanced, as all the predictor variables have one of the values in 80s and 90s % and the other one is very low in %\n\n<\/b><\/div>"}}