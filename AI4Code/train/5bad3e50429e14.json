{"cell_type":{"07101eac":"code","b0d3b631":"code","ced1dc21":"code","28072f24":"code","c040b922":"code","5f737374":"code","093a23c4":"code","ed52e3da":"code","99bb933b":"code","db9df025":"code","fea8bd2e":"code","c7005c60":"code","231295b7":"code","ff5f491a":"code","aa018aa8":"code","ba17e08f":"code","4f0754f2":"code","eb713ca8":"code","c07f7d36":"markdown","fa92bfe6":"markdown","c8b23f2c":"markdown","60746b88":"markdown","36fe1473":"markdown"},"source":{"07101eac":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","b0d3b631":"!pip uninstall -q typing --yes\n# !pip install -U git+https:\/\/github.com\/lezwon\/pytorch-lightning.git@2016_test\n# !pip install -qU git+https:\/\/github.com\/rohitgr7\/pytorch-lightning@fix_tpu_id\n# !pip install -U git+https:\/\/github.com\/lezwon\/pytorch-lightning.git@bugfix\/2016_slow_tpu_train\n!pip install -qU git+https:\/\/github.com\/PyTorchLightning\/pytorch-lightning.git","ced1dc21":"!rm -rf '\/kaggle\/working\/lightning_logs'","28072f24":"import os\n\nimport torch\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\nfrom torch.utils.data import SubsetRandomSampler\nimport pytorch_lightning as pl\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport torch_xla.core.xla_model as xm\n\n\nimport torch_xla","c040b922":"# Set a seed for numpy for a consistent Kfold split\nnp.random.seed(123)","5f737374":"# Download the dataset in advance\nMNIST(os.getcwd(), train=True, download=True)\nMNIST(os.getcwd(), train=False, download=True)","093a23c4":"class MNISTModel(pl.LightningModule):\n\n    def __init__(self, hparams=None):\n        super(MNISTModel, self).__init__()\n#         self.hparams = hparams\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=6.918309709189366e-07)\n\n    def training_step(self, batch, batch_nb):\n        # REQUIRED\n        # print(self.device, next(self.parameters()).device)\n        x, y = batch\n        y_hat = self.forward(x)\n        loss = F.cross_entropy(y_hat, y)\n        tensorboard_logs = {'train_loss': loss}\n        return {'loss': loss, 'log': tensorboard_logs, 'progress_bar': {'tpu': torch_xla._XLAC._xla_get_default_device()}}\n\n    def validation_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        return {'val_loss': F.cross_entropy(y_hat, y)}\n\n    def validation_epoch_end(self, outputs):\n        # OPTIONAL\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        tensorboard_logs = {'val_loss': avg_loss}\n        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n\n    # def test_step(self, batch, batch_nb):\n    #     # OPTIONAL\n    #     x, y = batch\n    #     y_hat = self(x)\n    #     return {'test_loss': F.cross_entropy(y_hat, y)}\n\n    # def test_epoch_end(self, outputs):\n    #     # OPTIONAL\n    #     avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n    #     logs = {'test_loss': avg_loss}\n    #     return {'avg_test_loss': avg_loss, 'log': logs, 'progress_bar': logs}\n\n    \n    def prepare_data(self):\n        dataset = MNIST(os.getcwd(), train=True, download=False, transform=transforms.ToTensor())\n        self.mnist_test = MNIST(os.getcwd(), train=False, download=False, transform=transforms.ToTensor())\n        \n        kf = KFold(n_splits=8)\n        splits = list(kf.split(dataset))\n\n        train_indices, val_indices = splits[1]\n        \n        self.mnist_train = torch.utils.data.Subset(dataset, train_indices)\n        self.mnist_val = torch.utils.data.Subset(dataset, val_indices)\n                \n    @classmethod\n    def load_from_checkpoint(cls, checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        model = cls()\n        model.load_state_dict(checkpoint['state_dict'])\n        return model\n\n    def train_dataloader(self):\n        # loader = DataLoader(self.mnist_train, batch_size=32, num_workers=4)\n        # return loader\n        mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transforms.ToTensor())\n        loader = DataLoader(mnist_train, batch_size=32, num_workers=4)\n        return loader\n\n    def val_dataloader(self):\n        loader = DataLoader(self.mnist_val, batch_size=32, num_workers=4)\n        return loader\n\n    # def test_dataloader(self):\n        # loader = DataLoader(self.mnist_test, batch_size=32, num_workers=4)\n        # return loader","ed52e3da":"# mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transforms.ToTensor())\n# loader = DataLoader(mnist_train, batch_size=32, num_workers=4)\n\n# for a in loader:\n#     print(a[0].shape)\n# #     break","99bb933b":"hparams = {'lr': 6.918309709189366e-07, 'fold': 1}","db9df025":"# model = MNISTModel(hparams)\n# trainer = pl.Trainer(tpu_cores=8, max_epochs=1)    ","fea8bd2e":"# Define a function to initialize and train a model\ndef train(tpu_id):\n    model = MNISTModel(hparams)\n    trainer = pl.Trainer(tpu_cores=tpu_id, max_epochs=2, checkpoint_callback=True, weights_summary=None)    \n    trainer.fit(model)\n    print('Training Done')\n#     trainer.test(model, ckpt_path=None)","c7005c60":"# Specifying tpu core id\n# train([1])","231295b7":"# Training on 1 core\n# train(1)","ff5f491a":"# Specifying tpu core id\ntrain(8)","aa018aa8":"# #use joblib to run the train function in parallel on different folds\n# import joblib as jl\n# parallel = jl.Parallel(n_jobs=8, backend=\"threading\", batch_size=1)\n# parallel(jl.delayed(train)(i+1) for i in range(8))","ba17e08f":"# weights are saved to checkpoints\n# !ls -lh checkpoints\/ ","4f0754f2":"# xm.xla_device(n=8, devkind='TPU')","eb713ca8":"# dev = xm.xla_device()\n# t1 = torch.ones(3, 3, device = dev)\n# print(t1)","c07f7d36":"# Parallel KFold training on TPU using Pytorch Lightning\nThis kernel demonstrates training K instances of a model parallely on each TPU core.","fa92bfe6":"## Install PyTorch Lightning","c8b23f2c":"# **Define a Lightning Module**\nDefine a lightning module that takes in fold number in hparams.","60746b88":"### Install XLA\nXLA powers the TPU support for PyTorch","36fe1473":"## Train\nUse the `trainer` to train an instance of a model. It takes care of all the TPU setup given a `tpu_id` in `tpu_cores`."}}