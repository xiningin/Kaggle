{"cell_type":{"c2107561":"code","d254c6f7":"code","c208250a":"code","06bf15cc":"code","faa3e8d5":"code","0460f4bb":"code","f4b85ba3":"code","a451b760":"code","5e754fbf":"code","af250471":"code","a0c2dff4":"code","eb882e71":"code","c305673a":"code","467b851d":"code","1a08213d":"code","0bb643f6":"code","d630fb99":"code","b1e71edb":"markdown","0d515006":"markdown","1954804e":"markdown","2dda8088":"markdown","a00b2fb2":"markdown","159f6171":"markdown","5817a77a":"markdown","101f9244":"markdown","c318b899":"markdown","6abcf50d":"markdown","23367adb":"markdown","48965d48":"markdown","e2c0ef3e":"markdown","84f20128":"markdown","5ffca1d7":"markdown"},"source":{"c2107561":"# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.\nimport os\nprint(os.listdir(\"..\/input\"))","d254c6f7":"data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')  # importing data in csv format with pandas library\ntest = pd.read_csv('..\/input\/boratest\/test.csv')\ndata.head()                           #  ","c208250a":"data.info()","06bf15cc":"x = data.drop(['label'], axis=1).values\/255 # make input values numpy array, then normalize by dividing with 255.\nfor i in range(9):   \n    \n    plt.subplot(3,3,i+1)\n    plt.imshow(x[i].reshape(28,28), cmap='gray')\n    plt.axis('off') \n\n","faa3e8d5":"### seperating label (y) values and One-Hot encoding for multi-label classification ###\n\n# It doesen't matter in what format you'll form y, but after one-hot encoding, it must be converted to array by .toarray()\n#y = pd.DataFrame(data.label)\ny = data.label.values.reshape(-1,1)\nx_test = test.values\/255\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(categorical_features='all')\ny = ohe.fit_transform(y).toarray()\n\n# Now every column in y corresponds to a class.\n\ny.shape","0460f4bb":"### train test split ###\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 23)","f4b85ba3":"### building ANN function\n# importing libraries\nfrom keras.models import Sequential # initializing neural network library\nfrom keras.layers import Dense, Dropout # building layers\n\n# feed-forward neural network classifier is assigned as \"model\".\nmodel = Sequential()  \n# we use dropout in the ratio of 0.25 to prevent overfitting.\nmodel.add(Dropout(0.25)) \n# 8 units for the first layer, also the input shape must be given in this line. \n# ReLU activation function is more useful than tanh function due to vanishing gradient problem.\n# weights are initialized as \"random uniform\".\nmodel.add(Dense(8, activation='relu', kernel_initializer='random_uniform', input_dim = x_train.shape[1])) \n# 16 nodes for the second layer\nmodel.add(Dense(16, activation='relu', kernel_initializer='random_uniform'))\n# since we have 10 outputs, in the last layer we need to enter 10 nodes. The output of the softmax function can be used to represent a categorical distribution. \nmodel.add(Dense(10, activation='softmax', kernel_initializer='random_uniform'))\n\n# we compile our model by using \"adadelta\" optimizer. \n# since we have categorical outputs, loss function must be the cross entropy. if you use grid search, you need to use \"sparse_categoricalentropy\".\nmodel.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# fit the model with below batch size and number of epochs.\n# verbose integers 0,1,2 sets the appearance of progress bar. \"2\" shows just a line.\nhistory = model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs = 10, batch_size = 155, verbose = 2)","a451b760":"# a look on test data\n\n# since we don't have any labels on test data that helps to find accuracy, we take a look at our first 9 predictions.\n\npredicted_classes = pd.DataFrame(model.predict(test)) # make a dataframe from prediction values because their index will be needed.\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(x_test[i].reshape(28,28), cmap='gray')\n    plt.title(predicted_classes.iloc[i].idxmax(axis=1)) # idmax gives us the column name(which are our outputs) of the maximum value in a row\n    plt.axis('off') # don't show the axis","5e754fbf":"### Test Loss Visualization ###\nloss = go.Scatter(y= history.history['val_loss'], x=np.arange(0,10), mode = \"lines+markers\", name='Test Loss') \naccuracy = go.Scatter(y= history.history['val_acc'], x=np.arange(0,10), mode = \"lines+markers\", name='Test Accuracy') \nlayout = dict(title = 'Test Loss & Accuracy Visualization',\n              xaxis= dict(title= 'Epochs',ticklen= 5,zeroline= True),\n              yaxis= dict(title= 'Loss & Accuracy',ticklen= 5,zeroline= True))\ndata = [loss, accuracy]\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","af250471":"# Importing confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Since we don't have the labels for \"test\" data like in real life, we will only create a confusion matrix of validation values.\n\n# Predict test values.\ny_predicted = model.predict(x_valid)\n\n# Find the column indices of maximum values which corresponds to predicted digits.\n# An alternative method to do this, as it's done in subplots above, to convert the matrix to dataframe first, then find maximum column indices with \"idxmax\".\ny_predicted = np.argmax(y_predicted, axis = 1) \ny_true = np.argmax(y_valid, axis = 1) \n\n# Create the confusion matrix.\nconfusion__matrix = confusion_matrix(y_true, y_predicted) \n\n# Plot it!\nplt.figure(figsize=(10,10))\nsns.heatmap(confusion__matrix, annot=True, linewidths=0.2, cmap=\"Blues\",linecolor=\"black\",  fmt= '.1f')\nplt.xlabel(\"Predicted Labels\", fontsize=15)\nplt.ylabel(\"True Labels\", fontsize=15)\nplt.title(\"Confusion Matrix\", color = 'red', fontsize = 20)\nplt.show()","a0c2dff4":"from sklearn.metrics import precision_recall_curve\nclasses = y.shape[1]\n\nprecision = dict()\nrecall = dict()\ny_predict = model.predict(x_valid)\nfor i in range(classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_valid[:, i], y_predict[:, i])","eb882e71":"colors = ['red', 'blue', 'green', 'black', 'cyan', 'purple', 'pink', 'navy', 'yellow', 'brown']\nlines = []\nlabels = []\nplt.figure(figsize=(10,10))\n\nfor i in range(classes):\n    plt.plot(recall[i], precision[i], color=colors[i])\n    labels.append('Precision-recall for class {0}'.format(i+1))\n    \nplt.ylim([0.0, 1.03])\nplt.xlim([0.0, 1.03])\nplt.xlabel('Recall',fontsize=15)\nplt.ylabel('Precision', fontsize = 15)\nplt.title('Recall vs Precision',fontsize = 20)\nplt.legend(labels, loc=(.3, .3), prop={'size':12})\nplt.show()","c305673a":"colors = ['red', 'blue', 'green', 'black', 'cyan', 'purple', 'pink', 'navy', 'yellow', 'brown']\nlines = []\nlabels = []\n\nplt.figure(figsize=(10,30))\n\nfor i in range(classes):\n    plt.subplot(5,2,i+1)\n    labels.append('Precision-recall for class {}'.format(i+1))\n    plt.plot(recall[i], precision[i], color=colors[i], label=labels[i])\n    plt.legend(loc=(.1, .3), prop={'size':12})\n    plt.title('Class {}'.format(i+1),fontsize = 15)\n    plt.xlabel('Recall',fontsize=10)\n    plt.ylabel('Precision', fontsize = 10)\n\n    \nplt.show()\n","467b851d":"from sklearn.metrics import f1_score\nprint('F1 Score: {}'.format(f1_score(y_true, y_predicted, average='macro')))","1a08213d":"y_predicted = y_predicted.T\ny_true = y_true.T\n\nfrom sklearn.preprocessing import label_binarize\ny_true_roc = label_binarize(y_true,classes=[0,1,2,3,4,5,6,7,8,9])\ny_pred_roc= label_binarize(y_predicted, classes=[0,1,2,3,4,5,6,7,8,9])\n\nfpr = {} # false positive rate\ntpr = {} #  true positive rate\nroc_auc = {}\nfrom sklearn.metrics import roc_curve, auc\nfor i in range(y_true_roc.shape[1]):\n    fpr[i], tpr[i], _ = roc_curve(y_pred_roc[:, i], y_true_roc[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])","0bb643f6":"colors = ['red', 'blue', 'green', 'black', 'cyan', 'purple', 'pink', 'navy', 'yellow', 'brown']\nlines = []\nlabels = []\n\nplt.figure(figsize=(10,10))\nfor i in range(y_true_roc.shape[1]):\n    labels.append('ROC curve for class {} & Area = {:f}'.format(i+1, roc_auc[i])) \n    plt.plot(fpr[i], tpr[i], color = colors[i],label=labels[i])\n    plt.legend(loc=(.2, .3), prop={'size':15})\n    plt.ylim([0.0, 1.03])\n    plt.xlim([0.0, 1.03])\n    plt.xlabel('False Positive Rate', fontsize=15)\n    plt.ylabel('True Positive Rate', fontsize =15)\n    plt.title('ROC curves & AUC scores'.format(i+1), fontsize=15)\nplt.show()","d630fb99":"colors = ['red', 'blue', 'green', 'black', 'cyan', 'purple', 'pink', 'navy', 'yellow', 'brown']\nlines = []\nlabels = []\n\nplt.figure(figsize=(10,30))\n\nfor i in range(classes):\n    plt.subplot(5,2,i+1)\n    labels.append('Precision-recall for class {}'.format(i+1))\n    plt.plot(recall[i], precision[i], color=colors[i], label=labels[i])\n    plt.legend(loc=(.1, .3), prop={'size':12})\n    plt.title('Class {}'.format(i+1),fontsize = 15)\n    plt.xlabel('Recall',fontsize=10)\n    plt.ylabel('Precision', fontsize = 10)\n\n    \nplt.show()","b1e71edb":"* ROC curves typically feature true positive rate(**TPR**) on the y axis, and false positive rate(**FPR**) on the x axis\n* Ideal point of a ROC curve is on top left where TP rate is 1 and FP rate is 0.\n* AUC score equals the area under the ROC curve. When it equals 1, then the classification is done without any errors.","0d515006":"* Now it's time to evaluate our model. We use \"Precision-Recall\" metric for evaluating classifier models. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n    * Precision is defined as the number of true positives(TP) over the number of true positives(TP) plus the number of false positives(FP). = TP \/ (TP + FP)\n    * Recall is defined as the number of true positives(TP) over the number of true positives(TP) plus the number of false negatives(FN).          = TP \/ (TP + FN)\n    * The relationship between recall and precision can be observed in the stairstep area of the plot which will be done below.\n    * You can think of positive(P) and negatives(N) as an evaluation between just 2 categories in \"confusion matrix\" we've plotted in heatmap above. But since we have 10 different labels in our model, we will use one-vs-all method by plotting \"precision-recall\" graph.","1954804e":"# **Confusion Matrix**\n\n* A confusion matrix gives us the number of correct and incorrect predictions of a classification model compared to the actual outcomes. Size of a confusion matrix is NxN, where N is the number of classes. Performance of such models is commonly evaluated using the data in the matrix.","2dda8088":"# **5. Model Evaluation**","a00b2fb2":"# **2. Data Overview**\n* The data I used which is imported as 'train.csv' has 785 columns and 42,000 rows. \n* Rows are the number of images of the digits we have in our data.\n* As for the columns, the first column is 'label' column, which contains the given answers, here are some label-encoded digits between 0 - 9 corresponding every images, like in any supervised learning.\n* The rest of the columns are the pixels of our 2D images, whose size is 28x28. Since we use a 2D matrix, it's already converted to 2D by multiplying the pixels with each other.\n\nLet me show you some of the images below:","159f6171":"I'll be thankful if you upvote this kernel, thanks in advance. \n\n**END**","5817a77a":"- To find ROC curve in multiclass problems, we need binarized labels.","101f9244":"# ** 1. Introduction**\n* In this kernel, I'll show you my multi-label classification of digits from 0 to 9 by using MLP.\n* Precision-recall curve and F1 score as well as ROC curve and AUC score will be demonstrated as metrics which are used in evaluation of classification models\n\nLet's start by importing libraries & data.","c318b899":"# **ROC Curve & AUC Score**","6abcf50d":"# **Test Loss & Accuracy Visualization**\n* The fit() method of Keras model returns a \"**history**\" object. The history.history attribute is a dictionary recording training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values.\n* In below figure, the visualization of change in validation loss and validation accuracy is shown. We can learn from this graph how many epochs are enough for our model where it started not to show a significant decreasing in loss after a certain epoch. Since the model parameters are assigned as random, it will change in every run of code.\n* You can also see the change in the loss and the accuracy interactively by holding mouse on scatter points","23367adb":"* Eventually, to find a specific score of precision-recall for all labels, we need to calculate F1 score. \n* The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n* Since we have multiple labels, we need to determine a method in \"average\" parameter in which I prefered using \"macro\" which calculate metrics for each label, and find their unweighted mean.\n* Notice that, both arrays including \"true\" and \"predicted\" labels must consist of only label values to be able to evaluate F1 score, NOT in one-hot encoded forms. But if you use 'samples' in average parameter, then the classes must be in one-hot encoded form.","48965d48":"* To observe every plot clearly, we can try to see all of them in subplots.","e2c0ef3e":"# **3. Preprocessing**","84f20128":"# **Precision - Recall Curve & F1 Score**","5ffca1d7":"# **4. Creating Model**"}}