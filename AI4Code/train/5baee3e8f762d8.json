{"cell_type":{"a8a32e83":"code","35eb8a55":"code","2b73d966":"code","ab8fd841":"code","0f438eb2":"code","1db5d0f5":"code","0114a5bd":"markdown","19ffc56d":"markdown","560ae375":"markdown","0bb826c8":"markdown","eb42ebc0":"markdown","bdd59993":"markdown","29d518ae":"markdown","5d46d34b":"markdown"},"source":{"a8a32e83":"import pandas as pd\n\n# Save train and test data as pandas dataframe objects\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# Investigate the NaN values in the data frames\ntrain_nulls = train_data.isnull().sum(axis=0)\ntest_nuls =  test_data.isnull().sum(axis=0)\n\n# Check only those columns which contain `null` data\nprint('Nulls in train data:\\n', train_nulls[train_nulls > 0], '\\n', sep='')\nprint('Nulls in test data:\\n', test_nuls[test_nuls > 0], '\\n', sep='')\n\n# Check the data for further assumptions\ntrain_data.head()","35eb8a55":"from sklearn.preprocessing import StandardScaler\n\nclass DataPreprocesser():\n    \"\"\"Data cleaner, transformer and normalizer.\"\"\"\n    \n    def __init__(self, train_data, test_data):\n        \"\"\"Initializes the preprocesser.\n        \n        Args:\n            train_data (pandas.DataFrame): The training data (without labels)\n            test_data (pandas.DataFrame):  The test data (without labels)\n        \"\"\"\n        # Save the datasets\n        self.train_data = train_data\n        self.test_data = test_data\n        \n        \n    def _clean(self, data):\n        \"\"\"Cleans and transforms the data.\n        \n        Args:\n            data (pandas.DataFrame): The data to be preprocessed\n        \n        Returns:\n            data (pandas.DataFrame): The preprocessed data\n        \"\"\"\n        # First, we drop columns which contain little to no useful info\n        data.drop(['PassengerId', 'Cabin', 'Name'], axis=1, inplace=True)\n        \n        # Second, we impute the missing values\n        data['Age'] = data['Age'].fillna(data['Age'].mean())\n        data['Fare'] = data['Fare'].fillna(data['Fare'].median())\n        data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n        \n        return data\n    \n    def _numericize(self, data):\n        \"\"\"Converts categorical to numeric values and normalizes.\n        \n        Args:\n            data (pandas.DataFrame): The data to be preprocessed\n        \n        Returns:\n            data (pandas.DataFrame): The preprocessed data\n        \"\"\"\n        # We transform the \"Ticket\" data to make it interpretable\n        data['Ticket'] = data['Ticket'].apply(lambda ticket: len(ticket))\n        \n        # Convert cat to num data\n        cat_cols = ['Sex', 'Embarked']\n        num_cols = pd.get_dummies(data[cat_cols], drop_first=True)\n        data = pd.concat([data.drop(cat_cols, axis=1), num_cols], axis=1)\n        \n        # Scale and shift the data (normalize)\n        data = StandardScaler().fit_transform(data)\n        \n        return data\n    \n    def get_prepared_data(self):\n        \"\"\"Returns the preprocessed data.\n        \n        This method preprocesses training and test data separately. This should not\n        happen in practice because we want to preprocess the test set based on\n        assumptions on the training set, however, we simplify things here.\n        \n        Returns:\n            A tuple of:\n                1. train_prep (pandas.DataFrame): The prepared training data\n                2. test_prep (pandas.DataFrame):  The prepared test data\n        \"\"\"\n        # The class should maintain unchanged data\n        train_raw = pd.DataFrame.copy(self.train_data)\n        test_raw = pd.DataFrame.copy(self.test_data)\n        \n        # Prepare each of the datasets separately\n        train_prep = self._numericize(self._clean(train_raw))\n        test_prep = self._numericize(self._clean(test_raw))\n        \n        return train_prep, test_prep","2b73d966":"from sklearn.model_selection import train_test_split\n\n# Create a data preprocesser and get the preprocessed datasets\npreprocesser = DataPreprocesser(train_data.drop(['Survived'], axis=1), test_data)\ntrain_prep, test_prep = preprocesser.get_prepared_data()\n\n# Check the final shape of the prepared data\nprint('Train data shape:', train_prep.shape)\nprint('Test data shape:', test_prep.shape, '\\n')\n\n# Define train data inputs and labels\nX = train_prep\ny = train_data['Survived']\n\n# Split the full training dataset to training and validation sets using magical numbers\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.69, random_state=69)\n\n# Check how many samples we have in the sets\nprint('Num train:', len(X_train))\nprint('Num val:', len(X_val))","ab8fd841":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.ensemble import VotingClassifier\n\ndef create_models(X, y):\n    \"\"\"Creates 3 distinct cassifiers.\n    \n    This method creates 3 models using *magical* parameters.\n    \n    Args:\n        X (ndarray): The input data of shape (N, D)\n        y (ndarray): The labels of shape (N,)\n        \n    Returns:\n        A tuple of tuples, each of the of the form ('name', model)\n    \"\"\"\n    knn = KNN(n_neighbors=69).fit(X, y)\n    svm = SVC(C=.69, gamma=.69).fit(X, y)\n    rfc = RFC(max_depth=69, random_state=69).fit(X, y)\n    \n    return ('knn', knn),  ('svm', svm), ('rfc', rfc)\n\n\ndef create_ensemble(X, y, models):\n    \"\"\"Creates unweighted ensemble.\n    \n    Args:\n        X (ndarray): The input data of shape (N, D)\n        y (ndarray): The labels of shape (N,)\n        models (tuple): The tuple of tuples, each of the of the form ('name', model)\n        \n    Returns:\n        A model ensemble which predicts based on the majority of votes\n    \"\"\"\n    return VotingClassifier(models).fit(X, y)","0f438eb2":"from sklearn.metrics import accuracy_score as acc\n\n# Check the individual models accuracies on the validation set\nmodels = create_models(X_train, y_train)\nprint(*(f'{name} acc: {acc(y_val, model.predict(X_val))}' for name, model in models), sep='\\n')\n\n# Check if ensemble accuracy on the validation set makes sense\nensemble = create_ensemble(X_train, y_train, models)\nprint(f'\\nensemble acc: {acc(y_val, ensemble.predict(X_val))}')","1db5d0f5":"# Train using full training data and make the final prediction\nensemble_full = create_ensemble(X, y, create_models(X, y))\npred_final = ensemble_full.predict(test_prep)\n\n# Save the submission file\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_final})\noutput.to_csv('ensemble_submission.csv', index=False)","0114a5bd":"# Submitting\nFor submission, we make a final prediction using _full_ training set and simply save the predicted results in the output (submission) file.","19ffc56d":"# End of the Notebook\n\n<sub>This notebook shows that this competition should not be taken seriously but that doesn't mean we can't have fun with it and push the limits of the algorithms using *magical* numbers:)<\/sub>","560ae375":"# Preparing for training\nWe use the defined **DataPreprocesser** class to clean and transform our features. We then split _training dataset_ to _train_ and _validation_ sets using *magical* numbers.","0bb826c8":"# Preprocessing the data\nTo preprocess the data, we need to make some assumptions - what categories impact the survival rate, what transformations should be made etc. How do we know that? We could create plots and examine the feature correlation, test different feature extractions etc but we can check others' notebooks with already generated flashy plots!\n\nHere we create a **DataPreprocesser** class which prepares both the _training_ and the _test_ data in the following way:\n1. **During cleaning**, `PassengerId`, `Name` and `Cabin` columns are all discarded because they don't contain a lot of interpretable information. `PassengerId` is just an ID - not a feature. `Name` is not really a feature, although it has titles from which could be seen that female titles are preferred more then male but we already have this trend in `Sex` column. `Cabin` has a lot of `NaN` values and it is hard to observe a clear relationship between survival rates.\n2. **During imputation**, we simply fill `NaN` values for `Age`, `Fare` and `Embarked` categories using _mean_, _median_ and _mode_ respectively.\n3. **During transformation**, for the `Ticket` column we convert the ticket name to the length of that name becasue tickets differ by length and may provide useful info, such as, the place on the ship, whether it's a group ticket, maybe it's a VIP ticket etc. `Sex` and `Embarked` columns are one-hot encoded and therefore converted to numerical values.\n4. **During normalization**, we remove the mean from the each dataset and scale the values by their unit variance.","eb42ebc0":"# Checking accuracy\nWe firstly check the accuracy on the validation set of each model individually and then we check the accuracy of the ensemble model. Bigger accuracy for the ensemble model indicates that this approach works and a large accuracy using a notably smaller training set than the validation set shows promising results.","bdd59993":"# Creating an ensemble\nWe first create `3` different estimators using *magical* hyperparameters (honestly, they work very well):\n1. **K-Nearest Neighbors** - predicts taking into account **K** most similar samples\n2. **C-Support Vector Machine** - classifies based on **hinge** loss function\n3. **Random Forests** - estimates using many decision trees\n\nWe then create an ensemble model, i.e., **Voting Classifier**, which predicts using the majority of votes.","29d518ae":"# Loading the data\nFirstly, we read the `csv` dataset files (_training_ and _test_) and check what data is missing. We also print the first few rows to see what features might be useful for the algorithms.","5d46d34b":"# Titanic - Machine Learning from Disaster\n## Overview\nThis notebook provides a solution for top ~`0.5%` score on the kaggle leaderboard for the **Titanic** competition. The final accuracy score is `0.81339` achieved with the help of an _ensemble_ model. This notebook is a legitimate proof that any hyperparameter that involves `69` is _magical_."}}