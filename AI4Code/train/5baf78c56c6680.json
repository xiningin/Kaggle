{"cell_type":{"ba6db65e":"code","17d9a514":"code","bd734ccd":"code","73b99d40":"code","5708008f":"code","8f0c92b7":"code","1445fe71":"code","a1bbb21d":"code","557b8036":"code","e72b0594":"code","b23fd6a7":"code","5ab91603":"code","aad0a1f8":"code","6e9b5e9c":"code","91d85469":"code","f7f6451c":"code","c672333f":"code","247efd9e":"code","6bd6c2a4":"code","2a0fe563":"code","338651f6":"code","4e79bbe6":"code","e4d81e9f":"code","947bb130":"code","0bb3597c":"code","2bf8257e":"code","cde16a16":"markdown","0956085d":"markdown","9a7bfe0d":"markdown","881bb29b":"markdown","67e7dd44":"markdown","fd68a176":"markdown","3752caa9":"markdown","8941ad0d":"markdown","53b53a54":"markdown","c9d49e2a":"markdown","f785cbee":"markdown","8cccb68a":"markdown","7f89a1fd":"markdown","fad07f48":"markdown","426a0e02":"markdown","e5cf7472":"markdown","0b4b4761":"markdown","6ba89495":"markdown","1e3ab8af":"markdown","8a79d2ae":"markdown","03c906c7":"markdown","0a045643":"markdown","3a2fc571":"markdown","2ed57031":"markdown","e8433fc2":"markdown"},"source":{"ba6db65e":"import numpy as np\nimport pandas as pd\nimport os\nimport copy\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwnl = WordNetLemmatizer()\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npd.set_option(\"display.max_colwidth\", 80)\nimport modeling_functions as mf\nimport nlp_preprocessing_functions as npf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier","17d9a514":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nprint('Train Set Shape = {}'.format(train.shape))\ntrain.head()","bd734ccd":"test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint('Test Set Shape = {}'.format(test.shape))\ntest.head()","73b99d40":"submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission.head()","5708008f":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(20, 5))\n\ntrain_sums = train[missing_cols].isnull().sum()\ntest_sums = test[missing_cols].isnull().sum()\nsns.barplot(x=train_sums.index, y=train_sums.values, ax=axes[0])\nsns.barplot(x=test_sums.index, y=test_sums.values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].set_title('Train Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\naxes[0].tick_params(axis='x', labelsize=12)\naxes[0].tick_params(axis='y', labelsize=12)\naxes[1].tick_params(axis='x', labelsize=12)\naxes[1].tick_params(axis='y', labelsize=12)\n\nplt.show()\n\ntrain_dropna, test_dropna = copy.copy(train), copy.copy(test)\nfor df in [train_dropna, test_dropna]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","8f0c92b7":"print(f'Unique Values in location column(train set) : {train_dropna[\"location\"].nunique()} \/ {len(train_dropna[\"location\"])}')\nprint(f'Unique Values in location column(test set) : {test_dropna[\"location\"].nunique()} \/ {len(test_dropna[\"location\"])}')","1445fe71":"print(f'Unique Values in keyword column(train set) : {train_dropna[\"keyword\"].nunique()} \/ {len(train_dropna[\"keyword\"])}')\nprint(f'Unique Values in keyword column(test set) : {test_dropna[\"keyword\"].nunique()} \/ {len(test_dropna[\"keyword\"])}')","a1bbb21d":"train_preprocessed, test_preprocessed = copy.copy(train_dropna), copy.copy(test_dropna)\ntrain_preprocessed.drop(['location'], axis=1, inplace=True)\nle = LabelEncoder()\nle.fit(train_preprocessed['keyword'])\ntrain_preprocessed['keyword'] = le.transform(train_preprocessed['keyword'])\ntrain_preprocessed.head()","557b8036":"test_preprocessed.drop(['location'], axis=1, inplace=True)\ntest_preprocessed['keyword'] = le.transform(test_preprocessed['keyword'])\ntest_preprocessed.head()","e72b0594":"train_preprocessed.drop(['id'], axis=1, inplace=True)\ntrain_preprocessed.head()","b23fd6a7":"test_preprocessed.drop(['id'], axis=1, inplace=True)\ntest_preprocessed.head()","5ab91603":"sentences_raw_train = train_preprocessed['text']\nsentences_raw_test = test_preprocessed['text']\nsentences_preprocessed_train = []\nsentences_preprocessed_test = []\nfor sentence in sentences_raw_train:\n    lemmas = npf.tokenizer(sentence)\n    sentence_without_stop_words = npf.remove_stop_words(lemmas, st_list=['amp','ca','ha','http http','new','rt','wa'])\n    sentences_preprocessed_train.append(sentence_without_stop_words)\nsentences_preprocessed_train = [\" \".join(doc) for doc in sentences_preprocessed_train]\nfor sentence in sentences_raw_test:\n    lemmas = npf.tokenizer(sentence)\n    sentence_without_stop_words = npf.remove_stop_words(lemmas, st_list=['amp','ca','ha','http http','new','rt','wa'])\n    sentences_preprocessed_test.append(sentence_without_stop_words)\nsentences_preprocessed_test = [\" \".join(doc) for doc in sentences_preprocessed_test]\nsentences_tfidf = copy.copy(sentences_preprocessed_train)\nsentences_tfidf.extend(sentences_preprocessed_test)","aad0a1f8":"tfidf_train, tfidf_test = npf.tfidf_features(docs_tfidf=sentences_tfidf, docs_train=sentences_preprocessed_train, docs_test=sentences_preprocessed_test, _max_features=1000)\ntfidf_train.head()","6e9b5e9c":"tfidf_test.head()","91d85469":"train_preprocessed = pd.concat([train_preprocessed, tfidf_train], axis=1)\ntrain_preprocessed.drop(['text'], axis=1, inplace=True)\ntrain_preprocessed.head()","f7f6451c":"test_preprocessed = pd.concat([test_preprocessed, tfidf_test], axis=1)\ntest_preprocessed.drop(['text'], axis=1, inplace=True)\ntest_preprocessed.head()","c672333f":"meta_feature_train = pd.DataFrame(columns=[\"word_count\", \"unique_word_count\", \"mean_word_count\", \"punctuation_count\", \"news_word_count\", \"disaster_word_count\"])\nmeta_feature_test = pd.DataFrame(columns=[\"word_count\", \"unique_word_count\", \"mean_word_count\", \"punctuation_count\", \"news_word_count\", \"disaster_word_count\"])\n\ntokenized_train =  sentences_raw_train.apply(lambda x: npf.tokenizer(x))\ntokenized_test =  sentences_raw_test.apply(lambda x: npf.tokenizer(x))\n\n#word_count\nmeta_feature_train[\"word_count\"] = tokenized_train.apply(lambda x: len(x))\nmeta_feature_test[\"word_count\"] = tokenized_test.apply(lambda x: len(x))\n\n#unique_word_count\nmeta_feature_train[\"unique_word_count\"] = tokenized_train.apply(lambda x: len(set(x)))\nmeta_feature_test[\"unique_word_count\"] = tokenized_test.apply(lambda x: len(set(x)))\n\n#mean_word_count\nmeta_feature_train[\"mean_word_count\"] = tokenized_train.apply(lambda x: np.mean([len(i) for i in x]))\nmeta_feature_test[\"mean_word_count\"] = tokenized_test.apply(lambda x: np.mean([len(i) for i in x]))\n\n#punctuation_count\nmeta_feature_train[\"punctuation_count\"] = sentences_raw_train.apply(lambda x: len([c for c in x if c in string.punctuation]))\nmeta_feature_test[\"punctuation_count\"] = sentences_raw_test.apply(lambda x: len([c for c in x if c in string.punctuation]))\n\n#news_word_count\nnews_related_word = [\"news\", \"report\",\"pm\", \"am\", \"utc\", \"breaking\", \"bbc\", \"abc\", \"fox\", \"gov\", \"government\", \"whitehouse\", \"huff\", \"journal\", \"cbc\", \"cbs\", \"official\", \"officer\", \"cnn\", \"yorker\", \"yahoo\", \"tv\", \"radio\"]\nmeta_feature_train[\"news_word_count\"] = tokenized_train.apply(lambda x: len([w for w in x if w in news_related_word]))\nmeta_feature_test[\"news_word_count\"] = tokenized_test.apply(lambda x: len([w for w in x if w in news_related_word]))\n\n#disaster_word_count\ndisaster_related_word = [\"disaster\", \"accudent\", \"kill\", \"killed\", \"killing\", \"died\", \"earthquake\", \"death\", \"bomb\", \"bombed\", \"bombing\", \"flood\", \"fire\", \"wildfire\", \"burn\", \"burning\", \"crash\", \"victims\", \"war\", \"weapons\", \"military\", \"force\", \"forces\", \"survive\", \"survived\", \"blood\"]\nmeta_feature_train[\"disaster_word_count\"] = tokenized_train.apply(lambda x: len([w for w in x if w in disaster_related_word]))\nmeta_feature_test[\"disaster_word_count\"] = tokenized_test.apply(lambda x: len([w for w in x if w in disaster_related_word]))\n\ntrain_preprocessed = pd.concat([train_preprocessed, meta_feature_train], axis=1)\ntrain_preprocessed.head()","247efd9e":"test_preprocessed = pd.concat([test_preprocessed, meta_feature_test], axis=1)\ntest_preprocessed.head()","6bd6c2a4":"y_train = train_preprocessed['target']\nx_train = train_preprocessed.drop(['target'], axis=1)\nx_test = test_preprocessed","2a0fe563":"target_model_set = {}\n\nparam_list_rf_1 = {\n    \"n_estimators\" : [\"int\",[170,200]],\n    \"max_depth\" : [\"int\",[6,8]],\n    \"random_state\" : 1234\n}\nmodel_rf_1 = RandomForestClassifier()\ntarget_model_set[\"random_forest_1\"] = [param_list_rf_1, model_rf_1]\n\nparam_list_knn_1 = {\n    \"n_neighbors\" : [\"int\",[5,20]],\n    \"weights\" : \"distance\",\n    \"p\" : [\"int\",[1,2]],\n    \"algorithm\" : \"auto\"\n}\nmodel_knn_1 =  KNeighborsClassifier()\ntarget_model_set[\"knn_1\"] = [param_list_knn_1, model_knn_1]\n\nparam_list_rc_1 = {\n    \"alpha\" : [\"discrete_uniform\",[0.1,1.0,0.01]],\n    \"random_state\" : 1234\n}\nmodel_rc_1 =  RidgeClassifier()\ntarget_model_set[\"rc_1\"] = [param_list_rc_1, model_rc_1]","338651f6":"#params_for_stacking, parameter_search_results = mf.parameter_search(target_model_set=target_model_set, kf_num=5, trial_num=10, x_train=x_train, y_train=y_train, x_test=x_test)\nparams_for_stacking, parameter_search_results = mf.parameter_search(target_model_set=target_model_set, kf_num=5, trial_num=2, x_train=x_train, y_train=y_train, x_test=x_test)","4e79bbe6":"for model_name, results in parameter_search_results.items():\n    validation_result = results[1]\n    print(model_name + \" => \" + validation_result)","e4d81e9f":"# params_for_stacking.pop(\"lc_1\")\nparams_for_stacking.keys()","947bb130":"stacking_result = mf.stacking_function(x_train=x_train, y_train=y_train, x_test=x_test, params_for_stacking=params_for_stacking, kf_num=5)","0bb3597c":"result_for_submission = [int(round(result)) for result in stacking_result]\nsubmission['target'] = result_for_submission\nsubmission.head()","2bf8257e":"submission.to_csv(\"submission.csv\", index=False)","cde16a16":"### Hyperprameter Search","0956085d":"## EDA","9a7bfe0d":"### TF-IDF","881bb29b":"### keyword \/ location column","67e7dd44":"### Meta Features\nthis idea is from [this notebook](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-read-before-start-eda)","fd68a176":"- Compared to location column, there are less unique values(and nulls) in keyword column, so keyword column is more reliable and may have relationship or rules with target column.\n- As described above, the column can be <u>used as a feature by itself<\/u>.","3752caa9":"## Table of Contents\n1. **[Library Load](#Library-Load)**\n2. **[Data Load](#Data-Load)**\n3. **[EDA](#EDA)**\n4. **[Preprocessing](#Preprocessing)**\n5. **[Feature Extraction](#Feature-Extraction)**\n6. **[Modeling](#Modeling)**\n7. **[Submission](#Submission)**","8941ad0d":"## Library Load","53b53a54":"### Processing keyword and location","c9d49e2a":"- Checking sentences from the text column, I found out that disaster tweets has longer texts and the style of texts are more formal than non-disaster tweeets.\n- I think this is because disaster tweets tend to be written by news agencies and non-disaster tweets tend to be written by individual twitter users.\n  \nSo, the meta features I extracted is ...\n- word_count\n- unique_word_count\n- mean_word_count\n- punctuation_count\n\nAnd I added news related features.\n- news_word_count\n- disaster_word_count","f785cbee":"## Submission","8cccb68a":"### Handling Missing Values","7f89a1fd":"### Stacking","fad07f48":"## Data Load","426a0e02":"- Since there are many unique values(and nulls) in the location column, it can be said that location value is not generated automatically and may be human-inputs.\n- As described above, the column is dirty and <u>shouldn't be used as a feature<\/u>.","e5cf7472":"- Since id column is not related with modeling, drop the column","0b4b4761":"### id column","6ba89495":"- load util functions for preprocessing.  \n    - [NLP Preprocessing Functions](https:\/\/www.kaggle.com\/koheimuramatsu\/nlp-preprocessing-functions)","1e3ab8af":"- drop location column.\n- encode keyword column into labeled feature.","8a79d2ae":"## Feature Extraction","03c906c7":"- select the high score model for stacking","0a045643":"- check out validation result for each model","3a2fc571":"According to `Data Description`, there are many missing values in the keyward\/location column.\n- there are so many missing values in 'location' columnn than 'keyword' column.\n- the ratios of missing values in train\/test set are so close.\n- the missing values are filled with 'no_keyword' and 'no_location' respectively.","2ed57031":"## Modeling","e8433fc2":"## Preprocessing"}}