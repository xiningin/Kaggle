{"cell_type":{"29050f91":"code","1fbced03":"code","ff814422":"code","66c9c3a5":"code","096d0cc8":"code","f2302ef9":"code","7da9110f":"code","818723d2":"code","3ec0beaf":"code","74107ac6":"code","f77691c9":"code","f8742a10":"code","cb7cb852":"code","c03e3f4a":"code","c2e37efc":"code","40b82d15":"code","e48fe04d":"code","98b3c2ab":"code","808cc6d8":"code","d06718bd":"code","2b61a930":"code","a1cce4a8":"code","07075a36":"code","ecd5a948":"code","2b0ca36b":"code","7229be74":"code","961430a4":"markdown","2a0a3275":"markdown","30db1c34":"markdown","bdaff2d4":"markdown","00fcccf3":"markdown","b45b2aa8":"markdown","d12a9bfa":"markdown","227ff680":"markdown","1d635796":"markdown","e2bd8349":"markdown","4fde99be":"markdown","527f21f2":"markdown","4a9b7e52":"markdown","d6db63ff":"markdown","a00b0c4e":"markdown","2761f4e5":"markdown","e90c26e1":"markdown","3e13b5c6":"markdown","a4a642c0":"markdown","2f3183b0":"markdown"},"source":{"29050f91":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color:#eebbcb; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n\ndiv.h2 {\n    background-color:#2ca9e1; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n<\/style>","1fbced03":"import itertools\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom sklearn import preprocessing","ff814422":"!ls ..\/input\/top-personality-dataset","66c9c3a5":"df_personality = pd.read_csv(\"..\/input\/top-personality-dataset\/2018-personality-data.csv\")\ndf_ratings = pd.read_csv(\"..\/input\/top-personality-dataset\/2018_ratings.csv\")","096d0cc8":"#Rename because some column name includes space.\nrename_dict = {' openness': 'openness', ' agreeableness': 'agreeableness', ' emotional_stability': 'emotional_stability',\n               ' conscientiousness': 'conscientiousness', ' extraversion': 'extraversion', ' assigned metric': 'assigned metric',\n               ' assigned condition': 'assigned condition', ' is_personalized': 'is_personalized', ' enjoy_watching ': 'enjoy_watching'}\ndf_personality = df_personality.rename(columns=rename_dict)","f2302ef9":"df_personality.head()","7da9110f":"df_personality.dtypes","818723d2":"df_personality.describe()","3ec0beaf":"assigned_metric_le = preprocessing.LabelEncoder()\nassigned_metric_le.fit(df_personality[\"assigned metric\"])\ndf_personality[\"assigned metric\"] = assigned_metric_le.transform(df_personality[\"assigned metric\"])\n\nassigned_condition_le = preprocessing.LabelEncoder()\nassigned_condition_le.fit(df_personality[\"assigned condition\"])\ndf_personality[\"assigned condition\"] = assigned_condition_le.transform(df_personality[\"assigned condition\"])","74107ac6":"df_personality","f77691c9":"cols_predicted_rating = [' predicted_rating_1',' predicted_rating_2',' predicted_rating_3',\n ' predicted_rating_4',  ' predicted_rating_5', ' predicted_rating_6', ' predicted_rating_7', \n ' predicted_rating_8', ' predicted_rating_9', ' predicted_rating_10', ' predicted_rating_11',' predicted_rating_12']\n\ncols_movie = [' movie_1',' movie_2',' movie_3',' movie_4',  ' movie_5', ' movie_6', ' movie_7', \n ' movie_8', ' movie_9', ' movie_10', ' movie_11',' movie_12']","f8742a10":"df_personality[cols_predicted_rating]","cb7cb852":"df_personality[\"movie_choice\"] = np.ndarray.argmax(df_personality[cols_predicted_rating].values, axis = 1)","c03e3f4a":"df_personality = df_personality.drop(columns = (cols_predicted_rating + cols_movie))","c2e37efc":"sns.countplot(data=df_personality,x=\"movie_choice\")","40b82d15":"from scipy.spatial import KDTree\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter","e48fe04d":"class Knn:\n    def __init__(self, k = 3):\n        self.N = 0\n        self.N_k = None\n        self.train_kdtree = None\n        self.train_labels = None\n        self.K = k\n\n    \n    def fit(self, X, y):\n        self.dim = len(X[0])\n        self.N = len(X)\n        self.train_kdtree = KDTree(X)\n        self.train_labels = y\n        self.N_k = Counter(y)\n\n    \n    def predict(self, x):\n        return np.array([self.predict_each_point(xi) for xi in x])\n    \n            \n    def predict_each_point(self, x):\n        _, idxs = self.train_kdtree.query(x, self.K) \n        c_K = Counter(self.train_labels[idxs])\n        #most_common([n]) returns like [(\"key\", value), ...].\n        return c_K.most_common(1)[0][0] \n    \n    \n    def predict_proba(self, x):\n        return np.array([self.predict_each_proba(xi) for xi in x])\n    \n    \n    def predict_each_proba(self, x):\n        _, idxs = self.train_kdtree.query(x, self.K)\n        c_K = Counter(self.train_labels[idxs])\n        p_Ck_x = {k: c_K[k]\/self.K for k in self.N_k.keys()}\n        p_Ck_x = [p_Ck_x[idx] for idx in sorted(p_Ck_x.keys())]\n        \n        return p_Ck_x","98b3c2ab":"X_train, X_test, y_train, y_test = train_test_split(df_personality[[col for col in df_personality.columns if col not in [\"userid\", \"movie_choice\"]]], \n                                                    df_personality[\"movie_choice\"], test_size=0.33, random_state=42)","808cc6d8":"knn = Knn(30)\n\nknn.fit(X_train.values, y_train.values)\n\npredicts = knn.predict(X_test.values[0:10])\npredict_probas = knn.predict_proba(X_test.values[0:10])\n\npredicts","d06718bd":"from sklearn.neighbors import KNeighborsClassifier","2b61a930":"neigh = KNeighborsClassifier(n_neighbors=30, algorithm='kd_tree')\n\nneigh.fit(X_train.values, y_train.values)\n\npredicts_val = neigh.predict(X_test.values[0:10])\npredict_probas_val = neigh.predict_proba(X_test.values[0:10])\n\npredicts_val","a1cce4a8":"print(\"My KNN model\")\nprint(predict_probas[2])\nprint(\"-------------------\")\nprint(\"Sklean KNN model\")\nprint(predict_probas_val[2])","07075a36":"print(\"My KNN model\")\nprint(predict_probas[9])\nprint(\"-------------------\")\nprint(\"Sklean KNN model\")\nprint(predict_probas_val[9])","ecd5a948":"import time","2b0ca36b":"start = time.time()\n\nk = 30\n\nknn = Knn(k)\n\nknn.fit(X_train.values, y_train.values)\n\npredicts = knn.predict(X_test.values)\npredict_probas = knn.predict_proba(X_test.values)\n\ntaken_time = time.time() - start\n\nprint(f\"Our KNN model takes {taken_time} seconds with k = {k}\")\nprint(f\"Shape of train data was {X_train.values.shape}\")\nprint(f\"Shape of test data was {X_test.values.shape}\")","7229be74":"start = time.time()\n\nk = 50\n\nknn = Knn(k)\n\nknn.fit(X_train.values, y_train.values)\n\npredicts = knn.predict(X_test.values)\npredict_probas = knn.predict_proba(X_test.values)\n\ntaken_time = time.time() - start\n\nprint(f\"Our KNN model takes {taken_time} seconds with k = {k}\")\nprint(f\"Shape of train data was {X_train.values.shape}\")\nprint(f\"Shape of test data was {X_test.values.shape}\")","961430a4":"<div class=\"h1\">About this notebook<\/div>\n\n![KnnClassification](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e7\/KnnClassification.svg\/220px-KnnClassification.svg.png)\n\nIn this notebook, we will gain a better understanding and insight of the KNN by implementing the method ourselves.\n\nWe can access the the dataset that contains user personality data and their movie preferences in this kaggle dataset.\n\nFirst, we view the dataset and create target feature.\n\nSecond, we check theory of KNN and implement it with KDTree. We also check it's validity.\n\nThird, we estimate target feature with our model.","2a0a3275":"### Data overview","30db1c34":"# <div class=\"h2\">Understand and implement k-nearest neighbor<\/div>","bdaff2d4":"I'll implement my k-nearest neighbor model for Knn class.\n\nThis class has following method.\n\n- **__init__**: Constructor. Especially, we can set k value here.\n\n- **fit**: Training. In fact, we create KDTree instance which include training data. And also we input other nessecery data to model.\n\n- **predict**: Predict labels for input vectors. To accommodate multiple vectors, we call following predict_each_point fuction here.\n\n- **predict_each_point**: Predict label for input vector. We query k nearest neighbor points from input vector using KDTree. And return most common label in the nearest neighbors.\n\n- **predict_proba**: Predict probabilities of labels for input vectors. To accommodate multiple vectors, we call following predict_each_proba fuction here.\n\n- **predict_each_proba**: Predict probability of labels for input vector. We query k nearest neighbor points from input vector using KDTree. And count the number of points per label. The number of points divided by the total number of points in the local space is returned as the probability.","00fcccf3":"Let's predict movie categories by our model! I create knn instance with k=30.\n\nTest data is too big for demo, I use only first 10 samples.","b45b2aa8":"## If you like, please Upvote\ud83d\ude39","d12a9bfa":"### Label encording\n\nThere are 2 categorical columns, I'll encode them with labelencording.","227ff680":"### Load library and dataset","1d635796":"### implementation\n\nI will implement the model according to the theory described above. If the points we want to estimate are given , we need to get the k neighboring points. In this implementation, I will use KDTree to implement this.\n\nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.spatial.KDTree.html\n\nUsing scipy.spatial.KDTree.query API, we can get k neighboring points easily. Of cource, you can implement bruteforce. For the sake of speed and simplicity, we will use this API.","e2bd8349":"The second and 10th estimates are interchanged. I think that this is because KDTree's implementation... If you know the reason, please tell me!","4fde99be":"# <div class=\"h2\">Data overview and create target feature<\/div>","527f21f2":"OK, I completed my model.\n\nNext, I'll split data to train and test.","4a9b7e52":"Here, we get \"movie_choice\" column. I'll check it's distribution.","d6db63ff":"# Reference\n\n1. wikipedia (refered top picture from here.)\n\n2. Pattern Recognition and Machine Learning ( Japanese Edition )\n\n3. Scipy doc. Especially, https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.spatial.KDTree.html","a00b0c4e":"### Validation of model\n\nI completed implementation of my k-nearest neighbor model. I will check its validity with KNeighborsClassifier of sklean. ","2761f4e5":"## theory \n\nk-nearest neighbors is one of the nonparametric method. With parametric method which assume distribution, there is a strong limitation that the data you want to analyze must be suitable for the assumed distribution. For example, we can't analyze multimodal distribution with single gauss distribution. We have to use dimore complex models like Gaussian mixture model. By nonparametric method, we can analyze such data with fewer assumptions.\n\nNote, I refer following discussion from reference [2] (check notebook's bottom).\n\n----------------\n\nFirst, we consider area R. R is very small and it's probability *P* is:\n\n$$\n   P = \\int_R p(x)dx\n$$\n\nIf there are N samples, the probability which K sample of them are in R is following:\n\n$$\n   Bin(K | N,P) = \\frac{N!}{K!(N-K)!} P^K (1-P)^{N-K}\n$$\n\nSince kth is either in or out of the region R, it's probabirity follows binomial distribution.\n\nWith binomial distribution, avarage and variance are:\n\n$$\n   E[K\/N] = P\n$$\n\n$$\n   var[K\/N] = P(1-P)\/N\n$$\n\nIf we assume N is so big. Then var[K\/N] approachs zero. So we can guess\u3000E[K\/N] = P.\n\nNow, we can write K in this small R,\n\n$$\n  K = NP\n$$\n\nIf we assume R is so small such that p(x) is same all over the R,\n\n$$\n  P = p(x) V\n$$\n\nV is the volume of R.\n\nFrom last two equation, we get\n\n$$\n  p(x) = \\frac{K}{NV}\n$$\n\n---------------------------\n\nNow, we get expression\u3000of p(x) by K, N and V. In KNN, we constrain K and change V. In other words, we give K when we create model instance and search for the nearest K points. And we consider as up to the Kth most distant point is included in this region R.\n\n---------------------------\n\nFor classification, we want to know p(C_k|x). By Bayes' theorem,\n\n$$\n  p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{p(x)}\n$$\n\nSince above discussion, we can guess probability that given vector is kth class,\n\n$$\n  p(x|C_k) = \\frac{K_k}{N_kV}\n$$\n\nand p(C_k) is simply,\n\n$$\n  p(C_k) = \\frac{N_k}{N}\n$$\n\nSo finally we get,\n\n$$\n  p(C_k|x) = \\frac{N_k}{N}\n$$\n\n---------------------------\n\nOkay, we get very simple p(C_k|x) representation. Using this, we can get probability that given vector is kth class!","e90c26e1":"You can see that the estimates are quite different.\ud83d\ude05\n\nFor example, 3rd sample is estimated 10 by my knn model, but done 2 by sklean.\n\nYou can see this reason by checking proba.","3e13b5c6":"### Create target feature\n\nIn this notebook, I'd like to estimate which users rate movies in which categories the most.\n\nTo do this, I'll create \"movie_choice\" column.","a4a642c0":"# <div class=\"h2\">Inference with our model<\/div>\n\nFinally, let's estimate test data with our model.\n\nHowever, it's roughly the same as the previous demo, so we'll also measure execution speed.","2f3183b0":"As you can see, the probabilities are the same. When the probabilities are the same, there seems to be a difference in which label choose.\n\nBut 10th sample seemsto have different reason."}}