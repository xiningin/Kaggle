{"cell_type":{"b9251162":"code","04511693":"code","8d8f8ce7":"code","3ef8a22f":"code","7f4cc18c":"code","ccde24c2":"code","e288c132":"code","cd6c9a68":"code","35fc61f1":"code","486cf22d":"code","2e252b3f":"code","ef6d9ac4":"code","3483db2a":"code","05152c42":"code","d2c7eff2":"code","94712593":"code","3accc20b":"code","09de0280":"code","d4fd0e76":"code","beccc57b":"code","a4914cff":"code","666b60da":"code","d5b91ced":"code","faf6ec05":"code","b858d3f2":"code","0a9fd8b1":"code","75fdf366":"code","3f2fe16d":"code","73aa0790":"code","d49834fa":"code","fb198bc5":"code","1afeb2e2":"code","f94d4936":"code","aa625bf1":"code","5ff20967":"code","6f2fab37":"code","686beefc":"code","b4b6f4e1":"code","ec3c3611":"code","ca3704d0":"code","f5719b99":"code","d9b58aaf":"code","1203f0e7":"code","601ccbf5":"code","137cfbdf":"code","1d803110":"code","9760b367":"code","19ac6eb2":"code","19572e9d":"code","10dd68c6":"code","1fe82dc0":"code","947933bb":"code","1a1026e7":"code","a9f7c8d9":"code","b03b5b1f":"code","c4c11576":"code","f1a39d5e":"code","40298652":"code","72b778b1":"code","7be60c32":"code","f113ce99":"code","f3f27995":"code","3cbc46ca":"code","a7a54a09":"code","251a3bb8":"code","8bf64c18":"code","d4d73d2e":"code","2934aa8d":"code","5018d7dd":"code","19dcc143":"markdown","cfb5d1a9":"markdown","f2d5875f":"markdown","b2e2796e":"markdown","6b025d66":"markdown","81aae73a":"markdown","4f1c3075":"markdown","7f14ca47":"markdown","e72eebc2":"markdown","829658c9":"markdown","48a9c931":"markdown","674b167f":"markdown","6ae5c8e8":"markdown","108ee781":"markdown","c7ae47d9":"markdown","66b371e1":"markdown"},"source":{"b9251162":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04511693":"## First of all, import the train & test data set.\ntrain_df=pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntrain_df2=pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\ntrain_df.info()","8d8f8ce7":"test_df.info()","3ef8a22f":"#EDA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef nom_plot(nom_col):\n    nom_data=train_df[nom_col]\n    nom_data_counts=nom_data.value_counts()\n    plt.title(nom_col)\n    plt.bar(nom_data_counts.index,nom_data_counts)\n    print('{}:\\n{}'.format(nom_col,nom_data_counts))\n    print('{}:\\n{}'.format(nom_col,nom_data_counts\/19158))\n    plt.show()","7f4cc18c":"nom=['target','city','gender','relevent_experience','enrolled_university','education_level','major_discipline','experience',\n    'company_size','company_type','last_new_job']\nfor i in nom:\n    nom_plot(i)","ccde24c2":"def num_plot(nom_col):\n    sns.boxplot(data=train_df,x=nom_col)\n    plt.title(nom_col)\n    plt.show()\nnum=['city_development_index','training_hours']\nfor i in num:\n    num_plot(i)","e288c132":"def nom_compare(nom_col):\n    pd.crosstab(train_df[nom_col],train_df['target']).plot(kind='barh')\n    plt.title(nom_col)\n    plt.show()\nfor i in nom:\n    nom_compare(i)","cd6c9a68":"def num_compare(num_col):\n    sns.boxplot(data=train_df,x=train_df['target'],y=train_df[num_col])\n    plt.title(num_col)\n    plt.show()\nfor i in num:\n    num_compare(i)","35fc61f1":"gender_map={\n    'Male':0,\n    'Female':1,\n    'Other':2\n}\nrelevent_experience_map={\n    'No relevent experience':0,\n    'Has relevent experience':1\n}\nenrolled_university_map={\n    'no_enrollment':0,\n    'Part time course':1,\n    'Full time course':2\n}\neducation_level_map={\n    'Primary School':0,\n    'High School':1,\n    'Graduate':2,\n    'Masters':3,\n    'Phd':4\n}\nmajor_discipline_map={\n    'STEM':0,\n    'Other':1,\n    'No Major':2,\n    'Humanities':3,\n    'Business Degree':4,\n    'Arts':5\n}\n\ncompany_type_map={\n    'Pvt Ltd':0,\n    'Public Sector':1,\n    'Other':2,\n    'NGO':3,\n    'Funded Startup':4,\n    'Early Stage Startup':5\n}\ncompany_size_map={\n    '<10':0,\n    '10\/49':1,\n    '50-99':2,\n    '100-500':3,\n    '500-999':4,\n    '1000-4999':5,\n    '5000-9999':6,\n    '10000+':7\n}\nlast_new_job_map={\n    'never':0,\n    '1':1,\n    '2':2,\n    '3':3,\n    '4':4,\n    '>4':5\n}\n\nexperience_map={\n    '<1':0,\n    '1':1,\n    '2':2,\n    '3':3,\n    '4':4,\n    '5':5,\n    '6':6,\n    '7':7,\n    '8':8,\n    '9':9,\n    '10':10,\n    '11':11,\n    '12':12,\n    '13':13,\n    '14':14,\n    '15':15,\n    '16':16,\n    '17':17,\n    '18':18,\n    '19':19,\n    '20':20,\n    '>20':21\n}\n\n\ntrain_df['gender']=train_df['gender'].map(gender_map)\ntrain_df['education_level']=train_df['education_level'].map(education_level_map)\ntrain_df['company_size']=train_df['company_size'].map(company_size_map)\ntrain_df['last_new_job']=train_df['last_new_job'].map(last_new_job_map)\ntrain_df['experience']=train_df['experience'].map(experience_map)\ntrain_df['relevent_experience']=train_df['relevent_experience'].map(relevent_experience_map)\ntrain_df['enrolled_university']=train_df['enrolled_university'].map(enrolled_university_map)\ntrain_df['major_discipline']=train_df['major_discipline'].map(major_discipline_map)\ntrain_df['company_type']=train_df['company_type'].map(company_type_map)\n","486cf22d":"# Because I use tree based algorithm, so don't need to do one-hot encoding\n# This is the way when you want to do one-hot encoding\n\n\n#train_df=train_df.drop(['city'],axis=1)\n#columns=train_df.select_dtypes(include=[object]).columns\n#train_df=pd.concat([train_df,pd.get_dummies(train_df[columns])],axis=1)\n#train_df=train_df.drop(['gender','relevent_experience','enrolled_university','major_discipline','company_type'],axis=1)\n#train_df['new_target']=train_df['target']\n#train_df=train_df.drop(['target'],axis=1)\n#train_df","2e252b3f":"train_df","ef6d9ac4":"num_2=['city_development_index','training_hours','education_level','experience','company_size','last_new_job']\nfor i in num_2:\n    num_compare(i)","3483db2a":"def num_compare2(num_col):\n    sns.kdeplot(data=train_df,x=train_df[num_col],hue=train_df['target'])\n    plt.title(num_col)\n    plt.show()\nfor i in num_2:\n    num_compare2(i)","05152c42":"#Using violinplot can more clearly show up the median(white dot).\ndef num_compare3(num_col):\n    sns.violinplot(data=train_df,y=train_df[num_col],x=train_df['target'])\n    plt.title(num_col)\n    plt.show()\nfor i in num_2:\n    num_compare3(i)","d2c7eff2":"fig,ax=plt.subplots(figsize=(13,13))\nsns.heatmap(train_df.corr(),annot=True,ax=ax)\nplt.show()","94712593":"## First, I want to do label encoding.\n## In order to maintain consistency, I combined the training and testing data sets.\n## So that's why I give the target columns to the testing data set.\n\ntest_df['target'] = -1 ## Set -1 as the target, and when we finish merging and label encoding, I will drop it out.\ntest_df","3accc20b":"##Combine these two DataFrame\ndata_df=pd.concat([train_df2,test_df],axis=0).reset_index(drop=True)\ndata_df","09de0280":"data_df.info()","d4fd0e76":"## So now let's doing label encoding\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_df['gender']=data_df['gender'].map(gender_map)\ndata_df['education_level']=data_df['education_level'].map(education_level_map)\ndata_df['company_size']=data_df['company_size'].map(company_size_map)\ndata_df['last_new_job']=data_df['last_new_job'].map(last_new_job_map)\ndata_df['experience']=data_df['experience'].map(experience_map)\ndata_df['relevent_experience']=data_df['relevent_experience'].map(relevent_experience_map)\ndata_df['enrolled_university']=data_df['enrolled_university'].map(enrolled_university_map)\ndata_df['major_discipline']=data_df['major_discipline'].map(major_discipline_map)\ndata_df['company_type']=data_df['company_type'].map(company_type_map)\n\n## The reason for merging these two dataframes is all because the feature of \"city\" is too disorderly.\n## Therefore, here I combine these two dataframes to maintain the consistency of the label encoding.\ncity_encoder=LabelEncoder()\ndata_df.loc[:,'city']=city_encoder.fit_transform(data_df.loc[:,'city'])\ndata_df","beccc57b":"## Is done! So now I separate to origin training and testing data set.\ntrain_dataset=data_df[data_df['target']!=-1].reset_index(drop=True) \ntest_dataset=data_df[data_df['target']==-1].reset_index(drop=True) ## That's why I give testing data set the fake target column.\ntest_dataset=test_dataset.drop(['target'],axis=1) ## Don't forget to drop it.\ntrain_dataset","a4914cff":"test_dataset","666b60da":"print(train_df.isnull().sum()\/19158)","d5b91ced":"## This is how we are going to use KNN to handle missing values.\n## I'm not going to use this because of few reasons.\n## First, it's hard and takes too much time to find the best k-value despite its can be found by cross-validation.\n## Secondly, the result does not very well for me.\n\n#from sklearn.impute import KNNImputer\n#knn_imputer=KNNImputer(n_neighbors=3)\n#x=np.round(knn_imputer.fit_transform(train_dataset))\n#data_df=pd.DataFrame(x,columns=train_dataset.columns)","faf6ec05":"## This is how I deal with missing values.\n\ntrain_dataset['gender'].fillna(value=-1,inplace=True)\ntrain_dataset['major_discipline'].fillna(value=-1,inplace=True)\ntrain_dataset['company_size'].fillna(value=-1,inplace=True) \ntrain_dataset['company_type'].fillna(value=-1,inplace=True)\ntrain_dataset.dropna(subset=['enrolled_university','education_level','experience','last_new_job'],axis=0,inplace=True)\ntrain_dataset.info()","b858d3f2":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import f_classif,SelectPercentile\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.pipeline import pipeline,make_pipeline\n\n# We can use Anova, Chi-square test...etc methods to do feature selection.\n##---------Anova--------------#\n#x=train_dataset.iloc[:,1:13]\n#y=train_dataset.iloc[:,13]\n#feature=SelectPercentile(f_classif,percentile=70).fit(x,y)\n#cols=feature.get_support(indices=True)\n#new_feature=x.iloc[:,cols]\n#new_feature.keys()\n##------------------------#\n\n#Also that we can use Random Forest or XGBoost to find out the important feature.\n## Here I use Random Forest \n\nx=train_dataset.iloc[:,1:13]\ny=train_dataset.iloc[:,13]\n\n# Because of imblanced data, so I'm resampling the data first.\n# We have many ways to resampling. Basically, we can use oversampling like SMOTE, undersampling like Tomek Links, or combine SMOTE + Tomek link or SMOTE + ENN.\n# I will try to use SMOTE, Tomek link and SMOTEENN for resampling, and then choose the best combination to find important features.\ndef resampling_choice(pipe):\n    k=StratifiedKFold(n_splits=5,shuffle=True)\n    score=cross_val_score(pipe,x,y,cv=k,scoring='roc_auc')\n    print(score)\n    print(score.mean())\n\nover_pipe=make_pipeline(SMOTE(),RandomForestClassifier())\nunder_pipe=make_pipeline(TomekLinks(),RandomForestClassifier())\ncombine_pipe=make_pipeline(SMOTEENN(),RandomForestClassifier())\n\nresampling_choice(over_pipe)\nresampling_choice(under_pipe)\nresampling_choice(combine_pipe)","0a9fd8b1":"xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTEENN(random_state=111).fit_resample(xtrain,ytrain)\nRFC=RandomForestClassifier(random_state=111)\nRFC.fit(x_res,y_res)\nimportance=RFC.feature_importances_\nindices=np.argsort(importance)\nfeature=train_dataset.drop(['target','enrollee_id'],axis=1).columns.tolist()\nfeat_imp=np.array(feature)[indices]\nvalues=importance[indices]\ny_ticks=np.arange(0,len(feature))\nfig,ax=plt.subplots(figsize=(18,10))\nax.barh(y_ticks,values)\nax.set_yticklabels(feat_imp)\nax.set_yticks(y_ticks)\nplt.title('Important Features')\nfig.tight_layout()\nplt.show()\n\n\n## We can use this result to pick some important features to build the model. But in fact, more features can make the model more accurate. So I'm not going to do this.\n## If the data set has thousands of features or is very complex, based on the curse of dimensionality theory It should do the feature selection.\n\n## Here is the way when I decide to go featrue selection.\n#best_feature=feat_imp[-8:,]\n#new_data=train_dataset[['enrollee_id'] + list(best_feature) + ['target']]\n#new_data","75fdf366":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import learning_curve\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.combine import SMOTEENN\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\n\n## This is a way that I try to preliminary choose the best model and resampling combination. \n## I will choose the combination that AUC scores over 75.\n\ndef model_observe(resample,model,title):\n    k=StratifiedKFold(n_splits=5,shuffle=True)\n    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\n    model_pipe=make_pipeline(resample,model)\n    model_score=cross_val_score(model_pipe,x_train,y_train,cv=k,scoring='roc_auc')\n    \n    print('------------------')\n    print(title)\n    print(model_score)\n    print(model_score.mean())\n    \nmodel_observe(SMOTE(),DecisionTreeClassifier(),'SMOTE + Tree')\nmodel_observe(TomekLinks(),DecisionTreeClassifier(),'TomekLinks + Tree')\nmodel_observe(SMOTEENN(),DecisionTreeClassifier(),'SMOTEENN + Tree')\n\nmodel_observe(SMOTE(),RandomForestClassifier(),'SMOTE + Random Forest')\nmodel_observe(TomekLinks(),RandomForestClassifier(),'TomekLinks + Random Forest')\nmodel_observe(SMOTEENN(),RandomForestClassifier(),'SMOTEENN Random Forest')\n\nmodel_observe(SMOTE(),BaggingClassifier(),'SMOTE + Bagging')\nmodel_observe(TomekLinks(),BaggingClassifier(),'TomekLinks + Bagging')\nmodel_observe(SMOTEENN(),BaggingClassifier(),'SMOTEENN + Bagging')\n\nmodel_observe(SMOTE(),GradientBoostingClassifier(),'SMOTE + Gradient Boosting')\nmodel_observe(TomekLinks(),GradientBoostingClassifier(),'TomekLinks + Gradient Boosting')\nmodel_observe(SMOTEENN(),GradientBoostingClassifier(),'SMOTEENN + Gradient Boosting')\n\nmodel_observe(SMOTE(),XGBClassifier(),'SMOTE + XGBoost')\nmodel_observe(TomekLinks(),XGBClassifier(),'TomekLinks + XGBoost')\nmodel_observe(SMOTEENN(),XGBClassifier(),'SMOTEENN + XGBoost')\n\nlgbm_model=lgb.LGBMClassifier()\nmodel_observe(SMOTE(random_state=111),lgbm_model,'SMOTE + LGBM')\nmodel_observe(TomekLinks(),lgbm_model,'TomekLinks LGBM')\nmodel_observe(SMOTEENN(random_state=111),lgbm_model,'SMOTEENN + LGBM')","3f2fe16d":"## data modeling function\n\nimport time\nfrom sklearn.metrics import roc_auc_score\naccuracy={}\nf1={}\nrecall={}\nauc={}\nspeed={}\n\ndef model_training(x_res,y_res,model,title):\n    #x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=111)\n    #x_res,y_res=resample.fit_resample(x_train,y_train)\n    start=time.time()\n    model.fit(x_res,y_res)\n    end=time.time()\n    model_pred=model.predict(x_test)\n    acc=accuracy_score(y_test,model_pred)\n    accuracy[title]=acc\n    train_score=model.score(x_res,y_res)\n    f_s=f1_score(y_test,model_pred)\n    f1[title]=f_s\n    recall_res=recall_score(y_test,model_pred)\n    recall[title]=recall_res\n    y_pred=model.predict_proba(x_test)[:,1]\n    auc_res=roc_auc_score(y_test,y_pred)\n    auc[title]=auc_res\n    total_time=end-start\n    speed[title]=total_time\n    cm=confusion_matrix(y_test,model_pred)\n    sns.heatmap(cm,annot=True,fmt='.2f')\n    plt.title(title)\n    plt.show()\n    \n    print(classification_report(y_test,model_pred))\n    print('Accuracy:%.2f'%(acc))\n    print('Training score:%.2f'%(train_score))\n    print('F1 score:%.2f'%(f_s))\n    print('Recall:%.2f'%(recall_res))\n    print('AUC:%.2f'%(auc_res))\n    print('Times:%.2f'%(total_time))\n    result_form=pd.DataFrame({\n                         'Accuracy':accuracy,\n                         'F1':f1,\n                         'Recall':recall,\n                         'AUC':auc,\n                         'Speed(s)':speed})\n    return result_form","73aa0790":"###Learning curve(roc_auc)\nfrom sklearn.model_selection import StratifiedKFold\ndef overfitting_check(model,title):\n    k=StratifiedKFold(n_splits=5,shuffle=True)\n    train_sizes,train_score,test_score=learning_curve(model,x,y,cv=k,scoring='roc_auc',train_sizes=np.linspace(0.01,1.0,8))\n    \n    train_score_mean=np.mean(train_score,axis=1)\n    train_score_std=np.std(train_score,axis=1)\n    test_score_mean=np.mean(test_score,axis=1)\n    test_score_std=np.std(test_score,axis=1)\n    \n    plt.fill_between(train_score_mean-train_score_std,train_score_mean+train_score_std,alpha=0.1,color='r')\n    plt.fill_between(test_score_mean-test_score_std,test_score_mean+test_score_std,alpha=0.1,color='g')\n    \n    plt.plot(train_sizes,train_score_mean,'-o',color='r',label='Training')\n    plt.plot(train_sizes,test_score_mean,'-o',color='g',label='Cross_Validation')\n    \n    plt.grid()\n    plt.ylabel('AUC')\n    plt.title(title)\n    plt.legend()\n    plt.show()","d49834fa":"#learning curve(cross-entropy)\ndef overfitting_check_loss(model,title):\n    k=StratifiedKFold(n_splits=5,shuffle=True)\n    train_sizes,train_loss,test_loss=learning_curve(model,x,y,cv=k,scoring='neg_log_loss',train_sizes=np.linspace(0.01,1.0,8))\n    \n    train_loss_mean=-np.mean(train_loss,axis=1)\n    test_loss_mean=-np.mean(test_loss,axis=1)\n    \n    plt.plot(train_sizes,train_loss_mean,'-o',color='r',label='Training')\n    plt.plot(train_sizes,test_loss_mean,'-o',color='g',label='Cross_Validation')\n    \n    plt.grid()\n    plt.ylabel('loss')\n    plt.title(title)\n    plt.legend()\n    plt.show()","fb198bc5":"# We can find the best parameters through hyperopt, random search or GridSearch\n# Here, I will conduct a random search.\n\nfrom scipy.stats import randint\nfrom sklearn.model_selection import StratifiedKFold\n\nk=StratifiedKFold(n_splits=5,shuffle=True)\nrfc_params={\n    'randomforestclassifier__n_estimators':randint(1,1000),\n    'randomforestclassifier__max_depth':randint(3,20),\n    'randomforestclassifier__min_samples_leaf':randint(1,100)\n}\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nsmote_rfc_pipe=make_pipeline(SMOTE(random_state=111),RandomForestClassifier(random_state=111))\nrfc_smote_optimal=RandomizedSearchCV(smote_rfc_pipe,rfc_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\nrfc_smote_optimal.fit(x_train,y_train)\nprint(rfc_smote_optimal.best_params_)\nprint(rfc_smote_optimal.best_score_)","1afeb2e2":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTE(random_state=111).fit_resample(x_train,y_train)\nrfc=RandomForestClassifier(n_estimators=742,max_depth=9,min_samples_leaf=50,random_state=111,n_jobs=-1)  ## I don't use the parameters suggested by \"random search\" because I want to try to find the result that I expect.\ntitle='SMOTE + Random Forest'\nform=model_training(x_res,y_res,rfc,title)\noverfitting_check_loss(rfc,title)\noverfitting_check(rfc,title)\nform","f94d4936":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nrfc_tomeklinks_pipe=make_pipeline(TomekLinks(),RandomForestClassifier(random_state=111))\nrfc_tomeklinks_optimal=RandomizedSearchCV(rfc_tomeklinks_pipe,rfc_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\nrfc_tomeklinks_optimal.fit(x_train,y_train)\nprint(rfc_tomeklinks_optimal.best_params_)\nprint(rfc_tomeklinks_optimal.best_score_)","aa625bf1":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=TomekLinks().fit_resample(x_train,y_train)\nrfc=RandomForestClassifier(n_estimators=860,max_depth=16,min_samples_leaf=45,random_state=111)\ntitle='TomekLinks + Random Forest'\nform=model_training(x_res,y_res,rfc,title)\noverfitting_check_loss(rfc,title)\noverfitting_check(rfc,title)\nform","5ff20967":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nrfc_smoteenn_pipe=make_pipeline(SMOTEENN(random_state=111),RandomForestClassifier(random_state=111))\nrfc_smoteenn_optimal=RandomizedSearchCV(rfc_smoteenn_pipe,rfc_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\nrfc_smoteenn_optimal.fit(x_train,y_train)\nprint(rfc_smoteenn_optimal.best_params_)\nprint(rfc_smoteenn_optimal.best_score_)","6f2fab37":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTEENN(random_state=111).fit_resample(x_train,y_train)\nrfc=RandomForestClassifier(n_estimators=191,max_depth=12,min_samples_leaf=30,random_state=111)\ntitle='SMOTEENN + Random Forest'\nform=model_training(x_res,y_res,rfc,title)\noverfitting_check_loss(rfc,title)\noverfitting_check(rfc,title)\nform","686beefc":"bagg_params={\n    'baggingclassifier__n_estimators':randint(1,1000),\n    'baggingclassifier__bootstrap':[True,False],\n    'baggingclassifier__bootstrap_features':[True,False],\n    'baggingclassifier__max_features':[0.5,0.6,0.7,0.8,0.9,1.0],\n    'baggingclassifier__max_samples':[0.5,0.6,0.7,0.8,0.9,1.0],\n    'baggingclassifier__random_state':[111]\n}\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nbagg_smote_pipe=make_pipeline(SMOTE(random_state=111),BaggingClassifier())\nbagg_smote_optimal=RandomizedSearchCV(bagg_smote_pipe,bagg_params,n_iter=10,cv=k,scoring='roc_auc')\nbagg_smote_optimal.fit(x_train,y_train)\nprint(bagg_smote_optimal.best_params_)\nprint(bagg_smote_optimal.best_score_)","b4b6f4e1":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTE(random_state=111).fit_resample(x_train,y_train)\nmodel=BaggingClassifier(n_estimators=63,bootstrap=True,bootstrap_features=False,max_features=0.9,max_samples=0.5,random_state=111,n_jobs=-1)\ntitle='SMOTE + Bagging'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","ec3c3611":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nbagg_tomeklink_pipe=make_pipeline(TomekLinks(),BaggingClassifier())\nbagg_tomeklink_optimal=RandomizedSearchCV(bagg_tomeklink_pipe,bagg_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\nbagg_tomeklink_optimal.fit(x_train,y_train)\nprint(bagg_tomeklink_optimal.best_params_)\nprint(bagg_tomeklink_optimal.best_score_)","ca3704d0":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=TomekLinks().fit_resample(x_train,y_train)\nmodel=BaggingClassifier(n_estimators=300,bootstrap=True,bootstrap_features=False,max_features=0.5,max_samples=0.5,random_state=111,n_jobs=-1)\ntitle='Tomeklink + Bagging'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","f5719b99":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nbagg_smoteenn_pipe=make_pipeline(SMOTEENN(random_state=111),BaggingClassifier())\nbagg_optimal=RandomizedSearchCV(bagg_smoteenn_pipe,bagg_params,n_iter=10,cv=k,n_jobs=-1,scoring='roc_auc')\nbagg_optimal.fit(x_train,y_train)\nprint(bagg_optimal.best_params_)\nprint(bagg_optimal.best_score_)","d9b58aaf":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTEENN(random_state=111).fit_resample(x_train,y_train)\nmodel=BaggingClassifier(n_estimators=186,bootstrap=True,bootstrap_features=True,max_features=0.9,max_samples=0.7,random_state=111,n_jobs=-1)\ntitle='SMOTEENN + Bagging'\nmodel_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)","1203f0e7":"grad_params={\n    'gradientboostingclassifier__n_estimators':randint(100,1000),\n    'gradientboostingclassifier__max_depth':randint(3,20),\n    'gradientboostingclassifier__max_features':[0.5,0.6,0.7,0.8,0.9,1],\n    'gradientboostingclassifier__subsample':[0.5,0.6,0.7,0.8,0.9,1],\n    'gradientboostingclassifier__learning_rate':[0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n}\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\ngrad_smote_pipe=make_pipeline(SMOTE(random_state=111),GradientBoostingClassifier())\ngrad_smote_optimal=RandomizedSearchCV(grad_smote_pipe,grad_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\ngrad_smote_optimal.fit(x_train,y_train)\nprint(grad_smote_optimal.best_params_)\nprint(grad_smote_optimal.best_score_)","601ccbf5":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTE(random_state=111).fit_resample(x_train,y_train)\nmodel=GradientBoostingClassifier(n_estimators=668,max_depth=3,max_features=1,subsample=1,learning_rate=0.01,random_state=111)\ntitle='SMOTE + Gradient Boosting'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","137cfbdf":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\ngrad_tomeklinks_pipe=make_pipeline(TomekLinks(),GradientBoostingClassifier())\ngrad_tomeklinks_optimal=RandomizedSearchCV(grad_tomeklinks_pipe,grad_params,n_iter=10,cv=k,scoring='roc_auc')\ngrad_tomeklinks_optimal.fit(x_train,y_train)\nprint(grad_tomeklinks_optimal.best_params_)\nprint(grad_tomeklinks_optimal.best_score_)","1d803110":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=TomekLinks().fit_resample(x_train,y_train)\nmodel=GradientBoostingClassifier(n_estimators=374,max_depth=4,max_features=0.9,subsample=1,learning_rate=0.01,random_state=111)\ntitle='Tomeklink + Gradient Boosting'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","9760b367":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\ngrad_smoteenn_pipe=make_pipeline(SMOTEENN(random_state=111),GradientBoostingClassifier())\ngrad_smoteenn_optimal=RandomizedSearchCV(grad_smoteenn_pipe,grad_params,n_iter=10,cv=k,scoring='roc_auc')\ngrad_smoteenn_optimal.fit(x_train,y_train)\nprint(grad_smoteenn_optimal.best_params_)\nprint(grad_smoteenn_optimal.best_score_)","19ac6eb2":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\ngrad=GradientBoostingClassifier(n_estimators=554,max_depth=4,max_features=0.8,subsample=0.6,learning_rate=0.01,random_state=111)\nx_res,y_res=SMOTEENN(random_state=111).fit_resample(x_train,y_train)\ntitle='SMOTEENN + Gradient Boosting'\nform=model_training(x_res,y_res,grad,title)\noverfitting_check_loss(grad,title)\noverfitting_check(grad,title)\nform","19572e9d":"xgb_params={\n    'xgbclassifier__n_estimators':randint(100,1000),\n    'xgbclassifier__subsample':[0.5,0.6,0.7,0.8,0.9,1],\n    'xgbclassifier__colsample_bytree':[0.5,0.6,0.7,0.8,0.9,1],\n    'xgbclassifier__learning_rate':[0.01,0.03,0.05,0.07,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n    'xgbclassifier__gamma':[0.1,0.3,0.5,0.7,0.9,1,2,3,4,5,6,7,8,9,10],\n    'xgbclassifier__random_state':[111],\n    'xgbclassifier__max_depth':randint(3,20),\n    'xgbclassifier__random_state':[111]\n}\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nxgb_smote_pipe=make_pipeline(SMOTE(random_state=111),XGBClassifier())\nxgb_smote_optimal=RandomizedSearchCV(xgb_smote_pipe,xgb_params,n_iter=10,cv=k,scoring='roc_auc')\nxgb_smote_optimal.fit(x_train,y_train)\nprint(xgb_smote_optimal.best_params_)\nprint(xgb_smote_optimal.best_score_)","10dd68c6":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTE(random_state=111).fit_resample(x_train,y_train)\nmodel=XGBClassifier(n_estimators=258,max_depth=5,colsample_bytree=1,subsample=0.8,gamma=8,learning_rate=0.01,random_state=111,n_jobs=-1)\ntitle='SMOTE + XGBoost'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","1fe82dc0":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nxgb_tomeklinks_pipe=make_pipeline(TomekLinks(),XGBClassifier())\nxgb_tomeklinks_optimal=RandomizedSearchCV(xgb_tomeklinks_pipe,xgb_params,n_iter=10,cv=k,scoring='roc_auc')\nxgb_tomeklinks_optimal.fit(x_train,y_train)\nprint(xgb_tomeklinks_optimal.best_params_)\nprint(xgb_tomeklinks_optimal.best_score_)","947933bb":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=TomekLinks().fit_resample(x_train,y_train)\nmodel=XGBClassifier(n_estimators=858,max_depth=11,colsample_bytree=0.7,subsample=1,gamma=10,learning_rate=0.01,random_state=111)\ntitle='TomekLinks + XGBoost'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","1a1026e7":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nxgb_smoteenn_pipe=make_pipeline(SMOTEENN(random_state=111),XGBClassifier())\nxgb_smoteenn_optimal=RandomizedSearchCV(xgb_smoteenn_pipe,xgb_params,n_iter=10,cv=k,scoring='roc_auc')\nxgb_smoteenn_optimal.fit(x_train,y_train)\nprint(xgb_smoteenn_optimal.best_params_)\nprint(xgb_smoteenn_optimal.best_score_)","a9f7c8d9":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTEENN(random_state=111).fit_resample(x_train,y_train)\nmodel=XGBClassifier(n_estimators=506,max_depth=11,colsample_bytree=0.9,subsample=0.6,gamma=12,learning_rate=0.01,random_state=111,n_jobs=-1)\ntitle='SMOTEENN + XGBoost'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","b03b5b1f":"lgb_params={\n    'lgbmclassifier__boosting_type':['gbdt','rf'],\n    'lgbmclassifier__max_depth':randint(3,20),\n    'lgbmclassifier__num_leaves':randint(1,512),\n    'lgbmclassifier__min_data_in_leaf':randint(1,512),\n    'lgbmclassifier__bagging_fraction':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    'lgbmclassifier__bagging_freq':randint(1,10),\n    'lgbmclassifier__learning_rate':[0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n}\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nlgbm_model=lgb.LGBMClassifier()\nlgbm_smote_pipe=make_pipeline(SMOTE(random_state=111),lgbm_model)\nlgbm_smote_optimal=RandomizedSearchCV(lgbm_smote_pipe,lgb_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\nlgbm_smote_optimal.fit(x_train,y_train)\nprint(lgbm_smote_optimal.best_params_)\nprint(lgbm_smote_optimal.best_score_)","c4c11576":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTE(random_state=111).fit_resample(x_train,y_train)\nmodel=lgb.LGBMClassifier(boosting_type='gbdt',\n                         max_depth=4,bagging_fraction=0.9,min_data_in_leaf=326,\n                         bagging_freq=12,learning_rate=0.03,num_leaves=706,n_jobs=-1)\ntitle='SMOTE + LGBM'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","f1a39d5e":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=TomekLinks().fit_resample(x_train,y_train)\nlgbm_model=lgb.LGBMClassifier()\nlgbm_tomeklinks_pipe=make_pipeline(TomekLinks(),lgbm_model)\nlgbm_tomeklinks_optimal=RandomizedSearchCV(lgbm_tomeklinks_pipe,lgb_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\nlgbm_tomeklinks_optimal.fit(x_train,y_train)\nprint(lgbm_tomeklinks_optimal.best_params_)\nprint(lgbm_tomeklinks_optimal.best_score_)","40298652":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=TomekLinks().fit_resample(x_train,y_train)\nmodel=lgb.LGBMClassifier(boosting_type='gbdt',\n                         max_depth=6,bagging_fraction=1,min_data_in_leaf=252,\n                         bagging_freq=6,learning_rate=0.03,num_leaves=440,n_jobs=-1)\ntitle='TomekLinks + LGBM'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform","72b778b1":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTEENN(random_state=111).fit_resample(x_train,y_train)\nlgbm_model=lgb.LGBMClassifier()\nlgbm_smoteenn_pipe=make_pipeline(SMOTEENN(random_state=111),lgbm_model)\nlgbm_smoteenn_optimal=RandomizedSearchCV(lgbm_smoteenn_pipe,lgb_params,n_iter=10,cv=k,scoring='roc_auc',n_jobs=-1)\nlgbm_smoteenn_optimal.fit(x_train,y_train)\nprint(lgbm_smoteenn_optimal.best_params_)\nprint(lgbm_smoteenn_optimal.best_score_)","7be60c32":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTEENN(random_state=111).fit_resample(x_train,y_train)\nmodel=lgb.LGBMClassifier(boosting_type='gbdt',\n                         max_depth=11,bagging_fraction=1,min_data_in_leaf=374,\n                         bagging_freq=12,learning_rate=0.04,num_leaves=27,n_jobs=-1)\ntitle='SMOTEENN + LGBM'\nform=model_training(x_res,y_res,model,title)\noverfitting_check_loss(model,title)\noverfitting_check(model,title)\nform\n## According to requirements, the highest AUC score is the first condition for selecting a model\n## But all the models here have a good AUC score. Therefore, my second condition depends on the recall rate and f1 score.","f113ce99":"sns.set_style('whitegrid')\nplt.figure(figsize=(20,15))\nsns.barplot(x=list(accuracy.values()),y=list(accuracy.keys()))\nplt.title('Accuracy')\nplt.show()","f3f27995":"sns.set_style('whitegrid')\nplt.figure(figsize=(20,15))\nsns.barplot(x=list(f1.values()),y=list(f1.keys()))\nplt.title('F1 Score')\nplt.show()","3cbc46ca":"sns.set_style('whitegrid')\nplt.figure(figsize=(20,15))\nsns.barplot(x=list(recall.values()),y=list(recall.keys()))\nplt.title('Recall')\nplt.show()","a7a54a09":"sns.set_style('whitegrid')\nplt.figure(figsize=(20,15))\nsns.barplot(x=list(auc.values()),y=list(auc.keys()))\nplt.title('AUC Score')\nplt.show()","251a3bb8":"sns.set_style('whitegrid')\nplt.figure(figsize=(20,15))\nsns.barplot(x=list(speed.values()),y=list(speed.keys()))\nplt.title('Time')\nplt.show()","8bf64c18":"test_dataset","d4d73d2e":"test_dataset['gender'].fillna(value=-1,inplace=True)\ntest_dataset['major_discipline'].fillna(value=-1,inplace=True)\ntest_dataset['company_size'].fillna(value=-1,inplace=True) \ntest_dataset['company_type'].fillna(value=-1,inplace=True)\ntest_dataset.dropna(subset=['enrolled_university','education_level','experience','last_new_job'],axis=0,inplace=True)\ntest_dataset.info()","2934aa8d":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=111)\nx_res,y_res=SMOTE(random_state=111).fit_resample(x_train,y_train)\nclf=lgb.LGBMClassifier(boosting_type='gbdt',\n                         max_depth=4,bagging_fraction=0.9,min_data_in_leaf=326,\n                         bagging_freq=12,learning_rate=0.03,num_leaves=706,n_jobs=-1)\nclf.fit(x_res,y_res)\npredict=clf.predict_proba(test_dataset.drop(['enrollee_id'],axis=1))[:,1]\n#predict=clf.predict(test_dataset.drop(['enrollee_id'],axis=1))\nsubmission=pd.DataFrame({'enrollee_id':test_dataset['enrollee_id'],\n                      'target':predict})\nsubmission","5018d7dd":"submission.to_csv('submission.csv',index=False)","19dcc143":"* The city development index seems to be an important factor that make employees want to resign.","cfb5d1a9":"**I once again apologize for my poor English and analytical skills. Therefore, if I have any grammatical or analytical errors, please feel free to correct me. Let me have the opportunity to improve my English and analytical skills at the same time. If you like this, please upvote!**","f2d5875f":"# Modeling","b2e2796e":"* The reason employees quit seems to be because the city they live in does not have a high development index.\n* Most of these resigned employees have only 5 years or less of work experience.\n* However, people who have worked for more than 10 years are more inclined to keep their jobs.\n* It is reasonable for young people to want to be promoted or to look for more promising jobs. In contrast, People who have been working for 10 years or more not only want to stabilize, but they may already have their own family, so in contrast, they are less willing to take the risk of changing jobs.","6b025d66":"* Based on preliminary EDA result, I realized that the data is actually not readable to me. Therefore, I decided to do label encoding first.","81aae73a":"*  As we can see, this is an imbalanced dataset. Because the people who leave their job only 25% in total. So we need to do resampling before we build the model.","4f1c3075":"* As I mentioned before, the city development index is negatively correlated with turnover intention(-.34).\n* last_new_job is mean the difference in years between previous job and current job. \n* last_new_job is positively correlated with experience(.48), it pretty much makes sense.","7f14ca47":"* SMOTEENN + random forest is the best choice, so I will use this combination for feature selection.","e72eebc2":"# Introduction\n* Hello everyone! First of all, I have to apologize for my poor English. Because English is not my native language, there must be many grammatical errors. Therefore, if I have any grammatical errors, please feel free to correct me.","829658c9":"**Now It's time to deal with missing values**\n* Basically, we have lots of ways can deal with missing values.\n* Just like we can imputing missing values by mode, median, or mean. Or we can use the KNN algorithm to imputing missing values.","48a9c931":"# Data Cleaning","674b167f":"* Okay, it's done. Let's keep doing EDA!","6ae5c8e8":"* As we have seen, LGBM + SMOTEENN seems to be the best model here. However, when you look at the learning curve, you will find that it is overfitting. So I'm going to use SMOTE + LGBM, the model still has a high AUC score and high speed, and even fitting very well.","108ee781":"**Feature Selection**\n* Feature selection means that we can select or filter features that are not really important and improve our prediction accuracy.","c7ae47d9":"* People who have work experience under 10 years seems to have higher intended to resign. ","66b371e1":"# EDA"}}