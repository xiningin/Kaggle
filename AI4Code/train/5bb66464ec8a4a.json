{"cell_type":{"728ef4ba":"code","2fe8fd51":"code","0d7c0dfb":"code","cacd1229":"code","b0360586":"code","73a0004d":"code","c9c8f956":"code","e9ab89c4":"code","b2ff7503":"code","d3a0729f":"code","f3658ca1":"code","c766b87c":"code","0bd3eaa5":"code","f3d2f273":"code","e5b12d85":"code","f6df9d99":"code","3feab2d7":"code","494fbb01":"code","6320cf5c":"code","03bba064":"code","6ee05257":"code","f6bc3a6d":"code","0d65fad4":"code","5a09c61d":"code","2c0ebd9b":"code","7c0a9558":"code","063abb41":"code","8740bae1":"code","586da29e":"code","1b4fcf9f":"code","8cbb1ac3":"code","a16d1f00":"code","e19a319c":"code","815739d0":"code","16319cc9":"code","626c4161":"code","06cdd830":"code","5655459b":"code","12bbe9a1":"code","6da5b600":"code","6acaeb3a":"code","2af91cf9":"code","8b5c97f7":"code","06356225":"code","41076ff9":"code","d8e258d3":"code","5070b54e":"code","eaf2ee3d":"code","3b51d3e4":"code","cfb465be":"code","483e447e":"code","548c456c":"code","c94e0bd6":"code","9befbf5e":"code","6475d137":"code","f2d9957a":"code","eb16dbc5":"code","9338633f":"code","1fdc00c4":"code","821615e8":"code","7cb23699":"code","dc3f0809":"code","b82d61e7":"code","665d4886":"code","1553574c":"code","dedfe649":"code","996e1d5e":"code","c7c6b23e":"code","d9c25785":"code","99e7068b":"code","9bfc0526":"code","806626ae":"code","803a8b07":"code","2501caf6":"code","9c0e2d06":"code","86a022a4":"code","0fce575b":"code","4c7fdd9c":"code","711d3509":"code","864acc18":"code","5f52716d":"code","d861b111":"code","6a8a48e5":"code","c148780a":"code","ede2dfa4":"code","bee9eada":"code","c47c284c":"code","5eccf59e":"code","3a68af22":"code","79d6e2dd":"code","e8881371":"code","f596e7e4":"code","4b8ee0a1":"markdown","f2a3b0db":"markdown","9c208801":"markdown","6cd79a37":"markdown","8f05bf89":"markdown","04605ead":"markdown","8e688849":"markdown","0e018bdc":"markdown","ede6db07":"markdown","cfaa3478":"markdown","b1bf0736":"markdown","0670baa2":"markdown","0cae24dd":"markdown","263615bf":"markdown","80312d56":"markdown","0ece4e7d":"markdown","172080ab":"markdown","60fff98b":"markdown","9f81f14e":"markdown","cdf38602":"markdown","b30424bc":"markdown","4f5bef70":"markdown","4ad7e089":"markdown","5e6cc5a4":"markdown","e964d434":"markdown","38770e64":"markdown","210d6bac":"markdown","77c7271b":"markdown","1de51a6c":"markdown","765a23db":"markdown","cb972104":"markdown","fc134fc8":"markdown","0d8658b3":"markdown","e3797a8b":"markdown","4297c287":"markdown","043d3a37":"markdown","e09c2bad":"markdown","41f06325":"markdown","000d66fc":"markdown","4cb6f1be":"markdown","c59120f2":"markdown"},"source":{"728ef4ba":"from datetime import datetime\nprint('Process start time :', datetime.now())\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fe8fd51":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","0d7c0dfb":"df = pd.read_csv('\/kaggle\/input\/craigslist-carstrucks-data\/vehicles.csv')","cacd1229":"df.shape","b0360586":"df.info()","73a0004d":"df.describe()","c9c8f956":"# show all columns\npd.set_option('display.max_columns', None)","e9ab89c4":"df.head(5)","b2ff7503":"# Remove column - 'url' since the details present in url is already available in \n# columns - 'region', 'county', 'state'\ndf = df.drop('url', axis=1)","d3a0729f":"df['region_url'].unique()[:10]","f3658ca1":"# The county column only has 'nan' in it. Therefore, take the county name from 'region_url' column.\ndf['county'].unique()","c766b87c":"# updating 'county' from 'region_from_url' column.\ndf['county'] = df['region_url'].str.replace('https:\/\/','').str.replace('.craigslist.org','')","0bd3eaa5":"# drop column - region_url\ndf = df.drop('region_url', axis=1)","f3d2f273":"# finding unique list of manufacturer. There are NULL values.\ndf['manufacturer'].unique()","e5b12d85":"model_df = df.loc[df['manufacturer'].isnull(), ['model']]","f6df9d99":"model_df['model'].unique()[:50]","3feab2d7":"model_df.shape","494fbb01":"df.shape","6320cf5c":"# update 'manufacturer' to 'other' when its NULL\ndf.loc[df['manufacturer'].isnull(), ['manufacturer']] = 'other'","03bba064":"df.head(5)","6ee05257":"df[df['region'].isnull() == True]","f6bc3a6d":"df['region'].unique()[:10]","0d65fad4":"# removing all records which has price = 0.\ndf = df[df['price'] != 0]","5a09c61d":"df['year'].unique()","2c0ebd9b":"# There are totally 82 records that have year higher than current year ie) 2020. They have to be removed as well.\ndf[df['year'] > 2020].shape","7c0a9558":"df = df[df['year'] < 2020]","063abb41":"# 'model' column has lot of inconsistent data eg). Anything, sequoia limited, 30 YEARS.EXP.\ndf['model'].unique()","8740bae1":"# drop columns - lat, long\ndf = df.drop(['lat','long'], axis=1)","586da29e":"len(df)","1b4fcf9f":"# Identify the no.of missing values in each column and their percentage compared to total.\nmissing_vals = df.isnull().sum().sort_values(ascending = False)\n(missing_vals\/len(df))*100","8cbb1ac3":"# Removing rows which has less than 5% of NULLs in columns.\ndf=df.dropna(subset=['model','fuel','transmission','title_status','description'])","a16d1f00":"df.shape","e19a319c":"df.head()","815739d0":"df['cylinders'].unique()","16319cc9":"df['type'].unique()","626c4161":"# Using forward fill for the columns - 'paint_color', 'drive', 'cylinders', 'type'\ndf['type'] = df['type'].fillna(method='ffill')\ndf['paint_color'] = df['paint_color'].fillna(method='ffill')\ndf['drive'] = df['drive'].fillna(method='ffill')\ndf['cylinders'] = df['cylinders'].fillna(method='ffill')","06cdd830":"df.isnull().sum()","5655459b":"df['condition'].unique()","12bbe9a1":"# updating the condition as 'new' for all vehicles whose year is 2019 and above\ndf.loc[df.year>=2019, 'condition'] = df.loc[df.year>=2019, 'condition'].fillna('new')","6da5b600":"df.groupby(['condition']).count()['year']","6acaeb3a":"df.isnull().sum()","2af91cf9":"# Addressing the NULLs in 'odometer' column.\n\n# Since odometer is related to the condition of the vehicle, it can be used to fill the missing odometer values.\n# The mean of odometer values for each condition is calculated and is used to fill the NULL values for those \n# corresponding condition.","8b5c97f7":"# Find the total distinct values for 'condition'\nconditions = list(df['condition'].unique())\nconditions.pop(3) # removing null value from list\nconditions","06356225":"# Find the corresponding mean value of 'odometer' for each value in 'condition'\nmean_odometer_per_condition_df = df.groupby('condition').mean()['odometer'].reset_index()\nmean_odometer_per_condition_df","41076ff9":"excellent_odo_mean = df[df['condition'] == 'excellent']['odometer'].mean()\ngood_odo_mean = df[df['condition'] == 'good']['odometer'].mean()\nlike_new_odo_mean = df[df['condition'] == 'like new']['odometer'].mean()\nsalvage_odo_mean = df[df['condition'] == 'salvage']['odometer'].mean()\nfair_odo_mean = df[df['condition'] == 'fair']['odometer'].mean()","d8e258d3":"print('Like new average odometer:', round( like_new_odo_mean,2))\nprint('Excellent average odometer:', round( excellent_odo_mean,2))\nprint('Good average odometer:', round( good_odo_mean,2))\nprint('Fair average odometer:', round( fair_odo_mean,2))\nprint('Salvage average odometer:', round( salvage_odo_mean,2))","5070b54e":"# Update the 'condition' based on the average 'odometer' values for each 'condition'\n\ndf.loc[df['odometer'] <= like_new_odo_mean, 'condition'] = df.loc[df['odometer'] <= like_new_odo_mean, 'condition'].fillna('like new')\n\ndf.loc[df['odometer'] >= fair_odo_mean, 'condition'] = df.loc[df['odometer'] >= fair_odo_mean, 'condition'].fillna('fair')\n\ndf.loc[((df['odometer'] > good_odo_mean) & \n       (df['odometer'] <= excellent_odo_mean)), 'condition'] = df.loc[((df['odometer'] > good_odo_mean) & \n       (df['odometer'] <= excellent_odo_mean)), 'condition'].fillna('excellent')\n\ndf.loc[((df['odometer'] > like_new_odo_mean) & \n       (df['odometer'] <= good_odo_mean)), 'condition'] = df.loc[((df['odometer'] > like_new_odo_mean) & \n       (df['odometer'] <= good_odo_mean)), 'condition'].fillna('good')\n\ndf.loc[((df['odometer'] > good_odo_mean) & \n       (df['odometer'] <= fair_odo_mean)), 'condition'] = df.loc[((df['odometer'] > good_odo_mean) & \n       (df['odometer'] <= fair_odo_mean)), 'condition'].fillna('salvage')","eaf2ee3d":"# 'model' can be related to 'size'.\n# checking the sizes for model = 'patriot'\ndf.loc[df['model'] == 'patriot', 'size'].unique()\n\n#There are more than 1 size for the same model. Therefore, this column is not reliable and has to be removed. \n# The car details can be scrapped from a different source and then combined to populate the correct car features.","3b51d3e4":"# dropping the column - 'size' since its not reliable\n# dropping the column - 'id' since it doesn't have any meaning\n# dropping the column - 'image_url' since it doesn't have any meaning\n# dropping the column - 'vin' since it doesn't have any meaning\n# dropping the column - 'description' - few rows contain important details. dropping for now.\n\ndf = df.drop(['size','id','image_url','vin','description'], axis = 1)","cfb465be":"import matplotlib.pyplot as plt","483e447e":"sns.countplot(x='condition', data=df, palette=(\"Paired\"))\nplt.title('Number of vehicles listed on craigslist across different conditions', fontsize=22)","548c456c":"region_count  = df['region'].value_counts()\nregion_count = region_count[:10,]\nplt.figure(figsize=(11,8))\nsns.barplot(region_count.values, region_count.index, alpha=1,palette=(\"Paired\"))\nplt.title('Top 10 Counties which has the highest cars listings on Craigslist', fontsize=22)\nplt.xlabel('Number of Cars', size=\"20\")\nplt.ylabel('County Names', size=\"20\")\nplt.show()","c94e0bd6":"region_count  = df['region'].value_counts()\nregion_count = region_count[-10:,]\nplt.figure(figsize=(11,8))\nsns.barplot(region_count.values, region_count.index, alpha=1,palette=(\"Paired\"))\nplt.title('Top 10 Counties which has the lowest cars listings on Craigslist', fontsize=22)\nplt.xlabel('Number of Cars', size=\"20\")\nplt.ylabel('County Names', size=\"20\")\nplt.show()","9befbf5e":"state_count  = df['state'].value_counts()\nstate_count = state_count[-10:,]\nplt.figure(figsize=(11,8))\nstate_count.plot(kind='pie', subplots=True, figsize=(8, 8))\nplt.title('Top 10 States which has the highest cars listings on Craigslist', fontsize=22)\nplt.xlabel('Number of Cars', size=\"20\")\nplt.ylabel('States Abbrevation', size=\"20\")\nplt.show()","6475d137":"top_priced_counties = df.groupby('county').sum()['price'].reset_index().sort_values('price', ascending=False)[:10]\ntop_priced_counties","f2d9957a":"plt.figure(figsize=(11,8))\nsns.barplot(top_priced_counties.price, top_priced_counties.county, alpha=1,palette=(\"Paired\"))\nplt.title('Top 10 Counties w.r.t total car price on Craigslist', fontsize=22)\nplt.xlabel('Total value of Cars in thousand million (1e9)', size=\"20\")\nplt.ylabel('County Names', size=\"20\")\nplt.show()","eb16dbc5":"manufacturer_count = df['manufacturer'].value_counts().iloc[:10]\nplt.figure(figsize=(10,6))\nmanufacturer_count.plot(kind='pie', subplots=True, figsize=(8, 8))\nplt.title('Top 10 car manufacturer listings on Craigslist', fontsize=22)\nplt.xlabel('No.of Listings', size=\"20\")\nplt.ylabel('Manufacturer', size=\"20\")\nplt.show()","9338633f":"df.info()","1fdc00c4":"df['cylinders'].unique()","821615e8":"df['transmission'].unique()","7cb23699":"df['title_status'].unique()","dc3f0809":"# Removing region since we already have 'county'\ndf=df.drop('region', axis=1)","b82d61e7":"# Removing rows which has NULLs in conditon and odometer.\ndf=df.dropna(subset=['odometer','condition'])","665d4886":"# Identify the no.of missing values in each column and their percentage compared to total.\nmissing_vals = df.isnull().sum().sort_values(ascending = False)\n(missing_vals\/len(df))*100","1553574c":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()","dedfe649":"# convert characters to numbers using label Encoding\ndf[['county','manufacturer', 'model', 'condition','cylinders', 'fuel', 'title_status', 'transmission','drive', 'type', 'paint_color', 'state']] = df[['county','manufacturer', 'model', 'condition','cylinders', 'fuel', 'title_status', 'transmission','drive','type', 'paint_color', 'state']].apply(le.fit_transform)","996e1d5e":"df","c7c6b23e":"df[\"odometer\"] = np.sqrt(preprocessing.minmax_scale(df[\"odometer\"]))","d9c25785":"df","99e7068b":"# Seperate Features and Outcome\nX = df.drop('price',axis=1).values\ny = df.price.values","9bfc0526":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=2)\nskf.get_n_splits(X, y)\n\nfor train_index, test_index in skf.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n# works for classification\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","806626ae":"print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)","803a8b07":"# Create a dataframe to store accuracy scores of different algorithms\naccuracy_df = pd.DataFrame(columns=('r2', 'rmse'))","2501caf6":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport math\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Fit\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Decision Tree Baseline']))","9c0e2d06":"accuracy_df","86a022a4":"from sklearn.model_selection import GridSearchCV\n\nscoring = metrics.make_scorer(metrics.mean_squared_error)\n\nparam_grid = {\n'criterion':['mse'] \n,'splitter':['best','random']\n,'max_depth':[4, 5, 6, 7, 8]\n,'min_samples_split':[0.8, 2]\n,'max_features':['auto','sqrt','log2']\n}\n\ng_cv = GridSearchCV(DecisionTreeRegressor(random_state=0),param_grid=param_grid,scoring=scoring, cv=5, refit=True)","0fce575b":"g_cv.fit(X_train, y_train)\ng_cv.best_params_\nresult = g_cv.cv_results_\n# print(result)\n\n# Predict\ny_pred = g_cv.best_estimator_.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Decision Tree HyperParam Tuning']))\naccuracy_df.sort_values('rmse')","4c7fdd9c":"from sklearn.ensemble import RandomForestRegressor\n\n# Fit\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Random Forest Baseline']))\naccuracy_df.sort_values('rmse')","711d3509":"from sklearn.ensemble import GradientBoostingRegressor\n\n# Fit\nmodel = GradientBoostingRegressor(random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Gradient Boosting Baseline']))\naccuracy_df.sort_values('rmse')","864acc18":"from xgboost import XGBRegressor\n\n# Fit\nmodel = XGBRegressor(random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['XGBoost Baseline']))\naccuracy_df.sort_values('rmse')","5f52716d":"import xgboost as xgb\n\nmodel = xgb.XGBRegressor(\n#     gamma=1,                 \n    learning_rate=0.05,\n#     max_depth=3,\n#     n_estimators=10000,                                                                    \n#     subsample=0.8,\n    random_state=34,\n    booster='gbtree',    \n    objective='reg:squarederror',\n    eval_metric='rmse'\n) \nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['XGBoost with Parameters']))\naccuracy_df.sort_values('rmse')","d861b111":"from sklearn.neural_network import MLPRegressor\n\nmlp = MLPRegressor()\nparam_grid = {\n#               'hidden_layer_sizes': [i for i in range(2,20)],\n              'activation': ['relu'],\n              'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [0.01],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n              'early_stopping': [True],\n              'warm_start': [False]\n}\nmodel = GridSearchCV(mlp, param_grid=param_grid, \n                   cv=10, verbose=True, pre_dispatch='2*n_jobs')\n\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['MLPRegressor with Parameter Tuning']))\naccuracy_df.sort_values('rmse')\n\n","6a8a48e5":"#Splitting the training data in to training and validation datasets for Model training\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n#         'max_depth': -1,\n#         'subsample': 0.8,\n#         'bagging_fraction' : 1,\n#         'max_bin' : 5000 ,\n#         'bagging_freq': 20,\n#         'colsample_bytree': 0.6,\n        'metric': 'rmse',\n#         'min_split_gain': 0.5,\n#         'min_child_weight': 1,\n#         'min_child_samples': 10,\n#         'scale_pos_weight':1,\n#         'zero_as_missing': False,\n#         'seed':0,        \n    }\nmodel = lgb.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=8000,\n                  verbose_eval=500, valid_sets=valid_set)\n\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['LightGBM with Parameters']))\naccuracy_df.sort_values('rmse')","c148780a":"#Splitting the training data in to training and validation datasets for Model training\n\nimport lightgbm as lgb1\nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n#Define categorical features, training and validation data\ncategorical_positions = []\ncat = ['manufacturer','model','condition','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','county','state']\nfor c, col in enumerate(df.columns):\n    for x in cat:\n        if col == x:\n            categorical_positions.append(c-1)\n\n\ntrain_set = lgb1.Dataset(Xtrain, label=Ztrain, categorical_feature=categorical_positions)\nvalid_set = lgb1.Dataset(Xval, label=Zval)\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n#         'max_depth': -1,\n#         'subsample': 0.8,\n#         'bagging_fraction' : 1,\n#         'max_bin' : 5000 ,\n#         'bagging_freq': 20,\n#         'colsample_bytree': 0.6,\n        'metric': 'rmse',\n#         'min_split_gain': 0.5,\n#         'min_child_weight': 1,\n#         'min_child_samples': 10,\n#         'scale_pos_weight':1,\n#         'zero_as_missing': False,\n#         'seed':0,        \n    }\nmodel = lgb1.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=8000,\n                  verbose_eval=500, valid_sets=valid_set)\n\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['LightGBM with Categories & Parameters']))\naccuracy_df.sort_values('rmse')","ede2dfa4":"from catboost import CatBoostRegressor, Pool\n    \nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain)\nvalid_set = lgb.Dataset(Xval, Zval)\n\nmodel = CatBoostRegressor()\n\nmodel.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], verbose=100, early_stopping_rounds=1000)\n\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['CatBoost Baseline']))\naccuracy_df.sort_values('rmse')","bee9eada":"from catboost import CatBoostRegressor, Pool\n    \nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)\n\n\n        \nmodel = CatBoostRegressor(\n                          iterations=1000, \n                          depth=8, \n                          learning_rate=0.01, \n                          loss_function='RMSE', \n                          eval_metric='RMSE', \n                          use_best_model=True)\n\nmodel.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], verbose=100, early_stopping_rounds=1000)\n\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\n# accuracy_df = accuracy_df.drop('CatBoost Parameter Tuning')\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['CatBoost Parameter Tuning']))\naccuracy_df.sort_values('rmse')","c47c284c":"# Plot\nplt.figure(figsize=[25,6])\nplt.tick_params(labelsize=14)\nplt.plot(accuracy_df.index, accuracy_df['rmse'], label = 'RMSE Scores')\nplt.legend()\nplt.title('RMSE Score comparison for 10 popular models for test dataset')\nplt.xlabel('Models')\nplt.ylabel('RMSE Scores')\nplt.xticks(accuracy_df.index, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","5eccf59e":"print('Process start time :', datetime.now())","3a68af22":"# Rerunning MLP Neural Network to save the model\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\nmlp = MLPRegressor()\nparam_grid = {\n#               'hidden_layer_sizes': [i for i in range(2,20)],\n              'activation': ['relu'],\n              'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [0.01],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n              'early_stopping': [True],\n              'warm_start': [False]\n}\nmodel = GridSearchCV(mlp, param_grid=param_grid, \n                   cv=10, pre_dispatch='2*n_jobs')\n\nmodel.fit(X_train, y_train)","79d6e2dd":"# Save the neural network model\nfrom joblib import dump, load\n\nfilename = 'mlp_neural_network_001.joblib'\nwith open(filename, 'wb') as file:  \n    dump(model, file)","e8881371":"# Predict\ny_pred = model.predict(X_test)\ndf1 = pd.DataFrame({\"y\":y_test,\"y_pred\":y_pred })","f596e7e4":"df1.head(50)","4b8ee0a1":"# LightGBM","f2a3b0db":"### What are the top counties in terms of total price of all cars?","9c208801":"### Which Manufacturer's cars are the most listed ones?","6cd79a37":"- Places such as north platte, kirksville, la salle, meridian lies in the central US and away from the sea. \nThese areas has the least number of used cars listed for sales. This proves that location plays a role in\nused car sales market.\n","8f05bf89":"There is no change in accuracy even after mentioning the categorical columns explicitly. This is because most of the columns in the dataset is categorical. LightGBM algorithm automatically selects the categorical columns if its not given.","04605ead":"# Importing the data","8e688849":"## Plotting the RMSE Scores","0e018bdc":"### What is the condition of cars listed on Craigslist ?","ede6db07":"## Random Forest","cfaa3478":"## Decision Tree","b1bf0736":"# Save Model","0670baa2":"from sklearn.model_selection import GridSearchCV\n\nscoring = metrics.make_scorer(metrics.mean_squared_error)\n\nparam_grid = {\n# 'n_estimators':[50,70,100,120,130] \n# 'max_features':['auto','sqrt','log2']\n#,'oob_score':[False, True] # whether to use out-of-bag samples to estimate the R^2 on unseen data.\n# ,'bootstrap':[False, True]\n# ,'random_state':[10, None]\n# ,'warm_start':[True, False]\n'max_depth':[4, 5, 6, 7, 8]\n# ,'min_samples_split':[0.8, 2, 3]\n}\n\ng_cv = GridSearchCV(RandomForestRegressor(random_state=0),param_grid=param_grid,scoring=scoring, cv=5, refit=True)","0cae24dd":"## MLP Regressor","263615bf":"- The places -  Springfield, jacksonville, grand rapids, tampa bay area, baltimore, columbus, milwaukee \nbelong to the costal areas. This means that more people wanted to sell their used cars at places near sea\ncompared to people living away from sea.\n- This might be because of the fact that more people would visit beaches and are excellent tourist spots.\nIt could have created a demand in the used cars market because of the need","80312d56":"## XGBoost with Parameters","0ece4e7d":"- <strong>Ford cars are most listed ones <\/strong> followed by Chevrolet, Toyota, Honda & Nissan","172080ab":"## Handling continous values - Scaling down ","60fff98b":"### Which counties have the lowest number of listings ?","9f81f14e":"# Data Cleaning","cdf38602":"## LightGBM with categorical variables","b30424bc":"## CatBoost with Parameters","4f5bef70":"## Decision Tree with auto Hyper Parameter Tuning with Grid Search","4ad7e089":"### factorize the categorical columns\ncats = ['manufacturer','model','condition','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','county','state']\nfor cat in cats:\n#     sorting_list=np.unique(sorted(df[cat],key=lambda x:(str.lower(x),x)))\n#     df[cat]=pd.Categorical(df[cat], sorting_list)\n    df=df.sort_values(cat)\n    df[cat]=pd.factorize(df[cat], sort=True)[0]","5e6cc5a4":"# Machine Learning Algorithms","e964d434":"### Which states have the highest number of listings ?","38770e64":"- washington D.C (abbrevated as dc) and Rhode Island (abbrevated as ri) has the highest car listings followed by \nSouth Dakota (abbrevated as sd) and nebraska (abbrevated as ne).\n\n- Washington D.C and Rhode Island are near sea.\n- South Dakota & Nebraska are adjacent states with highest listings","210d6bac":"- Atlanda, pullman, tucson are not present near the sea and has the highest sum of priced listings.\n- This proves that <strong>Although the costal areas have more no.of listings, the value of listings is high in non costal areas<\/strong>\n- People would use cheaper cars near the costal areas for leisure rides but people living in cities have more business & family needs and therefore may require costly cars","77c7271b":"## XGBoost","1de51a6c":"### Which counties have the highest number of listings ?","765a23db":"g_cv.fit(X_train, y_train)\ng_cv.best_params_\nresult = g_cv.cv_results_\n# print(result)\n\n# Predict\ny_pred = g_cv.best_estimator_.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Random Forest HyperParam Tuning']))","cb972104":"# Exploratory Data Analysis","fc134fc8":"There are 20% of records which doesn't have a 'manufacturer' name. They can be updated to 'other'","0d8658b3":"## Gradient Boosting","e3797a8b":"## Random Forest with auto Hyper Parameter Tuning with Grid Search","4297c287":"- Most of the vehicles listed are in good and excellent condition. This is followed by 'like new' cars. \n- Salvage cars are also listed in the website. \n- New cars are not much listed on the website. This makes sense as people are less likely to sell new cars unless there is a major problem due to accident or manufacturing issues. ","043d3a37":"## CatBoost","e09c2bad":"accuracy_df","41f06325":"# Train Test Split","000d66fc":"## Handle Categorical values","4cb6f1be":"# Feature Preprocessing","c59120f2":"## Accuracies of Models sorted by RMSE Scores"}}