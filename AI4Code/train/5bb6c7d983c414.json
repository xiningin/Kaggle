{"cell_type":{"4f3293b2":"code","68703fbb":"code","06096800":"code","2a4034b7":"code","ac129ce7":"code","eb7aca2c":"code","c9b53664":"code","0f3056f5":"code","69928707":"code","ff6d0d22":"code","5c5bb96f":"code","601fc352":"code","e4ba9c4d":"code","325a24ba":"code","f91868e2":"code","8ad6e755":"code","6a468e2c":"code","da9c77ad":"code","2be0b736":"code","ce717721":"code","b20ce7d3":"code","3bba7857":"code","1f99c810":"code","ad9c0ee8":"code","84b4522f":"code","24bc834b":"code","e7fbfcba":"code","0df671b4":"code","bd835e38":"code","386fc8ac":"code","40740042":"code","11feca31":"code","efc21260":"code","d255c195":"code","53a3754d":"code","e09f923d":"code","6040108c":"code","8b521c01":"code","bc9381cd":"code","77b10494":"code","78e5feeb":"code","f8c9ef78":"code","55cab20c":"code","906edcec":"code","588985ad":"code","b4429852":"code","011958e0":"code","c5f185b2":"code","be2e2d51":"code","a37ed7ed":"code","65678633":"code","07365cf5":"code","b698e429":"code","c423710b":"code","d423bbd8":"code","e8b8a5d0":"code","53ae1419":"code","41df6562":"code","8b82d2da":"code","760bd9ba":"code","ef627369":"code","de7b9e20":"markdown","6dd675d5":"markdown","5936417e":"markdown","55cdfa44":"markdown","50c256c0":"markdown","fd1c3dc0":"markdown","342e3b24":"markdown","8807bc5c":"markdown","eb17fd7d":"markdown","8f5a0bf3":"markdown","e8e90254":"markdown","f6deb95a":"markdown","e273ade4":"markdown"},"source":{"4f3293b2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_boston\nimport seaborn as sns\nfrom sklearn.metrics import r2_score","68703fbb":"boston = load_boston()\ndata = pd.DataFrame(data=boston.data, columns=boston.feature_names)\ndata['Price'] = boston.target\ndata.head()","06096800":"data.info()","2a4034b7":"Missing_value = data.shape[0] - data.count()\nMissing_value","ac129ce7":"data.Price.describe()","eb7aca2c":"plt.hist(data.Price, bins = 20)","c9b53664":"data.shape","0f3056f5":"var_ind = np.arange(0, 14)\nplot = plt.figure(figsize = (20, 10))\nplot.subplots_adjust(hspace = 0.5, wspace = 0.5)\nfor i in range(1, 15):\n    a = plot.add_subplot(4, 4, i)\n    a.hist(data.iloc[:, var_ind[i - 1]], alpha = 0.7, bins = 30)\n    a.title.set_text(data.columns[i - 1])","69928707":"var_ind","ff6d0d22":"data.shape","5c5bb96f":"cor_table = round(data.iloc[:, var_ind].corr(method = 'pearson'), 2)\ncor_table","601fc352":"plt.figure(figsize = (12, 6))\nsns.heatmap(cor_table, annot = True)","e4ba9c4d":"var_ind = np.arange(0, 14)\nplot = plt.figure(figsize = (20, 12))\nplot.subplots_adjust(hspace = 0.8, wspace = 0.5)\nfor i in range(1, 13):\n    a = plot.add_subplot(3, 5, i)\n    a.scatter(x = data.iloc[:, var_ind[i - 1]], y = data.Price, alpha = 0.7)\n    a.title.set_text(f'Salary vs. {data.columns[i - 1]}')","325a24ba":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size = 0.26, random_state = 1234)\nprint(f'The shape of train is: {train.shape}')\nprint(f'The shape of test is: {test.shape}')","f91868e2":"X_train = train.iloc[:,:-1]\ny_train = train.iloc[:, -1]\nX_test = test.iloc[:, :-1]\ny_test = test.iloc[:, -1]\n\nprint(f'The shape of X_train is: {X_train.shape}')\nprint(f'The shape of y_train is: {y_train.shape}')\nprint(f'The shape of X_test is: {X_test.shape}')\nprint(f'The shape of y_test is: {y_test.shape}')\n","8ad6e755":"import statsmodels.formula.api as smf\nmodel1 = smf.ols(formula = 'np.log(Price) ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + I(LSTAT ** 2)', data = train)\nmodel1 = model1.fit()\nmodel1.summary()","6a468e2c":"#Check Assumptions of Regression\n#Normality of residuals\n\n#Plot histogram of residuals\nsns.distplot(model1.resid, hist = True, \n             kde = True, color = 'green',\n             bins = np.linspace(min(model1.resid), max(model1.resid), 30))","da9c77ad":"#QQ-plot\nimport statsmodels.api as sm\nqqplot_Model1 = sm.qqplot(model1.resid, line = 's')\nplt.show()","2be0b736":"#Residuals vs. Fitted Values\nsns.regplot(x = model1.fittedvalues, y = model1.resid, lowess = True, \n                       scatter_kws = {\"color\": \"black\"}, line_kws = {\"color\": \"red\"})\nplt.xlabel('Fitted Values', fontsize = 12)\nplt.ylabel('Residuals', fontsize = 12)\nplt.title('Residuals vs. Fitted Values', fontsize = 12)\nplt.grid()","ce717721":"y_pred_model1 = model1.predict(X_test)\ny_pred_model1 = np.exp(y_pred_model1)\nabs_err_model1 = np.abs(y_pred_model1 - y_test)\nnp.mean(abs_err_model1)","b20ce7d3":"#Absolute error mean, median, sd, IQR, max, min\nfrom scipy.stats import iqr\nmodel_comp = pd.DataFrame({'Mean of AbsErrors':    abs_err_model1.mean(),\n                           'Median of AbsErrors' : abs_err_model1.median(),\n                           'SD of AbsErrors' :     abs_err_model1.std(),\n                           'IQR of AbsErrors':     iqr(abs_err_model1),\n                           'Min of AbsErrors':     abs_err_model1.min(),\n                           'Max of AbsErrors':     abs_err_model1.max()}, index = ['Model1: Classical Regression'])\nmodel_comp","3bba7857":"y_pred = y_pred_model1\ny_test = y_test\nmodel = 'model1_ClassicalRegression'\n\nplt.figure(figsize=(15, 15), dpi = 150);\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25);\n\nplt.subplots(figsize=(16,6));\nplt.scatter(x = y_test, y = y_pred, color = 'blue');\n#Add 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.xlabel('Actual Value');\nplt.ylabel('Predicted Value');\n\nplt.title(f'{model}');\nplt.subplots(figsize=(17,6));\nprint(f'The r2-score is: {r2_score(y_pred, y_test)}')\nx_points = list(range(len(y_test)));\nplt.plot(x_points, y_test, label='y_real', color = 'blue');\nplt.plot(x_points, y_pred, label='y_predict', color = 'red');\nplt.legend();\nplt.title(f'{model}');\nplt.show();","1f99c810":"abs_err = abs_err_model1\nname = 'abs_err_model1 Classical Reg'\n\nplt.figure(figsize=(15, 5), dpi = 200)\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25)\n\n\nplt.subplot(121)\nplt.boxplot(abs_err);\nplt.title(f'boxplot of {name}');\nplt.grid()\n\nplt.subplot(122)\nsns.distplot(abs_err, kde = False);\nplt.title(f'Histogram of {name}');\n","ad9c0ee8":"logy_train = np.log(y_train)","84b4522f":"lambda_grid = 10 ** np.linspace(-2, 5, 1000)\nlambda_grid[:10]","24bc834b":"from sklearn.linear_model import Ridge, RidgeCV\n","e7fbfcba":"#K-fold Cross Validation to Choose the Best Model\nridgecv = RidgeCV(alphas = lambda_grid, cv = 10, normalize = True)\nridgecv.fit(X_train, logy_train)\nridgecv.alpha_","0df671b4":"model2 = Ridge(normalize = True, alpha = ridgecv.alpha_)\nmodel2.fit(X_train, logy_train)\npred_model2 = model2.predict(X_test)\npred_model2 = np.exp(pred_model2)\nabs_err_model2 = np.abs(pred_model2 - y_test)\nnp.mean(abs_err_model2)","bd835e38":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model2.mean(),\n                                             'Median of AbsErrors' : abs_err_model2.median(),\n                                             'SD of AbsErrors' :     abs_err_model2.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model2),\n                                             'Min of AbsErrors':     abs_err_model2.min(),\n                                             'Max of AbsErrors':     abs_err_model2.max()}, index = ['Model2: Ridge']), \n                               ignore_index = False)\n\nmodel_comp","386fc8ac":"y_pred = pred_model2\ny_test = y_test\nmodel = 'model 2 Ridge Regression'\n\nplt.figure(figsize=(15, 15), dpi = 150);\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25);\n\nplt.subplots(figsize=(16,6));\nplt.scatter(x = y_test, y = y_pred, color = 'blue');\n#Add 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.xlabel('Actual Value');\nplt.ylabel('Predicted Value');\nprint(f'The r2-score is: {r2_score(y_pred, y_test)}')\nplt.title(f'{model}');\nplt.subplots(figsize=(17,6));\n\nx_points = list(range(len(y_test)));\nplt.plot(x_points, y_test, label='y_real', color = 'blue');\nplt.plot(x_points, y_pred, label='y_predict', color = 'red');\nplt.legend();\nplt.grid()\nplt.title(f'{model}');\nplt.show();","40740042":"abs_err = abs_err_model2\nname = 'abs_err_model2 Ridge Reg'\n\nplt.figure(figsize=(15, 5), dpi = 200)\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25)\n\n\nplt.subplot(121)\nplt.boxplot(abs_err);\nplt.title(f'boxplot of {name}');\nplt.grid()\n\nplt.subplot(122)\nsns.distplot(abs_err, kde = False);\nplt.title(f'Histogram of {name}');\n","11feca31":"from sklearn.tree import DecisionTreeRegressor\nmodel3 = DecisionTreeRegressor(max_depth = 5, min_samples_leaf = 7, ccp_alpha = 0.001)\nmodel3 = model3.fit(X_train, logy_train)\npred_model3 = model3.predict(X_test)\npred_model3 = np.exp(pred_model3)\nabs_err_model3 = np.abs(pred_model3 - y_test)\nnp.mean(abs_err_model3)","efc21260":"# # HyperParameter Tuning\n# from sklearn.model_selection import RandomizedSearchCV\n\n# max_depth = [3, 4, 5, 6]\n# min_samples_leaf = [5, 7, 9, 11, 13]\n# ccp_alpha = [0.001, 0.02, 0.01, 0.06, 0.0001]\n# objective = ['reg:squarederror']\n\n# random_grid = {'max_depth': max_depth, \n#                'min_samples_leaf': min_samples_leaf, \n#                'ccp_alpha': ccp_alpha}\n\n# decision = RandomizedSearchCV(estimator = model3, param_distributions = random_grid, scoring = 'neg_mean_squared_error', n_iter = 10, cv = 8, random_state = 40, n_jobs = 1)\n# decision.fit(X_train, logy_train)\n# decision.best_params_","d255c195":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model3.mean(),\n                                             'Median of AbsErrors' : abs_err_model3.median(),\n                                             'SD of AbsErrors' :     abs_err_model3.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model3),\n                                             'Min of AbsErrors':     abs_err_model3.min(),\n                                             'Max of AbsErrors':     abs_err_model3.max()}, index = ['Model3: Tree']), \n                               ignore_index = False)\n\nmodel_comp","53a3754d":"y_pred = pred_model3\ny_test = y_test\nmodel = 'model 3 Tree'\n\nplt.figure(figsize=(15, 15), dpi = 150);\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25);\n\nplt.subplots(figsize=(16,6));\nplt.scatter(x = y_test, y = y_pred, color = 'blue');\n#Add 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.xlabel('Actual Value');\nplt.ylabel('Predicted Value');\n\nplt.title(f'{model}');\nplt.subplots(figsize=(17,6));\nprint(f'The r2-score is: {r2_score(y_pred, y_test)}')\nx_points = list(range(len(y_test)));\nplt.plot(x_points, y_test, label='y_real', color = 'blue');\nplt.plot(x_points, y_pred, label='y_predict', color = 'red');\nplt.legend();\nplt.grid()\nplt.title(f'{model}');\nplt.show();","e09f923d":"abs_err = abs_err_model3\nname = 'abs_err_model3_Decision Tree'\n\nplt.figure(figsize=(15, 5), dpi = 200)\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25)\n\n\nplt.subplot(121)\nplt.boxplot(abs_err);\nplt.title(f'boxplot of {name}');\nplt.grid()\n\nplt.subplot(122)\nsns.distplot(abs_err, kde = False);\nplt.title(f'Histogram of {name}');\n","6040108c":"from sklearn.ensemble import BaggingRegressor\nmodel4 = BaggingRegressor(n_estimators = 500, oob_score = True)\nmodel4.fit(X_train, logy_train)\ny_pred_model4 = model4.predict(X_test)\ny_pred_model4 = np.exp(y_pred_model4)\nabs_err_model4 = np.abs(y_pred_model4 - y_test)\nnp.mean(abs_err_model4)","8b521c01":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model4.mean(),\n                                             'Median of AbsErrors' : abs_err_model4.median(),\n                                             'SD of AbsErrors' :     abs_err_model4.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model4),\n                                             'Min of AbsErrors':     abs_err_model4.min(),\n                                             'Max of AbsErrors':     abs_err_model4.max()}, index = ['Model4: Bagging']), \n                               ignore_index = False)\n\nmodel_comp","bc9381cd":"y_pred = y_pred_model4\ny_test = y_test\nmodel = 'model 4 Bagging Regression'\n\nplt.figure(figsize=(15, 15), dpi = 150);\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25);\n\nplt.subplots(figsize=(16,6));\nplt.scatter(x = y_test, y = y_pred, color = 'blue');\n#Add 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.xlabel('Actual Value');\nplt.ylabel('Predicted Value');\n\nplt.title(f'{model}');\nplt.subplots(figsize=(17,6));\nprint(f'The r2-score is: {r2_score(y_pred, y_test)}')\nx_points = list(range(len(y_test)));\nplt.plot(x_points, y_test, label='y_real', color = 'blue');\nplt.plot(x_points, y_pred, label='y_predict', color = 'red');\nplt.legend();\nplt.grid()\nplt.title(f'{model}');\nplt.show();","77b10494":"abs_err = abs_err_model4\nname = 'abs_err_model4 Bagging'\n\nplt.figure(figsize=(15, 5), dpi = 200)\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25)\n\n\nplt.subplot(121)\nplt.boxplot(abs_err);\nplt.title(f'boxplot of {name}');\nplt.grid()\n\nplt.subplot(122)\nsns.distplot(abs_err, kde = False);\nplt.title(f'Histogram of {name}');\n","78e5feeb":"from sklearn.ensemble import RandomForestRegressor\nmodel5 = RandomForestRegressor(n_estimators = 1220,\n                               oob_score = True, \n                               max_features = 'sqrt', \n                               max_depth = 33, \n                               min_samples_split = 2, \n                               min_samples_leaf = 1)\n\nmodel5.fit(X_train, logy_train)\ny_pred_model5 = model5.predict(X_test)\ny_pred_model5 = np.exp(y_pred_model5)\nabs_err_model5 = np.abs(y_pred_model5 - y_test)\nnp.mean(abs_err_model5)","f8c9ef78":"# from sklearn.model_selection import RandomizedSearchCV\n# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 40)]\n# max_features = ['auto', 'sqrt']\n# max_depth = [int(x) for x in np.linspace(5, 40, num = 6)]\n# min_samples_split = [2, 5, 10, 15, 20,25,30,35,40,100]\n# min_samples_leaf = [1, 2, 5, 10]\n\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf}\n\n# rf_random = RandomizedSearchCV(estimator = model5, \n#                                param_distributions = random_grid, \n#                                scoring='neg_mean_squared_error',\n#                                n_iter = 10, \n#                                cv = 5, \n#                                verbose=2, \n#                                random_state=42, \n#                                n_jobs = 1)\n\n# rf_random.fit(X_train, logy_train);\n# rf_random.best_params_","55cab20c":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model5.mean(),\n                                             'Median of AbsErrors' : abs_err_model5.median(),\n                                             'SD of AbsErrors' :     abs_err_model5.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model5),\n                                             'Min of AbsErrors':     abs_err_model5.min(),\n                                             'Max of AbsErrors':     abs_err_model5.max()}, index = ['Model5: RadomForest']), \n                               ignore_index = False)\n\nmodel_comp","906edcec":"y_pred = y_pred_model5\ny_test = y_test\nmodel = 'model 5 RandomForest Regression'\n\nplt.figure(figsize=(15, 15), dpi = 150);\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25);\n\nplt.subplots(figsize=(16,6));\nplt.scatter(x = y_test, y = y_pred, color = 'blue');\n#Add 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.xlabel('Actual Value');\nplt.ylabel('Predicted Value');\n\nplt.title(f'{model}');\nplt.subplots(figsize=(17,6));\nprint(f'The r2-score is: {r2_score(y_pred, y_test)}')\nx_points = list(range(len(y_test)));\nplt.plot(x_points, y_test, label='y_real', color = 'blue');\nplt.plot(x_points, y_pred, label='y_predict', color = 'red');\nplt.legend();\nplt.grid()\nplt.title(f'{model}');\nplt.show();","588985ad":"abs_err = abs_err_model5\nname = 'abs_err_model5 RandomForest'\n\nplt.figure(figsize=(15, 5), dpi = 200)\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25)\n\n\nplt.subplot(121)\nplt.boxplot(abs_err);\nplt.title(f'boxplot of {name}');\nplt.grid()\n\nplt.subplot(122)\nsns.distplot(abs_err, kde = False);\nplt.title(f'Histogram of {name}');\n","b4429852":"from sklearn.ensemble import GradientBoostingRegressor\nmodel6 = GradientBoostingRegressor(learning_rate = 0.2, \n                                   max_depth = 6, \n                                   min_samples_leaf = 3, \n                                   n_estimators = 1000, \n                                   subsample = 1)\n\nmodel6.fit(X_train, logy_train)\ny_pred_model6 = model6.predict(X_test)\ny_pred_model6 = np.exp(y_pred_model6)\nabs_err_model6 = np.abs(y_pred_model6 - y_test)\nnp.mean(abs_err_model6)","011958e0":"# from sklearn.model_selection import RandomizedSearchCV\n\n# learning_rate = [0.01,0.1, 0.2, 0.3, 0.4, 0.5]\n# n_estimators = [500, 600, 700, 800, 900, 1000, 1100, 1200]\n# subsample = [1, 2, 3, 4]\n# max_depth = [2, 3, 4, 5, 6, 7]\n# min_samples_leaf = [3, 4, 5, 6, 7, 8]\n\n\n# random_grid = {'n_estimators': n_estimators,\n#                'max_depth': max_depth,\n#                'min_samples_leaf': min_samples_leaf, \n#                'learning_rate':learning_rate, \n#                 'subsample': subsample}\n\n# rf_random = RandomizedSearchCV(estimator = model6, \n#                                param_distributions = random_grid, \n#                                scoring='neg_mean_squared_error', \n#                                n_iter = 10, \n#                                cv = 5, \n#                                verbose=0, \n#                                random_state=42, \n#                                n_jobs = 1)\n\n# rf_random.fit(X_train, logy_train);\n# rf_random.best_params_","c5f185b2":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model6.mean(),\n                                             'Median of AbsErrors' : abs_err_model6.median(),\n                                             'SD of AbsErrors' :     abs_err_model6.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model6),\n                                             'Min of AbsErrors':     abs_err_model6.min(),\n                                             'Max of AbsErrors':     abs_err_model6.max()}, index = ['Model6: Gradient Boost']), \n                               ignore_index = False)\n\nmodel_comp","be2e2d51":"from sklearn.metrics import r2_score\n\n\ny_pred = y_pred_model6\ny_test = y_test\nmodel = 'model 6 Gradient Boost Regression'\n\nplt.figure(figsize=(15, 15), dpi = 150);\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25);\n\nplt.subplots(figsize=(16,6));\nplt.scatter(x = y_test, y = y_pred, color = 'blue');\n#Add 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.xlabel('Actual Value');\nplt.ylabel('Predicted Value');\n\nplt.title(f'{model}');\nplt.subplots(figsize=(17,6));\nprint(f'The r2-score is: {r2_score(y_pred, y_test)}')\nx_points = list(range(len(y_test)));\nplt.plot(x_points, y_test, label='y_real', color = 'blue');\nplt.plot(x_points, y_pred, label='y_predict', color = 'red');\nplt.legend();\nplt.grid()\nplt.title(f'{model}');\nplt.show();","a37ed7ed":"abs_err = abs_err_model6\nname = 'abs_err_model6 Gradient Boost'\n\nplt.figure(figsize=(15, 5), dpi = 200)\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25)\n\n\nplt.subplot(121)\nplt.boxplot(abs_err);\nplt.title(f'boxplot of {name}');\nplt.grid()\n\nplt.subplot(122)\nsns.distplot(abs_err, kde = False);\nplt.title(f'Histogram of {name}');\n","65678633":"from xgboost import XGBRegressor\nmodel7 = XGBRegressor(n_estimators = 500, \n                       max_depth = 6, \n                       learning_rate = 0.01, \n                       subsample = 1, \n                       colsample_bytree = 0.3, \n                       reg_alpha = 0.04, \n                       reg_lambda = 0.05, \n                       n_jobs = -1, \n                       random_state = 1234)\n\nmodel7 = model7.fit(X_train, logy_train)\npred_model7 = model7.predict(X_test)\npred_model7 = np.exp(pred_model7)\nabs_err_model7 = np.abs(pred_model7 - y_test)\nnp.mean(abs_err_model7)","07365cf5":"# from sklearn.model_selection import RandomizedSearchCV\n\n# n_estimators = [500, 550, 600, 650, 620, 670, 700, 800]\n# max_depth = [5, 6, 7, 4]\n# learning_rate = [0.01, 0.015, 0.02, 0.001, 0.003]\n# subsample = [1, 2, 3, 4]\n# colsample_bytree = [0.3, 0.33, 0.39, 0.4, 0.45, 0.5, 0.6]\n# reg_alpha = [0.05, 0.06, 0.07, 0.04, 0.03]\n# reg_lambda = [0.05, 0.06, 0.07, 0.055, 0.052]\n\n\n\n\n# random_grid = {'n_estimators': n_estimators,\n#                'max_depth': max_depth, \n#                'learning_rate':learning_rate, \n#                'subsample': subsample, \n#                'colsample_bytree': colsample_bytree, \n#                'reg_alpha': reg_alpha, \n#                'reg_lambda': reg_lambda}\n\n# xgb_tuning = RandomizedSearchCV(estimator = model7, \n#                                param_distributions = random_grid, \n#                                scoring='neg_mean_squared_error', \n#                                n_iter = 10, \n#                                cv = 5, \n#                                verbose=0, \n#                                random_state=42, \n#                                n_jobs = 1)\n\n\n\n\n# xgb_tuning.fit(X_train, logy_train)\n# xgb_tuning.best_params_","b698e429":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model7.mean(),\n                                             'Median of AbsErrors' : abs_err_model7.median(),\n                                             'SD of AbsErrors' :     abs_err_model7.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model7),\n                                             'Min of AbsErrors':     abs_err_model7.min(),\n                                             'Max of AbsErrors':     abs_err_model7.max()}, index = ['Model7: XGradient Boost Reg']), \n                               ignore_index = False)\n\nmodel_comp","c423710b":"import catboost as cb\ntrain_dataset = cb.Pool(X_train, logy_train)\ntest_dataset = cb.Pool(X_test, y_test)","d423bbd8":"\nmodel8 = cb.CatBoostRegressor(loss_function = 'RMSE')\ngrid = {'iterations': [100, 150, 200], \n        'learning_rate': [0.03, 0.1], \n        'depth': [2, 4, 6, 8], \n        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n\nmodel8.grid_search(grid, train_dataset, verbose=0)","e8b8a5d0":"y_pred_model8 = model8.predict(X_test)\ny_pred_model8 = np.exp(y_pred_model8)\nabs_err_model8 = np.abs(y_pred_model8 - y_test)\nnp.mean(abs_err_model8)","53ae1419":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model8.mean(),\n                                             'Median of AbsErrors' : abs_err_model8.median(),\n                                             'SD of AbsErrors' :     abs_err_model8.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model8),\n                                             'Min of AbsErrors':     abs_err_model8.min(),\n                                             'Max of AbsErrors':     abs_err_model8.max()}, index = ['Model8: CatBoost Reg']), \n                               ignore_index = False)\n\nmodel_comp","41df6562":"from lightgbm import LGBMRegressor\nmodel9 = LGBMRegressor()  \nmodel9.fit(X_train, logy_train)\ny_pred_model9 = model9.predict(X_test)\ny_pred_model9 = np.exp(y_pred_model9)\nabs_err_model9 = np.abs(y_pred_model9 - y_test)\nnp.mean(abs_err_model9)\n","8b82d2da":"model_comp = model_comp.append(pd.DataFrame({'Mean of AbsErrors':    abs_err_model9.mean(),\n                                             'Median of AbsErrors' : abs_err_model9.median(),\n                                             'SD of AbsErrors' :     abs_err_model9.std(),\n                                             'IQR of AbsErrors':     iqr(abs_err_model9),\n                                             'Min of AbsErrors':     abs_err_model9.min(),\n                                             'Max of AbsErrors':     abs_err_model9.max()}, index = ['Model9: LightBoost Reg']), \n                               ignore_index = False)\n\nmodel_comp","760bd9ba":"abs_err = abs_err_model9\nname = 'abs_err_model9 Light Boost'\n\nplt.figure(figsize=(15, 5), dpi = 200)\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25)\n\n\nplt.subplot(121)\nplt.boxplot(abs_err);\nplt.title(f'boxplot of {name}');\nplt.grid()\n\nplt.subplot(122)\nsns.distplot(abs_err, kde = False);\nplt.title(f'Histogram of {name}');\n","ef627369":"from sklearn.metrics import r2_score\n\n\ny_pred = y_pred_model9\ny_test = y_test\nmodel = 'model 9 light Boost Regression'\n\nplt.figure(figsize=(15, 15), dpi = 150);\nplt.subplots_adjust(hspace = 0.25, wspace = 0.25);\n\nplt.subplots(figsize=(16,6));\nplt.scatter(x = y_test, y = y_pred, color = 'blue');\n#Add 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.xlabel('Actual Value');\nplt.ylabel('Predicted Value');\n\nplt.title(f'{model}');\nplt.subplots(figsize=(17,6));\nprint(f'The r2-score is: {r2_score(y_pred, y_test)}')\nx_points = list(range(len(y_test)));\nplt.plot(x_points, y_test, label='y_real', color = 'blue');\nplt.plot(x_points, y_pred, label='y_predict', color = 'red');\nplt.legend();\nplt.grid()\nplt.title(f'{model}');\nplt.show();","de7b9e20":"# model3: Decision Tree","6dd675d5":"# Model 2: Ridge Regression\nRidge Regression:\n\nThe goal is to optimize:\n\nRSS + lambda * Sum( beta_i ^ 2)\n\nlambda => 0,  a tuning parameter","5936417e":"# Model 4: Bagging","55cdfa44":"# Data Preparation","50c256c0":"# Model 7: XGradient Boost","fd1c3dc0":"# Model8: Cat Boost","342e3b24":"## In my opinion light boost and Gradient Boost approach good for this problem","8807bc5c":"## Model 1: Regression Model","eb17fd7d":"# Histogram Plot","8f5a0bf3":"# Load DataSets","e8e90254":"# Model 6 : Gradient Boost Regression","f6deb95a":"# Model 5: RandomForest","e273ade4":"# Model9: Light Boost"}}