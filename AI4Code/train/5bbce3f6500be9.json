{"cell_type":{"c5cb50b5":"code","f4559612":"code","05204715":"code","9de0cda6":"code","241c7fda":"code","85fb610e":"code","34aaee32":"code","2a62186b":"code","470dc36e":"code","6b592883":"code","23cae604":"code","468e1a94":"code","abc0cc3e":"code","6b28ce35":"code","cd78fe4f":"code","674059ce":"code","a92e3a56":"code","913f0e75":"code","26359dfb":"code","0aaed450":"code","924f5abe":"code","58dca62a":"code","f3b2e7e8":"code","8e26bd4f":"code","311ac62a":"code","ec94bc1b":"code","ebdce855":"code","e9fb060c":"code","cdb88b8f":"code","bf159bc7":"code","d12cef5c":"code","47b3e945":"code","8664b5d7":"code","1a24a72d":"code","695f0971":"code","511bdc07":"code","4d0c4d42":"code","55673ae3":"code","e1fff686":"code","14792707":"code","6cd3aca8":"code","93e4b3ca":"code","5ffad2e0":"markdown","8b8047ba":"markdown","54efbae7":"markdown","c83648bb":"markdown","1b5b5162":"markdown","f16f5099":"markdown","1558d435":"markdown","a1ec2287":"markdown","b6ba505a":"markdown","248490e7":"markdown","e1413b47":"markdown","09baef7c":"markdown","2395440c":"markdown","1353a0e4":"markdown","3cd47df0":"markdown","b78b4991":"markdown","f1000900":"markdown","f85a9462":"markdown","1455a91c":"markdown","a2252fa4":"markdown","1c50878a":"markdown","2976684d":"markdown","9fc78b72":"markdown","5744f361":"markdown","984674db":"markdown","61a1c219":"markdown","093e2ac5":"markdown","29415de1":"markdown","93f7d6ba":"markdown","d85ddc70":"markdown","7e0506e5":"markdown","5cef0a4d":"markdown","17b901b1":"markdown","b9cbbef0":"markdown","8b877a81":"markdown","231489eb":"markdown","e5dc8f8b":"markdown","cd98d5e6":"markdown","7bb9187f":"markdown","e6903410":"markdown","7a2cf9e8":"markdown","25b4b129":"markdown","7e13c759":"markdown","290487f3":"markdown","fbb80ac6":"markdown","1d0b4acc":"markdown","126aaff1":"markdown"},"source":{"c5cb50b5":"!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java","f4559612":"import os\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","05204715":"!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","9de0cda6":"COVID_INDEX = '..\/input\/luceneindexcovidparagraph20200410\/lucene-index-covid-paragraph-2020-04-10'","241c7fda":"searcher = pysearch.SimpleSearcher(COVID_INDEX)","85fb610e":"def get_articles(query):\n    hits = searcher.search(query)\n    #print(len(hits))\n    # Prints the first 10 hits\n    return hits","34aaee32":"query = 'range of incubation periods for COVID-19'\nhits = get_articles(query)\nfor i in range(0, 10):\n    #print some relevant fields\n    print(f'{i+1} {hits[i].docid} {hits[i].score} {hits[i].lucene_document.get(\"title\")} {hits[i].lucene_document.get(\"doi\")}')","2a62186b":"hits[0].contents.split('\\n')","470dc36e":"import json\ndef get_para_results(query):\n    hits = searcher.search(query,10) \n    print(len(hits))\n    temp = {} # to store the doi of the articles being returned so we know if the article is repeated\n    i = 0\n    output = []\n    while i<len(hits) and i<10:\n        outJson = {}\n        outJson['rank'] = i+1\n        # check if the current article has a paragraph returned or not ('has_full_text' in the dataset)\n        if '.' in hits[i].docid:\n            doc_id = hits[i].docid.split('.')[0]\n            para_id = hits[i].docid.split('.')[1]\n            doi = hits[i].lucene_document.get('doi')\n            paragraph = {}\n            paragraph['score'] = hits[i].score\n            paragraph['text'] = hits[i].contents.split('\\n')[-1] # get the last element, since the contents are sorted as [title, abstract, paragraph]\n            paragraph['id'] = para_id\n            # check if the doi (same article) has not appeared before in the list\n            if doi not in temp:\n                outJson['abstract'] = hits[i].lucene_document.get('abstract') # include abstract if new article\n                article_data = json.loads(searcher.doc(doc_id).lucene_document().get('raw')) # get all the relevant data from the dataset \n                if 'body_text' in article_data:\n                    outJson['body_text'] = article_data['body_text'] # include 'body_text' in case needed later\n                temp[doi] = i\n            outJson['paragraphs'] = []\n            outJson['paragraphs'].append(paragraph)\n        else:\n            # no paragraph present, which means article does not have full text available\n            outJson['abstract'] = hits[i].lucene_document.get('abstract')\n            outJson['score'] = hits[i].score\n        outJson['title'] = hits[i].lucene_document.get('title')\n        outJson['sha'] = hits[i].lucene_document.get('sha')\n        outJson['doi'] = hits[i].lucene_document.get('doi')\n        output.append(outJson)\n        i+=1\n    return output","6b592883":"query = 'range of incubation periods for COVID-19'\ni = 1\nfor item in get_para_results(query):\n    if i>10:\n        break\n    print(item)\n    i+=1","23cae604":"def information_retrieval(file_name, topk = 10):\n\n    with open(file_name) as f:\n        json_file = json.load(f)\n    subtasks = json_file[\"sub_task\"]\n    \n    all_results = []\n    data_for_qa = []\n    for item in subtasks:\n        questions = item[\"questions\"]\n        for query in questions:\n            result_item = {\"question\" : query}\n            retri_result = get_para_results(query)\n            result_item[\"data\"] = retri_result\n\n            qa_item = {\"question\": query}\n            context = []\n            titles = []\n            doi = []\n            count = 1\n            for item in retri_result:\n                if count>topk:\n                    break\n                if 'abstract' in item and len(item['abstract']) > 0:\n                    context.append(item['abstract'])\n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n                if 'paragraphs' in item:\n                    context.append(item['paragraphs'][0]['text'])   \n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n\n            qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n            all_results.append(result_item)\n            data_for_qa.append(qa_item)\n\n    return data_for_qa\n\ndef parse_ir_results(query, retri_result, topk = 10):\n    all_results = []\n    data_for_qa = []\n    qa_item = {\"question\": query}\n    result_item = {\"question\" : query}\n    result_item[\"data\"] = retri_result\n    context = []\n    titles = []\n    doi = []\n    count = 1\n    for item in retri_result:\n        if count>topk:\n            break\n        if 'abstract' in item and len(item['abstract']) > 0:\n            context.append(item['abstract'])\n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n        if 'paragraphs' in item:\n            context.append(item['paragraphs'][0]['text'])   \n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n    qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n    all_results.append(result_item)\n    data_for_qa.append(qa_item)    \n\n    return all_results, data_for_qa\n\n    \ndef information_retrieval_query(query):\n\n    retri_result = get_para_results(query)\n    all_results, data_for_qa = parse_ir_results(query, retri_result ,topk = 20)\n    \n    return all_results, data_for_qa","468e1a94":"### 3.1 install the prerequisite\nimport os\nimport sys\nimport json\n\n!pip uninstall tensorflow -y\n!pip uninstall tensorflow-gpu -y\n!pip install tensorflow==1.13.1\n!pip install caireCovid==0.1.8","abc0cc3e":"import tensorflow as tf\nimport caireCovid\nfrom caireCovid import QaModule\nfrom caireCovid.qa_utils import stop_words\nimport math","6b28ce35":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","cd78fe4f":"### 3.2 Check all version\nprint(tf.__version__)","674059ce":"# QA System\nclass QA_System():\n    def _init_(self):\n        # Load the QA models. Please refer to [Github](https:\/\/github.com\/yana-xuyan\/caireCovid) for details.\n        self.model = QaModule(['mrqa', 'biobert'], [\"\/kaggle\/input\/pretrained-qa-models\/mrqa\/1564469515\", \"\/kaggle\/input\/pretrained-qa-models\/biobert\/1585470591\"], \\\n                              \"\/kaggle\/input\/xlnetlargecased\/xlnet_cased_L-24_H-1024_A-16\/spiece.model\", \"\/kaggle\/input\/pretrained-qa-models\/bert_config.json\", \\\n                              \"\/kaggle\/input\/bert-base-cased\/vocab.txt\")\n    def getAnswer(self, query):\n        _, data_for_qa = information_retrieval_query(query)\n        answers =  self.model.getAnswers(data_for_qa)\n        return answers\n    def getAnswers(self, filename):\n        _, data_for_qa = information_retrieval(query)\n        answers = self.model.getAnswers(data_for_qa)\n        return answers\n    def makeFormatAnswers(self, answers):\n        format_answers = []\n        for i in range(len(answers[0]['data']['answer'])):\n                format_answer = {}\n                format_answer['question'] = answers[0]['question']\n                format_answer['answer'] = answers[0]['data']['answer'][i]\n                format_answer['context'] = answers[0]['data']['context'][i]\n                format_answer['doi'] = answers[0]['data']['doi'][i]\n                format_answer['title'] = answers[0]['data']['title'][i]\n                format_answer[\"confidence\"] = answers[0]['data']['confidence'][i]\n                format_answer[\"raw\"] = answers[0]['data']['raw'][i]\n                format_answers.append(format_answer)\n        return format_answers\n\ndef get_QA_answer_api(query):\n    url = \"http:\/\/eez114.ece.ust.hk:5000\/query_qa\"\n    payload = \"{\\n\\t\\\"text\\\": \\\"\"+query+\"\\\"\\n}\"\n    headers = {\n        'Content-Type': \"application\/json\",\n        'cache-control': \"no-cache\",\n        'Postman-Token': \"696fa512-5fed-45ca-bbe7-b7a1b4d19fe4\"\n    }\n    response = requests.request(\"POST\", url, data=payload, headers=headers)\n    response = response.json()\n    return response","a92e3a56":"import argparse\nimport sys\nimport pandas as pd\nimport csv\nimport requests\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.tokenize import sent_tokenize # use sentence tokenize\nfrom IPython.core.display import display, HTML","913f0e75":"from nltk import word_tokenize, pos_tag, sent_tokenize\nfrom caireCovid.qa_utils import stop_words\nstop_words.append('including')\n\ndef rankAnswers(answers):\n    for item in answers:\n        query = item[\"question\"]\n        context = item['context']\n        # make new query with only n. and adj.\n        tokens = word_tokenize(query.lower())\n        tokens = [word for word in tokens if word not in stop_words]\n        tagged = pos_tag(tokens)\n        query_token = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n        text = context.lower()\n        count = 0\n        text_words = word_tokenize(text)\n        for word in text_words:\n            if word in query_token:\n                count += 1\n            \n        match_number = 0\n        for word in query_token:\n            if word == 'covid-19':\n                continue\n            if word in text_words:\n                match_number += 1\n        matching_score = count \/ (1 + math.exp(-len(text_words)+50))\/ 5 + match_number*10\n        item['matching_score'] = matching_score\n        item['rerank_score'] = matching_score + 0.5 * item['confidence']\n    \n    # sort QA results\n    answers.sort(key=lambda k: k[\"rerank_score\"], reverse=True)\n#     print([item['rerank_score'] for item in answers])\n    return answers\n\ndef highlight_qaresult(qaresult):\n    if qaresult == []:\n        print('API broken')\n        return 1\n    ## tokenize query\n    query = qaresult[0]['question']\n    query_tokens = word_tokenize(query.lower())\n    query_tokens = [word for word in query_tokens if word not in stop_words]\n    tagged = pos_tag(query_tokens)\n    query_tokens = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n    ## highlihgt answer\n    for i in range(len(qaresult)):\n        context_1 = \"<style type='text\/css'>mark { background-color:yellow; color:black; } <\/style>\"\n        golden = qaresult[i]['answer']\n        context = qaresult[i]['context']\n        context_sents = sent_tokenize(context)\n        golden_sents = sent_tokenize(golden)\n        for sent in context_sents:\n            if sent not in golden:\n                context_1 += sent\n            else:\n                context_1 += \"<mark>\"\n                for word in sent.split():\n                    word_tokens = word_tokenize(word)\n                    if len(word_tokens) > 1:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \"<\/b>\"\n                            else:\n                                context_1 = context_1 + j\n                        context_1 = context_1 + \" \"\n                    else:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \" <\/b>\"\n                            else:\n                                context_1 = context_1 + j + \" \"\n                context_1 += \" <\/mark>\"\n        qaresult[i]['context'] = context_1\n    return qaresult\n\ndef display_QA(result):\n    result = highlight_qaresult(result)\n    pdata = []\n    count = 0\n    for i in range(len(result)):\n        count += 1\n        line = []\n        context_1 = \"<div> \"\n        context = result[i]['context']\n        context_1 = context_1 + context\n        context_1 += \" <\/div>\"\n        line.append(context_1)\n        context_2 = '<a href= \"https:\/\/doi.org\/'\n        context_2 += result[i]['doi']\n        context_2 += ' target=\"_blank\">'\n        context_2 += result[i]['title']\n        context_2 += '<\/a>'\n        line.append(context_2)\n        pdata.append(line)\n        if count > 5:\n            break\n    df = pd.DataFrame(pdata, columns = ['QA results', 'title'])\n    df = df.style.set_properties(**{'text-align': 'left','mark-color': 'red'})\n    display(df)","26359dfb":"!pip install easydict\n!pip install covidSumm==0.1.4\n!pip install fairseq","0aaed450":"import covidSumm\nimport requests\nimport json\nimport os\nimport argparse","924f5abe":"from covidSumm.abstractive_utils import get_ir_result, result_to_json, get_qa_result\nfrom covidSumm.abstractive_model import abstractive_summary_model\nfrom covidSumm.abstractive_config import set_config\nfrom covidSumm.abstractive_bart_model import *","58dca62a":"def get_summary_list(article_list, abstractive_model):\n    summary_list = []\n    for i in range(len(article_list)):\n        article = article_list[i]\n        summary_results = abstractive_model.generate_summary(article)\n        result = \"\"\n        for item in summary_results:\n            result += item.replace('\\n', ' ')\n        summary_list.append(result)\n    return summary_list\n\ndef get_answer_summary(query, abstractive_model):\n    paragraphs_list = get_qa_result(query, topk = 3)\n    answer_summary_list = abstractive_model.generate_summary(paragraphs_list)\n    answer_summary = \"\"\n    for item in answer_summary_list:\n        answer_summary += item.replace('\\n', ' ')\n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary\n    answer_summary_json['question'] = query\n    return answer_summary_json\n\ndef get_article_summary(query, abstractive_summary_model):\n    article_list, meta_info_list = get_ir_result(query, topk = 10)  \n    summary_list = get_summary_list(article_list, abstractive_summary_model)\n    summary_list_json = []\n    \n    for i in range(len(summary_list)):\n        json_summary = {}\n        json_summary = result_to_json(meta_info_list[i], summary_list[i])\n        summary_list_json.append(json_summary)\n\n    return summary_list_json\n\ndef get_bart_answer_summary_from_qa(query, qa_result, bart_model):\n    # we select top3\n    paragraphs_list = []\n    topk = 3\n\n    for i in range(topk):\n        if 'context' in qa_result[i].keys():\n            one_line = {}\n            one_line['src'] = qa_result[i]['context']\n            one_line['tgt'] = \"\"\n            paragraphs_list.append(one_line)\n    \n    answer_summary_list = bart_model.bart_generate_summary(paragraphs_list)\n    answer_summary_result = \"\"\n    for item in answer_summary_list:\n        answer_summary_result += item.replace('\\n', ' ')\n    \n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary_result\n    answer_summary_json['question'] = query\n    return answer_summary_json","f3b2e7e8":"args = set_config()\nargs['model_path'] = '\/kaggle\/input\/carieabssummmodel\/'\nsummary_model_1 = abstractive_summary_model(config = args)","8e26bd4f":"model_path = \"\/kaggle\/input\/bartsumm\/bart.large.cnn\"\nsummary_model_2 = Bart_model(model_path)","311ac62a":"from IPython.core.display import display, HTML\nimport pandas as pd\n\ndef display_summary(ans_summary_json, model_type):\n    question = ans_summary_json['question']\n    text = ans_summary_json['summary']\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+question+'<\/div>'\n    display(HTML(question_HTML))\n\n    execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>' + model_type + ' Abstractive Summary:<\/b>: '+text+'<\/div>'\n    display(HTML(execSum_HTML))\n\ndef display_article_summary(result, query):\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+query+'<\/div>'\n    pdata = []\n    abstract = \"\"\n    summary = \"\"\n    for i in range(len(result)):\n        if 'abstract' in result[i].keys():\n            line = []\n            context_2 = '<a href= \"https:\/\/doi.org\/'\n            context_2 += result[i]['doi']\n            context_2 += ' target=\"_blank\">'\n            context_2 += result[i]['title']\n            context_2 += '<\/a>'\n            line.append(context_2)\n            \n            abstract = \"<div> \" \n            abstract += result[i]['abstract']\n            abstract += \" <\/div>\"\n            line.append(abstract)\n            summary = \"<div> \" + result[i]['summary'] + \" <\/div>\"\n            line.append(summary)\n\n\n            pdata.append(line)\n    display(HTML(question_HTML))\n    df = pd.DataFrame(pdata, columns = ['Title','Abstract','Summary'])\n    HTML(df.to_html(render_links=True, escape=False))\n#     display(HTML(df.to_html(render_links=True, escape=False)))\n    df = df.style.set_properties(**{'text-align': 'left'})\n    display(df)\n","ec94bc1b":"query = \"How incubation period for COVID-19 varies across age?\"","ebdce855":"def run_example(query):\n    # Given one query, we retrieve the relevant paragraphs and feed the (paragraph, query) pairs into the QA system \n    qa_result = get_QA_answer_api(query)\n    # Answer Reranking\n    qa_result = rankAnswers(qa_result)\n    \n    # Input \"summary_model_2\" is the BART summarization model.\n    # Function \"get_bart_answer_summary\" is loaded from covidSumm.abstractive_bart_model\n    # Given one query, we take top-3 reranked paragraphs from the QA module and summarize them into one paragraph\n    answer_summary_2 = get_bart_answer_summary_from_qa(query, qa_result, summary_model_2)\n    display_summary(answer_summary_2, 'BART')\n    display_QA(qa_result)","e9fb060c":"run_example(query)","cdb88b8f":"query = \"What do we know about vaccines and therapeutics?\"\nrun_example(query)","bf159bc7":"query = \"What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?\"\nrun_example(query)","d12cef5c":"query = \"what the literature reports about effectiveness of drugs being developed and tried to treat COVID-19 patients?\"\nrun_example(query)","47b3e945":"query = \"what the literature reports about Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication?\"\nrun_example(query)","8664b5d7":"query = \"what the literature reports about Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients of covid-19?\"\nrun_example(query)","1a24a72d":"query = \"what do we know about using best animal models and their predictive value for a covid-19 human vaccine?\"\nrun_example(query)","695f0971":"query = \"what are the capabilities to discover a covid-19 therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents?\"\nrun_example(query)","511bdc07":"query = \"what do we know about alternative models to aid covid-19 decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up?\"\nrun_example(query)","4d0c4d42":"query = \"What are the approaches for expanding covid-19 production capacity to ensure equitable and timely distribution to populations in need?\"\nrun_example(query)","55673ae3":"query = \"Which efforts have been targeting at a universal coronavirus vaccine?\"\nrun_example(query)","e1fff686":"query = \"Which efforts have been developing animal models and standardize challenge studies?\"\nrun_example(query)","14792707":"query = \"Which efforts have been developing prophylaxis clinical studies and prioritize in healthcare workers?\"\nrun_example(query)","6cd3aca8":"query = \"which approaches have been evaluating risk for enhanced disease after vaccination?\"\nrun_example(query)","93e4b3ca":"query = \"What do we know about assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models?\"\nrun_example(query)","5ffad2e0":"# Project Description","8b8047ba":"# what do we know about alternative models to aid covid-19 decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up?","54efbae7":"Getting the pyserini library, which is anserini wrapped with python:","c83648bb":"# which approaches have been evaluating risk for enhanced disease after vaccination?","1b5b5162":"We use [Anserini](https:\/\/github.com\/castorini\/anserini) to create the search engine to retrieve a preliminary candidate set of documents. Anserini is an information retrieval module wrapped around the open source search engine **Lucene**. Although Lucene has been used widely to build industry search engine applications, its complex indexing and lack of documentation for ad hoc experimentation and testing on standard test sets, has made it less popular in the information retrieval community. Anserini uses the Lucene indexing to create an easy-to-understand module. Standard ranking algorithms(e.g. bag of words, BM25) have been implemented in the module, which enables us to use Lucene for our application. Thanks to Jimmy Lin, we make this platform based on his [notebook](https:\/\/github.com\/castorini\/anserini-notebooks\/blob\/master\/pyserini_covid19_paragraph.ipynb). Since the disk is not large enough for saving the whole dataset with index and other models, we use his API to get the information retrieval results.","f16f5099":"We can try running the function for the previous query, and check if the results are what we want.","1558d435":"For the question answering (QA) module, we have leveraged the BioBERT QA model which is finetuned on the SQuAD dataset and [our generalized QA model](http:\/\/https:\/\/github.com\/yana-xuyan\/caireCovid) for MRQA@EMNLP 2019 Shared Task[1]. Instead of fine-tuning the QA models on COVID-19 related datasets, we focus more on maintaining the generalization ability of our system so it can be easily applied to other similar tasks. For the MRQA model, we utilized six datasets, which vary from each other in terms of data source, context lengths, whether multi-hop reasoning is needed, strategies for data augmentation to reduce overfitting to the training data in order to enable generalization to out-of-domain data. Multi-task learning over six datasets is used to fine-tune large pre-trained language model XLNet[2] and it helped achieve promising results. To make the answers more readable, instead of providing small spans of answers, we provide the whole sentences and the surrounding context.  \n\nTo better evaluate the question answering results, we leverage the prediction probability of the QA models as the confidence score. The final answers of our system are re-ranked using this score as one of the factors, which will be talked about later in Section 2.2.\n \n\n[1]Su, Dan, et al. \"Generalizing Question Answering System with Pre-trained Language Model Fine-tuning.\" Proceedings of the 2nd Workshop on Machine Reading for Question Answering. 2019.\n[2]Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for language understanding.\" Advances in neural information processing systems. 2019.","a1ec2287":"# What do we know about assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]?","b6ba505a":"# Let's try now","248490e7":"## 1.2 Search Engine","e1413b47":"We now make another function to get the best matched paragraphs results from the dataset to our given query. If the article in concern does not have full text available then only the abstract is indexed. Since we know the doi field is unique to each article, we check if the article has already appeared before in the list returned. To avoid repetitions, we only include the 'abstract' and the 'body_text' fields form the dataset if the article is new and not a repeated article from before. The function is shown below:","09baef7c":"* We initiate our **Summerization model BART**.","2395440c":"In this project, for sake of efficiency, ***we build an API for the Search Engine module to integrate it with the Question Answering module into this system***.","1353a0e4":"# what the literature reports about Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication?","3cd47df0":"# 2. Relevant Snippet Selector\n## 2.1 Question Answering","b78b4991":"We can build the lucene index of the COVID-19 dataset from scratch, or get one of the pre-built indexes. Using the paragraph indexing which indexes each paragraph of an article (already uploaded the index as a dataset to use), can be downloaded from: https:\/\/www.dropbox.com\/s\/ivk87journyajw3\/lucene-index-covid-paragraph-2020-04-10.tar.gz","f1000900":"# what the literature reports about Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients of covid-19?","f85a9462":"# what are the capabilities to discover a covid-19 therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents?\n","1455a91c":"## 2.2 Highlights Generation","a2252fa4":"Write down whatever question you are interested in below. For example:","1c50878a":"# 3. Abstractive Summerization","2976684d":"# 1. Document retrieval\n## 1.1 Query Paraphrasing","9fc78b72":"# what the literature reports about effectiveness of drugs being developed and tried to treat COVID-19 patients?","5744f361":"The indexing is done based on each paragraph merged with the title and abstract. Given an article with id *doc_id*, the index will be as follows:\n* *doc_id* : title + abstract\n* *doc_id.00001* : title + abstract + 1st paragraph\n* *docid.00002*: title + abstract + 2nd paragraph\n* *docid.00003*: title + abstract + 3rd paragraph","984674db":"Automatic text summarization is a common problem in machine learning and natural language processing (NLP). Basically there are two main types of how to summarize text in NLP:\n* **Extraction-based summarization**, which involves pulling key phrases from the source document and combining them to make a summary, and;\n* **Abstraction-based summarization**, which creates new phrases and sentences that relay the most useful information from the original text \u2014 just like humans do. \nIn general, the abstractive method is a much harder task but performs better than an extractive method.\n\nIn our project, considering the requirements that people may still want to further read each paragraph containing the predicted QA answer spans, we summarize the top-k  (top-3) paragraphs that QA module passes, to generate a **paragraph-level abstractive summary**. \n\nOur model is based on two different abstractive summarization models: [Unilm](https:\/\/github.com\/microsoft\/unilm\/tree\/master\/s2s-ft) and [BART](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/bart), both of which have obtained SOTA results on the summarization tasks ([CNN\/DM datasets](https:\/\/cs.nyu.edu\/~kcho\/DMQA\/), and [XSUM](https:\/\/github.com\/EdinburghNLP\/XSum\/tree\/master\/XSum-Dataset) data). UniLM model is a unified pre-trained model for language understanding and generation. BART is a sequence-to-sequence model trained with denoising as a pre-training objective for language generation, translation, and comprehension.\n\nWe fine-tuned the UniLM model using [SumOnGraph](https:\/\/github.com\/coshiang\/SumOnGraph) biology dataset which includes literature for 5 types of diseases including Cancer, Cardiovascular Disease, Diabetes, Allergy, and Obesity. Original data is from PubMed which is a free resource supporting the search and retrieval of biomedical and life sciences literature with the aim of improving health\u2013both globally and personally. We used the BART model fine-tuned on CNN\/DailMail dataset. \n\nWe generate a summary for each answer-related paragraph from the QA module, then concatenate them directly to form our final **paragraph-level** answer summary.\n\n*(Actually we also implement **article-level summary**, even though in this Kaggle project, for the simplicity and legibility, we only display **the paragraph-level results** of the summarization models. It takes the whole article as input, and generate a summary for each sections (eg. Introductions section, Methodologies section)  of the articles, and then concatenate them together as a more fine-grained **article-level summary**, as complementary to the abstracts. You can refer to this [github repository](https:\/\/github.com\/Iamfinethanksu\/covidSumm) for more details.)*\n\nIf anyone is interested in our article-level results, please utilize ```summary = get_article_summary(query, abstractive_summary_model)``` to obtain the results from our system. ","61a1c219":"Let's write some code to print the results, which are top 10 articles matching a given query, along with the best matched paragraph. We are printing some of the fields corresponding to each article, a complete list of fields can be found [here](https:\/\/github.com\/castorini\/anserini\/blob\/master\/src\/main\/java\/io\/anserini\/index\/generator\/CovidGenerator.java#L46).","093e2ac5":"* install the prerequisite packages\n(The covidSumm packages are from [here](https:\/\/github.com\/Iamfinethanksu\/covidSumm).","29415de1":"### 1.2.1 install Python dependencies and pre-built index","93f7d6ba":"The objective of this sub-module is to break down a user\u2019s query and rephrase complex query sentences into several shorter and simpler questions that convey the same meaning.  In this way,  the search engine and the question answering modules will be able to find more relevant and less redundant results. \n\nCurrently, we manually convert the Task4 into several questions that we want to answer:\n\n* What do we know about vaccines and therapeutics? \n* What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?\n* what the literature reports about effectiveness of drugs being developed and tried to treat COVID-19 patients?\n* what the literature reports about Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication?\n* what the literature reports about Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients of covid-19?\n* what do we know about using best animal models and their predictive value for a covid-19 human vaccine?\n* what are the capabilities to discover a covid-19 therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents?\n* what do we know about alternative models to aid covid-19 decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up?\n* What are the approaches for expanding covid-19 production capacity to ensure equitable and timely distribution to populations in need?\n* Which efforts have been targeting at a universal coronavirus vaccine?\n* Which efforts have been developing animal models and standardize challenge studies?\n* Which efforts have been developing prophylaxis clinical studies and prioritize in healthcare workers?\n* which approaches have been evaluating risk for enhanced disease after vaccination?\n* What do we know about assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]?","d85ddc70":"# Which efforts have been developing prophylaxis clinical studies and prioritize in healthcare workers?\n","7e0506e5":"# Which efforts have been developing animal models and standardize challenge studies?","5cef0a4d":"The first two are title and abstract, and the last element is the matched paragraph. We can see that the paragraph matched is actually quite good at answering the query. The next step would be to get an answer in a concised form by passing the matched paragraphs to a QA model.","17b901b1":"# What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?","b9cbbef0":"# what do we know about using best animal models and their predictive value for a covid-19 human vaccine?","8b877a81":"### Please try it!","231489eb":"# What are the approaches for expanding covid-19 production capacity to ensure equitable and timely distribution to populations in need?\n","e5dc8f8b":"![image.png](attachment:image.png)\nIn this section, we will elaborate on the building blocks of each module in our system.\n \n1) Document Retriever\n+ **Query Paraphrasing**: It converts a long\/complicated query from a user to several shorter and simpler questions for search;\n+ **Search Engine**: We use Anserini with Lucene to retrieve publications from the candidate pool with high coverage. \n \n2) Relevant Snippet Selector: \n+ **Question Answering(QA)**: This sub-module looks for and integrates evidence from one or multiple paragraphs. We leverage an ensemble of two neural-based QA models which are pre-trained on SQuAD style QA datasets. Here we consider the QA module as a supporting fact selector to provide relevant snippets from the retrieved documents.\n+ **Answer Re-ranking & Highlight Generation**:  We rerank the retrieved result by a word matching score based on part-of-speech tagging as well as the QA system confidence score. We also highlight the answer span in order to enable easier reading of the QA results.\n\n3) Multi-document Summarizer:\n+ **Abstractive Summarization**: Another output of our system is an abstractive summary that synthesizes the answer from multiple retrieved snippets. This step aims to generate short pieces of fluent summaries based on the top relevant results. Using the neural-based summarizer, we generate summaries to improve the legibility of the results and help the user to have an overview of the relevant snippets in a short time.","cd98d5e6":"# What do we know about vaccines and therapeutics?","7bb9187f":"We can see some repititions in the results above. This can either be due to multiple paragraphs in the same article being matched with the query, or one article appearing more than once in the CORD-19 dataset, due to different sources. Let's now try printing out the actual paragraph that is being matched with each of the returned hits. First we print out contents of the indexing of the first hit for example:","e6903410":"# System Architecture Overview","7a2cf9e8":"# Which efforts have been targeting at a universal coronavirus vaccine?","25b4b129":"### 1.2.2 example for using Anserini\nHere we type query 'range of incubation periods for COVID-19' in the search engine and it will return the top10 revelant items Anserini gets in the dataset.","7e13c759":"To accelerate our QA system, instead of running corresponding process in notebook, we leverage an API to make better use of the local GPUs. We also make our QA system public to the community by making python package for downloading with the following command:\n> pip install caireCovid\n\nFor anyone who may be interested in the implementation details, lease refer to our [Github repository](https:\/\/github.com\/yana-xuyan\/caireCovid). Some examples and other useful resources are also available there.","290487f3":"* Now we initiate our **Summerization model UniLM**.","fbb80ac6":"In this part, we introduce the word matching highlight strategy. There are two main components in this part: (1) word matching score calculate. (2) rerank and display. The input is the Question Answering result and for the output we display the most relevant paragraph with the highlighted answer.\n+ **POS-tag based scoring:** \nWe calculate a similarity score between a QA result and a given query based on keyword matching. To obtain this score, we first select important keywords based on POS-tagging - we consider words with {NN(noun), VB (verb), JJ (adjective)} POS-tags to be important keywords. Based on the set of filtered keywords, we count the word-match between QA-result keywords and query keywords. Higher the count is, more similar the QA-result and the query are. To penalize the matching scores of short retrieved paragraphs, we normalize them with sigmoid value computed from paragraph length. Moreover, we reward the paragraphs with more diverse keywords from query, which is the major factor of matching scores.\n\n+ **rerank and display:**\nThe re-ranking score is based on both the word matching score above and the confidence score from the QA system. The QA results are again ranked and displayed. ","1d0b4acc":"Following the lucene+answerini information retrieval as described in: https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md\n\nSetting up JAVA sdk 11 first:","126aaff1":"In response to the COVID-19 pandemic, a lot of scholarly articles have been published recently and made freely available. At the same time, there are emerging requests from both the medical research community and the broader society to find answers to various questions regarding COVID-19. A system that can provide reliable answers to the COVID-19 related questions from the latest academic resources is crucial, especially for the medical community in the current time-critical race to treat patients and to find a cure for the virus. \n \nTo address the aforementioned requests by the medical community, we propose a machine learning-based system that uses state-of-the-art natural language processing(NLP) question answering(QA) techniques combined with summarization for mining the available scientific literature. The system is an end-to-end neural network-based open-domain QA system that can answer COVID-19 related questions, such as those questions proposed in the COVID-19 Kaggle task. Through our system, users can get two versions of the outcome:\n1. A ranked list of relevant snippets from the literature given a query;\n1. A fluent summary of the relevant results. We provide the paragraph-level summaries, which takes the paragraphs where the top three relevant snippets are located as input, to enable a more efficient way of understanding of the content.\n \nOur system consists of three different modules: **1) Document Retriever, 2) Relevant Snippet Selector, and 3) Multi-Document Summarizer**. The first module pre-processes a user\u2019s query and retrieves the most relevant k number of academic publications. The second module outputs a list of the most relevant answer snippets from the retrieved documents. It also highlights the relevant keywords. The last module is for generating the second output, namely a concise summary of the top-ranked retrieved relevant paragraphs in the two previous modules.\n\nWe have launched our [CAiRE-Covid website](https:\/\/caire.ust.hk\/covid), which showcases our results for each user query in real-time, so people can further experiment with our system.\n"}}