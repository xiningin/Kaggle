{"cell_type":{"cae9b1c6":"code","ca28d5d4":"code","f32bce61":"code","f60a7358":"code","feaae85e":"code","7c026e5e":"code","909e8ba5":"code","e498a8de":"code","abfc1bd6":"code","742aa14c":"code","cb087d7e":"code","c2ceb186":"code","2222e60e":"code","f7b683f7":"code","4b3e4629":"code","2f292abe":"code","4603a2fb":"code","fbd6a1fd":"code","b3662f22":"code","9562c058":"code","6256e41b":"code","97387859":"code","a8f626a0":"code","5aa9f47f":"code","df412f55":"code","4f72a127":"code","2aaad261":"code","843e6d1e":"code","009b28c7":"code","afd53b60":"code","c23d4cbb":"code","6810ec8b":"code","29d3dba3":"code","54bc3c61":"code","87377773":"code","eb2e28ce":"code","baeb888f":"code","eeaa7f1a":"code","4b366ad8":"markdown","dbbc9078":"markdown","38d6a5c7":"markdown","4de3201a":"markdown","637b9f63":"markdown"},"source":{"cae9b1c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n#tf.enable_eager_execution()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ca28d5d4":"os.getcwd()","f32bce61":"os.listdir()","f60a7358":"import tensorflow as tf\nimport pathlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom IPython.core.display import Image\nfrom IPython.display import display\n#print(tf.__version__)\n%matplotlib inline","feaae85e":"##### Read data\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nsample_sub_df = pd.read_csv('..\/input\/train.csv')","7c026e5e":"train_images_path =   \"..\/input\/train_images\"\ntest_images_path =  \"..\/input\/test_images\"\nprint(train_images_path)","909e8ba5":"train_df['image_path'] = \"..\/input\/train_images\/\" + train_df['id_code'] + \".png\"\ntest_df['image_path'] = \"..\/input\/test_images\/\" + test_df['id_code'] + \".png\"","e498a8de":"train_df.info()","abfc1bd6":"train_df[train_df.id_code == '5d024177e214']","742aa14c":"classes_dist = pd.DataFrame(train_df['diagnosis'].value_counts()\/train_df.shape[0]).reset_index()\n\n# barplot \nax = sns.barplot(x=\"index\", y=\"diagnosis\", data=classes_dist)\n\n# Imbalanced dataset with 49% - no DR, 8% proliferative - i.e most severe DR\n# Model Building - Need to do oversampling for minority classes","cb087d7e":"test_df.info()","c2ceb186":"root_path = pathlib.Path(train_images_path) # Returns POSIX path\n# for item in root_path.iterdir():\n#     print(item)\n#     break","2222e60e":"#!pip install ipython\n#!conda install -c anaconda ipython","f7b683f7":"all_paths = list(root_path.glob(\"*.png\"))\nall_paths[0:3]","4b3e4629":"#all_paths = list(root_path.glob(\"*.png\"))\n## Display few images of each class - \nall_paths = [str(path) for path in all_paths]\nrandom.shuffle(all_paths)\n\n\n# Image('..\/input\/train_images\/5d024177e214.png',width=300,height=300)\nfor n in range(3):\n    image_path = random.choice(all_paths)\n    print(image_path)\n    display(Image(image_path,width=300,height=300))","2f292abe":"# To decode datatype\ndef preprocess_image(image,labels):\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, [28,28])         # IMAGE RESIZING\n    image = tf.cast(image, tf.float32)                 \n    image \/= 255.0  # normalize to [0,1] range         # IMAGE NORMALIZE\n    \n    \n    return(image,labels)\n\n# Read image, and process\ndef load_and_preprocess_image(path,labels):\n    image = tf.read_file(path)\n    return(preprocess_image(image,labels))\n","4603a2fb":"labels = tf.convert_to_tensor(np.array(train_df['diagnosis']), dtype=tf.int32)\nfilenames  = tf.convert_to_tensor(train_df['image_path'].tolist(), dtype=tf.string)\nfilenames[0:5]\nlabels[0:4]","fbd6a1fd":"filenames.shape","b3662f22":"tf.one_hot(labels[0], 5)","9562c058":"def read_images(filenames,labels,batch_size):\n    \n    \n    dataset= tf.data.Dataset.from_tensor_slices((filenames,labels))\n    \n    \n    #labels =  tf.data.Dataset.from_tensor_slices(labels).map(lambda z: tf.one_hot(z, 5))\n    \n    #labels =  tf.data.Dataset.from_tensor_slices(labels)\n    #dataset = dataset.shuffle(len(labels))\n\n    # Image preprocessing\n    #dataset = dataset.map(load_and_preprocess_image,num_parallel_calls=4)\n    dataset = dataset.map(load_and_preprocess_image) \n    dataset.make_initializable_iterator()\n    \n    \n    #print(dataset)\n    # Get one batch\n    dataset = dataset.batch(batch_size,drop_remainder=False)\n    dataset = dataset.prefetch(1)\n    #dataset = dataset.shape\n    \n#     X,Y = tf.train.batch([dataset, labels], batch_size=batch_size,\n#                           capacity=batch_size * 8,\n#                           num_threads=4\n    #print(labels)\n    return(dataset,labels)\n\n","6256e41b":"# ## Create train & test datasets\n# filenames  = tf.convert_to_tensor(train['image_path'].tolist(), dtype=tf.string)\n# dx_train = tf.data.Dataset.from_tensor_slices(filenames)\n\n# labels = tf.convert_to_tensor(np.array(train['diagnosis']), dtype=tf.int32)\n# dy_train = tf.data.Dataset.from_tensor_slices(labels).map(lambda z: tf.one_hot(z, 5))\n\n# train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)","97387859":"# filenames  = tf.convert_to_tensor(valid['image_path'].tolist(), dtype=tf.string)\n# dx_valid = tf.data.Dataset.from_tensor_slices(filenames)\n\n# labels = tf.convert_to_tensor(np.array(valid['diagnosis']), dtype=tf.int32)\n# dy_valid = tf.data.Dataset.from_tensor_slices(labels).map(lambda z: tf.one_hot(z, 5))\n\n# valid_dataset = tf.data.Dataset.zip((dx_test, dy_test)).shuffle(500).repeat().batch(30)","a8f626a0":"# iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n#                                                train_dataset.output_shapes)\n# next_element = iterator.get_next()\n\n# training_init_op = iterator.make_initializer(train_dataset)\n# validation_init_op = iterator.make_initializer(valid_dataset)","5aa9f47f":"def cnn_model(in_data):\n    input_layer = tf.reshape(in_data, [-1, 28, 28, 3])\n    \n    input_layer = in_data\n    conv1 = tf.layers.conv2d(\n      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n     # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n      # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    #dense = tf.layers.dense(inputs=pool2_flat, units=128, activation=tf.nn.relu)\n#     dropout = tf.layers.dropout(\n#           inputs=dense, rate=0.4, training == tf.estimator.ModeKeys.TRAIN)\n\n      # Logits Layer\n    logits = tf.layers.dense(inputs=dense, units=5)\n\n    predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n      \"classes\": tf.argmax(input=logits, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n  }\n\n    return predictions","df412f55":"labels","4f72a127":"dataset,labels = read_images(filenames,labels,128)\niter = dataset.make_one_shot_iterator()\nx, y = iter.get_next()\n\npredictions = cnn_model(x)\npredictions","2aaad261":"predictions[\"classes\"]","843e6d1e":"predictions[\"probabilities\"]","009b28c7":"#tf.disable_eager_execution()","afd53b60":"x.shape","c23d4cbb":"predictions ","6810ec8b":"EPOCHS = 10\nBATCH_SIZE = 128\n# using two numpy arrays\n# features, labels = (np.array([np.random.sample((100,2))]), \n#                     np.array([np.random.sample((100,1))]))\n# dataset = tf.data.Dataset.from_tensor_slices((filenames,labels)).repeat().batch(BATCH_SIZE)\n# iter = dataset.make_one_shot_iterator()\n# x, y = iter.get_next()\n\ndataset,labels = read_images(filenames,labels,128)\niter = dataset.make_one_shot_iterator()\nx, y = iter.get_next()\n\n#tf.disable_eager_execution()\n# make a simple model\n# net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n# net = tf.layers.dense(net, 8, activation=tf.tanh)\npredictions = cnn_model(x)\nprediction = predictions[\"classes\"]\nloss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(EPOCHS):\n        _, loss_value = sess.run([train_op, loss])\n        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))","29d3dba3":"## Testing\n\n# with tf.Session() as sess:\n#     data,labels = read_images(filenames,labels,128)\n    \n#     iterator = data.make_initializable_iterator()\n#     image_batch,labels= iterator.get_next()\n    \n#     #image_batch,labels = next(iter(data))\n#     out = tf.layers.conv2d(image_batch,filters=32,kernel_size=(3,3))\n#     out = tf.nn.relu(out)\n#     out = tf.layers.max_pooling2d(out, 2, 2)\n#     out = tf.reshape(out, [-1, 32 * 32 * 16])\n#     # Now, logits is [batch_size, 6]\n#     logits = tf.layers.dense(out, 5)","54bc3c61":"# predictions = tf.argmax(logits, 1)\n# predictions","87377773":"# loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n# optimizer = tf.train.AdamOptimizer(0.01)\n\n# # Create the training operation\n# train_op = optimizer.minimize(loss)","eb2e28ce":"# N_CLASSES = train_df.diagnosis.nunique()\n# N_CLASSES","baeb888f":"#tf.reset_default_graph() ","eeaa7f1a":"# Build the data input\n#X, Y = read_images(filenames, labels, batch_size)","4b366ad8":"# Image preprocessing - Functions","dbbc9078":"#### Hi Guys, this is a starter code for anyone looking to build an image pipeline in Tensorflow. \n#### We are going to use tf.data API to build an image based dataset that reads the training data file\n- In batches \n- Applies Image preprocessing - Like resizing & normalization \n\n#### Please give a upvote if you like the kernel !! Cheers","38d6a5c7":"# Display few images ","4de3201a":"# Build Image Pipeline","637b9f63":"# Reading datasets"}}