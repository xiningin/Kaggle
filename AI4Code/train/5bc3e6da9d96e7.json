{"cell_type":{"b297faaf":"code","59627400":"code","d77a81b9":"code","5d04ffd7":"code","f8d05c0f":"code","e430005d":"code","ae45e70f":"code","b2cfc49b":"code","1830f24f":"code","25826044":"code","a8ea2fac":"code","78581c4e":"code","077e72b5":"code","f6f27135":"code","77e8709a":"code","d3a3d691":"code","70cf8d03":"code","0371fbc2":"code","ec5457a6":"code","ca291eb0":"code","ca8e9510":"code","0ef56b26":"code","31a58ad0":"code","998325d7":"code","e44a7fc9":"code","ef06d9c5":"code","d8561835":"code","91ecd605":"code","697ec4f2":"code","a8872b08":"code","5eb0b7e1":"code","34d2a242":"code","4e9e3b13":"code","21376425":"code","5c301796":"code","8e134599":"code","301e1876":"code","e2440f97":"code","a2eacc1b":"code","14fb8afb":"code","ed5ef0f6":"code","86466451":"code","be8a7639":"code","9bf7b8a6":"code","20efa3ff":"code","75db65b5":"code","247568ca":"code","7f36a7fa":"code","3aa5055e":"code","f8f2e8a2":"code","1d2c1c2d":"markdown","5e3d79ef":"markdown","7c774b32":"markdown","fe25140b":"markdown","ec56bf03":"markdown","96f1f996":"markdown","2c0b5871":"markdown","94e141d5":"markdown","4866b4e2":"markdown","c251cde0":"markdown","37e6a959":"markdown","b4b8f79c":"markdown","c7ea3b82":"markdown"},"source":{"b297faaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59627400":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import StratifiedKFold","d77a81b9":"df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf1=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","5d04ffd7":"df.shape","f8d05c0f":"df1.shape","e430005d":"df2=pd.concat([df,df1], ignore_index=True, axis=0)","ae45e70f":"df2.shape","b2cfc49b":"train_index=df.shape[0]\nsns.heatmap(df2.isnull(),yticklabels=False)","1830f24f":"df2.dropna(thresh=df2.shape[0]*0.3,how='all',axis=1,inplace=True)#drops columns who have atleast 30% of their inputs as NaN ","25826044":"pd.set_option('display.max_columns', None)\ndf2.drop([\"Id\"],axis=1,inplace=True)","a8ea2fac":"for features in df2.columns:\n    if df[features].isnull().sum() >=1:\n        print(features, df2[features].isnull().sum(),\"missing values\" )","78581c4e":"df2.drop([\"FireplaceQu\"],axis=1,inplace=True)","077e72b5":"df2[\"LotFrontage\"].fillna(df[\"LotFrontage\"].median(),inplace=True)","f6f27135":"df2['BsmtCond'] = df2['BsmtCond'].fillna(df2.BsmtCond.mode()[0])\ndf2['BsmtQual'] = df2['BsmtQual'].fillna(df2.BsmtCond.mode()[0])","77e8709a":"null_features=[]\nfor features in df2.columns:\n    if df[features].isnull().sum() >=1:\n        null_features.append(features)","d3a3d691":"for item in null_features:\n    df2[item] = df2[item].fillna(df2[item].mode()[0])","70cf8d03":"sns.heatmap(df2.isnull(),yticklabels=False)","0371fbc2":"numerical_features = [feature for feature in df2.columns if df2[feature].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(numerical_features))\n","ec5457a6":"discrete_feature=[feature for feature in numerical_features if len(df2[feature].unique())<25 ]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","ca291eb0":"for feature in discrete_feature:\n    data=df.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","ca8e9510":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","0ef56b26":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","31a58ad0":"for feature in continuous_feature:\n    data=df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalesPrice')\n        plt.title(feature)\n        plt.show()","998325d7":"categorical_features=[feature for feature in df2.columns if df2[feature].dtypes=='O']\nfor feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature,len(df2[feature].unique())))\nprint(\"Categorical Features Count: {}\".format(len(categorical_features)))","e44a7fc9":"numerical_with_nan=[feature for feature in numerical_features if df2[feature].isnull().sum()>1]\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(df2[feature].isnull().mean(),4)))","ef06d9c5":"df2[\"BsmtFullBath\"].fillna(df2[\"BsmtFullBath\"].mode())\ndf2[\"BsmtHalfBath\"].fillna(df2[\"BsmtHalfBath\"].mode())","d8561835":"for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    df2[feature]=df2['YrSold']-df2[feature]","91ecd605":"features_with_nan=[]\nfor features in df2.columns:\n    if df2[features].isnull().sum() >=1:\n        features_with_nan.append(features)\n        print(features, df2[features].isnull().sum(),\"missing values\" )","697ec4f2":"for features in features_with_nan:\n    if features!=\"SalePrice\":\n        df2[features]=df2[features].fillna(df2[features].mode()[0])","a8872b08":"df2.shape","5eb0b7e1":"for feature in categorical_features:\n    T=df2[feature].value_counts()\n    less_than_10=T[T<=10]\n    df2[feature]=df2[feature].apply(lambda x:\"other\" if x in less_than_10 else x)\n        ","34d2a242":"df_corr=df2[numerical_features].corr()\ndf_corr[\"SalePrice\"].sort_values(ascending=False)[:10]","4e9e3b13":"df_corr[\"SalePrice\"].sort_values(ascending=False)[-10:]","21376425":"corr_df = pd.DataFrame(df_corr['SalePrice'].sort_values(ascending=False))\ncorr_df = corr_df.reset_index()\ncorr_df.columns = ['cols', 'values']\ncols_to_drop = list(corr_df[corr_df['values'] <0]['cols'])","5c301796":"df2.drop(labels=cols_to_drop, axis=1,inplace=True)","8e134599":"df2.shape","301e1876":"for features in df2.columns:\n    if df2[features].isnull().sum() >=1:\n        features_with_nan.append(features)\n        print(features, df2[features].isnull().sum(),\"missing values\" )","e2440f97":"y=df2[\"SalePrice\"]\nX=df2.drop(\"SalePrice\",axis=1)","a2eacc1b":"print(X.shape)\nprint(y.shape)","14fb8afb":"X1=pd.get_dummies(X,drop_first=True)","ed5ef0f6":"X1.head()","86466451":"continuous_featureX=[feature for feature in numerical_features if feature in X.columns]\nfor feature in continuous_featureX:   \n    if 0 in X[feature].unique():\n        pass\n    else:\n        X1[feature]=np.log(X1[feature])\n            ","be8a7639":"X1.head()","9bf7b8a6":"X1_train=X1[:train_index]\nX1_test=X1[train_index:]\ny_train=y[:train_index]\ny_test=y[train_index:]\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier","20efa3ff":"print(X1_train.shape)\nprint(X1_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","75db65b5":"X11_train,X11_test,y11_train,y11_test=train_test_split(X1_train,y_train,test_size=0.15,random_state=42)","247568ca":"xgb = XGBRegressor(booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', max_delta_step=0,\n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1, learning_rate=0.1,\n             n_estimators=1000)\n\nxgb.fit(X11_train, y11_train)\n#preds=xgb.predict(X1_test)","7f36a7fa":"xgb.score(X11_test,y11_test)","3aa5055e":"xgb.fit(X1_train, y_train)\npreds=xgb.predict(X1_test)","f8f2e8a2":"sub_csv = pd.DataFrame({\n        \"Id\": df1[\"Id\"],\n        \"SalePrice\": preds\n    })\nsub_csv.to_csv('submission1.csv', index=False)","1d2c1c2d":"Lets see the number of discrete features","5e3d79ef":"Lets now drop all the features that have negative correlation with our output","7c774b32":"We see that numerous Discrete features are actually correlated for example OverallQual,OverallCond,FullBath, with our final output i.e SalePrice ","fe25140b":"It is now visible that a large number of Categorical Features have categories more than 10 lets try to minimize them to avoid Curse of Dimensionality","ec56bf03":"Lets now convert all the year features into a common one","96f1f996":"We see good amount of features with +ve correlation with our Output .However there also features with -ve Correlation","2c0b5871":"From the above graphs it is clearly visible that our Continuous variables need to be converted into LogNormal Distribution","94e141d5":"It\"s good because we don't have features with missing values within them","4866b4e2":"Let's now convert the previous unnormalized features into normal distribution","c251cde0":"Lets now see the number of continuous variables\n","37e6a959":"Let's see the number of null values in various rows of our dataset","b4b8f79c":"Lets get a list of remaining Categorical variables with NaN values","c7ea3b82":"We now spot cts features with good amount of +ve Correlation with our output "}}