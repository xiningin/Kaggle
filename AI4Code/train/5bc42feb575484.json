{"cell_type":{"b4dd4299":"code","386b0d4b":"code","38b74336":"code","0edba7e0":"code","b95de85a":"code","f0f8d0c9":"code","2545f19e":"code","7fdd34df":"code","ef94df43":"code","4be80a58":"code","67031248":"code","4df71eaa":"code","023d365f":"code","8af65802":"code","14641999":"code","730c430e":"code","3180e634":"code","23d48959":"code","aa92dfcd":"code","1d065036":"code","dc778f6a":"code","a9d0c123":"code","9137f9ac":"code","507bd904":"code","9f045312":"code","a934f9b3":"code","c192c4a8":"code","72e1ed29":"code","2ffb3a21":"code","e01faa55":"code","fde52c9b":"code","dee325f5":"code","0f16a0b3":"code","1b6aa944":"code","800b83d4":"code","4c77dfe2":"code","2da15b70":"code","834ec4c9":"code","cc2e886c":"code","97318cca":"code","99a399dc":"code","d39941ce":"code","3e45cd99":"code","32c053b4":"code","edb255c3":"code","de6ed830":"code","47c10bd4":"code","e28556bc":"code","eab9d5d2":"code","a9ff0551":"code","cbf14255":"code","c584e8e3":"markdown","189c19e6":"markdown","506f405e":"markdown","5c9bccc7":"markdown","18e0f799":"markdown","7205e9d0":"markdown","d5347531":"markdown","fcab3228":"markdown","fdcbdf83":"markdown","bcdd88a0":"markdown","8f3a707a":"markdown","3a380b33":"markdown","2c4b8845":"markdown","e1c5d655":"markdown","4088ed48":"markdown","eb4b51d7":"markdown","01946452":"markdown","a67351e8":"markdown","4523f04a":"markdown","1bffd8e8":"markdown","bbfe3f29":"markdown","32241971":"markdown"},"source":{"b4dd4299":"#Importing Libraries\nimport pandas as pd\n\n# For Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To Scale our data\nfrom sklearn.preprocessing import scale\n\n# To perform KMeans clustering \nfrom sklearn.cluster import KMeans\n\n# To perform Hierarchical clustering\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","386b0d4b":"help(KMeans)","38b74336":"#reading Dataset\nretail = pd.read_csv(\"..\/input\/OnlineRetail.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\n#if you dont use encoding option , you will get the following error \"\"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa3 in position 28: invalid start byte\"\"\n# parse date\nretail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'], format = \"%d-%m-%Y %H:%M\")","0edba7e0":"# Let's look top 5 rows\nretail.head()","b95de85a":"#Sanity Check\nretail.shape\nretail.describe()\nretail.info()\nretail.shape","f0f8d0c9":"#Na Handling\nretail.isnull().values.any()\nretail.isnull().values.sum()\nretail.isnull().sum()*100\/retail.shape[0]","2545f19e":"#dropping the na cells\norder_wise = retail.dropna()","7fdd34df":"#Sanity check\norder_wise.shape\norder_wise.isnull().sum()","ef94df43":"#RFM implementation\n\n# Extracting amount by multiplying quantity and unit price and saving the data into amount variable.\namount  = pd.DataFrame(order_wise.Quantity * order_wise.UnitPrice, columns = [\"Amount\"])\namount.head()","4be80a58":"#merging amount in order_wise\norder_wise = pd.concat(objs = [order_wise, amount], axis = 1, ignore_index = False)\n\n#Monetary Function\n# Finding total amount spent per customer\nmonetary = order_wise.groupby(\"CustomerID\").Amount.sum()\nmonetary = monetary.reset_index()\nmonetary.head()","67031248":"#monetary.drop(['level_1'], axis = 1, inplace = True)\n#monetary.head()","4df71eaa":"#Frequency function\nfrequency = order_wise[['CustomerID', 'InvoiceNo']]","023d365f":"# Getting the count of orders made by each customer based on customer ID.\nk = frequency.groupby(\"CustomerID\").InvoiceNo.count()\nk = pd.DataFrame(k)\nk = k.reset_index()\nk.columns = [\"CustomerID\", \"Frequency\"]\nk.head()","8af65802":"#creating master dataset\nmaster = monetary.merge(k, on = \"CustomerID\", how = \"inner\")\nmaster.head()","14641999":"recency  = order_wise[['CustomerID','InvoiceDate']]\nmaximum = max(recency.InvoiceDate)","730c430e":"maximum","3180e634":"#Generating recency function\n\n# Filtering data for customerid and invoice_date\nrecency  = order_wise[['CustomerID','InvoiceDate']]\n\n# Finding max data\nmaximum = max(recency.InvoiceDate)\n\n# Adding one more day to the max data, so that the max date will have 1 as the difference and not zero.\nmaximum = maximum + pd.DateOffset(days=1)\nrecency['diff'] = maximum - recency.InvoiceDate\nrecency.head()","23d48959":"# recency by customerid\na = recency.groupby('CustomerID')","aa92dfcd":"a.diff.min()","1d065036":"#Dataframe merging by recency\ndf = pd.DataFrame(recency.groupby('CustomerID').diff.min())\ndf = df.reset_index()\ndf.columns = [\"CustomerID\", \"Recency\"]\ndf.head()","dc778f6a":"#Combining all recency, frequency and monetary parameters\nRFM = k.merge(monetary, on = \"CustomerID\")\nRFM = RFM.merge(df, on = \"CustomerID\")\nRFM.head()","a9d0c123":"# outlier treatment for Amount\nplt.boxplot(RFM.Amount)\nQ1 = RFM.Amount.quantile(0.25)\nQ3 = RFM.Amount.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Amount >= (Q1 - 1.5*IQR)) & (RFM.Amount <= (Q3 + 1.5*IQR))]","9137f9ac":"# outlier treatment for Frequency\nplt.boxplot(RFM.Frequency)\nQ1 = RFM.Frequency.quantile(0.25)\nQ3 = RFM.Frequency.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Frequency >= Q1 - 1.5*IQR) & (RFM.Frequency <= Q3 + 1.5*IQR)]","507bd904":"# outlier treatment for Recency\nplt.boxplot(RFM.Recency)\nQ1 = RFM.Recency.quantile(0.25)\nQ3 = RFM.Recency.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Recency >= Q1 - 1.5*IQR) & (RFM.Recency <= Q3 + 1.5*IQR)]","9f045312":"RFM.head(20)\n","a934f9b3":"# standardise all parameters\nRFM_norm1 = RFM.drop([\"CustomerID\"], axis=1)\nRFM_norm1.Recency = RFM_norm1.Recency.dt.days\n\nfrom sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\nRFM_norm1 = standard_scaler.fit_transform(RFM_norm1)","c192c4a8":"RFM_norm1 = pd.DataFrame(RFM_norm1)\nRFM_norm1.columns = ['Frequency','Amount','Recency']\nRFM_norm1.head()","72e1ed29":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","2ffb3a21":"hopkins(RFM_norm1)","e01faa55":"# Kmeans with K=5\nmodel_clus5 = KMeans(n_clusters = 5, max_iter=50)\nmodel_clus5.fit(RFM_norm1)","fde52c9b":"from sklearn.metrics import silhouette_score\nsse_ = []\nfor k in range(2, 15):\n    kmeans = KMeans(n_clusters=k).fit(RFM_norm1)\n    sse_.append([k, silhouette_score(RFM_norm1, kmeans.labels_)])","dee325f5":"plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);","0f16a0b3":"# sum of squared distances\nssd = []\nfor num_clusters in list(range(1,21)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50)\n    model_clus.fit(RFM_norm1)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)","1b6aa944":"pd.RangeIndex(len(RFM.index))\n","800b83d4":"RFM_km = pd.concat([RFM, pd.Series(model_clus5.labels_)], axis=1)\nRFM_km.columns = ['CustomerID', 'Frequency', 'Amount', 'Recency', 'ClusterID']\n\nRFM_km.Recency = RFM_km.Recency.dt.days\nkm_clusters_amount = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Amount.mean())\nkm_clusters_frequency = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Frequency.mean())\nkm_clusters_recency = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Recency.mean())","4c77dfe2":"RFM_km.head()","2da15b70":"# analysis of clusters formed\nRFM.index = pd.RangeIndex(len(RFM.index))\nRFM_km = pd.concat([RFM, pd.Series(model_clus5.labels_)], axis=1)\nRFM_km.columns = ['CustomerID', 'Frequency', 'Amount', 'Recency', 'ClusterID']\n\nRFM_km.Recency = RFM_km.Recency.dt.days\nkm_clusters_amount = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Amount.mean())\nkm_clusters_frequency = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Frequency.mean())\nkm_clusters_recency = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Recency.mean())","834ec4c9":"km_clusters_amount","cc2e886c":"RFM_km.head()","97318cca":"df = pd.concat([pd.Series([0,1,2,3,4]), km_clusters_amount, km_clusters_frequency, km_clusters_recency], axis=1)\ndf.columns = [\"ClusterID\", \"Amount_mean\", \"Frequency_mean\", \"Recency_mean\"]\ndf.info()","99a399dc":"sns.barplot(x=df.ClusterID, y=df.Amount_mean)\n","d39941ce":"sns.barplot(x=df.ClusterID, y=df.Frequency_mean)","3e45cd99":"sns.barplot(x=df.ClusterID, y=df.Recency_mean)","32c053b4":"# heirarchical clustering\nmergings = linkage(RFM_norm1, method = \"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","edb255c3":"mergings = linkage(RFM_norm1, method = \"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","de6ed830":"clusterCut = pd.Series(cut_tree(mergings, n_clusters = 5).reshape(-1,))\nRFM_hc = pd.concat([RFM, clusterCut], axis=1)\nRFM_hc.columns = ['CustomerID', 'Frequency', 'Amount', 'Recency', 'ClusterID']","47c10bd4":"#summarise\nRFM_hc.Recency = RFM_hc.Recency.dt.days\nkm_clusters_amount = \tpd.DataFrame(RFM_hc.groupby([\"ClusterID\"]).Amount.mean())\nkm_clusters_frequency = \tpd.DataFrame(RFM_hc.groupby([\"ClusterID\"]).Frequency.mean())\nkm_clusters_recency = \tpd.DataFrame(RFM_hc.groupby([\"ClusterID\"]).Recency.mean())","e28556bc":"df = pd.concat([pd.Series([0,1,2,3,4]), km_clusters_amount, km_clusters_frequency, km_clusters_recency], axis=1)\ndf.columns = [\"ClusterID\", \"Amount_mean\", \"Frequency_mean\", \"Recency_mean\"]\ndf.head()","eab9d5d2":"#plotting barplot\nsns.barplot(x=df.ClusterID, y=df.Amount_mean)","a9ff0551":"sns.barplot(x=df.ClusterID, y=df.Frequency_mean)","cbf14255":"sns.barplot(x=df.ClusterID, y=df.Recency_mean)","c584e8e3":"### Let's look at KMeans package help to better understand the KMeans implementation in Python using SKLearn","189c19e6":"#### Monetary Value","506f405e":"Some usefull links to understand Hopkins Statistics:\n- [WikiPedia](https:\/\/en.wikipedia.org\/wiki\/Hopkins_statistic)\n- [Article](http:\/\/www.sthda.com\/english\/articles\/29-cluster-validation-essentials\/95-assessing-clustering-tendency-essentials\/)","5c9bccc7":"##### Merging Amount and Frequency columns","18e0f799":"#### Frequency Value","7205e9d0":"### RFM combined DataFrame","d5347531":"#### If in the above result you get a column with name level_1, uncomment the below code and run it, else ignore it and keeping moving.","fcab3228":"We will be using the online reatil trasnational dataset to build a RFM clustering and choose the best set of customers.\n","fdcbdf83":"## Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","bcdd88a0":"### Scaling the RFM data","8f3a707a":"<hr>","3a380b33":"## Hopkins Statistics:\nThe Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.\n\n- If the value is between {0.01, ...,0.3}, the data is regularly spaced.\n\n- If the value is around 0.5, it is random.\n\n- If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster.","2c4b8845":"## Sum of Squared Distances","e1c5d655":"# K-Mean Clustering","4088ed48":"## K-Means with some K","eb4b51d7":"### Recency Value","01946452":"### Data quality check and cleaning","a67351e8":"### Extracting R(Recency), F(Frequency), M(Monetary) columns form the data that we imported in.","4523f04a":"## Heirarchical Clustering","1bffd8e8":"### Reading the Data Set","bbfe3f29":"### Outlier Treatment","32241971":"**Overview**<br>\nOnline retail is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for an online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers."}}