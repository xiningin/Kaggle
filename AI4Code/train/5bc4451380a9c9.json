{"cell_type":{"c7525faf":"code","fbd21961":"code","85334287":"code","21f3c934":"code","66da5ae0":"code","83a871ba":"code","f177a6ec":"code","8345f1e8":"code","07be7258":"code","22418fc6":"code","8d639b91":"code","67e22ea3":"code","f4e55869":"code","141a9f2b":"code","3aa3d3a4":"code","8814ee9a":"code","48f4259a":"code","9c31f408":"code","25a1ce64":"markdown","559db30c":"markdown","4748eb39":"markdown","e1289e7d":"markdown","8abe9717":"markdown","918031cd":"markdown","69f47155":"markdown","1c2f7a2e":"markdown","82dd65ef":"markdown","45598956":"markdown","2299e7b7":"markdown","cb8b70af":"markdown","30b40f46":"markdown"},"source":{"c7525faf":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","fbd21961":"!ls ..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased","85334287":"from collections import defaultdict\nfrom dataclasses import dataclass\nimport functools\nimport gc\nimport itertools\nimport json\nfrom multiprocessing import Pool\nimport os\nfrom pathlib import Path\nimport random\nimport re\nimport shutil\nimport subprocess\nimport time\nfrom typing import Callable, Dict, List, Generator, Tuple\nfrom os.path import join as path_join\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json._json import JsonReader\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, Subset, DataLoader\n\nfrom transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig\nfrom transformers.optimization import get_linear_schedule_with_warmup","21f3c934":"DATA_DIR = Path('..\/input\/google-quest-challenge\/')\ntrain_df = pd.read_csv(path_join(DATA_DIR, 'train.csv'))\ntest_df = pd.read_csv(path_join(DATA_DIR, 'test.csv'))\nprint(train_df.shape, test_df.shape)","66da5ae0":"train_df['text'] = train_df['question_title'] + ' ' + train_df['question_body'] + ' ' + train_df['answer']\ntest_df['text'] = test_df['question_title'] + ' ' + test_df['question_body'] + ' ' + test_df['answer']","83a871ba":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title', 'question_body', 'answer']","f177a6ec":"start_time = time.time()\n\nseed = 42\n\nmax_seq_len = 512\n\nnum_labels = len(targets)\nn_epochs = 1\nlr = 2e-5\nwarmup = 0.05\nbatch_size = 64\naccumulation_steps = 4\n\nbert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\n\nbert_model = 'bert-base-uncased'\ndo_lower_case = 'uncased' in bert_model\ndevice = torch.device('cuda')\n\noutput_model_file = 'bert_pytorch.bin'\noutput_optimizer_file = 'bert_pytorch_optimizer.bin'\noutput_amp_file = 'bert_pytorch_amp.bin'\n\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","8345f1e8":"def convert_lines(example, max_seq_length, tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","07be7258":"class BertForSequenceClassification(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification\/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n    \"\"\"\n    def __init__(self, config):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask,\n                            inputs_embeds=inputs_embeds)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        return logits","22418fc6":"def loss_fn(preds, labels):\n    loss = nn.BCEWithLogitsLoss()\n    class_loss = loss(class_preds, class_labels)\n    return class_loss","8d639b91":"bert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = len(targets)\n\nmodel_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/')\n\nmodel = BertForSequenceClassification.from_pretrained(model_path, config=bert_config)\nmodel = model.to(device)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\ntokenizer = BertTokenizer.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt',\n                                          do_lower_case=do_lower_case)","67e22ea3":"X_test = convert_lines(test_df[\"text\"].fillna(\"DUMMY_VALUE\"), max_seq_len, tokenizer)","f4e55869":"test_preds = np.zeros((len(X_test), len(targets)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * batch_size:(i + 1) * batch_size] = pred[:,:].detach().cpu().squeeze().numpy()\n\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy()","141a9f2b":"test_pred.shape","3aa3d3a4":"submission = pd.DataFrame.from_dict({\n    'qa_id': test_df['qa_id']\n})","8814ee9a":"for i in range(len(targets)):\n    submission[targets[i]] = test_pred[:, i]","48f4259a":"submission.to_csv('submission.csv', index=False)","9c31f408":"submission.head()","25a1ce64":"For starters I've appended question_title, question_body and answer as a single text for BERT Inference. But there are better ways to give input to BERT, for example: see how BERT for Q&A input works","559db30c":"As we are not allowed to use internet I've created required datasets and commands to setup Hugging Face Transformers setup in offline mode. You can find the required github codebases in the datasets.\n\n* sacremoses dependency - https:\/\/www.kaggle.com\/axel81\/sacremoses\n* transformers - https:\/\/www.kaggle.com\/axel81\/transformers","4748eb39":"## Build Model","e1289e7d":"**Please upvote the kernel if you find it helpful**","8abe9717":"### Setup model","918031cd":"### Run Inference","69f47155":"**Pytorch BERT approach**\n\nIn this kernel I demonstrate how to use **BertForSequenceClassification** for inference on Q&A dataset. This kernel only demonstrates how to do a inference using BERT. I will publish another kernel to show how to finetune the BERT to create a new baseline.","1c2f7a2e":"### Read Train and Test Data","82dd65ef":"Define targets","45598956":"### Install HuggingFace transformers & sacremoses dependency","2299e7b7":"### Tokenize and Convert Input Data for BERT","cb8b70af":"### Required Imports\n\nI've added imports that will be used in training too","30b40f46":"### Generate Submission"}}