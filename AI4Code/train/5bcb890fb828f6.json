{"cell_type":{"8e94e769":"code","16ed5e33":"code","b771989f":"code","926003af":"code","92d0593b":"code","acbd9a41":"code","cf565d30":"code","7be6f68a":"code","fce8bffd":"code","0befd60f":"code","6c2b494d":"code","bfa6a39f":"code","1019dcc4":"code","ad11543a":"code","72ce1ef9":"code","57814ae5":"code","2f1d65a8":"code","94db9aed":"code","c9b5ba94":"code","5c79febb":"code","add3e424":"code","62d5cba3":"code","dc959831":"code","56723aa8":"code","4d5df651":"markdown","7be4f596":"markdown","a038179c":"markdown","732da11f":"markdown","2548dd57":"markdown","33aeca5c":"markdown","eb447d7b":"markdown","571378b2":"markdown","94ae3bbb":"markdown","cd95c674":"markdown","5a9d3fac":"markdown","1410cae4":"markdown","a8df7cb3":"markdown","c894a72e":"markdown","84e6139c":"markdown","cfd6b4a4":"markdown","dcfa3bd2":"markdown","9200f362":"markdown","c318ffe4":"markdown"},"source":{"8e94e769":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16ed5e33":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom platform import python_version\n\nprint('Tensorflow Version:', tf.__version__)\nprint('Python Version:', python_version())","b771989f":"path = '..\/input\/digit-recognizer\/train.csv'\ndf = pd.read_csv(path)\n\nprint(df.shape)\ndf.head()","926003af":"# Configure random state\nRANDOM_SEED = 123","92d0593b":"y_labels = df.pop('label')\nx_pixels = df","acbd9a41":"unique_labels = np.unique(y_labels)\nNUM_LABELS = (len(unique_labels))\nprint(\"Labels: \", unique_labels)\nprint(\"No. Labels: \", NUM_LABELS)","cf565d30":"plt.figure()\n# Reshape the array to 28x28 dimensions\nplt.imshow(np.array(x_pixels.iloc[0]).reshape(28, 28)) \nplt.colorbar()\nplt.show()","7be6f68a":"plt.figure()\nfor i in range(8): \n    ax = plt.subplot(2, 4, i + 1)\n    plt.imshow(np.array(x_pixels.iloc[i]).reshape(28, 28))\n    plt.title(y_labels[i])\n    plt.axis('off')","fce8bffd":"x_train, x_val, y_train, y_val = train_test_split(x_pixels, y_labels, test_size=0.2, random_state=RANDOM_SEED)","0befd60f":"print(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","6c2b494d":"train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))","bfa6a39f":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBUFFER_SIZE = 1000 # Shuffle buffer\nBATCH_SIZE = 32\n\ndef configure_dataset(dataset, shuffle=False, test=False):\n    # Configure the tf dataset for cache, shuffle, batch, and prefetch\n    if shuffle:\n        dataset = dataset.cache()\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\n                        .batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n    elif test:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=False).prefetch(AUTOTUNE)\n    else:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n    return dataset","1019dcc4":"train_ds = configure_dataset(train_ds, shuffle=True)\nval_ds = configure_dataset(val_ds)","ad11543a":"print(train_ds.element_spec)\nprint(val_ds.element_spec)","72ce1ef9":"# Reshape to (height, width, color channel)\nreshape_layer = layers.Reshape((28, 28, 1))\n# Include the decimal point when dividing to output floats \nrescale_layer = layers.experimental.preprocessing.Rescaling(scale=1.\/255)\n\nmodel = Sequential([\n                    layers.InputLayer(input_shape=[28, 28]),\n                    reshape_layer,\n                    rescale_layer,\n                    layers.Conv2D(128, kernel_size=3, activation='relu'),                                                 \n                    layers.BatchNormalization(), # Normalizes layer activations                     \n                    layers.MaxPooling2D(2,2), # Downsamples the feature map                    \n                    layers.Dropout(0.5), # Reduces overfitting\n                    layers.Conv2D(128, kernel_size=3, activation='relu'),\n                    layers.BatchNormalization(),\n                    layers.MaxPooling2D(2,2),\n                    layers.Dropout(0.5), \n                    layers.Flatten(),  \n                    layers.Dense(NUM_LABELS, activation='softmax')\n])\n\nmodel.summary()","57814ae5":"model.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer=Adam(lr=1e-3),\n    metrics=['accuracy']\n)\n\nEPOCHS = 20\n\nhistory = model.fit(\n    train_ds,\n    epochs=EPOCHS, \n    validation_data=val_ds\n)","2f1d65a8":"# Assign the loss and accuracy metrics\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']","94db9aed":"# Create a function to plot the metrics\ndef plot_history(train_history, val_history, label):\n    plt.plot(train_history, label=f'Training {label}')\n    plt.plot(val_history, label=f'Validation {label}')\n    plt.xlabel('Epochs')\n    plt.legend()\n    return plt.show()","c9b5ba94":"plot_history(train_loss, val_loss, 'Loss')\nplot_history(train_acc, val_acc, 'Accuracy')","5c79febb":"path_test = '..\/input\/digit-recognizer\/test.csv'\ndf_test = pd.read_csv(path_test)\nprint(df_test.shape)\ndf_test.head()","add3e424":"test_ds = tf.data.Dataset.from_tensor_slices((df_test))\ntest_ds = configure_dataset(test_ds, test=True) # Set test argument to True so that remainder samples are not dropped\ntest_ds.element_spec","62d5cba3":"y_pred = np.argmax(model.predict(test_ds), axis=-1) # Returns the highest probability label for each image\ny_pred = pd.DataFrame(y_pred)\ny_pred.columns = ['Label']\nprint(y_pred.shape)\ny_pred.head()\n","dc959831":"image_id = pd.DataFrame(y_pred.index + 1)\nimage_id.columns = ['ImageId']\nimage_id.head()","56723aa8":"submission = pd.concat([image_id, y_pred], axis=1)\nsubmission.to_csv('submission.csv', index=False)","4d5df651":"# 3. Build a sequential CNN with preprocessing layers\n\nThe reshaping and rescaling tasks are included in the model.  These preprocessing layers can then be saved and exported with the CNN.  This is the preferred approach only if you're training on a GPU. \n\nA CNN requires image dimensions and a color channel to function.  Here inputs are reshaped to include the grayscale color channel.  \n\nInputs need to be normalized to a range of 0 to 1.  Divide the pixel values by 255 to achieve this scale.  Insert this layer after reshaping. \n\nIn this example two convolutional layers are used, each with batch normalization, max pooling, and a dropout layer. A final classification layer is added with units the size of the number of classes.","7be4f596":"### 1.2 Load the training .csv file and explore the data","a038179c":"### 6.1 Load the test .csv file ","732da11f":"### 1.3 Isolate the label column from the pixel data.","2548dd57":"### 1.1 Import libraries and verify versions","33aeca5c":"### 2.1 The following code shows 4 different Dataset methods:\n* *Cache()*: Keeps the Dataset elements in memory to be used in later iterations.  \n* *Shuffle()*: Randomly shuffles the elements of the Dataset so each batch can reflect the overall distribution.  This reduces overfitting and variance when training.\n* *Batch()*: Packs the elements of the Dataset into batches.  This determines the number of samples processed during an iteration. \n* *Prefetch()*: Reduces time by overlapping data preprocessing and model execution of a training step.","eb447d7b":"With the data verified, it's time to create Datasets for training and validation.","571378b2":"# 2. Create Datasets from the training and validation data\nThe Dataset object is a Python iterable that can be configured for improved performance.  Datasets are used to build efficient input pipelines in TensorFlow.","94ae3bbb":"# Classification of the MNIST dataset\n\n# 1. Introduction\n\nThis notebook will demonstrate how to train a neural network to recognize images of handwritten digits using TensorFlow.  While there are many [MNIST](http:\/\/yann.lecun.com\/exdb\/mnist\/) computer vision tutorials that cover the basics, this guide will focus on developing a self contained model that can handle preprocessing.  \nThe contents of this notebook include: \n* Data exploration\n* Data preprocessing\n* Building a Convolutional Neural Network\n* Model training\n* Evaluation of results\n* Predicting outcomes with test data","cd95c674":"The training data has 42,000 images.  Let's examine one of them. ","5a9d3fac":"# 7. Predict label outcomes and submit results","1410cae4":"# 5. Visualize and evaluate the training and validation results","a8df7cb3":"The colorbar shows the pixel values are within the grayscale range of 0 to 255.  These values will need to be scaled to a range of 0 to 1 which will be covered later in this guide.\nLet's visualize some more images, this time matched with their labels.   ","c894a72e":"Observe the shape of the training and validation Datasets.  This is the batch size (128) and number of pixel columns (784).","84e6139c":"### 1.4 Review the labels to confirm the number of unique values.","cfd6b4a4":"# 6. Create a Dataset from the test data","dcfa3bd2":"The training and validation plots should converge for optimal results.","9200f362":"For this split 80% of the images will be used for training, and 20% for validation.","c318ffe4":"# 4. Train the CNN model \nSparse categorical crossentropy is used as the loss function with Adam as the optimizer."}}