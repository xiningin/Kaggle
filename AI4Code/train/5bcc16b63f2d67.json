{"cell_type":{"d92a8668":"code","aa8718a0":"code","a1001d80":"code","d90b1f3f":"code","12e2ae33":"code","1e8b87bb":"code","bd71e8b3":"code","0126e985":"code","4dd2ee52":"code","6a7871d7":"code","9bbb02e0":"code","003f4a41":"code","2799c966":"code","076aa6ef":"code","2f2e4cd0":"code","b17a1f1e":"code","c6d0f046":"code","c59b15a5":"code","a4c99624":"code","17467338":"code","c336f63f":"code","b1a1e85e":"markdown","59698912":"markdown","54fd336b":"markdown","ef0a6230":"markdown","ea8772a7":"markdown","e03dae19":"markdown","75bbec93":"markdown","0dd2bac8":"markdown","f832cc70":"markdown","d7ce614b":"markdown","0ddaeb60":"markdown","169bf944":"markdown","151b180d":"markdown"},"source":{"d92a8668":"!pip install ftfy regex tqdm\n!pip install git+https:\/\/github.com\/openai\/CLIP.git","aa8718a0":"from PIL import Image\nimport torch\nfrom torch import nn, optim\nimport glob\nimport os\nimport pandas as pd\nimport json\nimport numpy as np\nimport clip\nfrom torch.utils.data import Dataset, DataLoader, BatchSampler\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport random\nfrom matplotlib.pyplot import imshow\nimport torchtext\nimport nltk, re, string, collections\nfrom nltk.util import ngrams\nimport collections\n%matplotlib inline\nBATCH_SIZE = 128\nEPOCH = 5","a1001d80":"IMG_ROOT = \"..\/input\/meme-project-raw\"\nJSON_ROOT = \"..\/input\/meme-project-clean-json\"\nimg_paths = glob.glob(os.path.join(IMG_ROOT, \"*.jpg\"))\nd = {}\nfor i, img_path in enumerate(img_paths):\n    name = img_path.split(\"\/\")[-1].split(\".\")[0]\n    with open(os.path.join(JSON_ROOT, name+\".json\"), \"r\") as f:\n        captions = json.load(f)\n        temp = []\n        for cap in captions:\n            if \"http\" not in (cap[0]+ ' '+cap[1]) and len(cap[0]+ ' '+cap[1]) >= 8 and len(cap[0]+ ' '+cap[1]) <= 72:\n                temp.append(cap[0]+ ' '+cap[1])\n        d[img_path] = temp\nlen(d)","d90b1f3f":"train_img_paths, test_img_paths = train_test_split(img_paths, test_size=0.2, random_state=42)\nd_train = {k: d[k] for k in train_img_paths}\nd_test = {k: d[k] for k in test_img_paths}\nlen(d_train), len(d_test)","12e2ae33":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B\/32\", device=device, jit=False)\n\nimage = preprocess(Image.open(\"..\/input\/meme-project-raw\/-okay-.jpg\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\nimage.shape, text.shape","1e8b87bb":"class MemeDataset(Dataset):\n    def __init__(self, data, preprocess):\n        self.preprocess = preprocess\n        self.img_paths = []\n        self.captions = []\n        for img_path, captions in data.items():\n            for cap in captions:\n                self.img_paths.append(img_path)\n                self.captions.append(cap)\n        self.processed_cache = {}\n        for img_path in data:\n            self.processed_cache[img_path] = self.preprocess(Image.open(img_path))\n        self.img_paths_set = list(data.keys())\n        self.path2label = {path: self.img_paths_set.index(path) for path in self.img_paths_set}\n        \n    def __len__(self):\n        return len(self.captions)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        image = self.processed_cache[img_path]\n        caption = self.captions[idx]\n        label = self.path2label[img_path]\n        return image, caption, label\n\ntrain_dataset = MemeDataset(d_train, preprocess)\ntest_dataset = MemeDataset(d_test, preprocess)\nlen(train_dataset), len(test_dataset), train_dataset[0]","bd71e8b3":"i = 0\nfor k,v in train_dataset.path2label.items():\n    i+=1\n    print(k,v)\n    if i == 10:\n        break","0126e985":"# https:\/\/github.com\/pytorch\/pytorch\/blob\/e5742494f6080c8e6f43c37689fc18a7c4b39dfd\/torch\/utils\/data\/dataloader.py#L145\nclass BalancedBatchSampler(BatchSampler):\n    \"\"\"\n    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n    Returns batches of size n_classes * n_samples\n    \"\"\"\n\n    def __init__(self, labels, n_classes, n_samples):\n        self.labels = labels\n        self.labels_set = list(set(self.labels.numpy()))\n        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n                                 for label in self.labels_set}\n        for l in self.labels_set:\n            np.random.shuffle(self.label_to_indices[l])\n        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n        self.count = 0\n        self.n_classes = n_classes\n        self.n_samples = n_samples\n        self.n_dataset = len(self.labels)\n        self.batch_size = self.n_samples * self.n_classes\n\n    def __iter__(self):\n        self.count = 0\n        while self.count + self.batch_size < self.n_dataset:\n            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n            indices = []\n            for class_ in classes:\n                indices.extend(self.label_to_indices[class_][\n                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n                                                                         class_] + self.n_samples])\n                self.used_label_indices_count[class_] += self.n_samples\n                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n                    np.random.shuffle(self.label_to_indices[class_])\n                    self.used_label_indices_count[class_] = 0\n            yield indices\n            self.count += self.n_classes * self.n_samples\n\n    def __len__(self):\n        return self.n_dataset \/\/ self.batch_size\n    \ntrain_labels = torch.tensor([item[2] for item in train_dataset])\ntrain_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\ntrain_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n\ntest_labels = torch.tensor([item[2] for item in test_dataset])\ntest_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\ntest_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler)\n\n# train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n# test_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n# len(train_dataset), len(test_dataset), train_dataset[0]","4dd2ee52":"for i, item in enumerate(train_sampler):\n#     print(item)\n#     print(len(item))\n    labels = []\n    for idx in item:\n        label = train_dataset[idx][2]\n        labels.append(label)\n    break\nlen(labels), len(set(labels))","6a7871d7":"for batch in train_dataloader:\n    imgs, txts, labels = batch\n    print(imgs.shape)\n    print(len(txts))\n    print(labels)\n    print(labels.shape)\n    print(torch.unique(labels).shape)\n    break","9bbb02e0":"#https:\/\/github.com\/openai\/CLIP\/issues\/57\ndef convert_models_to_fp32(model): \n    for p in model.parameters(): \n        p.data = p.data.float() \n        p.grad.data = p.grad.data.float() \n\nif device == \"cpu\":\n    model.float()\n\nloss_img = nn.CrossEntropyLoss()\nloss_txt = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)","003f4a41":"best_te_loss = 1e5\nbest_ep = -1\nfor epoch in range(EPOCH):\n    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n    step = 0\n    tr_loss = 0\n    model.train()\n    pbar = tqdm(train_dataloader, leave=False)\n    for batch in pbar:\n        step += 1\n        optimizer.zero_grad()\n\n        images, texts, _ = batch\n        images = images.to(device)\n        texts = clip.tokenize(texts).to(device)\n#         print(images.shape, texts.shape)\n        logits_per_image, logits_per_text = model(images, texts)\n        ground_truth = torch.arange(BATCH_SIZE).to(device)\n\n        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))\/2\n        total_loss.backward()\n        tr_loss += total_loss.item()\n        if device == \"cpu\":\n            optimizer.step()\n            scheduler.step()\n        else:\n            convert_models_to_fp32(model)\n            optimizer.step()\n            scheduler.step()\n            clip.model.convert_weights(model)\n        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n    tr_loss \/= step\n    \n    step = 0\n    te_loss = 0\n    with torch.no_grad():\n        model.eval()\n        test_pbar = tqdm(test_dataloader, leave=False)\n        for batch in test_pbar:\n            step += 1\n            images, texts, _ = batch\n            images = images.to(device)\n            texts = clip.tokenize(texts).to(device)\n            logits_per_image, logits_per_text = model(images, texts)\n            ground_truth = torch.arange(BATCH_SIZE).to(device)\n\n            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))\/2\n            te_loss += total_loss.item()\n            test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n        te_loss \/= step\n        \n    if te_loss < best_te_loss:\n        best_te_loss = te_loss\n        best_ep = epoch\n        torch.save(model.state_dict(), \"best_model.pt\")\n    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\ntorch.save(model.state_dict(), \"last_model.pt\")","2799c966":"model.load_state_dict(torch.load(\"..\/input\/clipfinetuneweights\/best_model.pt\"))\nNUM_NEG = 127\nNUM_TEST = 1000","076aa6ef":"n_correct = 0\nfor i in tqdm(range(NUM_TEST)):\n    empty = True\n    while empty:\n        img_path = random.choice(list(d_test.keys()))\n        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n        name = img_path.split('\/')[-1].split('.')[0]\n        caps = d_test[img_path]\n        if len(caps) > 0:\n            pos_txt = random.choice(caps)\n        #         pos_txt = ' '.join(pos_txt)\n            empty = False\n#     print(pos_txt)\n    neg_i = 0\n    neg_txts = []\n    while neg_i < NUM_NEG:\n        img_path = random.choice(list(d_test.keys()))\n        neg_name = img_path.split('\/')[-1].split('.')[0]\n        if neg_name == name:\n            continue\n        caps = d_test[img_path]\n        if len(caps) == 0:\n            continue\n        neg_txt = random.choice(caps)\n        if neg_txt in neg_txts:\n            continue\n        neg_txts.append(neg_txt)\n        neg_i += 1\n#     print(name)\n#     print(f\"Positive caption: {pos_txt}\")\n#     print(f\"Negative caption: {neg_txts}\")\n    text = clip.tokenize([pos_txt]+neg_txts).to(device)\n\n    with torch.no_grad():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n\n        logits_per_image, logits_per_text = model(image, text)\n        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n#     print(\"Label probs:\", probs)\n#     print(np.argmax(probs))\n    if np.argmax(probs) == 0:\n        n_correct +=1\nprint(f\"Test precision {n_correct\/NUM_TEST}\")","2f2e4cd0":"def sample1Caption(img_path, corpus, model, num_cand):\n    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n    i = 0\n    txts = []\n    while i < num_cand:\n        txt = random.choice(corpus)\n        if txt in txts:\n            continue\n        if len(txt.split())<5 or len(txt)>72:\n            continue\n        txts.append(txt)\n        i += 1\n    #     print(name)\n    #     print(f\"Positive caption: {pos_txt}\")\n    #     print(f\"Negative caption: {neg_txts}\")\n    text = clip.tokenize(txts).to(device)\n\n    with torch.no_grad():\n        logits_per_image, logits_per_text = model(image, text)\n        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n    #     print(\"Label probs:\", probs)\n    #     print(np.argmax(probs))\n    #     imshow(np.asarray(Image.open(img_path)))\n    return txts[np.argmax(probs)]","b17a1f1e":"model.load_state_dict(torch.load(\"..\/input\/clipfinetuneweights\/best_model.pt\"))\ncorpus = []\nfor txtlist in d_train.values():\n    corpus += txtlist\nlen(corpus), corpus[0]","c6d0f046":"captions = {}\nfor img_path in tqdm(d_test.keys()):\n    caption = sample1Caption(img_path, corpus, model, 1000)\n    captions[img_path] = caption","c59b15a5":"for get_bleu in range(1,4):\n    bleu_x_lst = []\n    bleu_y_lst = []\n    for p, caps in d_test.items():\n        if not caps:\n            continue\n        bleu_x_lst.append(captions[p].split())\n        splittedcaps = [x.split() for x in caps]\n        bleu_y_lst.append(splittedcaps)\n    BLEU = torchtext.data.metrics.bleu_score(bleu_x_lst, bleu_y_lst, max_n=get_bleu, weights=[1\/get_bleu]*get_bleu)\n    print(f\"{get_bleu}-gram BLEU score: {BLEU}\")","a4c99624":"sentences = list(captions.values())\nBigramCtr = collections.Counter()\nUnigramCtr = collections.Counter()\nfor sentence in sentences:\n    BigramCtr.update(nltk.ngrams(sentence, 2))\n    UnigramCtr.update(nltk.ngrams(sentence, 1))\n# print(\"Unigram count:\",len(BigramCtr)\/len(sentences))\n# print(\"Bigram count:\",len(UnigramCtr)\/len(sentences))\nprint(\"Unigram count:\",len(BigramCtr))\nprint(\"Bigram count:\",len(UnigramCtr))","17467338":"seen_path = random.choice(list(d_train.keys()))\npred_cap_seen = sample1Caption(seen_path, corpus, model, 1000)\ngt_cap_seen = d_train[seen_path][:5]\nimshow(Image.open(seen_path))\nprint(f\"Some ground truth captions for this seen image: {gt_cap_seen}\")\nprint(f\"Caption sampled by fintuned CLIP for this seen image: {pred_cap_seen}\")","c336f63f":"unseen_path = random.choice(list(d_test.keys()))\npred_cap_unseen = sample1Caption(unseen_path, corpus, model, 1000)\nimshow(Image.open(unseen_path))\ngt_cap_unseen = d_test[unseen_path][:5]\nprint(f\"Some ground truth captions for this unseen image: {gt_cap_unseen}\")\nprint(f\"Caption sampled by fintuned CLIP for this unseen image: {pred_cap_unseen}\")","b1a1e85e":"# Evaluating Precision on Validation Set","59698912":"## Word Diversity","54fd336b":"## Loading Pre-trained CLIP Model and Preprocessor","ef0a6230":"# Training","ea8772a7":"## Sampling Captions for Validation Images According to CLIP Text-Image Proximity","e03dae19":"## BalancedBatchSampler (ensures no same class per batch)","75bbec93":"## BLEU Score","0dd2bac8":"# Installing CLIP","f832cc70":"# Preparing Model and Data","d7ce614b":"## MemeDataset","0ddaeb60":"# Evaluating BLEU and Word Diversity using Naive Sampling","169bf944":"## Splitting 20% for Validation","151b180d":"# Case Analysis on Seen and Unseen Images"}}