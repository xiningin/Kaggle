{"cell_type":{"9d61f3d4":"code","4630144b":"code","0c502908":"code","ef1ef352":"code","e60c9d5a":"code","44216edc":"code","91142a1c":"code","6e03fbdb":"code","18e9e556":"code","dc66b000":"code","42624c7f":"code","3d8a8cff":"code","1c6a146a":"code","b1a2427c":"code","c553e341":"code","c9fe676f":"code","ae42fb3b":"code","b6297b37":"code","d6a216d3":"code","b2af6d32":"code","fa582068":"code","44bdd95b":"code","39182cc1":"code","77cff938":"code","e122a366":"code","6baa0dbc":"code","a77560a3":"code","49bc7af4":"code","cadabe6b":"code","381dbf9c":"code","a064139a":"code","48b4ad09":"code","1f181a90":"code","bfa23732":"code","b4101130":"code","ac97649e":"code","fc3ab6c2":"code","0a3762d2":"code","86ae7bc1":"code","84a251c8":"code","650ce3d7":"code","70d3a6f2":"code","78e6bffb":"code","653abacb":"code","c48f8533":"code","ba69bb1b":"code","07a71cd4":"code","9f537037":"code","3619d86d":"code","128078b9":"code","74040754":"code","a5c6f639":"code","b8c044da":"code","859cddbb":"code","77dfa584":"code","85b6c6a6":"code","fe60a1fb":"code","ab622161":"code","91a11264":"code","5df55f12":"code","d43fd93f":"code","9d6436eb":"code","6b6c65ec":"code","111dc635":"code","8a33a456":"code","fd759906":"code","1cbb48eb":"code","16886bfd":"code","00e76cc2":"code","73c824b4":"code","1a07c17b":"code","1758ecbf":"code","70d2043b":"code","0737323a":"code","729367ff":"code","aa9a273e":"code","498b83dd":"code","0bf02914":"code","1dc95fb4":"code","7f119b7f":"markdown","264a370e":"markdown","e436bf3d":"markdown","d47a5854":"markdown","cace4e08":"markdown","17b85738":"markdown","bd1ebdc9":"markdown","78535af0":"markdown","19ed2b03":"markdown","ebeff3ab":"markdown","761283b6":"markdown","22ee0fe5":"markdown","0a42bb47":"markdown","4ad03558":"markdown","52b5074b":"markdown","b17d6c24":"markdown","f7b0d7d2":"markdown","93751759":"markdown","cd75383e":"markdown","9776c385":"markdown","01537fce":"markdown","f0dabf91":"markdown","4ae5dedb":"markdown","53e4fe7c":"markdown","bc218f14":"markdown","bb076f59":"markdown","4214c4c3":"markdown","6b3bddf8":"markdown","f1b11b3e":"markdown","0967c7a2":"markdown","94c37292":"markdown","ad907de8":"markdown","77d000b0":"markdown","82b0c300":"markdown","1200500b":"markdown","c03d9ec4":"markdown","b680dcbe":"markdown","a3ad872a":"markdown","700de2ca":"markdown","14f3d20e":"markdown","44ead9d8":"markdown","02da7246":"markdown","042319e1":"markdown","cb1adcf2":"markdown","eb28811f":"markdown","7fe89b43":"markdown","431a5344":"markdown","262c57d6":"markdown","26957146":"markdown","3ae8bebb":"markdown","59054d75":"markdown","2901a3a0":"markdown","ed76ebf1":"markdown","4223e501":"markdown","f8f0f61e":"markdown","e19eac99":"markdown"},"source":{"9d61f3d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4630144b":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n\nId = test_data['Id']\ntrain_data.drop(['Id'] , axis = 1 , inplace = True)\ntest_data.drop(['Id'] , axis = 1 , inplace = True)","0c502908":"train_data.head()","ef1ef352":"train_data.info() , test_data.info()","e60c9d5a":"missing_train_data = train_data.isnull().sum()\nmissing_train_data[missing_train_data > 0].sort_values(ascending = False)","44216edc":"missing_test_data = test_data.isnull().sum()\nmissing_test_data[missing_test_data > 0].sort_values(ascending = False)","91142a1c":"train_data.drop(['Alley' , 'Fence'] , axis = 1 , inplace = True)\ntest_data.drop(['Alley' , 'Fence'] , axis = 1 , inplace = True)","6e03fbdb":"train_data['PoolQC'].fillna('None' , inplace = True)\ntrain_data['MiscFeature'].fillna('None' , inplace = True)\ntrain_data['FireplaceQu'].fillna('None' , inplace = True)\n\ntest_data['PoolQC'].fillna('None' , inplace = True)\ntest_data['MiscFeature'].fillna('None' , inplace = True)\ntest_data['FireplaceQu'].fillna('None' , inplace = True)","18e9e556":"a = train_data.isnull().sum()\na[a > 0].sort_values(ascending = False)","dc66b000":"train_data[\"LotAreaCut\"] = pd.qcut(train_data.LotArea,10)\ntrain_data.groupby(['LotAreaCut'])[['LotFrontage']].agg(['mean','median','count'])","42624c7f":"train_data['LotFrontage'] = train_data.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Since some combinations of LotArea and Neighborhood are not available, so we just LotAreaCut alone.\ntrain_data['LotFrontage'] = train_data.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","3d8a8cff":"cols = ['MasVnrArea' , 'GarageCars' , 'GarageArea']\nfor col in cols:\n    train_data[col].fillna(0, inplace = True)\n    \ncols1 = ['MasVnrType' , 'GarageQual' , 'GarageCond' , 'GarageFinish' , 'GarageYrBlt' , \n         'GarageType' , 'BsmtExposure' , 'BsmtCond' , 'BsmtQual' , 'BsmtFinType2' , 'BsmtFinType1']\nfor col in cols1:\n    train_data[col].fillna('None' , inplace = True)\n    \ncols2 = ['Electrical']\nfor col in cols2:\n    train_data[col].fillna(train_data[col].mode()[0] , inplace = True)","1c6a146a":"b = test_data.isnull().sum()\nb[b > 0].sort_values(ascending = False)","b1a2427c":"test_data[\"LotAreaCut\"] = pd.qcut(test_data.LotArea,10)\ntest_data.groupby(['LotAreaCut'])[['LotFrontage']].agg(['mean','median','count'])","c553e341":"test_data['LotFrontage'] = test_data.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Since some combinations of LotArea and Neighborhood are not available, so we just LotAreaCut alone.\ntest_data['LotFrontage'] = test_data.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","c9fe676f":"cols=['MasVnrArea' , 'GarageCars' , 'GarageArea' , \n      'BsmtUnfSF' , 'BsmtFinSF2' , 'BsmtFinSF1' , \n      'TotalBsmtSF']\nfor col in cols:\n    test_data[col].fillna(0, inplace = True)\n    \ncols1 = ['MasVnrType' , 'GarageQual' , 'GarageCond' , 'GarageFinish' , 'GarageYrBlt' , 'GarageType' , \n         'BsmtExposure' , 'BsmtCond' , 'BsmtQual' , 'BsmtFinType2' , 'BsmtFinType1']\nfor col in cols1:\n    test_data[col].fillna(\"None\", inplace=True)\n    \n# fill in with mode\ncols2 = ['MSZoning' , \n         'BsmtFullBath' , 'BsmtHalfBath' , \n         'Utilities' , 'Functional' , 'KitchenQual' , 'SaleType' , \n         'Exterior1st' , 'Exterior2nd']\nfor col in cols2:\n    test_data[col].fillna(test_data[col].mode()[0], inplace=True)","ae42fb3b":"train_data.isnull().sum() , test_data.isnull().sum()","b6297b37":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15,8))\nsns.boxplot(train_data.YearBuilt, train_data.SalePrice)","d6a216d3":"plt.figure(figsize=(12,6))\nplt.scatter(x = train_data.GrLivArea, y = train_data.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","b2af6d32":"train_data.drop(train_data[(train_data['GrLivArea'] > 4000) & (train_data['SalePrice'] < 300000)].index , inplace = True)","fa582068":"corrmat = train_data.corr()\nf, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(corrmat, vmax=1.0, square=True)","44bdd95b":"corr = train_data.corr()\nfeatures = abs(corr['SalePrice']).sort_values(ascending = False)\nfeatures","39182cc1":"category_list = ['MSZoning' , 'Street' , 'LotShape' , 'LandContour' , 'Utilities' , 'LotConfig' , 'LandSlope' , 'Neighborhood' ,\n                 'Condition1' , 'Condition2' , 'BldgType' , 'HouseStyle' , 'RoofStyle' , 'RoofMatl' , 'Exterior1st' , 'Exterior2nd' ,    \n                 'MasVnrType' , 'ExterQual' , 'ExterCond' , 'Foundation' , 'BsmtQual' , 'BsmtCond' , 'BsmtExposure' , 'BsmtFinType1' ,     \n                 'BsmtFinType2' , 'Heating' , 'HeatingQC' , 'CentralAir' , 'Electrical' , 'KitchenQual' , 'Functional' ,\n                 'GarageType' , 'GarageFinish' , 'GarageQual' , 'GarageCond' , 'PavedDrive' , 'SaleType' , 'SaleCondition' ]","77cff938":"train_data['Street'].value_counts()","e122a366":"cate_drop_list = ['Street' , 'LandContour' , 'Utilities' , 'LandSlope' ,\n                  'Condition1' , 'Condition2' , 'BldgType' , 'RoofMatl' ,   \n                  'ExterCond' , 'BsmtCond' , 'BsmtExposure' ,     \n                  'BsmtFinType2' , 'Heating' , 'CentralAir' , 'Electrical' , 'Functional' , \n                  'GarageQual' , 'GarageCond' , 'PavedDrive' , 'SaleType' , 'SaleCondition']","6baa0dbc":"train_data.drop(cate_drop_list , axis = 1 , inplace = True)\ntest_data.drop(cate_drop_list , axis = 1 , inplace = True)","a77560a3":"Member_list1 = ['MSZoning','LotShape','LotConfig','RoofStyle','MasVnrType','ExterQual','BsmtQual',\n                'BsmtFinType1','HeatingQC','KitchenQual','GarageFinish','Foundation','GarageType',\n                'PoolQC','MiscFeature','FireplaceQu']","49bc7af4":"for i in Member_list1 :\n    train_data[i] = pd.factorize(train_data[i])[0].astype(np.int64)\n    test_data[i] = pd.factorize(test_data[i])[0].astype(np.int64)","cadabe6b":"train_data['Neighborhood'].replace({'MeadowV' : 1 , 'IDOTRR' : 2 , 'BrDale' : 2 ,\n                                    'OldTown' : 3 , 'Edwards' : 3 , 'BrkSide' : 3 ,\n                                    'Sawyer' : 4 , 'Blueste' : 4 , \n                                    'SWISU' : 4 , 'NAmes' : 4 ,\n                                    'NPkVill' : 5 , 'Mitchel' : 5 ,\n                                    'SawyerW' : 6 , 'Gilbert' : 6 , 'NWAmes' : 6 ,\n                                    'Blmngtn' : 7 , 'CollgCr' : 7 , \n                                    'ClearCr' : 7 , 'Crawfor' : 7 ,\n                                    'Veenker' : 8 , 'Somerst' : 8 , 'Timber' : 8 ,\n                                    'StoneBr' : 9 ,\n                                    'NoRidge' : 10 , 'NridgHt' : 10} , inplace = True)\n\ntrain_data['HouseStyle'].replace({'1.5Unf' : 1 , \n                                  '1.5Fin' : 2 , '2.5Unf' : 2 , 'SFoyer' : 2 , \n                                  '1Story' : 3 , 'SLvl' : 3 ,\n                                  '2Story' : 4 , '2.5Fin' : 4} , inplace = True)\n\ntrain_data['Exterior1st'].replace({'BrkComm' : 1 ,\n                                   'AsphShn' : 2 , 'CBlock' : 2 , 'AsbShng' : 2 ,\n                                   'WdShing' : 3 , 'Wd Sdng' : 3 , 'MetalSd' : 3 , \n                                   'Stucco' : 3 , 'HdBoard' : 3 ,\n                                   'BrkFace' : 4 , 'Plywood' : 4 ,                                                       \n                                   'VinylSd' : 5 ,\n                                   'CemntBd' : 6 ,\n                                   'Stone' : 7 , 'ImStucc' : 7} , inplace = True)\n\ntrain_data['Exterior2nd'].replace({'Other':1,\n                                   'AsphShn' : 2 , 'CBlock' : 2 , 'AsbShng' : 2 ,\n                                   'Wd Shng' : 3 , 'Wd Sdng' : 3 , 'MetalSd' : 3 , \n                                   'Stucco' : 3 , 'HdBoard' : 3 ,\n                                   'BrkFace' : 4 , 'Plywood' : 4 , 'Brk Cmn' : 4 ,\n                                   'VinylSd' : 5 ,\n                                   'CmentBd' : 6 ,\n                                   'Stone' : 7 , 'ImStucc' : 7} , inplace = True)","381dbf9c":"test_data['Neighborhood'].replace({'MeadowV' : 1 , 'IDOTRR' : 2 , 'BrDale' : 2 ,\n                                   'OldTown' : 3 , 'Edwards' : 3 , 'BrkSide' : 3 ,\n                                   'Sawyer' : 4 , 'Blueste' : 4 , \n                                   'SWISU' : 4 , 'NAmes' : 4 ,\n                                   'NPkVill' : 5 , 'Mitchel' : 5 ,\n                                   'SawyerW' : 6 , 'Gilbert' : 6 , 'NWAmes' : 6 ,\n                                   'Blmngtn' : 7 , 'CollgCr' : 7 , \n                                   'ClearCr' : 7 , 'Crawfor' : 7 ,\n                                   'Veenker' : 8 , 'Somerst' : 8 , 'Timber' : 8 ,\n                                   'StoneBr' : 9 ,\n                                   'NoRidge' : 10 , 'NridgHt' : 10} , inplace = True)\n\ntest_data['HouseStyle'].replace({'1.5Unf' : 1 , \n                                 '1.5Fin' : 2 , '2.5Unf' : 2 , 'SFoyer' : 2 , \n                                 '1Story' : 3 , 'SLvl' : 3 ,\n                                 '2Story' : 4 , '2.5Fin' : 4} , inplace = True)\n\ntest_data['Exterior1st'].replace({'BrkComm' : 1 ,\n                                  'AsphShn' : 2 , 'CBlock' : 2 , 'AsbShng' : 2 ,\n                                  'WdShing' : 3 , 'Wd Sdng' : 3 , 'MetalSd' : 3 , \n                                  'Stucco' : 3 , 'HdBoard' : 3 ,\n                                  'BrkFace' : 4 , 'Plywood' : 4 ,\n                                  'VinylSd' : 5 ,\n                                  'CemntBd' : 6 ,\n                                  'Stone' : 7 , 'ImStucc' : 7} , inplace = True)\n\ntest_data['Exterior2nd'].replace({'Other':1,\n                                  'AsphShn' : 2 , 'CBlock' : 2 , 'AsbShng' : 2 ,\n                                  'Wd Shng' : 3 , 'Wd Sdng' : 3 , 'MetalSd' : 3 , \n                                  'Stucco' : 3 , 'HdBoard' : 3 ,\n                                  'BrkFace' : 4 , 'Plywood' : 4 , 'Brk Cmn' : 4 ,\n                                  'VinylSd' : 5 ,\n                                  'CmentBd' : 6 ,\n                                  'Stone' : 7 , 'ImStucc' : 7} , inplace = True)","a064139a":"train_data.drop(['LotAreaCut'] , axis = 1 , inplace = True)\ntest_data.drop(['LotAreaCut'] , axis = 1 , inplace = True)","48b4ad09":"test_data[['HouseStyle' , 'Exterior1st']]","1f181a90":"test_data['HouseStyle'] = test_data['HouseStyle'].astype(np.int64)\ntest_data['Exterior1st'] = test_data['Exterior1st'].astype(np.int64)","bfa23732":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import Imputer","b4101130":"class labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        lab=LabelEncoder()\n        X[\"YearBuilt\"] = lab.fit_transform(X[\"YearBuilt\"])\n        X[\"YearRemodAdd\"] = lab.fit_transform(X[\"YearRemodAdd\"])\n        X[\"GarageYrBlt\"] = lab.fit_transform(X[\"GarageYrBlt\"])\n        return X\n\nclass skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.5):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        return X","ac97649e":"NumStr = ['YearBuilt' , 'YearRemodAdd' , 'GarageYrBlt']\nfor col in NumStr:\n    train_data[col] = train_data[col].astype(str)\n    test_data[col] = test_data[col].astype(str)","fc3ab6c2":"# build pipeline\npipe = Pipeline([('labenc', labelenc()),('skew_dummies', skew_dummies(skew=1))])\n\ntrain_data = pipe.fit_transform(train_data)\ntest_data = pipe.fit_transform(test_data)","0a3762d2":"train_data = train_data.join(pd.get_dummies(train_data['YrSold'] , prefix = 'YrSold'))\ntest_data = test_data.join(pd.get_dummies(test_data['YrSold'] , prefix = 'YrSold'))\ntrain_data.drop(['YrSold'] , axis = 1 , inplace = True)\ntest_data.drop(['YrSold'] , axis = 1 , inplace = True)","86ae7bc1":"train_data.head()","84a251c8":"test_data.head()","650ce3d7":"train_data.shape , test_data.shape","70d3a6f2":"SalePrice_train_data = train_data.SalePrice\ntrain_data.drop(['SalePrice'] , axis = 1 , inplace = True)\ntrain_data.insert(0 , 'SalePrice' , SalePrice_train_data)","78e6bffb":"corr = train_data.corr()\nfeatures_importance = abs(corr['SalePrice']).sort_values(ascending = False)\nfeatures_importance","653abacb":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score , train_test_split\nimport numpy as np\nX , y = train_data.iloc[: , 1:] , train_data.iloc[: , 0]\n\nstd = StandardScaler()\nX_std = std.fit_transform(X)\n\nmms = MinMaxScaler()\nX_mms = mms.fit_transform(X)\n\nnorm = Normalizer()\nX_norm = norm.fit_transform(X)","c48f8533":"lab_enc = LabelEncoder()\ny = lab_enc.fit_transform(y)","ba69bb1b":"neighbors = np.arange(1, 30)\nkfold = 10\nval_accuracy = { 'std' : [] , 'mms' : [] , 'norm' : [] }\n\nbestKnr = None\nbestAcc = 0.0\nbestScaling = None\n\n# \u5728\u4e0d\u540c\u7684K\u503c\u6761\u4ef6\u4e0b\u4e0e\u4e0d\u540c\u7684\u6807\u51c6\u5316\u540e\u7684\u6570\u503c\u5904\u7406\u540e\u7684\u6a21\u578b\u7cbe\u786e\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\nfor i, k in enumerate(neighbors):\n    \n    knr = KNeighborsRegressor(n_neighbors = k)\n    \n    # \u4ea4\u53c9\u9a8c\u8bc1\u96c6\u7684accuracy\n    \n    s1 = np.mean(cross_val_score(knr, X_std, y, cv=kfold))\n    val_accuracy['std'].append(s1)\n    \n    s2 = np.mean(cross_val_score(knr, X_mms, y, cv=kfold))\n    val_accuracy['mms'].append(s2)\n    \n    s3 = np.mean(cross_val_score(knr, X_norm, y, cv=kfold))\n    val_accuracy['norm'].append(s3)\n    \n    if s1 > bestAcc:\n        bestAcc = s1\n        bestKnr = knr\n        bestScaling = 'std'\n        \n    elif s2 > bestAcc:\n        bestAcc = s2\n        bestKnr = knr\n        bestScaling = 'mms'\n        \n    elif s3 > bestAcc:\n        bestAcc = s3\n        bestKnr = knr\n        bestScaling = 'norm'\n\n# Plotting\nplt.figure(figsize=[13,8])\n\nplt.plot(neighbors, val_accuracy['std'], label = 'CV Accuracy with std')\nplt.plot(neighbors, val_accuracy['mms'], label = 'CV Accuracy with mms')\nplt.plot(neighbors, val_accuracy['norm'], label = 'CV Accuracy with norm')\n\nplt.legend()\nplt.title('k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neighbors)\nplt.show()\n\nprint('Best Accuracy with feature scaling:', bestAcc)\nprint('Best kNN classifier:', bestKnr)\nprint('Best scaling:', bestScaling)","07a71cd4":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.001)\nlasso.fit(X , y)\n\nFI_lasso = pd.DataFrame({'Feature Importance' : lasso.coef_}, index = train_data.columns[1:61])\nFI_lasso.sort_values('Feature Importance' , ascending = False)\n\nFI_lasso[FI_lasso['Feature Importance'] != 0].sort_values('Feature Importance').plot(kind = 'barh' , figsize = (15 , 25))\nplt.xticks(rotation = 90)\nplt.show()","9f537037":"from sklearn import linear_model, svm, gaussian_process\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\nX_train , X_test , y_train , y_test = train_test_split(X_std , y , test_size = 0.2 , random_state = 42)\n\nclfs = {\n        'svm':svm.SVR(), \n        'RandomForestRegressor':RandomForestRegressor(n_estimators=400),\n        'BayesianRidge':linear_model.BayesianRidge()\n       }\nfor clf in clfs:\n    try:\n        clfs[clf].fit(X_train, y_train)\n        y_pred = clfs[clf].predict(X_test)\n        print(clf + \" cost:\" + str(np.sum(y_pred-y_test)\/len(y_pred)) )\n    except Exception as e:\n        print(clf + \" Error:\")\n        print(str(e))","3619d86d":"x = train_data.iloc[: , 1:].values\ny = train_data.iloc[: , 0].values\nX_train , X_test , y_train , y_test = train_test_split(x , y , test_size = 0.33 , random_state = 42)\n\nclf = RandomForestRegressor(n_estimators = 400)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# \u4fdd\u5b58clf\uff0c\u5171\u4e0b\u9762\u8ba1\u7b97\u6d4b\u8bd5\u96c6\u6570\u636e\u4f7f\u7528\n# rfr = clf","128078b9":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, title, X, y, ylim = None, cv = None, n_jobs = 1, \n                        train_sizes = np.linspace(.05, 1., 20), verbose = 0, plot = True):\n    \"\"\"\n    \u753b\u51fadata\u5728\u67d0\u6a21\u578b\u4e0a\u7684learning curve.\n    \u53c2\u6570\u89e3\u91ca\n    ----------\n    estimator : \u4f60\u7528\u7684\u5206\u7c7b\u5668\u3002\n    title : \u8868\u683c\u7684\u6807\u9898\u3002\n    X : \u8f93\u5165\u7684feature\uff0cnumpy\u7c7b\u578b\n    y : \u8f93\u5165\u7684target vector\n    ylim : tuple\u683c\u5f0f\u7684(ymin, ymax), \u8bbe\u5b9a\u56fe\u50cf\u4e2d\u7eb5\u5750\u6807\u7684\u6700\u4f4e\u70b9\u548c\u6700\u9ad8\u70b9\n    cv : \u505across-validation\u7684\u65f6\u5019\uff0c\u6570\u636e\u5206\u6210\u7684\u4efd\u6570\uff0c\u5176\u4e2d\u4e00\u4efd\u4f5c\u4e3acv\u96c6\uff0c\u5176\u4f59n-1\u4efd\u4f5c\u4e3atraining(\u9ed8\u8ba4\u4e3a3\u4efd)\n    n_jobs : \u5e76\u884c\u7684\u7684\u4efb\u52a1\u6570(\u9ed8\u8ba41)\n    \"\"\"\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    if plot:\n        plt.figure()\n        plt.title(title)\n        if ylim is not None:\n            plt.ylim(*ylim)\n        plt.xlabel(u\"number\")\n        plt.ylabel(u\"score\")\n        plt.gca().invert_yaxis()\n        plt.grid()\n    \n        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n                         alpha=0.1, color=\"b\")\n        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n                         alpha=0.1, color=\"r\")\n        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=u\"train\u2014score\")\n        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=u\"incross\u2014score\")\n    \n        plt.legend(loc=\"best\")\n        \n        plt.draw()\n        plt.gca().invert_yaxis()\n        plt.show()\n    \n    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) \/ 2\n    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n    return midpoint, diff\n\nplot_learning_curve(clf, \"learning curve\", X_train, y_train)","74040754":"features_drop_list = ['MasVnrArea' , 'YrSold_2007' , 'TotalBsmtSF' , 'GarageYrBlt' , '1stFlrSF' ,\n                      '2ndFlrSF' , 'GarageArea' , 'BsmtFinSF1' , 'YrSold_2010' , 'BsmtUnfSF']","a5c6f639":"train_data.drop(features_drop_list , axis = 1 , inplace = True)\ntest_data.drop(features_drop_list , axis = 1 , inplace = True)","b8c044da":"x = train_data.iloc[: , 1:].values\ny = train_data.iloc[: , 0].values\nX_train , X_test , y_train , y_test = train_test_split(x , y , test_size = 0.33 , random_state = 42)\n\nclf = RandomForestRegressor(n_estimators = 400)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# \u4fdd\u5b58clf\uff0c\u5171\u4e0b\u9762\u8ba1\u7b97\u6d4b\u8bd5\u96c6\u6570\u636e\u4f7f\u7528\n# rfr = clf","859cddbb":"plot_learning_curve(clf, \"learning curve\", X_train, y_train)","77dfa584":"add_train_features = pd.DataFrame()\n \nadd_train_features['GrLivArea_OverallQual']    = train_data['GrLivArea']    * train_data['OverallQual']\nadd_train_features['Neighborhood_OverallQual'] = train_data['Neighborhood'] + train_data['OverallQual']\nadd_train_features['LotArea_OverallQual']      = train_data['LotArea']      * train_data['OverallQual']\nadd_train_features['MSZoning_OverallQual']     = train_data['MSZoning']     + train_data['OverallQual']\nadd_train_features['MSZoning_YearBuilt']       = train_data['MSZoning']     + train_data['YearBuilt']\nadd_train_features['Neighborhood_YearBuilt']   = train_data['Neighborhood'] + train_data['YearBuilt']\nadd_train_features['Rooms']                    = train_data['FullBath']     + train_data['TotRmsAbvGrd']\nadd_train_features['PorchArea']                = train_data['OpenPorchSF']  + train_data['EnclosedPorch'] + train_data['3SsnPorch'] + train_data['ScreenPorch']","85b6c6a6":"add_test_features = pd.DataFrame()\n \nadd_test_features['GrLivArea_OverallQual']    = test_data['GrLivArea']    * test_data['OverallQual']\nadd_test_features['Neighborhood_OverallQual'] = test_data['Neighborhood'] + test_data['OverallQual']\nadd_test_features['LotArea_OverallQual']      = test_data['LotArea']      * test_data['OverallQual']\nadd_test_features['MSZoning_OverallQual']     = test_data['MSZoning']     + test_data['OverallQual']\nadd_test_features['MSZoning_YearBuilt']       = test_data['MSZoning']     + test_data['YearBuilt']\nadd_test_features['Neighborhood_YearBuilt']   = test_data['Neighborhood'] + test_data['YearBuilt']\nadd_test_features['Rooms']                    = test_data['FullBath']     + test_data['TotRmsAbvGrd']\nadd_test_features['PorchArea']                = test_data['OpenPorchSF']  + test_data['EnclosedPorch'] + train_data['3SsnPorch'] + train_data['ScreenPorch']","fe60a1fb":"train_data = pd.concat([train_data , add_train_features] , axis = 1)\ntest_data = pd.concat([test_data , add_test_features] , axis = 1)","ab622161":"corr = train_data.corr()\nfeatures = abs(corr['SalePrice']).sort_values(ascending = False)\nfeatures","91a11264":"# \u5404\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\n\ncorrmat = train_data.corr()\nf, ax = plt.subplots(figsize = (30, 24))\nsns.heatmap(corrmat , linewidth = 0.01 , vmax = 1.0 , vmin = 0.0 , fmt = '.2f' , square = True)","5df55f12":"x = train_data.iloc[: , 1:].values\ny = train_data.iloc[: , 0].values\n\n# \u6807\u51c6\u5316\nstd = StandardScaler()\nx = std.fit_transform(x)","d43fd93f":"# define cross validation strategy\ndef rmse_cv(model,x,y):\n    rmse = np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","9d6436eb":"from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR , LinearSVR\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\n\n# \u5bfc\u5165GridSearchCV\u6a21\u5757\u4e3a\u4e0b\u9762\u7684\u8c03\u53c2\u505a\u51c6\u5907\uff0c\u5f53\u7136xgboost\u53ef\u4ee5\u81ea\u52a8\u5730\u8c03\u53c2\uff0c\u66f4\u52a0\u65b9\u4fbf\u5feb\u6377\u3002\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\n\nmodels = [LinearRegression(),\n          Ridge(),\n          Lasso(alpha=0.01,max_iter=10000),\n          RandomForestRegressor(),\n          GradientBoostingRegressor(),\n          SVR(),\n          LinearSVR(),\n          ElasticNet(alpha=0.001,max_iter=10000),\n          SGDRegressor(max_iter=1000,tol=1e-3),\n          BayesianRidge(),\n          KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n          ExtraTreesRegressor(),\n          XGBRegressor()]\n\nnames = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\nfor name, model in zip(names, models):\n    score = rmse_cv(model, x , y)\n    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))","6b6c65ec":"from sklearn.model_selection import GridSearchCV\nclass grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,x,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(x,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","111dc635":"# LinearRegression\nparam_grid = {'normalize' : [True , False]}\ngrid(LinearRegression()).grid_get(x ,y , param_grid)","8a33a456":"# Ridge\nparam_grid = {'alpha' : [35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 80 , 90]}\ngrid(Ridge()).grid_get(x ,y , param_grid)","fd759906":"# ElasticNet\nparam_grid = {'alpha' : [0.001 , 0.004 , 0.005 , 0.008] , \n              'l1_ratio' : [0.1 , 0.3 , 0.5] ,\n              'max_iter' : [2000 , 4000 , 6000]}\ngrid(ElasticNet()).grid_get(x ,y , param_grid)","1cbb48eb":"# SGDRegressor\nparam_grid = {'alpha' : [0.005 , 0.01 , 0.02] , \n              'l1_ratio' : [0.1 , 0.3 , 0.5 , 0.8] ,\n              'max_iter' : [3000 , 5000 , 8000 , 9000]}\ngrid(SGDRegressor()).grid_get(x ,y , param_grid)","16886bfd":"# BayesianRidge\nparam_grid = {'alpha_1' : [6 , 7 , 8 , 9 , 10] , \n              'lambda_1' : [6 , 7 , 8 , 9 , 10]}\ngrid(BayesianRidge()).grid_get(x ,y , param_grid)","00e76cc2":"# Kernel Ridge\nparam_grid = {'alpha' : [2.0 , 3.0 , 4.0] , \n              'kernel' : ['polynomial'] , \n              'degree' : [2 , 3 , 4] , \n              'coef0' : [2.0 , 3.0 , 4.0]}\ngrid(KernelRidge()).grid_get(x ,y , param_grid)","73c824b4":"from sklearn.base import RegressorMixin , clone\n\nclass AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,x,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(x,y)\n        return self\n    \n    def predict(self,x):\n        w = list()\n        pred = np.array([model.predict(x) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","1a07c17b":"# LinearRegression\n\nlr = LinearRegression(normalize = False)\nridge = Ridge(alpha = 35)\nela = ElasticNet(alpha = 0.005 , l1_ratio = 0.3 , max_iter = 2000)\nsgdr = SGDRegressor(alpha = 0.02 , l1_ratio = 0.3 , max_iter = 8000)\nbayr = BayesianRidge(alpha_1 = 10 , lambda_1 = 6)\nker = KernelRidge(alpha = 4.0 , kernel = 'polynomial' , degree = 2 , coef0 = 4.0)\n\n# assign weights based on their gridsearch score\nw1 = 0.164\nw2 = 0.165\nw3 = 0.168\nw4 = 0.162\nw5 = 0.168\nw6 = 0.173\n\nweight_avg = AverageWeight(mod = [lr , ridge , ela , sgdr , bayr , ker] , weight = [w1 , w2 , w3 , w4 , w5 , w6])\n\nscore = rmse_cv(weight_avg , x , y)\nprint(score.mean())","1758ecbf":"weight_avg = AverageWeight(mod = [ela , ker] , weight = [0.3 , 0.7])\nscore = rmse_cv(weight_avg , x , y)\nprint(score.mean())","70d2043b":"from sklearn.model_selection import KFold\n\nclass stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model , n_folds = 5):\n        self.mod = mod\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in kfold.split(X, y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean","0737323a":"stack_model = stacking(mod = [ela , ker] , meta_model = ker)\nscore = rmse_cv(stack_model,x,y)\nprint(score.mean())","729367ff":"stack_model.fit(x,y)","aa9a273e":"test_data['PorchArea'].fillna(test_data['PorchArea'].mean() , inplace = True)","498b83dd":"test_x = std.fit_transform(test_data)","0bf02914":"pred = stack_model.predict(test_x)","1dc95fb4":"prediction = pd.DataFrame(pred , columns = ['SalePrice'])\nresult = pd.concat([Id , prediction] , axis = 1)\nresult.to_csv('submission10.csv',index=False)","7f119b7f":"#### We choose 13 models and use 5-folds cross-calidation to evaluate these models.\n1. LinearRegression\n2. Ridge\n3. Lasso\n4. RandomForestRegressor\n5. GradientBoostingRegressor\n6. SVR\n7. Linear Support Vector Regression(LinearSVR)\n8. ElasticNet\n9. Stochastic Gradient Descent Regressor(SGDRegressor)\n10. BayesianRidge\n11. KernelRidge\n12. ExtraTreesRegressor\n13. XgBoost","264a370e":"# Understanding the data set\n1. MSSubClass\uff1aIdentifies the type of dwelling involved in the sale           \u6807\u8bc6\u51fa\u552e\u4e2d\u6d89\u53ca\u7684\u4f4f\u5b85\u7c7b\u578b\n2. MSZoning\uff1aIdentifies the general zoning classification of the sale         \u6807\u8bc6\u9500\u552e\u7684\u4e00\u822c\u5206\u533a\u5206\u7c7b\n3. LotFrontage\uff1aLinear feet of street connected to property                   \u8fde\u63a5\u5230\u7269\u4e1a\u7684\u8857\u9053\u7684\u7ebf\u6027\u82f1\u5c3a\n4. LotArea\uff1aLot size in square feet                                           \u5e73\u65b9\u82f1\u5c3a\u5927\u5c0f\n5. Street\uff1aType of road access to property                                    \u901a\u5f80\u7269\u4e1a\u7684\u9053\u8def\u7c7b\u578b\n6. Alley\uff1aType of alley access to property                                    \u80e1\u540c\u8fdb\u5165\u8d22\u4ea7\u7684\u7c7b\u578b\n7. LotShape\uff1aGeneral shape of property                                        \u5c5e\u6027\u7684\u4e00\u822c\u5f62\u72b6\n8. LandContour\uff1aFlatness of the property                                      \u7269\u4e1a\u7684\u5e73\u5766\u5ea6\n9. utilities\uff1aType of utilities available                                     \u53ef\u7528\u7684\u5b9e\u7528\u7a0b\u5e8f\u7c7b\u578b\n10. LotConfig\uff1aLot configuration                                              \u6279\u6b21\u914d\u7f6e\n11. LandSlope\uff1aSlope of property                                              \u7269\u4e1a\u7684\u5761\u5ea6\n12. Neighbors\uff1aPhysical locations within Ames city limits                     \u57c3\u59c6\u65af\u5e02\u533a\u8303\u56f4\u5185\u7684\u5730\u7406\u4f4d\u7f6e\n13. Condition1\uff1aProximity to various conditions                               \u63a5\u8fd1\u5404\u79cd\u6761\u4ef6\uff1f\n14. Condition2\uff1aProximity to various conditions (if more than one is present) \u63a5\u8fd1\u5404\u79cd\u6761\u4ef6\uff08\u5982\u679c\u5b58\u5728\u591a\u4e2a\u6761\u4ef6\uff09\uff1f\n15. BldgType\uff1aType of dwelling                                                \u4f4f\u5b85\u7c7b\u578b\n16. HouseStyle\uff1aStyle of dwelling                                             \u4f4f\u5b85\u98ce\u683c\n17. OverallQual\uff1aRates the overall material and finish of the house           \u8bc4\u4f30\u623f\u5c4b\u7684\u6574\u4f53\u6750\u6599\u548c\u88c5\u9970\n18. OverallCond\uff1aRates the overall condition of the house                     \u8bc4\u4f30\u623f\u5c4b\u7684\u6574\u4f53\u72b6\u51b5\n19. Yearbulit\uff1aOriginal construction date                                     \u539f\u59cb\u65bd\u5de5\u65e5\u671f\n20. YearRemodAdd\uff1aRemodel date (same as construction date if no remodeling or additions)                                                                                                               \u6539\u578b\u65e5\u671f\uff08\u5982\u679c\u6ca1\u6709\u6539\u578b\u6216\u589e\u52a0\uff0c\u5219\u4e0e\u5efa\u9020\u65e5\u671f\u76f8\u540c\uff09\n21. RoofStyle\uff1aType of roof                                                           \u5c4b\u9876\u7c7b\u578b\n22. RoofMatl\uff1aRoof material                                                           \u5c4b\u9876\u6750\u6599\n23. Exterior1st\uff1aExterior covering on house                                           \u623f\u5c4b\u5916\u5899\n24. Exterior2nd\uff1aExterior covering on house (if more than one material)               \u623f\u5c4b\u5916\u5899\uff08\u5982\u679c\u6709\u591a\u79cd\u6750\u6599\uff09\n25. MasVnrType\uff1aMasonry veneer type                                                   \u77f3\u5de5\u9970\u9762\u7c7b\u578b\n26. MasVnrArea\uff1aMasonry veneer area in square feet                                    \u780c\u9762\u8d34\u9762\u9762\u79ef\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\n27. ExterQual\uff1aEvaluates the quality of the material on the exterior                  \u8bc4\u4f30\u5916\u90e8\u6750\u6599\u7684\u8d28\u91cf\n28. ExterCond\uff1aEvaluates the present condition of the material on the exterior        \u8bc4\u4f30\u5916\u90e8\u6750\u6599\u7684\u5f53\u524d\u72b6\u6001\n29. Foundation\uff1aType of foundation                                                    \u57fa\u7840\u7c7b\u578b\n30. BsmtQual\uff1aEvaluates the height of the basement                                    \u8bc4\u4f30\u5730\u4e0b\u5ba4\u7684\u9ad8\u5ea6\n31. BsmtCond\uff1aEvaluates the general condition of the basement                         \u8bc4\u4f30\u5730\u4e0b\u5ba4\u7684\u4e00\u822c\u72b6\u51b5\n32. BsmtExposure\uff1aRefers to walkout or garden level walls                             \u662f\u6307\u7f62\u5de5\u6216\u82b1\u56ed\u697c\u5c42\u5899\u58c1\n33. BsmtFinType1\uff1aRating of basement finished area                                    \u5730\u4e0b\u5ba4\u5b8c\u6210\u533a\u57df\u7684\u7b49\u7ea7\n34. BsmtFinSF1\uff1aType 1 finished square feet                                           1\u578b\u6210\u54c1\u5e73\u65b9\u82f1\u5c3a\n35. BsmtFinType2\uff1aRating of basement finished area (if multiple types)                \u5730\u4e0b\u5ba4\u6210\u54c1\u533a\u57df\u7684\u7b49\u7ea7\uff08\u5982\u679c\u6709\u591a\u79cd\u7c7b\u578b\uff09\n36. BsmtFinSF2\uff1aType 2 finished square feet                                           2\u578b\u6210\u54c1\u5e73\u65b9\u82f1\u5c3a\n37. BsmtUnfSF\uff1aUnfinished square feet of basement area                                \u5730\u4e0b\u5ba4\u672a\u5b8c\u6210\u7684\u5e73\u65b9\u82f1\u5c3a\uff08\u9762\u79ef\uff09\n38. TotalBsmtSF\uff1aTotal square feet of basement area                                   \u5730\u4e0b\u5ba4\u603b\u9762\u79ef\n39. Heating\uff1aType of heating                                                          \u52a0\u70ed\u65b9\u5f0f\n40. HeatingQC\uff1aHeating quality and condition                                          \u52a0\u70ed\u8d28\u91cf\u548c\u6761\u4ef6\n41. CentralAir\uff1aCentral air conditioning                                              \u4e2d\u592e\u7a7a\u8c03\n42. Electrical\uff1aElectrical system                                                     \u7535\u6c14\u7cfb\u7edf\n43. 1stFlrSF\uff1a First Floor square feet                                                \u4e00\u697c\u5e73\u65b9\u82f1\u5c3a\n44. 2ndFlrSF\uff1aSecond floor square feet                                                \u4e8c\u697c\u5e73\u65b9\u82f1\u5c3a\n45. LowQualFinSF\uff1aLow quality finished square feet (all floors)                       \u4f4e\u8d28\u91cf\u6210\u54c1\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\uff1f\uff08\u6240\u6709\u697c\u5c42\uff09\n46. GrLivArea\uff1aAbove grade (ground) living area square feet                           \u5730\u9762\u4ee5\u4e0a\u7684\u5c45\u4f4f\u9762\u79ef\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\n47. BsmtFullBath\uff1aBasement full bathrooms                                             \u5730\u4e0b\u5ba4\u5b8c\u6574\u7684\u6d74\u5ba4\n48. BsmtHalfBath\uff1aBasement half bathrooms                                             \u5730\u4e0b\u5ba4\u534a\u6d74\u5ba4\n49. FullBath\uff1a Full bathrooms above grade                                             \u5e74\u9650\u4e0a\u6d74\n50. HalfBath\uff1aHalf baths above grade                                                  \u5e74\u9650\u4ee5\u4e0a\u534a\u6d74\n51. BedroomAbvGr\uff1aBedrooms above grade (does NOT include basement bedrooms)           \u5730\u4e0a\u5367\u5ba4\uff08\u4e0d\u5305\u62ec\u5730\u4e0b\u5ba4\uff09\n52. Kitchen\uff1aKitchens above grade                                                     \u5730\u4e0a\u53a8\u623f\n53. KitchenQual\uff1aKitchen quality                                                      \u53a8\u623f\u8d28\u91cf\n54. TotRmsAbvGrd\uff1aTotal rooms above grade (does not include bathrooms)                \u5ba2\u623f\u603b\u6570\uff08\u4e0d\u5305\u62ec\u6d74\u5ba4\uff09\n55. Functional\uff1aHome functionality (Assume typical unless deductions are warranted)   \u5c45\u5bb6\u529f\u80fd\u6027\uff08\u5047\u5b9a\u4e3a\u5178\u578b\u503c\uff0c\u9664\u975e\u9700\u8981\u6263\u9664\uff09\n56. Fireplaces\uff1aNumber of fireplaces                                      \u58c1\u7089\u6570\u91cf\n57. FireplaceQu\uff1aFireplace quality                                        \u58c1\u7089\u8d28\u91cf\n58. GarageType\uff1aGarage location                                           \u8f66\u5e93\u4f4d\u7f6e\n59. GarageYrBlt\uff1aYear garage was built                                    \u8f66\u5e93\u5efa\u6210\u5e74\u4efd\n60. GarageFinish\uff1aInterior finish of the garage                           \u8f66\u5e93\u5185\u90e8\u88c5\u4fee\n61. GarageCars\uff1aSize of garage in car capacity                            \u8f66\u5e93\u5bb9\u8f66\u5927\u5c0f\n62. GarageArea\uff1aSize of garage in square feet                             \u8f66\u5e93\u5927\u5c0f\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\n63. GarageQual\uff1aGarage quality                                            \u8f66\u5e93\u8d28\u91cf\n64. GarageCond\uff1aGarage condition                                          \u8f66\u5e93\u6761\u4ef6\n65. PavedDrive\uff1aPaved driveway                                            \u94fa\u780c\u7684\u8f66\u9053\n66. WoodDeckSF\uff1aWood deck area in square feet                             \u6728\u5236\u7532\u677f\u9762\u79ef\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\n67. OpenPorchSF\uff1aOpen porch area in square feet                           \u5f00\u653e\u5f0f\u9633\u53f0\u9762\u79ef\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\n68. EnclosedPorch\uff1aEnclosed porch area in square feet                     \u5c01\u95ed\u5f0f\u95e8\u5eca\u9762\u79ef\uff08\u4ee5\u5e73\u65b9\u82f1\u5c3a\u4e3a\u5355\u4f4d\uff09\n69. 3SsnPorch\uff1aThree season porch area in square feet                     \u4e09\u5b63\u95e8\u5eca\u9762\u79ef\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\n70. ScreenPorch\uff1a Screen porch area in square feet                        \u5c4f\u5e55\u95e8\u5eca\u9762\u79ef\uff08\u4ee5\u5e73\u65b9\u82f1\u5c3a\u4e3a\u5355\u4f4d\uff09\n71. PoolArea\uff1aPool area in square feet                                    \u6cf3\u6c60\u9762\u79ef\uff08\u5e73\u65b9\u82f1\u5c3a\uff09\n72. PoolQC\uff1aPool quality                                                  \u6cf3\u6c60\u8d28\u91cf\n73. Fence\uff1aFence quality                                                  \u56f4\u680f\u8d28\u91cf\n74. MiscFeature\uff1aMiscellaneous feature not covered in other categories    \u5176\u4ed6\u672a\u6db5\u76d6\u7c7b\u522b\u7684\u529f\u80fd\n75. MiscVal\uff1aValue of miscellaneous feature                               \u6742\u9879\u529f\u80fd\u7684\u4ef7\u503c\n76. MoSold\uff1aMonth Sold (MM)                                               \u5df2\u552e\u6708\u4efd\uff08MM\uff09\n77. YrSold\uff1aYear Sold (YYYY)                                              \u5e74\u9500\u552e\uff08YYYY\uff09\n78. SaleType\uff1aType of sale                                                \u9500\u552e\u7c7b\u578b\n79. SaleCondition\uff1aCondition of sale                                      \u9500\u552e\u6761\u4ef6","e436bf3d":"\u4f7f\u7528\u5efa\u7acb\u540e\u7684\u6a21\u578b\u9884\u6d4btest_data","d47a5854":"1. View missing in the train_data","cace4e08":"To make the two approaches comparable (by using the same number of models) , we just average ElasticNet and KernelRidge , then we add BayesianRidge as meta-model.","17b85738":"It can be seen that the data is overfitting. We delete some features with too low feature importance and draw a learning curve again to see if it is overfitting. Repeat the operation until it does not fit.","bd1ebdc9":"\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f Feature importance ranking","78535af0":"# Less simple Stacking : Adding a Meta-model\nIn this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\nSplit the total training set into two disjoint sets (here train and .holdout )\n\nTrain several base models on the first part (train)\nTest these base models on the second part (holdout)\n\nUse the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.","19ed2b03":"\u5efa\u6a21\u524d\u5230\u6807\u51c6\u5316\u6bd4\u8f83 Standardization comparison before modeling","ebeff3ab":"## \u5bf9\u7c7b\u522b\u7279\u5f81\u8fdb\u884c\u5220\u9664\u4e00\u90e8\u5206\u540e\u8fd8\u5269\u4e0b\u4ee5\u4e0b\u7684\u7279\u5f81\u5217\u3002\n## After deleting a part of the category features, the following feature columns remain.\n1. 'MSZoning','LotShape','LotConfig','Neighborhood','HouseStyle','RoofStyle','Exterior1st'\n2. 'Exterior2nd','MasVnrType','ExterQual','Foundation','BsmtQual','BsmtFinType1','HeatingQC' \n3. 'KitchenQual','GarageType','GarageFinish'","761283b6":"### Fill None to 'PoolQC', 'MiscFeature' and 'FireplaceQu'.","22ee0fe5":"Looking at the heatmap, we know that some features have a strong correlation, and consider whether they can be deleted.","0a42bb47":"## \u5904\u7406Year\u76f8\u5173\u7279\u5f81 Handling Year-Related Features","4ad03558":"\u521b\u5efa\u5e76\u589e\u52a0\u4e00\u4e9b\u7279\u5f81 Create and add some features","52b5074b":"# We can see that data with many features in the data set are missing.\n# View missing data sets\uff08train_data , test_data\uff09","b17d6c24":"# See if there are missing values for test_data and train_data","f7b0d7d2":"# <center><font>Summary of Regression Algorithms and Kaggle House Price Prediction<\/font><\/center>","93751759":"# Ensemble learning","cd75383e":"The above is the first version of the model. After submission, adjust the model based on the results.","9776c385":"### Cancel the Feature 'Alley' and 'Fence'","01537fce":"2. View the missing of test_data","f0dabf91":"Plotting the learning curve","4ae5dedb":"Selecting LR \uff0cRidge , Ela \uff0cSGD , Bay \uff0cKer to tuning parameters","53e4fe7c":"\u67e5\u770b\u7279\u5f81\u5de5\u7a0b\u540e\u7684\u6570\u636e","bc218f14":"### It can be seen that there are more missing values for the following features in train_data\n* PoolQC          1456\n* MiscFeature     1408\n* Alley           1352\n* Fence           1169\n* FireplaceQu      730\n* LotFrontage      227\n\n### It can be seen that there are more missing values for the following characteristics in test_data\n* PoolQC          1456\n* MiscFeature     1408\n* Alley           1352\n* Fence           1169\n* FireplaceQu      730\n* LotFrontage      227\n\n### Analyzing the data set we can know:\n1. PoolQC\u7f3a\u5931\u7684\u503c\u5bf9\u5e94\u7684\u662fPoolArea = 0 \uff0c\u5373\u6ca1\u6709\u6cf3\u6c60\u7684\u623f\u5b50\u81ea\u7136\u6cf3\u6c60\u7684\u8d28\u91cf\u4e0d\u77e5\u9053\u4e86\u3002\u6211\u4eec\u5728\u8fd9\u91cc\u586b\u5145None\u8fdbPoolQC\u3002\n2. MiscFeature\u7f3a\u5931\u7684\u503c\u5bf9\u5e94\u7684\u662fMicsVal = 0 \uff0c\u5373\u6742\u9879\u529f\u80fd\u7684\u4ef7\u503c\u4e3a0\u65f6\uff0cMiscFeature\u7684\u6570\u636e\u672a\u77e5\u3002\u6211\u4eec\u586b\u5145None\u8fdbMicsFeature\u3002\n3. Alley\u4e0eFence\u6ca1\u6709\u4e0e\u4e4b\u76f8\u5e94\u7684\u7279\u5f81\u7684\u5173\u7cfb\uff0c\u800c\u4e14\u5b83\u4eec\u7684\u7f3a\u5931\u503c\u8fc7\u591a\uff0c\u6211\u4eec\u9009\u62e9\u5220\u9664\u8fd9\u4e24\u4e2a\u7279\u5f81\u3002\n4. FireplaceQu\u4e0ePoolQC\u7684\u60c5\u51b5\u7c7b\u4f3c\uff0c\u662f\u56e0\u4e3a\u5f53\u58c1\u7089\u6570\u91cf\u4e3a0\u65f6\uff08Fireplaces = 0\uff09\u4e0d\u5b58\u5728\u58c1\u7089\u8d28\u91cf\u7684\u6570\u636e\u3002\u6211\u4eec\u586b\u5145None\u8fdbFireplaceQu\u3002\n5. LotFrontage\u7f3a\u5931\u503c\u4e0d\u591a\uff0c\u53ef\u4ee5\u540c\u5176\u4ed6\u7f3a\u5931\u7684\u7279\u5f81\u4e00\u6837\u91c7\u53d6\u586b\u8865\u5904\u7406\u3002\n1. The missing value of PoolQC corresponds to PoolArea = 0, that is, the quality of the natural pool in a house without a pool is unknown. Here we fill None into PoolQC.\n2. The missing value of MiscFeature corresponds to MicsVal = 0, that is, the value of MiscFeature is unknown when the value of miscellaneous functions is 0. We populate None into MicsFeature.\n3. Alley and Fence have no relationship with their corresponding features, and they have too many missing values, so we choose to delete these two features.\n4. FireplaceQu is similar to PoolQC, because there is no fireplace quality data when the number of fireplaces is 0 (Fireplaces = 0). We populate None into FireplaceQu.\n5. LotFrontage does not have many missing values, and can be treated as other missing features.","bb076f59":"1. See the importance of numerical features","4214c4c3":"Sort the importance of the features again, select the appropriate features for modeling, or add some features appropriately.","6b3bddf8":"Arrange data \uff08train_data\uff09","f1b11b3e":"2. Analysis of categorical data","0967c7a2":"plotting the learning curve again","94c37292":"### Fill the feature 'LotFrontage' of test_data","ad907de8":"# \u67e5\u770b\u7b80\u5355\u7279\u5f81\u5de5\u7a0b\u540e\u7684\u6570\u636e\u5e76\u5220\u9664\u4e00\u4e9b\u4e0d\u9700\u8981\u7684\u7279\u5f81 View the data after simple feature engineering and delete some unwanted features\n1. \u53ef\u4ee5\u770b\u51faLotAreaCut\u5b58\u5728\uff0c\u4f46\u662f\u6211\u4eec\u5e76\u4e0d\u9700\u8981\u5b83\uff0c\u53ef\u4ee5\u5220\u9664\u3002\n2. test_data\u7684HouseStyle\u548cExterior1st\u7684\u7c7b\u578b\u4e3aobject\u3002\n3. GarageYear\u7684type\u662fobject\uff0c\u6211\u4eec\u4e00\u8d77\u5904\u7406Year\u6709\u5173\u7684\u4e09\u4e2a\u7279\u5f81\n[\u53c2\u8003\u94fe\u63a5](https:\/\/www.kaggle.com\/massquantity\/all-you-need-is-pca-lb-0-11421-top-4)","77d000b0":"# \u7279\u5f81\u6807\u51c6\u5316 Feature normalization","82b0c300":"# \u52a0\u8f7d\u6570\u636e","1200500b":"\u53ef\u4ee5\u770b\u51fa\u968f\u7740\u5e74\u9650\u7684\u589e\u52a0\uff0c\u623f\u5b50\u7684\u4ef7\u683c\u4e5f\u662f\u4e0a\u6da8\u7684\u3002\u5f53\u7136\u7406\u6240\u5f53\u7136\u7684\uff0c\u623f\u5b50\u7684\u4ef7\u683c\u4e5f\u6709\u9762\u79ef\u6709\u7740\u5f88\u5927\u7684\u5173\u7cfb\u3002\u6211\u4eec\u4e5f\u7b80\u5355\u5730\u67e5\u770b\u4e00\u4e0b\u3002\nIt can be seen that as the years increase, the price of the house also rises. Of course, the price of a house has a lot to do with its area. We also take a brief look.","c03d9ec4":"# making model","b680dcbe":"![hosue price](http:\/\/5b0988e595225.cdn.sohucs.com\/images\/20181119\/4668edcd51b145e8b2d3b9133d258d27.png)","a3ad872a":"#### Member_list2 = ['Neighborhood','HouseStyle','Exterior1st','Exterior2nd','Foundation','GarageType']","700de2ca":"# Check Data","14f3d20e":"### Fill the feature 'LotFrontage' of train_data","44ead9d8":"[\u6d88\u9664\u7279\u5f02\u70b9\u7684\u53c2\u8003\u94fe\u63a5](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)","02da7246":"It can be seen from the above figure that \u2018std\u2019 standardization is the best method.","042319e1":"Fill in other features","cb1adcf2":"\u4ece\u5b66\u4e60\u66f2\u7ebf\u53ef\u4ee5\u770b\u51fa\u6a21\u578b\u8fd8\u662f\u8fc7\u62df\u5408\uff0c\u4f46\u662f\u6211\u5728\u505a\u4e86\u7279\u5f81\u5220\u9664\u540e\u76f4\u63a5\u4f7f\u7528\u4e86RandomForestRegressor\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4e5f\u8bb8\u5176\u4ed6\u7684\u6a21\u578b\u7531\u66f4\u597d\u7684\u8868\u73b0\uff0c\u6211\u4f7f\u7528\u4e86\u5404\u79cd\u4e0d\u540c\u7684\u7b97\u6cd5\u6765\u8bad\u7ec3\u6a21\u578b\u3002\u6700\u540e\u9009\u62e9\u4e86\u4e0d\u9519\u7684\u6a21\u578b\u8fdb\u884c\u6700\u540e\u7684\u96c6\u6210\u6a21\u578b\u7684\u5efa\u7acb\u3002\n\u7ecf\u8fc7\u4e0d\u540c\u7684\u8c03\u8bd5\u4e4b\u540e\uff0c\u6211\u51b3\u5b9a\u589e\u52a0\u4e00\u4e9b\u7279\u5f81\uff0c\u5e76\u91cd\u65b0\u6392\u5e8f\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002\u7ed8\u5236\u5b66\u4e60\u66f2\u7ebf\u5e76\u91cd\u590d\u4ee5\u4e0a\u7684\u64cd\u4f5c\u76f4\u5230\u83b7\u5f97\u4e0d\u9519\u7684\u7ed3\u679c\u3002\nIt can be seen from the learning curve that the model is still over-fitting, but I have used RandomForestRegressor to train the model after performing the feature deletion. Maybe other models perform better. I used a variety of different algorithms to train the model. Finally, a good model was selected for the establishment of the final integrated model.\nAfter different debugging, I decided to add some features and reorder the importance of the features. Draw a learning curve and repeat the above operation until you get a good result.","eb28811f":"Fill in other features","7fe89b43":"View the correlation between the created feature and 'SalePrice'","431a5344":"# Feature Engineering:\n1. Data analysis\n2. Missing data filling\n3. Conversion of category data\n4. Standardization of data\n5. Feature creation and deletion\n\n# Summary of Regression Algorithms\uff1a\n1. Linear regression\n2. Ridge regression\n3. Lasso returns\n4. Polynomial regression\n5. Decision tree\n6. Neural Network\n7. ElasticNet Regression\n8. Bayesian linear regression\n9. Bagging (Bagging meta-estimator and RandomForest)  \n10. Boosting  \n   10.1 AdaBoost\n   10.2 GBM\n   10.3 XGB  (a good algorithm for kaggle)\n   10.4 Light GBM\n   10.5 CatBoost","262c57d6":"# Submit results","26957146":"# Simple visualization of SalePrice","3ae8bebb":"# First simple modeling","59054d75":"Street and SalePrice","2901a3a0":"## Encode and convert the class features of train_data and test_data.","ed76ebf1":"### View remaining missing data","4223e501":"It can be seen from the results that RandomForestRegressor works best.","f8f0f61e":"# Feature engineering","e19eac99":"\u53ef\u4ee5\u770b\u51faStreet\u7684\u503c\u5448\u73b0\u5355\u6781\u5316\u73b0\u8c61\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4e00\u4e2a\u7279\u5f81\u8f6c\u6362\u4e4b\u540e\u5448\u73b0\u51e0\u4e4e\u5168\u4e3a0\/1\u7684\u6570\u503c\u7279\u5f81\uff0c\u4f7f\u5f97\u77e9\u9635\u7a00\u758f(\u4f7f\u7528get_dummies\u7b49)\uff0c\u6211\u4eec\u51b3\u5b9a\u5220\u9664\u8fd9\u4e00\u7c7b\u7279\u5f81\u3002\u9700\u8981\u5220\u9664\u7684\u7279\u5f81\u6709\uff1a\nIt can be seen that the value of Street shows a unipolar phenomenon, which will result in a numerical feature that is almost all 0\/1 after feature conversion, making the matrix sparse (using get_dummies, etc.), we decided to delete this type of feature. The features that need to be deleted are:"}}