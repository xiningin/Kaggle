{"cell_type":{"67bf3289":"code","3bd5b61d":"code","fa4b182c":"code","50bd4844":"code","f254c5c1":"code","6f9a8f1f":"code","4a8f958f":"code","ed14a5ed":"code","592d60f9":"code","ad46801c":"code","b3f86777":"code","498697d3":"code","93b3bb4b":"code","1185a939":"code","d16059a6":"code","8a3f1934":"code","e6ed2617":"code","5cdc274a":"code","9e3abe93":"code","c09aad65":"code","56d389a1":"code","a2950190":"code","f3df0a60":"code","686d9f51":"code","97356541":"code","cafdc216":"code","1fece5dc":"code","74bceca8":"code","e2bf1a03":"code","e262c9a9":"code","d1bb6a3e":"code","f9cfc321":"code","1b3b84fe":"code","6ff2fa35":"code","a9e59059":"code","f2063170":"markdown","31430ebe":"markdown","1fd95452":"markdown","8ad3bf96":"markdown","7f514553":"markdown","756715eb":"markdown","3d9b5717":"markdown","0c551c86":"markdown","3998909a":"markdown","4f3ed3ae":"markdown","3659bbc2":"markdown","e2301ec1":"markdown","cf82edda":"markdown","ecfeaab1":"markdown","b022843e":"markdown","ed3fb4ac":"markdown","74d365cf":"markdown","a6ded8a8":"markdown"},"source":{"67bf3289":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.metrics import accuracy_score, roc_auc_score, plot_roc_curve, confusion_matrix, plot_confusion_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ncreditcard = '..\/input\/creditcardfraud\/creditcard.csv'\n\n","3bd5b61d":"# Read the dataset \ndf = pd.read_csv(creditcard)\ndf.head()","fa4b182c":"# Printing quick information about the dataset\ndf.info()","50bd4844":"# Checking missing values in each column\ndf.isnull().sum()","f254c5c1":"# Identify duplicate values and mark all the duplicates as true\n# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.duplicated.html\ndf[df.duplicated(keep=False)]","6f9a8f1f":"# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.drop_duplicates.html\ndf = df.drop_duplicates(keep='first')","4a8f958f":"# Check the distribution of the credit card fraud cases \nclass_proportion = df['Class'].value_counts()\nclass_proportion","ed14a5ed":"# Plotting a barchart to see the the distribution of the credit card fraud cases\nplt.style.use('seaborn')\nfont1 = {'family': 'serif', \n        'fontstyle': 'italic',\n        'fontsize': 16,\n        'fontweight': 'bold',\n        'color': 'DarkBlue'}\nfont2 = {'weight': 'bold', 'size': 12}\nfont3 = {'weight': 'normal', 'size': 12}\n\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.barplot(class_proportion.index, class_proportion.values, palette='Set2')\nax.set_title('Distribution of Credit Card Fraud Class', fontdict=font1)\nax.set_xlabel('Fraud class', fontdict=font2)\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nax.set_ylabel('Number of transactions', fontdict=font2)\nax.set_yscale('log')\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels=['0: Legitimate', '1: Fraudulent'], prop= font3, \n          title ='Fraud Class:', title_fontsize=14,\n          frameon=True, facecolor='white')\nplt.show()","592d60f9":"# Check the proportion of the fraud cases and identify the imbalance\ndf['Class'].value_counts(normalize=True)","ad46801c":"# Arrange the dataset into features matrix and target vector\n# Drop the 'Time' variable as it does not that much help our analysis\nX = df.drop(columns=(['Time', 'Class']))\ny = df['Class']","b3f86777":"# Make a SMOTE instance, then fit and apply it in one step \n# to create an oversampled version of our dataset.\n\nsm = SMOTE(sampling_strategy='auto', random_state=3, k_neighbors=5)\nX_oversampled , y_oversampled = sm.fit_resample(X, y)","498697d3":"# Summarize the fraud class distribution of the new SMOTE-transformed dataset\nunique_original, counts_original = np.unique(y, return_counts=True)\nunique_oversampled, counts_oversampled = np.unique(y_oversampled, return_counts=True)\n\nprint('Original fraud class distribution:', dict(zip(unique_original, counts_original)))\nprint('New transformed fraud class distribution:',dict(zip(unique_oversampled, counts_oversampled)))","93b3bb4b":"# Now visualize the SMOTE-transformed target variable\nplt.style.use('seaborn')\nfont1 = {'family': 'serif', \n        'fontstyle': 'italic',\n        'fontsize': 16,\n        'fontweight': 'bold',\n        'color': 'DarkBlue'}\nfont2 = {'weight': 'bold', 'size': 12}\n\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.countplot(y_oversampled, palette='Set2', ax=ax)\nax.set_title('Distribution of Imbalanced Fraud Class Transformed by SMOTE', fontdict=font1)\nax.set_xlabel('Fraud class', fontdict=font2)\nax.set_xticklabels(['Legitimate', 'Fraudulent'])\nax.set_ylabel('Number of transactions', fontdict=font2)\nplt.show()","1185a939":"# Separate the transformed features matrix and target vector into random train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, random_state=3)","d16059a6":"# Instantiate and fit the model\nrfc = RandomForestClassifier(n_estimators=150)\nrfc.fit(X_train, y_train)","8a3f1934":"# Model Evalution -classification accuracy\ntraining_rfc_accuracy = rfc.score(X_train, y_train)\ntesting_rfc_accuracy = rfc.score(X_test, y_test)\n\nprint(\"Training RFC Accuracy:\", training_rfc_accuracy)\nprint(\"Testing RFC Accuracy:\", testing_rfc_accuracy )","e6ed2617":"# Plotting the confusion matrix \nfig, ax = plt.subplots(figsize=(8, 8))\nfont1 = {'family': 'serif', \n        'fontstyle': 'italic',\n        'fontsize': 16,\n        'fontweight': 'bold',\n        'color': 'DarkRed'}\nfont2 = {'weight': 'bold', 'size': 12}\n\nsns.heatmap(confusion_matrix(y_test, rfc.predict(X_test)), \n            cmap='Blues', \n            square=True,\n            annot=True,\n            fmt='d',\n            cbar_kws={'shrink': 0.8},\n            xticklabels=['Predicted 0s', 'Predicted 1s'],\n            yticklabels=['Actual 0s', 'Actual 1s'])\nax.set_title('RFC Confusion Matrix', fontdict=font1)\nplt.show()","5cdc274a":"# Model evaluation - Sensitivity, Specificity and Precision \n\nTN, FP, FN, TP = confusion_matrix(y_test, rfc.predict(X_test)).flatten()\nprint(\"True Negatives:\", TN)\nprint(\"False Positives:\", FP)\nprint(\"False Negatives:\", FN)\nprint(\"True Positives:\", TP)\n\nsensitivity = TP\/(TP + FN)\nspecificity = TN\/(TN + FP)\nprecision = TP\/(TP + FP)\nprint(\"\\nSensitivity:\", sensitivity)\nprint(\"Specificity:\", specificity)\nprint(\"Precision:\", precision)","9e3abe93":"# Check the predicted probabilities for every observation in the test data subset \n# Note that the default classification threshold is 0.5\n\ntesting_probabilities= rfc.predict_proba(X_test)\ntesting_probabilities","c09aad65":"# Convert the testing probabilities into a dataframe\ntesting_probabilities_df = pd.DataFrame(testing_probabilities, columns=['1 - \ud835\udc5d(X_test)', '\ud835\udc5d(X_test)'])\ntesting_probabilities_df.head()","56d389a1":"# Get predictions\nrfc.predict(X_test)","a2950190":"# Model evaluation -AUC\n# Calculate AUC for both training and testing subsets\n# Only probabilities being in the positive class is needed for the calculation, that is the second column\ntraining_rfc_AUC = roc_auc_score(y_train, rfc.predict_proba(X_train)[:, 1]) \ntesting_rfc_AUC = roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1])\n\nprint(\"Training RFC AUC:\", training_rfc_AUC)\nprint(\"Testing RFC AUC:\", testing_rfc_AUC)","f3df0a60":"# Separate the transformed features matrix and target vector into random train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, random_state=3)","686d9f51":"# define dictionary of hyperparameters\nparams = {'penalty': ['l1', 'l2'],\n          'C': [0.0001, 0.001, 0.01, 10, 50, 100],\n          'class_weight': [None, 'balanced']}","97356541":"# Instantiate Logistic Regression model. N.B: the default solver doesn't support l1 regularization\n# Instantiate Grid Search to find the best hyperparameters and fit the model\nlgr = LogisticRegression(solver='liblinear')\ngs = GridSearchCV(lgr, params, cv = 5)\ngs.fit(X_train, y_train)","cafdc216":"# Model evaluation - accuracy \ntraining_lgr_accuracy = gs.score(X_train, y_train)\ntesting_lgr_accuracy = gs.score(X_test, y_test)\n\nprint(\"Training LGR Accuracy:\", training_lgr_accuracy)\nprint(\"Testing LGR Accuracy:\", testing_lgr_accuracy)","1fece5dc":"# Plotting the confusion matrix \nfont1 = {'family': 'serif', \n        'fontstyle': 'italic',\n        'fontsize': 16,\n        'fontweight': 'bold',\n        'color': 'DarkRed'}\n\nplot_confusion_matrix(gs, X_test, y_test, values_format='d')\nplt.title('LGR Confusion Matrix', fontdict=font1)\nplt.grid(False)\nplt.show()","74bceca8":"# Model evaluation - Sensitivity, Specificity and Precision \nTN, FP, FN, TP = confusion_matrix(y_test, gs.predict(X_test)).flatten()\nprint(\"True Negatives:\", TN)\nprint(\"False Positives:\", FP)\nprint(\"False Negatives:\", FN)\nprint(\"True Positives:\", TP)\n\nsensitivity = TP\/(TP + FN)\nspecificity = TN\/(TN + FP)\nprecision = TP\/(TP + FP)\nprint(\"\\nSensitivity:\", sensitivity)\nprint(\"Specificity:\", specificity)\nprint(\"Precision:\", precision)","e2bf1a03":"# Model evaluation -AUC\n# Calculate AUC for both training and testing subsets\n# Only probabilities being in the positive class is needed for the calculation, that is the second column\ntraining_lgr_AUC = roc_auc_score(y_train, gs.predict_proba(X_train)[:, 1]) \ntesting_lgr_AUC = roc_auc_score(y_test, gs.predict_proba(X_test)[:, 1])\n\nprint(\"Training LGR AUC:\", training_lgr_AUC)\nprint(\"Testing LGR AUC:\", testing_lgr_AUC)","e262c9a9":"# Separate the transformed features matrix and target vector into random train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, random_state=3)","d1bb6a3e":"# Instantiate and fit the model\ngbc = GradientBoostingClassifier(n_estimators=150)\ngbc.fit(X_train, y_train)","f9cfc321":"# Model evaluation - accuracy \ntraining_gbc_accuracy = gbc.score(X_train, y_train)\ntesting_gbc_accuracy = gbc.score(X_test, y_test)\n\nprint(\"Training GBC Accuracy:\", training_gbc_accuracy)\nprint(\"Testing GBC Accuracy:\", testing_gbc_accuracy)","1b3b84fe":"# Plotting the confusion matrix for Gradient Boosting \nfig, ax = plt.subplots(figsize=(7, 7))\nfont1 = {'family': 'serif', \n        'fontstyle': 'italic',\n        'fontsize': 16,\n        'fontweight': 'bold',\n        'color': 'DarkBlue'}\nfont2 = {'weight': 'bold', 'size': 12}\n\nsns.heatmap(confusion_matrix(y_test, gbc.predict(X_test)), \n            cmap='Oranges', \n            square=True,\n            annot=True,\n            fmt='d',\n            cbar_kws={'shrink': 0.8},\n            xticklabels=['Predicted 0s', 'Predicted 1s'],\n            yticklabels=['Actual 0s', 'Actual 1s'])\nax.set_title('GBC Confusion Matrix', fontdict=font1)\nplt.show()","6ff2fa35":"# Model evaluation - Sensitivity, Specificity and Precision \nTN, FP, FN, TP = confusion_matrix(y_test, gbc.predict(X_test)).flatten()\nprint(\"True Negatives:\", TN)\nprint(\"False Positives:\", FP)\nprint(\"False Negatives:\", FN)\nprint(\"True Positives:\", TP)\n\nsensitivity = TP\/(TP + FN)\nspecificity = TN\/(TN + FP)\nprecision = TP\/(TP + FP)\nprint(\"\\nSensitivity:\", sensitivity)\nprint(\"Specificity:\", specificity)\nprint(\"Precision:\", precision)","a9e59059":"# Visualize the ROC curve \nplt.style.use('seaborn')\nfont1 = {'family': 'serif', \n        'fontstyle': 'italic',\n        'fontsize': 16,\n        'fontweight': 'bold',\n        'color': 'DarkBlue'}\nfont2 = {'weight': 'bold', 'size': 12}\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplot_roc_curve(rfc, X_test, y_test, color='darkgreen', linewidth=3, ax=ax, label='RandomForestClassifier (AUC = 1.0)')\nplot_roc_curve(gbc, X_test, y_test, color='orange', linewidth=3, ax=ax, label='GradientBoostingClassifier (AUC = 99.94)')\nplot_roc_curve(gs, X_test, y_test, color='magenta', linewidth=3, ax=ax, label='LogisticRegression (AUC = 99.16)')\nplt.plot([0, 1], [0, 1], ls='--', color='red', linewidth=3, label='Baseline (AUC = 0.5)')\nax.legend(loc='lower right')\nax.set_title('ROC Curve for 3 Binary Classification Models', fontdict=font1)\nax.set_xlabel('False Positive Rate', fontdict=font2)\nax.set_ylabel('True Positive Rate', fontdict=font2)\nplt.show()","f2063170":"For highly imbalanced data classification with few samples of the minority class, using accuracy to evaluate the performance of the models can be misleading because we can get a pretty high accuracy without actually making useful predictions. In this case, the Area Under the Curve (AUC) value is one of the most common metrics to evaluate the models, where the values near 0.5 are equivalent to randomly guessing whether the transaction is a fraud or not, and values close to 1 are suggestive of highly performing models. To put it in another words, as a general rule, a good binary classification model stays as far away from the model baseline as possible toward the top-left corner. AUC value less than 0.5 is very bad!\n\nFrom the above AUC Curve, we can see that the Random Forest and Gradient Boosting models have the highest AUC values, making them the best models in accurately predicting the difference between the classes.","31430ebe":"# **Data Cleaning and Exploratory Data Analysis**","1fd95452":"From the above result, we can observe that the number of fraudulent transactions is relatively rare. It only makes up about 0.2% of the entire transactions. This is super imbalanced. If we take this proportion as a model baseline, we will get a pretty high accuracy of 99.80% by creating a model which just predicts everything to be 0 whatever input data comes in. But this will fail to capture the minority class, although typically it is the model performance on the minority class which is the most important.\n\nTo summarize, in a dataset with highly imbalanced cases, accuracy score can be misleading because a high accuracy rate is achieved not because our model is any good but simply because the classifier algorithm will always predict the majority class without performing any analysis of the features.","8ad3bf96":"From the class distribution summary and barchart above, we can see the dataset is transformed using the SMOTE and the new class distribution is balanced, having now with equal samples in the minority class.","7f514553":"From the barchart, we can see that the number of legitimate credit card transactions is extremely higher the number of fraudulent transactions. This is obviously expected because fraud detection is one of the problem domains where the class distribution is inherently imbalanced. I would have been surprised if the fraudulent transactions happened to be higher the legitimate ones because it indicates that these banking institutions were facing serious security breaches which could lead to loss of revenue, interruptions in operations and loss of both reputation and customers. Anyways, we need to deal with this huge data imbalance as it can hamper the accuracy of our classification model.","756715eb":"**Visualizing the ROC Curve**","3d9b5717":"# **Feature Engineering and Data Modeling**","0c551c86":"# **Logist Regression (LGR) Model**","3998909a":"From the result above, we can see that the dataset contains many duplicate values. So, I'm going to remove the repeated observations except for the first occurrence of each instance.","4f3ed3ae":"From the confusion matrix and the statistics above, we can see that our model is 100% accurate in detecting fraud. This means our model is very highly sensitive, which is in fact what the banking institutions may be more concerned for because a false negative is more dangerous than a false positive.\n\nOf course, in a credit card fraud detection system, a smart model with good classification accuracy rate should also have significantly less false positives because the errors can cost the banks billions of dollars in lost revenue as customers getting declined in legitimate transactions may get frustrated and then refrain from using that credit card anymore.","3659bbc2":"Automating hyperparameters:\n\n>I will use GridSearch to find the Logistic Regression model with the best performing hyperparameter combination. Here, note that we don't have to scale the data again as it were already done during the Principal Component Analysis transformation due confidentiality issues","e2301ec1":"# **Random Forest Classifier(RFC) Model**","cf82edda":"# **Conclusion**\n> Having evaluated the performance of each of our classification models using the metric accuracy, below are the values I have got: \n>>- Random Forest training accuracy: 100%\n>>- Random Forest testing accuracy: ~99.99%\n>>- Logistic Regression training accuracy: ~95.86%\n>>- Logistic Regression testing accuracy: ~95.77%\n>>- Gradient Boosting training accuracy: ~98.87%\n>>- Gradient Boosting  testing accuracy: ~98.83%\n>>- XGBoost training accuracy: ~100%\n>>- XGBoost testing accuracy: ~99.98\n\n>>N.B: The model baseline accuracy is 99.80%.\n\n> From the aforelisted accuracy values, we can see that all of the four models are not overfit, which is an indication of well-performing models. Nevertheless, in this case our goal is to have a model which which is set to beat the model baseline accuracy of 99.80% on previously unseen data. So, by comparing testing accuracy values of each model to the model baseline accuracy, we can observe that Random Forest and eXtreme Gradient Boosting models have higher accuracy values than that of the model baseline which is assumed to be predicting every transaction to be non-fraudulent whatever input data is being fed in. Plus, Random Forest Classifier seems to be performing better on unseen data as it has slightly higher testing accuracy value.\n\n> In conclusion, an ideally perfect credit card fraud detection model will have a zero negative rate and a very low positive rate because it is obvious that cases of fraud escaping detection are more harmful than the cases being falsely flagged as fraud. To put it simply, a false negative is very costly and a false positive is not so much costly. Because of the growing fraud risks, financial institutions sometimes tend to implement overly aggressive fraud detection algorith that flags every single transaction as potentially fraudulent, resulting in high false positive rates. This means legitimate users and bona-fide transactions are being blocked wrongly, leading to a loss of profit and a reputational hit. So, it is also important to note that a seamless user experience is very essential for digital channels because people always consider a good customer experience to be crucial for their loyalty to our service. Putting this tradeoff into consideration, we should develop a model that effectively filters transactions and improves the crux of our businesses while at the same time keeping our customers happy. ","ecfeaab1":"As we can see from the above result, the matrix of probabilities being returned indicates the predicted class is equal to 0 or 1. In this matrix, each each row corresponds to a single observation: The first column is the probability of the predicted output being zero(non-fraud), that is 1 - \ud835\udc5d(\ud835\udc65_test), and the second column is the probability that the output is one(fraud), or \ud835\udc5d(\ud835\udc65_test).","b022843e":"Earlier I mentioned that accuracy can be a misleading metric to evaluate a model for a dataset with highly imbalanced classes. But in our case, I've managed to beat the model baseline accuracy score(99.80%). From the result above, we can see that the accuracy score I got is about 99.99%, which is higher than the model baseline accuracy score.\n\nNext, I will be using the following four most useful metrics to evaluate the performance of a model for highly imbalanced binary-class datasets:\n\n1. Sensitivity (True Positive Rate)\n2. Specificity (True Negative Rate)\n3. Precision (Positive Predictive Value)\n4. Area Under ROC Curve (AUROC)","ed3fb4ac":"**Balancing out the dataset with resampling techniques**\n\n>There are many approaches to solve this type of highly imbalanced binary classification problem. The simplest and most widely adopted techniques are undersampling and oversampling. The former involves removing some records from the majority class, while the latter is adding more random copies to the minority class. Both techniques are done until the majority and minority class is balanced out. But I'm going to oversample the minority class using Synthetic Minority Oversampling Technique(SMOTE). This is a more sophisticated resampling technique which introduces small variations into the copies of the minority class observations instead of those exact copies, generating more diverse sythetic samples. The SMOTE class lives in the imblearn.over_sampling module of the Python imbalanced-learn libray(imported as imblearn), which in fact relies on scikit-learn(imported as sklearn) and is part of scikit-learn-contrib projects.","74d365cf":"From the results above, we can see that there are more false negatives than false positives. This cannot be a smart model because ignoring the probability of a transaction being fraud when there actually is one is very dangerous. It is a very serious data breach! So we need to lower the probability threshold in order to increase the true positive rate.","a6ded8a8":"# **Gradeint Boosting Classifier (GBC) Model**"}}