{"cell_type":{"624075fa":"code","027a17da":"code","80c46e2e":"code","2b93e196":"code","c39ca263":"code","14c26fdb":"code","b9bf5b1c":"code","0bfec0eb":"code","a8b3760f":"code","09870d65":"code","4c47e3a8":"code","b43e4cc0":"code","ec0be3d9":"code","ba62f9fd":"code","fdc1ba29":"code","37a84d7c":"code","94a42783":"code","e059723c":"code","38ac6bdf":"code","bb85b402":"code","db181297":"code","be5fb609":"code","f195ce21":"code","a3a28298":"code","f22616ae":"code","ed3622df":"code","a3063545":"code","0e47b3da":"code","1910bb66":"code","5f45f10c":"code","839acbf4":"markdown","49d60f4e":"markdown","df1136d5":"markdown","1767c4f8":"markdown","c35bafe6":"markdown","541cef7d":"markdown","cc532cd8":"markdown","52bc3e18":"markdown","ac7fbc50":"markdown","74352c1f":"markdown","3d20544a":"markdown","0c60c2cb":"markdown","155781f9":"markdown","b6465c63":"markdown","80573676":"markdown","357a4303":"markdown","49daa027":"markdown"},"source":{"624075fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom keras import models, layers\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\nimport matplotlib.font_manager as fm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","027a17da":"best_font = fm.FontProperties(fname='..\/input\/cusersmarildownloadsstaatlichesstaatttf\/staat.ttf')","80c46e2e":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\ndf = pd.read_csv('..\/input\/reviewstripadvisor\/tripadvisor_hotel_reviews.csv')\ndf = df[['Review','Rating']]\ndf = df.dropna()\n### I changed positive's sample count !  The original was sample 2370\npositive = df[(df['Rating'] == 5)].sample(3214,random_state=100)\nnegative = df[(df['Rating'] == 2) | (df['Rating'] == 1)]\ndf = pd.concat([negative,positive])\ndf = df.reset_index(drop=True)\n\ndf['Rating'] = df['Rating'].apply(lambda x : 1 if x==5 else 0)","2b93e196":"df.tail()","c39ca263":"df['Rating'].value_counts()","14c26fdb":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\ndef df_processing(text):\n    return_arr = []\n    text = re.sub(r\"[^a-zA-Z]\",\" \",text)\n    text = re.sub(r\" {2,}\",\" \",text)\n    text = text.lower()\n    words = word_tokenize(text)\n    s = PorterStemmer()\n    stopword = stopwords.words('english')\n    for t in words:\n        if t not in stopword:\n            return_arr.append(t)\n            \n    return_arr = [s.stem(w) for w in return_arr]\n    return return_arr\n\ndf['processing'] = df['Review'].apply(df_processing)","b9bf5b1c":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\nlen_arr = []\nfor i in range(len(df)):\n    len_arr.append(len(df.loc[i,'processing']))\n    \nimport seaborn as sns\nsns.histplot(len_arr);","0bfec0eb":"#X = pd.DataFrame(X)\n#X = X.fillna(0)\n#X = X.loc[:,:250]\n#X= X.to_numpy()\n#y = np.array(df['Rating'])\n#X = X.reshape(5584,251,1)\n#X.shape, y.shape","a8b3760f":"df","09870d65":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\ndef df_processing2(text):\n    return_arr = []\n    text = re.sub(r\"[^a-z]\",\" \",text)\n    text = re.sub(r\" {2,}\",\" \",text)\n    return text\n\nnegative_sentences = df[df['Rating'] ==0]['processing']\npositive_sentences = df[df['Rating'] ==1]['processing']\n\nnegative_sentences = str(list(negative_sentences))\nnegative_sentences = df_processing2(negative_sentences)\n\nnegative_sentences = negative_sentences.split(' ')\nnegative_sentences = pd.DataFrame(negative_sentences)[0].value_counts()\nnegative_sentences = pd.DataFrame(negative_sentences)\n\npositive_sentences = str(list(positive_sentences))\npositive_sentences = df_processing2(positive_sentences)\n\npositive_sentences = positive_sentences.split(' ')\npositive_sentences = pd.DataFrame(positive_sentences)[0].value_counts()\npositive_sentences = pd.DataFrame(positive_sentences)\n\ncon = pd.concat([negative_sentences,positive_sentences],axis=1)\n\ncon.columns = ['negative','positive']\n\ncon = con.dropna()\n\ncon['negative_value'] = con[['negative','positive']].apply(lambda x : x[0]\/(x[0]+x[1]) *-1,axis=1)\ncon['positive_value'] = con[['negative','positive']].apply(lambda x : x[1]\/(x[0]+x[1]),axis=1)\n\ncon['total_value'] = con['negative_value'] + con['positive_value']\ncon = con.reset_index()\ncon = con.drop(['negative','positive','negative_value','positive_value'],axis=1)\nword_index =con['index'].to_list()\ncon = np.array(con)","4c47e3a8":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\nfrom tqdm import tqdm\n\nX = []\nfor sen in tqdm(df['processing']):\n    word_arr =[]\n    for word in sen:\n        if word in word_index:\n            word_arr.append(float(con[con[:,0] ==word,1]))\n        else:\n            word_arr.append(0)\n    X.append(word_arr)","b43e4cc0":"#To write 250 I checked on the histplot len array some snippets above.\n\n#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\nX = pd.DataFrame(X)\nX = X.fillna(0)\nX = X.loc[:,:250]\nX= X.to_numpy()\ny = np.array(df['Rating'])\n#X = X.reshape(6428,251,1)  That was the original reshape\nX = X.reshape(-1,251,1)\nX.shape, y.shape","ec0be3d9":"#There was a fit error, fixed by JeongBin Park. Change Activation function and optimizer learning rate.\n\n#from tensorflow.keras.layers import Embedding, Dense,LSTM\n#from tensorflow.keras.models import Sequential\n\n#model = Sequential()\n#model.add(LSTM(251,input_shape=X.shape[1:],activation='relu'))\n#model.add(Dense(1, activation='sigmoid'))\n#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n#model.summary()","ba62f9fd":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\nfrom tensorflow.keras.layers import Embedding, Dense,LSTM\nfrom tensorflow.keras.models import Sequential\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(LSTM(251,input_shape=X.shape[1:]))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.00003),\n              loss='binary_crossentropy', metrics=['acc'])\n\nmodel.fit(X,y, batch_size=256, epochs=20, validation_split=0.3)","fdc1ba29":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\nnames = [weight.name for layer in model.layers for weight in layer.weights]\nweights = model.get_weights()\n\nkernel_weights = weights[0]\nrecurrent_kernel_weights = weights[1]\nbias = weights[2]\n\nn = 1\nunits = 251  # LSTM layers  \n\nWi = kernel_weights[:, 0:units]\nWf = kernel_weights[:, units:2 * units]\nWc = kernel_weights[:, 2 * units:3 * units]\nWo = kernel_weights[:, 3 * units:]\n\n\nUi = recurrent_kernel_weights[:, 0:units]\nUf = recurrent_kernel_weights[:, units:2 * units]\nUc = recurrent_kernel_weights[:, 2 * units:3 * units]\nUo = recurrent_kernel_weights[:, 3 * units:]\n\n\nbi = bias[0:units]\nbf = bias[units:2 * units]\nbc = bias[2 * units:3 * units]\nbo = bias[3 * units:]","37a84d7c":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","94a42783":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\ndef make_plot(number):\n    ht_1 = np.zeros(n * units).reshape(n, units)\n    Ct_1 = np.zeros(n * units).reshape(n, units)\n\n    h_t_value = []\n\n    influence_h_t_value = []\n    for t in range(0, len(X[number,:])):\n        xt = np.array(X[number,t])\n        ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate\n        influence_ft = (np.dot(ht_1, Uf))\/(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf) * ft\n\n        it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate\n        influence_it = (np.dot(ht_1, Ui))\/(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi) * it\n\n        ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate\n        influence_ot = np.dot(ht_1, Uo) \/ (np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo) * ot\n\n        gt =  np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)\n        influence_gt =np.dot(ht_1, Uc) \/ (np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc) * gt\n\n        Ct = ft * Ct_1 + it * gt\n        influence_ct = influence_ft * Ct_1 + influence_it * influence_gt\n        ht = ot * np.tanh(Ct)\n        influence_ht = influence_ot * (influence_ct\/Ct) * ht\n\n        influence_h_t_value.append(influence_ht)\n\n        ht_1 = ht  # hidden state, previous memory state\n        Ct_1 = Ct  # cell state, previous carry state\n\n        h_t_value.append(ht)\n\n    influence_h_t_value.append(h_t_value[-1])\n    for i in range(len(influence_h_t_value)-1,0,-1):\n        influence_h_t_value[i] = influence_h_t_value[i] - influence_h_t_value[i-1]\n\n    influence_h_t_value = influence_h_t_value[1:]\n\n    impact_columns = np.dot(influence_h_t_value,weights[3]) + (weights[4]\/units)\n\n    if model.predict(X[number:number+1]) > 0.5:\n        b_color = 'lightgreen'\n    else:\n        b_color ='lightcyan'\n\n    fig = plt.figure(figsize=(15,3),facecolor=b_color)\n\n    for k in range(len(df.loc[number,'processing'])):\n        s = df.loc[number,'processing'][k]\n        va = round(float(impact_columns[k]),2)\n        if va > 0.5:\n            color ='green'\n        elif va< -0.3:\n            color ='blue'\n        else:\n            color ='black'\n\n        if k < 17:\n            plt.text(s=s, x=k*0.7, y=0,font=best_font,fontsize=20,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7, y=-0.1,font=best_font,fontsize=20,color=color,va='center',ha='center')\n        elif k < 34:\n            plt.text(s=s, x=k*0.7 - 17*0.7, y=-0.2,font=best_font,fontsize=20,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7- 17*0.7, y=-0.3,font=best_font,fontsize=20,color=color,va='center',ha='center')\n        else:\n            plt.text(s=s, x=k*0.7 - 34*0.7, y=-0.4,font=best_font,fontsize=20,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7- 34*0.7, y=-0.5,font=best_font,fontsize=20,color=color,va='center',ha='center')\n\n    plt.xlim(0,10)\n    plt.ylim(-0.5,0.1)\n    plt.axis('off')\n    plt.show()","e059723c":"make_plot(6427)","38ac6bdf":"make_plot(6000)","bb85b402":"make_plot(220)","db181297":"make_plot(200)","be5fb609":"make_plot(216)","f195ce21":"make_plot(120)","a3a28298":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook\n\ndef make_plot(number):\n    ht_1 = np.zeros(n * units).reshape(n, units)\n    Ct_1 = np.zeros(n * units).reshape(n, units)\n\n    h_t_value = []\n\n    influence_h_t_value = []\n    for t in range(0, len(X[number,:])):\n        xt = np.array(X[number,t])\n        ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate\n        influence_ft = (np.dot(ht_1, Uf))\/(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf) * ft\n\n        it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate\n        influence_it = (np.dot(ht_1, Ui))\/(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi) * it\n\n        ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate\n        influence_ot = np.dot(ht_1, Uo) \/ (np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo) * ot\n\n        gt =  np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)\n        influence_gt =np.dot(ht_1, Uc) \/ (np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc) * gt\n\n        Ct = ft * Ct_1 + it * gt\n        influence_ct = influence_ft * Ct_1 + influence_it * influence_gt\n        ht = ot * np.tanh(Ct)\n        influence_ht = influence_ot * (influence_ct\/Ct) * ht\n\n        influence_h_t_value.append(influence_ht)\n\n        ht_1 = ht  # hidden state, previous memory state\n        Ct_1 = Ct  # cell state, previous carry state\n\n        h_t_value.append(ht)\n\n    influence_h_t_value.append(h_t_value[-1])\n    for i in range(len(influence_h_t_value)-1,0,-1):\n        influence_h_t_value[i] = influence_h_t_value[i] - influence_h_t_value[i-1]\n\n    influence_h_t_value = influence_h_t_value[1:]\n\n    impact_columns = np.dot(influence_h_t_value,weights[3]) + (weights[4]\/units)\n    print(model.predict(X[number:number+1]))\n    if model.predict(X[number:number+1]) > 0.5:\n        b_color = 'lightgreen'\n    else:\n        b_color ='lightcyan'\n\n    fig = plt.figure(figsize=(15,3),facecolor=b_color)\n\n    for k in range(len(df.loc[number,'processing'])):\n        s = df.loc[number,'processing'][k]\n        va = round(float(impact_columns[k]),2)\n        if va > 0.5:\n            color ='green'\n        elif va< -0.3:\n            color ='blue'\n        else:\n            color ='black'\n\n        if k < 17:\n            plt.text(s=s, x=k*0.7, y=0,fontsize=14,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7, y=-0.1,fontsize=14,color=color,va='center',ha='center')\n        elif k < 34:\n            plt.text(s=s, x=k*0.7 - 17*0.7, y=-0.2,fontsize=14,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7- 17*0.7, y=-0.3,fontsize=14,color=color,va='center',ha='center')\n        else:\n            plt.text(s=s, x=k*0.7 - 34*0.7, y=-0.4,fontsize=14,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7- 34*0.7, y=-0.5,fontsize=14,color=color,va='center',ha='center')\n\n    plt.xlim(0,10)\n    plt.ylim(-0.5,0.1)\n    plt.axis('off')\n    plt.show()","f22616ae":"make_plot(6427)","ed3622df":"make_plot(6000)","a3063545":"make_plot(220)","0e47b3da":"make_plot(200)","1910bb66":"make_plot(216)","5f45f10c":"make_plot(120)","839acbf4":"Just applied, rate of Negative words, and positive words. Which means:\n\nNegative Rate : -1 * Negatvie Counts \/ (Negative Counts + Positive Counts)\n\nPositive Rate : 1 * Positive Counts \/ (Negative Counts + Positive Counts)\n\nAfter Sum Two : Negative Rate + Positive Rate","49d60f4e":"Only use alphabet text, (delete special character or number). And, delete stopwords, changing analogous term.","df1136d5":"#That line was introduced after 27th line:   print(model.predict(X[number:number+1]))\n\n#So it was printed the model and the number. Even the letters are less messy with just that line. \n\n#Much better JeongBin Park. Thank you AGAIN. ","1767c4f8":"#Code by JeongBin Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai\/notebook","c35bafe6":"![](https:\/\/www.darpa.mil\/ddm_gallery\/xai-figure1-inline-graphic.png)darpa.mil","541cef7d":"#There was a fit error, fixed by Jeong\n\n#So, he changed Activation function, and optimizer's learning rate","cc532cd8":"If influence value is more than 0.5 : Green color. If influence value is lower than -0.5 : blue color.\n\nThe background color mean too (????). Jeong Bin Park's code didn't consider activation function like sigmoid. ","52bc3e18":"#Above rating  mean that 0 is Negative review, and 1 is Positive review","ac7fbc50":"#I was feeling like the User at the initial image. Thanks to Jeong (The brain behind the AI System) I'm less confused.","74352c1f":"#If you want to positive predict value, you use tail number like 6427 ~ 3400","3d20544a":"#Code by JeongBin_Park https:\/\/www.kaggle.com\/jeongbinpark\/looking-lstm-detailly-trying-xai","0c60c2cb":"#Jeong showed performance that 92% val_accuracy. I got 0.9150\n\nJeong's result: \n\nEpoch 20\/20\n18\/18 [==============================] - 1s 50ms\/step - loss: 0.2188 - acc: 0.9316 - val_loss: 0.3849 - val_acc: 0.9378\n\nEven copying his snippets I got less accuracy.","155781f9":"#A simple LSTM Model","b6465c63":"#The positive sample above was changed from that Rating value below.","80573676":"#On the chart above the distribution goes almost till 250. That's why I choose that number \n\nto write on that line: X = X.loc[:,:250]","357a4303":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Explainable AI (XAI) program<\/h1>\n\nThe Explainable AI (XAI) program aims to create a suite of machine learning techniques that:\n\nProduce more explainable models, while maintaining a high level of learning performance (prediction accuracy); and\n\nEnable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.\n\nhttps:\/\/www.darpa.mil\/program\/explainable-artificial-intelligence","49daa027":"#Above it's 6428, (tail is 6428), hence change below the original code X = X.reshape(4740,51,1)  to 6428 \n\n#Then the reshape was changed to X = X.reshape(-1,251,1)"}}