{"cell_type":{"af91a768":"code","f78d0754":"code","7c1b5f36":"code","bf69f098":"code","b9b16a8f":"code","8c73ed16":"code","186a771b":"code","47b1fe58":"code","4270d53d":"code","a4f0f3ae":"code","4fc830ec":"code","2fe31586":"code","b1e2bbfc":"code","6bfb5bb6":"code","9f3bc5b4":"code","3ea30f3f":"code","e0ffe43a":"code","26713a44":"code","a41af6b9":"code","4f251b34":"code","44783470":"code","c49fcade":"markdown","828d6c5b":"markdown","629980dc":"markdown","85b80f4d":"markdown","2ec4a847":"markdown","ea840380":"markdown","68a71a3d":"markdown","541caa67":"markdown","eb6ee9d0":"markdown","0293584f":"markdown"},"source":{"af91a768":"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout,SimpleRNN\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline ","f78d0754":"max_words = 10000\nmaxlen    = 200","7c1b5f36":"(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)","bf69f098":"x_train[0]","b9b16a8f":"x_train = pad_sequences(x_train, maxlen = maxlen, padding='post')\nx_test = pad_sequences(x_test, maxlen = maxlen, padding='post')","8c73ed16":"x_train[0][:50]\nlen(x_train[0])\ny_train[0]","186a771b":"max_len = 200\nmodel = Sequential()\nmodel.add(Embedding(max_words, 2, input_length=max_len))\nmodel.add(SimpleRNN(8))\nmodel.add(Dense(1, activation='sigmoid'))","47b1fe58":"model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","4270d53d":"history = model.fit(x_train,\n          y_train,\n          epochs=15,\n          validation_split=0.1)","a4f0f3ae":"plt.plot(history.history['accuracy'], \n         label='The share of correct answers on the training set')\nplt.plot(history.history['val_accuracy'], \n         label='The share of correct answers on the checking set')\nplt.xlabel('epochs')\nplt.ylabel('Share of correct answers')\nplt.legend()\nplt.show()","4fc830ec":"scores = model.evaluate(x_test, y_test, verbose=1)","2fe31586":"print(\"Percentage of correct answers on test data:\", round(scores[1] * 100, 4))","b1e2bbfc":"embedding_matrix = model.layers[0].get_weights()[0]\nembedding_matrix[:5]","6bfb5bb6":"#LOAD DICTIONARY WITH WORD'S INDEX\nword_index_org = imdb.get_word_index()","9f3bc5b4":"# We supplement the dictionary with service characters\n\nword_index = dict()\nfor word,number in word_index_org.items():\n    word_index[word] = number + 3\nword_index[\"<\u0417\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c>\"] = 0\nword_index[\"<\u041d\u0430\u0447\u0430\u043b\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438>\"] = 1\nword_index[\"<\u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e>\"] = 2  \nword_index[\"<\u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f>\"] = 3\n","3ea30f3f":"word = 'good'\nword_number = word_index[word]\nprint('Index of word', word_number)\nprint('Vector for word', embedding_matrix[word_number])","e0ffe43a":"reverse_word_index = dict()\nfor key, value in word_index.items():\n    reverse_word_index[value] = key","26713a44":"# We write dense vector representations to a file\nclass Vector_repr:\n  def __init__(self):\n    self.filename = 'imdb_embeddings.csv'\n    with open(self.filename, 'w') as f:\n      for word_num in range(max_words):\n        self.word = reverse_word_index[word_num]\n        self.vec = embedding_matrix[word_num]\n        f.write(self.word + \",\")\n        f.write(','.join([str(x) for x in vec]) + \"\\n\")\nVector_repr","a41af6b9":"plt.scatter(embedding_matrix[:,0], embedding_matrix[:,1])","4f251b34":"# We select the word codes by which you can determine the tone of the recall\n\nreview = ['brilliant', 'fantastic', 'amazing', 'good',\n          'bad', 'awful','crap', 'terrible', 'trash']\nenc_review = []\nfor word in review:\n    enc_review.append(word_index[word])\nenc_review\n\nreview_vectors = embedding_matrix[enc_review]\nreview_vectors","44783470":"plt.scatter(review_vectors[:,0], review_vectors[:,1])\nfor i, txt in enumerate(review):\n    plt.annotate(txt, (review_vectors[i,0], review_vectors[i,1]))","c49fcade":"> > * ## Explore the trained dense vector representation of words","828d6c5b":"We get a matrix of dense vector representations of words","629980dc":"> * ## Load Data","85b80f4d":"Create function one hot encoding","2ec4a847":" ## Visualization of dense vector representations of words","ea840380":"## Preparig data for education","68a71a3d":"## Visualization of a trained dense vector representation of words by which you can determine the emotional coloring of the text","541caa67":"<img src=\"https:\/\/www.dropbox.com\/s\/grd17bkapocb92o\/imdb_movie_reviews.png?dl=1\" width=\"600\">\n","eb6ee9d0":"> * ## Creating neural network for text classification using imdb dataset","0293584f":"## SAVE TRAINED DENSE VECTOR REPRESENTATIONS TO A FILE."}}