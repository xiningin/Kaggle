{"cell_type":{"f0c2f20e":"code","840db9f8":"code","bf2283b0":"code","b72f3cd4":"code","0bf03eb0":"code","008612cd":"code","a6d249cc":"code","39c71b5e":"code","cbce0b30":"code","7e0cdca5":"code","4b224da9":"code","834f8958":"code","96e0d35d":"code","e8759ba9":"code","9fc261fd":"code","1252b7e0":"code","c6f6664e":"code","806ee04d":"code","7b41b590":"code","3c360294":"code","370e4618":"code","1ae8de3b":"code","8db62e2a":"code","5fd69af4":"code","85a5549e":"code","238afd75":"code","7197aabd":"code","90268967":"code","07aaba5c":"code","b22cf2ae":"code","cf484a46":"code","86e3dcf0":"markdown","552a70ab":"markdown","3dcdf5e1":"markdown","d2868610":"markdown","bb47a54e":"markdown","0c986b98":"markdown","0b7e3f16":"markdown","3ceeab94":"markdown","cec0f957":"markdown","8557fc93":"markdown","a4acd7db":"markdown","65e03bc4":"markdown","5800eb13":"markdown","7f44d39c":"markdown","7afd6db1":"markdown","f4d6a071":"markdown","a0565ad9":"markdown","f42c8aeb":"markdown","3e5401d9":"markdown","49fc24f1":"markdown","99adf5b1":"markdown","73284697":"markdown","1ec3710a":"markdown","a6f19a1a":"markdown","74b7ac7f":"markdown"},"source":{"f0c2f20e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","840db9f8":"os.listdir(\"..\/input\/commonlitreadabilityprize\")","bf2283b0":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsub = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n\ntrain.shape, test.shape, sub.shape","b72f3cd4":"train.head()","0bf03eb0":"test.head()","008612cd":"print(\"-\"*25,\"First example from train\",\"-\"*25)\nprint(train.excerpt[0])\nprint(\"-\"*25,\"First example from test\",\"-\"*25)\nprint(test.excerpt[0])","a6d249cc":"plt.hist(train.target, bins = 100)\nplt.title(\"Histogram of target variable\")\nplt.show()\nprint(train.target.describe())","39c71b5e":"plt.hist(train.standard_error, bins = 100)\nplt.title(\"Histogram of standard_error variable\")\nplt.show()\nprint(train.standard_error.describe())","cbce0b30":"train[train.standard_error == 0]","7e0cdca5":"print(\"-\"*25,\"Text corresponding to highest target value in train\",\"-\"*25)\nprint(train.iloc[train.target.argmax(),:][\"excerpt\"],\"\\n\")\n\nprint(\"-\"*25,\"Text corresponding to lowest target value in train\",\"-\"*25)\nprint(train.iloc[train.target.argmin(),:][\"excerpt\"])","4b224da9":"plt.scatter(train.target, train.standard_error)\nplt.title(\"Scatter plot between target and standard_error\")\nprint(\"correlation between target and standard error is\", np.corrcoef(train.target, train.standard_error)[1,0])","834f8958":"train['word_count'] = train['excerpt'].apply(lambda x: len(str(x).split()))\ntest['word_count'] = test['excerpt'].apply(lambda x: len(str(x).split()))\nprint(train['word_count'].describe())\nplt.hist(train.word_count, bins = 100)\nplt.title(\"Distribution of number of words\")\nplt.show()","96e0d35d":"plt.scatter(train.target, train.word_count)\nplt.title(\"Scatter plot between target and word_count\")","e8759ba9":"!pip install textstat\nimport textstat","9fc261fd":"%%time\n\ndef feature_engineering(df):\n    df['sentence_count'] = df['excerpt'].apply(lambda x: textstat.sentence_count(x))\n    df['syllable_count'] = df['excerpt'].apply(lambda x: textstat.syllable_count(x,lang='en_US'))\n    df['word_per_sentence'] = df.apply(lambda row: row.word_count\/row.sentence_count, axis=1)\n    df['syllable_per_sentence'] = df.apply(lambda row: row.syllable_count\/row.sentence_count, axis=1)\n    df['syllable_per_word'] = df.apply(lambda row: row.syllable_count\/row.word_count, axis=1)\n\n    df['flesch_reading_ease'] = df['excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\n    df['automated_readability_index'] = df['excerpt'].apply(lambda x: textstat.automated_readability_index(x))\n    df['linsear_write_formula'] = df['excerpt'].apply(lambda x: textstat.linsear_write_formula(x))\n    \n    return df\n\ntrain = feature_engineering(train)\ntest = feature_engineering(test)","1252b7e0":"cols = [\"word_count\", \"sentence_count\", \"syllable_count\" , \n        \"word_per_sentence\" , \"syllable_per_sentence\" , \"syllable_per_word\" ,\n        \"flesch_reading_ease\", \"automated_readability_index\" , \"linsear_write_formula\" , \"target\" ]\n\ntemp = train[cols]\ntemp.describe()","c6f6664e":"temp.corr()","806ee04d":"columns = [c for c in cols if c not in [\"word_count\",\"target\"]]\n\nfig, ax = plt.subplots(1, 8, figsize = (30, 5))\n\nfor idx, col in enumerate(columns, 0):\n    ax[idx].plot(train['target'], train[col], 'o')\n    ax[idx].set_xlabel('target')\n    ax[idx].set_title(col)\n\nplt.show()","7b41b590":"from gensim.parsing.preprocessing import remove_stopwords\nprint(\"-\"*10,\"before removing stopwords\",\"-\"*10)\nprint(train.excerpt[0],\"\\n\")\nprint(\"-\"*10,\"After removing stopwords\",\"-\"*10)\nprint(remove_stopwords(train.excerpt[0]))","3c360294":"train['nostop_text'] = train['excerpt'].apply(lambda x: remove_stopwords(x))\ntrain[['excerpt', 'nostop_text']].head()","370e4618":"from sklearn.feature_extraction.text import CountVectorizer\ntrain_docs = train[\"nostop_text\"].tolist()\ncv = CountVectorizer(lowercase = True, \n                     ngram_range = (1,1), \n                     max_features=10000, \n                     min_df = 1, \n                     max_df = 0.8) ","1ae8de3b":"sparse_train = cv.fit_transform(train_docs)\ncounts = pd.DataFrame(sparse_train.toarray(),\n                      columns=cv.get_feature_names())\nprint(counts.shape)\ncounts.head()","8db62e2a":"import re\ntrain_docs_nonum = [re.sub(r'\\d+', '', i) for i in train_docs]\n\ncv = CountVectorizer(lowercase = True, \n                     ngram_range = (1,1), \n                     max_features=10000, \n                     min_df = 1, \n                     max_df = 0.8) \nsparse_train = cv.fit_transform(train_docs_nonum)\n\ncounts = pd.DataFrame(sparse_train.toarray(),\n                      columns=cv.get_feature_names())\n\ncounts = pd.DataFrame(sparse_train.toarray(),\n                      columns=cv.get_feature_names())\ncounts.head()","5fd69af4":"print(counts.sum().sort_values(ascending=False)[:20])\ncounts.sum().sort_values(ascending=False)[:20].plot.bar()","85a5549e":"cv = CountVectorizer(lowercase = True, \n                     ngram_range = (2,2), ## only consider bigrams ##\n                     max_features=10000,\n                     min_df = 1, \n                     max_df = 0.8) \nsparse_train = cv.fit_transform(train_docs_nonum)\n\ncounts = pd.DataFrame(sparse_train.toarray(),\n                      columns=cv.get_feature_names())\n\ncounts = pd.DataFrame(sparse_train.toarray(),\n                      columns=cv.get_feature_names())\n\nprint(counts.sum().sort_values(ascending=False)[:20])\ncounts.sum().sort_values(ascending=False)[:20].plot.bar()","238afd75":"import gensim\nfrom gensim.matutils  import Sparse2Corpus\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis","7197aabd":"import warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","90268967":"cv = CountVectorizer(lowercase = True, ## setting all characters to lower case ##\n                     ngram_range = (1,1), ## only consider unigram and bigrams ##\n                     max_features=10000, ## creating the corpus using 10000 most frequeqnt words\n                     min_df = 1, ## no words are ignored for smaller number of appearance ##\n                     max_df = 0.8) ## if there are some corpus specific stopwords ##\nsparse_train = cv.fit_transform(train_docs_nonum)\ncorpus_data_gensim = gensim.matutils.Sparse2Corpus(sparse_train, documents_columns=False)\n\nvocabulary_gensim = {}\nfor key, val in cv.vocabulary_.items():\n    vocabulary_gensim[val] = key\n    \ndict = Dictionary()\ndict.merge_with(vocabulary_gensim)","07aaba5c":"lda = LdaModel(corpus_data_gensim, num_topics = 5 )\n\ndef document_to_lda_features(lda_model, document):\n    topic_importances = lda.get_document_topics(document, minimum_probability=0)\n    topic_importances = np.array(topic_importances)\n    return topic_importances[:,1]\n\nlda_features = list(map(lambda doc:document_to_lda_features(lda, doc),corpus_data_gensim))\n\ndata_pd_lda_features = pd.DataFrame(lda_features)\ndata_pd_lda_features.columns = [\"topic\"+str(i) for i in range(5)]\ndata_pd_lda_features.head()","b22cf2ae":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_result { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_area { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.input_area { max-width:100% !important; }<\/style>\"))\npyLDAvis.enable_notebook()\nlda_viz = gensimvis.prepare(lda, corpus_data_gensim, dict)\nlda_viz","cf484a46":"for i in [\"topic\"+str(i) for i in range(5)]:\n    print(\"correlation of \", i, \"with target is\", np.corrcoef(train.target, data_pd_lda_features[i])[1,0])","86e3dcf0":"Well, now we see columns with only words. Note that there are 10000 columns corresponding to the most frequent 10000 words in the data. Now, we would be interested to see what are the most frequent words, let's have a look at them. We just use column sum in this data and sort the column sum to find out the most frequent words.","552a70ab":"Three files have been provided with us in this competition. Let's read these files.","3dcdf5e1":"Seems like the target variable is normally distributed ranging from -3.7 to 1.7 in the training data with 1.03 as standard deviation. Let's also look at the distribution of the standard_error variable","d2868610":"Test data seems to be hidden. Let's look at the train and test data.","bb47a54e":"Do we have a relationship between the number of words and the target variable? I do not see any though.","0c986b98":"Let's try with 5 topics ","0b7e3f16":"Well, yes, I think we can see why the first text has highest target value as it is easy to read and why the later one is more difficult to read and ease of readability score is low. Probably, number of words, number of passive sentences, complex sentences and length of sentences and other features in this direction could be useful. However, what is difficult for me could be easy to read for someone else. Hence, let's look at the correlation of target and standard error.","3ceeab94":"This was with the most frequent words or unigrams. Let's also look at the most frequent bigrams (combination of two subsequent words).","cec0f957":"Probably, we can extract some features from this analysis. For example, we see one of the top most frequent bigrams: \"for example\". We can create a count feature related to this bigram as we can hypothesize that the texts that give more example tends to make it easier to read. Who knows? \n\nIn this direction, there is another cool text analysis tool that we can use is topic modeling. Although we assume that features generated from topic modeling would be generalized on both train and test data. Let's use genism library for topic modeling. We would restrict topics which would consist of unigrams only. ","8557fc93":"For the sake of simplicity, we are creating the following variables, however, there are also some other index and readability measures like: FOG index etc. could be found in the library. We can create many more features from this library.","a4acd7db":"Now, let's look at the engineered features. We would also check how correlated these features and our target variable are.","65e03bc4":"Now, let's look at the texts with highest and lowest readability.","5800eb13":"Now, let's do some basic text preprocessing which are essential for text analysis. First we will start with removing stopwords. Stopwords are words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like: the, is, in, for etc. We illustrate stopword removal using the first sample of our data.","7f44d39c":"Now, let's try to engineer few features from texts. For feature engineering, we would use textstat library. Note that, this library is installed using internet in the following code cell. However, we can not access internet while submitting to Kaggle in this competition. Hence, we may need to add this libray as an external data in the kernel. Using textstat libray, we can engineer few features which could be related to text complexity. Read details about this library [here](https:\/\/github.com\/shivam5992\/textstat)","7afd6db1":"Well, although the standard error and target are not linearly related, seems like there is a dependency. So, mostly standard errors are high when the target values are either very high or very low. This means that, multiple rating system mostly disagreed when on average the texts are either easy to read or difficult to read.\n\nLet's calculate the numberof words in the data.","f4d6a071":"As, we can see, after removing stopwords, words like - the, to etc. have been removed. Let's create a separate text column in the training data by removing these stopwords.","a0565ad9":"Let's visualize the topics and see if we can understand any topic from keywords. We also check the correlation of these topic variables with our target variable.","f42c8aeb":"Note that, the column:target is the ease of readability score and standard_error is probably the corresponding standard error of the target measure across different rating scores. Now, let's look at the texts.","3e5401d9":"The texts are looking pretty clean as of now. Let's look at the distribution of the target variable","49fc24f1":"At least the standard deviation of standard_error variable is small, 0.03. Low standard error means that multiple rating systems mostly agreed regarding the ease of readability score and high standard error means that ratings from multiple rating systems are scattered. \n\nNote that, there is one observation with zero standard error, this is probably either a data entry error as the target is also zero or every rating system produced same score for the corresponding text. ","99adf5b1":"Now, we create a dataframe, where each column would be a single word and the elements represent the count of that word in the specific row corresponding to the train data. ","73284697":"If we see the right most column or the bottom most row of the above output correlation matrix, we see that the engineered features do have strong correlation with target. Now, plot the scatterplots of these variables with target variable","1ec3710a":"As expected, we see there are 10000 columns as we specified in the max_features parameters that, we would use 10000 most frequent words. However, we see that there are some columns which are only numbers. We may need to remove these numbers and prepare the dataframe again. ","a6f19a1a":"From the first glance, it is difficult to get an idea of what these topics are from looking at the keywords. However, we see some topic features do have some predictive potential. However, we need to be cautious regarding how to use these variables as topics may not generalize in the test data set.","74b7ac7f":"One of the important and easy to use functions for text analysis is CountVectorizer from sklean.This function can be used to essentially generate count of words (or words combinations like bigram etc.) from the text document. First, we putall our texts from the train data into a list and then we use this function. There are several paramters which we can play with regarding this function. Details can be checked [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html). For our purpose, we are using the following parameters:\n1. lowercase = True --> setting all characters to lower case \n2. ngram_range = (1,1) --> only consider unigrams \n3. max_features=10000 --> creating the corpus using 10000 most frequeqnt words\n4. min_df = 1 --> no words are ignored for smaller number of appearance \n5. max_df = 0.8) --> if there are some corpus specific stopwords, so we would be ignoring words which appeared in more than 80% of the documents "}}