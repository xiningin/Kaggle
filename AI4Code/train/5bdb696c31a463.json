{"cell_type":{"7a4427ce":"code","75b3c20d":"code","dc70ab2a":"code","2bf3befe":"code","32918b89":"code","200f3afb":"code","f443057e":"code","4f2238b1":"code","74ab7b82":"code","bc5b013f":"code","90e3c913":"code","d7cbadf4":"code","49e6c82a":"code","b9ae0a9d":"code","a242e709":"code","bedfa4e1":"code","79b1d073":"code","dec7e190":"code","9b767d10":"code","8474034d":"code","fe8c13f2":"markdown","da0dbafe":"markdown","b7bcde38":"markdown","6a55f6f2":"markdown","9e381c3b":"markdown","058756a3":"markdown","5c34e17e":"markdown","6d607478":"markdown","e5e21cc0":"markdown","be3b69c9":"markdown","19304e43":"markdown","32f64eaf":"markdown","e971fff7":"markdown","7da2bd4b":"markdown","ddad0fb2":"markdown","5abab7aa":"markdown","0b2a6c0c":"markdown"},"source":{"7a4427ce":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time","75b3c20d":"# 1D implementation\nclass TicTacToe:\n    def __init__(self, board=None, board_size=3, players=None, first_player=None):\n        self.board_size = board_size\n        if board is None:\n            self.board = np.zeros((self.board_size ** 2), dtype=int)\n        else:\n            # Make sure the board is of 1D array\n            self.board = np.array(board)\n\n        self.EMPTY = 0\n        if players is None:\n            self.players = {self.EMPTY: '?', 1: 'X', 2: 'O'}\n        else:\n            self.players = players\n\n        self.playerCount = len(self.players) - 1\n        self.history = list()\n        if first_player is None:\n            self.player = 1\n        else:\n            self.player = first_player\n\n    def visualize(self):\n        print(f'Round {len(self.history) + 1}:', end='')\n        for i, c in enumerate(self.board):\n            if i % self.board_size == 0:\n                print()\n            print(self.players[c], end='\\t')\n        print()\n\n    # The pos is 0-based\n    def placeMove(self, pos):\n        if self.isAvailable(pos):\n            self.board[pos] = self.player\n            self.history.append([pos, self.player])\n            self.player = self.getNextPlayer()\n        return self.checkGameEnds()\n\n    def isAvailable(self, pos):\n        if 0 <= pos < self.board_size ** 2:\n            return self.board[pos] == self.EMPTY\n        return False\n\n    def getCurrentPlayer(self):\n        return self.player\n\n    def getNextPlayer(self):\n        if self.player == self.playerCount:\n            self.player = 1\n        else:\n            self.player += 1\n        return self.player\n\n    def checkGameEnds(self):\n        # -1 means no one wins and all spaces has used up\n        # 0 means the game hasn't ended yet\n        # If a player won, it'll return the corresponding integer that represents the player\n        reshaped_board = self.board.reshape(self.board_size, self.board_size)\n        for row in reshaped_board:\n            if len(set(row)) == 1 and row[0] != 0:\n                return row[0]\n\n        for column in reshaped_board.transpose():\n            if len(set(column)) == 1 and column[0] != 0:\n                return column[0]\n\n        diagonal = [reshaped_board[i][i] for i in range(self.board_size)]\n        if len(set(diagonal)) == 1 and diagonal[0] != 0:\n            return diagonal[0]\n\n        inverted_diagonal = [np.flip(reshaped_board, 0)[i][i] for i in range(self.board_size)]\n        if len(set(inverted_diagonal)) == 1 and inverted_diagonal[0] != 0:\n            return inverted_diagonal[0]\n\n        if self.EMPTY in set(reshaped_board.flatten().tolist()):\n            return 0\n        else:\n            return -1\n\n    def explain(self):\n        state = self.checkGameEnds()\n        if state == -1:\n            print('No one wins and all spaces has used up.')\n        elif state == 0:\n            print('The game hasn\\'t ended yet.')\n        else:\n            print(f'Player {state} ({self.players[state]}) won.')","dc70ab2a":"class Environment(TicTacToe):\n    def __init__(self, first_player=1, debug=False, **kwargs):\n        '''\n        Initialize the environment.\n        Input:\n        first_player <int>: Representing the starting player.\n        debug <bool>: Default to False. If it was set to True, it will display debugging information.\n        kwargs <dict>: A dictionary for holding keyword arguments being passed.\n        '''\n        self.first_player = first_player\n        self.debug = debug\n        self.kwargs = kwargs\n        if self.kwargs == {}:\n            super().__init__()             # Initializes the game\n        else:\n            super().__init__(self.kwargs)  # Initializes the game with keyword arguments\n\n    @staticmethod\n    def getHash(board):\n        '''\n        Get hash of the current board.\n        Input:\n        board <list>: A board which is of list type. Eg: [1, 2, 1, 2, 1, 2, 1, 2, 1].\n        \n        Return:\n        <str>: The hash of the current state of the game.\n        '''\n        return str(board.tolist())\n\n    def reset(self, first_player=None):\n        '''\n        For resetting the environment after the game has finished. Keyword arguments being passed during initialization of the environment will be passed again for re-initialization.\n        Input:\n        first_player <int>: Representing the starting player.\n        \n        Return:\n        <str>: Hash of a board with only empty spaces.\n        '''\n        if self.kwargs == {}:\n            super().__init__()             # Initializes the game\n        else:\n            super().__init__(self.kwargs)  # Initializes the game with keyword arguments\n        if first_player is not None:\n            self.player = first_player\n        return self.getHash(self.board)\n\n    def render(self):\n        ''' Renders the current environment. '''\n        self.visualize()\n\n    def getAvailableMoves(self):\n        ''' Get available moves of the current board. '''\n        return np.where(self.board == self.EMPTY)[0]\n\n    def action_space(self):\n        ''' The action space of the current environment. A fancy wrap for consistency with Gym API. '''\n        return self.getAvailableMoves()\n\n    @staticmethod\n    def getReward(state, maximized_player):\n        '''\n        Get the reward given the game state and the player to be maximized for his reward.\n        Input:\n        state <int>: The game state of the game. Possible values: -1: Tie game; 0: The game hasn't ended yet; 1: Player 1 won; 2: Player 2 won, ...\n        maximized_player <int>: The player to be maximized for his reward.\n\n        Return:\n        <int>: Representing the reward in different situations.\n        '''\n        if state == -1:                  # No one wins\n            return 0\n        elif state == 0:                 # The game hasn't ended yet\n            return 0\n        elif state == maximized_player:  # The player which is to be maximized won\n            return 1\n        else:                            # Other players won\n            return -1\n\n    def step(self, action, player, maximized_player):\n        '''\n        Input:\n        action <int>: The position of the move to be placed. It is 0-based, with range [0.. <board_size>].\n        player <int>: The integer that represents the player. Eg 1 for player 1, 2 for player 2, ...\n        maximized_player <int>: The player to be maximized for his reward.\n\n        Return:\n        observation <str>: The hash of the board before the move\n        reward <int>: The reward due to the move\n        game_state <int>: The game state of the game. Possible values: -1: Tie game; 0: The game hasn't ended yet; 1: Player 1 won; 2: Player 2 won, ...\n        info <func>: For explaining the current situation of the game.\n        '''\n        if self.debug:\n            print('Player:', player, 'Action:', action, 'Action space:', self.action_space())\n        game_state = self.placeMove(action)\n        observation = self.getHash(self.board)\n        reward = self.getReward(game_state, maximized_player)\n        if self.debug:\n            print('Maximized Player:', maximized_player, 'Game State:', game_state, 'Reward:', reward, 'Observation:', observation)\n            self.visualize()\n            self.explain()\n            input()\n        info = self.explain\n        return observation, reward, game_state, info\n\n    def close(self):\n        ''' For shutting down the environment. A fancy wrap for consistency with Gym API. '''\n        self.game = None\n\n\nclass Agent:\n    def __init__(self, debug=False):\n        self.debug = debug\n        pass\n\n    def getAction(self, observation, action_space):\n        '''\n        To get action given the observation of the environment and the action space. As this is the most basic form of the agent, it will just return random actions.\n        Input:\n        observation <str>: The hash of the current state of the game.\n        action_space <list>: A list of integers indicating available moves of the current state of the game.\n        \n        Return:\n        action <int>: The move to be placed on the board.\n        '''\n        action = np.random.choice(action_space)\n        return action\n\n    def train(self, experience, f):\n        ''' To train the agent. As this is the most basic form of the agent, it could not be trained. '''\n        if self.debug:\n            print('The agent is unable to learn.')\n            input()\n        pass\n\n\nclass QAgent(Agent):\n    def __init__(self, discount_rate=0.9, learning_rate=1, epsilon=1, decay_rate=0.9999, debug=False):\n        '''\n        discount_rate <float>: To discount the reward if it takes more turns to get the reward. Eg if the reward is available after 2 turns, reward for the current move will be <reward> * discount_rate ** 2.\n        learning_rate <float>: To specify the speed of learning from moves given rewards. Since this is a deterministic game, learning_rate could be set to 1.\n        epsilon <float>: Representing the decay of trade-offs between exploration and exploitation.\n        debug <bool>: Default to False. If it was set to True, it will display debugging information.\n        '''\n        self.discount_rate = discount_rate\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.decay_rate = decay_rate\n        self.q_table = {}\n        self.debug = debug\n\n    @staticmethod\n    def select_argmax(dictionary):\n        '''\n        Returns the move that maximizes the reward. If there are several moves with the maximum reward, it will randomly choose one in these moves.\n        Input:\n        dictionary <dict>: A dictionary that contains available moves and their corresponding rewards.\n        \n        Return:\n        <int>: The action to be taken by the QAgent.\n        '''\n        if len(dictionary) == 0:\n            raise ValueError('The length of the dictionary should not be zero!')\n        max_q_value = max(dictionary.values())\n        choices = []\n        for key in dictionary:\n            if dictionary[key] == max_q_value:\n                choices.append(key)\n        return np.random.choice(choices)\n\n    @staticmethod\n    def select_argmin(dictionary):\n        '''\n        Returns the move that minimizes the reward. If there are several moves with the minimum reward, it will randomly choose one in these moves.\n        Input:\n        dictionary <dict>: A dictionary that contains available moves and their corresponding rewards.\n        \n        Return:\n        <int>: The exploitation (greedy) action to be taken by the QAgent.\n        '''\n        if len(dictionary) == 0:\n            raise ValueError('The length of the dictionary should not be zero!')\n        min_q_value = min(dictionary.values())\n        choices = []\n        for key in dictionary:\n            if dictionary[key] == min_q_value:\n                choices.append(key)\n        return np.random.choice(choices)\n\n    def getActionMax(self, observation, action_space):\n        '''\n        To get the action that maximizes the reward. It will be either an exploration or exploitation action, subject to the epsilon.\n        Input:\n        observation <str>: The hash of the current state of the game.\n        action_space <list>: A list of integers indicating available moves of the current state of the game.\n        \n        Return:\n        <int>: An action to be taken by the QAgent.\n        '''\n        action_random = super().getAction(observation, action_space)\n        q_value_dict = {}\n        for action in action_space:\n            q_value_dict[action] = self.q_table.get(observation, {}).get(action, 0)\n        action_greedy = self.select_argmax(q_value_dict)\n        if self.debug:\n            print('Action random:', action_random, 'Action greedy:', action_greedy)\n        return action_random if np.random.random() < self.epsilon else action_greedy\n\n    def getActionMin(self, observation, action_space):\n        '''\n        To get the action that minimizes the reward. It will be either an exploration or exploitation action, subject to the epsilon.\n        Input:\n        observation <str>: The hash of the current state of the game.\n        action_space <list>: A list of integers indicating available moves of the current state of the game.\n        \n        Return:\n        <int>: An action to be taken by the QAgent.\n        '''\n        action_random = super().getAction(observation, action_space)\n        q_value_dict = {}\n        for action in action_space:\n            q_value_dict[action] = self.q_table.get(observation, {}).get(action, 0)\n        action_greedy = self.select_argmin(q_value_dict)\n        if self.debug:\n            print('Action random:', action_random, 'action greedy:', action_greedy)\n        return action_random if np.random.random() < self.epsilon else action_greedy\n\n    def train(self, experience, f):\n        '''\n        To train the QAgent, given experience replay and a function representing whether to maximize or minimize the action.\n        Input:\n        experience <tuple>: The experiene replay of the game. It could be decomposed into five variables: observation, action, next_observation, reward, game_state.\n        f <func>: A function representing whether to maximize or minimize the action of the player.\n        \n        Important variables:\n        observation <str>: The hash of the board before the move\n        action <int>: The position of the move to be placed. It is 0-based, with range [0.. <board_size>].\n        next_observation <str>: The hash of the board after the move\n        reward <int>: The reward due to the move\n        game_state <int>: The game state of the game. Possible values: -1: Tie game; 0: The game hasn't ended yet; 1: Player 1 won; 2: Player 2 won, ...\n        current_state_q_value: The Q-value of the current state. It is retrieved from the QAgent's Q-table.\n        next_state_best_q_value: The predicted best Q-value of the next state. Since this is a minimax game, its objective is to minimize other players' rewards while maximizing self reward. It is retrieved from the QAgent's Q-table.\n        '''\n        observation, action, next_observation, reward, game_state = experience\n\n        if self.q_table.get(observation) is None:         # Create a new entry for the q_table with the newly observed state\n            self.q_table[observation] = {action: 0}       # Initialize the q_value of the action to 0\n\n        if self.q_table[observation].get(action) is None: # What if such observation exists, but such action does not exist?\n            self.q_table[observation][action] = 0         # Initialize the q_value of the action to 0\n\n        if self.debug:\n            print('observation:', observation)\n            print('action:', action)\n            print('self.q_table[observation]:', self.q_table[observation])\n            print('current_state_q_value:', self.q_table[observation][action])\n            print('next_state_best_q_value:', f(self.q_table.get(next_observation, {0: 0}).values()))\n\n        next_state_best_q_value = f(self.q_table.get(next_observation, {0: 0}).values())\n        current_state_q_value = self.q_table[observation][action]\n        if current_state_q_value != 1:\n            self.q_table[observation][action] = current_state_q_value + self.learning_rate * (reward + self.discount_rate * next_state_best_q_value - current_state_q_value)\n\n        if self.debug:\n            print(f'self.q_table[observation][action]: {self.q_table[observation][action]} = {current_state_q_value} + {self.learning_rate} * ({reward} + {self.discount_rate} * {next_state_best_q_value} - {current_state_q_value})')\n            input()\n\n        if game_state:\n            self.epsilon *= self.decay_rate","2bf3befe":"def visualize(board, board_size=3, players={0: '?', 1: 'X', 2: 'O'}):\n    '''\n    For fast visualization during debugging.\n    Input:\n    board <str>: Expected to be the board hash. Eg: '[1, 2, 1, 2, 1, 2, 1, 2, 1]'.\n    board_size <int>: Default to be 3. If you are using a board with different size, remember to specify this.\n    players <dict>: A dictionary for mapping numbers to symbols for better readability of the game board.\n    '''\n    for i, c in enumerate([int(x) for x in board[1:-1].split(',')]):\n        if i % board_size == 0:\n            print()\n        print(players[c], end='\\t')\n    print()\n\ndef collect_statistics(stat, result):\n    '''\n    stat <list>: a list that stores the result of every game play.\n    result <int>: the game state that indicates the result of the game.\n    '''\n    stat.append(result)\n    return stat","32918b89":"env = Environment(debug=False)\nrandom_agent = Agent(debug=False)\nepisode = 50000\nstat = []\n\nstart_time = time.time()\nfor i_episode in range(episode):\n    observation = env.reset()\n    i = 0\n    for t in range(100):\n        player = env.getCurrentPlayer()\n        if player == 1:\n            action = random_agent.getAction(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n            random_agent.train(None, None)\n        elif player == 2:\n            action = random_agent.getAction(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n            random_agent.train(None, None)\n        observation = next_observation\n        if game_state != 0:\n            stat = collect_statistics(stat, env.checkGameEnds())\n            if (i_episode + 1) % 10000 == 0:\n                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n                start_time = time.time()\n            break\n    env.close()","200f3afb":"# First decode to three columns\nsummary = pd.get_dummies(stat)\ndraw = summary[-1]\nagent_1 = summary[1]\nagent_2 = summary[2]\n\n# Then apply rolling(100).mean() to each of them\n# Finally plot them out\ninterval = 100  # Take samples to calculate game outcomes per <interval>\nplt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\nplt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\nplt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\nplt.legend()\nplt.title(f'{episode} Games Played by Random Agent vs Random Agent')\nplt.xlabel('Number of Games Played')\nplt.ylabel('Rate')\nplt.show()","f443057e":"# Now we set epsilon to 0, to stop exploration and only retain expolitation.\n# Let it play 10000 games we check the result of the game.\nenv.debug = False\nrandom_agent.debug = False\nepisode = 10000\ntest_stat = []\n\nstart_time = time.time()\nfor i_episode in range(episode):\n    first_player = 1\n    observation = env.reset(first_player=first_player)\n    i = 0\n    for t in range(100):\n        player = env.getCurrentPlayer()\n        if player == 1:\n            action = random_agent.getAction(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n        elif player == 2:\n            action = random_agent.getAction(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n        observation = next_observation\n        if game_state != 0:\n            test_stat = collect_statistics(test_stat, env.checkGameEnds())\n            if (i_episode + 1) % 10000 == 0:\n                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n                start_time = time.time()\n            break\n    env.close()\n\n# First decode to three columns\ntest_summary = pd.get_dummies(test_stat)\ndraw = test_summary.get(-1, pd.Series(index=test_stat, name=-1))\nagent_1 = test_summary.get(1, pd.Series([0] * len(test_stat), name=1))\nagent_2 = test_summary.get(2, pd.Series([0] * len(test_stat), name=2))\n\n# Then apply rolling(100).mean() to each of them\n# Finally plot them out\ninterval = 100  # Take samples to calculate game outcomes per <interval>\nplt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\nplt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\nplt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\nplt.legend()\nplt.title('Games Played by Random Agent vs Random Agent')\nplt.xlabel('Number of Games Played')\nplt.ylabel('Rate')\nplt.show()\n\nprint(f'QAgent vs QAgent, for the past {episode} games:')\nprint(f'Draw Rate        : {draw.mean()} ({pd.Series(test_stat).value_counts().get(-1, 0)} Games)')\nprint(f'Agent 1 Win Rate : {agent_1.mean()} ({pd.Series(test_stat).value_counts().get(1, 0)} Games)')\nprint(f'Agent 2 Win Rate : {agent_2.mean()} ({pd.Series(test_stat).value_counts().get(2, 0)} Games)')","4f2238b1":"episode = 100000\nenv = Environment(debug=False)\nq_agent = QAgent(debug=False, decay_rate=0.001 ** (1 \/ episode))\nstat = []\n\nstart_time = time.time()\nfor i_episode in range(episode):\n    observation = env.reset()\n    i = 0\n    for t in range(100):\n        player = env.getCurrentPlayer()\n        if player == 1:\n            action = q_agent.getActionMax(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n            q_agent.train((observation, action, next_observation, reward, game_state), min)\n        elif player == 2:\n            action = q_agent.getActionMin(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n            q_agent.train((observation, action, next_observation, reward, game_state), max)\n        observation = next_observation\n        if game_state != 0:\n            stat = collect_statistics(stat, env.checkGameEnds())\n            if (i_episode + 1) % 10000 == 0:\n                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n                start_time = time.time()\n            break\n    env.close()","74ab7b82":"# First decode to three columns\nsummary = pd.get_dummies(stat)\ndraw = summary[-1]\nagent_1 = summary[1]\nagent_2 = summary[2]\n\n# Then apply rolling(100).mean() to each of them\n# Finally plot them out\ninterval = 100  # Take samples to calculate game outcomes per <interval>\nplt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\nplt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\nplt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\nplt.legend()\nplt.title(f'{episode} Games Played by QAgent vs QAgent')\nplt.xlabel('Number of Games Played')\nplt.ylabel('Rate')\nplt.show()","bc5b013f":"# Now we set epsilon to 0, to stop exploration and only retain expolitation.\n# Let it play 10000 games we check the result of the game.\n\nq_agent.epsilon = 0\nepisode = 50000\ntest_stat = []\n\nstart_time = time.time()\nfor i_episode in range(episode):\n    observation = env.reset()\n    i = 0\n    for t in range(100):\n        player = env.getCurrentPlayer()\n        if player == first_player:\n            action = q_agent.getActionMax(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n        else:\n            action = q_agent.getActionMin(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n        observation = next_observation\n        if game_state != 0:\n            test_stat = collect_statistics(test_stat, env.checkGameEnds())\n            if (i_episode + 1) % 10000 == 0:\n                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n                start_time = time.time()\n            break\n    env.close()\n\n# First decode gameplay results to three columns\ntest_summary = pd.get_dummies(test_stat)\ndraw = test_summary.get(-1, pd.Series(index=test_stat, name=-1))\nagent_1 = test_summary.get(1, pd.Series([0] * len(test_stat), name=1))\nagent_2 = test_summary.get(2, pd.Series([0] * len(test_stat), name=2))\n\n# Then apply rolling(100).mean() to each of them to calculate average draw or win rate per 100 intervals\n# Finally plot them out\ninterval = 100  # Take samples to calculate game outcomes per <interval>\nplt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\nplt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\nplt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\nplt.legend()\nplt.title('Games Played by QAgent vs QAgent')\nplt.xlabel('Number of Games Played')\nplt.ylabel('Rate')\nplt.show()\n\nprint(f'QAgent vs QAgent, for the past {episode} games:')\nprint(f'Draw Rate        : {draw.mean()} ({pd.Series(test_stat).value_counts().get(-1, 0)} Games)')\nprint(f'Agent 1 Win Rate : {agent_1.mean()} ({pd.Series(test_stat).value_counts().get(1, 0)} Games)')\nprint(f'Agent 2 Win Rate : {agent_2.mean()} ({pd.Series(test_stat).value_counts().get(2, 0)} Games)')","90e3c913":"len(q_agent.q_table)","d7cbadf4":"q_agent.q_table","49e6c82a":"board = '[1, 1, 0, 2, 2, 0, 1, 2, 1]'\nvisualize(board)\nprint(q_agent.q_table[board])\nif q_agent.q_table[board][2] > q_agent.q_table[board][5]:\n    print('OK')\nelse:\n    print('Failed, position 2 should have greater q value than position 5.')","b9ae0a9d":"board = '[1, 0, 0, 2, 2, 1, 1, 2, 1]'\nvisualize(board)\nprint(q_agent.q_table[board])\n\nif q_agent.q_table[board][1] < q_agent.q_table[board][2]:\n    print('OK')\nelse:\n    print('Failed, position 1 should have smaller q value than position 2.')","a242e709":"board = '[1, 0, 0, 2, 2, 0, 1, 2, 1]'\nvisualize(board)\nprint(q_agent.q_table[board])\nif q_agent.q_table[board][1] == q_agent.q_table[board][5]:\n    print('OK')\nelse:\n    print('Failed, position 1 should have the same q value with position 5.')\n\nif q_agent.q_table[board][1] != q_agent.q_table[board][2] and q_agent.q_table[board][5] != q_agent.q_table[board][2]:\n    print('OK')\nelse:\n    print('Failed, position 1 and position 5 should have more negative q value when compared with position 2.')","bedfa4e1":"board = '[1, 0, 0, 0, 1, 0, 2, 0, 0]'\nvisualize(board)\nprint(q_agent.q_table[board])\nif q_agent.q_table[board][8] == min(q_agent.q_table[board].values()):\n    print('OK')\nelse:\n    print('Failed, position 8 should have the smallest q value.')","79b1d073":"board = '[2, 0, 2, 0, 1, 0, 1, 1, 0]'\nvisualize(board)\nprint(q_agent.q_table[board])\nif q_agent.q_table[board][1] == min(q_agent.q_table[board].values()):\n    print('OK')\nelse:\n    print('Failed, position 1 should have the smallest q value.')","dec7e190":"board = '[0, 0, 2, 1, 0, 2, 0, 1, 0]'\nvisualize(board)\nprint(q_agent.q_table[board])\nif q_agent.q_table[board][8] == max(q_agent.q_table[board].values()):\n    print('OK')\nelse:\n    print('Failed, position 8 should have either the largest q value.')","9b767d10":"# Now we set epsilon to 0, to stop exploration and only retain expolitation.\n# Let it play 10000 games we check the result of the game.\n\nq_agent.epsilon = 0\nepisode = 10000\ntest_stat = []\n\nstart_time = time.time()\nfor i_episode in range(episode):\n    observation = env.reset()\n    history = []\n    history.append(observation)\n    i = 0\n    for t in range(100):\n        player = env.getCurrentPlayer()\n        if player == 1:\n            if first_player == 1:\n                action = q_agent.getActionMax(observation, env.getAvailableMoves())\n            else:\n                action = q_agent.getActionMin(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n        elif player == 2:\n            action = random_agent.getAction(observation, env.getAvailableMoves())\n            next_observation, reward, game_state, info = env.step(action, player, 1)\n        observation = next_observation\n        history.append(observation)\n        if game_state != 0:\n            test_stat = collect_statistics(test_stat, env.checkGameEnds())\n            if (i_episode + 1) % 10000 == 0:\n                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n                start_time = time.time()\n            break\n    env.close()\n\n# First decode to three columns\ntest_summary = pd.get_dummies(test_stat)\ndraw = test_summary.get(-1, pd.Series(index=test_stat, name=-1))\nagent_1 = test_summary.get(1, pd.Series([0] * len(test_stat), name=1))\nagent_2 = test_summary.get(2, pd.Series([0] * len(test_stat), name=2))\n\n# Then apply rolling(100).mean() to each of them\n# Finally plot them out\ninterval = 100  # Take samples to calculate game outcomes per <interval>\nplt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\nplt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\nplt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\nplt.legend()\nplt.title('Games played by QAgent vs Random Agent')\nplt.xlabel('Number of games played')\nplt.ylabel('Rate')\nplt.show()\n\nprint(f'QAgent vs QAgent, for the past {episode} games:')\nprint(f'Draw Rate        : {draw.mean()} ({pd.Series(test_stat).value_counts().get(-1, 0)} Games)')\nprint(f'Agent 1 Win Rate : {agent_1.mean()} ({pd.Series(test_stat).value_counts().get(1, 0)} Games)')\nprint(f'Agent 2 Win Rate : {agent_2.mean()} ({pd.Series(test_stat).value_counts().get(2, 0)} Games)')","8474034d":"answer = input('Would you like to start first? Enter anything to start as the first player, or just press <Enter> to become the second player: ')\nif answer != '':\n    HUMAN_PLAYER = 1\n    COMPUTER_PLAYER = 2\n    first_player = HUMAN_PLAYER\nelse:\n    COMPUTER_PLAYER = 1\n    HUMAN_PLAYER = 2\n    first_player = COMPUTER_PLAYER\n\nobservation = env.reset(first_player=first_player)\nq_agent.epsilon = 0\nenv.render()\ni = 0\nfor t in range(100):\n    player = env.getCurrentPlayer()\n    if player == HUMAN_PLAYER:\n        action = int(input(f'Human\\'s move (0 to {env.board_size ** 2 - 1}): '))\n    elif player == COMPUTER_PLAYER:\n        if first_player == COMPUTER_PLAYER:\n            action = q_agent.getActionMax(observation, env.getAvailableMoves())\n        else:\n            action = q_agent.getActionMin(observation, env.getAvailableMoves())\n        print(f'QAgent\\'s move (0 to {env.board_size ** 2 - 1}): {action}')\n    next_observation, reward, done, info = env.step(action, player, True)\n    observation = next_observation\n    reward += reward\n    env.render()\n    if done:\n        break\n#         sleep(2)\n#     clear_output(wait=True)\nenv.explain()\nenv.close()","fe8c13f2":"Reinforcement learning is espeically useful for tackling problems by specifying environment and rewards only. Some of its applications are games, self-driving cars and algorithm tradings. In this way, we could save the time for building complicated rule-based systems while getting a well-performed model or, even surpassing human levels.\n\nHowever, reinforcement learning is known to be hard for its implementation. Unlike traditional machine learning approach, modules in reinforcement learning are closely related. It requires everything to be \"just right\". It is also known for its difficulty for debugging.\n\nTherefore, I provided detailed explanations in each modules. Hope it will be useful for people who also wants to get a glimpse about reinforcement learning.\n\nBelow is the implementation of Q-learning on the game tic-tac-toe. Q-learning is an algorithm that stores every state, action and reward in a tabular form. The table would be updated continuously during gameplays and finally learn the best action under different situations. It is especially suitable for environments with finite states, such as tic-tac-toe.\n\nTips here are super useful for debugging RL problems:\nhttps:\/\/www.reddit.com\/r\/reinforcementlearning\/comments\/9sh77q\/what_are_your_best_tips_for_debugging_rl_problems\/\n\nEnjoy the notebook!","da0dbafe":"## Let's play some games with our QAgent!","b7bcde38":"How about QAgent vs Random Agent? Does QAgent always superior to Random Agent?","6a55f6f2":"# Training Random Agent vs Random Agent","9e381c3b":"Implementing environment inherited from the game and agent.","058756a3":"# Testing QAgent vs QAgent","5c34e17e":"# Future Directions","6d607478":"# Test Cases","e5e21cc0":"How about extending the game?\n- Bigger boards\n- More agents\n- Bigger boards and more agents\n- Bigger boards and more Q agents","be3b69c9":"Implementing the game tic-tac-toe.","19304e43":"# Testing Random Agent vs Random Agent","32f64eaf":"Now we know the random baseline of the game. A good agent should surpass the draw rate given by two random agents. Therefore, we should expect to have a higher draw rate or win rate after training.","e971fff7":"# Testing QAgent vs Random Agent","7da2bd4b":"To make sure the agent works perfectly in different situations, we should write test cases and test them.","ddad0fb2":"From the above case, we found that when the computer thinks that it will lose anyway, it will just give up and treat these positions indifferently.","5abab7aa":"Utility functions for faster debugging.","0b2a6c0c":"# Training QAgent vs QAgent"}}