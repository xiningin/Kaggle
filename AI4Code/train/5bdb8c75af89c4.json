{"cell_type":{"26b1b05d":"code","014672f5":"code","0091cd73":"code","8af35b2c":"code","c746ba99":"code","f1eca9e3":"code","14bf8b03":"code","1d260a44":"code","3573267e":"code","071c58f1":"code","90657420":"code","591859ed":"code","8752fd9a":"code","12ab8f15":"code","93e265c8":"code","e6b529f4":"code","2fb458d5":"code","05594534":"code","6890b32d":"code","9ac1b6e1":"markdown","45d1f289":"markdown","59af8e5b":"markdown","95cccef0":"markdown","51d611e5":"markdown","62fbd42f":"markdown","2aec9a67":"markdown","32ff4a42":"markdown","b190d3b5":"markdown","16366de3":"markdown","7490fd32":"markdown","582dc18e":"markdown"},"source":{"26b1b05d":"from tensorflow.keras.datasets import imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=10000)","014672f5":"# train_data[0]","0091cd73":"train_labels[0]","8af35b2c":"max([max(sequence) for sequence in train_data])","c746ba99":"word_index = imdb.get_word_index()\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()])\ndecoded_review = \" \".join(\n    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n","f1eca9e3":"import numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        for j in sequence:\n            results[i, j] = 1.\n    return results\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","14bf8b03":"x_train[0]","1d260a44":"y_train = np.asarray(train_labels).astype(\"float32\")\ny_test = np.asarray(test_labels).astype(\"float32\")","3573267e":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])","071c58f1":"model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics = [\"accuracy\"])","90657420":"from keras import optimizers\n\nmodel.compile(optimizer=optimizers.RMSprop(lr =0.001),\n              loss = \"binary_crossentropy\",\n              metrics=[\"accuracy\"])\n\n\nfrom keras import losses \nfrom keras import metrics\n\nmodel.compile(optimizer=optimizers.RMSprop(lr =0.001),\n              loss = \"binary_crossentropy\",\n              metrics=[\"metrics.binary_accuracy\"])","591859ed":"val_x = x_train[:10000]\nx_train_partial = x_train[10000:]\nval_y = y_train[:10000]\ny_train_partial = y_train[10000:]\n\n#We create a validation set.","8752fd9a":"model.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",metrics = [\"accuracy\"])\n\nhist = model.fit(x_train_partial,\n                    y_train_partial,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(val_x, val_y))\n","12ab8f15":"# All the things that happened in training is on this dictionary.\nhistory_dict = hist.history\nhistory_dict.keys()","93e265c8":"history_dict = hist.history\n\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = np.arange(1, len(loss_values) + 1)\n\nimport plotly.graph_objects as go\n\nimport numpy as np\nnp.random.seed(20)\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=epochs, y=val_loss_values,\n                    mode='lines+markers',\n                    name='Validation Loss'))\nfig.add_trace(go.Scatter(x=epochs, y=loss_values,\n                    mode='lines+markers',\n                    name='Training Loss'))\nfig.update_layout(title='Loss of Validation & Training Graph',\n                   xaxis_title='Number of Epochs',\n                   yaxis_title='Loss')\nfig.show()","e6b529f4":"ac_values = history_dict[\"accuracy\"]\nval_ac_values = history_dict[\"val_accuracy\"]\nepochs = np.arange(1, len(loss_values) + 1)\n\nimport plotly.graph_objects as go\n\nimport numpy as np\nnp.random.seed(20)\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=epochs, y=val_ac_values,\n                    mode='lines+markers',\n                    name='Validation Accuracy'))\nfig.add_trace(go.Scatter(x=epochs, y=ac_values,\n                    mode='lines+markers',\n                    name='Training Accuracy'))\nfig.update_layout(title='Acc of Validation & Training Graph',\n                   xaxis_title='Number of Epochs',\n                   yaxis_title='Accuracy')\nfig.show()","2fb458d5":"model = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nr = model.evaluate(x_test, y_test)","05594534":"r\n#accuracy is 0.88% . He advised to get to close to %95.","6890b32d":"model.predict(x_test)","9ac1b6e1":"* Here, for further we can use 1 or 3 hidden layers, also change 32 or 64 units. \n* Change binary_crossentropy with mse.\n* Change relu with tanh activation funct.","45d1f289":"# We can also make a graph for accuracy of training & validation with the same way.","59af8e5b":"# In the life not everything is linear; if you dont use activ. func. , then it will assume it is an as linear model.\n\n## Relu also called non-linearity.","95cccef0":"# <span style=\"color:#e75480\">I am currently working with his book and saving my notes after the codes. <\/span>\n#    <span style=\"color:#e75480\"> PLEASE give your feedback. Thanks! <\/span>","51d611e5":"# **More on that:**","62fbd42f":"> #  For ex: 10 epochs mean 10 iterations over the samples. ","2aec9a67":" # \"Crossentropy measures the dist. btw probability distrubutions & btw the ground-truth distr. and your predictions.\" \n# \n![image.png](attachment:dfdeccbf-7bc6-4d33-ab30-483b4ff49d2a.png)","32ff4a42":"# The model definiton","b190d3b5":"# With more epochs, training accuracy increases.But it is not true for validation values. \n# <span style=\"color:green\">Because of overfitting!<\/span> For ex: Here epochs= 4 could be ideal.","16366de3":"## <span style=\"color:purple\">We can also configure the optimizer or can use custom losses and metrics<\/span>.  ","7490fd32":"#  <span style=\"color:red\">Always the thing to do: Multiply inputs by weights, add with bias, and apply activation<\/span>","582dc18e":"*******\/ N 0 T E S \/***************\n\n\n## Softmax activation function: \n\n* !Use for Mulitclass classifaction problem. If you use this, scores' sum will be 1. \n* Probability distrubition over different output classes.\n\n## <span style=\"color:red \">How to handle labels in multiclass classification? <\/span> \n\n## <span style=\"color:red \" > 1. Categorical Encoding\/ One-hot encoding: <\/span> \n\n* Categorical crossentropy measures(and we want to minimize) the distance between two probability distrubitions. \n \n##  <span style=\"color:red \" > 2. Sparse_categorical_crossentropy: <\/span>  \n\n* It is the same with Categorical crossentropy, mathematically. Use sparse_categorical_crossentropy  <span style=\"color:blue \"> with integer labels. <\/span>\n \n## If you want to classify data points with using N classes, Dense layer should be = N. \n \n## If there is little data, we can use K-fold validation for evaluting the model correctly. \n## If there is little training data , we can use one or two hidden layers for not facing overfitting in the further.\n \n "}}