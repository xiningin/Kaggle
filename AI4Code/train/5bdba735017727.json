{"cell_type":{"5ddae6b8":"code","963cc3d6":"code","b08ad1f5":"code","533e938a":"code","97c9887b":"code","fb289223":"code","6cd181b1":"code","fa517baf":"code","bcc268e2":"code","c4939133":"code","10a25aca":"code","fa9636fa":"code","5a46f843":"code","ecfe68cc":"code","9742f37f":"code","821759f4":"code","67ecdc6b":"code","b5a63dc7":"code","b79a5e40":"code","52372378":"code","9b7e7e0f":"code","d95a97d4":"code","db498999":"code","4efc74cb":"code","2b1268e4":"code","b7aa9514":"code","05af88f8":"code","73863c22":"code","1c89e7c2":"code","52a26d82":"code","e8d9c0ca":"code","1e7f9d35":"code","0405d8a5":"code","97e5d65a":"code","fb18f94a":"code","b2dd3a47":"code","361ae2af":"code","835c3621":"code","3791a9e6":"code","db6c7f6b":"code","1e0aa192":"code","3270dc5e":"code","5da31d32":"code","2791da73":"markdown","b02462e6":"markdown","ded9cb5e":"markdown","d53e76b3":"markdown","330a995c":"markdown"},"source":{"5ddae6b8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nfrom string import digits\nimport re\nfrom tensorflow.keras.layers import Input , LSTM , Dense , TimeDistributed , Embedding , Concatenate\nfrom tensorflow.keras.preprocessing.text import Tokenizer , one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model , load_model , model_from_json\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport pickle as pkl","963cc3d6":"#loading the data\nwith open(r'..\/input\/english-to-arabic\/ara.txt' , 'r' , encoding = 'utf-8') as f:\n    data = f.read()","b08ad1f5":"#splitting the data line by line\nuncleaned_data_split = data.split('\\n')\nlen(uncleaned_data_split)","533e938a":"uncleaned_data_split = uncleaned_data_split[:-1]","97c9887b":"#separate the english sentences from arabic sentences\nenglish_word = []\narabic_word = []\n\nfor word in uncleaned_data_split:\n    english_word.append(word.split('\\t')[:-1][0])\n    arabic_word.append(word.split('\\t')[:-1][1])\n    \nlen(english_word) , len(arabic_word)","fb289223":"#save the data as a csv data format\nlanguage_data = pd.DataFrame(columns=['English' , 'Arabic'])\nlanguage_data['English'] = english_word\nlanguage_data['Arabic'] = arabic_word\n\nlanguage_data.to_csv(r'.\/ara.csv' , index = False)","6cd181b1":"language_data.head()","fa517baf":"#separate the data\nenglish_text = language_data['English'].values\narabic_text = language_data['Arabic'].values\nlen(english_text) , len(arabic_text)","bcc268e2":"#lowering the data\nenglish_text = [w.lower() for w in english_text]\n\n#removing inverted commas\nenglish_text = [re.sub(\"'\" , '' , x) for x in english_text]\narabic_text = [re.sub(\"'\" , '' , x) for x in arabic_text]","c4939133":"#remove punctuations\ndef remove_punc(text):\n    table = str.maketrans('' , '' , string.punctuation)\n    removed_punc_text = []\n    \n    for sent in text:\n        sentence = [w.translate(table) for w in sent.split(' ')]\n        removed_punc_text.append(' '.join(sentence))\n        \n    return removed_punc_text\n\nenglish_text = remove_punc(english_text)\narabic_text = remove_punc(arabic_text)","10a25aca":"#remove english digits\nremove_digits = str.maketrans('', '',digits)\nremoved_digits_text = []\nfor sent in english_text:\n    sentence = [w.translate(remove_digits) for w in sent.split(' ')]\n    removed_digits_text.append(' '.join(sentence))\n    \nenglish_text = removed_digits_text\n\n#removing arabic digits\narabic_text = [re.sub('[\u0660\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u0669]' , '' , x) for x in arabic_text]","fa9636fa":"#removing the starting and ending whitespace\nenglish_text = [w.strip() for w in english_text]\narabic_text = [w.strip() for w in arabic_text]","5a46f843":"english_text[:7]","ecfe68cc":"arabic_text[:7]","9742f37f":"#adding the starting and end token to the arabic sentences\narabic_text = ['start '+ x +' end' for x in arabic_text]\narabic_text[:7]","821759f4":"#splitting the data\nX = english_text\ny = arabic_text\nXtrain ,Xtest , ytrain , ytest = train_test_split(X , y , test_size = 0.2)","67ecdc6b":"Xtrain[0]","b5a63dc7":"ytrain[0]","b79a5e40":"#determining the maximum length of our sentences\ndef max_length(data):\n    max_length = max([len(x.split(' ')) for x in data])\n    return max_length\n\n#max length of training data\nmax_len_english = max_length(Xtrain)\nmax_len_arabic = max_length(ytrain)\n\n#max length of testing data\nmax_len_english_test = max_length(Xtest)\nmax_len_arabic_test = max_length(ytest)\n\nmax_len_english , max_len_english_test , max_len_arabic , max_len_arabic_test","52372378":"#source language\nsource_tokenizer = Tokenizer()\nsource_tokenizer.fit_on_texts(Xtrain)\nsour_word2idx = source_tokenizer.word_index\nvocab_size_source = len(sour_word2idx) + 1\nprint(vocab_size_source)\n\nXtrain = source_tokenizer.texts_to_sequences(Xtrain)\nXtrain = pad_sequences(Xtrain , maxlen= max_len_english , padding = 'post')\n\nXtest = source_tokenizer.texts_to_sequences(Xtest)\nXtest = pad_sequences(Xtest , maxlen=max_len_english , padding = 'post')","9b7e7e0f":"#target language\ntarget_tokenizer = Tokenizer()\ntarget_tokenizer.fit_on_texts(ytrain)\ntar_word2idx = target_tokenizer.word_index\n\nvocab_size_target = len(tar_word2idx) + 1\nprint(vocab_size_target)\n\nytrain = target_tokenizer.texts_to_sequences(ytrain)\nytrain = pad_sequences(ytrain , maxlen= max_len_arabic , padding = 'post')\n\nytest = target_tokenizer.texts_to_sequences(ytest)\nytest = pad_sequences(ytest , maxlen= max_len_arabic , padding = 'post')","d95a97d4":"Xtrain[0]","db498999":"ytrain[0]","4efc74cb":"#saving important atributes using pickle\nwith open(r'.\/ara_eng_data.pkl' , 'wb') as f:\n    pkl.dump([Xtrain , ytrain , Xtest , ytest] , f)\n    \nwith open(r'.\/NMT_source_tokenizer.pkl' , 'wb') as f:\n    pkl.dump([vocab_size_source , sour_word2idx, source_tokenizer ] , f)\n    \nwith open(r'.\/NMT_target_tokenizer.pkl' , 'wb') as f:\n    pkl.dump([vocab_size_target , tar_word2idx , target_tokenizer] , f)","2b1268e4":"Xtrain = np.array(Xtrain)\nXtest  = np.array(Xtest)\nytrain = np.array(ytrain)\nytest  = np.array(ytest)","b7aa9514":"Xtrain[1]","05af88f8":"import tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https:\/\/arxiv.org\/pdf\/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","73863c22":"#building the model\nfrom keras import backend as k\nk.clear_session()\nlatent_dim = 500\n\n#Encoder\nencoder_input = Input(shape = (max_len_english , ))\nencoder_emb = Embedding(vocab_size_source , latent_dim , trainable = True)(encoder_input)\n\n#lstm_1\nencoder_lstm1 = LSTM(latent_dim , return_sequences = True , return_state = True)\nencoder_out1 , state_h1 , state_c1 = encoder_lstm1(encoder_emb)\n\n#lstm2\nencoder_lstm2 = LSTM(latent_dim , return_sequences = True , return_state = True)\nencoder_out2 , state_h2 , state_c2 = encoder_lstm2(encoder_out1)\n\n#lstm3\nencoder_lstm3 = LSTM(latent_dim , return_sequences = True , return_state = True)\nencoder_out3 , state_h3 , state_c3 = encoder_lstm3(encoder_out2)\n\n#Decoder\ndecoder_input = Input(shape = (None,))\ndecoder_emb = Embedding(vocab_size_target , latent_dim , trainable = True)(decoder_input)\n\n#lstm using encoder states as an initial states\ndecoder_lstm = LSTM(latent_dim , return_sequences = True , return_state = True)\ndecoder_out , decoder_fwd_state , decoder_back_state = decoder_lstm(decoder_emb , initial_state = [state_h3 , state_c3])\n\n#attention_layer\natten_layer = AttentionLayer(name = 'attention_layer')\natten_out , atten_state = atten_layer([encoder_out3 , decoder_out])\n\n#concatenate attention output and decoder output\ndecoder_concat_input = Concatenate(axis = -1 , name = 'concat')([decoder_out , atten_out])\n\n#Dense layer\ndecoder_dense = TimeDistributed(Dense(vocab_size_target , activation = 'softmax'))\ndecoder_out = decoder_dense(decoder_concat_input)","1c89e7c2":"#model summary\nmodel = Model([encoder_input , decoder_input] , decoder_out)\nmodel.summary()","52a26d82":"plot_model(model ,to_file = r'.\/train_model.png' ,  show_shapes= True)","e8d9c0ca":"#compiling the model\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","1e7f9d35":"#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","0405d8a5":"history = model.fit([Xtrain, ytrain[:,:-1]], ytrain.reshape(ytrain.shape[0], ytrain.shape[1],1)[:,1:], \n                    epochs=30, \n                    #callbacks=[es],\n                    batch_size=32,\n                    validation_data = ([Xtest, ytest[:,:-1]], ytest.reshape(ytest.shape[0], ytest.shape[1], 1)[:,1:]))","97e5d65a":"#visualizing model loss , val_loss\nplt.plot(history.history['loss'] , label = 'Train')\nplt.plot(history.history['val_loss'] , label = 'Test')\n\nplt.legend()\nplt.show()","fb18f94a":"#saving the model\nmodel_json = model.to_json()\n\nwith open(r'.\/NMT_model.json' , 'w') as json_file:\n    json_file.write(model_json)\n    \nmodel.save_weights(r'.\/NMT_model_weights.h5')\nprint('model saved to disk')","b2dd3a47":"#loading the model\njson_file = open(r'.\/NMT_model.json' , 'r')\njson_model = json_file.read()\njson_file.close()\nmodel_loaded = model_from_json(json_model , custom_objects={'AttentionLayer' : AttentionLayer})\n\n#load weights\nmodel_loaded.load_weights(r'.\/NMT_model_weights.h5')","361ae2af":"sour_idx2word = source_tokenizer.index_word\ntar_idx2word = target_tokenizer.index_word","835c3621":"latent_dim = 500\n\n#encoder inference\n#loading encoder inputs\nencoder_inputs = model_loaded.input[0]\n\n#loading encoder outputs\nencoder_outputs , state_h , state_c = model_loaded.layers[6].output\nprint(encoder_outputs.shape)\n\nencoder_model = Model(inputs = encoder_inputs , outputs = [encoder_outputs , state_h , state_c])","3791a9e6":"#decoder infernece\n#velow tensors will hold the state of the previous timestep\ndecoder_state_input_h = Input(shape = (latent_dim,))\ndecoder_state_input_c = Input(shape = (latent_dim,))\ndecoder_hidden_state_input = Input(shape = (34 , latent_dim))\n\n#decoder inputs\ndecoder_inputs = model_loaded.layers[3].output\nprint(decoder_inputs.shape)\n\n#decoder embeddings\ndec_emb_layer = model_loaded.layers[5]\ndec_emb2 = dec_emb_layer(decoder_inputs)\n\n#to predict the next word in the sequences, set the initial states to the states from the previous timestep\ndecoder_lstm = model_loaded.layers[7]\ndecoder_outputs2 , state_h2 , state_c2 = decoder_lstm(dec_emb2 , initial_state = [decoder_state_input_h , decoder_state_input_c])\n\n#attention inference\natten_layer = model_loaded.layers[8]\natten_out_inf , atten_state_inf = atten_layer([decoder_hidden_state_input , decoder_outputs2])\n\n#concat\nconcat = model_loaded.layers[9]\ndecoder_inf_concat = concat([decoder_outputs2 , atten_out_inf])\n\n#dense\ndecoder_dense = model_loaded.layers[10]\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n\n#final decoder model\ndecoder_model = Model([decoder_inputs] + [decoder_hidden_state_input , decoder_state_input_h , decoder_state_input_c],\n                     [decoder_outputs2] + [state_h2 , state_c2])","db6c7f6b":"#predictions\ndef decode_sequences(input_seq):\n    #encode the input as state vector\n    enc_out , enc_h , enc_c = encoder_model.predict(input_seq)\n    \n    #generate empty target sequence of length 1\n    target_seq = np.zeros((1,1))\n    \n    #choose the 'start' word as the first word of the target sequence\n    target_seq[0,0] = tar_word2idx['start']\n    \n    stop_condition = False\n    decoded_sequence = ''\n    \n    while not stop_condition:\n        output_tokens , h , c = decoder_model.predict([target_seq] + [enc_out , enc_h , enc_c])\n        \n        #sample a token\n        sampled_token_index = np.argmax(output_tokens[0 , -1 , :])\n        if sampled_token_index == 0:\n            break\n        else:\n            sampled_token = tar_idx2word[sampled_token_index]\n            \n            if (sampled_token != 'end'):\n                decoded_sequence +=' '+sampled_token\n            \n                #exit if 'end' token or if hit max_length\n                if (sampled_token =='end' or len(decoded_sequence.split()) >= (26-1)):\n                    stop_condition = True\n            \n            #update the target sequence\n            target_seq = np.zeros((1,1))\n            target_seq[0,0] = sampled_token_index\n            \n            #update internal states\n            enc_h , enc_c = h , c\n            \n    return decoded_sequence\n        ","1e0aa192":"def seq2text(input_seq):\n    newstring = ''\n    for i in input_seq:\n        if i != 0:\n            newstring = newstring + sour_idx2word[i] + ' '\n            \n    return newstring","3270dc5e":"def seq2summary(input_seq):\n    new_string = ''\n    for i in input_seq:\n        if ((i != 0 and i != tar_word2idx['start']) and  i != tar_word2idx['end']):\n            new_string = new_string + tar_idx2word[i] + ' '\n            \n    return new_string","5da31d32":"for i in range(10):\n    \n    print(\"Review : \" , seq2text(Xtest[i]))\n    print(\"Original summary : \" , seq2summary(ytest[i]))\n    print('predicted summary : ' , decode_sequences(Xtest[i].reshape(1,34)))\n    print('\\n')","2791da73":"Tokenizer & Pad_sequences","b02462e6":"inference model","ded9cb5e":"**Attention Layer:**\nThe major drawback of encoder-decoder model in sequence to sequence recurrent neural network is that it can only work on short sequences. It is difficult for the encoder model to memorize long sequences and convert it into a fixed-length vector. Moreover, the decoder receives only one information that is the last encoder hidden state. Hence it's difficult for the decoder to summarize large input sequence at once. So, how do we overcome this problem?\nthis is where the concept of \u2018Attention Mechanism\u2019 comes. The major intuition about this is that it predicts the next word by concentrating on a few relevant parts of the sequence rather than looking on the entire sequence.","d53e76b3":"Cleaning the data","330a995c":"Building the model"}}