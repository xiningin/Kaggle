{"cell_type":{"7ebfc3a8":"code","b83ac699":"code","e985d286":"code","8f056bce":"code","40ff2edb":"code","3aa1b3e1":"code","07040436":"code","ffafe960":"code","1cf3d6bf":"code","05a64dbd":"code","9a4dd5eb":"code","55b221a0":"code","67110d57":"code","aae739b6":"code","a167e028":"code","c9feb38a":"code","de413a9a":"code","daa2a49d":"code","4dae47ee":"code","f46df75c":"code","109dfe44":"code","38305149":"code","e31b7df7":"code","d47d9f62":"code","fa67595f":"code","b67c8522":"code","f3d3003b":"code","cab46533":"code","4048b517":"code","fbb466fc":"code","ebaf2d20":"code","0409151a":"markdown","5d8aaf62":"markdown","b0da0047":"markdown","4ebb3997":"markdown","8d06dc2f":"markdown","28d6cc0b":"markdown","609e378f":"markdown","387b9a91":"markdown","5b8961db":"markdown","733ab816":"markdown","2d66ae06":"markdown","975f1730":"markdown","8bbf3c19":"markdown","168e7f99":"markdown","e74c93c9":"markdown","ddb15815":"markdown","926ebe3f":"markdown","b7790377":"markdown","b0598eed":"markdown"},"source":{"7ebfc3a8":"import os\nos.chdir('\/kaggle\/input\/utilities')\nfrom data import DataLoader\nos.chdir(\"\/kaggle\/working\/\")\n\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom gensim.models import word2vec\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)","b83ac699":"dl = DataLoader('\/kaggle\/input\/word2vec-nlp-tutorial')\ndl.files","e985d286":"dl.load_data()","8f056bce":"train = pd.read_csv(\n    'labeledTrainData.tsv',\n    header=0,\n    delimiter=\"\\t\",\n    quoting=3)","40ff2edb":"train.head()","3aa1b3e1":"print(train['review'][0][500:1000])","07040436":"example = BeautifulSoup(train['review'][0])\nprint(example.get_text()[500:1000])","ffafe960":"letters_only = re.sub(\"[^a-zA-Z]\",\n                     \" \",\n                     example.get_text())","1cf3d6bf":"print(letters_only[500:1000])","05a64dbd":"lower_case = letters_only.lower()        # Convert to lower case\nwords = lower_case.split()               # Split into words\nprint(stopwords.words('english'))","9a4dd5eb":"words = [w for w in words if not w in stopwords.words(\"english\")]\nprint(words)","55b221a0":"def review_to_words(raw_review):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))  ","67110d57":"clean_review_example = review_to_words( train[\"review\"][0] )\nprint(clean_review_example)","aae739b6":"# Get the number of reviews based on the dataframe column size\nnum_reviews = train[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_train_reviews = ['']*num_reviews\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list \nfor i, review in enumerate(DataLoader.progressbar(train[\"review\"])):\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    clean_train_reviews[i] = review_to_words(review)","a167e028":"clean_train_reviews[0]","c9feb38a":"vectorizer = CountVectorizer(\n    analyzer = \"word\",\n    tokenizer = None,\n    preprocessor = None,\n    stop_words = None,\n    max_features = 5000)\n\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews).toarray()","de413a9a":"print(train_data_features.shape)","daa2a49d":"vocab = vectorizer.get_feature_names()\nprint(vocab[0:50])","4dae47ee":"dist = np.sum(train_data_features, axis=0)\nfor tag, count in zip(vocab[0:20], dist[0:20]):\n    print(count, tag)","f46df75c":"forest = RandomForestClassifier(n_estimators = 100)\nforest = forest.fit( train_data_features, train[\"sentiment\"] )","109dfe44":"# Read the test data\ntest = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\n\n\n# Create an empty list and append the clean reviews one by one\nnum_reviews = test[\"review\"].size\nclean_test_reviews = [' ']*num_reviews\n\nfor i, review in enumerate(DataLoader.progressbar(test[\"review\"])):\n    clean_test_reviews[i] = review_to_words(review)","38305149":"# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# Use the random forest to make sentiment label predictions\nresult = forest.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )","e31b7df7":"# Read data from files \ntrain = pd.read_csv(\n    \"labeledTrainData.tsv\",\n    header=0, \n    delimiter=\"\\t\",\n    quoting=3 )\ntest = pd.read_csv(\n    \"testData.tsv\",\n    header=0, delimiter=\"\\t\",\n    quoting=3 )\nunlabeled_train = pd.read_csv(\n    \"unlabeledTrainData.tsv\",\n    header=0,\n    delimiter=\"\\t\",\n    quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint(f'Read {train[\"review\"].size} labeled train reviews,' + \\\n      '{test[\"review\"].size} labeled test reviews, and' + \\\n      '{unlabeled_train[\"review\"].size} unlabeled reviews')","d47d9f62":"# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n\ndef review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    #  \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a \n    # list of sentences, where each sentence is a list of words\n    #\n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","fa67595f":"sentences = []\n\nfor review in DataLoader.progressbar(train[\"review\"], 'Parsing training set: ', 'Complete!'):\n    sentences += review_to_sentences(review, tokenizer)\n\nfor review in DataLoader.progressbar(unlabeled_train[\"review\"], 'Parsing unlabeled set: ', 'Complete!'):\n    sentences += review_to_sentences(review, tokenizer)","b67c8522":"print(len(sentences),\n     '\\n\\n',\n     sentences[0])","f3d3003b":"# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nprint('Training model...')\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)","cab46533":"model.doesnt_match(\"man woman child kitchen\".split())","4048b517":"model.doesnt_match(\"dog paris london france\".split())","fbb466fc":"model.most_similar(\"man\")","ebaf2d20":"model.most_similar(\"awful\")","0409151a":"Get insight into the model's word clusters","5d8aaf62":"Make the predictions","b0da0047":"Putting all the steps above together, we have the following function.","4ebb3997":"To deal with punctuation, numbers, and stop words, we can use NLTK and regular expressions. Note that some punctuation may be give us a better sentiment insight, such as \"!!!\" or \":-(\", but for simplicity we will remove all punctuation.","8d06dc2f":"Distinguish which word is most dissimilar from the others.","28d6cc0b":"To convert our reviews into a numeric representation we will use the Bag of Words approach. The Bag of Word approach simply maps each word in the review to how many times it was repeated in that review.","609e378f":"# [2.1] The Data","387b9a91":"# [3.2] Preprocessing\nSimilar to the Bag of Words processing, but we want to keep the stop words (because the algorithm relies on them for better understandings)\n\nAlso, we want a specific input format. Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.","5b8961db":"# [3] Word2Vec\nWord2vec was published by Google in 2013. It is a neural network implementation that learns distributed represenetations for words. Word2Vec is more impressive because it learns quickly relative to other models.\n\nIt does not need labels to create meaninful representations, and if the network is given enough training data it produces word vectors with intruiging characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. The famous example is that, with highly trained word vectors, \"king - man + woman = queen.\" ","733ab816":"# [3.3] Training","2d66ae06":"# [1] Overview\nThis notebook follows the [Bag of Words Meets Bags of Popcorn](https:\/\/www.kaggle.com\/c\/word2vec-nlp-tutorial\/overview). The tutorial covers sentiment analysis; specifically, text-processing and utilizing Bag-Of-Words and Word2Vec. Bag-Of-Words is the traditional method of understanding semantic relationships among words, while Word2Vec is Google's new deep-learning approach.  \n\nSentiment analysis is determining the meaning underlying a language. It is challenging as people express themselves in obscure ways using sarcasm or ambiguity.\n\nThe goal of the kernel is to create a model that can accurately determine if a movie review is positive or negative. The dataset consists of 100,000 IMDB positive and negative multi-paragraph movie reviews. Submissions are juded on area under the ROC curve.","975f1730":"# [2] Bag of Words","8bbf3c19":"Finally, we need to deal with \"stop words\". These words include \"a\", \"and\", \"is\", etc. NLTK can help us with this.","168e7f99":"# [2.3] Training\nWith these features we will be training a Random Forest","e74c93c9":"# [2.2] Preprocessing\n\nWe need to remove HTML tags, abbreviations, punctiations, etc.  \n\nWe can use BeautifulSoup to remove all HTML content.","ddb15815":"# [3.1] The Data","926ebe3f":"The vectorizer creates feature vector for us, where the colums are unique words and the rows are the reviews. For each review, the feature vector tells us how many times each of the 5000 words occurred.","b7790377":"Next, we want a specific input format. Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.","b0598eed":"Clean the test set"}}