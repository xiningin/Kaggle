{"cell_type":{"4274250d":"code","6ca5f893":"code","cebd0c33":"code","588ad0df":"code","0ee0d42b":"code","df8f423e":"code","141a0b4d":"code","5a776475":"code","b6f7bd8c":"code","93509d5f":"code","82b4fec4":"code","d5dbbf9a":"code","29976ccc":"code","addb744e":"code","554b26cd":"code","ce272e63":"code","14d4f4ab":"code","015948d8":"code","80d32467":"code","c2c53e38":"code","7a40902d":"code","caa498d1":"code","7899c4f0":"code","4341cc92":"code","528df160":"code","2c49b98e":"markdown","d11511fd":"markdown","de7e7166":"markdown","f161a073":"markdown","243db679":"markdown","1d9ca84c":"markdown","90b9df81":"markdown","3c66dbe6":"markdown"},"source":{"4274250d":"import random\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_scheduler","6ca5f893":"config = {\n    'fold_num': 5,\n    'seed': 1234,\n    #'model': 'roberta-base',\n    #'model': '..\/input\/robertalarge',\n    'model': 'allenai\/longformer-base-4096',\n    #'model': 'allenai\/longformer-large-4096',\n    'max_len': 1024,\n    'epochs': 5,\n    'train_bs': 6,\n    'valid_bs': 16,\n    'lr': 3e-5,\n    'num_workers': 0,\n    'weight_decay': 1e-2,\n    'num_warmup_steps': 1000,\n    'lr_scheduler_type': 'linear',\n    'gradient_accumulation_steps': 1,\n}\n","cebd0c33":"labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim',\n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\nlabels2index = {\n    'Lead': 1, 'Position': 3, 'Claim': 5, 'Counterclaim': 7, 'Rebuttal': 9, 'Evidence': 11, 'Concluding Statement': 13\n}","588ad0df":"def set_seed(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nset_seed(config['seed'])\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","0ee0d42b":"train_df = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain_df.head(2)","df8f423e":"train_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n    train_names.append(f.replace('.txt', ''))\n    with open('..\/input\/feedback-prize-2021\/train\/' + f, 'r', encoding='utf-8') as f:\n        text = ''\n        for line in f.readlines():\n            #text += line.replace('\\n', '').replace('\\xa0', '')\n            text += line.replace('\\n', ' ')\n        train_texts.append(text)\ntrain_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\ntrain_texts['text'] = train_texts['text'].apply(lambda x: x.split())","141a0b4d":"train_texts = train_texts.sort_values(by='id').reset_index(drop=True)\ntrain_df = train_df.sort_values(by=['id', 'discourse_start']).reset_index(drop=True)","5a776475":"text_index = dict.fromkeys(train_texts['id'].values.tolist())\nfor i in range(len(train_df)):\n    id = train_df.iloc[i]['id']\n    if not text_index[id]:\n        text_index[id] = [i]\n    else:\n        text_index[id].append(i)\n    if (i+1) % 20000 == 0:\n        print(\"Processed {0} discourses.\".format(i+1))","b6f7bd8c":"taggings = []\nessays = 0\nfor i in range(len(train_texts)):\n    text_id = train_texts.iloc[i]['id']\n    text = train_texts.iloc[i]['text']\n    tagging = [0] * config['max_len']\n    for k in text_index[text_id]:\n        if train_df.iloc[k]['id'] != train_texts.iloc[i]['id']:\n            break\n\n        discourse_type = train_df.iloc[k]['discourse_type']\n        predictionstring = train_df.iloc[k]['predictionstring'].split(' ')\n        label = labels2index[discourse_type]\n        if int(predictionstring[0]) > config['max_len'] - 2:\n            break\n        else:\n            tagging[int(predictionstring[0]) + 1] = label\n        for m in range(int(predictionstring[0]) + 2, int(predictionstring[-1]) + 2):\n            if m > config['max_len'] - 2:\n                break\n            else:\n                tagging[m] = label + 1\n    tagging[-1] = 0\n    taggings.append(tagging)\n    essays += 1\n    if essays % 2000 == 0:\n        print(\"Processed {0} essays.\".format(essays))","93509d5f":"train_texts['tagging'] = taggings","82b4fec4":"tokenizer = AutoTokenizer.from_pretrained(config['model'], add_prefix_space=True)","d5dbbf9a":"class MyDataset(Dataset):\n    def __init__(self, df, phase='Train'):\n        self.df = df\n        self.phase = phase\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = self.df.text.values[idx]\n        if self.phase == 'Train':\n            label = self.df.tagging.values[idx]\n            return {'text': text, 'label': label}\n        else:\n            return {'text': text}\n\n\ndef collate_fn(data):\n    input_ids, attention_mask = [], []\n    text = [item['text'] for item in data]\n    tokenized_inputs = tokenizer(\n        text,\n        max_length=config['max_len'],\n        padding='max_length',\n        truncation=True,\n        is_split_into_words=True,\n        return_tensors='pt'\n    )\n\n    words = []\n    for i in range(len(data)):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        words.append(word_ids)\n\n    tokenized_inputs[\"word_ids\"] = words\n    if 'label' in data[0].keys():\n        label = [item['label'] for item in data]\n        tokenized_inputs['labels'] = torch.LongTensor(label)\n\n    return tokenized_inputs","29976ccc":"train_dataset = MyDataset(train_texts, phase='Train')\ntrain_iter = DataLoader(train_dataset, batch_size=config['train_bs'], collate_fn=collate_fn, shuffle=False,\n                        num_workers=config['num_workers'])","addb744e":"model = AutoModelForTokenClassification.from_pretrained(config['model'], num_labels=15).to(device)\n\n\nno_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [\n    {\"params\": [p for n, p in model.named_parameters()\n                if not any(nd in n for nd in no_decay)],\n     \"weight_decay\": config['weight_decay'],\n     },\n    {\"params\": [p for n, p in model.named_parameters()\n                if any(nd in n for nd in no_decay)],\n     \"weight_decay\": 0.0,\n     },\n]\noptimizer = AdamW(optimizer_grouped_parameters,\n                  lr=config['lr'],\n                  betas=(0.9, 0.999),\n                  eps=1e-6\n                  )\nlr_scheduler = get_scheduler(\n    name=config['lr_scheduler_type'],\n    optimizer=optimizer,\n    num_warmup_steps=config['num_warmup_steps'],\n    num_training_steps=config['epochs'] * len(train_iter) \/  config['gradient_accumulation_steps'], )","554b26cd":"tk = tqdm(train_iter, total=len(train_iter), position=0, leave=True)\nmodel.train()\nstep = 0\nfor epoch in range(config['epochs']):\n    losses = []\n    print(\"Epoch {}\/{}\".format(epoch+1, config['epochs']))\n    for batch in tk:\n        batch = {k: v.to(device) for k, v in batch.items() if k != 'word_ids'}\n        loss = model(input_ids=batch['input_ids'],\n                     attention_mask=batch['attention_mask'],\n                     labels=batch['labels']).loss\n        loss \/= config['gradient_accumulation_steps']\n        loss.backward()\n        if (step + 1) % config['gradient_accumulation_steps'] == 0:\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        step += 1\n        losses.append(loss.item() * config['gradient_accumulation_steps'])\n    # print average loss\n    print(\"Epoch {}\/{}  Average Training Loss:{:6f}\".format(\n        epoch+1,\n        config['epochs'],\n        np.mean(losses)))\n\n","ce272e63":"model.save_pretrained(f'.\/roberta_trained')","14d4f4ab":"model = AutoModelForTokenClassification.from_pretrained(config['model'], num_labels=15).to(device)\nmodel.load_state_dict(torch.load('.\/roberta_trained\/pytorch_model.bin'))","015948d8":"test_df = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\ntest_df.head(5)","80d32467":"test_names, test_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/test'))):\n    test_names.append(f.replace('.txt', ''))\n    with open('..\/input\/feedback-prize-2021\/test\/' + f, 'r', encoding='utf-8') as f:\n        text = ''\n        for line in f.readlines():\n            #text += line.replace('\\n', '').replace('\\xa0', '')\n            text += line.replace('\\n', ' ')\n        test_texts.append(text)\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\ntest_texts['text'] = test_texts['text'].apply(lambda x: x.split())\ntest_texts","c2c53e38":"test_dataset = MyDataset(test_texts, phase='Test')\ntest_iter = DataLoader(test_dataset, batch_size=config['valid_bs'], collate_fn=collate_fn, shuffle=False,\n                        num_workers=config['num_workers'])","7a40902d":"y_pred = []\nwords = []\n\nwith torch.no_grad():\n    model.eval()\n    tk = tqdm(test_iter, total=len(test_iter), position=0, leave=True)\n    for step, batch in enumerate(tk):\n        word_ids = batch['word_ids']\n        words.extend(word_ids)\n        batch = {k: v.to(device) for k, v in batch.items() if k != 'word_ids'}\n\n        output = model(input_ids=batch['input_ids'],\n                       attention_mask=batch['attention_mask']).logits\n\n        y_pred.extend(output.argmax(-1).cpu().numpy())\n        \ny_pred = np.array(y_pred)","caa498d1":"y_pred[0]","7899c4f0":"final_preds = []\n\nfor i in tqdm(range(len(test_texts))):\n    idx = test_texts.id.values[i]\n    pred = ['']*len(y_pred[i]-2)\n\n    for j in range(1, len(y_pred[i])):\n        pred[j-1] = labels[y_pred[i][j]]\n\n    pred = [x.replace('B-','').replace('I-','') for x in pred]\n    \n    j = 0\n    while j < len(pred):\n        cls = pred[j]\n        if cls == 'O':\n            j += 1\n        end = j + 1\n        while end < len(pred) and pred[end] == cls:\n            end += 1\n            \n        if cls != 'O' and cls != '' and end - j > 10:\n            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n        \n        j = end\n        \nfinal_preds[0]","4341cc92":"sub = pd.DataFrame(final_preds)\nsub.columns = test_df.columns\nsub","528df160":"sub.to_csv('submission.csv', index=False)","2c49b98e":"## Load Model and Prepare Optimizer and LR Scheduler","d11511fd":"## Set Seed","de7e7166":"## Predict","f161a073":"## Start Training!","243db679":"## Save Model","1d9ca84c":"## Load Train Data","90b9df81":"## Config","3c66dbe6":"## Load Test Data"}}