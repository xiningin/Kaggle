{"cell_type":{"a607b065":"code","a9f73b7c":"code","8f199f71":"code","078faf9f":"code","beea472d":"code","6c36d97f":"code","6d93a503":"code","fdd4909b":"code","b8814aca":"code","f3f8a7ac":"code","18678e8c":"code","96c212c7":"code","124d9a5c":"code","20602967":"code","09dfe5d4":"code","6c47240c":"code","36293495":"code","9fd68c44":"code","3f3aabf7":"code","d177b7de":"code","d3d04961":"code","1e976b85":"code","fba931c5":"code","1d21eac1":"code","b5579bda":"code","0dc67012":"code","ac9ee183":"code","569b81d3":"code","f260e25e":"code","1470079e":"code","befc9431":"code","e92d0d5e":"code","9faf8c3a":"code","e3e3c291":"code","0139d0fe":"code","09d39028":"code","828e0828":"code","5029ae9a":"code","9cdbbce6":"code","47b6d117":"code","c2a5fdcc":"code","197b9371":"code","7db01e18":"code","b6e9e0bb":"code","2950ebec":"code","ea2f3564":"code","51076906":"code","0d0956fb":"code","e215e87f":"code","f2754751":"code","163570ef":"code","2daecbf3":"code","4aff198a":"code","760225e9":"code","cbbb6fd4":"code","d59e3688":"code","60127fc8":"code","5d6b4f46":"code","292bd3e3":"code","fb46011f":"code","0d163276":"code","78e350b6":"code","b210c143":"code","1d263b91":"code","53b70623":"code","fafdc9f5":"markdown","249bbb0f":"markdown","865d162b":"markdown","257b94cc":"markdown","6197d15c":"markdown","87673b44":"markdown","1c51ec4e":"markdown","de85f600":"markdown","97e4b5f6":"markdown","9ee94a58":"markdown","2f88313c":"markdown","496d7caf":"markdown","f9ded216":"markdown","e825fc09":"markdown","abf9b7b2":"markdown","bda733d2":"markdown","74d8cab3":"markdown","8995205c":"markdown","4b9b0480":"markdown","8fe95b6d":"markdown","4c647f5b":"markdown","61d70d9d":"markdown","ff5ed138":"markdown","d141b41a":"markdown","e30e283d":"markdown","88594126":"markdown","c7591234":"markdown","0ade3756":"markdown","1ee872c3":"markdown","e54a1e58":"markdown","383a73f7":"markdown","c58e1243":"markdown","99032339":"markdown","a0d72c63":"markdown","65ac7723":"markdown","5a66f001":"markdown","865ba699":"markdown","d3e144e6":"markdown","fecbb539":"markdown","08e5bc25":"markdown","e9001daf":"markdown","8d66885a":"markdown","431c6343":"markdown","f1e45d79":"markdown","5cb5ea4c":"markdown","ac2c4894":"markdown","1bc363f9":"markdown","537e9c95":"markdown","33bb12be":"markdown","e5dd34a0":"markdown","d4ab8df4":"markdown","8c37a895":"markdown","53b9f384":"markdown","d5b47e7c":"markdown","16b7e282":"markdown","a03ea484":"markdown","6e003454":"markdown","e8f0871d":"markdown","738492f3":"markdown","af61419a":"markdown","ffb93108":"markdown","f076bcaa":"markdown","08507504":"markdown","806ea2fb":"markdown","1c348f74":"markdown","295b3aa1":"markdown","33a69f4c":"markdown","99101423":"markdown","383f3435":"markdown","150fee0e":"markdown","ee38cfe2":"markdown","d2cfe001":"markdown","e2c37bf9":"markdown","319a8450":"markdown","aa8140d9":"markdown","c0d018cd":"markdown","c8d05774":"markdown","7f98c76f":"markdown","902aec02":"markdown","1b78c8b2":"markdown","1ad24abf":"markdown","24d50b8a":"markdown","a62b38a3":"markdown","052f309c":"markdown","377610de":"markdown","e45d4df8":"markdown","3c110355":"markdown","8e1c6e96":"markdown","08c84487":"markdown","1de4e754":"markdown","d21ccf4c":"markdown","65297acd":"markdown","0bcb1355":"markdown","ba4e3304":"markdown","78847e62":"markdown","75d0344a":"markdown"},"source":{"a607b065":"from IPython.display import Image\nImage('..\/input\/imagesannibm\/image-logo.png')\n ","a9f73b7c":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom imblearn.over_sampling import SMOTE\n\n#preprocess.\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder,OneHotEncoder\n\n# ann and dl libraraies\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\n\nimport tensorflow as tf\nimport random as rn","8f199f71":"df=pd.read_csv(r'..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","078faf9f":"df.head()","beea472d":"df.shape","6c36d97f":"df.columns","6d93a503":"df.info()  # no null or Nan values.","fdd4909b":"df.isnull().sum()","b8814aca":"msno.matrix(df) # just to visualize.","f3f8a7ac":"df.columns","18678e8c":"df.head()","96c212c7":"df.describe()","124d9a5c":"sns.factorplot(data=df,kind='box',size=10,aspect=3)","20602967":"sns.kdeplot(df['Age'],shade=True,color='#ff4125')","09dfe5d4":"sns.distplot(df['Age'])","6c47240c":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\nfig,ax = plt.subplots(5,2, figsize=(9,9))                \nsns.distplot(df['TotalWorkingYears'], ax = ax[0,0]) \nsns.distplot(df['MonthlyIncome'], ax = ax[0,1]) \nsns.distplot(df['YearsAtCompany'], ax = ax[1,0]) \nsns.distplot(df['DistanceFromHome'], ax = ax[1,1]) \nsns.distplot(df['YearsInCurrentRole'], ax = ax[2,0]) \nsns.distplot(df['YearsWithCurrManager'], ax = ax[2,1]) \nsns.distplot(df['YearsSinceLastPromotion'], ax = ax[3,0]) \nsns.distplot(df['PercentSalaryHike'], ax = ax[3,1]) \nsns.distplot(df['YearsSinceLastPromotion'], ax = ax[4,0]) \nsns.distplot(df['TrainingTimesLastYear'], ax = ax[4,1]) \nplt.tight_layout()\nplt.show()","36293495":"cat_df=df.select_dtypes(include='object')","9fd68c44":"cat_df.columns","3f3aabf7":"def plot_cat(attr,labels=None):\n    if(attr=='JobRole'):\n        sns.factorplot(data=df,kind='count',size=5,aspect=3,x=attr)\n        return\n    \n    sns.factorplot(data=df,kind='count',size=5,aspect=1.5,x=attr)","d177b7de":"plot_cat('Attrition')   ","d3d04961":"plot_cat('BusinessTravel')   ","1e976b85":"plot_cat('OverTime')","fba931c5":"plot_cat('Department')   ","1d21eac1":"plot_cat('EducationField')","b5579bda":"plot_cat('Gender') ","0dc67012":"plot_cat('JobRole')   ","ac9ee183":"# just uncomment the following cell.","569b81d3":"# num_disc=['Education','EnvironmentSatisfaction','JobInvolvement','JobSatisfaction','WorkLifeBalance','RelationshipSatisfaction','PerformanceRating']\n# for i in num_disc:\n#     plot_cat(i)\n\n# similarly we can intrepret these graphs.","f260e25e":"#corelation matrix.\ncor_mat= df.corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","1470079e":"df.columns","befc9431":"sns.factorplot(data=df,y='Age',x='Attrition',size=5,aspect=1,kind='box')","e92d0d5e":"df.Department.value_counts()","9faf8c3a":"sns.factorplot(data=df,kind='count',x='Attrition',col='Department')","e3e3c291":"pd.crosstab(columns=[df.Attrition],index=[df.Department],margins=True,normalize='index') # set normalize=index to view rowwise %.","0139d0fe":"pd.crosstab(columns=[df.Attrition],index=[df.Gender],margins=True,normalize='index') # set normalize=index to view rowwise %.","09d39028":"pd.crosstab(columns=[df.Attrition],index=[df.JobLevel],margins=True,normalize='index') # set normalize=index to view rowwise %.","828e0828":"sns.factorplot(data=df,kind='bar',x='Attrition',y='MonthlyIncome')","5029ae9a":"sns.factorplot(data=df,kind='count',x='Attrition',col='JobSatisfaction')","9cdbbce6":"pd.crosstab(columns=[df.Attrition],index=[df.JobSatisfaction],margins=True,normalize='index') # set normalize=index to view rowwise %.","47b6d117":"pd.crosstab(columns=[df.Attrition],index=[df.EnvironmentSatisfaction],margins=True,normalize='index') # set normalize=index to view rowwise %.","c2a5fdcc":"pd.crosstab(columns=[df.Attrition],index=[df.JobInvolvement],margins=True,normalize='index') # set normalize=index to view rowwise %.","197b9371":"pd.crosstab(columns=[df.Attrition],index=[df.WorkLifeBalance],margins=True,normalize='index') # set normalize=index to view rowwise %.","7db01e18":"pd.crosstab(columns=[df.Attrition],index=[df.RelationshipSatisfaction],margins=True,normalize='index') # set normalize=index to view rowwise %.","b6e9e0bb":"df.drop(['BusinessTravel','DailyRate','EmployeeCount','EmployeeNumber','HourlyRate','MonthlyRate'\n           ,'NumCompaniesWorked','Over18','StandardHours', 'StockOptionLevel','TrainingTimesLastYear'],axis=1,inplace=True)","2950ebec":"def transform(feature):\n    le=LabelEncoder()\n    df[feature]=le.fit_transform(df[feature])\n    print(le.classes_)\n    \n","ea2f3564":"cat_df=df.select_dtypes(include='object')\ncat_df.columns","51076906":"for col in cat_df.columns:\n    transform(col)\n","0d0956fb":"df.head() # just to verify.","e215e87f":"scaler=StandardScaler()\nscaled_df=scaler.fit_transform(df.drop('Attrition',axis=1))\nX=scaled_df\nY=df['Attrition'].as_matrix()","f2754751":"Y=to_categorical(Y)\nY","163570ef":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)","2daecbf3":"# oversampler=SMOTE(random_state=42)\n# x_train_smote,  y_train_smote = oversampler.fit_sample(x_train,y_train)","4aff198a":"np.random.seed(42)","760225e9":"rn.seed(42)","cbbb6fd4":"tf.set_random_seed(42)","d59e3688":"model=Sequential()\nmodel.add(Dense(input_dim=23,units=8,activation='relu'))\nmodel.add(Dense(units=16,activation='relu'))\nmodel.add(Dense(units=2,activation='sigmoid'))","60127fc8":"model.compile(optimizer=Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])","5d6b4f46":"model.summary()","292bd3e3":"History=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10,verbose=1)","fb46011f":"model.predict_classes(x_test)","0d163276":"model.predict(x_test)","78e350b6":"model.evaluate(x_test,y_test)","b210c143":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","1d263b91":"plt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","53b70623":"Image('..\/input\/imagesannibm\/image-hr.jpg')","fafdc9f5":"https:\/\/github.com\/mrc03\/IBM-HR-Analytics-Employee-Attrition-Performance","249bbb0f":"## 5.5 ) Summary of the model","865d162b":"1. ######  Note that you can view the same kernel on my Github acccount also::--","257b94cc":"## 4.2 ) Feature Scaling","6197d15c":"#### 3.1.5 ) Monthly Income","87673b44":"## 5.3 ) Building the Keras model","1c51ec4e":"Note that the median as well the maximum age of the peole with 'No' attrition is higher than that of the 'Yes' category. This shows that peole with higher age have lesser tendency to leave the organisation which makes sense as they may have settled in the organisation.","de85f600":"[**4) Preparing Dataset**](#content4)","97e4b5f6":"Let us now similalry analyze other categorical features.","9ee94a58":"#### 3.1.2 ) Department","2f88313c":"## 5.1.1 ) Oversampling the Minority or Undersampling the Majority Class\n \n  ","496d7caf":"####  3.1.1 ) Age","f9ded216":"I have made a function that accepts the name of a string. In our case this string will be the name of the column or attribute which we want to analyze. The function then plots the countplot for that feature which makes it easier to visualize.","e825fc09":"Note that most of the observations corresspond to 'No' as we saw previously also. About 81 % of the people in HR dont want to leave the organisation and only 19 % want to leave. Similar conclusions can be drawn for other departments too from the above cross-tabulation.","abf9b7b2":"## 5.8 ) Evaluating the Model Performance","bda733d2":"## 1.1 ) Importing Various Modules","74d8cab3":"<a id=\"content1\"><\/a>\n## 1 ) Exploratory Data Analysis","8995205c":"<a id=\"content3\"><\/a>\n## 3 ) Feature Selection","4b9b0480":"Note that all the features have pretty different scales and so plotting a boxplot is not a good idea. Instead what we can do is plot histograms of various continuously distributed features.\n\nWe can also plot a kdeplot showing the distribution of the feature. Below I have plotted a kdeplot for the 'Age' feature.\nSimilarly we plot for other numeric features also. We can also use a distplot from seaborn library.","8fe95b6d":"I have used the Label Encoder from the scikit library to encode all the categorical features.","4c647f5b":"#### 3.1.4 ) Job Level","61d70d9d":"Another important point while dealing with the imbalanced classes is the choice of right evaluation metrics. \n\nNote that accuracy is not a good choice. This is because since the data is skewed even an algorithm classifying the target as that belonging to the majority class at all times will achieve a very high accuracy. \nFor  eg if we have 20 observations of one type 980 of another ; a classifier predicting the majority class at all times will also attain a accuracy of 98 % but doesnt convey any useful information.\n\nHence in these type of cases we may use other metrics such as -->\n\n\n'Precision'-- (true positives)\/(true positives+false positives)\n\n'Recall'-- (true positives)\/(true positives+false negatives)\n\n'F1 Score'-- The harmonic mean of 'precision' and 'recall'\n\n'AUC ROC'-- ROC curve is a plot between 'senstivity' (Recall) and '1-specificity' (Specificity=Precision)\n\n'Confusion Matrix'-- Plot the entire confusion matrix","ff5ed138":"The above plot clearly shows that most of the people belong to the 'Travel_Rarely' class. This indicates that most of the people did not have a job which asked them for frequent travelling.","d141b41a":"######  Hence this is a Binary Classification problem. ","e30e283d":"#### 3.1.9 ) Work Life Balance","88594126":"###### Hence we have completed the analysis of the data and also made predictions using an Artificial Neural Network.      ","c7591234":"a) Number of hidden layers.\n\nb) Number of neurons in a particular layer.\n\nc) The activation function.\n\nd) The optimizer used.\n\ne) Number of epochs etc...","0ade3756":"###### Notice that I have plotted just some of the important features against out 'Target' variable i.e. Attrition in our case. Similarly we can plot other features against the 'Target' variable and analye the trends i.e. how the feature effects the 'Target' variable.","1ee872c3":"[ **2) Corelation b\/w Features**](#content2)","e54a1e58":"<a id=\"content4\"><\/a>\n##  4 ) Preparing Dataset","383a73f7":"Note this shows an interesting trend. Note that for higher values of job satisfaction( ie more a person is satisfied with his job) lesser percent of them say a 'Yes' which is quite obvious as highly contented workers will obvioulsy not like to leave the organisation.","c58e1243":"## 1.5 ) Univariate Analysis","99032339":" Let us first analyze the various numeric features. To do this we can actually plot a boxplot showing all the numeric features.","a0d72c63":"## 3.1 ) Plotting the Features against the 'Target' variable.","65ac7723":"#### 3.1.8 ) Job Involvement","5a66f001":"<a id=\"content7\"><\/a>\n## 7 ) Conclusions","865ba699":"#### 3.1.7 ) Environment Satisfaction ","d3e144e6":"Note that in order to get exactly same results after training an artificial neural network at different instances of time we need to specify the random seed for the Keras backend engine which is TensorFlow in my case. Also I have specified the seeds for the python random module as well as for the numpy.\n\nIn order to adjust the weights oof an ANN ; the BackProp algorithm starts with a random weights and hence after a given no of epochs the results can be different if the random initialisation of weights is different in starting. \n\nHence to obtain the same results it is necessary to specify the random seed to get the reproducible results.","fecbb539":"[ **1 ) Exploratory Data Analysis**](#content1)","08e5bc25":"## 1.4 ) The Features and the 'Target'","e9001daf":"Note that Age is a continuous quantity and therefore we can plot it against the Attrition using a boxplot.","8d66885a":" In all we have 34 features consisting of both the categorical as well as the numerical features. The target variable is the \n 'Attrition' of the employee which can be either a Yes or a No.","431c6343":"Again we can notice that the relative percent of 'No' in people with higher grade of environment satisfacftion.","f1e45d79":"######  Similarly we can continue for other categorical features. \n\n ","5cb5ea4c":" Note that the average income for 'No' class is quite higher and it is obvious as those earning well will certainly not be willing to exit the organisation. Similarly those employees who are probably not earning well will certainly want to change the company.","ac2c4894":"####  BREAKING IT DOWN\n\n1. First we need to build a model. For this we use the Sequential model provided by the Keras which is nothing but a linear stack of layers.\n\n\n2. Next we need to add the layers to our Sequential model. For this we use the model.add() function.\n\n\n3. Note that for each layer we need to specify the number of units ( or the number of neurons) and also the activation function used by the neurons.\n\n  Note that activation  function is used to model complex non-linear relationships and their are many choices. But generally it is preferred to use 'relu' function for the hidden layers and the 'sigmoid' or the 'logistic' function for the output layer. For a multi-class classification problem we can use the 'softmax' function as the activation function for the output layer.\n  \n\n4. Note that the first layer and ONLY the first layer expects the input dimensions in order to know the shape of the input numpy array.\n\n\n5. Finally note that the number of units or neurons in the final layer is equal to the number of classes of the target variable. In other words for a 'n' class classification problem we shall have 'n' neurons in the output layer. \n \n Each neuron represents a specific target class. The output of each neuron in the final layer thus represents the probability of given observation being classified to that target class. The observation is classified to the target class; the neuron corressponding to which has the highest value. ","1bc363f9":"About 85 % of females want to stay in the organisation while only 15 % want to leave the organisation. All in all 83 % of employees want to be in the organisation with only being 16% wanting to leave the organisation or the company.","537e9c95":"###### Note that the same function can also be used to better analyze the numeric discrete features like 'Education' ,'JobSatisfaction' etc...  ","33bb12be":"###### SOME INFERENCES FROM THE ABOVE HEATMAP\n\n1. Self relation ie of a feature to itself is equal to 1 as expected.\n\n2. JobLevel is highly related to Age as expected as aged employees will generally tend to occupy higher positions in the company.\n\n3. MonthlyIncome is very strongly related to joblevel as expected as senior employees will definately earn more.\n\n4. PerformanceRating is highly related to PercentSalaryHike which is quite obvious.\n\n5. Also note that TotalWorkingYears is highly related to JobLevel which is expected as senior employees must have worked for a larger span of time.\n\n6. YearsWithCurrManager is highly related to YearsAtCompany.\n\n7. YearsAtCompany is related to YearsInCurrentRole.\n\n  ","e5dd34a0":"Before feeding our data into a ML model we first need to prepare the data. This includes encoding all the categorical features (either LabelEncoding or the OneHotEncoding) as the model expects the features to be in numerical form. Also for better performance we will do the feature scaling ie bringing all the features onto the same scale by using the StandardScaler provided in the scikit library.","d4ab8df4":"<a id=\"content6\"><\/a>\n## 6 ) Hyperparameter Tuning","8c37a895":"## 5.1.2 ) Using the Right Evaluation Metric","53b9f384":"Provides overall description of the model.","d5b47e7c":"#### BREAKING IT DOWN\n\n1. Lastly we need to fit our model onto the training data just as we do for traditional ML algorithms.\n\n\n2. We have to specify the training(x_train ,y_train) and the testing (validation_data) sets.\n\n\n3. We also need to specify the 'number of epochs'. \n   An 'epoch' is one entire cycle of 'Forward & Backward propagation' through all the training examples.\n\n\n4. Verbose is an optional parameter that just ensures how the output of each epoch is displayed on the screen.\n\n\n5. We have assigned it to a 'History' variable to retrieve the model performance during each epoch in the future.","16b7e282":"In an imbalanced dataset the main problem is that the data is highly skewed ie the number of observations of certain class is more than that of the other. Therefore what we do in this approach is to either increase the number of observations corressponding  to the minority class (oversampling) or decrease the number of observations for the majority class (undersampling).\n\nNote that in our case the number of observations is already pretty low and so oversampling will be more appropriate.\n\nBelow I have used an oversampling technique known as the SMOTE(Synthetic Minority Oversampling Technique) which randomly creates some 'Synthetic' instances of the minority class so that the net observations of both the class get balanced out.\n\nOne thing more to take of is to use the SMOTE before the cross validation step; just to ensure that our model does not overfit the data; just as in the case of feature selection.","a03ea484":"## 1.2 ) Reading the data from a CSV file","6e003454":"## 5.1 ) Handling the Imbalanced dataset","e8f0871d":"#### 3.1.6 ) Job Satisfaction","738492f3":"#### In order to increase the model performance of the ANN we need to tune the hyperparameters. Some of the hyperparameters include","af61419a":"<a id=\"content2\"><\/a>\n## 2 ) Corelation b\/w Features","ffb93108":"## 5.4 ) Compiling the Keras model","f076bcaa":"The scikit library provides various types of scalers including MinMax Scaler and the StandardScaler. Below I have used the StandardScaler to scale the data.","08507504":"In this section I have done the univariate analysis i.e. I have analysed the range or distribution of the values that various features take. To better analyze the results I have plotted various graphs and visualizations wherever necessary.","806ea2fb":"## 4.4 ) Splitting the data into training and validation sets","1c348f74":"## [Please star\/upvote it if you like it.]","295b3aa1":"## 1.3 ) Missing Values Treatment","33a69f4c":"#### 3.1.10 ) RelationshipSatisfaction","99101423":"#### 3.1.3 ) Gender","383f3435":"## CONTENTS :","150fee0e":"# THE END. [please star\/upvote if u find it helpful.]","ee38cfe2":"## 5.6 ) Fitting the model on the training data and testing on the validation set","d2cfe001":"[ **3) Feature Selection**](#content3)","e2c37bf9":"[ **5) Making Predictions Using an Artificial Neural Network (ANN)**](#content5)\n\n Note that this notebook uses ANN. I have another notebook in which I have used traditional ML algorithms on the same dataset. To  check it out please follow the below link-->\n\nhttps:\/\/www.kaggle.com\/rajmehra03\/imbalanceddata-predictivemodelling-by-ibm-dataset\/","319a8450":"Note that there are two main things to watch out before feeding data into an ANN. \n\nThe first is that our data needs to be in the form of numpy arrays (ndarray).\n\nThe second that the target variable should be one hot encoded eg 2--> 0010 (assuming 0 based indexing) and so on.. \nIn this way for a 'n' class classification problems our target variable will have n classes and hence after one hot encoding we shall have n labels with each label corressponding to a particular target class.","aa8140d9":"Note that we can drop some highly corelated features as they add redundancy to the model but since the corelation is very less in genral let us keep all the features for now. In case of highly corelated features we can use something like Principal Component Analysis(PCA) to reduce our feature space.","c0d018cd":"Note that both Attrition(Target) as well as the Deaprtment are categorical. In such cases a cross-tabulation is the most reasonable way to analyze the trends; which shows clearly the number of observaftions for each class which makes it easier to analyze the results.","c8d05774":"###### BREAKING IT DOWN\nFirstly calling .corr() method on a pandas data frame returns a corelation data frame containing the corelation values b\/w the various attributes.\nnow we obtain a numpy array from the corelation data frame using the np.array method.\nnextly using the np.tril_indices.from() method we set the values of the lower half of the mask numpy array to False. this is bcoz on passing the mask to heatmap function of the seaborn it plots only those squares whose mask is False. therefore if we don't do this then as the mask is by default True then no square will appear. Hence in a nutshell we obtain a numpy array from the corelation data frame and set the lower values to False so that we can visualise the corelation. In order for a full square just use the [:] operator in mask in place of tril_ind... function.\nin next step we get the refernce to the current figure using the gcf() function of the matplotlib library and set the figure size.\nin last step we finally pass the necessary parameters to the heatmap function.\n\nDATA=the corelation data frame containing the 'CORELATION' values.\n\nMASK= explained earlier.\n\nvmin,vmax= range of values on side bar\n\nSQUARE= to show each individual unit as a square.\n\nANNOT- whether to dispaly values on top of square or not. In order to dispaly pass it either True or the cor_mat.\n\nCBAR= whether to view the side bar or not.","7f98c76f":"## 5.7 ) Making Predictions","902aec02":"## 4.1 ) Feature Encoding ","1b78c8b2":"[ **6) Hyperparameter Tuning**](#content6)","1ad24abf":"## 3.2 ) Feature Selection","24d50b8a":"[ **7)Conclusions**](#content7)","a62b38a3":"Note that the neural networks are quite sensitive towards the scale of the features. Hence it is always good to perform feature scaling on the data before feeding it into an Artificial Neural Network.","052f309c":"Again we notice a similar trend as people with better work life balance dont want to leave the organisation.","377610de":"#### BREAKING IT DOWN\n\n1. Now we need to compile the model. We have to specify the optimizer used by the model We have many choices like Adam, RMSprop etc.. Rfer to Keras doc for a comprehensive list of the optimizers available.\n\n\n2. Next we need to specify the loss function for the neural network which we seek to minimize.\n\n  I have used the 'binary_crossentropy' loss function since this is a binary classification problem. For a multi-class classification problems we may use the 'categorical_crossentropy'.\n\n\n3. Next we need to specify the metric to evaluate our models performance. Here I have used accuracy.","e45d4df8":"The feature Selection is one of the main steps of the preprocessing phase as the features which we choose directly effects the model performance. While some of the features seem to be less useful in terms of the context; others seem to equally useful. The better features we use the better our model will perform.\n\nWe can also use the Recusrive Feature Elimination technique (a wrapper method) to choose the desired number of most important features.\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n\nWe can use it directly from the scikit library by importing the RFE module or function provided by the scikit. But note that since it tries different combinations or the subset of features;it is quite computationally expensive.","3c110355":"Note that males are presnt in higher number.","8e1c6e96":"People in Joblevel 4 have a very high percent for a 'No' and a low percent for a 'Yes'. Similar inferences can be made for other job levels.","08c84487":"# IBM HR Analytics Employee Attrition & Performance          ","1de4e754":"## 5.2) Setting the random seeds","d21ccf4c":"<a id=\"content5\"><\/a>\n## 5 )  Making Predictions Using an Artificial Neural Network (ANN)","65297acd":"Similarly we can do this for all the numerical features. Below I have plotted the subplots for the other features.","0bcb1355":"######  Note that the number of observations belonging to the 'No'  category is way greater than that belonging to 'Yes' category. Hence we have skewed classes and this is a typical example of the 'Imbalanced Classification Problem'. To handle such types of problems we need to use the over-sampling or under-sampling techniques. I shall come back to this point later.","ba4e3304":"Let us now analyze the various categorical features. Note that in these cases the best way is to use a count plot to show the relative count of observations of different categories.","78847e62":"Note that we have a imbalanced dataset with majority of observations being of one type ('NO') in our case. In this dataset for example we have about 84 % of observations having 'No' and only 16 % of 'Yes' and hence this is an imbalanced dataset.\n\nTo deal with such a imbalanced dataset we have to take certain measures, otherwise the performance of our model can be significantly affected. In this section I have discussed two approaches to curb such datasets.","75d0344a":"## 4.3 ) One Hot Encoding the Target "}}