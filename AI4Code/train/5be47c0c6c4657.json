{"cell_type":{"a89c86b1":"code","5667add3":"code","966f02e1":"code","014903b4":"code","d4c3b79f":"code","ccd847a4":"code","b881e635":"code","e92e7a60":"code","c1b711bb":"code","daddfb3b":"code","320c419e":"code","a34eedf8":"code","c39f4b3f":"markdown","7e77ba48":"markdown","33093d1e":"markdown","7ce698e4":"markdown","ce93c820":"markdown","b375ad6a":"markdown"},"source":{"a89c86b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pickle\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib_venn import venn2\nimport seaborn as sns\nsns.set_context(\"talk\")\n# sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\nstyle.use('fivethirtyeight')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nimport optuna\n\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nfrom functools import partial\nimport scipy as sp\nfrom sklearn import metrics\nimport os, sys, gc\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings('ignore')","5667add3":"train = pd.read_csv(\"\/kaggle\/input\/killer-shrimp-invasion\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/killer-shrimp-invasion\/test.csv\")","966f02e1":"print(train.shape)\ntrain.head()","014903b4":"print(test.shape)\ntest.head()","d4c3b79f":"from sklearn import linear_model\n\ndef lin_model(cls, train_set, val_set):\n    \"\"\"\n    Linear model hyperparameters and models\n    \"\"\"\n\n    params = {\n            'max_iter': 8000,\n            'fit_intercept': True,\n            'random_state': cls.seed\n        }\n\n    if cls.task == \"regression\":\n        # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html\n        model = linear_model.Ridge(**{'alpha': 220, 'solver': 'lsqr', 'fit_intercept': params['fit_intercept'],\n                                'max_iter': params['max_iter'], 'random_state': params['random_state']})\n    elif (cls.task == \"binary\") | (cls.task == \"multiclass\"):\n        # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n        model = linear_model.LogisticRegression(**{\"C\": 1.0, \"fit_intercept\": params['fit_intercept'], \n                                \"random_state\": params['random_state'], \"solver\": \"lbfgs\", \"max_iter\": params['max_iter'], \n                                \"multi_class\": 'auto', \"verbose\":0, \"warm_start\":False})\n                                \n    model.fit(train_set['X'], train_set['y'])\n\n    # feature importance (for multitask, absolute value is computed for each feature)\n    if cls.task == \"multiclass\":\n        fi = np.mean(np.abs(model.coef_), axis=0).ravel()\n    else:\n        fi = model.coef_.ravel()\n\n    return model, fi","ccd847a4":"def get_oof_ypred(model, x_val, x_test, modelname=\"linear\", task=\"binary\"):  \n    \"\"\"\n    get oof and target predictions\n    \"\"\"\n    sklearns = [\"xgb\", \"catb\", \"linear\", \"knn\"]\n\n    if task == \"binary\": # classification\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n            oof_pred = oof_pred[:, 1]\n            y_pred = y_pred[:, 1]\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n            # NN specific\n            if modelname == \"nn\":\n                oof_pred = oof_pred.ravel()\n                y_pred = y_pred.ravel()        \n\n    elif task == \"multiclass\":\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n        oof_pred = np.argmax(oof_pred, axis=1)\n        y_pred = np.argmax(y_pred, axis=1)\n\n    elif task == \"regression\": # regression\n        oof_pred = model.predict(x_val)\n        y_pred = model.predict(x_test)\n\n        # NN specific\n        if modelname == \"nn\":\n            oof_pred = oof_pred.ravel()\n            y_pred = y_pred.ravel()\n\n    return oof_pred, y_pred","b881e635":"class RunModel(object):\n    \"\"\"\n    Model Fitting and Prediction Class:\n\n    train_df : train pandas dataframe\n    test_df : test pandas dataframe\n    target : target column name (str)\n    features : list of feature names\n    categoricals : list of categorical feature names\n    model : lgb, xgb, catb, linear, or nn\n    task : options are ... regression, multiclass, or binary\n    n_splits : K in KFold (default is 3)\n    cv_method : options are ... KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n    group : group feature name when GroupKFold or StratifiedGroupKFold are used\n    parameter_tuning : bool, only for LGB\n    seed : seed (int)\n    scaler : options are ... None, MinMax, Standard\n    verbose : bool\n    \"\"\"\n\n    def __init__(self, train_df, test_df, target, features, categoricals=[],\n                model=\"lgb\", task=\"regression\", n_splits=4, cv_method=\"KFold\", \n                group=None, parameter_tuning=False, seed=1220, scaler=None, verbose=True):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.target = target\n        self.features = features\n        self.categoricals = categoricals\n        self.model = model\n        self.task = task\n        self.n_splits = n_splits\n        self.cv_method = cv_method\n        self.group = group\n        self.parameter_tuning = parameter_tuning\n        self.seed = seed\n        self.scaler = scaler\n        self.verbose = verbose\n        self.cv = self.get_cv()\n        self.y_pred, self.score, self.model, self.oof, self.y_val, self.fi_df = self.fit()\n\n    def train_model(self, train_set, val_set):\n\n        # compile model\n        if self.model == \"lgb\": # LGB             \n            model, fi = lgb_model(self, train_set, val_set)\n\n        elif self.model == \"xgb\": # xgb\n            model, fi = xgb_model(self, train_set, val_set)\n\n        elif self.model == \"catb\": # catboost\n            model, fi = catb_model(self, train_set, val_set)\n\n        elif self.model == \"linear\": # linear model\n            model, fi = lin_model(self, train_set, val_set)\n\n        elif self.model == \"knn\": # knn model\n            model, fi = knn_model(self, train_set, val_set)\n\n        elif self.model == \"nn\": # neural network\n            model, fi = nn_model(self, train_set, val_set)\n        \n        return model, fi # fitted model and feature importance\n\n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        if self.model == \"lgb\":\n            train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n            val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        else:\n            if (self.model == \"nn\") & (self.task == \"multiclass\"):\n                n_class = len(np.unique(self.train_df[self.target].values))\n                train_set = {'X': x_train, 'y': onehot_target(y_train, n_class)}\n                val_set = {'X': x_val, 'y': onehot_target(y_val, n_class)}\n            else:\n                train_set = {'X': x_train, 'y': y_train}\n                val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n\n    def calc_metric(self, y_true, y_pred): # this may need to be changed based on the metric of interest\n        if self.task == \"multiclass\":\n            return f1_score(y_true, y_pred, average=\"macro\")\n        elif self.task == \"binary\":\n            return roc_auc_score(y_true, y_pred) # log_loss\n        elif self.task == \"regression\":\n            return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def get_cv(self):\n        # return cv.split\n        if self.cv_method == \"KFold\":\n            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df)\n        elif self.cv_method == \"StratifiedKFold\":\n            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target])\n        elif self.cv_method == \"TimeSeriesSplit\":\n            cv = TimeSeriesSplit(max_train_size=None, n_splits=self.n_splits)\n            return cv.split(self.train_df)\n        elif self.cv_method == \"GroupKFold\":\n            cv = GroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n        elif self.cv_method == \"StratifiedGroupKFold\":\n            cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n\n    def fit(self):\n        # initialize\n        oof_pred = np.zeros((self.train_df.shape[0], ))\n        y_vals = np.zeros((self.train_df.shape[0], ))\n        y_pred = np.zeros((self.test_df.shape[0], ))\n\n        # group does not kick in when group k fold is used\n        if self.group is not None:\n            if self.group in self.features:\n                self.features.remove(self.group)\n            if self.group in self.categoricals:\n                self.categoricals.remove(self.group)\n        fi = np.zeros((self.n_splits, len(self.features)))\n\n        # scaling, if necessary\n        if self.scaler is not None:\n            # fill NaN\n            numerical_features = [f for f in self.features if f not in self.categoricals]\n            self.train_df[numerical_features] = self.train_df[numerical_features].fillna(self.train_df[numerical_features].median())\n            self.test_df[numerical_features] = self.test_df[numerical_features].fillna(self.test_df[numerical_features].median())\n            self.train_df[self.categoricals] = self.train_df[self.categoricals].fillna(self.train_df[self.categoricals].mode().iloc[0])\n            self.test_df[self.categoricals] = self.test_df[self.categoricals].fillna(self.test_df[self.categoricals].mode().iloc[0])\n\n            # scaling\n            if self.scaler == \"MinMax\":\n                scaler = MinMaxScaler()\n            elif self.scaler == \"Standard\":\n                scaler = StandardScaler()\n            df = pd.concat([self.train_df[numerical_features], self.test_df[numerical_features]], ignore_index=True)\n            scaler.fit(df[numerical_features])\n            x_test = self.test_df.copy()\n            x_test[numerical_features] = scaler.transform(x_test[numerical_features])\n            if self.model == \"nn\":\n                x_test = [np.absolute(x_test[i]) for i in self.categoricals] + [x_test[numerical_features]]\n            else:\n                x_test = x_test[self.features]\n        else:\n            x_test = self.test_df[self.features]\n\n        # fitting with out of fold\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            # train test split\n            x_train, x_val = self.train_df.loc[train_idx, self.features], self.train_df.loc[val_idx, self.features]\n            y_train, y_val = self.train_df.loc[train_idx, self.target], self.train_df.loc[val_idx, self.target]\n\n            # fitting & get feature importance\n            if self.scaler is not None:\n                x_train[numerical_features] = scaler.transform(x_train[numerical_features])\n                x_val[numerical_features] = scaler.transform(x_val[numerical_features])\n                if self.model == \"nn\":\n                    x_train = [np.absolute(x_train[i]) for i in self.categoricals] + [x_train[numerical_features]]\n                    x_val = [np.absolute(x_val[i]) for i in self.categoricals] + [x_val[numerical_features]]\n\n            # model fitting\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model, importance = self.train_model(train_set, val_set)\n            fi[fold, :] = importance\n            y_vals[val_idx] = y_val\n\n            # predictions\n            oofs, ypred = get_oof_ypred(model, x_val, x_test, self.model, self.task)\n            oof_pred[val_idx] = oofs.reshape(oof_pred[val_idx].shape)\n            y_pred += ypred.reshape(y_pred.shape) \/ self.n_splits\n\n            # check cv score\n            print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_vals[val_idx], oof_pred[val_idx])))\n\n        # feature importance data frame\n        fi_df = pd.DataFrame()\n        for n in np.arange(self.n_splits):\n            tmp = pd.DataFrame()\n            tmp[\"features\"] = self.features\n            tmp[\"importance\"] = fi[n, :]\n            tmp[\"fold\"] = n\n            fi_df = pd.concat([fi_df, tmp], ignore_index=True)\n        gfi = fi_df[[\"features\", \"importance\"]].groupby([\"features\"]).mean().reset_index()\n        fi_df = fi_df.merge(gfi, on=\"features\", how=\"left\", suffixes=('', '_mean'))\n\n        # outputs\n        loss_score = self.calc_metric(y_vals, oof_pred)\n        if self.verbose:\n            print('Our oof loss score is: ', loss_score)\n        return y_pred, loss_score, model, oof_pred, y_vals, fi_df\n\n    def plot_feature_importance(self, rank_range=[1, 50]):\n        # plot feature importance\n        _, ax = plt.subplots(1, 1, figsize=(10, 20))\n        sorted_df = self.fi_df.sort_values(by = \"importance_mean\", ascending=False).reset_index().iloc[self.n_splits * (rank_range[0]-1) : self.n_splits * rank_range[1]]\n        sns.barplot(data=sorted_df, x =\"importance\", y =\"features\", orient='h')\n        ax.set_xlabel(\"feature importance\")\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        return sorted_df","e92e7a60":"features = test.columns.values.tolist()\ncategoricals = []\ntarget = \"Presence\"\ngroup = None\ndropcols = [\"pointid\"]\nfeatures = [f for f in features if f not in dropcols]\ncategoricals = [f for f in categoricals if f not in dropcols]","c1b711bb":"model = RunModel(train, test, target, features, categoricals=categoricals,\n            model=\"linear\", task=\"binary\", n_splits=4, cv_method=\"StratifiedKFold\", \n            group=group, parameter_tuning=False, seed=1220, scaler=\"Standard\", verbose=True)\nprint(\"Training done\")","daddfb3b":"fi = model.plot_feature_importance()","320c419e":"sub = pd.read_csv(\"\/kaggle\/input\/killer-shrimp-invasion\/temperature_submission.csv\")\nsub[\"Presence\"] = model.y_pred\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","a34eedf8":"sub[\"Presence\"].hist()","c39f4b3f":"# Modeling Pipeline\nThe following is a part of my modeling pipeline (https:\/\/github.com\/katsu1110\/DataScienceComp).","7e77ba48":"# Load data","33093d1e":"# Model Fitting","7ce698e4":"# Libraries","ce93c820":"# Feature importance\nI mean, coefficients.","b375ad6a":"# Submission"}}