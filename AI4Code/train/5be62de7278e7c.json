{"cell_type":{"f95e19eb":"code","ae0dc6d8":"code","b9007c7d":"code","26177f38":"code","9ca1b813":"code","5ef5a17a":"code","05a8b008":"code","a88c0643":"code","2fdfe78c":"code","e9fd9419":"code","e8d04e30":"code","03ab73b0":"code","9184e66b":"code","2da1a340":"code","a359236b":"code","550675f1":"code","5f11fbef":"code","8ec7c067":"code","76dfca91":"code","79183d3b":"code","75e722b0":"code","dc1a6202":"code","fdb2b942":"code","1bfbe4bf":"code","21383386":"code","bf9ba2c8":"code","67a092ee":"code","1d2446d3":"code","fccdd001":"code","a70b8111":"code","4bf4e561":"code","fe7f90af":"code","2cf6c620":"code","394b1308":"code","8720a42c":"code","3b6a6ef7":"code","89dbf4ce":"code","d16a3102":"code","bf79fad3":"code","b6df3906":"code","e8884971":"code","b6cdf063":"code","6a3bd8e6":"code","05d247b8":"code","f8a0a95a":"code","a4f91b84":"code","50a62d6c":"code","2a085fc4":"code","c6d815ea":"code","e92b7f41":"code","d8a1c29c":"code","b602e40d":"code","337696e4":"code","830a95a3":"code","0026707f":"code","b312aa2b":"code","755c5084":"code","4b19f4ac":"code","9c0b35e5":"code","b5fa6e1a":"code","b0d29956":"code","8d8f0379":"code","3980394b":"code","fde4d716":"code","e2b13527":"code","4dac04a0":"code","a705e5f1":"code","61986950":"code","f49847b5":"code","1e87e9e1":"code","b3a2de4d":"code","1e33ecdd":"code","f38116bc":"code","ba06904a":"code","95ab5a40":"code","77ad9a8d":"code","432d6205":"markdown","971d6e10":"markdown","689e065e":"markdown","5e95967e":"markdown","229211ba":"markdown","bf75ae80":"markdown","f4eb4200":"markdown","5efa1170":"markdown","a9cc71d0":"markdown","8055283c":"markdown","8eafd1cd":"markdown","72e8c60e":"markdown","b3e2bc64":"markdown","7f5b6efb":"markdown","6e0001bf":"markdown","10bc8ada":"markdown","d91cb40c":"markdown","df0f9cfc":"markdown","006ea090":"markdown","858b27cd":"markdown","6b1e7342":"markdown","85f12d4b":"markdown","6be1b5e6":"markdown","0207a893":"markdown","1727ab18":"markdown","1d4bde3e":"markdown","64c74955":"markdown","6ff91fc4":"markdown","2860020a":"markdown","01088e49":"markdown","c1f6dd60":"markdown","685fac0a":"markdown","ba942703":"markdown","f630b869":"markdown","427fb029":"markdown","c90ba8e3":"markdown","b55d426f":"markdown","d072ae56":"markdown","6b6aae69":"markdown","39a1288c":"markdown","e194bd2b":"markdown","74031f10":"markdown","35c68f86":"markdown","38fa09cb":"markdown","b409aa46":"markdown","35ce7edd":"markdown","d721c53e":"markdown","3ad85aed":"markdown","5b8af5d3":"markdown","5464fc41":"markdown","55092dcd":"markdown","5fbea812":"markdown","cffa4936":"markdown","07ab318e":"markdown","469be8c8":"markdown","922f1d7b":"markdown","dd7aacf9":"markdown","f6b81e18":"markdown","0700d846":"markdown","ab3f110a":"markdown","8b381334":"markdown","ba4064e9":"markdown","42a39138":"markdown","aa281b6c":"markdown","146d8caf":"markdown","53ad80a3":"markdown","bec26438":"markdown","63048a32":"markdown","ff3e3f76":"markdown","6d91ee4d":"markdown","c049e4dc":"markdown","b49cf345":"markdown","22fb8975":"markdown","3528cfbf":"markdown","686a3cdf":"markdown","bcc6b834":"markdown","be47e43d":"markdown","effe01b4":"markdown","1057516d":"markdown","e5fa8cbe":"markdown"},"source":{"f95e19eb":"import warnings\nwarnings.filterwarnings('ignore')\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# Any results you write to the current directory are saved as output.\n# import libraries for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium import plugins\n%matplotlib inline","ae0dc6d8":"data = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\nlocationData = pd.read_csv(\"..\/input\/loccsv\/location.csv\")\n\ndata.describe()","b9007c7d":"category_columns = [col for col in data.columns if data[col].dtype==\"object\" ]\nnumerical_columns = [col for col in data.columns if data[col].dtype!=\"object\" ]\n#Check if all columns are accouned for \n#print(len(category_columns)+len(numerical_columns)==len(data.columns))\nprint(\"There are {} columns. {} are category and {} are numerical\".format(len(data.columns),len(category_columns),len(numerical_columns)))","26177f38":"print(\"The category columns are {}\".format(category_columns))","9ca1b813":"print(\"There are {} missing values for the target\".format(data['RainTomorrow'].isnull().sum()))","5ef5a17a":"def balanceTarget (target):\n    rainTodayAnalysis = target.value_counts()\n    f,  ax = plt.subplots(nrows=1,ncols=2) \n    sns.barplot(rainTodayAnalysis.index,rainTodayAnalysis.values, ax=ax[0])\n    sns.barplot(rainTodayAnalysis.index,rainTodayAnalysis.values\/len(data), ax=ax[1])\n    ax[0].set(xlabel='Rain Tomorrow', ylabel='Number of Occurrences')\n    ax[1].set(xlabel='Rain Tomorrow', ylabel='Percentage of Occurrences')\n    plt.tight_layout()\n\nbalanceTarget(data[\"RainTomorrow\"])","05a8b008":"cat_features = list(filter(lambda x: x!=\"RainTomorrow\", category_columns))\ncat_features_miss = data[cat_features].isnull().sum()\nf,  ax = plt.subplots(nrows=1,ncols=2) \nsns.barplot(cat_features_miss.index,cat_features_miss.values, ax=ax[0])\nsns.barplot(cat_features_miss.index,cat_features_miss.values\/len(data), ax=ax[1])\nax[0].set(ylabel='Number of Occurrences')\nax[0].set_xticklabels(ax[0].get_xticklabels(),rotation=75)\nax[1].set( ylabel='Percentage of Occurrences')\nax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=75)\nax[1].set_ylim(0,1)                      \nplt.suptitle(\"Missing Values for Categorical Data\")\nplt.tight_layout(rect=[0, 0.03, 1, 0.90])","a88c0643":"cat_features = list(filter(lambda x: x!=\"RainTomorrow\", category_columns))\ncat_features_dict = {}\nfor features in cat_features:\n    cat_features_dict[features]=len(list(filter(lambda x: isinstance(x, str) or math.isnan(x)==False ,data[features].unique())))\ncat_features_dict\nuniqueCat = pd.DataFrame(cat_features_dict,index=[\"Number of Unique Values\"])\nuniqueCat","2fdfe78c":"locationData = locationData.dropna()\n\nm=folium.Map([-25.2744,133.7751],zoom_start=4,width=\"70%\",height=\"70%\",left=\"10%\")\nfor lat,lon,area in zip(locationData['Latitude'],locationData['Longitude'],locationData['Location']):\n     folium.CircleMarker([lat, lon],\n                            popup=area,\n                            radius=3,\n                            color='b',\n                            fill=True,\n                            fill_opacity=0.7,\n                            fill_color=\"green\",\n                           ).add_to(m)\nm.save('Australia.html')\nm","e9fd9419":"\nprint(\"The date ranges from {} to {}\".format(data[\"Date\"].sort_index().unique()[0],data[\"Date\"].sort_index().unique()[-1]))","e8d04e30":"dateAnalysis = data.Date.value_counts().value_counts()\ndateDict = {}\nfor i in range(1,max(dateAnalysis.index)+1):\n    if i in dateAnalysis.index:\n        dateDict[i]=dateAnalysis[i]\n    else:\n        dateDict[i]=0\ndateAnalysis=pd.DataFrame.from_dict(dateDict, orient='index',columns=[\"count\"])","03ab73b0":"f,  ax = plt.subplots(1,1,figsize=(14,4)) \nsns.barplot(dateAnalysis.index,dateAnalysis[\"count\"], ax=ax, color=\"blue\")\nax.set(ylabel='Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(),rotation=75)                  \nplt.suptitle(\"Count of locations per date\")\nplt.tight_layout(rect=[0, 0.03, 1, 0.90])","9184e66b":"import branca.colormap as cm\n\ncountRainToday = data[[\"Location\",\"RainToday\"]]\ncountRainToday=countRainToday.groupby(\"Location\")['RainToday'].apply(lambda x: (x=='Yes').sum()).reset_index(name='count')\ncountRainToday=countRainToday.set_index(\"Location\").join(locationData.set_index(\"Location\")).reset_index(\"Location\")\ncountRainToday['colour']=countRainToday['count'].apply(lambda count:\"darkblue\" if count>=1000 else\n                                         \"blue\" if count>=800 and count<1000 else\n                                         \"green\" if count>=600 and count<800 else\n                                         \"orange\" if count>=400 and count<600 else\n                                         \"tan\" if count>=200 and count<400 else\n                                         \"red\")","2da1a340":"m=folium.Map([-25.2744,133.7751],zoom_start=4,width=\"70%\",height=\"70%\",left=\"10%\")\nfor lat,lon,area,radius,colour in zip(countRainToday['Latitude'],countRainToday['Longitude'],countRainToday['Location'],countRainToday[\"count\"],countRainToday[\"colour\"]):\n     folium.CircleMarker([lat, lon],\n                            popup=area,\n                            radius=7,\n                            color='b',\n                            fill=True,\n                            fill_opacity=0.9,\n                            fill_color=colour,\n                           ).add_to(m)\nfrom branca.element import Template, MacroElement\n\ntemplate = \"\"\"\n{% macro html(this, kwargs) %}\n\n<!doctype html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  <title>jQuery UI Draggable - Default functionality<\/title>\n  <link rel=\"stylesheet\" href=\"\/\/code.jquery.com\/ui\/1.12.1\/themes\/base\/jquery-ui.css\">\n\n  <script src=\"https:\/\/code.jquery.com\/jquery-1.12.4.js\"><\/script>\n  <script src=\"https:\/\/code.jquery.com\/ui\/1.12.1\/jquery-ui.js\"><\/script>\n  \n  <script>\n  $( function() {\n    $( \"#maplegend\" ).draggable({\n                    start: function (event, ui) {\n                        $(this).css({\n                            right: \"auto\",\n                            top: \"auto\",\n                            bottom: \"auto\"\n                        });\n                    }\n                });\n});\n\n  <\/script>\n<\/head>\n<body>\n\n \n<div id='maplegend' class='maplegend' \n    style='position: absolute; z-index:9999; border:2px solid grey; background-color:rgba(255, 255, 255, 0.8);\n     border-radius:6px; padding: 10px; font-size:14px; right: 20px; bottom: 20px;'>\n     \n<div class='legend-title'>Legend<\/div>\n<div class='legend-scale'>\n  <ul class='legend-labels'>\n    <li><span style='background:darkblue;opacity:0.7;'><\/span>Over 1000 days<\/li>\n    <li><span style='background:blue;opacity:0.7;'><\/span>800-1000 days<\/li>\n    <li><span style='background:green;opacity:0.7;'><\/span>600-800 days<\/li>\n    <li><span style='background:orange;opacity:0.7;'><\/span>400-600 days<\/li>\n    <li><span style='background:tan;opacity:0.7;'><\/span>200-400 days<\/li>\n    <li><span style='background:red;opacity:0.7;'><\/span>0-200 days<\/li>\n\n  <\/ul>\n<\/div>\n<\/div>\n \n<\/body>\n<\/html>\n\n<style type='text\/css'>\n  .maplegend .legend-title {\n    text-align: left;\n    margin-bottom: 5px;\n    font-weight: bold;\n    font-size: 90%;\n    }\n  .maplegend .legend-scale ul {\n    margin: 0;\n    margin-bottom: 5px;\n    padding: 0;\n    float: left;\n    list-style: none;\n    }\n  .maplegend .legend-scale ul li {\n    font-size: 80%;\n    list-style: none;\n    margin-left: 0;\n    line-height: 18px;\n    margin-bottom: 2px;\n    }\n  .maplegend ul.legend-labels li span {\n    display: block;\n    float: left;\n    height: 16px;\n    width: 30px;\n    margin-right: 5px;\n    margin-left: 0;\n    border: 1px solid #999;\n    }\n  .maplegend .legend-source {\n    font-size: 80%;\n    color: #777;\n    clear: both;\n    }\n  .maplegend a {\n    color: #777;\n    }\n<\/style>\n{% endmacro %}\"\"\"\n\n# Code for the legend from https:\/\/nbviewer.jupyter.org\/gist\/talbertc-usgs\/18f8901fc98f109f2b71156cf3ac81cd\n\nmacro = MacroElement()\nmacro._template = Template(template)\n\nm.get_root().add_child(macro)\nm.save('RainToday.html')\n\nm","a359236b":"dateRainToday = data[[\"Date\",\"RainToday\"]]\ndateRainToday['Date'] = pd.to_datetime(dateRainToday['Date'])\ndateRainToday['Year'] = dateRainToday['Date'].dt.year\ndateRainToday['Month'] = dateRainToday['Date'].dt.month\ndateRainToday.drop(\"Date\", axis=1, inplace = True)\nyears = dateRainToday['Year'].unique().tolist()\ndateRainToday[\"Period\"] = dateRainToday['Year'].apply(str) +\"-\"+ dateRainToday['Month'].apply(str)\ndateRainToday =dateRainToday.groupby([\"Year\",\"Month\",\"Period\"])['RainToday'].apply(lambda x: (x=='Yes').sum()).reset_index(name='count')\ndateRainToday.drop([\"Month\"], axis=1, inplace = True)\nyears = sorted(years, key=lambda x: int(x))\ndateRainToday[dateRainToday[\"Year\"]==2012]","550675f1":"g = sns.FacetGrid(dateRainToday, col=\"Year\", col_wrap=4, height=4, ylim=(0, 500),margin_titles=True,sharey=True,sharex=False)\ng.map(sns.barplot, \"Period\", \"count\", ci=None,order=None);\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=75)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)","5f11fbef":"dirGust =  data[[\"WindGustDir\"]]\ndirGust =dirGust[\"WindGustDir\"].value_counts().rename_axis('direction').reset_index(name='count')\ndirGust[\"name\"]=\"WindGustDir\"\ndir9pm =  data[[\"WindDir9am\"]]\ndir9pm=dir9pm[\"WindDir9am\"].value_counts().rename_axis('direction').reset_index(name='count')\ndir9pm[\"name\"]=\"WindDir9am\"\ndir3pm =  data[[\"WindDir3pm\"]]\ndir3pm=dir3pm[\"WindDir3pm\"].value_counts().rename_axis('direction').reset_index(name='count')\ndir3pm[\"name\"]=\"WindDir3pm\"\ndirection = pd.concat([dirGust,dir9pm,dir3pm])\n#Graph the number of directions\ng = sns.FacetGrid(direction, col=\"name\", col_wrap=3, height=4, ylim=(0, direction.max()[\"count\"]*1.1),margin_titles=True,sharey=True,sharex=False)\ng.map(sns.barplot, \"direction\", \"count\", ci=None,order=[\"N\",\"NNE\",\"NE\",\"ENE\",\"E\",\"ESE\",\"SE\",\"SSE\",\"S\",\"SSW\",\"SW\",\"WSW\",\"W\",\"WNW\",\"NW\",\"NNW\"]);\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=75)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)","8ec7c067":"print(\"The category columns are {}\".format(numerical_columns))","76dfca91":"num_features = list(numerical_columns)\nnum_features_miss = data[num_features].isnull().sum()\nf,  ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8)) \nsns.barplot(num_features_miss.index,num_features_miss.values, ax=ax[0])\nsns.barplot(num_features_miss.index,num_features_miss.values\/len(data), ax=ax[1])\nax[0].set(ylabel='Number of Occurrences')\nax[0].set_xticklabels(ax[0].get_xticklabels(),rotation=75)\nax[1].set( ylabel='Percentage of Occurrences')\nax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=75)\nax[1].set_ylim(0,1)                      \nplt.suptitle(\"Missing Values for Numerical Data\")\nplt.tight_layout(rect=[0, 0.03, 1, 0.90])","79183d3b":"\nappended_data = []\nfor feature in numerical_columns:\n    name = pd.DataFrame()\n    data[\"binned\"]=pd.cut(data[feature], 10)\n    name[feature]=data[\"binned\"]\n    data.drop([\"binned\"], axis=1, inplace=True)\n    name=name[feature].value_counts().rename_axis('numerical').reset_index(name='count')\n    name[\"name\"]=feature\n    appended_data.append(name)\nnum_data = pd.concat(appended_data)\n\ng = sns.FacetGrid(num_data, col=\"name\", col_wrap=4, height=4,margin_titles=True,sharey=False,sharex=True)\ng.map(sns.barplot, \"numerical\", \"count\", ci=None,order=None);\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(labels=\"\")\n    ax.set(xlabel='', ylabel='')\nplt.subplots_adjust(hspace=0.4, wspace=0.4)","75e722b0":"data[numerical_columns].describe()","dc1a6202":"num_col_noRiskM =list(filter(lambda x: x!=\"RISK_MM\", numerical_columns))\nQ1 = data[num_col_noRiskM].quantile(0.25)\nQ3 = data[num_col_noRiskM].quantile(0.75)\nIQR = Q3 - Q1\nappended_data = []\nfor col in num_col_noRiskM:\n    aboveOutlier=data[data[col]>(Q3[col]+1.5*IQR[col])][\"RainTomorrow\"]\n    aboveOutlier=aboveOutlier.value_counts().rename_axis('Rain').reset_index(name='counts')\n    aboveOutlier[\"name\"]=col\n    aboveOutlier[\"total\"]=aboveOutlier[\"counts\"].sum() \n    aboveOutlier[\"percentageRain\"]=aboveOutlier[\"counts\"]\/aboveOutlier[\"total\"]\n    aboveOutlier[\"percentageTotal\"]=aboveOutlier[\"total\"]\/data.shape[0]\n    appended_data.append(aboveOutlier)\nResultAbove = pd.concat(appended_data)\nappended_data = []\nfor col in num_col_noRiskM:\n    belowOutlier=data[data[col]<(Q1[col]-1.5*IQR[col])][\"RainTomorrow\"]\n    belowOutlier=belowOutlier.value_counts().rename_axis('Rain').reset_index(name='counts')\n    belowOutlier[\"name\"]=col\n    belowOutlier[\"total\"]=belowOutlier[\"counts\"].sum() \n    belowOutlier[\"percentageRain\"]=belowOutlier[\"counts\"]\/belowOutlier[\"total\"]\n    belowOutlier[\"percentageTotal\"]=belowOutlier[\"total\"]\/data.shape[0]\n    appended_data.append(belowOutlier)\nResultBelow = pd.concat(appended_data)","fdb2b942":"g = sns.FacetGrid(ResultAbove, col=\"name\", col_wrap=4, height=4,margin_titles=True,sharey=False,sharex=False)\ng.map(sns.barplot, \"Rain\", \"percentageRain\", ci=None,order=[\"Yes\",\"No\"]);\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(labels=\"\")\n    ax.set(xlabel='', ylabel='')\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\ng.fig.suptitle('Percentage of RainTomorrow for the Higher IQR ')\ng.fig.subplots_adjust(top=0.9)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)","1bfbe4bf":"dataUpper = data[data[numerical_columns]>Q3+1.5*IQR].describe()\ndataUpper[[\"Rainfall\", \"Evaporation\", \"WindSpeed9am\", \"WindSpeed3pm\"]]\n#Description of the upper quartile of these Features","21383386":"ResultBelow\ng = sns.FacetGrid(ResultBelow, col=\"name\", col_wrap=4, height=4,margin_titles=True,sharey=False,sharex=False)\ng.map(sns.barplot, \"Rain\", \"percentageRain\", ci=None,order=[\"Yes\",\"No\"]);\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(labels=\"\")\n    ax.set(xlabel='', ylabel='')\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\ng.fig.suptitle('Percentage of RainTomorrow for the Lower IQR ')\ng.fig.subplots_adjust(top=0.9)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)","bf9ba2c8":"Q1 = data[numerical_columns].quantile(0.25)\nQ3 = data[numerical_columns].quantile(0.75)\nIQR = Q3 - Q1","67a092ee":"correlation = data.corr()\nplt.figure(figsize=(16,12))\nplt.suptitle('Correlation Heatmap of Rain in Australia Dataset', size=16, y=0.93);     \nax = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white',linewidths=.5, center=0)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)      \nplt.show()","1d2446d3":"corrTable = data.corr().unstack().sort_values(ascending = False)\ncorrTable=corrTable.rename_axis(['Feature 1',\"Feature 2\"]).reset_index(name='Correlation')\ncorrTable=corrTable[corrTable[\"Feature 1\"]!=corrTable[\"Feature 2\"]]\nTopCorr=corrTable[(corrTable[\"Correlation\"]>0.7)|(corrTable[\"Correlation\"]<-0.7)]\nTopCorr.drop_duplicates(subset='Correlation')","fccdd001":"corrTable[corrTable[\"Feature 1\"]==\"RISK_MM\"]","a70b8111":"f = plt.figure(figsize=(18,15))\ngs = f.add_gridspec(4,2, hspace=0.2, wspace=0.2)\n\nax1 = f.add_subplot(gs[0, 0])\nax2 = f.add_subplot(gs[0, 1])\nax11 = f.add_subplot(gs[1, 0])\nax22 = f.add_subplot(gs[1, 1])\nax3 = f.add_subplot(gs[2, 0])\nax4 = f.add_subplot(gs[2, 1])\nax5 =  f.add_subplot(gs[3, :])\n\nhumidity = data[data[\"Humidity3pm\"]>(Q3[\"Humidity3pm\"]+0.5*IQR[\"Humidity3pm\"])]\nhumidity=humidity[\"RainTomorrow\"].value_counts().rename_axis('Rain').reset_index(name='counts')\nhumidity[\"total\"]=humidity[\"counts\"].sum()\nhumidity[\"percentage\"]=humidity[\"counts\"]\/humidity[\"total\"]\nsns.barplot(humidity.Rain,humidity.percentage,ax=ax1,order=[\"Yes\",\"No\"])\nax1.set_title('Rainfall Tomorrow when Humidity 3pm > Q3 + 0.5 IQR = {} - Data size - {}'\n              .format(Q3[\"Humidity3pm\"]+0.5*IQR[\"Humidity3pm\"],humidity[\"counts\"].sum()), size=12, y=1.05)\n\nhumidity = data[data[\"Humidity9am\"]>(Q3[\"Humidity9am\"]+0.5*IQR[\"Humidity9am\"])]\nhumidity=humidity[\"RainTomorrow\"].value_counts().rename_axis('Rain').reset_index(name='counts')\nhumidity[\"total\"]=humidity[\"counts\"].sum()\nhumidity[\"percentage\"]=humidity[\"counts\"]\/humidity[\"total\"]\nsns.barplot(humidity.Rain,humidity.percentage,ax=ax2,order=[\"Yes\",\"No\"])\nax2.set_title('Rainfall Tomorrow when Humidity 9am > Q3 + 0.5 IQR = {} - Data size - {}'\n              .format(Q3[\"Humidity9am\"]+0.5*IQR[\"Humidity9am\"],humidity[\"counts\"].sum()), size=12, y=1.05)\n\nhumidity = data[data[\"Humidity3pm\"]==100]\nhumidity=humidity[\"RainTomorrow\"].value_counts().rename_axis('Rain').reset_index(name='counts')\nhumidity[\"total\"]=humidity[\"counts\"].sum()\nhumidity[\"percentage\"]=humidity[\"counts\"]\/humidity[\"total\"]\nsns.barplot(humidity.Rain,humidity.percentage,ax=ax11,order=[\"Yes\",\"No\"])\nax11.set_title('Rainfall Tomorrow when Humidity 3pm = {} - Data size - {}'\n              .format(100,humidity[\"counts\"].sum()), size=12, y=1.05)\n\nhumidity = data[data[\"Humidity9am\"]==100]\nhumidity=humidity[\"RainTomorrow\"].value_counts().rename_axis('Rain').reset_index(name='counts')\nhumidity[\"total\"]=humidity[\"counts\"].sum()\nhumidity[\"percentage\"]=humidity[\"counts\"]\/humidity[\"total\"]\nsns.barplot(humidity.Rain,humidity.percentage,ax=ax22,order=[\"Yes\",\"No\"])\nax22.set_title('Rainfall Tomorrow when Humidity 9am = {} - Data size - {}'\n              .format(100,humidity[\"counts\"].sum()), size=12, y=1.05)\n\nhumidity = data[data[\"Humidity9am\"]<data[\"Humidity3pm\"]]\nhumidity=humidity[\"RainTomorrow\"].value_counts().rename_axis('Rain').reset_index(name='counts')\nhumidity[\"total\"]=humidity[\"counts\"].sum()\nhumidity[\"percentage\"]=humidity[\"counts\"]\/humidity[\"total\"]\nsns.barplot(humidity.Rain,humidity.percentage,ax=ax3,order=[\"Yes\",\"No\"])\nax3.set_title('Rainfall Tomorrow when Humidity increases during the day - Data size - {}'.format(humidity[\"counts\"].sum()), size=12, y=1.05)\n\nhumidity = data[data[\"Humidity9am\"]>data[\"Humidity3pm\"]]\nhumidity=humidity[\"RainTomorrow\"].value_counts().rename_axis('Rain').reset_index(name='counts')\nhumidity[\"total\"]=humidity[\"counts\"].sum()\nhumidity[\"percentage\"]=humidity[\"counts\"]\/humidity[\"total\"]\nsns.barplot(humidity.Rain,humidity.percentage,ax=ax4,order=[\"Yes\",\"No\"])\nax4.set_title('Rainfall Tomorrow when Humidity decreases during the day - Data size - {}'.format(humidity[\"counts\"].sum()), size=12, y=1.05)\n\nhumidity = data[data[\"Humidity3pm\"]>(Q3[\"Humidity3pm\"]+0.5*IQR[\"Humidity3pm\"])]\nhumidity = humidity[humidity[\"Humidity9am\"]<humidity[\"Humidity3pm\"]]\nhumidity=humidity[\"RainTomorrow\"].value_counts().rename_axis('Rain').reset_index(name='counts')\nhumidity[\"total\"]=humidity[\"counts\"].sum()\nhumidity[\"percentage\"]=humidity[\"counts\"]\/humidity[\"total\"]\nsns.barplot(humidity.Rain,humidity.percentage,ax=ax5,order=[\"Yes\",\"No\"])\nax5.set_title('Rainfall Tomorrow when Humidity 3pm > Q3 + 0.5 IQR ={} and Humidity increases during the day- Data size - {}'\n              .format(Q3[\"Humidity3pm\"]+0.5*IQR[\"Humidity3pm\"],humidity[\"counts\"].sum()), size=12, y=1.05);     \n\n\nax1.set(xlabel='', ylabel='Percentage of Occurrences')\nax2.set(xlabel='', ylabel='Percentage of Occurrences')\nax3.set(xlabel='', ylabel='Percentage of Occurrences')\nax4.set(xlabel='', ylabel='Percentage of Occurrences')\nax5.set(xlabel='', ylabel='Percentage of Occurrences')\n\ngs.update( wspace=0.2, hspace=0.4)\nf.tight_layout(pad=4.0)","4bf4e561":"data.drop(['RISK_MM'], axis=1, inplace=True)\nnumerical_columns.remove(\"RISK_MM\")","fe7f90af":"data = data[~(data[\"Evaporation\"] > (Q3[\"Evaporation\"] + 1.5 * IQR[\"Evaporation\"]))]\ndata = data[~(data[\"WindSpeed9am\"] > (Q3[\"WindSpeed9am\"] + 1.5 * IQR[\"WindSpeed9am\"]))]\ndata = data[~(data[\"WindSpeed3pm\"] > (Q3[\"WindSpeed3pm\"] + 1.5 * IQR[\"WindSpeed3pm\"]))]","2cf6c620":"data.describe()","394b1308":"appended_data = []\nfor feature in numerical_columns:\n    name = pd.DataFrame()\n    data[\"binned\"]=pd.cut(data[feature], 10)\n    name[feature]=data[\"binned\"]\n    data.drop([\"binned\"], axis=1, inplace=True)\n    name=name[feature].value_counts().rename_axis('numerical').reset_index(name='count')\n    name[\"name\"]=feature\n    appended_data.append(name)\nnum_data = pd.concat(appended_data)\ng = sns.FacetGrid(num_data, col=\"name\", col_wrap=4, height=4,margin_titles=True,sharey=False,sharex=True)\ng.map(sns.barplot, \"numerical\", \"count\", ci=None,order=None);\nfor ax in g.axes.ravel():\n    ax.set_xticklabels(labels=\"\")\n    ax.set(xlabel='', ylabel='')\nplt.subplots_adjust(hspace=0.4, wspace=0.4)","8720a42c":"def missingValues (data):\n    numerical_columns = [col for col in data.columns if data[col].dtype!=\"object\" ]\n    num_features = list(numerical_columns)\n    num_features_miss = data[num_features].isnull().sum()\n    f,  ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8)) \n    sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis', ax=ax[0])\n    sns.barplot(data.isnull().sum().index,data.isnull().sum().values\/len(data), ax=ax[1])\n    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=70)\n    ax[1].set( ylabel='Percentage of Occurrences')\n    ax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=75)\n    ax[1].set_ylim(0,1)             \n    plt.suptitle('Missing Values in the Data', size=16, y=0.93)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.90])","3b6a6ef7":"missingValues(data)","89dbf4ce":"\ndef position (length,nLargest):\n    numFeatures = int(length\/nLargest)\n    pos = np.arange(1,length\/numFeatures+1) *0.4\n    res = np.empty(shape=[0,0])\n    for i in range(0,int(length\/nLargest)*2,2):\n        res=np.append(res,pos+i)\n    return res\ndef corrT (arr):\n    corrTable = data.corr().unstack().sort_values(ascending = False)\n    corrTable=corrTable.rename_axis(['Feature 1',\"Feature 2\"]).reset_index(name='Correlation')\n    corrTable[\"CorrelationAbs\"]=abs(corrTable[\"Correlation\"])\n    corrTable=corrTable[corrTable[\"Feature 1\"]!=corrTable[\"Feature 2\"]]\n    corrTable=corrTable[corrTable[\"Feature 1\"].isin(arr)]\n    corrTable=corrTable[~corrTable[\"Feature 2\"].isin(arr)]\n    corrTable=corrTable.loc[corrTable.groupby('Feature 1')['CorrelationAbs'].nlargest(3).index.get_level_values(1)]\n    length = len(corrTable)\n    pos = position(length,3)\n    fig, ax=plt.subplots(figsize=(16,5))\n    uelec, uind = np.unique(corrTable[\"Feature 2\"], return_inverse=1)\n    cmap = plt.cm.get_cmap(\"Set1\")\n    ax.bar(pos, corrTable[\"CorrelationAbs\"], width=0.4, align=\"edge\", ec=\"k\", color=cmap(uind)  )\n    handles=[plt.Rectangle((0,0),1,1, color=cmap(i), ec=\"k\") for i in range(len(uelec))]\n    ax.legend(handles=handles, labels=list(uelec),\n               prop ={'size':10}, loc=9, ncol=8, \n                title=r'Feature 2')\n    ax.set_xticks([x*2+1 for x in range(length\/\/3)])\n    ax.set_xticklabels(corrTable[\"Feature 1\"].unique())\n    ax.set_ylim(0, 1)\n    plt.show()","d16a3102":"arr = ['Evaporation',\"Sunshine\",\"Cloud9am\",\"Cloud3pm\"]\ncorrT(arr)","bf79fad3":"class medianBins():\n    def __init__(self,data, missingFeature, correlatedFeature,binSize):\n        self.data = data\n        self.missingFeature = missingFeature\n        self.correlatedFeature = correlatedFeature\n        self.binSize = binSize\n        print (self)\n        \n    def binD (self):\n        binData = pd.DataFrame()\n        binData[self.correlatedFeature]= self.data[self.correlatedFeature]\n        binData[self.missingFeature]= data[self.missingFeature]\n        binData[\"HCorr\"]= pd.cut(binData[self.correlatedFeature], self.binSize)\n        binData[\"HMissing\"]= pd.cut(binData[self.missingFeature], self.binSize)\n        binData=binData.dropna(subset=[\"HCorr\",\"HMissing\"])\n        binData = binData.groupby([\"HCorr\",\"HMissing\"])[self.missingFeature].count().rename_axis([\"HCorr\",\"HMissing\"]).reset_index(name='Count')\n        binData[\"cummulative\"]=binData.groupby([\"HCorr\"])[\"Count\"].apply(lambda x: x.cumsum())\n        binData[\"bin_centres\"] = binData[\"HMissing\"].apply(lambda x: x.mid)\n        return binData\n    \n    def median(self):\n        binData= self.binD()\n        median=binData.groupby([\"HCorr\"])[\"Count\"].agg(\"sum\")\/2\n        median=median.rename_axis([\"HCorr\"]).reset_index(name='median')\n        binData=pd.merge(binData,median,on=\"HCorr\")\n        median=binData[binData[\"cummulative\"]>binData[\"median\"]]\n        median =  median.loc[median.groupby([\"HCorr\"])['cummulative'].idxmin()]\n        median=median.drop(columns=[\"Count\",\"cummulative\",\"median\"])\n        median[\"bin_centres\"] = median[\"HMissing\"].apply(lambda x: x.mid)\n        return median\n    \n    def graph(self):\n        binData = self.binD()\n        pivot = binData.pivot(index=\"HCorr\",columns=\"HMissing\",values=\"Count\")\n        ax = pivot.plot(kind='bar', stacked=True, figsize=(18.5, 7))\n        ax.set( ylabel='Count' , xLabel=self.correlatedFeature)\n        ax.legend(title=self.missingFeature)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n        plt.title(\"Cumulative Count of the Combination {} and {} with median lines\".format(self.missingFeature,self.correlatedFeature))\n        return ax\n\nimpCloud3pm=medianBins(data,\"Cloud3pm\",\"Humidity3pm\",10)\nimpCloud9am=medianBins(data,\"Cloud9am\",\"Humidity3pm\",10)\nimpEvaporation=medianBins(data,\"Evaporation\",\"MaxTemp\",10)\nimpSunshine=medianBins(data,\"Sunshine\",\"Humidity3pm\",10)","b6df3906":"impCloud3pm.graph()","e8884971":"impCloud9am.graph()","b6cdf063":"impEvaporation.graph()","6a3bd8e6":"impSunshine.graph()","05d247b8":"def imputation(cols,binD):\n    missingFeature = cols[0]\n    corrFeature = cols[1]\n    if pd.isnull(missingFeature)&pd.notna(corrFeature):\n        temp=binD\n        temp=temp[temp[\"HCorr\"].apply(lambda x:x.left<corrFeature<=x.right)]\n        temp[\"median\"]=temp[\"Count\"].sum()\/2\n        temp=temp[temp[\"cummulative\"]>temp[\"median\"]]\n        #print(temp[\"bin_centres\"].iloc[0]  ,\"fixed\",corrFeature)        \n        try:    \n            result=temp.iloc[0, temp.columns.get_loc('bin_centres')]\n        except:return float('nan')\n        return result\n        #print(corrFeature,temp)\n    elif pd.isnull(missingFeature)&pd.isnull(corrFeature):\n        #print(\"nan\")\n        return float('nan')\n    else:\n        #print(missingFeature,\"original\")\n        return missingFeature","f8a0a95a":"\n\n\n\"\"\"\ndata[\"Evaporation\"]=data[['Evaporation','MaxTemp']].apply(imputation,binD=impEvaporation.binD(),axis=1)\ndata[\"Cloud3pm\"]=data[['Cloud3pm','Humidity3pm']].apply(imputation,binD=impCloud3pm.binD(),axis=1)\ndata[\"Cloud9am\"]=data[['Cloud9am','Humidity3pm']].apply(imputation,binD=impCloud9am.binD(),axis=1)\ndata[\"Sunshine\"]=data[['Sunshine','Humidity3pm']].apply(imputation,binD=impSunshine.binD(),axis=1)\ndata.to_csv('dataImputation.csv',index=False)\nprint(\"done\")\n\"\"\"\n\n#saved to datacleanv1","a4f91b84":"data1 = pd.read_csv(r\"..\/input\/datacleanv1\/dataImputation.csv\")","50a62d6c":"missingValues(data1)","2a085fc4":"corrT([\"WindGustSpeed\", \"Pressure9am\", \"Pressure3pm\"])","c6d815ea":"impWindGustSpeed=medianBins(data1,\"WindGustSpeed\",\"WindSpeed3pm\",10)\nimpPressure9am=medianBins(data1,\"Pressure9am\",\"MinTemp\",10)\nimpPressure3pm=medianBins(data1,\"Pressure3pm\",\"Temp9am\",10)","e92b7f41":"\n\"\"\"\ndata1[\"WindGustSpeed\"]=data1[['WindGustSpeed','WindSpeed3pm']].apply(imputation,binD=impWindGustSpeed.binD(),axis=1)\ndata1[\"Pressure9am\"]=data1[['Pressure9am','MinTemp']].apply(imputation,binD=impPressure9am.binD(),axis=1)\ndata1[\"Pressure3pm\"]=data1[['Pressure3pm','Temp9am']].apply(imputation,binD=impPressure3pm.binD(),axis=1)\ndata1.to_csv('dataImputation.csv',index=False)\nprint(\"done\")\n\"\"\"\n\n#saved to datacleanv2","d8a1c29c":"data2 = pd.read_csv(r\"..\/input\/datacleanv2\/dataImputation.csv\")","b602e40d":"missingValues(data2)","337696e4":"for col in numerical_columns:\n    data2[col].fillna(data2[col].median(), inplace=True)","830a95a3":"\nfor col in category_columns:\n    data2[col].fillna(data2[col].mode()[0], inplace=True)","0026707f":"missingValues(data2)","b312aa2b":"X = data2.drop(['RainTomorrow'], axis=1)\ny = data2['RainTomorrow']\ny=y.map(dict(Yes=1, No=0))","755c5084":"balanceTarget(data2[\"RainTomorrow\"])","4b19f4ac":"data2[\"RainTomorrow\"].value_counts()","9c0b35e5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0,stratify = y)","b5fa6e1a":"import category_encoders as ce\nencoder = ce.BinaryEncoder(cols=['RainToday'])\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.transform(X_test)","b0d29956":"X_train.columns","8d8f0379":"X_train['Date']= pd.to_datetime(X_train['Date']) \nX_train['Year'] = X_train['Date'].dt.year\nX_train['Month'] = X_train['Date'].dt.month\nX_train['Day'] = X_train['Date'].dt.day\n\nX_test['Date']= pd.to_datetime(X_test['Date']) \nX_test['Year'] = X_test['Date'].dt.year\nX_test['Month'] = X_test['Date'].dt.month\nX_test['Day'] = X_test['Date'].dt.day\n\n#Dropping the date column\nX_train.drop('Date', axis=1, inplace = True)\nX_test.drop('Date', axis=1, inplace = True)","3980394b":"X_train.columns","fde4d716":"X_train = pd.get_dummies(X_train, columns=['Location','WindGustDir','WindDir9am','WindDir3pm'])\n\nX_test = pd.get_dummies(X_test, columns=['Location','WindGustDir','WindDir9am','WindDir3pm'])","e2b13527":"X_train.columns","4dac04a0":"X_train.head()","a705e5f1":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())","61986950":"allColumns = X_train.columns\n\nfrom sklearn import preprocessing\nscaler = preprocessing.RobustScaler()\n\nX_train_res = scaler.fit_transform(X_train_res)\nX_train_res  = pd.DataFrame(X_train_res, columns=[allColumns])\n\nX_train = scaler.fit_transform(X_train)\nX_train  = pd.DataFrame(X_train, columns=[allColumns])\n\nX_test = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=[allColumns])","f49847b5":"X_train_res = pd.read_csv(r\"..\/input\/traintest\/X_train_res.csv\", index_col=0)\nX_train = pd.read_csv(r\"..\/input\/traintest\/X_train.csv\", index_col=0)\nX_test = pd.read_csv(r\"..\/input\/traintest\/X_test.csv\", index_col=0)\ny_train_res = pd.read_csv(r\"..\/input\/traintest\/y_train_res.csv\")\ny_train = pd.read_csv(r\"..\/input\/traintest\/y_train.csv\", index_col=0)\ny_test = pd.read_csv(r\"..\/input\/traintest\/y_test.csv\", index_col=0)\n\ny_train_res = np.array(y_train_res[\"0\"])\n\n\n","1e87e9e1":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, fbeta_score\n\nf2_score = make_scorer(fbeta_score, beta=2, pos_label=1)","b3a2de4d":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nfrom matplotlib import pyplot\nfrom sklearn.metrics import roc_curve,roc_auc_score\n\ndef results(y_test, y_pred_test,y_pred_proba,model,ROC):\n\n    print(\"Classifcation Report\")\n    print(\"\\n\")\n    print(\"1- Rain Tomorrow, 0 - No Rain Tomorrow\")\n    print(\"\\n\")\n    print(classification_report(y_test, y_pred_test))\n    \n    \n    cm = confusion_matrix(y_test, y_pred_test)\n    cm_df = pd.DataFrame(data=cm, columns=['Actual No Rain', 'Actual Rain'], \n                                     index=['Predict No Rain', 'Predict Rain'])\n    plt.figure(figsize=(6,6))\n    plt.suptitle('Confusion matrix', size=16, y=1.0);     \n    ax=sns.heatmap(cm_df, square=True, annot=True, fmt='d', cbar=True)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)      \n    plt.show()\n    \n    if ROC==True:\n        auc = roc_auc_score(y_test, y_pred_proba)\n        print(\"{} : ROC AUC = {}%\".format(model,round(auc, 3)))\n        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n        pyplot.plot(fpr, tpr, marker='.', label=model)\n        pyplot.plot([0,1], [0,1], 'k--' )\n\n        pyplot.xlabel('False Positive Rate')\n        pyplot.ylabel('True Positive Rate')\n        pyplot.legend()\n        pyplot.show()\n    print(\"\\n\")\n    print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))","1e33ecdd":"\"\"\"\n# Ran the modelling and then saved it to a pickle\n\nimport pickle\nfrom sklearn.linear_model import LogisticRegression\nparameters = {'solver':['liblinear'], 'C':[100,10000],\"penalty\":[\"l1\",\"l2\"],\n              \"class_weight\":[None,\"balanced\"]}\ngsc=GridSearchCV(estimator=LogisticRegression(),\n             param_grid=parameters,cv=5, scoring=f2_score, verbose=0, n_jobs=-1)\ngr_log_bal = gsc.fit(X_train_res, y_train_res)\n\nfilename = 'logReg.sav'\npickle.dump(gr_log_bal, open(filename, 'wb'))\n# Save to pickle\n\"\"\"","f38116bc":"import pickle\nlogReg = pickle.load(open(\"..\/input\/results\/logReg.sav\", 'rb'))\nprint(logReg.param_grid)\nprint(\"\\n\")\ny_pred_test = logReg.predict(X_test)\ny_pred1 = logReg.predict_proba(X_test)[:, 1]\nresults(y_test, y_pred_test, y_pred1,\"Logistic Regression Balance\",True)\n","ba06904a":"\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\n\nparameters = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto'],\n    'max_depth' : [4,8],\n    'criterion' :['gini', 'entropy'],\n    \"class_weight\":[None,\"balanced\"],\n    \"oob_score\":[True]\n}\ngsc=GridSearchCV(estimator=RandomForestClassifier(),\n             param_grid=parameters,cv=5, scoring=f2_score, verbose=0, n_jobs=-1)\n\ngrid_result_Rand_bal = gsc.fit(X_train_res, y_train_res)\n\nfilename = 'RandForest.sav'\npickle.dump(grid_result_Rand_bal, open(filename, 'wb'))\n# Save to pickle\n\"\"\"","95ab5a40":"import pickle\nrandForest = pickle.load(open(\"..\/input\/results\/RandForest.sav\", 'rb'))\nprint(randForest.param_grid)\nprint(\"\\n\")\ny_pred_test = randForest.predict(X_test)\ny_pred1 = randForest.predict_proba(X_test)[:, 1]\nresults(y_test, y_pred_test, y_pred1,\"Logistic Regression Balance\",True)","77ad9a8d":"y.value_counts()\/y.value_counts().sum()*100","432d6205":"<a id=\"5\"><\/a>\n## Conclusion","971d6e10":"When the maxTemp is below 5.62, we do not have any evaporation data. As a result, the imputation algorithm will return a nan for evaporation when the max temp is below 5.62.","689e065e":"<a id=\"4.1\"><\/a>\n### Performance Metrics","5e95967e":"<a id=\"2.21\"><\/a>\n#### Numerical Features","229211ba":"Since we do have outliers, we will use te RobustScaler","bf75ae80":"<a id=\"1.1\"><\/a>\n### Hypoyhesis","f4eb4200":"Features such as Rainfall, Evaporation, WindGustSpeed, WindSpeed9am, WindSpeed3pm and RISK_MM have extremely high maximum levels proportionate to their 75% quartile (463,19.59, 2.81,6.84,3.65 and 463 respectively). We shall see if these upper quartiles lead to a higher chance of rain fall tommorow. If they do, we should keep them but if they don't, we will remove them as it will help with the machine learning model.","5efa1170":" <a id=\"1.4132\"><\/a>\n###### Correlation","a9cc71d0":"<a id=\"4.21\"><\/a>\n#### Logistic Regression","8055283c":"We can see that Evaporation, Sunshine, Cloud9am and Cloud3pm are missing over 40% of their data. \n# \n#","8eafd1cd":"<a id=\"3.3\"><\/a>\n### Split the data to training and testing sets","72e8c60e":"* After binning each numerical feature by 10, we can see that all the features are skewed and thus we shouldn't use the z-score to find the outliers but use the IQR score","b3e2bc64":"\nAs stated in the data description, we need to remove RISKMM column. According to the column description, RISKMM is 'the amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the \"risk\"' As a result it contains information about our target (RainTomorrow) and thus will be bias.","7f5b6efb":"<a id=\"4.22\"><\/a>\n#### Random Forest Classifer","6e0001bf":"f2_score is a customer score that focuses on the F1 score for label 1 which is rain tomorrow","10bc8ada":"We will perform a grid search on both the logisctic regession and Random Forest. I will run both models and save it to a pickle which saves computation time when commiting.","d91cb40c":"<a id=\"1.4121\"><\/a>\n###### Location","df0f9cfc":"<a id=\"1.412\"><\/a>\n##### Categorical Features","006ea090":"<a id=\"1.2\"><\/a>\n### Import dependencies\n","858b27cd":"Australia has recently seen horrific droughts and bush fires. This has lead to the <font color=\"black\" size=\"4px\"><b>destruction<\/b><\/font> of the habitats of Australian wildlife, loss of livestock and local communities are in ruin. \n \n The World Meteorological Organisation says weather forecasting is a <font color=\"black\" size=\"4px\"><b> vital element<\/b><\/font> needed \u201cin order to meet the food, fodder, fibre and renewable agri-energy needs of rapidly growing populations\u201d.\n \n In the US, improved climate forecasting in the corn belt is expected to bring in an extra \\$1.2 billion to \\$2.9 billion over a ten-year period. The World Bank Group estimates that improved global weather forecasting would result in increases in productivity worth \\$30 billion per year, as well as reducing asset losses by \\$2 billion per year.\n\n https:\/\/www.vaisala.com\/en\/blog\/2019-07\/day-day-benefits-weather-detection\n \n If we can help predict rain, it may help farmers to better harvest their stock such that they could take advantage of the rain.","6b1e7342":"As stated above, there is a big imbalance in the target which can causes inaccurate and decreased predictive performance of many classification algorithms. Most classification algorithms, such as logistic regression, Naive Bayes and decision trees, output a probability for an instance belonging to the positive class: Pr(y=1|x).Thus, if we have over 70% (106512\/(106512+30223))=0.77 of the data in one class, it can cause problems. \n\nWe will stratify our data training and validation. Stratification is the technique to allocate the samples evenly based on sample classes so that training set and validation set have similar ratio of classes. It is essential to ensure your training and validation sets share approximately the same ratio of examples from each class, so that you can achieve consistent predictive performance scores in both sets. \n\nWe will also resample to get more balanced data. There are 3 techniques, \n*     Down-sampling (Under sampling) the majority class\n*     Up-sampling (Over sampling) the minority class\n*     Advanced sampling techniques, such as Synthetic Minority Over-sampling Technique (SMOTE)\n\nDown-sampling will mean we lose data while up-sampling may increase the likelihood of overfitting since it replicates the minority class events.\nSMOTE is an oversampling method which creates \u201csynthetic\u201d example rather than oversampling by replacements. The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any\/all of the k minority class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen. -https:\/\/www.datacamp.com\/community\/tutorials\/diving-deep-imbalanced-data\n ","85f12d4b":"<a id=\"2.1\"><\/a>\n### Treatment of Outliers\n\nI will keep the outliers because of the relationship of the outliers in humidity3pm and raintomorrow and if we removed the outliers, we would remove over 32% of the data. \nWe will however remove outliers from Evaporation, WindSpeed9am and WindSpeed3pm since the maximum values are significantly greater than then 75% quartile and the upper quartile (Q3+ 1.5 IQR) does not lead to higher chance of rain the next day. We will not drop the lower quartlie because the distribution of the numerical features are positively skewed\n","6be1b5e6":"After re runing the imputation algorithm on the WindGustSpeed, Pressure9am and Pressure3pm, we have dropped missing values of numerical features to under 5%. We could re run the algorithm  until there are no missing values but it takes computaiton space. \n\nWe will simply replace the remaining missing values with the median of their Feature","0207a893":"We can see that Evaporation, WindSpeed9am and WindSpeed3pm upper values (Q3+1.5 IQR) are relatively small compared to Rainfall. We can't remove (25228\/142193) 17% of our data. These features will be treated later in the report in Data cleaning","1727ab18":"Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.\n\nA one hot encoding is a representation of categorical variables as binary vectors which allows the representation of categorical data to be more expressive.\nLabel Encoder converts each string value to a whole number. \n\nThe problem here is since there are different numbers in the same column, the model will misunderstand the data to be in some kind of order, 0 < 1 <2.\n\nThe model may derive a correlation like as the country number increases the population increases but this clearly may not be the scenario in some other data or the prediction set. To overcome this problem, we use One Hot Encoder.\n\nhttps:\/\/towardsdatascience.com\/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b\n\nThe one hot encoding needs to be done to the training and testing sets","1d4bde3e":"Our focus on finding a suitable model will be idenifying which model has the highest F1 for Rain Tomorrow. We want to know a combination of if the model predicts Rain, what are the chance of it really raining tomorrow and If it does rain, what are the chances of the model predicting rain. \n\nWe will select a model that has the highest F1 for Rain Tomorrow","64c74955":"<a id=\"1.4124\"><\/a>\n###### WindGustDir, WindDir9am and WindDir3pm","6ff91fc4":"The missing values are less than 10% of the total data points for WindGustDir, WindDir9am, WindDir3pm and RainToday.","2860020a":"**Min-Max Scaler**\n\n MinMaxScaler is the probably the most famous scaling algorithm, and follows the following formula for each feature:\n\n![](https:\/\/miro.medium.com\/max\/521\/0*K2QwZ16bEAxA4hUe.jpg)\n\nIt essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values).\nThis scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.\n#However, it is sensitive to outliers, so if there are outliers in the data, you might want to consider the Robust Scaler.\n\n**RobustScaler**\n\nThe RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rather than the min-max, so that it is robust to outliers. Therefore it follows the formula:\nxi\u2013Q1(x)\/Q3(x)\u2013Q1(x)\nFor each feature.\nOf course, this means it is using the less of the data for scaling so it\u2019s more suitable for when there are outliers in the data.\n\nhttps:\/\/medium.com\/analytics-vidhya\/feature-scaling-in-scikit-learn-b11209d949e7","01088e49":"<a id=\"3.5\"><\/a>\n### Over sampling","c1f6dd60":"<a id=\"1.41\"><\/a>\n#### Cateogory Data","685fac0a":"We will find the features that correlate to the 4 features that have the most missing data. We have picked the top 3 correlated features for each of the 4 features. For each of these features that have missing data, we will bin (split it into category) the feature and its most correlated feature in order to determine which the median value of the feature with missing data given the value of the correlated feature. \nFor example, Cloud3pm (mising data feature) is most correlated with Humidity3pm. If Humidity3pm is in the range of 80 to 100, we would want to know what the median bin range of Cloud3pm so we could replace any missing Cloud3pm data that has a Humidity in the range of 80 to 100.\nWe will apply this method for 4 features.","ba942703":"<a id=\"2.22\"><\/a>\n#### Categorical Features","f630b869":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcRwF1wKjwoPgBqBjgn3r0E8MQj85mA6BfDm8SlJWQK9Lszo_MfF)","427fb029":"<a id=\"1.411\"><\/a>\n##### Target Column: RainTomorrow","c90ba8e3":"<a id=\"4\"><\/a>\n## Training the Model","b55d426f":"An interesting note is that there is very low correlation between RISK_MM which is the rain tommorow and the other numnerical features. The highest absolute correlation are Humdidy, Rainfall (rainfall today) and Sunshine but its at 30% absolute correlation. So in answering our hypothesis initally stated (Evaporation plays a major role in the rain fall) we find that it is false because Humidty at 3pm has the highest correlation at 30% than evaporation which is at -4%. \n\nHumidity is the concentration of water vapour present in the air and indicates the likelihood for precipitation, dew, or fog to be present. A high humidity is usually in the morning due to the change from overnight temperatures to day light temperatures. However, a high humidity in the afternoon could lead to rain fall the next day.","d072ae56":"<a id=\"3.1\"><\/a>\n### Target and Features","6b6aae69":" <a id=\"1.413\"><\/a>\n##### Numerical Features","39a1288c":"This kernel has been inspired by several kernels. Notably https:\/\/www.kaggle.com\/prashant111\/extensive-analysis-eda-fe-modelling by Prashant11.\nIf you have any questions please comment and if you like it please   <font color='red' size=\"4px\" ><b>upvote<b><\/font>","e194bd2b":"<a id=\"1.4122\"><\/a>\n###### Dates","74031f10":"<a id=\"3.2\"><\/a>\n### Imbalance of the Target","35c68f86":"<a id=\"3.4\"><\/a>\n### Encode cateogorical features","38fa09cb":" <a id=\"1.4133\"><\/a>\n###### Exploration of Humidity\n\nLets look at the higher levels of humidity to see if we can find a stronger relationship with rainfall tomorrow.","b409aa46":"In addition to address the imbalance of class, we will not focus on accuracy as a measure of performance because if our majority class was 85%, then a model that always predicted in favour of the majority class will have an accuracy of 85%. This is know as the Accuracy Paradox. We need to look at the True Positive, True Negative, False Positive and False Negative of the prediction. \n\n *     True Positive (TP) \u2013 An instance that is positive and is classified correctly as positive\n *     True Negative (TN) \u2013 An instance that is negative and is classified correctly as negative\n *     False Positive (FP) \u2013 An instance that is negative but is classified wrongly as positive\n *     False Negative (FN) \u2013 An instance that is positive but is classified incorrectly as negative\n \nA confusiong matrix represents these concepts visually\n \nA classification report introduces Precision and Recall\n \nPrecision can be thought of as a measure of a classifier's exactness. A low precision can also indicate a large number of False Positives.\nRecall can be thought of as a measure of a classifier's completeness. A low recall indicates many False Negatives.\n \nFinally we will also look at the AUC ROC (Area under the curve of the ROC curve (receiver operating characteristic curve) )\n \nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n \nhttps:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5","35ce7edd":"According to NASA, \n\"Precipitation, evaporation, freezing and melting and condensation are all part of the hydrological cycle - a never-ending global process of water circulation from clouds to land, to the ocean, and back to the clouds. \"\nMy initial hypothesis in the elementary data analysis is to look at the if evaporation is the main factor in the next day's rainfall. \n","d721c53e":"As a result, we have no missing data","3ad85aed":"<a id=\"4.2\"><\/a>\n### Grid Search","5b8af5d3":"<a id=\"2.2\"><\/a>\n### Treatment of Missing Data\n \nWe will assume MCAR (Missing Completely at Random) which means that the missing value has has nothing to do with its hypothetical value and with the values of other variables.\n","5464fc41":"<a id=\"3.41\"><\/a>\n#### Binary Categorcal Features\n\nRainToday is binary since it has results of Yes and No.","55092dcd":"We can see that the majority of the dates have over 42 locations.","5fbea812":"# EDA, Data Cleaning, Data Preprocessing and Modelling","cffa4936":"<a id=\"1.4\"><\/a>\n### Category and Numerical Features\n\nWe will split the features into category and numerical.\n","07ab318e":"<a id=\"1\"><\/a>\n## Elemntary Data Analysis\n","469be8c8":"<a id=\"1.3\"><\/a>\n### Import Data","922f1d7b":"From the analysis, we can see that most of the rain are in the costal regions of Australia while regions in the center barely get rain,","dd7aacf9":"<a id=\"3\"><\/a>\n## Data Preprocessing","f6b81e18":"If we discard the years with insignificant levels of data, we can observe the majority of the rainfall to be in the winter months (June, July August). The only exception was in 2010 while 2011 seems relatively flat. According to the Australian Government Bereau of Meteorology, \"In 2010, Australia experienced its third-wettest year since national rainfall records began in 1900, with second place taken by 2011\" - http:\/\/www.bom.gov.au\/climate\/enso\/history\/ln-2010-12\/rainfall-flooding.shtml","0700d846":"WindDir9am and WindDir3pm are almost uniform apart from a few spikes and WindGustDir is almost normally distrubuted.","ab3f110a":"<a id=\"3.43\"><\/a>\n#### Other Cateogrical Features\nWe will enocde the rest of the categorical features","8b381334":"The major features that are missing values are Evaporation, Sunshine, Cloud9am and Cloud3pm. We cannot remove the column nor the row because these features are missing over 40%. We will use imputation to solve the missing data issue.","ba4064e9":"As we can see, the max values over the 75% quartile of Evporation, WindSpeed9am and WindSpeed3pm  have dropped from (19.59,6.84 and 3.65 respectively ) to (2.05, 1.94 and 1.625 respectively). We have removed (1-136735\/142193) around 4% of the data due to outliers. \n#","42a39138":"There are 142193 data observations and 23 features.","aa281b6c":"In terms of the categorical features, WindGustDir, WindDir9am and WindDir3pm have missing data over 5%. We will replace all missing values with the mode of their column.","146d8caf":"<a id=\"1.4123\"><\/a>\n###### Rain Today","53ad80a3":"These graphs show the combination of the missing feature and its most correlated feature in bins. The imputaiton algorithm will change a \"nan\" of the missing feature by looking at the value of its most correlated value and then select the mid value of the bin where the median lies within. If both the correlated and missing feature values are \"nan\", then the algorithm will return \"nan\" and if there is a value for the missing feature, it will return the same value. \nFor example, Sunshine is the missing feature and Humidity3pm is it's most correlated feature. If the Humidity3pm has a value of 54, and Sunshine is \"nan\", the algorithm will select the Sunshine Bin 7.25-8.7 because the median is about 7000 fo the Humidity3pm bin of 50-60 (since 54 lies within the bin). As a result, the algorithm will change the \"nan\" with 7.975 ((7.25+8.7)\/2)","bec26438":"Once we remove the missing values, we find that RainToday is binary like RainTomorrow. A high number of unique values per category also has significant issues in machine learning models. Dates can be encoded before running the machine learning models.","63048a32":"<a id=\"2\"><\/a>\n## Data Clean","ff3e3f76":" <a id=\"1.4131\"><\/a>\n###### Outliers\nWhen using a machine learning model, we need to look at the outliers.","6d91ee4d":"<a id=\"3.42\"><\/a>\n#### Date\n \nWe will extract the year, month and day from the dates","c049e4dc":"We have dropped Evaporation,Sunshine,Cloud9am,Cloud3pm drastically. We will look at WindGustSpeed, Pressure9am and Pressure3pm and see if we can use the algorithm imputation to treat the missing values","b49cf345":"The random forest using the resample data has lower f1 scores than the logistic regression. However, it is able to predict rain when it will actually rain better than the logistic regression (higher recall for predicting rain).\n\nThe original goal of the model as stated above was to predict rain such that it could better prepare farmers to harvest their stock such that they could take advantage of the rain. A model that has a high recall for predicting rain such as the random forest is helpful in that farmers can heavily rely on the prediction of rain but they could of prepared for alot more since there is more rain than predicted (low precision).  \n\nA model with a higher f1 score (weighted average of precision and recall) since the farmer can both rely on the predicion and more frequently prepare for the rain.\n\nAs a result, the logistic regression is the better model with an overall f1 of 64%. In addition, the accuracy is better than than if the model just simply predicted no since its accuracy is 82.74% which is above 77.8%. However it should be noted that the accuracy is NOT important as the F1 score due to the Accuracy Paradox.","22fb8975":"<a id=\"3.6\"><\/a>\n### Feature Scaling\n \nFeature Scaling basically helps to normalise the data within a particular range. While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. \nSometimes, it also helps in speeding up the calculations in an algorithm.\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html","3528cfbf":"The following graphs show if any outliers (both upper and lower) will lead to rain. I have noticed after an extremely hot day, there is rain fall the next day. The majority of the outliers in the numerical features have lead to no rain fall except for upper outliers of wind gust speed. Thus, we will keep the outliers of wind gust speed and remove outliers for Rainfall, Evaporation, WindSpeed9am, WindSpeed3pm. \nI have removed RISK_MM because the features measure the rainfall the next day and thus, if rain fall the next day is greater than 0, rainTomorrow will be yes. \n","686a3cdf":"These are the areas in which observations have been recorded.As we can see it is more densed in the south eastern region of Australia.The north west region of Australia doesn't have an observations. This is could be due to the lack of population in the area and thus, the need for an observation post is minimal. In addition, there is one observation east of Australia (Norfolk Island)","bcc6b834":"There is clearly an imbalance in classes since No is over 70 percent. This will have significant issues when training a machine learning model","be47e43d":"The features that have high correlation show that the weather throughout the day is very consistent. For example, If the pressure it high at 9am, then it would remain high at 3pm.","effe01b4":"For the lower outliers, max tempurature, pressure 9m, pressure 3pm and temp 3pm have lead to a higher chance of rain tomorrow. These features should be focused on while performing machine learning models.","1057516d":" Table of Content\n \n * <a href=\"#1\"><font size=\"5px\">Elementary Data Analysis <\/font><\/a>\n     * <a href=\"#1.1\"><font size=\"4px\">Hypothesis<\/font><\/a>\n     * <a href=\"#1.2\"><font size=\"4px\">Import Dependencies<\/font><\/a>   \n     * <a href=\"#1.3\"><font size=\"4px\">Import Rain Data<\/font><\/a>\n     * <a href=\"#1.3\"><font size=\"4px\"> Category and Numerical Features<\/font><\/a>\n         * <a href=\"#1.31\"><font size=\"4px\"> Cateogory Data <\/font><\/a>\n             * <a href=\"#1.311\"><font size=\"4px\"> Target <\/font><\/a>\n             * <a href=\"#1.312\"><font size=\"4px\"> Categorical Features  <\/font><\/a>\n                 * <a href=\"#1.3121\"> <font size=\"3px\">Location   <\/font><\/a>\n                 * <a href=\"#1.3122\"> <font size=\"3px\">Dates <\/font><\/a>\n                 * <a href=\"#1.3123\"> <font size=\"3px\">Rain Today  <\/font><\/a>      \n                 * <a href=\"#1.3124\"> <font size=\"3px\">WindGustDir, WindDir9am and WindDir3pm <\/font><\/a>                \n         * <a href=\"#1.4\"><font size=\"4px\"> Numerical Features<\/font> <\/a>\n             * <a href=\"#1.41\">  <font size=\"4px\"> Outliers<\/font><\/a>\n             * <a href=\"#1.42\">  <font size=\"4px\"> Correlation<\/font><\/a>\n             * <a href=\"#1.43\">  <font size=\"4px\"> Exploration of Humidity <\/font><\/a>\n * <a href=\"#2\"> <font size=\"5px\">  Data Cleaning<\/font><\/a>\n     * <a href=\"#2.1\"> <font size=\"4px\">Treatment of Outliers<\/font><\/a>\n     * <a href=\"#2.2\"> <font size=\"4px\">Treatement of Missing Data<\/font><\/a>\n         * <a href=\"#2.21\"><font size=\"4px\">  Numerical Features<\/font><\/a>\n         * <a href=\"#2.22\"><font size=\"4px\"> Categorical Features<\/font><\/a>\n * <a href=\"#3\"><font size=\"5px\"> Data Preprocessing<\/font><\/a>\n     * <a href=\"#3.1\"> <font size=\"4px\"> Target and Features<\/font><\/a>\n     * <a href=\"#3.2\"> <font size=\"4px\"> Imbalance of the Target<\/font><\/a>\n     * <a href=\"#3.3\"><font size=\"4px\">  Split the data to training and testing sets<\/font><\/a>\n     * <a href=\"#3.4\"><font size=\"4px\"> Encode cateogorical features<\/font><\/a>\n         * <a href=\"#3.41\"><font size=\"4px\"> Binary Categorical Features<\/font><\/a>\n         * <a href=\"#3.42\"><font size=\"4px\"> Dates<\/font><\/a>\n         * <a href=\"#3.43\"><font size=\"4px\"> Other Cateogorical Feaures<\/font><\/a>\n     * <a href=\"#3.5\"><font size=\"4px\">Over sampling<\/font><\/a>\n     * <a href=\"#3.6\"><font size=\"4px\">Feature Scaling<\/font><\/a>\n * <a href=\"#4\"> <font size=\"5px\">Training the Model<\/font><\/a>\n     * <a href=\"#4.1\"> <font size=\"4px\">Performance Metrics<\/font><\/a>\n     * <a href=\"#4.2\"> <font size=\"4px\">Grid Search<\/font><\/a>\n         * <a href=\"#4.21\"><font size=\"4px\">Logistic Regression<\/font><\/a>\n         * <a href=\"#4.22\"><font size=\"4px\"> Random Foreset Classifer<\/font><\/a>\n * <a href=\"#5\"> <font size=\"5px\">Conclusion<\/font><\/a>\n \n","e5fa8cbe":"If we filter the data by the certain humidity criterias, we find that when the Humidity3pm is above the 80.5, over 70% of 12434 (12434\/142193 = 8.7% of all data observation) days lead to rain the next day. Even a humidity of 100 doesn't guarantee rain the next day and in fact humidity of 100 at 9am has a lower chance of rain the next day. A decrease in humidity (humidity 9am less than humidity 3pm) leads to almost 80% of 118880 (118880\/142193 = 83% of all data observation) observations to no rain the next day. \n\nOur model should try and take into consideration the relationship between high humidity3pm and rain fall the next day"}}