{"cell_type":{"d4d4ad5a":"code","27154221":"code","e1fc6708":"code","a58db77e":"code","ab42a683":"code","74990d66":"code","7a3e1376":"code","bcb0e845":"code","bc34af52":"code","dc8d4c9c":"code","67308193":"code","e308e747":"code","bfbd1222":"code","045b16c4":"code","c7853333":"code","519ec947":"code","8644e8b8":"code","97d13455":"code","d2529347":"code","3dfe7710":"code","1127679a":"code","1838a370":"code","fc98fa15":"code","527d2272":"code","f3926ead":"code","c9b92eba":"code","48ec4cd8":"code","101797f9":"code","4232d354":"markdown","2a85c660":"markdown","79632dd8":"markdown","9bb218ba":"markdown","12fe727e":"markdown","6b7200b6":"markdown","b947a5dd":"markdown","25a994a5":"markdown","0f5631b0":"markdown","949b5f78":"markdown","89914e28":"markdown","fd9af6d6":"markdown","1179413b":"markdown","af60653b":"markdown","e971ec60":"markdown","0d90d61f":"markdown","082d2e25":"markdown","380fa838":"markdown","99ed981c":"markdown","8bbf9e91":"markdown","b00fc4e0":"markdown","16304c85":"markdown","47d972f9":"markdown","be251007":"markdown","fe6de88d":"markdown"},"source":{"d4d4ad5a":"import numpy as np # linear algebra\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nimport math\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nsns.set(style='white', context='notebook', palette='deep')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nRandom_state=42\nnp.random.seed(0)","27154221":"#Models import\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n#import imputer:\nfrom sklearn.impute import KNNImputer\n#score\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import StackingClassifier","e1fc6708":"dataset=pd.read_csv(\"..\/input\/startup-success-prediction\/startup data.csv\",\\\n                    converters={'status': lambda x: int(x == 'acquired')},parse_dates=['founded_at','first_funding_at','last_funding_at'])\ndataset.head()\n","a58db77e":"dataset.rename(columns={'status':'is_acquired'}, inplace=True)","ab42a683":"def draw_heatmap(dataset):\n    \n    \n    f, ax = plt.subplots(figsize = (18, 18))\n    \n    corrMatt = dataset.corr(method='spearman')\n    \n    sns.heatmap(corrMatt, annot = True, linewidth = 0.5, fmt = '.1f', ax = ax)\n    plt.show()\n    \n    \nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \nnumerical_df_1=dataset.select_dtypes(numerics)\nnumerical_column_names = dataset.select_dtypes(numerics).columns\n\ndraw_heatmap(numerical_df_1)\n","74990d66":"def getOutliersMatrix(numerical_df, threshold=1.5):\n    Q1 = numerical_df.quantile(0.25)\n    Q3 = numerical_df.quantile(0.75)\n    IQR = Q3 - Q1\n    \n    outdata = (numerical_df < (Q1 - 1.5 * IQR)) | (numerical_df > (Q3 + 1.5 * IQR))\n    \n    for name in numerical_df.columns:\n        outdata.loc[(outdata[name] == True), name] = 1\n        outdata.loc[(outdata[name] == False), name] = 0\n    \n    return outdata\n\n\noutliersMatt = getOutliersMatrix(numerical_df_1)\n","7a3e1376":"outliersMatt = getOutliersMatrix(numerical_df_1)\n\ndataset[outliersMatt==1]= np.nan\n\nnumerical_df_2=dataset.select_dtypes(numerics)\n\ndraw_heatmap(numerical_df_2)","bcb0e845":"dataset.drop([\"Unnamed: 6\"],axis=1, inplace=True)\ndataset.drop([\"Unnamed: 0\"], axis=1, inplace=True)\ncomparison_column = np.where(dataset[\"state_code\"] != dataset[\"state_code.1\"], True, False)\ndataset[comparison_column]['state_code.1']\ndataset.drop([\"state_code.1\"], axis=1, inplace=True)","bc34af52":"print(dataset.isnull().sum())","dc8d4c9c":"def imputing_numeric_missing_values(dataset,n_neighbors=10):\n    numerical_column_names = dataset.select_dtypes([np.number]).columns\n    knn= KNNImputer()\n    knn_dataset= knn.fit_transform(dataset[numerical_column_names])\n    \n    dataset[numerical_column_names]=pd.DataFrame(knn_dataset)\n    return dataset\n\ndataset=imputing_numeric_missing_values(dataset)\n\nnumerical_df_3=dataset.select_dtypes(numerics)\n\n# Check for Null values\nprint(dataset.isnull().sum())","67308193":"dataset['closed_at']=dataset['closed_at'].fillna('temporary')\ndataset['closed_at'] = dataset.closed_at.apply(lambda x: 1 if x =='temporary' else 0)","e308e747":"dataset['months_between_first_and_last_funding'] = ((dataset.last_funding_at - dataset.first_funding_at)\/np.timedelta64(1, 'M'))\ndataset['months_between_foundation_and_first_funding']=((dataset.first_funding_at - dataset.founded_at)\/np.timedelta64(1, 'M'))","bfbd1222":"#delete unnecessary data\ndataset.drop([\"last_funding_at\"],axis=1, inplace=True)\ndataset.drop([\"first_funding_at\"], axis=1, inplace=True)\ndataset.drop([\"founded_at\"], axis=1, inplace=True)\n","045b16c4":"#convert object_id to numeric:\ndataset['object_id'] = dataset['object_id'].str.replace(\"c:\", '').astype(int)\ndataset['id'] = dataset['id'].str.replace(\"c:\", '').astype(int)\n\nnumerical_df_4=dataset.select_dtypes(numerics)\ndraw_heatmap(numerical_df_4)\n\n# Create correlation matrix\ncorr_matrix = numerical_df_4.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.loc[\"is_acquired\"]\nupper=upper.fillna(0)\nupper=upper.to_dict()\n# Find features with correlation greater than 0.95\nto_drop = [key for key in upper if upper[key]< 0.2]\n","c7853333":"print(to_drop)","519ec947":"dataset.drop(to_drop, axis=1, inplace=True)\nnumerical_df_5=dataset.select_dtypes(numerics)\ndataset.drop([\"labels\"], axis=1, inplace=True)\ndataset.drop([\"closed_at\"], axis=1, inplace=True)\ndataset.drop([\"months_between_first_and_last_funding\"], axis=1, inplace=True) #corelated to founding_rounds\nnumerical_df_5=dataset.select_dtypes(numerics)\ndraw_heatmap(numerical_df_5)\n\ndataset=pd.get_dummies(dataset)","8644e8b8":"y=dataset[\"is_acquired\"]\nX= dataset.loc[:, dataset.columns != 'is_acquired']\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=42)\n\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n\n# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nplt.show()","97d13455":"best_classifiers=[]\n# Adaboost\n\n### META MODELING\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsadaDTC.fit(X_train,y_train)\n\nada_best = gsadaDTC.best_estimator_\n\n# Best score\nprint(\"Adaboost score: \"+str(gsadaDTC.best_score_))\n\n\n### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsSVMC.fit(X_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\nbest_classifiers.append(SVMC_best)\n\n# Best score\nSVC_score = gsSVMC.best_score_\n\nprint(f' SVC classifier score is :{SVC_score}')\n\n#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [1,2,4],\n              \"max_features\": [1,100,1000],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\nbest_classifiers.append(ExtC_best)\n\n# Best score\n\nExtraTrees_score= gsExtC.best_score_\n\nprint(f' ExtraTrees score is :{ExtraTrees_score}')\n\n# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [1,2,4],\n              \"max_features\": [1, 10,100,1000],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\nbest_classifiers.append(RFC_best)\n\n# Best score\n\nRandomForests_score= gsRFC.best_score_\n\nprint(f' RandomForests score is :{RandomForests_score}')\n\n# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,1000],\n              'learning_rate': [0.001,0.1, 0.05, 0.01,1,10],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsGBC.fit(X_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\nbest_classifiers.append(GBC_best)\n\n# Best score\n\nGradientBoosting_score= gsGBC.best_score_\n\nprint(f' GradientBoosting score is :{GradientBoosting_score}')","d2529347":"def plotly_scatterplots(model_importances,model_title):\n    trace = go.Scatter(\n        y = feature_dataframe[model_importances].values,\n        x = feature_dataframe['features'].values,\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size = 25,\n    #       size= feature_dataframe['AdaBoost feature importances'].values,\n            #color = np.random.randn(500), #set color equal to a variable\n            color = feature_dataframe[model_importances].values,\n            colorscale='Portland',\n            showscale=True\n        ),\n        text = feature_dataframe['features'].values\n    )\n    data = [trace]\n\n    layout= go.Layout(\n        autosize= True,\n        title= model_title,\n        hovermode= 'closest',\n    #     xaxis= dict(\n    #         title= 'Pop',\n    #         ticklen= 5,\n    #         zeroline= False,\n    #         gridwidth= 2,\n    #     ),\n        yaxis=dict(\n            title= 'Feature Importance',\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig,filename='scatter2010')","3dfe7710":"# Create a dataframe with features\n\ncols = X_train.columns.values\n\nrf_feature = RFC_best.feature_importances_\net_feature = ExtC_best.feature_importances_\nada_feature = ada_best.feature_importances_\ngb_feature = GBC_best.feature_importances_\n\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_feature,\n     'Extra Trees  feature importances': et_feature,\n      'AdaBoost feature importances': ada_feature,\n    'Gradient Boost feature importances': gb_feature\n    })\nfeature_dataframe = feature_dataframe[feature_dataframe.astype(bool).sum(axis=1) > feature_dataframe.shape[1]\/(1.2)]\nmodel_importances=['Random Forest feature importances','Extra Trees  feature importances','AdaBoost feature importances','Gradient Boost feature importances']\nmodel_title=['Random Forest feature importance','Extra Trees  feature importance','AdaBoost feature importance','Gradient Boost feature importance']\n\nfor importances,title in zip(model_importances,model_title):\n    \n    plotly_scatterplots(importances,title)\n    ","1127679a":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","1838a370":"def mean_bar_plot(feature_dataframe):\n    y = feature_dataframe['mean'].values\n    x = feature_dataframe['features'].values\n    data = [go.Bar(\n                x= x,\n                 y= y,\n                width = 0.5,\n                marker=dict(\n                   color = feature_dataframe['mean'].values,\n                colorscale='Portland',\n                showscale=True,\n                reversescale = False\n                ),\n                opacity=0.6\n            )]\n\n    layout= go.Layout(\n        autosize= True,\n        title= 'Barplots of Mean Feature Importance',\n        hovermode= 'closest',\n    #     xaxis= dict(\n    #         title= 'Pop',\n    #         ticklen= 5,\n    #         zeroline= False,\n    #         gridwidth= 2,\n    #     ),\n        yaxis=dict(\n            title= 'Feature Importance',\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='bar-direct-labels')","fc98fa15":"mean_bar_plot(feature_dataframe)","527d2272":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                      train_scores_mean + train_scores_std, alpha=0.1,\n                      color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n              label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n              label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,y_train,cv=kfold)\nplt.show()\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,y_train,cv=kfold)\nplt.show()\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,y_train,cv=kfold)\nplt.show()\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,y_train,cv=kfold)\n\n","f3926ead":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1\n        \n\n\ntest_is_acquired_RFC = pd.Series(RFC_best.predict(X_test), name=\"RFC\")\ntest_is_acquired_ExtC = pd.Series(ExtC_best.predict(X_test), name=\"ExtC\")\ntest_is_acquired_AdaC = pd.Series(ada_best.predict(X_test), name=\"Ada\")\ntest_is_acquired_GBC = pd.Series(GBC_best.predict(X_test), name=\"GBC\")","c9b92eba":"ensemble_results = pd.concat([test_is_acquired_RFC,test_is_acquired_ExtC,test_is_acquired_AdaC,test_is_acquired_GBC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","48ec4cd8":"#voting Classifier:\n \nvotingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best)\n,('gbc',GBC_best)], voting='soft', n_jobs=-1)\n\nvotingC = votingC.fit(X_train, y_train)\n\n#stacking\ndef stacking(classifiers, X_train, X_test, y_train, y_test):\n    all_estimators = []\n    for classifier in classifiers:\n        all_estimators.append((str(classifier), classifier))\n    stack = StackingClassifier(estimators=all_estimators, final_estimator=GBC_best)\n    score= stack.fit(X_train, y_train).score(X_test, y_test)\n   \n    return score\n","101797f9":"test_is_acquired = pd.Series(votingC.predict(X_test), name=\"is_acquired\")\n\nresults = pd.concat([test_is_acquired],axis=1)\n\nscore= f1_score(y_test, results, average='macro')\n\nstacking_score = stacking(best_classifiers, X_train, X_test, y_train, y_test)\n\nprint(f'the voting score is: {score}')\n\nprint(f'the stacking score is: {stacking_score} ') ","4232d354":"read the dataset and convert the label (status) to binary\n","2a85c660":"### dealing with Time Series features\nConvert them to numeric numbers by adding and making new features:","79632dd8":"Concatenate all classifier results\n","9bb218ba":"We can clearly see stacking gave best classification score results","12fe727e":"# Ensemble modeling\n\n## Combining models","6b7200b6":"# Correlation between numeric parameters\n","b947a5dd":"## Plot learning curves","25a994a5":"Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n","0f5631b0":"#### Drop features \n","949b5f78":"## Imputing missing values with KNN Imputer:","89914e28":"# Detect outliers from numeric variables\n","fd9af6d6":"# MODELING\n","1179413b":"# Prediction","af60653b":"Feature importance of tree based classifiers\n","e971ec60":"It can be notice that we filled all the nan places except for closed_at which isnt numeric","0d90d61f":"Removing the columns \"Unnamed: 0\" and \"Unnamed: 6\", Unnamed: 6 has 493 missimg data as we dont have info about this cloumn.\nUnnamed:0 is unknown. ","082d2e25":"The idea is to add missimg values every outliers points, the outliers points are detected by IQR test.\nAfter data processing we will fill the overall missing data.","380fa838":"Check for Null values\n","99ed981c":"# Libraries import","8bbf9e91":"#### Features about to drop","b00fc4e0":"Changeing the label column name from \"status\" to \"is_acquired\" ,\nfor binary classification","16304c85":"### Convert closed_at values in a column into binary \nNotice that nan value mean the Startup is still operating. \nMoreover, with the none value we cant use it with time data analysis","47d972f9":"I decided to choose the SVC ,RandomForest,ExtraTrees and the GradientBoosting classifiers for the ensemble modeling.\n","be251007":"# Data processing\n","fe6de88d":"## Interactive feature importances via Plotly scatterplots\n\n"}}