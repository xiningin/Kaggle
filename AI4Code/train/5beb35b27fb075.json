{"cell_type":{"80727e27":"code","f2a1ec56":"code","e06b5686":"code","78311530":"code","c063ef24":"code","82526608":"code","3b5df59d":"code","b3301d92":"code","7f53ff3c":"code","fa183aed":"code","4634a120":"code","d6100c16":"code","e87082a4":"code","37a4315e":"code","0c9e548b":"code","c0d97746":"code","1de6b1e3":"code","465f824c":"code","caa83eaa":"code","9fc7b64e":"code","ab047819":"code","bb91a7eb":"code","02980b34":"code","28cefbee":"code","072a781d":"markdown","58045762":"markdown","0ab8b53d":"markdown","6aa945d4":"markdown","92398674":"markdown","09813dee":"markdown","7d3890cd":"markdown","37b2fd50":"markdown","79238583":"markdown"},"source":{"80727e27":"##survival:    Survival \n##PassengerId: Unique Id of a passenger. \n##pclass:    Ticket class     \n##sex:    Sex     \n##Age:    Age in years     \n##sibsp:    # of siblings \/ spouses aboard the Titanic     \n##parch:    # of parents \/ children aboard the Titanic     \n##ticket:    Ticket number     \n##fare:    Passenger fare     \n##cabin:    Cabin number     \n##embarked:    Port of Embarkation","f2a1ec56":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndataset= pd.read_csv(\"..\/input\/train.csv\")\ntest_dataset    = pd.read_csv(\"..\/input\/test.csv\")\ntitanic = dataset.append(test_dataset, ignore_index=True)\ntitanic.head()\n# create indexes to separate data later on\ntrain_idx = len(dataset)\ntest_idx = len(titanic) - len(test_dataset)","e06b5686":"# Data Visulization","78311530":"fig, axr=plt.subplots(2, 2, figsize=(15, 15))\ng=sns.factorplot('Survived', data=dataset, kind=\"count\", hue='Embarked',ax=axr[0][0])\nplt.close(g.fig)\ng=sns.factorplot('Survived', data=dataset, kind=\"count\", hue='Sex',ax=axr[0][1])\nplt.close(g.fig)\ng=sns.factorplot('Survived', data=dataset, kind=\"count\", hue='Pclass',ax=axr[1][0])\nplt.close(g.fig)\ng=sns.factorplot('Survived', data=dataset, kind=\"count\", hue='Parch',ax=axr[1][1])\nplt.close(g.fig)\nplt.show()","c063ef24":"g = sns.FacetGrid(titanic, col='Survived')\ng.map(plt.hist, 'Age', bins=20);","82526608":"# Data Preprocessing\n# Check null values\ntitanic.isnull().sum()","3b5df59d":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\ndata_with_imputed_values = my_imputer.fit_transform(titanic[['Age']])\ntitanic[['Age']]=data_with_imputed_values\n\n\ntitanic['Embarked'] = titanic['Embarked'].fillna('S')\ntitanic['FamilySize'] = titanic['SibSp'] + titanic['Parch'] + 1","b3301d92":"from sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\ntitanic[['Sex']]=lb.fit_transform(titanic[['Sex']])","7f53ff3c":"from sklearn.preprocessing import LabelEncoder\nlb_make = LabelEncoder()\ntitanic[\"Embarked\"] = lb_make.fit_transform(titanic[\"Embarked\"])\n\ntitanic['Title'] = titanic.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\ntitanic[\"Title\"] = lb_make.fit_transform(titanic[\"Title\"])\ntitanic.Fare = titanic.Fare.fillna(titanic.Fare.median())\n\ntitanic['Deck']=titanic['Cabin'][0]\ntitanic.Deck = titanic.Deck.fillna('U')\n\nlb = LabelBinarizer()\ntitanic[['Deck']]=lb.fit_transform(titanic[['Deck']])\n\ntitanic=titanic.drop(['Cabin'],axis=1)\ntitanic=titanic.drop(['Name'],axis=1);\ntitanic=titanic.drop(['Ticket'],axis=1);\n\ntitanic.head()","fa183aed":"# create train and test data\ntrain = titanic[ :train_idx]\ntest = titanic[test_idx: ]\n\n# convert Survived back to int\ntrain.Survived = train.Survived.astype(int)\n# create X and y for data and target values \nX = train.drop(['Survived','PassengerId'], axis=1).values \ny = train.Survived.values\n","4634a120":"# create array for test set\nX_test = test.drop(['Survived','PassengerId'], axis=1).values","d6100c16":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier","e87082a4":"def print_score(clf, X_train, y_train, train=True):\n    '''\n    print the accuracy score, classification report and confusion matrix of classifier\n    '''\n    if train:\n        '''\n        training performance\n        '''\n        print(\"Train Result:\\n\")\n        y_train_predict=clf.predict(X_train);\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, y_train_predict)))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train,y_train_predict)))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, y_train_predict)))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n ","37a4315e":"rf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X,y);\nprint_score(rf_clf, X, y, train=True)","0c9e548b":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nrf_clf = RandomForestClassifier(random_state=42)","c0d97746":"params_grid = {\"max_depth\": [3, None],\n               \"min_samples_split\": [2, 3, 10],\n               \"min_samples_leaf\": [1, 3, 10],\n               \"bootstrap\": [True, False],\n               \"criterion\": ['gini', 'entropy']}\ngrid_search = GridSearchCV(rf_clf, params_grid,\n                           n_jobs=-1, cv=5,\n                           verbose=1, scoring='accuracy')\ngrid_search.fit(X, y)\nprint_score(grid_search, X, y, train=True)","1de6b1e3":"from sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier()\nada_clf.fit(X, y);\nprint_score(ada_clf, X, y,train=True)","465f824c":"from sklearn.ensemble import RandomForestClassifier\nada_clf = AdaBoostClassifier(RandomForestClassifier())\nada_clf.fit(X, y);\nprint_score(ada_clf, X, y, train=True)","caa83eaa":"ada_clf = AdaBoostClassifier(base_estimator=RandomForestClassifier())\nada_clf.fit(X, y);\nprint_score(ada_clf, X, y, train=True)","9fc7b64e":"from sklearn.ensemble import GradientBoostingClassifier\ngbc_clf = GradientBoostingClassifier()\ngbc_clf.fit(X, y);\nprint_score(gbc_clf, X, y,train=True)\n","ab047819":"import xgboost as xgb\nxgb_clf = xgb.XGBClassifier(max_depth=5, n_estimators=10000, learning_rate=0.3,\n                            n_jobs=-1)\nxgb_clf.fit(X, y);\nprint_score(xgb_clf, X, y, train=True)","bb91a7eb":"# random forrest prediction on test set\nrmf_pred = xgb_clf.predict(X_test)","02980b34":"# dataframe with predictions\noutput = pd.DataFrame({'Survived': rmf_pred})\noutput['Survived'].value_counts()\ntest.Survived=rmf_pred\ntest[['PassengerId','Survived']].to_csv('data_pred.csv', index=False)","28cefbee":"pd.read_csv('data_pred.csv').head()","072a781d":"# Gradient Boosting \/ Gradient Boosting Machine (GBM)\nWorks for both regression and classification\n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting)\n\n* Sequentially adding predictors\n* Each one correcting its predecessor\n* Fit new predictor to the residual errors\n\nCompare this to AdaBoost: \n* Alter instance weights at every iteration","58045762":"# Boosting (Hypothesis Boosting)\n\n* Combine several weak learners into a strong learner. \n\n* Train predictors sequentially","0ab8b53d":"**Bagging Machine Learning Algorithm**\nB*ootstrap *Aggregating or Bagging\n* Scikit- Learn Reference\n* Bootstrap sampling: Sampling with replacement\n* Combine by averaging the output (regression)\n* Combine by voting (classification)\n* Can be applied to many classifiers which includes ANN, CART, etc.","6aa945d4":"# XGBoost (Extreme Gradient Boosting)\n\n[Documentation](http:\/\/xgboost.readthedocs.io\/en\/latest\/)\n\n[tqchen github](https:\/\/github.com\/tqchen\/xgboost\/tree\/master\/demo\/guide-python)\n\n[dmlc github](https:\/\/github.com\/dmlc\/xgboost)\n\n* \u201cGradient Boosting\u201d is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. \n* XGBoost is based on this original model. \n\n* Supervised Learning\n\n## Objective Function : Training Loss + Regularization\n\n$$Obj(\u0398)=L(\u03b8)+\u03a9(\u0398)$$\n\n* $L$ is the training loss function, and \n* $\u03a9$ is the regularization term. \n\n### Training Loss\n\nThe training loss measures how predictive our model is on training data.\n\nExample 1, Mean Squared Error for Linear Regression:\n\n$$L(\u03b8)= \\sum_i(y_i-\\hat{y}_i)^2$$\n\nExample 2, Logistic Loss for Logistic Regression:\n\n$$ L(\u03b8) = \\sum_i \\large[ y_i ln(1 + e^{-\\hat{y}_i}) + (1-y_i) ln(1 + e^{\\hat{y}_i}) \\large] $$\n\n### Regularization Term\n\nThe regularization term controls the complexity of the model, which helps us to avoid overfitting. \n\n[XGBoost vs GBM](https:\/\/www.quora.com\/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting\/answer\/Tianqi-Chen-1)\n\n* Specifically,\u00a0 xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.\n\n* For model, it might be more suitable to be called as regularized gradient boosting.","92398674":"Gradient boosting involves three elements:\n\n* **Loss function to be optimized**: Loss function depends on the type of problem being solved. In the case of regression problems, mean squared error is used, and in classification problems, logarithmic loss will be used. In boosting, at each stage, unexplained loss from prior iterations will be optimized rather than starting from scratch.\n\n* **Weak learner to make predictions**: Decision trees are used as a weak learner in gradient boosting.\n\n* **Additive model to add weak learners to minimize the loss function**: Trees are added one at a time and existing trees in the model are not changed. The gradient descent procedure is used to minimize the loss when adding trees.","09813dee":"## AdaBoost with Random Forest","7d3890cd":"**Step 1. **\n\n  $$Y = F(x) + \\epsilon$$\n\n**Step 2. **\n\n  $$\\epsilon = G(x) + \\epsilon_2$$\n\n  Substituting (2) into (1), we get:\n  \n  $$Y = F(x) + G(x) + \\epsilon_2$$\n    \n**Step 3. **\n\n  $$\\epsilon_2 = H(x)  + \\epsilon_3$$\n\nNow:\n  \n  $$Y = F(x) + G(x) + H(x)  + \\epsilon_3$$\n  \nFinally, by adding weighting  \n  \n  $$Y = \\alpha F(x) + \\beta G(x) + \\gamma H(x)  + \\epsilon_4$$","37b2fd50":"**Grid Search**","79238583":"# AdaBoost \/ Adaptive Boosting\n\n[Robert Schapire](http:\/\/rob.schapire.net\/papers\/explaining-adaboost.pdf)\n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/AdaBoost)\n\n[Chris McCormick](http:\/\/mccormickml.com\/2013\/12\/13\/adaboost-tutorial\/)\n\n[Scikit Learn AdaBoost](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#adaboost)\n\n1995\n\nAs above for Boosting:\n* Similar to human learning, the algo learns from past mistakes by focusing more on difficult problems it did not get right in prior learning. \n* In machine learning speak, it pays more attention to training instances that previously underfitted.\n\nSource: Scikit-Learn:\n\n* Fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. \n* The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction.\n* The data modifications at each so-called boosting iteration consist of applying weights $w_1, w_2, \u2026, w_N$ to each of the training samples. \n* Initially, those weights are all set to $w_i = 1\/N$, so that the first step simply trains a weak learner on the original data. \n* For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. \n* At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. \n* As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.\n\n"}}