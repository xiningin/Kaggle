{"cell_type":{"136ee031":"code","bc4262bb":"code","28ff0cf1":"code","12396957":"code","bb7f0000":"code","5274d1c4":"code","ca6b1d4e":"code","2515581d":"code","9bf7d801":"code","22df47d0":"code","375e5749":"code","3d154591":"code","4d0f8283":"code","cefd3502":"code","d0a4fb8b":"code","a9919467":"code","230f6692":"code","ab2694ae":"code","3061a9f4":"code","819cf955":"code","3059a00b":"code","7b65bc75":"code","35c036e3":"code","f28ae389":"code","0fdc616b":"code","0e56c6db":"code","0549bd2c":"code","e1caa74a":"code","99b85334":"code","cbcfcbe7":"code","6a5f20b2":"code","80b8a061":"code","5292e1e2":"code","14291c98":"code","b7f6ddf4":"code","11626865":"code","75ef4170":"code","657c7d7f":"code","e8229f89":"code","17cced4e":"code","13e97c8e":"code","4fa975e6":"code","80c457c9":"code","4935a0e4":"code","d585a5e3":"markdown","d91918c4":"markdown","2711031f":"markdown","43187405":"markdown","1bc8433e":"markdown","f82dc516":"markdown","cbfae5b9":"markdown","8b3f79d4":"markdown","43ba6ec2":"markdown","d23c68b8":"markdown","2cc6ea4d":"markdown","8695a6b4":"markdown","c8ca6a6e":"markdown","d39203d7":"markdown","f22d2198":"markdown","b884d09f":"markdown","1a26e630":"markdown","41fe1e56":"markdown","4666ce93":"markdown","e5657522":"markdown"},"source":{"136ee031":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn import metrics\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bc4262bb":"tweets_df = pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")","28ff0cf1":"tweets_df.head()","12396957":"tweets_df.shape","bb7f0000":"tweets_df.columns","5274d1c4":"# Check if any of the columns have unique values\nnonunique_cols = [featr for featr in tweets_df.columns if len(tweets_df[featr].unique()) <2]\nnonunique_cols","ca6b1d4e":"#Check for missing values\n100*tweets_df.isna().sum()\/len(tweets_df)","2515581d":"tweets_df.drop(['airline_sentiment_gold', 'negativereason_gold', 'tweet_coord'], axis=1, inplace =True)","9bf7d801":"100*tweets_df.isna().sum()\/len(tweets_df)","22df47d0":"tweets_df[['negativereason', 'negativereason_confidence', 'tweet_location', 'user_timezone']].head()","375e5749":"# Data balance\ndef createPieChartFor(t_df):\n    Lst = 100*t_df.value_counts()\/len(t_df)\n    \n    # set data for pie chart\n    labels = t_df.value_counts().index.values\n    sizes =  Lst \n    \n    # set labels\n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90)\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    plt.show()","3d154591":"createPieChartFor(tweets_df.airline_sentiment)","4d0f8283":"createPieChartFor(tweets_df.airline)","cefd3502":"airline_sentiment_df = tweets_df.groupby(['airline','airline_sentiment']).airline_sentiment.count().unstack()\nairline_sentiment_df.plot(kind='bar')\nplt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")","d0a4fb8b":"airline_sentiment_df.plot(kind='bar', stacked=True)\nplt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")","a9919467":"tweets_df.tweet_created= tweets_df.tweet_created.apply(pd.to_datetime).dt.date","230f6692":"temp_df = tweets_df.groupby(['tweet_created','airline']).airline_sentiment.count().unstack()\nax1 = temp_df.plot(kind='bar', figsize = (15,5))\nax1.set_ylabel('Tweets')\nplt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")","ab2694ae":"neg_tweet_df = tweets_df.groupby(['tweet_created','airline','airline_sentiment']).size()\nneg_tweet_df = neg_tweet_df.loc(axis=0)[:,:,'negative']\nax2 = neg_tweet_df.groupby(['tweet_created','airline']).sum().unstack().plot(kind='bar', figsize = (15,5), rot=70)\nax2.set_ylabel('Negative Tweets')\nplt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")","3061a9f4":"sns.set_style(\"whitegrid\")\nax = sns.barplot(x=\"airline_sentiment\", y=\"airline_sentiment_confidence\", data=tweets_df)","819cf955":"tweets_df.negativereason.value_counts()","3059a00b":"tweets_df.negativereason.value_counts().plot(kind='bar', figsize=(15,5))","7b65bc75":"plt.figure(figsize=(15, 4))\nsns.set(font_scale = 1.1)\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=\"negativereason\", y=\"negativereason_confidence\", data=tweets_df)\nplt.xticks(rotation=70)","35c036e3":"from wordcloud import WordCloud,STOPWORDS\ndef createWrdCloudForSentiment(sentiment):\n    temp_df = tweets_df[tweets_df.airline_sentiment==sentiment]\n    words = \" \".join(temp_df.text)\n    cleaned_words = \" \".join([w for w in words.split()\n                                  if 'http' not in w\n                                    and not w.startswith('@')\n                                    and w!='RT'])\n\n    wrdcld = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=1500,\n                      height=1000).generate(cleaned_words)\n    plt.figure(figsize=(10,10))\n    plt.imshow(wrdcld)\n    plt.axis('off')\n    plt.show","f28ae389":"createWrdCloudForSentiment('negative')","0fdc616b":"createWrdCloudForSentiment('positive')","0e56c6db":"tweets_df.columns","0549bd2c":"tweets_df.text","e1caa74a":"nltk.download('stopwords')\neng_stops = set(stopwords.words(\"english\"))","99b85334":"#nltk.download('wordnet')","cbcfcbe7":"## We'll check latter if stemmer will make any difference\n#from nltk.stem.porter import PorterStemmer\n#stemmer = PorterStemmer()\n#\n#from nltk.stem import WordNetLemmatizer \n#lemmatizer = WordNetLemmatizer() ","6a5f20b2":"def process_message(tweet):\n    # remove all the special characters\n    new_tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    # convert all letters to lower case\n    words = new_tweet.lower().split()\n    # remove stop words\n    words = [w for w in words if not w in eng_stops]        \n    # stemming\n    #words = [stemmer.stem(word) for word in words]\n    # lemmatizer\n    #words = [lemmatizer.lemmatize(word) for word in words]\n    # join all words back to text\n    return (\" \".join(words))","80b8a061":"tweets_df['clean_tweet']=tweets_df['text'].apply(lambda x: process_message(x))","5292e1e2":"tweets_df['clean_tweet'].to_list()","14291c98":"train_df, test_df = train_test_split(tweets_df, test_size=0.3, random_state=42)","b7f6ddf4":"train_tweets =[]\nfor tweet in train_df.clean_tweet:\n    train_tweets.append(tweet)\n    \ntest_tweets =[]\nfor tweet in test_df.clean_tweet:\n    test_tweets.append(tweet)","11626865":"# bag of words model\nvectorizer = TfidfVectorizer()\ntrain_tfidf_model = vectorizer.fit_transform(train_tweets)\ntest_tfidf_model = vectorizer.transform(test_tweets)","75ef4170":"# let's look at the dataframe\ntrain_tfidf = pd.DataFrame(train_tfidf_model.toarray(), columns=vectorizer.get_feature_names())\ntrain_tfidf","657c7d7f":"print(vectorizer.get_feature_names())","e8229f89":"cls = [LogisticRegression(),\n       MultinomialNB(), \n       DecisionTreeClassifier(),\n       RandomForestClassifier(n_estimators=200),\n       KNeighborsClassifier(n_neighbors = 5)]\n\ncls_name = []","17cced4e":"lbl_actual = test_df.airline_sentiment\ni = 0\naccuracy = []\nfor cl in cls:\n    model = cl.fit(train_tfidf_model,train_df.airline_sentiment)\n    lbl_pred = model.predict(test_tfidf_model)\n    a = (100*accuracy_score(lbl_pred, lbl_actual))\n    a = round(a,2)\n    accuracy.append(a)\n    cls_name.append(cl.__class__.__name__)\n    print (\"{}  Accuracy Score : {}%\".format(cls_name[i],a))\n    print ( classification_report(lbl_pred, lbl_actual))\n    i +=1","13e97c8e":"plt.bar(cls_name, accuracy)\nplt.xticks(rotation=70)","4fa975e6":"# Save to csv\n\nlg_model = LogisticRegression().fit(train_tfidf_model,train_df.airline_sentiment)\nlg_lbl_pred = model.predict(test_tfidf_model)","80c457c9":"lg_lbl_pred_df = pd.DataFrame({'tweet_id': test_df.tweet_id,\n                            'text' : test_df.text,\n                            'lg_reg' : lg_lbl_pred})\nlg_lbl_pred_df.head()","4935a0e4":"lg_lbl_pred_df.to_csv('sentiments.csv', index=False)","d585a5e3":"# Let us start analysing the tweets, \n## We'll now clean up the text data\n\nfor this we'll follow the below steps:\n1. Remove all the special characters\n2. convert all letters to lower case\n3. filter out english stop words\n4. stemmer (optional)","d91918c4":"## TF-IDF","2711031f":"Conclusion\n\nLogistic regression is better model for predicting the results","43187405":"Reference: \nhttps:\/\/www.kaggle.com\/jiashenliu\/how-can-we-predict-the-sentiment-by-tweets","1bc8433e":"#### Below are the steps we'll be following :\n     1. Data preparation\n         a. analyze missing values\n         b. remove redundant columns\n     2. EDA:\n         a. Analyze different moods\n     3. Clean the tweet messages:\n         a. Remove all the special characters\n         b. remove all single characters\n         c. Substituting multiple spaces with single space\n         d. convert all letters to lower case\n         e. stemming words\n         f. filter out engish stop words \n     4. Test-Train split\n     5. Use TF-IDF technique to create features from text\n     6. Attempt model: Descision Tree\n     7. Model using Decision Tree, Random forest compare accuracy\n     \n         \n         ","f82dc516":"from above we can see that we have majority of negative comments (63%) followed by neutral (21%) and positive (16%)","cbfae5b9":"# Problem statment: \n# Twitter US Airline Sentiment\n\n##  Objective: Analyze how travelers in February 2015 expressed their feelings on Twitter\n\n\n#### In current data set we have tweets for 6 US airlines and we need to predict whether the tweets are positive, negative or neutral\n\nThis is a typical supervised learning task, where we are given a problem statement and we need to clasify them into pre-defined categories.","8b3f79d4":"Let us :\n1. now check total tweets for each of the airlines and\n2. how many of these tweets per airline are negative, positive and neutral","43ba6ec2":"From above graph we can see that\n1. United, US Airways and American have substatially negative tweets, these also have got over all more tweets\n2. Virgin America, Delta and Southwest have fairly balanced tweets","d23c68b8":"Let's convert tweet_created to datetime check if we can get any insights ","2cc6ea4d":"# Now we''ll apply model to predicit sentiments from tweet text data","8695a6b4":"we observe that 'thank', 'flight', 'great', 'will', 'awesome' 'love' are present more frequently in positve statements.\n\nwe'll do more detailed analysis below","c8ca6a6e":"As we can see majority tweets have said the reason as \n1. Customer servicec issue\n2. Late flight","d39203d7":"## EDA","f22d2198":"For American we have the tweets coming in from 22-02-2015 onwards","b884d09f":"#### missing value analysis:","1a26e630":"## Output","41fe1e56":"we observe that 'flight', 'hour', 'hrlp', 'time' 'hold', 'bag', 'plane' are present more frequently in negative statements. ","4666ce93":"we observe that airline_sentiment_gold, negativereason_gold and tweet_coord have more tha 90% of missing values, let us drop them as they don't provide any constructive feedback","e5657522":"## Make test-train split"}}