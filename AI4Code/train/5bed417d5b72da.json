{"cell_type":{"39585455":"code","b13ecc08":"code","4a055e3a":"code","ba9e9636":"code","50748f0b":"code","2c4b7ea2":"code","bfd9641c":"code","90235efe":"code","576a5861":"code","c8a76f39":"code","d6ee4872":"code","e6fec54a":"code","4a7f20f8":"code","1bea0467":"code","919f72b4":"code","59fa8a39":"code","f7defd5c":"code","eda91145":"code","00e5dd0f":"code","7599183f":"code","858741c8":"code","e7888179":"code","26d53f62":"code","f0d83f6e":"code","4efcc9ad":"code","7679d945":"code","2e67a60f":"code","5d562d53":"code","5947b256":"code","dd807383":"code","472a5b6b":"code","2157c247":"code","daa24861":"code","aef8702a":"code","cd40e2cd":"code","98fba4bd":"code","2a706664":"code","6bc62a93":"code","9daa01b7":"code","456742cc":"code","4e03df94":"code","efd37cad":"code","aaf5ec75":"code","c6bea7ee":"code","87474747":"code","7ea03a9c":"markdown","fa9cc789":"markdown","49a68eff":"markdown","91896a36":"markdown","72619c49":"markdown","adcf5e13":"markdown","90276f22":"markdown","f48c21a7":"markdown","b81ca05b":"markdown","50d02884":"markdown","f9620a0f":"markdown","cc190a87":"markdown"},"source":{"39585455":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom scipy import stats\n%matplotlib inline","b13ecc08":"df_train = pd.read_csv(\"\/\/kaggle\/\/input\/\/house-prices-advanced-regression-techniques\/\/train.csv\")\ndf_test = pd.read_csv(\"\/\/kaggle\/\/input\/\/house-prices-advanced-regression-techniques\/\/test.csv\")","4a055e3a":"df_train.head()","ba9e9636":"print(\"Column Names:\")\ndf_train.columns","50748f0b":"df_train[[\"MSZoning\",\"GarageArea\",\"GrLivArea\",\"SaleType\",\"SalePrice\"]].head(10)","2c4b7ea2":"sns.stripplot(x=\"OverallQual\", y=\"SalePrice\", data=df_train)","bfd9641c":"print(\"SALE PRICE SUMMARY\")\ndf_train[\"SalePrice\"].describe()","90235efe":"sns.displot(df_train[\"SalePrice\"], kde=True, aspect=1.5)","576a5861":"# Correlation Matrix\ncorr_matrix = df_train.corr()\n\nplt.figure(figsize = (9,8)) # figure size\nsns.heatmap(corr_matrix)","c8a76f39":"# Lets see 10 variables which have the highest correlation with Sale Price\nn_var = 11\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False).head(n_var)","d6ee4872":"# for these variables, finding column names and plotting correlation matrix\ntop_cols = corr_matrix.sort_values(by=\"SalePrice\", ascending=False).head(n_var).index\ntop_corr_matrix = df_train[top_cols].corr() # correlation matrix\n\nplt.figure(figsize = (6,5)) # figure size\nsns.heatmap(top_corr_matrix, annot=True, square=True)","e6fec54a":"# Scatter plots \ncols = [\"SalePrice\",\"OverallQual\",\"GrLivArea\",\n        \"GarageCars\",\"TotalBsmtSF\",\"FullBath\",\"YearBuilt\"]\nsns.pairplot(data=df_train[cols])","4a7f20f8":"# calculating number of missing values in each columns\ndf_train_miss = df_train.isnull().sum().sort_values(ascending=False).to_frame(\"N_MissVal\")\n\n# also i would like to see type of the columns (categorical \/ numerical)\ncat_cols = df_train.select_dtypes(include=['object']).columns # categorical columns\ndf_train_miss[\"ValType\"] = np.where(df_train_miss.index.isin(cat_cols), \"Categorical\", \"Numerical\")\n\n# printing columns that have missing values\ndf_train_miss[df_train_miss[\"N_MissVal\"]>0]","1bea0467":"# for columns that have more than 1 missing value, I will drop that column\nmissing_drops = df_train_miss[df_train_miss[\"N_MissVal\"]>1].index\ndf_train = df_train.drop(missing_drops,1)\n\n# Electrical column has only 1 missing data, so I will drop corresponding row\nrow_ind = df_train.loc[df_train['Electrical'].isnull()].index\ndf_train = df_train.drop(row_ind)","919f72b4":"#checking if there is any missing value left\nprint(\"Remaining missing values: {}\".format(df_train.isnull().sum().sum()))","59fa8a39":"print(\"Shape of data frame before dummies: {}\".format(df_train.shape))","f7defd5c":"df_train = pd.get_dummies(df_train)\nprint(\"Shape of data frame after dummies: {}\".format(df_train.shape))","eda91145":"df_train[\"SalePrice\"] = np.log(df_train[\"SalePrice\"])","00e5dd0f":"sns.displot(df_train[\"SalePrice\"], kde=True)\nfig = plt.figure()\nres = stats.probplot(df_train[\"SalePrice\"], plot=plt)","7599183f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score","858741c8":"X = df_train.drop([\"SalePrice\",\"Id\"],1)\ny = df_train[[\"SalePrice\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","e7888179":"regressor = LinearRegression()\nregressor.fit(X_train, y_train)","26d53f62":"y_pred = regressor.predict(X_test)\nplt.scatter(y_pred, y_test)\nplt.title(\"Linear regression\")\nplt.xlabel(\"Y - Predicted\")\nplt.ylabel(\"Y - Validation\")\nplt.xlim([10, 14])\nplt.ylim([10, 14])\nplt.show()","f0d83f6e":"rmse = np.sqrt(-cross_val_score(regressor,X_test,y_test,scoring =\"neg_mean_squared_error\",cv=5)).mean()\nprint(\"RMSE: {}\".format(rmse))","4efcc9ad":"# Dropping same columns as train dataset\ndf_test = df_test.drop(missing_drops,1)","7679d945":"df_test_miss = df_test.isnull().sum().sort_values(ascending=False).to_frame(\"N_MissVal\")\ndf_test_miss[df_test_miss[\"N_MissVal\"]>0]","2e67a60f":"# We have still missing values\n# So for numerical variables, i will fill na with median\ndf_test = df_test.fillna(df_test.median())\n\n# for categorical variables, i will fill with most occuring class\ndf_test = df_test.apply(lambda x: x.fillna(x.value_counts().index[0]))","5d562d53":"print(\"Remaining missing values: {}\".format(df_test.isnull().sum().sum()))","5947b256":"# And, getdummies:\ndf_test = pd.get_dummies(df_test)\nprint(\"Shape of train data frame after dummies: {}\".format(df_train.shape))\nprint(\"Shape of test data frame after dummies: {}\".format(df_test.shape))","dd807383":"miss_columns = list(set(df_train.columns) - set(df_test.columns))\nmiss_columns","472a5b6b":"df_test[miss_columns] = 0\ndf_test.head()","2157c247":"# reordering columns of test data, same as train data\ndf_test = df_test[df_train.columns]\ndf_test.head()","daa24861":"# So, we are ready\n# lets drop saleprice and id columns, and predict\nX_to_predict = df_test.drop([\"SalePrice\",\"Id\"],1)\ny_to_pred = regressor.predict(X_to_predict)","aef8702a":"result = pd.DataFrame({'Id': df_test.Id})\nresult[\"SalePrice\"] = y_to_pred\nresult.nlargest(5, 'SalePrice')","cd40e2cd":"# After prediction, we have a huge outlier at row 133\n# so i want to equalize it to second_max_value\nresult.loc[133,\"SalePrice\"] = 1 * result.loc[1089,\"SalePrice\"]\nresult.nlargest(2, 'SalePrice')","98fba4bd":"result.nsmallest(5, 'SalePrice')","2a706664":"# Same logic, for the smaller outliers\nresult.loc[1444,\"SalePrice\"] = 1 * result.loc[1431,\"SalePrice\"]\nresult.loc[675,\"SalePrice\"] = 1 * result.loc[1431,\"SalePrice\"]\nresult.loc[660,\"SalePrice\"] = 1 * result.loc[1431,\"SalePrice\"]\nresult.nsmallest(4, 'SalePrice')","6bc62a93":"# Finally, to obtain real sale values we have to invert log transformation --> exponential\nresult[\"SalePrice\"] = np.exp(result[\"SalePrice\"])","9daa01b7":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X_train, y_train.values.ravel())\n\nrmse = np.sqrt(-cross_val_score(regressor,X_test,y_test.values.ravel(),scoring =\"neg_mean_squared_error\",cv=5)).mean()\nprint(\"RMSE: {}\".format(rmse))","456742cc":"X_to_predict = df_test.drop([\"SalePrice\",\"Id\"],1)\ny_to_pred = regressor.predict(X_to_predict)","4e03df94":"result = pd.DataFrame({'Id': df_test.Id})\nresult[\"SalePrice\"] = y_to_pred\nresult.nlargest(5, 'SalePrice')","efd37cad":"result.nsmallest(5, 'SalePrice')","aaf5ec75":"# Finally, to obtain real sale values we have to invert log transformation --> exponential\nresult[\"SalePrice\"] = np.exp(result[\"SalePrice\"])","c6bea7ee":"result.head()","87474747":"result.to_csv('submission.csv', index=False)","7ea03a9c":"# Exploratory Data Analysis","fa9cc789":"From the summary and the distribution plot, we can say that\n* Sale Price is a **numerical** variable and consists of **positive** values.\n* The **average** house price is **180k**\n* **Minimum** house price is **34.9k** and **maximum** is **755k**\n* The distribution is **positively skewed**\n* Even though %75 percentile is 214k, maximum price is much higher.  So there may be **outliers** in high values.","49a68eff":"**Sale Price Transformation**\n\nSince sale price is a positively skewed variable, we will apply log transformation so that the distribution will converge to Gauss distribution.","91896a36":"# MODELING\nSince this is my first project on Kaggle, i will first stick with linear reggession and see how it performs.","72619c49":"**Missing Value Imputation**","adcf5e13":"**Converting Categorical Variables into Dummies**","90276f22":"# House Prices - Regression Problem\n\nHi,\n\nThis is my first Kaggle Notebook for \"House Prices - Advanced Regression Techniques\" competition.\n\nLet's start with importing packages, loading and trying to understand the data.","f48c21a7":"**Predicting Test Dataset**","b81ca05b":"# MODELING 2 - Random Forest Regression\n\nFor the first try, I used Linear Regression model and obtained score 0.16522 (V2)\n\nNow, I would like to check how Random Forest performs.","50d02884":"# Submission","f9620a0f":"# Sale Price Analysis","cc190a87":"After transforming categorical variables, we have less columns compared to train data set.\n\nThat's because there are more values in categorical columns of train data set.\n\nIn order to apply my model to test data set, i will find missing columns, create and fill with 0."}}