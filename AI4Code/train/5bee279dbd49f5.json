{"cell_type":{"95790c5a":"code","be87fe8d":"code","8f95aa58":"code","73a9f3dd":"code","15047813":"code","dfa2655e":"code","ad599cd0":"code","b8cfa09f":"code","61752b79":"code","beaec5af":"code","98f0dd83":"code","fad2c7b8":"code","49899f22":"code","64914108":"code","0aca00d9":"code","198df351":"code","1792e5c5":"code","2b17cec8":"markdown","af67c517":"markdown","6383cf1a":"markdown","1e024797":"markdown","582631b2":"markdown","4027aa8c":"markdown","6d54a1e3":"markdown","94ff66f8":"markdown"},"source":{"95790c5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be87fe8d":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_df.drop('Id', axis = 1, inplace = True)\ntest_df.drop('Id', axis = 1, inplace = True)","8f95aa58":"#Simple EDA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nEDA_df = train_df.drop('SalePrice', axis = 1)\nprint('Basic Information')\ndisplay(train_df.info())","73a9f3dd":"print('Column length : {}'.format(len(EDA_df.columns)))\n#Null values counts -> PoolQC, \nprint('Null values counts')\nnull_df = EDA_df.isnull().sum().to_frame().rename(columns = {0 : 'counts'})\nnull_df = null_df.loc[null_df['counts'] != 0].sort_values('counts', ascending = False)\nnull_df = null_df \/ len(train_df)\nnull_high = list(null_df.index[:5])\ncolors = sns.color_palette('hls',len(null_df.index))\nplt.figure(figsize = (20,5))\nplt.title('Null value counts percentage')\nplt.bar(null_df.index, null_df.counts, color = colors)\nplt.xticks(rotation = 45)\nplt.show()\n\n#Intensive EDA : varibles with high null value count\nprice_diff = []\nplt.figure(figsize = (35,5))\nfor n, col in enumerate(null_high):\n    x,y = EDA_df.groupby(col).size().index, EDA_df.groupby(col).size()\n    plt.subplot(1,len(null_high), n+1)\n    plt.title(col)\n    #plt.title('total length : {}'.format(len(EDA_df.loc[EDA_df[col].isnull() == False])))\n    colors = sns.color_palette('hls',len(x))\n    plt.bar(x,y, color = colors)\n    price_diff.append(abs(round(train_df.loc[train_df[col].isnull() == False].SalePrice.mean()) - round(train_df.loc[train_df[col].isnull() == True].SalePrice.mean())))\nplt.show()\n\nplt.figure(figsize = (20,5))\nplt.title('The difference in average price depending on the existence of each factors')\ncolors = sns.color_palette('hls',len(null_high))\nplt.bar(null_high, price_diff, color = colors)\nplt.show()","15047813":"#Seperate columns\ncols = EDA_df.columns\ncat = [col for col in cols if EDA_df[col].dtype == 'object']\nnum = [col for col in cols if EDA_df[col].dtype != 'object']\n#Categorical columns EDA\nnunique = EDA_df[cat].nunique().sort_values(ascending = False).to_frame().rename(columns = {0 : 'counts'})\ncolors = sns.color_palette('hls',len(nunique.index))\nplt.figure(figsize = (20,5))\nplt.title('Cat columns nunique')\nplt.bar(nunique.index, nunique.counts, color = colors)\nplt.xticks(rotation = 45)\nplt.show()\n\n#Intensive EDA : Neighborhood\n#Sale Price by location\nplt.figure(figsize = (20,5))\nplt.title('Sale Price by location')\ncolors = sns.color_palette('hls',len(train_df.groupby('Neighborhood').SalePrice.mean()))\nsale_price_by_location = train_df.groupby('Neighborhood').SalePrice.mean().to_frame().sort_values('SalePrice', ascending = False)\nplt.bar(sale_price_by_location.index, sale_price_by_location['SalePrice'], color = colors)\nplt.xticks(rotation = 45)\nplt.show()","dfa2655e":"def process_null(df):\n    before = sum(df.isnull().sum())\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        df[col] = df[col].fillna('None')\n    df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)\n\n    df['MasVnrType'] = df['MasVnrType'].fillna('None')\n    df['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n\n    for col in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']:\n        df[col] = df[col].fillna('None')\n\n    for col in ['BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual']:\n        df[col] = df[col].fillna('None')\n\n    df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode())\n    df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].median())\n\n    total_null = sum(df.isnull().sum())\n    print('Sum of total null : {} ---> {}'.format(before,total_null))\n    \nprocess_null(EDA_df)","ad599cd0":"#Outlier Detector #1 (IQR, Box-plot)\nplt.figure(figsize = (20,20))\nfor n,num_ in enumerate(num):\n    plt.subplot(6,6,n+1)\n    plt.boxplot(EDA_df[num_], sym = 'rv')\n    plt.xticks([1], [num_])\nplt.show()","b8cfa09f":"#Time series \nfrom datetime import datetime\ndef to_time(df):\n    year = str(df['YrSold'])\n    month = str(df['MoSold'])\n    day = '01'\n    \n    if len(month) == 1:\n        date =  year+'-0'+month+'-'+day\n    else:\n        date = year+'-'+month+'-'+day\n    return datetime.strptime(date, '%Y-%m-%d')\ntrain_df['ts'] = train_df.apply(to_time, axis = 1)\n\n\n#mean sale price by year, month\nplt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.title('Average Sale Price by Year')\ncolors = sns.color_palette('hls',len(train_df.groupby(train_df['ts'].dt.year).mean().SalePrice))\nplt.bar(train_df.groupby(train_df['ts'].dt.year).mean().SalePrice.index, train_df.groupby(train_df['ts'].dt.year).mean().SalePrice, color = colors)\nplt.xticks(rotation = 45)\nplt.subplot(1,2,2)\nplt.title('Average Sale Price by Month')\ncolors = sns.color_palette('hls',len(train_df.groupby(train_df['ts'].dt.month).mean().SalePrice))\nplt.bar(train_df.groupby(train_df['ts'].dt.month).mean().SalePrice.index, train_df.groupby(train_df['ts'].dt.month).mean().SalePrice, color = colors)\nplt.show()\n#variance sale price by year, month\nplt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.title('Sale Price Variance by Year')\ncolors = sns.color_palette('hls',len(train_df.groupby(train_df['ts'].dt.year).var().SalePrice))\nplt.bar(train_df.groupby(train_df['ts'].dt.year).mean().SalePrice.index, train_df.groupby(train_df['ts'].dt.year).var().SalePrice, color = colors)\nplt.xticks(rotation = 45)\nplt.subplot(1,2,2)\nplt.title('Sale Price variance by Month')\ncolors = sns.color_palette('hls',len(train_df.groupby(train_df['ts'].dt.month).var().SalePrice))\nplt.bar(train_df.groupby(train_df['ts'].dt.month).mean().SalePrice.index, train_df.groupby(train_df['ts'].dt.month).var().SalePrice, color = colors)\nplt.show()\n#number of transactions by year, month\nplt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.title('number of transactions by Year')\ncolors = sns.color_palette('hls',len(train_df.groupby(train_df['ts'].dt.year).size()))\nplt.bar(train_df.groupby(train_df['ts'].dt.year).mean().SalePrice.index, train_df.groupby(train_df['ts'].dt.year).size(), color = colors)\nplt.xticks(rotation = 45)\nplt.subplot(1,2,2)\nplt.title('number of transactions by Month')\ncolors = sns.color_palette('hls',len(train_df.groupby(train_df['ts'].dt.month).size()))\nplt.bar(train_df.groupby(train_df['ts'].dt.month).mean().SalePrice.index, train_df.groupby(train_df['ts'].dt.month).size(), color = colors)\nplt.show()\n\n#2006 ~ 2010 mean sale price\n\ndf_2006 = train_df.loc[train_df['YrSold'] == 2006].groupby(train_df['ts'].dt.month).mean().SalePrice\ndf_2007 = train_df.loc[train_df['YrSold'] == 2007].groupby(train_df['ts'].dt.month).mean().SalePrice\ndf_2008 = train_df.loc[train_df['YrSold'] == 2008].groupby(train_df['ts'].dt.month).mean().SalePrice\ndf_2009 = train_df.loc[train_df['YrSold'] == 2009].groupby(train_df['ts'].dt.month).mean().SalePrice\ndf_2010 = train_df.loc[train_df['YrSold'] == 2010].groupby(train_df['ts'].dt.month).mean().SalePrice\nplt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.title('Average Sale Price by month in 2006')\ncolors = sns.color_palette('hls',12)\nplt.bar(df_2006.index, df_2006, color = colors)\nplt.subplot(1,2,2)\nplt.title('Average Sale Price by month in 2007')\ncolors = sns.color_palette('hls',12)\nplt.bar(df_2007.index, df_2007, color = colors)\nplt.show()\nplt.figure(figsize = (20,5))\nplt.subplot(1,3,1)\nplt.title('Average Sale Price by month in 2008')\ncolors = sns.color_palette('hls',12)\nplt.bar(df_2008.index, df_2008, color = colors)\nplt.subplot(1,3,2)\nplt.title('Average Sale Price by month in 2009')\ncolors = sns.color_palette('hls',12)\nplt.bar(df_2009.index, df_2009, color = colors)\nplt.subplot(1,3,3)\nplt.title('Average Sale Price by month in 2010')\ncolors = sns.color_palette('hls',12)\nplt.bar(df_2010.index, df_2010, color = colors)\nplt.show()\n\n#Jan has big variance for sale price - why?\nplt.title('Jan big variance why?')\nplt.plot(train_df[train_df['ts'].dt.month == 1].SalePrice, 'o')\nplt.axhline(y=train_df[train_df['ts'].dt.month == 1].SalePrice.mean(), color='r', linewidth=2)\nplt.show()\nout_idx = train_df[train_df['ts'].dt.month == 1].SalePrice.sort_values(ascending = False).head(3).index\npd.set_option('display.max_columns', None)\ndisplay(train_df.iloc[out_idx])","61752b79":"#Correlation (Heat-map)\n\ncorr_df = EDA_df[num].corr()\nfig, ax = plt.subplots( figsize=(20,10) )\n\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.title('Correlation Heatmap for numerical')\nsns.heatmap(corr_df, \n            cmap = 'RdYlBu_r', \n            annot = False,  \n            mask=mask,      \n            linewidths=.5,  \n            cbar_kws={\"shrink\": .5},\n            vmin = -1,vmax = 1   \n           )  \nplt.show()\n\n#Correlation score (>0.75, <-0,75)\nscore = corr_df.stack().sort_values(ascending = False).to_frame().rename(columns = {0 : 'score'})\nhigh_corr = score.loc[(score.score != 1) & ((score.score > 0.75) | (score.score < -0.75))]\ndisplay(high_corr)","beaec5af":"#Drop useless columns\ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_df.drop('Id', axis = 1, inplace = True)\ntest_df.drop('Id', axis = 1, inplace = True)\nprint('train_df :',train_df.shape, '\\ntest_df :',test_df.shape)","98f0dd83":"#Process Null values\n#Null change to None\n\n\nto_cat = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', \n          'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', \n          'Fireplaces', 'GarageCars', 'GarageYrBlt']\ncat = [col for col in cols if EDA_df[col].dtype == 'object']\nnum = [col for col in cols if EDA_df[col].dtype != 'object']\nfor col in to_cat:\n    cat.append(col)\n    num.remove(col)\n    \ntrain_df[cat] = train_df[cat].astype('object')\n\nnull_col = list(null_df.index)\n#Garage \/ MasVnr Group fill 'None'\ndef process_null(df):\n    before = sum(df.isnull().sum())\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageYrBlt', 'GarageCars', ]:\n        df[col] = df[col].fillna('None')\n\n    df['MasVnrType'] = df['MasVnrType'].fillna('None')\n    df['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n\n    for col in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']:\n        df[col] = df[col].fillna('None')\n\n    for col in ['BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual','BsmtFullBath', 'BsmtHalfBath']:\n        df[col] = df[col].fillna('None')\n    \n    for col in cat:\n        df[col] = df[col].fillna(df[col].mode()[0])\n        \n    for col in num:\n        df[col] = df[col].fillna(df[col].median())\n\n    total_null = sum(df.isnull().sum())\n    print('Sum of total null : {} ---> {}'.format(before,total_null))\n    \nprocess_null(train_df)\nprocess_null(test_df)","fad2c7b8":"#Process Outlier\n#Outlier Detector (Isolation Forest)\nfrom sklearn.ensemble import IsolationForest\nclf=IsolationForest(n_estimators=50, max_samples=50, contamination=float(0.004), \n                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=None, verbose=0,behaviour=\"new\")\n# 50\uac1c\uc758 \ub178\ub4dc \uc218, \ucd5c\ub300 50\uac1c\uc758 \uc0d8\ud50c\n# 0.04%\uc758 outlier \uc0c9\ucd9c.\nclf.fit(train_df[num])\npred = clf.predict(train_df[num])\ntrain_df['anomaly']=pred\noutliers=train_df.loc[train_df['anomaly']==-1]\ntrain_df = train_df.loc[train_df['anomaly'] == 1]\ntrain_df.drop('anomaly', axis = 1, inplace = True)\noutlier_index=list(outliers.index)\nprint('outlier index : ', outlier_index)\n#Find the number of anomalies and normal points here points classified -1 are anomalous\nprint('Complete removing outlier row')","49899f22":"#Dummy & Scaling (MinMaxScaler)\nfrom sklearn.preprocessing import MinMaxScaler\nx_train = pd.get_dummies(train_df.drop('SalePrice', axis = 1), columns = cat)\ntest_df = pd.get_dummies(test_df, columns = cat)\nx_train, test_df = x_train.align(test_df, fill_value = 0, axis = 1)\nscaler = MinMaxScaler()\nx_train[num] = scaler.fit_transform(x_train[num])\ntest_df[num] = scaler.fit_transform(test_df[num])\nprint('train_df :',train_df.shape, '\\ntest_df :',test_df.shape)","64914108":"#Data Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_train, train_df['SalePrice'], test_size = 0.33, random_state = 777)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","0aca00d9":"def get_score(predict):\n    r2 = r2_score(y_test, predict)\n    mae = mean_absolute_error(y_test, predict)\n    mse = mean_squared_error(y_test, predict)\n    rmse = mse ** 0.5\n    return r2,mae,mse,rmse","198df351":"#Training&evaluation\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,r2_score, mean_squared_error\n\n\nrf_rg = RandomForestRegressor(random_state= 26)\nrf_rg.fit(X_train, y_train)\nrf_rg_predict = rf_rg.predict(X_test)\nrf_r2, rf_mae, rf_mse, rf_rmse = get_score(rf_rg_predict)\nrf_list = [rf_r2, rf_mae, rf_mse, rf_rmse]\n\nxgb_rg = XGBRegressor(random_state = 26)\nxgb_rg.fit(X_train, y_train)\nxgb_rg_predict = xgb_rg.predict(X_test)\nxgb_r2, xgb_mae, xgb_mse, xgb_rmse = get_score(xgb_rg_predict)\nxgb_list = [xgb_r2,xgb_mae,xgb_mse,xgb_rmse]\n\ntrain_ds = lgb.Dataset(X_train, label = y_train) \ntest_ds = lgb.Dataset(X_test, label = y_test)\nparams = {'learning_rate': 1e-3, \n          'max_depth': 3,\n          'boosting': 'gbdt', \n          'objective': 'regression', \n          'metric': 'rmse', \n          'is_training_metric': True, \n          'num_leaves': 15, \n          'feature_fraction': 0.9, \n          'bagging_fraction': 0.7, \n          'bagging_freq': 5, \n          'seed':26,\n         'verbose' : -1}\nmodel = lgb.train(params, train_ds, 50000, test_ds, verbose_eval=1000, early_stopping_rounds=200)\nlgb_rg_predict = model.predict(X_test)\nlgb_r2, lgb_mae, lgb_mse, lgb_rmse = get_score(lgb_rg_predict)\nlgb_list = [lgb_r2,lgb_mae,lgb_mse,lgb_rmse]\n\npd.options.display.float_format = '{:.5f}'.format\nidx = ['R2', 'MAE', 'MSE', 'RMSE']\npd.DataFrame({'RF' : rf_list, 'XGB' : xgb_list, 'LGB' : lgb_list}, index = idx)","1792e5c5":"#submission\npredict = model.predict(test_df)\nsub_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub_df['SalePrice'] = predict\nsub_df.to_csv('submission.csv', index = False)","2b17cec8":"> # **Training & Evaluation!!!**\n\n* **Training ML with RandomForestRegressor \/ XGBoost \/ Lightgbm**\n\n* **Finding hyperparameter (Grid Search)**\n\n* **Submission**\n","af67c517":"> # Simple EDA\n\n* Basic information\n* Null values count\n* Outlier\n* Check by timeseires","6383cf1a":"* Most of variables has suitable number of unique. However, like Neighborhood, Exterior2nd and Exterior1st, these need to be taken measure some preprocessing (bidding, clustering ..)","1e024797":"Insight...\n* Average Sale price by year, month : Seasonality is pretty stable (No effect to Sale price)\n* Variance Sale price by year, month : Jan has big variance for sale price -> some trasactions have high sale price (one of them is maxium price)\n* Number of transaction by year, month : July, 2010 has shortest data record becuase Data record in 2010 is untill July. You can check high record of transaction btw Late of spring ~ early of summer.\n* Average Sale Price by month for each year : It is generally stable.","582631b2":"* found outlier with isolation Forest : [197, 520, 691, 803, 1298, 1386] -> only 6 records (consider removing)\n* OverallQual, OverallCond, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageYrBlt : though there are categorical variable","4027aa8c":"Insight...\n* High score pair : need to figure out the reason and consider whethere creating new one or remove one of them\\\n\nshould be kept both one firstly. (will be removed)","6d54a1e3":"Insight...\n* Column Length : Considering with transforming to dummy from categorical variable, it should do feature engineering (PCA , SVD)\n\nNull values process..\n* Electrical (mode), LotFrontage (median) -> these are simple null value (small quantity)\n* categorical : PoolQC, MiscFeature, Alley, Fence, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, BsmtExposure, BsmtFinType2, BsmtFinType1, BsmtCond, BsmtQual, MasVnrType (Nan -> 'None')\n* numerical : GarageYrBlt, MasVnrArea (Nan -> 0)","94ff66f8":"> # **Preprocessing**\n\n* **Drop useless columns**\n\n* **Null values**\n\n* **Process Outlier**\n\n* **Dummy & Scaling**\n\n* **Data Splite**"}}