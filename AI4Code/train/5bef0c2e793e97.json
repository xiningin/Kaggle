{"cell_type":{"8ff709ee":"code","c0098e55":"code","8db7a148":"code","7479a94b":"code","7709f0de":"code","a99663f6":"code","27f1833b":"code","667aeccb":"code","c44a2658":"code","176202d4":"code","ecfa0e4c":"code","db9b265f":"code","5e418321":"code","8238f80d":"code","6c094f6b":"code","0b5f0d78":"code","e5ffc71a":"code","0b6ae3b6":"code","9e9c8854":"code","3126f1c8":"code","ad176912":"code","5aea59b3":"code","54e3a18a":"code","491f9e03":"code","af7626f7":"code","b9440cb7":"code","2be3423b":"markdown","889b3110":"markdown","853f82e7":"markdown","3f1cd90e":"markdown","64dafb9f":"markdown","b9cf3e9d":"markdown","bfb034f9":"markdown","a3f3e166":"markdown","80a2468d":"markdown","d0764af7":"markdown","7ff7f1f3":"markdown","4f617f70":"markdown","997fb314":"markdown","e079ac93":"markdown","2049eda9":"markdown","412f6b79":"markdown","4b723fdd":"markdown","a73d898f":"markdown","bb7dcb4c":"markdown","c9ba5717":"markdown","bfba3f47":"markdown","e0da1223":"markdown","e591c849":"markdown","f95ed4b4":"markdown","6c06bbcc":"markdown"},"source":{"8ff709ee":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Important libraries loaded successfully\")","c0098e55":"data_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nprint(\"Data shape = \",data_train.shape)\ndata_train.head()","8db7a148":"#get total count of data including missing data\ntotal = data_train.isnull().sum().sort_values(ascending=False)\n\n#get percent of missing data relevant to all data\npercent = (data_train.isnull().sum()\/data_train.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(data_train.shape[1])","7479a94b":"data_train = data_train.drop(['location','keyword'], axis=1)\nprint(\"location and keyword columns droped successfully\")","7709f0de":"data_train = data_train.drop('id', axis=1)\nprint(\"id column droped successfully\")","a99663f6":"data_train.columns","27f1833b":"data_train[\"text\"].head(10)","667aeccb":"corpus  = []\npstem = PorterStemmer()\nfor i in range(data_train['text'].shape[0]):\n    #Remove unwanted words\n    tweet = re.sub(\"[^a-zA-Z]\", ' ', data_train['text'][i])\n    #Transform words to lowercase\n    tweet = tweet.lower()\n    tweet = tweet.split()\n    #Remove stopwords then Stemming it\n    tweet = [pstem.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    #Append cleaned tweet to corpus\n    corpus.append(tweet)\n    \nprint(\"Corpus created successfully\")    ","c44a2658":"print(pd.DataFrame(corpus)[0].head(10))","176202d4":"rawTexData = data_train[\"text\"].head(10)\ncleanTexData = pd.DataFrame(corpus, columns=['text after cleaning']).head(10)\n\nframes = [rawTexData, cleanTexData]\nresult = pd.concat(frames, axis=1, sort=False)\nresult","ecfa0e4c":"#Create our dictionary \nuniqueWordFrequents = {}\nfor tweet in corpus:\n    for word in tweet.split():\n        if(word in uniqueWordFrequents.keys()):\n            uniqueWordFrequents[word] += 1\n        else:\n            uniqueWordFrequents[word] = 1\n            \n#Convert dictionary to dataFrame\nuniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\nuniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\nuniqueWordFrequents.head(10)","db9b265f":"uniqueWordFrequents['Word Frequent'].unique()","5e418321":"uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\nprint(uniqueWordFrequents.shape)\nuniqueWordFrequents","8238f80d":"counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\nbagOfWords = counVec.fit_transform(corpus).toarray()","6c094f6b":"X = bagOfWords\ny = data_train['target']\nprint(\"X shape = \",X.shape)\nprint(\"y shape = \",y.shape)\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.20, random_state=55, shuffle =True)\nprint('data splitting successfully')","0b5f0d78":"decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = None, \n                                           splitter='best', \n                                           random_state=55)\n\ndecisionTreeModel.fit(X_train,y_train)\n\nprint(\"decision Tree Classifier model run successfully\")","e5ffc71a":"gradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 100,\n                                                   max_depth = 30,\n                                                   random_state=55)\n\ngradientBoostingModel.fit(X_train,y_train)\n\nprint(\"gradient Boosting Classifier model run successfully\")","0b6ae3b6":"KNeighborsModel = KNeighborsClassifier(n_neighbors = 7,\n                                       weights = 'distance',\n                                      algorithm = 'brute')\n\nKNeighborsModel.fit(X_train,y_train)\n\nprint(\"KNeighbors Classifier model run successfully\")","9e9c8854":"LogisticRegression = LogisticRegression(penalty='l2', \n                                        solver='saga', \n                                        random_state = 55)  \n\nLogisticRegression.fit(X_train,y_train)\n\nprint(\"LogisticRegression Classifier model run successfully\")","3126f1c8":"SGDClassifier = SGDClassifier(loss = 'hinge', \n                              penalty = 'l1',\n                              learning_rate = 'optimal',\n                              random_state = 55, \n                              max_iter=100)\n\nSGDClassifier.fit(X_train,y_train)\n\nprint(\"SGDClassifier Classifier model run successfully\")","ad176912":"SVClassifier = SVC(kernel= 'linear',\n                   degree=3,\n                   max_iter=10000,\n                   C=2, \n                   random_state = 55)\n\nSVClassifier.fit(X_train,y_train)\n\nprint(\"SVClassifier model run successfully\")","5aea59b3":"bernoulliNBModel = BernoulliNB(alpha=0.1)\nbernoulliNBModel.fit(X_train,y_train)\n\nprint(\"bernoulliNB model run successfully\")","54e3a18a":"gaussianNBModel = GaussianNB()\ngaussianNBModel.fit(X_train,y_train)\n\nprint(\"gaussianNB model run successfully\")","491f9e03":"multinomialNBModel = MultinomialNB(alpha=0.1)\nmultinomialNBModel.fit(X_train,y_train)\n\nprint(\"multinomialNB model run successfully\")","af7626f7":"modelsNames = [('LogisticRegression',LogisticRegression),\n               ('SGDClassifier',SGDClassifier),\n               ('SVClassifier',SVClassifier),\n               ('bernoulliNBModel',bernoulliNBModel),\n               ('multinomialNBModel',multinomialNBModel)]\n\nvotingClassifier = VotingClassifier(voting = 'hard',estimators= modelsNames)\nvotingClassifier.fit(X_train,y_train)\nprint(\"votingClassifier model run successfully\")","b9440cb7":"#evaluation Details\nmodels = [decisionTreeModel, gradientBoostingModel, KNeighborsModel, LogisticRegression, \n          SGDClassifier, SVClassifier, bernoulliNBModel, gaussianNBModel, multinomialNBModel, votingClassifier]\n\nfor model in models:\n    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n    \n    y_pred = model.predict(X_test)\n    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n    print('--------------------------------------------------------------------------')","2be3423b":"## 4.4 Logistic Regression Model","889b3110":"## 3.5 Create sparse matrix ( Bag of words )\n**Bag of word** contain only unique words in corpus.","853f82e7":"## 4.3 K - Nearest Neighbors Model","3f1cd90e":"## 4.8 Gaussian Naive Bayes Model","64dafb9f":"## 4.2 Gradient Boosting Model","b9cf3e9d":"As we see in above table almost 33% of **location** column is missing and very littel percentage of **keyword** column is missing.<br>\n\n## 2.2 How to Handle Missing Data ?\nOne of the most common problems we have faced in Data Analysis is handling the missing values.<br>\n\nI love put this image in my kernels because it give a roadmap to handle **missing data** \n<img src='https:\/\/miro.medium.com\/max\/1528\/1*_RA3mCS30Pr0vUxbp25Yxw.png' width=\"550px\" style='float:left;'>\n<div style='clear:both'><\/div>\n<br>\n\nIn **Deletion** I will use **Deleting Columns** technique. Now we will drop **location** and **keyword** columns.","bfb034f9":"# Real or Not? NLP with Disaster Tweets\n<br>\n<img src='https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTbmtImbdYE8HEt_GzzxuWvTAXcNTzdk-vC0q3q5wtzbWniXvQG' alt='twitter' style='float:left' width=100%>\n<div style='clear:both'><\/div>\n<hr>\n**Welcome all \ud83d\ude0a**<br>\n\nIn this kernel we will go together into **Disaster Tweets** data to learn how use basic natural language processing **NLP** techniques<br>\n\nThis kernel will be devided into the following parts<br>\n<ol>\n    <li><b>Data Exploration<\/b><\/li>\n    <li><b>Data Preprocessing<\/b><\/li>\n    <li><b>Basic NLP Techniques<\/b><\/li>\n    <li><b>Models Bulding<\/b><\/li>\n    <li><b>Models evaluation<\/b><\/li>\n<\/ol>\nNow we will import libraries and load our data.","a3f3e166":"Now we only have text and target columns only. let's make sure","80a2468d":"# 5 Models evaluation\n\nNow we will evaluate our model using **f1_score** let's go. ","d0764af7":"As we see some words repeated a lot and others repeated less, so we will get only words that repeated more than or equal 20 once.","7ff7f1f3":"We all know that **id** column isn't important to us, so we will drop it","4f617f70":"## 4.1 Decision Tree Model","997fb314":"## 4.6 Support Vector Machine Model","e079ac93":"# 1. Data Exploration","2049eda9":"# 4. Models Bulding\nNow we will build our models, we will use following models\n* Decision Tree Model\n* Gradient Boosting Model\n* K - Nearest Neighbors Model\n* Logistic Regression Model\n* Stochastic Gradient Descent Model\n* Support Vector Machine Model\n* Bernoulli Naive Bayes Model\n* Gaussian Naive Bayes Model\n* Multinomial Naive Bayes Model\n* Voting Classifier Model\n\nBut before using it we will split our data to train and test set first.","412f6b79":"# 3. Basic NLP Techniques\n<img src='https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcT6TkPpD8nWsbTVa9ExwfCQUnFmzkNE8zjZJ3uXSaBVd09ErhvZ' alt='text preprocessing' style='float:left' width=50% >\n<div style='clear:both'><\/div>\n<hr>\nBefore starting text preprocess steps we must we must know two terms **Corpus and Bag of word.**<br>\n\n**Corpus :** Is a large and structured set of texts, We can consider it as simplified version of our text data that contain clean and benefit data.<br>\n\n**Bag of word :** In practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the text [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model)<br>\n\nNow we will do the following steps to preprocess our text data\n<ol>\n    <li><b>Remove unwanted words<\/b><\/li>\n    <li><b>Transform words to lowercase<\/b><\/li>\n    <li><b>Remove stopwords<\/b><\/li>\n    <li><b>Stemming words<\/b><\/li>\n    <li><b>Create sparse matrix ( Bag of words )<\/b><\/li>\n<\/ol>  \nNow let's deal with our **text** column by exploar it.","4b723fdd":"## 4.7 Bernoulli Naive Bayes Model","a73d898f":"## 3.1 Remove unwanted words\nAs we see our **text** column contain unwanted words as **#, =>, numbers, or ... etc** these letters will not be useful in our problem so we will get only pure text without any markings or numbers.<br>\n\nWe will do it by **specify** our pattern using **re** library.\n\n## 3.2 Transform words to lowercase\nWe must transform words to lowercase because each letter has own **ASCII Code** that represent text in computers, Uppercase letter has different ASCII Code than same letter in lowercase format. **so that** 'A' letter differ from 'a' letter in computer.\n\n## 3.3 Remove stopwords\n**Stop words :** are generally the most common words in a language, so we will remove it to prevent misleading problem in our model.\n\n## 3.4 Stemming words\n**stemming :** is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Stemming)<br>\nWe use stemming to reduce **bag of words** dimensionality.","bb7dcb4c":"## 4.9 Multinomial Naive Bayes Model","c9ba5717":"As we see in above table we have some missing values. So let's deal with it\n\n# 2. Data Preprocessing\nData Preprocessing one of important steps in any data science or machine learning project so let's start.\n## 2.1 Missing Data","bfba3f47":"**Let's explore corpus, and discover the difference between raw and clean text data**","e0da1223":"## 4.5 Stochastic Gradient Descent Model","e591c849":"<p style='font-size:25px;font-weight:bold'>Please If you find this kernel useful, upvote it to help others see it \ud83d\ude0a<\/p>","f95ed4b4":"As we know that there some words that repeated so little in our tweets, so we must remove these words from our **Bag of words** to decrease dimensionality as possible.<br>\n\nWe will do it by create dictionary where **key** refer to **word** and **value** refer to **word frequents in all tweets**.","6c06bbcc":"## 4.10 Voting Classifier Model"}}