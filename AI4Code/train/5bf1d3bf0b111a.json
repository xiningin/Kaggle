{"cell_type":{"73717209":"code","3a591738":"code","cd417aaf":"code","f17afe98":"code","a78960f1":"code","1fdabab9":"code","a52bdd3c":"code","d477e297":"code","6ec7f033":"code","8f19aeea":"code","a738a6c3":"code","3537fd20":"code","e9674128":"code","236701cb":"code","124c6da3":"code","2179373f":"code","1e9fa666":"code","06afcd66":"code","4e7f6391":"code","bd4012bc":"code","c37fd844":"code","ccc8e6c0":"code","c9e38382":"code","5e5b52ba":"code","88b55150":"code","52dbd430":"code","a72dec6d":"code","5184c194":"code","75212295":"code","422a3eb2":"code","88a22495":"code","0898521b":"code","5afc299c":"code","a819de58":"code","8becb351":"code","ab0cd734":"code","fbacc5f5":"code","4cdd4e3b":"code","e6b9c008":"code","08eba488":"code","e2bf1b60":"code","e07770c1":"markdown","223826ed":"markdown","480d9d53":"markdown","3ca9663a":"markdown","4ddec775":"markdown","b84046b5":"markdown","dd89b8b3":"markdown","8d36ad58":"markdown","a82e77d2":"markdown","fd6d4937":"markdown","35cbf2ff":"markdown","fd7882c1":"markdown","7a21d1d5":"markdown","302b8429":"markdown","bd91f7fc":"markdown","7a2b9015":"markdown","4298452a":"markdown","ffe32e4c":"markdown","d037703d":"markdown","c2bd0488":"markdown","089ee3bd":"markdown","d7fe9096":"markdown","107510c9":"markdown","e4f31b58":"markdown","f8a74555":"markdown","a5336fcc":"markdown","7cdbe51e":"markdown","113a5563":"markdown","e9f62ce5":"markdown","f80386c4":"markdown","5619d3b5":"markdown","1fc1c541":"markdown","36818422":"markdown","8fe02247":"markdown","ecf69fca":"markdown","51049d52":"markdown","1036f8ba":"markdown","0836756f":"markdown","4bc50d5b":"markdown","631d9083":"markdown","577b90b6":"markdown","8c5ac047":"markdown","f5254575":"markdown","035acf9b":"markdown","1ccca6ba":"markdown","51980a11":"markdown","21094fe3":"markdown","305f0f23":"markdown","89dd570f":"markdown","f1902243":"markdown","bcf809a5":"markdown","84e26148":"markdown","6b1e0338":"markdown","dadf3dbc":"markdown","63b61aa4":"markdown","94840d59":"markdown","65a2d026":"markdown","0f4c763f":"markdown","12c2ed92":"markdown","3d7f2496":"markdown","b9c44980":"markdown","2bc92a8f":"markdown","c15bb919":"markdown","01800253":"markdown","21744341":"markdown","f88807f2":"markdown","7af98e7a":"markdown","d010b0fa":"markdown","07c32d83":"markdown","dd03efe8":"markdown","2c9af757":"markdown","0bd76d73":"markdown","6fadd4a5":"markdown","7e745e1a":"markdown","ebd0b055":"markdown","60a351ee":"markdown","f9aa9613":"markdown","0fbd292b":"markdown","09051187":"markdown","224aba8f":"markdown","980a44db":"markdown","ac8f7a3e":"markdown"},"source":{"73717209":"from __future__ import print_function, division\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.utils import check_random_state\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.naive_bayes import GaussianNB\nwarnings.filterwarnings('ignore')\nrs = check_random_state(4421)","3a591738":"data = pd.read_csv(\"..\/input\/league-of-legends\/games.csv\")","cd417aaf":"partidasT1 = data[['gameDuration','firstBlood','firstTower','firstInhibitor','firstBaron','firstDragon','firstRiftHerald','t1_towerKills','t1_inhibitorKills','t1_baronKills','t1_dragonKills','t1_riftHeraldKills','winner']]\npartidasT2 = data[['gameDuration','firstBlood','firstTower','firstInhibitor','firstBaron','firstDragon','firstRiftHerald','t2_towerKills','t2_inhibitorKills','t2_baronKills','t2_dragonKills','t2_riftHeraldKills','winner']]\npartidasT1['winner'] = partidasT1['winner'].replace({2: 0}, regex=True)\npartidasT2['winner'] = partidasT2['winner'].replace({1: 0}, regex=True)\npartidasT2['winner'] = partidasT2['winner'].replace({2: 1}, regex=True)\n\npartidasT2['firstBlood'] = partidasT2['firstBlood'].replace({2: 3}, regex=True)\npartidasT2['firstBlood'] = partidasT2['firstBlood'].replace({1: 2}, regex=True)\npartidasT2['firstBlood'] = partidasT2['firstBlood'].replace({3: 1}, regex=True)\n\npartidasT2['firstTower'] = partidasT2['firstTower'].replace({2: 3}, regex=True)\npartidasT2['firstTower'] = partidasT2['firstTower'].replace({1: 2}, regex=True)\npartidasT2['firstTower'] = partidasT2['firstTower'].replace({3: 1}, regex=True)\n\npartidasT2['firstInhibitor'] = partidasT2['firstInhibitor'].replace({2: 3}, regex=True)\npartidasT2['firstInhibitor'] = partidasT2['firstInhibitor'].replace({1: 2}, regex=True)\npartidasT2['firstInhibitor'] = partidasT2['firstInhibitor'].replace({3: 1}, regex=True)\n\npartidasT2['firstBaron'] = partidasT2['firstBaron'].replace({2: 3}, regex=True)\npartidasT2['firstBaron'] = partidasT2['firstBaron'].replace({1: 2}, regex=True)\npartidasT2['firstBaron'] = partidasT2['firstBaron'].replace({3: 1}, regex=True)\n\npartidasT2['firstDragon'] = partidasT2['firstDragon'].replace({2: 3}, regex=True)\npartidasT2['firstDragon'] = partidasT2['firstDragon'].replace({1: 2}, regex=True)\npartidasT2['firstDragon'] = partidasT2['firstDragon'].replace({3: 1}, regex=True)\n\npartidasT2['firstRiftHerald'] = partidasT2['firstRiftHerald'].replace({2: 3}, regex=True)\npartidasT2['firstRiftHerald'] = partidasT2['firstRiftHerald'].replace({1: 2}, regex=True)\npartidasT2['firstRiftHerald'] = partidasT2['firstRiftHerald'].replace({3: 1}, regex=True)\n","f17afe98":"partidasT1.columns = ['gameDuration','firstBlood','firstTower','firstInhibitor','firstBaron','firstDragon','firstRiftHerald','towerKills' ,'inhibitorKills','baronKills','dragonKills','riftHeraldKills','winner']\npartidasT2.columns = ['gameDuration','firstBlood','firstTower','firstInhibitor','firstBaron','firstDragon','firstRiftHerald','towerKills' ,'inhibitorKills','baronKills','dragonKills','riftHeraldKills','winner']\npartidas= pd.concat([partidasT1,partidasT2],axis=0)\n","a78960f1":"partidas.index = pd.RangeIndex(len(partidas.index))\npartidas['gameDuration'] = (partidas.gameDuration\/60).astype(int)","1fdabab9":"plt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.hist(partidas[\"gameDuration\"],bins =25,color = \"skyblue\",edgecolor='blue',linewidth=1)\nplt.xlabel('Minutos')\nplt.ylabel('N\u00famero de partidas')\nplt.title('Duraci\u00f3n de las partidas')\nplt.subplot(122)\nplt.ylabel('Minutos')\nplt.title('Gr\u00e1fico de caja y bigotes')\nplt.boxplot(partidas[\"gameDuration\"])\nplt.show()\npartidas[\"gameDuration\"].describe()\n","a52bdd3c":"partidas['gameDuration'] = np.where(partidas['gameDuration']<25,1,np.where(((partidas['gameDuration']>24)&(partidas['gameDuration']<36)),2,3))","d477e297":"killsWinner = partidas.drop(partidas[partidas.loc[:, 'winner'] == 0].index)\nkillsWinner.index = pd.RangeIndex(len(killsWinner.index))\nLoserKills = partidas.drop(partidas[partidas.loc[:, 'winner'] == 1].index)\nLoserKills.index = pd.RangeIndex(len(LoserKills.index))","6ec7f033":"firstBloodfi = killsWinner.firstBlood.value_counts()\/killsWinner.firstBlood.count()\nfirstBloodfi =firstBloodfi.to_frame()\nfirstBloodfi['index'] = firstBloodfi.index\n\nfirstTowerfi = killsWinner.firstTower.value_counts()\/killsWinner.firstTower.count()\nfirstTowerfi =firstTowerfi.to_frame()\nfirstTowerfi['index'] = firstTowerfi.index\n\nfirstInhibitorfi = killsWinner.firstInhibitor.value_counts()\/killsWinner.firstInhibitor.count()\nfirstInhibitorfi =firstInhibitorfi.to_frame()\nfirstInhibitorfi['index'] = firstInhibitorfi.index\n\nfirstBaronfi = killsWinner.firstBaron.value_counts()\/killsWinner.firstBaron.count()\nfirstBaronfi =firstBaronfi.to_frame()\nfirstBaronfi['index'] = firstBaronfi.index\n\n\nfirstDragonfi = killsWinner.firstDragon.value_counts()\/killsWinner.firstDragon.count()\nfirstDragonfi =firstDragonfi.to_frame()\nfirstDragonfi['index'] = firstDragonfi.index\n\nfirstRiftHeraldfi = killsWinner.firstRiftHerald.value_counts()\/killsWinner.firstRiftHerald.count()\nfirstRiftHeraldfi =firstRiftHeraldfi.to_frame()\nfirstRiftHeraldfi['index'] = firstRiftHeraldfi.index\n\n\ninhibitorKillsfi = killsWinner.inhibitorKills.value_counts()\/killsWinner.inhibitorKills.count()\ninhibitorKillsfi =inhibitorKillsfi.to_frame()\ninhibitorKillsfi['index'] = inhibitorKillsfi.index\n\ntowerKillsfi = killsWinner.towerKills.value_counts()\/killsWinner.towerKills.count()\ntowerKillsfi =towerKillsfi.to_frame()\ntowerKillsfi['index'] = towerKillsfi.index\n\nbaronKillsfi = killsWinner.baronKills.value_counts()\/killsWinner.baronKills.count()\nbaronKillsfi =baronKillsfi.to_frame()\nbaronKillsfi['index'] = baronKillsfi.index\n\n\ndragonKillsfi =killsWinner.dragonKills.value_counts()\/killsWinner.dragonKills.count()\ndragonKillsfi =dragonKillsfi.to_frame()\ndragonKillsfi['index'] = dragonKillsfi.index\n\n\nriftHeraldKillsfi =killsWinner.riftHeraldKills.value_counts()\/killsWinner.riftHeraldKills.count()\nriftHeraldKillsfi =riftHeraldKillsfi.to_frame()\nriftHeraldKillsfi['index'] = riftHeraldKillsfi.index\n","8f19aeea":"inhibitorKillsfi2 = LoserKills.inhibitorKills.value_counts()\/LoserKills.inhibitorKills.count()\ninhibitorKillsfi2 =inhibitorKillsfi2.to_frame()\ninhibitorKillsfi2['index'] = inhibitorKillsfi2.index\n\ntowerKillsfi2 = LoserKills.towerKills.value_counts()\/LoserKills.towerKills.count()\ntowerKillsfi2 =towerKillsfi2.to_frame()\ntowerKillsfi2['index'] = towerKillsfi2.index\n\nbaronKillsfi2 = LoserKills.baronKills.value_counts()\/LoserKills.baronKills.count()\nbaronKillsfi2 =baronKillsfi2.to_frame()\nbaronKillsfi2['index'] = baronKillsfi2.index\n\n\ndragonKillsfi2 =LoserKills.dragonKills.value_counts()\/LoserKills.dragonKills.count()\ndragonKillsfi2 =dragonKillsfi2.to_frame()\ndragonKillsfi2['index'] = dragonKillsfi2.index\n\n\nriftHeraldKillsfi2 =LoserKills.riftHeraldKills.value_counts()\/LoserKills.riftHeraldKills.count()\nriftHeraldKillsfi2 =riftHeraldKillsfi2.to_frame()\nriftHeraldKillsfi2['index'] = riftHeraldKillsfi2.index","a738a6c3":"plt.figure(figsize=(15, 5))\nplt.subplot(131).set_title('Primera Muerte')\nplt.bar(firstBloodfi.index-0.125,firstBloodfi.firstBlood,width = 0.25,color='blue')\nplt.subplot(132).set_title('Primera Torre')\nplt.bar(firstTowerfi.index-0.125,firstTowerfi.firstTower,width = 0.25,color='blue')\nplt.subplot(133).set_title('Primer Inhibidor')\nplt.bar(firstInhibitorfi.index-0.125,firstInhibitorfi.firstInhibitor,width = 0.25,color='blue')\nplt.show()\n\nplt.figure(figsize=(15, 5))\nplt.subplot(131).set_title('Primer Bar\u00f3n')\nplt.bar(firstBaronfi.index-0.125,firstBaronfi.firstBaron,width = 0.25,color='blue')\nplt.subplot(132).set_title('Primer Drag\u00f3n')\nplt.bar(firstDragonfi.index-0.125,firstDragonfi.firstDragon,width = 0.25,color='blue')\nplt.subplot(133).set_title('Primer Heraldo')\nplt.bar(firstRiftHeraldfi.index-0.125,firstRiftHeraldfi.firstRiftHerald,width = 0.25,color='blue')\nplt.show()\n\n","3537fd20":"plt.figure(figsize=(15, 5))\nplt.subplot(131).set_title('Inhibidores Destruidos')\nblue_legend = mpatches.Patch(color='blue', label='Ganadores')\nyellow_legend = mpatches.Patch(color='yellow', label='Perdedores')\nplt.legend(handles=[blue_legend,yellow_legend])\nplt.bar(inhibitorKillsfi.index-0.125,inhibitorKillsfi.inhibitorKills,width = 0.25,color='blue')\nplt.bar(inhibitorKillsfi2.index+0.125,inhibitorKillsfi2.inhibitorKills,width = 0.25,color='yellow')\nplt.subplot(132).set_title('Torres Destruidas')\nplt.legend(handles=[blue_legend,yellow_legend])\nplt.bar(towerKillsfi.index-0.125,towerKillsfi.towerKills,width = 0.25,color='blue')\nplt.bar(towerKillsfi2.index+0.125,towerKillsfi2.towerKills,width = 0.25,color='yellow')\nplt.subplot(133).set_title('Barones Muertos')\nplt.legend(handles=[blue_legend,yellow_legend])\nplt.bar(baronKillsfi.index-0.125,baronKillsfi.baronKills,width = 0.25,color='blue')\nplt.bar(baronKillsfi2.index+0.125,baronKillsfi2.baronKills,width = 0.25,color='yellow')\nplt.show()\nplt.figure(figsize=(15, 5))\nplt.subplot(131).set_title('Heraldos Muertos')\nplt.legend(handles=[blue_legend,yellow_legend])\nplt.bar(riftHeraldKillsfi.index-0.125,riftHeraldKillsfi.riftHeraldKills,width = 0.25,color='blue')\nplt.bar(riftHeraldKillsfi2.index+0.125,riftHeraldKillsfi2.riftHeraldKills,width = 0.25,color='yellow')\nplt.subplot(132).set_title('Dragones Muertos')\nplt.legend(handles=[blue_legend,yellow_legend])\nplt.bar(dragonKillsfi.index-0.125,dragonKillsfi.dragonKills,width = 0.25,color='blue')\nplt.bar(dragonKillsfi2.index+0.125,dragonKillsfi2.dragonKills,width = 0.25,color='yellow')\nplt.show()","e9674128":"plt.figure(figsize=(16, 16))\nplt.title('Matriz de correlaci\u00f3n')\nsns.heatmap(partidas.corr(), square=True, annot=True,cmap=\"YlGnBu\")","236701cb":"X = partidas.drop(['winner'], axis=1)\nY = partidas['winner']","124c6da3":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.25, random_state=rs)","2179373f":"fpr = dict()\ntpr = dict()\nroc_auc = dict()","1e9fa666":"rango = range(1,30)\nscore = 0\nprecisiones = []\nfor vecinos in rango:\n    knn = KNeighborsClassifier(n_neighbors = vecinos)\n    knn.fit(Xtrain,Ytrain)\n    if (score < knn.score(Xtest,Ytest)):\n        Y_pred = knn.predict(Xtest)\n        score = knn.score(Xtest,Ytest)\n        pos = vecinos\n    precisiones.insert(vecinos,knn.score(Xtest,Ytest))\nprint(\"precisi\u00f3n de predicci\u00f3n: {0: .3f}\".format(score))\nprint(\"n_neighbors: \"+format(vecinos))","06afcd66":"plt.title('Precisi\u00f3n frente n_neighbors')\nplt.ylabel('precisi\u00f3n')\nplt.xlabel('n_neighbors')\nplt.plot(precisiones)","4e7f6391":"confusion = confusion_matrix(Ytest, Y_pred)\nplt.clf()\nplt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues_r)\nclassNames = ['Negativo','Positivo']\nplt.title('Matriz de confusi\u00f3n k-Nearest Neighbor')\nplt.ylabel('Valor verdadero')\nplt.xlabel('Valor Predicho')\n\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['VN','FP'], ['FN', 'VP']]\ns = [['VN','FP'], ['FN', 'VP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i,str(s[i][j])+\" = \"+str(confusion[i][j]), horizontalalignment='center', verticalalignment='center')\nplt.show()","bd4012bc":"fpr[1], tpr[1], _ = roc_curve(Ytest, Y_pred,pos_label=2)\nroc_auc[1] = auc(fpr[1], tpr[1])","c37fd844":"nb = GaussianNB()\nnb.fit(Xtrain,Ytrain)\nprediccion = nb.predict(Xtest)\naccuracy_test = accuracy_score(Ytest,prediccion.round())\nprint( \"Accuracy (test)  =\", accuracy_test )","ccc8e6c0":"Y_pred = nb.predict(Xtest)\nconfusion = confusion_matrix(Ytest, Y_pred)\nplt.clf()\nplt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues_r)\nclassNames = ['Negativo','Positivo']\nplt.title('Matriz de confusi\u00f3n Naive Bayes')\nplt.ylabel('Valor verdadero')\nplt.xlabel('Valor Predicho')\n\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['VN','FP'], ['FN', 'VP']]\ns = [['VN','FP'], ['FN', 'VP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i,str(s[i][j])+\" = \"+str(confusion[i][j]), horizontalalignment='center', verticalalignment='center')\nplt.show()","c9e38382":"fpr[2], tpr[2], _ = roc_curve(Ytest, Y_pred,pos_label=2)\nroc_auc[2] = auc(fpr[2], tpr[2])","5e5b52ba":"lr = LogisticRegression(C=0.1,penalty='l2')\nlr.fit(Xtrain,Ytrain)","88b55150":"Y_pred = lr.predict(Xtest)","52dbd430":"accuracy_test = accuracy_score(Ytest,Y_pred.round())\nprint( \"Accuracy (test)  =\", accuracy_test )","a72dec6d":"confusion = confusion_matrix(Ytest, Y_pred)\nplt.clf()\nplt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues_r)\nclassNames = ['Negativo','Positivo']\nplt.title('Matriz de confusi\u00f3n Regresi\u00f3n Log\u00edstica')\nplt.ylabel('Valor verdadero')\nplt.xlabel('Valor Predicho')\n\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['VN','FP'], ['FN', 'VP']]\ns = [['VN','FP'], ['FN', 'VP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i,str(s[i][j])+\" = \"+str(confusion[i][j]), horizontalalignment='center', verticalalignment='center')\nplt.show()","5184c194":"fpr[3], tpr[3], _ = roc_curve(Ytest, Y_pred,pos_label=2)\nroc_auc[3] = auc(fpr[3], tpr[3])","75212295":"rango1 = range(2,40)\nrango2 = range(1,5)\nscore = 0\nprecisiones = []\n\nfor estimadores in rango1:\n    for estimadores2 in rango2:\n        dt = DecisionTreeRegressor(random_state = rs,min_samples_split = estimadores,min_samples_leaf=estimadores2)\n        dt.fit(Xtrain,Ytrain)\n        prediccion = dt.predict(Xtest)\n        accuracy_test = accuracy_score(Ytest,prediccion.round())\n        if (score < accuracy_test):\n            Y_pred = prediccion\n            score = accuracy_test\n            pos1= estimadores\n            pos2= estimadores2\n        precisiones.insert(estimadores,accuracy_test)\nprint(\"precisi\u00f3n de predicci\u00f3n: {0: .3f}\".format(score))\nprint(\"min_samples_split: \" +format(pos1))\nprint(\"min_samples_split: \" +format(pos2))","422a3eb2":"plt.title('Evoluci\u00f3n precisi\u00f3n con los distintos par\u00e1metros de entrada')\nplt.ylabel('precisi\u00f3n')\nplt.xlabel('N\u00ba de ejecuci\u00f3n')\nplt.plot(precisiones)","88a22495":"Y_pred = Y_pred.round()\nconfusion = confusion_matrix(Ytest, Y_pred)\nplt.clf()\nplt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues_r)\nclassNames = ['Negativo','Positivo']\nplt.title('Matriz de confusi\u00f3n \u00c1rbol de decisi\u00f3n')\nplt.ylabel('Valor verdadero')\nplt.xlabel('Valor Predicho')\n\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['VN','FP'], ['FN', 'VP']]\ns = [['VN','FP'], ['FN', 'VP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i,str(s[i][j])+\" = \"+str(confusion[i][j]), horizontalalignment='center', verticalalignment='center')\nplt.show()","0898521b":"fpr[4], tpr[4], _ = roc_curve(Ytest, Y_pred,pos_label=2)\nroc_auc[4] = auc(fpr[4], tpr[4])","5afc299c":"rango = range(1,500)\nscore = 0\nprecisiones = []\nfor estimadores in rango:\n    clf = RandomForestClassifier(n_estimators=estimadores, random_state=rs)\n    clf.fit(Xtrain,Ytrain)\n    prediccion = clf.predict(Xtest)\n    accuracy_test = accuracy_score(Ytest,prediccion.round())\n    if (score < accuracy_test):\n        Y_pred = prediccion\n        score = accuracy_test\n        pos = estimadores\n    precisiones.insert(estimadores,accuracy_test)\nprint(\"precisi\u00f3n de predicci\u00f3n: {0: .3f}\".format(score))\nprint(\"n_estimators: \" +format(pos))","a819de58":"plt.title('Precisi\u00f3n frente a n_estimators')\nplt.ylabel('precisi\u00f3n')\nplt.xlabel('n_estimators')\nplt.plot(precisiones)","8becb351":"confusion = confusion_matrix(Ytest, Y_pred)\nplt.clf()\nplt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues_r)\nclassNames = ['Negativo','Positivo']\nplt.title('Matriz de confusi\u00f3n Random Forest')\nplt.ylabel('Valor verdadero')\nplt.xlabel('Valor Predicho')\n\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['VN','FP'], ['FN', 'VP']]\ns = [['VN','FP'], ['FN', 'VP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i,str(s[i][j])+\" = \"+str(confusion[i][j]), horizontalalignment='center', verticalalignment='center')\nplt.show()","ab0cd734":"fpr[5], tpr[5], _ = roc_curve(Ytest, Y_pred,pos_label=2)\nroc_auc[5] = auc(fpr[5], tpr[5])","fbacc5f5":"rango = range(1,200)\nscore = 0\nprecisiones = []\nfor estimadores in rango:\n    gbr = GradientBoostingRegressor(n_estimators = estimadores,max_depth=7,min_samples_split=3,min_samples_leaf=2,max_features=2,random_state=rs)\n    gbr.fit(Xtrain,Ytrain)\n    prediccion = gbr.predict(Xtest)\n    accuracy_test = accuracy_score(Ytest,prediccion.round())\n    if (score < accuracy_test):\n        Y_pred = prediccion\n        score = accuracy_test\n        pos = estimadores\n    precisiones.insert(estimadores,accuracy_test)\nprint(\"precisi\u00f3n de predicci\u00f3n: {0: .3f}\".format(score))\nprint(\"n_estimators: \" +format(pos))","4cdd4e3b":"plt.title('Precisi\u00f3n frente a n_estimators')\nplt.ylabel('precisi\u00f3n')\nplt.xlabel('n_estimators')\nplt.plot(precisiones)","e6b9c008":"confusion = confusion_matrix(Ytest, Y_pred.round())\nplt.clf()\nplt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues_r)\nclassNames = ['Negativo','Positivo']\nplt.title('Matriz de confusi\u00f3n Gradient Boosting')\nplt.ylabel('Valor verdadero')\nplt.xlabel('Valor Predicho')\n\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['VN','FP'], ['FN', 'VP']]\ns = [['VN','FP'], ['FN', 'VP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i,str(s[i][j])+\" = \"+str(confusion[i][j]), horizontalalignment='center', verticalalignment='center')\nplt.show()","08eba488":"fpr[6], tpr[6], _ = roc_curve(Ytest, Y_pred,pos_label=2)\nroc_auc[6] = auc(fpr[6], tpr[6])","e2bf1b60":"plt.figure()\nlw = 2\nplt.plot(fpr[1], tpr[1], color='darkorange',lw=lw, label='Curva ROC k-Nearest Neighbor (area = %0.2f)' % roc_auc[1])\nplt.plot(fpr[2], tpr[2], color='blue', lw=lw, label='Curva ROC  Naive Bayes (area = %0.2f)' % roc_auc[2])\nplt.plot(fpr[3], tpr[3], color='green', lw=lw, label='Curva ROC  Regresion logistica (area = %0.2f)' % roc_auc[3])\nplt.plot(fpr[4], tpr[4], color='yellow', lw=lw, label='Curva ROC  \u00c1rbol de decisi\u00f3n (area = %0.2f)' % roc_auc[4])\nplt.plot(fpr[5], tpr[5], color='purple', lw=lw, label='Curva ROC  Random forest (area = %0.2f)' % roc_auc[5])\nplt.plot(fpr[6], tpr[6], color='red', lw=lw, label='Curva ROC curve Gradient Boosting (area = %0.2f)' % roc_auc[6])\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","e07770c1":"Se guardan los datos para poder dibujar m\u00e1s adelante la curva ROC","223826ed":"En estas cinco gr\u00e1ficas siguientes se observa las diferencias existentes entre los equipos ganadores y los equipos perdedores, comparando las frecuencias de los valores de las caracter\u00edsticas.\nComo se ha visto en la gr\u00e1fica anterior, los heraldos conseguidos (\u201cheraldKills\u201d) pr\u00e1cticamente nunca se consiguen y cuando estos se consiguen lo hacen pr\u00e1cticamente el mismo n\u00famero de veces los equipos ganadores que los perdedores. Esto nos hace pensar que esta caracter\u00edstica no es muy influyente en cuanto a la predicci\u00f3n de qui\u00e9n va a ganar la partida.\n\nCon el resto de caracter\u00edsticas podemos indicar que:\n* Inhibidores destruidos (\u201cinhibitorKills\u201d): Se puede sacar la conclusi\u00f3n de que pr\u00e1cticamente el 90% de los equipos perdedores no consiguen destruir ning\u00fan inhibidor, en cambio los equipos ganadores lo destruyen 1 (30%), 2 (30%) o 3 (20%) veces durante la partida.\n* Torres destruidas (\u201ctowerKills\u201d): Aqu\u00ed se observa que los equipos perdedores contienen los valores m\u00e1s bajos y los equipos ganadores los m\u00e1s altos.\n* Barones destruidos (\u201cBaronKills\u201d): Los equipos perdedores no consiguen pr\u00e1cticamente este logro y de los ganadores la mitad de las veces tampoco lo consiguen y la otra mitad solamente 1 vez.\n* Heraldos destruidos (\u201criftHeraldKills\u201d): Se puede ver que esta caracter\u00edstica no afectar\u00e1 mucho a la predicci\u00f3n ya que pr\u00e1cticamente los equipos ganadores y perdedores o no lo consiguen o lo consiguen una vez con la misma frecuencia.\n* Dragones muertos (\u201cdragonKills\u201d): En el 20% de los casos, tanto si es equipo ganador como si es equipo perdedor, se consigue 1 drag\u00f3n, pero los equipos ganadores consiguen m\u00e1s frecuentemente 2 o m\u00e1s dragones que los equipos perdedores. Adem\u00e1s, observamos que los equipos perdedores no consiguen ning\u00fan drag\u00f3n en el 50% de las veces.\n","480d9d53":"Los par\u00e1metros no especificados tienen el valor que se les da por defecto.\n\nComo se puede observar, todos los m\u00e9todos de clasificaci\u00f3n han obtenido un porcentaje de acierto de predicci\u00f3n bastante bueno, entre el 82 % y el 94 %. La diferencia es m\u00ednima en la mayor\u00eda de los casos, siendo los dos peores de estos los modelos que utilizan los algoritmos de \u201cNaive Bayes\u201d y \"Regresi\u00f3n log\u00edstica\". Esto es debido a que ests algoritmos asumen la independencia de las variables y en el conjunto de datos existen variables dependientes como ya se vio en la secci\u00f3n del an\u00e1lisis descriptivo.\nLa mejor precisi\u00f3n la ha obtenido el modelo realizado con el clasificador Gradient Boosting, pero la diferencia es tan peque\u00f1a comparado con los otros clasificadores que no merece la pena utilizar los recursos computacionales que se utilizan con este tipo de clasificadores (Ensemble). Por ejemplo, con el clasificador \u201cRandom forest\u201d se ha tardado mas de 2 horas en obtener el mejor nivel de precisi\u00f3n sobre el conjunto de test, en cambio utilizando el clasificador \u201ck-Nearest Neighbor\u201d en 5 minutos se ha obtenido un porcentaje de precisi\u00f3n bastante parecido.\n","3ca9663a":"Se va a calcular las frecuencias relativas de las variables de los equipos que han ganado las partidas para posteriormente dibujar las gr\u00e1ficas de frecuencias.","4ddec775":"Se puede observar en la gr\u00e1fica de m\u00e1s abajo que despu\u00e9s de entrenar el modelo con distintos valores para n_neighbors y realizar la predicci\u00f3nes se obtiene una precisi\u00f3n del 93,3% para un valor de n_neighbors = 29 y despu\u00e9s comienza a disminuir la precisi\u00f3n.","b84046b5":"En primer lugar vamos a dividir nuestro conjunto de datos en dos. Por un lado tendremos los datos de entrada (X) y por otro el objetivo (Y). D\u00e9spues los volveremos a dividir para obtener un conjunto de entrenamiento con el 75% de los datos (Xtrain e Ytrain) y otro de test con el 25% de los datos (Xtest e Ytest) para poder comprobar la efectividad de nuesgtro modelo.","dd89b8b3":"![image.png](attachment:image.png)","8d36ad58":"Si realizamos la gr\u00e1fica de distribuci\u00f3n de la variable \"gameDuration\" y el gr\u00e1fico de caja y bigotes se puede ver que la mayor parte de las partidas se concentran m\u00e1s o menos entre el minuto 20 y el 42. M\u00e1s concretamente el primer cuartil y el tercer cuartil se encuentran entre los minutos 25 y 35. Por ello, se ha decidido crear 3 valores: \"1\" si la partida dura menos de 25 minutos (Partida corta), \"2\" si la duraci\u00f3n de la partida se encuentra entre los minutos 25 y 35 (Partida normal) y \"3\" en otro caso (Partida larga). \nTenemos valores at\u00edpicos en partidas que duran m\u00e1s de 50 minutos. Tambi\u00e9n existen valores at\u00edpicos en partidas con duraciones de menos de 10 minutos, esto se debe a que alguno de los equipos decide abandonar la partida antes de llegar al minuto 10 y escoge la opci\u00f3n de rendirse. ","a82e77d2":"![image.png](attachment:image.png)","fd6d4937":"Despu\u00e9s de entrenar con valores distintos del par\u00e1metro \u201cn_estimators\u201d se obtiene sobre el conjunto de test una precisi\u00f3n del 93,8%","35cbf2ff":"Convertimos la duraci\u00f3n de la partida de segundos a minutos","fd7882c1":"En la matriz de confusi\u00f3n podemos observar:\n\n* Ha predicho correctamente 12285 veces que el equipo iba a perder la partida\n* Ha predicho correctamente 11870 veces que el equipo iba a ganar la partida\n* Ha predicho 898 veces que el equipo iba a perder la partida y realmente la ha ganado\n* Ha predicho 692 veces que el equipo iba a ganar la partida y realmente la ha perdido","7a21d1d5":"Se guardan los datos para poder dibujar m\u00e1s adelante la curva ROC","302b8429":"Matriz de confusi\u00f3n:","bd91f7fc":"1. https:\/\/www.aprendemachinelearning.com\/\n2. https:\/\/tatwan.github.io\/How-To-Plot-A-Confusion-Matrix-In-Python\/\n3. https:\/\/scikit-learn.org\n4. https:\/\/es.wikipedia.org\n5. http:\/\/www.cs.us.es\/~fsancho\/?e=104\n6. https:\/\/es.slideshare.net\/aselqui\/analisis-regresin-logistica\n7. https:\/\/www.paradigmadigital.com\/techbiz\/machine-learning-dummies\/\n8. https:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d\n9. http:\/\/uc-r.github.io\/gbm_regression\n10. https:\/\/www.apd.es\/algoritmos-del-machine-learning\/\n","7a2b9015":"El modelo de regresi\u00f3n log\u00edstica modeliza la probabilidad de un proceso binomial como la funci\u00f3n log\u00edstica de una combinaci\u00f3n lineal de las variables independientes.","4298452a":"En el siguiente apartado se eligen las variables que se van a utilizar para los modelos de predicci\u00f3n. Cada fila de la tabla contienes los datos del equipo ganador y del equipo perdedor de una partida. Se ha decidido separar en dos dataSet(PartidasT1 y PartidasT2) obteni\u00e9ndose cada uno los datos de cada equipo por separado. Para que los datos siguieran teniendo sentido se han realizado las siguientes transformaciones en las caracter\u00edsticas:\n* En los datos del equipo 1:\n    * winner:  Se reemplaza el valor \"2\" que indicaba que el ganador hab\u00eda sido el equipo 2 por un \"0\" el cual indica que se ha perdido la partida. El valor \"1\" ahora significa que se ha ganado la partida.\n    * firstBlood, firstTower, firstInhibitor, firstDragon, firstRiftHerald: Se dejan los datos originales, pero ahora toman el significado de \"0\" si nadie ha realizado la primera vez, \"1\" si la primera vez lo ha realizado este equipo y \"2\" si no ha sido este equipo el que lo ha realizado la primera vez.\n* En los datos del equipo 2:\n    * winner: EL valor \"1\" que indicaba que hab\u00eda ganado el equipo 1 se reemplaza por un \"0\" que indica que perdi\u00f3 la partida y el valor \"2\" que indicaba que hab\u00eda ganado el equipo 2 se reemplaza por un \"1\" que indica que ha ganado la partida.\n    * firstBlood, firstTower, firstInhibitor, firstDragon, firstRiftHerald: EL valor \"1\" que indicaba que la primera vez lo hab\u00eda conseguido el equipo 1 se reemplaza por un \"2\" que indica que no ha sido este equipo el que lo ha realizado la primera vez y el valor \"2\" que indicaba que la primera vez lo ha realizado el equipo 2 se reemplaza por un \"1\" que indica que la primera vez lo ha realizado este equipo. En resumen, \"0\" ninguno lo ha conseguido, \"1\" lo ha conseguido este equipo y \"2\" no lo ha conseguido este equipo.\nEn el resto de caracter\u00edsticas no hace falta realizar ninguna transformaci\u00f3n por el momento.","ffe32e4c":"Matriz de confusi\u00f3n:","d037703d":"**Predicci\u00f3n del resultado de una partida de League of Legends a trav\u00e9s de un conjunto de datos disponible en Kaggle: (LoL) League of Legends Ranked Games.**\n\nEste trabajo forma parte de un proceso de evaluaci\u00f3n del \"Master de BigData - UNED\"","c2bd0488":"En la matriz de confusi\u00f3n podemos observar:\n* Ha predicho correctamente 12076 veces que el equipo iba a perder la partida\n* Ha predicho correctamente 11938 veces que el equipo iba a ganar la partida\n* Ha predicho 830 veces que el equipo iba a perder la partida y realmente la ha ganado\n* Ha predicho 901 veces que el equipo iba a ganar la partida y realmente la ha perdido","089ee3bd":"Si se dibuja la matriz de confusi\u00f3n:","d7fe9096":"![image.png](attachment:image.png)","107510c9":"En estas primeras 6 gr\u00e1ficas solo se ha representado las frecuencias relativas de las caracter\u00edsticas de los equipos ganadores (\"firstBlood\", \"firstTower\", \"firstInhibitor\", \"firstBaron\", \"firstDragon\" y \"firstRiftHerald\"). No se comparan gr\u00e1ficamente con la de los equipos perdedores ya que los datos de los equipos perdedores y ganadores de nuestro conjunto provienen de la misma partida y por tanto, si en una partida un equipo ha sido el primero en conseguir uno de estos logros el otro equipo no ha sido capaz. En un conjunto de datos m\u00e1s amplio d\u00f3nde los equipos ganadores y los equipos perdedores no provengan de las mismas partidas s\u00ed tendr\u00eda sentido comparar las frecuencias relativas de los valores que toman estas caracter\u00edsticas.\n\nObservando las gr\u00e1ficas se sacan como conclusiones:\n* Primera muerte (\u201cfirstBlood\u201d): Pr\u00e1cticamente este logro lo consiguen con igual frecuencia los equipos ganadores y los perdedores.\n* Primera torre (\u201cfirstTower\u201d): Primer inhibidor (\u201cfirstInhibitor\u201d), primer drag\u00f3n (\u201cfirstDragon\u201d): Se puede observar que estas caracter\u00edsticas tienen una relaci\u00f3n bastante alta con los equipos ganadores ya que consiguen estos logros al rededor del 70% - 80% de las veces.\n* Primer heraldo (\u201cfirstRiftHerald\u201d) y primer bar\u00f3n (\u201cfiestaron\u201d): En la mayor\u00eda de las partidas no se consiguen estos logros y cuando se consiguen las frecuencias est\u00e1n pr\u00e1cticamente repartidas por igual entre equipo ganador y perdedor, inclin\u00e1ndose la balanza un poco m\u00e1s a favor de los equipos ganadores. \n\n","e4f31b58":"Se ha entrenado el modelo con varios par\u00e1metros C=0.1..1.0 y penalty='l1' \u00f3 penalti='l2' obtienese siempre una precisi\u00f3n \nmuy parecida al ejecutarlo sobre el conjunto de test. Alrededor del 82,47%","f8a74555":"# Carga y transformaciones de los datos","a5336fcc":"Para realizar un an\u00e1lisis descriptivos de los datos se ha dividido en dos el conjunto de datos. Uno con las partidas ganadas y otro con las partidas perdidas.","7cdbe51e":"Una vez concluido el trabajo se puede afirmar que uno de los aspectos que m\u00e1s tiempo requiere y que es clave para la obtenci\u00f3n de mejores resultados es el estudio, pre-procesado y transformaci\u00f3n de los datos. Saber entenderlos y prepararlos correctamente antes de empezar a realizar el modelo de predicci\u00f3n es important\u00edsimo y es una cualidad clave que debe tener un buen cient\u00edfico de datos. \nOtro aspecto clave es saber elegir correctamente los par\u00e1metros del clasificador utilizado, ya que una mala elecci\u00f3n puede empeorar los resultados de forma considerable.\nEn todos los modelos se han obtenido unos porcentajes de precisi\u00f3n muy buenos. Esto es debido a que existen caracter\u00edsticas de entrada que est\u00e1n muy ligadas a la variable a predecir y adem\u00e1s son datos de partidas ya terminadas.\nExisten bastantes m\u00e1s caracter\u00edsticas (no disponibles en este \u201cdataSet\u201d) a tener en cuenta en una partida (n\u00famero de \u201cminions\u201d, cantidad de oro, objetos comprados, \u2026). Lo ideal ser\u00eda tener todas estas caracter\u00edsticas nuevas junto con las que ya se disponen, pero en un cierto tiempo de la partida, no como se tiene ahora mismo que son datos una vez finalizada. Por ejemplo, como se vio en el apartado de an\u00e1lisis descriptivo la duraci\u00f3n media de las partidas son de 30 minutos, pues ser\u00eda muy interesante obtener un \u201cdataSet\u201d con los datos de un equipo a los 10, a los 15 y a los 20 minutos sabiendo si este finalmente ha conseguido ganar.\nEn este trabajo solamente se ha tenido en cuenta algoritmos de aprendizaje supervisado, pero para futuros trabajos se podr\u00eda realizar modelos de aprendizaje no supervisado para posteriormente comparar los resultados.","113a5563":"![image.png](attachment:image.png)","e9f62ce5":"K-Nearest-Neighbor es un m\u00e9todo de clasificaci\u00f3n supervisada basado en instancia (memoriza las instancias de entrenamiento que son usadas como \u00abbase de conocimiento\u00bb para la fase de predicci\u00f3n).\nSe caracteriza por ser un algoritmo muy sencillo de implementar, pero es un proceso costoso computacionalmente y requiere bastante almacenamiento debido a que almacena todos los datos en memoria. Por eso tiende a funcionar mejor en \u201cdatasets\u201d peque\u00f1os.\n\nPara la realizaci\u00f3n de este modelo se ha utilizado la funci\u00f3n \u201cKNeighborsClassifer\u201d que se encuentra en la librer\u00eda \u201cScikit-learn\u201d. ","f80386c4":"Teniendo una serie de datos de entrada de un equipo, el problema a resolver consiste en decidir si este equipo gana o pierde la partida. Esta situaci\u00f3n es claramente un problema de clasificaci\u00f3n binaria, es decir, solo hay dos posibles resultados, \u201c0\u201d si pierde la partida y \u201c1\u201d si la gana.\nExisten multitud de m\u00e9todos para resolver este tipo de problemas (clasificaci\u00f3n), pero entre los m\u00e1s comunes nos encontramos:\n1.\tK nearest neighbours (KNN)\n2.\tNa\u00efve Bayes \n3.\tSupport Vector Machines(SVM)\n4.\tRegresi\u00f3n lineal (No es v\u00e1lido en nuestro caso)\n5.\tRegresi\u00f3n log\u00edstica\n6.\t\u00c1rbol de decisi\u00f3n\n7.\tBosques Aleatorios (Random Forest)\n8.\tGradient Boosting\n9.\tExtremely Randomized trees\n\nEn este trabajo de car\u00e1cter acad\u00e9mico el plan de acci\u00f3n que se ha seguido es el siguiente:\nSe han elegido seis de los nueve m\u00e9todos dichos anteriormente, los cuales se explicar\u00e1n brevemente dando a conocer sus caracter\u00edsticas m\u00e1s importantes y se entrenar\u00e1n con distintos par\u00e1metros, guardando los mejores resultados de cada uno de ellos.  Posteriormente estos resultados se comparar\u00e1n y se decidir\u00e1 cu\u00e1l ha sido el mejor modelo para el conjunto de datos que tenemos.\n","5619d3b5":"# An\u00e1lisis descriptivo ","1fc1c541":"Se ha entrenado nuestro modelo modificando los par\u00e1metros \"min_samples_split\" (2..40) y \"min_samples_leaf\" (1..5) realizando predicciones sobre el conjunto de test obteniendo un 92,9 % de precisi\u00f3n con los par\u00e1metros \u201cmin_samples_split\u201d = 21 y \u201cmin_samples_leaf\u201d = 3","36818422":"Curvas ROC de los distintos modelos:","8fe02247":"En la matriz de confusi\u00f3n podemos observar:\n* Ha predicho correctamente 10625 veces que el equipo iba a perder la partida\n* Ha predicho correctamente 10890 veces que el equipo iba a ganar la partida\n* Ha predicho 1878 veces que el equipo iba a perder la partida y realmente la ha ganado\n* Ha predicho 2352 veces que el equipo iba a ganar la partida y realmente la ha perdido","ecf69fca":"Matriz de confusi\u00f3n:","51049d52":"![image.png](attachment:image.png)","1036f8ba":"En la matriz de confusi\u00f3n podemos observar:\n* Ha predicho correctamente 11301 veces que el equipo iba a perder la partida\n* Ha predicho correctamente 9932 veces que el equipo iba a ganar la partida\n* Ha predicho 2836 veces que el equipo iba a perder la partida y realmente la ha ganado\n* Ha predicho 1676 veces que el equipo iba a ganar la partida y realmente la ha perdido","0836756f":"A modo de gu\u00eda para interpretar las curvas ROC se han establecido los siguientes intervalos para los valores de AUC:\n* [0.5]: Es como lanzar una moneda.\n*\t[0.5, 0.6): Test malo.\n*\t[0.6, 0.75): Test regular.\n*\t[0.75, 0.9): Test bueno.\n*\t[0.9, 0.97): Test muy bueno.\n*\t[0.97, 1): Test excelente.\n\nSe puede observar que los resultados en todos los casos se encuentran en los intervalos \u201cbueno\u201d y \u201cexcelente\u201d.","4bc50d5b":"Este clasificador se basa en aplicar el Teorema de Bayes asumiendo independencia entre cada par de caracter\u00edsticas.\nEste clasificador es utilizado para situaciones reales como filtro de correo spam o para la clasificaci\u00f3n de documentos\nEntre otras ventajas se tiene:\n* Consigue buenos resultados con pocos datos de entrenamiento.\n* Es bastante r\u00e1pido si lo comparamos con otros m\u00e9todos m\u00e1s complejos.\n","631d9083":"Matriz de confusi\u00f3n:","577b90b6":"Se guardan los datos para poder dibujar m\u00e1s adelante la curva ROC","8c5ac047":"Despu\u00e9s de entrenar con este clasificador se obtiene una precisi\u00f3n del 83,56%. Aunque no es un es un valor muy bajo, s\u00ed que es peor que el que se obtiene con otros clasificadores. Quiz\u00e1s es porque se asume que las caracter\u00edsticas son independientes y como se ha visto en el an\u00e1lisis descriptivo no lo son, si no que tienen relaci\u00f3n unas con otras.","f5254575":"Parece ser que existe cierta relaci\u00f3n entre nuestra variable de salida (\u201cwinner\u201d) y las caracter\u00edsticas de entrada, en mayor medida con \u201cfirstInhibitor\u201d (primer inhibidor), \u201ctowerKills\u201d (torres destruidas), \u201cinhibitorKills\u201d (inhibidores destruidos). Esta afirmaci\u00f3n es posible corroborarla con la Matriz de correlaci\u00f3n.","035acf9b":"Se pueden sacar como conclusiones importantes que los equipos vencedores consiguen el primer logro (drag\u00f3n, torre, bar\u00f3n, inhibidor o heraldo) con una frecuencia mayor que los equipos perdedores. Teniendo mayor diferencia sobre todo en el primer inhibidor, lo cual tiene sentido ya que si un equipo ha conseguido destruir el primer inhibidor significa que han conseguido unas ventajas sobre el otro equipo, aparte de traspasar sus defensas. \nTambi\u00e9n se puede observar que los equipos ganadores de las partidas consiguen con mayor frecuencia los valores m\u00e1s altos de logros que los equipos perdedores, los cuales consiguen con mayor frecuencia los valores m\u00e1s bajos de logros.","1ccca6ba":"![image.png](attachment:image.png)","51980a11":"# Conclusiones","21094fe3":"En la matriz de confusi\u00f3n podemos observar:\n* Ha predicho correctamente 12145 veces que el equipo iba a perder la partida\n* Ha predicho correctamente 11783 veces que el equipo iba a ganar la partida\n* Ha predicho 985 veces que el equipo iba a perder la partida y realmente la ha ganado\n* Ha predicho 832 veces que el equipo iba a ganar la partida y realmente la ha perdido","305f0f23":"## k-Nearest Neighbor","89dd570f":"Se guardan los datos para poder dibujar m\u00e1s adelante la curva ROC","f1902243":"# Resumen resultados","bcf809a5":"# Modelos de predicci\u00f3n","84e26148":"Se guardan los datos para poder dibujar m\u00e1s adelante la curva ROC","6b1e0338":"Se va a entrenar este modelo con distintos valores de n_neighbors(del 1 al 30) qued\u00e1ndonos con el que mejor precisi\u00f3n de predicci\u00f3n tenga al ejecutarlo sobre el conjunto de test.","dadf3dbc":"Se observa en la gr\u00e1fica que cuando \u201cn_estimators\u201d es mayor de 30 la precisi\u00f3n toma valores entre 0,93 y 0,934 siendo este \u00faltimo el mejor valor de precisi\u00f3n obtenido.","63b61aa4":"En todos nuestros modelos vamos a entrenar el modelo con Xtrain e Ytrain. En Y_pred guardamos las prediciones que realiza con nuestros datos de test.","94840d59":"Se puede observar en la siguiente gr\u00e1fica que la precisi\u00f3n tiende a aumentar cuanto mayor es el valor de n_estimators.","65a2d026":"Para poder unir los dos dataSet uno debajo del otro, se va a renombrar las variables para que los dos sean iguales","0f4c763f":"## Random forest","12c2ed92":"## Regresi\u00f3n log\u00edstica","3d7f2496":"Funci\u00f3n log\u00edstica:","b9c44980":"# Referencias","2bc92a8f":"Si se dibuja la matriz de confusi\u00f3n:","c15bb919":"Declaramos las siguientes variables donde guardaremos los datos necesarios para posteriormente poder ralizar las gr\u00e1ficas con las curvas ROC de cada modelo.","01800253":"En la matriz de confusi\u00f3n podemos observar:\n\n* Ha predicho correctamente 12240 veces que el equipo iba a perder la partida\n* Ha predicho correctamente 11814 veces que el equipo iba a ganar la partida\n* Ha predicho 954 veces que el equipo iba a perder la partida y realmente la ha ganado\n* Ha predicho 737 veces que el equipo iba a ganar la partida y realmente la ha perdido","21744341":"Se va a calcular las frecuencias relativas de las variables de los equipos que han perdido las partidas para posteriormente dibujar las gr\u00e1ficas de frecuencias.\nEn este caso no hace falta sacar las de las variables firstBlood, firstTower, firstInhibitor, firstBaron, firstDragon, firstRiftHerald ya que los datos de los equipos que han perdido son los contrarios a los equipos que han ganado.","f88807f2":"## \u00c1rbol de decisi\u00f3n","7af98e7a":"Un espacio ROC se define por \u201cFalsos positivos\u201d y \u201cVerdaderos Positivos\u201d como ejes x e y respectivamente, y representa los intercambios entre verdaderos positivos y falsos positivos.\nEl indicador m\u00e1s utilizado en muchos contextos es el \u00e1rea bajo la curva ROC. Este \u00edndice se puede interpretar como la probabilidad de que un clasificador puntuar\u00e1 una instancia positiva elegida aleatoriamente m\u00e1s alta que una negativa.","d010b0fa":"Se guardan los datos para poder dibujar m\u00e1s adelante la curva ROC","07c32d83":"## Gradient Boosting","dd03efe8":"Para la realizaci\u00f3n de este modelo se ha utilizado la funci\u00f3n \u201cGaussianNB\u201d que se encuentra en la librer\u00eda \u201cScikit-learn\u201d. ","2c9af757":"Este clasificador utiliza un \u00e1rbol de decisi\u00f3n. Este \u00e1rbol est\u00e1 formado por nodos interiores (nodos de decisi\u00f3n) y nodos hoja (nodos respuesta).\n* Un nodo de decisi\u00f3n est\u00e1 asociado a una de las caracter\u00edsticas. Tiene dos o m\u00e1s ramas que salen de \u00e9l, cada una de ellas representando los posibles valores que puede tomar la caracter\u00edstica asociada. De alguna forma, un nodo de decisi\u00f3n es como una pregunta que se le hace al ejemplo analizado, y dependiendo de la respuesta que d\u00e9, el flujo tomar\u00e1 una de las ramas salientes.\n* Un nodo respuesta est\u00e1 asociado a la clasificaci\u00f3n que se quiere proporcionar, y devuelve la decisi\u00f3n del \u00e1rbol con respecto al ejemplo de entrada.\n\nPara la realizaci\u00f3n de este modelo se ha utilizado la funci\u00f3n \u201cDecisionTreeRegressor\u201d que se encuentra en la librer\u00eda \u201cScikit-learn\u201d.","0bd76d73":"\u201cA2daBoost\u201d es un algoritmo que funciona eligiendo un algoritmo base (\u00e1rboles de decisi\u00f3n) y mejor\u00e1ndolo iterativamente al tomar en cuenta los casos incorrectamente clasificados en el conjunto de entrenamiento.\nEl m\u00e9todo Gradient Boostig se basa en \u201cAdaboost\u201d e implica tres elementos:\n* Una funci\u00f3n de perdida a optimizar.\n* Un algoritmo de aprendizaje d\u00e9bil para hacer las predicciones (\u00e1rboles de decisi\u00f3n).\n* Un modelo aditivo para a\u00f1adir los algoritmos de aprendizaje d\u00e9biles que minimizan la funci\u00f3n de perdida. \n\n\nLa funci\u00f3n \u201cGradientBoostingRegressor\u201d que nos proporciona la librer\u00eda \u201cScikit-learn\u201d es utilizada para este modelo.","6fadd4a5":"Podemos observar en la gr\u00e1fica anterior el nivel de predicci\u00f3n sobre nuestro conjunto de test seg\u00fan se van modificando los par\u00e1metros de entrada obteniendose un valor maximo 92,9 %","7e745e1a":"La regresi\u00f3n log\u00edstica es un modelo de regresi\u00f3n en el cual la salida que se obtiene ser\u00e1 discreta, normalmente solo con dos resultados posibles. La funci\u00f3n principal consiste en explicar la relaci\u00f3n entre la variable a estimar y las variables conocidas (variables independientes).\n\nPara la realizaci\u00f3n de este modelo se ha utilizado la funci\u00f3n \u201cLogisticRegression\u201d que se encuentra en la librer\u00eda \u201cScikit-learn\u201d.\n","ebd0b055":"Analizando la matriz, se pueden sacar como conclusiones, que existe una relaci\u00f3n entre las torres destruidas (\u201ctowerKills\u201d) y los inhibidores destruidos (\u201cinhibitorKills\u201d). Parece l\u00f3gico, ya que son elementos del juego que hay que destruir para ganar. Pero realmente las relaciones que interesan son las que existen entre la variable \"winner\", ya que es nuestra variable a predecir y las otras variables. Por lo tanto, se puede apreciar que mantiene una relaci\u00f3n positiva moderada con los dragones destruidos (\"dragonKills\"), barones destruidos (\u201cbaronKills\u201d), primera torre (\"firstTower\") y primer inhibidor (\"firstInhibitor\") y una relaci\u00f3n algo m\u00e1s fuerte con los inhibidores destruidos (\"inhibitorKills\") y las torres destruidas (\"towerKills\").","60a351ee":"Aunque requiere algo m\u00e1s de recursos computacionales se ha realizado el entrenamiento variando el par\u00e1metro \u201cn_estimators\u201d desde 1 hasta 500 para poder observar la eficiencia del modelo, obteni\u00e9ndo el mejor resultado sobre el conjunto de test del 93,4% con un valor de \u201cn_estimators\u201d = 405","f9aa9613":"Una vez que se han preparado las tablas, se va a proceder a realizar un an\u00e1lisis gr\u00e1fico comparando las frecuencias de las caracterisitcas de los equipos que han ganado frente a la de los equipos que han perdido.","0fbd292b":"En este m\u00e9todo se utiliza una colecci\u00f3n de \u00e1rboles de decisi\u00f3n, los cuales se entrenan con distintos subconjuntos aleatorios de los datos. De esta manera, se tiene que cada \u00e1rbol se entrena con dos tercios de los datos y el tercio restante se reserva para la validaci\u00f3n del modelo.\nCada entrada nueva se eval\u00faa con todo el conjunto de datos dando como resultado la predicci\u00f3n m\u00e1s frecuente de entre todo el conjunto de \u00e1rboles.\n\nLa funci\u00f3n \u201cRandomForestClassifier\u201d que nos proporciona la librer\u00eda \u201cScikit-learn\u201d es la utilizada para este modelo.","09051187":"#  Importaci\u00f3n de paquetes y librer\u00edas","224aba8f":"Cargamos los datos del fichero csv en la variable data","980a44db":"## Naive Bayes","ac8f7a3e":"Existen multitud de algoritmos de \u201cmachine learning\u201d, los cuales se pueden dividir en 3 categor\u00edas. Aprendizaje supervisado, aprendizaje no supervisado y aprendizaje por refuerzo.\nEn este trabajo de fin de m\u00e1ster se ha decidido escoger distintos algoritmos de aprendizaje supervisado. En la secci\u00f3n anterior se puso una lista con los nueve m\u00e9todos m\u00e1s comunes de esta categor\u00eda y se eligieron seis de ellos para realizar los modelos de aprendizaje, obteni\u00e9ndose los resultados de precisi\u00f3n que se pueden observar en la siguiente tabla:\n"}}