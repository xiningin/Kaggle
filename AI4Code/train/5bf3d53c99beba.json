{"cell_type":{"e0fb7147":"code","1bf65e5d":"code","db827365":"code","ccce9cbc":"code","38354786":"code","55f3370a":"code","853b13da":"code","ee9ab8b5":"code","40df9cc2":"code","5e7829df":"code","0cf9b14c":"code","3bc6629c":"code","8577935b":"code","6770ba92":"code","309a2c22":"code","28be3f89":"code","64534d8d":"code","12c33dc7":"code","19f3df55":"code","f90c3e97":"code","336a799e":"code","aa9aac64":"code","c6646146":"code","ed8986c0":"code","8b73582d":"code","fb3b2b8d":"code","2eec0312":"code","f5cb820b":"code","2b37f825":"code","c70f4c81":"code","c93846fa":"code","8af470fe":"code","5ee8642f":"code","6d43e476":"code","15cde60f":"code","afccf6c1":"code","3b3fbfb0":"code","2d68e120":"code","d6c3b9ac":"code","37eeaaed":"code","f1219196":"code","7c2e9523":"code","09887cb2":"code","32e6b5a2":"code","7eb4b583":"code","fd4599d1":"code","9ec613bd":"code","3ae097bb":"code","8970955c":"code","60c3ae65":"code","ba8a4672":"code","c83ad4a2":"code","6399f09d":"code","569e356e":"code","137d730a":"code","8169c406":"code","56d2a488":"code","a1684dbc":"code","3481e5f3":"code","1ce75790":"code","5f179c93":"code","a6f5f2c8":"code","5dd43a38":"code","5025e673":"code","431a7215":"code","21a377f8":"code","ffc9e949":"code","58166543":"code","2bbb7c96":"code","dbbf18ba":"code","644dd7cd":"code","f2ac5388":"code","2f81b5db":"code","42a90297":"code","db9ea52f":"code","59f77167":"code","fae60e3b":"code","13f90cab":"code","8b649f95":"code","3b79bdff":"code","c3d2ee0b":"code","4877ecfb":"code","704fa445":"code","fd9378f5":"code","bec0ea43":"code","df574825":"code","eb5ee751":"code","eda09809":"code","70857a92":"code","4ac804f3":"code","4c2bdc5c":"code","048096b4":"code","4487987d":"code","8648136d":"code","8bcf7b5e":"code","590afd54":"code","fb36cdd1":"code","1f577b54":"code","70574d6d":"code","32e1cb64":"code","271c56ba":"code","ab67dbc2":"code","f44ce415":"markdown","af4e502f":"markdown","61d2a33a":"markdown","a1e93b14":"markdown","dacb21a1":"markdown","37d9be3b":"markdown","13c7ac6b":"markdown","9b4925ce":"markdown","d972636e":"markdown","f7d64328":"markdown","37a31ab1":"markdown","a1aabf51":"markdown","f3c2d069":"markdown","4204d2d2":"markdown","6a7a4893":"markdown","98550607":"markdown","0601e7c7":"markdown","9d0e80a4":"markdown","48e63753":"markdown","f153bfdb":"markdown","821ac77f":"markdown","cb6879a4":"markdown","b94246da":"markdown","b25c206e":"markdown","dec0b1da":"markdown","2ca3585f":"markdown","3651f93d":"markdown","01c60418":"markdown","e0e5505b":"markdown","8ccb37de":"markdown","08c895f4":"markdown","2d546acc":"markdown","311ac9a4":"markdown","dc0e8709":"markdown","9d2cb540":"markdown","f7fe10e5":"markdown","0b093f21":"markdown","30065b1b":"markdown","ea121049":"markdown","f20f327e":"markdown","be3cfab6":"markdown","68e6629d":"markdown","175653ac":"markdown","0e9660ce":"markdown","4034dcf2":"markdown","80412edd":"markdown","e257d22c":"markdown","85cf933a":"markdown","24ac8b18":"markdown","28ad46e5":"markdown","9c423ab1":"markdown","c4dc8a8b":"markdown","2a8c6dc5":"markdown","8eccad15":"markdown","f1014285":"markdown","0fb51c77":"markdown","d43438eb":"markdown","9449c7d3":"markdown","e1324e60":"markdown"},"source":{"e0fb7147":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline ","1bf65e5d":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","db827365":"train_df.head()","ccce9cbc":"train_df.shape","38354786":"train_df.info()","55f3370a":"train_df.describe()","853b13da":"# Check number of unique values for each features to see if can plot them or not\nfor col in train_df.columns:\n    unique_vals = train_df[col].nunique()\n    print(f'{col}: {unique_vals} as unique values')\n    print(f'Data type: {train_df[col].dtypes}')\n    print('')","ee9ab8b5":"# check the value type is one type or different types\ntrain_df['Ticket'].value_counts()[:10]","40df9cc2":"# check the value type is one type or different types\ntrain_df['Cabin'].value_counts()[:10]","5e7829df":"def subplots_features_count_dists(data):\n    '''\n    return to distribution of continuse features or count of discreate features\n    ignore specific features they do not have any meaning \n    '''\n    \n    ignored_cols = ['PassengerId','Name', 'Ticket', 'Cabin']\n    cols = [ col for col in data.columns if col not in ignored_cols]\n    \n    nrows= int(np.ceil(len(cols)\/2))\n    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize=(12,8), constrained_layout=True)\n    ax = ax.ravel()\n    \n    for i in range(len(cols)):\n\n        if (data[cols[i]].dtypes == 'object') or (len(data[cols[i]].unique().tolist()) < 10):\n            sns.countplot(y = data[cols[i]], ax=ax[i])\n            ax[i].set_title(f'{cols[i]} count')\n        \n        else:\n            sns.histplot(x = data[cols[i]], ax=ax[i])\n            ax[i].set_title(f'{cols[i]} distribution')\n\n    plt.show()","0cf9b14c":"subplots_features_count_dists(train_df)","3bc6629c":"def subplots_two_cols(data, cols):\n    \"\"\"\n    if number of uniques in col from cols less than 10 \n    then create kdeplot otherwise create countplot\n    \"\"\"\n    plt.figure(figsize=[8,5])\n    \n    if data[cols[1]].nunique() > 10:\n        sns.kdeplot(x = cols[1], hue=cols[0], data=data)\n        plt.title(f'{cols[1]} by Passengers')\n    else:\n        sns.countplot(x = cols[0], hue=cols[1], data=data)\n        plt.title(f'Survivors count grouped by {cols[1]}')\n        plt.legend(loc='upper right')\n        plt.show()","8577935b":"subplots_two_cols(train_df, ['Survived','Pclass'])","6770ba92":"subplots_two_cols(train_df, ['Survived', 'Sex'])","309a2c22":"subplots_two_cols(train_df, ['Survived', 'Age'])","28be3f89":"subplots_two_cols(train_df, ['Survived', 'SibSp'])","64534d8d":"subplots_two_cols(train_df, ['Survived', 'Parch'])","12c33dc7":"subplots_two_cols(train_df, ['Survived', 'Fare'])","19f3df55":"subplots_two_cols(train_df, ['Survived', 'Embarked'])","f90c3e97":"train_df.isna().sum()","336a799e":"test_df.isna().sum()","aa9aac64":"# we will drop it because it isn't add any meaning \ntrain_df = train_df.drop('PassengerId', axis=1)\ntest_df = test_df.drop('PassengerId', axis=1)","c6646146":"age_mean = int(train_df['Age'].mean())\nage_mean","ed8986c0":"train_df['Age'] = train_df['Age'].fillna(age_mean)","8b73582d":"train_df['Age'].isna().sum()","fb3b2b8d":"age_mean = int(test_df['Age'].mean())\nage_mean","2eec0312":"test_df['Age'] = test_df['Age'].fillna(age_mean)","f5cb820b":"test_df['Age'].isna().sum()","2b37f825":"fare_med = test_df['Fare'].median()\nfare_med","c70f4c81":"test_df['Fare'] = test_df['Fare'].fillna(fare_med)","c93846fa":"test_df['Fare'].isna().sum()","8af470fe":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.histplot(train_df['Fare'], ax=ax[0]).set_title('Fare Distribution')\nsns.histplot(test_df['Fare'], ax=ax[1]).set_title('Fare Distribution');","5ee8642f":"# drop zero fares and then apply log transformation\ntrain_df = train_df[train_df['Fare'] != 0]\ntrain_df['Fare'] = np.log(train_df['Fare'])\ntest_df = test_df[test_df['Fare'] != 0]\ntest_df['Fare'] = np.log(test_df['Fare'])","6d43e476":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.histplot(train_df['Fare'], ax=ax[0]).set_title('Fare Distribution')\nsns.histplot(test_df['Fare'], ax=ax[1]).set_title('Fare Distribution');","15cde60f":"# We will drop it from data because we have lots of missing values\ntrain_df = train_df.drop(labels='Cabin', axis=1)\ntest_df = test_df.drop(labels='Cabin', axis=1)","afccf6c1":"# We will fill it with the most value\nstats.mode(train_df['Embarked'])\ntrain_df['Embarked'] = train_df['Embarked'].fillna('S')\ntrain_df['Embarked'].isna().sum()","3b3fbfb0":"# We will split each value by scpace then \n# check if it is numeric don't change it else \n# take the the first of number of the lst elemnt\n\ntrain_df['Ticket'] = train_df['Ticket'].apply(lambda x: x.split()[0][0] if x.isnumeric() else x.split(' ')[-1][0])\ntrain_df['Ticket'].value_counts()","2d68e120":"train_df['Ticket'] = train_df['Ticket'].astype('int')","d6c3b9ac":"test_df['Ticket'] = test_df['Ticket'].apply(lambda x: x.split()[0][0] if x.isnumeric() else x.split(' ')[-1][0])\ntest_df['Ticket'] = test_df['Ticket'].astype('int')","37eeaaed":"# drop name because it isn't add any meaning\ntrain_df = train_df.drop('Name', axis=1)\ntest_df = test_df.drop('Name', axis=1)","f1219196":"train_df.head(2)","7c2e9523":"test_df.head(2)","09887cb2":"# decod specific features from category to binary for modling\ntrain_df = pd.get_dummies(train_df, columns=['Pclass', 'Sex', 'Embarked'])\ntest_df = pd.get_dummies(test_df, columns=['Pclass', 'Sex', 'Embarked'])","32e6b5a2":"# check for correlation is there any correlation between features\nplt.figure(figsize=(12,5))\n\nsns.set(font_scale=1)\n\nsns.heatmap(\n    data=train_df.corr(),\n    annot=True,\n    fmt = '0.2f',\n);","7eb4b583":"# We will drop highly correlated features Sex_male, Embarked_C\ntrain_df = train_df.drop(labels=['Sex_female', 'Embarked_C'], axis=1)\ntest_df = test_df.drop(labels=['Sex_female', 'Embarked_C'], axis=1)","fd4599d1":"X_train_df = train_df.drop('Survived', axis=1)\ny_train_df = train_df['Survived'].values\nX_test_df = test_df\ny_test_df = y_train_df[:416]","9ec613bd":"sc = StandardScaler()\nX_train_df_sc = sc.fit_transform(X_train_df)\nX_test_df_sc = sc.transform(X_test_df)","3ae097bb":"# The baseline score \ntrain_df['Survived'].value_counts(normalize=True)","8970955c":"# Function that runs the requested algorithm and returns the accuracy score\ndef fit_model_algo(algo, X_train, y_train, X_tet, y_test, cv):\n    \n    # Cross Validation \n    acc_cv = cross_val_score(\n                                    algo, \n                                    X_train, \n                                    y_train, \n                                    cv=cv, \n                                    n_jobs = -1)\n    \n    # Cross-validation mean accuracy score\n    acc_cv = np.mean(acc_cv)\n    \n    # Test prediction and accuracy score\n    algo.fit(X_train, y_train)\n    test_score = algo.score(X_test_df_sc, y_test)\n    test_pred = algo.predict(X_test_df_sc)\n    return test_pred, acc_cv, test_score","60c3ae65":"# creat dataframe with two columns the frist columns with model name \n# the second model with score after model is optimised by grid search\n\nmodel_score_lst = pd.DataFrame(\n    {\n        'Model':[\n            'Logistic Regression', \n            'KNN Classifier',\n            'Decision Tree Classifier', \n            'Random Forest Classifier',\n            'Bagging Classifier',\n            'Extra tree Classifier'], \n        'Grid Search Score': None,\n    }, \n         index = np.arange(6)\n)\nmodel_score_lst","ba8a4672":"lr = LogisticRegression()\ntest_pred_lr, acc_cv, test_score =  fit_model_algo(lr, X_train_df_sc, y_train_df, X_test_df_sc, y_test_df, 10)\nprint(f'CV Average Score: {acc_cv}')\nprint(f'Test Scroe: {test_score}')","c83ad4a2":"knn = KNeighborsClassifier(n_neighbors=10)\ntest_pred_knn, acc_cv, test_score =  fit_model_algo(knn, X_train_df_sc, y_train_df, X_test_df_sc, y_test_df, 10)\nprint(f'CV Average Score: {acc_cv}')\nprint(f'Test Scroe: {test_score}')","6399f09d":"dt = DecisionTreeClassifier()\ntest_pred_dt, acc_cv, test_score =  fit_model_algo(dt, X_train_df_sc, y_train_df, X_test_df_sc, y_test_df, 10)\nprint(f'CV Average Score: {acc_cv}')\nprint(f'Test Scroe: {test_score}')","569e356e":"rf = RandomForestClassifier()\ntest_pred_rf, acc_cv, test_score =  fit_model_algo(rf, X_train_df_sc, y_train_df, X_test_df_sc, y_test_df, 10)\nprint(f'CV Average Score: {acc_cv}')\nprint(f'Test Scroe: {test_score}')","137d730a":"be = BaggingClassifier()\ntest_pred_rf, acc_cv, test_score =  fit_model_algo(be, X_train_df_sc, y_train_df, X_test_df_sc, y_test_df, 10)\nprint(f'CV Average Score: {acc_cv}')\nprint(f'Test Scroe: {test_score}')","8169c406":"et = ExtraTreesClassifier()\ntest_pred_rf, acc_cv, test_score =  fit_model_algo(et, X_train_df_sc, y_train_df, X_test_df_sc, y_test_df, 10)\nprint(f'CV Average Score: {acc_cv}')\nprint(f'Test Scroe: {test_score}')","56d2a488":"param_grid = {\n     'penalty' : ['l1', 'l2', 'elasticnet'],\n    'C' : [3, 5, 7, 9, 11, 20, 50, 100],\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}","a1684dbc":"gs = GridSearchCV(lr, \n                  param_grid, \n                  cv=5,\n                  scoring='roc_auc',\n                  verbose=1)\n","3481e5f3":"gs.fit(X_train_df_sc, y_train_df)","1ce75790":"gs.best_estimator_","5f179c93":"gs.best_score_","a6f5f2c8":"# Model accuracy score from unseen data\ngs_score = gs.score(X_test_df_sc, y_test_df)\ngs_score","5dd43a38":"# append model score after the optiming \nmodel_score_lst['Grid Search Score'][0] = gs_score\nmodel_score_lst","5025e673":"param_grid = {\n    'n_neighbors': [3, 5, 7, 9, 11, 20, 50, 100],\n    'weights': ['uniform', 'distance'],\n    'metric': ['manhattan', 'euclidean']\n}","431a7215":"gs = GridSearchCV(knn, \n                  param_grid, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1)\ngs.fit(X_train_df_sc, y_train_df)","21a377f8":"gs.best_estimator_","ffc9e949":"gs.best_score_","58166543":"gs_score = gs.score(X_test_df_sc, y_test_df)\ngs_score","2bbb7c96":"# append model score after the optemising \nmodel_score_lst['Grid Search Score'][1] = gs_score\nmodel_score_lst","dbbf18ba":"param_grid= {\n    'criterion': [\"gini\", \"entropy\"],\n    'splitter': [\"best\", \"random\"],\n    'max_depth': [10, 100, 1000],\n    'max_features' : list(range(6,32,5))\n}","644dd7cd":"gs = GridSearchCV(dt, \n                  param_grid, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1)\ngs.fit(X_train_df_sc, y_train_df)","f2ac5388":"gs.best_estimator_","2f81b5db":"gs.best_score_","42a90297":"# Model accuracy score from unseen data\ngs_score = gs.score(X_test_df_sc, y_test_df)\ngs_score","db9ea52f":"# append model score after the optemizing\nmodel_score_lst['Grid Search Score'][2] = gs_score\nmodel_score_lst","59f77167":"param_grid = {\n    'n_estimators' : list(range(10,101,10)),\n    'max_features' : list(range(6,32,5))\n             }","fae60e3b":"gs = GridSearchCV(rf, \n                  param_grid, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1)\ngs.fit(X_train_df_sc, y_train_df)","13f90cab":"gs.best_estimator_","8b649f95":"gs.best_score_","3b79bdff":"# Model accuracy score from unseen data\ngs_score = gs.score(X_test_df_sc, y_test_df)\ngs_score","c3d2ee0b":"# append model score after the optiming \nmodel_score_lst['Grid Search Score'][3] = gs_score\nmodel_score_lst","4877ecfb":"param_grid = {\n    'n_estimators' : list(range(10,101,10)),\n    'max_features' : list(range(6,32,5))\n             }","704fa445":"gs = GridSearchCV(be, \n                  param_grid, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1)\ngs.fit(X_train_df_sc, y_train_df)","fd9378f5":"gs.best_estimator_","bec0ea43":"gs.best_score_","df574825":"# Model accuracy score from unseen data\ngs_score = gs.score(X_test_df_sc, y_test_df)\ngs_score","eb5ee751":"# append model score after the optiming \nmodel_score_lst['Grid Search Score'][4] = gs_score\nmodel_score_lst","eda09809":"param_grid = {\n    'criterion' : [\"gini\", \"entropy\"],\n    'n_estimators' : list(range(10,101,10)),\n    'max_features' : list(range(6,32,5))\n             }","70857a92":"gs = GridSearchCV(et, \n                  param_grid, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1)\ngs.fit(X_train_df_sc, y_train_df)","4ac804f3":"gs.best_estimator_","4c2bdc5c":"gs.best_score_","048096b4":"# Model accuracy score from unseen data\ngs_score = gs.score(X_test_df_sc, y_test_df)\ngs_score","4487987d":"# append model score after the optiming \nmodel_score_lst['Grid Search Score'][5] = gs_score\nmodel_score_lst","8648136d":"test_pred_gs_et = gs.predict(X_test_df_sc)","8bcf7b5e":"def features_important(df, model):\n    \n    feat_imp = pd.DataFrame({\n                 'Featrues Name': df.columns.tolist(),\n                 'Important Rate': model.feature_importances_.tolist()\n    })\n    \n    feat_imp = feat_imp.sort_values(by='Important Rate', ascending=False)\n    \n    plt.figure(figsize=(12,6))\n    _ = sns.barplot(y = feat_imp['Featrues Name'], x = feat_imp['Important Rate'], data=feat_imp, orient='h')\n    plt.title('Feature Important')\n    \n    return feat_imp","590afd54":"features_important(test_df, et)","fb36cdd1":"target_names = ['Unsurvivors{class 0}', 'Survivors{class 1}']\n\nprint(f'Confusion Matrix:\\n{confusion_matrix(y_test_df, test_pred_gs_et)}\\n')\nprint(f\"Classification Report:\\n{classification_report(y_test_df, test_pred_gs_et, target_names=target_names)}\")","1f577b54":"submission_file = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","70574d6d":"submission_file.index = submission_file['PassengerId']\nsubmission_file.drop(labels=['PassengerId','Survived'], axis=1, inplace=True)","32e1cb64":"pred_lst = test_pred_gs_et.tolist()\npred_lst.append(0)\npred_lst.append(0)\ny_test_df = np.array(pred_lst)\ny_test_df.shape","271c56ba":"submission_file['Survived'] = y_test_df","ab67dbc2":"submission_file.to_csv('test_pred_gs_et.csv')","f44ce415":"**Metrics**","af4e502f":"**Extra Tree Classifier**","61d2a33a":"**Fare**","a1e93b14":"__Survived__: 0 = No, 1 = Yes\n\n__Pclass__:  (Ticket class) 1 = 1st, 2 = 2nd, 3 = 3rd\n\n__Sex__: Male, Femal\n\n__Age__: Age in years\n\n__SibSp__: number of siblings\/spouses aboard the Titanic\n\n__Parch__: number of parents\/children aboard the Titanic\n\n__Ticket__: Ticket number\n\n__Fare__: Passenger fare\n\n__Cabin__: Cabin number\n\n__Embarked__: Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","dacb21a1":"**Survived vs Pclass**","37d9be3b":"**Check each feature with survived feature**","13c7ac6b":"This is matched with class categories, so most unsurvivors were in a low class with low fare a few of them were survivors while a high class had a high chance to be survival.","9b4925ce":"## Explore Data","d972636e":"**KNN Classifier**","f7d64328":"**Survived vs Sex**","37a31ab1":"The majorty of unsurvivors and survivors embarked from Sou and few them embared from c and q","a1aabf51":"The sinking of the Titanic is one of the most infamous shipwrecks in history. During her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. \n\nUnfortunately, there weren\u2019t enough lifeboats for everyone onboard, ending the lives of in excess of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. \n\nWe have used two data sets, one to create a model and the other to test it which is provided by Kaggle to create a prediction model. At last, our prediction is evaluated according to whether we effectively predicted the result for each passenger in the test file.","f3c2d069":"**Seperate to X_train, X_test, y_train, y_test**","4204d2d2":"We can see some features have observations less than entries 891\n\n- Age 715 entries\n- Cabin 204 entries\n\nThat means there are null values in these features","6a7a4893":"**Scale X_train and X_test**","98550607":"We will should reduce the variation and skewness to be normal or seminormal distribution","0601e7c7":"**Random Forest**","9d0e80a4":"## Clean Data","48e63753":"There some high correlation:\n- Sex_female and Sex_male \n- Embarked_C and Embarked_S","f153bfdb":"**Survived vs SibSp**","821ac77f":"**Name**","cb6879a4":"## Select the best model and show metrics","b94246da":"## Features Describtion","b25c206e":"**Sbumission file**","dec0b1da":"I Think now prettey good than first","2ca3585f":"**Survived vs Parch**","3651f93d":"Finally, we will select the best score after optimizing the model and submission the prediction of that model.\nWe can see the __Extra Tree Classifier__ has the highest score so, we will save the prediction of model and submit it.","01c60418":"**Logistic Regression**","e0e5505b":"**Bagging Classifier**","8ccb37de":"**Random Forest Classifier**","08c895f4":"There are different types of values such as numbers, letters and mix of them, so we can't plot this type of data","2d546acc":"Similar to Tickit different types of values numbers, letters and mix of them.","311ac9a4":"Almostly same with grouped by but the small spike in survivors near 0 may be return to childern and women first","dc0e8709":"**Embarked**","9d2cb540":"Unfortuntly, most unsurvivors were male and who survivors were the majorty female","f7fe10e5":"**Cabin**","0b093f21":"## Optimize models","30065b1b":"**Passinger Id**","ea121049":"**Baseline score**","f20f327e":"**Extra Tree Classifier**","be3cfab6":"**Decision Tree**","68e6629d":"Most unsurvivors and survivors had not siblings\/spouses abroad Titanic and a few percent had one siblings\/spouses.","175653ac":"**KNN**","0e9660ce":"**Logistic Regression**","4034dcf2":"The columns that will be ignored from EDA\n\n- __PassengerId__ is like an id number will ignore it.\n- __Name__ is do not give us any benefit will ignore it.\n- __Tickit__ is do not give us any benefit will ignore it.\n- __Cabin__ is do not give us any benefit will ignore it.","80412edd":"**Decision Tree Classifier**","e257d22c":"**Survived vs Embarked**","85cf933a":"**Tickit**","24ac8b18":"**Age**","28ad46e5":"## Problem Statement","9c423ab1":"we can see count of categories and distibution of numerics variabels.\n\n- __Survived variable__: number of dead persons were 60% while survivors persons were 40%\n- __Pclass variabel__: 55% of values were in class 3 while 45% for class 1 and 2 \n- __Sex variable__: 65% of persons in Titanic were male and 35% were femal\n- __Age variable__: a little right skewness wherease the majority distribution were in the range from 20 to 38\n- __SibSp variable__: 67% of persons did not have Siblings\/spouses abroad Titanic wherease 22% had one person and 11% had different number of person abroad Titanic\n- __Parch variable__: 75% of persons did not have Parent\/children that means they unmmarried or thier parents were dead or with them on Titanic while 13% had one person abroad Titanic and 9% had two persons.\n- __Fare varialbe__: a high right skewness wherease most fares were less than 100\n- __Embarked variable__: 72% of persons embarked from Southampton, 20% from Cherbourg and 8% from Queenstown","c4dc8a8b":"Most unsurvivors and survivors had not parents\/children abroad Titanic may been with them, were unmarried or dead while few percent had one or two parents\/children.","2a8c6dc5":"**Analyze Explanatory Data**","8eccad15":"## Create Models","f1014285":"## Import Libraries","0fb51c77":"**Bagging Classifier**","d43438eb":"# Team 4\n\n#### Abdulaziz Alsulami\n#### Razan Alsulieman\n#### Smouh Alqahtani","9449c7d3":"**Survived vs Fare**","e1324e60":"We can see that most persons in the low class '3' were unsurvivors while in the high class '3' the majority were survivors and in class '2' the percent was equally."}}