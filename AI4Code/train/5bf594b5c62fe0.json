{"cell_type":{"924da725":"code","6c998053":"code","d92084db":"code","922f90e1":"code","ce6c86c8":"code","d074696d":"code","b68b2740":"code","e73e6ae8":"code","c7ab33a6":"code","c347abbe":"code","56e97c4e":"code","ebf1ba89":"code","be5bbf8b":"code","917fa91a":"code","2aad54c9":"code","4a0284c8":"code","999a4624":"code","f0c8cb9e":"code","9f676276":"code","dcb1622e":"code","504feb54":"code","1b5ebe62":"code","46417296":"code","249bb00b":"code","c818f445":"code","9cc1e140":"code","201ee178":"code","291bba7b":"code","c02a4e48":"code","89c09b3b":"code","e48703be":"code","874c63de":"code","15db5750":"code","5ac21807":"code","a2d97c1b":"code","cf382e58":"markdown","c5957c44":"markdown","b21f18b8":"markdown","59524e66":"markdown","8c64ef2a":"markdown","f9c0b13c":"markdown","d4be03ea":"markdown","7f12481a":"markdown","d002e205":"markdown","34aa2c25":"markdown","4f7a66e5":"markdown","5e2312ce":"markdown","41358d7d":"markdown","14c05f24":"markdown","bf7106ad":"markdown","927b51fc":"markdown","35145257":"markdown","e821899c":"markdown","5f326d74":"markdown","1514a212":"markdown","ff538ae3":"markdown","e01e950a":"markdown"},"source":{"924da725":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # for data visualiztions\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6c998053":"# read the dataset\ndf = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndf.head()","d92084db":"# get the dimensions of the dataset\ndf.shape","922f90e1":"# find the data types of the attributes\ndf.dtypes","ce6c86c8":"# concise summary of the data\ndf.describe()","d074696d":"# find if missing values is present\ndf.isnull().sum()","b68b2740":"# get the distribution of the target variable\nsns.countplot(x=\"Species\", data = df)","e73e6ae8":"# Separate the dependent and independent features\nX = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny = df['Species']","c7ab33a6":"# Standardize the data and then concatenate it with y\ndata = X\ndata_std = (data - data.mean())\/(data.max() - data.min())\ndata = pd.concat([data_std,y], axis=1)","c347abbe":"# reshape the dataframe using melt()\ndata = pd.melt(data, id_vars = 'Species', var_name = 'features',value_name = 'value')\ndata","56e97c4e":"# swarmplot for analysing the different attributes\nplt.figure(figsize = (6,6))\nsns.swarmplot(x = 'features', y = 'value', hue = 'Species', data = data)\nplt.show()","ebf1ba89":"# obtain a correlation heatmap\nsns.heatmap(X.corr(), annot=True)","be5bbf8b":"# split the dataset into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nprint('Training set shape: ', X_train.shape, y_train.shape)\nprint('Testing set shape: ', X_test.shape, y_test.shape)","917fa91a":"from sklearn.feature_selection import chi2, SelectKBest, f_classif","2aad54c9":"# Get the two best(k = 2) features using the SelectKBest method\nft = SelectKBest(chi2, k = 2).fit(X_train, y_train)\nprint('Score: ', ft.scores_)\nprint('Columns: ', X_train.columns)","4a0284c8":"ft = SelectKBest(f_classif, k= 2).fit(X_train, y_train)\nprint('Score: ', ft.scores_)\nprint('Columns: ', X_train.columns)","999a4624":"X_train_2 = ft.transform(X_train)\nX_test_2 = ft.transform(X_test)","f0c8cb9e":"from sklearn import preprocessing\nX_train = preprocessing.StandardScaler().fit(X_train_2).transform(X_train_2.astype(float))\nX_test = preprocessing.StandardScaler().fit(X_test_2).transform(X_test_2.astype(float))","9f676276":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn","dcb1622e":"from sklearn import metrics","504feb54":"# calculating the accuracy of models with different values of k\nmean_acc = np.zeros(20)\nfor i in range(1,21):\n    #Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n    yhat= knn.predict(X_test)\n    mean_acc[i-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc","1b5ebe62":"loc = np.arange(1,21,step=1.0)\nplt.figure(figsize = (10, 6))\nplt.plot(range(1,21), mean_acc)\nplt.xticks(loc)\nplt.xlabel('Number of Neighbors ')\nplt.ylabel('Accuracy')\nplt.show()","46417296":"from sklearn.model_selection import GridSearchCV","249bb00b":"grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n               'weights' : ['uniform','distance'],\n               'metric' : ['minkowski','euclidean','manhattan']}","c818f445":"gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)","9cc1e140":"# fit the model on our train set\ng_res = gs.fit(X_train, y_train)","201ee178":"# find the best score\ng_res.best_score_","291bba7b":"# get the hyperparameters with the best score\ng_res.best_params_","c02a4e48":"# use the best hyperparameters\nknn = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform',algorithm = 'brute',metric = 'minkowski')\nknn.fit(X_train, y_train)","89c09b3b":"# get a prediction\ny_hat = knn.predict(X_train)\ny_knn = knn.predict(X_test)","e48703be":"print('Training set accuracy: ', metrics.accuracy_score(y_train, y_hat))\nprint('Test set accuracy: ',metrics.accuracy_score(y_test, y_knn))","874c63de":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_knn))","15db5750":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_knn))","5ac21807":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(knn, X, y, cv =5)","a2d97c1b":"print('Model accuracy: ',np.mean(scores))","cf382e58":"As we see, we have obtained a very high model accuracy of 0.97. It is possible that the accuracy may be increased further by using more hyperparameters or with a different model.","c5957c44":"We can now confirm our results and use only Petal Length and Petal Width for prediction","b21f18b8":"All the species have equal division which is good since all features will have equal influence on predciting the species.","59524e66":"k- Nearest Neighbors is one of the most basic algorithms used in supervised machine learning. It classifies new data points based on similarity index which is usually a distance metric. It uses a majority vote will classifying the new data. For example, if there are 3 blue dots and 1 dot near the new data point, it will classify it as a blue dot.","8c64ef2a":"A hyperparameter is a parameter of the model that is set before the start of learning process. Different machine learning models have different hyperparameters. You can find out more about the different hyperparameters of k-NN <a href =  'https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html'>here<\/a>.","f9c0b13c":"# Feature Selection","d4be03ea":"There are a range of values from 5 to 16 where the accuracy is the highest.","7f12481a":"By reshaping our data using melt(), we have converted it into a more computer friendly form, where initially we had more than one identifier features. Now, all of them have been packed into one feature which will help us in visualisation.\n","d002e205":"In this kernel, we will try to classify the Iris species using the k-Nearest Neighbors algorithm. We will also find the best parameters for the model using hyperparameter tuning. ","34aa2c25":"Since we have provided the class validation score as 3( cv= 3), Grid Search will evaluate the model 6 x 2 x 3 x 3 = 108 times with different hyperparameters.","4f7a66e5":"Feature Selection is a techinque of finding out the features that contribute the most to our model i.e. the best predictors.","5e2312ce":"We will use the Exhaustive Grid Search technique for hyperparameter optimization. An exhaustive grid search takes in as many hyperparameters as you would like, and tries every single possible combination of the hyperparameters as well as as many cross-validations as you would like it to perform. An exhaustive grid search is a good way to determine the best hyperparameter values to use, but it can quickly become time consuming with every additional parameter value and cross-validation that you add.","41358d7d":"No missing values- our dataset is clean","14c05f24":"# Hyperparameter Tuning","bf7106ad":"Before we move on to classification, let us see some basic information about our dataset.","927b51fc":"We will use three hyperparamters- n-neighbors, weights and metric.\n1. n_neighbors: Decide the best k based on the values we have computed earlier.\n2. weights: Check whether adding weights to the data points is beneficial to the model or not. 'uniform' assigns no weight, while 'distance' weighs points by the inverse of their distances meaning nearer points will have more weight than the farther points.\n3. metric: The distance metric to be used will calculating the similarity.","35145257":"# Model Evaluation","e821899c":"# k-Nearest Neighbors","5f326d74":"# Preprocessing","1514a212":"It is clear from the plot that Petal Length and Petal Width provide very clear distinctions between the different classes whereas the same cannot be said for Sepal Length and Sepal Width ","ff538ae3":"# Import the libraries","e01e950a":"One of the challenges in a k-NN algorithm is finding the best 'k' i.e. the number of neighbors to be used in the majority vote while deciding the class. Generally, it is advisable to test the accuracy of your model for different values of k and then select the best one from them."}}