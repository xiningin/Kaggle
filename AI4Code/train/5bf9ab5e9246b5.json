{"cell_type":{"0cb21d35":"code","a57d895a":"code","a3f5b80c":"code","56f027fa":"code","38dc2ec2":"code","a3a3ad32":"code","651f8a0d":"code","5d6600a0":"code","cccce917":"markdown","1d1c0f80":"markdown","035c62b3":"markdown","d78e4990":"markdown","bdacc703":"markdown"},"source":{"0cb21d35":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.display.max_rows = 999\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a57d895a":"X_train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col=\"Id\")\nX_test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col=\"Id\")","a3f5b80c":"X_train.drop(X_train[(X_train['GrLivArea'] > 4000) & (X_train['SalePrice'] < 250000)].index, inplace=True)\n\n# Log the target value and set y = target\nX_train['SalePrice'] = np.log1p(X_train['SalePrice'])\ny = X_train['SalePrice']\nX_train.drop(axis=1, columns=['SalePrice'], inplace=True)","56f027fa":"X_train['MSSubClass'] = X_train['MSSubClass'].astype(str)\nX_test['MSSubClass'] = X_test['MSSubClass'].astype(str)\n\nX_train['YearBuilt'] = X_train['YearBuilt'].astype(str)\nX_test['YearBuilt'] = X_test['YearBuilt'].astype(str)\n\nX_train['GarageYrBlt'] = X_train['GarageYrBlt'].astype(str)\nX_test['GarageYrBlt'] = X_test['GarageYrBlt'].astype(str)\n\nX_train['YearRemodAdd'] = X_train['YearRemodAdd'].astype(str)\nX_test['YearRemodAdd'] = X_test['YearRemodAdd'].astype(str)\n\nX_train['MoSold'] = X_train['MoSold'].astype(str)\nX_test['MoSold'] = X_test['MoSold'].astype(str)\n\nX_train['YrSold'] = X_train['YrSold'].astype(str)\nX_test['YrSold'] = X_test['YrSold'].astype(str)","38dc2ec2":"import itertools\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport category_encoders as ce\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass Drop_Features(BaseEstimator, TransformerMixin):\n    def __init__(self, drop_threshold=0.95):\n        self.drop_threshold = drop_threshold\n    \n    def fit(self, X, y=None):\n        self.cols_to_drop = [cname for cname in X.columns if X[cname].isnull().sum() >= self.drop_threshold*X.shape[0]]\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy.drop(axis=1, columns=self.cols_to_drop, inplace=True)\n        return X_copy\n\n# Create interactions between object columns\nclass Create_Interactions(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        cat_cols = [cname for cname in X.columns if X[cname].dtype == 'object']\n        \n        interactions = pd.DataFrame(index=X.index)\n        \n        for cname1, cname2 in itertools.combinations(cat_cols, 2):\n            new_cname = \"_\".join([cname1, cname2])\n            new_col = X[cname1].map(str) + \"_\" + X[cname2].map(str)\n            interactions[new_cname] = LabelEncoder().fit_transform(new_col)\n        \n        X_copy = pd.concat([X, interactions], axis=1)\n        \n        return X_copy \n    \n# Find total number of bathrooms\nclass Add_Features(BaseEstimator, TransformerMixin):\n    def __init__(self, add_feature=True):\n        self.add_feature = add_feature\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        if self.add_feature:\n            total_bathrooms = X['FullBath'] + X['BsmtFullBath'] + 0.5*(X['BsmtHalfBath']+X['HalfBath'])\n            total_area = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']\n            \n            new_features = pd.DataFrame(np.c_[total_bathrooms, total_area], dtype='float64', columns=['total_bathrooms', 'total_area'], index=X.index)\n            X_copy = pd.concat([X, new_features], axis=1)\n            return X_copy\n        else:\n            return X\n    \nclass Skewed_Features(BaseEstimator, TransformerMixin):\n    def __init__(self, skew_threshold=0.8):\n        self.skew_threshold = skew_threshold\n    \n    def fit(self, X, y=None):\n        skewed_cols = X.select_dtypes(include=[np.number]).skew()\n        self.top_skewed_cols = skewed_cols[abs(skewed_cols > self.skew_threshold)].index\n        return self\n    \n    def transform(self, X):\n        # Make a copy so that X is not changed\n        X_copy = X.copy()\n        \n        for cname in self.top_skewed_cols:\n            X_copy[cname] = np.log1p(X_copy[cname])\n        return X_copy\n        \nclass Colinear_Features(BaseEstimator, TransformerMixin):\n    def __init__(self, corr_threshold=0.8):\n        self.corr_threshold = corr_threshold\n    \n    def fit(self, X, y=None):\n        num_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n        \n        feature_correllation = X.corr(method='kendall')\n        # iterate over rows\n        columns = np.full((feature_correllation.shape[0],), True, dtype=bool)\n        for i in range(feature_correllation.shape[0]):\n            for j in range(i+1, feature_correllation.shape[0]):\n                if feature_correllation.iloc[i,j] >= self.corr_threshold:\n                    if columns[j]:\n                        columns[j] = False\n        self.cols_to_drop = list(set(num_cols) - set(feature_correllation.columns[columns]))\n        return self\n    \n    def transform(self, X):\n        # Make a copy so that X is not changed\n        X_copy = X.copy()\n        X_copy.drop(axis=1, columns=self.cols_to_drop, inplace=True)\n        return X_copy\n    \nclass Imputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n        self.num_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n        self.cat_cols = [cname for cname in X.columns if X[cname].dtype == 'object']\n        self.zero_imputer = SimpleImputer(strategy='constant', fill_value=0.0)\n        self.medi_imputer = SimpleImputer(strategy='median')\n        self.none_imputer = SimpleImputer(strategy='constant', fill_value='none')\n        self.freq_imputer = SimpleImputer(strategy='most_frequent')\n        \n        cols_to_clean = {}\n\n        # Numeric data\n        cols_to_clean['num_cols_zeros'], cols_to_clean['num_cols_median'], cols_to_clean['num_cols_skew'] = [],[],[]\n\n        for column in self.num_cols:\n            if X[column].min() == 0:\n                cols_to_clean['num_cols_zeros'].append(column)\n            else:\n                cols_to_clean['num_cols_median'].append(column)\n        \n        # Categorical data\n        cols_to_clean['cat_cols_freq'], cols_to_clean['cat_cols_none'] = [], []\n\n        for column in self.cat_cols:\n            if X[column].isnull().sum() <= 30:\n                cols_to_clean['cat_cols_freq'].append(column)\n            else:\n                cols_to_clean['cat_cols_none'].append(column)\n        \n        # Fit all of the transforms\n        for cname in cols_to_clean['num_cols_zeros']:\n            self.zero_imputer.fit(X[[cname]])\n            \n        for cname in cols_to_clean['num_cols_median']:\n            self.medi_imputer.fit(X[[cname]])\n        \n        for cname in cols_to_clean['cat_cols_none']:\n            self.none_imputer.fit(X[[cname]])\n            \n        for cname in cols_to_clean['cat_cols_freq']:\n            self.freq_imputer.fit(X[[cname]])\n        \n        self.cols_to_clean = cols_to_clean\n        return self\n    \n    def transform(self, X):\n        \n        # Make a copy so that X is not changed\n        X_copy = X.copy()\n        \n        # Apply transformations\n        for cname in self.cols_to_clean['num_cols_zeros']:\n            X_copy[cname] = self.zero_imputer.transform(X[[cname]])\n            \n        for cname in self.cols_to_clean['num_cols_median']:\n            X_copy[cname] = self.medi_imputer.transform(X[[cname]])\n        \n        for cname in self.cols_to_clean['cat_cols_none']:\n            X_copy[cname] = self.none_imputer.transform(X[[cname]])\n            \n        for cname in self.cols_to_clean['cat_cols_freq']:\n            X_copy[cname] = self.freq_imputer.transform(X[[cname]])\n            \n        return X_copy\n    \nclass Encoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n        self.num_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n        self.cat_cols = [cname for cname in X.columns if X[cname].dtype == 'object']\n        \n        self.count_encoder = ce.CountEncoder()\n        self.target_encoder = ce.TargetEncoder(cols=self.cat_cols)\n        self.OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        \n        self.count_encoder.fit(X[self.cat_cols])\n        self.target_encoder.fit(X[self.cat_cols], y)\n        self.OH_encoder.fit(X[self.cat_cols])\n        \n        return self\n    \n    def transform(self, X):\n        # Make a copy so that X is not changed\n        X_copy = X.copy()\n        \n        count_encoded = np.log1p(self.count_encoder.transform(X[self.cat_cols]))\n        X_copy = X_copy.join(count_encoded.add_suffix(\"_count\"))\n        \n        target_encoded = self.target_encoder.transform(X[self.cat_cols])\n        X_copy = X_copy.join(target_encoded.add_suffix(\"_target\"))\n        \n        OH_X = pd.DataFrame(self.OH_encoder.transform(X[self.cat_cols]))\n        OH_X.index = X_copy.index\n        \n        X_copy = X_copy.drop(self.cat_cols, axis=1)\n        X_copy = pd.concat([X_copy, OH_X], axis=1)\n        \n        return X_copy\n    \nclass Scaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n        self.num_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n        self.scaler = StandardScaler()\n        for cname in self.num_cols:\n            self.scaler.fit(X[[cname]]) \n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        for cname in self.num_cols:\n            X_copy[cname] = self.scaler.transform(X[[cname]])\n        return X_copy","a3a3ad32":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.feature_selection import SelectFromModel\n\nclass Select_Features(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=\"1.1*mean\"):\n        self.threshold = threshold\n    \n    def fit(self, X, y=None):\n        decision_tree = DecisionTreeRegressor(random_state=8)\n        select_features = SelectFromModel(decision_tree, threshold=self.threshold) # Threshold parameter can be optimised in CV\n        self.select_features = select_features.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy = self.select_features.transform(X)\n        feature_index = self.select_features.get_support()\n        feature_name = X.columns[feature_index]\n        return X_copy","651f8a0d":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Set of parameters to adjust randomly for cross validation\nparams = {\n    'model__n_estimators' : np.arange(100,2000,50),\n    'model__learning_rate' : np.arange(0.01,0.2,0.01),\n    'model__max_depth' : np.arange(1,4,1),\n    'drop_features__drop_threshold' : np.arange(0.65,1.0, 0.05),\n    'log_skew__skew_threshold' : np.arange(0.75, 1.0, 0.05)\n}\n\nmodel = XGBRegressor()\n\nfull_pipeline = Pipeline([\n    ('drop_features', Drop_Features()),\n    ('add_features', Add_Features()),\n    ('log_skew', Skewed_Features()),\n    ('remove_colinear', Colinear_Features()),\n    ('impute', Imputer()),\n    ('encode', Encoder()),\n#     ('cat_interactions', Create_Interactions()),\n    ('scale', Scaler()),\n#     ('select_features', Select_Features()),\n    ('model', model)\n])\n\n# Cross validation with a random search\nparam_search = RandomizedSearchCV(full_pipeline, param_distributions=params,\n                                 n_iter=300,\n                                 cv=5,\n                                 scoring='neg_mean_squared_error',\n                                 verbose=False,\n                                 random_state=8)\n\nparam_search.fit(X_train, y)\n\n# Print randomized CV results in a list\nprint(\"Random search:\")\ncv_results = param_search.cv_results_\nfor mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n    print(-1*mean_score, params)\n\nprint(\"\\nBest random search results:\")\nprint(-1*param_search.best_score_,param_search.best_params_)\n\n# Best results:\n# 0.014161684193906307 {'model__n_estimators': 500, 'model__max_depth': 2, 'model__learning_rate': 0.060000000000000005}\n\n# Now use grid search to find more optimised hyper parameters\nparams = {\n    'model__n_estimators' : np.arange(param_search.best_params_['model__n_estimators']-20,param_search.best_params_['model__n_estimators']+20,2),\n    'model__learning_rate' : np.arange(param_search.best_params_['model__learning_rate']-0.01,param_search.best_params_['model__learning_rate']+0.01,0.005),\n    'model__max_depth' : [param_search.best_params_['model__max_depth']],\n    'drop_features__drop_threshold' : [param_search.best_params_['drop_features__drop_threshold']],\n    'log_skew__skew_threshold' : [param_search.best_params_['log_skew__skew_threshold']]\n}\n\n# Best so far tuning\n# 0.014075478857627461 {'model__learning_rate': 0.065, 'model__max_depth': 2, 'model__n_estimators': 493}\n\nparam_search = GridSearchCV(full_pipeline, param_grid=params,\n                           cv=5,\n                           scoring='neg_mean_squared_error',\n                           verbose=False)\n\nparam_search.fit(X_train, y)\n\nprint(\"\\nGrid search:\")\ncv_results = param_search.cv_results_\nfor mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n    print(-1*mean_score, params)\n    \nprint(\"\\nBest grid search results:\")\nprint(-1*param_search.best_score_,param_search.best_params_)\n\n# Refit the model using the whole dataset\nparam_search.refit\n\n# Make predictions\npreds = param_search.predict(X_test)\npreds = np.expm1(preds)","5d6600a0":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","cccce917":"# Turn numeric categorical data into categorical data","1d1c0f80":"# Find good features (L1 regularization)\nDecreased performance","035c62b3":"# Feature engineering transformers","d78e4990":"# Cross validation","bdacc703":"# Remove outliers"}}