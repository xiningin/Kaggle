{"cell_type":{"bbd3a00b":"code","1c66a8e1":"code","3f3583c7":"code","029f4d68":"code","3fa74d6b":"code","0f987207":"code","033b5c29":"code","86642ac6":"code","55bfee63":"code","3a9113e0":"code","43c40aaf":"code","057a6cce":"code","f6f20941":"code","ad7ee2ff":"code","680827e8":"code","76262ae8":"code","f3b0bda9":"code","d1fd0970":"code","db39183c":"markdown","02f28aaf":"markdown","31226f0f":"markdown","760ccfeb":"markdown","c07b7abd":"markdown","83a45391":"markdown","3c922b39":"markdown","428139c5":"markdown","0fe5a17c":"markdown","4be821a2":"markdown"},"source":{"bbd3a00b":"#We will be using packages: transformers, torch, nlp, datasets\n!pip install accelerate==0.3.0 transformers==4.10.3 nlp==0.4.0 datasets==1.9.0","1c66a8e1":"import json\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport datasets\nfrom datasets import load_dataset, dataset_dict\nimport nlp\nimport dataclasses\nimport logging\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport transformers\n#torch \nimport torch\nimport torch.nn  as nn\nfrom torch.utils.data.dataloader import DataLoader\nfrom transformers.training_args import is_tpu_available\nfrom transformers.trainer import get_tpu_sampler\nfrom transformers.data.data_collator import DataCollator, InputDataClass\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler\nfrom typing import List, Union, Dict\nlogging.basicConfig(level=logging.info)","3f3583c7":"#Loading datasets \ndataset_dict = {\n    \"mnli\": datasets.load_dataset('glue', 'mnli'),\n    \"commonsense_qa\": datasets.load_dataset('commonsense_qa',name='commonsense_qa'),\n    \"stsb_multi_mt\": datasets.load_dataset(\"stsb_multi_mt\",name=\"en\")\n}","029f4d68":"for key, value in  dataset_dict.items():\n    print(\"task:\", key)\n    print(dataset_dict[key]['train'][2])\n    print(\"\\n\")","3fa74d6b":"l=dataset_dict['stsb_multi_mt']['train']['similarity_score']\nfig=plt.figure()\nfig=plt.figure()\nax = fig.add_axes([0,0,1,1])\nlangs = ['Not equivalent', 'meduim equivalent', 'most equivalent']\nstudents = [len([x for x in l if int(x)<2]),len([x for x in l if int(x)>2]),len([x for x in l if int(x)==2])]\nax.bar(langs,students)\nplt.xlabel(\"Similarity\")\nplt.ylabel(\"#Samples\")\nplt.title(\"Distribution of samples according to similarity score\")\nplt.show()","0f987207":"l=dataset_dict['mnli']['train']['label']\nl1=dataset_dict['mnli']['train']['label']\nlabels = ['Positive', 'Negative']\ndata = [len([x for x in l if int(x)==0]),  len([x for x in l if int(x)==1]) ]\n\n#test=[len([x for x in l if int(x)==1]),len([x for x in l1 if int(x)==1])]\n           \nx = np.arange(len(labels)) \nwidth = 0.35\nfig, ax = plt.subplots()\nax.bar( labels, data)\n#rects2 = ax.bar( x + width\/2, test, width, label='Positive')\n\nax.set_ylabel('#Samples')\nax.set_title('Distribution of training data by similarity')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nfig.tight_layout()\nplt.show()","033b5c29":"class MultitaskModel(transformers.PreTrainedModel):\n\n    def __init__(self, encoder, taskmodels_dict):\n        \n        \"\"\"\n        Setting MultitaskModel up as a PretrainedModel allows us\n        to take better advantage of Trainer features\n        \"\"\"\n        super().__init__(transformers.PretrainedConfig())\n\n        self.encoder = encoder\n        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n    \n    @classmethod\n    def create_model(cls, model_name, model_type_dict, model_config_dict):\n        \"\"\"\n        This creates a MultitaskModel using the model class and config objects\n        from single-task models. \n\n        We do this by creating each single-task model, and having them share\n        the same encoder transformer.\n        \"\"\"\n        shared_encoder = None\n        task_models_dict = {}\n        for task, model_type in model_type_dict.items():\n            print(task)\n            print(model_type)\n            model = model_type.from_pretrained(\n                model_name, \n                config=model_config_dict[task],\n            )\n            if shared_encoder is None:\n                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n            else:\n                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n            task_models_dict[task] = model\n        return cls(encoder=shared_encoder, taskmodels_dict=task_models_dict)\n    \n        \n    @classmethod              \n    def get_encoder_attr_name(cls, model):\n        \n        \n        \"\"\"\n        Each encoder has its attributes according to model architecture: BERT, Roberta,Alberta \n        This function gets attribute of the encoder.\n        \"\"\"\n        model_class_name = model.__class__.__name__\n        if model_class_name.startswith('Bert'):\n            return 'bert'\n        if model_class_name.startswith('Roberta'):\n            return 'roberta'\n        if model_class_name.startswith('Albert'):\n            return 'albert'\n        if model_class_name.startswith():\n            return \n            \n        else:\n                raise KeyError(f\"Add support for new model {model_class_name}\")\n                \n        \n    def forward(self, task, **kwargs):\n        return self.taskmodels_dict[task](**kwargs)","86642ac6":"model_name='roberta-base'\nmultitask_model=MultitaskModel.create_model(model_name=model_name, \n         model_type_dict={\n                 \"mnli\" : transformers.AutoModelForSequenceClassification,\n                 \"commonsense_qa\": transformers.AutoModelForMultipleChoice,\n                 \"stsb_multi_mt\"          : transformers.AutoModelForSequenceClassification\n         },\n         model_config_dict={\n                 \"mnli\" : transformers.AutoConfig.from_pretrained(model_name,num_labels=1),\n                 \"commonsense_qa\": transformers.AutoConfig.from_pretrained(model_name),\n                 \"stsb_multi_mt\"          : transformers.AutoConfig.from_pretrained(model_name,num_labels=1)\n             \n         }                                   \n            )","55bfee63":"if model_name.startswith('roberta-base'):\n    print(multitask_model.encoder.embeddings.word_embeddings.weight.data_ptr())\n    print(multitask_model.taskmodels_dict['mnli'].roberta.embeddings.word_embeddings.weight.data_ptr())\n    print(multitask_model.taskmodels_dict['commonsense_qa'].roberta.embeddings.word_embeddings.weight.data_ptr())\n    print(multitask_model.taskmodels_dict['stsb_multi_mt'].roberta.embeddings.word_embeddings.weight.data_ptr())\nelse:\n    print(\"Ckeck the model name\")","3a9113e0":"tokenizer=transformers.AutoTokenizer.from_pretrained(model_name)","43c40aaf":"max_length=256\ndef convert_to_mnli_inputs(example_batch):\n    \n    inputs=list(zip(example_batch['premise'],example_batch['hypothesis']))\n    features=tokenizer.batch_encode_plus(inputs, max_length=max_length, \n                                         pad_to_max_length=True,truncation=True)\n    features['labels']=example_batch['label']\n    return features\n\ndef convert_to_stsb_multi_mt_inputs(example_batch):\n    \n    inputs=list(zip(example_batch['sentence1'],example_batch['sentence2']))\n    features=tokenizer.batch_encode_plus(inputs, max_length=max_length, \n                                         pad_to_max_length=True,truncation=True)\n    features['labels']=example_batch['similarity_score']\n    return features\n\ndef convert_to_commonsense_qa_inputs(example_batch):\n    \n    number_examples=len(example_batch['question'])\n    number_choices=len(example_batch['choices'][0]['text'])\n    features={}\n    for example in range(number_examples):\n        choices=tokenizer.batch_encode_plus(\n            list(zip([example_batch['question'][example]]*number_choices,\n                        example_batch['choices'][example]['text'])),\n            max_length=max_length, pad_to_max_length=True ,truncation=True\n                    )\n        for k,v in choices.items():\n            if k not in  features:\n                features[k]=[]\n            features[k].append(v)\n    labels2id={char: x for x, char in enumerate('ABCDE')}\n    if example_batch['answerKey'][0]:\n        features['labels']= [labels2id[ans] for ans in example_batch['answerKey']]\n    else:\n        features['labels']=[0]*number_examples\n    return features\n\n#Construct the featurized input data\nfeaturized_funct_dict={\n                 \"mnli\"          : convert_to_mnli_inputs,\n                 \"commonsense_qa\": convert_to_commonsense_qa_inputs,\n                 \"stsb_multi_mt\" : convert_to_stsb_multi_mt_inputs\n}\n\ncolumn_dict={\n                 \"mnli\"          : ['input_ids', 'attention_mask','labels'],\n                 \"commonsense_qa\": ['input_ids', 'attention_mask','labels'],\n                 \"stsb_multi_mt\" : ['input_ids', 'attention_mask','labels']\n}\n#Featurizing datasets\nfeatures_dict={}\nfor  task, dataset in dataset_dict.items():\n    print(\"--------------task---------:\",task)\n    features_dict[task]={}\n    for phase, phase_dataset in dataset.items():\n       \n        features_dict[task][phase]=phase_dataset.map(featurized_funct_dict[task],\n                                                     batched=True, \n                                                     load_from_cache_file=False)\n        print(task, phase, len(phase_dataset), len(features_dict[task][phase]))\n        features_dict[task][phase].set_format(\n            type='torch',\n            columns=column_dict[task]\n        )\n        print(task, phase, len(phase_dataset), len(features_dict[task][phase]))","057a6cce":"#Class to sample bacth from the featurized datasets, The Datac is extended to work with batches\n#Data collators are objects that will form a batch by using a list of dataset elements as input\nclass NLPDataCollator(DataCollator):\n    def collate_batch(self, features:List[Union[InputDataClass,Dict]]) -> Dict[str,torch.Tensor]:\n        first=features[0]\n        if isinstance(first,dict):\n            \n            #featurized dataset are in the form of list of dictionaries\n            #Adapt the DataCollator to have a list of dictionary\n            if \"labels\" in first and first[\"labels\"] is not None:\n                if first[\"labels\"].dtype == torch.int64:\n                    labels = torch.tensor(\n                        [f[\"labels\"] for f in features], dtype=torch.float\n                    )\n                else:\n                    labels = torch.tensor(\n                        [f[\"labels\"] for f in features], dtype=torch.float\n                    )\n                batch = {\"labels\": labels}\n            for k, v in first.items():\n                if k != \"labels\" and v is not None and not isinstance(v, str):\n                    batch[k] = torch.stack([f[k] for f in features])\n            return batch\n        else:\n            # otherwise, revert to using the default collate_batch\n            return DefaultDataCollator().collate_batch(features)\n\n                    \nclass StrIgnoreDevice(str):\n    \"\"\"\n    This is a hack. The Trainer is going call .to(device) on every input\n    value, but we need to pass in an additional `task_name` string.\n    This prevents it from throwing an error\n    \"\"\"\n    def to(self, device):\n        return self        ","f6f20941":"#Class to load data with its task name. Decorator for changing Dataloader function to use a task name\nclass DataLoaderTaskname:\n    def __init__(self, task, data_loader):\n        self.task=task\n        self.data_loader=data_loader\n        self.batch_size=data_loader.batch_size\n        self.dataset=data_loader.dataset\n        \n    def __len__(self):\n        return len(self.data_loader)\n        \n    def __iter__(self):\n        for batch in self.data_loader:\n            batch[\"task\"]=StrIgnoreDevice(self.task)\n            yield batch","ad7ee2ff":"#Class to combine several data loaders into a single \"data loader\" \nclass MultitaskDataLoader:\n    \n    def __init__(self, dataloader_dict):\n        self.dataloader_dict =dataloader_dict\n        self.num_batches_dict= {task:len(dataloader) for task, dataloader in self.dataloader_dict.items()}\n        self.task_lst        =list(self.dataloader_dict)\n        self.dataset = [None] * sum(len(dataloader.dataset) for dataloader in self.dataloader_dict.values())\n    def __len__(self):\n        return sum(self.num_batches_dict.values())\n    \n    def __iter__(self):\n        \"\"\"\n        For each batch, sample a task, and get a batch from the respective task Dataloader.\n\n        We use size-proportional sampling, but you could easily modify this\n        to sample from some-other distribution.\n        \"\"\"\n        task_choice_list = []\n        for i, task in enumerate(self.task_lst):\n            task_choice_list += [i] * self.num_batches_dict[task]\n        task_choice_list = np.array(task_choice_list)\n        np.random.shuffle(task_choice_list)\n        dataloader_iter_dict = {\n            task: iter(dataloader) \n            for task, dataloader in self.dataloader_dict.items()\n        }\n        for task_choice in task_choice_list:\n            task = self.task_lst[task_choice]\n            yield next(dataloader_iter_dict[task])   ","680827e8":"#class to set up the trainer \nclass MultitaskTrainer(transformers.Trainer):\n    \n    def single_task_dataloader(self,task,train_dataset):\n        \"\"\"\n        returns the single task data loader of a given task \n        \"\"\"\n        if self.train_dataset is None:\n            raise ValueError(\"Trainer needs a dataset...:(\")\n        if is_tpu_available():\n            train_sampler=get_tpu_sampler(train_dataset)\n        else:\n            train_sampler=(RandomSampler(train_dataset)  if self.args.local_rank== -1 \n                                                            else DistributedSampler(train_dataset))\n    \n        data_loader=DataLoaderTaskname(task=task,data_loader=DataLoader(train_dataset,\n                                                                       batch_size=self.args.train_batch_size,\n                                                                       sampler=train_sampler,\n                                                                       collate_fn=self.data_collator.collate_batch\n                                                                      ))   \n        if is_tpu_available():\n            data_loader=pl.ParallelLoader(\n                data_loader, [self.args.device]\n            ).per_device_loader(self.args.device)\n        return data_loader\n    \n    \n    def get_train_dataloader(self):\n        \"\"\"\n        Returns a MultitaskDataLoader, which is not actually a Dataloader\n        but an iterable that returns a generator that samples from each \n        task Dataloader\n        \"\"\"\n        return MultitaskDataLoader({\n            task: self.single_task_dataloader(task, task_dataset)\n            for task, task_dataset in self.train_dataset.items()\n        })","76262ae8":"train_dataset = {\n    task: dataset[\"train\"] \n    for task, dataset in features_dict.items()\n}\ntrainer = MultitaskTrainer(\n    model=multitask_model,\n    args=transformers.TrainingArguments(\n        output_dir=\".\/models\/multitask_model\",\n        overwrite_output_dir=True,\n        learning_rate=1e-5,\n        do_train=True,\n        num_train_epochs=2,\n        # Adjust batch size if this doesn't fit on the Colab GPU\n        per_device_train_batch_size=8,  \n        save_steps=3000,\n    ),\n    data_collator=NLPDataCollator(),\n    train_dataset=train_dataset,\n)\ntrainer.train()","f3b0bda9":"preds_dict = {}\nfor task in [\"mnli\", \"commonsense_qa\", \"stsb_multi_mt\"]:\n    eval_dataloader = DataLoaderTaskname(\n        task,\n        trainer.get_eval_dataloader(eval_dataset=features_dict[task][\"validation\"])\n    )\n    print(eval_dataloader.data_loader.collate_fn)\n    preds_dict[task] = trainer._prediction_loop(\n        eval_dataloader, \n        description=f\"Validation: {task}\",\n    )","d1fd0970":"def save_model(model_name, multitask_model):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n    for task_name in [\"spaadia_squad_pairs\", \"quora_keyword_pairs\"]:\n        multitask_model.taskmodels_dict[task_name].config.to_json_file(\n            f\".\/{task_name}_model\/config.json\"\n        )\n        torch.save(\n            multitask_model.taskmodels_dict[task_name].state_dict(),\n            f\".\/{task_name}_model\/pytorch_model.bin\",\n        )\n        tokenizer.save_pretrained(f\".\/{task_name}_model\/\")","db39183c":"## This notebook demonstrates the multitask learning process with single encoder and with shared encoder transformers. It shows the ability of single vs shared encoder multihead attention based tasks.\n### Three tasks to learn:\n**>    Task # 1- Multiple-choice question\/answering**\n\n**>    Task # 2- Two-sentence similarity**\n\n**>    Task # 3-Two-sentence entailment predicts**\n\n### 1- Additional prediction heads to the BERT model:\n![image.png](attachment:589ce78e-d3af-41b3-908c-03b887b14f96.png)\n\n\n\n### 2- BERT-based model shared across multiple tasks being learned:\n![image.png](attachment:1292ad42-9f45-44e2-a5df-e6c88c654dbc.png)","02f28aaf":"#### More detailed codes are [here](https:\/\/github.com\/shahrukhx01\/multitask-learning-transformers) ","31226f0f":"# Build a Multi-task Model","760ccfeb":"**Some visualization**","c07b7abd":"## Tasks to learn: \n### 1- Multiple-choice question\/answering prediction: \"CommonsenseQA\" dataset [link]((https:\/\/huggingface.co\/datasets\/commonsense_qa)).  \n### An example:\n\n> {\n\n>     \"answerKey\": \"B\",\n\n>     \"choices\": {\n\n>         \"label\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n\n>         \"text\": [\"mildred's coffee shop\", \"mexico\", \"diner\", \"kitchen\", \"canteen\"]\n\n>     },\n\n>     \"question\": \"In what Spanish speaking North American country can you get a great cup of coffee?\"\n> }\n### 2- Two-sentence entailment predicts whether the premise entails the hypothesis: \"mnli\" dataset [link](https:\/\/huggingface.co\/datasets\/glue).\n### An example:\n\n> {\n> \n> 'premise': 'One of our number will carry out your instructions minutely.', \n> \n> 'hypothesis': 'A member of my team will execute your orders with immense precision.',\n> \n> 'label': 0, \n> \n> 'idx': 2\n> \n> }\n\n### 3- Two-sentence similarity: stsb_multi_mt dataset [link](https:\/\/huggingface.co\/datasets\/stsb_multi_mt)\n> {\n> \n>     \"sentence1\": \"A man is playing a large flute.\",\n>     \n>     \"sentence2\": \"A man is playing a flute.\",\n>     \n>     \"similarity_score\": 3.8\n> \n> }","83a45391":"### check if all three models to be trained share the same encoder ","3c922b39":"**We have created our multi-task model by fusing several single-task Transformer models.**\n\n**We have featurized NLP datasets to construct inputs for each of our tasks.**\n\n> **Now, its is time to set updata loader and set up the Trainer then , train the single encoder model with three tasks included. We should feed batches into the multitask model by sampling from the featured dataset** ","428139c5":"## Data preprocesing:\n\n\n#### 1- We should transform the dictionary of dataset to NLP model inputs: Get the tokenizer corresponding to our model.\n\n#### 2- Preparing the inputs for a model: **mnli** and **sst2** datasets are two-sentence inputs data we should concatenate the inputs in one vector. While **commonsense_qa** has multi-choice inputs with question. One   ","0fe5a17c":"# Training Multitask model","4be821a2":"# Prediction "}}