{"cell_type":{"af6d434e":"code","899fa9d4":"code","c6dc5797":"code","3deb2539":"code","c5786739":"code","4fed6085":"code","164deb5d":"code","5518ec1d":"code","5b71ab72":"code","a8388f8a":"code","bb2886b7":"code","06cc9898":"code","cb3ca32e":"code","6882c411":"code","68aa434f":"code","6f3fb462":"code","b64d0fc1":"code","fd7ba5fb":"code","02e223ce":"code","51ba1871":"code","de3740b8":"code","8613117a":"code","92f148fd":"code","c8039010":"code","401f88df":"code","01b92966":"code","0797705a":"code","13110a6c":"code","bd603633":"code","b0e1e6bc":"code","c572dae0":"code","74a4b8b9":"code","78e8f077":"code","26d33337":"code","d0145e85":"code","cedd1e96":"markdown","bb0b7f01":"markdown","27d167f7":"markdown","db3f42b7":"markdown","310ea0a3":"markdown","05037974":"markdown","9120e9c1":"markdown","e734cee4":"markdown"},"source":{"af6d434e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport dask.dataframe as dd\npd.options.mode.chained_assignment = None  # default='warn'\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","899fa9d4":"# https:\/\/gdmarmerola.github.io\/big-data-ml-training\/\n# https:\/\/github.com\/gdmarmerola\/big-data-ml-training\/blob\/master\/track_memory.py\n\n# libs to help us track memory via sampling\nimport numpy as np\nimport tracemalloc\nfrom time import sleep\nimport matplotlib.pyplot as plt\n\n# sampling time in seconds\nSAMPLING_TIME = 0.001\n\nclass MemoryMonitor:\n    def __init__(self, close=True):\n        \n        # start tracemalloc and sets\n        # measurement atribute to True\n        tracemalloc.start()\n        self.keep_measuring = True\n        self.close = close\n        \n    def measure_usage(self):\n        \n        \"\"\"\n        Takes measurements of used memory on\n        regular intevals determined by the \n        global SAMPLING_TIME constant\n        \"\"\"\n        \n        # list to store memory usage samples\n        usage_list = []\n        \n        # keeps going until someone changes this parameter to false\n        while self.keep_measuring:\n            \n            # takes a sample, stores it in the usage_list and sleeps\n            current, peak = tracemalloc.get_traced_memory()\n            usage_list.append(current\/1e6)\n            sleep(SAMPLING_TIME)\n            \n        # stop tracemalloc and returns list\n        if self.close:\n            tracemalloc.stop()\n        return usage_list\n\n# imports executor\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import wraps\n\ndef plot_memory_use(history, fn_name, open_figure=True, offset=0, **kwargs):\n    \n    \"\"\"Function to plot memory use from a history collected\n        by the MemoryMonitor class\n    \"\"\"\n\n    # getting times from counts and sampling time\n    times = (offset + np.arange(len(history))) * SAMPLING_TIME\n    \n    # opening figure and plotting\n    if open_figure:\n        plt.figure(figsize=(10,3), dpi=120)\n    plt.plot(times, history, 'k--', linewidth=1)\n    plt.fill_between(times, history, alpha=0.5, **kwargs)\n    \n    # axes titles\n    plt.ylabel('Memory usage [MB]')\n    plt.xlabel('Time [seconds]')\n    plt.title(f'{fn_name} memory usage over time')\n    \n    # legend\n    plt.legend();\n\ndef track_memory_use(plot=True, close=True, return_history=False):\n    \n    def meta_wrapper(fn):\n    \n        \"\"\"\n        This function is meant to be used as a decorator\n        that informs wrapped function memory usage\n        \"\"\"\n        \n        # decorator so we can retrieve original fn\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n\n            \"\"\"\n            Starts wrapped function and holds a process \n            to sample memory usage while executing it\n            \"\"\"\n\n            # context manager for executor\n            with ThreadPoolExecutor() as executor:\n\n                # start memory monitor\n                monitor = MemoryMonitor(close=close)\n                mem_thread = executor.submit(monitor.measure_usage)\n\n                # start wrapped function and get its result\n                try:\n                    fn_thread = executor.submit(fn, *args, **kwargs)\n                    fn_result = fn_thread.result()\n\n                # when wrapped function ends, stop measuring\n                finally:\n                    monitor.keep_measuring = False\n                    history = mem_thread.result()\n\n                # inform results via prints and plot\n                print(f'Current memory usage: {history[-1]:2f}')\n                print(f'Peak memory usage: {max(history):2f}')\n                if plot:\n                    plot_memory_use(history, fn.__name__)\n            if return_history:\n                return fn_result, history\n            else:\n                return fn_result\n\n        return wrapper\n    \n    return meta_wrapper","c6dc5797":"import multiprocessing\n# using a function so we can track memory usage\n@track_memory_use(close=False, return_history=False)\ndef pandas_read():\n    \n    # reading train data\n    train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\n    test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\n    ddX = dd.from_pandas(train_df.drop(['Survived'], axis=1), npartitions=4*multiprocessing.cpu_count())\n    ddy = dd.from_pandas(train_df.Survived, npartitions=4*multiprocessing.cpu_count())\n    test_df = dd.from_pandas(test_df, npartitions=4*multiprocessing.cpu_count())\n    \n#     X = train_df.drop(['Survived'], axis=1)\n#     y = train_df.Survived\n    return train_df, test_df, ddX, ddy\n\n# executing\ntrain_df, test_df, X, y = pandas_read()","3deb2539":"numerical_features = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\nnumerical_features.remove('Pclass')\nprint('\\nNumerical columns:', numerical_features)","c5786739":"categorical_features = [col for col in X.columns if X[col].dtype == \"object\"] + ['Pclass']\nprint('\\nCategorical columns:', categorical_features)","4fed6085":"import numpy as np \nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, PowerTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import FeatureUnion, Pipeline \nimport string\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass FeatureSelector( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self.feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n        return X[ self.feature_names ] ","164deb5d":"#Custom transformer \nclass CategoricalTransformer( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self):\n        pass\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    ### Helper Functions\n    def sex_label_encoder(self, sex):\n        if(sex == 'male'):\n            return 0\n        elif(sex == 'female'):\n            return 1\n        return 2 # fallback - unknown sex\n    \n    #Helper function to extract cabin_prefix from column 'cabin' \n    def get_cabin_prefix(self, obj):\n        return str(obj)[0].upper()\n#         if (isinstance(obj, str)):\n#             return obj[0]\n#         return obj # fallback for np.nan (float type) --> missing value\n        \n#         try:\n# #             return obj[0]\n#             if (obj[0] == 'n'):\n#                 return obj\n#             return str(obj)[0]\n#         except Exception as e: # handle np.nan\n#             return e\n# #             return 'U' # stands for unkown cabin\n    \n    def remove_bracket_from_name(self, name):\n        if('(' in name):\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n        return name_no_bracket\n    \n    def extract_title_from_name(self, name):\n        try:\n            title = name.split(',')[1].strip().split(' ')[0].replace('.','')\n        except:\n            title = \"\"\n        return title\n    \n    def extract_family_from_name(self, name):    \n        family = name.split(',')[0]\n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n        return family\n    \n    def correct_title_names(self, title):\n        if(title == 'Mlle'):\n            return 'Miss'\n        elif(title == 'Ms'):\n            return 'Miss'\n        elif(title == 'Mme'):\n            return 'Mrs'\n        return title\n    \n    def clean_rare_title_names(self, X): \n#     def clean_rare_title_names(self): \n        stat_min = 10 #common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\n        title_names = (X['Title'].value_counts() < stat_min)\n        return X['Title'].apply(lambda x: 'Rare' if title_names.loc[x] == True else x)\n    \n    #Transformer method we wrote for this transformer \n    @track_memory_use(close=False, return_history=False)\n    def transform(self, X , y = None ):\n#         X.Sex = X.Sex.apply(func = self.sex_label_encoder, meta=('Sex', 'int64'))\n        # Label encode Sex\n        X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n        # Create Cabin_Prefix\n        X['Cabin_Prefix'] = X.Cabin.map_partitions(lambda row: row.apply(lambda cabin: self.get_cabin_prefix(cabin)), meta=('Cabin_Prefix', 'O'))\n# #         # normalize Name\n#         X.Name = X.Name.map_partitions(lambda row: row.apply(lambda name: self.remove_bracket_from_name(name)), meta=('Name', 'O'))\n#         # create Family_Name\n#         X['Family_Name'] = X.Name.map_partitions(lambda row: row.apply(lambda name: self.extract_family_from_name(name)), meta=('Family_Name', 'O'))\n    \n    \n    \n    \n    \n    \n    \n# #         # create Title\n#         X['Title'] = X.Name.map_partitions(lambda row: row.apply(lambda name: self.extract_title_from_name(name)), meta=('Title', 'O'))\n# #         # correct title names\n#         X.Title = X.Title.map_partitions(lambda row: row.apply(lambda title: self.correct_title_names(title)), meta=('Title', 'O'))\n        \n        # clean rare title names\n#         X.Title = X.map_partitions(lambda df: self.clean_rare_title_names(df))\n    \n                           \n\n\n        \n#         X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n#         X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n#         X.Sex = X.Sex.map_partitions(lambda row: row.apply(lambda sex: self.sex_label_encoder(sex)), meta=('Sex', 'int64'))\n    \n    \n    \n#         X.Sex = X.\\\n#                 map_partitions(\\\n#                                lambda df: df.apply\\\n#                                     ((lambda row: self.sex_label_encoder(row.Sex)), axis = 1)\\\n#                                , meta=X)\n# #                                , meta=pd.Series([], dtype=int, name='Sex'))\n        \n#        #using the helper functions written above \n#         # label encode sex\n#         X.loc[:, 'Sex'] = X['Sex'].apply( self.sex_label_encoder )\n#         # Create Cabin_Prefix\n#         X.loc[:,'Cabin_Prefix'] = X['Cabin'].apply( self.get_cabin_prefix )\n#         # normalize Name\n#         X.loc[:, 'Name'] = X['Name'].apply( self.remove_bracket_from_name )\n#         # create Title\n#         X.loc[:, 'Title'] = X['Name'].apply( self.extract_title_from_name )\n#         # correct title names\n#         X.loc[:,'Title'] = X['Title'].apply( self.correct_title_names )\n#         # clean rare title names\n# #         X.loc[:,'Title'] = X['Title'].apply( self.clean_rare_title_names )\n# #         X.loc[:,'Title'] = self.clean_rare_title_names(X)\n#         # create Family_Name\n#         X.loc[:, 'Family_Name'] = X['Name'].apply( self.extract_family_from_name )\n#         # create Ticket_Frequency\n#         X.loc[:, 'Ticket_Frequency'] = X.groupby('Ticket')['Ticket'].transform('count')\n       \n       # drop columns\n        X = X.drop(['Name', 'Ticket', 'Cabin'], axis = 1)\n       # debugging Dask:\n        X = X.compute(scheduler='processes')\n       #returns numpy array\n        return X.values","5518ec1d":"X.dtypes","5b71ab72":"#Custom transformer\nclass NumericalTransformer(BaseEstimator, TransformerMixin):\n    #Class Constructor\n    def __init__( self ):\n        pass\n        \n    #Return self, nothing else to do here\n    def fit( self, X, y = None ):\n        return self \n    \n    def create_family_size(self, df):\n        return df.SibSp + df.Parch + 1\n    \n    def create_calculated_fare(self, df):\n        return df.Fare \/ df.Family_Size\n    \n    #Custom transform method we wrote that creates aformentioned features and drops redundant ones \n    @track_memory_use(close=False, return_history=False)\n    def transform(self, X, y = None):\n        # create Family_Size\n        X['Family_Size'] = X.map_partitions(lambda df: self.create_family_size(df), meta=('Family_Size', 'int64'))\n        # create Calculated_Fare                                 \n        X['Calculated_Fare'] = X.map_partitions(lambda df: self.create_calculated_fare(df), meta=('Calculated_Fare', 'int64'))\n#         # create Calculated_Fare\n#         X.loc[:, 'Calculated_Fare'] = X['Fare'] \/ X['Family_Size']\n#         # drop redundant features\n#         X = X.drop(['PassengerId', 'Fare'], axis=1)\n        \n        X = X.drop(['PassengerId', 'Fare'] ,axis = 1) # 'Age', 'SibSp', 'Parch', \n        # debugging Dask:\n        X = X.compute(scheduler='processes')\n        \n        #returns a numpy array\n        return X.values","a8388f8a":"print('\\nCategorical columns:', categorical_features)\nprint('\\nNumerical columns:', numerical_features)","bb2886b7":"#Defining the steps in the categorical pipeline \ncategorical_pipeline = Pipeline( steps = \\\n    [ ( 'cat_selector', FeatureSelector(categorical_features) ),\n      ( 'cat_transformer', CategoricalTransformer() ), \n      ( 'imputer', SimpleImputer(strategy = 'most_frequent') ),\n      ( 'one_hot_encoder', OneHotEncoder(handle_unknown='ignore', sparse = False ) )\n     ])\n    \n#Defining the steps in the numerical pipeline     \nnumerical_pipeline = Pipeline( steps = \\\n  [ ( 'num_selector', FeatureSelector(numerical_features) ),\n    ( 'num_transformer', NumericalTransformer() ),                                  \n    ( 'imputer', SimpleImputer(strategy = 'median') ),\n    ( 'std_scaler', StandardScaler()),\n   # MinMaxScaler, StandardScaler, RobustScaler\n    ( 'power_transform', PowerTransformer(method = 'yeo-johnson'))\n   ])\n\n#Combining numerical and categorical piepline into one full big pipeline horizontally \n#using FeatureUnion\nfull_pipeline = FeatureUnion( transformer_list = \\\n    [ ( 'categorical_pipeline', categorical_pipeline ),         \n      ( 'numerical_pipeline', numerical_pipeline ) \n    ])","06cc9898":"# from sklearn.model_selection import train_test_split\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n# random_state = 42)","cb3ca32e":"# https:\/\/stackoverflow.com\/a\/54363480 - DEBUG --> CHANGE Dask loc assignment\n# https:\/\/stackoverflow.com\/a\/38776838 - DEBUG --> Change df.loc[row, column] syntax\n# full_pipeline.fit(X, y)","6882c411":"full_pipeline.fit(X, y)","68aa434f":"X_transformed = pd.DataFrame(full_pipeline.transform(X))\ny_transformed = y.compute()\nX_transformed","6f3fb462":"X_transformed.head(50)","b64d0fc1":"# a.loc[:, a.columns[3]].unique()","fd7ba5fb":"!pip install dask_ml\n! python -m pip install --upgrade dask\n! python -m pip install fsspec\n! python -m pip install --upgrade joblib","02e223ce":"# from dask_ml.model_selection import train_test_split\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 42)\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_transformed, y_transformed, train_size=0.8, test_size=0.2, random_state = 42)","51ba1871":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n# # Define the model\n# clf = XGBClassifier(n_estimators = 1000, learning_rate = 0.05, eval_metric = 'error') # Your code here\n\n# params = {'booster': ['gbtree', 'gblinear', 'dart']}\n\n# class XGBTransformer(BaseEstimator, TransformerMixin):\n#     #Class Constructor\n#     def __init__( self ):\n#         pass\n        \n#     #Return self, nothing else to do here\n#     def fit( self, X, y = None):\n#         return self \n    \n#     @track_memory_use(close=False, return_history=False)\n#     def transform(self, X, y = None):\n#         return xgb.DMatrix(X)\n\n# xgb_pipeline = Pipeline(steps=[\\\n#                                 ('data_wrangling', full_pipeline),\n#                                 ('xgb_dmatrix', XGBTransformer())\n#                                ])\n# xgb_pipeline.fit(X, y)\n# X_DMatrix = xgb_pipeline.transform(X)","de3740b8":"# https:\/\/www.kaggle.com\/prashant111\/a-guide-on-xgboost-hyperparameters-tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.metrics import accuracy_score\nfrom hyperopt.pyll import scope\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, q=1)),\n        'seed': 0,\n        'learning_rate' : hp.uniform('learning_rate', 0.01,0.1),\n        'booster' : [None, 'gbtree', 'gblinear', 'dart']\n    }\n\ndef objective(space):\n    clf=xgb.XGBClassifier(\n                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( X_train, y_train), ( X_valid, y_valid)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_valid)\n    accuracy = accuracy_score(y_valid, pred>0.5)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }\n\ntrials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 1000,\n                        trials = trials)","8613117a":"from pprint import pprint\nprint('best loss: -0.77535\\n')\nprint(\"The best hyperparameters are : \")\npprint(best_hyperparams)\n\n# best loss: -0.77535\n\n# The best hyperparameters are : \n# {'colsample_bytree': 0.8378618694781489,\n#  'gamma': 2.8242880151334067,\n#  'learning_rate': 0.07069892867801585,\n#  'max_depth': 14.0,\n#  'min_child_weight': 5.0,\n#  'n_estimators': 202.0,\n#  'reg_alpha': 40.0,\n#  'reg_lambda': 0.9194811165570513}","92f148fd":"# from pprint import pprint\n# print('best loss: -0.77535\\n')\n# print(\"The best hyperparameters are : \")\n# pprint(best_hyperparams)\n\n# best loss: -0.77535\n\n# The best hyperparameters are : \n# {'colsample_bytree': 0.8585598855009294,\n#  'gamma': 1.3997603558344998,\n#  'learning_rate': 0.03708950942418698,\n#  'max_depth': 10.0,\n#  'min_child_weight': 7.0,\n#  'n_estimators': 231.0,\n#  'reg_alpha': 90.0,\n#  'reg_lambda': 0.44294659050314755}","c8039010":"clf = xgb.XGBClassifier().set_params(**best_hyperparams)","401f88df":"from distributed import Client\nclient = Client(processes=False, n_workers=1, memory_limit='16GB')\nclient","01b92966":"client","0797705a":"from dask_ml.model_selection import HyperbandSearchCV\n\nclf = XGBClassifier(n_estimators = 1000, learning_rate = 0.05, eval_metric = 'error') # Your code here\nmodel = clf\nparams = {}\nspace={'max_depth': range(3, 18),\n        'gamma': range(1,9),\n        'reg_alpha' : range('reg_alpha', 40,180),\n        'reg_lambda' : range(0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, q=1)),\n        'seed': 0,\n        'learning_rate' : hp.uniform('learning_rate', 0.01,0.1),\n        'booster' : [None, 'gbtree', 'gblinear', 'dart']\n    }\n\nn_examples = 15 * len(X_train)\nn_params = 15\n\nmax_iter = n_params  # number of times partial_fit will be called\nchunks = n_examples \/\/ n_params  # number of examples each call sees\n\nprint((max_iter, chunks))\nsearch = HyperbandSearchCV(\n    model,\n    params,\n    max_iter=max_iter,\n    patience=True,\n)\nsearch.metadata[\"partial_fit_calls\"]","13110a6c":"# %%time\n# search.fit(X_train_dask, y_train_dask, classes=[0, 1])","bd603633":"# before_mean_train_score, before_mean_test_score, before_mean_test_score_std = [], [], []\n# after_mean_train_score, after_mean_test_score, after_mean_test_score_std = [], [], []\n# best_param = []\n# from tqdm import tqdm\n# for model_name, model in tqdm(MLA_dict.items()):\n#     base_results = model_selection.cross_validate(model, train_df[train_df_x_calc], train_df[Target], cv  = cv_split, return_train_score = True)\n#     model.fit(train_df[train_df_x_calc], train_df[Target])\n#     before_mean_train_score.append(base_results['train_score'].mean()*100)\n#     before_mean_test_score.append(base_results['test_score'].mean()*100)\n#     before_mean_test_score_std.append(base_results['test_score'].std()*100*3)\n    \n#     param_grid = parameters_dict[model_name]\n#     print(\"\")\n#     print(model_name)\n#     print(param_grid)\n#     tune_model = model_selection.GridSearchCV(model, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score = True)\n#     tune_model.fit(train_df[train_df_x_calc], train_df[Target])\n#     try:\n#         print(\"Best parameters for model: \" + model_name)\n#     except Exception as e:\n#         pass\n#     print(tune_model.best_params_)\n    \n#     after_mean_train_score.append(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)\n#     after_mean_test_score.append(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100)\n#     after_mean_test_score_std.append(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3)\n#     best_param.append(tune_model.best_params_)\n# print(\"DONE\")\n","b0e1e6bc":"# param = {}\n# param['booster'] = 'gbtree'\n# param['objective'] = 'binary:logistic'\n# param[\"eval_metric\"] = \"error\"\n# param['eta'] = 0.3\n# param['gamma'] = 0\n# param['max_depth'] = 6\n# param['min_child_weight']=1\n# param['max_delta_step'] = 0\n# param['subsample']= 1\n# param['colsample_bytree']=1\n# param['silent'] = 0\n# param['seed'] = 0\n# param['base_score'] = 0.5\n# param['random_state'] = 42\n# param['learning_rate'] = 0.05\n# param['n_estimators'] = 1000\n\n# clf = XGBClassifier(use_label_encoder=False)\n# clf.set_params(**param)\n# full_pipeline.fit(X, y)\n# # full_pipeline.transform(X_train)\n# clf.fit(pd.DataFrame(full_pipeline.transform(X)), y.compute())\n# # clf.fit(pd.DataFrame(full_pipeline.transform(X_train)), y_train.compute())\n# # clf.score(pd.DataFrame(full_pipeline.transform(X_valid)), y_valid.compute())","c572dae0":"# # make predictions which we will submit. \n# test_preds = clf.predict(pd.DataFrame(full_pipeline.transform(test_df)))\n\n# # The lines below shows how to save predictions in format used for competition scoring\n# # Just uncomment them.\n\n# output = pd.DataFrame({'PassengerId': test_df.compute().PassengerId,\n#                       'Survived': test_preds})\n# output.to_csv('submission.csv', index=False)\n# output","74a4b8b9":"# from xgboost import XGBClassifier\n# from sklearn.metrics import mean_absolute_error\n\n# # Define the model\n# clf = XGBClassifier(n_estimators = 1000, learning_rate = 0.05, eval_metric = 'error') # Your code here\n\n# # # Fit the model\n# clf.fit(X_train, y_train, early_stopping_rounds=5, \n#              eval_set=[(X_valid, y_valid)], \n#              verbose=False) \n\n# # # Get predictions\n# predictions = clf.predict(X_valid) \n\n# # # Calculate MAE\n# mae = mean_absolute_error(y_valid, predictions) # Your code here\n\n# # # Uncomment to print MAE\n\n# print(\"Mean Absolute Error:\" , mae)","78e8f077":"# from sklearn.model_selection import cross_val_score\n\n# # import joblib\n\n\n\n# model_pipeline = Pipeline( steps = \\\n# [ ( 'data_processing', X_transformed),\n#     ( 'num_transformer', NumericalTransformer() ),                                  \n#     ( 'imputer', SimpleImputer(strategy = 'median') ),\n#     ( 'std_scaler', StandardScaler()),\n#    # MinMaxScaler, StandardScaler, RobustScaler\n#     ( 'power_transform', PowerTransformer(method = 'yeo-johnson'))\n#    ])\n\n# full_pipeline = FeatureUnion( transformer_list = \\\n#     [ ( 'full_pipeline', full_pipeline ),         \n#       ( 'model_pipeline', model_pipeline ) \n#     ])\n\n# from dask.distributed import Client\n# import joblib\n\n# client = Client(processes=False)             # create local cluster\n# # client = Client(\"scheduler-address:8786\")  # or connect to remote cluster\n\n# with joblib.parallel_backend('dask', scatter = [X, y]):\n#     cross_val_score(full_pipeline, X, y)\n\n# # # Multiply by -1 since sklearn calculates *negative* MAE\n# # scores = -1 * cross_val_score(full_pipeline, X, y_transformed,\n# #                               cv=5,\n# #                               scoring='neg_mean_absolute_error')\n\n# # print(\"Average MAE score:\", scores.mean())","26d33337":"### ERROR ###\n# full_pipeline.transform(X)\n### ERROR ###","d0145e85":"# XGBoost Optimization\n# https:\/\/gist.github.com\/dirusali\n# import xgboost as xgb\n\n# class XGBTransformer(BaseEstimator, TransformerMixin):\n#     #Class Constructor\n#     def __init__( self ):\n#         pass\n        \n#     #Return self, nothing else to do here\n#     def fit( self, X, y = None ):\n#         return self \n    \n#     #Custom transform method we wrote that creates aformentioned features and drops redundant ones \n#     def transform(self, X, y = None):\n#         return xgb.DMatrix(X, label=y)\n\n# cat_num_pipeline = FeatureUnion( transformer_list = [\\\n#                                                   ( 'categorical_pipeline', categorical_pipeline ),         \n#                                                   ( 'numerical_pipeline', numerical_pipeline ) \n#                                                  ])\n# xgb_pipeline = Pipeline(steps=[\\\n#                                 ('data_wrangling', cat_num_pipeline),\n#                                 ('xgb_dmatrix', XGBTransformer())\n#                                ])\n# xgb_pipeline.fit(X, y)\n# # help(xgb_pipeline.transform(X_train))","cedd1e96":"# Pipeline","bb0b7f01":"Measure memory","27d167f7":"## Categorical Transformers\nhttps:\/\/towardsdatascience.com\/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65","db3f42b7":"# Create Transformers","310ea0a3":"# Dask HyperbandSearchCV","05037974":"## Numerical Transformers","9120e9c1":"# XGBoost Modeling","e734cee4":"# ERROR - pipeline.transform(X)\n## Your notebook tried to allocate more memory than is available. It has restarted.\nWhat's wrong with the transformers' loc[:, column_name]?  \nHow should I avoid:  \nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead"}}