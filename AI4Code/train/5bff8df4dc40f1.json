{"cell_type":{"e32edb40":"code","4b93a167":"code","6fd68c43":"code","44ef5baa":"code","a712ae91":"code","5f618f81":"code","c7183aa1":"code","6417ec38":"code","eadd241f":"code","4b88a992":"code","4c0bc89d":"code","eba19aa0":"code","be85b98e":"code","58764d87":"code","b9a14e59":"code","9c0c292f":"code","481a2807":"code","0efc46ec":"code","1495a29e":"code","061d30fd":"code","c6580fd9":"code","20238b51":"code","85c5dcb7":"markdown","a77bd5df":"markdown"},"source":{"e32edb40":"from kaggle.competitions import twosigmanews\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n%matplotlib inline","4b93a167":"# get the data from two sigma environment\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()","6fd68c43":"# gets used later to aggregate news into marker data \nnews_cols_agg = {\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n}\n# specify categorical columns\ncategorical_cols = ['assetName', 'dayofweek', 'month', 'year']\n# lengths of embeddings of categorical columns\nembedding_lengths = [100, 2, 2, 3]\nencodings = {}","44ef5baa":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")    \n    \n    # Expand assetCodes -- converts ['AAPL', 'GOOG'] --> 'APPL', 'GOOG'\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df","a712ae91":"def get_xy(market_train_df, news_train_df):\n    x = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y\n\ndef label_encode(series, min_counts=2):\n    vc = series.value_counts()\n    #reserve 0 for unknown\n    le = {c : i+1 for i, c in enumerate(vc.index[vc > min_counts])}\n    le['UNKN'] = 0\n    return le\n\ndef get_encodings(df, cat_cols):\n    if len(encodings) == 0:\n        for col in cat_cols:\n            encodings[col] = label_encode(df[col])\n    return encodings\n\ndef map_encodings(df, cat_cols, encs):\n    for col in cat_cols:\n        df[col] = df[col].map(encs[col]).fillna(0).astype(int)\n        \ndef get_x(market_train_df, news_train_df, isTrain=True):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)    \n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    x['dayofweek'], x['day'], x['month'], x['year'] = x.time.dt.dayofweek, x.time.dt.day, x.time.dt.month, x.time.dt.year\n\n    encodings = get_encodings(x, categorical_cols)\n    map_encodings(x, categorical_cols, encodings) \n    if isTrain:\n        cols_to_drop = ['returnsOpenNextMktres10', 'universe', 'time']\n    else: \n        cols_to_drop = ['time']\n    \n    x.drop(columns=cols_to_drop, inplace=True)\n        \n    return x","5f618f81":"# This will take some time...\nX, y = get_xy(market_train_df, news_train_df)","c7183aa1":"#Save universe data for latter use\nuniverse = market_train_df['universe']\ntime = market_train_df['time']\n\n# Free memory\ndel market_train_df, news_train_df","6417ec38":"#get all the numeric columns\nnum_cols = [x for x in X.columns if x not in categorical_cols]\n\n#remove assetCode from num_cols\nnum_cols = [x for x in num_cols if x not in ['assetCode']]","eadd241f":"#scale numeric cols\ndef scale_numeric(df):\n    df[num_cols] = df[num_cols].fillna(0)\n\n    scaler = StandardScaler()\n    \n    #need to do this due to memory contraints\n    for i in range(0, len(num_cols), 4):\n        cols = num_cols[i:i + 3]\n        df[cols] = scaler.fit_transform(df[cols].astype(float))\n        \nscale_numeric(X)","4b88a992":"#split dataset into 80% for training and 20% for validation \nn_train = int(X.shape[0] * 0.8)\n\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]","4c0bc89d":"# For valid data, keep only those with universe > 0. This will help calculate the metric\nu_valid = (universe.iloc[n_train:] > 0)\nt_valid = time.iloc[n_train:]\n\nX_valid = X_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]\n\nd_valid = t_valid.dt.date\n\ndel u_valid","eba19aa0":"#seperate the columns into categorical and numerical\ndef get_cat_num_split(df):\n    X = {} \n    X['num'] = df.loc[:, num_cols].values\n    X['num'] = np.reshape(X['num'], (X['num'].shape[0], 1, X['num'].shape[1]))\n    for cat in categorical_cols:\n        X[cat] = df.loc[:, cat].values\n    return X","be85b98e":"#seperate the columns into categorical and numerical\nX_train_split = get_cat_num_split(X_train)\nX_valid_split = get_cat_num_split(X_valid)\n\n#set y to a binary representation of returns, true if it's 0-1 and false if i'ts -1-0\ny_train_bin = (y_train >= 0).values\ny_valid_bin = (y_valid >= 0).values","58764d87":"encoding_len = {k: len(encodings[k]) + 1 for k in encodings}","b9a14e59":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, LSTM, Dropout, Reshape\nfrom keras.losses import binary_crossentropy, mse\nfrom keras.regularizers import l2\nimport keras.backend as K\n\nDROPOUT_RATE = 0.2\n\ncat_inputs = [Input(shape=[1], name=cat) for cat in categorical_cols]\nembeddings = [Embedding(encoding_len[cat], embedding_lengths[i])(cat_inputs[i]) for i, cat in enumerate(categorical_cols)]\ncategorical_logits = Concatenate()([(cat_emb) for cat_emb in embeddings])\ncategorical_logits = LSTM(128, activation='relu', input_shape=(1, len(categorical_cols)), return_sequences=True,\n                         kernel_regularizer=l2(1e-5), kernel_initializer='random_uniform')(categorical_logits)\n\nnumerical_inputs = Input(shape=(1, len(num_cols)), name='num')\nnumerical_logits = LSTM(256, activation='relu', input_shape=(1, len(num_cols)), return_sequences=True,\n                        kernel_regularizer=l2(1e-5), kernel_initializer='random_uniform')(numerical_inputs)\nnumerical_logits = Dropout(DROPOUT_RATE)(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = LSTM(256, activation='relu', kernel_initializer='random_uniform')(logits)\nout = Dense(1, activation='sigmoid', name='confidence_level')(logits)\n\nmodel = Model(inputs = cat_inputs + [numerical_inputs], outputs=out)\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","9c0c292f":"model.summary()","481a2807":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=1,verbose=True)\nmodel.fit(X_train_split, y_train_bin,\n          validation_data=(X_valid_split, y_valid_bin),\n          epochs=1,\n          verbose=True,\n          callbacks=[early_stop,check_point]) ","0efc46ec":"from sklearn.metrics import accuracy_score\n\n# distribution of confidence that will be used as submission\nmodel.load_weights('model.hdf5')\n# scaled condifence value  from 0 - 1 to -1 - 1\nconfidence_valid = model.predict(X_valid_split)[:,0]*2 -1\n\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","1495a29e":"#r_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * y_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","061d30fd":"def make_predictions(predictions_template_df, market_obs_df, news_obs_df):\n    inp = get_x(market_obs_df, news_obs_df, False)\n    scale_numeric(inp)\n    inp_split = get_cat_num_split(inp)\n    scaled_pred = model.predict(inp_split) * 2 - 1    \n    predictions_template_df.confidenceValue = np.clip(scaled_pred, -1, 1)","c6580fd9":"days = env.get_prediction_days()\n\nx1,y1,z1 = None, None, None\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    x1,y1,z1 = predictions_template_df, market_obs_df, news_obs_df\n    make_predictions(x1,y1,z1)\n    env.predict(predictions_template_df)\nprint('Done!')","20238b51":"env.write_submission_file()","85c5dcb7":"## A simple LSTM model\n\nI combined techinques and code from two notebooks that I found, and converted it to an LSTM. \n\nReferences to other notebooks used: \n\nhttps:\/\/www.kaggle.com\/christofhenkel\/market-data-nn-baseline\n\nhttps:\/\/www.kaggle.com\/bguberfain\/a-simple-model-using-the-market-and-news-data","a77bd5df":"# Train full model\nNow we train a full model with `num_boost_round` found in validation."}}