{"cell_type":{"1e9081bc":"code","ad016862":"code","a5f876d9":"code","cc50cb4f":"code","d70e9931":"code","c236c9f3":"code","396add83":"code","4eb1b05f":"code","bb8f9d2b":"code","52899885":"code","1019560d":"code","dfae9a37":"code","a0d083ff":"code","3a76a8d1":"code","778da31c":"code","9ca496ce":"code","ab7c6a9c":"code","08e590b5":"code","1b975465":"code","c3d7777b":"code","d8a8f25f":"code","ca8f4839":"code","725ff7d6":"code","4e3410c9":"code","0a62bd71":"code","ae90a1f0":"code","e3e71237":"code","e03ec717":"code","fc3824ac":"code","7c70818a":"code","f435d825":"code","cf43579f":"code","c41e85dc":"code","34a8f988":"code","f62a21b3":"code","287322e8":"code","8d9f4606":"code","59215de3":"code","4e23f9fd":"code","c1fa41ee":"code","0084fa7b":"code","4649e079":"code","c7eb04e8":"code","74e51b9b":"code","e3ce1789":"code","36a28a4a":"markdown","41014b09":"markdown","d070d191":"markdown","ebd225ca":"markdown","cfec7cf5":"markdown","5e36b55b":"markdown","50342741":"markdown","bb047c03":"markdown","4f4a92d1":"markdown"},"source":{"1e9081bc":"# Data Loading and Numerical Operations\nimport pandas as pd\nimport numpy as np\n# Data Visualizations\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Data Resampling\nfrom sklearn.utils import resample\n# Data Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# Data Splitting\nfrom sklearn.model_selection import train_test_split\n# Data Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n# Data Modeling\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.neural_network import MLPClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad016862":"# Reading Dataset\ndata = pd.read_csv(\"\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv\")","a5f876d9":"data.head()","cc50cb4f":"data.shape","d70e9931":"data.info(10)","c236c9f3":"data.isnull().sum()","396add83":"data.duplicated().sum()","4eb1b05f":"print((data[\"glucose\"].mode())[0])","bb8f9d2b":"data[\"glucose\"].fillna((data[\"glucose\"].mode())[0], inplace=True)","52899885":"data.dropna(inplace=True)\ndata.isnull().sum()","1019560d":"plt.figure(figsize=(30,10), facecolor='w')\nsns.boxplot(data=data)\nplt.show()","dfae9a37":"print(data['totChol'].max())\nprint(data['sysBP'].max())","a0d083ff":"data.shape\ndata = data[data['totChol']<600.0]\ndata = data[data['sysBP']<295.0]","3a76a8d1":"data.describe()","778da31c":"#Checking relationship between variables\ncor=data.corr()\nplt.figure(figsize=(20,10), facecolor='w')\nsns.heatmap(cor,xticklabels=cor.columns,yticklabels=cor.columns,annot=True)\nplt.title(\"Correlation among all the Variables of the Dataset\", size=20)\ncor","9ca496ce":"#categorical  features\ncategorical_features = ['male', 'education', \n                        'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes']","ab7c6a9c":"for feature in categorical_features:\n    print(feature,':')\n    print(data[feature].value_counts())\n    print(\"-----------------\")","08e590b5":"num_plots = len(categorical_features)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols + 1\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), facecolor='w', constrained_layout=True)\nfor i, var in enumerate(categorical_features):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.countplot(x=var, data=data, ax=axs[row][pos])\n","1b975465":"\n#Numerical Features\n\nnumeric_features = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\nfor feature in numeric_features:\n    plt.figure(figsize=(18, 10), facecolor='w')\n    sns.distplot(data[feature])\n    plt.title('{} Distribution'.format(feature), fontsize=20)\n    plt.show()\n\n","c3d7777b":"\n\n#Distribution of outcome variable, Heart Disease\nplt.figure(figsize=(12, 10), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=data)\nplt.title(\"Count distribution of TenYearCHD\", size=20)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"grey\",\"green\"])\nplt.show()\n\n","d8a8f25f":"#Bivariate ananlysis\n#checking for which gender has more risk of coronary heart disease CHD\n\ngraph_2 = data.groupby(\"male\", as_index=False).TenYearCHD.sum()\n#Ploting the above values\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_2[\"male\"], y=graph_2[\"TenYearCHD\"])\nplt.title(\"Graph showing which gender has more risk of coronary heart disease CHD\", size=20)\nplt.xlabel(\"Gender\\n0 is female and 1 is male\",size=20)\nplt.ylabel(\"TenYearCHD cases\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","ca8f4839":"#Relation between cigsPerDay and risk of coronary heart disease.\nplt.figure(figsize=(30,12), facecolor='w')\nsns.countplot(x=\"TenYearCHD\",data=data,hue=\"cigsPerDay\")\nplt.legend(title='cigsPerDay', fontsize='large')\nplt.title(\"Graph showing the relation between cigsPerDay and risk of coronary heart disease.\", size=30)\nplt.xlabel(\"Risk of TenYearCHD\", size=20)\nplt.ylabel(\"Count of TenYearCHD\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)\nplt.show()","725ff7d6":"# Multivariate analysis\n\n#Plotting a linegraph to check the relationship between age and cigsPerDay, totChol, glucose.\n\ngraph_5 = data.groupby(\"age\").cigsPerDay.mean()\ngraph_6 = data.groupby(\"age\").totChol.mean()\ngraph_7 = data.groupby(\"age\").glucose.mean()\n\nplt.figure(figsize=(16,10), facecolor='w')\nsns.lineplot(data=graph_5, label=\"cigsPerDay\")\nsns.lineplot(data=graph_6, label=\"totChol\")\nsns.lineplot(data=graph_7, label=\"glucose\")\nplt.title(\"Graph showing totChol and cigsPerDay in every age group.\", size=20)\nplt.xlabel(\"age\", size=20)\nplt.ylabel(\"count\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","4e3410c9":"# Resampling\ntarget1=data[data['TenYearCHD']==1]\ntarget0=data[data['TenYearCHD']==0]","0a62bd71":"target1=resample(target1,replace=True,n_samples=len(target0),random_state=40)\ntarget=pd.concat([target0,target1])\ntarget['TenYearCHD'].value_counts() \n\n\ndata=target\nnp.shape(data)\n\n","ae90a1f0":"#Distribution of heart disease cases in the balanced dataset, the outcome variable\nplt.figure(figsize=(12, 10), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=data)\nplt.title(\"Count of TenYearCHD column\", size=20)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"yellow\",\"grey\"])\nplt.show()","e3e71237":"#To idenfify the features that have larger contribution towards the outcome variable, TenYearCHD\nX=data.iloc[:,0:15]\ny=data.iloc[:,-1]\nprint(\"X - \", X.shape, \"\\ny - \", y.shape)\n\n","e03ec717":"#Apply SelectKBest and extract top 10 features\nbest=SelectKBest(score_func=chi2, k=10)\nfit=best.fit(X,y)\ndata_scores=pd.DataFrame(fit.scores_)\ndata_columns=pd.DataFrame(X.columns)\n\n#Join the two dataframes\nscores=pd.concat([data_columns,data_scores],axis=1)\nscores.columns=['Feature','Score']\nprint(scores.nlargest(11,'Score'))","fc3824ac":"\n\n#To visualize feature selection\nscores=scores.sort_values(by=\"Score\", ascending=False)\nplt.figure(figsize=(20,7), facecolor='w')\nsns.barplot(x='Feature',y='Score',data=scores,palette='BuGn_r')\nplt.title(\"Plot showing the best features in descending order\", size=20)\nplt.show()\n\n","7c70818a":"#Select 10 features\nfeatures=scores[\"Feature\"].tolist()[:10]\nfeatures\n\ndata=data[['sysBP','glucose','age','cigsPerDay','totChol','diaBP','prevalentHyp','male','BPMeds','diabetes','TenYearCHD']]\ndata.head()","f435d825":"y = data['TenYearCHD']\nX = data.drop(['TenYearCHD'], axis=1)\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=1)","cf43579f":"scaler = MinMaxScaler()\ntrain_x = scaler.fit_transform(train_x)\ntest_x = scaler.transform(test_x)","c41e85dc":"\n\nm1 = 'LogisticRegression'\nlr = LogisticRegression(random_state=1, max_iter=1000)\nmodel = lr.fit(train_x, train_y)\nlr_predict = lr.predict(test_x)\nlr_conf_matrix = confusion_matrix(test_y, lr_predict)\nlr_acc_score = accuracy_score(test_y, lr_predict)\nprint(\"confusion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(test_y,lr_predict))\n\n","34a8f988":"m2 = 'Gradient Boosting Classifier'\ngvc =  GradientBoostingClassifier()\ngvc.fit(train_x,train_y)\ngvc_predicted = gvc.predict(test_x)\ngvc_conf_matrix = confusion_matrix(test_y, gvc_predicted)\ngvc_acc_score = accuracy_score(test_y, gvc_predicted)\nprint(\"confusion matrix\")\nprint(gvc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Gradient Boosting Classifier:\",gvc_acc_score*100,'\\n')\nprint(classification_report(test_y,gvc_predicted))\n\n","f62a21b3":"m3 = 'LGBMClassifier'\nlg=LGBMClassifier(boosting_type='gbdt',n_estimators=5000,learning_rate=0.05,objective='binary',metric='accuracy',is_unbalance=True,\n                 colsample_bytree=0.7,reg_lambda=3,reg_alpha=3,random_state=500,n_jobs=-1,num_leaves=35)\nlg.fit(train_x,train_y)\nlg_predicted = lg.predict(test_x)\nlg_conf_matrix = confusion_matrix(test_y, lg_predicted)\nlg_acc_score = accuracy_score(test_y, lg_predicted)\nprint(\"confusion matrix\")\nprint(lg_conf_matrix)\nprint(\"\\n\")\nprint(\"LGBMClassifier:\",lg_acc_score*100,'\\n')\nprint(classification_report(test_y,lg_predicted))\n","287322e8":"m4 = 'XGBClassifier'\nxg = XGBClassifier(learning_rate=0.05, n_estimators=100,max_depth=4, subsample = 0.9,colsample_bytree = 0.1, gamma=1,random_state=42)\nxg.fit(train_x,train_y)\nxg_predicted = xg.predict(test_x)\nxg_conf_matrix = confusion_matrix(test_y, xg_predicted)\nxg_acc_score = accuracy_score(test_y, xg_predicted)\nprint(\"confusion matrix\")\nprint(xg_conf_matrix)\nprint(\"\\n\")\nprint(\"XGBClassifier:\",xg_acc_score*100,'\\n')\nprint(classification_report(test_y,xg_predicted))","8d9f4606":"m5 = 'MLPClassifier'\nmlp=MLPClassifier(solver='adam', learning_rate_init = 0.0005, learning_rate = 'adaptive', activation=\"relu\", max_iter=3000, random_state=10)\nmlp.fit(train_x,train_y)\nmlp_predicted = mlp.predict(test_x)\nmlp_conf_matrix = confusion_matrix(test_y, mlp_predicted)\nmlp_acc_score = accuracy_score(test_y, mlp_predicted)\nprint(\"confusion matrix\")\nprint(mlp_conf_matrix)\nprint(\"\\n\")\nprint(\"MLPClassifier:\",mlp_acc_score*100,'\\n')\nprint(classification_report(test_y,mlp_predicted))","59215de3":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","4e23f9fd":"m6 = 'KNeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=1)\nmodel = knn.fit(train_x, train_y)\nknn_predict = knn.predict(test_x)\nknn_conf_matrix = confusion_matrix(test_y, knn_predict)\nknn_acc_score = accuracy_score(test_y, knn_predict)\nprint(\"confusion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of k-NN Classification:\",knn_acc_score*100,'\\n')\nprint(classification_report(test_y, knn_predict))","c1fa41ee":"m7 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=200, random_state=0,max_depth=12)\nrf.fit(train_x,train_y)\nrf_predicted = rf.predict(test_x)\nrf_conf_matrix = confusion_matrix(test_y, rf_predicted)\nrf_acc_score = accuracy_score(test_y, rf_predicted)\nprint(\"confusion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(test_y,rf_predicted))","0084fa7b":"m8 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 30)\ndt.fit(train_x,train_y)\ndt_predicted = dt.predict(test_x)\ndt_conf_matrix = confusion_matrix(test_y, dt_predicted)\ndt_acc_score = accuracy_score(test_y, dt_predicted)\nprint(\"confusion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(test_y,dt_predicted))","4649e079":"m9 = 'Support Vector Classifier'\nsvc = SVC(kernel = 'linear')\nsvc.fit(train_x,train_y)\nsvc_predicted = svc.predict(test_x)\nsvc_conf_matrix = confusion_matrix(test_y, svc_predicted)\nsvc_acc_score = accuracy_score(test_y, svc_predicted)\nprint(\"confusion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(test_y,svc_predicted))\n\n# kernels = ['linear', 'poly', 'rbf', 'sigmoid']","c7eb04e8":"m10 = 'Naive Bayes Classifier'\nnbc = GaussianNB()\nnbc.fit(train_x,train_y)\nnbc_predicted = nbc.predict(test_x)\nnbc_conf_matrix = confusion_matrix(test_y, nbc_predicted)\nnbc_acc_score = accuracy_score(test_y, nbc_predicted)\nprint(\"confusion matrix\")\nprint(nbc_conf_matrix)\nprint(\"\\n\")\nprint(\"Naive Bayes Classifier:\",nbc_acc_score*100,'\\n')\nprint(classification_report(test_y,nbc_predicted))","74e51b9b":"lr_false_positive_rate,lr_true_positive_rate,lr_threshold = roc_curve(test_y,lr_predict)\nknn_false_positive_rate,knn_true_positive_rate,knn_threshold = roc_curve(test_y,knn_predict)\nrf_false_positive_rate,rf_true_positive_rate,rf_threshold = roc_curve(test_y,rf_predicted)                                                             \ndt_false_positive_rate,dt_true_positive_rate,dt_threshold = roc_curve(test_y,dt_predicted)\ngvc_false_positive_rate,gvc_true_positive_rate,gvc_threshold = roc_curve(test_y,gvc_predicted)\nsvc_false_positive_rate,svc_true_positive_rate,svc_threshold = roc_curve(test_y,svc_predicted)\nnbc_false_positive_rate,nbc_true_positive_rate,nbc_threshold = roc_curve(test_y,nbc_predicted)\nlg_false_positive_rate,lg_true_positive_rate,lg_threshold = roc_curve(test_y,lg_predicted)\nxg_false_positive_rate,xg_true_positive_rate,xg_threshold = roc_curve(test_y,xg_predicted)\nmlp_false_positive_rate,mlp_true_positive_rate,mlp_threshold = roc_curve(test_y,mlp_predicted)","e3ce1789":"\nsns.set_style('whitegrid')\nplt.figure(figsize=(15,15), facecolor='w')\nplt.title('Reciever Operating Characterstic Curve')\nplt.plot(lr_false_positive_rate,lr_true_positive_rate,label='Logistic Regression')\nplt.plot(knn_false_positive_rate,knn_true_positive_rate,label='K-Nearest Neighbor')\nplt.plot(rf_false_positive_rate,rf_true_positive_rate,label='Random Forest')\nplt.plot(dt_false_positive_rate,dt_true_positive_rate,label='Desion Tree')\nplt.plot(gvc_false_positive_rate,gvc_true_positive_rate,label='Gradient Boosting Classifier')\nplt.plot(svc_false_positive_rate,svc_true_positive_rate,label='Support Vector Classifier')\nplt.plot(nbc_false_positive_rate,nbc_true_positive_rate,label='Naive Bayes Classifier')\nplt.plot(lg_false_positive_rate,lg_true_positive_rate,label='LGBMClassifier')\nplt.plot(xg_false_positive_rate,xg_true_positive_rate,label='XGBClassifier')\nplt.plot(mlp_false_positive_rate,mlp_true_positive_rate,label='MLPClassifier')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()\n\n","36a28a4a":"# Different Predictive Models","41014b09":"# ROc   Curve to compare  all the classifiers","d070d191":"Compared to all the independent data, the correlation coefficient\nbetween education and and target\nvariable TenYearCHD is very low and actually negative","ebd225ca":"\n\nFilling the missing spaces of glucosecolumn with \nthe mode of the data (Mode = 75) present to reduce the number of missing data in our dataset\n","cfec7cf5":"\n\nThe distribution is highly imbalanced.\nAs in, the number of negative cases outweigh the number of positive cases. \nThis would lead to class imbalance problem while fitting our models.\nTherefore, this problem needs to be addressed and taken care of.\n","5e36b55b":"\n\nAmong the categorical features:\n\n    BPmeds, prevalentStroke and diabetes are highly imbalanced.\n    There are four levels of education whereas the rest categorical features are all binary\n    The number of Smokers and non-Smokers in currentSmoker is almost the same\n\n","50342741":"# Bravo!","bb047c03":"\n\nAmong the numerical features:\n\n    totChol, sysBP, diaBPand BMI has an uniform distribution and the rest are unevenly distributed\n    cigsPerDay has a highly uneven distribution with the most data present in 0\n    cigsPerDay and sysBP shows quite a bit and slight right skewness respectively.\n\n","4f4a92d1":"\n\nRemovable Outliers are detected in totChol and sysBP columns of our dataset. \nOutliers in all other numerical columns are important and thus cannot be removed.\nThe Outlier present in totChol is 600.\nThe Outlier present in sysBP is 295.\n"}}