{"cell_type":{"8ab117f3":"code","84e7a0a0":"code","9e8e9a58":"code","3c9068b4":"code","56486c0d":"code","cb7cfd37":"code","73973b87":"code","dc202c2c":"code","436ffc77":"code","0fe9eb7c":"code","345f6a78":"code","1d685875":"code","0f9ee9db":"code","f43e720f":"code","c899d866":"code","2a1fdf34":"code","d342e6d7":"code","e3e69ab1":"code","02b76587":"code","55a147cb":"code","9c46f4a8":"code","cca971f0":"code","ae0e70f7":"code","713eaf9e":"code","56976e0b":"code","32bc2afb":"code","6aa32d8f":"code","d827598d":"code","9f6bf564":"code","c98d7d9b":"code","c481f3ad":"code","fbed4db1":"code","a97c06de":"code","262d1d19":"code","509e0aa4":"code","40b9cfd1":"code","ea82f150":"code","f0a16eb8":"markdown","515384a2":"markdown","8c4fc344":"markdown","672670e7":"markdown"},"source":{"8ab117f3":"!pip install -U dataprep","84e7a0a0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dataprep.eda import plot ,plot_diff, plot_missing,plot_correlation,create_report\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9e8e9a58":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngs = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","3c9068b4":"train_df.head()","56486c0d":"test_df.head()","cb7cfd37":"train_df.describe()","73973b87":"train_df.info()","dc202c2c":"test_df.info()","436ffc77":"plot(train_df)","0fe9eb7c":"plot_correlation(train_df)","345f6a78":"pom = train_df.drop(\"Survived\",axis=1,inplace=False)\nplot_diff([pom,test_df])","1d685875":"plot_missing(train_df)","0f9ee9db":"plot_missing(test_df)","f43e720f":"train_data_report = create_report(train_df)\n\ntrain_data_report.show()\n\ntrain_data_report.save(filename='train_data_report')","c899d866":"test_data_report = create_report(test_df)\n\ntest_data_report.show()\n\ntest_data_report.save(filename='test_data_report')","2a1fdf34":"categorical_cols = [cname for cname in train_df.columns if\n                    train_df[cname].dtype == \"object\"]\nnumerical_cols = [cname for cname in train_df.columns if \n                train_df[cname].dtype in ['int64', 'float64']]\n\ncategorical_cols","d342e6d7":"numerical_cols","e3e69ab1":"train_df.isnull().sum()","02b76587":"test_df.isnull().sum()","55a147cb":"train_df['title'] = np.NaN\ntest_df['title'] = np.NaN\ntrain_df['cabin_class'] = np.NaN\ntest_df['cabin_class'] = np.NaN\ntrain_df['alone'] = np.NaN\ntest_df['alone'] = np.NaN\n\n\nfor i,row in enumerate(train_df['Name']): \n    train_df['title'][i] = row.split(',')[1].split('.')[0]\n    \nfor i,row in enumerate(test_df['Name']): \n    test_df['title'][i] = row.split(',')[1].split('.')[0]\n    \nfor i,_ in enumerate(train_df['alone']):\n    if train_df['SibSp'][i] + train_df['Parch'][i] == 0: train_df['alone'][i] = 1\n    else: train_df['alone'][i] = 0\n        \nfor i,_ in enumerate(test_df['alone']):\n    if test_df['SibSp'][i] + test_df['Parch'][i] == 0: test_df['alone'][i] = 1\n    else: test_df['alone'][i] = 0\n        \nfor i,row in enumerate(train_df['Cabin']):\n    if str(row) != \"nan\":\n        train_df['cabin_class'][i] =  str(row)[:1]\n    \nfor i,row in enumerate(test_df['Cabin']):\n    if str(row) != \"nan\":\n        test_df['cabin_class'][i] =  str(row)[:1]\n","9c46f4a8":"train_df.drop('Cabin',axis=1,inplace=True)\ntest_df.drop('Cabin',axis=1,inplace=True)\n\ntrain_df.drop('Name',axis=1,inplace=True)\ntest_df.drop('Name',axis=1,inplace=True)\n\ntrain_df.drop('Ticket',axis=1,inplace=True)\ntest_df.drop('Ticket',axis=1,inplace=True)\n\ntrain_df.drop('PassengerId',axis=1,inplace=True)\ntest_ids = test_df.PassengerId\ntest_df.drop('PassengerId',axis=1,inplace=True)\n\nclasses = train_df.Survived\ntrain_df.drop('Survived',axis=1,inplace=True)\n","cca971f0":"train_df.head()","ae0e70f7":"test_df.head()","713eaf9e":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","56976e0b":"categorical_cols = [cname for cname in train_df.columns if\n                    train_df[cname].dtype == \"object\"]\nnumerical_cols = [cname for cname in train_df.columns if \n                train_df[cname].dtype in ['int64', 'float64']]","32bc2afb":"numerical_cols","6aa32d8f":"categorical_cols","d827598d":"numerical_transformer = SimpleImputer(strategy=\"constant\")\n\n\ncategorical_transformer = Pipeline(steps=[\n                                        (\"imputer\",SimpleImputer(strategy=\"constant\")),\n                                        (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\"))])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","9f6bf564":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","c98d7d9b":"X_train, X_valid, y_train, y_valid = train_test_split(train_df, classes, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=123)","c481f3ad":"lr_model = LogisticRegression(max_iter=1000,random_state=123,C=0.175)\nrf_model = RandomForestClassifier(n_estimators=1000,random_state=123)\nxgb_model = XGBClassifier(n_estimators=1000,random_state=123,learning_rate=0.01)","fbed4db1":"lr_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', lr_model)\n                     ])\nrf_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', rf_model)\n                     ])\nxgb_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', xgb_model)\n                     ])","a97c06de":"lr_pipe.fit(X_train,y_train)\nrf_pipe.fit(X_train,y_train)\nxgb_pipe.fit(X_train,y_train)","262d1d19":"lr_pred = lr_pipe.predict(X_valid)\nrf_pred = rf_pipe.predict(X_valid)\nxgb_pred = xgb_pipe.predict(X_valid)","509e0aa4":"print(f\"Logistic regression accuracy: {accuracy_score(y_valid, lr_pred)}\")\nprint(f\"Random forest accuracy: {accuracy_score(y_valid, rf_pred)}\")\nprint(f\"XGB accuracy: {accuracy_score(y_valid, xgb_pred)}\")","40b9cfd1":"final_predictions = lr_pipe.predict(test_df)\n","ea82f150":"submission = pd.DataFrame({'PassengerId':test_ids,'Survived':final_predictions})\nsubmission.to_csv('submission.csv',index = False)","f0a16eb8":"# FINAL NOTE:\n\nThis notebook is intended to be simple, so i used automated eda tools and did very minimum modeling.\nTo increase model score we should test cross validation score, tune hyper parameters, test more models, etc.\n\nHope this notebook helps you!\n\nAnd check out christodoulos from whom i took and modified feature engineering part and logistic regression hp.","515384a2":"# SIMPLE EDA","8c4fc344":"# SIMPLE FEATURE ENGINEERING\n\n# Modified version of @Chris Solomou's (christodoulos - username) version of feature engineering in his notebook","672670e7":"# MODELING"}}