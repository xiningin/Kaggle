{"cell_type":{"02220c52":"code","bf2b264f":"code","c53c6b47":"code","d98e9efa":"code","61568e0b":"code","2332a538":"code","e387f918":"code","593aaaa5":"code","fadf6a8a":"code","b185c982":"code","710977ac":"code","c57759a4":"code","1ef6a190":"code","eceb513c":"code","ba8f3ae9":"code","67997c7a":"code","47c5d58a":"code","0a2b6515":"code","662f85ff":"code","4697a5d8":"code","13e57323":"code","bc335140":"code","663d704d":"code","47bae93a":"code","810ec10c":"code","a6081c09":"code","ae236111":"code","58f37be7":"code","b83adfe9":"code","3bb6264e":"code","d98e7321":"code","26942d40":"code","37012dd4":"code","ffac0f3d":"code","2cb9b0e7":"code","312dbcec":"code","2b65c4e8":"code","07858568":"code","ade37b15":"code","4276aa3e":"code","3669a345":"code","9a988fca":"code","3393038c":"code","8d816b2c":"code","37c09858":"code","19615f74":"code","d2ca34e6":"code","803f59bc":"code","5e81e04c":"code","c59883b0":"code","d3d1b27d":"code","86b0bdf7":"code","bc25d790":"markdown","ee1dea94":"markdown","a66e4c73":"markdown","b617bd3e":"markdown","3825a1b4":"markdown","05d882ae":"markdown","0091f271":"markdown","1f601eaf":"markdown","73de708d":"markdown","5120e471":"markdown","0d383006":"markdown","d5d6592b":"markdown","89471719":"markdown","740742b0":"markdown","2dea925c":"markdown","a652a786":"markdown","35c2e291":"markdown","103ec562":"markdown","62195fd7":"markdown","bdf0af71":"markdown","f7ba0990":"markdown","f2f00b55":"markdown","7cc89eea":"markdown","e21fd708":"markdown","79656335":"markdown","b17cf252":"markdown","a9660774":"markdown","464a0a97":"markdown","bb1b4612":"markdown"},"source":{"02220c52":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bf2b264f":"job_data = pd.read_csv(\"\/kaggle\/input\/jobposting_revised.csv\")\n\n# Ignore the warnings \nimport warnings\nwarnings.filterwarnings('ignore')","c53c6b47":"job_data.head().T","d98e9efa":"job_data.describe().T","61568e0b":"job_data.info()","2332a538":"job_data.drop_duplicates()","e387f918":"# Missing Values\njob_data.isnull().sum().sort_values(ascending=False)","593aaaa5":"# I chose to drop company_revenue and company_scale because they are not related with the issues I want to discuss.\njob_data = job_data.drop(['company_revenue','company_scale'], axis=1)","fadf6a8a":"job_data = job_data.dropna(subset = ['location','skill','description','company_name','experience'])","b185c982":"job_data.isnull().sum().sort_values(ascending=False)","710977ac":"clean_data = job_data.drop(['title','company_type','company_name','skill','NC','CA','NY','VA','TX','MA','IL','WA','MD','DC','other_states','description','location'], axis=1)","c57759a4":"clean_data.info()","1ef6a190":"clean_data.describe().T","eceb513c":"print(clean_data['salary'].value_counts())\nprint(clean_data['experience'].value_counts())\nprint(clean_data['job_type'].value_counts())","ba8f3ae9":"# Use map to transfer String type to integer\nsalary_mapping = {\"<80000\": 70000,\"80000-99999\": 90000, \"100000-119999\": 110000, \"120000-139999\": 130000,\"140000-159999\":150000,\">160000\":170000}\nclean_data['salary'] = clean_data['salary'].map(salary_mapping)\njob_data['salary'] = job_data['salary'].map(salary_mapping)\nexperience_mapping = {\"entry_level\": 1,\"mid_level\": 2, \"senior_level\": 3}\nclean_data['experience'] = clean_data['experience'].map(experience_mapping)\nplot_data = clean_data.copy()","67997c7a":"type_mapping = {\"data_scientist\": 1,\"data_analyst\": 2, \"data_engineer\": 3}\nclean_data['job_type'] = clean_data['job_type'].map(type_mapping)\nclean_data.head(3)","47c5d58a":"plt.subplots(figsize=(15,15))\nax = plt.axes()\nax.set_title(\"Job Post Analysis Correlation Heatmap\")\ncorr = clean_data.corr()\nsns.heatmap(corr, square = True, xticklabels=corr.columns.values,yticklabels=corr.columns.values, annot = True)","0a2b6515":"pair = clean_data[['experience','num_skills','salary','job_type','sql','r','machine_learning','python']]\nsns.pairplot(pair, kind=\"scatter\", hue=\"job_type\", markers=[\"o\", \"s\", \"D\"], palette=\"Set2\")","662f85ff":"import statsmodels.formula.api as smf\nresults = smf.ols('salary ~experience', data=clean_data).fit()\nprint(results.summary())","4697a5d8":"#graph distribution of qualitative data: Job Type\nplt.subplots(figsize=(8,6))\naxis1 = sns.barplot(x = 'experience', y = 'salary', hue = 'job_type', data=job_data)\naxis1.set_title('Experience vs Job Type Salary Comparison')","13e57323":"fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does experience factor with salary compare\nsns.pointplot(x=\"experience\", y=\"salary\", hue=\"job_type\",markers=[\"o\", \"s\", \"D\"], palette=\"Set2\",data=clean_data, ax = maxis1)\n\n#how does num_skills factor with salary compare\nsns.pointplot(x=\"num_skills\", y=\"salary\", hue=\"job_type\",markers=[\"o\", \"s\", \"D\"], palette=\"Set2\",data=clean_data, ax = maxis2)","bc335140":"loc = pd.DataFrame(job_data['location'].value_counts())\ncities_plot = loc.head(15).plot(kind = 'bar', width = .7, figsize = (10, 6), rot=45, title='States With The Most job Opptunities')","663d704d":"job_perstate = pd.DataFrame({'State':job_data['location'].value_counts().index, 'Counts':job_data['location'].value_counts().values})\njob_perstate.head()","47bae93a":"import plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode()\n# plotly code for choropleth map\nscale = [[0, 'rgb(229, 239, 245)'],[1, 'rgb(1, 97, 156)']]\ndata1 = [ dict(\n        type = 'choropleth',\n        colorscale = scale,\n        autocolorscale = False,\n        showscale = False,\n        locations = job_perstate['State'],\n        z = job_perstate['Counts'],\n        locationmode = 'USA-states',\n        marker = dict(\n            line = dict (\n                color = 'rgb(255, 255, 255)',\n                width = 2\n            ) ),\n        ) ]\nlayout = dict(\n        title = 'Data Related Jobs in United States by State',\n        geo = dict(\n            scope = 'usa',\n            projection = dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)',\n            countrycolor = 'rgb(255, 255, 255)')        \n             )\n \nfigure = dict(data=data1, layout=layout)\niplot(figure)","810ec10c":"job_count = job_data\njob_count.replace(0,np.nan, inplace = True)\nc_py=job_count['python'].value_counts().values\nc_r=job_count['r'].value_counts().values\nc_sql =job_count['sql'].value_counts().values\nc_ml=job_count['machine_learning'].value_counts().values\nc_h=job_count['hadoop'].value_counts().values\nc_t =job_count['tableau'].value_counts().values\nc_s=job_count['sas'].value_counts().values\nc_sp=job_count['spark'].value_counts().values\nc_j =job_count['java'].value_counts().values\nc_bd =job_count['big_data'].value_counts().values\nc_dm =job_count['data_mining'].value_counts().values\nc_st =job_count['stat'].value_counts().values\nc_tf =job_count['tensorflow'].value_counts().values\nd = {'Skill': ['python', 'r','sql','machine_learning','hadoop','tableau','sas','spark','java','big_data','data_mining','stat','tensorflow'], \n     'Counts': [c_py.item(0), c_r.item(0),c_sql.item(0),c_ml.item(0),c_h.item(0),c_t.item(0),c_s.item(0),c_sp.item(0),c_j.item(0),c_bd.item(0),c_dm.item(0),c_st.item(0),c_tf.item(0)]}\nskills = pd.DataFrame(data=d)\n\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n# Example data\nsk = 'python', 'r','sql','machine learning','hadoop','tableau','sas','spark','java','big_data','data_mining','stat','tensorflow'\n\nax.barh(skills['Skill'], skills['Counts'])\nax.set_yticks(skills['Skill'])\nax.set_yticklabels(sk)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Counts of Skills in Job Post')\nax.set_title('What data-related skills do job seeker need to pocess?')\nplt.show()","a6081c09":"job_data.dropna()\njob_data['company_type'].describe()","ae236111":"com_type = pd.DataFrame({'Company_type':job_data['company_type'].value_counts().index, 'Counts':job_data['company_type'].value_counts().values})\ncom_type.head()","58f37be7":"plt.rcdefaults()\nfig, ax = plt.subplots()\nax.barh(com_type['Company_type'].head(), com_type['Counts'].head())\nax.set_yticks(com_type['Company_type'].head())\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Counts of company types appeared in Job Post')\nax.set_title('What type of company want to hire more data analysts?')\nplt.show()","b83adfe9":"clean_data.info()","3bb6264e":"clean_data.info()","d98e7321":"x = clean_data[['salary','experience','num_skills','python','sql','machine_learning','r','hadoop','tableau','sas','spark','java','stat','tensorflow','big_data','data_analysis','data_mining','natural_language_processing']]\ny = clean_data['job_type']\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3)","26942d40":"def build_eval_clf(clf,x_train,testX,y_test):\n    clf.fit(x_train,y_train)\n    #accuracy\n    y_predict = clf.predict(testX)\n    acc = sklearn.metrics.accuracy_score(y_test,y_predict)\n    #f1_score\n    f1 = sklearn.metrics.f1_score(y_test,y_predict,average='weighted')\n    #precision\n    p = sklearn.metrics.precision_score(y_test,y_predict,average='weighted')\n    print('accuracy score is ' + str(acc))\n    print('f1 score is ' + str(f1))\n    print('precision score is ' + str(p))","37012dd4":"import sklearn.metrics\nfrom sklearn.linear_model import LogisticRegression\nclf_lg = LogisticRegression()\nbuild_eval_clf(clf_lg,x_train,x_train,y_train)","ffac0f3d":"from sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier()\nbuild_eval_clf(clf_knn,x_train,x_train,y_train)","2cb9b0e7":"from sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier()\nbuild_eval_clf(clf_dt,x_train,x_train,y_train)","312dbcec":"from sklearn.naive_bayes import GaussianNB\nclf_nb = GaussianNB()\nbuild_eval_clf(clf_nb,x_train,x_train,y_train)","2b65c4e8":"from sklearn.svm import LinearSVC\nclf_svm = LinearSVC()\nbuild_eval_clf(clf_svm,x_train,x_train,y_train)","07858568":"from sklearn.ensemble import RandomForestClassifier\nclf_ec = RandomForestClassifier()\nbuild_eval_clf(clf_ec,x_train,x_train,y_train)","ade37b15":"build_eval_clf(clf_lg,x_train,x_test,y_test)","4276aa3e":"build_eval_clf(clf_knn,x_train,x_test,y_test)","3669a345":"build_eval_clf(clf_dt,x_train,x_test,y_test)","9a988fca":"build_eval_clf(clf_nb,x_train,x_test,y_test)","3393038c":"build_eval_clf(clf_svm,x_train,x_test,y_test)","8d816b2c":"build_eval_clf(clf_ec,x_train,x_test,y_test)","37c09858":"from sklearn.model_selection import GridSearchCV\nrf = RandomForestClassifier()\nparam_grid = { \n    'n_estimators': [5, 100],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\ngsearch = GridSearchCV(rf,param_grid = param_grid, cv=5)\ngsearch.fit(x_train, y_train)","19615f74":"gsearch.best_params_","d2ca34e6":"rfc1=RandomForestClassifier(random_state=42, max_features='sqrt', n_estimators= 100, max_depth=8, criterion='gini')","803f59bc":"rfc1.fit(x_train, y_train)","5e81e04c":"pred=rfc1.predict(x_test)","c59883b0":"build_eval_clf(rfc1,x_train,x_test,y_test)","d3d1b27d":"plot_data = plot_data.drop(['salary','experience','num_skills','other_skills'], axis=1)\nplot_data = plot_data.groupby(['job_type']).mean()\nplot_data.head()","86b0bdf7":"import plotly.graph_objs as go\nplt_cols = ['python','sql', 'machine_learning', 'r', 'hadoop', 'tableau','sas', 'spark', 'java', 'big_data', 'data_mining',\n       'data_analysis', 'stat', 'natural_language_processing']\nplot_data.reset_index(inplace=True)\nplt_data = [] \nfor i in range(plot_data.shape[0]):\n    trace = go.Scatterpolar(\n        r = plot_data.loc[i,plt_cols],\n        theta = plt_cols,\n        name = plot_data.loc[i,'job_type'],\n    )\n    plt_data.append(trace)\n    \nlayout = go.Layout(\n  polar = dict(\n    radialaxis = dict(\n      visible = True,\n      range = [0, 1],\n    )\n  ),\n    height = 700,\n    width = 700,\n    title = \"Needing Skills for Data Analyst\/Engineer\/Scientist\",\n    showlegend = True\n)\n\nfig = go.Figure(data=plt_data, layout=layout)\niplot(fig)","bc25d790":"#### 6.6 Random Forest","ee1dea94":"#### 5.4 Naive Bayes","a66e4c73":"### (1) Deal with Duplicated Records ","b617bd3e":"#### 6.4 Naive Bayes","3825a1b4":"### (2) Deal with Missing Values","05d882ae":"## 6. Make predictions and evaluate performance on the test set","0091f271":"Years of experience do positively related to the earned salary. A job seeker would generally be provided with higher salary if he had more previous experience invloved in data field. In other words, data scientists\/analysts\/engineers seem to be everlasting occupations since their income won't decline as their age increase. Furthur, data analyst are likely to have more salary than data engineers and data scientists.","1f601eaf":"#### 6.3 Decision Tree","73de708d":"#### 5.2 k-Nearest Neighbors","5120e471":"## 4. EDA","0d383006":"## 5. Model Data","d5d6592b":"## 2. Load the Data","89471719":"#### 6.5 Support Vector Machine","740742b0":"From above graph, we are sure to see that related skills of data scientists are: machine learning, R and python; related skills of data engineers are: spark, java, big data and hadoop. However, the realted skills of data analysts have a great doublication with the other two. Therefore, we can conclude that to some extent, most employeers tend to confuse the concepts of data analysts\/engineers\/scientists in their job posts. In this way, a job seeker really need to focus on the responsibilities on the job post before he or she applies for the job. ","2dea925c":"#### 6.2 k-Nearest Neighbors","a652a786":"#### 5.6 Random Forest","35c2e291":"## 3. Clean the Data","103ec562":"#### 5.1 Logistic Regressions","62195fd7":"Since the accuracy of the improved model is only about 83%, I still want to take a closer look of the classification based on the needing skills. ","bdf0af71":"Spilt the dataset into train set and test set. ","f7ba0990":"## 7. Improve the Model","f2f00b55":"Based on the results, skills with the highest frequency according to employer's demand are popular languages used for data analysis -- Python and R as well as the traditional query-searching language -- SQL. ","7cc89eea":"#### 5.5 Support Vector Machine","e21fd708":"## 1. Introduction\n\nIndeed is one of the greatest job seeking website in the world. It includes all the job listings from major job boards, newspapers, associations, and company career pages, and employers can even post jobs directly to Indeed that may not be available anywhere else. To help job hunters (including me) to better understand the job market of data science, I created this notebook to dig the hidden relationships among numeric features which may affect the possibility of getting a job. \nThe major problems that I mainly considered:\n1. What kind of ability do employers want a candidate to pocess when they are hiring a data scientist\/data engineer\/data analyst?\n2. Can employeers properly define a data scientist\/data engineer\/data analyst in their job post?\n3. Do years of experiences related to employers' consideration or salary?\n4. Which states have the greatest opportunities for data scientists?\n5. How does the salary distribution look like in the whole field of data science?\n\nData Sources: www.indeed.com  ","79656335":"Here, I use grid search to narrow down the suitable hyperparameters. ","b17cf252":"#### 5.3 Decision Tree","a9660774":"The job opptunities provided by California is way more than the opptunities provided than other states. Other states that a job seeker may consider could be New York, Virginia, Texas. ","464a0a97":"#### 6.1 Logistic Regressions","bb1b4612":"## 8. Conclusion"}}