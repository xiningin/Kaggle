{"cell_type":{"f4daf27c":"code","cd29e035":"code","0fd11855":"code","a7c45a40":"code","998a8f2e":"code","320441cf":"code","3a68dbe7":"code","539c61ed":"code","3bd52f2f":"code","5a431049":"code","3eb63d99":"code","eb218ed6":"code","2b87eafc":"code","c1bb6324":"code","0a9ffa89":"markdown","81ad952e":"markdown","86b57491":"markdown","4369b0e2":"markdown","08c92f85":"markdown","5debf360":"markdown","3f850e39":"markdown","13e032df":"markdown"},"source":{"f4daf27c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom keras import regularizers\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\nnp.set_printoptions(suppress=True)","cd29e035":"raw_data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndisplay(raw_data.head())\nprint(raw_data.Outcome.value_counts())","0fd11855":"x_train, x_test, y_train, y_test = train_test_split(raw_data.drop('Outcome',axis=1), raw_data.Outcome, test_size=0.15, stratify=raw_data.Outcome)","a7c45a40":"train_min = x_train.min()\ntrain_max = x_train.max()\nx_train = (x_train - train_min) \/ (train_max - train_min)\nx_test = (x_test - train_min) \/ (train_max - train_min)","998a8f2e":"positives = x_train[y_train == 1]\nnegatives = x_train[y_train == 0]","320441cf":"## input layer \ninput_layer = Input(shape=negatives.shape[1:])\n\n## encoding part\nencoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = BatchNormalization()(encoded)\nencoded = Dense(75, activation='tanh')(encoded)\nencoded = BatchNormalization()(encoded)\nencoded = Dense(50, activation='relu')(encoded)\nencoded = BatchNormalization()(encoded)\nencoded = Dense(25, activation='relu')(encoded)\nencoded = BatchNormalization()(encoded)\nencoded = Dense(7, activation='relu')(encoded)\n\n## decoding part\ndecoded = Dense(7, activation='relu')(encoded)\ndecoded = BatchNormalization()(decoded)\ndecoded = Dense(25, activation='relu')(decoded)\ndecoded = BatchNormalization()(decoded)\ndecoded = Dense(50, activation='relu')(decoded)\ndecoded = BatchNormalization()(decoded)\ndecoded = Dense(75, activation='tanh')(decoded)\ndecoded = BatchNormalization()(decoded)\ndecoded = Dense(100, activation='tanh')(decoded)\n\n## output layer\noutput_layer = Dense(negatives.shape[1], activation='relu')(decoded)","3a68dbe7":"autoencoder = Model(input_layer, output_layer)\nautoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")","539c61ed":"autoencoder.fit(negatives, negatives, batch_size = 15, epochs = 1000, shuffle = True)","3bd52f2f":"hidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])\nhidden_representation.add(autoencoder.layers[3])\nhidden_representation.add(autoencoder.layers[4])\nhidden_representation.add(autoencoder.layers[5])\nhidden_representation.add(autoencoder.layers[6])\nhidden_representation.add(autoencoder.layers[7])\nhidden_representation.add(autoencoder.layers[8])\nhidden_representation.add(autoencoder.layers[9])","5a431049":"x_train_transformed = hidden_representation.predict(x_train)\nx_test_transformed = hidden_representation.predict(x_test)","3eb63d99":"predictor = Sequential()\npredictor.add(Dense(64, activation='relu', input_shape=x_train_transformed.shape[1:]))\npredictor.add(BatchNormalization())\npredictor.add(Dropout(0.25))\npredictor.add(Dense(64, activation='relu'))\npredictor.add(BatchNormalization())\npredictor.add(Dense(64, activation='relu'))\npredictor.add(BatchNormalization())\npredictor.add(Dense(64, activation='tanh'))\npredictor.add(BatchNormalization())\npredictor.add(Dense(1, activation='sigmoid'))","eb218ed6":"predictor.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\npredictor.fit(x_train_transformed, y_train, batch_size=10, epochs=1000, shuffle=True)","2b87eafc":"y_predict = predictor.predict(x_test_transformed)","c1bb6324":"print(classification_report(y_test, y_predict >= 0.35))","0a9ffa89":"The finale !\nLet's see how we perform over the test set.","81ad952e":"## The AutoEncoder Architecture\n\nThere are no written rules to create an autoencoder network but make sure that it is symmetric around a bottleneck.\n* The network has 3 parts - encoder, decoder and bottleneck.\n* The 100, 75, 50, 25 layers in encoder and decoder are symmetric.\n* The 7-7 layer is the bottleneck.","86b57491":"We train the autoencoder only over the negative examples in training set (persons without diabetes). The idea is, if the network is able to accurately learn to represent these records, it should misfire for the positive records.","4369b0e2":"# Prediction using AutoEncoder network\n\nAutoencoders are unsupervised neural networks that transform an input to a lower dimension, thus it highlights the major features of the data.\n* After trying several sizes for the bottleneck, I noticed that a size less than the input dimension (here, 8) will learn more meaningful representations of the data. If you choose a larger bottleneck, you are giving the network too much flexibility and it becomes almost like a linear identity mapper. If you choose a bottleneck too narrow, it will result in excessive loss of information.\n* The representations were learned more accurately when the fall in dimensions in the encoder (and so the rise in dimensions of decoder) were gradual.\n* If you introduce noise in the encoder, such as a regularizer, you force the network to learn more information.\n* Batch Normalization made a huge difference.","08c92f85":"Since we are going to pass this data into neural networks, we MUST scale the columns. Here I will use the Min-Max method. (Don't forget to use the same transform on both training and test data.)","5debf360":"## Transforming the Data\n\nNow that the autoencoder network is ready, we will create another network using only the encoding part.\nWe will use this network to transform both, the train sets and test sets (this includes the positive as well as negative examples).","3f850e39":"## Conclusion\n\nUsing a threshold of 0.35, we have accurately identified a high number of the diabetes patients without augmenting the data.","13e032df":"## Prediction\n\nYou can now perform your regular classification with this transformed data. I will use a neural network again (because I love it)."}}