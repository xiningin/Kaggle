{"cell_type":{"309c624c":"code","576819cd":"code","68f5ea08":"code","0023b158":"code","99418ab8":"code","dd556d30":"code","66a45396":"code","d2553c9b":"code","6d75d930":"code","af5c5616":"code","fa494cb2":"code","077d58a6":"code","90dce0d9":"code","078df6b2":"code","48b85584":"code","94882354":"code","2486c15f":"code","1889954b":"code","9d94cbfd":"code","52195cd1":"code","83fd0ac0":"code","c404e616":"code","56ed618f":"code","a6f91651":"code","4de98a85":"code","f77b780f":"code","d945c9d9":"code","ad493d69":"code","eee96dcd":"code","2ac691cf":"code","01c76b4d":"code","f93e5b06":"code","e527dce0":"code","3d1c5a53":"code","0446594b":"code","71c6b058":"code","a5b57a3f":"code","46a93bf6":"code","aa075bbf":"code","32242806":"code","8935b5fb":"code","65d02c24":"code","e8a5fecd":"markdown","58121ce7":"markdown","2e5eea82":"markdown","46e349df":"markdown","c2aae4f3":"markdown","3bc05a1f":"markdown","6337d423":"markdown","aa74d54e":"markdown","3d267220":"markdown","09c6cc7d":"markdown","96f779d1":"markdown","89725728":"markdown","51857a39":"markdown","e6e5b97d":"markdown","914d6859":"markdown","4bcb9f0b":"markdown","1bc7df26":"markdown","cd174686":"markdown","8b28f1a9":"markdown"},"source":{"309c624c":"import numpy as np\nimport pandas as pd\nimport os\nimport email\nimport email.policy\nfrom bs4 import BeautifulSoup\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Let's explore the directory segmentation\nos.listdir('\/kaggle\/input\/ham-and-spam-dataset\/')","576819cd":"ham_filenames = [name for name in sorted(os.listdir('\/kaggle\/input\/ham-and-spam-dataset\/ham\/')) if len(name) > 20]\nspam_filenames = [name for name in sorted(os.listdir('\/kaggle\/input\/ham-and-spam-dataset\/spam')) if len(name) > 20]","68f5ea08":"#How's the dataset structure? How many ham\/spam emails does it contain?\n\nprint('Total ham emails: ',len(ham_filenames))\nprint('Total spam emails: ',len(spam_filenames))\nprint('Spam percentage: ',100*(len(spam_filenames)\/(len(ham_filenames)+len(spam_filenames))))","0023b158":"#Let's load an email to see how it looks like:\n\n#Using email.parser: https:\/\/docs.python.org\/3\/library\/email.parser.html\n#\"The email package provides a standard parser that understands most email document structures, including MIME documents\"\n\nwith open(os.path.join('\/kaggle\/input\/ham-and-spam-dataset\/ham\/', ham_filenames[0]), \"rb\") as file:\n    ham_email =  email.parser.BytesParser(policy=email.policy.default).parse(file)\n\nprint('Header field names: ',ham_email.keys())\nprint('\\n -------------------------------------- \\n')\nprint('Message field values: ',ham_email.values())\nprint('\\n -------------------------------------- \\n')\nprint('Message content:',ham_email.get_content()[:500])","99418ab8":"#Let's extract some email fields\n\nemail_subject = ham_email.get_all('Subject')\nemail_from = ham_email.get_all('From')\nemail_to = ham_email.get_all('To')\n\nprint('Email from: ',email_from)\nprint('Email to: ',email_to)\nprint('Email subject: ',email_subject)","dd556d30":"def upload_ham(filename):\n    \"\"\"This function process a ham email file located at a specified directory and returns it as an email object\"\"\"\n    directory = '\/kaggle\/input\/ham-and-spam-dataset\/ham\/'\n    with open(os.path.join(directory, filename), \"rb\") as file:\n        return email.parser.BytesParser(policy=email.policy.default).parse(file)\n\ndef upload_spam(filename):\n    \"\"\"This function process a spam email file located at a specified directory and returns it as an email object\"\"\"\n    directory = '\/kaggle\/input\/ham-and-spam-dataset\/spam\/'\n    with open(os.path.join(directory, filename), \"rb\") as file:\n        return email.parser.BytesParser(policy=email.policy.default).parse(file)\n    \nham_emails = [upload_ham(filename=name) for name in ham_filenames]\nspam_emails = [upload_spam(filename=name) for name in spam_filenames]","66a45396":"#Checking if everything was uploaded properly:\n\nprint(ham_emails[0].get_all('Subject'))\nprint(ham_emails[0].get_content())\nprint('\\n\\n -----------------------------------------------------------\\n\\n')\nprint(spam_emails[1].get_all('Subject'))\nprint(spam_emails[1].get_content())","d2553c9b":"#Let's research about what email content types are:\n\nham_email_types = []\nspam_email_types = []\n\nfor i in range(len(ham_filenames)):\n    ham_email_types.append(ham_emails[i].get_content_type())\n\nfor i in range(len(spam_filenames)):\n    spam_email_types.append(spam_emails[i].get_content_type())\n\nprint('Ham content types: ',set(ham_email_types))\nprint('Spam content types: ',set(spam_email_types))","6d75d930":"#We need to identify what the multipart emails are structured of\n\ndef email_content_type(email):\n    \"\"\"This function returns the content type of an email and if it has a multipart shape then returns the multiparts type\"\"\"\n    if isinstance(email, str):\n        return email\n    payload = email.get_payload()\n    if isinstance(payload, list):\n        return \"multipart({})\".format(\", \".join([email_content_type(sub_email) for sub_email in payload]))\n    else:\n        return email.get_content_type()","af5c5616":"ham_email_types = []\nspam_email_types = []\n\nfor i in range(len(ham_filenames)):\n    ham_email_types.append(email_content_type(ham_emails[i]))\n\nfor i in range(len(spam_filenames)):\n    spam_email_types.append(email_content_type(spam_emails[i]))\n\nprint('Ham content types: ',set(ham_email_types))\nprint('Spam content types: ',set(spam_email_types))","fa494cb2":"#Now that we've identified what are the email content types, we need to transform all html emails to plain format.\n\nfrom bs4 import BeautifulSoup\nhtml = spam_emails[1].get_content()\nsoup = BeautifulSoup(html)\nprint(soup.get_text().replace('\\n\\n',''))","077d58a6":"#Let's build a function with the previous process to convert all html emails into plain text\n\ndef html_to_plain(email):\n    soup = BeautifulSoup(email.get_content())\n    return soup.get_text().replace('\\n\\n','').replace('\\n',' ') ","90dce0d9":"#Now all emails which have\/contain HTML tags will be converted to plain text\n\ndef email_to_plain(email):\n    content_type = email_content_type(email)\n    for part in email.walk(): \n        #The .walk() documentation at https:\/\/docs.python.org\/3\/library\/email.message.html\n        #\"The walk() method is an all-purpose generator which can be used to iterate over all \n        #the parts and subparts of a message object tree, in depth-first traversal order.\"\n        partContentType = part.get_content_type()\n        if partContentType not in ['text\/plain','text\/html']:\n            continue\n        try:\n            partContent = part.get_content()\n        except: \n            partContent = str(part.get_payload())\n        if partContentType == 'text\/plain':\n            return partContent\n        else:\n            return html_to_plain(part)","078df6b2":"#Let's test this out.\nemail_test1 = email_to_plain(spam_emails[1])\nemail_test2 = email_to_plain(spam_emails[227])\nprint(email_test1)\nprint('\\n\\n')\nprint(email_test2[:1000])","48b85584":"#Spam email #226 contains an unknown encoding so we will remove it from the list\ndel spam_emails[226]","94882354":"ham_dataset = []\nspam_dataset = []\n\n#Ham processing\nfor i in range(len(ham_emails)):\n    ham_dataset.append(email_to_plain(ham_emails[i]))\nham_dataset = pd.DataFrame(ham_dataset,columns=['Email content'])\nham_dataset['Label'] = 0\n\n#Spam processing\nfor i in range(len(spam_emails)):\n    spam_dataset.append(email_to_plain(spam_emails[i]))\nspam_dataset = pd.DataFrame(spam_dataset,columns=['Email content'])\nspam_dataset['Label'] = 1\n\ndataset = pd.concat([ham_dataset,spam_dataset])\ndataset.head()","2486c15f":"#We will shuffle the data and also reset indexes\ndataset = dataset.dropna()\ndataset = dataset.sample(frac=1).reset_index(drop=True)\ndataset.head()","1889954b":"#Removing special chars because they just add noise and make the models poor when predicting.\nfor i in range(len(dataset)):\n    dataset.at[i,'Email content'] = dataset.loc[i]['Email content'].replace('!','').replace('?','').replace(',','').replace('[','').replace(']','').replace('(','').replace(')','').replace('...','')\n    dataset.at[i,'Email content'] = dataset.loc[i]['Email content'].replace('>','').replace('<','').replace('\\n',' ').replace('-','').replace('+','').replace('#','')\ndataset.head()","9d94cbfd":"def input_preprocessing(text):\n    text = text.replace('!','').replace('?','').replace(',','').replace('[','').replace(']','').replace('(','').replace(')','').replace('...','')\n    text = text.replace('>','').replace('<','').replace('\\n',' ').replace('-','').replace('+','').replace('#','')\n    return text","52195cd1":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['Email content'],dataset['Label'],shuffle=True,random_state=0)","83fd0ac0":"#checking if everything went OK.\nprint (len(X_train),len(X_test),len(y_train),len(y_test))","c404e616":"pipe = make_pipeline(CountVectorizer(min_df=5,analyzer='char_wb'), LogisticRegression(max_iter=10000))\nparam_grid = {\"logisticregression__C\": [0.1, 1, 10, 100],\n\"countvectorizer__ngram_range\": [(1, 2), (1, 3),(2,5)]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))","56ed618f":"vect = CountVectorizer(min_df=5, analyzer='char_wb', ngram_range=[2,5])\n\nX_train_vectorized = vect.fit_transform(X_train)\nX_test_vectorized = vect.transform(X_test)\n\nclf = LogisticRegression(C=0.1,max_iter=500).fit(X_train_vectorized, y_train)\ny_predicted = clf.predict(X_test_vectorized)","a6f91651":"print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predicted)))\nprint(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_predicted)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_predicted)))\nprint(\"AUC score: {:.2f}%\".format(100 * roc_auc_score(y_test, y_predicted)))","4de98a85":"pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=10000))\n# running the grid search takes a long time because of the\n# relatively large grid and the inclusion of trigrams\nparam_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3),(2,5)]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))","f77b780f":"vect = TfidfVectorizer(min_df=5,ngram_range=(1,2)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nclf = LogisticRegression(C=100,max_iter=10000).fit(X_train_vectorized, y_train)\ny_predicted = clf.predict(X_test_vectorized)","d945c9d9":"print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predicted)))\nprint(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_predicted)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_predicted)))\nprint(\"AUC score: {:.2f}%\".format(100 * roc_auc_score(y_test, y_predicted)))","ad493d69":"vect = TfidfVectorizer(min_df=5,ngram_range=(2,5)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nparam_grid = {'C':[1,10,100,1000,10000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\ngrid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\ngrid.fit(X_train_vectorized, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))","eee96dcd":"vect = TfidfVectorizer(min_df=5,ngram_range=(2,5)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nclf = SVC(C=100,gamma= 0.1,kernel='rbf').fit(X_train_vectorized, y_train)\ny_predicted = clf.predict(X_test_vectorized)","2ac691cf":"print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predicted)))\nprint(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_predicted)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_predicted)))\nprint(\"AUC score: {:.2f}%\".format(100 * roc_auc_score(y_test, y_predicted)))","01c76b4d":"vect = TfidfVectorizer(min_df=5,ngram_range=(2,5)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nparam_grid = {'max_depth':[2,5,10,20,30,50,100,200],'n_estimators':[10,20,50]}\ngrid = GridSearchCV(RandomForestClassifier(random_state=0),param_grid,refit = True, verbose=0)\ngrid.fit(X_train_vectorized, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))","f93e5b06":"vect = TfidfVectorizer(min_df=5,ngram_range=(2,5)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nclf = RandomForestClassifier(n_estimators=50,max_depth=100,random_state=0).fit(X_train_vectorized, y_train)\ny_predicted = clf.predict(X_test_vectorized)","e527dce0":"print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predicted)))\nprint(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_predicted)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_predicted)))\nprint(\"AUC score: {:.2f}%\".format(100 * roc_auc_score(y_test, y_predicted)))","3d1c5a53":"vect = TfidfVectorizer(min_df=5,ngram_range=(2,5)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nparam_grid = {'max_depth':[30,50,100]}\ngrid = GridSearchCV(GradientBoostingClassifier(random_state=0,n_estimators=50),param_grid,refit = True, verbose=0)\ngrid.fit(X_train_vectorized, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))","0446594b":"vect = TfidfVectorizer(min_df=5,ngram_range=(2,5)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nclf = GradientBoostingClassifier(max_depth=30,random_state=0).fit(X_train_vectorized, y_train)\ny_predicted = clf.predict(X_test_vectorized)","71c6b058":"print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predicted)))\nprint(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_predicted)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_predicted)))\nprint(\"AUC score: {:.2f}%\".format(100 * roc_auc_score(y_test, y_predicted)))","a5b57a3f":"vect = TfidfVectorizer(min_df=5,ngram_range=(2,5)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)\nclf = SVC(C=100,gamma= 0.1,kernel='rbf').fit(X_train_vectorized, y_train)\ny_predicted = clf.predict(X_test_vectorized)","46a93bf6":"print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Features with highest idf:\\n{}\".format(vect.get_feature_names()[-50:]))","aa075bbf":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nmatrix = confusion_matrix(y_test, y_predicted)","32242806":"import seaborn as sns\nconf_matrix = pd.DataFrame(matrix, index = ['Ham','Spam'],columns = ['Ham','Spam'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') \/ conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nsns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","8935b5fb":"email_test = ['Good morning mates. This is just to let you all know we have scheduled a meeting for EOD.']\nemail_test[0] = input_preprocessing(email_test[0])\nemail_test = vect.transform(email_test)\nprediction = clf.predict(email_test)\n\nif prediction == 0:\n    print('The email has not been flagged as SPAM.')\nelse:\n    print('The email has been flagged as SPAM.')","65d02c24":"email_test = ['Dear Sergio, Flash Sale at Walmart! 25% OFF all weekend!']\nemail_test[0] = input_preprocessing(email_test[0])\nemail_test = vect.transform(email_test)\nprediction = clf.predict(email_test)\n\nif prediction == 0:\n    print('The email has not been flagged as SPAM.')\nelse:\n    print('The email has been flagged as SPAM.')","e8a5fecd":"## **TFIDF** vectorizer and **Support Vector Classifier** ML model.","58121ce7":"## TFIDF Vectorizer with Random Forest Classifier","2e5eea82":"# Model training and selection\n\nWe'll go through the use of TfidfVectorizer and CountVectorizer as NLP resources to transform the dataset into a format the classifiers can interpret it. Right after that, we'll train a LogisticRegressor and a Support Vector classifier and the one that performs better will  be selected.","46e349df":"## Confusion Matrix","c2aae4f3":"Let's simulate some emails to see how our model performs","3bc05a1f":"# **Converting emails to plain text**\nIn the previous output you could notice that there are some emails with HTML format. We need them to be plain text format.","6337d423":"## TFIDF Vectorizer with GradientBoosted Classifier","aa74d54e":"## Model selected: **Support Vector Classifier** with **TFIDF** vectorizer.","3d267220":"The below function will be used later on the notebook to process the prediction inputs.","09c6cc7d":"## **TFIDF** vectorizer and **LogisticRegression** classifier ML model.","96f779d1":"# Data Importing\nNow that we've explored how the data is structured, we are going to import all emails to be processed later on the notebook.","89725728":"## **CountVectorizer** and **LogisticRegression** classifier ML model.","51857a39":"# Data exploration","e6e5b97d":"# Building the dataset to train the model\nEssentially we'll create a dataframe with the emails' content and their particular label. This will allow us to implement NLP to feed a ML model","914d6859":"For this opportunity we're going to explore **Ham and Spam emails from SpamAssasin** provided at https:\/\/www.kaggle.com\/veleon\/ham-and-spam-dataset - It contains several email files all readable by Python Email library. I'll import all of them, explore them, determine commonly used words in Spam emails, will train some models based on the files in order to find the best that fits the data and finally start predicting.","4bcb9f0b":"# Project description","1bc7df26":"## Model testing","cd174686":"## Dataset structure","8b28f1a9":"We're gonna implement some NLP in order to facilitate the model to predict well."}}