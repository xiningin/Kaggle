{"cell_type":{"2d2de445":"code","b26c0e30":"code","77d9d9f5":"code","08198d0b":"code","6bc714df":"code","80c70e84":"code","217228a7":"code","bdf8343d":"code","f2ab673a":"code","6dfcb98d":"code","8c801ed0":"code","8e398337":"code","35e64038":"code","7bafb826":"code","dd571535":"code","9ab05fb8":"code","c6ed70fd":"code","c56bc455":"code","b96184e2":"code","2454a7f7":"code","9ca252a7":"code","0ade83fa":"code","f13d4409":"code","5c2c68eb":"code","0ebb3976":"code","f016710d":"code","1d8558ea":"code","1b9ae682":"code","4d23bad6":"code","041d345b":"code","2b3b2bfc":"code","a928270f":"code","9b07df15":"code","70534609":"code","7374ddb3":"code","fb196487":"code","088086df":"code","64f952c4":"code","7e07dde0":"code","41eba826":"code","0bd78630":"code","f30fc047":"code","a41a2cbc":"code","39139d94":"code","a34e74c0":"code","8d6d7403":"code","c1ff9c19":"code","ae5bfcf6":"code","e03d90f3":"code","211ae9d1":"code","1eec23c9":"code","080a531b":"code","48c235d9":"code","5e559685":"code","19749483":"code","c3bb88c8":"code","8ae011a8":"code","ac8a3221":"code","c1867fce":"code","7f4653b7":"code","4d422d01":"code","7b111477":"code","6833e8f7":"markdown","0a5ba7b7":"markdown","ff103bc1":"markdown","76e3a261":"markdown","d13cea78":"markdown","f2e38632":"markdown","611c9bb3":"markdown","2a97f9bc":"markdown","37603289":"markdown","aeacffc4":"markdown","69cdc3c9":"markdown","008be468":"markdown","5160117f":"markdown","420c79f0":"markdown","f454bb48":"markdown","279b86bd":"markdown","467b9e1b":"markdown","197d70eb":"markdown","95f61a87":"markdown","13222ae8":"markdown","e68023e5":"markdown","64bde777":"markdown","408c9d47":"markdown","f37ee798":"markdown","3014b8d4":"markdown","6bb89319":"markdown","e9f2af65":"markdown","4c7b5266":"markdown","173ca906":"markdown","cb38832c":"markdown","147f532f":"markdown","3cc1c8b2":"markdown","fe05d5c2":"markdown","ee360df3":"markdown","a67e1e62":"markdown","fbe895e9":"markdown","237d3563":"markdown","7bb40305":"markdown","7f84bbea":"markdown","ff6edf6d":"markdown","7d071b48":"markdown","6f7f05c7":"markdown","0ba61292":"markdown","975f3e7b":"markdown","272e090b":"markdown","8978d408":"markdown","ac8cee1d":"markdown","8b8570ea":"markdown","077b7801":"markdown","0e42b0b3":"markdown","e5efe353":"markdown","93db064f":"markdown","ef8b18e6":"markdown","760e9570":"markdown","ec7b6486":"markdown"},"source":{"2d2de445":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import datasets, linear_model\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b26c0e30":"bank=pd.read_csv(\"\/kaggle\/input\/bank-marketing\/bank-additional-full.csv\", error_bad_lines=False, sep=\";\", na_values='unknown')\ndf=bank.copy()\ndf.T","77d9d9f5":"featuresInformations=pd.read_csv(\"\/kaggle\/input\/columns-information\/columnInformations.csv\")\nfeaturesInformations.drop(featuresInformations.index)\nfeaturesInformations","08198d0b":"def histogram(df,feature, xlabel='Target Segments', ylabel='Number of Observations',\n              title='Histogram of Binary Target Categories'):\n    import matplotlib.ticker as ticker\n    ncount = len(df)\n    ax = sns.countplot(x = feature, data=df ,palette=\"hls\")\n    sns.set(font_scale=1)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    fig = plt.gcf()\n    fig.set_size_inches(12,5)\n    \n    # Make twin axis\n    ax2=ax.twinx()\n    \n    # Switch so count axis is on right, frequency on left\n    ax2.yaxis.tick_left()\n    ax.yaxis.tick_right()\n    \n    # Also switch the labels over\n    ax.yaxis.set_label_position('right')\n    ax2.yaxis.set_label_position('left')\n    ax2.set_ylabel('Frequency [%]')\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.2f}%'.format(100.*y\/ncount), (x.mean(), y), ha='center', va='bottom') \n        # set the alignment of the text\n        \n    # Use a LinearLocator to ensure the correct number of ticks\n    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\n    \n    # Fix the frequency range to 0-100\n    ax2.set_ylim(0,100)\n    ax.set_ylim(0,ncount)\n    \n    # And use a MultipleLocator to ensure a tick spacing of 10\n    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n    \n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\n    ax2.grid(None)\n    plt.title(title, fontsize=20, y=1.08)\n    plt.show();\n    del ncount, x, y","6bc714df":"histogram(df,\"y\")","80c70e84":"df.columns=['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', \n            'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', \n            'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']","217228a7":"df[\"housing\"]=df[\"housing\"].replace(\"yes\", 1)\ndf[\"housing\"]=df[\"housing\"].replace(\"no\", 0)\n\ndf[\"loan\"]=df[\"loan\"].replace(\"yes\", 1)\ndf[\"loan\"]=df[\"loan\"].replace(\"no\", 0)   \n\ndf[\"y\"]=df[\"y\"].replace(\"yes\",1)\ndf[\"y\"]=df[\"y\"].replace(\"no\",0)","bdf8343d":"df[\"job\"]=df[\"job\"].astype(\"category\")\ndf[\"marital\"]=df[\"marital\"].astype(\"category\")\ndf[\"default\"]=df[\"default\"].astype(\"category\")\ndf[\"housing\"]=df[\"housing\"].astype(\"category\")\ndf[\"loan\"]=df[\"loan\"].astype(\"category\")\ndf[\"contact\"]=df[\"contact\"].astype(\"category\")\ndf[\"month\"]=df[\"month\"].astype(\"category\")\ndf[\"day_of_week\"]=df[\"day_of_week\"].astype(\"category\")","f2ab673a":"from pandas.api.types import CategoricalDtype\neducationOrder=CategoricalDtype(categories=[\"illiterate\",\"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\n                                            \"university.degree\",\"professional.course\"],ordered=True)\ndf[\"education\"]=df[\"education\"].astype(educationOrder)\npoutcomeOrder=CategoricalDtype(categories=[\"nonexistent\",\"failure\",\"success\"],ordered=True)\ndf[\"poutcome\"]=df[\"poutcome\"].astype(poutcomeOrder)","6dfcb98d":"numerical_columns=[\"age\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"emp_var_rate\", \n                   \"cons_price_idx\", \"cons_conf_idx\", \"nr_employed\"]\n\ncategoricalColumns=['job', 'marital', 'education', 'default', 'housing', 'loan', \n                    'contact', 'month', 'day_of_week', 'poutcome', 'euribor3m',]","8c801ed0":"def ZeroVarianceFinder(df, numerical_columns):\n    zerovariance_numerical_features=[]\n    for col in numerical_columns:\n        try:\n            if pd.DataFrame(df[col]).describe().loc['std'][0] == 0.00 or \\\n            np.isnan(pd.DataFrame(df[col]).describe().loc['std'][0]):\n                zerovariance_numerical_features.append(col)\n        except:\n            print(\"Error:\", col)\n    return zerovariance_numerical_features\nlen(ZeroVarianceFinder(df, numerical_columns))","8e398337":"singleton_categorics = []\nfor col in categoricalColumns :\n    if len(df[col].unique()) <=1:\n        singleton_categorics.append(col)\nlen(singleton_categorics)","35e64038":"df['nr_employed']=[int(x) for x in df['nr_employed']]","7bafb826":"histogram(df,\"pdays\",xlabel=\"Name of Data\", title=\"Distribution of pdays before feature importance\")","dd571535":"df.loc[(df.pdays==999),\"pdays\"]=1 # Main DF changed\ndf.loc[(df.pdays!=1),\"pdays\"]=0 # Main DF changed","9ab05fb8":"histogram(df,\"pdays\",xlabel=\"Name of Data\", title=\"Distribution of pdays after feature importance\")","c6ed70fd":"df.hist(column=numerical_columns, figsize=(30,30))\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5)\nplt.show()","c56bc455":"def dataframeInformations(df):\n    totalEntryList=[]\n    totalMissingValueList=[]\n    missingValRateList=[]\n    dataTypeList=[]\n    uniqueValuesList=[]\n    totalUniqueValList=[]\n    variableNameList=[]\n\n    for element in df.columns:\n        missingValRate=round((df[element].isna().sum()\/len(df[element]))*100,2) #to show correct decimal and float part of number.\n        totalEntryList.append(len(df[element]))\n        totalMissingValueList.append(df[element].isna().sum())\n        missingValRateList.append(missingValRate)\n        dataTypeList.append(df[element].dtype)\n        uniqueValuesList.append(list(df[element].unique()))\n        totalUniqueValList.append(len(df[element].unique()))\n        variableNameList.append(element)\n    #create a dataframe to show all informations together\n    dataInfoDf=pd.DataFrame({'Variable':variableNameList,'#_Total_Entry':totalEntryList,\\\n                             '#_Missing_Value':totalMissingValueList,'%_Missing_Value':missingValRateList,\\\n                             'Data_Type':dataTypeList,'Unique_Values':uniqueValuesList,\\\n                             '#_Uniques_Values':totalUniqueValList})\n    return dataInfoDf.sort_values(by=\"Variable\")","b96184e2":"dataInfo=dataframeInformations(df)\nvariableList=[element for element in dataInfo['Variable'] ]\ndataInfo=dataInfo.set_index('Variable')","2454a7f7":"variableDefinition=list(featuresInformations['Variable_Definition'])\nvariableStructure=(featuresInformations['Variable_Structure'])\nvariableStructure=[x.lower() for x in variableStructure] #uppercase not working well in dataFrame therefore we converted all elements to lowercase\ndataInfo['Variable_Definition']=variableDefinition\ndataInfo['Variable_Structure']=variableStructure","9ca252a7":"def findMethod(df,variableList):\n    df['Imputation_Technique']=\"\"\n    for element in variableList:\n        missingRate=float(dataInfo['%_Missing_Value'][element])\n        if missingRate == 0:\n            df['Imputation_Technique'][element]='No Missing Value'\n        elif missingRate <= 5:\n            df['Imputation_Technique'][element]='Simple'\n        elif missingRate < 25:\n            df['Imputation_Technique'][element]='Tree-based'\n        elif missingRate < 50 :\n            df['Imputation_Technique'][element]='Model'\n\nfindMethod(dataInfo,variableList)\ndataInfo","0ade83fa":"from sklearn.model_selection import train_test_split\nX = df.drop(\"y\", axis=1)\ny=df.y\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nX_train.shape , X_test.shape , y_train.shape, y_test.shape","f13d4409":"histogram(X_train,y_train, title=\"Histogram of Binary Target Categories for X_train\")","5c2c68eb":"histogram(X_test,y_test, title=\"Histogram of Binary Target Categories for X_test\")","0ebb3976":"sparse_columns=[]\nfor col in numerical_columns:\n    if (X_train[col].mode()[0]==X_train[col].quantile(0.99)==X_train[col].quantile(0.75)):\n        sparse_columns.append(col)\nsparse_columns","f016710d":"left_skewed_columns = []\nfor col in numerical_columns:\n    if X_train.loc[X_train[col]!=X_train[col].mode()[0],col].median() < -1:\n        left_skewed_columns.append(col)\nleft_skewed_columns","1d8558ea":"from pylab import rcParams\ndef box_plot(X_train, y_train, column):\n    rcParams['figure.figsize'] = 20, 10\n    fig, axs = plt.subplots(3,3)\n    plt.tight_layout()\n    fig.subplots_adjust(top=0.7)\n    sns.set(style=\"ticks\", palette=\"rainbow\")\n    j = 0\n    k = 0\n    for i in range(len(column)):\n        sns.boxplot(x=y_train, y=column[i], data=X_train, ax=axs[j,k])\n        if(k==2):\n            k = 0\n            j += 1\n        else:\n            k += 1\n    plt.tight_layout()\n    plt.show()\n\nbox_plot(X_train, y_train,numerical_columns)","1b9ae682":"def HardEdgeReduction(df,numerical_columns,sparse_columns,upper_quantile=0.99,lower_quantile=0.01):\n    \n    import pandas as pd\n\n    import psutil, os, gc, time\n    print(\"HardEdgeReduction process has began :\\n\")\n    proc = psutil.Process(os.getpid())\n    gc.collect()\n    mem_0 = proc.memory_info().rss\n    start_time = time.time()\n    \n    # Do outlier cleaning in only one loop\n    epsilon = 0.0001 # for zero divisions\n\n   \n    data_outlier_cleaned = df.copy()\n\n    print(\"Detected outliers will be replaced with edged quantiles\/percentiles: %1 and %99 !\\n\")\n    print(\"Total number of rows : %s\\n\"%data_outlier_cleaned.shape[0])\n\n    outlier_boundries_dict={}\n\n    for col in numerical_columns:\n\n        if col in sparse_columns:\n\n            # Find nansparse columns\n            nonsparse_data = pd.DataFrame(data_outlier_cleaned[data_outlier_cleaned[col] !=\\\n                                                             data_outlier_cleaned[col].mode()[0]][col]) \n            # we used mode to detect sparse columns bcs it is efficient\n\n            # Find the bounds for outliers\n            # Lower bound\n            if nonsparse_data[col].quantile(lower_quantile) < data_outlier_cleaned[col].mode()[0]: \n                lower_bound_sparse = nonsparse_data[col].quantile(lower_quantile)\n            else:\n                lower_bound_sparse = data_outlier_cleaned[col].mode()[0]\n            \n            # Upper bound\n            if nonsparse_data[col].quantile(upper_quantile) < data_outlier_cleaned[col].mode()[0]: \n                upper_bound_sparse = data_outlier_cleaned[col].mode()[0]\n            else:\n                upper_bound_sparse = nonsparse_data[col].quantile(upper_quantile)\n\n            outlier_boundries_dict[col]=(lower_bound_sparse,upper_bound_sparse)\n\n            # Information about outlier values' number\n            number_of_outliers = len(data_outlier_cleaned[(data_outlier_cleaned[col] < lower_bound_sparse) |\\\n                                                        (data_outlier_cleaned[col] > upper_bound_sparse)][col])\n            print(\"Sparse: Outlier number in %s is equal to : \"%col,number_of_outliers\/(nonsparse_data.shape[0] -\n                                                                                       nonsparse_data.isnull().sum()))\n\n            # Making equal to outliers to the bounds\n            if number_of_outliers > 0:\n\n                data_outlier_cleaned.loc[data_outlier_cleaned[col] < lower_bound_sparse,col] = lower_bound_sparse - epsilon # VER\u0130M\u0130Z DE\u011e\u0130\u015eT\u0130 !\n                data_outlier_cleaned.loc[data_outlier_cleaned[col] > upper_bound_sparse,col] = upper_bound_sparse + epsilon # VER\u0130M\u0130Z DE\u011e\u0130\u015eT\u0130 !\n\n        else:\n            number_of_outliers = len(data_outlier_cleaned[(data_outlier_cleaned[col] < \\\n                                                         data_outlier_cleaned[col].quantile(lower_quantile))|\\\n                                                        (data_outlier_cleaned[col] > \\\n                                                         data_outlier_cleaned[col].quantile(upper_quantile))] [col])\n            result = number_of_outliers\/(df[col].shape[0] - df[col].isnull().sum())\n            print(\"Other: Outlier number in {} is equal to : \".format(col),round(result,4)) \n\n            # 'Standart' ayk\u0131r\u0131 de\u011ferler de\u011fi\u015ftiriliyor : Changing standard outliers\n            if number_of_outliers > 0:\n                \n                lower_bound_sparse = data_outlier_cleaned[col].quantile(lower_quantile)\n                data_outlier_cleaned.loc[data_outlier_cleaned[col] < lower_bound_sparse,col] = lower_bound_sparse  - epsilon\n\n                upper_bound_sparse = data_outlier_cleaned[col].quantile(upper_quantile)\n                data_outlier_cleaned.loc[data_outlier_cleaned[col] > upper_bound_sparse,col]= upper_bound_sparse  + epsilon\n\n            outlier_boundries_dict[col]=(lower_bound_sparse,upper_bound_sparse)\n\n\n    print(\"HardEdgeReduction has been completed in %s minutes !\" % ((time.time() - start_time)\/60))\n    return data_outlier_cleaned, outlier_boundries_dict","4d23bad6":"X_train, outlier_boundries_dict_for_train = HardEdgeReduction(X_train,numerical_columns,sparse_columns)\nX_test, outlier_boundries_dict_for_test = HardEdgeReduction(X_test,numerical_columns,sparse_columns)","041d345b":"box_plot(X_train, y_train,numerical_columns)","2b3b2bfc":"#!pip install missingno\nimport missingno as msno\nmsno.matrix(X_train);","a928270f":"msno.bar(X_train);","9b07df15":"#if missing rate between 0-5 and categorical data use this method\ndef missingvalImputeMF(df,column):\n    valueList=list(df[column])\n    mostFrequentVal=max(set(valueList), key = valueList.count)\n    df[column]=df[column].fillna(mostFrequentVal) ","70534609":"def missingImpute(df):\n    for col in df.columns:\n        infoStr=\"\"\n        missingRatioRate=round((df[col].isna().sum()\/len(df[col]))*100,2)\n        if missingRatioRate==0:\n            infoStr+=\"{} - There is no missing value\".format(col)\n            if str(df[col].dtype)==\"category\":\n                infoStr+=\" - Category\"\n                print(infoStr)\n            else:\n                infoStr+=\" - Numerical\"\n                print(infoStr)\n            print(\"--------------------------\")\n        elif missingRatioRate<=5:\n            infoStr+=\" {} - Ratio:{}% 0-5\".format(col,missingRatioRate)\n            if str(df[col].dtype)==\"category\":\n                #if column's missing rate between 0% - 5% and categorical variable\n                missingvalImputeMF(df,col)\n                missingRatioRate=round((df[col].isna().sum()\/len(df[col]))*100,2)\n                infoStr+=\"{} -  There is no missing value\".format(col)\n                infoStr+=\" - Category\"\n                print(\"DONE {} \".format(infoStr))\n                \n            else:\n                infoStr+=\" - Numerical\"\n            print(infoStr)\n            print(\"--------------------------\")\n        elif missingRatioRate<=25:\n            infoStr+=\"{} - Ratio:{}% 5-25\".format(col,missingRatioRate)\n            if str(df[col].dtype)==\"category\":\n                infoStr+=\" - Category\"\n                print(infoStr)\n            else:\n                infoStr+=\" - Numerical\"\n                print(infoStr)\n                print(\"--------------------------\")\n        elif missingRatioRate<=50:\n            infoStr+=\"{} - Ratio:{}% 25-50\".format(col,missingRatioRate)\n            if str(df[col].dtype)==\"category\":\n                infoStr+=\" - Category\"\n                print(infoStr)\n            else:\n                infoStr+=\" - Numerical\"\n                print(infoStr)\n            print(\"--------------------------\")\n        elif missingRatioRate<=95:\n            infoStr+=\"{} - Ratio:{}% 50-95\".format(col,missingRatioRate)\n            if str(df[col].dtype)==\"category\":\n                infoStr+=\" - Category\"\n                print(infoStr)\n            else:\n                infoStr+=\" - Numerical\"\n                print(infoStr)\n            print(\"--------------------------\")\n        else:\n            infoStr+=\"{} - Ratio:{}% 95-100\".format(col,missingRatioRate)\n            if str(df[col].dtype)==\"category\":\n                infoStr+=\" - Category\"\n                print(infoStr)\n            else:\n                infoStr+=\" - Numerical\"\n                print(infoStr)\n            print(\"--------------------------\")","7374ddb3":"missingImpute(X_train)","fb196487":"missingImpute(X_test)","088086df":"X_train['default'] = X_train['default'].cat.add_categories('unknown')\nX_train['default'].fillna('unknown', inplace =True) \n\nX_test['default'] = X_test['default'].cat.add_categories('unknown')\nX_test['default'].fillna('unknown', inplace =True) ","64f952c4":"def binaryEncoding(df,column):\n    from category_encoders import BinaryEncoder\n    encoder=BinaryEncoder(cols=[column])\n    df=encoder.fit_transform(df)\n    return df\n\ndef oneHotEncoding(df,column):\n    from category_encoders import OneHotEncoder\n    encoder=OneHotEncoder(cols=[column])\n    df = encoder.fit_transform(df)\n    return df\n\ndef encodingForCategoricalFeature(df,categoricalColumns):\n    for element in categoricalColumns:\n        if len(df[element].unique())<=5:\n            df=oneHotEncoding(df,element)\n        else:\n            df=binaryEncoding(df,element)\n    return df\n\nX_train=encodingForCategoricalFeature(X_train,categoricalColumns)\nX_test=encodingForCategoricalFeature(X_test,categoricalColumns)","7e07dde0":"from sklearn import preprocessing\nX_train.loc[:,numerical_columns]=preprocessing.normalize(X_train.loc[:,numerical_columns])\nX_test.loc[:,numerical_columns]=preprocessing.normalize(X_test.loc[:,numerical_columns])","41eba826":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)\n\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances = feat_importances.sort_values(ascending=True)\nfeat_importances.plot(kind='barh',figsize=(20,20))\nplt.show();","0bd78630":"X_train.drop(labels=[\"euribor3m_0\", \"month_0\", \"default_2\", \"education_0\", \"job_0\"],axis=1,inplace=True)\nX_test.drop(labels=[\"euribor3m_0\", \"month_0\", \"default_2\", \"education_0\", \"job_0\"],axis=1,inplace=True)","f30fc047":"plt.subplots(figsize=(20,20))\nsns.heatmap(X_train.corr())\nplt.show();","a41a2cbc":"def models(X_train,Y_train):\n    \n    #use logistic regression\n    from sklearn.linear_model import LogisticRegression\n    log=LogisticRegression(random_state=0)\n    log.fit(X_train,Y_train)\n    \n    #use KNeighbors\n    from sklearn.neighbors import KNeighborsClassifier\n    knn=KNeighborsClassifier(n_neighbors=5,metric=\"minkowski\",p=2)\n    knn.fit(X_train,Y_train)\n    \n    #use SVC (linear kernel)\n    from sklearn.svm import SVC\n    svc_lin=SVC(kernel=\"linear\",random_state=0,probability=True)\n    svc_lin.fit(X_train,Y_train)\n    \n    #use SVC (RBF kernel)\n    svc_rbf=SVC(kernel=\"rbf\",random_state=0,probability=True)\n    svc_rbf.fit(X_train,Y_train)\n    \n    #use GaussianNB\n    from sklearn.naive_bayes import GaussianNB\n    gauss=GaussianNB()\n    gauss.fit(X_train,Y_train)\n    \n    #use Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\n    tree.fit(X_train,Y_train)\n    \n    #use Random Forest Classifier\n    from sklearn.ensemble import RandomForestClassifier\n    forest=RandomForestClassifier(n_estimators=10,criterion=\"entropy\",random_state=0)\n    forest.fit(X_train,Y_train)\n    \n    # use Hist Gradient Boosting Classifier\n    from sklearn.experimental import enable_hist_gradient_boosting\n    from sklearn.ensemble import HistGradientBoostingClassifier\n    histgrad=HistGradientBoostingClassifier()\n    histgrad.fit(X_train,y_train)\n    \n    # use GBM\n    from sklearn.ensemble import GradientBoostingClassifier\n    gbm=GradientBoostingClassifier()\n    gbm.fit(X_train,y_train)\n    \n    # use XGBoost\n    #!pip install xgboost\n    from xgboost import XGBClassifier\n    xgboost=XGBClassifier()\n    xgboost.fit(X_train,y_train)\n    \n    # use LightGBM\n    #!conda install -c conda-forge lightgbm\n    from lightgbm import LGBMClassifier\n    lightgbm=LGBMClassifier()\n    lightgbm.fit(X_train,y_train)\n\n    #print the training scores for each model\n    print('[0] Logistic Regression Training Score:',log.score(X_train,Y_train))\n    print('\\n[1] K Neighbors Training Score:',knn.score(X_train,Y_train))\n    print('\\n[2] SVC Linear Training Score:',svc_lin.score(X_train,Y_train))\n    print('\\n[3] SVC RBF Training Score:',svc_rbf.score(X_train,Y_train))\n    print('\\n[4] Gaussian Training Score:',gauss.score(X_train,Y_train))\n    print('\\n[5] Decision Tree Training Score:',tree.score(X_train,Y_train))\n    print('\\n[6] Random Forest Training Score:',forest.score(X_train,Y_train))\n    print('\\n[7] Hist Gradient Boosting Training Score:',histgrad.score(X_train,Y_train))\n    print('\\n[8] Gradient Boosting Training Score:',gbm.score(X_train,Y_train))\n    print('\\n[9] XGBoost Training Score:',xgboost.score(X_train,Y_train))\n    print('\\n[10] Light GBM Training Score:',lightgbm.score(X_train,Y_train))\n    \n    return log,knn,svc_lin,svc_rbf,gauss,tree,forest,histgrad,gbm,xgboost,lightgbm","39139d94":"log,knn,svc_lin,svc_rbf,gauss,tree,forest,histgrad,gbm,xgboost,lightgbm=models(X_train,y_train)","a34e74c0":"models=[log,knn,svc_lin,svc_rbf,gauss,tree,forest,histgrad,gbm,xgboost,lightgbm]\nmodels_name=[\"log\",\"knn\",\"svc_lin\",\"svc_rbf\",\"gauss\",\"tree\",\"forest\",\"histgrad\",\"gbm\",\"xgboost\",\"lightgbm\"]","8d6d7403":"before_tune_accuracy_score={}\ndef accuracy_score_calculator(model,model_name):\n    from sklearn.metrics import accuracy_score\n    y_pred=model.predict(X_test)\n    before_tune_accuracy_score[model_name]=accuracy_score(y_test,y_pred)","c1ff9c19":"before_tune_roc_score={}\ndef roc_score_calculator(model,model_name):\n    from sklearn.metrics import roc_auc_score\n    y_pred=model.predict_proba(X_test)[:,1]\n    before_tune_roc_score[model_name]=roc_auc_score(y_test,y_pred)","ae5bfcf6":"for i in range(len(models)):\n    roc_score_calculator(models[i],models_name[i])\n    accuracy_score_calculator(models[i],models_name[i])","e03d90f3":"size=np.arange(len(models))\nplt.ylabel=\"Percentage\"\nplt.bar(size-0.2, before_tune_roc_score.values(), color='g', width=0.4,tick_label=models_name)\nplt.bar(size+0.2, before_tune_accuracy_score.values(),color='b', width=0.4,tick_label=models_name)\nplt.legend([\"Before Roc Score\", \"Before Accuracy Score\"]);","211ae9d1":"from sklearn.metrics import plot_roc_curve\nplt.figure(figsize=(10,10))\nax = plt.gca()\nlog_disp = plot_roc_curve(log, X_test, y_test, ax=ax, alpha=0.8)\nknn_disp = plot_roc_curve(knn, X_test, y_test, ax=ax, alpha=0.8)\nsvc_lin_disp = plot_roc_curve(svc_lin, X_test, y_test, ax=ax, alpha=0.8)\nsvc_rbf_disp = plot_roc_curve(svc_rbf, X_test, y_test, ax=ax, alpha=0.8)\ngauss_disp = plot_roc_curve(gauss, X_test, y_test, ax=ax, alpha=0.8)\ntree_disp = plot_roc_curve(tree, X_test, y_test, ax=ax, alpha=0.8)\nforest_disp = plot_roc_curve(forest, X_test, y_test, ax=ax, alpha=0.8)\nhistgrad_disp = plot_roc_curve(histgrad, X_test, y_test, ax=ax, alpha=0.8)\ngbm_disp = plot_roc_curve(gbm, X_test, y_test, ax=ax, alpha=0.8)\nxgboost_disp = plot_roc_curve(xgboost, X_test, y_test, ax=ax, alpha=0.8)\nlightgbm_disp = plot_roc_curve(lightgbm, X_test, y_test, ax=ax, alpha=0.8)\nplt.legend(loc = 'lower right', prop={'size': 16})\nplt.show()","1eec23c9":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nlgbm_params={'n_estimators':[100, 500, 1000, 2000],\n            'subsample':[0.6,0.8, 1.0], 'max_depth':[3,4,5,6],\n            'learning_rate':[0.1, 0.01,0.02, 0.05], 'min_child_samples':[5,10,20]}\n            \nfrom lightgbm import LGBMClassifier\nlgbm=LGBMClassifier()\nlgbm_cv_model=GridSearchCV(lgbm,lgbm_params,cv=10,n_jobs=-1,verbose=2)\nlgbm_cv_model.fit(X_train,y_train)\nlgbm_cv_model.best_params_\n\"\"\"","080a531b":"after_tune_roc_scores={}\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nlgbm=LGBMClassifier(learning_rate= 0.1 , max_depth= 6, subsample= 0.6, n_estimators= 100, min_child_samples= 10)\nlgbm_tuned=lgbm.fit(X_train,y_train)\ny_pred=lgbm_tuned.predict_proba(X_test)[:,1]\nafter_tune_roc_scores[\"Light GBM Classifier\"]=roc_auc_score(y_test,y_pred)\nroc_auc_score(y_test,y_pred)","48c235d9":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nxgb_params={'n_estimators':[100, 500, 1000, 2000],\n            'subsample':[0.6,0.8, 1.0], 'max_depth':[3,4,5,6],\n            'learning_rate':[0.1, 0.01,0.02, 0.05], 'min_samples_split':[2,5,10]}\nfrom xgboost import XGBClassifier\nxgb=XGBClassifier()\nxgb_cv_model=GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,verbose=2)\nxgb_cv_model.fit(X_train,y_train)\nxgb_cv_model.best_params_\n\"\"\"","5e559685":"from xgboost import XGBClassifier\nxgb=XGBClassifier()\nxgb_tuned=xgb.fit(X_train,y_train)\ny_pred=xgb_tuned.predict_proba(X_test)[:,1]\nafter_tune_roc_scores[\"XGBoost Classifier\"]=roc_auc_score(y_test,y_pred)\nroc_auc_score(y_test,y_pred)","19749483":"\"\"\"\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nhistgrad_params={'max_depth':[4,6,8], 'max_leaf_nodes':np.arange(0,20), 'learning_rate':[0.1, 0.01]}\nhistgrad=HistGradientBoostingClassifier()\nhistgrad_cv_model=GridSearchCV(histgrad,histgrad_params,cv=10,n_jobs=-1)\nhistgrad_cv_model.fit(X_train,y_train)\nhistgrad_cv_model.best_params_\n\"\"\"","c3bb88c8":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nhistgrad=HistGradientBoostingClassifier(max_depth=8,max_leaf_nodes=15,learning_rate=0.1)\nhistgrad_tuned=xgb.fit(X_train,y_train)\ny_pred=histgrad_tuned.predict_proba(X_test)[:,1]\nafter_tune_roc_scores[\"Histogram Gradient Boosting\"]=roc_auc_score(y_test,y_pred)\nroc_auc_score(y_test,y_pred)","8ae011a8":"\"\"\"\ngbm_params={'learning_rate':[0.001, 0.01,0.1, 0.05], 'n_estimators':[100, 500, 1000],\n             'max_depth':[3, 5, 10], 'min_samples_split':[2,5,10]}\n             \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm=GradientBoostingClassifier()\ngbm_cv_model=GridSearchCV(gbm,gbm_params,cv=10,n_jobs=-1,verbose=2)\ngbm_cv_model.fit(X_train,y_train)\ngbm_cv_model.best_params_\n\"\"\"","ac8a3221":"from sklearn.ensemble import GradientBoostingClassifier\ngbm=GradientBoostingClassifier()\ngbm_tuned=gbm.fit(X_train,y_train)\ny_pred=gbm_tuned.predict_proba(X_test)[:,1]\nafter_tune_roc_scores[\"Gradient Boosting Classifier\"]=roc_auc_score(y_test,y_pred)\nroc_auc_score(y_test,y_pred)","c1867fce":"roc_curve_values_df=pd.DataFrame.from_dict(after_tune_roc_scores, orient='index',columns=[\"After Tune Roc Score\"])\nroc_curve_values_df = roc_curve_values_df.sort_values(by=['After Tune Roc Score'])\nroc_curve_values_df.plot(kind='barh', y='After Tune Roc Score', color=\"orange\", figsize=(10,6), title=\"After Tune Model's Roc Scores\");","7f4653b7":"roc_curve_values_df.index=[\"gbm\", \"xgboost\", \"histgrad\", \"lightgbm\" ]\nbefore_tune_roc_score_df=pd.DataFrame.from_dict(before_tune_roc_score,orient='index', columns=[\"Before Tune Roc Score\"])\nbefore_tune_roc_score_df","4d422d01":"final_scores=pd.concat([roc_curve_values_df, before_tune_roc_score_df], axis=1)\nfinal_scores.dropna(inplace=True)\nfinal_scores","7b111477":"size=np.arange(len(final_scores))\nplt.ylabel=\"Percentage\"\nplt.bar(size-0.2, list(final_scores[\"After Tune Roc Score\"].values), color='orange', width=0.4, \n        tick_label=final_scores[\"After Tune Roc Score\"].index)\nplt.bar(size+0.2, list(final_scores[\"Before Tune Roc Score\"].values), color='black', width=0.4)\nplt.legend([\"After Tune Roc Score\", \"Before  Tune Roc Score\"]);","6833e8f7":"<a id=\"17\"><\/a>\n### Finding and fixing outliers","0a5ba7b7":"95% of this line has a value of 999. The remainder has different values. So let's convert them to binary values 999 ->1 others ->0","ff103bc1":"Transforming some strings to 0 and 1","76e3a261":"<a id=\"16\"><\/a>\n### Visualization before outlier detection","d13cea78":"<a id=\"19\"><\/a>\n## Missing Value Imputation","f2e38632":"Convert object to categorical variable","611c9bb3":"Changed the columns' names to use for loops effective","2a97f9bc":"<a id=\"11\"><\/a>\n## Buff Method","37603289":"<a id=\"29\"><\/a>\n### Light GBM Classifier","aeacffc4":"<a id=\"22\"><\/a>\n## Feature Enconding","69cdc3c9":"Dropped following features bcs there is no relation","008be468":"<a id = \"2\"><\/a><br>\n## Explanation for features","5160117f":"<a id=\"5\"><\/a>\n# Distribution of Target","420c79f0":"<a id=\"13\"><\/a>\n## After train test split, checking for distibution of target","f454bb48":"This is data from a Portuguese bank. The aim is to predict if the client will subscribe a bank term deposit.\n\nWe have 41K rows and 20 features. \n\nMissing Attribute Values: There are several missing values in some categorical attributes, all coded with the \"unknown\" label. So first, we will find them all. Then will impute them with methodology what that feature needs.","279b86bd":"<a id=\"7\"><\/a>\n## Preparing numerical and categorical columns","467b9e1b":"ROC CURVE VISUALIZATION","197d70eb":"<a id=\"4\"><\/a>\n# Import Data and Python Packages","95f61a87":"<a id=\"12\"><\/a>\n# Train Test split","13222ae8":"<a id=\"6\"><\/a>\n# Data Analysis","e68023e5":"Creating index for data information DataFrame","64bde777":"<a id=\"20\"><\/a>\n### According to photo, improved different imputation models for our model.","408c9d47":"<a id=\"10\"><\/a>\n## Visualization for Numerical Features","f37ee798":"unknown values are missing values. So we converted them to nan","3014b8d4":"Add columns explanation and columns type to dataInfo","6bb89319":"Data type convertion","e9f2af65":"<a id=\"9\"><\/a>\n## Which categorical columns has 0 variance","4c7b5266":"<a id=\"25\"><\/a>\n## Correlation Matrix Between Features","173ca906":"# Conclusion: \n## Here, if we use Light GBM Model, we will get the best result. So for us, Light GBM is perfect match for our dataset.","cb38832c":"1. [Introduction](#1)\n      * [Explanation for features](#2)\n      * [Which Models Used](#3)\n2. [Import Data and Python Packages](#4)\n3. [Distribution of Target](#5)\n4. [Data Analysis](#6)\n    * [Preparing numerical and categorical columns](#7)\n    * [Which numerical columns has 0 variance](#8)\n    * [Which categorical columns has 0 variance](#9)\n    * [Visualization for Numerical Features](#10)\n    * [Buff Method](#11)\n5. [Train Test split](#12)\n    * [After train test split, checking for distibution of target](#13)\n6. [Feature Engineering](#14)\n    * [Outlier Detection](#15)\n        * [Visualization before outlier detection](#16)\n        * [Finding and fixing outliers](#17)\n        * [Visualization After Cleaning Outlier](#18)\n    * [Missing Value Imputation](#19)\n        * [According to photo, improved different imputation models for our model.](#20)\n    * [String Conversion](#21)\n    * [Feature Enconding](#22)\n    * [Normalization](#23)\n    * [Feature Importance](#24)\n    * [Correlation Matrix Between Features](#25)\n7. [MODELLING](#26)\n    * [Roc and Accuracy Score Before Tune](#27)\n    * [Model Tuning](#28)\n        * [Light GBM Classifier](#29)\n        * [XGBoost Classifier](#30)\n        * [Histogram-based Gradient Boosting Classification Tree](#31)\n        * [Gradient Boosting Classifier](#32)\n    * [Roc and Accuracy Score After Tune](#33)","147f532f":"<a id=\"8\"><\/a>\n## Which numerical columns has 0 variance","3cc1c8b2":"###### bank client data:\n1. age \n2. job : type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n3. marital : marital status (categorical: \"divorced\",\"married\",\"single\",\"unknown\"; note: \"divorced\" means divorced or widowed)\n4. education (categorical: \"basic.4y\", \"basic.6y\", \"basic.9y\", \"high.school\", \"illiterate\", \"professional.course\", \"university.degree\", \"unknown\")\n5. default: has credit in default? (categorical: \"no\",\"yes\",\"unknown\")\n6. housing: has housing loan? (categorical: \"no\",\"yes\",\"unknown\")\n7. loan: has personal loan? (categorical: \"no\",\"yes\",\"unknown\")\n\n###### related with the last contact of the current campaign:\n8. contact: contact communication type (categorical: \"cellular\",\"telephone\") \n9. month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n10. day_of_week: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n11. duration: last contact duration, in seconds (numeric). Important note:  this attribute highly affects the output target (e.g., if duration=0 then y=\"no\"). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n###### other attributes:\n12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14. previous: number of contacts performed before this campaign and for this client (numeric)\n15. poutcome: outcome of the previous marketing campaign (categorical: \"failure\",\"nonexistent\",\"success\")\n\n###### social and economic context attributes\n16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17. cons.price.idx: consumer price index - monthly indicator (numeric)     \n18. cons.conf.idx: consumer confidence index - monthly indicator (numeric)     \n19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n20. nr.employed: number of employees - quarterly indicator (numeric)\n\n###### target feature\n21. y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")","fe05d5c2":"<a id=\"23\"><\/a>\n## Normalization","ee360df3":"All 4 models in one graph","a67e1e62":"<a id=\"3\"><\/a>\n## Which models used","fbe895e9":"We see last 4 models gave better score, so let's try to find their best params and tune them ","237d3563":"<a id=\"24\"><\/a>\n## Feature Importance","7bb40305":"<a id=\"14\"><\/a>\n# Feature Engineering","7f84bbea":"<a id=\"18\"><\/a>\n### Visualization After Cleaning Outlier","ff6edf6d":"![Missing_Value.png](attachment:Missing_Value.png)","7d071b48":"<a id=\"28\"><\/a>\n## Model Tuning","6f7f05c7":"![Distibution%20of%20target.png](attachment:Distibution%20of%20target.png)","0ba61292":"<a id=\"15\"><\/a>\n## Outlier Detection","975f3e7b":"<a id=\"26\"><\/a>\n# MODELLING","272e090b":"<a id=\"27\"><\/a>\n## Roc and Accuracy Score Before Tune","8978d408":"We used the following models on this notebook:\n\n1. Logistic Regression\n2. KNeighbors Classifier\n3. SVC (Linear)\n4. SVC (RBF)\n5. Naive Bayes\n6. Decision Tree Classifier\n7. Random Forest Classifier\n8. Hist Gradient Boosting Classifier\n9. Gradient Boosting Classifier\n10. XGBoost Classifier\n11. Ligth GBM Classifier","ac8cee1d":"Finding which missing imputation method we should use","8b8570ea":"<a id=\"31\"><\/a>\n### Histogram-based Gradient Boosting Classification Tree","077b7801":"Converting floats of number of employers to int bcs of meaningless","0e42b0b3":"<a id=\"1\"><\/a>\n# Introduction","e5efe353":"<a id=\"30\"><\/a>\n### XGBoost Classifier","93db064f":"<a id=\"21\"><\/a>\n## String Conversion","ef8b18e6":"#### **Distibution of Target Data**","760e9570":"<a id=\"32\"><\/a>\n### Gradient Boosting Classifier","ec7b6486":"<a id=\"33\"><\/a>\n## Roc and Accuracy Score After Tune"}}