{"cell_type":{"b53bf8b9":"code","9175341a":"code","cc3d65c4":"code","374075fd":"code","bceb443f":"code","bb0c72a8":"code","a9f346f8":"code","caaf8b4e":"code","12b692b8":"code","3b470513":"code","6202da92":"code","f2b02f64":"code","f94263c2":"code","d6e10e06":"code","00ddacf9":"code","3fe6deb6":"code","7989c49f":"code","b943361c":"code","e75e904a":"code","35164c30":"code","d14eb067":"code","c8ac268d":"code","798bd32d":"code","9410a6fa":"code","99f7f5bd":"code","ad6db584":"code","a4695379":"code","d474330e":"code","0779344c":"code","dbb07aca":"code","729e3fef":"code","b8007669":"code","f72a14a6":"code","70b82f4d":"code","7c6a10b0":"code","77259888":"code","7ad40178":"code","e53ae4ae":"code","07a68256":"code","5714b04d":"code","76761396":"code","06a8e636":"code","5e806ea8":"code","8ee9f842":"code","03b6c9d5":"code","8194a660":"code","ffebd420":"code","f79896fe":"code","6a26f4cb":"code","55302ed8":"code","da9aef05":"code","ecc2d43d":"code","aa85074c":"code","78833f7a":"code","a34f4873":"code","73b08a1f":"code","0c8e09fe":"code","14e3e613":"code","968c36b4":"code","e0c4e77f":"code","b47eae86":"code","7218264f":"code","7178c8ad":"code","103d9e65":"code","da925d9e":"code","0c41b582":"code","7d96c97d":"code","3526c764":"code","d068c6a6":"code","b4ae9cc2":"code","12b4d2dd":"code","c03ff8df":"code","62f3373a":"code","db11ca29":"code","a8f4c30b":"code","c3c6b0bd":"code","6fb2d05b":"code","1d3ff54d":"code","0028ebec":"code","d32fe39a":"code","2d0405fe":"code","0826ad3c":"code","9562b290":"code","d3f12b17":"code","491983ca":"code","6a4c75c2":"code","52719287":"code","ffa70a82":"code","d7db2937":"code","42dec8db":"code","fa64d644":"code","a083c037":"markdown","b511612e":"markdown","2cd8be9b":"markdown","92263f86":"markdown","f5d47e8d":"markdown","b9699ae3":"markdown","ff9c4bcb":"markdown","edae8ac7":"markdown","6a4f679b":"markdown","0e2dd03c":"markdown","d95eddb5":"markdown","706a46aa":"markdown","fd0f2c2f":"markdown","883ba4d8":"markdown","642bc5a4":"markdown","8d279d2c":"markdown","13715631":"markdown","c3de3724":"markdown","177e4df3":"markdown","a237451e":"markdown","be41da5f":"markdown","f2b2bd31":"markdown","578be3e8":"markdown","3ed7bfb7":"markdown","70c4aafd":"markdown","128ba7b1":"markdown","7bf76fd4":"markdown","c2849304":"markdown","7b196de0":"markdown","c755309b":"markdown","088e4bf0":"markdown","12b87b50":"markdown","1497f5da":"markdown","7254fb39":"markdown","f87843fe":"markdown","0ee9d5d1":"markdown","19854abb":"markdown","43aa2782":"markdown","95f166f0":"markdown","37ddf6a5":"markdown","04c97d93":"markdown","2eecf00a":"markdown","e8dd85c8":"markdown","32e1c64c":"markdown","c90c9eec":"markdown","f864735c":"markdown","216bbb49":"markdown","ec162edd":"markdown","c6672ca6":"markdown","d37d3432":"markdown","061b261c":"markdown","652953b6":"markdown","79834396":"markdown"},"source":{"b53bf8b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9175341a":"df = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')","cc3d65c4":"df.info()","374075fd":"df.head(10)","bceb443f":"for col in df.columns:\n    print(f'{col} has {df[col].nunique()} unique values.')","bb0c72a8":"df.duplicated().sum()","a9f346f8":"cat_cols = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\nnum_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']","caaf8b4e":"for col in cat_cols:\n    print(f'The unique values in {col} are: {df[col].unique()}')","12b692b8":"df.isnull().sum()","3b470513":"corr = df.corr(method = 'spearman')\nplt.figure(figsize=(20,6))\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='Blues')\nplt.title('Spearman Correlation Heatmap')\nplt.show()","6202da92":"pairplot_cols = num_cols\npairplot_cols.append('HeartDisease')\nfigure = plt.figure(figsize=(20,10))\nsns.pairplot(df[pairplot_cols], hue='HeartDisease', palette='GnBu')\nplt.show()","f2b02f64":"df.loc[df['Cholesterol'] == 0, 'Cholesterol'].count()","f94263c2":"df.loc[(df['Cholesterol'] == 0) & (df['HeartDisease'] == 1), 'Cholesterol'].count()","d6e10e06":"num_cols.remove('Cholesterol')","00ddacf9":"fig, axes = plt.subplots(4, 3, figsize=(20,25))\nfor i, col in zip(range(4), num_cols):\n    sns.stripplot(ax=axes[i][0], x='HeartDisease', y=col, data=df, palette='GnBu', jitter=True)\n    axes[i][0].set_title(f'{col} Stripplot')\n    sns.histplot(ax=axes[i][1], x=col, data=df, kde=True, bins=10, palette='GnBu', hue='HeartDisease', multiple='dodge')\n    axes[i][1].set_title(f'{col} Displot')\n    sns.boxplot(ax=axes[i][2], x='HeartDisease', y=col, data=df, palette='GnBu', hue='HeartDisease')\n    axes[i][2].set_title(f'{col} Boxplot')","3fe6deb6":"def outlier_limits(df, col_name, q1 = 0.25, q3 = 0.75):\n    quartile1 = df[col_name].quantile(q1)\n    quartile3 = df[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_limits(df, variable, q1 = 0.25, q3 = 0.75):\n    low_limit, up_limit = outlier_limits(df, variable, q1 = q1, q3 = q3)\n    df.loc[(df[variable] < low_limit), variable] = low_limit\n    df.loc[(df[variable] > up_limit), variable] = up_limit\n    \nfor variable in df[num_cols].columns:\n    replace_with_limits(df, variable)","7989c49f":"fig, axes = plt.subplots(2, 2, figsize=(20,15))\nfor i, col in zip(range(4), num_cols):\n    sns.boxplot(ax=axes[i\/\/2][i%2], x='HeartDisease', y=col, data=df, palette='GnBu', hue='HeartDisease')\n    axes[i\/\/2][i%2].set_title(f'{col} Boxplot')","b943361c":"fig, axes = plt.subplots(2, 3, figsize=(20,12))\nfor i, col in zip(range(6), cat_cols):\n    sns.histplot(ax=axes[i\/\/3][i%3], x=col, data=df, palette='GnBu', hue='HeartDisease', multiple='dodge', bins='auto')\n    axes[i\/\/3][i%3].set_title(f'{col} Countplot')","e75e904a":"fig, axes = plt.subplots(2, 3, figsize=(20,12))\nfor i, col in zip(range(6), cat_cols):\n    sns.stripplot(ax=axes[i\/\/3][i%3], x=col, y='Age', data=df, palette='GnBu', hue='HeartDisease', jitter=True)\n    axes[i\/\/3][i%3].set_title(f'{col} Countplot')","35164c30":"eda_num_cols = ['RestingBP', 'MaxHR', 'Oldpeak']","d14eb067":"fig, axes = plt.subplots(1, 3, figsize=(20,7))\nfor i, col in zip(range(3), eda_num_cols):\n    sns.scatterplot(ax=axes[i], x='Age', y=col, hue=\"HeartDisease\", style=\"Sex\", data=df.iloc[0:889,:], palette=\"GnBu\")","c8ac268d":"num_cols.remove('HeartDisease')","798bd32d":"print(cat_cols)\nprint(num_cols)","9410a6fa":"X = df.iloc[:,:11]\ny = df['HeartDisease']\nX.head()","99f7f5bd":"from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npreprocessor1 = ColumnTransformer(\n    transformers = [\n        ('onehotcat', OneHotEncoder(), ['ChestPainType', 'ST_Slope', 'RestingECG', 'Sex', 'ExerciseAngina']),\n        ('num', MinMaxScaler(), num_cols),\n    ],\n    remainder = 'passthrough',\n)\npreprocessor2 = ColumnTransformer(\n    transformers = [\n        ('pca', pca, num_cols),\n        ('dropper', 'drop', ['x0_ASY', 'x1_Down', 'x2_LVH', 'x3_F', 'x4_N', 'Age', 'RestingBP', 'MaxHR', 'Oldpeak', 'Cholesterol'])\n    ],\n    remainder = 'passthrough',\n)","ad6db584":"preprocessor1_features = ['x0_ASY', 'x0_ATA', 'x0_NAP', 'x0_TA', 'x1_Down', 'x1_Flat', 'x1_Up', 'x2_LVH', 'x2_Normal', 'x2_ST', 'x3_F', 'x3_M', 'x4_N', 'x4_Y', 'Age', 'RestingBP', 'MaxHR', 'Oldpeak', 'Cholesterol', 'FastingBS']\nfinal_features = ['PC-1', 'PC-2', 'x0_ATA', 'x0_NAP', 'x0_TA', 'x1_Flat', 'x1_Up', 'x2_Normal', 'x2_ST', 'x3_M', 'x4_Y', 'FastingBS']","a4695379":"X = pd.DataFrame(preprocessor1.fit_transform(X), columns=preprocessor1_features)\nX = pd.DataFrame(preprocessor2.fit_transform(X), columns=final_features)\nX.head()","d474330e":"import statsmodels.api as sm\ndef calculate_vif(data):\n    vif_df = pd.DataFrame(columns = ['Feature', 'VIF'])\n    x_var_names = X.columns\n    for i in range(0, x_var_names.shape[0]):\n        y = X[x_var_names[i]]\n        x = X[x_var_names.drop([x_var_names[i]])]\n        r_squared = sm.OLS(y,x).fit().rsquared\n        vif = round(1\/(1-r_squared),2)\n        vif_df.loc[i] = [x_var_names[i], vif]\n    return vif_df.sort_values(by = 'VIF', axis = 0, ascending=False, inplace=False)\n\ncalculate_vif(X)","0779344c":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\nfrom sklearn.feature_selection import VarianceThreshold\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier","dbb07aca":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","729e3fef":"def fit(clf, params, cv=10, X_train=X_train, y_train=y_train):\n    grid = GridSearchCV(clf, params, cv=KFold(n_splits=cv), n_jobs=1, verbose=1, return_train_score=True, scoring='accuracy', refit=True) #verbose and n_jobs help us see the computation time and score of a cv. Higher the value of verbose, more the information printed out.\n    grid.fit(X_train, y_train)\n    return grid\n\ndef make_predictions(model, X_test=X_test):\n    return model.predict(X_test)\n\ndef best_scores(model):\n    # print(f'The mean cross validation test score is: {model.cv_results_.mean_test_score}') #for some reason this wasn't working for me even though the attribute exists so lets just leave it.\n    print(f'The best parameters are: {model.best_params_}')\n    print(f'The best score that we got is: {model.best_score_}')\n    return None\n\ndef plot_confusion_matrix(y_pred):\n    print('00: True Negatives\\n01: False Positives\\n10: False Negatives\\n11: True Positives\\n')\n    conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    fig, ax = plt.subplots(figsize=(5, 5))\n    ax.matshow(conf_matrix, cmap='GnBu', alpha=0.75)\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='large') \n    plt.xlabel('Predictions', fontsize=14)\n    plt.ylabel('Actuals', fontsize=14)\n    plt.title('Confusion Matrix', fontsize=14)\n    plt.show()\n    return None\n\ndef check_scores(y_pred):\n    print('Precision: %.3f' % precision_score(y_test, y_pred))\n    print('Recall: %.3f' % recall_score(y_test, y_pred))\n    print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n    print('F1 Score: %.3f' % f1_score(y_test, y_pred))\n    print('ROC-AUC Score: %.3f' % roc_auc_score(y_test, y_pred))\n    return None","b8007669":"lr_params = {'C':[0.001,.009,0.01,.09,1,5,10,25], 'penalty':['l1', 'l2']} #lasso and ridge regression\nlr_clf = LogisticRegression(solver='saga', max_iter=5000)\nlr_model = fit(lr_clf, lr_params)","f72a14a6":"best_scores(lr_model)","70b82f4d":"lr_y_pred = make_predictions(lr_model)\ncheck_scores(lr_y_pred)","7c6a10b0":"plot_confusion_matrix(lr_y_pred)","77259888":"lr_feature_scores = lr_model.best_estimator_.coef_[0].tolist()\nlr_fi = pd.DataFrame({'Feature': final_features, 'Feature Importance': lr_feature_scores})\nplt.figure(figsize=(10,6))\nsns.barplot(x='Feature Importance', y='Feature', data=lr_fi, palette='GnBu')\nplt.show()","7ad40178":"gnb_params = {'priors': [None], 'var_smoothing': np.logspace(0,-9, num=100)}\ngnb_clf = GaussianNB()\ngnb_model = fit(gnb_clf, gnb_params)","e53ae4ae":"best_scores(gnb_model)","07a68256":"gnb_y_pred = make_predictions(gnb_model)\ncheck_scores(gnb_y_pred)","5714b04d":"plot_confusion_matrix(gnb_y_pred)","76761396":"knns_params = {'n_neighbors': list(range(1, 31)), 'weights': ['uniform', 'distance'], \n               'metric': ['euclidean', 'manhattan']}\nknns_clf = KNeighborsClassifier()\nknns_model = fit(knns_clf, knns_params)","06a8e636":"best_scores(knns_model)","5e806ea8":"knns_y_pred = make_predictions(knns_model)\ncheck_scores(knns_y_pred)","8ee9f842":"plot_confusion_matrix(knns_y_pred)","03b6c9d5":"svm_params = {'C':[1,10,100,1000], 'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\nsvm_clf = SVC()\nsvm_model = fit(svm_clf, svm_params)","8194a660":"best_scores(svm_model)","ffebd420":"svm_y_pred = make_predictions(svm_model)\ncheck_scores(svm_y_pred)","f79896fe":"plot_confusion_matrix(svm_y_pred)","6a26f4cb":"dt_params = {'criterion': ['gini', 'entropy'], 'max_depth': range(1,10), \n             'min_samples_leaf': range(1,5)}\ndt_clf = DecisionTreeClassifier()\ndt_model = fit(dt_clf, dt_params)","55302ed8":"best_scores(dt_model)","da9aef05":"dt_y_pred = make_predictions(dt_model)\ncheck_scores(dt_y_pred)","ecc2d43d":"plot_confusion_matrix(dt_y_pred)","aa85074c":"fig = plt.figure(figsize=(50,40))\ntree.plot_tree(dt_model.best_estimator_, feature_names=final_features,  class_names=['0','1'], filled=True, fontsize=14, rounded=True)\nplt.show()","78833f7a":"rf_params = {'criterion' :['gini', 'entropy'], 'min_samples_leaf': [3, 4, 5], \n             'min_samples_split': [8, 10, 12], 'n_estimators': [100,250,500,600,700,800,900,1000]}\nrf_clf = RandomForestClassifier()\nrf_model = fit(rf_clf, rf_params)","a34f4873":"best_scores(rf_model)","73b08a1f":"rf_y_pred = make_predictions(rf_model)\ncheck_scores(rf_y_pred)","0c8e09fe":"plot_confusion_matrix(rf_y_pred)","14e3e613":"rf_feature_scores = rf_model.best_estimator_.feature_importances_.tolist()\nrf_fi = pd.DataFrame({'Feature': final_features, 'Feature Importance': rf_feature_scores})\nplt.figure(figsize=(10,6))\nsns.barplot(x='Feature Importance', y='Feature', data=rf_fi, palette='GnBu')\nplt.show()","968c36b4":"ada_params = {'n_estimators': [100, 200, 400, 500, 600, 800, 1000, 2000], \n              'learning_rate': [0.01, 0.1, 0.2, 0.5, 1]}\nada_clf = AdaBoostClassifier()\nada_model = fit(ada_clf, ada_params)","e0c4e77f":"best_scores(ada_model)","b47eae86":"ada_y_pred = make_predictions(ada_model)\ncheck_scores(ada_y_pred)","7218264f":"plot_confusion_matrix(ada_y_pred)","7178c8ad":"ada_feature_scores = ada_model.best_estimator_.feature_importances_.tolist()\nlr_fi = pd.DataFrame({'Feature': final_features, 'Feature Importance': ada_feature_scores})\nplt.figure(figsize=(10,6))\nsns.barplot(x='Feature Importance', y='Feature', data=lr_fi, palette='GnBu')\nplt.show()","103d9e65":"gb_params = {\"loss\": [\"exponential\"], \"learning_rate\": [0.001, 0.0025, 0.005, 0.0075, 0.01],\n             \"max_depth\": [4, 6, 8, 10], \"max_features\": [\"log2\", \"sqrt\"], \n             \"n_estimators\": [100, 250, 400, 500, 600, 750, 1000]}\ngb_clf = GradientBoostingClassifier()\ngb_model = fit(gb_clf, gb_params, cv=10)","da925d9e":"best_scores(gb_model)","0c41b582":"gb_y_pred = make_predictions(gb_model)\ncheck_scores(gb_y_pred)","7d96c97d":"plot_confusion_matrix(gb_y_pred)","3526c764":"gb_feature_scores = gb_model.best_estimator_.feature_importances_.tolist()\ngb_fi = pd.DataFrame({'Feature': final_features, 'Feature Importance': gb_feature_scores})\nplt.figure(figsize=(10,6))\nsns.barplot(x='Feature Importance', y='Feature', data=gb_fi, palette='GnBu')\nplt.show()","d068c6a6":"lgbm_params = {'num_leaves':[5, 10, 15, 20, 25], 'min_child_samples':[5, 10, 15],\n               'learning_rate':[0.001, 0.0025, 0.005, 0.0075, 0.01], 'objective': ['binary']}\nlgbm_clf = lgb.LGBMClassifier()\nlgbm_model = fit(lgbm_clf, lgbm_params)","b4ae9cc2":"best_scores(lgbm_model)","12b4d2dd":"lgbm_y_pred = make_predictions(lgbm_model)\ncheck_scores(lgbm_y_pred)","c03ff8df":"plot_confusion_matrix(lgbm_y_pred)","62f3373a":"lgbm_feature_scores = lgbm_model.best_estimator_.feature_importances_.tolist()\nlgbm_fi = pd.DataFrame({'Feature': final_features, 'Feature Importance': lgbm_feature_scores})\nplt.figure(figsize=(10,6))\nsns.barplot(x='Feature Importance', y='Feature', data=lgbm_fi, palette='GnBu')\nplt.show()","db11ca29":"xgb_params = {'max_depth': range (2, 10, 1), 'n_estimators': [50, 100, 250, 400, 500, 600, 750, 1000],\n              'learning_rate': [0.001, 0.0025, 0.005, 0.0075, 0.01],\n              'objective': ['binary:hinge', 'binary:logistic', 'binary:logitraw']\n}\nxgb_clf = xgb.XGBClassifier()\ntxgb_model = fit(xgb_clf, xgb_params)","a8f4c30b":"best_scores(xgb_model)","c3c6b0bd":"xgb_y_pred = make_predictions(xgb_model)\ncheck_scores(xgb_y_pred)","6fb2d05b":"plot_confusion_matrix(xgb_y_pred)","1d3ff54d":"xgb_feature_scores = xgb_model.best_estimator_.feature_importances_.tolist()\nxgb_fi = pd.DataFrame({'Feature': final_features, 'Feature Importance': xgb_feature_scores})\nplt.figure(figsize=(10,6))\nsns.barplot(x='Feature Importance', y='Feature', data=xgb_fi, palette='GnBu')\nplt.show()","0028ebec":"cb_params = {'depth': [4, 6, 8, 10], 'learning_rate': [0.001, 0.0025, 0.005, 0.0075, 0.04],\n             'n_estimators': [10, 25, 50, 75, 100], 'loss_function': ['Logloss', 'CrossEntropy']}\ncb_clf = CatBoostClassifier()\ncb_model = fit(cb_clf, cb_params)","d32fe39a":"best_scores(cb_model)","2d0405fe":"cb_y_pred = make_predictions(cb_model)\ncheck_scores(cb_y_pred)","0826ad3c":"plot_confusion_matrix(cb_y_pred)","9562b290":"cb_feature_scores = cb_model.best_estimator_.feature_importances_.tolist()\ncb_fi = pd.DataFrame({'Feature': final_features, 'Feature Importance': cb_feature_scores})\nplt.figure(figsize=(10,6))\nsns.barplot(x='Feature Importance', y='Feature', data=cb_fi, palette='GnBu')\nplt.show()","d3f12b17":"from keras.models import Sequential\nfrom keras.layers import Dense, LeakyReLU\nfrom keras import metrics\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier","491983ca":"nn_params = {'batch_size': [30, 60, 90], 'activation': ['relu', 'tanh', 'sigmoid'], \n             'kernel_initializer': ['HeNormal', 'GlorotNormal'],\n             'neurons': [8,9,10], 'epochs': [500] \n            }\nlearning_rate = [0.001, 0.01]\nnn_params['learning_rate'] = learning_rate\n\ndef create_network(learning_rate=0.01, activation='tanh', kernel_initializer='HeNormal', neurons=9):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=12, kernel_initializer=kernel_initializer, activation=activation))\n    model.add(Dense(1, activation='sigmoid'))\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_accuracy'])\n    return model","6a4c75c2":"network = KerasClassifier(build_fn=create_network, epochs=500, verbose=2)\nnn_model = fit(network, nn_params, cv=5)","52719287":"best_scores(nn_model)","ffa70a82":"plt.plot(nn_model.best_estimator_.model.history.history['binary_accuracy'], color='Green')\nplt.plot(nn_model.best_estimator_.model.history.history['loss'], color='Blue')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy', 'Loss'], loc='center right')\nplt.show()","d7db2937":"nn_y_pred = nn_model.predict(X_test)\nnn_y_pred = nn_y_pred > 0.5 #0.5 being the threshold value\ncheck_scores(nn_y_pred)","42dec8db":"plot_confusion_matrix(nn_y_pred)","fa64d644":"nn_model_json = nn_model.best_estimator_.model.to_json()\nwith open(\"nn_model.json\", \"w\") as json_file:\n    json_file.write(nn_model_json)\nnn_model.best_estimator_.model.save_weights(\"nn_model_weights.h5\")","a083c037":"Oldpeak still has a lot of outliers. We will look into it later when checking multicollinearity.","b511612e":"Let's now do some EDA for more than one feature, keeping the hue same as our target column.","2cd8be9b":"Time elapsed: 36.2s","92263f86":"<a id=\"lr\"><\/a>  \n## Logistic Regression","f5d47e8d":"<a id=\"cat\"><\/a>  \n## CatBoost","b9699ae3":"<a id=\"ml\"><\/a>  \n# Machine Learning  \nWe will split the data first, apply different models and optimize them.","ff9c4bcb":"Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$  \nPrecision = $\\frac{TP}{TP + FP}$  \nRecall = $\\frac{TP}{TP + FN}$  \nF1 Score = $\\frac{2 * Precision * Recall}{Precision + Recall}$","edae8ac7":"<a id=\"dt\"><\/a>  \n## Decision Trees","6a4f679b":"Time elapsed: 27.4 min","0e2dd03c":"<a id=\"outliers\"><\/a>  \n## Replacing Outliers","d95eddb5":"<a id=\"rf\"><\/a>  \n## Random Forests","706a46aa":"The best parameters are: {'learning_rate': 0.005, 'loss': 'exponential', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 750}  \nThe best score that we got is: 0.8567752684191042   \nPrecision: 0.848  \nRecall: 0.873  \nAccuracy: 0.842  \nF1 Score: 0.860  \nROC-AUC Score: 0.839","fd0f2c2f":"Time elapsed: 1.4s","883ba4d8":"The best parameters are: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 4}  \nThe best score that we got is: 0.8350055534987042  \nPrecision: 0.843  \nRecall: 0.843  \nAccuracy: 0.826  \nF1 Score: 0.843  \nROC-AUC Score: 0.824","642bc5a4":"Time elapsed: 10 min","8d279d2c":"The best parameters are: {'priors': None, 'var_smoothing': 0.08111308307896872}  \nThe best score that we got is: 0.8554794520547946   \nPrecision: 0.898  \nRecall: 0.863  \nAccuracy: 0.870  \nF1 Score: 0.880  \nROC-AUC Score: 0.870","13715631":"The best parameters are: {'criterion': 'entropy', 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 100}  \nThe best score that we got is: 0.8567937800814514  \nPrecision: 0.853  \nRecall: 0.853  \nAccuracy: 0.837  \nF1 Score: 0.853  \nROC-AUC Score: 0.835","c3de3724":"When using column transformer, the items in array will be in the order in which they get encoded. So the OHE variables come first, followed by scaled variables and then the variables that didn't require preprocessing.  \nWe will be dropping 1 feature from each feature that was OHE to avoid dummy variable trap which causes multicollinearity.","177e4df3":"Time elapsed: 97.5 min","a237451e":"We want to apply different types of encoding techniques for different variables. Let's use column transformer to achieve this.","be41da5f":"The best parameters are: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 750, 'objective': 'binary:logistic'}  \nThe best score that we got is: 0.85272121436505  \nPrecision: 0.838  \nRecall: 0.863  \nAccuracy: 0.832  \nF1 Score: 0.850  \nROC-AUC Score: 0.828","f2b2bd31":"All features seem to be useful ahead as you can see that some categorical variables have certain values that have a lot of people with heart disease. The features aren't correlated which is good. With this, we are done with our EDA.","578be3e8":"The best parameters are: {'activation': 'relu', 'batch_size': 90, 'epochs': 500, 'kernel_initializer': 'HeNormal', 'learning_rate': 0.001, 'neurons': 9}  \nThe best score that we got is: 0.8595937004938963  \nPrecision: 0.864  \nRecall: 0.931  \nAccuracy: 0.880  \nF1 Score: 0.896  \nROC-AUC Score: 0.874","3ed7bfb7":"Time elapsed: 2.9 min","70c4aafd":"<a id=\"preprocessing\"><\/a>  \n# Data Preprocessing","128ba7b1":"<a id=\"nn\"><\/a>    \n# Feedforward Neural Networks","7bf76fd4":"There are some outliers in the numerical columns. Let's replace them with the threshold values using interquantile range.","c2849304":"The numerical columns show multicollinearity. Rather than removing the numerical columns, it is better to reduce their dimension using PCA which will also reduce VIF. Numerical features do contain important information. We can add the preprocessor2 which contains the PCA step to our pipeline.","7b196de0":"You can use the code below for VIF to check whether multicollinearity between features exists or not (features with VIF>5 show multicollinearity). We will run it to see which features show multicollinearity and then go back up to the column transformer to add a PCA step.","c755309b":"The best parameters are: {'C': 0.09, 'penalty': 'l2'}  \nThe best score that we got is: 0.855460940392447  \nPrecision: 0.873  \nRecall: 0.873  \nAccuracy: 0.859  \nF1 Score: 0.873  \nROC-AUC Score: 0.857","088e4bf0":"Time elapsed: 4.7s","12b87b50":"<a id=\"eda-continued\"><\/a>  \n## EDA (Continued)","1497f5da":"Time elapsed: 5.8s","7254fb39":"Time elapsed: 173 min","f87843fe":"<a id=\"knn\"><\/a>  \n## K-NNs","0ee9d5d1":"Something that I noticed in the cholesterol column is that there are a lot of entries with value as 0. Let's see how many such entries are there.","19854abb":"Time elapsed: 42s","43aa2782":"<a id=\"lightgbm\"><\/a>  \n## LightGBM","95f166f0":"The best parameters are: {'learning_rate': 0.01, 'n_estimators': 400}  \nThe best score that we got is: 0.843169196593854   \nPrecision: 0.842  \nRecall: 0.833  \nAccuracy: 0.821  \nF1 Score: 0.837  \nROC-AUC Score: 0.819","37ddf6a5":"The best parameters are: {'learning_rate': 0.01, 'min_child_samples': 5, 'num_leaves': 10, 'objective': 'binary'}  \nThe best score that we got is: 0.8526656793780083  \nPrecision: 0.817    \nRecall: 0.873    \nAccuracy: 0.821    \nF1 Score: 0.844    \nROC-AUC Score: 0.814","04c97d93":"<a id=\"introduction\"><\/a>  \n# Introduction\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\n<img src= \"https:\/\/health.clevelandclinic.org\/wp-content\/uploads\/sites\/3\/2018\/08\/GettyImages-944106494.jpg\" width=\"1100\">\n\n## Attribute Information\n\n1. Age: age of the patient [years]\n2. Sex: sex of the patient [M: Male, F: Female]\n3. ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n4. RestingBP: resting blood pressure [mm Hg]\n5. Cholesterol: serum cholesterol [mm\/dl]\n6. FastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n7. RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n10. Oldpeak: oldpeak = ST [Numeric value measured in depression]\n11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n12. HeartDisease: output class [1: heart disease, 0: Normal]\n\nWe will be predicting 'HeartDisease' using various Machine Learning algorithms first which will be optimized too. After that, we will make a neural network for the predictions.","2eecf00a":"<a id=\"dataframe\"><\/a>\n# Accessing the Dataframe","e8dd85c8":"<a id=\"gb\"><\/a>  \n## Gradient Boosting","32e1c64c":"<a id=\"missing-values\"><\/a>  \n# Checking for Missing Values","c90c9eec":"The best parameters are: {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'distance'}  \nThe best score that we got is: 0.8541651240281377     \nPrecision: 0.877  \nRecall: 0.912  \nAccuracy: 0.880  \nF1 Score: 0.894  \nROC-AUC Score: 0.877","f864735c":"<a id=\"xgboost\"><\/a>  \n## XGBoost","216bbb49":"<a id=\"svm\"><\/a>  \n## SVMs","ec162edd":"The best parameters are: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}  \nThe best score that we got is: 0.8567937800814512    \nPrecision: 0.847  \nRecall: 0.922  \nAccuracy: 0.864  \nF1 Score: 0.883  \nROC-AUC Score: 0.857","c6672ca6":"The best parameters are: {'depth': 6, 'learning_rate': 0.04, 'loss_function': 'Logloss', 'n_estimators': 100}  \nThe best score that we got is: 0.8622362088115514  \nPrecision: 0.840  \nRecall: 0.873  \nAccuracy: 0.837  \nF1 Score: 0.856  \nROC-AUC Score: 0.833","d37d3432":"<a id=\"ada\"><\/a>  \n## AdaBoost","061b261c":"Great! We don't have to deal with null values. We have separated the categorical and numerical columns and so we can now begin EDA.  \n<a id=\"eda\"><\/a>  \n# EDA","652953b6":"There are 172 values with cholesterol value 0 and I think this has been done to fill the missing data. Out of 172 values, 152 have heart disease. Let's just remove this column because these 172 values are basically missing values imputed with 0. We will remove it later using column transformer.","79834396":"<a id=\"gnb\"><\/a>  \n## Gaussian Naive Bayes"}}