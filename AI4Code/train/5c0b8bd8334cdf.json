{"cell_type":{"aeb71dbc":"code","6c36000e":"code","804321b7":"code","e3be6f09":"code","093d9ccd":"code","264e0d3f":"code","810c30d3":"code","169e46e8":"code","391d97a7":"code","e574d54e":"code","29ef2056":"code","f506003d":"code","4cbfd8d7":"code","5913a4c7":"code","c505c0ed":"code","e9a2ae80":"code","a0372726":"code","75c5ac33":"code","14c7652c":"code","bf3517a6":"code","1de6c85c":"code","8d5d166d":"code","275bd03f":"code","82845e79":"code","85c90275":"code","3b18b5fa":"code","8df31091":"code","0f4f6da5":"code","5dff787e":"code","97efb7f5":"code","7a75c3bd":"code","21c181ce":"code","8d6f85b8":"code","3ba5a007":"code","fb7e39ab":"markdown","4cf0fb52":"markdown","8923c9d5":"markdown","f89f00c4":"markdown","bb55a3c2":"markdown","fbb696a9":"markdown","e52b1b12":"markdown","8227061a":"markdown","9f7c0403":"markdown","2d2fe86a":"markdown","d0e27246":"markdown","7cc7a78a":"markdown","8de2d053":"markdown","2c518e46":"markdown","bcb776c6":"markdown","d3c6e52d":"markdown","ac5478ad":"markdown","6d6e367b":"markdown","28576d43":"markdown","29a84bce":"markdown","07525212":"markdown","df71a826":"markdown","77e4263d":"markdown","24c2e87e":"markdown","a73e61e6":"markdown","080a2a2d":"markdown","e6817ae0":"markdown","b226f5f7":"markdown","b22cf197":"markdown","f9ee7393":"markdown","eb61a5ea":"markdown","352e8d6e":"markdown","9da73bc4":"markdown","de0aed21":"markdown","94105d98":"markdown"},"source":{"aeb71dbc":"import tensorflow as tf\n\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '\/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')","6c36000e":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","804321b7":"# !pip install transformers=='2.8.0'","e3be6f09":"import pandas as pd\nimport re\nimport os\nimport math\nimport torch\n# import torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss, NLLLoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n# from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\nfrom transformers import AdamW, XLNetTokenizer, XLNetModel, TFXLNetModel, XLNetLMHeadModel, XLNetConfig, XLNetForSequenceClassification\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom tqdm import tqdm, trange\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# import emoji\n\n# Load the dataset into a pandas dataframe.\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv',encoding='UTF-8')\n# subsetting the data to not run out of memory\n# train = train.head(100)\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv',encoding='UTF-8')\n\n# Report the number of sentences.\nprint('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n\n# Display 10 random rows from the data.\ntrain.sample(10)","093d9ccd":"# #HappyEmoticons\n# emoticons_happy = set([\n#     ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n#     ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n#     '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n#     'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n#     '<3'\n#     ])\n\n# # Sad Emoticons\n# emoticons_sad = set([\n#     ':L', ':-\/', '>:\/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n#     ':-[', ':-<', '=\\\\', '=\/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n#     ':c', ':{', '>:\\\\', ';('\n#     ])\n\n# #combine sad and happy emoticons\n# emoticons = emoticons_happy.union(emoticons_sad)\n\n\n#Emoji patterns\nemoji_pattern = re.compile(\"[\"\n         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)","264e0d3f":"from nltk.tokenize import WordPunctTokenizer\nimport re\n# import emoji\nfrom bs4 import BeautifulSoup\nimport itertools\n\ntok = WordPunctTokenizer()\npat1 = r'@[A-Za-z0-9]+'\npat2 = r'https?:\/\/[A-Za-z0-9.\/]+'\n\ndef tweet_cleaner(text): # ref: https:\/\/towardsdatascience.com\/another-twitter-sentiment-analysis-bb5b01ebad90\n    # removing UTF-8 BOM (Byte Order Mark)\n    try:\n        text1 = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") # The UTF-8 BOM is a sequence of bytes (EF BB BF) that allows the reader to identify a file as being encoded in UTF-8\n    except:\n        text1 = text\n    \n    \n    #replace consecutive non-ASCII characters with a space\n    text1 = re.sub(r'[^\\x00-\\x7F]+',' ', text1)\n    \n    #remove emojis from tweet\n    text2 = emoji_pattern.sub(r'', text1)\n    \n    # Remove emoticons\n    # text3 = [word for word in text2.split() if word not in emoticons]\n    # text3 = \" \".join(text3)\n    \n    # contradictions and special characters \n    # text4 = spl_ch_contra(text3)\n    \n    # HTML encoding\n    soup = BeautifulSoup(text2, 'lxml') #HTML encoding has not been converted to text, and ended up in text field as \u2018&amp\u2019,\u2019&quot\u2019,etc.\n    text5 = soup.get_text()\n    \n    # removing @ mentions\n    text6 = re.sub(pat1, '', text5)\n    \n    # Removing URLs\n    text7 = re.sub(pat2, '', text6)\n    \n    # Removing punctuations\n    # text8 = re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\(\\)\\[\\]\\\"\\'\\%\\*\\#\\@]\", \" \", text7)\n    \n    # Fix misspelled words\n    text9 = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text7))# checking that each character should occur not more than 2 times in every word\n\n    # Tokenizing ,change cases & join together to remove unneccessary white spaces\n    text9_list = tok.tokenize(text9.lower())\n    text10 = (\" \".join(text9_list)).strip()\n    \n    return text10","810c30d3":"# cleaning tweets\ntrain['text_cleaned'] = list(map(lambda x:tweet_cleaner(x),train['text']) )\n","169e46e8":"# checking out few samples\ntrain.sample(10)","391d97a7":"# Get the lists of sentences and their labels.\nsentences = train.text_cleaned.values\nlabels = train.target.values","e574d54e":"# This is the identifier of the model. The library need this ID to download the weights and initialize the architecture\n# here is all the supported ones:\n# https:\/\/huggingface.co\/transformers\/pretrained_models.html\ntokenizer = XLNetTokenizer.from_pretrained('\/kaggle\/input\/xlnetbasecased\/xlnet_cased_L-12_H-768_A-12\/', do_lower_case=True)","29ef2056":"# Print the original sentence.\nprint(' Original: ', sentences[1])\n\n# Print the tweet split into tokens.\nprint('Tokenized: ', tokenizer.tokenize(sentences[1]))\n\n# Print the tweet mapped to token ids.\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[1])))","f506003d":"def tokenize_inputs(text_list, tokenizer, num_embeddings=120):\n    \"\"\"\n    Tokenizes the input text input into ids. Appends the appropriate special\n    characters to the end of the text to denote end of sentence. Truncate or pad\n    the appropriate sequence length.\n    \"\"\"\n    # tokenize the text, then truncate sequence to the desired length minus 2 for\n    # the 2 special characters\n    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n    # convert tokenized text into numeric ids for the appropriate LM\n    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n    # append special token \"<s>\" and <\/s> to end of sentence\n    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n    # pad sequences\n    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n    return input_ids\n\ndef create_attn_masks(input_ids):\n    \"\"\"\n    Create attention masks to tell model whether attention should be applied to\n    the input id tokens. Do not want to perform attention on padding tokens.\n    \"\"\"\n    # Create attention masks\n    attention_masks = []\n\n    # Create a mask of 1s for each token followed by 0s for padding\n    for seq in input_ids:\n        seq_mask = [float(i>0) for i in seq]\n        attention_masks.append(seq_mask)\n    return attention_masks","4cbfd8d7":"# Tokenize all of the sentences and map the tokens to thier word IDs.\n\ninput_ids = tokenize_inputs(sentences, tokenizer, num_embeddings=120)\nattention_masks = create_attn_masks(input_ids)\n\n# Convert the lists into tensors.\n# input_ids = torch.cat(input_ids, dim=0)\n# attention_masks = torch.cat(attention_masks, dim=0)\ninput_ids = torch.from_numpy(input_ids)\nattention_masks = torch.tensor(attention_masks)\nlabels = torch.tensor(labels)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', sentences[1])\nprint('Token IDs:', input_ids[1])\nprint('Token IDs:', attention_masks[1])","5913a4c7":"from torch.utils.data import TensorDataset, random_split\n\n# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 80-20 train-validation split.\n\n# Calculate the number of samples to include in each set.\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Checking whether the distribution of target is consitent across both the sets\nlabel_temp_list = []\nfor a,b,c in train_dataset:\n  label_temp_list.append(c)\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} training samples with real disater tweets'.format(sum(label_temp_list)))\n\n\nlabel_temp_list = []\nfor a,b,c in val_dataset:\n  label_temp_list.append(c)\n\nprint('{:>5,} validation samples'.format(val_size))\nprint('{:>5,} validation samples with real disater tweets'.format(sum(label_temp_list)))","c505c0ed":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# The DataLoader needs to know our batch size for training, so we specify it \n# here. Batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","e9a2ae80":"#config = XLNetConfig()\n     \nclass XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n  \n  def __init__(self, num_labels=2):\n    super(XLNetForMultiLabelSequenceClassification, self).__init__()\n    self.num_labels = num_labels\n    self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n    self.classifier = torch.nn.Linear(768, num_labels)\n\n    torch.nn.init.xavier_normal_(self.classifier.weight)\n\n  def forward(self, input_ids, token_type_ids=None,\\\n              attention_mask=None, labels=None):\n       \n    # last hidden layer\n    last_hidden_state = self.xlnet(input_ids=input_ids,\\\n                                   attention_mask=attention_mask,\\\n                                   token_type_ids=token_type_ids\n                                  )\n    # pool the outputs into a mean vector\n    mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n    logits = self.classifier(mean_last_hidden_state)\n#     print(logits.view(-1, self.num_labels))\n    logits = logits[:, 1] - logits[:, 0]\n    if labels is not None:\n#       loss_fct = BCEWithLogitsLoss()\n      loss = BCEWithLogitsLoss()(logits, labels.float())\n#       loss = loss_fct(logits.view(-1, self.num_labels),\\\n#                       labels.view(-1, self.num_labels))\n    \n      return loss\n    else:\n      return logits\n    \n  def freeze_xlnet_decoder(self):\n    \"\"\"\n    Freeze XLNet weight parameters. They will not be updated during training.\n    \"\"\"\n    for param in self.xlnet.parameters():\n      param.requires_grad = False\n    \n  def unfreeze_xlnet_decoder(self):\n    \"\"\"\n    Unfreeze XLNet weight parameters. They will be updated during training.\n    \"\"\"\n    for param in self.xlnet.parameters():\n      param.requires_grad = True\n    \n  def pool_hidden_state(self, last_hidden_state):\n    \"\"\"\n    Pool the output vectors into a single mean vector \n    \"\"\"\n    last_hidden_state = last_hidden_state[0]\n    mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n    return mean_last_hidden_state\n    \nmodel = XLNetForMultiLabelSequenceClassification(num_labels=len(labels.unique()))\n# model = torch.nn.DataParallel(model)\n# model.cuda()","a0372726":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5\n                  # eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                 weight_decay=0.01,\n                 correct_bias=False\n                )\n\n\n# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler","75c5ac33":"def train(model, num_epochs,\\\n          optimizer,\\\n          train_dataloader, valid_dataloader,\\\n          model_save_path,\\\n          train_loss_set=[], valid_loss_set = [],\\\n          lowest_eval_loss=None, start_epoch=0,\\\n          device=\"cpu\"\n          ):\n  \"\"\"\n  Train the model and save the model with the lowest validation loss\n  \"\"\"\n  # We'll store a number of quantities such as training and validation loss, \n  # validation accuracy, and timings.\n  training_stats = []\n  # Measure the total training time for the whole run.\n  total_t0 = time.time()\n\n  model.to(device)\n\n  # trange is a tqdm wrapper around the normal python range\n  for i in trange(num_epochs, desc=\"Epoch\"):\n    # if continue training from saved model\n    actual_epoch = start_epoch + i\n\n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set. \n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(actual_epoch, num_epochs))\n    print('Training...')\n    \n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    \n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n\n    # Tracking variables\n    tr_loss = 0\n    num_train_samples = 0\n\n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n            \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        loss = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        # store train loss\n        tr_loss += loss.item()\n        num_train_samples += b_labels.size(0)\n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n        #scheduler.step()\n\n    # Update tracking variables\n    epoch_train_loss = tr_loss\/num_train_samples\n    train_loss_set.append(epoch_train_loss)\n\n#     print(\"Train loss: {}\".format(epoch_train_loss))\n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    \n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n    \n    # Put model in evaluation mode to evaluate loss on the validation set\n    model.eval()\n\n    # Tracking variables \n    eval_loss = 0\n    num_eval_samples = 0\n\n    # Evaluate data for one epoch\n    for batch in valid_dataloader:\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Telling the model not to compute or store gradients,\n        # saving memory and speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate validation loss\n            loss = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            # store valid loss\n            eval_loss += loss.item()\n            num_eval_samples += b_labels.size(0)\n\n    epoch_eval_loss = eval_loss\/num_eval_samples\n    valid_loss_set.append(epoch_eval_loss)\n\n#     print(\"Valid loss: {}\".format(epoch_eval_loss))\n    \n    # Report the final accuracy for this validation run.\n#     avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n#     print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n#     avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(epoch_eval_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': actual_epoch,\n            'Training Loss': epoch_train_loss,\n            'Valid. Loss': epoch_eval_loss,\n#             'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\n    \n    if lowest_eval_loss == None:\n      lowest_eval_loss = epoch_eval_loss\n      # save model\n      save_model(model, model_save_path, actual_epoch,\\\n                 lowest_eval_loss, train_loss_set, valid_loss_set)\n    else:\n      if epoch_eval_loss < lowest_eval_loss:\n        lowest_eval_loss = epoch_eval_loss\n        # save model\n        save_model(model, model_save_path, actual_epoch,\\\n                   lowest_eval_loss, train_loss_set, valid_loss_set)\n  \n  print(\"\")\n  print(\"Training complete!\")\n\n  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n  return model, train_loss_set, valid_loss_set, training_stats","14c7652c":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","bf3517a6":"# function to save and load the model form a specific epoch\ndef save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n  \"\"\"\n  Save the model to the path directory provided\n  \"\"\"\n  model_to_save = model.module if hasattr(model, 'module') else model\n  checkpoint = {'epochs': epochs, \\\n                'lowest_eval_loss': lowest_eval_loss,\\\n                'state_dict': model_to_save.state_dict(),\\\n                'train_loss_hist': train_loss_hist,\\\n                'valid_loss_hist': valid_loss_hist\n               }\n  torch.save(checkpoint, save_path)\n  print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n                                                                     lowest_eval_loss))\n  return\n  \ndef load_model(save_path):\n  \"\"\"\n  Load the model from the path directory provided\n  \"\"\"\n  checkpoint = torch.load(save_path)\n  model_state_dict = checkpoint['state_dict']\n  model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n  model.load_state_dict(model_state_dict)\n\n  epochs = checkpoint[\"epochs\"]\n  lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n  train_loss_hist = checkpoint[\"train_loss_hist\"]\n  valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n  \n  return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist","1de6c85c":"torch.cuda.empty_cache()","8d5d166d":"num_epochs = 3\n\ncwd = os.getcwd()\nmodel_save_path = output_model_file = os.path.join(cwd, \"xlnet_base_disaster_tweet_classification\/xlnet_tweet.bin\")\nos.mkdir('\/kaggle\/working\/xlnet_base_disaster_tweet_classification')\n\n# model_save_path = '\/content\/drive\/My Drive\/Disaster_Tweets\/XLNet_tweet_classification_model\/xlnet_tweet.bin'\nmodel, train_loss_set, valid_loss_set, training_stats = train(model=model,\\\n                                                              num_epochs=num_epochs,\\\n                                                              optimizer=optimizer,\\\n                                                              train_dataloader=train_dataloader,\\\n                                                              valid_dataloader=validation_dataloader,\\\n                                                              model_save_path=model_save_path,\\\n                                                              device=\"cuda\"\n                                                              )","275bd03f":"import pandas as pd\n\n# Display floats with two decimal places.\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table.\ndf_stats","82845e79":"#  Plot loss\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([0, 1, 2, 3, 4, 5])\n\nplt.show()","85c90275":"# cwd = os.getcwd()\n# model_save_path = output_model_file = os.path.join(cwd, \"xlnet_base_disaster_tweet_classification\/xlnet_tweet.bin\")\n# model, start_epoch, lowest_eval_loss, train_loss_hist, valid_loss_hist = load_model(model_save_path)","3b18b5fa":"# optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)","8df31091":"# num_epochs=3\n# model, train_loss_set, valid_loss_set, training_stats = train(model=model,\\\n#                                                               num_epochs=num_epochs,\\\n#                                                               optimizer=optimizer,\\\n#                                                               train_dataloader=train_dataloader,\\\n#                                                               valid_dataloader=validation_dataloader,\\\n#                                                               model_save_path=model_save_path,\\\n#                                                               train_loss_set=train_loss_hist,\\\n#                                                               valid_loss_set=valid_loss_hist,\\\n#                                                               lowest_eval_loss=lowest_eval_loss,\\\n#                                                               start_epoch=start_epoch,\\\n#                                                               device=\"cuda\")","0f4f6da5":"# import pandas as pd\n\n# # Display floats with two decimal places.\n# pd.set_option('precision', 2)\n\n# # Create a DataFrame from our training statistics.\n# df_stats = pd.DataFrame(data=training_stats)\n\n# # Use the 'epoch' as the row index.\n# df_stats = df_stats.set_index('epoch')\n\n# # A hack to force the column headers to wrap.\n# #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n\n# # Display the table.\n# df_stats","5dff787e":"# # Plot loss\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n# import seaborn as sns\n\n# # Use plot styling from seaborn.\n# sns.set(style='darkgrid')\n\n# # Increase the plot size and font size.\n# sns.set(font_scale=1.5)\n# plt.rcParams[\"figure.figsize\"] = (12,6)\n\n# # Plot the learning curve.\n# plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n# plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# # Label the plot.\n# plt.title(\"Training & Validation Loss\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Loss\")\n# plt.legend()\n# plt.xticks([1, 2, 3, 4, 5, 6, 7])\n\n# plt.show()","97efb7f5":"# Cleaning text\ntest['text_cleaned'] = list(map(lambda x:tweet_cleaner(x),test['text']) )\n\n# Get the lists of sentences and their labels\nsentences = test.text_cleaned.values\n\n# input_ids = torch.from_numpy(input_ids)\n# attention_masks = torch.tensor(attention_masks)\n# labels = torch.tensor(labels)\n\ntest_input_ids = tokenize_inputs(sentences, tokenizer, num_embeddings=120)\ntest_attention_masks = create_attn_masks(test_input_ids)\n\ntest[\"features\"] = test_input_ids.tolist()\ntest[\"masks\"] = test_attention_masks","7a75c3bd":"def generate_predictions(model, df, device=\"cpu\", batch_size=16):\n  num_iter = math.ceil(df.shape[0]\/batch_size)\n  \n  pred_probs = []\n\n  model.to(device)\n  model.eval()\n  \n  for i in range(num_iter):\n    df_subset = df.iloc[i*batch_size:(i+1)*batch_size,:]\n    X = df_subset[\"features\"].values.tolist()\n    masks = df_subset[\"masks\"].values.tolist()\n    X = torch.tensor(X)\n    masks = torch.tensor(masks, dtype=torch.long)\n    X = X.to(device)\n    masks = masks.to(device)\n    with torch.no_grad():\n      logits = model(input_ids=X, attention_mask=masks)\n      logits = logits.sigmoid().detach().cpu().numpy()\n#       pred_probs = np.vstack([pred_probs, logits])\n      pred_probs.extend(logits.tolist())\n        \n  return pred_probs","21c181ce":"pred_probs = generate_predictions(model, test, device=\"cuda\", batch_size=16)\n# pred_probs\nimport statistics\nstatistics.mean(pred_probs)","8d6f85b8":"test['target'] = pred_probs\ntest['target'] = np.array(test['target'] >= 0.5, dtype='int')\ntest[['id', 'target']].to_csv('submission.csv', index=False)","3ba5a007":"# # Load XLNEtForSequenceClassification, the pretrained XLNet model with a single linear classification layer on top. \n# model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=len(labels.unique()))\n# model.cuda()\n\n# param_optimizer = list(model.named_parameters())\n# no_decay = ['bias', 'gamma', 'beta']\n# optimizer_grouped_parameters = [\n#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n#      'weight_decay_rate': 0.01},\n#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n#      'weight_decay_rate': 0.0}\n# ]\n\n# # This variable contains all of the hyperparemeter information our training loop needs\n# optimizer = AdamW(optimizer_grouped_parameters,\n#                      lr=2e-5,correct_bias=False)\n\n# # Function to calculate the accuracy of our predictions vs labels\n# def flat_accuracy(preds, labels):\n#     pred_flat = np.argmax(preds, axis=1).flatten()\n#     labels_flat = labels.flatten()\n#     return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\n# torch.cuda.empty_cache()\n\n# import time\n# import datetime\n\n# def format_time(elapsed):\n#     '''\n#     Takes a time in seconds and returns a string hh:mm:ss\n#     '''\n#     # Round to the nearest second.\n#     elapsed_rounded = int(round((elapsed)))\n    \n#     # Format as hh:mm:ss\n#     return str(datetime.timedelta(seconds=elapsed_rounded))\n\n# # Store our loss and accuracy for plotting\n# train_loss_set = []\n\n# # Number of training epochs (authors recommend between 2 and 4)\n# num_epochs = 4\n# start_epoch = 0\n\n# # We'll store a number of quantities such as training and validation loss, \n# # validation accuracy, and timings.\n# training_stats = []\n# # Measure the total training time for the whole run.\n# total_t0 = time.time()\n\n\n# # trange is a tqdm wrapper around the normal python range\n# for i in trange(num_epochs, desc=\"Epoch\"):\n#     # if continue training from saved model\n#     actual_epoch = start_epoch + i\n\n#     # Training\n#     print(\"\")\n#     print('======== Epoch {:} \/ {:} ========'.format(actual_epoch, num_epochs))\n#     print('Training...')\n#     # Measure how long the training epoch takes.\n#     t0 = time.time()\n#     # Reset the total loss for this epoch.\n#     total_train_loss = 0  \n#     # ========================================\n#     #               Training\n#     # ========================================\n    \n#     # Perform one full pass over the training set.  \n#     # Set our model to training mode (as opposed to evaluation mode)\n#     model.train()\n  \n#     # Tracking variables\n# #     total_train_loss = 0\n# #     nb_tr_examples, nb_tr_steps = 0, 0\n    \n#     # Train the data for one epoch\n#     for step, batch in enumerate(train_dataloader):\n#         # Train the data for one epoch\n#         # Progress update every 40 batches.\n#         if step % 40 == 0 and not step == 0:\n#             # Calculate elapsed time in minutes.\n#             elapsed = format_time(time.time() - t0)\n#             # Report progress.\n#             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n    \n#         # Add batch to GPU\n#         batch = tuple(t.to(device) for t in batch)\n#         # Unpack the inputs from our dataloader\n#         b_input_ids, b_input_mask, b_labels = batch\n#         # Clear out the gradients (by default they accumulate)\n#         optimizer.zero_grad()\n#         # Forward pass\n#         loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n#         total_train_loss += loss.item()   \n#         # Backward pass\n#         loss.backward()\n#         # Update parameters and take a step using the computed gradient\n#         optimizer.step()\n    \n  \n#     # Calculate the average loss over all of the batches.\n#     avg_train_loss = total_train_loss \/ len(train_dataloader)            \n    \n#     # Measure how long this epoch took.\n#     training_time = format_time(time.time() - t0)\n\n#     print(\"\")\n#     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n#     print(\"  Training epcoh took: {:}\".format(training_time))\n    \n#     # ========================================\n#     #               Validation\n#     # ========================================\n#     # After the completion of each training epoch, measure our performance on\n#     # our validation set.\n\n#     print(\"\")\n#     print(\"Running Validation...\")\n#     # Put model in evaluation mode to evaluate loss on the validation set\n#     model.eval()\n\n#     # Tracking variables \n#     total_eval_accuracy = 0\n#     total_eval_loss = 0\n# #     nb_eval_steps = 0\n\n#     # Evaluate data for one epoch\n#     for batch in validation_dataloader:\n#         # Add batch to GPU\n#         batch = tuple(t.to(device) for t in batch)\n#         # Unpack the inputs from our dataloader\n#         b_input_ids, b_input_mask, b_labels = batch\n#         # Telling the model not to compute or store gradients, saving memory and speeding up validation\n#         with torch.no_grad():\n#             # Forward pass, calculate logit predictions\n#             loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask ,labels=b_labels)\n#             # Accumulate the validation loss.\n#             total_eval_loss += loss.item()\n            \n#     # Move logits and labels to CPU\n#     logits = logits.detach().cpu().numpy()\n#     label_ids = b_labels.to('cpu').numpy()\n    \n#     # Calculate the accuracy for this batch of test sentences, and\n#     # accumulate it over all batches.\n#     total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n#     # Report the final accuracy for this validation run.\n#     avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n#     print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n#     # Calculate the average loss over all of the batches.\n#     avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n#     # Measure how long the validation run took.\n#     validation_time = format_time(time.time() - t0)\n    \n#     print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n#     print(\"  Validation took: {:}\".format(validation_time))\n    \n#     # Record all statistics from this epoch.\n#     training_stats.append(\n#         {\n#             'epoch': actual_epoch,\n#             'Training Loss': avg_train_loss,\n#             'Valid. Loss': avg_val_loss,\n#             'Valid. Accur.': avg_val_accuracy,\n#             'Training Time': training_time,\n#             'Validation Time': validation_time\n#         }\n#     )\n    \n# print(\"\\n\")\n# print(\"\")\n# print(\"Training complete!\")\n\n# print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n\n# import os\n\n# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n\n# output_dir = '\/kaggle\/working\/xlnet_base_disaster_tweet_classification'\n\n# # Create output directory if needed\n# if not os.path.exists(output_dir):\n#     os.makedirs(output_dir)\n\n# print(\"Saving model to %s\" % output_dir)\n\n# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# # They can then be reloaded using `from_pretrained()`\n# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed\/parallel training\n# model_to_save.save_pretrained(output_dir)\n# tokenizer.save_pretrained(output_dir)\n\n# # Good practice: save your training arguments together with the trained model\n# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n\n# # Load a trained model and vocabulary that you have fine-tuned\n# model = model_class.from_pretrained(output_dir)\n# tokenizer = tokenizer_class.from_pretrained(output_dir)\n\n# # Copy the model to the GPU.\n# model.to(device)\n\n# model = XLNetForSequenceClassification.from_pretrained(output_dir,num_labels=2)\n# tokenizer = tokenizer.from_pretrained(output_dir)","fb7e39ab":"## 7. Appenddix","4cf0fb52":"Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.","8923c9d5":"## 4.1 XLNetForMultiLabelSequenceClassification","f89f00c4":"# 2. Loading Data","bb55a3c2":"# # XLNet Fine-Tuning with PyTorch\n\nBy Jaskaran Singh","fbb696a9":"Predicting Disaster no Disaster on the test data for submission\n","e52b1b12":"We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory.","8227061a":"## 2.1 Load Data","9f7c0403":"## 3.2 Tokenize Dataset","2d2fe86a":"Although we can have variable length input sentences, XLNet does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n\nTo \u201cpad\u201d our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n\nIf a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n\nWe pad and truncate our sequences so that they all become of length maxlen (\u201cpost\u201d indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we\u2019re borrowing from Keras. It simply handles the truncating and padding of Python lists.","d0e27246":"Now that our input data is properly formatted, it\u2019s time to fine tune the XLNet model.\n\nFor this task, we first want to modify the pre-trained model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained model, each has different top layers and output types designed to accomodate their specific NLP task.\n\nWe\u2019ll load **XLNetForSequenceClassification**. This is the normal XLNet model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained XLNet model and the additional untrained classification layer is trained on our specific task.","7cc7a78a":"## 3.1 XLNet Tokenizer","8de2d053":"The two properties we actually care about are the the `tweets(text)` and its `target`, which is referred to as the \"Real Disater or not\" (0=Not Real, 1=Real).","2c518e46":"# 4. Train Our Classification Model","bcb776c6":"## 3.4. Training & Validation Split","d3c6e52d":"# 3. Tokenization & Input Formatting\n\nIn this section, we'll transform our dataset into the format that BERT can be trained on.","ac5478ad":"# Introduction","6d6e367b":"Because the pre-trained model layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it\u2019s as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n\nOK, let\u2019s load XLNet! There are a few different pre-trained XLNet models available. \u201cxlnet-base-cased\u201d means the version that has both upper and lowercase letters (\u201ccased\u201d) and is the smaller version of the two (\u201cbase\u201d vs \u201clarge\u201d).","28576d43":"In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. ","29a84bce":"Below is our training loop. There\u2019s a lot going on, but fundamentally for each pass in our loop we have a training phase and a validation phase. At each pass we need to:\n\n**Training loop:**\n\n* Tell the model to compute gradients by setting the model in train mode\n* Unpack our data inputs and labels\n* Load data onto the GPU for acceleration\n* Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n* Forward pass (feed input data through the network)\n* Backward pass (backpropagation)\n* Tell the network to update parameters with optimizer.step()\n* Track variables for monitoring progress\n\n\n**Evalution loop:**\n\n* Tell the model not to compute gradients by setting th emodel in evaluation mode\n* Unpack our data inputs and labels\n* Load data onto the GPU for acceleration\n* Forward pass (feed input data through the network)\n* Compute loss on our validation data and track variables for monitoring progress","07525212":"Divide up our training set to use 80% for training and 20% for validation.","df71a826":"## 4.2 Training Evaluation","77e4263d":"## 1.2. Installing the Hugging Face Library","24c2e87e":"## 1.1. Using GPU for Training","a73e61e6":"XLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n\n* **input ids**: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n* **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n* **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens\n* **labels**: a single value of 1 or 0. In our task 1 means \u201cDisaster\u201d and 0 means \u201cNot Disaster\u201d","080a2a2d":"## 6. Get Predictions","e6817ae0":"Let's extract the sentences and labels of our training set as numpy ndarrays.","b226f5f7":"Let's apply the tokenizer to one sentence just to see the output.","b22cf197":"\nTo feed our text to XLNet, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n\nThe tokenization must be performed by the tokenizer included with XLNet--the below cell will download this for us. We'll be using the \"uncased\" version here.\n","f9ee7393":"\n## Intention\n\nTransfer learning models like XLNet, OpenAI's Open-GPT, and Google's BERT have broken multiple benchmarks with minimal task-specific fine-tuning. I will be attempting differnet transfer learning models to see which one performs better in a series of notebooks. In this notebook I will be showing you how we can use XLNet for tweet classification\n","eb61a5ea":"## 2.2 Pre-processing Tweets","352e8d6e":"Clearly optimim no. of epochs is around 2","9da73bc4":"# 1. Setup","de0aed21":"\nNext, let's install the [transformers](https:\/\/github.com\/huggingface\/transformers) package from Hugging Face which will give us a pytorch interface for working with XLNet. This package strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of unnecessary details).","94105d98":"In this notebook I'll use XLNet with the huggingface PyTorch library to fine-tune a model to get near state of the art performance in Tweet Disaster Classification.\n\nIn order the build this notebook references were taken from a blog post [here](https:\/\/towardsdatascience.com\/multi-label-text-classification-with-xlnet-b5f5755302df) and a Colab Notebook [here](https:\/\/colab.research.google.com\/drive\/1o3cv-YSPGiKftCvFnCiMcygARqdaxrM7). "}}