{"cell_type":{"4cdef5ab":"code","41afdfd0":"code","9f36df72":"code","736390ba":"code","62404f0a":"code","cb6b821a":"code","88d7d55c":"code","8e4c2ae2":"code","0541979b":"code","3533ea79":"code","95a02ffb":"code","d70a9fec":"code","7c84c90c":"code","eef4c802":"code","64eb7cee":"code","3f3e206c":"code","6f971402":"code","e00828cc":"code","f6ecfeaf":"code","36d871cf":"code","b8e71ef9":"code","ec9f3b17":"code","f5cf83b1":"code","2fa8bd8c":"code","92d7cbbe":"code","a892c79f":"code","9ff03911":"code","37513091":"code","fc41f15e":"code","6d917ebf":"code","0e52f59e":"code","3d158f37":"code","cd7feea3":"markdown","dfc2452b":"markdown","569fcbf7":"markdown","81218254":"markdown","24ea8b27":"markdown","59d15747":"markdown","bc7bd287":"markdown","23f962f7":"markdown","78445976":"markdown","25016cdc":"markdown","081323fd":"markdown","552ddf30":"markdown","b744f27c":"markdown","df966602":"markdown","9b5210ab":"markdown","ef72258c":"markdown","9a1aee02":"markdown","3efaa147":"markdown","ba516702":"markdown","9d4a312d":"markdown","96c3ade1":"markdown","004a1baf":"markdown","e6a1e34c":"markdown","535f0edb":"markdown","608c1b07":"markdown","4347bef5":"markdown","c62be29f":"markdown","135ee54e":"markdown"},"source":{"4cdef5ab":"from scipy.io import loadmat\nimport numpy as np\nimport pandas as pd\nimport csv\nimport pickle\nimport os\nimport seaborn as sns\nfrom scipy.signal import butter, filtfilt\nfrom scipy.stats import zscore\nimport scipy.io\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport io\nimport community as community_louvain\nimport matplotlib.cm as cm\nfrom collections import defaultdict\n","41afdfd0":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print('*** ',dirname)\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        print(filename)","9f36df72":"data_root = '\/kaggle\/input\/eeg-data-for-mental-attention-state-detection\/EEG Data\/'\nfiles = os.listdir(data_root)\nlen(files)","736390ba":"\n\ndef bp_filter(data, f_lo, f_hi, fs):\n    \"\"\" Digital band pass filter (6-th order Butterworth)\n    Args:\n        data: numpy.array, time along axis 0\n        (f_lo, f_hi): frequency band to extract [Hz]\n        fs: sampling frequency [Hz]\n    Returns:\n        data_filt: band-pass filtered data, same shape as data \"\"\"\n    data_filt = np.zeros_like(data)\n    f_ny = fs \/ 2.  # Nyquist frequency\n    b_lo = f_lo \/ f_ny  # normalized frequency [0..1]\n    b_hi = f_hi \/ f_ny  # normalized frequency [0..1]\n    # band-pass filter parameters\n    p_lp = {\"N\":6, \"Wn\":b_hi, \"btype\":\"lowpass\", \"analog\":False, \"output\":\"ba\"}\n    p_hp = {\"N\":6, \"Wn\":b_lo, \"btype\":\"highpass\", \"analog\":False, \"output\":\"ba\"}\n    bp_b1, bp_a1 = butter(**p_lp)\n    bp_b2, bp_a2 = butter(**p_hp)\n    data_filt = filtfilt(bp_b1, bp_a1, data, axis=0)\n    data_filt = filtfilt(bp_b2, bp_a2, data_filt, axis=0)\n    return data_filt\n\n\n#  Function to read in the EEG data and extract the valid lead data, low and high pass filter and z-transform the data.\n#  Returns a dataframe.\ndef get_EEG_data(data_root, filename):\n    # Extract the data from one of these files.\n    hz = 128\n    #filename = 'eeg_record30.mat'\n    mat = scipy.io.loadmat(data_root + filename)\n    data = pd.DataFrame.from_dict(mat[\"o\"][\"data\"][0,0])\n\n    # Limit the data to the 7 valid EEG leads.\n    dat = data.filter(list(range(3, 17)))\n    dat.columns = list(range(1, 15))\n    dat = dat.filter([1,2, 3, 4,5,6, 7, 8, 9,10,11,12,13,14,17], axis=1)\n    labels = ['AF3','F7', 'F3','FC5','T7','P7','O1', 'O2','P8','T8', 'FC6','F4','F8','AF4']  # FP2 should really be AF4\n    dat.columns = labels\n\n\n    # Filter the data, high pass 2 Hz, low pass 40 Hz.\n    lo, hi = 2, 40\n    # Do the filtering.\n    datf = bp_filter(dat.to_numpy(), lo, hi, hz)\n\n    # Convert back to a dataframe.\n    dat = pd.DataFrame({c: datf[:, i] for i,c in enumerate(labels)})\n\n    # Z-transform each column\n    #dat = dat.apply(zscore)\n    \n    return dat\n\n\n","62404f0a":"# For each file, print # minutes of data\nfor filename in files:\n    dat = get_EEG_data(data_root, filename)\n    # Compute microstates for this data\n    eegdat = dat.to_numpy()\n    print(filename, ' --->', np.round(len(eegdat)\/128\/60, 3))","cb6b821a":"os.mkdir('\/kaggle\/working\/thre08\/')\nos.mkdir('\/kaggle\/working\/thre08\/EEGSignalPlot\/')\n","88d7d55c":"d=\".\/thre08\/EEGSignalPlot\/\"\ndef plotleads(dat, leads, start, seconds, hz,state):\n    for i, lead in enumerate(leads):   \n        #print('Lead ' + lead)\n        a=d+'Lead_'+str(lead)+'_'+str(state)+\".png\"\n        plt.figure(figsize=(16,10))\n        ax = plt.gca()\n        k=(start+seconds*hz)\n        print(str(start) + ':'+ str(k))\n        plt.plot(dat.loc[start: k, lead])   \n        plt.title('Lead ' + lead + state)\n        plt.savefig(a)\n        plt.show()\n        _ = ax.axis('off')","8e4c2ae2":"EEG_file_number = 3    # can be 1..34\nfilename = 'eeg_record' + str(EEG_file_number) + '.mat'\n#dat = get_EEG_data(data_root, filename)\ndat = get_EEG_data(data_root, files[EEG_file_number])\nprint(type(dat))\n\ndat","0541979b":"labels = ['F7', 'F3', 'P7', 'O1', 'O2', 'P8', 'AF4']\nstart = 5*60*128\nseconds = 5\nhz = 128\nstate=\"focused\"\nplotleads(dat, labels, start, seconds, hz,state)","3533ea79":"labels = ['F7', 'F3', 'P7', 'O1', 'O2', 'P8', 'AF4']\nstart =15* 60*128\nseconds = 5\nhz = 128\nstate=\"unfocused\"\nplotleads(dat, labels, start, seconds, hz,state)","95a02ffb":"labels = ['F7', 'F3', 'P7', 'O1', 'O2', 'P8', 'AF4']\nstart = 25*60*128\nseconds = 5\nhz = 128\nstate=\"drowsed\"\nplotleads(dat, labels, start, seconds, hz,state)","d70a9fec":"samp_freq = 128 \ntotal_subjects = 5\ntotal_trials=5\nmkpt1 = int(samp_freq*10*60) # first 10 min for \"focused\" state\nmkpt2 = int(samp_freq*20*60) #second 10 min for \"unfocused\" state\nmkpt3= int(mkpt2+60940) #min of all trials","7c84c90c":"subject_map = {}\nfor s in range(1, total_subjects+1):\n    a =  int(7*(s-1)) + 3\n    if s!=5: \n        b = a + 5\n    else:\n        b = a + 4 #subject5 has 4 trials\n    subject_map[s] = [i for i in range(a, b)]\nprint(subject_map)","eef4c802":"channels = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\nuseful_channels = ['F7','F3','P7','O1','O2','P8','AF4']\nuse_channel_inds = []\nfor c in useful_channels:\n    if c in channels:\n        use_channel_inds.append(channels.index(c))\nuse_channel_inds","64eb7cee":"\n\nfile = scipy.io.loadmat('..\/input\/eeg-data-for-mental-attention-state-detection\/EEG Data\/eeg_record1.mat')# change filename accordingly\nprint(file.keys()) # file is a nested dictionary, data is in 'o' key\ndata = pd.DataFrame.from_dict(file[\"o\"][\"data\"][0,0])\nprint(data)","3f3e206c":"inp_dir = '..\/input\/eeg-data-for-mental-attention-state-detection\/EEG Data\/'","6f971402":"for s in range(1, total_subjects+1):\n    data = {}\n    data['channels'] = useful_channels\n    data['samp_freq'] = samp_freq\n    for i, t in enumerate(subject_map[s]):\n        trial = {}\n        trial_data = loadmat(inp_dir + f'eeg_record{t}.mat')\n        eeg = trial_data['o']['data'][0][0][:, 3:17]\n        eeg = eeg[:, use_channel_inds]\n        trial['focussed'] = eeg[:mkpt1]\n        trial['unfocussed'] = eeg[mkpt1:mkpt2]\n        trial['drowsed'] = eeg[mkpt2:mkpt3]\n        data[f'trial_{i+1}'] = trial\n    with open(f'subject_{s}.pkl', 'wb') as f: \n        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)","e00828cc":"\nos.mkdir('\/kaggle\/working\/thre08\/heatmap\/')\nos.mkdir('\/kaggle\/working\/thre08\/heatmap\/focussed\/')\nos.mkdir('\/kaggle\/working\/thre08\/heatmap\/unfocussed\/')\nos.mkdir('\/kaggle\/working\/thre08\/heatmap\/drowsed\/')\n\nos.mkdir('\/kaggle\/working\/thre08\/adjmat\/')\nos.mkdir('\/kaggle\/working\/thre08\/adjmat\/focussed\/')\nos.mkdir('\/kaggle\/working\/thre08\/adjmat\/unfocussed\/')\nos.mkdir('\/kaggle\/working\/thre08\/adjmat\/drowsed\/')","f6ecfeaf":"with open('subject_1.pkl', 'rb') as f: \n    data = pickle.load(f)\ndata","36d871cf":"hm=\".\/thre08\/heatmap\/\"\nam=\".\/thre08\/adjmat\/\"\nfor i in range(1,total_subjects+1):\n    s='subject_' + str(i) +'.pkl'\n    with open(s,'rb') as f:\n        data = pickle.load(f)\n    trials=total_trials\n    if i==5:\n        trials=trials-1\n    for j in range(1,trials+1):\n        t='trial_' + str(j)\n        df1=data[t]['focussed']\n        df2=data[t]['unfocussed']\n        df3=data[t]['drowsed']\n        arr11 = np.array(df1)\n        \n        arr12 = np.array(df2)\n        arr13 = np.array(df3)\n        dataset1 = pd.DataFrame({'F7': arr11[:, 0],'F3': arr11[:, 1],'P7': arr11[:, 2],'O1': arr11[:, 3],'O2': arr11[:, 4],'P8': arr11[:, 5],'AF4': arr11[:, 6]})\n#         print(dataset1)\n        dataset2 = pd.DataFrame({'F7': arr12[:, 0],'F3': arr12[:, 1],'P7': arr12[:, 2],'O1': arr12[:, 3],'O2': arr12[:, 4],'P8': arr12[:, 5],'AF4': arr12[:, 6]})\n#         print(dataset2)\n        dataset3 = pd.DataFrame({'F7': arr13[:, 0],'F3': arr13[:, 1],'P7': arr13[:, 2],'O1': arr13[:, 3],'O2': arr13[:, 4],'P8': arr13[:, 5],'AF4': arr13[:, 6]})\n#         print(dataset3)\n#         df_row = pd.concat([dataset1, dataset2,dataset3],ignore_index=True)\n#         print(df_row)\n        matrix1 = dataset1.corr()\n    \n        matrix2 = dataset2.corr()\n        matrix3 = dataset3.corr()\n        print(\"Correlation Matrix for subject \",i,\" trial \",j,\" state 'Focused':\")\n        for m in useful_channels:\n            for n in useful_channels:\n                if matrix1[m][n]<0.8:\n                    matrix1[m][n]=0\n#                 else:\n#                     matrix1[m][n]=1\n        print(matrix1)\n        print(\"Correlation Matrix for subject \",i,\" trial \",j,\" state 'Unfocused':\")\n        for m in useful_channels:\n            for n in useful_channels:\n                if matrix2[m][n]<0.8:\n                    matrix2[m][n]=0\n#                 else:\n#                     matrix2[m][n]=1\n        print(matrix2)\n        print(\"Correlation Matrix for subject \",i,\" trial \",j,\" state 'Drowsy':\")\n        for m in useful_channels:\n            for n in useful_channels:\n                if matrix3[m][n]<0.8:\n                    matrix3[m][n]=0\n#                 else:\n#                     matrix3[m][n]=1\n        print(matrix3)\n#         print(matrix)\n#         print(type(matrix))\n        \n        print(\"\")\n        print(\"----------------------------------------------------------------------------------\")\n        print(\"\")\n        hmf=hm+\"s_\"+  str(i) + \"_t\" + str(j)+\"_focused\"+\".png\"\n        hmu=hm+\"s_\"+  str(i) + \"_t\" + str(j)+\"_unfocused\"+\".png\"\n        hmd=hm+\"s_\"+  str(i) + \"_t\" + str(j)+\"_drowsed\"+\".png\"\n        \n        plt.figure(figsize=(8,8))\n        ax = plt.gca()\n        ax.set_title(hmf)\n        p1 = sns.heatmap(matrix1,cmap='PuOr')\n        plt.savefig(hmf)\n        _ = ax.axis('off')\n        \n        \n        plt.figure(figsize=(8,8))\n        ax = plt.gca()\n        ax.set_title(hmu)\n        p1 = sns.heatmap(matrix2,cmap='PuOr')\n        plt.savefig(hmu)\n        _ = ax.axis('off')\n        \n        \n        plt.figure(figsize=(8,8))\n        ax = plt.gca()\n        ax.set_title(hmd)\n        p1 = sns.heatmap(matrix3,cmap='PuOr')\n        plt.savefig(hmd)\n        _ = ax.axis('off')\n        \n        \n        \n        amf=am+\"s_\" + str(i) + \"_t\" + str(j)+\"_focused\"+\".csv\"\n        amu=am+\"s_\" + str(i) + \"_t\" + str(j)+\"_unfocused\"+\".csv\"\n        amd=am+\"s_\" + str(i) + \"_t\" + str(j)+\"_drowsed\"+\".csv\"\n        \n        matrix1.to_csv(amf,index=True)\n        matrix2.to_csv(amu,index=True)\n        matrix3.to_csv(amd,index=True)\n        ","b8e71ef9":"def characteristic_path_length(G):\n  return (nx.average_shortest_path_length(G))","ec9f3b17":"def assortativity_coefficient(G):\n  r = nx.degree_assortativity_coefficient(G)\n  return(f\"{r:3.1f}\")","f5cf83b1":"def modularity_(G):\n  partition = community_louvain.best_partition(G)\n  return community_louvain.modularity(partition, G)","2fa8bd8c":"def link_density(G):\n  return (nx.density(G))","92d7cbbe":"def network_transitivity(G):\n  return (nx.transitivity(G))","a892c79f":"def node_degree(G):\n  node=nx.nodes(G)\n  sum=0\n  for n in node:\n    sum=sum+G.degree(n)\n  length=len(node)\n  return (sum\/length)","9ff03911":"def betweenness_centrality_(G):\n  M=nx.betweenness_centrality(G, k=None, normalized=True, weight=None, endpoints=False, seed=None)\n  s=0\n  node=nx.nodes(G)\n  length=len(node)\n  for n in node:\n    s=s+M[n]\n  return s\/length","37513091":"def closeness_centrality_(G):\n  M=nx.closeness_centrality(G, u=None, distance=None, wf_improved=True)\n  s=0\n  node=nx.nodes(G)\n  length=len(node)\n  for n in node:\n    s=s+M[n]\n  return s\/length","fc41f15e":"from collections import defaultdict\ndef participation_coefficient_util(G, module_partition):\n    '''\n    Computes the participation coefficient of nodes of G with partition\n    defined by module_partition.\n    (Guimera et al. 2005).\n\n    Parameters\n    ----------\n    G : :class:`networkx.Graph`\n    module_partition : dict\n        a dictionary mapping each community name to a list of nodes in G\n\n    Returns\n    -------\n    dict\n        a dictionary mapping the nodes of G to their participation coefficient\n        under the participation specified by module_partition.\n    '''\n    # Initialise dictionary for the participation coefficients\n    pc_dict = {}\n     # printing original dictionary\n    #print(\"The original dictionary : \" +  str(module_partition))\n \n# Using sorted() + items() + defaultdict()\n# Grouping dictionary keys by value\n    res = defaultdict(list)\n    for key, val in sorted(module_partition.items()):\n      res[val].append(key)\n     \n# printing result\n    #print(\"Grouped dictionary is : \" + str(dict(res)))\n    #print(dict(res))\n    #print(\"\")\n\n    # Loop over modules to calculate participation coefficient for each node\n    for m in res.keys():\n        # Create module subgraph\n        #print(m)\n        #print(res[m])\n        M = set(res[m])\n        #print(M)\n        for v in M:\n            # Calculate the degree of v in G\n          degree = float(nx.degree(G=G, nbunch=v))\n\n            # Calculate the number of intramodule degree of v\n          count=0\n          for u in M:\n            if (u,v) in G.edges():\n              count=count+1\n          wm_degree = float(count)\n\n            # The participation coeficient is 1 - the square of\n            # the ratio of the within module degree and the total degree\n          pc_dict[v] = 1 - ((float(wm_degree) \/ float(degree))**2)\n\n    return pc_dict\n\ndef participation_coefficient(G):\n  partition = community_louvain.best_partition(G)\n  pc_dictt=participation_coefficient_util(G,partition)\n  pc=0\n  node=nx.nodes(G)\n  length=len(node)\n  for x in pc_dictt:\n    pc=pc+pc_dictt[x]\n  return pc\/length","6d917ebf":"from collections import defaultdict\n\ndef z_score_util(G, module_partition):\n    '''\n    Calculate the z-score of the nodes of G under partition module_partition.\n\n    Parameters\n    ----------\n    G : :class:`networkx.Graph`\n    module_partition : dict\n        a dictionary mapping each community name to a lists of nodes in G\n\n    Returns\n    -------\n    dict\n        a dictionary mapping the nodes of G to their z-score under\n        module_partition.\n    '''\n    # Initialise dictionary for the z-scores\n    z_scores = {}\n    # printing original dictionary\n    #print(\"The original dictionary : \" +  str(module_partition))\n \n# Using sorted() + items() + defaultdict()\n# Grouping dictionary keys by value\n    res = defaultdict(list)\n    for key, val in sorted(module_partition.items()):\n      res[val].append(key)\n     \n# printing result\n    #print(\"Grouped dictionary is : \" + str(dict(res)))\n    #print(dict(res))\n    #print(\"\")\n    # Loop over modules to calculate z-score for each node\n    for m in res.keys():\n        # Create module subgraph\n        M = G.subgraph(set(res[m]))\n     #   print(M.nodes())\n      #  print(M.edges())\n       # print(M.degree('A'))\n        # Calculate relevant module statistics\n        M_degrees = list(dict(M.degree()).values())\n\n        #print(\"M_degrees: \"+str(M_degrees))\n        M_degree = np.mean(M_degrees)\n        #print(\"M_degree: \"+str(M_degree))\n        M_std = np.std(M_degrees)\n       # print(\"M_std: \"+str(M_std))\n        #print(\"\")\n        for v in M.nodes:\n            # Calculate the number of intramodule edges\n            wm_edges = float(nx.degree(G=M, nbunch=v))\n         #   print(\"v:\"+str(v))\n          #  print(\"wm_edges:\"+str(wm_edges))\n           # print(\"\")\n            # Calculate z score as the intramodule degree of v\n            # minus the mean intramodule degree, all divided by\n            # the standard deviation of intramodule degree\n            if M_std != 0:\n                zs = (wm_edges - M_degree)\/M_std\n            else:\n                # If M_std is 0, then all M_degrees must be equal.\n                # It follows that the intramodule degree of v must equal\n                # the mean intramodule degree.\n                # It is therefore valid to assign a 0 value to the z-score\n                zs = 0\n            #print(\"v:\"+str(v)+\"   zs\"+str(zs))\n            z_scores[v] = zs\n            #print(\"-------------------------------------------\")\n        #print(\"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\n\n    return z_scores\n\ndef z_score(G):\n  partition = community_louvain.best_partition(G)\n  zscore_dictt=z_score_util(G,partition)\n  zscore=0\n  node=nx.nodes(G)\n  length=len(node)\n  for x in zscore_dictt:\n    zscore=zscore+zscore_dictt[x]\n  return zscore\/length","0e52f59e":"os.mkdir('\/kaggle\/working\/thre08\/graph\/')","3d158f37":"my_dict = {\"characteristic_path_length\":[],\"assortativity_coefficient\":[],\"modularity_\":[],\n           \"link_density\":[],\"network_transitivity\":[],\"node_degree\":[],\"betweenness_centrality_\":[],\"closeness_centrality_\":[],\n           \"participation_coefficient\":[],\"z_score\":[]};\nam=\".\/thre08\/adjmat\/\"\ngm=\".\/thre08\/graph\/\"\nfor i in range(1,total_subjects+1):\n    trials=total_trials\n    if i==5:\n        trials=trials-1\n    for j in range(1,trials+1):\n        amf=am+\"s_\" + str(i) + \"_t\" + str(j)+\"_focused\"+\".csv\"\n        print(\"For adjancy matrix : \",amf)\n        mat1 = pd.read_csv(amf,index_col=0)\n#         print(mat)\n        plt.figure(figsize=(5,5))\n        ax = plt.gca()\n        s = \"Graph \" + amf\n        ax.set_title(s)\n        G = nx.from_pandas_adjacency(mat1)\n        nx.draw_circular(G,with_labels=True, node_color='lightgreen', ax=ax)\n        fig=gm+\"s\"+str(i)+\"_t\"+str(j)+\"_focused\"+\".png\"\n        plt.savefig(fig)\n        _ = ax.axis('off')\n        my_dict[\"characteristic_path_length\"].append(0) #Since the graphs will not fully connected hence give it as 0\n        my_dict[\"assortativity_coefficient\"].append(assortativity_coefficient(G))\n        my_dict[\"modularity_\"].append(modularity_(G))\n        my_dict[\"link_density\"].append(link_density(G))\n        my_dict[\"network_transitivity\"].append(network_transitivity(G))\n        my_dict[\"node_degree\"].append(node_degree(G))\n        my_dict[\"betweenness_centrality_\"].append(betweenness_centrality_(G))\n        my_dict[\"closeness_centrality_\"].append(closeness_centrality_(G))\n        my_dict[\"participation_coefficient\"].append(participation_coefficient(G))\n        my_dict[\"z_score\"].append(z_score(G))\n        print(\"characteristic_path_length: \",0)\n        print(\"assortativity_coefficient: \",assortativity_coefficient(G))\n        print(\"modularity_: \",modularity_(G))\n        print(\"link_density: \",link_density(G))\n        print(\"network_transitivity: \",network_transitivity(G))\n        print(\"node_degree: \",node_degree(G))\n        print(\"betweenness_centrality_: \",betweenness_centrality_(G))\n        print(\"characteristic_path_length: \",closeness_centrality_(G))\n        print(\"participation_coefficient: \",participation_coefficient(G))\n        print(\"z_score: \",z_score(G))\n        print(\"---------------------------------------------------------------------\")\n        \n        \n        \n        amu=am+\"s_\" + str(i) + \"_t\" + str(j)+\"_unfocused\"+\".csv\"\n        print(\"For adjancy matrix : \",amu)\n        mat2 = pd.read_csv(amu,index_col=0)\n#         print(mat)\n        plt.figure(figsize=(5,5))\n        ax = plt.gca()\n        s = \"Graph \" + amu\n        ax.set_title(s)\n        G = nx.from_pandas_adjacency(mat2)\n        nx.draw_circular(G,with_labels=True, node_color='lightgreen', ax=ax)\n        fig=gm+\"s\"+str(i)+\"_t\"+str(j)+\"_unfocused\"+\".png\"\n        plt.savefig(fig)\n        _ = ax.axis('off')\n        my_dict[\"characteristic_path_length\"].append(0)\n        my_dict[\"assortativity_coefficient\"].append(assortativity_coefficient(G))\n        my_dict[\"modularity_\"].append(modularity_(G))\n        my_dict[\"link_density\"].append(link_density(G))\n        my_dict[\"network_transitivity\"].append(network_transitivity(G))\n        my_dict[\"node_degree\"].append(node_degree(G))\n        my_dict[\"betweenness_centrality_\"].append(betweenness_centrality_(G))\n        my_dict[\"closeness_centrality_\"].append(closeness_centrality_(G))\n        my_dict[\"participation_coefficient\"].append(participation_coefficient(G))\n        my_dict[\"z_score\"].append(z_score(G))\n        print(\"characteristic_path_length: \",0)\n        print(\"assortativity_coefficient: \",assortativity_coefficient(G))\n        print(\"modularity_: \",modularity_(G))\n        print(\"link_density: \",link_density(G))\n        print(\"network_transitivity: \",network_transitivity(G))\n        print(\"node_degree: \",node_degree(G))\n        print(\"betweenness_centrality_: \",betweenness_centrality_(G))\n        print(\"characteristic_path_length: \",closeness_centrality_(G))\n        print(\"participation_coefficient: \",participation_coefficient(G))\n        print(\"z_score: \",z_score(G))\n        print(\"---------------------------------------------------------------------\")\n        \n        \n        amd=am+\"s_\" + str(i) + \"_t\" + str(j)+\"_drowsed\"+\".csv\"\n        print(\"For adjancy matrix : \",amd)\n        mat3 = pd.read_csv(amd,index_col=0)\n#         print(mat)\n        plt.figure(figsize=(5,5))\n        ax = plt.gca()\n        s = \"Graph \" + amd\n        ax.set_title(s)\n        G = nx.from_pandas_adjacency(mat3)\n        nx.draw_circular(G,with_labels=True, node_color='lightgreen', ax=ax)\n        fig=gm+\"s\"+str(i)+\"_t\"+str(j)+\"_drowsed\"+\".png\"\n        plt.savefig(fig)\n        _ = ax.axis('off')\n        my_dict[\"characteristic_path_length\"].append(0)\n        my_dict[\"assortativity_coefficient\"].append(assortativity_coefficient(G))\n        my_dict[\"modularity_\"].append(modularity_(G))\n        my_dict[\"link_density\"].append(link_density(G))\n        my_dict[\"network_transitivity\"].append(network_transitivity(G))\n        my_dict[\"node_degree\"].append(node_degree(G))\n        my_dict[\"betweenness_centrality_\"].append(betweenness_centrality_(G))\n        my_dict[\"closeness_centrality_\"].append(closeness_centrality_(G))\n        my_dict[\"participation_coefficient\"].append(participation_coefficient(G))\n        my_dict[\"z_score\"].append(z_score(G))\n        print(\"characteristic_path_length: \",0)\n        print(\"assortativity_coefficient: \",assortativity_coefficient(G))\n        print(\"modularity_: \",modularity_(G))\n        print(\"link_density: \",link_density(G))\n        print(\"network_transitivity: \",network_transitivity(G))\n        print(\"node_degree: \",node_degree(G))\n        print(\"betweenness_centrality_: \",betweenness_centrality_(G))\n        print(\"characteristic_path_length: \",closeness_centrality_(G))\n        print(\"participation_coefficient: \",participation_coefficient(G))\n        print(\"z_score: \",z_score(G))\n        print(\"---------------------------------------------------------------------\")\n       \n        \nnew = pd.DataFrame.from_dict(my_dict)\nnew.to_csv(\"M2.csv\",index=True)\nnew\n        \n        \n        \n        \n        ","cd7feea3":"5. **Within Module Degree (Z-Score):**\n\n-->The Z-score represents the strength of connectivity of a\nnode in a module i.e. how well a node i is connected to the\nother nodes in same module. The Z-score of a node can be\ncalculated using eq.  as follows.\n<br>$Z_{i}=\\frac{K_{i}-K_{\\mu}}{\\Gamma_{K_{\\mu}}}$<br>\n\n\nHere, Ki\nis the degree of node i in module \u03bc, K\u03bc\nis the\naverage degree of all the nodes in module \u03bc and \u0393K\u03bc\nrepresents the standard deviation of K.","dfc2452b":"1. **Characteristic path length:**\n-->Integration is an important characteristic of network, represents the ability of network to become interconnected and exchange the information. This ability of the network is quantified using characteristic path length. The characteristic path length is calculated using eq. (1).\n**<br>$L=\\frac{1}{N}\\sum_{i=1}^N l_{i}$**\n\nwhere, \ud835\udc59\ud835\udc56 is the average shortest path length between node i and every other node, it is calculated using eq. (2)\n\n**<br>$l_{i}=\\frac{1}{N-1}\\sum_{{i}\\neq{j}} l_{ij}$**\n\nHere, \ud835\udc59\ud835\udc56\ud835\udc57 is path length from node i to node j, N is the number of nodes in the networ","569fcbf7":"2. **Betweenness Centrality:**\n-->This graph based measure represents the impact of particular region in information flow over the brain network. It counts that, how many times, particular node act as a bridge along the shortest path of two other nodes. The betweenness centrality for ith node can be calculated using equation (10).\n<br>$C_{b}(i)=\\frac{2}{(N-1)(N-2)}\\sum_{{j}\\neq{h}\\neq{i}} \\frac{n_{hj}(i)}{n_{hj}}$<br>\nHere, \ud835\udc5b\u210e\ud835\udc57(\ud835\udc56) is the number of shortest paths between node h and j that passes through node i.\n\ud835\udc5b\u210e\ud835\udc57 , represents the total number of shortest paths between node h and j.","81218254":"3. **Modularity:**\n-->It is the measure of segregation of the network. Modularity represents the measure of the ability of the network to get divided into modules. Where, module is nothing but the collection of nodes having denser internal connections and sparser external connections. The modularity of the network with nodes N is calculated using eq. (4).\n**<br>$Q=\\frac{1}{2N(N-1)}\\sum_{xy}(A_{xy}- \\frac{K_{x}K_{y}}{N(N-1)})\\delta(\\omega_{x}\\omega_{y})$**\n\nHere, \u03c9x and \u03c9y are the different modules containing node x and node y, respectively. Axy is connectivity matrix having value 1 if there is link between node x and y otherwise 0. Kx is the degree of node \u2032x\u2032 and Ky is the degree of node y.","24ea8b27":"# Importing all required libraries\n","59d15747":"2. **Assortativity coefficient:**\n-->This is an important measure also called as degree of correlation. It represents the tendency of a node with many connections in the network to connect to the other nodes with many connections and vice versa. The value of assortativity coefficient (r) lies between -1 to +1. Where, -1 means the network is disassortative and +1 means network is fully assortative. The assortative coefficient of the network can be calculated using eq (3).\n<br>$r=\\frac{l^{-1}\\sum_{({i,j})\\epsilon{L}}k_{i}k_{j}-[l^{-1}\\sum_{({i,j})\\epsilon{L}}\\frac{1}{2}(k_{i}+k_{j})]^2}       {l^{-1}\\sum_{({i,j})\\epsilon{L}}\\frac{1}{2}(k_{i}^2+k_{j}^2)-[l^{-1}\\sum_{({i,j})\\epsilon{L}}\\frac{1}{2}(k_{i}+k_{j})]^2}$\n\nwhere, ki is the degree of node i, L is the characteristic path length, l is the number of edges with degree greater than k.","bc7bd287":"## Special Thanks to <br>\n[Cigdem Inan ACI](https:\/\/www.kaggle.com\/inancigdem)<br>\n[Nitin Singh](https:\/\/www.kaggle.com\/nitinsss)<br>\n[mathprof](https:\/\/www.kaggle.com\/rafaelespericueta)<br>\n","23f962f7":"# Channels names:","78445976":"# Defining functions for extracting features from graph metrics","25016cdc":"# Saving the pickel object for each subject","081323fd":"<br>We have sampling frequency = 128Hz\n<br>1st 10 minutes of data(t=0 min to t=10 min) is for \"focussed\" state\n<br>2nd 10 minutes of data(t=10 min to t=20 min) is for \"unfocussed\" state\n<br>Remaining data(t=20 min to onwards) is for \"Drowsed\" state\n\n<br>Therefore,\n<br>\"focused\": row=$0$ to row=$128\\times10\\times60$\n<br>\"unfocused\": row=$128\\times10\\times60$ to row=$128\\times20\\times60$\n<br>\"drowsed\": row=$128\\times20\\times60$ to last row","552ddf30":"# **Feature Extraction and Comparison of EEG Based Brain Connectivity Networks using Graph Metrics**\n","b744f27c":"4. **Participation Coefficient:**\n-->Participation coefficient is an important measure\nrepresents the edge distribution of the node. The node having\nsame number of links to all the modules in the network will\nhave participation coefficient 1. The node having all the links\nwithin its own model will have participation coefficient\nequal to 0. The participation coefficient for node i from\nmodule \u03bc in the network having total C modules can be\ncalculated using eq.  -\n<br>$PC_{i}=1-\\sum_{\\mu=1}^C \\frac{K_{i,\\mu}}{K_{i}}$<br>\n\n\nHere, Ki,\u03bc represents the degree of ith node within module \u03bc.","df966602":"## 'Drowsed'","9b5210ab":"As mentioned in the research paper, there are 5 subjects and they have recorded data on 7 separate day (except for subject-5 who has done that for 6 days). Out of this 7 days or trials first 2 were for getting habitual with the process. Therefore we will consider only last five trials. \n<br>Therefore, we have:\n<br>\n<br>Subjects=5\n<br>Trials=5\n<br>(Note: subject 5 has only 4 trials)\n\nBelow cell is not really important. I didn't want to manually write the relevant file names for each subject. So, I did the following. It based on what the data contributor has mentioned in an comment. There are 5 subjects, and they have recorded data on 7 separate days (except for subject-5 who has only done that for 6 days). I have segregated each day's data as a trial. Out of these 7, first 2 were used for getting the subject to familiarise with the process. That is why I have only considered the last 5 trials of each subject as training data.","ef72258c":"3. **Closeness Centrality:**\n-->It is most important measure of centrality; it measures the closeness of a node with all the other nodes. The closer to all the other nodes will have higher closeness centrality. This measure represents indirect impact of brain region over other brain regions. It is calculated using eq (11).\n<br>$C_{c}(i)=\\frac{N-1}{\\sum_{{i}\\neq{j}}l_{ij}}$<br>\n\nHere, \ud835\udc59\ud835\udc56\ud835\udc57 is the shortest path length between node i and j. N is the total number of nodes.","9a1aee02":"## 'Unfocused'","3efaa147":"# Plotting EEG signals","ba516702":"1. **Node Degree (Average):**\n-->It is an important elementary measure of brain connectivity network. Degree of a node represents the number of links connecting that node to all the other nodes. It is used to show whether the node is important in the network or not, i.e. node having maximum degree is more important in the network. The node degree for node i can be calculated using eq. (9).\n            <br>$K_{i}=\\sum_{{i}\\neq{j}} A_{ij}$\n<br>Here, A is the adjacency matrix. \u2032\ud835\udc34\ud835\udc56\ud835\udc57\u2032will have value 1 if there is link between node i and j otherwise 0.","9d4a312d":"## a) Node independent graph measures (Network level measures):","96c3ade1":"5. **Network Transitivity:**\n-->It is the variant of classical clustering coefficient, reflects the connectivity of a specialized region to its neighbors. This measure represents the ability of the brain to segregate into independent, local neighborhoods. Network transitivity can be calculated using eq. (6).\n**<br>$T=\\frac{\\sum_{{i}\\epsilon{N}}2t_{i}}{\\sum_{{i}\\epsilon{N}}K_{i}(K_{i}-1)}$**\n\nWhere, \ud835\udc61\ud835\udc56 is the number of triangles around node i, \ud835\udc3e\ud835\udc56 is the degree of node i.","004a1baf":"4. **Link Density:**\n-->It is also called as connectivity density, represents the ability of the network to resist connection failure. It is defined as the ratio of number of edges present to the maximum possible number of edges can be present in the network. It can be calculated using eq. (5).\n**<br>$L=\\frac{2E}{N(N-1)}$**\n\nWhere, N is the number of nodes in the network, E is the number of edges\/links available in the network.","e6a1e34c":"### For working with  EEG data and finding similarity between electrode, first we will preprocess the EEG data to desired format (Csv adjancy matrix), then we will extract features and find links between electrode","535f0edb":"## 'Focused state'","608c1b07":"# Understanding data","4347bef5":"## a) Node dependent graph measures :","c62be29f":"# Loading the pickel objects and building adjancy matrix for each subject for each trial and saving to csv files","135ee54e":"## For Subject 1 trial 1 "}}