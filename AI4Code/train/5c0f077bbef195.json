{"cell_type":{"3baeaba9":"code","6d05a7d9":"code","bba769da":"code","83bbc664":"code","8368c73a":"code","38c8b71f":"code","4f458203":"code","6c4064fd":"code","aa2c7472":"code","cf81af47":"code","207ce366":"code","84ed98f2":"code","4f769cfe":"code","a642fab2":"code","317cdd89":"code","1abb02f2":"code","df995d6d":"code","77fe0abd":"code","54ca042c":"code","269a2dae":"code","3647a4e9":"code","de41a9ad":"code","01d2e2e5":"code","cece1ae5":"code","709cc066":"code","2013bc73":"code","f064cbc9":"code","2491359c":"code","c855b309":"code","ce0aea68":"code","53dcbf0a":"code","637c65e9":"code","a7015317":"code","09f63265":"code","1a2aee97":"code","f0a92b10":"code","d11b7de6":"code","995f00ff":"code","b94786ab":"code","3c172b31":"code","78132c7b":"code","9c53f21d":"code","5876ff3b":"code","c2ba8270":"code","4135d57d":"code","0d4a7559":"code","cbef6e1c":"code","b88b08f4":"code","716df2ab":"code","1e65df67":"code","23bbf008":"code","1869516f":"code","d6ec0bbc":"code","9cfcf755":"code","c3d49696":"code","a7860835":"code","4d2cc68d":"code","c4f5b7a4":"code","ed313e1c":"code","f5a218d0":"code","90d58b8d":"code","3173f5ed":"code","7ad9d00c":"code","fee697f6":"code","ee5b9580":"code","e4a14fc3":"code","7f87d179":"code","253a3712":"markdown","2b4f4bb9":"markdown","8db4f224":"markdown","12d27835":"markdown","38ce88e2":"markdown","86544a9a":"markdown","aa61edb2":"markdown","2fc705de":"markdown","af8a99e3":"markdown","17237a28":"markdown","1f825607":"markdown","e26f67bf":"markdown","f9abc79c":"markdown","cd9f53ac":"markdown","2bb24df5":"markdown","8537e44b":"markdown","5604e05e":"markdown","3d4424f9":"markdown","43a4f92b":"markdown","7d5f0b8f":"markdown","1044a7e2":"markdown","28e06704":"markdown","a6cea491":"markdown","70e69475":"markdown","966c9e69":"markdown","86d50152":"markdown","034efc02":"markdown","af90bcc9":"markdown","ec8332d2":"markdown","4857c60f":"markdown","f4f7c72e":"markdown","024fb0f9":"markdown","fc17559b":"markdown","bfb1f65e":"markdown","0f748a81":"markdown","e187f656":"markdown","3d6e410f":"markdown","d6dd5a27":"markdown","f1d2d589":"markdown","608fb7cb":"markdown","79fb75f1":"markdown","46e2ced6":"markdown","dffd0396":"markdown","47941ac1":"markdown","b6d9b074":"markdown","4a737950":"markdown","d307ad21":"markdown","ea77509a":"markdown","04842b8c":"markdown","24551161":"markdown","a92c65e5":"markdown","39b2a08b":"markdown","1278d4fb":"markdown","507671e4":"markdown","71dae0e4":"markdown","d34d7944":"markdown","02a37ea1":"markdown","389e9883":"markdown"},"source":{"3baeaba9":"!pip install plot_metric\n!pip install catboost\nfrom catboost import CatBoostClassifier\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\n\nimport seaborn as sns\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nimport xgboost as xgb\nfrom plot_metric.functions import BinaryClassification\n!pip install category_encoders\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import base\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,classification_report,confusion_matrix\nfrom IPython.display import Image\nfrom category_encoders import TargetEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6d05a7d9":"path = \"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\"\n\ndata = pd.read_csv(path)","bba769da":"data.head()","83bbc664":"data.info()","8368c73a":"data.describe()","38c8b71f":"data.drop('RISK_MM', axis=1 ,inplace=True)","4f458203":"data.RainTomorrow = [0 if each=='No' else 1 for each in data.RainTomorrow]","6c4064fd":"data['Date'] = pd.to_datetime(data['Date'])\ndata['Year'] = data['Date'].dt.year\ndata['Month'] = data['Date'].dt.month\ndata['Day'] = data['Date'].dt.day\ndata.drop('Date', axis=1 ,inplace=True)\ndata","aa2c7472":"# The Class Distribution\nfig, ax =plt.subplots(nrows=1,ncols=2, figsize=(10,4))\nlabels=['No', 'Yes']\nsns.countplot(x=data.RainTomorrow, data=data, palette=\"pastel\",ax=ax[0])\ndata['RainTomorrow'].value_counts().plot.pie(autopct=\"%1.2f%%\", ax=ax[1], colors=['#66b3ff','#ffcc99'], \n                                             labels=labels, explode = (0, 0.1), startangle=90)\nplt.show()","cf81af47":"rain_no = data[data['RainTomorrow']== 0]\nrain_yes = data[data['RainTomorrow']== 1]\n\nfig = go.Figure([go.Bar(x=['Rain-Yes', 'Rain-No'], y=[len(rain_yes),len(rain_no)], marker_color='lightsalmon')])\nfig.update_layout(title_text='Is Rain Tomorrow')\nfig.show()","207ce366":"msno.matrix(data)\nplt.show()","84ed98f2":"msno.bar(data,sort='descending',color='#008599')\nplt.show()","4f769cfe":"def Missing_Values(data):\n    variable_name=[]\n    total_value=[]\n    total_missing_value=[]\n    missing_value_rate=[]\n    unique_value_list=[]\n    total_unique_value=[]\n    data_type=[]\n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()\/data[col].shape[0],3))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n    missing_data=pd.DataFrame({\"Variable\":variable_name,\"Total_Value\":total_value,\\\n                             \"Total_Missing_Value\":total_missing_value,\"Missing_Value_Rate\":missing_value_rate,\n                             \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value})\n    return missing_data.sort_values(\"Missing_Value_Rate\",ascending=False)","a642fab2":"data_info=Missing_Values(data)\ndata_info","317cdd89":"data_info[\"Scales_of_measurement\"]=[\"Continuous\",\"Continuous\",\"Ordinal\",\"Ordinal\",\"Continuous\",\\\n\"Continuous\",\"Nominal\",\"Nominal\",\"Continuous\",\"Nominal\",\"Continuous\",\"Continuous\",\"Continuous\",\\\n\"Continuous\",\"Continuous\",\"Nominal\",\"Continuous\",\"Continuous\",\"Continuous\",\"Continuous\",\"Nominal\",\\\n\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\"]\n\ndata_info = data_info.set_index(\"Variable\")\ndata_info","1abb02f2":"numerical_columns = list(data_info.loc[(data_info.loc[:,\"Scales_of_measurement\"]==\"Continuous\")].index)\nlen(numerical_columns), numerical_columns","df995d6d":"categorical_columns = list(data_info.loc[(data_info.loc[:,\"Scales_of_measurement\"]==\"Nominal\") |\n                                       (data_info.loc[:,\"Scales_of_measurement\"]==\"Ordinal\")].index)\nlen(categorical_columns), categorical_columns","77fe0abd":"f,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), annot=True, linewidths=0.5,linecolor=\"black\", fmt= '.1f',ax=ax,cmap=\"coolwarm\")\nplt.show()","54ca042c":"labels = data_info.Scales_of_measurement.value_counts().index\nsizes = data_info.Scales_of_measurement.value_counts().values\nplt.figure(figsize = (6,6))\nplt.pie(sizes,  labels=labels, colors=sns.color_palette('bright'), autopct='%1.1f%%')\nplt.title('Variable Types',fontsize = 17,color = 'brown')\nplt.show()","269a2dae":"fig, ax =plt.subplots(nrows=1,ncols=2, figsize=(20,7))\nsns.barplot(x=data.Month,y=data.MinTemp,hue=\"RainTomorrow\",data=data,ax=ax[0],palette=\"pastel\")\nsns.barplot(x=data.Month,y=data.MaxTemp,hue=\"RainTomorrow\",data=data,ax=ax[1],palette=\"pastel\")\nplt.show()","3647a4e9":"def pairplot(data,lst):\n    sns.set(style=\"ticks\")\n    sns.pairplot(data[lst],hue=\"RainTomorrow\")\nlst=[\"MinTemp\",\"MaxTemp\",\"Temp9am\",\"Temp3pm\",\"RainTomorrow\"]\ndata_2016=data[data.Year==2016]\npairplot(data_2016,lst)","de41a9ad":"from plotly.offline import iplot\nfig, ax =plt.subplots(nrows=1,ncols=1, figsize=(18,8))\nsns.pointplot(x=\"Year\",y=\"Cloud3pm\",data=data,hue=\"RainToday\")\nsns.pointplot(x=\"Year\",y=\"Cloud3pm\",data=data,hue=\"RainTomorrow\",color=\"red\")\nplt.show()","01d2e2e5":"x1=data.iloc[:,0:21]\nx2=data.iloc[:,22:]\nX=pd.concat((x1,x2),axis=1)\nY=data[\"RainTomorrow\"]","cece1ae5":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)","709cc066":"x_train.shape,y_train.shape","2013bc73":"def boxplot_for_outlier(df,columns):\n    count = 0\n    fig, ax =plt.subplots(nrows=2,ncols=7, figsize=(20,8))\n    for i in range(2):\n        for j in range(7):\n            sns.boxplot(x = df[columns[count]], palette=\"Set2\",ax=ax[i][j])\n            count = count+1\nboxplot_for_outlier(x_train,numerical_columns)","f064cbc9":"lower_and_upper={}\nx_train_outlier=x_train.copy()\n\nfor col in numerical_columns:\n    if(col==\"Rainfall\"): \n        sparse_value = x_train[col].mode()[0]\n        nonsparse_data = pd.DataFrame(x_train[x_train[col] != sparse_value][col])\n        q1=nonsparse_data[col].describe()[4]\n        q3=nonsparse_data[col].describe()[6]\n        iqr=q3-q1\n        lowerbound = q1 - (1.5*iqr)\n        upperbound = q3 + (1.5*iqr)\n        lower_and_upper[col]=(lowerbound,upperbound)\n        nonsparse_data.loc[(nonsparse_data.loc[:,col]<lowerbound),col] =  lowerbound*0.75\n        nonsparse_data.loc[(nonsparse_data.loc[:,col]>upperbound),col] =  upperbound*1.25\n        x_train_outlier[col][nonsparse_data.index]=nonsparse_data[col]\n        \n    else:\n        q1=x_train_outlier[col].describe()[4]\n        q3=x_train_outlier[col].describe()[6]\n        iqr=q3-q1\n        lowerbound = q1 - (1.5 * iqr)\n        upperbound = q3 + (1.5 * iqr)\n        lower_and_upper[col]=(lowerbound,upperbound)\n        number_of_outlier = x_train_outlier.loc[(x_train_outlier.loc[:,col]<lowerbound)\\\n                                                           | (x_train_outlier.loc[:,col]>upperbound)].shape[0]\n        if(number_of_outlier>0):\n            print(number_of_outlier,\" outlier values cleared in\" ,col)\n            x_train_outlier.loc[(x_train_outlier.loc[:,col]<lowerbound),col] =  lowerbound*0.75\n            x_train_outlier.loc[(x_train_outlier.loc[:,col]>upperbound),col] =  upperbound*1.25","2491359c":"x_test_outlier=x_test.copy()\n\nfor col in numerical_columns:\n    if(col ==\"Rainfall\"):\n        sparse_value = x_test[col].mode()[0]\n        nonsparse_data = pd.DataFrame(x_test[x_test[col] != sparse_value][col])\n        nonsparse_data.loc[(nonsparse_data.loc[:,col]<lower_and_upper[col][0]),col] =  lower_and_upper[col][0]*0.75\n        nonsparse_data.loc[(nonsparse_data.loc[:,col]>lower_and_upper[col][1]),col] =  lower_and_upper[col][1]*1.25\n        x_test_outlier[col][nonsparse_data.index]=nonsparse_data[col]\n        \n    else:\n        \n        number_of_outlier_test = x_test_outlier.loc[(x_test_outlier.loc[:,col]<lower_and_upper[col][0]) |\\\n                                                    (x_test_outlier.loc[:,col]>lower_and_upper[col][1])].shape[0]\n        if(number_of_outlier_test>0):\n            print(number_of_outlier_test,\" outlier values cleared in\" ,col)\n            x_test_outlier.loc[(x_test_outlier.loc[:,col]<lower_and_upper[col][0]),col] =  lower_and_upper[col][0]*0.75\n            x_test_outlier.loc[(x_test_outlier.loc[:,col]>lower_and_upper[col][1]),col] =  lower_and_upper[col][1]*1.25","c855b309":"boxplot_for_outlier(x_train_outlier,numerical_columns)","ce0aea68":"x_test[numerical_columns]=x_test_outlier[numerical_columns]\nx_train[numerical_columns]=x_train_outlier[numerical_columns]","53dcbf0a":"msno.heatmap(data, figsize=(18,8))\nplt.show()","637c65e9":"zero_missing_rate=list(data_info[data_info[\"Missing_Value_Rate\"]==0].index)\nlow_missing_rate=list(data_info[(data_info['Missing_Value_Rate']>0)&(data_info['Missing_Value_Rate']<=0.05)].index)\nlow_missing_rate.remove(\"RainToday\")\nlow_missing_rate,zero_missing_rate","a7015317":"def simple_imputer(data,columns):\n    \n    for col in columns:\n        total_nan=int(data[col].isnull().sum())\n        \n        if(col in categorical_columns):\n            \n            most_frequent_value=data[col].value_counts().index[0]\n            data[col]=data[col].fillna(most_frequent_value)\n            \n            print(\"A total of {} Categorical variable {} have been imputed.\".format(total_nan,col))\n            \n        else:\n            mean=data[col].mean()\n            std=data[col].std()\n            \n            random_normal=np.random.normal(loc=mean,scale=std,size=total_nan) \n            data[col][data[col].isnull()]=random_normal\n            \n            print(\"A total of {} Numerical variable {} have been imputed.\".format(total_nan,col))","09f63265":"simple_imputer(x_train,low_missing_rate)","1a2aee97":"simple_imputer(x_test,low_missing_rate)","f0a92b10":"Missing_Values(x_train[low_missing_rate])","d11b7de6":"list1=pd.Series(x_train[x_train[\"RainToday\"].isnull()][\"Rainfall\"])\nlist2=pd.Series(x_test[x_test[\"RainToday\"].isnull()][\"Rainfall\"])","995f00ff":"x_train[\"RainToday\"].fillna(pd.Series([\"Yes\" if x>1 else \"No\" for x in list1],index=list1.index),inplace=True)","b94786ab":"x_test[\"RainToday\"].fillna(pd.Series([\"Yes\" if x>1 else \"No\" for x in list2],index=list2.index),inplace=True)","3c172b31":"Missing_Values(x_train)","78132c7b":"def target_encoder(train,test,columns):\n    for col in columns:\n        encoder = TargetEncoder()\n        train[col]=encoder.fit_transform(train[col],y_train)\n        test[col]=encoder.transform(test[col])\n        print(test.loc[:,[col]].isnull().sum())\n        print(train.loc[:,[col]].isnull().sum())","9c53f21d":"target_encoder_cols = [\"WindDir9am\",\"WindGustDir\"]\nx_train_encoder=x_train.copy()\nx_test_encoder=x_test.copy()\ntarget_encoder(x_train_encoder,x_test_encoder,target_encoder_cols)","5876ff3b":"data_info_2=Missing_Values(x_train_encoder)\nmodel_based_list=list(data_info_2[\"Variable\"][data_info_2[\"Missing_Value_Rate\"]>0.06])\nmodel_based_list","c2ba8270":"from sklearn.impute import KNNImputer","4135d57d":"knn_imputer=KNNImputer(n_neighbors=3)\nx_test_mbi=x_test_encoder.copy()\nx_train_mbi=x_train_encoder.copy()\nfor col in model_based_list:\n    x_train_mbi[col] = knn_imputer.fit_transform(np.array(x_train_mbi[col]).reshape(-1,1),y_train)\n    x_test_mbi[col] = knn_imputer.transform(np.array(x_test_mbi[col]).reshape(-1,1))\n    print(x_test_mbi.loc[:,[col]].isnull().sum())\n    print(x_train_mbi.loc[:,[col]].isnull().sum())","0d4a7559":"Missing_Values(x_train_mbi)","cbef6e1c":"def Label_Encoder(df,columns,train_or_test):\n    for col in columns:\n        le = LabelEncoder()\n        if(train_or_test == \"test\"):\n\n            le.fit(x_train_mbi[col].copy().astype(str))\n            df[col] = le.transform(df[col].copy().astype(str))\n\n        else:\n            df[col] = le.fit_transform(df[col].copy().astype(str))\n    return df","b88b08f4":"x_test_mbi = Label_Encoder(x_test_mbi,[\"Location\",\"RainToday\",\"WindDir3pm\"],\"test\")  ","716df2ab":"x_train_mbi = Label_Encoder(x_train_mbi,[\"Location\",\"RainToday\",\"WindDir3pm\"],\"train\")","1e65df67":"before_importance_scores=pd.DataFrame(columns=[\"scores\"])","23bbf008":"from sklearn import metrics\nimport time\nstart_time = time.process_time()\nxgb_model = xgb.XGBClassifier(n_estimators=150,random_state=0,learning_rate=0.1,eta=0.4,booster=\"gbtree\",base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        max_depth=6,min_child_weight=7,reg_lambda=0.27611902459972926,subsample=0.9300916052594785)\n\nxgb_model.fit(x_train_mbi, y_train)\nprint(time.process_time()-start_time)\ny_pred = xgb_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nbefore_importance_scores.loc[\"XGboost Classifier\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","1869516f":"start_time = time.process_time()\nlgbm_model = lgb.LGBMClassifier(min_child_samples=25,n_estimators=150,subsample=0.11,\n                                boosting_type=\"dart\",learning_rate=0.25)\n\nlgbm_model.fit(x_train_mbi, y_train)\nprint(time.process_time()-start_time)\ny_pred = lgbm_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nbefore_importance_scores.loc[\"LGBM Classifier\"]=roc_auc\n\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","d6ec0bbc":"start_time = time.process_time()\ncat_model = CatBoostClassifier(depth=10,max_bin=60,bagging_temperature= 0.2,random_strength=5)\n\ncat_model.fit(x_train_mbi, y_train,verbose=False)\nprint(time.process_time()-start_time)\ny_pred = cat_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nbefore_importance_scores.loc[\"CatBoost Classifier\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","9cfcf755":"from sklearn.ensemble import GradientBoostingClassifier\nstart_time = time.process_time()\ngradient_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                                   max_depth=7, random_state=0)\n\ngradient_model.fit(x_train_mbi, y_train)\nprint(time.process_time()-start_time)\ny_pred = gradient_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nbefore_importance_scores.loc[\"GradientBoosting Classifier\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","c3d49696":"from sklearn.linear_model import LogisticRegression\nstart_time = time.process_time()\nlog_reg_model = LogisticRegression(C= 0.1, solver= 'liblinear',class_weight={1: 0.5, 0: 0.5},penalty=\"l1\")\n\nlog_reg_model.fit(x_train_mbi, y_train)\nprint(time.process_time()-start_time)\ny_pred = log_reg_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nbefore_importance_scores.loc[\"Logistic Regression\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","a7860835":"import plotly.express as px\nfig = px.bar(before_importance_scores, x=before_importance_scores.index, y='scores',height=400,width=900,text=round(before_importance_scores.scores,3),title=\"Visualization before feature importance\")\nfig.update_traces(marker_color='rgb(158,20,225)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6)\nfig.show()","4d2cc68d":"import operator\nxgb_params = {\"objective\": \"reg:linear\", \"eta\": 0.01, \"max_depth\": 8, \"seed\": 42, \"silent\": 1}\nnum_rounds = 1000\n\ndtrain = xgb.DMatrix(x_train_mbi, label=y_train)\ngbdt = xgb.train(xgb_params, dtrain, num_rounds)\n\nimportance = gbdt.get_fscore()\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\ndf['fscore'] = df['fscore'] \/ df['fscore'].sum()\n\nplt.figure()\ndf.plot()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nplt.xlabel('relative importance')\nplt.show()","c4f5b7a4":"lst=list(df[\"feature\"][df[\"fscore\"]<0.03])\nlst","ed313e1c":"x_train_importance=x_train_mbi.drop(lst,axis=1)\nx_test_importance=x_test_mbi.drop(lst,axis=1)","f5a218d0":"after_importance_scores=pd.DataFrame(columns=[\"scores\"])","90d58b8d":"start_time = time.process_time()\nxgb_model = xgb.XGBClassifier(n_estimators=150,random_state=0,learning_rate=0.1,eta=0.4,booster=\"gbtree\",\n                              base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                              max_depth=6,min_child_weight=7,reg_lambda=0.27611902459972926,\n                              subsample=0.9300916052594785)\n\nxgb_model.fit(x_train_importance, y_train)\nprint(time.process_time()-start_time)\ny_pred = xgb_model.predict_proba(x_test_importance)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nafter_importance_scores.loc[\"XGboost Classifier\"]=roc_auc\n\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","3173f5ed":"start_time = time.process_time()\nlgbm_model = lgb.LGBMClassifier(min_child_samples=25,n_estimators=150,subsample=0.11,\n                                boosting_type=\"dart\",learning_rate=0.25)\n\nlgbm_model.fit(x_train_mbi, y_train)\nprint(time.process_time()-start_time)\ny_pred = lgbm_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nafter_importance_scores.loc[\"LGBM Classifier\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","7ad9d00c":"start_time = time.process_time()\ncat_model = CatBoostClassifier(depth=10,max_bin=60,bagging_temperature= 0.2,random_strength=5)\n\n\ncat_model.fit(x_train_mbi, y_train,verbose=False)\nprint(time.process_time()-start_time)\ny_pred = cat_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nafter_importance_scores.loc[\"CatBoost Classifier\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","fee697f6":"start_time = time.process_time()\ngradient_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                                   max_depth=7, random_state=0)\n\ngradient_model.fit(x_train_mbi, y_train)\nprint(time.process_time()-start_time)\ny_pred = gradient_model.predict_proba(x_test_mbi)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nafter_importance_scores.loc[\"GradientBoosting Classifier\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","ee5b9580":"from sklearn.linear_model import LogisticRegression\nstart_time = time.process_time()\nlog_reg_model = LogisticRegression(C= 0.1, solver= 'liblinear',class_weight={1: 0.5, 0: 0.5},penalty=\"l1\")\n\nlog_reg_model.fit(x_train_importance, y_train)\nprint(time.process_time()-start_time)\ny_pred = log_reg_model.predict_proba(x_test_importance)\ny_pred = y_pred[:, 1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nafter_importance_scores.loc[\"Logistic Regression\"]=roc_auc\n\nbc = BinaryClassification(y_test, y_pred, labels=[\"0\", \"1\"])\n\n# Figures\nplt.figure(figsize=(5,5))\nbc.plot_roc_curve()\nplt.show()","e4a14fc3":"import plotly.express as px\nfig = px.bar(after_importance_scores, x=before_importance_scores.index, y='scores',height=400,width=900,text=round(after_importance_scores.scores,3),title=\"Visualization after feature importance\")\nfig.update_traces(marker_color='rgb(180,60,50)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.7)\nfig.show()","7f87d179":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=before_importance_scores.index,\n    y=before_importance_scores.scores,\n    name='Before Importance',text=round(before_importance_scores.scores,3),textposition='auto',\n    marker_color='purple'\n))\nfig.add_trace(go.Bar(\n    x=after_importance_scores.index,\n    y=after_importance_scores.scores,\n    name=\"After Importance\",text=round(after_importance_scores.scores,3),textposition='auto',\n    marker_color='pink'\n))\n\nfig.update_layout(barmode='group', xaxis_tickangle=-30,title=\"Visualization for After Feature Importance and Before Feature Importance\")\nfig.show()","253a3712":"## Missing Value Imputation","2b4f4bb9":"## Lightm-GBM Classifier","8db4f224":"![](https:\/\/cdn.discordapp.com\/attachments\/703002285994672192\/715889102012022835\/rain_yes_no.png)","12d27835":"## GradientBoosting Classifier","38ce88e2":"### Simple Imputer for Low Missing Values","86544a9a":"### Since our models could not train categorical variables, they were converted to numerical value with LabelEncoder.\n","aa61edb2":"## Visualization for After Feature Importance and Before Feature Importance\n","2fc705de":"## Representation of missing values, unique values, etc.","af8a99e3":"### We see an unbalanced distribution in the dataset","17237a28":"Note: The blue line is Today-No, The black line is Tomorrow-No,The red line is Today-Yes, The brown line is Tomorrow-Yes","1f825607":"## Determination of numerical columns.","e26f67bf":"## Import Data","f9abc79c":"# 3. Split data into training and test set","cd9f53ac":"## Determination of categorical columns.","2bb24df5":"![](https:\/\/cdn.discordapp.com\/attachments\/703002285994672192\/716668021233680394\/before.png)","8537e44b":"### Missing Value Heatmap","5604e05e":"### Visualization after feature importance\n","3d4424f9":"### Outlier cleaning for Test","43a4f92b":"## CatBoost Classifier","7d5f0b8f":"# Will it rain tomorrow?\n","1044a7e2":"### Visualization after outlier","28e06704":"### Visualization before outlier","a6cea491":"## Types of data measurement scales","70e69475":"## Outlier cleaning","966c9e69":"### It is given in the dataset description, that we should drop the RISK_MM feature variable from the dataset description. So, we should drop it as follows-","86d50152":"## Logistic Regression","034efc02":"## Conclusion \n\n### In this project, 5 machine learning algorithms were used. The CatBoost Algorithm has a higher accuracy rate  than other algorithms by a little difference. We saw that There is no big difference in accuracy rate after feature importance is done. The training time of LightGBM algorithm is faster than others.\n\n### <p style='color:#561225'><i>I hope you find this kernel useful. If you like it please do an upvote.<\/i><p>","af90bcc9":"## Logistic Regression","ec8332d2":"# 1. Exploratory data analysis","4857c60f":"# 5. Modelling","f4f7c72e":"# 7.Predict results","024fb0f9":"## Modelling after feature importance","fc17559b":"### In this project, we tried to predict whether it will rain tomorrow. It is a classification project. There is a Yes or No class (a binary classification problem).  Yes, it will rain tomorrow. If No, it will not rain tomorrow. In this project,Rain in Tomorrow dataset was used.","bfb1f65e":"![](https:\/\/cdn.discordapp.com\/attachments\/703002285994672192\/716668019140460584\/after.png)","0f748a81":"### Number and rate of Target Variables","e187f656":"## Light-GBM Classifier","3d6e410f":"## Information about Missing values.","d6dd5a27":"# 2. Visualization","f1d2d589":"### The date was divided into days, months, years.","608fb7cb":"<img src=\"https:\/\/media.giphy.com\/media\/IuVFGSQZTd6TK\/giphy.gif\">","79fb75f1":"## Data Analysis\n","46e2ced6":"## CatBoost Classifier","dffd0396":"### We made Model based filling for other empty columns. These models were XGBoost and KNN. Continued with KNN, which provides the best performance.","47941ac1":"### Target content changed to 1-0.","b6d9b074":"### Since our target variable is categorical, we will use these models. These can be listed as follows:\n\n\n1.   XGboost Classifier\n2.   Light-GBM Classifier\n1.   CatBoost Classifier\n2.   GradientBoosting Classifier\n1.   Logistic Regression\n\n\n\n\n\n\n\n","4a737950":"# 4. Feature Engineering","d307ad21":"### Outlier cleaning for Train","ea77509a":"![](https:\/\/cdn.discordapp.com\/attachments\/703002285994672192\/716668024374951977\/vs.png)","04842b8c":"### Visualization before feature importance","24551161":"# 6. Feature Importance","a92c65e5":"### List for low missing values","39b2a08b":"### TargetEncoder was made for columns with empty and categorical values, considering 'NaN' as a variable.","1278d4fb":"###### Correlation of Rainfall and RainToday column missing values is 1. Column RainToday was Yes when the values in column Rainfall were greater than 1, and No when it was less than 1. Accordingly, a filling process will be made.","507671e4":"## XGBoost Classifier","71dae0e4":"## GradientBoosting Classifier","d34d7944":"### Showing missing values","02a37ea1":"## XGBoost Classifier","389e9883":"## Content\n### This dataset contains daily weather observations from numerous Australian weather stations such as Rainfall, Wind and Humidity.\n### The following steps were followed in this project:\n\n1.   Exploratory data analysis\n2.   Visualization\n2.   Split data into training and test set\n2.   Feature Engineering\n1.   Modelling\n1.   Feature Importance\n1.   Predict results\n\n\n"}}