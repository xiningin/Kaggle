{"cell_type":{"277e5264":"code","b7357d46":"code","71ba2721":"code","e1ce12f0":"code","7c366299":"code","dc4f2db3":"code","2710e4be":"code","b3d44cce":"code","ebf4edc8":"code","7e4b2541":"markdown","4f3ec704":"markdown","3885cef6":"markdown","1adcffc2":"markdown","ab57159c":"markdown","93b322d2":"markdown","023f0c0e":"markdown","679e2b89":"markdown","7c8bf389":"markdown","a874c7d9":"markdown","c22b8ccf":"markdown","f897a7db":"markdown","cca7d5dd":"markdown","0710b930":"markdown","9b52c4e9":"markdown","67e258cf":"markdown","6d18a275":"markdown","2c179580":"markdown"},"source":{"277e5264":"# read in libraries\nimport tensorflow as tf\nfrom tensorflow.keras import backend, models, layers, optimizers\nimport numpy as np\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import plot_model\nfrom IPython.display import display\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os, shutil\nfrom tensorflow.keras.models import Model\nnp.random.seed(42)","b7357d46":"# Specify the base directory where images are located.\nbase_dir = '\/kaggle\/input\/fruits\/fruits-360\/'\n\n\n# Specify the traning, validation, and test dirrectories.  \ntrain_dir = os.path.join(base_dir, 'Training')\ntest_dir = os.path.join(base_dir, 'Test')\n\n# Normalize the pixels in the train data images, resize and augment the data.\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,# The image augmentaion function in Keras\n    shear_range=0.2,\n    zoom_range=0.2, # Zoom in on image by 20%\n    horizontal_flip=True) # Flip image horizontally \n\n# Normalize the test data imagees, resize them but don't augment them\ntest_datagen = ImageDataGenerator(rescale=1.\/255) \n\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(299, 299),\n    batch_size=16,\n    class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(299, 299),\n    batch_size=16,\n    class_mode='categorical')","71ba2721":"# Load InceptionV3 library\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Always clear the backend before training a model\nbackend.clear_session()\n\n# InceptionV3 model and use the weights from imagenet\nconv_base = InceptionV3(weights = 'imagenet', #Useing the inception_v3 CNN that was trained on ImageNet data.  \n                  include_top = False)\n                  ","e1ce12f0":"# Connect the InceptionV3 output to the fully connected layers\nInceptionV3_model = conv_base.output\npool = GlobalAveragePooling2D()(InceptionV3_model)\ndense_1 = layers.Dense(512, activation = 'relu')(pool)\noutput = layers.Dense(120, activation = 'softmax')(dense_1)","7c366299":"# Create an example of the Archictecture to plot on a graph\nmodel_example = models.Model(inputs=conv_base.input, outputs=output)\n# plot graph\nplot_model(model_example)","dc4f2db3":"# Define\/Create the model for training\nmodel_InceptionV3 = models.Model(inputs=conv_base.input, outputs=output)\n\n# Compile the model with categorical crossentropy for the loss function and SGD for the optimizer with the learning\n# rate at 1e-4 and momentum at 0.9\nmodel_InceptionV3.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])","2710e4be":"# Import from tensorflow the module to read the GPU device and then print\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","b3d44cce":"# Execute the model with fit_generator within the while loop utilizing the discovered GPU\nimport tensorflow as tf\nwith tf.device(\"\/device:GPU:0\"):\n    history = model_InceptionV3.fit_generator(\n        train_generator,\n        epochs=5,\n        validation_data=test_generator,\n        verbose = 1,\n        callbacks=[EarlyStopping(monitor='val_accuracy', patience = 5, restore_best_weights = True)])","ebf4edc8":"# Create a dictionary of the model history \nimport matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, len(history_dict['accuracy']) + 1)\n\n# Plot the training\/validation loss\nplt.plot(epochs, loss_values, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Plot the training\/validation accuracy\nplt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n# Evaluate the test accuracy and test loss of the model\ntest_loss, test_acc = model_InceptionV3.evaluate_generator(test_generator)\n\nprint('Model testing accuracy\/testing loss:', test_acc, \" \", test_loss)","7e4b2541":"# 4. Summary of the Model\nThe Transfer learning model is called InceptionV3. The InceptionV3 model is a convolutional neural network designed to be 48 layers deep with images 299 by 299 (\u201cVisibleBreadcrumbs,\u201d 2016). The original architectural design is modified from the original Inception version called \u201cGoogLeNet,\u201d which was a 22 layer deep convolutional neural network made back in 2014. The name of the model derives from the Inception movie concept of going deeper into a dream \u201cA dream within a dream,\u201d translating to a convolutional neural network within a convolutional neural network. \n\nThe idea behind GoogLeNet was to eliminate the issues of overfitting which is caused by deeper neural networks. Overfitting is usually an issue when the dataset is too small when being trained within a large neural network and the training accuracy doesn\u2019t represent the validation accuracy (testing accuracy). The solution to creating a huge model to produce accurate results was to design sparsely connected neural networks in place of fully connected neural networks. Back in 2014, the GoogLeNet model won the ImageNet Visual Recognition Challenge with a predictive accuracy of 80%+.\n\nThe InceptionV3 modified version of GoogLeNet is a much better performing and deeper model that gives a much higher accuracy. The pre-trained model is trained to classify thousands of different object categories. This pre-trained model being connected to the bottom dense layers where GlobalAvgPooling2D will reduce the dimensionality from 3D to 1D and output 1 response for every feature matrix. The pooling layer will then be connected to the first hidden dense layer of 512 neurons and then connected to the final output layer of all with the number of neurons\/outputs that there are classes. This will hopefully give a high predictive testing accuracy. The archictecture of this model will be displayed in the next step.\n","4f3ec704":"# 2. Description of the Data\n\nThe dataset itself contains 81,104 different images of 120 different classes of fruits and vegetables that contain multi-fruit (multi-label) sets of at least 103 different labels per fruit or vegetable class. The total number of images is split into training and testing datasets. The training dataset has 60,486 images and the testing dataset has 20,618 images. \n\nThe size of all images is 100x100 pixels and was collected with a logitech C920 camera that was used to film the fruits\/vegetables (Mihai Oltean, 2019). All fruits and vegetables were planted inside of a shaft with a low-speed motor where they were recorded in a short duration of 20 seconds each. The fruits and vegetables testing images were taken with a Nexus 5X smartphone.","3885cef6":"**Display the Model's Testing Accuracy and Testing Loss Value**\n\nNow plot the training accuracy\/validation accuracy and training loss\/validation loss across the epochs, then print the final test accuracy and test loss.","1adcffc2":"**Create a Functional API Model.**\n\nNow combine the pre-trained InceptionV3 model weights with the dense layers (fully connected layers) and reduce the dimensionality of the model in between the two.","ab57159c":"**Display the Functional API Model.**\n\nTo get an understanding of the model archictecture display the functional API model as a whole to visually see the depth of the network.","93b322d2":"**Load the Data and Prepare it.**\n\nTo prepare the data set up a train_datagen and test_datagen with ImageDataGenerator. Then with those generators resize the images of the training data and testing data to match the pre-trained model's pixel image inputs. To ensure that the neural network doesn't learn irrelevant patterns and in return boosts overall performance.","023f0c0e":"# **Fruit 360 - Classifying 120 different fruits with multiple varieties using transfer learning.**\n\n*   This notebook is an overview of transfer learning with the Fruits 360 [dataset](https:\/\/https:\/\/www.kaggle.com\/moltean\/fruits)\n*   Contents of the data: 81,104 images of 120 fruits\n*   Transfer Learning with InceptionV3\n*   99% accuracy\n\n**Contents:**\n*   **1. Project Overview**\n*   **2. Description of the data**\n*   **3. Summary of Transfer Learning**\n*   **4. Summary of the Model\/Network**\n*   **5. Preparing the Data**\n*   **6. Analysis of the Results**\n*   **7. Conclusion**\n*   **8. References**","679e2b89":"**Prepare the InceptionV3 Model**\n\nNow that the images are prepared it is time to import and set up the pre-trained InceptionV3 model for transfer learning.","7c8bf389":"**Train the Model.**\n\nTrain the model with the GPU with the train_generator for the training data and the validation_data parameter set to the test_generator.","a874c7d9":"# 6. Analysis of the Results\n\nThe results for accurately predicting 120 classes of fruits and vegetable images come out to being 99% testing accuracy with a 1.8% loss value. Loss value is the measure of distance for how far off our outputs are and what we expected. The training was done through 5 epochs and took around 20-25min each to complete to achieve this accuracy with the help of kaggle\u2019s GPU to speed up the process. The training data was a 3781 step process (iterations), taking a batch size of data by every 16 samples to be propagated forward and backwards to give us one pass. One pass equals one iteration.","c22b8ccf":"**Load the Libraries.**\n\nWhen loading the libraries ensure that all neccessary modules needed are imported in order to prepare the data and train the model.","f897a7db":"# 5. Preparing the Data and Training the Network","cca7d5dd":"# 3. Summary of Transfer Learning\nTransfer learning makes machine learning simpler for those just starting out. It falls under a CNN (Convolutional Neural Network). CNNs are constructed to learn complex mapping functions when trained on enough data (Aditya Ananthram, 2018). \n\n**A Brief Overview of What a Convolution Does.**\n\nAt the baseline, CNNs have configured weights that consist of a *kernel*. This can be represented as an n x n which is a matrix consisting of lots of certain values. What this kernel does is it *convolutes* (slides and multiplies) through the *input* image. If the input image is (10,10) and the kernel is (3,3) the first slide (stride) will multiply by 9 pixels on the very top left corner of the input image to produce an output of another (3,3) matrix or also known as a *feature matrix* with a single pixel on the top left \ncorner of that. ![alt text](https:\/\/drive.google.com\/uc?id=10jUuhAcMFG8IpFVgLqkiGy4QII83Ciwv)(Krut Patel, 2019; annotated by: James Nelson, 2020). This multiplication continues every time the filter slides across the input image ![alt text](https:\/\/drive.google.com\/uc?id=10mSXbxRhRLikhIObMW8u4re0sulHM4zU) in order to produce the (3,3) feature matrix. ![alt text](https:\/\/drive.google.com\/uc?id=10pwUA1g1uNIrorfflVedNDpsM7F3Fgh7)\n\nWhen the feature matrix is completed it is stacked inside of the convolutional layer and then if the network is designed to do so, another kernel will produce another feature matrix with the same input image. ![alt text](https:\/\/drive.google.com\/uc?id=10pzqWF_sPQYQpurx-VcNRP86oq-1FNZB)(Dertat, 2017; annotated by: James Nelson, 2020).\n\nTo summarize everything, the training of a CNN is about locating all the right values on each of the kernels so that when as the input image passes through the layers it will activate different neurons on the last output layer to predict and accurately classify the image.\n\n**How Transfer learning Comes Into Play.**\n\nTransfer learning make things better for everyone because CNNs created from scratch require a lot of computational power. So we use transfer learning (pre-trained weights) on an already trained model (network) to enhance our training. These pre-trained networks have already been trained on millions of images that belonged to thousands of classes on many high-end powerful GPUs for many days (Aditya Ananthram, 2018). So these pre-trained models can be used to predict new classes on sorts of different datasets. \n\nTransfer learning has many advantages. There is no need for enormous datasets or for a lot of computational power since the pre-trained weights are only needed to learn the weights of the bottom layers. Since these models have been pre-trained on other multiple images out there, they have been specifically trained to recognize the very detailed shapes and smallest parts of different objects in the initial layer which makes transfer learning easy to use. All that will be added to the pre-trained model is some dense layers on the end of it.\n","0710b930":"# 1. Project Overview\nThe purpose of this project is to showcase how using the approach of transfer learning, a method used in deep learning, accurately predicts the given dataset \"Fruit 360\". This provides the audience a well-guided representation and visuals on the functions. A summary will be provided to explain transfer learning, the model\/network, the analysis results, and my conclusion on the project overall.","9b52c4e9":"# 8. References\n\nAditya Ananthram. (2018, October 17). Deep Learning For Beginners Using Transfer Learning In Keras. Retrieved April 24, 2020, from Medium website: https:\/\/towardsdatascience.com\/keras-transfer-learning-for-beginners-6c9b8b7143e\n\nAnalytics Vidhya. (2018, October 18). Understanding Inception Network from Scratch (with Python codes). Retrieved April 27, 2020, from Analytics Vidhya website: https:\/\/www.analyticsvidhya.com\/blog\/2018\/10\/understanding-inception-network-from-scratch\/\n\nDertat, A. (2017, November 8). Applied Deep Learning - Part 4: Convolutional Neural Networks. Retrieved April 28, 2020, from Medium website: https:\/\/towardsdatascience.com\/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\n\nKrut Patel. (2019, September 8). Convolutional Neural Networks \u2014 A Beginner\u2019s Guide - Towards Data Science. Retrieved April 24, 2020, from Medium website: https:\/\/towardsdatascience.com\/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022\n\nPrakhar Ganesh. (2019, October 18). Types of Convolution Kernels\u202f: Simplified - Towards Data Science. Retrieved April 24, 2020, from Medium website: https:\/\/towardsdatascience.com\/types-of-convolution-kernels-simplified-f040cb307c37\n\nVisibleBreadcrumbs. (2016). Retrieved April 27, 2020, from Mathworks.com website: https:\/\/www.mathworks.com\/help\/deeplearning\/ref\/inceptionv3.html\n","67e258cf":"# 7. Conclusion\n\nIn conclusion, transfer learning is a very effective way to train datasets to recognize and classify images. It allows for fast setup without going in detail with designing a convolutional neural network architecture from scratch and it provides high accuracy with the pre-trained model's previous training.\n","6d18a275":"**Define the Model and Compile it.**\n\nDefine the functional API model and compile the model with categorical crossentropy as the loss function and Stochastic Gradient Descent with a learning rate and momentum parameters.","2c179580":"**Check the Device List for the GPU to Use.**\n\nFind the GPU device usable so the training process can be sped up."}}