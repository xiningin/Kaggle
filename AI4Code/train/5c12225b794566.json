{"cell_type":{"ee9a31bb":"code","d0bcabc5":"code","de4b4632":"code","e72ed265":"code","96fb2dfe":"code","47543703":"code","b77beec6":"code","bfca9bd3":"code","d0adef06":"code","9c3a2817":"code","d9a18aa9":"code","726eee66":"code","7f80d792":"code","b215049e":"code","1941ecac":"code","ee03470c":"code","cd416306":"code","19e24825":"code","d4acad40":"code","8873ea0f":"code","150fc420":"code","55a3d89f":"code","bd705581":"code","06d2b043":"code","82fd00b6":"code","810842b1":"code","18e305e2":"code","a5ac2665":"code","a2a40ba1":"code","767a1f9e":"code","a6768bc2":"code","e260e396":"code","0c03c886":"code","11ad1e6e":"code","ab6cdb8b":"markdown","74d58918":"markdown","60994714":"markdown","9aace051":"markdown","608f2b85":"markdown","edcf1028":"markdown","1178f9a7":"markdown","87c2951f":"markdown","8b1d3306":"markdown","0ee13694":"markdown","8d0bb897":"markdown","3e1d93b1":"markdown","719fb0b8":"markdown","db9b9c90":"markdown","bcde9312":"markdown","14dc814d":"markdown","947cd052":"markdown","5b0e26d8":"markdown","53bc411d":"markdown","98e284dc":"markdown","41e991ee":"markdown","54aef3db":"markdown","03f62ce8":"markdown","077d5f41":"markdown","20092f23":"markdown","8ec83fb7":"markdown","335f2961":"markdown","08637f8e":"markdown","3bc14d40":"markdown","608772a2":"markdown","914421ed":"markdown","caa4540a":"markdown","552e9f43":"markdown","46360e33":"markdown","ac1e03db":"markdown"},"source":{"ee9a31bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nstyle.use('seaborn-colorblind')\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","d0bcabc5":"train_l=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_l=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndesc=pd.read_fwf(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt\")\ndataset =  pd.concat(objs=[train_l, test_l], axis=0,sort=False).reset_index(drop=True)\nsample=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n","de4b4632":"n_r=0.6                # Remove Null value ratio more than n_r. For example 0.6 means if column null ratio more than %60 then remove column\ns_r=0.50               # If skewness more than %75 transform column to get normal distribution\nc_r=1                  # Remove correlated columns\nn_f= dataset.shape[1]  # n_f number of features. dataset.shape[1] means all columns. If you change it to 10, it will select 10 most correlated feature\nr_s=42                  # random seed","e72ed265":"def plotting_3_chart(df, feature): \n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## crea,ting a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n \n\nprint('Skewness: '+ str(dataset['SalePrice'].skew())) \nprint(\"Kurtosis: \" + str(dataset['SalePrice'].kurt()))\nplotting_3_chart(dataset, 'SalePrice')","96fb2dfe":"#log transform the target:\ndataset[\"SalePrice\"] = np.log1p(dataset[\"SalePrice\"])","47543703":"print('Skewness: '+ str(dataset['SalePrice'].skew()))   \nprint(\"Kurtosis: \" + str(dataset['SalePrice'].kurt()))\nplotting_3_chart(dataset, 'SalePrice')","b77beec6":"\ndataset_isna=dataset.isna()\ndataset_isna_sum=dataset_isna.sum()\ndataset_isna_ratio=dataset_isna_sum\/len(dataset)\nif \"SalePrice\" in dataset_isna_ratio:\n    dataset_isna_ratio.drop(\"SalePrice\",inplace=True)\nremove_columns=dataset_isna_ratio[dataset_isna_ratio>n_r]\ncolumns=pd.DataFrame(remove_columns)\nprint(\"This Columns will be remove because of null ratio higher than %\"+str(n_r*100)+\": \")\nprint(remove_columns)\ndataset=dataset.drop(columns.index,axis=1)","bfca9bd3":"cat=dataset.select_dtypes(\"object\")\nfor column in cat:\n    dataset[column].fillna(dataset[column].mode()[0], inplace=True)\n    #dataset[column].fillna(\"NA\", inplace=True)\n\n\nfl=dataset.select_dtypes([\"float64\",\"int64\"]).drop(\"SalePrice\",axis=1)\nfor column in fl:\n    dataset[column].fillna(dataset[column].median(), inplace=True)\n    #dataset[column].fillna(0, inplace=True)\n\n","d0adef06":"train_o=dataset[dataset[\"SalePrice\"].notnull()]\nfrom sklearn.neighbors import LocalOutlierFactor\ndef detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx\n\nouts = detect_outliers(train_o['GrLivArea'], train_o['SalePrice'],top=5)\nouts\nplt.show()","9c3a2817":"outs","d9a18aa9":"from collections import Counter\noutliers=outs\nall_outliers=[]\nnumeric_features = train_o.dtypes[train_o.dtypes != 'object'].index\nfor feature in numeric_features:\n    try:\n        outs = detect_outliers(train_o[feature], train_o['SalePrice'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\nfor i in outliers:\n    if i in all_outliers:\n        print(i)\ntrain_o = train_o.drop(train_o.index[outliers])\ntest_o=dataset[dataset[\"SalePrice\"].isna()]\ndataset =  pd.concat(objs=[train_o, test_o], axis=0,sort=False).reset_index(drop=True)","726eee66":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox\nlam = 0.15\n\n#log transform skewed numeric features:\nnumeric_feats = dataset.dtypes[dataset.dtypes != \"object\"].index\n\nskewed_feats = dataset[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > s_r]\nskewed_feats = skewed_feats.index\n\ndataset[skewed_feats] = boxcox1p(dataset[skewed_feats],lam)\n","7f80d792":"dataset.columns[dataset.isnull().any()]","b215049e":"train_heat=dataset[dataset[\"SalePrice\"].notnull()]\ntrain_heat=train_heat.drop([\"Id\"],axis=1)\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train_heat.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train_heat.corr(), \n            cmap=sns.diverging_palette(255, 133, l=60, n=7), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","1941ecac":"feature_corr = train_heat.corr().abs()\ntarget_corr=dataset.corr()[\"SalePrice\"].abs()\ntarget_corr=pd.DataFrame(target_corr)\ntarget_corr=target_corr.reset_index()\nfeature_corr_unstack= feature_corr.unstack()\ndf_fc=pd.DataFrame(feature_corr_unstack,columns=[\"corr\"])\ndf_fc=df_fc[(df_fc[\"corr\"]>=.80)&(df_fc[\"corr\"]<1)].sort_values(by=\"corr\",ascending=False)\ndf_dc=df_fc.reset_index()\n\n#df_dc=pd.melt(df_dc, id_vars=['corr'], var_name='Name')\ntarget_corr=df_dc.merge(target_corr, left_on='level_1', right_on='index',\n          suffixes=('_left', '_right'))\n\ncols=target_corr[\"level_0\"].values\n\ntarget_corr","ee03470c":"dataset=dataset.drop([\"GarageArea\",\"TotRmsAbvGrd\"],axis=1)","cd416306":"dataset=pd.get_dummies(dataset,columns=cat.columns)\n","19e24825":"all_features = dataset.keys()\n# Removing features.\ndataset = dataset.drop(dataset.loc[:,(dataset==0).sum()>=(dataset.shape[0]*0.9994)],axis=1)\ndataset = dataset.drop(dataset.loc[:,(dataset==1).sum()>=(dataset.shape[0]*0.9994)],axis=1) \n# Getting and printing the remaining features.\nremain_features = dataset.keys()\nremov_features = [st for st in all_features if st not in remain_features]\nprint(len(remov_features), 'features were removed:', remov_features)","d4acad40":"train=dataset[dataset[\"SalePrice\"].notnull()]\ntest=dataset[dataset[\"SalePrice\"].isna()]\n","8873ea0f":"k = n_f # if you change it 10 model uses most 10 correlated features\ncorrmat=abs(dataset.corr())\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ntrain_x=train[cols].drop(\"SalePrice\",axis=1)\ntrain_y=train[\"SalePrice\"]\nX_test=test[cols].drop(\"SalePrice\",axis=1)","150fc420":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.20, random_state=r_s)","55a3d89f":"\nfrom sklearn.utils.testing import all_estimators\nfrom sklearn import base\n\nestimators = all_estimators()\n\nfor name, class_ in estimators:\n    if issubclass(class_, base.RegressorMixin):\n       print(name+\"()\")","bd705581":"np.random.seed(seed=r_s)\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,ExtraTreesRegressor,HistGradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Ridge,RidgeCV,BayesianRidge,LinearRegression,Lasso,LassoCV,ElasticNet,RANSACRegressor,HuberRegressor,PassiveAggressiveRegressor,ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.cross_decomposition import CCA\nfrom sklearn.neural_network import MLPRegressor\n\n\n\nmy_regressors=[ \n               ElasticNet(alpha=0.001,l1_ratio=0.70,max_iter=100,tol=0.01, random_state=r_s),\n               ElasticNetCV(l1_ratio=0.9,max_iter=100,tol=0.01,random_state=r_s),\n               CatBoostRegressor(logging_level='Silent',random_state=r_s),\n               GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber',random_state =r_s),\n               LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       random_state=r_s\n                                       ),\n               RandomForestRegressor(random_state=r_s),\n               AdaBoostRegressor(random_state=r_s),\n               ExtraTreesRegressor(random_state=r_s),\n               SVR(C= 20, epsilon= 0.008, gamma=0.0003),\n               Ridge(alpha=6),\n               RidgeCV(),\n               BayesianRidge(),\n               DecisionTreeRegressor(),\n               LinearRegression(),\n               KNeighborsRegressor(),\n               Lasso(alpha=0.00047,random_state=r_s),\n               LassoCV(),\n               KernelRidge(),\n               CCA(),\n               MLPRegressor(random_state=r_s),\n               HistGradientBoostingRegressor(random_state=r_s),\n               HuberRegressor(),\n               RANSACRegressor(random_state=r_s),\n               PassiveAggressiveRegressor(random_state=r_s)\n               #XGBRegressor(random_state=r_s)\n              ]\n\nregressors=[]\n\nfor my_regressor in my_regressors:\n    regressors.append(my_regressor)\n\n\nscores_val=[]\nscores_train=[]\nMAE=[]\nMSE=[]\nRMSE=[]\n\n\nfor regressor in regressors:\n    scores_val.append(regressor.fit(X_train,y_train).score(X_val,y_val))\n    scores_train.append(regressor.fit(X_train,y_train).score(X_train,y_train))\n    y_pred=regressor.predict(X_val)\n    MAE.append(mean_absolute_error(y_val,y_pred))\n    MSE.append(mean_squared_error(y_val,y_pred))\n    RMSE.append(np.sqrt(mean_squared_error(y_val,y_pred)))\n\n    \nresults=zip(scores_val,scores_train,MAE,MSE,RMSE)\nresults=list(results)\nresults_score_val=[item[0] for item in results]\nresults_score_train=[item[1] for item in results]\nresults_MAE=[item[2] for item in results]\nresults_MSE=[item[3] for item in results]\nresults_RMSE=[item[4] for item in results]\n\n\ndf_results=pd.DataFrame({\"Algorithms\":my_regressors,\"Training Score\":results_score_train,\"Validation Score\":results_score_val,\"MAE\":results_MAE,\"MSE\":results_MSE,\"RMSE\":results_RMSE})\ndf_results","06d2b043":"best_models=df_results.sort_values(by=\"RMSE\")\nbest_model=best_models.iloc[0][0]\nbest_stack=best_models[\"Algorithms\"].values\nbest_models","82fd00b6":"\nbest_model.fit(X_train,y_train)\ny_test=best_model.predict(X_test)\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\nmy_submission.to_csv('submission_bm.csv', index=False)\nprint(\"Model Name: \"+str(best_model))\nprint(best_model.score(X_val,y_val))\ny_pred=best_model.predict(X_val)\nprint(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))","810842b1":"plt.figure(figsize=(10,7))\ny_pred=best_model.predict(X_val)\nsns.regplot(x=y_val,y=y_pred,truncate=False)\nplt.show()","18e305e2":"i_num=[]\nj_num=[]\nscore=[]\nRMSE=[]\nfor i in range(1,8):\n    stack_models=i\n    for j in range(1,4):\n        base_model=j\n        best_n_models=best_models.head(stack_models).index\n        regressors_top_n=list( regressors[i] for i in best_n_models)\n\n        from mlxtend.regressor import StackingCVRegressor\n        stack = StackingCVRegressor(regressors=regressors_top_n,meta_regressor= best_stack[base_model], use_features_in_secondary=True)\n        comb=stack.fit(X_train,y_train)\n        y_pred=comb.predict(X_val)\n        score.append(comb.score(X_val,y_val))\n        RMSE.append(np.sqrt(mean_squared_error(y_val,y_pred)))\n        i_num.append(i)\n        j_num.append(j)\n        \nopt_resr=zip(score,RMSE,i_num,j_num)\nopt_resr=set(opt_resr)\nopt_resr_score=[item[0] for item in opt_resr]\nopt_resr_RMSE=[item[1] for item in opt_resr]\nopt_resr_i_num=[item[2] for item in opt_resr]\nopt_resr_j_num=[item[3] for item in opt_resr]\n\n\n\ndf_opt_resr=pd.DataFrame({\"Score\":opt_resr_score,\"RMSE\":opt_resr_RMSE,\"i_num\":opt_resr_i_num,\"j_num\":opt_resr_j_num})\ndf_opt_resr=df_opt_resr.sort_values(by=\"RMSE\")\nopt_best_model_i_num=df_opt_resr.iloc[0][2].astype(int)\nopt_best_model_j_num=df_opt_resr.iloc[0][3].astype(int)\n","a5ac2665":"best_n_models=best_models.head(opt_best_model_i_num).index\nregressors_top_n=list( regressors[i] for i in best_n_models)\nfrom mlxtend.regressor import StackingCVRegressor\nstack = StackingCVRegressor(regressors=regressors_top_n,meta_regressor= regressors[opt_best_model_j_num], use_features_in_secondary=True)\ncomb=stack.fit(X_train,y_train)\nprint(comb.score(X_val,y_val))\ny_pred=comb.predict(X_val)\nprint(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))","a2a40ba1":"plt.figure(figsize=(10,7))\ny_pred=comb.predict(X_val)\nsns.regplot(x=y_val,y=y_pred,truncate=False)\nplt.show()","767a1f9e":"y_test=comb.predict(X_test)\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\n#my_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\nmy_submission.to_csv('submission_stack.csv', index=False)","a6768bc2":"def blended_predictions(X):\n    return ((0.40 * comb.predict(X)) + \\\n            (0.40 * best_models.iloc[0][0].predict(X)) + \\\n            (0.10 * best_models.iloc[1][0].predict(X)) + \\\n            (0.10 * best_models.iloc[2][0].predict(X)))\n\ny_pred=blended_predictions(X_val)\n\n\n#y_pred=comb.predict(X_val)\nprint(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))\n\ny_test=blended_predictions(X_test)\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\nmy_submission.to_csv('submission_blend3.csv', index=False)\n\n","e260e396":"import shap  # package used to calculate Shap values\n\nxgb=XGBRegressor(random_state=2)\nxgb=xgb.fit(X_train,y_train)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(X_val)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values, X_val)\n","0c03c886":"shap.force_plot(explainer.expected_value, shap_values[0,:], X_val.iloc[0,:],matplotlib=True)","11ad1e6e":"most_corr=dataset.corr()[\"SalePrice\"].abs().sort_values(ascending=False).index[1]\nshap.dependence_plot(most_corr, shap_values, X_val, interaction_index=\"SaleCondition_Partial\")\n","ab6cdb8b":"<a id='2'><\/a><br>\n## 2. Understand the Target (SalePrice) distribution","74d58918":"You can change null ratio (n_r) on parameters section","60994714":"This code block finds best combinations for you. It's taking time but worth it.","9aace051":"<h2> Introduction <\/h2>\nIn this notebook we will use various predictive models to see how accurate they  are.<br>\nThen combine them and try to improve model performance and analyzing best parameters<br>\n<font color='red'>\n### Our goal is create a Auto Machine Learning model for any dataset.<br>\n<font color='black'>\nLet's Begin\n\n<font color='red'>\n\n<h2> Road Map: <\/h2>\n1. [Parameters](#1)  \n1. [Understand the Target (SalePrice) distribution](#2)  \n1. [Drop high null ratio features](#3)\n1. [Fill null values with Mode\/Median (for categorical features -Mode and for numbers-Median)](#4)\n1. [Auto Detect Outliers](#5)\n1. [Check Skewness and fit transormations if needed](#6)\n1. [Check Correlation between features and remove features with high correlations](#7)\n1. [Create regression models and compare the accuracy to our best regressor.](#8)\n1. [Find best model and make a submission](#9)\n1. [Improving models performance with StackingCVRegressor](#10)\n1. [Create stacked model and make a new submission](#11)\n1. [Use Shap and find features importances](#12)\n\n","608f2b85":"## Let's try Blending our Models","edcf1028":"<h2>Before we Begin:  <\/h2>\nIn models with a lot of variables, as in this dataset, you may not always have time to understand the meaning of all variables and what they are. For example, I usually try to make more than 300 variables meaningful. Sometimes this figure exceeds thousands. In such cases, someone has to tell us which variable is important and what are the meanings of them. A corelation matrix is usually can tell, but this alone is not enough. It is also necessary to use the techniques that regression models offer us.<br><br>","1178f9a7":"We have skewed target so we need to transofmation. I ll use log but you try other transformation","87c2951f":"<a id='10'><\/a><br>\n## 10. Create stacked model and make a new submission","8b1d3306":"Now our Target normalized. \n<font color='red'>\nNote: When making submission , this transofmation need to be undone.","0ee13694":"<h1 align=\"center\">![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png) <h1\/>","8d0bb897":"<a id='8'><\/a><br>\n## 8. Find best model and make a submission","3e1d93b1":"<a id='4'><\/a><br>\n## 4. Fill null values with Mode\/Median (for categorical features -Mode and for numbers-Median)","719fb0b8":"* Converting categorical features to numerical (some models doesn't need this conversion)","db9b9c90":"* Train Test Split - Classic","bcde9312":"<a id='11'><\/a><br>\n## 11. Use Shap and find features importances","14dc814d":"<a id='7'><\/a><br>\n## 7. Create regression models and compare the accuracy to our best regressor","947cd052":"<a id='5'><\/a><br>\n## 5. Check Skewness and fit transormations if needed","5b0e26d8":"You can see your models improvement","53bc411d":"Detect and Remove outliers","98e284dc":"<a id='9'><\/a><br>\n## 9. Improving models performance with StackingCVRegressor","41e991ee":"<h1 align=\"center\"> HOUSE PRICES ADVANCED REGRESSION TECHNIQUES <\/h1>","54aef3db":"Note: SHAP doesnt support all models that we have. So if model which we used is unsupported then we will get error. In my case no problem.","03f62ce8":"<h1 align=\"center\">![](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor_files\/stacking_cv_regressor_overview.png) <h1\/>","077d5f41":"<a id='3'><\/a><br>\n## 3. Drop high null ratio features","20092f23":"Remove correlated features","8ec83fb7":"Remove low features with low variances","335f2961":"<a id='1'><\/a><br>\n## 1. Parameters","08637f8e":"<a id='6'><\/a><br>\n## 6. Check Correlation between features and remove features with high correlations","3bc14d40":"Sort models results according to RMSE and select best model for submission.","608772a2":"<a id='5'><\/a><br>\n## 5. Auto Detect Outliers","914421ed":"First Import Data","caa4540a":"How is look like our predictions. Are they close to real target values? <br>\nLet's look at the graph","552e9f43":"Do you know all models names in sckitlearn?\n","46360e33":"### Now we don't have any missing value","ac1e03db":"I use mode for cats and for median for numeric features but you can change it whatever you want."}}