{"cell_type":{"0876591f":"code","a83dbeb4":"code","d4c9a390":"code","fbab3b12":"code","be441ff0":"code","83724f05":"code","7a8a1182":"code","13b06b4b":"code","420a9c8b":"code","8c5566a9":"code","e6b5ca69":"code","69757dbf":"code","68c1a867":"code","8ae3bbbc":"code","6f2a2c84":"code","6914362b":"code","e24c4e7b":"code","28222821":"code","0213ac77":"code","7166bfba":"code","12fcd7d1":"code","29db597e":"code","8f032cba":"code","f4e347d2":"code","a66fb7a3":"code","ec1963a0":"code","a30d2e7a":"code","45d8628a":"code","1d6773d7":"code","25f50f7a":"code","b88b2a8f":"code","6c9b3381":"code","0ca9e69d":"code","e2019d2d":"code","9eff84f9":"code","e993addc":"code","a00094ee":"code","6874cec2":"code","487eac07":"code","aa198686":"code","878f8d1f":"code","c276f76c":"code","84a035f3":"code","8a7ac1d1":"code","fafb248a":"code","238f6459":"code","193db508":"code","eea0abfe":"code","dd0141f5":"code","fa26e858":"code","7f719be7":"code","7ed95ec2":"code","c5016dc4":"code","8ea9cf4f":"code","be2c9c4e":"code","5fd12cf5":"code","b88794c8":"code","24a36976":"code","e48165cb":"code","2ad267e3":"code","6389b0f0":"code","77034e8f":"code","3a4d08a0":"code","8c05b1d0":"code","9dbcca61":"code","d24f4c1b":"code","2d5f2156":"code","392e3dfb":"code","6ed772db":"code","f6775438":"code","449ee17e":"code","85a2f51c":"code","0d5f0ebe":"code","2bf7c31a":"code","ee903542":"code","c38496fd":"code","3801572d":"code","f1dbea67":"code","b594f942":"code","0ff87ad2":"code","5fe93b85":"code","eae6a873":"code","bfb6b10e":"code","d170e43a":"code","f125bfd0":"code","ea5012e2":"code","d07cde71":"code","b136f469":"code","6e3819b4":"code","174cf838":"code","545ed9d3":"code","36b1d997":"code","09fbaf7e":"code","74f37f0d":"code","11dd0397":"code","31c2308b":"code","06abb960":"code","78b5b48b":"code","32c575cd":"code","57f67bcf":"code","203bda1a":"code","0b483578":"code","9a9cc664":"markdown","fda24eea":"markdown","d53d3ed1":"markdown","f4414766":"markdown","ffa95111":"markdown","a85b6ae6":"markdown","74ac1793":"markdown","395b3390":"markdown","7ecf7662":"markdown","ae64f0ce":"markdown","2b963079":"markdown","19229461":"markdown","29d05a5d":"markdown","cd501555":"markdown","88ef3a1f":"markdown","da98b936":"markdown","59101f7d":"markdown","b84ae634":"markdown","f96d5d9d":"markdown","703a40af":"markdown","63cd4405":"markdown","c2897c16":"markdown","fbc5725a":"markdown","2ea6c1f2":"markdown","6afcbc41":"markdown","8f37356b":"markdown","572aad16":"markdown","79c34be3":"markdown","507d0334":"markdown","34339a59":"markdown","35198838":"markdown","cc06e8a8":"markdown","efe24b2e":"markdown","761ec6e0":"markdown","bb73ebc6":"markdown","62d41dce":"markdown","fcb634a5":"markdown","514a073e":"markdown","8ff382d5":"markdown","fa823d46":"markdown","429d28b7":"markdown","910ea251":"markdown","a0e28663":"markdown","f9fcb239":"markdown","1d7f1b64":"markdown","5aa297e4":"markdown","aa20505b":"markdown","bbacac27":"markdown","de9c2bec":"markdown","20eaf01b":"markdown","d73bdab8":"markdown","4d1390db":"markdown","4ec37f6d":"markdown","ffd87b26":"markdown","9f9ad9d6":"markdown","c9cca643":"markdown","a39f7047":"markdown","88bc03ec":"markdown","d22cc8fe":"markdown","75d7004c":"markdown","77b161de":"markdown","16ce7b72":"markdown","b3f571ca":"markdown","3dffa894":"markdown","ff749ae6":"markdown","d2bd26b8":"markdown","d0c0bb8f":"markdown","d3b9a61c":"markdown","3497c025":"markdown","eddda7ec":"markdown","9186c117":"markdown","80d9f354":"markdown","1b247ac5":"markdown","78c1cc40":"markdown","8e26a2f5":"markdown","f2d068bb":"markdown","33a7ccf6":"markdown"},"source":{"0876591f":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n%config IPCompleter.greedy=True\nimport statistics\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","a83dbeb4":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","d4c9a390":"df = pd.read_csv('..\/input\/telecom_churn_data.csv')","fbab3b12":"df.info()","be441ff0":"df.head()","83724f05":"df.isnull().sum()","7a8a1182":"avg_rech = (df.total_rech_amt_6 + df.total_rech_amt_7) \/ 2\ndf = df.loc[avg_rech > avg_rech.quantile(.7)]\ndf.info()","13b06b4b":"df = df.drop(df.loc[:,list((100*(df.isnull().sum()\/len(df.index)) > 60))].columns, 1)","420a9c8b":"df.info()","8c5566a9":"df = df.loc[df.isnull().sum(axis=1) < 5]","e6b5ca69":"df.isnull().sum()","69757dbf":"df.info()","68c1a867":"df = df.drop(['last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8', 'last_date_of_month_9'], axis=1)","8ae3bbbc":"df = df.drop(['std_og_t2c_mou_6', 'std_og_t2c_mou_7', 'std_og_t2c_mou_8', 'std_og_t2c_mou_9'], axis=1)","6f2a2c84":"df = df.drop(['loc_og_t2o_mou', 'std_og_t2o_mou', 'loc_ic_t2o_mou'], axis=1)","6914362b":"df = df.drop(['mobile_number', 'circle_id'], axis=1)","e24c4e7b":"df = df.drop(['std_ic_t2o_mou_6', 'std_ic_t2o_mou_7', 'std_ic_t2o_mou_8', 'std_ic_t2o_mou_9'], axis=1)","28222821":"df = df.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8', 'date_of_last_rech_9'], axis=1)","0213ac77":"df = df.drop(['last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8', 'last_day_rch_amt_9'], axis=1)","7166bfba":"df['churn'] = df['total_ic_mou_9'] + df['total_og_mou_9'] + df['vol_2g_mb_9'] + df['vol_3g_mb_9']","12fcd7d1":"df['churn'] = df['churn'].apply(lambda x: 0 if x > 0 else 1)","29db597e":"df['churn'].value_counts()","8f032cba":"churns = df.loc[df['churn'] == 1]\nnonchurns = df.loc[df['churn'] == 0]\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plotFig(feature, df, str):\n    f, ax = plt.subplots(figsize=(10, 3))\n    sns.distplot(df[feature + '_6'], hist=False, label=feature + '_6')\n    sns.distplot(df[feature + '_7'], hist=False, label=feature + '_7')\n    sns.distplot(df[feature + '_8'], hist=False, label=feature + '_8')\n    sns.distplot(df[feature + '_9'], hist=False, label=feature + '_9')\n    ax.set(xlabel=(feature + '_' + str))\n    plt.legend();","f4e347d2":"# ic mou plots for churns\nplotFig('loc_ic_mou', churns, 'churn')\nplotFig('std_ic_mou', churns, 'churn')\nplotFig('isd_ic_mou', churns, 'churn')\nplotFig('total_ic_mou', churns, 'churn')\n\n# og plots mou for churns\nplotFig('loc_og_mou', churns, 'churn')\nplotFig('std_og_mou', churns, 'churn')\nplotFig('isd_og_mou', churns, 'churn')\nplotFig('total_og_mou', churns, 'churn')","a66fb7a3":"# ic plots for non-churns\nplotFig('loc_ic_mou', nonchurns, 'non_churn')\nplotFig('std_ic_mou', nonchurns, 'non_churn')\nplotFig('isd_ic_mou', nonchurns, 'non_churn')\nplotFig('total_ic_mou', nonchurns, 'non_churn')\n\n# og plots for non-churns\nplotFig('loc_og_mou', nonchurns, 'non_churn')\nplotFig('std_og_mou', nonchurns, 'non_churn')\nplotFig('isd_og_mou', nonchurns, 'non_churn')\nplotFig('total_og_mou', nonchurns, 'non_churn')","ec1963a0":"plotFig('vol_2g_mb', churns, 'churn')\nplotFig('vol_3g_mb', churns, 'churn')\nplotFig('monthly_2g', churns, 'churn')\nplotFig('monthly_3g', churns, 'churn')\nplotFig('sachet_2g', churns, 'churn')\nplotFig('sachet_3g', churns, 'churn')","a30d2e7a":"plotFig('vol_2g_mb', nonchurns, 'non_churn')\nplotFig('vol_3g_mb', nonchurns, 'non_churn')\nplotFig('monthly_2g', nonchurns, 'non_churn')\nplotFig('monthly_3g', nonchurns, 'non_churn')\nplotFig('sachet_2g', nonchurns, 'non_churn')\nplotFig('sachet_3g', nonchurns, 'non_churn')","45d8628a":"plotFig('total_rech_num', churns, 'churn')\nplotFig('total_rech_amt', churns, 'churn')\nplotFig('max_rech_amt', churns, 'churn')","1d6773d7":"plotFig('total_rech_num', nonchurns, 'non_churn')\nplotFig('total_rech_amt', nonchurns, 'non_churn')\nplotFig('max_rech_amt', nonchurns, 'non_churn')","25f50f7a":"plotFig('arpu', churns, 'churn')\nplotFig('arpu', nonchurns, 'non_churn')","b88b2a8f":"def createDerivedFeature(df, feature):\n    newFeature = 'delta_' + feature\n    \n    feature6 = feature + '_6'\n    feature7 = feature + '_7'\n    feature8 = feature + '_8'\n    \n    feature_avg = df[[feature6, feature7, feature8]].mean(axis=1)\n    df[newFeature] = ((df[feature8] - ((df[feature6] + df[feature7]) \/ 2)) \/ feature_avg)\n                   \n    df.drop([feature6, feature7, feature8], axis=1, inplace=True)","6c9b3381":"# Incoming\ncreateDerivedFeature(df, 'loc_ic_t2t_mou')\ncreateDerivedFeature(df, 'loc_ic_t2m_mou')\ncreateDerivedFeature(df, 'loc_ic_t2f_mou')\ncreateDerivedFeature(df, 'loc_ic_mou')\n\ncreateDerivedFeature(df, 'std_ic_t2t_mou')\ncreateDerivedFeature(df, 'std_ic_t2m_mou')\ncreateDerivedFeature(df, 'std_ic_t2f_mou')\ncreateDerivedFeature(df, 'std_ic_mou')\n\ncreateDerivedFeature(df, 'spl_ic_mou')\ncreateDerivedFeature(df, 'isd_ic_mou')\ncreateDerivedFeature(df, 'ic_others')\ncreateDerivedFeature(df, 'roam_ic_mou')\n\ncreateDerivedFeature(df, 'total_ic_mou')\n\n# Outgoing\ncreateDerivedFeature(df, 'loc_og_t2t_mou')\ncreateDerivedFeature(df, 'loc_og_t2m_mou')\ncreateDerivedFeature(df, 'loc_og_t2f_mou')\ncreateDerivedFeature(df, 'loc_og_t2c_mou')\ncreateDerivedFeature(df, 'loc_og_mou')\n\ncreateDerivedFeature(df, 'std_og_t2t_mou')\ncreateDerivedFeature(df, 'std_og_t2m_mou')\ncreateDerivedFeature(df, 'std_og_t2f_mou')\ncreateDerivedFeature(df, 'std_og_mou')\n\ncreateDerivedFeature(df, 'spl_og_mou')\ncreateDerivedFeature(df, 'isd_og_mou')\ncreateDerivedFeature(df, 'og_others')\ncreateDerivedFeature(df, 'roam_og_mou')\n\ncreateDerivedFeature(df, 'total_og_mou')\n\ncreateDerivedFeature(df, 'onnet_mou')\ncreateDerivedFeature(df, 'offnet_mou')\n\n# 2G and 3G\ncreateDerivedFeature(df, 'vol_2g_mb')\ncreateDerivedFeature(df, 'vol_3g_mb')\ncreateDerivedFeature(df, 'monthly_2g')\ncreateDerivedFeature(df, 'monthly_3g')\ncreateDerivedFeature(df, 'sachet_2g')\ncreateDerivedFeature(df, 'sachet_3g')\n\n# Recharge and revenure\ncreateDerivedFeature(df, 'total_rech_num')\ncreateDerivedFeature(df, 'max_rech_amt')\ncreateDerivedFeature(df, 'total_rech_amt')\ncreateDerivedFeature(df, 'arpu')\n\ndf['vbc_3g_6'] = df['jun_vbc_3g']\ndf['vbc_3g_7'] = df['jul_vbc_3g']\ndf['vbc_3g_8'] = df['aug_vbc_3g']\ncreateDerivedFeature(df, 'vbc_3g')\ndf.drop(['jun_vbc_3g', 'jul_vbc_3g', 'aug_vbc_3g'], axis=1, inplace=True)","0ca9e69d":"df = df.fillna(0)","e2019d2d":"cols = df.columns\ncols = [i for i in cols if '_9'  in i] \ncols.append('sep_vbc_3g')\ndf = df.drop(cols, axis=1)","9eff84f9":"df.churn.value_counts()","e993addc":"from imblearn.over_sampling import SMOTE\ny = df.pop('churn')\nX = df","a00094ee":"# Keep the ratio of minority : majority = 0.8\nsmote = SMOTE(ratio=0.8, random_state=42)\nX_blncd, y = smote.fit_sample(X, y)\n\n# Check the new ratio. \nX = pd.DataFrame(X_blncd , columns=X.columns)\nunique, counts = np.unique(y, return_counts=True)\nprint(np.asarray((unique, counts)).T)","6874cec2":"from sklearn.feature_selection import RFE\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.linear_model import LogisticRegression","487eac07":"# Running RFE with the output number of the variable equal to 20\nlogreg = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20)             \nrfe = rfe.fit(X, y)","aa198686":"col = X.columns[rfe.support_]\ncol","878f8d1f":"plt.figure(figsize = (16,10)) \nsns.heatmap(X[col].corr(),annot = True)","c276f76c":"rfe_df = X[col]","84a035f3":"rfe_df.drop(['delta_loc_ic_t2m_mou',\n             'delta_loc_og_t2m_mou',\n             'delta_loc_og_t2t_mou',\n             'delta_loc_ic_t2t_mou',\n             'delta_roam_og_mou',\n             'delta_offnet_mou',\n             'delta_onnet_mou',\n             'delta_arpu',\n             'delta_max_rech_amt',\n             'delta_total_rech_amt',\n             'delta_total_rech_num'], axis=1, inplace=True)","8a7ac1d1":"plt.figure(figsize = (16,10)) \nsns.heatmap(rfe_df.corr(),annot = True)","fafb248a":"rfe_df.columns","238f6459":"from sklearn import preprocessing\nX_scaler = preprocessing.StandardScaler().fit(X)\nX = X_scaler.transform(X) ","193db508":"from sklearn.decomposition import PCA\n\npca = PCA(svd_solver='randomized', random_state=101)\npca.fit(X)\n\nfig = plt.figure(figsize = (8,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","eea0abfe":"pca = PCA(n_components=20,random_state=100)\n\n# fit_transform and transform to get the reduced data\nX = pca.fit_transform(X)","dd0141f5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7 ,test_size = 0.3, random_state=100)","fa26e858":"cols=['a', 'b', 'c', 'd', 'e', 'f',\n         'g', 'h', 'i', 'j', 'k', 'l',\n         'm', 'n', 'o', 'p', 'q', 'r',\n         's', 't']\nX_train = pd.DataFrame(X_train, columns=cols)\nX_test = pd.DataFrame(X_test, columns=cols)","7f719be7":"X_train_sm = sm.add_constant(X_train)\nlogm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nprint(res.summary())","7ed95ec2":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nprint(vif)","c5016dc4":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)","8ea9cf4f":"y_train_pred_final = pd.DataFrame({'Churn':y_train, 'Churn_Prob':y_train_pred})\ny_train_pred_final.head()","be2c9c4e":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","5fd12cf5":"from sklearn import metrics","b88794c8":"confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","24a36976":"def calcSensi(confusion):\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n\n    print(TP \/ float(TP+FN))\n    \ndef calcAccuracy(confusion):\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n\n    print((TP + TN) \/ float(TP + TN + FP + FN))","e48165cb":"calcSensi(confusion)","2ad267e3":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","6389b0f0":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )","77034e8f":"draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","3a4d08a0":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","8c05b1d0":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","9dbcca61":"X_test_sm = sm.add_constant(X_test)","d24f4c1b":"y_test_pred = res.predict(X_test_sm)","2d5f2156":"y_test_pred = pd.DataFrame(y_test_pred)\ny_test_pred.reset_index(drop=True, inplace=True)\ny_test_pred = y_test_pred.rename(columns={ 0 : 'Churn_Prob'})\ny_test_pred.head()","392e3dfb":"y_test = pd.DataFrame(y_test)\ny_test.reset_index(drop=True, inplace=True)\ny_test= y_test.rename(columns={ 0 : 'Churn'})\ny_test.head()","6ed772db":"y_pred_final = pd.concat([y_test, y_test_pred],axis=1)","f6775438":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.350 else 0)","449ee17e":"metrics.accuracy_score(y_pred_final.Churn, y_pred_final.final_predicted)","85a2f51c":"confusion = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion","0d5f0ebe":"calcSensi(confusion)","2bf7c31a":"from sklearn.svm import SVC\nsvm = SVC(kernel='rbf', C=10, gamma=0.1)\nsvm.fit(X_train, y_train)","ee903542":"y_test_pred = svm.predict(X_test)","c38496fd":"confusion = metrics.confusion_matrix(y_test, y_test_pred )\nconfusion","3801572d":"calcAccuracy(confusion)","f1dbea67":"calcSensi(confusion)","b594f942":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","0ff87ad2":"n_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(1, 20)}\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\n\ntree.fit(X_train, y_train)","5fe93b85":"scores = tree.cv_results_\npd.DataFrame(scores).head()","eae6a873":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","bfb6b10e":"dtree = DecisionTreeClassifier(criterion = \"gini\", max_depth=3)\ndtree.fit(X_train, y_train)\nprint((dtree.score(X, y))*100)","d170e43a":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","f125bfd0":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","ea5012e2":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","d07cde71":"# tree with max_depth = 3\ntree = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=10, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\ntree.fit(X_train, y_train)","b136f469":"from sklearn.metrics import classification_report,confusion_matrix\ny_pred = tree.predict(X_test)\nprint(classification_report(y_test, y_pred))","6e3819b4":"# confusion matrix\nprint(confusion_matrix(y_test,y_pred))","174cf838":"calcSensi(confusion_matrix(y_test,y_pred))","545ed9d3":"calcAccuracy(confusion_matrix(y_test,y_pred))","36b1d997":"from sklearn.ensemble import RandomForestClassifier\n\n# number of CV folds\nn_folds = 5\n\n# parameters to build the model on\n# max_depth - [0-100]\n# n_extimators - 20\nparameters = {'max_depth': range(1, 15)}\ndtree = RandomForestClassifier(criterion = \"gini\",\n                               n_estimators= 20,\n                               random_state = 100)\n\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","09fbaf7e":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","74f37f0d":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","11dd0397":"from sklearn.ensemble import RandomForestClassifier\n\n# number of CV folds\nn_folds = 5\n\n# parameters to build the model on\n# min_sample_leaf - [5-200]\n# n_extimators - 20\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\ndtree = RandomForestClassifier(criterion = \"gini\",\n                               n_estimators= 20,\n                               random_state = 100)\n\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","31c2308b":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","06abb960":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","78b5b48b":"dtree = RandomForestClassifier(criterion = \"gini\",\n                               n_estimators= 200,\n                               max_depth=12,\n                               min_samples_leaf=50,\n                               min_samples_split=50)\ndtree.fit(X_train, y_train)","32c575cd":"from sklearn.metrics import classification_report,confusion_matrix\ny_pred = dtree.predict(X_test)\nprint(classification_report(y_test, y_pred))","57f67bcf":"# confusion matrix\nprint(confusion_matrix(y_test,y_pred))","203bda1a":"calcSensi(confusion_matrix(y_test,y_pred))","0b483578":"calcAccuracy(confusion_matrix(y_test,y_pred))","9a9cc664":"# 6. Model generation","fda24eea":"# 1. Load data","d53d3ed1":"# 3. Data Analysis","f4414766":"There are some features (f) for which mean is zero (the value across three months is zero), and hence the **delta_f** comes out to be **NaN**. Replace those with zero(0).","ffa95111":"#### 6.5.3 Calculate accuracy and sensitivity","a85b6ae6":"| Algorithm   | Accuracy | Sensitivity |\n| ----------- | ----------- |-----------|\n| Logistic Regression | 0.84     | 0.88       |\n| SVM        | 0.97     | 0.99       |\n| Decision Tree | 0.85 | 0.88 |\n| Random Forest | 0.89 | 0.90 |","74ac1793":"At low values of min_samples_leaf, the tree gets a bit overfitted. At values > 100, however, the model becomes more stable and the training and test accuracy start to converge. 50 is a middle ground. ","395b3390":"# Telecome churn case study","7ecf7662":"### 6.6 Decision Tree","ae64f0ce":"#### 6.4.1 Fit the model","2b963079":"#### 3.1.1 Incoming and outgoing mou analysis","19229461":"#### 6.4.7 Calculate accuracy and sensitivity on test data","29d05a5d":"### 2.2 Identify high value customers\nThe high value customers are those customers who are above 0.70 quantile of the average recharge amount for first **two months**. The below snipper identifies such customers.\n\nTotal # of high value customers = **29979**","cd501555":"#### 3.1.4 ARPU analysis","88ef3a1f":"#### 6.6.2 Tuning min_samples_leaf","da98b936":"Again, from the above graphs, it can be inferred that for churned users, the ARPU steadily decreasing. But for the non-churners, the distributions are stable across the first three months.","59101f7d":"#### Choose cut-off to be 0.5 for now","b84ae634":"# 8. Recommendation","f96d5d9d":"In the above graph, there is a rapid increase in the accuracy of test data till max_depth = 10. After that it gets stable till max_depth=17.\n\nBut in order to keep the model simple, max_depth=10 is a good choice.","703a40af":"#### 6.6.1 Find the right hyperparams\nLet's start with max_depth","63cd4405":"###  3.2 Create derived columns","c2897c16":"The above graphs for churned customers clearly suggest that there has been drop in incoming and outgoing calls MOU across 3 months.\n\nLet's look at the same features for non-churners.","fbc5725a":"One of the asks in the case study is to identify the features which are responsible for identifying if a customer will churn or not. \n\nDue to the high number of features, we will be using **RFE** to select top 20 driving feartures fot this data.","2ea6c1f2":"After removeing such columns, we are still left with **186 columns**. Let's see if we can reduce the features even further.","6afcbc41":"# 5. Identify the relavant features and provide recommendations ","8f37356b":"# 4. Handle class imbalance\n\nThere is a ridiculously high class imbalance. If we go ahead and build models on with such imbalanced data, then the model will also be biased. \n\nTo avoid this, we will do over-sampling to increase the count to minority label. We will be using **SMOTE** technique to improve the class balance.\n","572aad16":"### 2.4 Remove those rows who have > 5 NaNs\nThere are still some rows with null values. Remove them gracefully.","79c34be3":"#### 6.5.1 Fit the model\nSVM is very slow on this data.\nRunning GridSearch takes a lot of time. \nBy trying out individual C and gamma, the best accuracy and sensitivity was scored by **C=10, gamma = 0.1**\n\nTried values :\n- C - [ 0.1, 1, 10 ]\n- gamma - [ 0.001, 0.01, 0.1, 1 ]","507d0334":"The above graph are for non-churns. Clearly, the 2g\/3g vols are stable across the months.","34339a59":"**The given data has 99999 rows and 226 features. Let's look at the first few rows to get an idea.**","35198838":"We used four algorithms to generate models. The summary is as follows.","cc06e8a8":"The sensitivity for the 0.5 cut-off is 0.83. Let's look at ROC curve.","efe24b2e":"#### 6.7.1 Tune hyperparametres\nStart with max_depth","761ec6e0":"#### 6.7.5 Calculate accuracy and sensitivity","bb73ebc6":"### 2.6 Calculate Churn label\nOnce the data has been cleaned and high value customers have been identified, we can now calculate the **Churn** label for each row.\n\nA customer is going is churning if his ic\/og is zero and 2g\/3g vol is zero.","62d41dce":"#### 3.1.2 2G & 3G volume analysis","fcb634a5":"#### 6.4.4 Plotting the ROC Curve","514a073e":"#### Again, from the above graphs, it can be inferred that for churned users, the rech_num  and rech_amt are steadily decreasing. But for the non-churners, the distributions hardly vary across the first three months.","8ff382d5":"### 6.5 SVM","fa823d46":"At low values of min_samples_leaf, the tree gets a bit overfitted. At values > 100, however, the model becomes more stable and the training and test accuracy start to converge. 50 is a middle ground.","429d28b7":"# 2. Prepare data","910ea251":"#### 8.2 Business insights\nIf we look closely, then the two strong indicators groups for the churners are decline in ic\/og calls and 2g\/3g volume. Thus, the user is planning to switch to other telecom provider. \n\nThe most common reasons why a user switches the provider are -\n- Bad service of the current provider.\n    - The population of the area might have increased.\n    - The user might have moved to another location where service is not good.\n    - A competitor with better service might have launched in his area.\n- Costly tariff\n    - The competitior might have reduced their tariff.\n\n","a0e28663":"### 6.3 Split test\/train","f9fcb239":"The above graphs for churned customers clearly suggest that there has been drop in i2g\/36 volumes across 3 months.\n\nLet's look at the same featurss for non-churners.","1d7f1b64":"#### 6.4.3 Predict the test data","5aa297e4":"--> Ends here <--","aa20505b":"### 6.2 Feature reduction using PCA\nSince there are too many features, it will be useful to reduuce them so that the analysis is fast and efective. We will lose the reasoning behind the churn cause though.","bbacac27":"#### 6.6.4 Calculate accuracy and sensitivity","de9c2bec":"The above graph are for non-churns. Clearly, the ic\/og MOUs are stable across the months.","20eaf01b":"### 2.5 Drop irrelavant cols\nDrop those rows who have only single values","d73bdab8":"### 2.3 Remove those columns for which for than 60% of data is missing","4d1390db":"#### Scree plot\nGoing ahead with 20 components as they are able to explain 88% of variance. If the accuracy of models is less going forward, will increase the components.","4ec37f6d":"#### 6.7.4 Predict the test data","ffd87b26":"#### 8.1 We did RFE in an earlier step to determine strong indicators for churn.  From the analysis, we can make following conclusion.\n\nif a customer is seen to perform following actions, then it might be a strong indicator for them to about to churn very soon.\n- If they are decreasing their\n    - local incoming t2t\/t2m mou\n    - local outgoing t2t\/t2m mou\n    - std incoming t2t mou\n    - roaming incoming mou\n    - roaming outgoing mou\n    - offnet\/onnet mou\n    - 3g volume\n    - 3g\/2g sachet\n    - total recharge amount\n    - max recharge amount\n    \nThe telecom company should watch out for these signs and take actions before the customer churns.","9f9ad9d6":"#### 6.4.2 Verify the VIF values.","c9cca643":"### 6.1 Scaling ","a39f7047":"# 7. Model evaluation and conclusion","88bc03ec":"### 6.7 Random forest","d22cc8fe":"### 3.3 Remove _9 columns","75d7004c":"#### From the above graph, it can be inferred that for churned users, the **MOUs** and **VOLs** are steadily decreasing. But for the non-churners, the distributions hardly vary across the first three months.","77b161de":"#### 6.4.5 Find optimal cut-off","16ce7b72":"##### 0.35 looks like a good cut-off for high sensitivity and significant accuracy","b3f571ca":"#### 6.4.6 Prection on test data","3dffa894":"#### Hence the most significant features are :\n- delta_std_ic_t2t_mou\n- delta_roam_ic_mou\n- delta_total_ic_mou\n- delta_std_og_t2t_mou\n- delta_og_others\n- delta_vol_3g_mb\n- delta_sachet_2g\n- delta_sachet_3g\n- delta_vbc_3g\n\n\nAll of these features are derived features. The name **delta_feature** tells the magnitude of change in the **feature** in first three months. (Note that other features that have been removed are also important. It's just that they are collinear with this above features, so they have been removed.)","ff749ae6":"#### 6.5.2 Predict the test data","d2bd26b8":"### 3.1 Analyze churn data vs non-churn data","d0c0bb8f":"### 2.1 Check for null values\nLooks like there are lots of columns with null values.","d3b9a61c":"In the above graph, there is a rapid increase in the accuracy of test data till max_depth = 12. After that it gets stable.\n\nBut in order to keep the model simple, **max_depth = 12 is a good choice**.","3497c025":"#### 6.7.2 Tune min_samples_leaf","eddda7ec":"#### 6.7.3 Fit the model","9186c117":"### Note - In our case study, we do not want to lose the high value customers. So, our aim is to identify them correctly. Hence, the model should have high sensitivity.","80d9f354":"Some of these columns can be highly related. Let's remove such columns.","1b247ac5":"#### 3.1.3 Recharge amount analysis","78c1cc40":"### 6.4 Logistic regression","8e26a2f5":"From the above graphs, seems like the users who are going to churn have monotonically decreasing ic\/og mou and 2g\/3g data volumes. To capture the decreasing trends across the first three months, we can derive new features as follows.\n\n**delta_f = (f_8 - mean(f_6, f_7)) \/ mean(f_6, f_7, f_8)**\n\nwhere, **f** is a feature, such as *loc_ic_mou*\n\nFor example,\n\n*delta_loc_ic_mou = \n   (loc_ic_mou_8 - mean(loc_ic_mou_6, loc_ic_mou_7)) \/ mean(loc_ic_mou_6, loc_ic_mou_7, loc_ic_mou_8)*\n   \nBy this formula, we are trying to capture the change that is happening in 8th month w.r.t. 6th and 7th months.","f2d068bb":"For our case, the SVM model we created works the best because it has high **sensitivity** w.r.t. to other models.","33a7ccf6":"#### 6.6.3 Let's try to create the model using the hyperparametres."}}