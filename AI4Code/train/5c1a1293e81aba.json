{"cell_type":{"5a8975de":"code","114f1461":"code","e8372c7c":"code","987b497a":"code","8e8539dc":"code","62d80b52":"code","44c2ac2d":"code","d3e7ac15":"code","61d3af54":"code","0e73fce9":"code","f023a4d6":"code","3f4730ce":"code","851047f5":"code","1f74d749":"code","ccc55c45":"code","9237ef6c":"code","d8e905b9":"code","d6e4f0d0":"code","f25abbdf":"code","03cf7d47":"code","49db6699":"markdown","86ac157f":"markdown","4f123738":"markdown","87b25fa6":"markdown","7e8faefd":"markdown","7d384088":"markdown","883eabb6":"markdown","941a8d9a":"markdown","3f8e3151":"markdown","e2a9380a":"markdown","ac797118":"markdown","1e77d9c2":"markdown","385ed61a":"markdown","fcce92dd":"markdown","a238a527":"markdown","b48a8af7":"markdown","faa66c4e":"markdown","535a86ed":"markdown","dcffcaf8":"markdown","7027c042":"markdown","c8604b54":"markdown","298e4110":"markdown","c69f6b56":"markdown","a7635605":"markdown","aeb9b35d":"markdown","db6355ba":"markdown","185d42f4":"markdown","9ad365cd":"markdown","f6e9fc29":"markdown","f0f88768":"markdown","6af350ed":"markdown"},"source":{"5a8975de":"from IPython.display import Image\nImage(\"KNN-Algorithm-using-Python-1.png\")","114f1461":"from IPython.display import Image\nImage(\"class_prediction.jpg\")","e8372c7c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","987b497a":"# read csv (comma separated value) into data\ndata = pd.read_csv('column_2C_weka.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('_classic_test')\n","8e8539dc":"data.head()","62d80b52":"data.shape","44c2ac2d":"print('No of columns in the dataset:',data.columns.size)\nprint(\"Name of Columns:\\n\",data.columns.values)","d3e7ac15":"data.info()","61d3af54":"data.describe()","0e73fce9":"sns.pairplot(data,hue=\"class\",palette=\"Set2\")\nplt.show()","f023a4d6":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","3f4730ce":"data.loc[:,'class'].value_counts()","851047f5":"sns.countplot(x=\"class\", data=data)\n","1f74d749":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","ccc55c45":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","9237ef6c":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('Value VS Accuracy',fontsize=20)\nplt.xlabel('Number of Neighbors',fontsize=20)\nplt.ylabel('Accuracy',fontsize=20)\nplt.xticks(neig)\nplt.grid()\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","d8e905b9":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,25)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x,y)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))\n","d6e4f0d0":"from sklearn.neighbors import KNeighborsClassifier\ngrid = {'n_neighbors': np.arange(1,25)}\nknn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x_train, y_train)\n\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn_cv.score(x_train, y_train)))\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn_cv.score(x_test, y_test)))\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))\n","f25abbdf":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=12, p=6, metric='minkowski')\nknn.fit(x_train, y_train)\n\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn.score(x_train, y_train)))\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn.score(x_test, y_test)))\n","03cf7d47":"from sklearn.neighbors import KNeighborsClassifier\ntrain_score_knn=[]\ntest_score_knn=[]\nfor i in range(1,10):\n    knn = KNeighborsClassifier(n_neighbors=12, p=i, metric='minkowski')\n    knn.fit(x_train, y_train)\n    train_score_knn.append(knn.score(x_train, y_train))\n    test_score_knn.append(knn.score(x_test, y_test))\n    print('For p value=',i)\n    print('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn.score(x_train, y_train)))\n    print('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn.score(x_test, y_test)))\n    print()\n","49db6699":" - KNN is a simple yet powerful classification algorithm.\n - It requires no training for making predictions, which is typically one of the most difficult parts of a machine learning algorithm.\n - The KNN algorithm have been widely used to find document similarity and pattern recognition","86ac157f":"# Classification and Regression Tree (CART)\n\n# KNN ","4f123738":"## KNN's Idea:- ","87b25fa6":"## Okay, scatter matrix there are relations between each feature but how many normal(green) and abnormal(red) classes are there.","7e8faefd":"#### ROWS AND COLUMNS","7d384088":"#### Class Distributuon","883eabb6":"##  Split our data train and test sets.\n\n    train: use train set by fitting\n    test: make prediction on test set.\n    With train and test sets, fitted data and tested data are completely different\n    train_test_split(x,y,test_size = 0.3,random_state = 1)\n    x: features\n    y: target variables (normal,abnormal)\n    test_size: percentage of test size. Example test_size = 0.3, test size = 30% and train size = 70%\n    random_state: sets a seed. If this seed is same number, train_test_split() produce exact same split at each time\n    fit(x_train,y_train): fit on train sets\n    score(x_test,y_test)): predict and give accuracy on test sets","941a8d9a":"## About Data :-\n    The data have been organized in two different but related classification tasks.\n\n    column_3C_weka.csv (file with three class labels)\n\n    The first task consists in classifying patients as belonging to one out of three categories: Normal (100 patients), Disk Hernia (60 patients) or Spondylolisthesis (150 patients).\n    \n    column_2C_weka.csv (file with two class labels)\n\n    For the second task, the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus, the second task consists in classifying patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients).","3f8e3151":"#### CASE 2:-","e2a9380a":"#### Summary of the Data","ac797118":"  - KNN: Look at the K closest labeled data points\n  - Classification method.\n  - First we need to train our data. Train = fit\n  - fit(): fits the data, train the data.\n  - predict(): predicts the data \n  - x: features\n  - y: target variables(normal, abnormal)\n  - n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points","1e77d9c2":"## SUPERVISED LEARNING\n    Supervised learning: It uses data that has labels. Example, there are orthopedic patients data that have labels normal and abnormal.\n    There are features(predictor variable) and target variable. Features are like pelvic radius or sacral slope\n    Target variables are labels normal and abnormal\n    Aim is that as given features(input) predict whether target variable(output) is normal or abnormal\n    Classification: target variable consists of categories like normal or abnormal\n    Regression: target variable is continious like stock market\n   \n","385ed61a":"#### CASE 1:-","fcce92dd":"#### Scatter Matrix on the Data to find the relations among Features","a238a527":"#### CASE 4:-","b48a8af7":"#### TUNING AND HYPERPARAMETERS","faa66c4e":"## Measuring model performance:\n    Accuracy which is fraction of correct predictions is commonly used metric.","535a86ed":"#### To check about Numerics\/Categorical Columns","dcffcaf8":"## Pros and Cons of KNN\n\n\n## Pros\n- It is extremely easy to implement\n- It is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc.\n- Since the algorithm requires no training before making predictions, new data can be added seamlessly.\n- There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n\n## Cons\n- The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension.\n- The KNN algorithm has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher.\n- The KNN algorithm doesn't work well with categorical features since it is difficult to find the distance between dimensions with categorical features.","7027c042":"## Step 1:- Imports","c8604b54":"## STEP 2:-EXPLORATORY DATA ANALYSIS (EDA)","298e4110":"- Features are pelvic_incidence, pelvic_tilt numeric, lumbar_lordosis_angle, sacral_slope, pelvic_radius and degree_spondylolisthesis\n- Target variable  is class","c69f6b56":"##### pd.plotting.scatter_matrix:\n\n    green: normal and red: abnormal\n    c: color\n    figsize: figure size\n    diagonal: histogram of each features\n    alpha: opacity\n    s: size of marker\n    marker: marker type","a7635605":"## Field Descriptions:\n\n    Each patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):\n\n    pelvic incidence\n    pelvic tilt\n    lumbar lordosis angle\n    sacral slope\n    pelvic radius\n    grade of spondylolisthesis","aeb9b35d":"## Problem Statement :- Data for classifying patients based on two classes\n\n## Biomechanical features of orthopedic patients\n\n## Columns:-\n    pelvic_incidence\n    pelvic_tilt numeric\n    lumbar_lordosis_angle\n    sacral_slope\n    pelvic_radius\n    degree_spondylolisthesis\n    class","db6355ba":"#### KNN Invoking","185d42f4":"## Model complexity:\n\n    K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performance.\n    Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n    If k is big, model that is less complex model can lead to underfit.\n    At below, I range K value from 1 to 25(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 18, model is lead to underfit. Again accuracy is not enough. However look at when K is 18(best performance), accuracy has highest value almost 88%.","9ad365cd":"#### CASE 3:-","f6e9fc29":" - length: 310 (range index)\n - Features are float\n - Target variables are object that is like string","f0f88768":"- The intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. \n- It simply calculates the distance of a new data point to all other training data points.\n- The distance can be of any type e.g Euclidean or Manhattan etc.\n- It then selects the K-nearest data points, where K can be any integer. \n- Finally it assigns the data point to the class to which the majority of the K data points belong.","6af350ed":"#### Pairplot on the data"}}