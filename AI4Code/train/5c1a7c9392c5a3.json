{"cell_type":{"8a683e5b":"code","1da70145":"code","02e7d154":"code","f19898f2":"code","3737e090":"code","6aed905d":"code","3a7d4587":"code","4a20dea1":"code","eb4101f7":"code","956750f6":"code","d1f3aab4":"code","1e0dad15":"code","83826bcd":"code","81fb5f6f":"code","00c81fcb":"code","f34d1d0a":"code","32ae1a43":"code","859368ff":"code","a310251e":"code","b21f5529":"code","0e43c53d":"code","79fb0647":"markdown","abc0093f":"markdown","4d8aee0f":"markdown","d6fd28a8":"markdown"},"source":{"8a683e5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport datatable as dt  # pip install datatable\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom matplotlib.lines import Line2D\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import precision_recall_curve\n\n\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# import warnings\n# warnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('..\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1da70145":"%%time\n# Read the data\ntrain = dt.fread(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\").to_pandas().set_index(\"id\")\ntest = dt.fread(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\").to_pandas().set_index(\"id\")\n","02e7d154":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","f19898f2":"train = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)","3737e090":"print(\"(train, test) na --> \",(train.isna().sum().sum(), test.isna().sum().sum()))","6aed905d":"is_na_train_df = train.drop(columns=\"claim\").isna().sum(axis = 1)\nprint(is_na_train_df.shape)\n\nis_na_test_df = test.isna().sum(axis = 1)\nprint(is_na_test_df.shape)","3a7d4587":"is_na_train_sum = train.drop(columns=\"claim\").isna().sum(axis=1)\nprint(is_na_train_sum.shape)\n\nis_na_test_sum = test.isna().sum(axis=1)\nprint(is_na_test_sum.shape)","4a20dea1":"# from https:\/\/www.kaggle.com\/sgiuri\/sep21tp-na-feature-importance \nna_fi = ['f74', 'f91', 'f107', 'f53', 'f7', 'f12', 'f48', 'f19', 'f100', 'f69', 'f18', 'f112', 'f66', 'f17', 'f113']\nis_na_train_df = train.drop(columns=\"claim\").isna().loc[:,na_fi].astype(int)\nprint(is_na_train_df.shape)\n\nis_na_test_df = test.isna().loc[:,na_fi].astype(int)\nprint(is_na_test_df.shape)","eb4101f7":"train = train.join(is_na_train_df, rsuffix='_isNa')\ntest = test.join(is_na_test_df, rsuffix='_isNa')","956750f6":"train[\"isNA\"] =is_na_train_sum\nprint(train.shape)\ntest[\"isNA\"] = is_na_test_sum\nprint(test.shape)\n\n# train = train.head(1000)\n# test = test.head(1000)\n# if len(train == 1000):\n#     print(\"Train test reduced to increase speed in try and test \")\n    ","d1f3aab4":"x_Mm_scaler = MinMaxScaler()\nX = pd.DataFrame(x_Mm_scaler.fit_transform(train.drop(\"claim\", axis=1)),\n                 columns=train.drop(\"claim\", axis=1).columns)\ny = train.claim.astype(int)\nX_test = pd.DataFrame(x_Mm_scaler.transform(test), columns=test.columns)","1e0dad15":"imputer_zeros = SimpleImputer(strategy=\"median\")\nX = pd.DataFrame(imputer_zeros.fit_transform(train.drop(\"claim\", axis=1)),\n                 columns=train.drop(\"claim\", axis=1).columns)\nX_test = pd.DataFrame(imputer_zeros.transform(test), columns=test.columns)\nX = pd.DataFrame(x_Mm_scaler.fit_transform(X),\n                 columns=train.drop(\"claim\", axis=1).columns)\nX_test = pd.DataFrame(x_Mm_scaler.transform(X_test), columns=test.columns)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))","83826bcd":"# X = reduce_memory_usage(X, verbose=True)\n# X_test = reduce_memory_usage(X_test, verbose=True)","81fb5f6f":"def train_model_optuna_xgb(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n       \n    #A set of hyperparameters to optimize by optuna\n    xgb_params = {\n                 \"n_estimators\": trial.suggest_categorical('n_estimators', [10000]),\n                 \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.8),\n                 \"subsample\": trial.suggest_float('subsample', 0.5, 0.95),\n                 \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.5, 0.95),\n                 \"max_depth\": trial.suggest_int(\"max_depth\", 5, 16),\n                 \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n                 \"tree_method\": trial.suggest_categorical('tree_method', [\"gpu_hist\"]),\n                 \"reg_lambda\": trial.suggest_float('reg_lambda', 2, 100),\n                 \"reg_alpha\": trial.suggest_float('reg_alpha', 1, 50),\n                 \"random_state\": trial.suggest_categorical('random_state', [42]),\n                 \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n                    }\n\n    # Model loading and training\n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    print(f\"Number of boosting rounds: {model.best_iteration}\")\n    oof = model.predict(X_valid)\n    oof[oof<0] = 0\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","00c81fcb":"%%time\nn_trials = 200\n\nskf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\nif len(train)== 1000:\n    n_trials = 2\n\nfor fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n    \n    X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n    y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n\n# Setting optuna verbosity to show only warning messages\n# If the line is uncommeted each iteration results will be shown\noptuna.logging.set_verbosity(optuna.logging.WARNING)\ntime_limit = 3600 * 3\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(lambda trial: train_model_optuna_xgb(trial, \n                                                X_train, \n                                                X_valid,\n                                                y_train, \n                                                y_valid),\n               n_trials = n_trials,\n               timeout=time_limit\n              )\n # Showing optimization results\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)\n","f34d1d0a":"xgb_params = study.best_params\n# xgb_params = {'n_estimators': 10000, \n#               'learning_rate': 0.08625196792060146, \n#               'subsample': 0.5959773829663169, \n#               'colsample_bytree': 0.7603045913120982, \n#               'max_depth': 7, 'booster': 'gbtree', \n#               'tree_method': 'gpu_hist', \n#               'reg_lambda': 74.60593770387143, \n#               'reg_alpha': 33.38858560681472, \n#               'random_state': 42, \n#               'n_jobs': 4}","32ae1a43":"def strati_fit(X, y, X_test, \n               splits=6, random_state=42,\n               model = XGBClassifier(**xgb_params)):\n    \n    splits = splits\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=random_state)\n    oof_preds = np.zeros((X.shape[0],))\n    preds = 0\n    train_preds = 0\n    model_fi = 0\n    total_mean_rmse = 0\n    total_mean_roc_auc_score = 0\n    \n    \n    for fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n\n        X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n        y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n        print(fold, f\"X_train = {X_train.shape} - y_train: {y_train.shape}\")\n        print(fold, f\"X_valid = {X_valid.shape} - y_valid: {y_valid.shape}\")\n        \n        model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"auc\",\n              early_stopping_rounds=100,\n              verbose=False)\n        # print(\"fitted\")\n        preds += (model.predict_proba(X_test))[:,1] \/ splits\n        train_preds += (model.predict_proba(X))[:,1] \/ splits\n        print(train_preds.shape)\n        # print(\"preds ok\")\n        model_fi += model.feature_importances_\n        # print(\"model_fi ok\")\n        oof_preds[valid_indicies] = model.predict_proba(X_valid)[:,1]\n        oof_preds[oof_preds < 0] = 0\n    #     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_indicies]))\n        fold_roc_auc_score = roc_auc_score(y_valid, oof_preds[valid_indicies])\n        \n        print(f\"\/nFold {fold} ROC AUC Score: {fold_roc_auc_score}\")\n        \n        print(f\"Fold {fold} RMSE: {fold_rmse}\")\n        total_mean_rmse += fold_rmse \/ splits\n        total_mean_roc_auc_score += fold_roc_auc_score \/ splits\n    return preds, model_fi, total_mean_rmse, total_mean_roc_auc_score, train_preds\n    ","859368ff":"random_states = [0,3,42,69,666]\npredictions = pd.DataFrame()\npredictions[\"id\"] = test.index\ntrain_predictions = pd.DataFrame()\ntrain_predictions[\"id\"] = X.index\n\nstudy = pd.DataFrame()\n\nfor random_state in random_states:\n    \n    preds, model_fi, total_mean_rmse, total_mean_roc_auc_score, train_preds = strati_fit(X, y, X_test, \n           splits=10, random_state=random_state, model = XGBClassifier(**xgb_params))\n    predictions[random_state] = preds\n    train_predictions[random_state] = train_preds\n    study[random_state] = model_fi, total_mean_rmse, total_mean_roc_auc_score\n","a310251e":"study.to_csv('study.csv', index=False, header=study.columns)\nstudy.head()","b21f5529":"train_predictions.to_csv('train_predictions.csv', index=False, header=train_predictions.columns)\ntrain_predictions.head()","0e43c53d":"predictions.to_csv('submission_pre_ensamble.csv', index=False, header=predictions.columns)\npredictions.head()","79fb0647":"# Libraries and Data import","abc0093f":"Hello everybody.","4d8aee0f":"## Data preparation: Siple Imputer + NA to median","d6fd28a8":"# Memory reducing\ntaken from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro"}}