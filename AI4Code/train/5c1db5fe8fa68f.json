{"cell_type":{"86b94312":"code","29ab068a":"code","d285e127":"code","d62e4eac":"code","4aa7691c":"code","9d1136a7":"code","8237fa2c":"code","cad4f20f":"code","fe207146":"code","9ffd4c50":"code","7063b595":"code","5d1bbbec":"code","093cb824":"code","cd9dce6a":"code","746f7674":"code","fb86c889":"code","80183358":"code","d95a5ea9":"code","c4df71ec":"code","5cd7f840":"code","ef1e14ae":"code","5c6341ce":"code","456d6136":"code","f6a0dd35":"code","cdc2566e":"code","e46e4b00":"code","f9bd8120":"code","4d6adaae":"code","f4e3da05":"code","253479cf":"code","b6299dfa":"code","9f0d6978":"code","720c4cf3":"code","ddf57585":"code","e7325255":"code","4ce01874":"code","f4b5187d":"code","fec4793d":"code","7ac85661":"code","88a0531f":"code","35101984":"code","9ed3acbc":"code","f552a612":"code","3b595e69":"code","651cf629":"code","2e6101e4":"code","8217b6d9":"code","9a00942a":"code","25fdf54c":"code","91d70a85":"code","70cd161c":"code","ff30482e":"code","2b18084f":"code","2d36ac90":"code","7f4c42fc":"code","00e70876":"code","239b080d":"code","9675ec1b":"code","1f879ec0":"code","8e838458":"code","c984d04c":"code","03a2f5a1":"code","ac3d3dbe":"code","8a4f98d5":"code","17bf1cd2":"code","1e07e11d":"code","fd29674f":"code","542e400f":"code","ff709228":"code","974da73e":"code","05a2ebb4":"code","56499a7c":"code","e046a066":"code","53443c3c":"markdown","444520f5":"markdown","94714fce":"markdown","62d42a73":"markdown","e123c80f":"markdown","b2668571":"markdown","cd6d1021":"markdown","638027cc":"markdown","8fa742f5":"markdown","2c62f4e4":"markdown"},"source":{"86b94312":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29ab068a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\n","d285e127":"train_data=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntrain_data.head()","d62e4eac":"test_data=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest_data.head()","4aa7691c":"# Shuffling the Data for some Randomness and it is okay since it is not sequential\ntrain_data_shuffle = train_data.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\ntrain_data_shuffle.head()","9d1136a7":"# How many Disasters\ntrain_data.target.value_counts()","8237fa2c":"import random\nrandom_index=random.randint(0,len(train_data)-5)\nfor row in train_data_shuffle[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n  _, text, target = row\n  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n  print(f\"Text:\\n{text}\\n\")\n  print(\"---\\n\")","cad4f20f":"from sklearn.model_selection import train_test_split\n\n# Use train_test_split to split training data into training and validation sets\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(train_data_shuffle[\"text\"].to_numpy(),\n                                                                            train_data_shuffle[\"target\"].to_numpy(),\n                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n                                                                            random_state=42) # random state for reproducibility\ntest_sentences=test_data[\"text\"].to_numpy()\n","fe207146":"import tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization","9ffd4c50":"text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n                                    split=\"whitespace\", # how to split tokens\n                                    ngrams=None, # create groups of n-words?\n                                    output_mode=\"int\", # how to map tokens to numbers\n                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None","7063b595":"round(sum([len(i.split()) for i in train_sentences])\/len(train_sentences))","5d1bbbec":"# Custom Vectorization with Custom Variable\nmax_vocab_length = 10000 # max number of words to have in our vocabulary\nmax_length = 15 # max length our sequences will be \ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode=\"int\",\n                                    output_sequence_length=max_length)","093cb824":"# Fit the text vectorizer to the training text\ntext_vectorizer.adapt(train_sentences)\ntext_vectorizer.adapt(test_sentences)","cd9dce6a":"words_in_vocab = text_vectorizer.get_vocabulary()","746f7674":"# Creating an Emmbedding\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\n\nembedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n                             output_dim=128, # set size of embedding vector\n                             embeddings_initializer=\"uniform\", # default, intialize randomly\n                             input_length=max_length, # how long is each input\n                             name=\"embedding_1\")","fb86c889":"# So now Each statement in the training data Undergoes two Steps \n# 1 Vectorization\n# 2 Embedding\n# Now Building Multiple Models and them Ensembling them to get the Optimum Results\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n","80183358":"# Create tokenization and modelling pipeline\nmodel_0 = Pipeline([\n                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n                    (\"clf\", MultinomialNB()) # model the text\n])\n\n# Fit the pipeline to the training data\nmodel_0.fit(train_sentences, train_labels)","d95a5ea9":"baseline_score = model_0.score(val_sentences, val_labels)\nprint(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")","c4df71ec":"baseline_preds = model_0.predict(val_sentences)","5cd7f840":"# Function to evaluate: accuracy, precision, recall, f1-score\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef calculate_results(y_true, y_pred):\n#   \"\"\"\n#   Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n\n#   Args:\n#   -----\n#   y_true = true labels in the form of a 1D array\n#   y_pred = predicted labels in the form of a 1D array\n\n#   Returns a dictionary of accuracy, precision, recall, f1-score.\n#   \"\"\"\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate model precision, recall and f1 score using \"weighted\" average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n  model_results = {\"accuracy\": model_accuracy,\n                  \"precision\": model_precision,\n                  \"recall\": model_recall,\n                  \"f1\": model_f1}\n  return model_results","ef1e14ae":"baseline_results = calculate_results(y_true=val_labels,\n                                     y_pred=baseline_preds)\nbaseline_results","5c6341ce":"sub=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsub.head()","456d6136":"mod0_prediction=model_0.predict(test_sentences)","f6a0dd35":"mod0_prediction","cdc2566e":"# The Predictions for Naive Bayes Model\n# prediction=pd.DataFrame({'id':test_data.id,'target':mod0_prediction})\n# prediction.to_csv('submission.csv',index=False)","e46e4b00":"# prediction.head()","f9bd8120":"import datetime\n\ndef create_tensorboard_callback(dir_name, experiment_name):\n  \"\"\"\n  Creates a TensorBoard callback instand to store log files.\n  Stores log files with the filepath:\n    \"dir_name\/experiment_name\/current_datetime\/\"\n  Args:\n    dir_name: target directory to store TensorBoard log files\n    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n  \"\"\"\n  log_dir = dir_name + \"\/\" + experiment_name + \"\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=log_dir\n  )\n  print(f\"Saving TensorBoard log files to: {log_dir}\")\n  return tensorboard_callback","4d6adaae":"# Now we will be Building a TON  of Models and Testing Them to See which Ones perform the best and \n# at the end I will ensemble them or Stack them for Optimum performance\nSAVE_DIR=\"model_logs\"","f4e3da05":"from tensorflow.keras import layers\ninputs=layers.Input(shape=(1,),dtype=tf.string)\nX=text_vectorizer(inputs)\n# Creating an Embedding for the Input Layer\nX=embedding(X)\n# We need to perform this step so that we do not get predictions for each token\n# But for each of the Tweets\nX=layers.GlobalAveragePooling1D()(X)\n\n# Dense Output Layer\n# 1 Hidden Neuron as it is a Binary Classification\noutputs=layers.Dense(1,activation=\"sigmoid\")(X)\nmodel_1=tf.keras.Model(inputs,outputs,name=\"Dense_Model\")\n","253479cf":"model_1.summary()","b6299dfa":"model_1.compile(loss=\"binary_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\nmodel_1.history=model_1.fit(x=train_sentences,y=train_labels,epochs=10,validation_data\n                           =(val_sentences,val_labels),\n                           callbacks=create_tensorboard_callback(dir_name=SAVE_DIR,experiment_name=\"Model_1\"))","9f0d6978":"model_1.evaluate(val_sentences,val_labels)","720c4cf3":"# Make predictions (these come back in the form of probabilities)\nmodel_1_pred_probs = model_1.predict(val_sentences)\nmodel_1_pred_probs[:10] # only print out the first 10 prediction probabilities","ddf57585":"# Turn prediction probabilities into single-dimension tensor of floats\nmodel_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\nmodel_1_preds[:20]","e7325255":"model_1_results = calculate_results(y_true=val_labels, \n                                    y_pred=model_1_preds)\nmodel_1_results","4ce01874":"# These are the Numerical Representation of each Token \nembed_weights=model_1.get_layer(\"embedding_1\").get_weights()[0]\nembed_weights","f4b5187d":"embed_weights.shape","fec4793d":"# Projector Tool to visualize the Embedding Matrix\n# # Code below is adapted from: https:\/\/www.tensorflow.org\/tutorials\/text\/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\nimport io\n\n# # Create output writers\nout_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\nout_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n\n# # Write embedding vectors and words to file\nfor num, word in enumerate(words_in_vocab):\n    if num == 0: \n#    continue # skip padding token\n     vec = embed_weights[num]\n     out_m.write(word + \"\\n\") # write words to file\n     out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\nout_v.close()\nout_m.close()\n\n","7ac85661":"# RNN LSTM and RGU Models\n# Note: The reason we use a new embedding layer for each model is since the embedding layer is a learned representation of words (as numbers), if we were to use the same embedding layer (embedding_1) for each model, we'd be mixing what one model learned with the next. And because we want to compare our models later on, starting them with their own embedding layer each time is a better idea.\n# LSTM Long Short Term Memory\n\n","88a0531f":"from tensorflow.keras import layers\ninputs=layers.Input(shape=(1,),dtype=tf.string)\nx=text_vectorizer(inputs)\nx=embedding(x)\nx=layers.LSTM(64,return_sequences=True)(x)\n# x=layers.LSTM(64,return_sequences=True)(x)\n# Now since it is an RNN output of one Layer will be fed to another \nx=layers.LSTM(64)(x)\nx=layers.Dense(64,activation=\"relu\")(x)\n# The final Output Layer will be a Dense Layer and it will be 1 Hidden Neuron as Binary Labelling Output\noutputs=layers.Dense(1,activation=\"sigmoid\")(x)\nmodel_2=tf.keras.Model(inputs,outputs,name=\"Model_2_LSTM\")\n","35101984":"model_2.summary()\n# While Sttacking RNN Models we need to return sequences mandatorily\n","9ed3acbc":"# Comiling and Fitting the Model\nmodel_2.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\n","f552a612":"history_2_model=model_2.fit(x=train_sentences,y=train_labels, epochs=7,validation_data=(val_sentences,val_labels),\n                           callbacks=[create_tensorboard_callback(\"SAVE_DIR\",\"Model_2_LSTM\")])","3b595e69":"model_2_predict_probs=model_2.predict(val_sentences)\nmodel_2_predict_probs[:20]","651cf629":"# Converting these Probabilites to Labels of 0 and 1\nmodel_2_predictions=tf.squeeze(tf.round(model_2_predict_probs))\nmodel_2_predictions","2e6101e4":"model_2_res=calculate_results(y_true=val_labels,y_pred=model_2_predictions)\nmodel_2_res","8217b6d9":"# Building a GRU Powered RNN\n # Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_3\")","9a00942a":"inputs=layers.Input(shape=(1,),dtype=tf.string)\nx=text_vectorizer(inputs)\nx = model_3_embedding(x)\nx = layers.GRU(64, return_sequences=True) (x)\nx = layers.GRU(64, return_sequences=True)(x)\nx = layers.GRU(64)(x) \nx = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")","25fdf54c":"model_3.summary()","91d70a85":"model_3.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","70cd161c":"model_3_history = model_3.fit(train_sentences,\n                              train_labels,\n                              epochs=9,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"Model_3_GRU\")])","ff30482e":"# Make predictions on the validation data\nmodel_3_pred_probs = model_3.predict(val_sentences)\nmodel_3_pred_probs.shape, model_3_pred_probs[:10]","2b18084f":"# Convert prediction probabilities to prediction classes\nmodel_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\nmodel_3_preds[:10]","2d36ac90":"model_3_results = calculate_results(y_true=val_labels, \n                                    y_pred=model_3_preds)\nmodel_3_results","7f4c42fc":"tf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_4\")","00e70876":"inputs=layers.Input(shape=(1,),dtype=tf.string)\nx=text_vectorizer(inputs)\nx=model_4_embedding(x)\nx=layers.Bidirectional(layers.LSTM(64,return_sequences=True))(x)\nx=layers.Bidirectional(layers.LSTM(64))(x)\nx=layers.Dense(64,activation=\"relu\")(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_4=tf.keras.Model(inputs,outputs,name=\"Model_4_Bidirectional_RNN\")\n\n","239b080d":"model_4.summary()","9675ec1b":"model_4.compile(loss=\"binary_crossentropy\",optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","1f879ec0":"model_4_history = model_4.fit(train_sentences,\n                              train_labels,\n                              epochs=6,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])","8e838458":"# Make predictions with bidirectional RNN on the validation data\nmodel_4_pred_probs = model_4.predict(val_sentences)\nmodel_4_pred_probs[:10]","c984d04c":"model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\nmodel_4_preds[:10]","03a2f5a1":"model_4_results = calculate_results(val_labels, model_4_preds)\nmodel_4_results","ac3d3dbe":"import tensorflow_hub as hub\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\") # load Universal Sentence Encoder","8a4f98d5":"# We can use this encoding layer in place of our text_vectorizer and embedding layer\nsentence_encoder_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\",\n                                        input_shape=[], # shape of inputs coming to our model \n                                        dtype=tf.string, # data type of inputs coming to the USE layer\n                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n                                        name=\"USE\") ","17bf1cd2":"# Create model using the Sequential API\nmodel_5 = tf.keras.Sequential([\n  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(1, activation=\"sigmoid\")\n], name=\"model_5_USE\")\n\n# Compile model\nmodel_5.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\nmodel_5.summary()","1e07e11d":"# Train a classifier on top of pretrained embeddings\nmodel_5_history = model_5.fit(train_sentences,\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n                                                                     \"tf_hub_sentence_encoder\")])","fd29674f":"# Make predictions with USE TF Hub model\nmodel_5_pred_probs = model_5.predict(val_sentences)\nmodel_5_pred_probs[:10]","542e400f":"# Convert prediction probabilities to labels\nmodel_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\nmodel_5_preds[:10]","ff709228":"mod5_prediction=model_5.predict(test_sentences)\nmod5_prediction\nmodel_5_preds_final = tf.squeeze(tf.round(mod5_prediction))\nmodel_5_preds_final.shape","974da73e":"a = tf.constant(model_5_preds_final)\nproto_tensor = tf.make_tensor_proto(a)  # convert `tensor a` to a proto tensor\narr=tf.make_ndarray(proto_tensor) \n\npred_arr=arr[0]\npred_arr=int(pred_arr)\n","05a2ebb4":"# The Predictions for the Universal Sentence Embedding sub\npredictions=pd.DataFrame({'id':test_data.id,'target':pred_arr})\npredictions.to_csv('submission.csv',index=False)\n","56499a7c":"predictions.head()","e046a066":"from IPython.display import FileLink\nFileLink('submission.csv')","53443c3c":"\n<a href=\".\/embedding_metadata.tsv\"> Download Metadata File <\/a> <br>\n<a href=\".\/embedding_vectors.tsv\"> Download Vectors  File <\/a>","444520f5":"Initially I will be training some NLP Models and then Try and make a subsequent NLI model extrapolating the learnings.","94714fce":"Now we will be Embedding and Tokenization. <br>\nThis is an extremely crucial step while working with NLP Questions.<br>\nTokenization is Breaking down the sentencing in Numerical Formats and <br>\nIn Embedding Format each Word is turned into a vectorr and we can specify the size of the vector <br>\nRepresentation comes in the form of a feature vector. <br>\n","62d42a73":"<h2> Visuallizing the Embedding <h2>","e123c80f":"<b><h2>Now on to a Bidirectional RNN Model","b2668571":"1 -> Disaster <br>\n0-> Not Disaster","cd6d1021":"<h2><b> Dense Model <\/b><\/h2>\n","638027cc":"<h2>TensorFlow Hub Pretrained Sentence Encoder<h2>","8fa742f5":"Structure -->\nInput-->\nTokenize-->\nEmbed-->\nLayers of RNN-->\nOutput\n","2c62f4e4":" In this Notebook I am going to Train some NLP Models in Tensorflow\n At the End I will also be making a Submission to the Competition Contradictory NLP Getting started with Disaster Tweets.\n<!-- Also I will be using [MNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) for additional help with Inferences as I saw that in a Couple of Notebooks like one by [Andrej Marichenko](https:\/\/www.kaggle.com\/andrej0marinchenko\/public-score-0-95072-contradictory-my-dwatson) -->\nCourse Referred: https:\/\/www.udemy.com\/course\/tensorflow-developer-certificate-machine-learning-zero-to-mastery\/\n\n\n"}}