{"cell_type":{"f2c321ef":"code","99c7a1cd":"code","d0ccf2f4":"code","87a894f6":"code","c6e2fd4a":"code","1f66b778":"code","d647d4c7":"code","5854ccf5":"code","68f66de9":"code","0b1a29dd":"code","fb56e0e2":"code","98a3acba":"code","97a4c236":"code","d63761bc":"code","9bd8ce0a":"code","e5fdc835":"code","e24e52f6":"code","bdc147ea":"code","ba4f7414":"code","39dd1302":"code","43300a6a":"code","f348c3cc":"code","c3a207ce":"code","370616c4":"code","cd20a8f5":"code","fb23480f":"code","daa81713":"code","50215007":"code","58a709a5":"code","736f238e":"code","0241a89b":"code","7fc3440f":"code","315adc52":"code","f712f58c":"code","57ebb7f3":"code","9cf6b140":"code","fd22a232":"code","9237ed21":"code","eb58c7dc":"code","582984dd":"code","7855cd10":"code","ac64c907":"code","bff30d65":"code","2c73134a":"code","cb499f1e":"code","781695e5":"code","d1ee51bf":"code","2bddadf8":"code","4d8bf646":"code","39159742":"code","96e356f9":"code","65dfb90e":"code","c88da538":"code","835caf13":"code","42fd3baa":"code","2353b2f8":"code","7957a6b9":"code","c2e9032c":"code","deb3b880":"code","f412e5cb":"code","c39aa6de":"code","20f97c76":"code","763a1c47":"code","1f6cf860":"code","99ea4d31":"code","efc6d807":"code","eaeb622e":"code","7fe7e539":"code","0c9602e2":"code","cdb83722":"code","93ab6aec":"code","cdcca57a":"code","ac8accad":"code","0017543c":"code","3a9fd291":"markdown","21bd7074":"markdown","e754ae11":"markdown","6eccdbb6":"markdown","4f86928c":"markdown","befff6c4":"markdown","0b9c30c0":"markdown","26391a55":"markdown","4c586228":"markdown","883051ab":"markdown","16bf7eb8":"markdown","592a330f":"markdown","5904772b":"markdown"},"source":{"f2c321ef":"import random\nimport pandas as pd\nimport numpy as np\nbank=pd.read_csv(\"..\/input\/bank-training.csv\", index_col=0) # index_col=0 will remove the index column from the csv file\nbank.head()","99c7a1cd":"bank.shape # which tellls us the 3090 rows with 20 features","d0ccf2f4":"# Assign outcome as 0 if y=no and as 1 if y=yes\nbank['y'] = [0 if x == 'no' else 1 for x in bank['y']]\nX = bank.drop('y', 1) # 1 represents column, we are dropping as we are doing classification\ny = bank.y","87a894f6":"bank['y'].value_counts()\n# 339 people opened term deposit account and 2751 have not opened the term deposit account as per training data","c6e2fd4a":"X.head() # dsiplays first 5 tuples of the tarining datapoints","1f66b778":"y.head() # dsiplays first 5 tuples of the tarining datapoints labels","d647d4c7":"# Decide which categorical variables you want to use in model\nfor col_name in X.columns:\n    if X[col_name].dtypes == 'object':# in pandas it is object\n        unique_cat = len(X[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} unique categories\".format(col_name=col_name, unique_cat=unique_cat))\n        print(X[col_name].value_counts())\n        print()","5854ccf5":"# Create a list of features to dummy\ntodummy_list = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact','month','day_of_week','poutcome']","68f66de9":"# Function to dummy all the categorical variables used for modeling\n# dummy_na : bool, default False\n# Add a column to indicate NaNs, if False NaNs are ignored even though we know thier are no missing values # Just for Reference\n\ndef dummy_df(df, todummy_list):\n    for x in todummy_list:\n        dummies = pd.get_dummies(df[x], prefix=x, dummy_na=False) # prefix give name\n        df = df.drop(x, 1)\n        df = pd.concat([df, dummies], axis=1)\n    return df","0b1a29dd":"X = dummy_df(X, todummy_list)\nprint(X.shape) # now we have 62 features instead of 20 features","fb56e0e2":"# we have to same pre processing for test data set to match the algorithm to work\nimport pandas as pd\nimport numpy as np\nbanktest=pd.read_csv(\"..\/input\/bank-test.csv\", index_col=0) # index_col will remove the index column from the csv file\nbanktest.head()","98a3acba":"banktest.shape # we have 1029 training datapoints with 20 features","97a4c236":"# Assign outcome as 0 if no and as 1 if yes\nbanktest['y'] = [0 if x == 'no' else 1 for x in banktest['y']]\n\n\nX_test = banktest.drop('y', 1) # 1 represents column, we are dropping as we are doing classification\ny_test = banktest.y","d63761bc":"banktest['y'].value_counts()","9bd8ce0a":"# Create a list of features to dummy\ntodummy_list_test = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact','month','day_of_week','poutcome']","e5fdc835":"# Function to dummy all the categorical variables used for modeling\ndef dummy_df(df, todummy_list):\n    for x in todummy_list:\n        dummies = pd.get_dummies(df[x], prefix=x, dummy_na=False) # prefix give name\n        df = df.drop(x, 1)\n        df = pd.concat([df, dummies], axis=1)\n    return df","e24e52f6":"X_test = dummy_df(X_test, todummy_list)\nX_test.shape # features increased to 60 by using dummy fucntion","bdc147ea":"print(X.shape)\nprint(X_test.shape)\nprint(y.shape)\nprint(y_test.shape)\n\n# we can drop extra columns to match the test and train data set and need to deleted two columns from X","ba4f7414":"X_test.columns","39dd1302":"X.columns","43300a6a":"# Get missing columns in the training test\nmissing_cols = set( X.columns ) - set( X_test.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    X_test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\nX_test = X_test[X.columns]","f348c3cc":"print(X.shape)\nprint(X_test.shape)\nprint(y.shape)\nprint(y_test.shape)","c3a207ce":"# Use PolynomialFeatures in sklearn.preprocessing to create two-way interactions for all features\nfrom itertools import combinations\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef add_interactions(df):\n    # Get feature names\n    combos = list(combinations(list(df.columns), 2))\n    colnames = list(df.columns) + ['_'.join(x) for x in combos] # new name giving to feature\n    \n    # Find interactions\n    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n    df = poly.fit_transform(df)\n    df = pd.DataFrame(df)\n    df.columns = colnames\n    \n    # Remove interaction terms with all 0 values            \n    noint_indicies = [i for i, x in enumerate(list((df == 0).all())) if x]\n    df = df.drop(df.columns[noint_indicies], axis=1)\n    \n    return df","370616c4":"X = add_interactions(X)\nprint(X.head(5)) # no of features increased to 1662 for the training datsaset by using above add interaction function","cd20a8f5":"X_test = add_interactions(X_test)\nprint(X_test.head(5))# no of features increased to 1540 for the test dataset by using above add interaction function","fb23480f":"# Get missing columns in the training test\nmissing_cols = set( X.columns ) - set( X_test.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    X_test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\nX_test = X_test[X.columns]","daa81713":"# lets check all the feaures are equal in both the datasets\n\nprint(X.shape)\nprint(X_test.shape)\nprint(y.shape)\nprint(y_test.shape)\n","50215007":"# Such a large set of features can cause overfitting and also slow computing\n# Use feature selection to select the most important features\nimport sklearn.feature_selection\n\nselect = sklearn.feature_selection.SelectKBest(k=15) #which variables are selected\nselected_features = select.fit(X, y)\nindices_selected = selected_features.get_support(indices=True)\ncolnames_selected = [X.columns[i] for i in indices_selected]\n\nX_train_selected = X[colnames_selected]\nX_test_selected = X_test[colnames_selected]","58a709a5":"print(colnames_selected)","736f238e":"print(X_train_selected.shape)\nprint(X_test_selected.shape)\nprint(y.shape)\nprint(y_test.shape)\n","0241a89b":"#Create an Logistic classifier and train it on 70% of the data set.\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf","7fc3440f":"clf.fit(X, y)","315adc52":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')","f712f58c":"scores.mean() # Cross Validation mean score","57ebb7f3":"#Prediction using test data\ny_pred = clf.predict(X_test)","9cf6b140":"#classification accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred))","fd22a232":"# Imports\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Confusion matrix\ncm=confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# New line\nprint('\\n')\n\n# Classification report\nprint(classification_report(y_test,y_pred))","9237ed21":"# As we know beta value should be more as we are looking senistivity or recall\n#consider taking beta as 10 0r 100 0r 1000 also used weighted as measure because it considers label imbalance into account\nfrom sklearn.metrics import fbeta_score\nfbeta_score(y_test, y_pred, average='binary', beta=10)","eb58c7dc":"# Area under curve\nfrom sklearn.metrics import roc_auc_score\nauc = roc_auc_score(y_test, y_pred)\nauc","582984dd":"# plot confusion matrix to describe the performance of classifier.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nclass_label = [\"No\", \"Yes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","7855cd10":"from sklearn.tree import DecisionTreeClassifier\nclass_tree = DecisionTreeClassifier(min_samples_split=30,min_samples_leaf=10,random_state=10)\nclass_tree.fit(X,y)","ac64c907":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(class_tree, X, y, cv=10, scoring='accuracy')\nscores.mean()","bff30d65":"#Predictions\npredictions1 = class_tree.predict(X_test)","2c73134a":"#classification accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, predictions1))","cb499f1e":"from sklearn.metrics import roc_auc_score\naucd = roc_auc_score(y_test, predictions1)\naucd","781695e5":"# Imports\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Confusion matrix\ncm1=confusion_matrix(y_test, predictions1)\nprint(cm1)\n\n# New line\nprint('\\n')\n\n# Classification report\nprint(classification_report(y_test,predictions1))","d1ee51bf":"from sklearn.metrics import fbeta_score\nfbeta_score(y_test, predictions1, average='binary', beta=10)","2bddadf8":"# plot confusion matrix to describe the performance of classifier.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nclass_label = [\"No\", \"Yes\"]\ndf_cm1 = pd.DataFrame(cm1, index = class_label, columns = class_label)\nsns.heatmap(df_cm1, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","4d8bf646":"from sklearn.naive_bayes import GaussianNB\nNBC=GaussianNB()\nNBC.fit(X,y)","39159742":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(NBC, X, y, cv=10, scoring='accuracy')\nscores.mean()","96e356f9":"#Prediction using test data\ny_pred2 = NBC.predict(X_test)\n#classification accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred2))","65dfb90e":"from sklearn.metrics import roc_auc_score\naucnb = roc_auc_score(y_test, y_pred2)\naucnb","c88da538":"#Predictions\npredictions2 = NBC.predict(X_test)\n# Imports\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Confusion matrix\ncm2=confusion_matrix(y_test, predictions2)\nprint(cm2)\n\n# New line\nprint('\\n')\n\n# Classification report\nprint(classification_report(y_test,predictions2))","835caf13":"from sklearn.metrics import fbeta_score\nfbeta_score(y_test, predictions2, average='binary', beta=10)","42fd3baa":"# plot confusion matrix to describe the performance of classifier.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nclass_label = [\"No\", \"Yes\"]\ndf_nb = pd.DataFrame(cm2, index = class_label, columns = class_label)\nsns.heatmap(df_nb, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","2353b2f8":"#Import svm model\nfrom sklearn import svm\n\n#Create a svm Classifier\nsv = svm.SVC(kernel='linear') # Linear Kernel\n\n#Train the model using the training sets\nsv.fit(X, y)\n\n","7957a6b9":"# Note: Cross validation takes so much time to run for Suppor vector machines \nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(sv, X, y, cv=10, scoring='accuracy')\nscores.mean()","c2e9032c":"#Predict the response for test dataset\ny_pred3 = sv.predict(X_test)","deb3b880":"#classification accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred3))","f412e5cb":"# Imports\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Confusion matrix\ncm3=confusion_matrix(y_test, y_pred3)\nprint(cm3)\n\n# New line\nprint('\\n')\n\n# Classification report\nprint(classification_report(y_test,y_pred3))","c39aa6de":"from sklearn.metrics import fbeta_score\nfbeta_score(y_test, y_pred3, average='binary', beta=10)","20f97c76":"from sklearn.metrics import roc_auc_score\nauc3 = roc_auc_score(y_test, y_pred3)\nauc3","763a1c47":"# plot confusion matrix to describe the performance of classifier.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nclass_label = [\"No\", \"Yes\"]\ndf_cm3 = pd.DataFrame(cm3, index = class_label, columns = class_label)\nsns.heatmap(df_cm3, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","1f6cf860":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nmlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500,random_state=10)\nmlp.fit(X,y)\n","99ea4d31":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\nscores.mean()","efc6d807":"#Predict the response for test dataset\ny_pred4 = mlp.predict(X_test)","eaeb622e":"#classification accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred4))","7fe7e539":"from sklearn.metrics import roc_auc_score\nauc4 = roc_auc_score(y_test, y_pred4)\nauc4","0c9602e2":"# Imports\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Confusion matrix\ncm4=confusion_matrix(y_test, y_pred4)\nprint(cm4)\n\n# New line\nprint('\\n')\n\n# Classification report\nprint(classification_report(y_test,y_pred4))","cdb83722":"from sklearn.metrics import fbeta_score\nfbeta_score(y_test, y_pred4, average='binary', beta=10)","93ab6aec":"# plot confusion matrix to describe the performance of classifier.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nclass_label = [\"No\", \"Yes\"]\ndf_nn = pd.DataFrame(cm4, index = class_label, columns = class_label)\nsns.heatmap(df_nn, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","cdcca57a":"from IPython.display import HTML, display\nimport tabulate\ntable = [[\"Algorithm Name\",\"Accuracy\",\"Senestivity\",\"Precision\",\"Auc Score\",\"CV score\"],\n    [\"Logistic Regression\",0.90,0.17,0.70,0.58,0.89],\n         [\"Decision Tree\",0.895,0.26,0.54,0.615,0.889,],\n         [\"Naive Bayes\",0.8367,0.54,0.34,0.708,0.8217],\n         [\"Support Vector Machines\",0.896,0.10,0.69,0.546,0.896],\n        [\"MLP Neural Netrworks\",0.884,0.33,0.46,0.641,0.824]]\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))","ac8accad":"from IPython.display import HTML, display\nimport tabulate\ntable = [[\"Algorithm Name\",\"Senestivity\",\"Precision\",\"Fbeta score\",\"Auc score\",\"CV score\"],\n    [\"Logistic Regression\",0.17,0.70,0.17,0.58,0.89],\n         [\"Decision Tree\",0.26,0.54,0.260,0.615,0.889,],\n         [\"Naive Bayes\",0.54,0.34,0.54,0.708,0.8217],\n         [\"Support Vector Machines\",0.10,0.69,0.09,0.546,0.896],\n        [\"MLP Neural Netrworks\",0.33,0.46,0.33,0.641,0.824]]\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))","0017543c":"from IPython.display import HTML, display\nimport tabulate\ntable = [[\"Algorithm Name\",\"True Positives\",\"True Negatives\",\"False Positives\",\"False Negatives\",\"Total Cost\",\"Total Sales\",\"Profit\"],\n    [\"Logistic Regression\",19,909,8,93,93*10+8*1,19*10+909*1,(19*10+909*1)-(93*10+8*1)],\n         [\"Decision Tree\",29,892,25,83,83*10+25*1,29*10+892*1,(29*10+892*1-(83*10+25*1))],\n         [\"Naive Bayes\",61,800,117,51,51*10+117*1,61*10+800*1,(61*10+800*1)-(51*10+117*1)],\n         [\"Support Vector Machines\",11,912,5,101,101*10+5*1,11*10+912*1,(11*10+912*1)-(101*10+5*1)],\n        [\"MLP Neural Netrworks\",37,873,44,75,75*10+44*1,37*10+873*1,(37*10+873*1)-(75*10+44*1)]]\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))","3a9fd291":"__ In our business case, Recall should be considered as we would like know people who open term deposit accounts__\nThe F measure is the harmonic mean of precision and recall.F scores gives equal weight to precision and recall. \nThe beta parameter determines the weight of precision in the combined score.\nbeta < 1 lends more weight to precision, while beta > 1 favors recall \n(beta -> 0 considers only precision  beta -> inf only recall).\nSource:http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.fbeta_score.html\n\nwe used scikit learn module to calculate f1 beta by taking value of 10( few cases it is showing optimal value) for all the model as we indicated in last paragraph as beta value should be greater than 1. Navie Bayes is considerd as best model with F beta score 0.541 and second best models are DT and NN.\n","21bd7074":"The beta parameter determines the weight of precision in the combined score. beta < 1 lends more weight to precision, while beta > 1 favors recall (beta -> 0 considers only precision, beta -> inf only recall). Source:http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.fbeta_score.html","e754ae11":"## __2__ Comparing Weighted FBeta","6eccdbb6":"## Neural Networks","4f86928c":"## LOGISTIC REGRESSION","befff6c4":"## Support Vector Machines","0b9c30c0":"## Decision Tree","26391a55":"## 3 Cost and Profit Analysis on different models","4c586228":"## __1__  Comparing all the models in terms of Key performance metrics","883051ab":"## Naive Bayes","16bf7eb8":"__Accuracy:__ It is calculated by correctly classified points divided by total no of points.The Model which gives highest accuarcy 90% after 10 fold cross validation is Logistic Regression which is a simple classification model and Least accuracy with NB model with 83.6%.\n\n__Senestivity or Recall__: NB gives the highest recall of 54% by predicting 61 people open term deposit out of 112 people i.e. more than half of the entire. Thier is no other model which is close to NB.\n\n__Precision__: Though Precision is high for SVM, Recall is almost 0 which is neglected. In our business problem we wont consider precision much and it tells us that higher precision means predicted positves but actually they are negative\n\n__AUC Score__: NB hold high Area under curve score i.e. 0.708 which indicates more points are classified and it is a good model. Decision trees and Neural Networks also accounts good AUC score. However all the model are above 0.5 which indicates decent model\n\n__CV Score__: It will judge the quality of the fit (or the prediction) on new data. Bigger is better. In our case all models predict good for the future unseen points and average highest values accounts to LR, DT and SVM's.","592a330f":"we can say that Naive Bayes model helps us to gain the profit of 783 units compared to all other models. The least delivered profit is by SVM. As True Positives increses in the above model, the profit also increases\n\n","5904772b":"#Use of a cost function: In this approach, a cost associated with misclassifying data is evaluated with the help of a cost matrix (similar to the confusion matrix, but more concerned with False Positives and False Negatives).\n\nThe main aim is to reduce the cost of misclassifying. The cost of a False Negative is always more than the cost of a False Positive. e.g. wrongly predicting a cancer patient to be cancer-free is more dangerous than wrongly predicting a cancer-free patient to have cancer.\n\nTotal Cost = Cost of FN Count of FN + Cost of FP Count of FP\nSource:KD nuggets\n\n#3. The cost of a phone call is 1 unit and cost of lost business is 10 units.\n\nConfusion matrix: \n\nTrue Positive : Predicted as positive and actually they are positive\n\nTrue Negative: Predicted as negative and actually they are negative\n\nFalse Positive: Predicted as positive and actually they are negative\n\nFalse Negative: Predicted as Negative and actually they are positive\n\nGiven cost in terms of units FN=10, FP=1, which implies TP=10, TN=1"}}