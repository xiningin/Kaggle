{"cell_type":{"e6698e28":"code","4d0a59e5":"code","08316fdf":"code","75a457a4":"code","7949786b":"code","f8f2d21f":"code","1c2b19ab":"code","11f5ee3d":"code","f7570779":"code","70de4df1":"code","36ed099f":"code","51f26786":"code","0e9e9c09":"code","150fe846":"code","da663869":"code","6c1e8f71":"code","6210675d":"code","aab54f40":"code","f914f684":"code","07fd6ae1":"code","413a15ab":"code","19883698":"code","0a6a97e4":"code","c77eda23":"code","3fe0da8b":"code","0239daa2":"code","ae892529":"code","08cb8a4b":"code","bb59c04a":"code","f117134f":"code","75703283":"code","59ffbef4":"code","f67c824d":"code","89cfddae":"code","c56900c0":"code","bfd1f0d0":"code","17ac441d":"code","d5b45bb3":"code","38abd804":"code","82609b8d":"code","3e25a50d":"code","25ac8715":"code","451d47bc":"code","e225928c":"code","bd8df481":"code","4a5a63ff":"code","ee9b676b":"code","d02fd5a7":"code","276bb7e3":"code","566cbf9f":"code","940ed0ae":"code","6ac95f67":"code","27b030f2":"code","4deca611":"code","7346180e":"code","a5065592":"code","e92d14b6":"code","a5b9dc37":"code","7968d4fe":"code","80b8626a":"code","1c8462ce":"code","9add846c":"code","deb6398e":"code","7a2bb0e5":"code","069deb83":"code","4cd47a81":"code","1c16cd48":"code","538365c6":"code","21663cdd":"code","8051e6db":"code","437e1973":"code","f1b20932":"code","1ab47dc1":"markdown","4475dd91":"markdown","788c0cb9":"markdown","d0e9274e":"markdown","35242610":"markdown","f169d5ec":"markdown","abfb5d50":"markdown","f12cb07d":"markdown","344a7966":"markdown","3893e371":"markdown","073b7003":"markdown","afaa14b9":"markdown","b6deedce":"markdown","f0930170":"markdown","714be2f4":"markdown","bfbe3483":"markdown","513e9647":"markdown","348b881b":"markdown","6aebefd2":"markdown","2a120316":"markdown","0bd2287c":"markdown","fff0f445":"markdown","84379411":"markdown","d28b5740":"markdown","3c58c8ce":"markdown","f1f63aff":"markdown","2d932f14":"markdown","2a648c3a":"markdown","6987cc86":"markdown","23f2d6f2":"markdown","2eaa8f0b":"markdown","bd594351":"markdown","5f045fef":"markdown","7fc354bc":"markdown","598001f5":"markdown","d6a6e43d":"markdown","9d5ec57a":"markdown","7705443a":"markdown","76581c8b":"markdown","00815dac":"markdown","dbaa4889":"markdown","1df015f5":"markdown","2ffbc518":"markdown","28245511":"markdown","53f15e0c":"markdown","642701c1":"markdown","a50ef4e5":"markdown","8f5433f4":"markdown","e2519c32":"markdown","070dfa48":"markdown","f5fff1c9":"markdown","5057b6db":"markdown","485052a0":"markdown","9347193f":"markdown","bb849017":"markdown","8b61cc40":"markdown","32754f00":"markdown","14bc4cc9":"markdown","cacfc41d":"markdown"},"source":{"e6698e28":"!pip install python-gdcm -q","4d0a59e5":"!git clone https:\/\/github.com\/asvcode\/fmi.git","08316fdf":"from fmi.fmi.explore import *\nfrom fmi.fmi.preprocessing import *\nfrom fmi.fmi.pipeline import *\nfrom fmi.fmi.retinanet import *","75a457a4":"from fastai.vision.all import *\nfrom fastai.medical.imaging import *\nimport pydicom\nimport cv2\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport gdcm\nfrom torchvision.utils import save_image","7949786b":"system_info()","f8f2d21f":"source = '..\/input\/siim-covid19-detection'\nprint(os.listdir(source))\n\ntrain_images = get_dicom_files(f'{source}\/train')\ntest_images = get_dicom_files(f'{source}\/test')\n\ndicom_dataframe = pd.read_csv('..\/input\/fmipackage\/dicom_dataframe.csv')\ntest_dataframe = pd.read_csv('..\/input\/fmipackage\/test_dataframe.csv')","1c2b19ab":"dcm =  train_images[0]\ndcm.dcmread()","11f5ee3d":"get_image_info(train_images[7])","f7570779":"get_pii(train_images[7])","70de4df1":"train_image_level = pd.read_csv('..\/input\/siim-covid19-detection\/train_image_level.csv')\ntrain_study_level = pd.read_csv('..\/input\/siim-covid19-detection\/train_study_level.csv')","36ed099f":"train_study_level.rename(columns = {'id':'StudyInstanceUID'}, inplace = True)\ntrain_study_level[:1]","51f26786":"train_study_level['StudyInstanceUID'] = train_study_level['StudyInstanceUID'].apply(lambda x: f'{x[:12]}')\ntrain_study_level[:1]","0e9e9c09":"merged = pd.merge(train_image_level, train_study_level, on='StudyInstanceUID')\nmerged[:1]","150fe846":"merged['id'] = merged['id'].apply(lambda x: f'{x[:-6]}')\nmerged.loc[merged['Negative for Pneumonia']==1, 'class_y'] = 'Negative'\nmerged.loc[merged['Typical Appearance']==1, 'class_y'] = 'Typical'\nmerged.loc[merged['Indeterminate Appearance']==1, 'class_y'] = 'Indeterminate'\nmerged.loc[merged['Atypical Appearance']==1, 'class_y'] = 'Atypical'\nmerged.drop(['boxes', 'Negative for Pneumonia', 'Typical Appearance', \n             'Indeterminate Appearance', 'Atypical Appearance', 'StudyInstanceUID'], axis=1, inplace=True)\nmerged[:1]","da663869":"num_of_boxes = []\nfor i in merged.index:\n    val_len = len(merged['label'][i].split(' '))\n    val = merged['label'][i].split(' ')\n    label = merged['class_y'][i]\n    box_count = val_len\/\/6\n    num_of_boxes.append(box_count)","6c1e8f71":"merged['num_of_boxes'] = num_of_boxes\nmerged[:5]","6210675d":"merged['num_of_boxes'].value_counts()","aab54f40":"m = []\nfor i in merged.index:\n    num_of_boxes = merged['num_of_boxes'][i]\n    val = merged['label'][i].split(' ')\n    if num_of_boxes == 1: boxes = val[2:6]\n    if num_of_boxes == 2: boxes = val[2:6] + val[8:12]\n    if num_of_boxes == 3: boxes = val[2:6] + val[8:12] + val[14:18]\n    if num_of_boxes == 4: boxes = val[2:6] + val[8:12] + val[14:18] + val[20:24]\n    m.append(boxes)","f914f684":"merged['new_label'] = m\ndel merged['label']\nmerged[:2]","07fd6ae1":"merged.rename(columns = {'id':'SOPInstanceUID'}, inplace = True)\nmerged[:1]","413a15ab":"dicom_merge = pd.merge(dicom_dataframe, merged, on='SOPInstanceUID')\ndicom_merge[:1]","19883698":"dicom_merge['fname'][100]","0a6a97e4":"dicom_merge[\"SOPInstanceUID\"][100]","c77eda23":"dicom_merge.to_csv('dicom_merge.csv', index=False)","3fe0da8b":"analysis_flds = ['ImageType', 'ImageType1', 'BitsStored']\ndicom_merge.pivot_table(values=['img_mean','img_max','img_min', 'PatientID'], index=analysis_flds,\n                   aggfunc={'img_mean':'mean','img_max':'max','img_min':'min', 'PatientID':'count'})","0239daa2":"analysis_flds = ['ImageType', 'ImageType1', 'class_y', 'BitsStored']\ndicom_merge.pivot_table(values=['img_mean','img_max','img_min', 'PatientID'], index=analysis_flds,\n                   aggfunc={'img_mean':'mean','img_max':'max','img_min':'min', 'PatientID':'count'})","ae892529":"analysis_flds = ['ImageType', 'ImageType1', 'BitsStored']\ntest_dataframe.pivot_table(values=['img_mean','img_max','img_min', 'PatientID'], index=analysis_flds,\n                   aggfunc={'img_mean':'mean','img_max':'max','img_min':'min', 'PatientID':'count'})","08cb8a4b":"df = dicom_merge[['SOPInstanceUID', 'class_y', 'new_label', 'fname']].copy()\ndf[:5]","bb59c04a":"def ViewBBImage(df):\n    fp = pydicom.dcmread(Path(df['fname'].item())).pixel_array\n    title = df['class_y'].item()\n    boxes = np.array(df['new_label'])\n    box_len = len(boxes[0])\n    if box_len ==16:\n        bb = np.array(boxes[0][:4] + boxes[0][4:8] + boxes[0][8:12] + boxes[0][12:16]).astype(np.float16) \n    if box_len ==12:\n        bb = np.array(boxes[0][:4] + boxes[0][4:8] + boxes[0][8:12]).astype(np.float16)\n    if box_len ==8:\n        bb = np.array(boxes[0][:4] + boxes[0][4:8]).astype(np.float16)\n    if box_len ==4:\n        bb = np.array(boxes[0][:4]).astype(np.float16)\n    \n    box1 = TensorBBox([bb])\n    ctx = show_image(fp, title=title)\n    box1.show(ctx=ctx)","f117134f":"df[10:20]","75703283":"ViewBBImage(df[19:20])","59ffbef4":"ViewBBImage(df[17:18])","f67c824d":"im_df = df['fname'].unique()\nim_df = [fn for fn in im_df]\nfns = [Path(str(f'{fn}')) for fn in im_df]\nfns[:5]","89cfddae":"def get_items(noop): return fns","c56900c0":"df_np = df.to_numpy()\ndf_np[0]","bfd1f0d0":"df_np[0][1]","17ac441d":"df_np[0][-2]","d5b45bb3":"def get_tmp_bbox(fn):\n    rows = np.where(df_np[:,0] == fn.name[:-4])\n    bboxs = df_np[rows][:,-2][0]\n    return np.array([np.fromstring(b, sep=',') for b in bboxs])","38abd804":"get_tmp_bbox(get_items(source)[1])","82609b8d":"def get_tmp_lbl(fn):\n    rows = np.where((df_np[:, 0] == fn.name[:-4]))\n    bboxs = len(df_np[rows][:,-2][0])\n    if bboxs > 12:\n        return np.concatenate(([df_np[rows][:,1]]*4))\n    if bboxs > 8:\n        return np.concatenate(([df_np[rows][:,1]]*3))\n    if bboxs > 4:\n        return np.concatenate(([df_np[rows][:,1]]*2))\n    else:\n        return df_np[rows][:,1]","3e25a50d":"get_tmp_lbl(get_items(source)[920])","25ac8715":"get_tmp_lbl(get_items(source)[0])","451d47bc":"bboxs = get_tmp_bbox(fns[0])\nlbls = get_tmp_lbl(fns[0])\narr = np.array([fns[0].name[:-4], bboxs, lbls], dtype=object)","e225928c":"arr","bd8df481":"for fname in fns[1:]:\n    bbox = get_tmp_bbox(fname)\n    lbl = get_tmp_lbl(fname)\n    arr2 = np.array([fname.name[:-4], bbox, lbl], dtype='object')\n    arr = np.vstack((arr, arr2))","4a5a63ff":"def get_bbox(fn):\n    idx = np.where((arr[:,0] == fn.name[:-4]))\n    return arr[idx][0][1]","ee9b676b":"get_bbox(get_items(source)[1])","d02fd5a7":"def get_lbl(fn):\n    idx = np.where((arr[:,0] == fn.name[:-4]))\n    return arr[idx][0][-1]","276bb7e3":"get_lbl(get_items(source)[1])","566cbf9f":"set_seed(77)\nsiim = DataBlock(blocks=(ImageBlock(cls=DicomView), BBoxBlock, BBoxLblBlock),\n                 get_items=get_items,\n                 splitter=RandomSplitter(),\n                 get_y=[get_bbox, get_lbl],\n                 item_tfms=Resize(512, method=ResizeMethod.Pad),\n                 n_inp=1)\n\ndls = siim.dataloaders(f'{source}\/train', bs=64)\ndls.show_batch(max_n=20, ncols=5)","940ed0ae":"set_seed(77)\nsiim = DataBlock(blocks=(ImageBlock(cls=HistView), BBoxBlock, BBoxLblBlock),\n                 get_items=get_items,\n                 splitter=RandomSplitter(),\n                 get_y=[get_bbox, get_lbl],\n                 item_tfms=Resize(512, method=ResizeMethod.Pad),\n                 n_inp=1)\n\ndls = siim.dataloaders(f'{source}\/train', bs=64)\ndls.show_batch(max_n=20, ncols=5)","6ac95f67":"class FixMonochrome(PILDicom):\n    @classmethod\n    def create(cls, fn:(Path, str, bytes), fix_monochrome = True)->None:\n        if isinstance(fn, bytes): im = pydicom.dcmread(pydicom.filebase.DicomBytesIO(fn))\n        if isinstance(fn, (Path, str)): im = pydicom.dcmread(fn)\n        scaled = np.array(im.pixel_array)   \n        if fix_monochrome and im.PhotometricInterpretation == \"MONOCHROME1\":\n            scaled = np.amax(scaled) - scaled\n        scaled = scaled - np.min(scaled)\n        scaled = scaled \/ np.max(scaled)\n        scaled = (scaled * 255).astype(np.uint8)\n        pill_im = Image.fromarray(scaled)\n        return cls(pill_im)","27b030f2":"set_seed(77)\nsiim = DataBlock(blocks=(ImageBlock(cls=FixMonochrome), BBoxBlock, BBoxLblBlock),\n                 get_items=get_items,\n                 splitter=RandomSplitter(),\n                 get_y=[get_bbox, get_lbl],\n                 item_tfms=Resize(32, method=ResizeMethod.Pad),\n                 n_inp=1)\n\ndls = siim.dataloaders(f'{source}\/train', bs=128)\ndls.show_batch(max_n=20, ncols=5)","4deca611":"img, bbox, lbl = dls.one_batch()","7346180e":"img.shape","a5065592":"show_images(img, nrows=4)","e92d14b6":"bbox.shape","a5b9dc37":"bbox[0]","7968d4fe":"lbl.shape","80b8626a":"lbl[0]","1c8462ce":"num_of_classes = len(dls.vocab[1:])\nnum_of_classes","9add846c":"encoder = create_body(resnet34, pretrained=True)\n\nl = nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2),\n                    padding=(3,3), bias=False)\nl.weight = nn.Parameter(l.weight.sum(dim=1, keepdim=True))\n\nencoder[0] = l","deb6398e":"arch = RetinaNet(encoder, num_of_classes, final_bias=-4)","7a2bb0e5":"aspect_ratios = [0.5, 1, 2]\nscales = [2**0, 2**(1\/3), 2**(2\/3)]","069deb83":"crit = RetinaNetFocalLoss(arch, scales=scales, ratios=aspect_ratios)","4cd47a81":"map_metric = mAP()\nmetrics = [map_metric]","1c16cd48":"learn = Learner(dls, \n                arch, \n                loss_func=crit, \n                metrics=metrics, \n                cbs=[ShowGraphCallback()])","538365c6":"learn.lr_find()","21663cdd":"learn.fit_one_cycle(1,1e-4)","8051e6db":"learn.save('siim_one')","437e1973":"learn.cbs[3].threshold = 0.7","f1b20932":"learn.show_results()","1ab47dc1":"Create the model\n\nWe use `Resnet34` as the backbone network, however we need to make a slight modification so that the body can accept 1D images.","4475dd91":"Check to see the `get_bbox` function returns the box co-ordinates","788c0cb9":"### Load Dependancies","d0e9274e":"Convert the dataframe into a numpy array to improve processing times","35242610":"We now extract out the binding box co-ordinates but first we check to see how many boxes are present for each image","f169d5ec":"And if there are only 4 co-ordinates there should only be 1 label","abfb5d50":"#### Sanity Check\n\nChecking to see that the file path `fname` corresponds to the correct `SOPINstanceUID`","f12cb07d":"Noted that this is a kernels competition and during submission internet will not be allowed.  However for initial prototyping I have not taken that into consideration.\n\nGDCM will be required to prevent error messages","344a7966":"The following below was inspired by this fantastic [notebook](https:\/\/www.kaggle.com\/muellerzr\/fastai2-starter-kernel)","3893e371":"We can now merge this to the dicom_dataframe created earlier.  In order to do this we need to rename `id` to `SOPInstanceUID` and merge on `SOPInstanceUID`","073b7003":"Create the `DataBlock`","afaa14b9":"For a sanity check, check to see that the class label and binding boxes are accessible from the array","b6deedce":"Although this works, you can clearly see the differences between images that have `PhotometricInterpretation` of `MONOCHROME1` and `MONOCHROME2`.  This is compenstated for further below.\n\n**HistView**\n\nAs mentioned in this [notebook](https:\/\/www.kaggle.com\/jhoward\/don-t-see-like-a-radiologist-fastai) do computers really need to worry about how many levels of grayscale are displayed on the screen?  Converting 16 bit or 12 bit images to 8 bit are purely done for human reasons. \n\nComputers prefer well scaled inputs.  A neural network uses 32 bit floating point inputs that provides a high level of precision, particulary for numbers close to zero. `HistView` is a way of viewing dicom images that have been scaled to values between 0 and 1.","f0930170":"From the pivot table above we can see that the data is unsigned ie have a minimum pixel value of 0 but have a variety of max values ranging from 255 (8 bit), 1023 (10 bit), 4095 (12 bit), 16383 (14 bit), 32415 (15 bit) and 65535 (16 bit).  The majority of the images are 12 bit.\n\nWhat about if we break it down further by class:","714be2f4":"### Contents!\n\n![img1.PNG](attachment:a444bd79-9174-4344-bd76-558674abae44.PNG)\n\nThis notebook goes through the intial setup and pipeline details.\n\n- Load dependancies - ..done\n- DataFrame creation - ..done\n- Analysis - ..in progress\n- DataBlock - ..done\n- Model - ..RetinaNet(with mAP)\n- Training - ..in progress\n- Submission - ..to do","bfbe3483":"### More about DICOMS\n\n- For more information about DICOMS view [this](https:\/\/www.kaggle.com\/avirdee\/understanding-dicoms) notebook and\/or [this](https:\/\/asvcode.github.io\/MedicalImaging\/) blog","513e9647":"The paper states that the **Focal Loss** is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training.  Focal Loss is an enhancemnet over Cross-Entropy Loss.  \n\nOne-stage models can suffer from extreme foreground-background class imbalance problems due to dense sampling of possible object locations (anchor boxes).  In RetinaNet, at each pyramid layer there can be thousands of achor boxes and only a few will be assigned to the ground-truth object whilst the vast majority (easy examples) will be assigned to the background class.\n\nFocal Loss reduces the loss contribution from the easy examples and hence increase the importance of correcting missclassified examples.","348b881b":"Create a function to easily view images and bounding boxes","6aebefd2":"Note that I had previously created `dicom_dataframe` and `test_dataframe` to extract all the metadata from the dicom images using `dicom_dataframe = pd.DataFrame.from_dicoms(train_images, px_summ = True, window=dicom_windows.lungs)`.  Set px_summ to `True` if you want additional image statistics but bare in mind that this will slow down the process","2a120316":"Out of the box using `ImageBlock(cls=PILDicom)` will throw a ValueError: image has wrong mode. [fmi](https:\/\/github.com\/asvcode\/fmi) however has a couple of options that prevents this error from occuring.\n\n**DicomView**","0bd2287c":"To view the number of classes","fff0f445":"We then get rid of the `_study` in `StudyInstanceUID`","84379411":"### Analysis","d28b5740":"The model used is RetinaNet which is based on the [Focal Loss for Dense Object Detection](https:\/\/arxiv.org\/pdf\/1708.02002.pdf) paper.  The code is originally from [this](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl2\/pascal.ipynb) notebook and integrated into the [fmi](https:\/\/github.com\/asvcode\/fmi) library\n\n**RetinaNet**\n\n\n![retinanet.PNG](attachment:3ca75d5b-5224-46ff-80f6-ad13439edb39.PNG)\n[Image Credit](https:\/\/developers.arcgis.com\/python\/guide\/how-retinanet-works\/)\n\n\n\nThere are 4 major components:\n\n**1) Bottom-up Pathway** - The backbone network in this example `Resnet34`\n\nIf image size is (256, 256) and the backbone is a resnet then there would be the following feature maps:\n\n- input (256, 256)\n- C1 (128, 128)\n- C2 (64, 64)\n- C3 (32, 32)\n- C4 (16, 16)\n- C5 (8, 8)\n- The paper states that two additional features maps at C6 and C7 of sizes (4,4) and (2,2) by using stride-2 convolutions are also added\n\n**2) Top-down pathway and Lateral connections** - The top down pathway upsamples the spatially coarser feature maps from higher pyramid levels, and the lateral connections merge the top-down layers and the bottom-up layers with the same spatial size.\n\n- C7 moves to P7 (2, 2)\n- P6 (4, 4)\n- P5 (8, 8)\n- P4 (16, 16)\n- P3 (32, 32)\n- P2 (64, 64)\n\nThe P7 feature map is used in detecting larger objects whilst P2 would be used to detect smaller objects\n\n**3) Classification subnetwork** - Predicts the probability of an object being present at each spatial location for each anchor box and object class.\n\n**4) Regression subnetwork** - Regresses the offset for the bounding boxes from the anchor boxes for each ground-truth object.","3c58c8ce":"**Parameters**\n\n**Scales of anchor boxes** The default is set to [2^0, 2^\u2153, 2^\u2154 ] which is what the paper uses.\n\n\n**Aspect Ratios of anchor boxes** The default is set to [0.5, 1, 2] which means the anchor boxes will be of aspect ratios 1:2, 1:1, 2:1.","f1f63aff":"Do the same for the whole dataset","2d932f14":"On train_study_level we will rename the `id` column to `StudyInstanceUID` so that we can then merge it with the train_image_level file.","2a648c3a":"We can also view the batch images like so:","6987cc86":"We still suffer from the variations in monochrome(which we fix next) but the images specifically in the lungs area look alot better.  We create a new class `FixMonochrome` that inherits from `PILDicom` and here we can fix the monochrome issue. (you can also use `hist_scaled` when accessing the pixel_array, this however does really increase the training time)","23f2d6f2":"The good thing about this is that most medical datasets are biased because they only typically have data from 1 institution but in this case it looks like the data may be from a number of institutions which should help with better generalization.","2eaa8f0b":"Sanity check, if there are 12 co-ordinates there should be 4 labels","bd594351":"### Training","5f045fef":"### More to come!","7fc354bc":"The differences above in the image quality is because the first one has a `PhotochromicInterpretation` of `MONOCHROME1`and the second one is `MONOCHROME2`.  This is fixed further below.","598001f5":"Now we extract the box co-ordinates","d6a6e43d":"### DataFrame Creation","9d5ec57a":"### DataBlock","7705443a":"Each batch has 128 images (due to the batch size of 128)","76581c8b":"[fmi](https:\/\/github.com\/asvcode\/fmi) has a number of handy features that breakdown the metadata into useful chunks. Firstly you may want to quickly see what image information is contained in the metadata.  You can access this using `get_image_info`","00815dac":"### Model","dbaa4889":"Each DICOM contains metadata and this is typically how you see it(usually alot of info)","1df015f5":"Each image may have multiple boxes so in order for this to work in `fastai` we need to ensure that each box has a label","2ffbc518":"This dataset contains a wide variety of pixel data characteristics and we can explore this using the merged dataframe earlier.\n\n**ORIGINAL :**  an image whose pixel values are based on original or source data\n\n**DERIVED :**  an image whose pixel values have been derived in some manner from the pixel value of one or more other images\n\n**PRIMARY :** an image created as a direct result of the patient examination\n\n**SECONDARY :** an image created after the initial patient examination","28245511":"We can now merge the train_image_level and train_study_level files","53f15e0c":"Load the train_image_level and train_study_level `csv` files","642701c1":"And then we do some clean up! We create a single column that contains the class and get rid of a number of columns.  Note we also get rid of the `boxes` column as we will be using the `label` column for generating the binding boxes.","a50ef4e5":"There are a couple of things `get_pii` looks for like image comments, sometimes this can contain information that is handy.  Not all DICOMs will mention the de-identification method but if it is included it is handy to review.  More information about DICOM attributes can be found [here](https:\/\/dicom.innolitics.com\/ciods\/segmentation\/general-image\/00080008)","8f5433f4":"To make it easier we create a new dataframe from the previous created dataframe using only the columns that are important for using in the DataBlock","e2519c32":"**Test Set Breakdown**\n\nLooking at the `BitsStored` distribution in the test set","070dfa48":"Load the other dependancies","f5fff1c9":"The code for the mAP is credited to [this](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl2\/pascal.ipynb](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl2\/pascal.ipynb) and [this](https:\/\/github.com\/jaidmin\/Practical-Deep-Learning-for-Coders-2.0](https:\/\/github.com\/jaidmin\/Practical-Deep-Learning-for-Coders-2.0\/blob\/master\/Computer%20Vision\/06_Object_Detection_changed.ipynb) notebook.\n\nThe general definition for **Average Precision(AP)** is finding the area under the Precision-Recall curve. Precision-Recall curves are generally used when there is a moderate to large class imbalance, as opposed to ROC curves which are used when there are roughly equal numbers of observations for each class.\n\nTo calculate the AP for object detection we need to understand **Intersection of Union(IoU)** which is the ratio of the area of intersection and the area of union (union between predicted bounding box and ground truth bounding box).\n\n![iou.PNG](attachment:47193c54-7dbf-41fe-9ef3-c434fe334fae.PNG)\n\n**mAP(mean average precision)** is the average of AP.\n\nThe mean Average Precision or mAP score is calculated by taking the mean AP over all classes and\/or overall IoU thresholds, depending on different detection challenges that exist.\n\nFor example in the PASCAL VOC2007 challenge, AP for one object class is calculated for an IoU threshold of 0.5. So the mAP is averaged over all object classes and for the COCO 2017 challenge, the mAP is averaged over all object categories and 10 IoU thresholds.","5057b6db":"Check the same for the label","485052a0":"Note that this dataset contains multiple modalities and photometric interpretations but what `get_image_info` does do is tell you what attributes are contained within the metadata.\n\nAn important factor when dealing with medical data is to check to see if there are any personally identifiable pieces of information within the metadata.  This potentially can have an effect on the final outcome of the model, for example if the training set only contained images of male patients and the test set had images of both male and female patients.  Understanding what identifiable information is contained within the metadata is important in creating a fair non-biased model.\n\nusing `get_pii` quickly accesses this information. ","9347193f":"To check the shapes of the batches we can use `one_batch`","bb849017":"Each batch also has 128 sets of `TensorBBox`'s","8b61cc40":"You can easily review your system info using `system_info`","32754f00":"As well as 128 sets of y_labels","14bc4cc9":"[fmi](https:\/\/github.com\/asvcode\/fmi) is a package that adds additional functionality to fastai's medical imaging module","cacfc41d":"Load the data"}}