{"cell_type":{"f1feda29":"code","6bb260d0":"code","4be27f5f":"code","962dec02":"code","e288acb2":"code","379340e4":"code","013453c2":"code","2b418132":"code","2e11021a":"code","30dddb4a":"code","09ef3feb":"code","4c7d739c":"code","0842eb3e":"code","28a56be4":"code","291b2b8d":"code","31a6b288":"code","f247d590":"code","5bd5f9bb":"code","a2eec07f":"code","2556a322":"code","cfb3eb58":"code","931a8f0f":"code","53734fe8":"code","e46da392":"code","b539c763":"code","2630c573":"code","f504592f":"code","a7cf8b10":"code","8169168c":"code","d03a49ff":"code","31718218":"code","3849586c":"code","282ece25":"code","17498606":"code","422baa67":"code","f6bb068c":"code","feeba57a":"code","b29b5c0e":"code","d45884f1":"code","eb19782d":"code","fe4ea6fe":"code","70cad73d":"code","f92a4d35":"code","0bb8a707":"code","f645a1cb":"code","befc3564":"code","102bcc77":"code","1a35a702":"markdown","43d7d397":"markdown","6caa778a":"markdown","9ba14257":"markdown","603fbddc":"markdown","2343f466":"markdown","187374b5":"markdown","16a0d379":"markdown","78d71ffd":"markdown","e6cd6ee1":"markdown","415ba84e":"markdown","a11695dc":"markdown"},"source":{"f1feda29":"import pandas as pd \nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport re\nimport string\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom gensim.models import Word2Vec\nfrom numpy import asarray\nfrom numpy import zeros\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom nltk.tokenize import RegexpTokenizer\nimport plotly\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n# general imports\nimport math\nfrom bs4 import BeautifulSoup\nimport tensorflow as tf\nimport numpy as np\nimport skimage\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\nimport missingno as msno\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport warnings\nwarnings.filterwarnings('ignore')","6bb260d0":"#Load train data\ntweet= pd.read_csv('..\/input\/analytics-vidhya-identify-the-sentiments\/train.csv')\ntweet.head(5)","4be27f5f":"#Load test data\ntest= pd.read_csv('..\/input\/analytics-vidhya-identify-the-sentiments\/test.csv')\ntest.head(5)","962dec02":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","e288acb2":"#let's check the class distribution.There are only two classes 0 and 1.\nx=tweet.label.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","379340e4":"#Number of characters in tweets\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['label']==1]['tweet'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('Negative tweets')\ntweet_len=tweet[tweet['label']==0]['tweet'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Positive tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","013453c2":"#Number of words in a tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['label']==1]['tweet'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('Negative tweets')\ntweet_len=tweet[tweet['label']==0]['tweet'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Positive tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","2b418132":"#Average word length in a tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['label']==1]['tweet'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Negative')\nword=tweet[tweet['label']==0]['tweet'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Positive')\nfig.suptitle('Average word length in each tweet')","2e11021a":"#Common stopwords in tweets - First we will analyze tweets with class 0.\ndef create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['label']==target]['tweet'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n","30dddb4a":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx,y=zip(*top)\nplt.bar(x,y)\n","09ef3feb":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx,y=zip(*top)\nplt.bar(x,y)","4c7d739c":"#Analyzing punctuations - Lets check class 1\nplt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)\n","0842eb3e":"#move on to class 0.\nplt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')\n","28a56be4":"# Identify common words\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y,y=x)","291b2b8d":"#Ngram analysis\n#we will do a bigram (n=2) analysis over the tweets.\n#Let's check the most common bigrams in tweets.\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","31a6b288":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['tweet'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","f247d590":"#Data cleaning starts\ndf=pd.concat([tweet,test])\ndf.shape","5bd5f9bb":"#Step 1 : Removing urls\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n","a2eec07f":"df['tweet_NoURL']=df['tweet'].apply(lambda x : remove_URL(x))","2556a322":"df[['tweet','tweet_NoURL']].head(10)","cfb3eb58":"#Step 2 : Removing HTML tags\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n","931a8f0f":"df['tweet_NoHTML']=df['tweet_NoURL'].apply(lambda x : remove_html(x))\ndf[['tweet','tweet_NoURL','tweet_NoHTML']].sample(10)","53734fe8":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","e46da392":"df['tweet_NoEMOJI']=df['tweet_NoHTML'].apply(lambda x: remove_emoji(x))\ndf[['tweet','tweet_NoURL','tweet_NoHTML','tweet_NoEMOJI']].head(10)","b539c763":"#Step 4 :Removing punctuations\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","2630c573":"df['tweet_NoPUNKT']=df['tweet_NoEMOJI'].apply(lambda x : remove_punct(x))\ndf[['tweet','tweet_NoURL','tweet_NoHTML','tweet_NoEMOJI','tweet_NoPUNKT']].head(10)","f504592f":"#Step 5: transform to lowercase\ndf['tweet_tolower']= df['tweet_NoPUNKT'].str.lower()\ndf[['tweet_NoPUNKT','tweet_tolower']].head(10)","a7cf8b10":"#Step 6: remove stopwords 'n punctuation\nsw = stopwords.words('english')\n\ndef transform_text(s):\n    \n    # remove html\n    html=re.compile(r'<.*?>')\n    s = html.sub(r'',s)\n    \n    # remove numbers\n    s = re.sub(r'\\d+', '', s)\n    \n    # remove punctuation\n    # remove stopwords\n    tokens = nltk.word_tokenize(s)\n    \n    new_string = []\n    for w in tokens:\n        # remove words with len = 2 AND stopwords\n        if len(w) > 2 and w not in sw:\n            new_string.append(w)\n \n    s = ' '.join(new_string)\n    s = s.strip()\n\n    exclude = set(string.punctuation)\n    s = ''.join(ch for ch in s if ch not in exclude)\n    \n    return s.strip()\n\ndf['tweet_sw'] = df['tweet_tolower'].apply(transform_text)\ndf[['tweet_NoPUNKT','tweet_tolower', 'tweet_sw']].head(10)","8169168c":"#Step 7: lemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatizer_text(s):\n    tokens = nltk.word_tokenize(s)\n    \n    new_string = []\n    for w in tokens:\n        lem = lemmatizer.lemmatize(w, pos=\"v\")\n        # exclude if lenght of lemma is smaller than 2\n        if len(lem) > 2:\n            new_string.append(lem)\n    \n    s = ' '.join(new_string)\n    return s.strip()\n\ndf['tweet_lm'] = df['tweet_sw'].apply(lemmatizer_text)\ndf[['tweet_sw', 'tweet_lm']].head(20)","d03a49ff":"#Step 8: transform to lowercase, select only alphabets and remove stopwords\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['tweet_lm']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word.lower() not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus=create_corpus(df)","31718218":"#see the word cloud with treated text\n# -ve wordcloud\ndf_neg = df[df['label']==1]['tweet_lm']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_neg))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","3849586c":"# +ve wordcloud\ndf_pos = df[df['label']==0]['tweet_lm']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_pos))\n\nplt.figure(1,figsize=(10, 10))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","282ece25":"MAX_LEN = max(len(x) for x in corpus)\nprint(\"MAX_LEN = \", MAX_LEN)","17498606":"# Modelling starts with GloVe pre-trained 100\nembedding_dict={}\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r',encoding='utf-8') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","422baa67":"tokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","f6bb068c":"#Lets check unique words\nword_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))\n\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        print(\"Inside i greater than\")\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        #print(\"i =\" , i)\n        embedding_matrix[i]=emb_vec\n","feeba57a":"#Baseline Model\nmodel=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=True)\n\nmodel.add(embedding)\nmodel.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())","b29b5c0e":"LEARNING_RATE = 1e-4\noptimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","d45884f1":"# Segregating train and test from corpus\ntrain=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","eb19782d":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['label'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","fe4ea6fe":"history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=1)","70cad73d":"# Train-Test Split starts ######\n\n'''\nTotal rows in train - 7920 \n\nlabel=1 : Total 2026\nwe can use the first 1418(70%) for training and 608(30%) remaining for testing\ni.e. 2026=1418+608\n\nlabel=0 : Total 5894\nwe can use the first 4125(70%) for training and 1769(30%) remaining for testing\ni.e. 5894=4125+1769\n'''\n\n# Train dataset\npos_train = df[df['label']==0][['tweet_lm', 'label']].head(4125)\nneg_train = df[df['label']==1][['tweet_lm', 'label']].head(1418)\n\n\n# Test dataset\n#pos_test = df[df['label']==0][['tweet_lm', 'label']].tail(1769)\n#neg_test = df[df['label']==1][['tweet_lm', 'label']].tail(608)\n\npos_test = df[df['label']==0][['tweet_lm', 'label']].iloc[4125:5894]\nneg_test = df[df['label']==1][['tweet_lm', 'label']].iloc[1418:2026]\n\n\n# put all toghether \ntrain_df = pd.concat([pos_train, neg_train]).sample(frac = 1).reset_index(drop=True)\ntest_df = pd.concat([pos_test, neg_test]).sample(frac = 1).reset_index(drop=True)\n\ntrain_df.head()\ntest_df.head()\n\nprint('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))\n\n\nX_train = train_df['tweet_lm']\nX_test  = test_df['tweet_lm']\ny_train = train_df['label']\ny_test  = test_df['label']\n\n\n###Prepare actual Test data without label for submission\ntest1 = df['tweet_lm'].iloc[7920:]","f92a4d35":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nNGRAM_RANGE = (1, 2)\nTOP_K = 40000\nTOKEN_MODE = 'word'\nMIN_DOC_FREQ = 1","0bb8a707":"vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\nvectorizer.fit(X_train)\nx_train = vectorizer.transform(X_train)\nx_val   = vectorizer.transform(X_test)\n\n\n#Select top 'k' of the vectorized features. top_k = 6000\nselector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\nprint(\"x_train.shape[1] , TOP_K\", x_train.shape[1], TOP_K)\nselector.fit(x_train, y_train)\nx_train_1 = selector.transform(x_train)\nx_val_1   = selector.transform(x_val)\n\nx_train_1 = x_train_1.astype('float32')\nx_val_1 = x_val_1.astype('float32')","f645a1cb":"# Generic function for model building\ndef fit_and_test(classifier, X_train, y_train, X_test, y_test, only_return_accuracy=False):\n  classifier.fit(X_train, y_train)\n  y_hat = classifier.predict(X_test)\n  print('accuracy:', accuracy_score(y_test, y_hat))\n  if not only_return_accuracy:\n    print('f1_score:', f1_score(y_test, y_hat,average='micro'))\n","befc3564":"#MultinomialNB\nmnb = MultinomialNB() #88.85% for binary=False, ngram_range=(1, 2)\nfit_and_test(mnb, x_train_1, y_train, x_val_1, y_test)\n","102bcc77":"#Making our submission\nsample_submission = pd.read_csv('..\/input\/analytics-vidhya-identify-the-sentiments\/sample_submission.csv')\ntest_all = vectorizer.transform(df['tweet_lm'].iloc[7920:].values)\ntest_all = selector.transform(test_all)\n\ny_predict = (mnb.predict(test_all.toarray()) > 0.5).astype(\"int32\")\nsample_submission[\"label\"] = y_predict\nsample_submission.to_csv(\".\/sample_submission-mnb.csv\", index=False)\nsample_submission.head()","1a35a702":"Data cleaning Completed","43d7d397":"> **We got 88.80% accuracy with Multinomial Naive Bayes and using N-gram = (1,2) with top 40k features**","6caa778a":"Approach 2 : Multinomial Naive Bayes","9ba14257":"* Model Building starts ","603fbddc":"We will use SelectKBest to select Top 40k features****","2343f466":"Run the Model for 10 Epochs","187374b5":"> **If you have liked my Kernel - Please UpVote**","16a0d379":"* Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing. \n* This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc,\n* The task is to identify if the tweets have a negative sentiment towards such companies or products.","78d71ffd":"> ****The submission achieved 88.94% Accuracy (Rank : 248 on Christmas 2020) for Twitter Sentiment Analysis competion in AnalyticsVidhya :) ","e6cd6ee1":"> ****We got 88% accuracy with GloVe Embeddings (Keras+CONV1D) ","415ba84e":"* Details of the files : \n\n* train.csv - For training the models, we provide a labelled dataset of 7920 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet.\n\n* test.csv - The test data file contains only tweet ids and the tweet text with each tweet in a new line.\n\n* sample_submission.csv - The exact format for a valid submission","a11695dc":"> ****Approach 1 : Using GloVe Embeddings with Keras + Convolutional Layer"}}