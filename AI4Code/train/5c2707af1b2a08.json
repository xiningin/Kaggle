{"cell_type":{"691d56a1":"code","697afec1":"code","16892c55":"code","e3ffa777":"code","39e3b0b2":"code","61bfd3f2":"code","b73de042":"code","5876f9c4":"code","c6611351":"code","ecd2f24f":"code","fe26cf03":"code","5a302d6c":"code","22bf2f34":"code","83553467":"code","485916e3":"code","8da37f50":"markdown","d8c8c71d":"markdown","bd9bef19":"markdown","f19655ac":"markdown","3bc32c80":"markdown","a472222d":"markdown","367f9bbf":"markdown","c254a591":"markdown","a661536e":"markdown","e27cc937":"markdown","5ef87a0e":"markdown","2c7163a1":"markdown","32c7adfc":"markdown"},"source":{"691d56a1":"from IPython.display import Image\nImage(width= 550, height= 300, filename= '..\/input\/bert-final-data\/bert-diagram.png')","697afec1":"%%capture\n!pip install \/kaggle\/input\/bert-for-tf2\/py-params-0.8.2\/py-params-0.8.2\/\n!pip install \/kaggle\/input\/bert-for-tf2\/params-flow-0.7.4\/params-flow-0.7.4\/\n!pip install \/kaggle\/input\/bert-for-tf2\/bert-for-tf2-0.13.2\/bert-for-tf2-0.13.2\/\n!pip install sentencepiece\n\n%pylab inline\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Model\nfrom bert.tokenization.bert_tokenization import FullTokenizer\nimport matplotlib.pyplot as plt\nimport pickle\nimport numpy as np\nimport tensorflow.keras\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom bs4 import BeautifulSoup\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom IPython.display import Image","16892c55":"# Define the three functions that prepare the input for bert.\n# Load data (see Appendix A)\ntrain_data = np.load(\"..\/input\/imbddatao\/data.npz\")\ntest_data = np.load(\"..\/input\/imbddatao\/test.npz\")\ntrain = train_data[\"a\"]\ntrain_labels = train_data[\"b\"]\ntest = test_data[\"a\"]\ntest_labels = test_data[\"b\"]\n\n# Text maximum length for Bert Base\nmax_seq_length = 512","e3ffa777":"# Load the Bert base layer.\nBert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",trainable=True)","39e3b0b2":"# Processing Functions\n# to distinguish padding from real words ids\ndef get_masks(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        #Cutting down the excess length\n        tokens = tokens[0:max_seq_length]\n        return [1]*len(tokens)\n    else:\n        return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\n# needed to sentencize (we don't use it really).\ndef get_segments(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        #Cutting down the excess length\n        tokens = tokens[:max_seq_length]\n        segments = []\n        current_segment_id = 0\n        for token in tokens:\n            segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n        return segments\n    else:\n        segments = []\n        current_segment_id = 0\n        for token in tokens:\n            segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n        return segments + [0] * (max_seq_length - len(tokens))\n# a bow model\ndef get_ids(tokens, tokenizer, max_seq_length):    \n    if len(tokens)>max_seq_length:\n        tokens = tokens[:max_seq_length]\n        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n        return token_ids\n    else:\n        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n        return input_ids\n\n# create a vocabulary\nvocab_file = Bert_layer.resolved_object.vocab_file.asset_path.numpy()\n# lower case bert function\ndo_lower_case = Bert_layer.resolved_object.do_lower_case.numpy()\n# tokenizer\ntokenizer = FullTokenizer(vocab_file, do_lower_case)\n\n# preprocessing function\ndef bert_encode(texts):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        stokens = tokenizer.tokenize(text)\n        stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n        \n        input_ids = get_ids(stokens, tokenizer, max_seq_length)\n        input_masks = get_masks(stokens, max_seq_length)\n        input_segments = get_segments(stokens, max_seq_length)\n        \n        all_tokens.append(input_ids)\n        all_masks.append(input_masks)\n        all_segments.append(input_segments)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef review_to_words(raw_body):\n    body_text = BeautifulSoup(raw_body).get_text() \n    return(body_text)  ","61bfd3f2":"print(\"Esempio input:\")\nprint(bert_encode(train[1]))\n\ntrain_text = [review_to_words(x) for x in train]\ntest_text =  [review_to_words(x) for x in test]\n\ntrain_input = bert_encode(train_text)\ntest_input = bert_encode(test_text)\n\ntest_labels = test_labels.astype(float)\ntrain_labels = train_labels.astype(float)","b73de042":"# Define the model.\nMETRICS = [ \n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.AUC(name='auc'),\n]\n\ninput_word_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n\n_ , sequence_output = Bert_layer([input_word_ids, input_mask, segment_ids])\nclf_output = sequence_output[:, 0, :] # opzione 1\nmean = tf.reduce_mean(sequence_output, 1) # opzione 2\nout = Dense(512, activation='relu')(clf_output)\n# note: here we could insert a dropout layer: we chose not to put it (so drop = 0) because drop exploration (see Appendix B) pointed no signicant performance oscillation when we vary the drop.\nfinal = Dense(1, activation='sigmoid')(out)\nmodel = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=final)\n# compile and summary\nmodel.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=METRICS)\nmodel.summary()","5876f9c4":"# plot model structure\nplot_model(model,show_layer_names  = False,show_shapes = True)","c6611351":"# early stopping\ncallbacks = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        min_delta=1e-2,\n        patience=2,\n        verbose=1)\n]\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=5,\n    batch_size=12,\n    callbacks=callbacks\n)\n\n#model.save('model.h5')","ecd2f24f":"# predict\nprint('\\n# Evaluate on test data')\nresults = model.evaluate(test_input, test_labels, batch_size=12)\nprint('test loss, test acc, test auc:', results)","fe26cf03":"# plot performance-related parameters. Once set drop = 0 (see Appendix B), we trained up to 15 epocs to explore the model.\nwith open('..\/input\/bert-final-data\/trainHistoryDict', 'rb') as f:\n        long = pickle.load(f)\n        f.close()\n\nfigure(figsize=(25, 9))\ni = 0\nepoch = list(range(15))\n\nfor _ in [\"auc\",\"accuracy\",\"loss\"]:\n        i = i+1\n        plt.subplot(1, 3, i)\n        val = long[\"val_\"+_]\n        train = long[_]\n        plt.title(_ ,fontsize=20)\n        plt.plot(epoch,val,train)\n        \nplt.tight_layout()","5a302d6c":"Bert_layerL = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\",trainable=True)\n\nvocab_file = Bert_layerL.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = Bert_layerL.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)\n\ndef bert_encode(texts):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        stokens = tokenizer.tokenize(text)\n        stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n        \n        input_ids = get_ids(stokens, tokenizer, max_seq_length)\n        input_masks = get_masks(stokens, max_seq_length)\n        input_segments = get_segments(stokens, max_seq_length)\n        \n        all_tokens.append(input_ids)\n        all_masks.append(input_masks)\n        all_segments.append(input_segments)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ntrain_input = bert_encode(train_text)\ntest_input = bert_encode(test_text)\n\ntest_labels = test_labels.astype(float)\ntrain_labels = train_labels.astype(float)\n\ninput_word_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n\n_ , sequence_output = Bert_layerL([input_word_ids, input_mask, segment_ids]) \nclf_output = sequence_output[:, 0, :] \nmean = tf.reduce_mean(sequence_output, 1) \nout = tf.keras.layers.Dense(512, activation='relu')(clf_output) #We tried mean three times, with similar results. We couldn't perform futher investigation because of computational limits.\nfinal = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n\n    \nmodelL = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=final)\nmodelL.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=METRICS) \n\ncallbacksL = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        min_delta=1e-2,\n        patience=2,\n        verbose=1)\n]\n\ntrain_historyL = modelL.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=4,\n    batch_size=4,\n    callbacks=callbacksL\n)","22bf2f34":"# predict Large\nprint('\\n# Evaluate on test data')\nresultsL = modelL.evaluate(test_input, test_labels, batch_size=4)\nprint('test loss, test acc, test auc:', resultsL)","83553467":"for i in [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]:\n    with open('..\/input\/bert-final-data\/histories\/trainHistoryDict_0.'+ i, 'rb') as f:\n        globals()[\"history\"+i] = pickle.load(f)\n        f.close()\n        \nprint(\"history_0['roc'] = \",max(history0['val_auc']),\"\\n\", \"history_1['roc'] = \",max(history1['val_auc']),\"\\n\",\"history_2['roc'] = \",max(history2['val_auc']),\"\\n\",\"history_3['roc'] = \",max(history3['val_auc']),\"\\n\",\n     \"history_4['roc'] = \",max(history4['val_auc']),\"\\n\",\"history_5['roc'] = \",max(history5['val_auc']),\"\\n\",\"history_6['roc'] = \",max(history6['val_auc']),\"\\n\",\"history_7['roc'] = \",max(history7['val_auc']),\"\\n\",\n     \"history_8['roc'] = \",max(history8['val_auc']),\"\\n\",\"history_9['roc'] = \",max(history9['val_auc']),\"\\n\")\n\n\nprint(\"history_0['val_accuracy'] = \",max(history0['val_accuracy']),\"\\n\", \"history_1['val_accuracy'] = \",max(history1['val_accuracy']),\"\\n\",\"history_2['val_accuracy'] = \",max(history2['val_accuracy']),\"\\n\",\"history_3['val_accuracy'] = \",max(history3['val_accuracy']),\"\\n\",\n     \"history_4['val_accuracy'] = \",max(history4['val_accuracy']),\"\\n\",\"history_5['val_accuracy'] = \",max(history5['val_accuracy']),\"\\n\",\"history_6['val_accuracy'] = \",max(history6['val_accuracy']),\"\\n\",\"history_7['val_accuracy'] = \",max(history7['val_accuracy']),\"\\n\",\n     \"history_8['val_accuracy'] = \",max(history8['val_accuracy']),\"\\n\",\"history_9['val_accuracy'] = \",max(history9['val_accuracy']),\"\\n\")\n\nfigure(figsize=(25, 60))\ni = 0\ndrop = 0\nepoch = list(range(6))\nfor history in (history0,history1,history2,history3,history4,history5,history6,history7,history8,history9):\n    drop = drop + 1\n    for _ in [\"auc\",\"accuracy\",\"loss\"]:\n        i = i+1\n        plt.subplot(10, 3, i)\n        val = history[\"val_\"+ _]\n        train = history[_]\n        plt.title(_ + \", drop = 0.\" + str(drop),fontsize=20)\n        plt.plot(epoch,val,train)\n        \nplt.tight_layout()","485916e3":"Image(width= \"100%\",filename= r'..\/input\/bert-final-data\/LAR2.png')","8da37f50":"## Data Encoding","d8c8c71d":"### Model Training & Validation\n\nWe train on 20000 and validate on 5000 with early stopping (once set drop = 0 via the exploration). This training has been performed using kaggle's GPU. An unsuccesful attempt with TPU has been done, see [our GitHub repo](https:\/\/github.com\/InPhyT\/DataMiningBert). ","bd9bef19":"The following plots have been outputted separately because of computational limitations. ","f19655ac":"#### Note: to achieve better performance, one may retrain the model on all 25000 data. It has been not done here for computational time reasons.\n\n\n```python\ntrain_history = modelL.fit(\n    train_input, train_labels,\n    epochs=5,\n    batch_size=12,\n    callbacks=callbacksL\n)\n```","3bc32c80":"# Data Mining Project\n\n\n## Problem: *Sentiment Classification*\n\nA sentiment classification problem consists, roughly speaking, in detecting a piece of text and predicting if the author likes or dislikes what he\/she is talking about: the input $X$ is a piece of text and the output $Y$ is the sentiment we want to predict, such as the rating of a movie review.\n\nIf we can train a model to map $X$ to $Y$ based on a labelled dataset then it can be used to predict sentiment of a reviewer after watching a movie.\n\n\n## Data: *Large Movie Review Dataset v1.0*\n\nThe [dataset](https:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz) contains movie reviews along with their associated binary sentiment polarity labels. \n\n* The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. \n* The overall distribution of labels is balanced (25k pos and 25k neg). \n* 50,000 unlabeled documents for unsupervised learning are included, but they won't be used. \n* The train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.  \n* In the labeled train\/test sets, a negative review has a score $\\leq$ 4 out of 10, and a positive review has a score $\\geq$ 7 out of 10. Thus reviews with more neutral ratings are not included in the train\/test sets. \n* In the unsupervised set, reviews of any rating are included and there are an even number of reviews > 5 and $\\leq$ 5.\n\n#### Reference \nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). [Learning Word Vectors for Sentiment Analysis](https:\/\/ai.stanford.edu\/~amaas\/papers\/wvSent_acl2011.pdf). The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n\n## Theory\n\n### The encoder-decoder sequence\n\nRoughly speaking, an encoder-decoder sequence is an ordered collection of steps (*coders*) designed to automatically translate  sentences from a language to another (e.g. the English \"the pen is on the table\" into the Italian \"la penna \u00e8 sul tavolo\"), which could be useful to visualize as follows: **input sentence** \u2192 (*encoders*) \u2192 (*decoders*) \u2192 **output\/translated sentence**. <br>\n\nFor our practical purpose, encoders and decoders are effectively indistinguishable (that's why we will call them *coders*): both are composed of two layers: a **LSTM or GRU neural network** and an **attention module (AM)**. They only differ in the way in which their output is processed. \n\n#### LSTM or GRU neural network\nBoth the input and the output of an LSTM\/GRU neural network consists of two vectors: \n1. the **hidden state**: the representation of what the network has learnt about the sentence it's reading;\n2. the **prediction**: the representation of what the network predicts (e.g. translation). \n\nEach word in the English input sentence is translated into its word embedding vector (WEV) before being processed by the first coder (e.g. with `word2vec`). \nThe WEV of the first word of the sentence and a random hidden state are processed by the first coder of the sequence. Regarding the output: the prediction is ignored, while the hidden state and the WEV of the second word are passed as input into the second coder and so on to the last word of the sentence. Therefore in this phase the coders work as *encoders*.\n\nAt the end of the sequence of N encoders (N being the number of words in the input sentence), the **decoding phase** begins: \n1. the last hidden state and the WEV of the \"START\" token are passed to the first *decoder*;\n2. the decoder outputs a hidden state and a prection; \n3. the hidden state and the prediction are passed to the second decoder; \n4. the second decoder outputs a new hidden state and the second word of the translated\/output sentence\n\nand so on up until the whole sentence has been translated, namely when a decoder of the sequence outputs the WEV of the \"END\" token. Then there is an external mechanism to convert prediction vectors into real words, so it's very importance to notice that **the only purpose of decoders is to predict the next word**.  \n\n#### Attention module (AM)\n\nThe attention module is a further layer that is placed before the network which provides the collection of words of the sentence with a relational structure. Let's consider the word \"table\" in the sentence used as an exampe above. Because of the AM, the encoder will weight the preposition \"on\" (processed by the previous encoder) more than the article \"the\" which refers to the subject \"cat\". \n\n### Bidirectional Encoder Representations from Transformers (BERT)\n\n#### Transformer\nThe transformer is a coder endowed with the AM layer. Transformers have been observed to work much better than the basic encoder-decoder sequences.\n\n#### BERT-Base\n\nBERT is a sequence of encoder-type transformers which was pre-trained to *predict* a word or sentence (i.e. used as decoder). The benefit of improved performance of Transformers comes at a cost: the loss of *bidirectionality*, which is the ability to predict both next word and the previous one. BERT is the solution to this problem, a Tranformer which preserves *biderectionality*.\n\n##### Notes  \nThe first token is not \"START\". In order to use BERT as a pre-trained language model for sentence-classification, we need to input the BERT prediction of \"CLS\" into a linear regression because \n* the model has been trained to predict the next sentence, not just the next word; \n* the semantic information of the sentence is encoded in the prediction output of \"CLS\" as a document vector of 512 elements.\n\n## Documentation \nThe full description of the project can be found in this [Github page](https:\/\/inphyt.github.io\/DataMiningBert\/), while all the relevant notebooks are publicly available in the associated [Github repository](https:\/\/github.com\/InPhyT\/DataMiningBert).","a472222d":"## BERT-Large \n\nAlthough we've not been able to explore all possibilities because of computational limits, BERT-Large shows better performance than BERT-Base as expected.  ","367f9bbf":"# Code\n## Import Modules","c254a591":"## Appendix A: Data Preprocessing\n\nThe [dataset](https:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/) could not be directly imported into kaggle, so we pre-processed it with the following code on a local machine.\n\n```python\nimport os\nimport re\nimport numpy as np\nfrom os import listdir\n\ndocneg = []\ndocpos = []\n# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename,encoding=\"utf8\")\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n \ndef process_docs(directory,doc):\n    # walk through all files in the folder\n    for filename in listdir(directory):\n        # skip files that do not have the right extension\n        if not filename.endswith(\".txt\"):\n            continue\n        # create the full path of the file to open\n        path = directory + '\/' + filename\n        # load document\n        doc.append([load_doc(path)]) \n        \n\ndirectoryn = 'IMBDdataset\/neg'\ndirectoryp = 'IMBDdataset\/pos'\nprocess_docs(directoryn,docneg)\nprocess_docs(directoryp,docpos)\ndocneg = np.array(docneg)\ndocpos = np.array(docpos)\ndocneg = np.insert(docneg, 0, np.ones(docneg.shape[0]), axis=1)\ndocpos = np.insert(docpos, 0, np.zeros(docpos.shape[0]), axis=1)\n\n#concatenate and shuffle\ndata = np.comcatenate((docneg,docpos), axis = 0)\nnp.random.shuffle(data)\n\nlabels = data[:,0]\ndata = data[:,1]\n\nnp.savez_compressed(\"data\",a=data1,b=labels)\n```\n\n## Appendix B: Drop Exploration\n\nWe explored drop values [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] For each of them, the ROC, accuracy and loss have been saved and plotted as function of the epoch. This is the code we used. \n\n**NB**: Do not run, as it takes about 7 hours with the kaggle GPU for 3 drop values.\n\n```python\nmax_seq_length = 512  \nmax_len = max_seq_length\n\ndropouts = [] #Three values per run\n\nMETRICS = [ \n      tensorflow.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tensorflow.keras.metrics.AUC(name='auc'),\n]\n\ninput_word_ids = tensorflow.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tensorflow.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tensorflow.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\nfor drop in dropouts:\n    bert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",trainable=True)\n    _ , sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) #[batch_size, 768]\n    clf_output = sequence_output[:, 0, :] #\n    mean = tf.reduce_mean(sequence_output, 1) #testare due opzioni\n    out = tensorflow.keras.layers.Dense(768, activation='relu')(mean)\n    out = tf.nn.dropout(out,drop)\n    final = tensorflow.keras.layers.Dense(1, activation='sigmoid')(out)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=final)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=METRICS)\n    train_labels = train_labels.astype(float)\n    history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=6,\n    batch_size=12,\n    verbose = 100),\n    with open('..\/working\/trainHistoryDict'+'_'+str(drop), 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)\n    \n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy_'+str(drop))\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(r\"..\/working\/accuracy\"+\"_\"+str(drop)+\".png\")\n    plt.show()\n     \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss_'+str(drop))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(r\"..\/working\/loss\"+\"_\"+str(drop)+\".png\")\n    plt.show()\n     \n    plt.plot(history.history['auc'])\n    plt.plot(history.history['val_auc'])\n    plt.title('model roc_'+str(drop))\n    plt.ylabel('roc')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(r\"..\/working\/roc\"+\"_\"+str(drop)+\".png\")\n    plt.show() \n    \n```","a661536e":"## Appendix C: Learning curve scikit's style (but with no CV)\n\nIn this appendix, we study the learning curve emulating what scikit's [`learning_curve`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.learning_curve.html) does, even though with no cross-validation. \n\n```python\ndef bert():\n    Bert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",trainable=True)\n    max_seq_length = 512\n    METRICS = [ \n          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n          tf.keras.metrics.AUC(name='auc'),\n    ]\n\n    input_word_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n\n    _ , sequence_output = Bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :] # opzione 1\n    mean = tf.reduce_mean(sequence_output, 1) # opzione 2\n    out = Dense(512, activation='relu')(clf_output)\n    # note: here we could insert a dropout layer: we chose not to put it (so drop = 0) because drop exploration (see Appendix B) pointed no signicant performance oscillation when we vary the drop.\n    final = Dense(1, activation='sigmoid')(out)\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=final)\n    # compile and summary\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=METRICS)\n    return model\n\n\n# function to split the dataset in varying train_sizes.\ndef bsp(a):\n    l = len(train_input[0])\n    f_t,f_v = train_input[0][:m.floor((l\/10.)*a)], train_input[0][m.ceil((l\/10.)*a):]\n    s_t,s_v = train_input[1][:m.floor((l\/10.)*a)], train_input[1][m.ceil((l\/10.)*a):]\n    t_t,t_v = train_input[2][:m.floor((l\/10.)*a)], train_input[2][m.ceil((l\/10.)*a):]\n    t_labels, v_labels = train_labels[:m.floor((l\/10.)*a)], train_labels[m.ceil((l\/10.)*a):]\n    return ([f_t,s_t,t_t],t_labels,[f_v,s_v,t_v],v_labels)\n\ntv_list = [bsp(1), bsp(2),bsp(3),bsp(4),bsp(5),bsp(6),bsp(7),bsp(8),bsp(9)] \n\nhistories = []\nscores = []\ni = 0\nfor train, train_labels, val, val_labels in tv_list:\n    model = bert()\n    histories.append(model.fit(train, train_labels, epochs = 1, batch_size = 12, verbose = 10, validation_data =(val, val_labels)))\n    scores.append(model.evaluate(test_input, test_labels, batch_size=12))\n    \nwith open(\"..\/working\/histories\",\"wb\") as f:\n    pickle.dump(histories, f)\n    f.close()\n    \nwith open(\"..\/working\/scores.txt\",\"wb\") as f:\n    pickle.dump(scores,f)\n    f.close()\n    \n    \ntrain_sizes = [2500, 5000, 7500, 10000, 12500, 15000,17500, 20000,22500]\n# Value registrered from multiple external runs. They indicatively come from the scores array.\nval_loss = [0.3274896603425344,0.26186237920895217,0.23470225021753993,0.22762819954156877,\n           0.216497525832057,0.2152899844624102,0.20779933777451515,0.21191885358542203,\n           0.20974656864404678]\nval_acc = [0.8679111,0.89575,0.90445715,0.90966666,0.91568,0.9156,0.91746664,0.9148,0.9168]\nval_auc = [0.94070125,0.96067715,0.96674746,0.9698138,0.97157085,0.9726143,0.97413814,0.9736698,\n          0.97376865]\ntest_loss =[0.3185957675123215,0.25469263674438,0.22447200367122888,0.21847399472922086,\n            0.20618302688077092,0.20828087054461242,0.1975343844014406,0.195971505715549,\n           0.19205443421170115] \ntest_acc = [0.87296,0.8956,0.9102, 0.91256,0.91952,0.91804,0.92284,0.9228,0.92576]\ntest_auc = [ 0.9453321,0.9641768,0.9700355,0.97299325,0.97432345,0.9750733,0.97692096, 0.97718656,\n           0.9778782]\nplt.figure(figsize=(25,6.5))\nplt.subplot(1, 3, 1)\n\nplt.title(\"loss\" ,fontsize=20)\nplt.xlabel(\"train_size\")\nplt.ylabel(\"loss\")\n\nplt.plot(train_sizes,val_loss, \"o-\", label = \"val_loss\")\nplt.plot(train_sizes,test_loss, \"o-\", label = \"test_loss\")\nplt.legend()\n\nplt.subplot(1, 3, 2)\n\nplt.title(\"accuracy\" ,fontsize=20)\nplt.xlabel(\"train_size\")\nplt.ylabel(\"accuracy\")\n\nplt.plot(train_sizes,val_acc, \"o-\", label = \"val_acc\")\nplt.plot(train_sizes,test_acc, \"o-\", label = \"test_acc\")\nplt.legend()\n\nplt.subplot(1, 3, 3)\n\nplt.title(\"roc_auc\" ,fontsize=20)\nplt.xlabel(\"train_size\")\nplt.ylabel(\"roc_auc\")\n\nplt.plot(train_sizes,val_auc, \"o-\", label = \"val_auc\")\nplt.plot(train_sizes,test_auc, \"o-\", label = \"test_auc\")\nplt.legend()\nplt.savefig(r\"..\/working\/LAR1.png\")\n\nplt.tight_layout()\n```","e27cc937":"#### Note: to achieve better performance, one may retrain the model on all 25000 data.  It has been not done here for computational time reasons.\n\n```python\ntrain_history = model.fit(\n    train_input, train_labels,\n    epochs=5,\n    batch_size=12,\n    callbacks=callbacks\n)\n```","5ef87a0e":"## Data Loading","2c7163a1":"For further details on parameters setting (seq length and gpu specifics) see the [github repo](https:\/\/github.com\/google-research\/bert) by Google. ","32c7adfc":"## Model Definition"}}