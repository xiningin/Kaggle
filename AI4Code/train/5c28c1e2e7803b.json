{"cell_type":{"4382e2f0":"code","868339c2":"code","e8ab26ee":"code","8d32630e":"code","fa29a5bf":"code","edc45cd5":"code","8b0017b0":"code","d97add90":"code","9ca36c54":"code","f6d88c87":"code","29b9d354":"code","5b2cb35c":"code","5f37148c":"code","6f991537":"code","6d695640":"code","9af38c1e":"code","e710487c":"code","e8c660cb":"code","c34d612e":"code","d83d4277":"code","d79f904d":"code","67ad2691":"code","dd6b6909":"code","59ec982f":"code","e49b62c4":"code","94812974":"markdown","8be29544":"markdown","60545d46":"markdown","313be6e7":"markdown","b952bb2d":"markdown","d40bd2c2":"markdown","36779639":"markdown","6363c6fd":"markdown","c615b451":"markdown","48edcc11":"markdown","da572dea":"markdown","8b7e1ceb":"markdown","95ebd945":"markdown","c9e3c987":"markdown","3f44f333":"markdown","74fffe7e":"markdown","c9e30d08":"markdown","70fc6942":"markdown","3d21a937":"markdown","afd1fb71":"markdown","b9a8543e":"markdown"},"source":{"4382e2f0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom xgboost import XGBRegressor","868339c2":"!tree ..\/input\/","e8ab26ee":"df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv', \n           parse_dates=['date'], \n           index_col = ['date'])\ndf.head(5)","8d32630e":"print('Country info in the data-\\n', df['country'].value_counts(), ' \\n')\nprint('Store info in the data-\\n', df['store'].value_counts(), ' \\n')\nprint('Product info in the data-\\n', df['product'].value_counts(), ' \\n')","fa29a5bf":"df = df.loc[(df['country']=='Sweden') & (df['store']=='KaggleMart') & (df['product']=='Kaggle Mug')]\ndf = df.drop(['row_id', 'country', 'store', 'product'], axis=1)\ndf = df[:'2015-12-31']\ndf","edc45cd5":"plt.figure(figsize=(12,6))\nplt.plot(df['num_sold'])","8b0017b0":"def moving_avg(col, last_n):\n    '''Calculate of the average of last n observations'''\n    \n    return np.mean(col[-last_n:])\n\nmoving_avg(df, 30)","d97add90":"moving_avg = df.rolling(window=30).mean()\n#Pandas way of calculating the same for every 30 days automatically\n\nplt.figure(figsize=(15, 5))\nplt.title(\"Moving average\\n window size = {}\".format(30))\nplt.plot(moving_avg, label=\"Rolling Average\")\nplt.plot(df[30:], label='actual values')\nplt.legend(loc='best')","9ca36c54":"w_moving_avg = df.rolling(window=30, win_type='cosine').mean()\n# The weight comes from the win_type parameter. Check out the official docs for all available parameter options\n\nplt.figure(figsize=(15, 5))\nplt.title(\"Moving average\\n window size = {}\".format(30))\nplt.plot(w_moving_avg, label=\"Rolling Average\")\nplt.plot(df[30:], label='actual values')\nplt.legend(loc='best')","f6d88c87":"ses = smt.SimpleExpSmoothing(df, \n                            initialization_method='heuristic').fit(smoothing_level=0.3, \n                                                                    optimized=False)","29b9d354":"plt.figure(figsize=(12,8))\nplt.plot(df, marker=\"o\", color=\"orange\")\nplt.plot(ses.fittedvalues, marker=\"o\", color=\"blue\")\nplt.legend(loc='best')","5b2cb35c":"holt = smt.Holt(endog=df,\n            initialization_method='heuristic').fit(smoothing_level=0.3, \n                                                   smoothing_trend =0.9, \n                                                   optimized=False)","5f37148c":"plt.figure(figsize=(15,7))\nplt.plot(df, marker=\"o\", color=\"orange\")\nplt.plot(holt.fittedvalues, marker=\"o\", color=\"blue\")","6f991537":"y = pd.Series(df.num_sold)\np_value = sm.tsa.stattools.adfuller(y)[1]\nprint(p_value)","6d695640":"y = pd.Series(np.log(df[\"num_sold\"])).diff().dropna()\np_value = sm.tsa.stattools.adfuller(y)[1]\nprint(p_value)","9af38c1e":"data = pd.DataFrame(df.num_sold.copy())\ndata.columns = [\"y\"]\nfor i in range(6, 20):\n    data[\"lag_{}\".format(i)] = data.y.shift(i)\n\ndata.tail(5)","e710487c":"data.index = pd.to_datetime(data.index)\ndata[\"weekday\"] = data.index.weekday\ndata[\"is_weekend\"] = data.weekday.isin([5, 6]) * 1\ndata.tail(5)","e8c660cb":"train = data[:'2015-11-30'].copy()\ntest = data['2015-12-01':].copy()","c34d612e":"train.tail(5)","d83d4277":"test.tail(5)","d79f904d":"y_train = train.dropna().y\nX_train = train.dropna().drop([\"y\"], axis=1)\n\ny_test = test.dropna().y\nX_test = test.dropna().drop([\"y\"], axis=1)","67ad2691":"def plot_results(model, X_train = X_train, X_test = X_test):\n    \n    '''Helper function to plot the actual & predicted values from the model'''\n    \n    prediction = model.predict(X_test)\n\n    plt.figure(figsize=(15, 7))\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    error = metrics.mean_squared_error(prediction, y_test)\n    plt.title(\"Mean squared error {0:.2f}\".format(error))\n    plt.legend(loc='best')","dd6b6909":"scaler = preprocessing.StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","59ec982f":"xgb = XGBRegressor(verbosity=0)\nxgb.fit(X_train_scaled, y_train)","e49b62c4":"plot_results(xgb,X_train=X_train_scaled,X_test=X_test_scaled)","94812974":"# EXPONENTIAL SMOOTHING\n\nInstead of weighting the last K values, we weight all the data points. Here as we move back in time, the weights are exponentially decreased. It can be expressed by:\n\n$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1}$$\n\nHere, aplha is the weight of smoothing factor. It defines how much the last observation affects the current. The smaller the alpha, the more influence the previous data point has --> the statsmodels parameter for this is 'smoothing_level'","8be29544":"# Stationarity\n\nStationarity mean that the time-series properties like mean, variance etc stay the same throughout the series. It is easier to make predictions on a time-series if we know that the future properties of the series will same as the current data points\n\nWe can check for stationarity of a time series using the *dickey-fuller test*. The resuling p value should be less than 0.05 to reject the null hypothesis i.e to reject that the series is non-stationary","60545d46":"**I am leaving the implementation of Holt-Winters as exercise for the reader.**\n\n**Even I want to implement holt-winters from scratch in plain python. So expect a separate notebook on it in the near future**","313be6e7":"# Boosting models for time-series\n\nTo create and validate an ML model, we need train & test set. \n\nHowever, in time-series we cannot use the scikit's learning train_test_split() function as it shuffles the data & thereby messing with the time factor. So to keep the series intact, I have reserved the decemeber month's data as test set & the rest as train set","b952bb2d":"# Holt-Winter model\n\nAlong with trend & level, we add the seasonal component here. So the formula becomes:\n\n$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n\n$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n\nHere, gamma is the seasonal component","d40bd2c2":"# Holt model\n\nPreviously, we only took the smoothing level into account. Now we also take the smoothing trend into account. So the formula is updated as: \n\n$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n\nHere, beta is the trend factor. The final formula as hown depends on both trend & level","36779639":"We only use the training data from TPS for this tutorial","6363c6fd":"# Share your thoughts & feedback in the comments\n##### Stay safe & enjoy a happy 2022 everyone. Cheers!","c615b451":"# MOVING AVERAGE\n\nMoving average is nothing but the average of last n observations in a rolling manner i.e an average of every 30 data points. In our case, here I take the average of last 30 observations","48edcc11":"Another technique is to extract the date properties like weekday, weekend, hour etc as separate features","da572dea":"The input features of the data here is in different scales with lag, weekday & is_weekend. We need to scale the data before proceeding to create a boosting model","8b7e1ceb":"### In Part 2 of the notebook, I will discuss how to optimize the prediction of this XGBoost model & discuss some advance concepts including:\n## - Time-series cross validation\n## - ARIMA family of models\n## - Reading ACF & PACF plots\n## - LSTM based time-series\n## - Prophet library for time series","95ebd945":"# TIME SERIES tutorial using JAN 2022 TPS data \ud83d\udcc8\ud83d\udcca\n\nHi to the community!\n\nIn this public notebook, I build a tutorial on time series using both classic and ML based methods. Some of the concepts we will be discussing includes:\n- Moving Average\n- Weighted Moving Average\n- Exponential Smoothing\n- Holt model\n- Holt-Winter model\n- Stationarity\n- Feature extraction for time-series\n- Boosting models for time-series\n\nI also use the following libraries to the create the models:\n- Statsmodels\n- Scikit-learn\n- XGBoost\n\n**Note: This is a tutorial on time series using the TPS data. Therefore, I do not focus on the full data or a submission. I leverage the TPS data in the simplest way possible to teach\/practise a tutorial on time-series**","c9e3c987":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give an upvote before forking it <\/center><\/h2>","3f44f333":"There is a huge error in predicting the last 5 days of the month. Others have been predicted decently well","74fffe7e":"Since the data is equally distributed across the various categories in the columns, I am filtering the data for the following values, so that the data is simple for the tutorial:\n- Country : Sweden\n- Store: KaggleMart\n- Product: Kaggle Mug\n- Index: Only 1 year of data i.e the year of 2015","c9e30d08":"Here, the p-value id 0.9, which is greater than 0.05. So the null hypothesis told true & the series is non-stationary","70fc6942":"One way to deal with stationarity is to take the log of the series & then take the first difference i.e the difference of the series with itself\n\n(Please breakdown the code in the following cell to see the differencing process in action)","3d21a937":"# Feature extraction for time-series\n\nAny ML model needs features, but all we have here is a 1-dimensional feature (which we simplified wantedly). So, based on this 1-d feature, how to get more features? Thats where a few feature extraction techniques come in handy\n\nOne such technique is to calculate the lag iteratively for a given range","afd1fb71":"Now, the p-value is 0.002 i.e 2%. Therefore we can reject the null hypothesis & say that the series is stationary","b9a8543e":"# WEIGHTED MOVING AVERAGE\nA slightly more complex approach is possible, which is weighted moving average - the total sums up to 1, where the recent K data points are given more weight"}}