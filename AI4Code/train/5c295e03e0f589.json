{"cell_type":{"3dc647c1":"code","574aa6c7":"code","518231e2":"code","36c83f20":"code","2e90096a":"code","aca9c51b":"code","a8466d1e":"code","106c21d4":"code","ae2be806":"code","1bf33400":"code","6705f99b":"code","00abd75a":"code","a6532f20":"code","97994c41":"code","e45a1731":"code","07240303":"code","e6a5f032":"code","e934747b":"code","1fef3680":"code","e5b55830":"code","2265f81e":"code","a8620828":"code","fcc3327c":"code","c0d6376c":"code","447e603a":"code","6369544b":"code","17e789de":"code","ae6f5365":"code","261176d6":"code","537577ea":"code","47267b31":"code","faa2fe8f":"code","01ce26a4":"code","2c8d0d20":"code","dbbb03d3":"code","887855e6":"code","0eb7fe66":"code","ff620ba5":"code","4bfbba11":"code","824912a6":"code","7064a49d":"code","50d9f595":"code","1b8b0ce5":"code","4aad753b":"code","19d2b5dc":"code","f8b0151b":"code","30ee9919":"code","837f0bdc":"code","64a26a7c":"code","17d5b896":"code","30bb62d9":"code","583bb962":"code","0733bedf":"code","28db4909":"code","23f94709":"code","9b48cbed":"code","fb1dac90":"code","1a8ccf17":"code","09c8cfc5":"code","149921c7":"code","6229bbd9":"code","8b458887":"code","ebfcbc51":"code","148c607f":"code","2b9ee836":"code","76246783":"code","99c3441c":"code","e83d5de8":"code","392a17b8":"code","67f0663f":"code","1c43df38":"code","394ec630":"code","95c87f05":"code","24616b31":"code","0f10df63":"code","c44ea0b7":"code","a2820c1c":"code","6a56f317":"code","0e95507d":"code","327729f1":"code","9704e2cf":"code","6887741a":"markdown","e80fcacd":"markdown","ecb6bd99":"markdown","f889a0c1":"markdown","bf72aa32":"markdown","0202722f":"markdown","4ec98e28":"markdown","24c181d4":"markdown","ff3ca66b":"markdown","2f4a0ece":"markdown","e091689f":"markdown","71746bd5":"markdown","3510d3f6":"markdown","7abaa00e":"markdown","d8eee08f":"markdown","ebc6f6ab":"markdown","821e36bb":"markdown","e8895632":"markdown","bb0a1cd9":"markdown","076151b2":"markdown","ef35a01b":"markdown","e3669576":"markdown","b6e7bcb0":"markdown","d05da95c":"markdown","389318c8":"markdown","ab3e905e":"markdown","ea8222e8":"markdown","2e05f4fc":"markdown","604b7401":"markdown","7a3c69c2":"markdown","65993ef3":"markdown","02db78c8":"markdown","4d3d1968":"markdown","dbf531cf":"markdown","8a5a607a":"markdown","f7bc7d40":"markdown","1b3f3f8c":"markdown","aa673cac":"markdown","a0b8bd72":"markdown","393ff124":"markdown","d76aea20":"markdown","9de0ff29":"markdown","afbb38cf":"markdown","410cba49":"markdown","151655a2":"markdown","faccbff2":"markdown","e274f4c7":"markdown","616330b8":"markdown","7fb615d2":"markdown"},"source":{"3dc647c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","574aa6c7":"import matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\n\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"darkgrid\", color_codes=True)\n\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, classification_report","518231e2":"df = pd.read_csv(\"..\/input\/indian-liver-patient-records\/indian_liver_patient.csv\")","36c83f20":"df.shape","2e90096a":"df.dtypes","aca9c51b":"df.head(5)","a8466d1e":"df.tail(5)","106c21d4":"df.info()","ae2be806":"print(df.drop('Dataset', axis=1).dtypes)","1bf33400":"print(\"Dataset : values are {}, dtype is {}\".format(df['Dataset'].unique(),\n                                                          df['Dataset'].dtype))","6705f99b":"df.isna().sum()","00abd75a":"print(\"\\nThere are 4 Null\/Missing values in the dataset\\n\")","a6532f20":"df[df['Albumin_and_Globulin_Ratio'].isna()]    ","97994c41":"# Drop Nan values as there are only 4 NaN's\ndf.dropna(inplace=True)","e45a1731":"df.isna().sum().value_counts()","07240303":"(df.drop('Gender', axis=1) < 0).sum()","e6a5f032":"print(\"\\nThere are no Negative values in the dataset\\n\")","e934747b":"df.duplicated().sum()","1fef3680":"df[df.duplicated()]","e5b55830":"print(\"\\nThere are 13 duplicate records in the dataset\\n\")","2265f81e":"#Removing Duplicate Rows\n\ndf.drop_duplicates(inplace=True)","a8620828":"df.reset_index(drop=True, inplace=True)","fcc3327c":"#check changed shape\ndf.shape","c0d6376c":"#check columns\ndf.columns","447e603a":"num_columns = ['Age','Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase', \n               'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin', \n               'Albumin_and_Globulin_Ratio']","6369544b":"cat_columns = ['Gender','Dataset']","17e789de":"df.describe().T","ae6f5365":"df.describe().drop('count',axis=0).plot(figsize=(20,8))\nplt.show()","261176d6":"#Identifying Outliers in Numeric columns using IQR (Inter Quartile Range) and Q1 (25% Quantile), Q3(75% Quantile).\n\ndef identify_outliers(col):    \n    q1 = df[col].quantile(0.25)\n    q3 = df[col].quantile(0.75)\n    iqr = q3 - q1\n    lower_limit = q1 - 1.5*iqr\n    upper_limit = q3 + 1.5*iqr\n    return(col, q1, q3, iqr, lower_limit, upper_limit)","537577ea":"#Checking for Outliers and identifying them by calling identify_outliers() function.\n#observations below Q1- 1.5*IQR, or those above Q3 + 1.5*IQR  are defined as outliers.\n\nfor col in num_columns :\n    col, q1, q3, iqr, lower_limit, upper_limit = identify_outliers(col)\n    print(\"\\nColumn name : {}\\n Q1 = {} \\n Q3 = {}\\n IQR = {}\".format(col, q1, q3, iqr))\n    print(\" Lower limit = {}\\n Upper limit = {}\\n\".format(lower_limit, upper_limit))\n    outlier_count = len(df.loc[(df[col] < lower_limit) | (df[col] > upper_limit)])\n    if outlier_count != 0 :\n        print(outlier_count, \"OUTLIERS ARE PRESENT in {} column.\".format(col))\n        print(\"Outlier datapoints in {} column are:\".format(col))\n        print(np.array(df.loc[(df[col] < lower_limit) | (df[col] > upper_limit)][col]))\n    else:\n        print(\"OUTLIERS ARE NOT PRESENT in {} column\\n\".format(col))","47267b31":"#Visualizing Outliers in dataset using boxplot\n\nprint('\\n\\t\\tBoxplot to check the presence of outliers in numeric columns')\nprint('\\t\\t==============================================================\\n')\n#num_columns = ['Age','Income', 'CCAvg', 'Mortgage']\nfig, ax = plt.subplots(3,3,figsize=(15, 10))\nfor col,subplot in zip(num_columns,ax.flatten()) :\n    sns.boxplot(x=df[[col]], width=0.5, color='orange', ax=subplot)\n    #subplot.set_title('Boxplot for {}'.format(col))\n    subplot.set_xlabel(col)    \nplt.show()","faa2fe8f":"df[num_columns].var()","01ce26a4":"plt.xticks(rotation = 90, fontsize=10)\nplt.yticks(fontsize=10)\nplt.plot(df[num_columns].var(), color='green', marker='s',linewidth=2, markersize=5)\nplt.yscale('log')\nplt.show()","2c8d0d20":"fig, ax = plt.subplots(3,3,figsize=(15, 10))\nfor col,subplot in zip(num_columns,ax.flatten()) :\n    ax =sns.distplot(df[col], ax=subplot, hist_kws={'color':'g','alpha':1}, kde_kws={'color':'black', 'lw':2})","dbbb03d3":"# Apart from Dataset which is the Target column there is only one other categorical column, Gender\n# Value counts and distribution of Gender column\n\ndf.Gender.value_counts()","887855e6":"ax = sns.countplot(df.Gender)","0eb7fe66":"# The Target column is 'Dataset'.\n# Value counts and distribution of Target column\ndf.groupby(by='Dataset').count()","ff620ba5":"sns.countplot(df['Dataset'], palette = 'plasma')\nplt.show()","4bfbba11":"for col in df.drop('Dataset', axis=1).columns :\n    pd.crosstab(df[col], df['Dataset']).plot(kind='bar',color=('b', 'r'), figsize=(20,5))","824912a6":"sns.pairplot(vars=df.drop(['Gender', 'Dataset'], axis=1).columns,hue='Dataset',data=df)\nplt.show()","7064a49d":"#Dropping categorical column and target for finding correlation\ncorr = df[num_columns].corr()\ncorr.style.background_gradient(cmap='YlGnBu')","50d9f595":"plt.figure(figsize=(10,10))\nsns.heatmap(corr, annot=True, square=True)\nplt.show()","1b8b0ce5":"le = LabelEncoder()","4aad753b":"df['Gender'] = le.fit_transform(df['Gender'])","19d2b5dc":"le.classes_","f8b0151b":"df['Gender'].value_counts()","30ee9919":"X = df.drop('Dataset',axis=1)\ny = df['Dataset']","837f0bdc":"print('Shape of Feture-set : ', X.shape)\nprint('Shape of Target-set : ', y.shape)","64a26a7c":"(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.30, random_state=7)","17d5b896":"print(\"Training Set Shape:\\nFeatures : {0}  Target : {1}\\n\".format(X_train.shape, y_train.shape))\nprint(\"Test Set Shape:\\nFeatures : {0}  Target : {1}\".format(X_test.shape, y_test.shape))","30bb62d9":"#Standardization using Standard Scaler class of sklearn.preprocessing module\n\nscaler = StandardScaler().fit(X_train)","583bb962":"#Training set transformed to fit Standard Scaler\n\nX_trainS = scaler.transform(X_train)","0733bedf":"#Test set transformed to fit Standard Scaler\n\nX_testS = scaler.transform(X_test)","28db4909":"print(X_trainS.mean(), X_trainS.std())\nprint(X_testS.mean(), X_testS.std())","23f94709":"#DataFrame to store model Performance metrics of all the classification methods\ncompare_metrics_df = pd.DataFrame(index=('K-NearestNeighbors', 'Logistic Regression', 'Gaussian Naive Bayes'), \n                                  columns=('Trainingset Accuracy', 'Testset Accuracy', 'Precision Score', \n                                           'Recall Score', 'F1 Score', 'ROC_AUC Score'))","9b48cbed":"compare_metrics_df.index.name = 'Classifier Name'","fb1dac90":"#Implementing KNN Classifier for default k value 5\n\nknn_clf = KNeighborsClassifier(n_neighbors=5, weights='distance')","1a8ccf17":"#Fit the model to the training set\n\nknn_clf.fit(X_trainS, y_train)","09c8cfc5":"# Predict classes using the built model\n\nyhat_knn = knn_clf.predict(X_testS)","149921c7":"# Model accuracy score using score() function on Training data set\n\ncompare_metrics_df.loc['K-NearestNeighbors','Trainingset Accuracy'] = round(knn_clf.score(X_trainS, y_train), 2)\nknn_clf.score(X_trainS, y_train)","6229bbd9":"# Model accuracy score using score() function on Test data set\n\ncompare_metrics_df.loc['K-NearestNeighbors','Testset Accuracy'] = round(knn_clf.score(X_testS, y_test), 2)\nknn_clf.score(X_testS, y_test)","8b458887":"k_range = 100\nmean_train_acc_knn = np.zeros(k_range)\nmean_test_acc_knn = np.zeros(k_range)\n\nfor n in range(1,k_range+1) :\n    KNN = KNeighborsClassifier(n_neighbors=n, weights='distance')\n    KNN.fit(X_trainS, y_train)\n    mean_train_acc_knn[n-1] = KNN.score(X_trainS, y_train)\n    mean_test_acc_knn[n-1] = KNN.score(X_testS, y_test)","ebfcbc51":"print('\\nBest test accuracy is {0} for a K value of {1}'.format(mean_test_acc_knn.max(), mean_test_acc_knn.argmax()+1))\nprint('\\nThe train accuracy for best test accuracy is {}'.format(mean_train_acc_knn[mean_test_acc_knn.argmax()+1]))\nprint('\\nThe Best K-value for the classification is K = {}'.format(mean_test_acc_knn.argmax()+1))","148c607f":"confusion_matrix_knn = confusion_matrix(y_test, yhat_knn)\nconfusion_matrix(y_test, yhat_knn)","2b9ee836":"print(\"Accuracy Score: \",accuracy_score(y_test, yhat_knn))\ncompare_metrics_df.loc['K-NearestNeighbors','Precision Score'] = round(precision_score(y_test, yhat_knn), 2)\nprint(\"Precision Score: \",precision_score(y_test, yhat_knn))\ncompare_metrics_df.loc['K-NearestNeighbors','Recall Score'] = round(recall_score(y_test, yhat_knn), 2)\nprint(\"Recall Score: \",recall_score(y_test, yhat_knn))\ncompare_metrics_df.loc['K-NearestNeighbors','F1 Score'] = round(f1_score(y_test, yhat_knn), 2)\nprint(\"F1 Score: \",f1_score(y_test, yhat_knn))\ncompare_metrics_df.loc['K-NearestNeighbors','ROC_AUC Score'] = round(roc_auc_score(y_test, yhat_knn), 2)\nprint(\"ROC_AUC Score: \",roc_auc_score(y_test, yhat_knn))\nprint(\"Classification Report\\n\",classification_report(y_test, yhat_knn))","76246783":"#Implementing Logistic Regression Classifier\n\nlgr_clf = LogisticRegression(solver='lbfgs', random_state=7)","99c3441c":"#Fit the model to the training set\n\nlgr_clf.fit(X_trainS, y_train)","e83d5de8":"# Predict classes using the built model\n\nyhat_lgr = lgr_clf.predict(X_testS)","392a17b8":"# Model accuracy score using score() function on Training data set\n\ncompare_metrics_df.loc['Logistic Regression','Trainingset Accuracy'] = round(lgr_clf.score(X_trainS, y_train), 2)\nlgr_clf.score(X_trainS, y_train)","67f0663f":"# Model accuracy score using score() function on Test data set\n\ncompare_metrics_df.loc['Logistic Regression','Testset Accuracy'] = round(lgr_clf.score(X_testS, y_test), 2)\nlgr_clf.score(X_testS, y_test)","1c43df38":"confusion_matrix_lgr = confusion_matrix(y_test, yhat_lgr)\nconfusion_matrix(y_test, yhat_lgr)","394ec630":"print(\"Accuracy Score: \",accuracy_score(y_test, yhat_lgr))\ncompare_metrics_df.loc['Logistic Regression','Precision Score'] = round(precision_score(y_test, yhat_lgr), 2)\nprint(\"Precision Score: \",precision_score(y_test, yhat_lgr))\ncompare_metrics_df.loc['Logistic Regression','Recall Score'] = round(recall_score(y_test, yhat_lgr), 2)\nprint(\"Recall Score: \",recall_score(y_test, yhat_lgr))\ncompare_metrics_df.loc['Logistic Regression','F1 Score'] = round(f1_score(y_test, yhat_lgr), 2)\nprint(\"F1 Score: \",f1_score(y_test, yhat_lgr))\ncompare_metrics_df.loc['Logistic Regression','ROC_AUC Score'] = round(roc_auc_score(y_test, yhat_lgr), 2)\nprint(\"ROC_AUC Score: \",roc_auc_score(y_test, yhat_lgr))\nprint(\"Classification Report\\n\",classification_report(y_test, yhat_lgr))","95c87f05":"#Implementing Logistic Regression Classifier\n\ngnb_clf = GaussianNB()","24616b31":"#Fit the model to the training set\n\ngnb_clf.fit(X_trainS, y_train)","0f10df63":"# Predict classes using the built model\n\nyhat_gnb = gnb_clf.predict(X_testS)","c44ea0b7":"# Model accuracy score using score() function on Training data set\n\ncompare_metrics_df.loc['Gaussian Naive Bayes','Trainingset Accuracy'] = round(gnb_clf.score(X_trainS, y_train), 2)\ngnb_clf.score(X_trainS, y_train)","a2820c1c":"# Model accuracy score using score() function on Test data set\n\ncompare_metrics_df.loc['Gaussian Naive Bayes','Testset Accuracy'] = round(gnb_clf.score(X_testS, y_test), 2)\ngnb_clf.score(X_testS, y_test)","6a56f317":"confusion_matrix_gnb = confusion_matrix(y_test, yhat_gnb)\nconfusion_matrix(y_test, yhat_gnb)","0e95507d":"print(\"Accuracy Score: \",accuracy_score(y_test, yhat_gnb))\ncompare_metrics_df.loc['Gaussian Naive Bayes','Precision Score'] = round(precision_score(y_test, yhat_gnb), 2)\nprint(\"Precision Score: \",precision_score(y_test, yhat_gnb))\ncompare_metrics_df.loc['Gaussian Naive Bayes','Recall Score'] = round(recall_score(y_test, yhat_gnb), 2)\nprint(\"Recall Score: \",recall_score(y_test, yhat_gnb))\ncompare_metrics_df.loc['Gaussian Naive Bayes','F1 Score'] = round(f1_score(y_test, yhat_gnb), 2)\nprint(\"F1 Score: \",f1_score(y_test, yhat_gnb))\ncompare_metrics_df.loc['Gaussian Naive Bayes','ROC_AUC Score'] = round(roc_auc_score(y_test, yhat_gnb), 2)\nprint(\"ROC_AUC Score: \",roc_auc_score(y_test, yhat_gnb))\nprint(\"Classification Report\\n\",classification_report(y_test, yhat_gnb))","327729f1":"compare_metrics_df","9704e2cf":"print(\"Confusion Matrix of all the 3 models\")\nprint(\"====================================\")\nprint(\"\\nK-Nearest Neighbors:\\n\")\nprint(confusion_matrix_knn)\nprint(\"\\nLogistic Regression:\\n\")\nprint(confusion_matrix_lgr)\nprint(\"\\nGaussian Naive Bayes:\\n\")\nprint(confusion_matrix_gnb)","6887741a":"## Visualizing value counts of categorical columns through countplot","e80fcacd":"### Model Performance using KNN Classifier","ecb6bd99":"## Target column","f889a0c1":"### Observations on univariate analysis of numerical columns:\n - Distplots for all the columns show that the columns Total_Bilirubin, Direct_Bilirubin, Alkaline_Phosphotase, Alamine_Aminotransferase, Aspartate_Aminotransferase and Albumin_and_Globulin_Ratio are extremely right skewed with long tails to the right side of the distribution.For all these columns, the mean is greater than the median\n - Distribution of Age column is nearly normal and has very less skewness in both the tails.Mean and median are approximately equal in Age column\n - Distribution for Total_Protiens, Albumin and Albumin_and_Globulin_Ratio columns is also nearly normal.The mean and median meadin for these columns is also approximately equal","bf72aa32":"### Observations on outlier analysis of numerical columns:\n- Maximum number of outliers, which is 83 is seen in Total_Bilirubin column and is extremely right skewed\n- Age and Albumin columns do not have any outliers\n- Total_Protiens has 8 outliers and Albumin_and_Globulin_Ratio has 10 outliers\n- Direct_Bilirubin, Alkaline_Phosphotase, Alamine_Aminotransferase, Aspartate_Aminotransferase columns also have huge number of outliers and are extremely right skewed","0202722f":"## Correlation between numerical columns","4ec98e28":"### Load Data and read the data as a data frame","24c181d4":"### Outliers analysis of numerical columns","ff3ca66b":"### Build Logistic Regression Classification Model using train Dataset and predict the class on test dataset.","2f4a0ece":"### Build KNN Classification model using train Dataset and predict the class on test dataset.","e091689f":"# Univariate Analysis","71746bd5":"### Observations on bivariate analysis using pairplot:\n- From the pairplot we can see that the columns Total_Bilirubin and Direct_Bilirubin, Alamine_Aminotransferase and Aspartate_Aminotransferase, Total_Protiens and Albumin, Total_Protiens and Albumin_and_Globulin_Ratio, Albumin and Albumin_and_Globulin_Ratio show a positive correlation and the correlation appers to be strong\n- The formation of two good clusters is evident from the KDE plots alng the diagnol","3510d3f6":"# Bivariate Analysis","7abaa00e":"### Duplicated data","d8eee08f":"### Observations on distribution of Target column:\n- Liver patients with 60 years of age are the maximum\n- Most of the liver Patients are in the age group of 32 to 60 years\n- Most of the liver patients are are male\n- People with Total_Bilirubin  value less than 1.0 are the most having a liver disease.So, we can say that, Total_Bilirubin is an important feature which decides the health of liver\n- People with Direct_Bilirubin value of 0.2 and in general people with Direct_Bilirubin value less than 1.6 are having liver disease, which also tells that Direct_Bilirubin is also an important factor which decides liver health\n- The plots for Total_Proteins, Albumin and Albumin_and_Globulin_ratio do not reveal any feature importance, because people with both high values and low values are seen having a liver disease","ebc6f6ab":"### Observations on Correlation between numerical columns:\n- Total_Bilirubin and Direct_Bilirubin columns are having a very strong correlation of 0.87\n- Alamine_Aminotransferase and Aspartate_Aminotransferase columns also have a strong correlation of 0.79\n- Albumin and Total_Protien columns also have a good correlation of 0.78\n- Albumin also has a correlation of 0.69 with Albumin_and_Globulin_Ratio","821e36bb":"### Split the data as train set and test set with a ratio of 70:30.","e8895632":"### Observations on univariate analysis of categorical columns:\n- 428 liver patients (76%) are male and 138 patients (24%) are female","bb0a1cd9":"   __Context and Content__\n- Patients with Liver disease have been continuously increasing because of excessive consumption of alcohol, inhale of harmful gases, intake of contaminated food, pickles and drugs.\n- This data set contains 10 variables that are age, gender, total Bilirubin, direct Bilirubin, total proteins, albumin, A\/G ratio, SGPT, SGOT and Alkphos.\n- This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The \"Dataset\" column is a class label used to divide groups into liver patient (liver disease) or not (no disease). This data set contains 441 male patient records and 142 female patient records.Any patient whose age exceeded 89 is listed as being of age \"90\".\n\n__Acknowledgements__\n- This dataset was downloaded from the UCI ML Repository:\n\n- Lichman, M. (2013). UCI Machine Learning Repository [http:\/\/archive.ics.uci.edu\/ml]. Irvine, CA: University of California, School of Information and Computer Science.","076151b2":"### 5 point summary of numerical attributes","ef35a01b":"## About the Data:","e3669576":"### Build Gaussian Na\u00efve Bayes Classification Model using train Dataset and predict the class on test dataset.\n- For Gaussian Naive Bayes, the estimator learns the mean and standard deviation of each feature (per class). At prediction time the probability of a value being in a class is a function of the distance from the center of the distribution. The function used is Probability Density Function (PDF), of a Normal\/Gaussian distribution. And the Normal PDF is just a Standard Normal distribution (0 mean, unit variance) that is scaled by variance and shifted by mean. So a value which is at mean+(0.5*std) has the same probability.\n- With standardization the mean and stddev changes, but probabilities stay exactly the same, and thus classification results. In essence Gaussian Naive Bayes performs standardization internally.\n- So, even if we use a Standardised Dataset or not, the classification results and accuracies will be the same.","b6e7bcb0":"### Drop NaN values","d05da95c":"### numerical columns","389318c8":"## Visualizing Correlation between numerical columns through Heat map","ab3e905e":"### Create the X(Feature-set) and Y(Target-set) sets for your Data.","ea8222e8":"#### Description of The Target Variables\n- __Dataset__\t       Dataset: field used to split the data into two sets (patient with liver disease, or no disease)\n- __\"1\" stands for LiverPatients and \"2\" stands for NonLiverPatients.__","2e05f4fc":"## Comparision of K-NN, Logistic Regression and Na\u00efve Bayes Classification Models","604b7401":"## Visualizing Outliers in dataset using boxplot","7a3c69c2":"### Null Values","65993ef3":"  ### Target variable","02db78c8":"### Standardize the training dataset and test dataset","4d3d1968":"## Visualizing variance of numerical columns through lineplot","dbf531cf":"### Observations on variance of numerical columns\n- Variance in all the numerical columns vary exponentially\n- Variance in Albumin_and_Globulin_Ratio column is very less with a value of 0.1\n- Variance in Aspartate_Aminotransferase column is very high and is close to 10^5","8a5a607a":"### Input variables","f7bc7d40":"- Gender is the only categorical independent variable.Dataset is categorical and is the target variable","1b3f3f8c":"### categorical columns","aa673cac":"### Negative values","a0b8bd72":"### Observations on value counts of Target column:\n1. __A status of '1' refers to a 'Liver Patient'.__\n2. Among the 566 Patient's data, 404 customers (= 71%) are Liver patients\n3. __A status of '2' refers to a 'Non Liver Patient'.__\n4. Among the 566 Patient's data, 162 customers (= 29%) are Not Liver patients","393ff124":"## Variance in numerical columns","d76aea20":"### Model Performance using Logistic Regression Classifier","9de0ff29":"# Implementing KNN, Logistic and Na\u00efve Bayes Classification models","afbb38cf":"- There are 9 numerical columns.Though Dataset is numeric, it is the dependent variable for classification and is considered categorical","410cba49":"### Model Performance using Naive Bayes Classifier","151655a2":"## Visualizing Distribution of numerical columns through distplots","faccbff2":"## Visualizing frequency of each feature column by target column","e274f4c7":"## Problem Statement :\n## Classify the patients as having liver problems and not having liver problems based on 10 features related to liver","616330b8":"### Interpretation of metrics from accuracy, precision, recall, roc_auc and f1 scores\n- Logistic Regression classifier is generalizing and fitting well on the dataset with test accuracy of 70%  and train accuracy of 73%\n- Though Naive Bayes classifier is giving 100% Precision score and 0 False Psitives, the test accuracy is very low with 58%\n- KNN classifier is ovefitting with a train accuracy of 100%\n- __So Logistic Regression Classifier can be applied on this dataset with test accuarcy of 70%__","7fb615d2":"## Exploratory Data Analysis (EDA)"}}