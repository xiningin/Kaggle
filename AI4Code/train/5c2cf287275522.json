{"cell_type":{"634dabc5":"code","415e13d5":"code","eeb10e43":"code","7fa25f2f":"code","46614b0c":"code","7b957a51":"code","fdd211d8":"code","a4398315":"code","b61aa019":"code","21726f87":"code","4f326dbd":"code","84af36e8":"code","0817561f":"code","47bbdf38":"code","57ec98f0":"code","8fdb016c":"code","15da0b21":"code","bcc3e24d":"code","a6fc72a1":"code","6e916299":"code","e36199d0":"code","4d8d417d":"code","86dca833":"code","248cef5d":"code","b2c2540a":"code","b5671ed3":"code","40bbf219":"code","bacd0291":"code","5732b069":"code","417be74d":"code","c53c3ce4":"code","e747749a":"code","11dd25b1":"code","0b45a9f6":"code","07e12a8b":"code","f29166de":"code","b569b864":"code","b4e4a363":"code","8ff43c07":"code","f0a58df1":"code","88896450":"code","4c5d59b6":"code","1fa1caf5":"code","c7dedad4":"code","b22a996c":"code","9a49f440":"code","02b81da5":"code","4e032759":"code","131f901b":"code","75f80a74":"code","5a9986b8":"code","f495ef9c":"code","727ec44e":"code","90cdea3b":"code","12319090":"code","658fd8a0":"code","77f5d07d":"code","3231910e":"code","87bd825b":"code","5700389e":"code","8ff5b851":"code","c73b91e1":"markdown","ec484be7":"markdown","8b067bb9":"markdown","f282e1ae":"markdown","bb130574":"markdown","6b4e5ff2":"markdown","935b59ef":"markdown","83af41fe":"markdown","38e92a6c":"markdown","f70ac40a":"markdown","9862968b":"markdown","1ded7bd9":"markdown","8ce53167":"markdown","3692d752":"markdown","1041186d":"markdown","420e1a06":"markdown"},"source":{"634dabc5":"#Import Important libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\nfrom fbprophet.plot import plot_plotly, plot_components_plotly\nfrom fbprophet import Prophet\nfrom statsmodels.tsa.seasonal import seasonal_decompose as sd\nimport plotly.graph_objects as go\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nimport missingno as msno\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom datetime import timedelta\nimport warnings\nwarnings.filterwarnings(\"ignore\")","415e13d5":"# Load our data\n\nfeatures = pd.read_csv(\"..\/input\/walmart-sales-prediction\/features.csv\", parse_dates=['Date'])\nstores = pd.read_csv(\"..\/input\/walmart-sales-prediction\/stores.csv\")\ntrain = pd.read_csv(\"..\/input\/walmart-sales-prediction\/train.csv\", parse_dates=['Date'])\ntest = pd.read_csv(\"..\/input\/walmart-sales-prediction\/test.csv\", parse_dates=['Date'])","eeb10e43":"# Display the first 3 rows\n\nprint(features.head(3))\nprint('\\n')\nprint(stores.head(3))\nprint('\\n')\nprint(train.head(3))","7fa25f2f":"# Display the dataset shape\n\nprint(features.shape)\nprint(stores.shape)\nprint(train.shape)","46614b0c":"# We will merge our datasets\n\ntdf = train.merge(features, 'left').merge(stores, 'left')","7b957a51":"# Merged data head\n\ntdf.head(5)","fdd211d8":"# Display general information\n\ntdf.info()","a4398315":"# Data Description\n\ntdf.describe().transpose()","b61aa019":"# Percentage of missing Values\n\ntdf.isna().sum()\/len(tdf)*100","21726f87":"# Visualize our missing data\n\nmsno.bar(tdf, color=\"dodgerblue\")\nplt.show()","4f326dbd":"# Missing data is for Markdowns only (Quantitative veriables). We can imput the missing data \n# using a 0, which indicates that there is no markdown.\n\ntdf= tdf.fillna(0)\n# DISPLAY MISSING DATA\nmsno.bar(tdf, color=\"dodgerblue\")\nplt.show()","84af36e8":"# Correlation matrix\n\nplt.figure(figsize= (15,10))\nsns.heatmap(tdf.corr(), annot= True, cmap= 'coolwarm')","0817561f":"# DISTRIBUTION OF THE DEPENDENT VARIABLE\n\nplt.figure(figsize=(20,5))\nsns.distplot(tdf['Weekly_Sales'], bins=40, kde=True, color='red')\nplt.title('Weekly_Sales distribution')\nplt.show()","47bbdf38":"# Sales by different variables\n\nfig, ax = plt.subplots(2, 2, figsize= (10,10))\nax[0,0].scatter(tdf['Temperature'], tdf['Weekly_Sales'])\nax[0,0].set_title('Weekly_Sales by tempreture')\nax[0,1].scatter(tdf['Fuel_Price'], tdf['Weekly_Sales'])\nax[0,1].set_title('Weekly_Sales by fuel price')\nax[1,0].scatter(tdf['CPI'], tdf['Weekly_Sales'])\nax[1,0].set_title('Weekly_Sales by CPI')\nax[1,1].scatter(tdf['IsHoliday'], tdf['Weekly_Sales'])\nax[1,1].set_title('Weekly_Sales in holidays and not holidays')\nplt.show()","57ec98f0":"ts=train.groupby(\"Date\")[\"Weekly_Sales\"].sum()","8fdb016c":"# Visualize Residuals, Seasonal, Trend, and level\n\nres = sm.tsa.seasonal_decompose(ts.values,period=52,model=\"multiplicative\")\nres.plot()\nplt.show()","15da0b21":"# Display the top rows in ts\nts.head()","bcc3e24d":"# Visualize the weekly sales by year\nfig = go.Figure()\nyears = pd.date_range(\"2010-01-01\",\"2013-01-01\", freq=\"AS\").tolist() # range dates by year\nfor i in range(len(years)-1):\n    ts_year = ts[years[i]:years[i+1]]\n    fig.add_trace(\n        go.Scatter(\n            y=ts_year.values,\n            x=ts_year.index.week,\n            name=years[i].year,\n        ))\n\nfig.update_layout(\n    title=\"weekly sales by year\",\n    xaxis_title=\"weeks\",\n    yaxis_title=\"sales\",\n    legend_title=\"year\",\n    yaxis_tickprefix = '$',\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    ))\nfig.show()","a6fc72a1":"# Reset index\np_ts = ts.reset_index()\np_ts.columns = [\"ds\",\"y\"]\np_ts.head()","6e916299":"# Fitting Prophet Model\nm = Prophet(yearly_seasonality = True)\nm.fit(p_ts)","e36199d0":"# Display the future data (26 weeks)\nfuture = m.make_future_dataframe(periods=26, freq='W')\nfuture.tail()","4d8d417d":"# Predict future sales\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","86dca833":"# Visualize the forcasted sales\nplot_plotly(m, forecast)","248cef5d":"# Visualize the components\nplot_components_plotly(m, forecast)","b2c2540a":"def tsplot(ts, lags):\n    with plt.style.context(\"bmh\"):    \n        fig = plt.figure(figsize=(12, 7))\n        ts_ax = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid((2, 2), (1, 0))\n        pacf_ax = plt.subplot2grid((2, 2), (1, 1))\n        ts.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(ts)[1]\n        ts_ax.set_title('Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(ts, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(ts, lags=lags, ax=pacf_ax)\n        plt.tight_layout()\n        \ntsplot(ts, 26)","b5671ed3":"ts_diff = ts - ts.shift(52)\ntsplot(ts_diff[52:], 26)","40bbf219":"ts_diff = ts_diff - ts_diff.shift(1)\ntsplot(ts_diff[52+1:], 40)","bacd0291":"# Specify SARIMA Components & fit the model\np = 2\nd=1 \nq = 3\nP = 2\nD=1 \nQ = 3\ns = 52\nmodel=sm.tsa.statespace.SARIMAX(ts, order=(p, d, q), seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprint(model.summary())","5732b069":"tsplot(model.resid[24+1:], lags=40)","417be74d":"# Culaculate MAE\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","c53c3ce4":"# Visualize MAE\n\ndef plotSARIMA(ts, model, n_steps):\n    data = pd.DataFrame(ts)\n    data.columns = ['actual']\n    data['model'] = model.fittedvalues\n    data['model'][:s+d] = np.NaN\n    \n    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n    forecast = data.model.append(forecast)\n    error = mean_absolute_percentage_error(data['actual'][s+d:], data['model'][s+d:])\n\n    fig = go.Figure()\n    fig.add_trace(\n        go.Scatter(\n            x=data.index,\n            y=data[\"actual\"],\n            name=\"Actual\",\n        ))\n    fig.add_trace(\n        go.Scatter(\n            x=forecast.index,\n            y=forecast,\n            name=\"Model\",\n    ))\n    fig.add_vrect(\n    x0=data.index[-1], x1=forecast.index[-1],\n    fillcolor=\"LightSalmon\", opacity=0.5,\n    layer=\"below\", line_width=0)\n    \n    fig.update_layout(\n        title=f\"Mean Absolute Percentage Error: {error:.2f}%\",\n        xaxis_title=\"weeks\",\n        yaxis_title=\"sales\",\n        yaxis_tickprefix = '$',\n        font=dict(\n            family=\"Courier New, monospace\",\n            size=18,\n            color=\"RebeccaPurple\"\n        ))\n    fig.update_xaxes(rangeslider_visible=True)\n    fig.show()\n    \nplotSARIMA(ts, model, 26)","e747749a":"# Adding lags\nl_ts = pd.DataFrame(ts)\nl_ts.columns = [\"y\"]\n\nfor i in range(26, 54):\n    l_ts[f\"l{i}\"] = l_ts.y.shift(i)\n    \nmsno.bar(l_ts,color=\"lightgreen\")","11dd25b1":"# Drop NANs\nl_ts.dropna(inplace=True)\nmsno.bar(l_ts,color=\"lightgreen\");\nl_ts.shape","0b45a9f6":" #  5 folds cross-validation\ntscv = TimeSeriesSplit(n_splits=5)\n\ndef ts_train_test_split(X, y, test_size):\n    index = int(test_size*len(X))+1\n    \n    X_train = X.iloc[:-index]\n    y_train = y.iloc[:-index]\n    X_test = X.iloc[-index:]\n    y_test = y.iloc[-index:]\n    \n    return X_train, X_test, y_train, y_test","07e12a8b":"# Visualize the Mean Absolute error\ndef plotLMResults(model, X_train, X_test):\n    pred = model.predict(X_test)\n    \n    fig = go.Figure()\n    fig.add_trace(\n        go.Scatter(\n            x=X.index,\n            y=y,\n            name=\"Actual\",\n        ))\n    fig.add_trace(\n        go.Scatter(\n            x=X_test.index,\n            y=pred,\n            name=\"Model\",\n        ))\n\n    cv = cross_val_score(model, X_train, y_train, cv=tscv, scoring=\"neg_mean_squared_error\")\n    \n    deviation = np.sqrt(cv.std())\n    lower = pred - (1.5 * deviation)\n    upper = pred + (1.5 * deviation)\n    \n    fig.add_trace(\n        go.Scatter(\n            x=X_test.index,\n            y=lower,\n            name=\"lower bond\",\n            line = dict(shape = 'linear', color = 'rgb(255, 12, 24)', width=0.7, dash = 'dash')\n        ))\n    fig.add_trace(\n        go.Scatter(\n            x=X_test.index,\n            y=upper,\n            name=\"upper bond\",\n            line = dict(shape = 'linear', color = 'rgb(255, 12, 24)', width=0.7, dash = 'dash')\n        ))\n    \n    error = mean_absolute_percentage_error(pred, y_test)\n    fig.update_layout(\n        title=f\"Mean Absolute Percentage Error: {error:.2f}%\",\n        xaxis_title=\"weeks\",\n        yaxis_title=\"sales\",\n        yaxis_tickprefix = '$',\n        font=dict(\n            family=\"Courier New, monospace\",\n            size=18,\n            color=\"RebeccaPurple\"\n        ))\n    fig.update_xaxes(rangeslider_visible=True)\n    fig.show()\n    \ndef plotCoefs(model):\n    coefs = pd.DataFrame(model.coef_, X_train.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    fig = px.bar(coefs.coef)\n    fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='gold')\n    fig.show()\n    \n\ny = l_ts.y\nX = l_ts.drop(['y'], axis=1)\n\nX_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_size=0.3)\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nplotLMResults(lr, X_train, X_test)\nplotCoefs(lr)","f29166de":"holidays = train.groupby([\"Date\"])[\"IsHoliday\"].agg(lambda x: bool(any(x))).sort_index()\nfig = px.line(ts, title='Holidays')\n\nfor holiday in holidays[holidays].index:\n    fig.add_vrect(\n        x0=holiday- timedelta(weeks=1) , x1=holiday,\n        fillcolor=\"LightSalmon\", opacity=0.7,\n        layer=\"below\", line_width=0)\n\nfig.show()","b569b864":"# Tempreture by date\ntemperature = features.groupby([\"Date\"])[\"Temperature\"].mean().sort_index()\nfig = px.scatter(ts, title='Temperature', color=ts.index.map(lambda x: round(temperature[x])),\n                 color_continuous_scale=[\"blue\", \"yellow\", \"red\"], labels={\"color\":\"Temperature\",\"value\":\"total sales\"})\nfig.update_traces(mode='lines+markers')\nfig.update_yaxes(tickprefix=\"$\")\nfig.show()","b4e4a363":"# Fuel prices by date\nfuel_price = features.groupby([\"Date\"])[\"Fuel_Price\"].mean().sort_index()\nfig = px.scatter(ts, title='Fuel Price', color=ts.index.map(lambda x: round(fuel_price[x],2)),\n                 labels={\"color\":\"Fuel Price\",\"value\":\"total sales\"})\nfig.update_traces(mode='lines+markers')\nfig.update_yaxes(tickprefix=\"$\")\nfig.show()","8ff43c07":"ts.index[0], ts.index[0]-timedelta(weeks=0)","f0a58df1":"lxgb_ts = l_ts.copy()\nfor i in range(0,8):\n    lxgb_ts[f\"h{i}\"] = lxgb_ts.index.map(lambda x: holidays[x-timedelta(weeks=i)])\n    lxgb_ts[f\"t{i}\"] = lxgb_ts.index.map(lambda x: temperature[x-timedelta(weeks=i)])\n    lxgb_ts[f\"f{i}\"] = lxgb_ts.index.map(lambda x: fuel_price[x-timedelta(weeks=i)])","88896450":"standard_scaler = StandardScaler()\n\ny = lxgb_ts.y\nX = lxgb_ts.drop(['y'], axis=1)\n\nX_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_size=0.3)\n\nX_train_standard = pd.DataFrame(standard_scaler.fit_transform(X_train)).set_index(X_train.index)\nX_test_standard =  pd.DataFrame(standard_scaler.transform(X_test)).set_index(X_test.index)","4c5d59b6":"from xgboost import XGBRegressor \n\nxgb = XGBRegressor()\nxgb.fit(X_train_standard, y_train)\n\nplotLMResults(xgb, X_train_standard, X_test_standard)","1fa1caf5":"# Stores Time series\nsts = train.groupby([\"Store\",\"Date\"])[\"Weekly_Sales\"].sum().reset_index()","c7dedad4":"# Display top rows\nsts.head()","b22a996c":"vsts = sts.groupby([\"Store\"])[\"Weekly_Sales\"].agg([\"sum\",\"mean\"]).reset_index()\nfig = px.bar(vsts, x='Store', y='sum',\n             hover_data=['Store', 'sum', 'mean'], color='mean',\n             labels={'sum':'Weekly Sales'}, height=400)\nfig.show()","9a49f440":"# Walmart weekly sales by Store and Date(Using Plotly)\nfig = go.Figure()\nfor s in sts.Store.unique():\n    fig.add_trace(\n        go.Scatter(\n            x=sts[sts.Store==s].Date,\n            y=sts[sts.Store==s].Weekly_Sales,\n            name=\"Store_\"+str(s)\n        ))\nfig.show()","02b81da5":"l_sts = pd.DataFrame()\nfor s in sts.Store.unique():\n    df = pd.DataFrame(sts[sts.Store==s])\n    for i in range(26, 54):\n        df[f\"l{i}\"] = df.Weekly_Sales.shift(i)\n    df.dropna(inplace=True)    \n    l_sts = l_sts.append(df)","4e032759":"l_sts = l_sts.set_index(\"Date\")\nl_sts.head()","131f901b":"def sts_train_test_split(l_sts, test_size):\n    train_set = pd.concat([l_sts[l_sts.Store==s].iloc[:-int(test_size*len(l_sts[l_sts.Store==1]))+1] for s in l_sts.Store.unique()])\n    test_set = pd.concat([l_sts[l_sts.Store==s].iloc[-int(test_size*len(l_sts[l_sts.Store==1]))+1:] for s in l_sts.Store.unique()])\n    \n    y_train = train_set.Weekly_Sales\n    X_train = train_set.drop(['Weekly_Sales'], axis=1)\n    \n    y_test = test_set.Weekly_Sales\n    X_test = test_set.drop(['Weekly_Sales'], axis=1)\n    return X_train, X_test, y_train, y_test","75f80a74":"X_train, X_test, y_train, y_test = sts_train_test_split(l_sts, test_size=0.2)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","5a9986b8":"standard_scaler = StandardScaler()\nX_train_standard = pd.DataFrame(standard_scaler.fit_transform(X_train)).set_index(X_train.index)\nX_test_standard =  pd.DataFrame(standard_scaler.transform(X_test)).set_index(X_test.index)","f495ef9c":"xgb = XGBRegressor()\nxgb.fit(X_train_standard, y_train)","727ec44e":"pred = xgb.predict(X_test_standard)\nmatest = X_test.copy()\nmatest[\"Pred_Sales\"] = pred\nmatest[\"Actual_Sales\"] = y_test\nmatest.head()","90cdea3b":"fig = go.Figure()\nfor s in range(1,5):\n    fig.add_trace(\n        go.Scatter(\n            x=matest[matest.Store==s].index,\n            y=matest[matest.Store==s].Pred_Sales,\n            name=\"Store_\"+str(s)+\"_pred\"\n        ))\n    fig.add_trace(\n        go.Scatter(\n            x=matest[matest.Store==s].index,\n            y=matest[matest.Store==s].Actual_Sales,\n            name=\"Store_\"+str(s)+\"_actual\",\n            line = dict(shape = 'linear', color = 'rgb(255, 12, 24)', width=0.7, dash = 'dash')\n        ))\nerror = mean_absolute_percentage_error(pred, y_test)\nfig.update_layout(\n    title=f\"Mean Absolute Percentage Error: {error:.2f}%\",\n    xaxis_title=\"weeks\",\n    yaxis_title=\"sales\",\n    yaxis_tickprefix = '$',\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    ))\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","12319090":"store = tdf.groupby([\"Store\",\"Size\",\"Type\"])[\"Weekly_Sales\"].sum().reset_index()\n\n\nfig = px.bar(store, x='Store', y=\"Weekly_Sales\",\n             hover_data=['Store', 'Weekly_Sales'], color='Type',height=400, title=\"Weekly_Sales by Store Type\")\nfig.show()\n\n\nfig = px.bar(store, x='Store', y=\"Weekly_Sales\",\n             hover_data=['Store', 'Size'], color='Size', height=400, title=\"Weekly_Sales by Store Size\")\nfig.show()","658fd8a0":"lxgb_sts = pd.DataFrame()\nfor s in sts.Store.unique():\n    df = pd.DataFrame(sts[sts.Store==s]).set_index(\"Date\")\n    for i in range(26, 54):\n        df[f\"l{i}\"] = df.Weekly_Sales.shift(i)\n    \n    df.dropna(inplace=True)    \n    lxgb_sts = lxgb_sts.append(df)\n    \nfor i in range(0,12):\n        lxgb_sts[f\"h{i}\"] = lxgb_sts.index.map(lambda x: holidays[x-timedelta(weeks=i)])\n        lxgb_sts[f\"t{i}\"] = lxgb_sts.index.map(lambda x: temperature[x-timedelta(weeks=i)])\n        lxgb_sts[f\"f{i}\"] = lxgb_sts.index.map(lambda x: fuel_price[x-timedelta(weeks=i)])\n        \nlxgb_sts[\"Size\"] = lxgb_sts.Store.map(lambda x: store[store.Store==x][\"Size\"].item())\nlxgb_sts[\"Type\"] = lxgb_sts.Store.map(lambda x: store[store.Store==x][\"Type\"].item()).astype('category').cat.codes","77f5d07d":"X_train, X_test, y_train, y_test = sts_train_test_split(lxgb_sts, test_size=0.2)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","3231910e":"standard_scaler = StandardScaler()\nX_train_standard = pd.DataFrame(standard_scaler.fit_transform(X_train)).set_index(X_train.index)\nX_test_standard =  pd.DataFrame(standard_scaler.transform(X_test)).set_index(X_test.index)","87bd825b":"xgb = XGBRegressor()\nxgb.fit(X_train_standard, y_train)","5700389e":"pred = xgb.predict(X_test_standard)\nmatest = X_test.copy()\nmatest[\"Pred_Sales\"] = pred\nmatest[\"Actual_Sales\"] = y_test\nmatest.head()","8ff5b851":"fig = go.Figure()\nfor s in range(1,5):\n    fig.add_trace(\n        go.Scatter(\n            x=matest[matest.Store==s].index,\n            y=matest[matest.Store==s].Pred_Sales,\n            name=\"Store_\"+str(s)+\"_pred\"\n        ))\n    fig.add_trace(\n        go.Scatter(\n            x=matest[matest.Store==s].index,\n            y=matest[matest.Store==s].Actual_Sales,\n            name=\"Store_\"+str(s)+\"_actual\",\n            line = dict(shape = 'linear', color = 'rgb(255, 12, 24)', width=0.7, dash = 'dash')\n        ))\nerror = mean_absolute_percentage_error(pred, y_test)\nfig.update_layout(\n    title=f\"Mean Absolute Percentage Error: {error:.2f}%\",\n    xaxis_title=\"weeks\",\n    yaxis_title=\"sales\",\n    yaxis_tickprefix = '$',\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    ))\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","c73b91e1":"# Model 1: Prophet","ec484be7":"<img src=\"https:\/\/media.giphy.com\/media\/3o85xBiolEFwFtOw3C\/giphy.gif\" width=\"800\" height=\"300\">","8b067bb9":"<h1 style=\"text-align:center;color:#a4161a;\">Thank you for reading!<\/h1>\n<h3 style=\"text-align:center;color:#161a1d;\">if you liked the notebook<br>give us an upvote \ud83d\ude4f<br>and follow us for more content like this!<br>\n<a href=\"https:\/\/www.kaggle.com\/wardalaknaoui1\" target=\"_blank\" style=\"float:left;margin-left:30%;\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-avatars\/images\/3706441-kg.png\" style=\"width:150px;height:150px;border-radius:15%;border:4px solid #277da1;\" title=\"Follow\"><\/a>\n<a href=\"https:\/\/www.kaggle.com\/yehyachali\" target=\"_blank\" style=\"float:left;margin-left:20px;\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-avatars\/images\/2769315-kg.jpeg\" style=\"width:150px;height:150px;border-radius:15%;border:4px solid #277da1;\" title=\"Follow\"><\/a><\/h3>\n","f282e1ae":"# IMPUTING MISSING DATA","bb130574":"## Fitting the model","6b4e5ff2":" # Modeling","935b59ef":"***","83af41fe":"## Decomposing Time Series Data into Trend and Seasonality\nA Series is an aggregate or combination of 4 components. All series have a level and noise. The trend and seasonality components are optional.\n* Level: The average value in the series.\n* Trend: The increasing or decreasing value in the series.\n* Seasonality: The repeating short-term cycle in the series.\n* Noise: The random variation in the series.","38e92a6c":"# Model 2: SARIMA","f70ac40a":"# Model 3: Linear Models","9862968b":"<h1 style=\"text-align:center;font-size:48pt;color:#f1faee; text-shadow: 0px 0px 8px #101010;\">Walmart Sales Forecasting<\/h1>","1ded7bd9":"with only adding a few lags to our linear model, we can get almost the same results as the SARIMA model. ","8ce53167":"here we can see a very small increasing trend and an obvious seasonality.","3692d752":"# stores","1041186d":"The highest sales were on Dec\/24 and Dec\/23, these are thanksgiving holidays.","420e1a06":"# Model 4: XGBoost"}}