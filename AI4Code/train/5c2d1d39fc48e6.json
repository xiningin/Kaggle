{"cell_type":{"f469779a":"code","bb4bb43f":"code","7ca1de0a":"code","8ee98d17":"code","eaa7792b":"code","d408d03d":"code","506c8c1a":"code","29442907":"code","8ba1e704":"code","71c84f01":"code","7fbbd0d6":"code","1282bc89":"code","305bb983":"code","2fa88748":"code","fdd6f87f":"code","1afe98fd":"code","89033539":"code","7c898f29":"markdown","ca5afd80":"markdown","1cbe0ac0":"markdown","cef2348e":"markdown","dbdcfc82":"markdown","94c73e66":"markdown","2e46ad76":"markdown","0f458212":"markdown","92a3c491":"markdown","9725d011":"markdown","2df426a0":"markdown","ce9d8920":"markdown","0c703dac":"markdown","f13c3c7c":"markdown","8b9548ad":"markdown","b76247d5":"markdown","228de10a":"markdown","193e471d":"markdown","ebf404c6":"markdown"},"source":{"f469779a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bb4bb43f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import shapiro\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.stats.diagnostic as smd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\n\nrandom_state = 10","7ca1de0a":"data = pd.read_csv(\"\/kaggle\/input\/fish-market\/Fish.csv\")\ndisplay(data.head(3))\ndisplay(data.describe(include=\"all\"))","8ee98d17":"data[data[\"Weight\"] == 0]","eaa7792b":"data = data[data[\"Weight\"] > 0]","d408d03d":"ax = sns.pairplot(data)","506c8c1a":"fitted_data, fitted_lambda = stats.boxcox(data[\"Weight\"]) \ndata[\"Weight\"] = fitted_data","29442907":"data = data[[\"Length1\", \"Height\", \"Width\", \"Species\", \"Weight\"]]","8ba1e704":"ax = sns.pairplot(data, hue=\"Species\")","71c84f01":"y = data[\"Weight\"]\nX = data[[\"Length1\", \"Height\", \"Width\", \"Species\"]]\nX = pd.get_dummies(X, columns=[\"Species\"])","7fbbd0d6":"model_ols = sm.OLS(y, X)\nfitted = model_ols.fit()\nprint(fitted.summary())","1282bc89":"fig, ax = plt.subplots(figsize=(16,4), ncols=2)\nax[0] = sns.scatterplot(x=y, y=fitted.resid, ax=ax[0])\nax[1] = sns.histplot(fitted.resid, ax=ax[1])\n\nstatistic, p_value = shapiro(fitted.resid)\nif p_value>0.05:\n    print(\"Distribution is normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))\nelse:\n    print(\"Distribution is not normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))","305bb983":"influence = fitted.get_influence()\n(c, p) = influence.cooks_distance\ndistances = pd.DataFrame(c, index=X.index)\n\ndsc = distances.describe()\n# For outliers, we will consider those points that deviate from the average value by more than 2 times.\ncut_off = dsc.loc[\"mean\"].values[0]*2\n\nfig, ax = plt.subplots(figsize=(16, 4))\ncut_off_df = np.full(distances.shape[0], cut_off )\nx_ = range(distances.shape[0])\nax = sns.scatterplot(y=distances[0], x=x_, ax=ax)\nax = sns.lineplot(y=cut_off_df, x=x_, ax=ax, color=\"red\")","2fa88748":"idx = distances[distances[0] < cut_off].index\n\nX_cleared = X.loc[idx, :]\ny_cleared = y.loc[idx]","fdd6f87f":"model_ols_cleared = sm.OLS(y_cleared, X_cleared)\nfitted_cleared = model_ols_cleared.fit()\nprint(fitted_cleared.summary())","1afe98fd":"fig, ax = plt.subplots(figsize=(16,4), ncols=2)\nax[0] = sns.scatterplot(x=y_cleared, y=fitted_cleared.resid, ax=ax[0])\nax[1] = sns.histplot(fitted_cleared.resid, ax=ax[1])\n\nstatistic, p_value = shapiro(fitted_cleared.resid)\nif p_value>0.05:\n    print(\"Distribution is normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))\nelse:\n    print(\"Distribution is not normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))","89033539":"model_linear = LinearRegression()\nscore = cross_val_score(model_linear, X_cleared, y_cleared, cv=3, n_jobs=-1, scoring='r2')\n\nprint(\"Statistic R2: {0:.3}\".format(score.mean()))","7c898f29":"# EDA","ca5afd80":"1. After removing outliers, the model errors have a normal distribution. The error normality assumption is fulfilled.\n2. There is no heteroscedasticity. The assumption of homoscedasticity is fulfilled.\n3. All predictors in the model are significant (P> | t | <0.05)\n4. The resulting model explains 99.7% of the variation in the dependent variable.","1cbe0ac0":"There is a record in the data where the fish has zero weight. Obviously this is a data error.","cef2348e":"1. # Imports and setup","dbdcfc82":"Let's check the normal distribution of errors.\n\nThe value of the Jarque-Bera statistic is > 6 - hence the error values are not normally distributed.\n\nAnd look at the graph:\n","94c73e66":"# Model fitting","2e46ad76":"Let's remove tha bad data","0f458212":"# Read the data","92a3c491":"# OLS regression analysis","9725d011":"## Outliars","2df426a0":"This dataset is a record of 7 common different fish species in fish market sales. With this dataset, a predictive model can be performed using machine friendly data and estimate the weight of fish can be predicted.","ce9d8920":"At the same time, i will exclude features with high correlation","0c703dac":"Let's build another pairplot","f13c3c7c":"1. We have a non-linear relationship between all predictors and the dependent variable Weight.\n\n2. Length parameters have a correlation close to 1.\n\n----\nBecause there is a non-linear relationship between the dependent variable and all predictors, let's try to transform the dependent variable. Taking the logarithm did not bring the desired result, so i will try to apply the Box-Cox transformation (according to the assignment, there is no goal to explain the dependence, only to predict)\n","8b9548ad":"The data looks much better - there is a linear relationship between the predictors and the dependent variable, and we removed the correlated features.\n\nThus, 2 assumptions of linear regression have already been met:\n  - linearity of the relationship\n  - no multicollinearity","b76247d5":"To remove outliers, we use Cook's distance - exclude all points that have the greatest impact.","228de10a":"The assumption that the errors are normal was not met. We cannot use statistical tests to determine the significance of predictors.\n\nHeteroscedasticity is observed.\n\nLet's try to do something about it.","193e471d":"# Dataset","ebf404c6":"## Conclusion\nIn the course of solving the problem, the following was done:\n- correlated features are excluded\n- corrected non-linearity of the relationship\n- outliers were removed, which led to the normalization of errors and homoscedasticity of the data\n\nThe cross-validated model explains 98.6% of the variance in the dependent variable.\n\n\n\n"}}