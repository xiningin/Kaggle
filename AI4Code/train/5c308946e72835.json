{"cell_type":{"f353d0a8":"code","62282825":"code","df460731":"code","a1ae419c":"code","e3c65440":"code","05eb197b":"code","f67b6020":"code","834f9e06":"code","08623918":"code","ab52eac1":"code","ae7d4212":"code","6b6c2831":"code","638c47f6":"code","302c173e":"code","095eead3":"code","a810691a":"code","0e41aa63":"code","690e3dc9":"markdown","222781e4":"markdown","0d67a431":"markdown","3d1f7d72":"markdown","7c5eda62":"markdown","dc44a7b1":"markdown","642cccf7":"markdown","82b3a174":"markdown","5b60c2d7":"markdown","7abbac78":"markdown","2ff15c4c":"markdown","2acddb1b":"markdown","3fba7d35":"markdown","0ec5983a":"markdown","98c3699b":"markdown","aa418a43":"markdown","bf170f7e":"markdown","cab10b76":"markdown","f2a1f349":"markdown"},"source":{"f353d0a8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom statistics import mean \n\nimport string \n\n#Sklearn Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n#Plotting Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#To set precision to 3 decimals\npd.options.display.float_format = \"{:,.2f}\".format","62282825":"#unzipping all the zip folders and saving it \/kaggle\/working and saving the verbose in \/dev\/null to keep it quiet\n# -o for overwrite -d for destination directory of unzipped file\n!unzip -o '\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/*.zip' -d \/kaggle\/working > \/dev\/null","df460731":"#Reading input csv files\ntrain_text = pd.read_csv(\"train.csv\")\ntest_text = pd.read_csv(\"test.csv\")\nsample_submission = pd.read_csv(\"sample_submission.csv\")\n\nprint(train_text.shape, test_text.shape, sample_submission.shape)\ntrain_text.head()","a1ae419c":"test_text.head()","e3c65440":"sample_submission.head()","05eb197b":"#Looking at the distribution of traget variables\ny_cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\ntrain_text[y_cols].apply(pd.Series.value_counts, args = (True, True, False, None, False))","f67b6020":"#Dependent Variable\nX = train_text.comment_text\n#Independent Variables\ny = train_text[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]]\n#Splitting for checking the performance of the models on a holdout dataset\nX_train, X_val, y_train, y_val = train_test_split(X, y, shuffle = True, random_state = 123)","834f9e06":"X_train.shape, X_val.shape","08623918":"#importing stop words like in, the, of so that these can be removed from texts\n#as these words dont help in determining the classes(Whether a sentence is toxic or not)\nstop_words = stop_words.ENGLISH_STOP_WORDS\n#Function for basic cleaning\/preprocessing texts\ndef clean(doc):\n    # Removal of punctuation marks (.,\/\\][{} etc) and numbers\n    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n    # Removal of stopwords\n    doc = \" \".join([token for token in doc.split() if token not in stop_words])\n    return doc.lower()","ab52eac1":"vect = CountVectorizer(max_features= 5000, preprocessor=clean)\nX_train_dtm = vect.fit_transform(X_train)\nX_val_dtm = vect.transform(X_val)\n\nprint(X_train_dtm.shape, X_val_dtm.shape)","ae7d4212":"pd.DataFrame(X_train_dtm.A[:5], columns = vect.get_feature_names())","6b6c2831":"#Initializing and fitting models on Training Data\n#Naive Bayes Model\nnb = MultiOutputClassifier(MultinomialNB()).fit(X_train_dtm, y_train)\n#Logistic Regression Model (As we have unbalanced dataset, we use class_weight which will use inverse\n#of counts of that class. It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1)\nlr = MultiOutputClassifier(LogisticRegression(class_weight='balanced', max_iter=3000)) \\\n                    .fit(X_train_dtm, y_train)","638c47f6":"#Function for calculating roc auc with given actual binary values across target variables\n#and the probability score made by the model\ndef calculate_roc_auc(y_test, y_pred):\n    aucs = []\n    #Calculate the ROC-AUC for each of the target column\n    for col in range(y_test.shape[1]):\n        aucs.append(roc_auc_score(y_test[:,col],y_pred[:,col]))\n    return aucs","302c173e":"#Creating an empty list of results\nresults = []\n#Making predictions from all the trained models and measure performance for each\nfor model in [nb,lr]:\n    #Extracting name of the model\n    est = type(model.estimator).__name__\n    #Actual output variables\n    y_vals = y_val.to_numpy()\n    #Model Probabilities for class 1 of each of the target variables\n    y_preds = np.transpose(np.array(model.predict_proba(X_val_dtm))[:,:,1])\n    #Calculate Mean of the ROC-AUC\n    mean_auc = mean(calculate_roc_auc(y_vals,y_preds))\n    #Append the name of the model and the mean_roc_auc into the results list\n    results.append([est, mean_auc])\n    \n#Output the results as a table\npd.DataFrame(results, columns = [\"Model\",\"Mean AUC\"])","095eead3":"# Merging the test dataset with sample_submission to have all the columns:\n#id,text_data and the target variables in one dataframe\ndf_test = pd.merge(test_text, sample_submission, on = \"id\")\n#Transform the test dataset as well based on Bag of Words\/ Count Vectorizer as the Logistic model would \n#expect the same\nX_test_dtm = vect.transform(df_test[\"comment_text\"])\n#Use the Logistic Regression model to output probabilities and take the probability for class 1\ny_preds = np.transpose(np.array(lr.predict_proba(X_test_dtm))[:,:,1])\n#Assign the predictions by the model in the final test dataset\ndf_test[y_cols] = y_preds\n#Drop Comment Text as the sample submission doesnt have it and wouldnt be expected\ndf_test.drop([\"comment_text\"], axis = 1, inplace = True)\n#Save the dataset as a csv to submit it\ndf_test.to_csv(\"sample_submission.csv\", index = False)","a810691a":"#Assigning the feature names to an empty list\nfeat_impts = [vect.get_feature_names()]\n#For all the models save the feature importances in the list.estimators_ would give the internal models used by the multioutput regressor\nfor clf in lr.estimators_:\n    feat_impts.append(clf.coef_.flatten())\n#Saving the results in a dataframe\ndf_feats_impts = pd.DataFrame(np.transpose(np.array(feat_impts)), columns = [\"word\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"])\n#Converting Feature Importance Columns from string to float\ndf_feats_impts[y_cols] = df_feats_impts[y_cols].astype(\"float32\")\ndf_feats_impts.head()","0e41aa63":"#Creating Individual Feature Importance table by sorting on specific toxic-type column and selecting top 5 words\ntoxic_fi = df_feats_impts[[\"word\",\"toxic\"]].sort_values(by = \"toxic\", ascending = False).head()\nsevere_toxic_fi = df_feats_impts[[\"word\",\"severe_toxic\"]].sort_values(by = \"severe_toxic\", ascending = False).head()\nobscene_fi = df_feats_impts[[\"word\",\"obscene\"]].sort_values(by = \"obscene\", ascending = False).head()\nthreat_fi = df_feats_impts[[\"word\",\"threat\"]].sort_values(by = \"threat\", ascending = False).head()\ninsult_fi = df_feats_impts[[\"word\",\"insult\"]].sort_values(by = \"insult\", ascending = False).head()\nidentity_hate_fi = df_feats_impts[[\"word\",\"identity_hate\"]].sort_values(by = \"identity_hate\", ascending = False).head()\n\n#Plotting top 5 words based on coefficient values from the LR model\nfig,(ax1, ax2) =  plt.subplots(2,3,figsize=(10,5))\nsns.barplot(x = \"toxic\", y = \"word\", ax = ax1[0], data = toxic_fi)\nsns.barplot(x = \"severe_toxic\", y = \"word\", ax = ax1[1], data = severe_toxic_fi)\nsns.barplot(x = \"obscene\", y = \"word\", ax = ax1[2], data = obscene_fi)\nsns.barplot(x = \"threat\", y = \"word\", ax = ax2[0], data = threat_fi)\nsns.barplot(x = \"insult\", y = \"word\", ax = ax2[1], data = insult_fi)\nsns.barplot(x = \"identity_hate\", y = \"word\", ax = ax2[2], data = identity_hate_fi)\nplt.suptitle(\"Feature Importance\")\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","690e3dc9":"## 0. Introduction\nIn this notebook, we will be using CountVectorizer (an sklearn implementation of Bag-of-Words) model to convert the texts to a numerical dataset which can be then mapped against the output variables **\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"** and any model can be used learn the dependency of the output variable i.e. toxic type in this case on occurence of words. For now, we will be using **Naive Bayes and Logistic Regression** on top of the dataset created by CountVectorizer and will choose the one giving the best results on validation dataset to predict on the test dataset. We will be using **Multi Output Classifier** wrapper from sklearn to create models for all the 6 output variables\n![](data:image\/jpeg;base64,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)\n\n\n### Table of Contents:\n[1. Importing Libraries](#1)\n\n[2. Reading Dataset](#2)\n\n[3. Splitting Dataset into training and validation sets](#3)\n\n[4. Basic Preprocessing and Creating Bag of Words](#4)\n\n[5. Initialising and Training Multi Output Classifier Models](#5)\n\n[6. Measuring performance on Validation data](#6)\n\n[7. Predicting and Submitting for Test Data](#7)\n\n[8. Model Interpretation](#8)\n\n[9. TODOs](#9)","222781e4":"## 6. Measuring performance on Validation data <a class=\"anchor\" id=\"6\"><\/a>\nSince, the competition uses mean ROC-AUC as the evaluation metric, we will be using the same in the notebook.\nWe will compare the mean ROC-AUC across all the 3 models we have trained.\nWe will be using predict_proba function of models instead of predict which gives us the probability scores instead of predicted value based on a threshold of 0.5, as it is used by the roc_auc_measure.","0d67a431":"## 8. Model Interpretation <a class=\"anchor\" id=\"8\"><\/a>\nThis is the most exciting part atleast for me. Since, we are just using a simple Logistic Regression model, we can directly use the coefficient values of the model to get an understanding of the predictions made. By doing so, which feature is importance or which word makes a sentence toxic. If we would use a complex model, we could go for SHAP or LIME.\nAlso, since we have 6 output variables, we will have 6 feature importances which will be interesting to see","3d1f7d72":"## 1. Importing Libraries <a class=\"anchor\" id=\"1\"><\/a>","7c5eda62":"I tried using Support Vector Classifier as well, but that took a lot of time to train without giving the best results","dc44a7b1":"## 3. Splitting Dataset into training and validation sets <a class=\"anchor\" id=\"3\"><\/a>","642cccf7":"We will look at Top 5 words which determine if the sentence is a toxic-type or not according to the model","82b3a174":"***Do upvote if you find it helpful \ud83d\ude01***","5b60c2d7":"## 4. Basic Preprocessing and Creating Bag of Words <a class=\"anchor\" id=\"4\"><\/a>","7abbac78":"As we can see the same number of rows in train and validation dataset but 5000 columns which are essentially number of occurences of the 5000 most common words in each sentence","2ff15c4c":"We can see that the models are quite rightly selecting the most important features and it makes complete sense\n\nFor e.g. for threats - words like kill, shoot, destroy etc are most important\n\nfor identity hate - words like nigger, nigga, homosexual, faggot\n\nmost important words for toxic are less extreme than most important words for severe toxic.","2acddb1b":"## 2.Reading Dataset <a class=\"anchor\" id=\"2\"><\/a>\nAll the datasets are provided as zipped files. First we will have to unzip them and then read them into dataframes","3fba7d35":"Above, we can see the bag of words.For e.g. abide is present in 1st sentence 0 times\nThe Bag of words is pretty much sparse\nThis will be the input for a Machine Learning Classifier","0ec5983a":"## 7. Predicting and Submitting for Test Data <a class=\"anchor\" id=\"7\"><\/a>","98c3699b":"We have around 160k training texts and about 153k test texts\n\nWe have a class imbalance in almost all the training target variables.\n1. Toxic Statements: 10%\n2. Severe Toxic Statements: 1%\n3. Obscene Statements: 5%\n4. Threat Statements: 0.3%\n5. Insult Statements: 5%\n6. Identity Hate Statements: 0.9%","aa418a43":"Creating a bag of words model with a maximum of 5000 most-frequent words (as including all the words will make the dataset sparse and will only add noise).Also, Clean the dataset when creating the dataset using bag of words","bf170f7e":"As we can see, Both the models perform really good with LR performing slightly better. So, we will use it as the final model to submit the predictions for the test data. Also, these simple models give pretty good results without much of a hassle or technical know-how, that is why they are still used widely.\n\nA bit on Logistic Regression is no harm.\n\nThe ***logistic model*** (or logit model) is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. **Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.**\n\nBelow is the image most commonly used image for Logistic Regression\n![](https:\/\/www.saedsayad.com\/images\/LogReg_1.png)\n\nBy simple transformation, the logistic regression equation can be written in terms of an odds ratio.\n![](https:\/\/www.saedsayad.com\/images\/Logistic_odd.png)\n\nFinally, taking the natural log of both sides, we can write the equation in terms of log-odds (logit) which is a linear function of the predictors.\n![](https:\/\/www.saedsayad.com\/images\/Logit.png)\n\nThe coefficient (b1) is the amount the logit (log-odds) changes with a one unit change in x. \n\nFor e.g. if the equation is 1+2x i.e. b0 = 1 and b1 = 2. \nIncreasing x by 1 increases the log-odds by 2 and the odds that Y=1 increase by a factor of 10^2 . Note that the probability of Y=1 has also increased, but it has not increased by as much as the odds have increased.\n\nThis was about the logistic function.\n\nNow to find the best dividing line (in other terms reduce the loss function), Logistic Regression also use Gradient Descent but with a different loss function (Linear Regression uses Mean squared error). Logistic Regression uses log loss\/ maximum likelihood estimation (MLE) function\n\n![](https:\/\/i.stack.imgur.com\/XbU4S.png)\n\nwhere m is the number of samples (as we take the average), y is the actual value and h(x) is the ouput of the model","cab10b76":"## 9. TODOs: <a class=\"anchor\" id=\"9\"><\/a>\n1. Try TF-IDF instead of CountVectorizer \nTF-IDF tend to perform better than CountVectorizer in some cases\n2. Try ensemble models instead of Vanilla ML models\nBagging and Boosting models give better results than classic ML techniques in most cases\n3. Better Text Preprocessing\nTypo correction etc can be done to further improve the model","f2a1f349":"## 5. Initialising and Training Multi Output Classifier Models <a class=\"anchor\" id=\"5\"><\/a>\n\nSince we need to classify each sentence as toxic or not, severe_toxic or not, obscene or not, threat or not, insult or not and identity_hate or not, we need to classify the sentence against 6 output variables (This is called Multi-Label Classification which is different from mult-class classification where a target variable has more than 2 options e.g. a sentence can be positive, negative and neutral)\n\nFor the same, we will be using MultiOutputClassifier from sklearn which as mentioned earlier is a wrapper.This strategy consists of fitting one classifier per target."}}