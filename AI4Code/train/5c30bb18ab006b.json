{"cell_type":{"599f2be5":"code","7261c6ab":"code","3f58c923":"code","c17484be":"code","0e1c931b":"code","85ac8914":"code","1830f7ab":"code","4c4a4119":"code","45a198c2":"code","7cb89885":"code","b91d1535":"code","f2f1866a":"code","268b0dc1":"code","c648f846":"code","aa3bdd7d":"code","0417006c":"code","595e0d12":"code","9e158cc9":"code","a20a743a":"code","925f0eb3":"code","3f804658":"code","ead5f36e":"code","23c80ad7":"code","8c404021":"code","e13546e7":"code","6fc8baf8":"code","b081c912":"markdown","5e8bf855":"markdown","4c31078d":"markdown","5ef83f91":"markdown","8d56319a":"markdown","c74ae76e":"markdown","c247e00c":"markdown","5aad3971":"markdown","4d9f2488":"markdown","c24a6049":"markdown","4d126e6f":"markdown","30fbd596":"markdown","a4e6c153":"markdown","ce15ce7b":"markdown","60162aaa":"markdown","0c802bbc":"markdown"},"source":{"599f2be5":"### Import required libraries\n\nimport numpy as np\nimport pandas as pd\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\nimport warnings\nwarnings.filterwarnings('ignore')","7261c6ab":"# Read train and test files\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","3f58c923":"train_df.head()","c17484be":"test_df.head()","0e1c931b":"train_df.info()","85ac8914":"test_df.info()","1830f7ab":"#### Check if there are any NULL values in Train Data\nprint(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\nif (train_df.columns[train_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","4c4a4119":"#### Check if there are any NULL values in Test Data\nprint(\"Total Test Features with NaN Values = \" + str(test_df.columns[test_df.isnull().sum() != 0].size))\nif (test_df.columns[test_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(test_df.columns[test_df.isnull().sum() != 0])))\n    test_df[test_df.columns[test_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","45a198c2":"# check and remove constant columns\ncolsToRemove = []\nfor col in train_df.columns:\n    if col != 'ID' and col != 'target':\n        if train_df[col].std() == 0: \n            colsToRemove.append(col)\n        \n# remove constant columns in the training set\ntrain_df.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\ntest_df.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","7cb89885":"%%time\ndef duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n\n    for t, v in groups.items():\n\n        cs = frame[v].columns\n        vs = frame[v]\n        lcs = len(cs)\n\n        for i in range(lcs):\n            ia = vs.iloc[:,i].values\n            for j in range(i+1, lcs):\n                ja = vs.iloc[:,j].values\n                if np.array_equal(ia, ja):\n                    dups.append(cs[i])\n                    break\n\n    return dups\n\ncolsToRemove = duplicate_columns(train_df)\nprint(colsToRemove)","b91d1535":"# remove duplicate columns in the training set\ntrain_df.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\ntest_df.drop(colsToRemove, axis=1, inplace=True)\n\nprint(\"Removed `{}` Duplicate Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","f2f1866a":"def drop_sparse(train, test):\n    flist = [x for x in train.columns if not x in ['ID','target']]\n    for f in flist:\n        if len(np.unique(train[f]))<2:\n            train.drop(f, axis=1, inplace=True)\n            test.drop(f, axis=1, inplace=True)\n    return train, test","268b0dc1":"%%time\ntrain_df, test_df = drop_sparse(train_df, test_df)","c648f846":"gc.collect()\nprint(\"Train set size: {}\".format(train_df.shape))\nprint(\"Test set size: {}\".format(test_df.shape))","aa3bdd7d":"X_train = train_df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)","0417006c":"dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","595e0d12":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.004,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=150, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","9e158cc9":"# Training LGB\npred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"LightGBM Training Completed...\")","a20a743a":"# feature importance\nprint(\"Features Importance...\")\ngain = model.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':model.feature_name(), \n                   'split':model.feature_importance('split'), \n                   'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\nprint(featureimp[:50])","925f0eb3":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","3f804658":"# Training XGB\npred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"XGB Training Completed...\")","ead5f36e":"cb_model = CatBoostRegressor(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='RMSE',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)","23c80ad7":"cb_model.fit(dev_X, dev_y,\n             eval_set=(val_X, val_y),\n             use_best_model=True,\n             verbose=True)","8c404021":"pred_test_cat = np.expm1(cb_model.predict(X_test))","e13546e7":"sub = pd.read_csv('..\/input\/sample_submission.csv')\n\nsub_lgb = pd.DataFrame()\nsub_lgb[\"target\"] = pred_test\n\nsub_xgb = pd.DataFrame()\nsub_xgb[\"target\"] = pred_test_xgb\n\nsub_cat = pd.DataFrame()\nsub_cat[\"target\"] = pred_test_cat\n\nsub[\"target\"] = (sub_lgb[\"target\"] * 0.5 + sub_xgb[\"target\"] * 0.3 + sub_cat[\"target\"] * 0.2)","6fc8baf8":"print(sub.head())\nsub.to_csv('sub_lgb_xgb_cat.csv', index=False)","b081c912":"So there are a total of 4992 columns in the test set out of which 4991 are of type float64 and 1 is object (ID is the object column)","5e8bf855":"## Check and Remove Constant Features","4c31078d":"## Load Train and Test Data","5ef83f91":"## Build Train and Test Data for Modeling","8d56319a":"## Drop Sparse Data","c74ae76e":"## LightGBM","c247e00c":"## XGB Modeling","5aad3971":"## Combine Predictions","4d9f2488":"So there are a total of 4993 columns out of which 1845 are of type float64, 3147 are int64 and 1 is object (ID is the object column)","c24a6049":"### Test Data","4d126e6f":"### Train Data","30fbd596":"## Remove Duplicate Columns","a4e6c153":"## Check for Missing Values","ce15ce7b":"## Train and Test Data Info","60162aaa":"## Catboost","0c802bbc":"## Load Required Libraries"}}