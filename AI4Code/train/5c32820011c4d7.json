{"cell_type":{"838fe52c":"code","3c65f273":"code","c9b660d8":"code","37a487c2":"code","89bdf831":"code","e706bef9":"code","40987f3c":"code","b29d3f82":"code","890169fc":"code","2ba1c455":"code","3cd28394":"code","4f52d0d7":"code","280f21c6":"code","9ad3c7e4":"code","49bb74cc":"code","d5641aba":"code","74df9006":"code","89c7ed4b":"code","4800645a":"code","82001f7f":"code","f5f590d4":"code","b00d0669":"code","60d0d629":"code","e5a19797":"code","82fc3b3d":"code","f48c018b":"code","60451a10":"code","f0244ee1":"code","bedb0423":"code","370420fc":"code","37950917":"code","82023e55":"code","b90b3860":"code","699f5b3b":"code","f4770a16":"code","247ffef0":"code","1d8458c1":"code","73ba2e9a":"code","43568583":"code","32c3d1d8":"code","3b28cd97":"code","993e932d":"code","af36b63f":"code","6df46869":"code","bde66746":"code","b145b2e9":"code","90d354be":"code","dc82b1c2":"markdown","729a96b5":"markdown","5b030ea2":"markdown","3f8f559b":"markdown","7404e247":"markdown","9d1af6e7":"markdown","9e3a2706":"markdown","7c83b6e7":"markdown","5817cc4b":"markdown","c7f02749":"markdown","ef9bf1c8":"markdown","77587247":"markdown","a93ce4e2":"markdown","bf891d54":"markdown","d39ed476":"markdown","a84dbc19":"markdown","df83cffc":"markdown","408340ac":"markdown","fc480fe9":"markdown","ec786b2e":"markdown","6721c2f6":"markdown","5c06e624":"markdown","076608fc":"markdown","af0fce6e":"markdown","810b1c39":"markdown","bfef8a35":"markdown","3349cd38":"markdown","b75868ff":"markdown","88f7b74b":"markdown","036583f0":"markdown","f47cd512":"markdown","bb96e59d":"markdown","cf1c9cfd":"markdown","b4f687fe":"markdown","b0f41827":"markdown","20934603":"markdown","11b3b3ed":"markdown","d9760cf3":"markdown","9d7d6d3c":"markdown"},"source":{"838fe52c":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom xgboost import XGBRegressor\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","3c65f273":"df_test=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf_train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nprint(\"Train shape:\",df_train.shape)\nprint(\"Test Shape:\",df_test.shape)","c9b660d8":"X_trainfull=df_train.drop([\"SalePrice\"], axis=1)\ny=df_train.SalePrice","37a487c2":"plt.figure(figsize=(8,4))\nplt.title(\"Distribution of Sales Price (y)\")\nsns.distplot(y)\nplt.show()","89bdf831":"y=np.log1p(y)\n\nplt.figure(figsize=(8,4))\nplt.title(\"Distribution of log Sales Price (y)\")\nsns.distplot(y)\nplt.xlabel(\"Log of Sales Price\")\nplt.show()","e706bef9":"d_temp=X_trainfull.isna().sum().sort_values(ascending=False)\nd_temp=d_temp[d_temp>0]\nd_temp=d_temp\/df_train.shape[0]*100\n\nplt.figure(figsize=(8,5))\nplt.title(\"Features Vs Percentage Of Null Values\")\nsns.barplot(y=d_temp.index,x=d_temp, orient='h')\nplt.xlim(0,100)\nplt.xlabel(\"Null Values (%)\")\nplt.show()","40987f3c":"na_index=(d_temp[d_temp>20]).index\nX_trainfull.drop(na_index, axis=1, inplace=True)","b29d3f82":"num_cols=X_trainfull.corrwith(y).abs().sort_values(ascending=False).index\nX_num=X_trainfull[num_cols]\nX_cat=X_trainfull.drop(num_cols,axis=1)","890169fc":"X_num.sample(5)","2ba1c455":"high_corr_num=X_num.corrwith(y)[X_num.corrwith(y).abs()>0.5].index\nX_num=X_num[high_corr_num]","3cd28394":"plt.figure(figsize=(10,6))\nsns.heatmap(X_num.corr(), annot=True, cmap='coolwarm')\nplt.show()\n\nprint(\"Correlation of Each feature with target\")\nX_num.corrwith(y)","4f52d0d7":"X_num=X_num[high_corr_num]\nX_num.drop(['TotRmsAbvGrd','GarageArea','1stFlrSF','GarageYrBlt'],axis=1, inplace=True)","280f21c6":"#function to handle NA\ndef handle_na(df, func):\n    \"\"\"\n    Input dataframe and function \n    Returns dataframe after filling NA values\n    eg: df=handle_na(df, 'mean')\n    \"\"\"\n    na_cols=df.columns[df.isna().sum()>0]\n    for col in na_cols:\n        if func=='mean':\n            df[col]=df[col].fillna(df[col].mean())\n        if func=='mode':\n            df[col]=df[col].fillna(df[col].mode()[0])\n    return df","9ad3c7e4":"X_num=handle_na(X_num, 'mean')","49bb74cc":"# Function to scale df \ndef scale_df(df):\n    \"\"\"\n    Input: data frame\n    Output: Returns minmax scaled Dataframe \n    eg: df=scale_df(df)\n    \"\"\"\n    scaler=MinMaxScaler()\n    for col in df.columns:\n        df[col]=scaler.fit_transform(np.array(df[col]).reshape(-1,1))\n    return df","d5641aba":"X_num=scale_df(X_num)","74df9006":"X_train, X_val, y_train, y_val=train_test_split(X_num,y, test_size=0.2)\nmodel=LinearRegression()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","89c7ed4b":"model=SVR()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","4800645a":"model=RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","82001f7f":"model=XGBRegressor(learning_rate=0.1)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","f5f590d4":"num_features=X_num.columns","b00d0669":"X_cat.sample(5)","60d0d629":"X_cat.describe()","e5a19797":"for feature in X_cat.columns:\n    print(\n        f\"{feature} :{len(X_cat[feature].unique())}: {X_cat[feature].unique()}\"\n    )","82fc3b3d":"cat_na=X_cat.isna().sum().sort_values(ascending=False)\ncat_na=cat_na[cat_na>30]\nX_cat.drop(cat_na.index, axis=1, inplace=True)","f48c018b":"for feature in X_cat.columns:\n    plt.figure(figsize=(4,6))\n    plt.title(f\"{str(feature)} vs log Sale Price\")\n    sns.boxplot(X_cat[feature],y)\n    plt.show()","60451a10":"X_cat=handle_na(X_cat, 'mode')","f0244ee1":"le=LabelEncoder()\nX_cat_le=pd.DataFrame()\nfor col in X_cat.columns:\n    X_cat_le[col] = le.fit_transform(X_cat[col])","bedb0423":"Xc_train, Xc_test, yc_train,yc_test=train_test_split(X_cat_le,y, test_size=0.2)","370420fc":"model=RandomForestRegressor()\nmodel.fit(Xc_train,yc_train)","37950917":"print(f\"Train score : {model.score(Xc_train,yc_train)}\")\nprint(f\"Test score : {model.score(Xc_test,yc_test)}\")","82023e55":"feat_imp=pd.DataFrame({\"Feature\":Xc_train.columns,\"imp\":model.feature_importances_})\nfeat_imp=feat_imp.sort_values('imp', ascending=False)\n\nplt.figure(figsize=(10,4))\nplt.title(\"Feature Importance\", fontsize=16)\nsns.barplot('Feature', 'imp', data=feat_imp)\nplt.xticks(rotation=80)\nplt.show()","b90b3860":"feat=[]\nscore_train=[]\nscore_test=[]\nfor i in range(29):\n    imp_ft=feat_imp.head(i+1).Feature.unique()\n\n    X_cat_imp=pd.DataFrame()\n    for col in imp_ft:\n        X_cat_imp[col] = le.fit_transform(X_cat[col])\n\n    Xc_train, Xc_test, yc_train,yc_test=train_test_split(X_cat_imp,y, test_size=0.2)\n\n    model=RandomForestRegressor(n_estimators=100)\n    model.fit(Xc_train,yc_train)\n    feat.append(i+1)\n    score_train.append(model.score(Xc_train,yc_train))\n    score_test.append(model.score(Xc_test,yc_test))\n    \nacc_feat_df=pd.DataFrame({\"Feature\":feat,\"TrainAcc\":score_train,\"ValAcc\":score_test})","699f5b3b":"plt.figure(figsize=(10,5))\nsns.lineplot('Feature', 'TrainAcc', data=acc_feat_df, label=\"Training Accuracy\")\nsns.lineplot('Feature', 'ValAcc', data=acc_feat_df, label=\"Validation Accuracy\")\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"R2 Score\")\nplt.xticks(rotation=80)\nplt.xlim(1,29)\nplt.show()","f4770a16":"cat_features=list(feat_imp.iloc[:17,0])","247ffef0":"# Selecting only important features\nX_cat=X_cat[cat_features]\n# OHE features\nX_cat=pd.get_dummies(X_cat)\n# Scaling the data\nX_cat=scale_df(X_cat)","1d8458c1":"X_train, X_val, y_train, y_val=train_test_split(X_cat,y, test_size=0.2)","73ba2e9a":"model=LinearRegression()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","43568583":"model=SVR()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","32c3d1d8":"model=RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","3b28cd97":"model=XGBRegressor(learning_rate=0.1)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","993e932d":"#Combine train and test data\nXtt=pd.concat([X_trainfull,df_test])\n\n#Split into Numeric and categoric features\nXtt_num= Xtt[num_features]\nXtt_cat= Xtt[cat_features]\n\n#Handling null values\nXtt_cat=handle_na(Xtt_cat, 'mode')\nXtt_num=handle_na(Xtt_num,'mean')\n\n#OHE Categoric features\nXtt_cat=pd.get_dummies(Xtt_cat,drop_first=True)\n\n#Combine Numeric and Categorical features\nXtt=pd.concat([Xtt_num,Xtt_cat], axis=1)\n\n#Scale Features\nXtt=scale_df(Xtt)\n\n#Training and Testing Features after Feature Engineering\nX=Xtt.iloc[:df_train.shape[0],:]\nX_test=Xtt.iloc[df_train.shape[0]:,:]\n\n#Training and Validation features and target\nX_train, X_val, y_train, y_val=train_test_split(X,y, test_size=0.2)","af36b63f":"model=LinearRegression()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")\ny_LR=model.predict(X_test)","6df46869":"model=SVR()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")\ny_SVR=model.predict(X_test)","bde66746":"model=RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")\ny_RF=model.predict(X_test)","b145b2e9":"model=XGBRegressor(learning_rate=0.1)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")\ny_XGB=model.predict(X_test)","90d354be":"sub = pd.DataFrame()\nsub[\"Id\"] = df_test.Id\nsub[\"SalePrice\"] = np.expm1(y_XGB)\nsub.to_csv(\"submission.csv\", index=False)","dc82b1c2":"# Drop features with more than 30 null values","729a96b5":"## Model Testing : Only Numerical Features","5b030ea2":"# Read Data","3f8f559b":"# Fit and Evaluate Random Forest Model","7404e247":"# Explore Data","9d1af6e7":"# Handling Null values","9e3a2706":"# Model Testing Only catagorical Featues","7c83b6e7":"# Plot Number of Features vs Model Performance","5817cc4b":"# Identify Features Highly correlated with target","c7f02749":"# CATEGORICAL DATA FEATURE SELECTION AND ENGINEERING","ef9bf1c8":"# Prepare Submission file","77587247":"It can be observed from above that y is right-skewed, log transform can be applied to make it normal distribution.","a93ce4e2":"# Percentage of null valued features in Train data","bf891d54":"# Separate features and target","d39ed476":"Observation:\n* Performance of Linear Regression is very poor in validation data\n* Accuracy of SVM model is reasonable\n* RF model is overfitting, still gives validation accuracy better than SVM model\n* XGBoost model gives the best result in validation data. ","a84dbc19":"# Heat-map of highly correlated Features","df83cffc":"# Feature importance from RF Model","408340ac":"# List the Numerical features required","fc480fe9":"# Label encode features","ec786b2e":"# NUMERICAL FEATURES: FEATURE SELECTION AND ENGINEERING","6721c2f6":"# Calculate Training and Validation Accuracy for different number of features","5c06e624":"# UPVOTE THE KERNEL IF YOU FIND IT HELPFUL ","076608fc":"# Split into Train and validation set","af0fce6e":"# Training, Evaluation and Prediction","810b1c39":"# Remove multi-colinear features ","bfef8a35":"# EDA: Relation between each feature and saleprice","3349cd38":"# Handling Null Values","b75868ff":"# Import Libraries","88f7b74b":"# Drop features where more than 20% records are null","036583f0":"# Split Categorical and Numeric Features","f47cd512":"Observation:\n* We can observe significant increase in train and validation accuracy with increase in features intitially.\n* After around 10 features, no significant improvement can be observed in either train or validation accuracy.\n* This is known as Curse of Dimensionality.\n* We can select the ideal number of features depending \n* I am selecting top 17 features for training","bb96e59d":"Observation:\n* Linear Regression preforms very poorly in validation.\n* Other three models have similar accuracy in validation, eventhough Random Forest model is overfitting.","cf1c9cfd":"# Scale values","b4f687fe":"# Distribution of Saleprice","b0f41827":"Observation:\n* Linear Regression and SVM models show similar performance moderately good score in both training and validation data\n* Random forest model is overfitting\n* XGB Regressor seems to be the best suited model","20934603":"# FEATURE ENGINEERING IN COMBINED TRAIN AND TEST DATA","11b3b3ed":"# List of selected Categorical Features ","d9760cf3":"# View sample data","9d7d6d3c":"Drop na>20% fields"}}