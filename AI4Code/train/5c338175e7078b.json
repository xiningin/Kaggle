{"cell_type":{"239fb4c4":"code","ac086aaa":"code","560c0371":"code","b7f1137d":"code","b0fc7e00":"code","2d888bc9":"code","59f5913a":"code","26fab3c4":"code","1b44771d":"code","0d126353":"code","e3bd4c83":"code","75d5ea6d":"code","53237241":"code","8132364b":"code","2de8e432":"code","49c3164e":"code","c0287297":"code","c474c2c3":"code","e8a27877":"code","9f3753fa":"code","6a3afef5":"code","20f7403d":"code","dd14aaae":"code","b214ab9e":"code","4cc39304":"code","a2b1760f":"code","9417be35":"markdown","57333d47":"markdown","40deff08":"markdown","ddb1886b":"markdown"},"source":{"239fb4c4":"import os\nimport copy\nimport pickle\nimport random\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom gensim.models import KeyedVectors, FastText\n\n#torch packages\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#transformer packages\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import AutoTokenizer, AutoModel","ac086aaa":"def set_seed(seed=5080):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nclass JigsawTestDataset(Dataset):\n    def __init__(\n        self, df, max_length, tokenizer = None, use_tfidf=False, \n        tfidf_matrix=None, use_sentence_embedding=False, embed_matrix=None\n    ):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        self.use_tfidf = use_tfidf\n        self.use_sentence_embedding = use_sentence_embedding\n        if use_tfidf:\n            self.tfidf_matrix = tfidf_matrix\n        elif use_sentence_embedding:\n            self.embed_matrix = embed_matrix\n            \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs_text = self.tokenizer.encode_plus(\n                                text,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        text_ids = inputs_text['input_ids']\n        text_mask = inputs_text['attention_mask']\n        \n        if self.use_tfidf:\n            tfidf = self.tfidf_matrix[index]\n            return {\n                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n                'text_mask': torch.tensor(text_mask, dtype=torch.long),\n                'tfidf': torch.tensor(tfidf, dtype=torch.long)\n            }\n        elif use_sentence_embedding:\n            sent_embed = self.embed_matrix[index]\n            return {\n                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n                'text_mask': torch.tensor(text_mask, dtype=torch.long),\n                'sent_embed': torch.tensor(sent_embed, dtype=torch.long)\n            }\n        else:\n            return {\n                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n                'text_mask': torch.tensor(text_mask, dtype=torch.long)\n            }","560c0371":"class NN(nn.Module):\n    def __init__(\n        self, bert_drop_out, HID_DIM=768, tfidf_len=0, use_tfidf=False, \n        use_sentence_embedding=False, embed_len=0\n    ):\n        super().__init__()\n        if use_tfidf:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768+tfidf_len, 1)\n            )\n        elif use_sentence_embedding:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768+embed_len, 1)      \n            )\n        else:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768, 1)\n            )\n            \n    def forward(self, x):\n        score = self.net(x)\n        return score\n                \nclass JigsawModel(nn.Module):\n    def __init__(self, BERT, NN):\n        super(JigsawModel, self).__init__()\n        self.bert = BERT\n        self.fc = NN\n        \n    def forward(\n        self, ids, mask, tfidf_vec=None, use_tfidf=False, \n        sent_embed=None, use_sentence_embedding=False\n    ):        \n        out = self.bert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        if use_tfidf:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], tfidf_vec), dim=1\n            )\n        elif use_sentence_embedding:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], sent_embed), dim=1\n            )        \n        else:\n            fc_in = out[\"pooler_output\"]\n        outputs = self.fc(fc_in)\n        return outputs","b7f1137d":"def create_test_corpus(df_test):\n    return df_test[\"text\"].to_list()\n\ndef tokenize_test_by_bert_tokenizer(bert_tokenizer, corpus):\n    corpus_tokenized = [\n        bert_tokenizer.tokenize(sentence) for sentence in corpus\n    ]\n    return corpus_tokenized\n\ndef testCorpus2tfidf(tfidf_transfomer, corpus_tokenized):\n    tfidf_matrix_sparse = tfidf_transfomer.transform(corpus_tokenized)\n    tfidf_matrix = tfidf_matrix_sparse.toarray()\n    return tfidf_matrix\n\ndef identity_tokenizer(text):\n    return text","b0fc7e00":"set_seed(5080)\ndata_test = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nPRETRAINED_MODEL_NAME = \"..\/input\/transformers\/roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","2d888bc9":"TFIDF_PATH = \"..\/input\/temp-model\/tfidf_roberta_obj (1).pickle\"\nwith open(TFIDF_PATH, 'rb') as f:\n    tfidf_transformer = pickle.load(f)\n    \ntest_corpus = create_test_corpus(data_test)\ncorpus_tokenized = tokenize_test_by_bert_tokenizer(bert_tokenizer, test_corpus)\ntfidf_matrix = testCorpus2tfidf(tfidf_transformer, corpus_tokenized)","59f5913a":"token2idx = tfidf_transformer.vocabulary_\ntoken_list = list(\n    tfidf_transformer.vocabulary_.keys()\n)","26fab3c4":"tfidf_matrix.shape","1b44771d":"# fmodel = FastText.load(\n#     '..\/input\/jigsaw-regression-based-data\/FastText-jigsaw-256D\/Jigsaw-Fasttext-Word-Embeddings-256D.bin'\n# )","0d126353":"# w2v_embed_dim = 256\n# w2v = np.zeros(\n#     (len(token2idx), w2v_embed_dim)\n# )\n# for tok in token_list:\n#     token_idx = token2idx[tok]\n#     w2v[token_idx] = fmodel.wv[tok]","e3bd4c83":"# sentence_embedding = np.dot(\n#     tfidf_matrix, w2v\n# )","75d5ea6d":"# sentence_embedding.shape","53237241":"batch_size = 16\nmax_token_length = 128\nuse_tfidf = True\nuse_sentence_embedding = False\nif use_tfidf:\n    tfidf_len = tfidf_matrix.shape[1]\n    test_dataset = JigsawTestDataset(\n        data_test, max_length=max_token_length, tokenizer=bert_tokenizer,\n        use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\nelif use_sentence_embedding:\n    embed_len = sentence_embedding.shape[1]\n    test_dataset = JigsawTestDataset(\n        data_test, max_length=max_token_length, tokenizer=bert_tokenizer,\n        use_sentence_embedding=use_sentence_embedding, embed_matrix=sentence_embedding\n    )\nelse:\n    test_dataset = JigsawTestDataset(\n        data_test, max_length=max_token_length, tokenizer=bert_tokenizer\n    )\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False,\n    num_workers=2)","8132364b":"LR = 1e-5\nEPOCH = 10\nHID_DIM = 768\nMARGIN = 0.5\nDATE = \"0115\"\nLR = 1e-5\nWD = 0\nBDR = 0.2\n\nMODEL_PATH = f\"..\/input\/temp-model\/0116_roberta_LR_0.0001_WD_1e-06_BDR_0.3.pth\"\n\nbert = AutoModel.from_pretrained(PRETRAINED_MODEL_NAME).to(device)\nif use_tfidf:\n    dnn = NN(\n        BDR, HID_DIM, \n        tfidf_len, use_tfidf\n    ).to(device)\nelif use_sentence_embedding:\n    dnn = NN(BDR, HID_DIM, \n             embed_len=embed_len, use_sentence_embedding=True\n            ).to(device)   \nelse:\n    dnn = NN(\n        BDR, HID_DIM\n    ).to(device)\n\nmodel = JigsawModel(bert, dnn).to(device)\nif torch.cuda.is_available():\n    checkpoint = torch.load(MODEL_PATH)\nelse:\n    checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\nmodel.bert.load_state_dict(checkpoint[\"BERT\"])\nmodel.fc.load_state_dict(checkpoint[\"NN\"])\n# bert.load_state_dict(checkpoint[\"BERT\"])\n# dnn.load_state_dict(checkpoint[\"NN\"])","2de8e432":"# class JigsawDataset(Dataset):\n#     def __init__(self, df, tokenizer, max_length, use_tfidf=False, tfidf_matrix=None):\n#         self.df = df\n#         self.max_len = max_length\n#         self.tokenizer = tokenizer\n#         self.more_toxic = df['more_toxic'].values\n#         self.less_toxic = df['less_toxic'].values\n#         self.use_tfidf = use_tfidf\n#         if use_tfidf:\n#             self.more_toxic_tfidf_idx = df['more_toxic_tfidf_idx'].values\n#             self.less_toxic_tfidf_idx = df['less_toxic_tfidf_idx'].values\n#             self.tfidf_matrix = tfidf_matrix\n\n#     def __len__(self):\n#         return len(self.df)\n    \n#     def __getitem__(self, index):\n#         more_toxic = self.more_toxic[index]\n#         less_toxic = self.less_toxic[index]\n#         inputs_more_toxic = self.tokenizer.encode_plus(\n#                                 more_toxic,\n#                                 truncation=True,\n#                                 add_special_tokens=True,\n#                                 max_length=self.max_len,\n#                                 padding='max_length'\n#                             )\n#         inputs_less_toxic = self.tokenizer.encode_plus(\n#                                 less_toxic,\n#                                 truncation=True,\n#                                 add_special_tokens=True,\n#                                 max_length=self.max_len,\n#                                 padding='max_length'\n#                             )\n#         target = 1\n        \n#         more_toxic_ids = inputs_more_toxic['input_ids']\n#         more_toxic_mask = inputs_more_toxic['attention_mask']        \n#         less_toxic_ids = inputs_less_toxic['input_ids']\n#         less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n#         if self.use_tfidf:\n#             more_toxic_tfidf_idx = self.more_toxic_tfidf_idx[index]\n#             less_toxic_tfidf_idx = self.less_toxic_tfidf_idx[index]\n#             more_toxic_tfidf = self.tfidf_matrix[more_toxic_tfidf_idx]\n#             less_toxic_tfidf = self.tfidf_matrix[less_toxic_tfidf_idx]\n#             return {\n#                 'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n#                 'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n#                 'more_toxic_tfidf': torch.tensor(more_toxic_tfidf, dtype=torch.long),\n#                 'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n#                 'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n#                 'less_toxic_tfidf': torch.tensor(less_toxic_tfidf, dtype=torch.long),\n#                 'target': torch.tensor(target, dtype=torch.long)\n#             }\n#         else:\n#             return {\n#                 'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n#                 'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n#                 'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n#                 'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n#                 'target': torch.tensor(target, dtype=torch.long)\n#             }","49c3164e":"# def validate_all_combine(\n#     model, criterion, \n#     valid_loader, device, use_tfidf=False\n# ):\n#     epoch_loss = 0\n#     y_preds = []\n    \n#     model.eval()\n#     with torch.no_grad():\n#         for data in valid_loader:\n#             more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#             more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#             less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#             less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#             targets = data['target'].to(device, dtype=torch.long)\n            \n#             if use_tfidf:\n#                 more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n#                 less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n#                 more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n#                 less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n#             else:\n#                 more_out = model(more_toxic_ids, more_toxic_mask)\n#                 less_out = model(less_toxic_ids, less_toxic_mask)\n            \n#             loss = criterion(more_out, less_out, targets)\n\n#             epoch_loss += loss.item()\n#             for i in range(len(data['more_toxic_ids'])):\n#                 y_preds.append([less_out[i].item(), more_out[i].item()])\n#         df_score = pd.DataFrame(y_preds,columns=['less','more'])\n#         accuracy = validate_accuracy(df_score)\n#     return df_score, accuracy, (epoch_loss \/ len(valid_loader))\n\n# def validate_accuracy(df_score):\n#     return len(df_score[df_score['less'] < df_score['more']]) \/ len(df_score)\n\n# def return_wrong_text(df_score, df_valid):\n#     df_score_text = pd.concat((df_valid.reset_index().drop('index',axis=1),df_score),axis=1)\n#     return df_score_text[df_score_text['less'] > df_score_text['more']]","c0287297":"# def remove_duplicates(df, used_col):\n#     \"\"\"Combine `less_toxic` text and `more_toxic` text,\n#     then remove duplicate pair of comments while keeping the last pair\n#     \"\"\"\n#     df[\"combine\"] = df[\"less_toxic\"] + df[\"more_toxic\"]\n#     df = df.drop_duplicates(subset=used_col, keep=\"last\")\n#     return df","c474c2c3":"# data_train = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n# #data_train_remove_duplicates = remove_duplicates(data_train, \"combine\")\n\n# train_dataset = JigsawDataset(\n#         data_train, tokenizer=bert_tokenizer, \n#         max_length=max_token_length, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n#     )\n\n# train_loader = DataLoader(\n#         train_dataset, batch_size=batch_size, shuffle=True,\n#         num_workers=2)\n    \n# criterion = nn.MarginRankingLoss(margin=MARGIN)\n\n# df_score, valid_acc, valid_loss = validate_all_combine(\n#     model, criterion, all_loader, \n#     device, use_tfidf\n# )\n\n# print(f\"Recheck: Accuracy = {valid_acc}, loss = {valid_loss}\")","e8a27877":"def predict_combine(\n    model, test_loader, device, \n    use_tfidf=False, use_sentence_embedding=False\n):\n    predict = []\n    with torch.no_grad():\n        model.eval()\n        for data in test_loader:\n            text_ids = data['text_ids'].to(device, dtype = torch.long)\n            text_mask = data['text_mask'].to(device, dtype = torch.long)\n            if use_tfidf:\n                text_tfidf = data['tfidf'].to(device, dtype = torch.long)\n                score = model(text_ids, text_mask, text_tfidf, use_tfidf)\n            elif use_sentence_embedding:\n                text_sent_embed = data[\"sent_embed\"].to(device, dtype = torch.long)\n                score = model(\n                    text_ids, text_mask, \n                    sent_embed=text_sent_embed, use_sentence_embedding=use_sentence_embedding\n                )\n            else:\n                score = model(text_ids, text_mask)\n            score = score.view(-1).cpu().detach().numpy()\n            for pred in score:\n                predict.append(pred)\n    return predict","9f3753fa":"predict = predict_combine(\n    model, test_loader, device, \n    use_tfidf=use_tfidf, use_sentence_embedding=use_sentence_embedding\n)","6a3afef5":"predict = np.array(predict)","20f7403d":"print(f\"Total Predictiions: {predict.shape[0]}\")\nprint(f\"Total Unique Predictions: {np.unique(predict).shape[0]}\")","dd14aaae":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf.head()","b214ab9e":"df['score'] = predict\ndf.head()","4cc39304":"df['score'] = df['score'].rank(method='first')\ndf.head()","a2b1760f":"df.drop('text', axis=1, inplace=True)\ndf.to_csv(\"submission.csv\", index=False)","9417be35":"# Build sentence embedding","57333d47":"# Check training set accuracy \n## (Please annotate the following sections before submit to competition)","40deff08":"# Dataset and Dataloader","ddb1886b":"# Inference"}}