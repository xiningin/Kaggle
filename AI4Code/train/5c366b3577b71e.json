{"cell_type":{"6ca7bdbd":"code","65a9355c":"code","3938449e":"code","76bf0a18":"code","0e719306":"code","4ecfb390":"code","fe2e248a":"code","d2b27559":"code","36e503e1":"code","ec8c9e31":"code","a2658cf7":"code","dfc2daf2":"code","88bf4383":"code","58524aaa":"code","fa51f98c":"code","f2db83d6":"code","94f73b2e":"code","df7cec15":"code","eee55e9d":"code","bc923844":"code","55195f12":"code","7c1004ac":"code","0701e335":"code","0497109c":"code","2f2522d1":"code","c835f7f2":"code","198c279b":"code","f283310c":"code","64ffb349":"code","5a8019a8":"code","93411bcd":"code","efb80e65":"code","027953e7":"code","28040822":"code","e2710052":"markdown","ae858d7d":"markdown","015f50b7":"markdown","0fd543d7":"markdown","1dcc1fd2":"markdown","3d5c7080":"markdown","c09aadb8":"markdown","0bc4c14c":"markdown","14293a4f":"markdown","a8e448a7":"markdown","dc8711e6":"markdown","65b55e2f":"markdown","5b4201b9":"markdown","c2e8ed35":"markdown","56156f2c":"markdown","bcb715b7":"markdown","6c146126":"markdown","2fcdb281":"markdown","d735a57b":"markdown","abf92371":"markdown","2a750177":"markdown","83b5ce6c":"markdown","9313d302":"markdown","0dc07558":"markdown","c83091e3":"markdown"},"source":{"6ca7bdbd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_palette(\"bright\")\n%matplotlib inline\n\nprint(\"Data & File Sizes\")\ndata_dir = '..\/input\/cat-in-the-dat\/'\nfor f in os.listdir(data_dir):\n    if 'zip' not in f:\n        print(f.ljust(30) + str(round(os.path.getsize(f'{data_dir}{f}') \/ 1000000, 2)) + 'MB')","65a9355c":"df_train = pd.read_csv(f'{data_dir}train.csv')\npd.set_option('display.max_columns', 200) # show all columns\ndf_train.head()","3938449e":"df_test = pd.read_csv(f'{data_dir}test.csv')\ndf_test.head(1)","76bf0a18":"sns.countplot(df_train['target']).set_title('train')","0e719306":"bin_columns = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\nnom_columns = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nord_columns = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\ncyc_columns = ['day', 'month']","4ecfb390":"df_train[nom_columns+ord_columns].nunique()","fe2e248a":"lc_nom_columns = nom_columns[0:5]\nhc_nom_columns = nom_columns[5:10]\nlc_ord_columns = ord_columns[0:5]\nhc_ord_columns = ord_columns[5:6]","d2b27559":"fig, ax = plt.subplots(2, 3, figsize=(16,8))\nfig.suptitle('Binary Distribution vs. Distribution of Target Variable')\n\nfor ax, name in zip(ax.flatten(), list(df_train[bin_columns].columns)):\n    sns.countplot(x=df_train[name], ax=ax, hue=df_train['target'], saturation=1)","36e503e1":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndf_train['bin_3'] = le.fit_transform(df_train['bin_3'])\ndf_train['bin_4'] = le.fit_transform(df_train['bin_4'])\ndf_test['bin_3'] = le.fit_transform(df_test['bin_3'])\ndf_test['bin_4'] = le.fit_transform(df_test['bin_4'])\n\ndf_train[bin_columns].head()","ec8c9e31":"fig, ax = plt.subplots(5, 1, figsize=(18,10))\nfig.suptitle('Distribution of Target Variable Ratio = 1 \\n (lowest \u2192 highest ratio)')\n\nordinal_ordering = {}\n\nfor ax, name in zip(ax.flatten(), list(df_train[lc_ord_columns].columns)):\n    # calculate the ratio of target counts\n    ct = pd.crosstab(df_train[name], df_train['target']).apply(lambda r: r\/r.sum(), axis = 1)\n    # unstack the cross-tabulated df\n    stacked = ct.stack().reset_index().rename(columns = {0: 'ratio'}) \n    # sort by target ratio\n    stacked = stacked.sort_values(['target', 'ratio'], ascending = [False, True]) \n    sns.barplot(x = stacked[name], y = stacked['ratio'], ax = ax, hue = stacked['target'])\n    \n    # create mapping for encoding\n    ordinal_ordering[name] = stacked[name].unique()","a2658cf7":"# show the order of encoding for each ordinal column\nordinal_ordering","dfc2daf2":"# loop through low-cardinality ordinal columns and encode them\nfor col in lc_ord_columns:\n    nbr_to_replace = len(ordinal_ordering[col])\n    # print(nbr_to_replace) # quality control\n    df_train[col].replace(to_replace = ordinal_ordering[col], \n                          # had to drop a pythonic line \u00af\\_(\u30c4)_\/\u00af\n                          value = [x for x in range(0, len(ordinal_ordering[col]))], \n                          inplace = True)\n    df_test[col].replace(to_replace = ordinal_ordering[col], \n                          # had to drop a pythonic line \u00af\\_(\u30c4)_\/\u00af\n                          value = [x for x in range(0, len(ordinal_ordering[col]))], \n                          inplace = True)\n    \n#df_train[lc_ord_columns].nunique() # quality control - should match nbr_to_replace\n\ndf_train[lc_ord_columns].head()","88bf4383":"ord_5_num_unique = len(df_train['ord_5'].unique().tolist())\nprint(f'unique values in ord_5: {ord_5_num_unique}')\n\nsample = list(df_train['ord_5'].sample(10))\nprint(f'ex of ord_5 values: {sample}')\n\nstr_lengths = df_train['ord_5'].str.len().nunique()\nprint(f'different string lengths in ord_5: {str_lengths}')","58524aaa":"fig, ax = plt.subplots(figsize=(16,6))\n\nordinal_ordering = {}\n\n\nfig.suptitle('Distribution Target Variable ratio \\n (lowest \u2192 highest ratio)')\n\n# calculate the ratio of target counts\nct = pd.crosstab(df_train['ord_5'], df_train['target']).apply(lambda r: r\/r.sum(), axis = 1)\nstacked = ct.stack().reset_index().rename(columns = {0: 'ratio'})\nstacked = stacked.sort_values(['target', 'ratio'], ascending = [False, True])\n\nordinal_ordering['ord_5'] = stacked['ord_5'].unique() # for encoding\n\nsns.barplot(x = stacked['ord_5'], y = stacked['ratio'], hue = stacked['target'])\n\n# show less x-ticks\nevery_nth = 10\nfor n, label in enumerate(ax.xaxis.get_ticklabels()):\n    if n % every_nth != 0:\n        label.set_visible(False)","fa51f98c":"nbr_to_replace = len(ordinal_ordering['ord_5'])\n\ndf_train['ord_5'].replace(to_replace = ordinal_ordering['ord_5'], \n                          value = [x for x in range(0, len(ordinal_ordering['ord_5']))],\n                          inplace = True)\ndf_test['ord_5'].replace(to_replace = ordinal_ordering['ord_5'], \n                          value = [x for x in range(0, len(ordinal_ordering['ord_5']))],\n                          inplace = True)\n    \ndf_train['ord_5'].head()","f2db83d6":"# one hot encode low-cardinality nominal variables\ndf_train = pd.get_dummies(df_train, columns = lc_nom_columns)\ndf_test = pd.get_dummies(df_test, columns = lc_nom_columns)\ndf_train.filter(regex='nom_[0-4]_').head()","94f73b2e":"def freq_encoding(df, cols):\n    for col in cols:\n        # get variable frequencies\n        frequencies = (df.groupby(col).size()) \/ len(df) \n        # encode frequencies\n        df[f'{col}_freq'] = df[col].apply(lambda x : frequencies[x]) \n    return df","df7cec15":"df_train = freq_encoding(df_train, hc_nom_columns)\ndf_test = freq_encoding(df_test, hc_nom_columns)\ndf_train.filter(regex='nom_[5-9]_freq').head()","eee55e9d":"def feature_hashing(df, cols):\n    for col in cols:\n        df[f'{col}_hashed'] = df[col].apply(lambda x: hash(str(x)) % 5000)\n    return df","bc923844":"df_train = feature_hashing(df_train, hc_nom_columns)\ndf_test = feature_hashing(df_test, hc_nom_columns)\ndf_train.filter(regex='nom_[5-9]_hashed').head()","55195f12":"def encode_target_smooth(data, target, categ_variables, smooth):\n    \"\"\"    \n    Apply target encoding with smoothing.\n    \n    Parameters\n    ----------\n    data: pd.DataFrame\n    target: str, dependent variable\n    categ_variables: list of str, variables to encode\n    smooth: int, number of observations to weigh global average with\n    \n    Returns\n    --------\n    encoded_dataset: pd.DataFrame\n    code_map: dict, mapping to be used on validation\/test datasets \n    defaul_map: dict, mapping to replace previously unseen values with\n    \"\"\"\n    train_target = data.copy()\n    code_map = dict()    # stores mapping between original and encoded values\n    default_map = dict() # stores global average of each variable\n    \n    for col in categ_variables:\n        prior = data[target].mean()\n        n = data.groupby(col).size()\n        mu = data.groupby(col)[target].mean()\n        mu_smoothed = (n * mu + smooth + prior) \/ (n + smooth)\n        \n        train_target.loc[:, col] = train_target[col].map(mu_smoothed)\n        code_map[col] = mu_smoothed\n        default_map[col] = prior\n    return train_target, code_map, default_map","7c1004ac":"# additive smoothing\ntrain_target_smooth, target_map, default_map = encode_target_smooth(df_train, 'target', hc_nom_columns, 500)\ntest_target_smooth = df_train.copy()\nfor col in hc_nom_columns:\n    encoded_col = test_target_smooth[col].map(target_map[col])\n    mean_encoded = pd.DataFrame({f'{col}_mean_enc': encoded_col})\n    df_train = pd.concat([df_train, mean_encoded], axis=1)\n    \ndf_train.filter(regex='nom_[5-9]_mean_enc').head()","0701e335":"def impact_coding_leak(data, feature, target, n_folds=20, n_inner_folds=10):\n    from sklearn.model_selection import StratifiedKFold\n    '''\n    ! Using oof_default_mean for encoding inner folds introduces leak.\n    \n    Source: https:\/\/www.kaggle.com\/tnarik\/likelihood-encoding-of-categorical-features\n    \n    Changelog:    \n    a) Replaced KFold with StratifiedFold due to class imbalance\n    b) Rewrote .apply() with .map() for readability\n    c) Removed redundant apply in the inner loop\n    '''\n    impact_coded = pd.Series()\n    \n    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True) # KFold in the original\n    oof_mean_cv = pd.DataFrame()\n    split = 0\n    for infold, oof in kf.split(data[feature], data[target]):\n\n        kf_inner = StratifiedKFold(n_splits=n_inner_folds, shuffle=True)\n        inner_split = 0\n        inner_oof_mean_cv = pd.DataFrame()\n        oof_default_inner_mean = data.iloc[infold][target].mean()\n        \n        for infold_inner, oof_inner in kf_inner.split(data.iloc[infold], data.loc[infold, target]):\n            # The mean to apply to the inner oof split (a 1\/n_folds % based on the rest)\n            oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n\n            # Also populate mapping (this has all group -> mean for all inner CV folds)\n            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n            inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n            inner_split += 1\n\n        # compute mean for each value of categorical value across oof iterations\n        inner_oof_mean_cv_map = inner_oof_mean_cv.mean(axis=1)\n\n        # Also populate mapping\n        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n        oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n        split += 1\n\n        feature_mean = data.loc[oof, feature].map(inner_oof_mean_cv_map).fillna(oof_default_mean)\n        impact_coded = impact_coded.append(feature_mean)\n            \n    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n\ndef impact_coding(data, feature, target, n_folds=20, n_inner_folds=10):\n    from sklearn.model_selection import StratifiedKFold\n    '''\n    ! Using oof_default_mean for encoding inner folds introduces leak.\n    \n    Source: https:\/\/www.kaggle.com\/tnarik\/likelihood-encoding-of-categorical-features\n    \n    Changelog:    \n    a) Replaced KFold with StratifiedFold due to class imbalance\n    b) Rewrote .apply() with .map() for readability\n    c) Removed redundant apply in the inner loop\n    d) Removed global average; use local mean to fill NaN values in out-of-fold set\n    '''\n    impact_coded = pd.Series()\n        \n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True) # KFold in the original\n    oof_mean_cv = pd.DataFrame()\n    split = 0\n    for infold, oof in kf.split(data[feature], data[target]):\n\n        kf_inner = StratifiedKFold(n_splits=n_inner_folds, shuffle=True)\n        inner_split = 0\n        inner_oof_mean_cv = pd.DataFrame()\n        oof_default_inner_mean = data.iloc[infold][target].mean()\n        \n        for infold_inner, oof_inner in kf_inner.split(data.iloc[infold], data.loc[infold, target]):\n                    \n            # The mean to apply to the inner oof split (a 1\/n_folds % based on the rest)\n            oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n            \n            # Also populate mapping (this has all group -> mean for all inner CV folds)\n            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n            inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n            inner_split += 1\n\n        # compute mean for each value of categorical value across oof iterations\n        inner_oof_mean_cv_map = inner_oof_mean_cv.mean(axis=1)\n\n        # Also populate mapping\n        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n        oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True) # <- local mean as default\n        split += 1\n\n        feature_mean = data.loc[oof, feature].map(inner_oof_mean_cv_map).fillna(oof_default_inner_mean)\n        impact_coded = impact_coded.append(feature_mean)\n    \n    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n\ndef encode_target_cv(data, target, categ_variables, impact_coder=impact_coding):\n    \"\"\"Apply original function for each <categ_variables> in  <data>\n    Reduced number of validation folds\n    \"\"\"\n    train_target = data.copy() \n    \n    code_map = dict()\n    default_map = dict()\n    for f in categ_variables:\n        print(f'cv impact encoding {f}')\n        train_target.loc[:, f], code_map[f], default_map[f] = impact_coder(train_target, f, target)\n        \n    return train_target, code_map, default_map\n","0497109c":"train_target_cv, code_map, default_map = encode_target_cv(df_train[hc_nom_columns+['target']], \n                                                          'target', hc_nom_columns, \n                                                          impact_coder=impact_coding)\n\ntrain_target_cv = train_target_cv.drop('target', axis=1)","2f2522d1":"for col in train_target_cv.columns:\n    train_target_cv = train_target_cv.rename(columns={col: f'{col}_cvmean_enc'})\ntrain_target_cv.head()","c835f7f2":"df_train = pd.concat([df_train, train_target_cv], axis=1)","198c279b":"day_values = sorted(df_train['day'].unique().tolist())\nprint(f'day values: {day_values}')\nplt.plot(day_values)","f283310c":"def sin_cos_encode(df, cols):\n    for col in cols:\n        col_max_val = max(df[col])\n        df[f'{col}_sin'] = np.sin(2*np.pi * df[col]\/ col_max_val) # sin transform\n        df[f'{col}_cos'] = np.cos(2*np.pi * df[col]\/ col_max_val) # cos transform\n    return df","64ffb349":"df_train = sin_cos_encode(df_train, cyc_columns)\ndf_train.filter(regex='_(sin|cos)').head()","5a8019a8":"sample = df_train[['month_sin', 'month_cos']].sample(100)\nsample.plot.scatter('month_sin', 'month_cos').set_aspect('equal')","93411bcd":"# drop hexadecimal nominal columns\nX_train = df_train.drop(columns=['target', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis=1)\ny_train = df_train['target']","efb80e65":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=31429)","027953e7":"import xgboost as xgb\n\n# set parameters for xgboost\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'eta': 0.02,\n    'max_depth': 4\n}\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatch_list = [(d_train, 'train'), (d_valid, 'valid')]\n\nmodel = xgb.train(params, d_train, 400, watch_list, early_stopping_rounds=50, verbose_eval=25)","28040822":"#model.get_score(importance_type='gain')\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","e2710052":"Done!\n\n### High-cardinality Nominal Variables\n\nOkay, now we up the level of complication. I don't particularly have much experience here but I know there are a few options for techniques. Some I'll implement on my own, some I'll refer to the community for (if I forget to attribute the source, please notify me in the comments). \n1. Frequency encoding: encode the frequency of values in the column\n2. Feature hashing: create a hash of string values\n3. Mean encoding (also target, likelihood, impact encoding): each distinct value of a categorical value is replaced with the average value of the target variable we're trying to predict. All code for this is attributed to @dustinthewind's notebook [Making sense of mean encoding](https:\/\/www.kaggle.com\/dustinthewind\/making-sense-of-mean-encoding). Two techniques are used here:\n    1. Additive smoothing - reduce overfitting by relying on the global average of the target variable.\n    2. Cross-validation - introduce variability into encoding estimates by averaging mean over folds.","ae858d7d":"Sweet! This follows our intuition exactly - I say that because we can observe a linear relationship in every one of these plots between the target ratio and our logical ordering. We can reliably move forward encoding the ordinality of this intuition.\n\n*Note:*\n* *Our training set follows our assumption. This may not be true for the train\/validation set.*\n* *Make assumptions on the target ratio could very well lead to overfitting*\n\nIf you have a better way of doing this encoding, please leave a comment!","015f50b7":"# Cyclical Features\n\nWhat do we mean by cyclical features? Well, we have some in our dataset in the form of **time**. Days of the week, hour of the day, etc - they all follow cycles. \n\n<img src=\"https:\/\/i.imgur.com\/ZctoWQ4.png\" width=\"400\">\n\nCyclical features aren't only in the form of time though - \"Ecological features like tide, astrological features like position in orbit, spatial features like rotation or longitude, visual features like color wheels are all naturally cyclical.\" - quoted from Ian London's [blog post](https:\/\/ianlondon.github.io\/blog\/encoding-cyclical-features-24hour-time\/) which also has a great talk through of the techniques we're going to use here.\n\nThe problem with raw cyclical data is that it doesn't explicitly show the relationship between the nature of it's cycle. For example, if we were to plot the unique values of our `day` column:","0fd543d7":"There's a pretty clear cardinality separation between each. \n* `nom_{0,4}`=low-card\n* `nom_{5,9}`=high-card\n* `ord_{0,4}`=low-card\n* `ord_5`=high-card. \n\nLet's make those splits:","1dcc1fd2":"Ta-da! These features can now be fed into algorithms and the cyclical nature will be maintained.","3d5c7080":"Another technique I tried to no avail was converting the `ord_5` strings to their unicode point values. I then plotted their target ratio like I did above but the linear relationship was broken and I scrapped it. \n\nThe code for applying that transformation to a column is as follows:\n```\ndef sum_string(string): \n    return sum(ord(char) for char in string)\n    \nord_5['ord_5_ascii_val'] = ord_5['ord_5'].apply(sum_string)\n```","c09aadb8":"Okay, so we have some pretty clear groupings of features here\n* **id: ** a simple row id\n* **bin_{0,4}: ** binary features\n* **nom_{0,9}: ** nominal features. Assumption - {0,4} have low cardinality, {5,9} have high cardinality\n* **ord_{0,1}: ** ordinal features. Assumption - {0,3?} have low cardinality, {4?,5} have high cardinality\n* **day, month: ** cyclical features\n* **target: ** the **label** we are trying to predict. Looks to be binary.\n\nLet's make sure the test data follows the same form.\n\n### Test Data","0bc4c14c":"We're given 3 files, one training set, one test set and one sample submission. They're all quite small, weighing it at less than 50MB a pop.\n\nLet's check out the training data.\n\n### Training set","14293a4f":"We're good! Onwards\n\n### High-cardinality Ordinal Features\n\nI opted to split these out separately in case any additional steps (vs. the ones we took on the low-cardinality ord features) would be needed. The only difference here is that this column contains many more unique values. \n\nBefore we make any decisions, lets get another look at this column's data","a8e448a7":"Nice! Looks like theres a linear relationship here between alphabetical position and target ratio. We can move forward with encoding these the same way we did above:","dc8711e6":"The issue here? Look at the y-axis. If we call 1 Monday and 7 Sunday, it's clear to see that their cyclical relationship dissolves. As it stands, these points are **6 days apart**...which is true...except the day after Sunday is Monday **so they're actually also 1 day apart**. \n\n**The solution:**\n\nWe create two new features - one a sine transformation and one a cosine transformation. I'll illustrate their combined power after encoding them.","65b55e2f":"Boom! Done. We can move on.\n\n# Ordinal Features\n\nOrdinal variables inherently possess a natural order. For example, if we had an `cat-age-group` column and the following image served as our data:\n![cats growing](https:\/\/www.catcare4life.org\/app\/uploads\/2018\/03\/lifestages.jpg)\nWe would expect these to be labeled left to right as something like ['kitten', ... ,'mature']\n\n### Low-cardinality Ordinal Features\n\nFor the five low-cardinality features that we have (`ord_{0,4}`)...we can assume that they follow some form like:\n* `ord_0`: **Already correct!** Values range from 1 to 3\n* `ord_1`: Novice > ... > Grandmaster\n* `ord_2`: Cold > .. > Hot\n* `ord_3`: a > ... > o (or is it o < .. < a?)\n* `ord_4`: A > ... > Z (or see above)\n\nBefore encoding these via intuition, let's compare each feature's values against their distribution of labels:","5b4201b9":"So we have 192 unique values - each a 2 character string with varying positions of capital characters. \n\nLet's plot this column using the same ratio technique above and see if we can uncover some underlying form:","c2e8ed35":"Okay, heaviest stuff is done.\n<img src=\"https:\/\/www.vtfoodbank.org\/wp-content\/uploads\/2016\/04\/cat-on-computer.jpg\" width=\"400\">","56156f2c":"Let's get into it now.","bcb715b7":"# Nominal Features\n\nNominal features differ from ordinal features in one way: **they hold no intrinsic order**. For example, if we had a feature `breed` and the following image was split in half:\n![two cats](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTbErhwsYXKnUcdlWa-mBKfLL_IO9bX9Ay1N-2-3ZNUmFP0ud45)\n\n...this nominal feature would be scored as `Siamese` for the left cat and `Tabby` for the right (he atacc).\n\n### Low-cardinality Nominal Features\n\nThe only thing we need to do to these is one hot encode them. \n\nOne hot encoding works by converting every category in a column into it's own binary column - Like this:\n\n![OHE](https:\/\/i.imgur.com\/mtimFxh.png)\n\nAs for our data: `nom_0` has 3 unique values, `nom_{1,3}` each have 6 unique values and `nom_4` has 4 unique values. \n\nOne hot encoding the lot of these will give us grow our DataFrame by 3 + 6 + 6 + 6 + 4 columns i.e., 25 new columns. **This process would be unyieldy for high-cardinality nominal variables as we would begin knocking on the door of memory issues.** Why? Because the column count would grow at a much larger rate.\n\nBack to the work. We can simply use `panda`'s built-in `get_dummies` function to achieve this one hot encoding:","6c146126":"What do we see here?\n* **The bad**\n    * Hash encoded nominal features performed poorly. \n    * Frequency encoded nominal features didn't fare much better.\n    * Mean encoded nominal features weren't killer either.\n* **The good**: \n    * Cross-validated impact encoded nominal features were sort of important!\n    * Some of the simple one-hot encoded features turned out to be important.","2fcdb281":"# Binary Features\nBinary features are features that contain either a 1 or a 0. For example, if we had a column `is-tabby-cat` and the following two images:\n<img src=\"https:\/\/www.thehappycatsite.com\/wp-content\/uploads\/2017\/06\/tabby-kitten.jpg\" width=\"300\"><img src=\"https:\/\/www.dogster.com\/wp-content\/uploads\/2015\/05\/husky-puppies-01.jpg\" width=\"300\">\n\nWe would probably expect a 1 for the first image and a 0 for the second.\n\nBefore doing any encoding, let's see if we can glean some relationship between each feature's binary distribution vs. the distribution of target labels:","d735a57b":"Obvious difference: no target variable in the test set. \n\nNow, let's look at the distribution of target variables in the training set:","abf92371":"Next, we're going to need to understand which nominal & ordinal features to section off as low vs. high cardinality. \n\n### Cardinality\nWhat is cardinality? Mathematically it's defined as *the number of elements in a set or other grouping, as a property of that grouping*. Basically, if a column has many unique values, we could define it as having high-cardinality; low-cardinality would be the opposite.\n\n**For Example:**\n\nLow-cardinality `breed-of-cats` column:\n\n<img src=\"https:\/\/i.imgur.com\/xCgDQq4.jpg\" width=\"400\">\n\n*There are many cats here but all of the same breed.*\n\nHigh-cardinality `breed-of-cats` column: \n\n<img src=\"http:\/\/duckboss.com\/wp-content\/uploads\/2016\/02\/cats1.png\" width=\"400\">\n\n*There are many cats of differing breeds.*\n\nWe can separate the features by cardinality via looking at unique counts:","2a750177":"## Encoding Categorical Features\n\nWelcome to Categorical Feature Encoding Challenge presented by [Kaggle](https:\/\/www.kaggle.com)! A common task in machine learning pipelines is encoding categorical variables into a form digestable for algorithms without losing valuable information. We are tasked with taking a dataset  **solely comprised of categorical variables** and creating appropriate encoding schemes. I won't be doing any feature engineering - just encoding schemes.\n\nThese categorical variables include:\n* **Binary** features\n* **Nominal** features (of varying cardinality)\n* **Ordinal** features (of varying cardinality)\n* **Cyclical** features\n\n#### Table of Contents\n1. Exploration of the dataset\n2. Handling of binary features\n3. Handling of ordinal features\n4. Handling of nominal features\n5. Handling of cyclical features\n\n**If this notebook was of help to you, upvotes are very much appreciated - they're what keep me going.**\n\nLet's dive in!\n\n# Data Exploration","83b5ce6c":"Nothing here really sticks out to me that's worth doing anything about.\n\nNow we'll get down to **Encoding**. What our algorithms are looking for are numerical values. Out of these binary features the only ones that need to be transformed are `bin_3` and `bin4`. To do this, we'll use `sklearn`'s `LabelEncoder` which transforms categories into numbers.","9313d302":"The 0 labeled target has twice the occurences as the 1 label. Not a ton to glean from this at the moment, so let's move on the meat of this notebook: **the encoding**. \n\nFirst task will be bucketing the different groups of labels by type (binary, ordinal and so on):","0dc07558":"## Modeling\n<img src=\"https:\/\/merriam-webster.com\/assets\/mw\/images\/gallery\/gal-wap-slideshow-slide\/cat-using-abacus-for-arithmetic-4133-38c20f1d3412a2ecf4e756e82c1bc11e@1x.jpg\" width=\"400\">\n\nNow that we've done a load of encoding, it's time to see how these new variables perform. I'll train an xgboost classifier and plot feature importances.\n\n*Note - I've created this notebook as a resource and exercise in encoding, so I won't be seeing this section through to perfection.*","c83091e3":"Let's see what that actually did:"}}