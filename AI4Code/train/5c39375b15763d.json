{"cell_type":{"895fd884":"code","ca4a35eb":"code","c4630a38":"code","261d467b":"code","83b2c782":"code","0c4b5916":"code","c2b81a2a":"code","608f3ca0":"code","a335797e":"code","3ac78ec4":"code","fc14197a":"code","69f97ae9":"code","6cb57974":"code","1f3989f3":"code","59c9faa2":"code","b61dc2a8":"code","63ef3ac9":"code","771c429c":"code","6bb56916":"markdown","de190d6e":"markdown","8c4a2469":"markdown","ba5f996a":"markdown","d5927ac7":"markdown","b7669f2e":"markdown","2329e935":"markdown","18f79652":"markdown"},"source":{"895fd884":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","ca4a35eb":"train_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')","c4630a38":"train_df.head()","261d467b":"plt.figure(figsize=(10,8))\nsns.countplot(train_df.label)","83b2c782":"f, ax = plt.subplots(2,5) \nf.set_size_inches(10, 10)\nk = 0\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(x_train_data[k].reshape(28, 28) , cmap = \"gray\")\n        k += 1\n    plt.tight_layout() ","0c4b5916":"encoder = OneHotEncoder() # used for encoding y values as this is a multiclass classification problem.\ny_train_data = train_df.label.values\ny_train_data = encoder.fit_transform(y_train_data.reshape(-1,1)).toarray()\ndel train_df['label']\n\nx_train_data = train_df.values\nx_train_data.shape, y_train_data.shape","c2b81a2a":"x_test_data = test_df.values\nx_test_data.shape","608f3ca0":"# normalizing our data.\nscaler = MinMaxScaler(feature_range=(0,1))\nx_train_data = scaler.fit_transform(x_train_data)\nx_test_data = scaler.transform(x_test_data)","a335797e":"# Reshaping the data (i.e. images) for model from 1-D to 3-D.\nx_train_data = x_train_data.reshape(-1, 28, 28, 1)\nx_test_data = x_test_data.reshape(-1, 28, 28, 1)","3ac78ec4":"x_train_data.shape, y_train_data.shape","fc14197a":"# splitting training data into validation and training to train our model on.\nx_train, x_validation, y_train, y_validation = train_test_split(x_train_data, y_train_data, random_state=0)","69f97ae9":"model=Sequential()\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units = 512 , activation = 'relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units = 10 , activation = 'softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","6cb57974":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\nhistory = model.fit(x_train, y_train, batch_size=40, epochs=20, validation_data=(x_validation, y_validation), callbacks=[rlrp])","1f3989f3":"print(\"Accuracy of our model on Validation Data : \" , model.evaluate(x_validation,y_validation)[1]*100 , \"%\")\nplt.title('Accuracf training and validation data.')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()","59c9faa2":"# predicting on validation data.\npred_validation = model.predict_classes(x_validation)\nnew_y_validation=[]\nfor i in y_validation:\n    new_y_validation.append(np.argmax(i))\n\ncm = confusion_matrix(new_y_validation, pred_validation)\ncm = pd.DataFrame(cm , index = [i for i in range(10)] , columns = [i for i in range(10)])\nplt.figure(figsize = (10,10))\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')","b61dc2a8":"predictions = model.predict_classes(x_test_data)\npredictions[:5]","63ef3ac9":"submission['Label'] = predictions\nsubmission.to_csv(\"submission.csv\" , index = False)","771c429c":"submission.head()","6bb56916":"# Now lets predict for our test data.","de190d6e":"# Reading our data","8c4a2469":"**Preview of Images**","ba5f996a":"# Import Essential Libraries","d5927ac7":"# Data Visualisation.","b7669f2e":"# Data Preprocessing","2329e935":"# **If you like the notebook please give an upvote.**\n# **Comment down for any kind of suggestion.**","18f79652":"# Creating Model"}}