{"cell_type":{"dbd606da":"code","29737ef5":"code","56fc0f07":"code","c303a443":"code","66961adf":"code","c27ba8ee":"code","343f1518":"code","704e8e0b":"code","0f0ff179":"code","4d7d8041":"code","00de36c8":"markdown","ee6386b1":"markdown","3e7f76c2":"markdown","912ece6b":"markdown","f31322e1":"markdown","f75732f7":"markdown","d75a8bbc":"markdown"},"source":{"dbd606da":"# Import important libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Plot\nfrom sklearn.model_selection import train_test_split # train test split\nfrom sklearn.neighbors import KNeighborsClassifier # knn model\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","29737ef5":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata.head()","56fc0f07":"data[\"class\"][::20]","c303a443":"A = data[data[\"class\"] == \"Abnormal\"]\nN = data[data[\"class\"] == \"Normal\"]\n# scatter plot\nplt.scatter(A.sacral_slope,A.pelvic_incidence,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(N.sacral_slope,N.pelvic_incidence,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"sacral_slope\")\nplt.ylabel(\"pelvic_incidence\")\nplt.legend()\nplt.show()","66961adf":"data[\"class\"] = [1 if each == \"Abnormal\" else 0 for each in data[\"class\"]]\ny = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)","c27ba8ee":"# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","343f1518":"# train test split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)","704e8e0b":"# knn model\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" k = {} , score = {} \".format(3,knn.score(x_test,y_test)))","0f0ff179":"score_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","4d7d8041":"knn = KNeighborsClassifier(n_neighbors = 13) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" k = {} , score = {} \".format(13,knn.score(x_test,y_test)))","00de36c8":"As you see, we find better k value with the simple optimization code. ","ee6386b1":"* It seems like the best k value is 13. Let's try.","3e7f76c2":"* As you see, our output contains Normal and Abnormal observations\n* We will convert Normal as 0, Abnormal as 1\n* Let's Plot our data first","912ece6b":"This notebook was prepared with the help of DATAI Team Machine Learning Udemy Course and DATAI Team Machine Learning Tutorial for Beginners Notebook on Kaggle.","f31322e1":"* Accuracy is 75.27% so is it good ?\n* Now the question is why we choose K = 3 or what value we need to choose K.\n\n* K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace.\n* Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit.\n* Let's write the code that find better k value","f75732f7":"* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict(): predicts the data\n* If you do not understand what is KNN, look at youtube there are videos like 4-5 minutes. You can understand better with it.\n* Lets learn how to implement it with sklearn\n* x: features\n* y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points","d75a8bbc":"## K-NEAREST NEIGHBORS (KNN)\n* The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\n* The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n![image.png](attachment:image.png)\n* Notice in the image above that most of the time, similar data points are close to each other. The KNN algorithm hinges on this assumption being true enough for the algorithm to be useful. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood\u2014 calculating the distance between points on a graph."}}