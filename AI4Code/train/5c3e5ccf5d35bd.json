{"cell_type":{"e19c3251":"code","0dc66eb6":"code","e98e2d36":"code","302b31ab":"code","ae5c9e85":"code","fcef20c6":"code","63e8319e":"code","ec126c44":"code","ba13d7ce":"code","473db194":"code","9736e4ae":"code","ccd85a60":"code","c36a19aa":"code","ade5afd3":"code","38b1bb57":"markdown","87495ef6":"markdown","024fe7a4":"markdown","272853af":"markdown","dd5adf09":"markdown","0540ca9d":"markdown","ed92dc7e":"markdown","e3ecbfe4":"markdown","6784e1b4":"markdown"},"source":{"e19c3251":"!git clone https:\/\/github.com\/Apunti\/covid19-kaggle.git","0dc66eb6":"cd covid19-kaggle","e98e2d36":"!git clone https:\/\/github.com\/huggingface\/transformers","302b31ab":"cd transformers","ae5c9e85":"pip install .","fcef20c6":"cd ..","63e8319e":"#from qa_pipeline import *","ec126c44":"from transformers import pipeline\nfrom ranking import Ranking\nfrom get_result import get_level_evidence, get_design\n\nimport pandas as pd\n\ndef get_answer_from_doc(query, doc, qa_model):\n\n    output = []\n    \n    seq_length = 250\n    stride = 150\n    splitted_doc = doc.split(' ')\n    \n    i = 0\n    while len(splitted_doc) > seq_length:\n        paragraph = ' '.join(splitted_doc[:seq_length])\n        m_input = {'question': query,\n                 'context': paragraph}\n        try:\n            output_dict = qa_model(m_input)\n        except:\n            if i!=0:\n                splitted_doc = splitted_doc[stride:]\n                i = 0\n            i += 1\n            continue\n        answer = output_dict['answer']\n        score = output_dict['score']\n\n        output.append((answer, score, paragraph))\n        \n        splitted_doc = splitted_doc[stride:]\n        i = 0\n        \n    error = False    \n    if len(splitted_doc) > 127:\n        \n        paragraph = ' '.join(splitted_doc)\n        m_input = {'question': query,\n                 'context': paragraph}\n        print('processing paragraph...', end= '')\n        try:\n            output_dict = qa_model(m_input)\n        except:\n            error = True\n            pass\n        if error:\n            error = False\n        else:\n            answer = output_dict['answer']\n            score = output_dict['score']\n        \n            output.append((answer, score, paragraph))\n    \n    sorted_output = sorted(output, key=lambda x: x[1], reverse=True)\n    \n    if len(sorted_output) == 0:\n        sorted_output = [('-', 0, '-')]\n        \n    #print('### ANSWER ###')\n    #print(sorted_output[0])\n\n    if sorted_output[0][1] > 0.3:\n        return sorted_output[0][0], sorted_output[0][2]\n    else:\n        return '-', sorted_output[0][2]\n    \ndef get_documents(dataset, ranking, query, top_k = 10):\n\n    similar = ranking.most_similar(query, dataset, k = top_k, func='bm25', data='text')\n    print('similar length: {}'.format(len(similar)))\n\n    return similar\n\ndef get_information(row):\n    date = row['date'].values[0]\n    url = row['url'].values[0]\n    authors = row['authors'].values[0]\n    if len(authors) > 20:\n        authors = authors[:20] + ' [...]'\n    title = row['title'].values[0]\n    if len(title) > 60:\n        title = title[:60] + ' [...]'\n    design = get_design(row)\n    level_of_evidence = get_level_evidence(row)\n    \n    new_line = {'date': date,\n                'title': '<a href=\"' + url + '\">' + title + '<\/a>',\n                'authors': authors,\n                'design': design,\n                'level_of_evidence': level_of_evidence}\n    \n    return new_line\n    \n\ndef get_csv(df, csv_path, risk_factor, questions, top_k = 1, device = -1, dict_path = 'Data\/ranking_dict'):\n\n    dataset = df #pd.read_csv(df_path, sep=';')\n\n    ranking = Ranking('texts', path= dict_path)\n    qa_model = pipeline('question-answering', device=device, model='bert-large-uncased-whole-word-masking-finetuned-squad')\n\n    print('All loaded')\n\n    all_query = questions[0] #' '.join(questions)\n    documents = get_documents(dataset, ranking, all_query, top_k = top_k)\n    print('Length documents: {}'.format(len(documents)))\n\n    results = pd.DataFrame(columns=['date', 'title', 'authors', 'design', 'level_of_evidence'] + questions)\n\n    print('Starting docs for {}: \\n'.format(risk_factor))\n\n    for doc in documents:\n        skip = False\n        row = dataset.loc[dataset.text == doc]\n        new_line = get_information(row)\n        #new_line = {'paper_id': paper_id}\n        for query in questions:\n            answer, paragraph = get_answer_from_doc(query, doc, qa_model)\n            if answer == '-' or type(answer) != str:\n                skip = True\n                continue\n            else:\n                index = paragraph.find(answer)\n                paragraph = '[...] ' + paragraph[index-50:index+70] + ' [...]'\n                new_line[query] = str(paragraph.replace(answer, f\"<mark>{answer}<\/mark>\")) \n        if skip == True:\n            continue\n        results = results.append(new_line, ignore_index=True)\n        \n    results.to_csv(csv_path, sep=';', index=False)","ba13d7ce":"import pandas as pd\n\ndf = pd.read_csv('\/kaggle\/input\/processed-data-v8\/processed_data_v8_2.csv', sep=';')\nprint(df.columns)","473db194":"materials = ['plastic', 'stainless steel', 'cardboard', 'stool']\n\nd = {}\nfor material in materials:\n    d[material] = ['Up to how many days is the persistance of the virus on ' + material]\n                   #'How long is the persistance and stability of the virus on ' + material,\n                   #'How long remains viral activity on ' + material,\n                   #'What is the viral shedding in ' + material]","9736e4ae":"import os\n\nos.mkdir('csv')","ccd85a60":"for key in d:\n    get_csv(df, 'csv\/' + key + '.csv', key, d[key], top_k = 10, device = -1, dict_path='\/kaggle\/input\/ranking-dict-v8')","c36a19aa":"from IPython.core.display import HTML\npd.set_option('display.max_colwidth', 200)","ade5afd3":"for material in materials:\n    display(HTML('<h2>{}<\/h2>'.format(material.upper())))\n    table = pd.read_csv(\"csv\/{}.csv\".format(material), sep=';')\n    df_table=HTML(table.to_html(escape=False,index=False))#, col_space=150))\n    display(df_table)","38b1bb57":"<h2> <span style=\"color:green\"> Information Extraction <\/span> <\/h2>\n\n> The aim of the information extraction is to find an answer for a given question in a given article. First we need to exptract the paragraph(s) that contain(s) the answer and then find the answer in a given paragraph.\n\nWe use a new baseline for the SQuAD based on BERT. \n\n**BERT** (Bidirectional Encoder Representations from Transformers) is a recent model introduced by researchers at Google AI Language. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words in a text. In its vanilla form, Transformer includes two separate mechanisms \u2014 an encoder that reads the text input and a decoder that produces a prediction for the task.\n\nThe chart below is a high-level description of the Transformer encoder. The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size H, in which each vector corresponds to an input token with the same index.\n\nBefore feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n\n<img src=\"https:\/\/user-images.githubusercontent.com\/28005338\/84592125-87a4fc80-ae43-11ea-9d18-09678cd9e43f.png\" width=\"450\" align=\"center\"\/>\n\nGenerally speaking during training BERT learns to understand a language and the relation between words. Then BERT has to be fine-tuned for a specific task: *the model receives a question regarding a text sequence and is required to mark the answer in the paragraph*.\n\nFor this reason we use **SQuAD 2.2 dataset** (Stanford Question and Answers Dataset) is a question answering dataset containing  100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.\n\nThe model tokenize every example in QA, then generate multiple instances per example by concatenating a \u201c[CLS]\u201d token, the tokenized question, a \u201c[SEP]\u201d token, tokens from the content of the document, and a final \u201c[SEP]\u201d token, limiting the total size of each instance to 512 tokens. For each document we generate all possible instances, by listing the document content. For each training instance start and end token indices are computed  to represent the target answer span.\n\nThe chart below is a high-level description of the fine-tuning BERT:\n<img src=\"https:\/\/user-images.githubusercontent.com\/28005338\/84592410-d5baff80-ae45-11ea-970e-f93d3ac717a8.png\" align=\"center\"\/>\n\n**Example continuation**\n\nAfter document selection we stay with the Document B:\n\n*Document B: \"The children sat around the fire\"*\n\nAnd now the idea is to extract the answer for the following question:\n\n*Query: \"Who is sitting around the fire?\"*\n\nThen the model output will be\n\n<img src=\"https:\/\/user-images.githubusercontent.com\/28005338\/84592625-6c3bf080-ae47-11ea-9cd3-5820fdede06f.png\" align=\"center\"\/>\n\nSince we are working with the long documents, the model will first select only those paragraphs that contain the answer and for each paragraph will perform the extraction.","87495ef6":"<h2> <span style=\"color:green\"> Document Retrieval <\/span> <\/h2>\n\n> The aim of the document retrieval is to select the most relevant articles for a query. To do so, for a given query, we rank each document using BM25+ scoring. \n\n**BM25+** is the next generation of TFIDF (term frequency\u2013inverse document frequency) and stands for \u201cBest Match 25 +\u201d. It is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. The score increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. BM25+ is the version of BM25 adapted for a long documents.\n\n**Example**\n\n*Document A: \"The man went out for a walk\"*\n\n*Document B: \"The children sat around the fire\"*\n\nFirst we create a dictionary of words and their occurence for each document in the corpus (collection of documents):\n\n![freq](https:\/\/user-images.githubusercontent.com\/28005338\/84591334-c3d55e80-ae3d-11ea-9b59-addde9b0fc76.png)\n\nin which row 0 stands for the document A and row 1 - for the document B.\n\n**Term Frequency (TF)** is defined as *the number of times a word appears in a document divided by the total number of words in the document.* Every document has its own term frequency.\n\n**Inverse Data Frequency (IDF)** id defined as *the log of the number of documents divided by the number of documents that contain the word w.* Inverse data frequency determines the weight of rare words across all documents in the corpus.\n\nLastly, the **TF-IDF** score is simply *the TF multiplied by IDF*.\n\n![fff](https:\/\/user-images.githubusercontent.com\/28005338\/84591503-3dba1780-ae3f-11ea-8e30-d0f365a4a30a.png)\n\nAs one can see, TF-IDF score for the word \"fire\" much higher for the document B (row 1), than for the document A (row 0) when \"walk\" is more significant for the document A. \nThus, if the query would be **\"Who is sitting around the fire?\"**, our document retrieval will select the document B as the most related document to the query.\n\nWe need to clarify, that certain words are used to formulate sentences but do not add any semantic meaning to the text. For example, the most commonly used word in the english language is **the** which represents 7% of all words written or spoken. You couldn\u2019t make deduce anything about a text given the fact that it contains the word **the**. On the other hand, words like **fatality** and **death** could be used to determine a death rate for example. In natural language processing, useless words are referred to as stop words, for this reason we filter all stop words.","024fe7a4":"<h2> <span style=\"color:green\"> Approach <\/span> <\/h2> \n\nWe designed a pipeline that consists of three parts: \n- document retrieval, \n- information extraction and \n- creating table of results for each material. ","272853af":"<h2> <span style=\"color:green\"> Difference with other approaches <\/span> <\/h2> \n\nWe have seen that most approaches rely entirely on embedding techniques to find the answer or on document retrieval techniques to find the most relevant documents. We tried to use the best things of both approaches and improve them using the most recent achievements of NLP.","dd5adf09":"<h2> <span style=\"color:green\"> Data Preprocessing <\/span>  <\/h2>\nWe go through each row of the metadata and if it has full text, we append it to our dataframe, extracting the text from pmc folder, if possible, or from the pdf one. The preprocessing that we do for each row is the following:\n\n- We get rid off the documents that are not written in English with the package `langdetect`.\n- We get rid off the texts that has less than 200 words.\n- The abstract and the text are strings with the paragraphs separated by new line character (`\\n`). \n\n<h3> <span style=\"color:green\"> Tagging <\/span>  <\/h3>\n\nFollowing the contribution of the [notebook](https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions) of Andy White we decided to add a disease and design tag. We think the disease tag is needed because we are mostly interested in the COVID-19 information, and we thougth that the level of evidence of the studies is really useful for the medical research community when looking for answers. the level of evidence is based on the design following the [guides](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137027) of Savanna Reid. The main weakness of this tagging is that it relies on looking for keywords through the document, so it's not an optimal classification.","0540ca9d":"<h2> <span style=\"color:green\"> Installing and importing requiered libraries <\/span> <\/h2>  ","ed92dc7e":"![retriv](https:\/\/user-images.githubusercontent.com\/28005338\/84590315-c7fd7e00-ae35-11ea-935d-462d1b15434a.png)","e3ecbfe4":"<center>\n<img src=\"https:\/\/www.schwarzwaelder-bote.de\/media.media.c5d3a492-5f32-4bcc-83f3-27e779ad4d46.original1024.jpg\" width=\"1000\" align=\"center\"\/>\n <br><br>\n <h1>  <span style=\"color:green\"> COVID-19 Open Research Dataset Challenge (CORD-19) <\/span>  <\/h1>\n    <h2>  <span style=\"color:green\"> TASK : summary tables about material studies <\/span>  <\/h2>\n\n<br>\n\n**The aim of this notebook is to provide a robust algorithm that can help the medical community to find useful information about COVID-19**\n<\/center>","6784e1b4":"<h2> <span style=\"color:green\"> Results <\/span>  <\/h2>"}}