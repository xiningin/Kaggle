{"cell_type":{"0e4a6110":"code","cdc264a5":"code","8b88d1f2":"code","6545d5ef":"code","038c7d8a":"code","0cc925c4":"code","e89527dc":"code","e62305ce":"code","f7d65d53":"code","ef729f33":"code","9c2ef286":"code","cb816586":"code","0c4db7f5":"code","395a8bd6":"code","4a08d04d":"code","98504fb4":"code","a51c1b43":"code","d6b618f7":"code","6572d1ee":"code","8ce806ec":"code","9807cd22":"code","758ffb59":"code","09955966":"code","5dcdfd61":"code","5a74d11f":"code","b747a35c":"code","83dc61c2":"code","66b7fc75":"code","75980fbf":"code","7bbc9dba":"code","229f9432":"code","f68d042b":"code","1868fec1":"code","96fcca57":"code","ddd44de4":"code","d9aeb6b4":"code","88eff2dd":"code","b8a75db6":"code","292ffaa7":"code","c0b9acda":"code","dec08972":"code","863a3b81":"code","dbc33d5f":"code","329b2f35":"code","840c1ca3":"code","d63bb953":"code","f120a623":"code","2a02ad4e":"code","340cb951":"code","cf1a1d46":"code","117108d5":"code","88d2084e":"code","5722e1c3":"code","3039dc95":"code","2faaf9e3":"code","224f36e6":"code","d21fb972":"code","e9565270":"code","038fa21e":"code","8cc3336e":"code","87cc4f66":"code","78b725d3":"code","3e220376":"code","afaeb2a4":"code","76e034dd":"code","4a2d690f":"code","f3a504b1":"code","0e51f872":"code","d89cfd4a":"code","8638c571":"code","35314b5e":"code","4e2f3e0a":"code","fafb0939":"code","13fabcd1":"markdown","c716e7bf":"markdown","f3b2986a":"markdown","79d6f8a6":"markdown","dbddc129":"markdown","b5c3016d":"markdown","7aa9c8a0":"markdown","9b657a6f":"markdown","0956fe13":"markdown","438c17f5":"markdown","9b60eb78":"markdown","c01b885a":"markdown","97812336":"markdown","b99f4bac":"markdown","f5366d14":"markdown","f6019d74":"markdown","85fd0756":"markdown","8d44459c":"markdown","f77d7f51":"markdown","17b8218b":"markdown","c5883f0f":"markdown","6be7b9fd":"markdown","1ece8241":"markdown","069c9646":"markdown","d6883715":"markdown","c36a0187":"markdown","2577f22b":"markdown","67f89d33":"markdown","adc7d720":"markdown","3bd52493":"markdown","ce40fe56":"markdown","b9d8f52c":"markdown","70770b41":"markdown","42b5cffa":"markdown","a9693c33":"markdown","2e7888b4":"markdown","88871f15":"markdown","7893b336":"markdown","39eaa4da":"markdown","0bd7d034":"markdown","a38df9ed":"markdown","683a427b":"markdown","14a471f9":"markdown","b7c26e55":"markdown"},"source":{"0e4a6110":"# importing libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\n# tf.test.gpu_device_name()","cdc264a5":"df=pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')\ndf.head()","8b88d1f2":"df.info()","6545d5ef":"#  Checking missing values in dataset\ndf.isnull().sum().sum()","038c7d8a":"# Checking unique values in a column to categorize into continuous and categorical columns.\ndf.nunique()","0cc925c4":"# Dropping columns which are not necessary for prediction\ndf = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)","e89527dc":"df.shape","e62305ce":"df.dtypes","f7d65d53":"labels = 'Exited', 'Retained'\nsizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots(figsize=(9, 7))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')\nplt.title(\"Proportion of customer churned and retained\", size = 20)\nplt.show()","ef729f33":"sns.countplot(x='Geography', hue = 'Exited',data = df).set_title('Countplot-Geography Column')\n","9c2ef286":"sns.countplot(x='Gender', hue = 'Exited',data = df).set_title('Countplot-Gender Column')","cb816586":"sns.countplot(x='HasCrCard', hue = 'Exited',data = df).set_title('Countplot-HasCreditCard Column')","0c4db7f5":"sns.countplot(x='IsActiveMember', hue = 'Exited',data = df).set_title('Countplot-IsActiveMember Column')","395a8bd6":" # Relations based on the continuous data attributes\nfig, axarr = plt.subplots(3, 2, figsize=(20, 12))\nsns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = df, ax=axarr[0][0]).set_title('Boxplot- Credit Score Column')\nsns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = df , ax=axarr[0][1]).set_title('Boxplot- Age Column')\nsns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][0])\nsns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][1])\nsns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][0])\nsns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][1])","4a08d04d":"# 1st Attribute - Balance Salary Ratio\ndf['BalanceSalaryRatio'] = df.Balance\/df.EstimatedSalary\nsns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = df)\nplt.ylim(-1, 5)","98504fb4":"#  2nd Attribute-Tenure By Age\ndf['TenureByAge'] = df.Tenure\/(df.Age)\nsns.boxplot(y='TenureByAge',x = 'Exited', hue = 'Exited',data = df)\nplt.ylim(-0.2, 0.7)\nplt.show()","a51c1b43":"# 3rd Attribute- Credit Score Given Age\ndf['CreditScoreGivenAge'] = df.CreditScore\/(df.Age)\nsns.boxplot(y='CreditScoreGivenAge',x = 'Exited', hue = 'Exited',data = df)\nplt.show()","d6b618f7":"df.head()","6572d1ee":"df.shape","8ce806ec":"# Arranging columns by data type for easier manipulation\n\ncontinuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n                   'TenureByAge','CreditScoreGivenAge']\ncategorical_vars = ['HasCrCard', 'IsActiveMember','Geography', 'Gender']\ndf = df[['Exited'] + continuous_vars + categorical_vars]\ndf.head()","9807cd22":"sns.set()\nsns.set(font_scale = 1.25)\nsns.heatmap(df[continuous_vars].corr(), annot = True,fmt = \".1f\")\nplt.show()","758ffb59":"# Changing values of column HasCrCard and IsActiveMember from 0 to -1 so that they will influence negatively to the model instead of no effect.\ndf.loc[df.HasCrCard == 0, 'HasCrCard'] = -1\ndf.loc[df.IsActiveMember == 0, 'IsActiveMember'] = -1\ndf.head()","09955966":"df['Gender'].unique()","5dcdfd61":"df['Geography'].unique()","5a74d11f":"from sklearn.preprocessing import LabelEncoder \n  \nle = LabelEncoder() \n  \ndf['Gender']= le.fit_transform(df['Gender']) \ndf['Geography']= le.fit_transform(df['Geography']) \n\n# Gender 0-Female,1-Male\n# Geography 0-France,1-Germany,2-Spain","b747a35c":"df.head()","83dc61c2":"df1 = pd.get_dummies(data=df, columns=['Gender','Geography'])\ndf1.columns","66b7fc75":"df1.head()","75980fbf":"continuous_vars","7bbc9dba":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf1[continuous_vars] = scaler.fit_transform(df1[continuous_vars])","229f9432":"for col in df1:\n    print(f'{col}: {df1[col].unique()}')","f68d042b":"# Support functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform\n\n# Fit models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# Scoring functions\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","1868fec1":"df1.head()\ndf1.shape","96fcca57":"X = df1.drop('Exited',axis='columns')\ny = df1['Exited']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)","ddd44de4":"X_train.shape","d9aeb6b4":"# We perform training on the Random Forest model and generate the importance of the features\n\nfeatures_label = X_train.columns\nforest = RandomForestClassifier (n_estimators = 1000, random_state = 0, n_jobs = -1)\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","88eff2dd":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X_train.shape[1]), importances[indices], color = \"green\", align = \"center\")\nplt.xticks(range(X_train.shape[1]), features_label, rotation = 90)\nplt.show()","b8a75db6":"# Function to give best model score and parameters\ndef best_model(model):\n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n","292ffaa7":"# Fit primal logistic regression\nlog_primal = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=250, multi_class='auto',n_jobs=None, \n                                penalty='l2', random_state=None, solver='lbfgs',tol=1e-05, verbose=0, warm_start=False)\nlog_primal.fit(X_train,y_train)","c0b9acda":"# Fit logistic regression with pol 2 kernel\npoly2 = PolynomialFeatures(degree=2)\ndf_train_pol2 = poly2.fit_transform(X_train)\nlog_pol2 = LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=300, multi_class='auto', n_jobs=None, \n                              penalty='l2', random_state=None, solver='liblinear',tol=0.0001, verbose=0, warm_start=False)\nlog_pol2.fit(df_train_pol2,y_train)","dec08972":"# Fit SVM with RBF Kernel\nSVM_RBF = SVC(C=150, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf', max_iter=-1, probability=True, \n              random_state=None, shrinking=True,tol=0.001, verbose=False)\nSVM_RBF.fit(X_train,y_train)","863a3b81":"# Fit SVM with Pol Kernel\nSVM_POL = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',  max_iter=-1,\n              probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)\nSVM_POL.fit(X_train,y_train)","dbc33d5f":"# Fit Random Forest classifier\nRF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',max_depth=8, max_features=7, max_leaf_nodes=None,min_impurity_decrease=0.0,\n                            min_impurity_split=None,min_samples_leaf=1, min_samples_split=3,min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n                            oob_score=False, random_state=None, verbose=0,warm_start=False)\nRF.fit(X_train,y_train)","329b2f35":"# Fit Extreme Gradient Boost Classifier\nXGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=5,\n                    min_child_weight=1, missing=None, n_estimators=100,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None,  subsample=1)\nXGB.fit(X_train,y_train)","840c1ca3":"# Normal logistic regression\nprint(classification_report(y_train, log_primal.predict(X_train)))","d63bb953":"# Logistic Regression with degree 2 polynomial kernel\nprint(classification_report(y_train,  log_pol2.predict(df_train_pol2)))","f120a623":"# SVM with RBF kernel\nprint(classification_report(y_train,  SVM_RBF.predict(X_train)))","2a02ad4e":"# SVM with polynomial kernel\nprint(classification_report(y_train,  SVM_POL.predict(X_train)))","340cb951":"# Random Forest Classifier\nprint(classification_report(y_train,  RF.predict(X_train)))","cf1a1d46":"# Xtreme Gradient Boosting\nprint(classification_report(y_train,  XGB.predict(X_train)))","117108d5":"print(classification_report(y_test,  XGB.predict(X_test)))","88d2084e":"X_train.shape","5722e1c3":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten","3039dc95":"# creating the model\nmodel = tf.keras.Sequential()\n\nfrom keras.layers import Dropout\n\n# first hidden layer\nmodel.add(Dense(8,activation = 'relu', input_dim = 16))\nmodel.add(Dropout(0.1))\n\n# second hidden layer\nmodel.add(Dense( 8, activation = 'relu'))\nmodel.add(Dropout(0.1))\n\n# output layer\nmodel.add(Dense( 1,activation = 'sigmoid'))\n\n# Compiling the NN\n# binary_crossentropy loss function used when a binary output is expected\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n\nmodel.fit(X_train, y_train, batch_size = 10, epochs = 50)","2faaf9e3":"# creating the model\nmodel = Sequential()\n\nfrom keras.layers import Dropout\n\n# first hidden layer\nmodel.add(Dense(8,activation = 'relu', input_dim = 16))\n\n# second hidden layer\nmodel.add(Dense( 8, activation = 'relu'))\n\n# output layer\nmodel.add(Dense( 1,activation = 'sigmoid'))\n\n# Compiling the NN\n# binary_crossentropy loss function used when a binary output is expected\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n\nmodel.fit(X_train, y_train, batch_size = 10, epochs = 50)","224f36e6":"y_test.shape","d21fb972":"model.evaluate(X_test, y_test)","e9565270":"# Manually verifying some predictions\nyp = model.predict(X_test)\nyp[:10]","038fa21e":"y_pred = []\nfor element in yp:\n    if element > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\ny_pred[:10]","8cc3336e":"y_test[:10]","87cc4f66":"from sklearn.metrics import confusion_matrix , classification_report\n\nprint(classification_report(y_test,y_pred))","78b725d3":"import seaborn as sn\ncm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\n\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","3e220376":"X.shape","afaeb2a4":"y.shape","76e034dd":"y.value_counts()","4a2d690f":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\ny_sm.value_counts()","f3a504b1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)","0e51f872":"y_train.value_counts()","d89cfd4a":"XGB2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.2, max_delta_step=0,max_depth=7,\n                    min_child_weight=1, missing=None, n_estimators=100,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None,  subsample=1)\nXGB2.fit(X_train,y_train)","8638c571":"a=XGB2\nprint(classification_report(y_train,  a.predict(X_train)))","35314b5e":"print(classification_report(y_test,  a.predict(X_test)))","4e2f3e0a":"XGB2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.2, max_delta_step=0,max_depth=7,\n                    min_child_weight=1, missing=None, n_estimators=100,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None,  subsample=1)","fafb0939":"import joblib \n  \n# Save the model as a pickle in a file \njoblib.dump(XGB2, 'churnXGB.pkl') \n  \n# Load the model from the file \n# XGB_from_joblib = joblib.load('churnXGB.pkl')  \n  \n# Use the loaded model to make predictions \n# XGB_from_joblib.predict(X_test) ","13fabcd1":"### Clearly XG Boost is giving the best training data acuracy of 89% for our dataset.","c716e7bf":"### Confusion Matrix","f3b2986a":"## 6.Handling the problem of Imbalanced dataset","79d6f8a6":"### Training set accuracy is coming to be 97 % which is great in itself.","dbddc129":"### In this project, we will be doing an Exploratory Data Analysis(EDA) and churn prediction through machine learning and deep learning techniques on the bank customers dataset which is taken from Kaggle.","b5c3016d":"### Testing set accuracy is coming to be 91 % which has increased from 86% which we got in from our previous XGB model .","7aa9c8a0":"### From the above boxplots we can infer that-\n\n#### -- There is no significant difference in Credit score,estimated salary and number of products they possess  between customers who churned and who don't.\n#### -- The older customers are churning more than the young ones which indicates that the bank need to focus on older customers more.\n#### -- Customers with tenure period with bank either too less or too more tends to churn more.\n#### -- Customers who churned generally have more bank balance which is a bad indications as it will lead to capital deficiency in the bank.","9b657a6f":"## Introduction.","0956fe13":"### From above pie chart,we can see that around 20% of customers had churned i.e exited and 80% retained.This shows that our dataset is a little imbalanced so we have to predict customer churn with a good accuracy as this 20% customers are of more interest to the bank. ","438c17f5":"### Customer churn refers to the phenomenon when a customer leaves a company or an organization,in our case a bank. Some studies shows that accquiring new coustomers can cost 5 times than that of satisfying and retaining existing customers. Thus tracking of bank customer churn rate through prediction will help in reducing marketing costs, lead to increase in capital ,expanding total customers and a lot more.","9b60eb78":"### Fitting with the XGB model generated using GridSearchCV.","c01b885a":"### For Model fitting, we will try a couple of different machine learning algorithms in order to get an idea about which machine learning algorithm performs better.Since this is a classification problem,we will try the following algorithms :\n### 1. Logistic Regression\n### 2. Logistic Regression with degree 2 polynomial kernel\n### 3.SVM with Rbf kernel and poly kernel\n### 4. Random Forest Classifier\n### 5. Extreme Gradient Boosting Classifier\n","97812336":"### Scaling the continuous attributes using MinMaxScaler","b99f4bac":"## 2. Exploratory Data Analysis\u00b6","f5366d14":"### Final accuracy for the test data is coming to be 86 % which is quite good but as we have seen that our dataset is a little imbalanced thatswhy our accuracy for customers who had exited is coming low.\n","f6019d74":"### Removing the imbalance of our dataset by SMOTE oversampling technique  ","85fd0756":"### One-hot encoding categorical columns","8d44459c":"### Classification Report for this model is almost same as that of Random Forest Model.","f77d7f51":"## 3. Feature Engineering","17b8218b":"## Using Artificial Neural Network technique","c5883f0f":"### We would like to add features that are likely to have an impact on the probability of churning.","6be7b9fd":"### Fitted different models to GridSearchCV to find out the best parameters.","1ece8241":"## 4. Data Preparation for the Model fitting","069c9646":"### Reviewing best model fit accuracy. Our keen interest is on the performance in predicting 1's (Customers who churn)","d6883715":"### Fitting model with 2 hidden layers along without appling dropout regularization.Final accuracy for training data is coming to be 86.21 %,better than the previous one.","c36a0187":"### Figuring out the importance of features in our dataset","2577f22b":"## 7.Conclusion","67f89d33":"### Fitting model with 2 hidden layers along with appling dropout regularization.Final accuracy for training data is coming to be 85.28 %","adc7d720":"## 5. Model fitting and selection\n","3bd52493":"# BANK CUSTOMER CHURN PREDICTION\n","ce40fe56":"### We can see that by balancing the dataset has increased our overall testing data accuracy to 91% , also it has invidually increased the accuracy for the customers who had churned (57% previously to 91% now) from the bank which matters to us more than the customers who retained.","b9d8f52c":"### Fitting our training dataset with the model with best parameters got from GridSearchCV for each of the machine learning techniques.","70770b41":"### Now visualizing countplots for categorical columns.","42b5cffa":"## 1. Load and Manipulate Data","a9693c33":"### We can see from the correlation matrix that only the columns which we have created have some significant correlation with columns they are made from.","2e7888b4":"<img align=\"left\" width=\"500\" height=\"400\" src=\"https:\/\/drive.google.com\/uc?export=view&id=1cndfDAb6JDdtMtxxSl6bIyZfDztJeTkS\">","88871f15":"## We will also use deep learning  after these techniques.","7893b336":"### Clearly we can see that customers with high BalanceSalaryRatio is churning more,which balance or salary feature didn't showed up.","39eaa4da":"## Overview of Notebook\n\n### 1. Load and Manipulate Data\n### 2. Exploratory Data Analysis\u00b6\n### 3. Feature Engineering for the baseline model\n### 4. Data Preparation for the Model fitting\n### 5. Model fitting and selection\n### 6.Handling the problem of Imbalanced dataset\n### 7. Conclusion.\n\n### Check my github repo for more info- https:\/\/github.com\/tanish265\/Bank-Customers-Churn-Prediction","0bd7d034":"### Now we have equal number of churned and retaining customers.","a38df9ed":"### Checking accuracy for test data with XG Boost Model","683a427b":"### From the above countplots we can infer that-\n\n#### 1.Total umber of customers who retained is highest from France and those who exited are highest from Germany,which means the bank needs to focus more on customers from Germany followed by France so that they don't churn.\n#### 2. The proportion of female customers churning is greater than that of male customers.\n#### 3. Suprisingly,coustomers who had credit card churned more which can be a coincidence.\n#### 4. As usual,the inactive members churned more. ","14a471f9":"#### Correlation Matrix for continuous attributes","b7c26e55":"### Evaluating test data with this model and accuracy is coming to be 85.85 % which is almost similar to our Random Forest Model."}}