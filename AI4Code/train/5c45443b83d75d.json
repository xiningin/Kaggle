{"cell_type":{"00539be7":"code","823325fc":"code","0a7f73ee":"code","2507deaf":"code","972e81e0":"code","2702cbb9":"code","dee63c9f":"code","d372efa3":"code","ea63c5af":"code","b4e1cbba":"code","2cf89e59":"code","d8e9accc":"code","ceb405c6":"code","31f11d36":"code","e2f62e06":"code","a79b2793":"code","3460f715":"code","c0ecab4c":"code","f42c389c":"code","148f44e5":"code","fb271a8b":"code","98a38b0c":"code","023cdab9":"code","d99a9ef3":"code","844eaa61":"markdown","4e97161b":"markdown","a8d12fdc":"markdown","2f3bbc17":"markdown","ef82c233":"markdown","bb0101e4":"markdown","5eefa78f":"markdown","71516aca":"markdown","9a79333c":"markdown","dd1b386d":"markdown"},"source":{"00539be7":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import is_numeric_dtype, is_string_dtype","823325fc":"raw_adult_train = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")\n\nraw_adult_train.describe()","0a7f73ee":"print('Dataset_shape:', raw_adult_train.shape)","2507deaf":"raw_adult_train.isna().any()  #Checks if any variable has NaN values","972e81e0":"#Plots the percentage of NaN values for each class\nmissing_count = raw_adult_train.isna().sum()\nvalue_count = raw_adult_train.isna().count()\npercentage = round((missing_count\/value_count)*100,2)\nmissing_df = pd.DataFrame({'count':missing_count, 'percentage':percentage})\nsns.set_theme(style='darkgrid')\n\nbarplot = missing_df.plot.bar(y='percentage')\n","2702cbb9":"adult_train = raw_adult_train.fillna(value = 'Unknown',axis='index') #Delete NaN values rows\ndel raw_adult_train     #Delete raw data to save memory\nadult_train.shape","dee63c9f":"adult_train.isna().any()     #Checks if the dataset has NaN values","d372efa3":"adult_train.head() ","ea63c5af":"raw_adult_test = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\",\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")\n\nadult_test = raw_adult_test.fillna(value = 'Unknown',axis='index') #Delete NaN values rows\nadult_test.describe()\n","b4e1cbba":"#Data visualization of the training dataset\ndef eda(dataset):\n    eda_data = dataset.drop( ['fnlwgt','Id'], axis = 'columns')    #prepares exploratory data analysis dataset\n    sns.set_theme(style='darkgrid')\n    for column in eda_data:\n        plt.figure(column)\n        plt.title(column)\n        if is_numeric_dtype(eda_data[column]):\n            eda_data[column].plot(kind= 'hist')\n        else:\n            eda_data[column].value_counts().plot(kind='bar')\n\neda(adult_train)","2cf89e59":"#Data visualization of the testing dataset\neda(adult_test)","d8e9accc":"import sklearn\nfrom sklearn.preprocessing import LabelEncoder\n\n\nadult_train = adult_train.apply(LabelEncoder().fit_transform) #categoric features encoding\nadult_test = adult_test.apply(LabelEncoder().fit_transform) #categoric features encoding\n# education.num is the numeric representant of education -> repeated information\nX_train = adult_train.drop(['income','fnlwgt','Id','education'], axis = 'columns')\nX_test = adult_test.drop(['fnlwgt','Id','education'], axis = 'columns')\nY_train = adult_train['income']\n\nX_train.head()\n","ceb405c6":"X_test.isna().any() ","31f11d36":"X_train.isna().any()     #Checks if the dataset has NaN values","e2f62e06":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nclf = KNeighborsClassifier()\n\nk = {'n_neighbors':[*range(3, 35)]} #parameters value interval\n\ngridsearch = GridSearchCV(clf, k, cv=10, n_jobs=4) #automatic parameter search\n\ngridsearch.fit(X_train, Y_train) #model fit\n","a79b2793":"gridsearch.best_params_ #gives the best estimator of all trials","3460f715":"gridsearch.best_score_ #gives the score of the best estimator","c0ecab4c":"gridsearch.cv_results_ ","f42c389c":"#Parameter value x CV_score curve\n\nfrom sklearn.model_selection import validation_curve\n\nparam_range = np.arange(3,36)\n\ntrain_scores, test_scores = validation_curve(\n    clf, X_train, Y_train, param_name=\"n_neighbors\", param_range=param_range,\n    scoring=\"accuracy\", n_jobs=4)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title(\"Validation Curve with k-NN\")\nplt.xlabel(r\"$\\k$\")\nplt.ylabel(\"Score\")\nlw = 2\nplt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\nplt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\nplt.show()","148f44e5":"#Plots the model learning curve\n\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\nknn = KNeighborsClassifier(n_neighbors = gridsearch.best_params_['n_neighbors']) #classifier with selected value of k\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n   \n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\nplot_learning_curve(knn, \"Learning Curves: 24-NN\", X_train, Y_train, cv = 10, n_jobs = 4)\n","fb271a8b":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nsns.set_theme(style = 'dark')\n\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(X_train, Y_train, test_size=0.25, random_state=7) #splitting the dataset\n\nknn.fit(X2_train,y2_train)\ny_pred = knn.predict(X2_test) #predicted values of 25% of the test dataset not used in training\ntarget_names = ['<=50K', '>50K']\nprint(\"--------------- Classification Report K = 24 ---------------\")\n\nprint(classification_report(y2_test, y_pred, target_names=target_names))\n\nprint(\"--------------- Confusion Matrix K = 24 ---------------\")\n\n#confmatx = confusion_matrix(y2_test, y_pred)\n#print(confmatx)\n#sns.heatmap(confmatx)\ncm = confusion_matrix(y2_test, y_pred, normalize = 'true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\ndisp.plot()\nplt.show()","98a38b0c":"final_classifier = KNeighborsClassifier(n_neighbors = gridsearch.best_params_['n_neighbors']) #classifier with selected value of k\nfinal_classifier.fit(X_train, Y_train)\nY_test = final_classifier.predict(X_test)\npredict = {'income': Y_test}\nexport = pd.DataFrame(predict)\nexport['income'] = export['income'].apply(lambda x: '<=50K' if x == 0 else '>50K')\nexport.head()","023cdab9":"export.shape","d99a9ef3":"export.to_csv(\"submission.csv\", index=True, index_label='Id')","844eaa61":"## Data Prep","4e97161b":"Importing test dataset","a8d12fdc":"Another analysis that can be made on the selected model is the computation of the classification report and the confusion matrix. To perform this analysis the training dataset is divided in 75% train and 25% test.","2f3bbc17":"# K-NN Classification for Adult dataset \n\n## PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n\n## PMR3508-2021-65","ef82c233":"## Test dataset prediction","bb0101e4":"## Data import","5eefa78f":"## Model parameter selection\n\nTo select the parameter k of k-NN classifier the cross_validation metric was used. The search for the best parameter was automatized by GridSearch function.\n","71516aca":"## Exploratory Data Analysis","9a79333c":"## Feature Engineering\n\nUnnecessary features removal and categoric data codification","dd1b386d":"Dataset data presents less than 10% of unknown values in each class, therefore the removal of unknown samples has low impact in its size.\n"}}