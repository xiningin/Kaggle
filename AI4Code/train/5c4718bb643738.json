{"cell_type":{"6de4bfe0":"code","850dfad7":"code","d7f2c186":"code","9f753600":"code","204b19af":"code","28019de8":"code","2f80eb9f":"code","8cd886b7":"code","1258eda3":"code","24aa7406":"code","b22029de":"code","8b43f2b0":"code","5736853c":"code","a4d39511":"code","92d53355":"code","f9851288":"code","4d071453":"code","e5764d89":"code","4618cd8f":"code","cb8aec7a":"code","bacf2f2b":"code","7b5748a7":"code","f6915ffb":"code","36396637":"code","b1111bdc":"code","a727dbf4":"code","38fb0c23":"markdown","2d5b2d4a":"markdown","45e53adc":"markdown","fec8de26":"markdown","366902bb":"markdown","7b0f7b38":"markdown","f69a6ea1":"markdown","493ea802":"markdown"},"source":{"6de4bfe0":"pip install transformers -U","850dfad7":"pip install torch -U","d7f2c186":"pip install --upgrade --force-reinstall tensorflow -U","9f753600":"pip install --upgrade --force-reinstall keras==2.6.0 -U","204b19af":"import gc\nimport json\nimport torch\nimport itertools\nimport time\nimport datetime\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport numpy.ma as ma\n\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler, random_split\n\nimport importlib, pkg_resources\nimportlib.reload(pkg_resources)\n\n#from transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import AdamW \nfrom transformers import get_linear_schedule_with_warmup\n\nfrom sklearn.model_selection import train_test_split\n#from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","28019de8":"MAX_LENGTH = 512\nBATCH_SIZE = 32     # Smaller batch sizes are generally recommended for fine-tuning BERT \nEPOCHS = 4          # recommendations for BERT models are between 2-4\nLR1 = 2e-6\nLR2 = 1e-4\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\", index_col='id')\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\", index_col='id')\ntrain_df.head(5)\ntest_df.head(5)\n\ntarget = train_df.target.values\ntrain_data = train_df['text']\ntest_data = test_df['text']","2f80eb9f":"print(target.shape)\nprint(train_data.shape)\ntrain_data.head()","8cd886b7":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n                                                           num_labels=2,\n                                                           max_position_embeddings=512,\n                                                           output_attentions=False,\n                                                           output_hidden_states=False)","1258eda3":"def preProcess(in_data):\n    # Tokenize all of excerpts and map their tokens to their word IDs\n    input_ids = []\n    attention_masks = []\n\n    for tweet in in_data:\n        # `encode_plus` will:\n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        encoded_dict = tokenizer.encode_plus(\n                            tweet,                       # tweet to encode\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            truncation = True,\n                            padding = 'max_length',\n                            max_length = MAX_LENGTH,          # Pad & truncate all sentences        \n                            #pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks\n                            return_tensors = 'pt',     # Return pytorch tensors\n                       )\n\n        # Add the encoded sentence to the list.    \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(target).float()\n    \n    return input_ids, attention_masks, labels","24aa7406":"def preProcessTestData(in_data):\n    # Tokenize all of excerpts and map their tokens to their word IDs\n    input_ids = []\n    attention_masks = []\n\n    for tweet in in_data:\n        # `encode_plus` will:\n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        encoded_dict = tokenizer.encode_plus(\n                            tweet,                       # tweet to encode\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            truncation = True,\n                            padding = 'max_length',\n                            max_length = MAX_LENGTH,          # Pad & truncate all sentences        \n                            #pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks\n                            return_tensors = 'pt',     # Return pytorch tensors\n                       )\n\n        # Add the encoded sentence to the list.    \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    return input_ids, attention_masks","b22029de":"# Combine the training inputs into a TensorDataset.\nprocessed_data = preProcess(train_data)\ndataset = TensorDataset(processed_data[0], processed_data[1], processed_data[2]) #.long())\n\n# Create a 85-15 train-validation split and calc sizes of each.\ntrain_size = int(0.85 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","8b43f2b0":"# Combine the testing inputs into a TensorDataset.\nprocessed_data = preProcessTestData(test_data)\ntest_dataset = TensorDataset(processed_data[0], processed_data[1]) #.long())\n\nprint('{:>5,} testing samples'.format(len(test_dataset)))","5736853c":"# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = BATCH_SIZE # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = BATCH_SIZE # Evaluate with this batch size.\n        )","a4d39511":"# For testing the order doesn't matter, so we'll just read them sequentially.\ntest_dataloader = DataLoader(\n            test_dataset, # The validation samples.\n            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n            batch_size = BATCH_SIZE # Evaluate with this batch size.\n        )","92d53355":"model.cuda()\n\n# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The DistilBERT model number of layers: {}.\\n'.format(len(params)))\n\nfor i, p in enumerate(params):\n    print(\"layer {:>3}: {:<55} {:>12}\".format(i, p[0], str(tuple(p[1].size()))))","f9851288":"# Create our own optimizer that sets a different (much lower) learning rate for the layers \n# that are already pre-trained, and then a larger learning rate for the two final linear\n# layers that have not been trained at all (but are instead initialized to random values).\ndef create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    bert_parameters = named_parameters[:100]\n    classifier_parameters = named_parameters[100:]\n        \n    bert_group = [params for (name, params) in bert_parameters]\n    classifier_group = [params for (name, params) in classifier_parameters]\n\n    parameters = []\n\n    #for layer_num, (name, params) in enumerate(bert_parameters):\n    for name, params in bert_parameters:        \n        lr = LR1\n        parameters.append({\"params\": params,\n                           \"lr\": lr})\n\n    #for layer_num, (name, params) in enumerate(regressor_parameters):\n    for name, params in classifier_parameters:\n        lr = LR2 \n        parameters.append({\"params\": params,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","4d071453":"criterion = nn.MSELoss()\noptimizer = create_optimizer(model)\n#optimizer = AdamW(model.parameters(),\n#                  lr = 1e-5, # args.learning_rate - default is 5e-5\n#                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n#                )","e5764d89":"# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * EPOCHS\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","4618cd8f":"def format_time(elapsed):\n    ''' Convert time in seconds and returns a string hh:mm:ss '''\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","cb8aec7a":"# Set the seed value all over the place to make this reproducible.\nseed_val = 1\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntorch.set_default_dtype(torch.float64)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, EPOCHS):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, EPOCHS))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    y_train = {'actual':[], 'predicted':[]}\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 25 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks, not needed for DistilBERT\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_labels = batch[2].long().to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is sometimes desired \n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).        \n        outputs = model(b_input_ids, \n                        labels=b_labels)\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += outputs[0].item()\n\n        # Perform a backward pass to calculate the gradients.\n        #loss = criterion(outputs[1].flatten(), b_labels.float())#.sqrt()\n        loss = criterion(outputs[1].softmax(dim=-1)[:,1], b_labels.float())#.sqrt()\n        \n        # backpropagation\n        loss.backward()\n        \n        # for plotting results later on\n        y_train['actual'] += b_labels.float().cpu().numpy().flatten().tolist()\n        y_train['predicted'] += outputs[1].softmax(dim=-1)[:,1].detach().cpu().numpy().flatten().tolist()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n        \n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss \/ len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put model in evaluation mode (don't calculate gradients, no dropout, etc.)\n    model.eval()\n\n    # Tracking variables \n    total_eval_loss = 0\n\n    # Evaluate data for one epoch\n    y_val = {'actual':[], 'predicted':[]}\n    for step, batch in enumerate(validation_dataloader):\n                \n        # Progress update every 40 batches.\n        if step % 5 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(validation_dataloader), elapsed))\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks, not needed for DistilBERT\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_labels = batch[2].long().to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate predictions\n            outputs = model(b_input_ids, \n                            labels=b_labels)\n\n        # Accumulate the validation loss.\n        loss = outputs[0]\n        total_eval_loss += loss.item()\n        \n        # Move labels\/targets and predictions to CPU\n        preds = outputs[1].softmax(dim=-1)[:,1].detach().cpu().numpy()\n        targets = b_labels.to('cpu').numpy()\n        \n        # for plotting results later on\n        y_val['actual'] += targets.flatten().tolist()\n        y_val['predicted'] += preds.flatten().tolist()\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","bacf2f2b":"train_acc = accuracy_score(y_train['actual'], [1 if y > 0.5 else 0 for y in y_train['predicted']])\nvalid_acc = accuracy_score(y_val['actual'], [1 if y > 0.5 else 0 for y in y_val['predicted']])\nprint(f\"DistilBERT model training MSE = {train_acc:.6f}\")\nprint(f\"DistilBERT model validation MSE = {valid_acc:.6f}\")","7b5748a7":"training_losses = [epoch_stats['Training Loss'] for epoch_stats in training_stats]\nvalidation_losses = [epoch_stats['Valid. Loss'] for epoch_stats in training_stats]\nplt.plot(range(1,len(training_losses)+1), training_losses, c='r')\nplt.plot(range(1,len(validation_losses)+1), validation_losses, c='b')\nplt.xticks(range(1, len(training_losses)+1))\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()","f6915ffb":"# Evaluate test data for one submission\npredictions = []\nfor step, batch in enumerate(test_dataloader):\n               \n    # Progress update every 40 batches.\n    if step % 5 == 0 and not step == 0:\n        # Calculate elapsed time in minutes.\n        elapsed = format_time(time.time() - t0)\n    \n        # Report progress.\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n\n    # Unpack this training batch from our dataloader. \n    #\n    # As we unpack the batch, we'll also copy each tensor to the GPU using \n    # the `to` method.\n    #\n    # `batch` contains three pytorch tensors:\n    #   [0]: input ids \n    #   [1]: attention masks, not needed for DistilBERT\n    test_input_ids = batch[0].to(device)\n    test_labels = torch.randint(0, 2, (len(test_input_ids),)).long().to(device)\n\n    # Tell pytorch not to bother with constructing the compute graph during\n    # the forward pass, since this is only needed for backprop (training).\n    with torch.no_grad():        \n\n        # Forward pass, calculate predictions\n        outputs = model(test_input_ids, labels=test_labels)\n\n    # Move predictions to CPU\n    preds = outputs[1].softmax(dim=-1)[:,1]\n    # preds = preds.detach().cpu().numpy()\n    preds = preds.to('cpu').numpy()\n\n    # for outputting results later on\n    predictions += preds.flatten().tolist()","36396637":"simple_predictions = [1 if y > 0.5 else 0 for y in predictions]\nprint(len(simple_predictions))","b1111bdc":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nprint(len(sample_submission[\"target\"]))","a727dbf4":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = simple_predictions\nsample_submission.to_csv(\"submission.csv\", index=False)","38fb0c23":"### Setup the Model","2d5b2d4a":"### Generate Output for Submission","45e53adc":"### Set-up Data Loaders for Model","fec8de26":"## NLP - Processing Disaster Tweets Deep Learning Model\nAuthor: Matthew Williams\n\nThis noteboook shows the model I am using to classify the tweets in the dataset provided by the Natural Language Processing with Disaster Tweets competion. This notebook started as a copy of a [Getting Started Tutorial](https:\/\/www.kaggle.com\/philculliton\/nlp-getting-started-tutorial) provided by [phil culliton](http:\/\/https:\/\/www.kaggle.com\/philculliton).","366902bb":"### Analyze results of Training","7b0f7b38":"### A quick look at the data\n\nGoing over the competition page's data section they describe the contents of the data as a csv with the following columns:\n\n    id - a unique identifier for each tweet\n    text - the text of the tweet\n    location - the location the tweet was sent from (may be blank)\n    keyword - a particular keyword from the tweet (may be blank)\n    target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\nThe total number of datapoints the dataset contains is 7613. Further data analyis can be found [here]().","f69a6ea1":"### Train the Model","493ea802":"### Data Pre-Processing"}}