{"cell_type":{"15c26336":"code","dcbc3083":"code","c2fe3917":"code","e5a181dd":"code","b93b51d5":"code","1f6152db":"code","f99da48a":"code","8a000fe8":"code","96b913ae":"code","adcbe6f0":"code","e2093c71":"code","c9e2a604":"code","7b95ec80":"code","51078e5e":"code","9a9c6e1a":"code","19e7450d":"code","c177b2e3":"code","285b8518":"code","43f7b94b":"code","63e3d972":"code","64968fd3":"code","675559d4":"code","589f50ed":"code","285a7ded":"code","59b91109":"code","236783aa":"code","6a20a2ed":"code","051e0c62":"code","93064522":"code","bdb9df75":"markdown","8b4ee831":"markdown","39caed31":"markdown","12fe2da5":"markdown","e51fb619":"markdown","632ecf00":"markdown","fecf0326":"markdown","c4cd18ed":"markdown","e7813996":"markdown","71ea75ba":"markdown","d1f0a9d3":"markdown","970c14e5":"markdown","d9052a7b":"markdown","11861890":"markdown","fc1bcc08":"markdown","76e2ce38":"markdown","aca322dc":"markdown","611a18e0":"markdown"},"source":{"15c26336":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","dcbc3083":"df= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ndf.head()","c2fe3917":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig,ax=plt.subplots(1,2,figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.hist(df[(df.Sex=='male') & (df.Survived==1)]['Age'], \n         bins=50, label='Survived', alpha=0.5, color='orange')\nplt.hist(df[(df.Sex=='male') & (df.Survived==0)]['Age'], \n         bins=50, label='Not Survived', alpha=0.5, color='blue')\nplt.grid()\nplt.title( label='Age Male Survived')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.legend()\n\n\nplt.subplot(1,2,2)\nplt.hist(df[(df.Sex=='female') & (df.Survived==1)]['Age'], \n         bins=50, label='Survived', alpha=0.5, color='orange')\nplt.hist(df[(df.Sex=='female') & (df.Survived==0)]['Age'], \n         bins=50, label='Not Survived', alpha=0.5, color='blue')\nplt.grid()\nplt.title( label='Age Female Survived')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.legend()\n","e5a181dd":"plt.scatter( df[df.Survived==1]['Fare'], df[df.Survived==1]['Age'], color='orange', alpha=0.5, label='Survived')\nplt.scatter( df[df.Survived==0]['Fare'], df[df.Survived==0]['Age'], color='blue', alpha=0.5, label='Not survived')\nplt.xlabel('Fare')\nplt.ylabel('Age')\nplt.grid()\nplt.title('Fare Vs Age')\nplt.legend()","b93b51d5":"def boxplot_plots (df, target_column, feature):\n    ax = sns.boxplot(x=df[target_column], y=df[feature])\n    plt.ylabel(feature+' value')\n    plt.tight_layout()\n    plt.grid()","1f6152db":"plt.subplots(1,2,figsize=(10,6))\n\nplt.subplot(1,2,1)\nboxplot_plots (df, 'Pclass', 'Fare')\nplt.title('Class and Fare')\nplt.legend()\n\nplt.subplot(1,2,2)\nboxplot_plots (df, 'Pclass', 'Age')\nplt.title('Class and Age')\nplt.legend()","f99da48a":"plt.style.use(\"fivethirtyeight\")\nplt.pie(df['Survived'].value_counts(), labels=['Not Survived', 'Survived'],autopct=\"%1.1f%%\", colors=['blue','orange'])\nplt.show()\nplt.style.use('default')","8a000fe8":"print('Percentage of female survived: '+str(round(len(df[(df.Survived==1) &(df.Sex=='female')])\n                                                  \/len(df[df.Sex=='female']),2))+'%')\nprint('Percentage of male survived: '+str(round(len(df[(df.Survived==1) &(df.Sex=='male')])\n                                                  \/len(df[df.Sex=='male']),2))+'%')\nprint('Percentage of first, second and third class survived: '+str(round(len(df[(df.Survived==0) &(df.Pclass==1)])\n                                                  \/len(df),2))+'%,'+str(round(len(df[(df.Survived==0) &(df.Pclass==2)])\n                                                  \/len(df),2))+'%,'+str(round(len(df[(df.Survived==0) &(df.Pclass==3)])\n                                                  \/len(df),2))+'%')\nprint('Percentage of male survived: '+str(round(len(df[(df.Survived==1) &(df.Sex=='male')])\n                                                  \/len(df[df.Sex=='male']),2))+'%')","96b913ae":"df.groupby('Embarked').count()","adcbe6f0":"df.describe()","e2093c71":"# Number of Nan values \ndf.isna().sum()","c9e2a604":"plt.subplots(1,2,figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.bar(['C','Q','S'], [sum(df.Embarked=='C'),sum(df.Embarked=='Q'),sum(df.Embarked=='S')])\nplt.grid()\nplt.title('Embarked division')\nplt.xlabel('Embarked')\nplt.ylabel('Count')\n\nplt.subplot(1,2,2)\nplt.bar([1,2,3], [sum(df.Pclass==1),sum(df.Pclass==2),sum(df.Pclass==3)], color='orange')\nplt.grid()\nplt.title('Class division')\nplt.xlabel('Class')\nplt.ylabel('Count')","7b95ec80":"import plotly.express as px\nfig = px.imshow(df.corr())\nfig.show()","51078e5e":"df_drop=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_drop.drop(['Cabin','Name','Ticket','PassengerId'],axis=1, inplace=True)\n\n\ndf_drop['Sex'].replace({'male':0, 'female':1}, inplace=True)\nprint('Initially we have '+str(len(df_drop))+' rows. But some of them show missing values (Ages)')\n\ndf_drop=df_drop[df_drop['Age'].isnull()==False]\ndf_drop=df_drop[df_drop['Embarked'].isnull()==False]\n\nprint('I decide to delete this rows, they are '+str(len(df[df['Age'].isnull()==True]))+' values. We will create another set replacing these missing values with the mean of the column.')\n\ndf.head()\nprint('We have '+str(len(df_drop))+' rows. Without missing values')\n\ncols_with_missing = [col for col in df_drop.columns if df_drop[col].isnull().any()]\nprint(cols_with_missing)","9a9c6e1a":"from sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\n\ndf=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nEmbarked=df.Embarked\nEmbarked_test=df_test.Embarked\n\nSurvived=df.Survived\n\ndf.drop(['Cabin','Name','Ticket','PassengerId','Embarked','Age','SibSp','Parch','Survived'],axis=1, inplace=True)\ndf_test.drop(['Cabin','Name','Ticket','PassengerId','Embarked','Age','SibSp','Parch'],axis=1, inplace=True)\n\n# Replace the string with numbers\ndf['Sex'].replace({'male':0, 'female':1}, inplace=True)\ndf_test['Sex'].replace({'male':0, 'female':1}, inplace=True)\n\n\nmy_imputer= SimpleImputer(missing_values=np.nan, strategy='median')\nappo=df\ndf=pd.DataFrame(my_imputer.fit_transform(df))\ndf_test=pd.DataFrame(my_imputer.transform(df_test))\n# The imputer removes the names of the columns\ndf.columns=appo.columns\ndf_test.columns=df.loc[:, df.columns !='Survived'].columns\n\ncat_imputer=SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nEmbarked=pd.DataFrame(cat_imputer.fit_transform(np.array(Embarked).reshape(-1, 1)))\nEmbarked_test=pd.DataFrame(cat_imputer.transform(np.array(Embarked_test).reshape(-1, 1)))\n\ndf['Survived']=Survived","19e7450d":"OrdinalEnc = preprocessing.OrdinalEncoder()\ndf['Embarked']=OrdinalEnc.fit_transform(np.array(Embarked).reshape(-1, 1))\ndf_test['Embarked']=OrdinalEnc.transform(np.array(Embarked_test).reshape(-1, 1))\ndf.groupby('Embarked').count()","c177b2e3":"df.isna().any()","285b8518":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import mean_absolute_error","43f7b94b":"X=df.drop('Survived',axis=1)\ny=df.Survived\n\nX_train, X_val, y_train, y_val= train_test_split(X, y, test_size=0.2,random_state=1)","63e3d972":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier","64968fd3":"RandomForest=RandomForestClassifier()\n\nRandomForest.fit(X_train,y_train)\npred=RandomForest.predict(X_val)\n\nlog_acc=accuracy_score(pred,y_val)\n\nprint(log_acc)","675559d4":"estimators=[1,2,3,4,5,6,7,8,9,10,12,15,20]\ndepths=[2,3,4,5,6,7,8,10,20]\ntrain_scores=[]\nvalid_scores=[]\nfor n_estimators in estimators:\n    model=RandomForestClassifier(n_estimators=n_estimators)\n    model.fit(X_train,y_train)\n    train_scores.append(accuracy_score(model.predict(X_train), y_train))\n    valid_scores.append(accuracy_score(model.predict(X_val), y_val))\n\ntrain_scores_depth=[]\nvalid_scores_depth=[]\nfor max_depth in depths:\n    model=RandomForestClassifier(max_depth=max_depth)\n    model.fit(X_train,y_train)\n    train_scores_depth.append(accuracy_score(model.predict(X_train), y_train))\n    valid_scores_depth.append(accuracy_score(model.predict(X_val), y_val))\n\n","589f50ed":"plt.subplots(1,2,figsize=(10,6))\n\nplt.subplot(1,2,1)\nplt.plot(estimators, train_scores, label='Train accuracy', color='blue')\nplt.plot(estimators, valid_scores, label='Validation accuracy', color='orange')\nplt.grid()\nplt.title('Number estimators')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(depths, train_scores_depth, label='Train accuracy', color='blue')\nplt.plot(depths, valid_scores_depth, label='Validation accuracy', color='orange')\nplt.grid()\nplt.title('Maximum depth')\nplt.legend()","285a7ded":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'criterion':('gini','entropy'), \n              'max_depth':[2,3,4,5,10,50],\n              'n_estimators':[5,10,20,50,100,200,300,500,1000]}\n\nclf = GridSearchCV(RandomForest, parameters)\n\nclf.fit(X_train, y_train)","59b91109":"best_RandomForest=clf.best_estimator_\n#best_RandomForest=RandomForestClassifier(criterion='entropy',max_depth=10, n_estimators=1000)\n#best_RandomForest.fit(X_train,y_train)\n\nlog_acc=accuracy_score(best_RandomForest.predict(X_val),y_val)\n\nprint(log_acc)","236783aa":"from sklearn.metrics import precision_recall_curve\n\nprobas=[]\nfor row in RandomForest.predict_proba(X_val):\n    probas.append(row[1])\n\nprecision, recall, thresholds = precision_recall_curve(y_val, probas)\n\nplt.plot(recall,precision)\nplt.title('Precision Recall Curve')\nplt.grid()\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\n","6a20a2ed":"from sklearn.ensemble import GradientBoostingClassifier\n\nGBC=GradientBoostingClassifier(n_estimators=500, learning_rate=0.1,max_depth=10, random_state=0)\nGBC.fit(X_train,y_train)\n\n\nprint(accuracy_score(GBC.predict(X_val),y_val))","051e0c62":"from sklearn.neighbors import KNeighborsClassifier\nKNN=KNeighborsClassifier(n_neighbors=9)\nKNN.fit(X_train,y_train)\n\nprint(accuracy_score(KNN.predict(X_val),y_val))\n\n","93064522":"test_prediction = RandomForest.predict(df_test)\n\nSubmission=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\nsubmission=pd.DataFrame({\"PassengerId\": Submission[\"PassengerId\"],\"Survived\":test_prediction})\n\nsubmission.to_csv('submission.csv',index=False)","bdb9df75":"Select the target variable used for the classification and split the dataset into the training and the validation one.","8b4ee831":"## Submission","39caed31":"Another way to evaluate the accuracy of your model is to predict not the target values but the probabilities, this will give you the opportunity to plot the precision-recall curves like I did in the following cell.","12fe2da5":"## Model Selection \n<a id=\"ModelSelection\"><\/a>\n\nDefine the method you want to use and fit it using the training sample. \nThen predict the results and calculate the performance using the validation one. ","e51fb619":"# Introduction\n\nLoad the datasets available on kaggle, the training one and the test one. The first will be used for the trainig part and the validation, while the second for the submission parte. Indeed in the test dataset we don't have the labels of the target variable.\n","632ecf00":"In the next cell I use the drop-missing-values approch. I simply drop the data (rows) where we have some missing values. This is a technique that can be used for example if the dataset is very big and the missing vales don't afflict the dataset. This is not the case. ","fecf0326":"# Machine Learning\n<a id=\"Machinelearning\"><\/a>\n\nIn this last section I will explore some sumply machine learning classification methods from sklearn. We will divide the dataset into the trainig anf the validation set and will calculate the performance over this two sets. I also implemented some cross validation operations like a grid search over the parameters of a random forest classifier. ","c4cd18ed":"## Model Evaluation\n<a id=\"ModelEvaluation\"><\/a>\n\nTo find the best parameters for the model you can use a grid search. Set the values of the parameters and the method 'GridSearchCV' will train alla the models with these parameters. Then you can extract the best one and use it for further analysis. ","e7813996":"## Statistics \n<a id=\"Statistics\"><\/a>\n\nNow we can explore the dataset in more detail, let's see some statistics to have a better understanding about the problem.","71ea75ba":"Thank you for the attention, upvote if you liked the notebook and have a nice day!","d1f0a9d3":"# Titanic Challange \ud83d\udef3\ufe0f\n<a id=\"Title\"><\/a>\n\nThis is the first challange that every new Kaggler should make.\nIt is a very easy and fast way to approch some simple machine learning techniques about binary classification.\n\nThe goal is to predict if a particular passenger in the Titanic survived after the incident. There are just a few features and some of them are not very useful, as we will see later. \n\nSo, fix the char, sit well and start with your classification!\n\nIn this notebook we will cover the main steps in approching a classification problem. \n\n[Titanic Challange](#Title)\n\n1. [Exploratory Data Analysis](#EDA)\n - [Data Visualizations](#DV)\n - [Statistics](#Statistics)\n \n2. [Preprocessing](#Preprocessing)\n - [Imputation](#Imputation)\n - [Encoding](#Enconding)\n - [Data Selection](#DataSelection)\n \n3. [Machine learning](#Machinelearning)\n - [Model selection](#Modelselection)\n - [Model evaluation](#Modelvaluation)\n \n","970c14e5":"You can use any model you want, sklearn is plenty of them so just relax and enjoy some classifications!","d9052a7b":"## Encoding\n<a id=\"Encoding\"><\/a>\n\nEncoding is a fundamental part of your work when you have to deal with strings or not-numerical data. There are different ways to encode for example strings (like in this case), now I will use the simpler one. Assign a number to a particular label. You will use diffrent encoding methods in dependence on the kind of data you will use, like text or images.\n","11861890":"I report a simpy cross validation over some parameters of the random forest classifier. It is possibile to see an high regime of overfitting for the model. Maybe because the number of data available are not very big. ","fc1bcc08":"## Data Visualization\n<a id=\"DV\"><\/a>\n\nLet's see some plots! These are just some ideas trying to cover different aspects, \nunleash your creativity!","76e2ce38":"# EDA\n<a id=\"EDA\"><\/a>\n\nIn this section I will try to give a general introduction about the dataset. The Exploratory Data Analysis or EDA is a foundamental part of every data science project. It is used to look in front of the data and understand some patterns and characteristic.\nIt can be divided into two part, the first is the visualization of the dataset through different plots for examples histograms, barplot or scatterplot. The second it concerns some statistics about the dataset for example the distribution of the features of the presence of missing values. ","aca322dc":"# Preprocessing\n<a id=\"Preprocessing\"><\/a>\n\nData preprocessing can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. Data preprocessing can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process.","611a18e0":"## Imputation\n<a id=\"Imputation\"><\/a>\n\nAnother approch to handle missing values is to replace these values with others. For example using the mean of the columns or the most frequent value. In this case I used the median over the values of the column.\n\nThere are some missing values both for the Age and the Embarked features. The first is numerical while the second is categorial, so I used the mostr frequent approch. The following cell shows the procedure.\n\nI tried different approches and I notice how some features like the Age, the Parch and the SinSp aren't effective for the classification. So I simpy drop them. It is possibile also notice that from the correlation matrix. These are not correlated with the target variable."}}