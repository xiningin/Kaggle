{"cell_type":{"ecf431b6":"code","65c98eb2":"code","64e3e8e3":"code","25c48bb5":"code","29f6cf0a":"code","330e8c1f":"code","9ce73c2d":"code","4d484937":"code","dd9c6549":"code","125e24e7":"code","72ba8ad6":"code","c5b2792e":"code","dadc2012":"code","edd13dc7":"code","a4952c30":"code","5b3b993c":"code","af5972f3":"code","37d28c61":"code","146c2143":"code","cd64ab96":"code","32d588bb":"code","b077ae73":"code","a92b39b5":"code","f8b4f034":"code","52969fd1":"code","08abe997":"code","8eca37c3":"markdown","f65deef8":"markdown","f5f717c6":"markdown","5f95968a":"markdown","e677a0f6":"markdown","55a33e04":"markdown","bb49cc61":"markdown","1548f335":"markdown","0d2b72b1":"markdown","ca4238dd":"markdown","62f8874e":"markdown","c0fc2e2d":"markdown","2c88c9ea":"markdown","e1b771c7":"markdown","d1d1ca50":"markdown","06fd2a67":"markdown","a42fc17a":"markdown","1c7e0541":"markdown","8ee20928":"markdown"},"source":{"ecf431b6":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\n\nfrom fastai.tabular import * \nfrom fastai import *\n\nimport os, shutil\nimport sys","65c98eb2":"# Notebook Settings\n\n# np.set_printoptions(threshold=sys.maxsize)","64e3e8e3":"kaggle_path = Path('.')\nkaggle_input_path = Path('\/kaggle\/input\/trends-assessment-prediction')\n\n#for dirname, _, filenames in os.walk(kaggle_input_path):\n#    print(dirname, filenames)","25c48bb5":"INCLUDE_FNC_DATA = False\nIMPUTATION_STRAT = 'IGNORE_ON_TRAIN' # 'IGNORE_ON_TRAIN', 'MEAN' \nLOSS_BASE = 'MSE' # 'L1'\nLOSS_WEIGHTS = [0.4, 0.15, 0.15, 0.15, 0.15]\nBS = 128","29f6cf0a":"l_data = pd.read_csv(kaggle_input_path\/'loading.csv')\n\nif INCLUDE_FNC_DATA:\n    f_data = pd.read_csv(kaggle_input_path\/'fnc.csv')\n    l_data = l_data.merge(f_data, on='Id', how = 'inner')\n\ny_data = pd.read_csv(kaggle_input_path\/'train_scores.csv')\n\nidx_site2 = pd.read_csv(kaggle_input_path\/'reveal_ID_site2.csv')\n#submission = pd.read_csv(kaggle_input_path\/'sample_submission.csv')","330e8c1f":"display(y_data.head())\ndisplay(y_data.describe())\ny_data.shape","9ce73c2d":"display(l_data.tail())\ndisplay(l_data.describe()),\nl_data.shape","4d484937":"if IMPUTATION_STRAT == 'IGNORE_ON_TRAIN':\n    ## will later ignore the value when executing the loss function\n    y_data = y_data.fillna(0)\nelse: #'MEAN'\n    y_data = y_data.fillna(mean())\n    \ny_data","dd9c6549":"train = l_data.merge(y_data, on='Id', how='inner').sort_values(by='Id').reset_index(drop = True)\nidx_train = train.pop('Id') \ntrain","125e24e7":"test = l_data.merge(y_data, on='Id', how='outer', indicator = True)\ntest = test[test['_merge'] == 'left_only'].drop(['age',\n                                                 'domain1_var1', \n                                                 'domain1_var2',\n                                                 'domain2_var1',\n                                                 'domain2_var2',\n                                                 '_merge'], axis = 1).sort_values(by='Id').reset_index(drop = True)\nidx_test = test.pop('Id') \ntest","72ba8ad6":"# variation of https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/metrics.py#L85\n\ndef norm_absolute_error(preds, targs):\n    \"Normalized absolute error between `pred` and `targ`.\"\n    sg=targs.sign()\n    y=targs*sg\n        \n    pred, targ = flatten_check(preds*sg, y)\n    return torch.abs(targ - pred).sum() \/ targ.sum()\n\ndef weighted_nae(preds, targs):\n    return 0.3 * norm_absolute_error(preds[:,0],targs[:,0]) + \\\n           0.175 * norm_absolute_error(preds[:,1],targs[:,1]) + \\\n           0.175 * norm_absolute_error(preds[:,2],targs[:,2]) + \\\n           0.175 * norm_absolute_error(preds[:,3],targs[:,3]) + \\\n           0.175 * norm_absolute_error(preds[:,4],targs[:,4])","c5b2792e":"# variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964\n\nclass Metric_idx(Callback):\n    def __init__(self, idx):\n        super().__init__()\n        self.idx = idx\n        \n    def on_epoch_begin(self, **kwargs):\n        self.targs, self.preds = Tensor([]), Tensor([])\n    \n    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n        last_output = last_output[self.idx]\n        last_target = last_target[self.idx]\n        \n        self.preds = torch.cat((self.preds, last_output.float().cpu()))\n        self.targs = torch.cat((self.targs, last_target.float().cpu()))\n        \n    def _norm_absolute_error(self):\n        return norm_absolute_error(self.preds, self.targs)\n    \n    def on_epoch_end(self, last_metrics, **kwargs): \n        return add_metrics(last_metrics, self._norm_absolute_error())\n\n    \nMetric_age = partial(Metric_idx,0)\nMetric_domain1_var1 = partial(Metric_idx,1)\nMetric_domain1_var2 = partial(Metric_idx,2)\nMetric_domain2_var1 = partial(Metric_idx,3)\nMetric_domain2_var2 = partial(Metric_idx,4)\n\nclass Metric_total(Callback):\n    def __init__(self):\n        super().__init__()\n        self.age = Metric_idx(0)\n        self.domain1_var1 = Metric_idx(1)\n        self.domain1_var2 = Metric_idx(2)\n        self.domain2_var1 = Metric_idx(3)\n        self.domain2_var2 = Metric_idx(4)\n        \n    def on_epoch_begin(self, **kwargs):\n        self.age.on_epoch_begin(**kwargs)\n        self.domain1_var1.on_epoch_begin(**kwargs)\n        self.domain1_var2.on_epoch_begin(**kwargs)\n        self.domain2_var1.on_epoch_begin(**kwargs)\n        self.domain2_var2.on_epoch_begin(**kwargs)\n        \n    \n    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n        self.age.on_batch_end(last_output, last_target, **kwargs)\n        self.domain1_var1.on_batch_end(last_output, last_target, **kwargs)\n        self.domain1_var2.on_batch_end(last_output, last_target, **kwargs)\n        self.domain2_var1.on_batch_end(last_output, last_target, **kwargs)\n        self.domain2_var2.on_batch_end(last_output, last_target, **kwargs)\n \n        \n    def on_epoch_end(self, last_metrics, **kwargs): \n        return add_metrics(last_metrics, \n                           0.3 * self.age._norm_absolute_error() +\n                           0.175*self.domain1_var1._norm_absolute_error()  +\n                           0.175*self.domain1_var2._norm_absolute_error()  +\n                           0.175*self.domain2_var1._norm_absolute_error()  +\n                           0.175*self.domain2_var2._norm_absolute_error()\n                          )","dadc2012":"# variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964\n\nclass Loss_combine(nn.Module):\n    def __init__(self, loss_weights = [0.4,0.15,0.15,0.15,0.15], loss_base = 'MSE'):\n        super().__init__()\n        \n        self.loss_base = loss_base\n        \n        self.loss_weights = loss_weights\n        self.fw = Tensor(LOSS_WEIGHTS).cuda()\n          \n            \n    def forward(self, input, target,reduction='mean'): #mean\n        \n        x0,x1,x2,x3,x4 = input.T\n        \n        if IMPUTATION_STRAT == 'IGNORE_ON_TRAIN':\n            sg = target.float().sign()\n            x0,x1,x2,x3,x4 = x0.float()*sg[:,0],x1.float()*sg[:,1],x2.float()*sg[:,2],x3.float()*sg[:,3],x4.float()*sg[:,4]\n        else: # 'MEAN'\n            sg = 1\n            x0,x1,x2,x3,x4 = x0.float(),x1.float(),x2.float(),x3.float(),x4.float()\n            \n        y = target.float()*sg\n        \n        if self.loss_base == 'MSE':\n            loss_func = F.mse_loss \n            reduction = 'sum'\n            return self.fw[0]*loss_func(x0,y[:,0],reduction=reduction)\/sum(y[:,0]**2) + \\\n               self.fw[1]*loss_func(x1,y[:,1],reduction=reduction)\/sum(y[:,1]**2) + \\\n               self.fw[2]*loss_func(x2,y[:,2],reduction=reduction)\/sum(y[:,2]**2) + \\\n               self.fw[3]*loss_func(x3,y[:,3],reduction=reduction)\/sum(y[:,3]**2) + \\\n               self.fw[4]*loss_func(x4,y[:,4],reduction=reduction)\/sum(y[:,4]**2)\n        else: # 'L1'\n            loss_func = F.l1_loss \n            reduction = 'mean'\n            return self.fw[0]*loss_func(x0,y[:,0],reduction=reduction) + \\\n               self.fw[1]*loss_func(x1,y[:,1],reduction=reduction) + \\\n               self.fw[2]*loss_func(x2,y[:,2],reduction=reduction) + \\\n               self.fw[3]*loss_func(x3,y[:,3],reduction=reduction) + \\\n               self.fw[4]*loss_func(x4,y[:,4],reduction=reduction)\n","edd13dc7":"# Vartiation https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/callbacks\/mixup.py#L8\n# and https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964#MixUp\n# and https:\/\/forums.fast.ai\/t\/tabulardata-mixup\/52011\/6\n\nclass MixUpLoss(Module):\n    \"Adapt the loss function `crit` to go with mixup.\"\n    \n    def __init__(self, crit, reduction='mean'): #mean\n        super().__init__()\n        if hasattr(crit, 'reduction'): \n            self.crit = crit\n            self.old_red = crit.reduction\n            setattr(self.crit, 'reduction', 'none')\n        else: \n            self.crit = partial(crit, reduction='none')\n            self.old_crit = crit\n        self.reduction = reduction\n        \n    def forward(self, output, target):\n        if len(target.shape) == 2 and target.shape[1] == 11:\n            loss1, loss2 = self.crit(output,target[:,0:5].long()), self.crit(output,target[:,5:10].long())\n            d = loss1 * target[:,-1] + loss2 * (1-target[:,-1])\n        else:  d = self.crit(output, target)\n        \n        if self.reduction == 'mean':    return d.mean()\n        elif self.reduction == 'sum':   return d.sum()\n        return d\n    \n    def get_old(self):\n        if hasattr(self, 'old_crit'):  return self.old_crit\n        elif hasattr(self, 'old_red'): \n            setattr(self.crit, 'reduction', self.old_red)\n            return self.crit\n\nclass MixUpCallback(LearnerCallback):\n    \"Callback that creates the mixed-up input and target.\"\n    def __init__(self, learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True):\n        super().__init__(learn)\n        self.alpha,self.stack_x,self.stack_y = alpha,stack_x,stack_y\n    \n    def on_train_begin(self, **kwargs):\n        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)\n        \n    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n        \"Applies mixup to `last_input` and `last_target` if `train`.\"\n        if not train: return\n        \n        # last_input[0] ==> embedded categorical data\n        # last_input[1] ==> continous data\n        rnd = 1\n        #if .5<np.random.uniform():\n        #    rnd=np.random.uniform()\/20\n        \n        l_org = last_input[1] * rnd\n        last_input = last_input[1] *rnd #0\n        \n        lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))\n        lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n        \n        lambd = last_input.new(lambd)\n        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)\n        x1, y1 = last_input[shuffle], last_target[shuffle]\n        if self.stack_x:\n            new_input = [last_input, last_input[shuffle], lambd]\n        else: \n            out_shape = [lambd.size(0)] + [1 for _ in range(len(x1.shape) - 1)]\n            new_input = (last_input * lambd.view(out_shape) + x1 * (1-lambd).view(out_shape))\n        if self.stack_y:\n            new_target = torch.cat([last_target.float(), y1.float(), lambd[:,None].float()], 1)\n        else:\n            if len(last_target.shape) == 2:\n                lambd = lambd.unsqueeze(1).float()\n            new_target = last_target.float() * lambd + y1.float() * (1-lambd)\n        \n        return {'last_input': [l_org, new_input], 'last_target': new_target}  \n    \n    def on_train_end(self, **kwargs):\n        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()\n            \n","a4952c30":"bs = 128\nvalid_idx = range(200, 400)\ndep_var = ['age','domain1_var1','domain1_var2', 'domain2_var1', 'domain2_var2']\n    \ndef prep_data(bs, valid_idx):\n    procs = [FillMissing, Categorify, Normalize]\n    cont_names = list(set(train.columns) - set(['age','domain1_var1','domain1_var2', 'domain2_var1', 'domain2_var2'])-set(dep_var))\n    cat_names = []\n\n    tlist = (TabularList.from_df(train, \n                                path=kaggle_path, \n                                cat_names=cat_names, \n                                cont_names=cont_names, \n                                procs=procs))\n\n    if valid_idx == None:\n        tlist = tlist.split_none()\n    else:\n        tlist = tlist.split_by_idx(valid_idx)\n\n    data = (tlist.label_from_df(cols=dep_var)\n                 .add_test(TabularList.from_df(test, \n                                               cat_names=cat_names,\n                                               cont_names=cont_names, \n                                               procs = procs))\n                 .databunch(path = kaggle_path, bs = bs))\n    \n    return data\n\n\ndef prep_learn(data):\n    \n    learn = tabular_learner(data, \n                        layers = [256,128,256,128,64], #[1024,1024,128,1024,128,1024,1024],#\n                        ps = 0.3,\n                        loss_func = Loss_combine(loss_weights = LOSS_WEIGHTS,  loss_base= LOSS_BASE),\n                        metrics=[Metric_age(),\n                                 Metric_domain1_var1(),\n                                 Metric_domain1_var2(),\n                                 Metric_domain2_var1(),\n                                 Metric_domain2_var2(),\n                                 Metric_total()],\n                       y_range=(Tensor([12,12,0,0,0]).cuda(),Tensor([90,90,100,100,100]).cuda())\n                       )#.to_fp16()\n\n    learn.clip_grad = 1.0\n    \n    return learn\n    ","5b3b993c":"data = prep_data(bs, valid_idx)\nlearn = prep_learn(data)","af5972f3":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","37d28c61":"lr = 2e-2\nreduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\nlearn.fit_one_cycle(10, lr, callbacks=[reduceLR])","146c2143":"yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\nyt, yt_truth= learn.get_preds(ds_type=DatasetType.Train)\n\nprint(f'Without augmentation:')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(yv,yv_truth)}')\nprint(f'Weighted normalized absolute error (Train): {weighted_nae(yt,yt_truth)}')\n","cd64ab96":"data = prep_data(bs, valid_idx)\nlearn = prep_learn(data)\n\nlr = 2e-2\nreduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\nlearn.fit_one_cycle(10, lr, callbacks=[MixUpCallback(learn, alpha=0.4),reduceLR])","32d588bb":"yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\nyt, yt_truth= learn.get_preds(ds_type=DatasetType.Train)\n\nprint(f'With augmentation:')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(yv,yv_truth)}')\nprint(f'Weighted normalized absolute error (Train): {weighted_nae(yt,yt_truth)}')\n","b077ae73":"def make_submission(learn, postfix = ''):\n    preds = learn.get_preds(ds_type=DatasetType.Test)[0]\n    \n    rec = pd.DataFrame(idx_test)\n    rec['Id'] = rec['Id'].astype(str)+'_'\n    rec['Predicted'] = preds[:,1]\n    \n    submission=None\n\n    for t, tcol in enumerate(dep_var):\n        rec = pd.DataFrame(idx_test)\n        rec['Id'] = rec['Id'].astype(str)+'_'+tcol\n        rec['Predicted'] = preds[:,t]\n        if isinstance(submission, pd.DataFrame):\n            submission = submission.append(rec)\n        else:\n            submission = rec\n\n    submission = submission.sort_values('Id').reset_index(drop=True)\n    \n    display(submission.head(10))\n    submission.to_csv('submission'+postfix+'.csv', index=False)","a92b39b5":"data = prep_data(bs, valid_idx = None)\nlearn = prep_learn(data)\n\nlr = 2e-2\nlearn.fit_one_cycle(10, lr)","f8b4f034":"make_submission(learn, postfix = '_wo_aug')","52969fd1":"data = prep_data(bs, valid_idx = None)\nlearn = prep_learn(data)\n\nlr = 2e-2\nlearn.fit_one_cycle(10, lr, callbacks=[MixUpCallback(learn, alpha=0.4)])","08abe997":"make_submission(learn, postfix = '_with_aug')","8eca37c3":"## Loss function\nThe customized metric callback is a variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964. It weights and combines the five single losses.","f65deef8":"## Training experiment without augmentation","f5f717c6":"# Model","5f95968a":"## Training experiment with augmentation","e677a0f6":"## Impute missing data","55a33e04":"## Training conclution\n\nThe choosen augmentation technique in this configuration doesn't improve the validation score. \n\nLet's see if it generalizes better on the LB.","bb49cc61":"## Predict without augmentation","1548f335":"# Parameters","0d2b72b1":"### Augmentation\nAugmentation is broadly used for sample generation and regularization in image classification. This is a try to use the techniques on tabular data.\n\nThe Mixup implementation is a variation of https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/callbacks\/mixup.py following this [example](https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964#MixUp) and adaptied for tabular data following this [thread](https:\/\/forums.fast.ai\/t\/tabulardata-mixup\/52011\/6).","ca4238dd":"# Conclusion\nLB without augmentation: 0.164\n\nLB with augmentation: 0.165\n\n==> No advantage from augmentation is this particular setting. Got to try something else ... ;)","62f8874e":"# Prepare data","c0fc2e2d":"## Combine Xs and Ys","2c88c9ea":"## Build the model","e1b771c7":"Augmentation is broadly used for sample generation and regularization in image classification. This is an experiment to use the techniques on tabular data. I'm starting here with mixup and may add more in the future, if I feel motivated.\n\n## Credits\nMany ideas of my notebook are derived from this [notebook](https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964#MixUp) from the Bengaliai competition earlier this year. Please go there and upvote if you find this or other references usefull.\nHere are the references in detail:\n- https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964#MixUp: Multi head - metrics, - loss function, - mixup.\n- https:\/\/github.com\/fastai\/fastai\/tree\/master\/fastai: mixup and many more\n- https:\/\/forums.fast.ai\/t\/tabulardata-mixup\/52011\/6: tabular mixup","d1d1ca50":"# TReNDS - Exploring tabular augmentation","06fd2a67":"## Predict with augmentation","a42fc17a":"The customized metric callback is a variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964. \nIt is used to keep track of the five single targets and the combined metric. ","1c7e0541":"## Metrics","8ee20928":"# Prediction"}}