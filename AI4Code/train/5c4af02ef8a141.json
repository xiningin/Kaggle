{"cell_type":{"e0a254ca":"code","1c1d578c":"code","b6ab8bfc":"code","10c3d8b0":"code","ad091d80":"code","ed43147e":"code","d79cd26c":"code","73ad2a9e":"code","cd0644e2":"code","1e00c431":"code","8ef9d2be":"code","b1606876":"code","f371f719":"code","0e3d7e20":"code","994a7798":"code","e03072b7":"code","0f4db357":"code","bb07fb8a":"code","9620f07b":"code","4a22f780":"code","d4115550":"code","7f683ee5":"code","a118f277":"code","9b34b5d2":"code","2be8bfbc":"code","bfbb368d":"code","2ff17fbc":"code","d44d8dbd":"code","f1bc1ac8":"code","aacea34a":"code","a1a59a5b":"code","e37d3f7e":"code","ae6cf595":"code","bdd2cce5":"code","1869efae":"code","e2053f86":"code","45031f30":"code","c3341f48":"code","1f27d7bf":"code","46501ec7":"code","bf09d54e":"code","5940ad84":"code","f1f9a686":"code","7aa45553":"code","ffb004dd":"code","9af11c47":"code","4136c4fe":"code","2c1e3d0d":"code","602b37cd":"code","d44039fc":"code","9bf09b8c":"code","6dc6546d":"code","7c4a7ce0":"code","aaf07c36":"code","9bc43c64":"code","b219b436":"code","bcb434c8":"code","cbe3d166":"code","5c0becd4":"code","86099d63":"code","86853938":"code","4b85289d":"code","ef36f7fc":"code","6fd6ea81":"code","d8adf06b":"code","5d709082":"code","adf40522":"code","e97fe824":"code","af0267e6":"code","2e1f5eca":"code","3da0335d":"code","2c24d89e":"code","8b0e32f4":"code","a3ec2835":"code","6a0584de":"code","a85e9003":"code","dc8d882a":"code","903fe725":"code","f9cec410":"code","4e9745e8":"code","9b7de16c":"code","1da0c78e":"code","f57253ee":"markdown","493eff5d":"markdown","a8d81ae4":"markdown","966508f8":"markdown","60853164":"markdown","0f6f415f":"markdown","99d66ae4":"markdown","cd927a1e":"markdown","382cbbeb":"markdown","b047200f":"markdown","c59afaa6":"markdown","96e79a09":"markdown","0bdfcc8b":"markdown","f6d8e531":"markdown","8f896364":"markdown","5e6c8058":"markdown","0dcb76d4":"markdown","4693dec0":"markdown","bdac44b8":"markdown","1d4114c5":"markdown","37dfd7ce":"markdown","6c89bc83":"markdown","25e2a0e2":"markdown","359e40f3":"markdown","3bbed276":"markdown","1c02d83e":"markdown"},"source":{"e0a254ca":"import numpy as np\nimport pandas as pd\nimport warnings \nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import zscore\nimport statsmodels\nimport scipy.stats as stats\nimport statsmodels.stats.proportion as smpt\nfrom sklearn import model_selection\nfrom sklearn.naive_bayes import GaussianNB\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix,accuracy_score, classification_report,f1_score,cohen_kappa_score","1c1d578c":"d1=pd.read_csv('..\/input\/delinquency-telecom-dataset\/sample_data_intw.csv')","b6ab8bfc":"d1.isnull().sum()","10c3d8b0":"d1.info()","ad091d80":"lst = [d1]\nfor column in lst:\n    column.loc[column[\"rental30\"] <= 0,  'balance_group'] = 'no balance'\n    column.loc[(column[\"rental30\"] > 0) & (column[\"rental30\"] <= 19766), 'balance_group'] = 'low balance'\n    column.loc[(column[\"rental30\"] > 19766) & (column[\"rental30\"] <= 118572), 'balance_group'] = 'average balance'\n    column.loc[(column[\"rental30\"] > 118572), 'balance_group'] = 'high balance'","ed43147e":"count_balance_response_pct = pd.crosstab(d1['label'],d1['balance_group']).apply(lambda x: x\/x.sum() * 100)\ncount_balance_response_pct = count_balance_response_pct.transpose()","d79cd26c":"bal = pd.DataFrame(d1['balance_group'].value_counts())\n#bal['% Contacted'] = bal['balance_group']*100\/bal['balance_group'].sum()\nbal['% Subscription'] = count_balance_response_pct[1]\nbal.drop('balance_group',axis = 1,inplace = True)\n\nbal['bal'] = [1,2,0,3]\nbal = bal.sort_values('bal',ascending = True)","73ad2a9e":"plot_balance = bal['% Subscription'].plot(kind = 'bar',\n                                               color = ('royalblue'),\n                                               figsize = (8,6))\n\nplt.title('Average main balance account vs loan pay back rate with in 5 days')\nplt.ylabel('Loan pay back rate with in 5 days')\nplt.xlabel('Balance Category (rental30)')\nplt.xticks(rotation = 'horizontal')\n\n# label the bar\nfor rec, label in zip(plot_balance.patches,\n                      bal['% Subscription'].round(1).astype(str)):\n    plot_balance.text(rec.get_x() + rec.get_width()\/2, \n                      rec.get_height() + 1, \n                      label+'%',  \n                      ha = 'center', \n                      color = 'black')","cd0644e2":"lst = [d1]\nfor column in lst:\n    column.loc[column[\"fr_ma_rech30\"] <=0,  'frequency_group'] = 'no frequency'\n    column.loc[(column['fr_ma_rech30'] > 0) & (column['fr_ma_rech30'] <=1 ), 'frequency_group'] = 'low frequency'\n    column.loc[(column['fr_ma_rech30'] >1) & (column['fr_ma_rech30'] <=2), 'frequency_group'] = 'medium frequency'\n    column.loc[(column['fr_ma_rech30'] >2), 'frequency_group'] = 'high frequency'","1e00c431":"count_fre_response_pct = pd.crosstab(d1['label'],d1['frequency_group']).apply(lambda x: x\/x.sum() * 100)\ncount_fre_response_pct = count_fre_response_pct.transpose()","8ef9d2be":"fre = pd.DataFrame(d1['frequency_group'].value_counts())\n#bal['% Contacted'] = bal['balance_group']*100\/bal['balance_group'].sum()\nfre['% Subscription'] = count_fre_response_pct[1]\nfre.drop('frequency_group',axis = 1,inplace = True)\n\nfre['fre'] = [1,2,0,3]\nfre= fre.sort_values('fre',ascending = True)","b1606876":"plot_fre = fre['% Subscription'].plot(kind = 'bar',\n                                               color = ('orange'),\n                                               figsize = (8,6))\n\nplt.title('Frequency of main account recharged in last 30 days vs loan pay back rate with in 5 days')\nplt.ylabel('Loan pay back rate with in 5 days')\nplt.xlabel('Frequency Category (fr_ma_rech30)')\nplt.xticks(rotation = 'horizontal')\n\n# label the bar\nfor rec, label in zip(plot_fre.patches,\n                      fre['% Subscription'].round(1).astype(str)):\n    plot_fre.text(rec.get_x() + rec.get_width()\/2, \n                      rec.get_height() + 1, \n                      label+'%',  \n                      ha = 'center', \n                      color = 'black')","f371f719":"lst = [d1]\nfor column in lst:\n    column.loc[column[\"cnt_loans30\"] <=0,  'loan_frequency_group'] = 'no loans'\n    column.loc[(column['cnt_loans30'] > 0) & (column['cnt_loans30'] <=1 ), 'loan_frequency_group'] = 'low num of loans'\n    column.loc[(column['cnt_loans30'] >1) & (column['cnt_loans30'] <=4), 'loan_frequency_group'] = 'medium num of loans'\n    column.loc[(column['cnt_loans30'] >4), 'loan_frequency_group'] = 'high num of loans'","0e3d7e20":"count_loan_response_pct = pd.crosstab(d1['label'],d1['loan_frequency_group']).apply(lambda x: x\/x.sum() * 100)\ncount_loan_response_pct = count_loan_response_pct.transpose()","994a7798":"fre = pd.DataFrame(d1['loan_frequency_group'].value_counts())\n#bal['% Contacted'] = bal['balance_group']*100\/bal['balance_group'].sum()\nfre['% Subscription'] = count_loan_response_pct[1]\nfre.drop('loan_frequency_group',axis = 1,inplace = True)\n\nfre['fre'] = [1,2,0,3]\nfre= fre.sort_values('fre',ascending = True)","e03072b7":"plot_fre = fre['% Subscription'].plot(kind = 'barh',\n                                               color = ('green'), figsize = (8,6))\n\nplt.title('Number of loans taken by user in last 30 days vs loan pay back rate with in 5 days')\nplt.ylabel('Number of loans (cnt_loans30)')\nplt.xlabel('Loan pay back rate with in 5 days')\nplt.xticks(rotation = 'horizontal')\n\n# label the bar\nfor rec, label in zip(plot_fre.patches,\n                      fre['% Subscription'].round(1).sort_values(ascending = True).round(1).astype(str)):\n    plot_fre.text(rec.get_width()+5, \n                  rec.get_y()+ rec.get_height()-0.3, \n                  label+'%', \n                  ha = 'center', \n                  va='bottom')","0f4db357":"lst = [d1]\nfor column in lst:\n    column.loc[column[\"amnt_loans30\"] <=0,  'loanamnt_frequency_group'] = 'no loans'\n    column.loc[(column['amnt_loans30'] > 0) & (column['amnt_loans30'] <=6 ), 'loanamnt_frequency_group'] = 'low amnt of loans'\n    column.loc[(column['amnt_loans30'] >6) & (column['amnt_loans30'] <=24), 'loanamnt_frequency_group'] = 'medium amnt of loans'\n    column.loc[(column['amnt_loans30'] >24), 'loanamnt_frequency_group'] = 'high amnt of loans'","bb07fb8a":"count_loanamnt_response_pct = pd.crosstab(d1['label'],d1['loanamnt_frequency_group']).apply(lambda x: x\/x.sum() * 100)\ncount_loanamnt_response_pct = count_loanamnt_response_pct.transpose()","9620f07b":"fre1 = pd.DataFrame(d1['loanamnt_frequency_group'].value_counts())\n#bal['% Contacted'] = bal['balance_group']*100\/bal['balance_group'].sum()\nfre1['% Subscription'] = count_loanamnt_response_pct[1]\nfre1.drop('loanamnt_frequency_group',axis = 1,inplace = True)\n\nfre1['fre1'] = [1,2,0,3]\nfre1= fre1.sort_values('fre1',ascending = True)","4a22f780":"plot_fre1 = fre1['% Subscription'].plot(kind = 'bar',\n                                               color = ('red'), figsize = (8,6))\n\nplt.title('Total amount of loans taken by user in last 30 days vs loan pay back rate with in 5 days')\nplt.ylabel('Loan pay back rate with in 5 days')\nplt.xlabel('Total amount of loans (amnt_loans30)')\nplt.xticks(rotation = 'horizontal')\n\n# label the bar\nfor rec, label in zip(plot_fre1.patches,\n                      fre1['% Subscription'].round(1).astype(str)):\n    plot_fre1.text(rec.get_x() + rec.get_width()\/2, \n                      rec.get_height() + 1, \n                      label+'%',  \n                      ha = 'center', \n                      color = 'black')","d4115550":"d1.drop(['msisdn', 'pcircle', 'pdate','balance_group','frequency_group','loan_frequency_group','loanamnt_frequency_group'],axis=1,inplace=True)\nd1","7f683ee5":"corr_data = d1\ncorr = corr_data.corr()\ncorr","a118f277":"corr_data = d1\ncorr = corr_data.corr()\nsns.heatmap(corr,annot_kws={'size':10})","9b34b5d2":"d6=d1.copy()","2be8bfbc":"d6.drop(['last_rech_date_da','fr_da_rech30','maxamnt_loans30','medianamnt_loans30','maxamnt_loans90','medianamnt_loans90','cnt_da_rech30','cnt_da_rech90','fr_da_rech90'],axis=1,inplace=True)","bfbb368d":"columns=d6.columns","2ff17fbc":"columns=['aon', 'daily_decr30', 'daily_decr90', 'rental30', 'rental90',\n       'last_rech_date_ma', 'last_rech_amt_ma', 'cnt_ma_rech30',\n       'fr_ma_rech30', 'sumamnt_ma_rech30', 'medianamnt_ma_rech30',\n       'medianmarechprebal30', 'cnt_ma_rech90', 'fr_ma_rech90',\n       'sumamnt_ma_rech90', 'medianamnt_ma_rech90', 'medianmarechprebal90',\n       'cnt_loans30', 'amnt_loans30', 'cnt_loans90', 'amnt_loans90',\n       'payback30', 'payback90']\nfig,ax = plt.subplots(8,3,figsize=(16,20))\nax = ax.flatten()\nfor i,col in enumerate(columns):\n    sns.distplot(d6[col],ax=ax[i],color='red')\nplt.tight_layout()\nplt.show()","d44d8dbd":"d3=d1.copy()\nd3","f1bc1ac8":"columns=['aon', 'daily_decr30', 'daily_decr90', 'rental30', 'rental90',\n       'last_rech_date_ma', 'last_rech_date_da', 'last_rech_amt_ma',\n       'cnt_ma_rech30', 'fr_ma_rech30', 'sumamnt_ma_rech30',\n       'medianamnt_ma_rech30', 'medianmarechprebal30', 'cnt_ma_rech90',\n       'fr_ma_rech90', 'sumamnt_ma_rech90', 'medianamnt_ma_rech90',\n       'medianmarechprebal90', 'cnt_da_rech30', 'fr_da_rech30',\n       'cnt_da_rech90', 'fr_da_rech90', 'cnt_loans30', 'amnt_loans30',\n       'maxamnt_loans30', 'medianamnt_loans30', 'cnt_loans90', 'amnt_loans90',\n       'maxamnt_loans90', 'medianamnt_loans90', 'payback30', 'payback90']\nfor i in range(len(columns)):\n    d3[columns[i]]=zscore(d3[columns[i]])\n    for j in range(len(d3[columns[i]])):\n        if d3[columns[i]][j]>3 or d3[columns[i]][j]<-3:\n            d3[columns[i]].replace(d3[columns[i]][j],d3[columns[i]].median(),inplace=True)\n    d3[columns[i]]=np.cbrt((d3[columns[i]]))","aacea34a":"columns=['aon', 'daily_decr30', 'daily_decr90', 'rental30', 'rental90',\n       'last_rech_date_ma', 'last_rech_amt_ma', 'cnt_ma_rech30',\n       'fr_ma_rech30', 'sumamnt_ma_rech30', 'medianamnt_ma_rech30',\n       'medianmarechprebal30', 'cnt_ma_rech90', 'fr_ma_rech90',\n       'sumamnt_ma_rech90', 'medianamnt_ma_rech90', 'medianmarechprebal90',\n       'cnt_loans30', 'amnt_loans30', 'cnt_loans90', 'amnt_loans90',\n       'payback30', 'payback90']\nfig,ax = plt.subplots(8,3,figsize=(16,20))\nax = ax.flatten()\nfor i,col in enumerate(columns):\n    sns.distplot(d3[col],ax=ax[i],color='red')\nplt.tight_layout()\nplt.show()","a1a59a5b":"d3.drop(['label'],axis=1,inplace=True)","e37d3f7e":"def two_sample_ttest(target_variable, numerical_column):\n    reject = []\n    not_reject = []\n    print('H0: The mean of ' + numerical_column.name + ' is equal for both categories of ' + target_variable.name)\n    print('Ha: The mean of ' + numerical_column.name + ' is NOT equal for both categories of ' + target_variable.name)\n    print()\n    grp0 = numerical_column[target_variable == 0]\n    grp1 = numerical_column[target_variable == 1]\n    ttest = stats.ttest_ind(grp0, grp1)\n    print(ttest)\n    rejectH0 = ttest[1] < 0.05\n    print()\n    #return rejectH0\n    if rejectH0:\n        print('Reject H0')\n        reject.append(col)\n        print('\\n')\n        print('-------------------------------------------------------------------------')\n    else:\n        print('Failed to Reject H0')   \n        not_reject.append(col)\n        print()\n        print('-------------------------------------------------------------------------')\n    #print(reject)\n    #print(not_reject)","ae6cf595":"num_cols = ['aon', 'daily_decr30', 'daily_decr90', 'rental30', 'rental90',\n       'last_rech_date_ma', 'last_rech_date_da', 'last_rech_amt_ma',\n       'cnt_ma_rech30', 'fr_ma_rech30', 'sumamnt_ma_rech30',\n       'medianamnt_ma_rech30', 'medianmarechprebal30', 'cnt_ma_rech90',\n       'fr_ma_rech90', 'sumamnt_ma_rech90', 'medianamnt_ma_rech90',\n       'medianmarechprebal90', 'cnt_da_rech30', 'fr_da_rech30',\n       'cnt_da_rech90', 'fr_da_rech90', 'cnt_loans30', 'amnt_loans30',\n       'maxamnt_loans30', 'medianamnt_loans30', 'cnt_loans90', 'amnt_loans90',\n       'maxamnt_loans90', 'medianamnt_loans90', 'payback30', 'payback90']\n\n#reject = []\n#not_reject = []\nfor col in num_cols:\n    rejectH0 = two_sample_ttest(d1['label'], d3[col])","bdd2cce5":"X1=d3","1869efae":"from statsmodels.stats.outliers_influence import variance_inflation_factor","e2053f86":"vif= pd.DataFrame()\nvif['Features'] = d3.columns\nvif['vif']=[variance_inflation_factor(d3.values,i) for i in range(d3.shape[1])]","45031f30":"vif[vif['vif']>4]","c3341f48":"from sklearn.decomposition import PCA","1f27d7bf":"pca=PCA()\npca.fit(d3)","46501ec7":"data_pca= pca.transform(d3)","bf09d54e":"cumsum=np.cumsum(pca.explained_variance_ratio_)\ncumsum","5940ad84":"plt.figure(figsize=(10,6))\n\nplt.plot(range(0,32), cumsum, color='k', lw=2)\n\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\n\nplt.axvline(8, c='b')\nplt.axhline(0.9, c='r')\n\nplt.show()","f1f9a686":"cov_matrix = np.cov(d3.T)\nprint('Covariance Matrix \\n%s', cov_matrix)","7aa45553":"eig_vals, eig_vecs = np.linalg.eig(cov_matrix)","ffb004dd":"eigen_pairs = [(np.abs(eig_vals[i]), eig_vecs[ :, i]) for i in range(len(eig_vals))]","9af11c47":"tot = sum(eig_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var_exp)","4136c4fe":"plt.figure(figsize=(6 , 4))\nplt.bar(range(32), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(32), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","2c1e3d0d":"pca = PCA(n_components=13)\npca.fit(d3)\ndata_pca = pd.DataFrame(pca.transform(d3))\ndata_pca.shape","602b37cd":"X1=data_pca\ny1=d1['label']","d44039fc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","9bf09b8c":"from sklearn.linear_model import LogisticRegression\nmodel1 = LogisticRegression()\nmodel1.fit(X_train, y_train)","6dc6546d":"print('Training score =', model1.score(X_train, y_train))\nprint('Test score =', model1.score(X_test, y_test))","7c4a7ce0":"from sklearn.metrics import confusion_matrix\nypred = model1.predict(X_test)\ncm = confusion_matrix(y_test, ypred)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.show()","aaf07c36":"tn = cm[0,0]  #True Negative\ntp = cm[1,1]  #True Positives\nfp = cm[0,1]  #False Positives\nfn = cm[1,0]  #False Negatives\n\naccuracy = (tp+tn)\/(tp+fn+fp+tn)\nprecision = tp \/ (tp+fp)\nrecall = tp \/ (tp+fn)\nf1 = 2*precision*recall \/ (precision+recall)\n\nprint('Accuracy =',accuracy)\nprint('Precision =', precision)\nprint('Recall =', recall)\nprint('F1 Score =', f1)","9bc43c64":"from sklearn.metrics import roc_curve,roc_auc_score\nypred = model1.predict_proba(X_test)\nfpr,tpr,threshold = roc_curve(y_test,ypred[:,1])\nroc_auc = roc_auc_score(y_test,ypred[:,1])\n\nprint('ROC AUC =', roc_auc)\nplt.figure()\nlw = 2\nplt.plot(fpr,tpr,color='darkorange',lw=lw,label='ROC Curve (area = %0.2f)'%roc_auc)\nplt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()","b219b436":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier","bcb434c8":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nknn=KNeighborsClassifier()\nparam={'n_neighbors':np.arange(5,30),'weights':['uniform','distance']}\nGS=RandomizedSearchCV(knn,param,cv=3,scoring='f1_weighted',n_jobs=-1)\nGS.fit(X_train,y_train)","cbe3d166":"GS.best_params_","5c0becd4":"dt=DecisionTreeClassifier(random_state=0)","86099d63":"param={'max_depth':np.arange(3,50),'criterion':['entropy','gini'],'min_samples_leaf':np.arange(3,20)}\nGS=RandomizedSearchCV(dt,param,cv=3,scoring='f1_weighted')\nGS.fit(X_train,y_train)","86853938":"GS.best_params_","4b85289d":"LR=LogisticRegression()\nNB=GaussianNB()\nKNN=KNeighborsClassifier(n_neighbors=5,weights='distance')\nDT=DecisionTreeClassifier(criterion='gini',max_depth=14,min_samples_leaf=19,random_state=0)\nRF=RandomForestClassifier(criterion='entropy',n_estimators=7,random_state=0)\nBag=BaggingClassifier(n_estimators=3,random_state=0)\nAB=AdaBoostClassifier(n_estimators=16,random_state=0)\n#ABL=AdaBoostClassifier(base_estimator=LR,n_estimators=50,random_state=0)\nGB=GradientBoostingClassifier(n_estimators=17)\n#svm=SVC(C=10,gamma=0.001,kernel='rbf')\n#stacked = StackingClassifier(classifiers=[Bag,RF,AB], meta_classifier=KNN)","ef36f7fc":"RF_var=[]\nfor val in np.arange(1,50):\n  RF=RandomForestClassifier(criterion='gini',n_estimators=val,random_state=0)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(RF, X_train,y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  RF_var.append(np.var(cv_results,ddof=1))","6fd6ea81":"x_axis=np.arange(1,50)\nplt.plot(x_axis,RF_var)","d8adf06b":"np.argmin(RF_var)","5d709082":"Bag_var=[]\nfor val in np.arange(1,20):\n  Bag=BaggingClassifier(n_estimators=val,random_state=0)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(Bag, X_train,y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  Bag_var.append(np.var(cv_results,ddof=1))\n  #print(val,np.var(cv_results,ddof=1))","adf40522":"x_axis=np.arange(1,20)\nplt.plot(x_axis,Bag_var)","e97fe824":"np.argmin(Bag_var)","af0267e6":"Ada_bias=[]\nfor val in np.arange(1,20):\n  Ada=AdaBoostClassifier(n_estimators=val,random_state=0)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(Ada, X_train, y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  Ada_bias.append(1-np.mean(cv_results))\n  #print(val,1-np.mean(cv_results))","2e1f5eca":"x_axis=np.arange(1,20)\nplt.plot(x_axis,Ada_bias)","3da0335d":"np.argmin(Ada_bias)","2c24d89e":"GB_bias=[]\nfor val in np.arange(1,20):\n  gb=GradientBoostingClassifier(n_estimators=val)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(gb, X_train, y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  GB_bias.append(1-np.mean(cv_results))\n  #print(val,1-np.mean(cv_results))\n","8b0e32f4":"x_axis=np.arange(1,20)\nplt.plot(x_axis,GB_bias)","a3ec2835":"np.argmin(GB_bias)","6a0584de":"models = []\nmodels.append(('Logistic', LR))\nmodels.append(('NaiveBayes', NB))\nmodels.append(('KNN',KNN))\nmodels.append(('DecisionTree',DT))\nmodels.append(('RandomForest',RF))\nmodels.append(('BaggingClassifier',Bag))\nmodels.append(('AdaBoost',AB))\nmodels.append(('GBoost',GB))\n#models.append(('Stacked',stacked))","a85e9003":"results = []\nnames = []\nfor name, model in models:\n\tkfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n\tcv_results = model_selection.cross_val_score(model, X_train, y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint(\"%s: %f (%f)\" % (name, np.mean(cv_results),np.var(cv_results,ddof=1)))\n   # boxplot algorithm comparison\nfig = plt.figure(figsize=(10,9))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,rotation=45)\nplt.show()","dc8d882a":"RF.fit(X_train,y_train)","903fe725":"predictions = RF.predict(X_test)","f9cec410":"cm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.show()","4e9745e8":"# Classification Report\nprint(classification_report(y_test, predictions))","9b7de16c":"tn = cm[0,0]  #True Negative\ntp = cm[1,1]  #True Positives\nfp = cm[0,1]  #False Positives\nfn = cm[1,0]  #False Negatives\n\naccuracy = (tp+tn)\/(tp+fn+fp+tn)\nprecision = tp \/ (tp+fp)\nrecall = tp \/ (tp+fn)\nf1 = 2*precision*recall \/ (precision+recall)\n\nprint('Accuracy =',accuracy)\nprint('Precision =', precision)\nprint('Recall =', recall)\nprint('F1 Score =', f1)","1da0c78e":"from sklearn.metrics import roc_curve,roc_auc_score\nypred = RF.predict_proba(X_test)\nfpr,tpr,threshold = roc_curve(y_test,ypred[:,1])\nroc_auc = roc_auc_score(y_test,ypred[:,1])\n\nprint('ROC AUC =', roc_auc)\nplt.figure()\nlw = 2\nplt.plot(fpr,tpr,color='darkorange',lw=lw,label='ROC Curve (area = %0.2f)'%roc_auc)\nplt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()","f57253ee":"## Ada Boost Classifier","493eff5d":"## KNN","a8d81ae4":"## Checking the normality of the features","966508f8":"# EDA Part","60853164":"## Statistical Analysis","0f6f415f":"#### From the above results it is observed that all the metrics accuracy, precision, recall and f1-score are good. In order to improve the score very much high I also tried with various other models such as decision tree, random forest, na\u00efve bays, knn, and ensemble models also.\n\n#### Four different classification algorithms (Logistic Regression, K-Neighbours Classifier, Decision Tree Classifier, and Gaussian NB, Random Forest, Ada boost, Gradient Boosting) were run on the dataset through K-fold cross validation and the best-performing one was (identified by observing bias and variance errors) and used to build the classification model.\n","99d66ae4":"## PCA","cd927a1e":"## VIF","382cbbeb":"## Random Forest","b047200f":"#### From the above results it is observed that 90% of the data is covered at the pca components (n=13). So total number of pca components were taken as 13.","c59afaa6":"## Gradient Boost Clsssifier","96e79a09":"## evaluate each model in turn","0bdfcc8b":"#### From the above results we can infer that the many features are having strong multicollinearity in the data set. This resembles that there is need to go for PCA (Principal Component Analysis). \n#### If we won\u2019t perform PCA the noise or correlation between the independent variables will affect the model prediction and model results. More than 50% of the features are having vif >4 so it is mandatory to perform PCA in order to reduce the multicollinearity effect among the independent variables. \n","f6d8e531":"#### The above bar plot infers us how customers with different main balance levels are paying back the loan with in five days. The high balance level people are with 100% rate i.e they are paying loan within 5 days. Coming to the average and low balance people it is observed that around 10%-12% of people are not paying the loan within 5 days. \n#### Coming to low balance level people, it is observed that around 30% of people are not paying back the loan with in stipulated 5 days of time. The 30% of people with no balance or negative balance people are creating a major loss to the company without paying back the loan within five days of time.\n#### In order to decrease loss to the company, the company should start some marketing strategies like sms alerting and notifications and others on the people with no balance, average and high balance level people notifying them to pay the loan back within five days of time.\n","8f896364":"## To get n-estimators","5e6c8058":"#### As 'msisdn', 'pcircle', 'pdate' features are not having much importance, we can ignore them. And also removing the extra columns created for the EDA part.","0dcb76d4":"## Random Forest","4693dec0":"## Total no of records in X_test and y_test is 62,878. The results of our random forest classifier are better than the before base line model.\n## TP=53133\n## TN=3030\n## FP=1960 (type-1 error)\n## FN=4755 (type-2 error)\n\n## From the above results we can infer that only 10% of the data are under type-1 & type- 2 errors. Our model is unable to classify well only 10% records whether the customer is loan defaulter or not.\n","bdac44b8":"## Check for the correlation with the dependent variable 'Label","1d4114c5":"## Decision Tree","37dfd7ce":"#### From the above results it is observed that Random Forest is the best performing model. By comparing all algorithms bias error and variance error, random forest is observed to be the best so it would be used to predict loan defaulters. The test of random forest with base estimator (Decision Tree (which is default for random forest), n_estimators=7) model successfully achieved a weighted F1_score of 98%, suggesting high level of strength of this model to classify loan defaulter\u2019s.\n","6c89bc83":"#### The above bar plot infers us how customers with different loans levels taken are paying back the loan within five days. In the data set people not taken loans are labelled as \u20181\u2019. So we should not consider the people with no loans labelled in the above graph.\n#### Considering the remaining levels, there is no 100% rate in any of the loan levels to pay back the loan within 5 days. Coming to the low amount level people it is observed that around 25% of people are not paying the loan within 5 days. Only 2% of the people taken high amount of loans are not paying the loan within 5 days. This is followed by the people with medium number of loans having defaulters of 7% approximately.\n#### In order to decrease loss to the company, the company should start some marketing strategies like sms alerting and notifications and others on the people with all loan levels and especially on low & high level people notifying them to pay the loan back within five days of time.\n","25e2a0e2":"## Logistic regression","359e40f3":"## Outlier Transformation:\n#### Several changes were made to the dataset to prepare it for analysis. As there are no null values in the data set there is no need to perform any null value imputation for the data set. There are outliers for many variables in the data set.\n\n#### By observing these features, I found way of doing an outlier\u2019s imputation technique for the data of the features whose z-score >3. There are many ways to deal with outliers such as imputing outlier\u2019s with mean, median, mode (categorical), k-NN imputation, mice imputation or simply removing and others.\n\n#### For this data set I simply choose mean for imputing the outliers with the respective features. After performing mean, I also applied cube root for the data to bring data closer as to make the distribution normal.\n\n#### After performing the mean imputation and also applying cube root to the data become so what normally distributed compared to the data which haven\u2019t undergone any type of imputation or outlier transformation.\n\n#### So, outlier imputation is far better than simply removing the outliers from the data. As the data set belongs to the loan defaulters or not the outliers are also important for us to get the unbiased results after performing machine learning algorithms.\n","3bbed276":"#### The above bar plot infers us how customers with different frequency levels (main account recharge) are paying back the loan within five days. The is no 100% rate in any of the frequency levels to pay back the loan within 5 days. Coming to the average and low & medium frequency people it is observed that around 5%-6% of people are not paying the loan within 5 days. \n#### Coming to low frequency level people, it is observed that around 25% of people are not paying back the loan with in stipulated 5 days of time. The 25% people who are not getting their main account recharge for 30 days creating a major loss to the company without paying back the loan within five days of time.\n#### In order to decrease loss to the company, the company should start some marketing strategies like sms alerting and notifications and others on the people with all frequency levels and especially on no frequency level people notifying them to pay the loan back within five days of time.\n","1c02d83e":"#### The above bar plot infers us how customers with different loans levels taken are paying back the loan within five days. In the data set people not taken loans are labelled as \u20181\u2019. So we should not consider the people with no loans labelled in the above graph.\n#### Considering the remaining levels, there is no 100% rate in any of the loan levels to pay back the loan within 5 days. Coming to the high number of loan level people it is observed that around 25% of people are not paying the loan within 5 days. Only 2% of the people from low number of loans category are not paying the loan within 5 days. This is followed by the people with medium number of loans having defaulters of 7% approximately.\n#### In order to decrease loss to the company, the company should start some marketing strategies like sms alerting and notifications and others on the people with all loan levels and especially on low & high level people notifying them to pay the loan back within five days of time.\n"}}