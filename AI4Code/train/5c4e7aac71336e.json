{"cell_type":{"e92a3617":"code","f08ca292":"code","54073681":"code","793d2d09":"code","b0207349":"code","bcda3c27":"code","e0009e57":"code","d1fabc2b":"code","36556f1b":"code","194bdbec":"code","369dc10f":"code","e437d64e":"code","4ce2af43":"code","244a1c59":"code","2a44ea27":"code","59b5d0bf":"code","1768fda4":"code","ffa60ec5":"code","e1dc07ea":"code","00e16dab":"code","31a29e03":"code","fe041478":"code","0a7cd70f":"code","d51a56ce":"code","46f264f6":"code","0760ec6b":"code","a7725f11":"code","9e476c7b":"code","d3ca6533":"code","a0a8c124":"code","1c77e8ec":"code","81712889":"code","2aa1fa27":"code","cf589704":"code","7752f063":"code","65638f7c":"code","a84d21b8":"code","d52a4e38":"code","0b453811":"code","224ecaec":"code","df8d3725":"code","7d64fc3c":"code","9f89fbdc":"code","7b0d2ed4":"code","63661895":"code","a75916e5":"markdown","956fabc3":"markdown","139f684a":"markdown","5bfc7773":"markdown","797db1ce":"markdown","70e17c37":"markdown","84db6ce2":"markdown","7c5975e4":"markdown","fae8fc11":"markdown","49b93d5d":"markdown","738013f5":"markdown","7fed9035":"markdown","f3a1363a":"markdown","e1caa36b":"markdown","d9f651e4":"markdown","3404b066":"markdown"},"source":{"e92a3617":"%%bash\npip install ..\/input\/pytorch-pfn-extras\/pytorch-pfn-extras-0.2.1\/","f08ca292":"import os\nimport os\nimport sys\nsys.path = [\n    '..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master',\n] + sys.path\nimport gc\nimport time\nimport shutil\nimport random\nimport warnings\nimport typing as tp\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nimport yaml\nfrom joblib import delayed, Parallel\n\nimport cv2\nimport librosa\nimport audioread\nimport soundfile as sf\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\n# import resnest.torch as resnest_torch\n\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.training import extensions as ppe_extensions\n\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500","54073681":"Path(\"\/root\/.cache\/torch\/checkpoints\").mkdir(parents=True)","793d2d09":"!cp ..\/input\/efficientnet-pytorch\/efficientnet-b4-e116e8b3.pth \/root\/.cache\/torch\/checkpoints\/","b0207349":"def set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n#     torch.backends.cudnn.deterministic = True  # type: ignore\n#     torch.backends.cudnn.benchmark = True  # type: ignore\n    \n\n@contextmanager\ndef timer(name: str) -> None:\n    \"\"\"Timer Util\"\"\"\n    t0 = time.time()\n    print(\"[{}] start\".format(name))\n    yield\n    print(\"[{}] done in {:.0f} s\".format(name, time.time() - t0))","bcda3c27":"ROOT = Path.cwd().parent\nINPUT_ROOT = ROOT \/ \"input\"\nRAW_DATA = INPUT_ROOT \/ \"birdsong-recognition\"\nTRAIN_AUDIO_DIR = RAW_DATA \/ \"train_audio\"\nTRAIN_RESAMPLED_AUDIO_DIRS = [\n  INPUT_ROOT \/ \"birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5)\n]\nTEST_AUDIO_DIR = RAW_DATA \/ \"test_audio\"","e0009e57":"\ntrain = pd.read_csv(TRAIN_RESAMPLED_AUDIO_DIRS[0] \/ \"train_mod.csv\")","d1fabc2b":"train.head().T","36556f1b":"if not TEST_AUDIO_DIR.exists():\n    TEST_AUDIO_DIR = INPUT_ROOT \/ \"birdcall-check\" \/ \"test_audio\"\n    test = pd.read_csv(INPUT_ROOT \/ \"birdcall-check\" \/ \"test.csv\")\nelse:\n    test = pd.read_csv(RAW_DATA \/ \"test.csv\")","194bdbec":"test.head().T","369dc10f":"settings_str = \"\"\"\nglobals:\n  seed: 1213\n  device: cuda\n  num_epochs: 40\n  output_dir: \/kaggle\/training_output\/\n  use_fold: 0\n  target_sr: 32000\n\ndataset:\n  name: SpectrogramDataset\n  params:\n    img_size: 256\n    melspectrogram_parameters:\n      n_mels: 128\n      fmin: 20\n      fmax: 16000\n    \nsplit:\n  name: StratifiedKFold\n  params:\n    n_splits: 5\n    random_state: 42\n    shuffle: True\n\nloader:\n  train:\n    batch_size: 32\n    shuffle: True\n    num_workers: 2\n    pin_memory: True\n    drop_last: True\n  val:\n    batch_size: 48\n    shuffle: False\n    num_workers: 2\n    pin_memory: True\n    drop_last: False\n\nmodel:\n  name: E_net\n  params:\n    pretrained: True\n    n_classes: 264\n\nloss:\n  name: BCEWithLogitsLoss\n  params: {}\n\noptimizer:\n  name: Adam\n  params:\n    lr: 0.001\n\nscheduler:\n  name: CosineAnnealingLR\n  params:\n    T_max: 10\n\"\"\"","e437d64e":"settings = yaml.safe_load(settings_str)","4ce2af43":"# if not torch.cuda.is_available():\n#     settings[\"globals\"][\"device\"] = \"cpu\"","244a1c59":"for k, v in settings.items():\n    print(\"[{}]\".format(k))\n    print(v)","2a44ea27":"def resample(ebird_code: str,filename: str, target_sr: int):    \n    audio_dir = TRAIN_AUDIO_DIR\n    resample_dir = TRAIN_RESAMPLED_DIR\n    ebird_dir = resample_dir \/ ebird_code\n    \n    try:\n        y, _ = librosa.load(\n            audio_dir \/ ebird_code \/ filename,\n            sr=target_sr, mono=True, res_type=\"kaiser_fast\")\n\n        filename = filename.replace(\".mp3\", \".wav\")\n        sf.write(ebird_dir \/ filename, y, samplerate=target_sr)\n    except Exception as e:\n        print(e)\n        with open(\"skipped.txt\", \"a\") as f:\n            file_path = str(audio_dir \/ ebird_code \/ filename)\n            f.write(file_path + \"\\n\")","59b5d0bf":"# train_org = train.copy()\n# TRAIN_RESAMPLED_DIR = Path(\"\/kaggle\/processed_data\/train_audio_resampled\")\n# TRAIN_RESAMPLED_DIR.mkdir(parents=True)\n\n# for ebird_code in train.ebird_code.unique():\n#     ebird_dir = TRAIN_RESAMPLED_DIR \/ ebird_code\n#     ebird_dir.mkdir()\n\n# warnings.simplefilter(\"ignore\")\n# train_audio_infos = train[[\"ebird_code\", \"filename\"]].values.tolist()\n# Parallel(n_jobs=NUM_THREAD, verbose=10)(\n#     delayed(resample)(ebird_code, file_name, TARGET_SR) for ebird_code, file_name in train_audio_infos)\n\n# train[\"resampled_sampling_rate\"] = TARGET_SR\n# train[\"resampled_filename\"] = train[\"filename\"].map(\n#     lambda x: x.replace(\".mp3\", \".wav\"))\n# train[\"resampled_channels\"] = \"1 (mono)\"","1768fda4":"BIRD_CODE = {\n    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n}\n\nINV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}","ffa60ec5":"PERIOD = 5\n\ndef mono_to_color(\n    X: np.ndarray, mean=None, std=None,\n    norm_max=None, norm_min=None, eps=1e-6\n):\n    # Stack X as [X,X,X]\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    X = X - mean\n    std = std or X.std()\n    Xstd = X \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\nclass SpectrogramDataset(data.Dataset):\n    def __init__(\n        self,\n        file_list: tp.List[tp.List[str]], img_size=224,\n        waveform_transforms=None, spectrogram_transforms=None, melspectrogram_parameters={}\n    ):\n        self.file_list = file_list  # list of list: [file_path, ebird_code]\n        self.img_size = img_size\n        self.waveform_transforms = waveform_transforms\n        self.spectrogram_transforms = spectrogram_transforms\n        self.melspectrogram_parameters = melspectrogram_parameters\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx: int):\n        wav_path, ebird_code = self.file_list[idx]\n\n        y, sr = sf.read(wav_path)\n\n        if self.waveform_transforms:\n            y = self.waveform_transforms(y)\n        else:\n            len_y = len(y)\n            effective_length = sr * PERIOD\n            if len_y < effective_length:\n                new_y = np.zeros(effective_length, dtype=y.dtype)\n                start = np.random.randint(effective_length - len_y)\n                new_y[start:start + len_y] = y\n                y = new_y.astype(np.float32)\n            elif len_y > effective_length:\n                start = np.random.randint(len_y - effective_length)\n                y = y[start:start + effective_length].astype(np.float32)\n            else:\n                y = y.astype(np.float32)\n\n        melspec = librosa.feature.melspectrogram(y, sr=sr, **self.melspectrogram_parameters)\n        melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n        if self.spectrogram_transforms:\n            melspec = self.spectrogram_transforms(melspec)\n        else:\n            pass\n\n        image = mono_to_color(melspec)\n        height, width, _ = image.shape\n#         image = cv2.resize(image, (int(width * self.img_size \/ height), self.img_size))\n        image = cv2.resize(image,  (self.img_size, self.img_size))\n        image = np.moveaxis(image, 2, 0)\n        image = (image \/ 255.0).astype(np.float32)\n\n#         labels = np.zeros(len(BIRD_CODE), dtype=\"i\")\n        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n        labels[BIRD_CODE[ebird_code]] = 1\n\n        return image, labels","e1dc07ea":"def get_loaders_for_training(\n    args_dataset: tp.Dict, args_loader: tp.Dict,\n    train_file_list: tp.List[str], val_file_list: tp.List[str]\n):\n    # # make dataset\n    train_dataset = SpectrogramDataset(train_file_list, **args_dataset)\n    val_dataset = SpectrogramDataset(val_file_list, **args_dataset)\n    # # make dataloader\n    train_loader = data.DataLoader(train_dataset, **args_loader[\"train\"])\n    val_loader = data.DataLoader(val_dataset, **args_loader[\"val\"])\n    \n    return train_loader, val_loader","00e16dab":"from efficientnet_pytorch import model as enet\npretrained_model = {\n    'efficientnet-b2': '..\/input\/efficientnet-pytorch\/efficientnet-b2-27687264.pth'\n}\n\nenet_type = 'efficientnet-b2'\n\ndevice = torch.device('cuda')","31a29e03":"class enetv2(nn.Module):\n    def __init__(self, backbone):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n\n        self.myfc = nn.Sequential(\n        nn.Linear(self.enet._fc.in_features,512), nn.ReLU(), nn.Dropout(p=0.2),\n        nn.Linear(512, 264))\n        \n        self.enet._fc = nn.Identity()\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        x = self.extract(x)\n#         print(x.shape)\n        x = self.myfc(x)\n        return x","fe041478":"def get_model(args: tp.Dict):\n    model =enetv2(enet_type)\n    model = model.to(device)\n        \n    return model","0a7cd70f":"from tqdm import tqdm_notebook","d51a56ce":"\ndef train_loop(\n    manager, args, model, device,\n    train_loader, optimizer, scheduler, loss_func\n):\n    \"\"\"Run minibatch training loop\"\"\"\n    while not manager.stop_trigger:\n        model.train()\n        bar =tqdm_notebook(enumerate(train_loader), total = len(train_loader))\n        \n        for batch_idx, (data, target) in bar:\n            with manager.run_iteration():\n                data, target = data.to(device), target.to(device)\n                optimizer.zero_grad()\n                output = model(data)\n                loss = loss_func(output, target)\n                ppe.reporting.report({'train\/loss': loss.item()})\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n\ndef eval_for_batch(\n    args, model, device,\n    data, target, loss_func, eval_func_dict={}\n):\n    \"\"\"\n    Run evaliation for valid\n    \n    This function is applied to each batch of val loader.\n    \"\"\"\n    model.eval()\n    data, target = data.to(device), target.to(device)\n    output = model(data)\n    # Final result will be average of averages of the same size\n    val_loss = loss_func(output, target).item()\n    ppe.reporting.report({'val\/loss': val_loss})\n    \n    for eval_name, eval_func in eval_func_dict.items():\n        eval_value = eval_func(output, target).item()\n        ppe.reporting.report({\"val\/{}\".format(eval_aame): eval_value})","46f264f6":"def set_extensions(\n    manager, args, model, device, test_loader, optimizer,\n    loss_func, eval_func_dict={}\n):\n    \"\"\"set extensions for PPE\"\"\"\n        \n    my_extensions = [\n        # # observe, report\n        ppe_extensions.observe_lr(optimizer=optimizer),\n        # ppe_extensions.ParameterStatistics(model, prefix='model'),\n        # ppe_extensions.VariableStatisticsPlot(model),\n        ppe_extensions.LogReport(),\n        ppe_extensions.PlotReport(['train\/loss', 'val\/loss'], 'epoch', filename='loss.png'),\n        ppe_extensions.PlotReport(['lr',], 'epoch', filename='lr.png'),\n        ppe_extensions.PrintReport([\n            'epoch', 'iteration', 'lr', 'train\/loss', 'val\/loss', \"elapsed_time\"]),\n#         ppe_extensions.ProgressBar(update_interval=100),\n\n        # # evaluation\n        (\n            ppe_extensions.Evaluator(\n                test_loader, model,\n                eval_func=lambda data, target:\n                    eval_for_batch(args, model, device, data, target, loss_func, eval_func_dict),\n                progress_bar=True),\n            (1, \"epoch\"),\n        ),\n        # # save model snapshot.\n        (\n            ppe_extensions.snapshot(\n                target=model, filename=\"snapshot_epoch_{.updater.epoch}.pth\"),\n            ppe.training.triggers.MinValueTrigger(key=\"val\/loss\", trigger=(1, 'epoch'))\n        ),\n    ]\n           \n    # # set extensions to manager\n    for ext in my_extensions:\n        if isinstance(ext, tuple):\n            manager.extend(ext[0], trigger=ext[1])\n        else:\n            manager.extend(ext)\n        \n    return manager","0760ec6b":"tmp_list = []\nfor audio_d in TRAIN_RESAMPLED_AUDIO_DIRS:\n    if not audio_d.exists():\n        continue\n    for ebird_d in audio_d.iterdir():\n        if ebird_d.is_file():\n            continue\n        for wav_f in ebird_d.iterdir():\n            tmp_list.append([ebird_d.name, wav_f.name, wav_f.as_posix()])\n            \ntrain_wav_path_exist = pd.DataFrame(\n    tmp_list, columns=[\"ebird_code\", \"resampled_filename\", \"file_path\"])\n\ndel tmp_list\n\ntrain_all = pd.merge(\n    train, train_wav_path_exist, on=[\"ebird_code\", \"resampled_filename\"], how=\"inner\")\n\nprint(train.shape)\nprint(train_wav_path_exist.shape)\nprint(train_all.shape)","a7725f11":"train_all.head()","9e476c7b":"# # for test run\n# test_run_idx = sorted(np.random.choice(len(train_all), len(train_all) \/\/ 10, replace=False))\n# train_all = train_all.iloc[test_run_idx, :].reset_index(drop=True)\n# settings[\"globals\"][\"num_epochs\"] = 20\n# settings[\"scheduler\"][\"params\"][\"T_max\"] = 4","d3ca6533":"skf = StratifiedKFold(**settings[\"split\"][\"params\"])\n\ntrain_all[\"fold\"] = -1\nfor fold_id, (train_index, val_index) in enumerate(skf.split(train_all, train_all[\"ebird_code\"])):\n    train_all.iloc[val_index, -1] = fold_id\n    \n# # check the propotion\nfold_proportion = pd.pivot_table(train_all, index=\"ebird_code\", columns=\"fold\", values=\"xc_id\", aggfunc=len)\nprint(fold_proportion.shape)","a0a8c124":"fold_proportion","1c77e8ec":"use_fold = settings[\"globals\"][\"use_fold\"]\ntrain_file_list = train_all.query(\"fold != @use_fold\")[[\"file_path\", \"ebird_code\"]].values.tolist()\nval_file_list = train_all.query(\"fold == @use_fold\")[[\"file_path\", \"ebird_code\"]].values.tolist()\n\nprint(\"[fold {}] train: {}, val: {}\".format(use_fold, len(train_file_list), len(val_file_list)))","81712889":"set_seed(settings[\"globals\"][\"seed\"])\ndevice = torch.device(settings[\"globals\"][\"device\"])\noutput_dir = Path(settings[\"globals\"][\"output_dir\"])\n\n# # # get loader\ntrain_loader, val_loader = get_loaders_for_training(\n    settings[\"dataset\"][\"params\"], settings[\"loader\"], train_file_list, val_file_list)\n\n# # # get model\nmodel = get_model(settings[\"model\"])\nmodel = model.to(device)\n\n# # # get optimizer\noptimizer = getattr(\n    torch.optim, settings[\"optimizer\"][\"name\"]\n)(model.parameters(), **settings[\"optimizer\"][\"params\"])\n\n# # # get scheduler\nscheduler = getattr(\n    torch.optim.lr_scheduler, settings[\"scheduler\"][\"name\"]\n)(optimizer, **settings[\"scheduler\"][\"params\"])\n\n# # # get loss\nloss_func = getattr(nn, settings[\"loss\"][\"name\"])(**settings[\"loss\"][\"params\"])\n\n# # # create training manager\ntrigger = None\n\nmanager = ppe.training.ExtensionsManager(\n    model, optimizer, settings[\"globals\"][\"num_epochs\"],\n    iters_per_epoch=len(train_loader),\n    stop_trigger=trigger,\n    out_dir=output_dir\n)\n\n# # # set manager extensions\nmanager = set_extensions(\n    manager, settings, model, device,\n    val_loader, optimizer, loss_func,\n)","2aa1fa27":"# # runtraining\ntrain_loop(\n    manager, settings, model, device,\n    train_loader, optimizer, scheduler, loss_func)","cf589704":"del train_loader\ndel val_loader\ndel model\ndel optimizer\ndel scheduler\ndel loss_func\ndel manager\n\ngc.collect()","7752f063":"%%bash\nls \/kaggle\/training_output","65638f7c":"for f_name in [\"log\",\"loss.png\", \"lr.png\"]:\n    shutil.copy(output_dir \/ f_name, f_name)","a84d21b8":"log = pd.read_json(\"log\")\nbest_epoch = log[\"val\/loss\"].idxmin() + 1\nlog.iloc[[best_epoch - 1],]","d52a4e38":"shutil.copy(output_dir \/ \"snapshot_epoch_{}.pth\".format(best_epoch), \"best_model.pth\")","0b453811":"m = get_model({\n    'name': settings[\"model\"][\"name\"],\n    'params': {'pretrained': False, 'n_classes': 264}})\nstate_dict = torch.load('best_model.pth')\nm.load_state_dict(state_dict)","224ecaec":"train_file_list[0]","df8d3725":"temp = SpectrogramDataset(train_file_list[0:5],**settings[\"dataset\"][\"params\"])","7d64fc3c":"img ,lb = temp.__getitem__(0)","9f89fbdc":"# from PIL import Image\n# import numpy as np\n\n# img = Image.fromarray(img.values)\n\nimg.shape","7b0d2ed4":"settings[\"dataset\"][\"params\"]['img_size']=224","63661895":"\n# from matplotlib import pyplot as plt\n# plt.imshow(img, interpolation='nearest')\n# plt.show()","a75916e5":"### Training Utility","956fabc3":"### Dataset\n* forked from: https:\/\/github.com\/koukyo1994\/kaggle-birdcall-resnet-baseline-training\/blob\/master\/src\/dataset.py\n* modified partialy\n","139f684a":"### prepare data","5bfc7773":"# Training EfficientNet For bird_song","797db1ce":"#### get wav file path","70e17c37":"### read data","84db6ce2":"## Definition","7c5975e4":"### define utilities","fae8fc11":"### import libraries","49b93d5d":"### preprocess audio data\n\nCode is forked from: https:\/\/github.com\/koukyo1994\/kaggle-birdcall-resnet-baseline-training\/blob\/master\/input\/birdsong-recognition\/prepare.py\n\nI modified this partially. \n\nHowever, in this notebook, I used uploaded resampled audio because this preprocessing is too heavy for kaggle notebook.","738013f5":"## save results","7fed9035":"## Credits\n\n* https:\/\/www.kaggle.com\/ttahara\/training-birdsong-baseline-resnest50-fast\n\n## Inference Notebook \n\n* https:\/\/www.kaggle.com\/rsinda\/ensemble-resnest50-efficient-net","f3a1363a":"## Training","e1caa36b":"## run training","d9f651e4":"#### split data","3404b066":"### settings"}}