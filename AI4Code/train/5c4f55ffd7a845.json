{"cell_type":{"8b7c61ba":"code","36b360d0":"code","a75bbecb":"code","715be8a7":"code","5b49f16c":"code","bd745921":"code","35737ffa":"code","07a4326f":"code","d28ec485":"code","3815a20d":"code","0163fc1d":"code","2be3b534":"code","062fd2fd":"code","07725c8f":"code","18e7a7fb":"code","0d01bd45":"code","4964e6a0":"code","461d3acb":"markdown","6c015149":"markdown","f5a3ed27":"markdown","abc81a3b":"markdown","2531619f":"markdown","b63b11be":"markdown","3c4cce68":"markdown"},"source":{"8b7c61ba":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tqdm.notebook import tqdm\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","36b360d0":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\ntrain2['toxic'] = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","a75bbecb":"def def1(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    dir_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        dir_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(dir_ids)","715be8a7":"def def2(texts, tokenizer,maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,        \n        return_attention_masks=False, \n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","5b49f16c":"def build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","bd745921":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \",strategy.num_replicas_in_sync)","35737ffa":"AUTO = tf.data.experimental.AUTOTUNE\n\nfrom kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","07a4326f":"epochs = 5\nbatch_size = 16 * strategy.num_replicas_in_sync\nmax_len = 192\nMODEL='jplu\/tf-xlm-roberta-large'","d28ec485":"tokenizer = AutoTokenizer.from_pretrained(MODEL)","3815a20d":"train = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000)\n])","0163fc1d":"x_train = def2(train.comment_text.values, tokenizer,maxlen=max_len)\nx_valid = def2(valid.comment_text.values, tokenizer,maxlen=max_len)\nx_test = def2(test.content.values, tokenizer,maxlen=max_len)\n\ny_train = train['toxic'].values\ny_valid = valid['toxic'].values","2be3b534":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalidation_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","062fd2fd":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=192)","07725c8f":"model.summary()","18e7a7fb":"steps = x_train.shape[0] \/\/ batch_size\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=steps,\n    validation_data=validation_dataset,\n    epochs=1\n)","0d01bd45":"steps = x_valid.shape[0] \/\/ batch_size\ntrain_history_2 = model.fit(\n    validation_dataset.repeat(),\n    steps_per_epoch=steps,\n    epochs=10\n)","4964e6a0":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","461d3acb":"# Buiding a sequential model using Accuracy as metrics and Binary_Crossentropy as Loss function. ","6c015149":"# Training my XLM-Roberta_Model with english only data.","f5a3ed27":"Note: It took me more than 15 minutes for single epoch training.","abc81a3b":"# Training my model with multiple_language data!","2531619f":"# Importing necessary datasets and arrange them into Train, test,and Validation usecases","b63b11be":"# Importing necesaary libraries and packages","3c4cce68":"# Start the TPU Accelerator!"}}