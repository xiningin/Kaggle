{"cell_type":{"a67c6111":"code","da85e141":"code","08bc17dd":"code","37e24a4b":"code","2f112cb1":"code","876407ac":"code","22a2a483":"code","334bdb76":"code","c6e9e124":"code","4a687893":"code","f819d8fc":"code","612c9146":"code","c1f262a8":"markdown","b3e1737e":"markdown"},"source":{"a67c6111":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da85e141":"import numpy as np # \u5bfc\u5165NumPy\nimport pandas as pd # \u5bfc\u5165pandas\ndataset = pd.read_csv('..\/input\/customer-cluster\/Customers Cluster.csv')\ndataset.head() # \u663e\u793a\u4e00\u4e9b\u6570\u636e\n# \u53ea\u9488\u5bf9\u4e24\u4e2a\u7279\u5f81\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u65b9\u4fbf\u4e8c\u7ef4\u7684\u5c55\u793a\nX = dataset.iloc[:, [3,4]].values","08bc17dd":"from sklearn.cluster import KMeans # \u5bfc\u5165\u805a\u7c7b\u6a21\u578b\ncost=[] # \u521d\u59cb\u5316\u635f\u5931(\u8ddd\u79bb)\u503c\nfor i in range(1,11): # \u5c1d\u8bd5\u4e0d\u540c\u7684K\u503c\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(X)\n    cost.append(kmeans.inertia_) #inertia_ \u662f\u6211\u4eec\u9009\u62e9\u7684\u65b9\u6cd5\uff0c\u5176\u4f5c\u7528\u76f8\u5f53\u4e8e\u635f\u5931\u51fd\u6570","37e24a4b":"import matplotlib.pyplot as plt # \u5bfc\u5165matplotlib\u5e93 \nimport seaborn as sns  # \u5bfc\u5165seaborn\u5e93\n# \u7ed8\u5236ELBOW\uff08\u624b\u8098\uff09\u56fe\u627e\u5230\u6700\u4f73K\u503c \nplt.plot(range(1,11), cost)\nplt.title('The Elbow Method')\nplt.xlabel('No of clusters')\nplt.ylabel('Cost')\nplt.show()","2f112cb1":"kmeansmodel = KMeans(n_clusters= 4, init='k-means++') # \u9009\u62e94\u4f5c\u4e3a\u805a\u7c7b\u4e2a\u6570\ny_kmeans= kmeansmodel.fit_predict(X) # \u8fdb\u884c\u805a\u7c7b\u7684\u62df\u5408\u548c\u5206\u7c7b","876407ac":"# \u4e0b\u9762\u628a\u5206\u597d\u7684\u805a\u7c7b\u53ef\u89c6\u5316\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], \n            s = 100, c = 'cyan', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], \n            s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], \n            s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], \n            s = 100, c = 'red', label = 'Cluster 4')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n            s = 200, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.show()","22a2a483":"import numpy as np # \u5bfc\u5165NumPy\nimport pandas as pd # \u5bfc\u5165pandas\ndataset = pd.read_csv('..\/input\/customer-cluster\/Customers Cluster.csv')\ndataset.head() # \u663e\u793a\u4e00\u4e9b\u6570\u636e","334bdb76":"# \u53ea\u9488\u5bf9\u4e24\u4e2a\u7279\u5f81\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u65b9\u4fbf\u4e8c\u7ef4\u7684\u5c55\u793a\nX= dataset.iloc[:, [2,4]].values","c6e9e124":"from sklearn.cluster import KMeans # \u5bfc\u5165\u805a\u7c7b\u6a21\u578b\ncost=[] # \u521d\u59cb\u5316\u635f\u5931(\u8ddd\u79bb)\u503c\nfor i in range(1,11): # \u5c1d\u8bd5\u4e0d\u540c\u7684K\u503c\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(X) # \u62df\u5408\u6a21\u578b\n    cost.append(kmeans.inertia_) #inertia_\u662f\u5ea6\u91cf\u6570\u636e\u70b9\u5230\u805a\u7c7b\u4e2d\u5fc3\u7684\u5ea6\u91cf\u516c\u5f0f","4a687893":"import matplotlib.pyplot as plt # \u5bfc\u5165Matplotlib\nimport seaborn as sns  # \u5bfc\u5165Seaborn\n# \u7ed8\u5236\u624b\u8098\u56fe \nplt.plot(range(1,11), cost)\nplt.title('The Elbow Method')\nplt.xlabel('no of clusters')\nplt.ylabel('Cost')\nplt.show()","f819d8fc":"# \u6784\u5efa\u805a\u7c7b\u6a21\u578b\nkmeansmodel = KMeans(n_clusters= 4, init='k-means++') # \u9009\u62e94\u4f5c\u4e3a\u805a\u7c7b\u4e2a\u6570\ny_kmeans= kmeansmodel.fit_predict(X) # \u8fdb\u884c\u805a\u7c7b\u7684\u62df\u5408\u548c\u5206\u7c7b","612c9146":"# \u628a\u5206\u597d\u7684\u805a\u7c7b\u53ef\u89c6\u5316\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n# plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.show()","c1f262a8":"# K-means - Customer Cluster","b3e1737e":"# K-means - Age and Spending"}}