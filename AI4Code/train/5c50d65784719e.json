{"cell_type":{"e200068a":"code","3e3b46ac":"code","fbce2583":"code","a63154b6":"code","062a1065":"code","62972ef1":"code","958dbb9d":"code","80ad8fd7":"code","8fbd80ac":"code","fd5e086c":"code","0044dc11":"code","b7a1b7b1":"code","940003c0":"markdown","46e89cfd":"markdown","b3978073":"markdown","2cdc11c3":"markdown","c4655f59":"markdown","3c969a8d":"markdown"},"source":{"e200068a":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","3e3b46ac":"dftrain = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")","fbce2583":"Xcheck=np.array(dftrain.iloc[:,1:])\nycheck=np.array(dftrain.iloc[:,0])\n\n# scale the X vector, dividing for the maximum value (i.e. 255)\nX=Xcheck\/255\n\n# convert y in dummy variable. This is necessary as output of the neural network\ny=np.array(pd.get_dummies(ycheck))","a63154b6":"# reshape of the vector, to visualize the image\n\ndef numimg2(t,X):\n    vecreshape=np.reshape(X[t,:],[28,28])\n    return vecreshape","062a1065":"# visuale a number\n\nprint(ycheck[5016])\n\nplt.imshow(numimg2(5016,X),cmap='gray')\nplt.colorbar()\nplt.show()","62972ef1":"# beginning of deep learning\n\nncols=X.shape[1]       # number of elements for the input layer\nnumutput=10            # number of elements for output layer","958dbb9d":"# I use train_test_split function to split in 80% training data and 20% test data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","80ad8fd7":"import keras\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\nfrom keras.layers.core import Dense, Dropout, Activation","8fbd80ac":"\nmodel = Sequential()\n\n# input and 1th hidden layer with 100 neurons. \nmodel.add(Dense(500, input_shape=(ncols,)))\nmodel.add(Activation('relu'))                            \nmodel.add(Dropout(0.2))   # setting 20% of neurons to 0 (randomly). This should help in avoiding overfitting\n\n# 2th hidden layer with 200 neurons. \nmodel.add(Dense(500))\nmodel.add(Activation('relu'))                            \nmodel.add(Dropout(0.2))\n\n# output layer\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\n# Here I compile the model\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# I set 100 epochs as default, with an Early Stop in case of no improvement after 5 epochs\nearly_stopping_monitor = EarlyStopping(patience=5)\n\nhistory = model.fit(X_train,y_train,\n                    batch_size=128, epochs=100,\n                    validation_data=(X_test,y_test),callbacks=[early_stopping_monitor])\n\nprint(\"Loss function: \" + model.loss)","fd5e086c":"model.summary()","0044dc11":"# importing the validation dest\ndftest = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_test.csv\")\n\nX_val_test=np.array(dftest.iloc[:,1:])\/255\ny_val_test=np.array(dftest.iloc[:,0])\n\n#prediction of the model (probability)\nresult=np.round(model.predict(X_val_test),1)\n\n#prediction of the model (I select the number with the maximum probability)\nprediction=np.argmax(result,axis=1)\n\n# build the confusion matrix\npd.DataFrame(confusion_matrix(y_val_test,prediction))","b7a1b7b1":"accuracy=round(np.sum(y_val_test==prediction)\/len(prediction)*100,1)\n\nprint('The accuracy of the model on the validation data is: '+str(accuracy)+'%')","940003c0":"### image visualization","46e89cfd":"### feature manipulation","b3978073":"### Sequential Neural Network, with 2 hidden layers","2cdc11c3":"### import data, using Pandas","c4655f59":"### Neural Network: building and training","3c969a8d":"### Neural Network: performance on the validation test"}}