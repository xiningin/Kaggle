{"cell_type":{"1c09b9c0":"code","2f11270e":"code","52236ea2":"code","a9f3eb38":"code","acb5b83e":"code","334272c8":"code","261b0088":"code","056f7dcb":"code","18ff8b53":"code","6766a75c":"code","943de617":"code","f5c42c11":"code","ea07f0d0":"code","bb518311":"code","5997a893":"code","61c8e1d6":"code","c79384b7":"code","62839930":"code","f72aeff3":"code","f4f9e6fc":"code","e226826a":"code","3f63aa55":"markdown","aeb883b3":"markdown","c1bd5229":"markdown","c9646282":"markdown","435fb348":"markdown","dfb0664b":"markdown","f61b64ab":"markdown","844d4063":"markdown"},"source":{"1c09b9c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f11270e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report , confusion_matrix","52236ea2":"dataset=pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")","a9f3eb38":"dataset.head()","acb5b83e":"dataset.keys()","334272c8":"dataset.info()","261b0088":"dataset['Species'].unique()  ##there are three types of the species","056f7dcb":"sns.FacetGrid(dataset , hue='Species', size=5).map(plt.scatter, 'SepalLengthCm','SepalWidthCm').add_legend()","18ff8b53":"sns.pairplot(dataset, 'Species')","6766a75c":"## check the correlation of the feature \nsns.heatmap(dataset.corr(), cmap='viridis', annot=True)","943de617":"#\ndataset.drop('Id', axis=1, inplace=True)","f5c42c11":"dataset.head()  ## dependet feature","ea07f0d0":"y=dataset['Species']     \nx=dataset.iloc[:, :-1]","bb518311":"x.shape, y.shape","5997a893":"x_train, x_test , y_train ,y_test =train_test_split(x, y, test_size=0.3, random_state=0)\n","61c8e1d6":"## Logistic regression\nlogistic=LogisticRegression()\nlogistic.fit(x_train ,y_train)\nprint(\"training set score : \", logistic.score(x_train ,y_train))\nprint(\"testing set score :\", logistic.score(x_test, y_test))","c79384b7":"predict1=logistic.predict(x_test)\nprint(\"confusion matrix :\" ,confusion_matrix(y_test, predict1))\nprint(\"classification report :\", classification_report(y_test , predict1))","62839930":"## check for the another model\ntree=DecisionTreeClassifier(max_depth=3)\ntree.fit(x_train, y_train)\nprint(tree.score(x_train ,y_train))\nprint(tree.score(x_test , y_test))","f72aeff3":"## check for ensemble technique\nrandom_forest=RandomForestClassifier(max_depth=3)\nrandom_forest.fit(x_train ,y_train)\nprint(random_forest.score(x_train ,y_train))\nprint(random_forest.score(x_test , y_test))","f4f9e6fc":"## decision tree and logistic regression give the better accuracy and recall score then we choose any one becoz they give the same score","e226826a":"print(predict1)","3f63aa55":"## import the libraray","aeb883b3":"### load the dataset","c1bd5229":"#### same testing set score","c9646282":"## apply the machine learning model","435fb348":"### bivariate feature","dfb0664b":"## check for the multivariate feature we use the pairplot","f61b64ab":"### a lot of overlapping in iris-versicolor and iris-virginica\n","844d4063":"### split the dataset into training and testing "}}