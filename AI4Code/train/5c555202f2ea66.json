{"cell_type":{"6900f7c7":"code","26678c91":"code","d1d36a3d":"code","c2c82665":"code","07fba6ca":"code","867d5b41":"code","3fc62e3e":"code","16607d6c":"code","9abe6bee":"code","6486f202":"code","9a93cf44":"code","24527130":"code","830a13e4":"code","cbb6bec4":"markdown","562cb996":"markdown","6c559d95":"markdown","d72f33f3":"markdown","67e7eb6b":"markdown","7483f639":"markdown","44c452a6":"markdown","e55fe466":"markdown","c427f0f7":"markdown","1a5019b6":"markdown"},"source":{"6900f7c7":"%load_ext tensorboard","26678c91":"import datetime\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout","d1d36a3d":"import cv2\nimport urllib\nimport requests\nimport PIL.Image\nimport numpy as np\nfrom bs4 import BeautifulSoup\n\n#ship synset\npage = requests.get(\"http:\/\/www.image-net.org\/api\/text\/imagenet.synset.geturls?wnid=n04194289\")\nsoup = BeautifulSoup(page.content, 'html.parser')\n#bicycle synset\nbikes_page = requests.get(\"http:\/\/www.image-net.org\/api\/text\/imagenet.synset.geturls?wnid=n02834778\")\nbikes_soup = BeautifulSoup(bikes_page.content, 'html.parser')\n\nstr_soup=str(soup)\nsplit_urls=str_soup.split('\\r\\n')\n\nbikes_str_soup=str(bikes_soup)\nbikes_split_urls=bikes_str_soup.split('\\r\\n')","c2c82665":"os.mkdir('.\/content')\nos.mkdir('.\/content\/train')\nos.mkdir('.\/content\/train\/ships')\nos.mkdir('.\/content\/train\/bikes')\nos.mkdir('.\/content\/validation')\nos.mkdir('.\/content\/validation\/ships')\nos.mkdir('.\/content\/validation\/bikes')","07fba6ca":"img_rows, img_cols = 32, 32\ninput_shape = (img_rows, img_cols, 3)\n\ndef url_to_image(url):\n    resp = urllib.request.urlopen(url)\n    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n    return image\n\nn_of_training_images=100\nfor progress in tqdm(range(n_of_training_images)):\n    if not split_urls[progress] == None:\n        try:\n            I = url_to_image(split_urls[progress])\n            if (len(I.shape))==3:\n                save_path = '.\/content\/train\/ships\/img'+str(progress)+'.jpg'\n                cv2.imwrite(save_path,I)\n        except:\n            None\n\nfor progress in tqdm(range(n_of_training_images)):\n    if not bikes_split_urls[progress] == None:\n        try:\n            I = url_to_image(bikes_split_urls[progress])\n            if (len(I.shape))==3:\n                save_path = '.\/content\/train\/bikes\/img'+str(progress)+'.jpg'\n                cv2.imwrite(save_path,I)\n        except:\n            None\n\n\nfor progress in tqdm(range(50)):\n    if not split_urls[progress] == None:\n        try:\n            I = url_to_image(split_urls[n_of_training_images+progress])\n            if (len(I.shape))==3:\n                save_path = '.\/content\/validation\/ships\/img'+str(progress)+'.jpg'\n                cv2.imwrite(save_path,I)\n        except:\n            None\n\n\nfor progress in tqdm(range(50)):\n    if not bikes_split_urls[progress] == None:\n        try:\n            I = url_to_image(bikes_split_urls[n_of_training_images+progress])\n            if (len(I.shape))==3:\n                save_path = '.\/content\/validation\/bikes\/img'+str(progress)+'.jpg'\n                cv2.imwrite(save_path,I)\n        except:\n            None","867d5b41":"# AlexNet model\nclass AlexNet(Sequential):\n    def __init__(self, input_shape, num_classes):\n        super().__init__()\n\n        self.add(Conv2D(96, kernel_size=(11,11), strides= 4,\n                        padding= 'valid', activation= 'relu',\n                        input_shape= input_shape,\n                        kernel_initializer= 'he_normal'))\n        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n                              padding= 'valid', data_format= None))\n\n        self.add(Conv2D(256, kernel_size=(5,5), strides= 1,\n                        padding= 'same', activation= 'relu',\n                        kernel_initializer= 'he_normal'))\n        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n                              padding= 'valid', data_format= None)) \n\n        self.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n                        padding= 'same', activation= 'relu',\n                        kernel_initializer= 'he_normal'))\n\n        self.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n                        padding= 'same', activation= 'relu',\n                        kernel_initializer= 'he_normal'))\n\n        self.add(Conv2D(256, kernel_size=(3,3), strides= 1,\n                        padding= 'same', activation= 'relu',\n                        kernel_initializer= 'he_normal'))\n\n        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n                              padding= 'valid', data_format= None))\n\n        self.add(Flatten())\n        self.add(Dense(4096, activation= 'relu'))\n        self.add(Dense(4096, activation= 'relu'))\n        self.add(Dense(1000, activation= 'relu'))\n        self.add(Dense(num_classes, activation= 'softmax'))\n\n        self.compile(optimizer= tf.keras.optimizers.Adam(0.001),\n                    loss='categorical_crossentropy',\n                    metrics=['accuracy'])","3fc62e3e":"num_classes = 2\nmodel = AlexNet((227, 227, 3), num_classes)","16607d6c":"# training parameters\nEPOCHS = 100\nBATCH_SIZE = 32\nimage_height = 227\nimage_width = 227\ntrain_dir = \".\/content\/train\"\nvalid_dir = \".\/content\/validation\"\nmodel_dir = \".\/my_model.h5\"","9abe6bee":"train_datagen = ImageDataGenerator(\n                  rescale=1.\/255,\n                  rotation_range=10,\n                  width_shift_range=0.1,\n                  height_shift_range=0.1,\n                  shear_range=0.1,\n                  zoom_range=0.1)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size=(image_height, image_width),\n                                                    color_mode=\"rgb\",\n                                                    batch_size=BATCH_SIZE,\n                                                    seed=1,\n                                                    shuffle=True,\n                                                    class_mode=\"categorical\")\n\nvalid_datagen = ImageDataGenerator(rescale=1.0\/255.0)\nvalid_generator = valid_datagen.flow_from_directory(valid_dir,\n                                                    target_size=(image_height, image_width),\n                                                    color_mode=\"rgb\",\n                                                    batch_size=BATCH_SIZE,\n                                                    seed=7,\n                                                    shuffle=True,\n                                                    class_mode=\"categorical\"\n                                                    )\ntrain_num = train_generator.samples\nvalid_num = valid_generator.samples","6486f202":"os.mkdir('.\/logs')\nos.mkdir('.\/logs\/fit')","9a93cf44":"log_dir=\".\/logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\ncallback_list = [tensorboard_callback]\n\n# start training\nmodel.fit(train_generator,\n                    epochs=EPOCHS,\n                    steps_per_epoch=train_num \/\/ BATCH_SIZE,\n                    validation_data=valid_generator,\n                    validation_steps=valid_num \/\/ BATCH_SIZE,\n                    callbacks=callback_list,\n                    verbose=0)\nmodel.summary()\n\n# save the whole model\nmodel.save(model_dir)","24527130":"%tensorboard --logdir logs\/fit","830a13e4":"class_names = ['bike', 'ship']\n\nx_valid, label_batch  = next(iter(valid_generator))\n\nprediction_values = model.predict_classes(x_valid)\n\n# set up the figure\nfig = plt.figure(figsize=(10, 6))\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n# plot the images: each image is 227x227 pixels\nfor i in range(8):\n    ax = fig.add_subplot(2, 4, i + 1, xticks=[], yticks=[])\n    ax.imshow(x_valid[i,:],cmap=plt.cm.gray_r, interpolation='nearest')\n  \n    if prediction_values[i] == np.argmax(label_batch[i]):\n        # label the image with the blue text\n        ax.text(3, 17, class_names[prediction_values[i]], color='blue', fontsize=14)\n    else:\n        # label the image with the red text\n        ax.text(3, 17, class_names[prediction_values[i]], color='red', fontsize=14)","cbb6bec4":"## Why does AlexNet achieve better results?\n\n### **Relu activation function is used:**\n- Relu function: f (x) = max (0, x)\n\n![alex1](https:\/\/raw.githubusercontent.com\/blurred-machine\/Data-Science\/master\/Deep%20Learning%20SOTA\/img\/alex512.png)\n\n- ReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based networks. The following figure shows the number of iterations for a four-layer convolutional network based on CIFAR-10 that reached 25% training error in tanh and ReLU:\n\n![alex1](https:\/\/raw.githubusercontent.com\/blurred-machine\/Data-Science\/master\/Deep%20Learning%20SOTA\/img\/alex612.png)\n\n### **Standardization (Local Response Normalization):**\n- After using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the tanh and sigmoid functions, so a normalization will usually be done after ReLU, and the LRU is a steady proposal (Not sure here, it should be proposed?) One method in neuroscience is called \"Lateral inhibition\", which talks about the effect of active neurons on its surrounding neurons.\n\n![alex1](https:\/\/raw.githubusercontent.com\/blurred-machine\/Data-Science\/master\/Deep%20Learning%20SOTA\/img\/alex3.jpg)\n\n\n### **Dropout:**\n- Dropout is also a concept often said, which can effectively prevent overfitting of neural networks. Compared to the general linear model, a regular method is used to prevent the model from overfitting. In the neural network, Dropout is implemented by modifying the structure of the neural network itself. For a certain layer of neurons, randomly delete some neurons with a defined probability, while keeping the individuals of the input layer and output layer neurons unchanged, and then update the parameters according to the learning method of the neural network. In the next iteration, rerandom Remove some neurons until the end of training.\n\n\n![alex1](https:\/\/raw.githubusercontent.com\/blurred-machine\/Data-Science\/master\/Deep%20Learning%20SOTA\/img\/alex4.jpg)\n\n\n### **Enhanced Data (Data Augmentation):**\n\n**In deep learning, when the amount of data is not large enough, there are generally 4 solutions:**\n\n- Data augmentation- artificially increase the size of the training set-create a batch of \"new\" data from existing data by means of translation, flipping, noise\n\n- Regularization\u2014\u2014The relatively small amount of data will cause the model to overfit, making the training error small and the test error particularly large. By adding a regular term after the Loss Function , the overfitting can be suppressed. The disadvantage is that a need is introduced Manually adjusted hyper-parameter.\n\n- Dropout- also a regularization method. But different from the above, it is achieved by randomly setting the output of some neurons to zero\n\n- Unsupervised Pre-training- use Auto-Encoder or RBM's convolution form to do unsupervised pre-training layer by layer, and finally add a classification layer to do supervised Fine-Tuning\n\n\n\n\n","562cb996":"## I hope you enjoyed reading it :)<br>Give this notebook an upvote up if you like it.\n\n### <a href=\"https:\/\/www.kaggle.com\/general\/166349\">Read the complete series of SOTA Deep Learning Model Architectures here! <\/a>","6c559d95":"### This notebook is a part of state-of-the-art Deep Learning Models Discussion.<br> This is third notebook in the series.<br>\n> Visit here: [SOTA Papers Discussion](https:\/\/www.kaggle.com\/general\/166349)<br>\n> Sample data used for demo: [Dataset](http:\/\/www.image-net.org\/api\/text\/imagenet.synset.geturls?wnid=n04194289)<br>\n> Read my other notebooks: [Notebooks](https:\/\/www.kaggle.com\/blurredmachine\/notebooks)<br>\n> SOTA Model Notebooks have been released earlier. [View previous notebooks](https:\/\/www.kaggle.com\/general\/166349)<br>","d72f33f3":"#### In this post I showed how to implement AlexNet in TensorFlow 2.0. I have used just a part of the ImageNet dataset, and that is why we did not get the best results. Since the aim of this notebook is for learning and not to achieve best accuracy, for better accuracy, more data and longer training time is required.","67e7eb6b":"### Introduction\nAlexNet was designed by **Hinton**, winner of the ***2012*** ***ImageNet competition***, and his student Alex Krizhevsky. It was also after that year that more and deeper neural networks were proposed, such as the excellent vgg, GoogleLeNet. Its official data model has an accuracy rate of **57.1%** and top 1-5 reaches **80.2%**. This is already quite outstanding for traditional machine learning classification algorithms.\n\n\n![title](https:\/\/raw.githubusercontent.com\/blurred-machine\/Data-Science\/master\/Deep%20Learning%20SOTA\/img\/alexnet.png)\n\n\n![title](https:\/\/raw.githubusercontent.com\/blurred-machine\/Data-Science\/master\/Deep%20Learning%20SOTA\/img\/alexnet2.png)","7483f639":"# Code Implementation","44c452a6":"### AlexNet Model Architecture:","e55fe466":"## AlexNet Complete Architecture","c427f0f7":"### Image Preprocessing:","1a5019b6":"#### The following table below explains the network structure of AlexNet:\n\n<table>\n<thead>\n\t<tr>\n\t\t<th>Size \/ Operation<\/th>\n\t\t<th>Filter<\/th>\n\t\t<th>Depth<\/th>\n\t\t<th>Stride<\/th>\n\t\t<th>Padding<\/th>\n\t\t<th>Number of Parameters<\/th>\n\t\t<th>Forward Computation<\/th>\n\t<\/tr>\n<\/thead>\n<tbody>\n\t<tr>\n\t\t<td>3* 227 * 227<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Conv1 + Relu<\/td>\n\t\t<td>11 * 11<\/td>\n\t\t<td>96<\/td>\n\t\t<td>4<\/td>\n\t\t<td><\/td>\n\t\t<td>(11*11*3 + 1) * 96=34944<\/td>\n\t\t<td>(11*11*3 + 1) * 96 * 55 * 55=105705600<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>96 * 55 * 55<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Max Pooling<\/td>\n\t\t<td>3 * 3<\/td>\n\t\t<td><\/td>\n\t\t<td>2<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>96 * 27 * 27<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Norm<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Conv2 + Relu<\/td>\n\t\t<td>5 * 5<\/td>\n\t\t<td>256<\/td>\n\t\t<td>1<\/td>\n\t\t<td>2<\/td>\n\t\t<td>(5 * 5 * 96 + 1) * 256=614656<\/td>\n\t\t<td>(5 * 5 * 96 + 1) * 256 * 27 * 27=448084224<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>256 * 27 * 27<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Max Pooling<\/td>\n\t\t<td>3 * 3<\/td>\n\t\t<td><\/td>\n\t\t<td>2<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>256 * 13 * 13<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Norm<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Conv3 + Relu<\/td>\n\t\t<td>3 * 3<\/td>\n\t\t<td>384<\/td>\n\t\t<td>1<\/td>\n\t\t<td>1<\/td>\n\t\t<td>(3 * 3 * 256 + 1) * 384=885120<\/td>\n\t\t<td>(3 * 3 * 256 + 1) * 384 * 13 * 13=149585280<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>384 * 13 * 13<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Conv4 + Relu<\/td>\n\t\t<td>3 * 3<\/td>\n\t\t<td>384<\/td>\n\t\t<td>1<\/td>\n\t\t<td>1<\/td>\n\t\t<td>(3 * 3 * 384 + 1) * 384=1327488<\/td>\n\t\t<td>(3 * 3 * 384 + 1) * 384 * 13 * 13=224345472<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>384 * 13 * 13<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Conv5 + Relu<\/td>\n\t\t<td>3 * 3<\/td>\n\t\t<td>256<\/td>\n\t\t<td>1<\/td>\n\t\t<td>1<\/td>\n\t\t<td>(3 * 3 * 384 + 1) * 256=884992<\/td>\n\t\t<td>(3 * 3 * 384 + 1) * 256 * 13 * 13=149563648<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>256 * 13 * 13<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Max Pooling<\/td>\n\t\t<td>3 * 3<\/td>\n\t\t<td><\/td>\n\t\t<td>2<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>256 * 6 * 6<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Dropout (rate 0.5)<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>FC6 + Relu<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td>256 * 6 * 6 * 4096=37748736<\/td>\n\t\t<td>256 * 6 * 6 * 4096=37748736<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>4096<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Dropout (rate 0.5)<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>FC7 + Relu<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td>4096 * 4096=16777216<\/td>\n\t\t<td>4096 * 4096=16777216<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>4096<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>FC8 + Relu<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td>4096 * 1000=4096000<\/td>\n\t\t<td>4096 * 1000=4096000<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>1000 classes<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Overall<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td>62369152=62.3 million<\/td>\n\t\t<td>1135906176=1.1 billion<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td>Conv VS FC<\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td><\/td>\n\t\t<td>Conv:3.7million (6%) , FC: 58.6 million  (94% )<\/td>\n\t\t<td>Conv: 1.08 billion (95%) , FC: 58.6 million (5%)<\/td>\n\t<\/tr>\n<\/tbody>\n<\/table>\n\n\n\n"}}