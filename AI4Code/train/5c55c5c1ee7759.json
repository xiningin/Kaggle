{"cell_type":{"169a3004":"code","38de246b":"code","fb741271":"code","66009ccd":"code","8af6a839":"code","ef7aaaab":"code","1e557c4f":"code","385e4404":"code","192a765b":"code","0e43ce10":"code","6644e734":"code","99ba25c6":"code","a94bcf66":"code","a068bb3f":"markdown","2e001e89":"markdown","66e9aac2":"markdown","fac1e3c3":"markdown","44465025":"markdown","84fcc5a8":"markdown"},"source":{"169a3004":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","38de246b":"def logloss(y,yp):\n    yp = np.clip(yp,1e-5,1-1e-5)\n    return -y*np.log(yp)-(1-y)*np.log(1-yp)\n    \ndef reverse(tr,te):\n    reverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n                22,24,25,26,27,41,29,\n                32,35,37,40,48,49,47,\n                55,51,52,53,60,61,62,103,65,66,67,69,\n                70,71,74,78,79,\n                82,84,89,90,91,94,95,96,97,99,\n                105,106,110,111,112,118,119,125,128,\n                130,133,134,135,137,138,\n                140,144,145,147,151,155,157,159,\n                161,162,163,164,167,168,\n                170,171,173,175,176,179,\n                180,181,184,185,187,189,\n                190,191,195,196,199]\n    reverse_list = ['var_%d'%i for i in reverse_list]\n    for col in reverse_list:\n        tr[col] = tr[col]*(-1)\n        te[col] = te[col]*(-1)\n    return tr,te\n\ndef scale(tr,te):\n    for col in tr.columns:\n        if col.startswith('var_'):\n            mean,std = tr[col].mean(),tr[col].std()\n            tr[col] = (tr[col]-mean)\/std\n            te[col] = (te[col]-mean)\/std\n    return tr,te\n\ndef getp_vec_sum(x,x_sort,y,std,c=0.5):\n    # x is sorted\n    left = x - std\/c\n    right = x + std\/c\n    p_left = np.searchsorted(x_sort,left)\n    p_right = np.searchsorted(x_sort,right)\n    p_right[p_right>=y.shape[0]] = y.shape[0]-1\n    p_left[p_left>=y.shape[0]] = y.shape[0]-1\n    return (y[p_right]-y[p_left])\n\ndef get_pdf(tr,col,x_query=None,smooth=3):\n    std = tr[col].std()\n    df = tr.groupby(col).agg({'target':['sum','count']})\n    cols = ['sum_y','count_y']\n    df.columns = cols\n    df = df.reset_index()\n    df = df.sort_values(col)\n    y,c = cols\n    \n    df[y] = df[y].cumsum()\n    df[c] = df[c].cumsum()\n    \n    if x_query is None:\n        rmin,rmax,res = -5.0, 5.0, 501\n        x_query = np.linspace(rmin,rmax,res)\n    \n    dg = pd.DataFrame()\n    tm = getp_vec_sum(x_query,df[col].values,df[y].values,std,c=smooth)\n    cm = getp_vec_sum(x_query,df[col].values,df[c].values,std,c=smooth)+1\n    dg['res'] = tm\/cm\n    dg.loc[cm<500,'res'] = 0.1\n    return dg['res'].values\n\ndef get_pdfs(tr):\n    y = []\n    for i in range(200):\n        name = 'var_%d'%i\n        res = get_pdf(tr,name)\n        y.append(res)\n    return np.vstack(y)\n\ndef print_corr(corr_mat,col,bar=0.95):\n    #print(col)\n    cols = corr_mat.loc[corr_mat[col]>bar,col].index.values\n    cols_ = ['var_%s'%(i.split('_')[-1]) for i in cols]\n    print(col,\"#####\", cols)\n    return cols","fb741271":"%%time\npath = '..\/input\/'\ntr = pd.read_csv('%s\/train.csv'%path)\nte = pd.read_csv('%s\/test.csv'%path)","66009ccd":"%%time\ntr,te = reverse(tr,te)\ntr,te = scale(tr,te)","8af6a839":"%%time\nprob = get_pdf(tr,'var_0')\nplt.plot(prob)","ef7aaaab":"%%time\npdfs = get_pdfs(tr)","1e557c4f":"%%time\ndf_pdf = pd.DataFrame(pdfs.T,columns=['var_prob_%d'%i for i in range(200)])\ncorr_mat = df_pdf.corr(method='pearson')","385e4404":"corr_mat.head()","192a765b":"plt.figure(figsize=(15,10))\nsns.heatmap(corr_mat, cmap='RdBu_r', center=0.0) \nplt.title('PDF Correlations',fontsize=16)\nplt.show() ","0e43ce10":"plt.figure(figsize=(10,5))\nplt.plot(pdfs[0],color='b',label='var_0')\nplt.plot(pdfs[2],color='r',label='var_2')\nplt.legend(loc='upper right')","6644e734":"cols = print_corr(corr_mat,'var_prob_12')\ncorr_mat.loc[cols,cols]","99ba25c6":"groups =[]\nskip_list = []\nfor i in range(0,200):\n    if(i not in skip_list):\n        cols = print_corr(corr_mat,'var_prob_'+str(i))\n        if(len(cols)>1):\n            groups.append(cols)\n            for e,v in enumerate(cols):\n                skip_list.append(int(v[9:]))\nprint(len(groups))\n    ","a94bcf66":"for i,v in enumerate(groups):\n    print(i,\"########\", v)","a068bb3f":"**Based on Probablity corelation, There seem to be 30 groups with more than one variable. There seem to be some groups with common members. **","2e001e89":"**We can find the group of a var using the following functions.**","66e9aac2":"**load data & group vars**","fac1e3c3":"**Functions**","44465025":"**We can group features using this correlation matrix. For example, var_0 and var_2's pdfs is 0.97+ correlated. We can confirm it using the figure below.**","84fcc5a8":"In this kernel, I implement vectorized PDF caculation (without for loop) to get their correlation matrix. This is helpful to study feature grouping.\ncredits to @sibmike https:\/\/www.kaggle.com\/sibmike\/are-vars-mixed-up-time-intervals"}}