{"cell_type":{"0f414b9e":"code","b24515b8":"code","9375f253":"code","8da70bfb":"code","5996b058":"code","86a26b58":"code","9f68945d":"code","080d73e8":"code","79b99dec":"code","60e5d1dd":"code","65a0f777":"code","16e65e1a":"code","3788c06c":"code","f5488343":"code","3105c63d":"code","43edcd2b":"code","546d968a":"code","4294d30f":"code","8c3ac86e":"code","04e9dbbe":"code","6f05c3d6":"markdown","629f3de7":"markdown","e40e350e":"markdown","fdf9a811":"markdown","d97acc33":"markdown","3ba1b88e":"markdown","90be566b":"markdown","09ca37c4":"markdown","26c184e9":"markdown","78cefb46":"markdown","3e75826b":"markdown","dcdf6e46":"markdown","248693f4":"markdown","b730e746":"markdown","6543a2a3":"markdown","78a150b7":"markdown","62595342":"markdown","a22d6f3b":"markdown","92c30446":"markdown","e36339ec":"markdown"},"source":{"0f414b9e":"import tensorflow as tf\nimport keras \nfrom keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport re\nfrom keras.preprocessing.image import img_to_array","b24515b8":"# to get the files in proper order\ndef sorted_alphanumeric(data):  \n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n    return sorted(data,key = alphanum_key)\n# defining the size of the image\nSIZE = 128\n_img = []\npath = '..\/input\/face-mask-lite-dataset\/without_mask'\nfiles = os.listdir(path)\nfiles = sorted_alphanumeric(files)\nfor i in tqdm(files):    \n        if i == 'seed9090.png':\n            break\n        else:    \n            img = cv2.imread(path + '\/'+i,1)\n            # open cv reads images in BGR format so we have to convert it to RGB\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            #resizing image\n            img = cv2.resize(img, (SIZE, SIZE))\n            img = (img - 127.5) \/ 127.5\n            imh = img.astype(float)\n            _img.append(img_to_array(img))","9375f253":"def plot_images(sqr = 5):\n    plt.figure(figsize = (10,10))\n    plt.title(\"Real Images\",fontsize = 35)\n    for i in range(sqr * sqr):\n        plt.subplot(sqr,sqr,i+1)\n        plt.imshow(_img[i]*0.5 + 0.5 )\n        plt.xticks([])\n        plt.yticks([])\n\n# to plot images\nplot_images(6)\n    ","8da70bfb":"batch_size = 32\ndataset=tf.data.Dataset.from_tensor_slices(np.array(_img)).batch(batch_size)","5996b058":"latent_dim = 100\ndef Generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(128*128*3, use_bias=False, input_shape=(latent_dim,)))\n    model.add(layers.Reshape((128,128,3)))\n    # downsampling\n    model.add(tf.keras.layers.Conv2D(128,4, strides=1, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(256,4, strides=1, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    \n    model.add(tf.keras.layers.LeakyReLU())\n    #upsampling\n    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2DTranspose(256, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.Conv2DTranspose(256, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    \n    model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2DTranspose(3,4,strides = 1, padding = 'same',activation = 'tanh'))\n    \n    \n\n    return model","86a26b58":"generator = Generator()\ngenerator.summary()","9f68945d":"def Discriminator():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input((SIZE, SIZE, 3)))\n    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(1,activation = 'sigmoid'))\n    return model\n  \n","080d73e8":"discriminator = Discriminator()\ndiscriminator.summary()","79b99dec":"noise = np.random.normal(-1,1,(1,100))\nimg = generator(noise)\nplt.imshow(img[0,:,:,0])\nplt.show()","60e5d1dd":"optimizer = tf.keras.optimizers.RMSprop(\n        lr=.0001,\n        clipvalue=1.0,\n        decay=1e-8\n    )\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)","65a0f777":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output),fake_output)\ndef discriminator_loss(fake_output, real_output):\n    fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output)\n    real_loss = cross_entropy(tf.ones_like(real_output),real_output)\n    return fake_loss + real_loss","16e65e1a":"def train_steps(images):\n    noise = np.random.normal(0,1,(batch_size,latent_dim))\n    with tf.GradientTape() as gen_tape , tf.GradientTape() as disc_tape:\n        generated_images = generator(noise)\n        fake_output = discriminator(generated_images)\n        real_output = discriminator(images)\n        \n        gen_loss = generator_loss(fake_output)\n        dis_loss = discriminator_loss(fake_output, real_output)\n        \n        \n    gradient_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)    \n    gradient_of_discriminator = disc_tape.gradient(dis_loss, discriminator.trainable_variables)\n    \n    optimizer.apply_gradients(zip(gradient_of_generator,generator.trainable_variables))\n    optimizer.apply_gradients(zip(gradient_of_discriminator, discriminator.trainable_variables))\n    \n    loss = {'gen loss':gen_loss,\n           'disc loss': dis_loss}\n    return loss","3788c06c":"def plot_generated_images(square = 5, epochs = 0):\n    \n    \n  plt.figure(figsize = (10,10))\n  for i in range(square * square):\n    if epochs != 0:    \n        if(i == square \/\/2):\n            plt.title(\"Generated Image at Epoch:{}\\n\".format(epochs), fontsize = 32, color = 'black')\n    plt.subplot(square, square, i+1)\n    noise = np.random.normal(0,1,(1,latent_dim))\n    img = generator(noise)\n    plt.imshow(np.clip((img[0,...]+1)\/2, 0, 1))\n    \n    plt.xticks([])\n    plt.yticks([])\n    plt.grid()","f5488343":"import time\ndef train(epochs,dataset):\n    \n    for epoch in range(epochs):\n        start = time.time()\n        print(\"\\nEpoch : {}\".format(epoch + 1))\n        for images in dataset:\n            loss = train_steps(images)\n        print(\" Time:{}\".format(np.round(time.time() - start),2)) \n        print(\"Generator Loss: {} Discriminator Loss: {}\".format(loss['gen loss'],loss['disc loss']))\n            \n        \n        \n    ","3105c63d":"\ntrain(5,dataset)\n# i had train model previously for more than 10 epochs so generated images are quiet good","43edcd2b":"plot_generated_images(1)","546d968a":"plot_generated_images(2)","4294d30f":"plot_generated_images(5)","8c3ac86e":"plot_generated_images(7)","04e9dbbe":"generator.save('generator.h5')\ndiscriminator.save(\"discriminator.h5\")","6f05c3d6":"### Defining loss function and optimizer ","629f3de7":"# Introduction\n<img src = 'https:\/\/cdn-images-1.medium.com\/max\/900\/1*TKr1dtcNgJCA8uYY1OhmSg.png'>","e40e350e":"### Defining training steps","fdf9a811":"## Load data\nHere I have used face-mask-lite-dataset, out of available 10000 images i have only use 9090 image. I have read image using opencv since opencv reads image in bgr format i have converted it back to rgb format using cvtColor function. These images are resize into 128 by 128 using resize function and are finally converted to array and are appended in empty array","d97acc33":"Here, i have defined batch size so that these batches of images can be fed directly to the discriminator network","3ba1b88e":"# Generator \nHere, I have defined generator network. It take random vector from normal distribution as input. This random vector is passed through dense layer and is reshaped and is finally fed through Convolution layers. Here, convolution layers does downsampling of our latent vector, after series of convolution batch normalization and leakyrelu layer our downsampled latent vector is upsampled using Conv2DTranspose.\n\nThe final output layer of Generator generate 128 by 128 by 3 image. The final layer of generator uses hyperbolic tangent as activation to squash the value in between -1 and 1. Generator model looks like simple autoencoder model, where input data is downsampled first and is finally upsampled .","90be566b":"## Application of GANs\n\n## 1. Generating fake faces\n<img src = \"https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Examples-of-Photorealistic-GAN-Generated-Faces.png\">\n\n## 2. Generate Examples for Image Datasets\n\n## 3. Face Aging\n<img src = 'https:\/\/www.baycare.net\/media\/5076\/botox-aging-face-plastic-surgery.jpg' height = '600px' width = '500px'>\n\n\n\n## 4. Super Resolution\n<img src = 'https:\/\/miro.medium.com\/max\/700\/1*E-JmUwv7zbwjzFm1hJLxPA.png'>\n\n\n## 5. Image-to-Image Translation\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-Photographs-of-Daytime-Cityscapes-to-Nighttime-with-pix2pix.png'>\n\n## 6.  Photos to Emojis\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-Celebrity-Photographs-and-GAN-Generated-Emojis.png'>\n\n\n\n\n## 7.Text to image Translation\n\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-Textual-Descriptions-and-GAN-Generated-Photographs-of-Birds.png'>\n\n\n## 8. Generate Cartoon characters\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-GAN-Generated-Anime-Character-Faces.png'>\n   \n","09ca37c4":"## Training","26c184e9":"## function to plot generated images\n","78cefb46":"#### ref: <a href = 'https:\/\/machinelearningmastery.com\/what-are-generative-adversarial-networks-gans\/'> machinelearningmastery <\/a>, <a href = 'https:\/\/towardsdatascience.com\/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391'> towardsdatascience <\/a>","3e75826b":"## Import necessary Libraries","dcdf6e46":"# Some Generated Images\n","248693f4":"# Discriminator\nHere, discriminator model take 128 by 128 by 3 image that can be real or generated. This input image is downsampled using Convolution layer and is finally flattened and is fed to single neuron so that it can distinguish real and fake image. Since, final layer uses sigmoid function as activation, it output value in between 0 and 1. Here value greater than 0.5 refers to real and less than 0.5 refers to fake image. The output of discriminator is used in training of generator.","b730e746":"### Components of GANs\n<b> Generator <\/b>: Generator are neural network that learns to generate data which resemble with the input distribution. The generator model take fixed dimension random vector from Gaussian distribution as input and generate the sample out of it which resemble with input.\n\n<b> Discriminator <\/b>: Discriminator are simple neural network that distinguish fake and real data.The discriminator model takes an example from the domain as input (real or generated) and predicts a binary class label of real or fake.\n\nGenerative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary. The generator network directly produces samples. Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples drawn from the generator.","6543a2a3":"### Let's plot image generated by generator before training","78a150b7":"## Visailze our images","62595342":"# Working of GANs\n<img src = 'https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/06\/11000153\/g1.jpg'>\n\nFirst of all we take batch of random vector from the Gaussian distribution and generate fake image out of it using generator. Since generator isn't trained so generated image donot resemble with the real input distribution. We take batches of image from the input distribution along with generated fake images and fed it to discriminator so that it learns to distinguish between real and fake images.\nNow, after training discriminator, we take the batch of images that generator generated and fed them through discriminator again (here we donot fed real images), discriminator will provide an output probabilities, these values are then compared with the probability that the generator should generated (ie 1), error is calculated and backpropagated through the generator and the weight are updated.\nThis above process is repeated until generated images resemble with the input distribution.","a22d6f3b":"## Objective: To generate fake faces of human","92c30446":"### Thanks for your visit\n## Any suggestion to improve generated images is really appreciated\n## Feel free to comment and upvote.....\n# Thank You","e36339ec":"Generative Adversarial Networks (GANs) are generative models. They are uses unsupervised technique to generate new things. GAN models learns pattern in input data in such a way that they can generate new sample which resemble with the input data. The main aim of generative adversarial network is to match generated distribution with the original data distribution.\n\nGANs are an exciting and rapidly changing field, delivering generative models ability to generate realistic examples across a range of problem domains, most notably in image-to-image translation tasks such as translating photos of summer to winter or day to night,coloring images and in generating fake photos that even human cannot categorized as fake image.\nThis <a href = 'https:\/\/thispersondoesnotexist.com\/'> site <\/a> uses GAN to generate fake human faces which are similar to real human"}}