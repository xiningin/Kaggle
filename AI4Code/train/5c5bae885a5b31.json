{"cell_type":{"1045d8ee":"code","aaf7772e":"code","f86410d4":"code","d1c61b26":"code","f1ccde78":"code","960360a7":"code","013d45d4":"code","4489bc77":"code","1ec80045":"code","e6a3baf3":"code","3629c169":"code","fab41c19":"code","07c9928d":"code","2bf53647":"code","522dcba8":"code","d5eb0967":"code","ceb77ee9":"code","6a45bd9c":"code","19b96ac2":"code","56c0f5f0":"markdown","728ab95d":"markdown","c5429b7b":"markdown","8099bfc4":"markdown","b6ad1bc9":"markdown","354c4671":"markdown","c0fcc6c2":"markdown","14b93fcd":"markdown","ccea80eb":"markdown","42959a63":"markdown","ca918b63":"markdown","deff8724":"markdown","5f907b97":"markdown","c49a1c35":"markdown","f87265c9":"markdown","569b604b":"markdown","fbca509e":"markdown","a0ecac38":"markdown","6f6ac971":"markdown","d2798eee":"markdown","66e31dd7":"markdown","7a643a9c":"markdown","25b32364":"markdown"},"source":{"1045d8ee":"import torch                        # The core package of Torch\nimport torch.nn as nn               # The nn package is used for building neural networks\nimport torch.nn.functional as F     # Contains all the functions in the torch.nn library\n\nfrom sklearn.model_selection import train_test_split\nimport scikitplot as skplt\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","aaf7772e":"# read data\ntrain_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\n# check that we loaded data correctly\nprint(\"Train data shape:\", train_data.shape)\nprint(\"Test data shape:\", test.shape)","f86410d4":"label_counts  = train_data[\"label\"].value_counts().sort_index()\nlabel_counts.plot.bar()\nplt.hlines(xmin = -1, xmax = 10, y = 4000, linestyles='dotted')","d1c61b26":"train_all = train_data.iloc[:,1:]     # only features (pixels)\ntrain_all_label = train_data[\"label\"] # labels\n\n# convert to numpy array\ntrain_all_numpy = train_all.to_numpy()\ntrain_all_label_numpy = train_all_label.to_numpy()\ntest_numpy = test.to_numpy()","f1ccde78":"def plot_img(data, label):\n    fig, axs = plt.subplots(3, 3) # 9 images\n    k = 0\n    for i in range(3):\n        for j in range(3):        \n            axs[i, j].imshow(data[k].astype('uint8').reshape(28, 28))   # plot image            \n            axs[i, j].set_ylabel(\"label:\" + str(label[k].item()))       # print label\n            k +=1\nplot_img(train_all_numpy, train_all_label_numpy)","960360a7":"# split train on train and validation\ntrain, validation, train_label, validation_label = train_test_split(train_all_numpy, train_all_label_numpy, test_size=0.2)\n\nprint(train.shape)\nprint(train_label.shape)\nprint(validation.shape)\nprint(validation_label.shape)","013d45d4":"unique, counts_train = np.unique(train_label, return_counts=True)\nplt.subplot(1, 2, 1)\nplt.bar(unique, counts_train\/len(train_label))\nunique, counts_val = np.unique(validation_label, return_counts=True)\nplt.subplot(1, 2, 2)\nplt.bar(unique, counts_val\/len(validation_label))\n#print (np.asarray((unique, counts_train\/len(train_label), counts_val\/len(validation_label))).T)","4489bc77":"train_all_tensor = torch.as_tensor(train_all_numpy).type(torch.FloatTensor)\ntrain_all_label_tensor = torch.as_tensor(train_all_label_numpy)\ntest_tensor = torch.as_tensor(test_numpy).type(torch.FloatTensor)\n\ntrain_tensor = torch.as_tensor(train).type(torch.FloatTensor)\ntrain_label = torch.as_tensor(train_label)\n\nvalidation_tensor = torch.as_tensor(validation).type(torch.FloatTensor)\nvalidation_label = torch.as_tensor(validation_label)","1ec80045":"                   # nn.Module - Base class for all neural network modules.\nclass FNet(nn.Module):             \n    def __init__(self):\n        super(FNet, self).__init__()\n                   # 784 inputs, connects to hidden layer with 600 nodes\n        self.fc1 = nn.Linear(in_features=784, out_features=600)\n                   # 600 nodes connects to hidden layer with 500 nodes\n        self.fc2 = nn.Linear(in_features=600, out_features=500) \n                   # 500 nodes connects to hidden layer with 250 nodes\n        self.fc3 = nn.Linear(in_features=500, out_features=250) \n                   # connection between the last hidden layer \n                   # and the output layer (with 10 nodes)\n        self.fc4 = nn.Linear(in_features=250, out_features=10)  \n                                                                \n    def forward(self, x):\n        x = x.view(-1,784)          # Put all the entries of the image in the vector\n        x = F.relu(self.fc1(x))     # Input x into first layer and apply a ReLU\n                                    # to the nodes in this layer\n        x = F.relu(self.fc2(x))        \n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x     ","e6a3baf3":"def get_accuracy(predictions, true_labels):\n    _, predicted = torch.max(predictions, 1)\n    corrects = (predicted == true_labels).sum()\n    accuracy = 100.0 * corrects\/len(true_labels)\n    return accuracy.item()","3629c169":"def training (dataloader, epochs, model, criterion, optimizer):\n\n    train_accuracies, train_losses = [], []\n    \n    # set the train mode\n    model.train()\n    \n    # loop over training dataset multiple times\n    for epoch in range(epochs):        \n\n        train_loss = 0 \n        train_accuracy = 0\n        num_batch = 0\n        \n        # iterate over all batches\n        for data, labels in dataloader:\n            \n            # zero the parameters gradient to not accumulate gradients from previous iteration\n            optimizer.zero_grad()\n            \n            # put data into the model\n            predictions = net(data)\n            \n            # calculate loss\n            loss = criterion(predictions, labels)\n            \n            # calculate accuracy\n            accurasy = get_accuracy(predictions, labels)\n            \n            # compute gradients\n            loss.backward()\n            \n            # change the weights\n            optimizer.step()\n            \n            num_batch += 1\n            train_loss += loss.item()\n            train_accuracy += accurasy\n    \n        epoch_accuracy = train_accuracy\/num_batch\n        epoch_loss = train_loss\/num_batch        \n        train_accuracies.append(epoch_accuracy)\n        train_losses.append(epoch_loss)\n        \n        print(\"Epoch: {}\/{} \".format(epoch + 1, epochs),\n              \"Training Loss: {:.4f} \".format(epoch_loss),\n              \"Training accuracy: {:.4f}\".format(epoch_accuracy))\n    \n    return train_accuracies, train_losses\n    ","fab41c19":"torch.manual_seed(0)                                      # set seed to make results reproducible\n\nbatch_size = 128                                          # Set the batch_size\ntrain_dataset = torch.utils.data.TensorDataset(train_tensor, train_label)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n\n\nepochs = 40                                               # set number of epohs\n\nnet = FNet()                                              # initialize network\ncriterion = nn.CrossEntropyLoss()                         # set criterion\noptimizer = torch.optim.Adam(net.parameters(), lr = 3e-4) # set optimizer\n\n                                                          # start training process\ntrain_accuracies, train_losses = training(trainloader, epochs, net, criterion, optimizer)","07c9928d":"def train_curves(epochs, train_losses, train_accuracies):\n    iters = range(1, epochs+1)\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 5))\n    fig.suptitle('Training Curve')\n    ax1.plot(iters, train_losses)\n    ax1.set_xlabel(\"Iterations\")\n    ax1.set_ylabel(\"Loss\")\n    ax2.plot(iters, train_accuracies, color = 'g')\n    ax2.set_xlabel(\"Iterations\")\n    ax2.set_ylabel(\"Training Accuracy\")\n    plt.show()\n    \ntrain_curves(epochs, train_losses, train_accuracies)","2bf53647":"# set net in test (evaluation) mode\nnet.eval()    \n# get predictions\nval_predictions = net(validation_tensor)\nval_loss = criterion(val_predictions, validation_label)\nval_accurasy = get_accuracy(val_predictions, validation_label)\n \nprint(\"Loss: \", str(val_loss.item()), \"Accuracy: \", str(val_accurasy))\n\n# to get class with the maximum score as prediction\n_, val_predicted = torch.max(val_predictions.data,1)            \n\n# confusion matrix\nskplt.metrics.plot_confusion_matrix(validation_label, val_predicted, figsize=(5,5))","522dcba8":"plot_img(validation, val_predicted)","d5eb0967":"# wrong predicted\n\nval_wrong = validation[validation_label != val_predicted]\nval_lab_wrong = val_predicted[validation_label != val_predicted]\n\nplot_img(val_wrong, val_lab_wrong)","ceb77ee9":"torch.manual_seed(0)                                      # set seed to make results reproducible\n\nbatch_size = 128                                          # Set the batch_size\ntrain_all_dataset = torch.utils.data.TensorDataset(train_all_tensor, train_all_label_tensor)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n\nepochs = 40                                               # set number of epohs\n\nnet = FNet()                                              # initialize network\ncriterion = nn.CrossEntropyLoss()                         # set criterion\noptimizer = torch.optim.Adam(net.parameters(), lr = 3e-4) # set optimizer\n\n                                                          # start training process\ntrain_accuracies, train_losses = training(trainloader, epochs, net, criterion, optimizer)","6a45bd9c":"train_curves(epochs, train_losses, train_accuracies)","19b96ac2":"# set net in test (evaluation) mode\nnet.eval()  \n\n# get predictions for test data\ntest_predictions = net(test_tensor)\n\n# to get class with the maximum score as prediction\n_, test_predicted = torch.max(test_predictions.data,1)\n\n# Save results in the required format\noutput = pd.DataFrame({'ImageId': test.index + 1,\n                       'Label': test_predicted})\noutput.to_csv('submission.csv', index=False)\noutput.head()","56c0f5f0":"### 4.2.3 Train\n\n* **Loss function** - For the loss function I will use **Softmax Cross-Entropy Loss** (nn.CrossEntropyLoss()) - transforms numbers into probabilities. A loss function computes how \"bad\" a predictions was, compared to the ground truth value for the input. The more accurate the network is, the smaller the loss is. The loss function transforms a problem of finding good weights to perform a task, into an optimization problem: finding the weights that minimize the loss function (or the average value of the loss function across some training data).\n\n* **Optimizer** - I will use **Adam optimizer**(torch.optim.Adam) - a version of gradient descent (by going in the direction of the gradient we are actually going in direction of the local minima of the function).\nParameter lr is learning rate - controls the size of each parameter update step we take. An epoch is a measure of the number of times all training data is used once to update the parameters.\n\n* **DataLoader** - represents a Python iterable over a dataset.\n* **batch_size** -  denotes the number of samples contained in each generated batch.\n* **epochs** - number of passes through the full training set. 1 Epoch = 1 Forward pass + 1 Backward pass for ALL training samples.","728ab95d":"Split labels and features of training dataset and convert to numpy array:","c5429b7b":"# 3. Prepare data","8099bfc4":"Check if both datasets represent all classes in fair proportion:","b6ad1bc9":"# 6. Train all \nFor the submission I will use model, traind with all possible information,entire train dataset - 42000 labeled examples (60% of the whole MNIST dataset):","354c4671":"# 2. Import libraries\nPytorch:  \n* **torch** - the core package of PyTorch  \n* **torch.nn** - the nn package is used for building neural networks (imported as nn)  \n* **torch.nn.functional** - contains all the functions in the torch.nn library (imported as F)\n\nOther:  \n* **train_test_split** from sklearn.model_selection - to split our train data on train and validation datasets  \n* **scikitplot** - to create a confusion matrix (imported as skplt)  \n* **pandas** - to load and transform data  (imported as pd)\n* **matplotlib.pyplot** - to create some visualizations  (imported as plt)\n\n","c0fcc6c2":"## 3.1 Load data\n\nAs input information I have two CSV files:  \n* **train.csv** - contains information about 42 000 (60% of the whole MNIST dataset) gray-scale images of hand-drawn digits (from 0 to 9).  \nFirst column is the digit. The rest 784 columns represent pixels of hand-drawn digit (each image has 28 x 28 pixels shape). The value of each pixel-column is an intager between 0 and 255 - represent darkness or lightness of the given pixel. \n* **test.csv** - contains information about 28 000 gray-scale images of hand-drawn digits (from 0 to 9). Doesn't contain labels. ","14b93fcd":"As result I got accuracy score = 0.97871  \n![submission_1.JPG](attachment:submission_1.JPG)\n\nIn the next notebook I will use CNN and additional data preparation","ccea80eb":"Let's create a barchart to check the number of digits in each class:","42959a63":"Some examples with predicted label:","ca918b63":"## 3.3 Tensors\nTensor is a multi-dimensional matrix containing elements of a single data type. To use PyTorch, we need to covert our data to tensors.","deff8724":"# 1. Introduction\n**MNIST** - the famous dataset of handwritten digits, which commonly used to learn computer vision basics. Dataset consists of black-and-white images of hand-drawn digits (between 0 and 9).  \nIn current notebook I will use PyTorch library to create Fully Connected Neural Network in order to determine digit by its handwritten image.\n\nThe task was completed as part of the [Getting Started Prediction Competition](https:\/\/www.kaggle.com\/c\/digit-recognizer\/).  \nThe submission file should contain a header and have the following format:  \nImageId,Label  \n1,0  \n2,0  \n3,0  \netc.  ","5f907b97":"## 3.2 Train and validation \n\nI would like to check my model on unseen data (data I didn't use in the training process) before submitting it.  \nSince I don't have labels in test dataset, I will randomly split my treaining data on training and validation datasets in proportion 8:2.  \nTo do so, I am using train_test_split function from sklearn module:","c49a1c35":"### 4.2.2 Training function\n\nTraining process:  \n1. Do a forward pass\n2. Calculate loss function\n3. Calculate the gradients\n4. Change the weights based on gradients\n\n\nThe entire training of the network is based on minimizing the loss function. An optimizer determines, based on the loss function, how each parameter should change.  \nThe optimizer solves the credit assignment problem: how do we assign credit to the parameters when the network performs poorly?","f87265c9":"### 4.1.2 Layers\nA fully connected neural network consists of a series of fully connected layers.  \nThe first layer takes the 784 (28 x 28) input pixels and connects to the first 600 node hidden layer. Then we have 600 to 500 hidden layer, 500 to 250 hidden layer and in the end the connection between the last hidden layer and the output layer (with 10 nodes).\n\n* input: 784 (image shape)\n* 3 hidden layers (600, 500, 250 nodes)\n* output: 10 neurons for each possible class\n\nI use **nn.Linear** to constructs a fully connected layer. The first argument is the number of nodes in layer l and the next argument is the number of nodes in layer l+1.\n\n### 4.1.3 Forward method\n* First, I need to reshape tensor x, so I use the view function for it.  \n* Second, input data x into the first fully connected layer (self.fc1(x)) and then apply a ReLU activation to the nodes in this layer using. * * Because of the hierarchical nature of this network, we replace x at each stage, feeding it into the next layer. \n* We do this through our three fully connected layers, except for the last one.\n\nAs activation function, I am using **ReLU** (The Rectified Linear Unit). It gives an output 0 if x is negative and x otherwise. The purpose of the activation function is to introduce non-linearity into the network (non-linear means that the output can not be reproduced from a linear combination of the inputs). To do it in code, I am using F.relu - applies the rectified linear unit function element-wise.\n\n$$\nf(x)=max(0,x)\n$$","569b604b":"And examples of the wrong classified images (with predicted labels):","fbca509e":"A training curve is a chart that shows:\n\n* The epochs on the x-axis\n* The loss and accuracy on the y-axis","a0ecac38":"## 4.2.1 Accuracy\nBefore start the training process, let's define an accuracy function.  \n**Accuracy** - fraction of the time classifier is correct.","6f6ac971":"Lets visualize some train examples with it's labels:","d2798eee":"# 4. FNN\n## 4.1 neural network architecture","66e31dd7":"# 5. Evaluation \nLet\u2019s see the test loss after the training. We switch the module mode to the evaluation mode and check the test loss.","7a643a9c":"# 7. Save results","25b32364":"## 4.2 Training the NN"}}