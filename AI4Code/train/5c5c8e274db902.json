{"cell_type":{"742c8ede":"code","76ac3348":"code","8a3d493d":"code","75ddfcc1":"code","2d050111":"code","45fa1026":"code","15b65b57":"code","164b0e44":"code","045959d9":"code","49d516fa":"code","1b0a7b3e":"code","bbccc773":"code","0c41f13d":"code","79c29b2f":"code","b53d62b3":"code","40ca4e80":"code","812c33c2":"code","88c1d5ba":"code","59678a4e":"code","5553aa58":"code","9eed2ac8":"code","0acd6aaa":"code","a93df6f3":"code","cab4eec9":"code","ad04e739":"code","9f3788bf":"code","a5aa9fa4":"code","f6ae8faa":"code","07b89c6a":"code","0f0ba72f":"code","f83d2bb7":"code","fa368560":"code","f9de97a3":"code","30ee4aa6":"markdown","c8d78daa":"markdown","d92f6313":"markdown","dcebcac5":"markdown","6bbdb15c":"markdown","b1627a7d":"markdown","23e834c2":"markdown","a866cfbc":"markdown","8274629d":"markdown","e612531b":"markdown","23cd1ddb":"markdown","3451dd8c":"markdown","918570ce":"markdown","dd0f306e":"markdown"},"source":{"742c8ede":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cupy as cp\nimport pandas as pd\nimport cudf\nimport dask_cudf\n\nimport gc #to manage ram \nimport subprocess\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\nimport optuna","76ac3348":"%%time\ntrain = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\n\nprint(train.shape)\nprint(test.shape)","8a3d493d":"train.describe()","75ddfcc1":"test.describe()","2d050111":"train.isna().sum()","45fa1026":"test.isna().sum()","15b65b57":"print(f'Number of missing values in training data: {train.isna().sum().sum()}')\nprint(f'Number of missing values in testing data: {test.isna().sum().sum()}')","164b0e44":"Features = [col for col in train.columns if col not in ['id', 'target']]","045959d9":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","49d516fa":"df = pd.concat([train[Features], test[Features]], axis=0)\n\ncat_features = [col for col in Features if df[col].nunique() < 25]\ncont_features = [col for col in Features if df[col].nunique() >= 25]\n\nprint(f'Total number of features: {len(Features)}')\nprint(f'Number of categorical features: {len(cat_features)}')\nprint(f'Number of continuos features: {len(cont_features)}')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuos'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()\n\ndel df","1b0a7b3e":"print(train['target'].value_counts())\nsns.countplot(x = train['target'],data = train);","bbccc773":"#creating a random temperory dataframe to get an idea of how the data is distributed \n#For plotting distributions\n\nnp.random.seed(2110)\ntmp_train = train.sample(10000)\ntmp_test = test.sample(10000)","0c41f13d":"print(\"Feature distribution of features: \")\nncols = 5\nnrows = 20\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 50), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = Features[r*ncols+c]\n        sns.kdeplot(x=tmp_train[col], ax=axes[r, c], label='Train data')\n        sns.kdeplot(x=tmp_test[col], ax=axes[r, c], color=\"orange\", label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()\n\ndel tmp_train\ndel tmp_test\ngc.collect()","79c29b2f":"columns = 10\nrows = 10\nf=0\nfig, ax_array = plt.subplots(rows, columns, squeeze=False)\nfor i,ax_row in enumerate(ax_array):\n    for j,axes in enumerate(ax_row):\n        axes.set_title('f'+str(f))\n        axes.set_yticklabels([])\n        axes.set_xticklabels([])\n        col = 'f'+str(f)\n        sns.set(rc = {'figure.figsize':(30,20)})\n        g2 = sns.boxplot(train[col],ax=axes)\n        g2.set(ylabel=None)\n        g2.set(xticklabels=[])\n        g2.set(yticklabels=[])\n        f=f+1\nplt.show()","b53d62b3":"train[\"mean\"] = train[Features].mean(axis=1)\ntrain[\"std\"] = train[Features].std(axis=1)\ntrain[\"min\"] = train[Features].min(axis=1)\ntrain[\"max\"] = train[Features].max(axis=1)\ntrain[\"sum\"] = train[Features].sum(axis=1)\n\ntest[\"mean\"] = test[Features].mean(axis=1)\ntest[\"std\"] = test[Features].std(axis=1)\ntest[\"min\"] = test[Features].min(axis=1)\ntest[\"max\"] = test[Features].max(axis=1)\ntest[\"sum\"] = test[Features].sum(axis=1)\n\nFeatures.extend(['mean', 'std', 'min', 'max', 'sum'])\n\ngc.collect()","40ca4e80":"print(train.shape)\nprint(test.shape)","812c33c2":"corr = train[Features+['target']].corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(labels=corr.columns,fontsize=12)\nax.set_yticklabels(labels=corr.columns,fontsize=12)\nsns.heatmap(corr,mask=mask,cmap='tab20c',linewidth=0.1)\nplt.title('Correlation Map',color='blue',fontsize=12)\nplt.show()","88c1d5ba":"y = train['target']\ntrain = train.drop(['target'], axis=1)\n\ngc.collect()","59678a4e":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ntrain[Features] = scaler.fit_transform(train[Features])\ntest[Features] = scaler.transform(test[Features])","5553aa58":"cat_params = {    \n    \"objective\": \"CrossEntropy\",\n    \"eval_metric\" : \"AUC\",\n    \"task_type\": \"GPU\",\n    \"grow_policy\": \"SymmetricTree\",\n    \"learning_rate\": 0.08,\n    \"n_estimators\":  10_000,\n    \"random_strength\" : 1.0,\n    \"max_bin\": 128,\n    \"l2_leaf_reg\": 0.002550319996478972,\n    \"max_depth\": 4,\n    \"min_data_in_leaf\": 193,\n    'verbose': 0\n}","9eed2ac8":"folds = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    \n    print(f\"Fold: {fold}\")\n    \n    X_train, X_test = train.iloc[trn_idx], train.iloc[val_idx]\n    y_train, y_test = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostClassifier(**cat_params)\n    \n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=400, verbose=False)\n    #model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric = 'auc', verbose = 500, early_stopping_rounds = 200)\n    \n    pred = model.predict_proba(X_test)[:,1]\n    roc = roc_auc_score(y_test, pred)\n    print(f\" roc_auc_score: {roc}\")\n    print(\"-\"*50)","0acd6aaa":"#lightGBM didn't work \n\"\"\"import lightgbm as lgb\nlgb_params = {\n    'objective': 'binary',\n    'n_estimators': 20000,\n    'random_state': 42,\n    'learning_rate': 8e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device': 'gpu',\n}used these parameters to get an initial score\nparams = {\n        'objective': 'binary',\n        'metric': 'AUC',\n        'boosting_type': 'dart', # To improve AUC\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.1, 0.9),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.1, 0.9),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'verbose': 0\n    }range for optuna to search\n\ndef objective(trial):\n    params = {\n        'objective': 'binary',\n        'metric': 'AUC',\n        'boosting_type': 'dart', # To improve AUC\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.1, 0.9),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.1, 0.9),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'verbose': 0\n    }\n    # Learning\n    gbm = LightGBM(params, lgb_train, lgb_valid, num_boost_round, verbose_eval, FEATS)\n    # Prediction\n    y_pred = gbm.predict(valid[FEATS], num_iteration=gbm.best_iteration)\n    accuracy = roc_auc_score(y_va, y_pred, labels='ROC curve', average='weighted')\n    print('ROC curve:', accuracy)\n    ROC_curve(y_va, y_pred)\n    # Finish\n    print(\"Operation completed.\")\n    # Output\n    return accuracy\"\"\"","a93df6f3":"def fit_cat(trial, x_train, y_train, x_test, y_test):\n    \n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 100000),\n              'od_wait':trial.suggest_int('od_wait', 500, 5000),\n              'task_type':\"GPU\",\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.02 , 0.06),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.30 , 0.33),\n              'subsample': trial.suggest_uniform('subsample',0.8,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,50),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n              'bootstrap_type':'Poisson'\n               }\n    \n    \n    model = CatBoostClassifier(**params)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict_proba(x_train)[:,1]\n    \n    y_test_pred = model.predict_proba(x_test)[:,1]\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train roc_auc\": roc_auc_score(y_train, y_train_pred),\n        \"valid roc_auc\": roc_auc_score(y_test, y_test_pred)\n    }\n    \n    return model, log","cab4eec9":"def objective(trial):\n    roc_auc = 0\n    x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.30)\n    model, log = fit_cat(trial, x_train, y_train, x_test, y_test)\n    roc_auc += log['valid roc_auc']\n        \n    return roc_auc","ad04e739":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n\nprint(\"Number of completed trials: {}\".format(len(study.trials)))","9f3788bf":"print(\"Best trial:\")\ntrial = study.best_trial\nprint(trial)","a5aa9fa4":"optuna.visualization.plot_param_importances(study)","f6ae8faa":"optuna.visualization.plot_optimization_history(study)","07b89c6a":"optuna.visualization.plot_slice(study, params=['depth', 'learning_rate', 'subsample'])","0f0ba72f":"new_params = {\n     'iterations': 80203,\n     'od_wait': 1765,\n     'learning_rate': 0.02010888271017379,\n     'reg_lambda': 0.3051769003766273,\n     'subsample': 0.9155353016941578,\n     'random_strength': 31.905377503941313,\n     'depth': 6,\n     'min_data_in_leaf': 14,\n     'leaf_estimation_iterations': 7,\n     'task_type':\"GPU\",\n     'bootstrap_type':'Poisson',\n}","f83d2bb7":"folds = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    \n    print(f\"Fold: {fold}\")\n    \n    X_train, X_test = train.iloc[trn_idx], train.iloc[val_idx]\n    y_train, y_test = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostClassifier(**new_params)\n   \n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=400, verbose=False)\n    \n    pred = model.predict_proba(X_test)[:,1]\n    roc = roc_auc_score(y_test, pred)\n    print(f\" roc_auc_score: {roc}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict_proba(test[Features])[:,1] \/ folds.n_splits ","fa368560":"sub = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","f9de97a3":"sub['target'] = predictions\nsub.to_csv('submission.csv', index = 0)\nsub","30ee4aa6":"By initial impressions the data seems to have no categorical features. Let's check if that is the case.","c8d78daa":"Lots of plots at our disposal. We can see how score is changing with respect to each parameter. Try tuning a little bit more, it might improve training time if not the final score. There are many more graphs and we can get better search space and we can use pruning to improve training time.","d92f6313":"No missing Values! ","dcebcac5":"# TPS - NOV 2021","6bbdb15c":"The above hyper-parameters used in training are borrowed from the catboost model from the previous TPS(tuned using Optuna).","b1627a7d":"Let's our model with new hyperparameters.","23e834c2":"Yes! We don't have any categorical fearures.","a866cfbc":"Adding these features improved the score. We could add more features by performing clustering and PCA(next time, maybe).\n\nScaling is important!","8274629d":"The main objective of this notebook is to learn for myself. I'm implementing different techniques that I learned in the previous TPS. But I cannot guarantee a high scoring notebook! Read on if you like this might help a few.","e612531b":"As the trails are randomly initialised different runs result in different parameters but generally they are fairly close.","23cd1ddb":"Distribution of the target value is balanced :)","3451dd8c":"Using Optuna for hyperparameter tuning for the first time. I am attaching this article to get better insight into Optuna implementation.\nhttps:\/\/towardsdatascience.com\/hyper-parameter-optimization-with-optuna-4920d5732edf. For Optuna we are required to define an Objective function with the loss function or evaluation metric to optimize. The below code is mostly reusable.","918570ce":"Let's train our Catboost model.","dd0f306e":"Depth is most important feature followed by learning rate and subsample respectively."}}