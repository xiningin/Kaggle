{"cell_type":{"5523e8a1":"code","f0502cf9":"code","a573ff90":"code","0487614a":"code","e057f59c":"code","04e22975":"code","e21f8397":"code","b7cef2ac":"code","d2711d0d":"code","71256cd9":"code","014b9605":"code","5140c749":"code","dfb1d4dc":"code","30402a5e":"code","2d2a1bfd":"code","7cc3df7b":"code","c0614226":"code","42e8b2b8":"code","114ca745":"code","060c4b36":"code","735138ef":"code","6647abc9":"code","b889b71a":"code","daf23762":"code","d44f0b36":"code","dc3a6ce5":"code","14ccfaa1":"code","6aab64b1":"code","f02bc384":"code","24c286ec":"code","7e355ee1":"code","45259a05":"code","c0c9dfd3":"code","a3d29ed9":"code","94f8611e":"code","a63bfa54":"code","d029b2f4":"code","00304250":"code","ce66ae42":"code","9aa7580a":"code","833b6165":"code","d00d6333":"code","ed498796":"code","ea7d3423":"code","cedcfb57":"code","d8f5a358":"code","379c40e5":"code","ed394556":"code","1f565a96":"code","c96d9235":"code","2469687c":"code","52708574":"code","73b47138":"code","af3148fb":"code","5189d77a":"code","a62aad1e":"code","36bdd5e2":"code","106d3764":"code","be29b354":"code","417d79c8":"code","9c3f4d88":"code","2d6c9440":"code","234524cf":"code","ba885d4b":"code","01d8d9c7":"code","7c68ce85":"code","ed8b96b2":"code","2cf1b396":"code","335e33cf":"code","2541b85b":"code","bc18a0b2":"code","8ecbebd2":"code","6a5aa828":"code","b3a67345":"code","812134b2":"code","dd826028":"code","ea589c0a":"code","830e04cc":"code","0ddb3924":"code","1d29ba38":"code","e19770ff":"code","fa1b671f":"code","608d5e6f":"code","5a70c2a8":"code","388fe8be":"code","88e82492":"code","97e8d5a3":"code","a38f11cc":"code","8622f08a":"markdown","f7360b1b":"markdown","353b5771":"markdown","67962fe5":"markdown","139cb58b":"markdown","47db1f81":"markdown","90533d57":"markdown"},"source":{"5523e8a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f0502cf9":"# Importing libraries & magic functions\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n%config InlineBackend.figure_format ='retina'\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1)","a573ff90":"# Putting Test and Train Set back together\n\ndstest = pd.read_csv('\/kaggle\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv')\ndstrain = pd.read_csv('\/kaggle\/input\/kuc-hackathon-winter-2018\/drugsComTrain_raw.csv')\n\ndstest.shape\ndstrain.shape\n\nds = pd.concat([dstest, dstrain],ignore_index=True)\nds.shape","0487614a":"# First glimpse at data\nds.head()\n\n# Statistics Summary - Numerical variables\nds.describe()\n\n# check types\nds.dtypes\n\n# check for missing data\nds.isna().sum()\n\n# check for duplicates\nduplicate_ds = ds[ds.duplicated()]\nduplicate_ds\nduplicate_ds.shape\n","e057f59c":"# drop rows with missing values\nds = ds.dropna()","04e22975":"ds.isna().sum()\nds.shape","e21f8397":"# change date to datetime format\n\nds.date = pd.to_datetime(ds.date)\nds.dtypes","b7cef2ac":"# set date as index\n\nds.set_index('date', inplace=True)\nds.info()\nds.head()","d2711d0d":"ds.condition.value_counts()","71256cd9":"duplicate_review = ds[ds.duplicated(['review'])]\nduplicate_review","014b9605":"ds.review.nunique()","5140c749":"regex_pattern = r'I wrote my first report in Mid-October(?!$)'\nds[ds['review'].str.contains(regex_pattern)]","dfb1d4dc":"# Many reviews are duplicated and have been assigned to 2 drugNames where 1 is usually a broader term for the drug and the second one the brand name. \n# We will therefore remove the duplicates and always keep the first value.\n\nds = ds.drop_duplicates(subset='review', keep=\"first\")\nds.shape","30402a5e":"ds.describe()","2d2a1bfd":"# checking for outliers\n\n#sns.boxplot(ds.usefulCount)\nsns.boxplot(ds.rating)\n","7cc3df7b":"# check distribution\/correlation\/outliers\nsns.pairplot(ds)","c0614226":"ds.head()","42e8b2b8":"ds.groupby('condition').agg('sum')","114ca745":"ds.condition.unique()","060c4b36":"ds.shape","735138ef":"ds_comment = ds[ds['condition'].str.contains('comment')]\nds_comment","6647abc9":"# Dropping rows that contain incorrect information \n\nds = ds[~ds['condition'].str.contains('comment')]\nds.shape","b889b71a":"ds_clean = ds","daf23762":"ds_clean.condition.nunique()\nds_clean.drugName.nunique()","d44f0b36":"# Distribution of drugs within conditions\n\nds_drugs_per_cond = ds_clean.groupby('condition').drugName.nunique().sort_values(ascending=False)\npd.DataFrame (data=ds_drugs_per_cond)\nds_drugs_per_cond = ds_drugs_per_cond.reset_index()","dc3a6ce5":"# Plotting Distribution of drugs within conditions\nplt.figure(figsize=(10,8))\nsns.barplot(x='drugName', y='condition', data=ds_drugs_per_cond[0:10], color='lightblue')\nplt.box(False)\nplt.xlabel(\"\", fontsize = 12)\nplt.ylabel(\"\", fontsize = 14)\nplt.title(\"Top 10 Number of Drugs by Condition\", fontsize = 18)\nplt.show()","14ccfaa1":"ds_drugs_per_cond.head()\nds_drugs_per_cond['drugName'].describe()","6aab64b1":"#Plotting Distribution Drugs Numbers per Condition\nmean=ds_drugs_per_cond['drugName'].mean()\nplt.figure(figsize=(11,5))\n#sns.set_style(\"white\")\nax = sns.distplot(ds_drugs_per_cond['drugName'],color='lightblue')\nax.axvline(mean, color='r', linestyle='--')\nplt.box(False)\nplt.legend({'Mean= 8.65':mean})\nplt.xlabel(\"\\n Number of different drugs\", fontsize = 12)\nplt.ylabel(\"\", fontsize = 12)\nplt.title(\"Distribution Drugs per Condition\", fontsize = 16)\nplt.show()","f02bc384":"# Spread? Standard Deviation","24c286ec":"# Displaying number of reviewed drugs by condition\n\nds_reviews_per_condition = ds_clean.groupby('condition').agg({'review':'count'})\nds_reviews_per_condition = ds_reviews_per_condition.sort_values(by='review', ascending=False)#[0:20]\nds_reviews_per_condition = ds_reviews_per_condition.reset_index()\nds_reviews_per_condition.head()","7e355ee1":"# Plotting reviews per conditions \nplt.figure(figsize=(12,6))\nsns.barplot(x='review', y='condition', data=ds_reviews_per_condition[0:10], color='lightblue')\nplt.box(False)\nplt.xlabel(\"\", fontsize = 12)\nplt.ylabel(\"\", fontsize = 14)\nplt.title(\"Top 10 Number of Reviews by Condition\\n\", fontsize = 18)\nplt.show()","45259a05":"# Group Rating and Condition\nds_rating = ds.groupby('condition').agg({'rating':'mean', 'review':'count'}).sort_values(by='rating',ascending=False)\nds_rating_150 = ds_rating[ds_rating.review>150] # we want to exclude those ratings that only received 1 review so we set the threshold approx. to the mean \nds_rating_150 = ds_rating_150.reset_index()","c0c9dfd3":"ds_rating_150.head()","a3d29ed9":"ds_rating_drug = pd.merge(left=ds_rating_150,right=ds_drugs_per_cond, how='left', left_on='condition', right_on='condition')\nds_rating_drug.head()","94f8611e":"# Correlation between rating scores and number of drug - grouped by condition\nnp.corrcoef(ds_rating_drug[\"rating\"], ds_rating_drug[\"drugName\"])\nsns.scatterplot(x='rating', y='drugName', data=ds_rating_drug, color='lightblue')\nplt.box(False)\nplt.xlabel(\"Rating\", fontsize = 10)\nplt.ylabel(\"Number of Drugs\\n\", fontsize = 12)\nplt.title(\"Rating and Number of Drugs\", fontsize = 15)\nplt.show()","a63bfa54":"ds_rating_150.head()","d029b2f4":"ds_rating_150['rating'].describe()","00304250":"# Plotting Rating for more than 150 reviews received\nmean=ds_rating_150['rating'].mean()\n\nax=sns.distplot(ds_rating_150['rating'], color='lightblue')\nax.axvline(mean, color='r', linestyle='--')\n\nplt.legend({'Mean= 7.152':mean})\n\nplt.box(False)\nplt.xlabel(\"Rating\", fontsize = 10)\nplt.ylabel(\"\", fontsize = 12)\nplt.title(\"Rating Distribution\", fontsize = 15)\nplt.show()","ce66ae42":"ds_outlier = ds_rating_150[ds_rating_150[\"review\"]>15000]\n# Remove outlier\n\nds_rating_150 = ds_rating_150.drop([ds_rating_150.index[91]])","9aa7580a":"# Correlation between rating scores and numbe rof reviews received - grouped by condition\nnp.corrcoef(ds_rating_150[\"rating\"], ds_rating_150[\"review\"])\nsns.scatterplot(x='rating', y='review', data=ds_rating_150, color='lightblue')\nplt.box(False)\nplt.xlabel(\"Rating\", fontsize = 10)\nplt.ylabel(\"Number of Reviews\\n\", fontsize = 12)\nplt.title(\"Rating and Number of Reviews\", fontsize = 15)\nplt.show()","833b6165":"ds_rating_150.head()","d00d6333":"#ds_clean['reviews_per_cond'] = ds_rating[\"review\"]\nds_merged_left = pd.merge(left=ds_clean,right=ds_rating, how='left', left_on='condition', right_on='condition')\n\nds_merged_left.head()\nds_merged_left.shape\nds_merged_left.isna().sum()\nds_merged_left_150 = ds_merged_left[ds_merged_left['review_y'] > 150]\nds_merged_left_150.head()\nds_merged_left_150.shape","ed498796":"# Renaming columns\nds_merged_left_150.columns\n\nds_merged_left_150 = ds_merged_left_150.rename(columns={'rating_y':'Mean Rating',\n                        'review_y':'Number of Reviews per condition'})","ea7d3423":"ds_merged_left_150.head()","cedcfb57":"ds_time['Year'] = ds_time.index.year","d8f5a358":"# Time Series\nds_time= ds_clean.sort_index()\nds_time.head()","379c40e5":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nmod=ols('rating_x  ~ condition',data=ds_merged_left_150).fit()\n\naov_table = sm.stats.anova_lm(mod, typ=2)\nprint(aov_table)","ed394556":"esq_sm = aov_table['sum_sq'][0]\/(aov_table['sum_sq'][0]+aov_table['sum_sq'][1])\naov_table['EtaSq'] = [esq_sm, 'NaN']\nprint(aov_table)\naov_table","1f565a96":"#plt.boxplot(scores~group,data=data)\nmod.summary()","c96d9235":"\nds_rating_150.head()\nds_rating_150 = ds_rating_150.replace({'mance Anxiety': 'Anxiety'})","2469687c":"# Plotting Ratings per condition \nplt.figure(figsize=(10,5))\nsns.barplot(x='condition', y='rating', data=ds_rating_150[0:5].sort_values(by='rating', ascending=False), color='lightgreen')\nplt.box(False)\nplt.xlabel(\"\\nCondition\", fontsize = 14)\nplt.ylabel(\"Average Rating\", fontsize = 14)\nplt.title(\"5 best Ratings by Condition\", fontsize = 20)\n#plt.setp(ax.get_xticklabels(), fontsize=14)\n\nplt.show()\n","52708574":"# Plotting Ratings per condition\nplt.figure(figsize=(12,5))\nsns.barplot(x='condition', y='rating', data=ds_rating_150[-6:-1].sort_values(by='rating', ascending= False), color=\"darkred\")\n#sns.set(rc={'figure.figsize':(22,10)})\nplt.box(False)\nplt.xlabel(\"\\nCondition\", fontsize = 14)\nplt.ylabel(\"Average Rating\", fontsize = 14)\nplt.title(\"5 worst Ratings by Condition\", fontsize = 20)\n#plt.setp(ax.get_xticklabels(), fontsize=14)\naxes = plt.gca()\n#axes.set_xlim([xmin,xmax])\naxes.set_ylim([0,10])\nplt.show()\n","73b47138":"ds_rating.head()","af3148fb":"ds_clean.head()","5189d77a":"ds_time_drug = ds_time.groupby('Year')['drugName'].nunique()\nds_time_drug\nds_time_drug.plot(kind='line')\nplt.title('Number of Drugs by Year')","a62aad1e":"ds_time_review = ds_time.groupby('Year')['review'].agg(['count'])","36bdd5e2":"ds_time_review.plot(kind='line')\nplt.title('Number of Reviews collected per Year')","106d3764":"ds_time_rating = ds_time.groupby('Year')['rating'].agg(['mean'])\nds_time_rating","be29b354":"ds_time_rating.plot(kind='line')\nplt.title('Average Rating Score by Year')","417d79c8":"ds_merged_left_150.head()","9c3f4d88":"from nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image","2d6c9440":"def generate_wordcloud(ds_merged_left_150):\n    \n    stopwords_list = stopwords.words('english') + list(STOPWORDS) \n    \n    raw_text = \" \".join(ds_merged_left_150['review_x'].values)\n    \n    #mask = np.array(Image.open(\"\/kaggle\/input\/lalalaa\/pill.png\"))\n    \n    wc = WordCloud(stopwords=stopwords_list, background_color=\"white\",width= 1600, height=800, max_words=150).generate(raw_text)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    fig = plt.gcf()\n    fig.set_size_inches(16, 8)\n    plt.show()\n\nreviews_by_comments = ds_merged_left_150.sort_values(by=\"usefulCount\", ascending=False)\n\ntop_100_useful_comments = reviews_by_comments.head(100)\n\ngenerate_wordcloud(ds_merged_left_150)\n\ngenerate_wordcloud(top_100_useful_comments)\n","234524cf":"#importing libraries\nimport spacy\nfrom spacy.lang.en.examples import sentences \nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata","ba885d4b":"nlp = spacy.load('en_core_web_sm', parse = True, tag=True, entity=True)\n#nlp = spacy.load()\n#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, entity=True)\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')","01d8d9c7":"# Remove HTML\n\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text\n\nstrip_html_tags('<html><h2>Some important text<\/h2><\/html>')","7c68ce85":"# Remove accented characters\n\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\nremove_accented_chars('S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt')","ed8b96b2":"# Remove special characters\n\ndef remove_special_characters(text, remove_digits=False):\n    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n\nremove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n                          remove_digits=True)","2cf1b396":"# Text lemmatization\n\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n\nlemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")","335e33cf":"# Text stemming\n\ndef simple_stemmer(text):\n    ps = nltk.porter.PorterStemmer()\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text\n\nsimple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")","2541b85b":"# Remove stopwords\n\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\nremove_stopwords(\"The, and, if are stopwords, computer is not\")","bc18a0b2":"# Text normalizer\n\ndef normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True, remove_digits=True):\n    \n    normalized_corpus = []\n    # normalize each document in the corpus\n    for doc in corpus:\n        # strip HTML\n        if html_stripping:\n            doc = strip_html_tags(doc)\n        # remove accented characters\n        if accented_char_removal:\n            doc = remove_accented_chars(doc)\n        # lowercase the text    \n        if text_lower_case:\n            doc = doc.lower()\n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        # lemmatize text\n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n        # remove special characters and\\or digits    \n        if special_char_removal:\n            # insert spaces between special characters to isolate them    \n            special_char_pattern = re.compile(r'([{.(-)!}])')\n            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        # remove stopwords\n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n            \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus","8ecbebd2":"# Sampling (takes too long otherwise)\n\nds_merged_left_150.shape\nds_merged_left_150.head()\n\nds_merged_left_150_sample = ds_merged_left_150.sample(frac=0.02, replace=False, random_state=5)\nds_merged_left_150_sample = ds_merged_left_150_sample.reset_index()\nds_merged_left_150_sample.shape","6a5aa828":"# Add column with cleaned text to the dataframe\nds_merged_left_150_sample['clean_text'] = normalize_corpus(ds_merged_left_150_sample['review_x'])\nds_merged_left_150_sample.head()","b3a67345":"from textblob import TextBlob\n    \ndef detect_polarity(text):\n    return TextBlob(text).sentiment.polarity\n\nds_merged_left_150_sample['polarity'] = ds_merged_left_150_sample['clean_text'].apply(detect_polarity)\nds_merged_left_150_sample.head()\n    \n","812134b2":"# Adding sentiment scores for raw review text\n\nds_merged_left_150_sample['polarity_raw'] = ds_merged_left_150_sample['review_x'].apply(detect_polarity)\nds_merged_left_150_sample.head()\n     ","dd826028":"# Dropping polsub columns (not necessary anymore since we split Polarity and Subjectivity)\n\n#ds_merged_left_150_sample = ds_merged_left_150_sample.drop(['pol_sub','pol_sub2'], axis=1)","ea589c0a":"# correlation btw polarity scores and rating scores\n# Spearman correlation between computed polarity and given rating\n\nfrom scipy.stats import spearmanr\nspearmanr(ds_merged_left_150_sample['polarity'], ds_merged_left_150_sample['rating_x'])\n\n# Testing with raw data\nspearmanr(ds_merged_left_150_sample['polarity_raw'], ds_merged_left_150_sample['rating_x'])","830e04cc":"np.corrcoef(ds_merged_left_150_sample[\"rating_x\"], ds_merged_left_150_sample[\"polarity\"])\n\n# Testing with raw data\nnp.corrcoef(ds_merged_left_150_sample[\"rating_x\"], ds_merged_left_150_sample[\"polarity_raw\"])","0ddb3924":"plt.figure(figsize=(6,6))\nsns.scatterplot(x='rating_x', y='polarity', data=ds_merged_left_150_sample)\nplt.box(False)\nplt.xlabel(\"Rating\", fontsize = 12)\nplt.ylabel(\"Polarity\", fontsize = 12)\nplt.title(\"Correlation Rating and Polarity\", fontsize = 18)\nplt.show()","1d29ba38":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.boxplot(x=ds_merged_left_150_sample[\"rating_x\"],y=ds_merged_left_150_sample[\"polarity\"])\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Polarity\")\nplt.title(\"Polarity vs Ratings\")\nplt.show()","e19770ff":"# Distribution of Polarity scores across sample\n\nplt.figure(figsize=(6,6))\nsns.set_style('white')\nsns.distplot(ds_merged_left_150_sample['polarity'])\nplt.box(False)\nplt.title(\"Polarity Distribution\", fontsize=18)","fa1b671f":"# Distribution of Rating scores across sample\nplt.figure(figsize=(6,6))\nsns.distplot(ds_merged_left_150_sample['rating_x'])\nplt.box(False)\nplt.xlabel('Rating')\nplt.title(\"Rating Distribution\", fontsize=16)","608d5e6f":"# Adding pos\/neg\/neutral labels\n\ndef f(row):\n    if row['polarity'] >= 0.3:\n        val = 'positive'\n    elif row['polarity'] <=-0.3:\n        val = 'negative'\n    else:\n        val = 'neutral'\n    return val\n\nds_merged_left_150_sample['Sentiment'] = ds_merged_left_150_sample.apply(f, axis=1)\n","5a70c2a8":"ds_merged_left_150_sample[ds_merged_left_150_sample.polarity == -1].head()\nds_merged_left_150_sample[ds_merged_left_150_sample.polarity == 1].head()\n#ds_merged_left_150_sample.tail(20)","388fe8be":"sns.countplot(ds_merged_left_150_sample['Sentiment'])#.sort_values(by)","88e82492":"ds_merged_left_150_sample = ds_merged_left_150_sample.drop(['index'], axis=1)\n#ds_merged_left_150_sample.head()","97e8d5a3":"ds_merged_left_150_sample_displ = ds_merged_left_150_sample.drop(['usefulCount','Mean Rating','Number of Reviews per condition','polarity_raw'], axis=1) \ntype(ds_merged_left_150_sample_displ)\nds_merged_left_150_sample_displ.loc[[336]]","a38f11cc":"ds_merged_left_150_sample_displ[ds_merged_left_150_sample_displ.polarity == -1].head()\nds_merged_left_150_sample_displ[ds_merged_left_150_sample_displ.polarity == 1].head()","8622f08a":"## Exploratory Data Analysis <a name=\"paragraph2\"><\/a>","f7360b1b":"# UCI Machine Learning - Drug Review Dataset\n\n**Content:**\n1. [Introduction](#introduction)\n2. [Data Preparation](#paragraph1)\n3. [Exploratory Data Analysis](#paragraph2)\n4. [ANOVA Test](#paragraph3)\n5. [Sentiment Analysis](#paragraph4)\n\n## Introduction <a name=\"introduction\"><\/a>\n\n**Attribute Information:**\n\n* drugName (categorical): name of drug\n* condition (categorical): name of condition\n* review (text): patient review\n* rating (numerical): 10 star patient rating\n* date (date): date of review entry\n* usefulCount (numerical): number of users who found review useful\n\nThe structure of the data is that a patient with a unique ID purchases a drug that meets his condition and writes a review and rating for the drug he\/she purchased on the date. Afterwards, if the others read that review and find it helpful, they will click usefulCount, which will add 1 for the variable.\n","353b5771":"## Sentiment Analysis<a name=\"paragraph4\"><\/a>","67962fe5":"## Data Preparation<a name=\"paragraph1\"><\/a>","139cb58b":"### Wordcloud","47db1f81":"## ANOVA TEST <a name=\"paragraph3\"><\/a>","90533d57":"### Development over Time"}}