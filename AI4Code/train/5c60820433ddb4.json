{"cell_type":{"9ea267f8":"code","f8b95579":"code","961e60ca":"code","45abfc00":"code","cfb8d61c":"code","2734370a":"code","1349bd6d":"code","7af10661":"code","bf22539d":"code","b1874b9b":"code","595e5778":"code","374629f7":"code","4c417d6d":"code","13602fd0":"code","921b470a":"code","4ef99aa7":"code","1f16df3b":"code","bea04ae1":"code","9bb590f5":"code","cd8eea39":"code","fc632f14":"code","98ec7b9f":"code","7619e2a1":"code","0d20022d":"code","964a640b":"code","e55dc307":"code","9d31f66d":"code","3ea013d2":"code","936bcf69":"code","5efe345f":"code","66e522e2":"code","26a708f3":"code","94c7a61d":"code","c586e787":"code","7178a386":"code","4fbc91b3":"code","224fb4a4":"code","f457b8db":"code","2f115f98":"code","b736483b":"code","0aa92522":"code","87da831d":"code","1d61c1c3":"code","b81b242d":"code","cce2655e":"code","9b4545b7":"code","cf2854ba":"code","5a21c25f":"markdown","e432812e":"markdown","3c2ad895":"markdown","d7db6f6f":"markdown","fcd42d93":"markdown","f0bd4b32":"markdown","746f2381":"markdown","1f61656c":"markdown","b213efa2":"markdown","b9f51c28":"markdown","81c63051":"markdown","e731654f":"markdown","1d7b64ae":"markdown","7d815c36":"markdown","a3a34792":"markdown","0a51d969":"markdown","f45e9ad3":"markdown","d56a94d5":"markdown","714d4a96":"markdown","25d9d8c7":"markdown","20b5af62":"markdown","65b521e2":"markdown","319db6dd":"markdown","358c29ff":"markdown","b2b1a2bd":"markdown","efdb161d":"markdown","8c08d078":"markdown","669f3c74":"markdown","3ad1ae92":"markdown","88f900ef":"markdown","a3f21a18":"markdown","4ea3e388":"markdown"},"source":{"9ea267f8":"# This is a code cell\na = 3","f8b95579":"b = 5\na   # The last variable is always printed","961e60ca":"a += b","45abfc00":"list()","cfb8d61c":"# First we will import all the modules we will use\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression","2734370a":"# Use ! to execute a shell command from the notebook\n!ls \/kaggle\/input\/weather-postprocessing","1349bd6d":"# We can just peek at the data from the command line\n!head \/kaggle\/input\/weather-postprocessing\/pp_train.csv","7af10661":"df_train = pd.read_csv('\/kaggle\/input\/weather-postprocessing\/pp_train.csv', index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/weather-postprocessing\/pp_test.csv', index_col=0)","bf22539d":"df_train.head()","b1874b9b":"df_train.head().T    # Use .T for a transposed view","595e5778":"df_train.describe().T","374629f7":"# How big is each dataset\nlen(df_train), len(df_test)","4c417d6d":"# What are the columns\ndf_train.columns","13602fd0":"df_test.columns","921b470a":"df_train.dtypes","4ef99aa7":"df_train.station.nunique()","1f16df3b":"df_train.t2m_obs.hist(bins=50);","bea04ae1":"df_train[::1000].plot.scatter('t2m_obs', 't2m_fc_mean', alpha=0.5);","9bb590f5":"def linear_model(x, a, b):\n    return a*x + b","cd8eea39":"x = df_train.t2m_fc_mean\ny_pred = linear_model(x, a=1, b=0)\n# y_pred = linear_model(x, a=1, b=0.1)","fc632f14":"df_train[::1000].plot.scatter('t2m_obs', 't2m_fc_mean', alpha=0.5);\nplt.scatter(x, y_pred, c='r', alpha=0.5);","98ec7b9f":"y_true = df_train.t2m_obs","7619e2a1":"def mse(y_true, y_pred):\n    return ((y_true - y_pred)**2).mean()","0d20022d":"mse(y_true, y_pred)","964a640b":"df_train.isna().sum()","e55dc307":"df_test.isna().sum()","9d31f66d":"df_train = df_train.dropna(subset=['t2m_obs'])","3ea013d2":"# Replace missing soil moisture values with mean value\ndf_train.loc[:, 'sm_fc_mean'].replace(np.nan, df_train['sm_fc_mean'].mean(), inplace=True)\n# Same for test dataset, using the training values\ndf_test.loc[:, 'sm_fc_mean'].replace(np.nan, df_train['sm_fc_mean'].mean(), inplace=True)","936bcf69":"split_date = '2015-01-01'\nX_train = df_train[df_train.time < split_date][['t2m_fc_mean']]\ny_train = df_train[df_train.time < split_date]['t2m_obs']\nX_valid = df_train[df_train.time >= split_date][['t2m_fc_mean']]\ny_valid = df_train[df_train.time >= split_date]['t2m_obs']","5efe345f":"X_train.shape, X_valid.shape","66e522e2":"lr = LinearRegression()","26a708f3":"lr.fit(X_train, y_train)","94c7a61d":"y_pred = lr.predict(X_valid)","c586e787":"mse(y_pred, y_valid)","7178a386":"a = lr.coef_\nb = lr.intercept_\na, b","4fbc91b3":"plt.scatter(X_train[::1000], y_train[::1000], alpha=0.5)\nx = np.array([-15, 20])\nplt.plot(x, a*x+b, c='r');","224fb4a4":"lr.score(X_valid, y_valid)","f457b8db":"def print_scores(model):\n    r2_train = model.score(X_train, y_train)\n    r2_valid = model.score(X_valid, y_valid)\n    mse_train = mse(y_train, model.predict(X_train))\n    mse_valid = mse(y_valid, model.predict(X_valid))\n    print(f'Train R2 = {r2_train}\\nValid R2 = {r2_valid}\\nTrain MSE = {mse_train}\\nValid MSE = {mse_valid}')","2f115f98":"print_scores(lr)","b736483b":"split_date = '2015-01-01'\nX_train = df_train[df_train.time < split_date].drop(['t2m_obs', 'station', 'time'], axis=1)\ny_train = df_train[df_train.time < split_date]['t2m_obs']\n\nX_valid = df_train[df_train.time >= split_date].drop(['t2m_obs', 'station', 'time'], axis=1)\ny_valid = df_train[df_train.time >= split_date]['t2m_obs']\n\nX_test  = df_test.drop(['station', 'time'], axis=1)","0aa92522":"X_train.shape, X_test.shape","87da831d":"lr = LinearRegression()\nlr.fit(X_train, y_train)","1d61c1c3":"print_scores(lr)","b81b242d":"preds = lr.predict(X_test)\npreds.shape","cce2655e":"sub =  pd.DataFrame({'id': range(len(preds)), 'Prediction': preds})\nsub.head()","9b4545b7":"sub.to_csv('submission.csv', index=False)","cf2854ba":"X_train.to_csv('X_train.csv')\ny_train.to_csv('y_train.csv')\nX_valid.to_csv('X_valid.csv')\ny_valid.to_csv('y_valid.csv')\nX_test.to_csv('X_test.csv')","5a21c25f":"Let's chose a and b and seen how well this fits the data.","e432812e":"# Notebook 1 - Linear regression with scikit-learn\n\nHello, you are now in a Kaggle kernel. Kaggle kernels are basically the same as Jupyter notebooks but are running on Kaggle's servers.\n\nThere are two types of cells in notebooks:\n1. Code cells: Used for writing python code.\n2. Markdown cells: Used for text.\n\nCells can be executed (=run) by pressing the play button or, more conveniently by pressing `shift + Enter`.","3c2ad895":"The data is stored in this directory:","d7db6f6f":"The training and testing datasets have the same variables except for `t2m_obs`, our $y$, which is only available for the training dataset.\n\nWe also want to check what datatype each column has.","fcd42d93":"# Your turn\n\nNow comes the most important part in learning ML: doing ML!\n\nYour first task is to reproduce what I have done in this notebook. So open up a new, empty notebook and go through the basic steps\n\n1. Opening up the data\n2. Preprocess\/clean the data and create train\/valid\/test datasets (X\/y)\n3. Train a linear regression\n4. Submit your predictions to Kaggle\n\nTry to do as much as possible from memory and look at this notebook only when necessary (and if you do: no copy-paste!). It is also not necessary to do everything exactly the same way.\n\nWhen you are done with that, here are some tasks to explore:\n\n1. What is the MSE of the climatology (mean over training dataset)?\n2. Can you compute the climatology for each station and submit this as a prediction?\n3. Now let's try to get a better prediction: Why not try training a linear regression for each station separately?\n4. Or try using a better ML model. Try replacing LinearRegression with sklearn.ensemble.RandomForestRegressor. What do you find?","f0bd4b32":"The rest works exactly the same.","746f2381":"## Let the machine learn\n\nManually tweaking the parameters seems rather inefficient. Let's have an algorithm do this.\n\nBut wait! Before we do this, we need to do two things:\n\n1. Clean the dataset\n2. Split the data into training and validation set.","1f61656c":"We can now check what our line parameters `a` and `b` are.","b213efa2":"## Data preparation\n\nNow let's do machine learning!\n\nIn (supervised) machine learning, the task is to predict a quantity $y$ as a function of some inputs $X$ with a model $M$.\n\n$y = M(X)$\n\nIn our first example, we will do some NWP post-processing. We want to predict the temperature measured at some surface stations (our $y$) based on the NWP forecast of temperature plus a bunch of other variables, as well as some properties of the station like the altitude. All of these inputs are our $X$.\n\nSome terminology:\n- $X$ = inputs = features = predictors = independend variables\n- $y$ = outputs = targets = predictands = dependent variable\n\nHow do we find $M$? We learn it from some training data. Let's now actually look at the dataset","b9f51c28":"### Split into train\/valid\/test\n\nWe have been given two datasets: train and test. `df_train` contains all the data that we are allowed to use to train our models. `df_test` contains only the features for the samples that we actually want to predict. So why do we need a third `validation` dataset?\n\nIn the real world the testing data would usually be in the future, so we cannot actually use it to verify how good our model is. We also shouldn't use the training data to evaluate our model since we could literally just build a lookup table and get a perfect score. So we want a third validation dataset to check model performance.\n\nIt is up to us which data from `df_train` to take for validation. Let's assess two options:\n\n1. Randomly pick samples\n2. Pick the last year 2015 from df_train (2007-2015)\n\nWhich one would be a better validation dataset?\n\n*More info:* https:\/\/www.fast.ai\/2017\/11\/13\/validation-sets\/","81c63051":"Now we can try fiddling with the two parameters a and b to reduce the error.","e731654f":"## The R2 score\n\nA common score to evaluate the skill of regression models is the R2 score: https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination\n\nIt tell us (sort of) which fraction of the variance in the data is explained by our model. [Poll]","1d7b64ae":"These are tabular datasets saved in CSV format, something you could also open in Excel. In Python, we use Pandas for this type of data. Pandas = Excel in Python.\n\nThe training file contains data from 2007 to 2015.\nThe testing file is from 2016.","7d815c36":"What are all these variables? --> https:\/\/journals.ametsoc.org\/view-large\/11181633\n\n`fc` indicates that these are NWP forecasts (specifically 2 day forecasts from the ECMWF ensemble). `mean` indicates that these values are the ensemble mean.","a3a34792":"Next, we fit the model using the training data. What this is doing internally is finding the line through the training data that has the lowest MSE.","0a51d969":"## Multiple linear regression\n\nSo far we have only used a single feature, but our dataset has many more. So let's use all of these in our linear model:\n\n$y = \\sum_i a_i X_i + b$\n\nFirst we need to prepare our datasets again. We will use all the columns except \n\n- `station`: This is a categorical variable where the value of the station id does not have any meaning. \n- `time`: Again, this is not a real valued variable, so we can't just throw it into our linear regression.","f45e9ad3":"This looks like a pretty straight line to me. So we could build a simple linear model by hand.\n\n$y = ax + b$","d56a94d5":"We will use `sklearn` to train our linear regression. All `sklearn` models (=estimators) use the same API.\n\nFirst, we create a model object.","714d4a96":"### Clean the dataset\n\nLet's check for missing values.","25d9d8c7":"Let's now define a handy little function to print the R2 and MSE for the training and validation set.","20b5af62":"Here we can see that all columns are real numbers except `time` which is a time string and `station` which is an integer and represents the station ID. We can seehow many unique stations we have.","65b521e2":"Let's plot the dependent variable `t2m_obs` against `t2m_fc_mean`, i.e. the ensemble mean forecast.","319db6dd":"Caution: The order of execution matters!\n\nA really useful trick:\nHold `shift + tab` for two seconds inside a function to bring up the docstring.","358c29ff":"We can see that the training scores are a little better than the validation score but let's not worry about this now (we will worry about this a lot very soon).","b2b1a2bd":"Two columns have missing values:\n\n1. `t2m_obs`: That is our target. If we don't have that we can't learn. So we should remove these rows.\n2. `sm_fc_mean`: Quite a lot of soil moisture values are missing. We can't just remove those because there are also missing values in the test set. Plus we would throw away quite a lot of training data. Here we will fill in the missing values.","efdb161d":"So we improved a little bit compared to our simple model. Let's now actually submit this to Kaggle.","8c08d078":"Pandas also has some easy-to-use plotting functions.","669f3c74":"To submit this file, click \"Save Version\" and chose \"Save and Run All\". This will rerun the entire notebook and save the output. \n\nOnce done, click the back arrow on the top left (this will close the notebook though) or navigate to the notebook in another tab. After clicking on the notebook scroll down until you see the Outputs, select the right file and click submit.","3ad1ae92":"And that's it. We have trained a ML model. Now we just need to figure out how good it is.","88f900ef":"Can we quantitatively measure how good the fit is?\n\nLet's use the mean squared error\n\nMSE = $\\frac{1}{N} \\sum (y - y_{pred})^2$","a3f21a18":"For a list of kernel shortcuts see: https:\/\/www.kaggle.com\/naushads\/keyboard-shortcuts-for-kaggle-kernels\n\nThey are mostly the same as for regular Jupyter notebooks but some are different.\n\nThe basic commands are:\n- `Esc` to get into command mode\n- `Enter` to get into edit mode\n- `dd` in command mode to delete a cell\n- `z` to undo\n- `a`\/`b` to insert a cell above\/below","4ea3e388":"Finally, we want to save the pre-processed data so that we can reuse it in later notebooks."}}