{"cell_type":{"f20e7e6b":"code","5e397182":"code","f7a8a32b":"code","5becb26e":"code","a196dd0a":"code","febce053":"code","d80a71da":"code","6f379b69":"code","a53d5692":"code","56f3fa95":"code","7d746b51":"code","e1f3b966":"code","74416b92":"code","e43b631a":"code","55ad875b":"code","4fcf56ab":"code","a8677327":"code","9a695b30":"code","66d12fda":"code","a71c5093":"code","050cb668":"code","435865b0":"code","6fbc2530":"code","a9755d23":"code","bcb54851":"code","906adc1d":"code","73735ed3":"code","0f435a73":"code","095c20bf":"code","f49341ec":"code","16c71a38":"code","58b8ba3e":"code","1318ffce":"code","19b973f4":"code","51617924":"code","58eb7d48":"code","57dc01ff":"code","5d558ee4":"code","a7e15a4a":"code","0f47aff4":"code","cec93d37":"code","86084742":"code","c932b2ba":"code","79055b14":"code","d40e9258":"code","100953fd":"code","17bc5ff5":"code","85ec2dff":"code","a0635af1":"code","08a4883c":"code","55736967":"code","9682da69":"code","51d120ef":"markdown","8076dae7":"markdown","b76339cc":"markdown","fd1eb35b":"markdown","5dc83b92":"markdown","43dce875":"markdown","4ed616a0":"markdown","65df10bd":"markdown","7ebe6e07":"markdown","4392e446":"markdown","6ee18636":"markdown","d08a459e":"markdown","3a9c528f":"markdown","85ca9d87":"markdown","0224c3c8":"markdown"},"source":{"f20e7e6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e397182":"from PIL import Image\n\nimage = Image.open(\"..\/input\/kaggle-pog-series-s01e01\/thumbnails\/-4sfXSHSxzA.jpg\")\nimage","f7a8a32b":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/\/test.parquet')\nsub = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","5becb26e":"train.head()","a196dd0a":"import re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\nfrom tqdm.auto import tqdm\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')","febce053":"train.isnull().sum()","d80a71da":"#92273nd row, 2nd column \n\ntrain.iloc[92273,1]","6f379b69":"#Codes by https:\/\/www.kaggle.com\/muhammadismail99\/data-analysis-of-guf-pub-dataset\/notebook\n\nsns.countplot(data = train, x = 'comments_disabled')\nplt.title('Comments disabled');","a53d5692":"#Codes by https:\/\/www.kaggle.com\/muhammadismail99\/data-analysis-of-guf-pub-dataset\/notebook\n\nsns.countplot(data = train, x = 'ratings_disabled')\nplt.title('Ratings Disabled');","56f3fa95":"#Codes by https:\/\/www.kaggle.com\/muhammadismail99\/data-analysis-of-guf-pub-dataset\/notebook\n\nsns.countplot(data = train, x = 'has_thumbnail')\nplt.title('Has Thumbnail');","7d746b51":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nplt.figure(figsize=(13,5))\n\nsns.kdeplot(train['target'], shade=  True);","e1f3b966":"#Code by Puru Behl https:\/\/www.kaggle.com\/accountstatus\/mt-cars-data-analysis\n\nsns.distplot(train['target'])\nplt.axvline(train['target'].values.mean(), color='red', linestyle='dashed', linewidth=1)\nplt.title('Target Distribution');","74416b92":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nprint(len(train[train['dislikes'] < 5000]), 'videos with less than 5000 dislikes')\nprint(len(train[train['dislikes'] > 5000]), 'videos with more than 5000 dislikes')","e43b631a":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# video with the most comments\n\ntrain[train['dislikes'] == train['dislikes'].max()]['title'].iloc[0]","55ad875b":"dis = train[(train['video_id']=='vRXZj0DzXIA')].reset_index(drop=True)\ndis.head(2)","4fcf56ab":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# video with the most comments\n\ntrain[train['likes'] == train['likes'].max()]['title'].iloc[0]","a8677327":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndef remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\n\ndef clean_text(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","9a695b30":"#https:\/\/stackoverflow.com\/questions\/55557004\/getting-attributeerror-float-object-has-no-attribute-replace-error-while\n#To avoid with tqdm AttributeError: 'float' object has no attribute\n\ntrain[\"title\"] = train[\"title\"].astype(str)\ntrain[\"title\"] = [x.replace(':',' ') for x in train[\"title\"]]","66d12fda":"train['clean_title'] = pd.Series([clean_text(i) for i in tqdm(train['title'])])","a71c5093":"words = train[\"clean_title\"].values","050cb668":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nls = []\n\nfor i in words:\n    ls.append(str(i))","435865b0":"ls[:5]","6fbc2530":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# The wordcloud \nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"lightblue\", colormap='Set2', max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(ls))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'Set2' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","a9755d23":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nmost_pop = train.sort_values('target', ascending =False)[['title', 'target']].head(12)\n\nmost_pop['target1'] = most_pop['target']\/1000","bcb54851":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nplt.figure(figsize = (20,25))\n\nsns.barplot(data = most_pop, y = 'title', x = 'target1', color = 'c')\nplt.xticks(fontsize=27, rotation=0)\nplt.yticks(fontsize=31, rotation=0)\nplt.xlabel('Votes in Thousands', fontsize = 21)\nplt.ylabel('')\nplt.title('Most popular videos', fontsize = 30);","906adc1d":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk","73735ed3":"stemmer = SnowballStemmer('english')","0f435a73":"nltk.download('wordnet')","095c20bf":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","f49341ec":"train['title'].iloc[2]","16c71a38":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndoc_sample = train['title'].iloc[1]\nprint('original document: ')\n\nwords = []\n\nfor word in doc_sample.split(' '):\n    words.append(word)\n    \n    \nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))","58b8ba3e":"train['clean_title'] = train['clean_title'].astype(str)","1318ffce":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nwords = []\n\nfor i in train['clean_title']:\n        words.append(i.split(' '))","19b973f4":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndictionary = gensim.corpora.Dictionary(words)\n\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","51617924":"# Filter out tokens in the dictionary by their frequency.\n\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","58eb7d48":"bow_corpus = [dictionary.doc2bow(doc) for doc in words]\nbow_corpus[4310]","57dc01ff":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nbow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                               dictionary[bow_doc_4310[i][0]], \nbow_doc_4310[i][1]))","5d558ee4":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nfrom gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nfrom pprint import pprint\n\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","a7e15a4a":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nlda_model = gensim.models.LdaMulticore(bow_corpus,\n                                       num_topics=10,\n                                       id2word=dictionary,\n                                       passes=2,\n                                       workers=2)","0f47aff4":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nfor idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","cec93d37":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                             num_topics=10,\n                                             id2word=dictionary,\n                                             passes=2,\n                                             workers=4)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","86084742":"#5th row, 2nd column \n\ntrain.iloc[5,1]","c932b2ba":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nunseen_document = 'I Haven t Been Honest About My Injury.. Here s THE TRUTH'\nbow_vector = dictionary.doc2bow(preprocess(unseen_document))\n\nfor index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))","79055b14":"train['comments_disabled'] = train['comments_disabled'].astype(int)","d40e9258":"train['comments_disabled'] = pd.Categorical(train['comments_disabled']) ","100953fd":"(train['comments_disabled'].value_counts(normalize=True))","17bc5ff5":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer","85ec2dff":"processed_text = train['clean_title']","a0635af1":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nvectorizer = TfidfVectorizer()\ntfidf = vectorizer.fit_transform(processed_text)\nprint(tfidf.shape)\nprint('\\n')\n#print(vectorizer.get_feature_names())","08a4883c":"corrmat = round(train.corr(method='pearson'),2)","55736967":"plt.figure(figsize=(12,6))\nsns.heatmap(corrmat.iloc[1:2,:25], vmax=1.0, vmin=-1.0, square=True, annot=True, cmap='summer')\nplt.show()","9682da69":"from PIL import Image\n\nimage = Image.open(\"..\/input\/kaggle-pog-series-s01e01\/thumbnails\/-JQz3DfuZLk.jpg\")\nimage","51d120ef":"#Video with more dislikes: \"BLACKPINK - 'Ice Cream (with Selena Gomez)' M\/V\"","8076dae7":"#Create TF\/IDF again??","b76339cc":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQpMozJypSzr58bRN3aoRv7PLSjnjEb3NlDBw&usqp=CAU)instafollowers.co","fd1eb35b":"#After some hours of unsuccessful imputation\/encoding\/feature engineering I gave up. And left a Thumbnail image.  ","5dc83b92":"#Create Corpus -> term document frequency\n\ndoc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word ID and returns the result as a sparse vector.","43dce875":"#Using Reverse Psychology To Get ANYTHING You Want","4ed616a0":"#Check class balance","65df10bd":"#More Likes: BTS (\ubc29\ud0c4\uc18c\ub144\ub2e8) 'Butter' Official MV ","7ebe6e07":"#TF\/IDF","4392e446":"#Videos with Dislikes","6ee18636":"\"POG is an acronym that means \u201cplay of the game\u201d, but is mainly used by gamers as an expression after something incredible, epic, or exciting has taken place. POGGERS can be used synonymously with POG.\"\n\nhttps:\/\/www.quora.com\/What-does-pog-mean-on-Twitch","d08a459e":"#Cleaning functions","3a9c528f":"#Create the dictionary\n\nEvery unique word in titles","85ca9d87":"#Predict comments disabled. It was supposed to predict the target (the ratio of like to view count). Though I'm doing another thing","0224c3c8":"#Show the output of the model"}}