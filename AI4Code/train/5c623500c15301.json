{"cell_type":{"4a04dd4d":"code","10902501":"code","ea9bd019":"code","823a6a2a":"code","fd80091e":"code","7a0175f7":"code","68dc392d":"code","0850c0d0":"code","a9cb7e08":"code","9cd22d7c":"code","1ecab7a1":"code","ac1159ec":"code","adbdb32f":"code","264ad201":"code","1ccdbded":"code","4aa5fca7":"markdown","3b268e41":"markdown","2a9d4a4f":"markdown","29a6d72c":"markdown","936be187":"markdown","fb7fa95b":"markdown","5a06839d":"markdown"},"source":{"4a04dd4d":"! curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n! python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","10902501":"! pip install transformers sentencepiece accelerate","ea9bd019":"from accelerate import Accelerator\nfrom accelerate import notebook_launcher","823a6a2a":"import os\nimport pandas as pd\nimport numpy as np\nimport copy\nimport csv\nimport json\nimport random\nfrom tqdm.auto import tqdm\nimport time\nimport matplotlib.pyplot as plt\nfrom bs4 import BeautifulSoup\nfrom pprint import pprint\nimport pickle\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport torch\n\nimport transformers\nfrom sklearn.metrics import (\n    roc_auc_score\n)\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler","fd80091e":"SEED = 42\n\n# paths\nDATA_DIR = \"..\/input\/stumbleupon\/\"\nTF_MODEL_PATH = \"roberta-base\"\nTRAINED_MODELS_PATH = f\"stumbleupon_{TF_MODEL_PATH}.pt\"\n\n# data\nTITLE_MAX_LENGTH = 64\nBODY_MAX_LENGTH = 1024\nTRAIN_BATCH_SIZE = 8\nVAL_BATCH_SIZE = 8\nVALIDATION_SPLIT = 0.2\nN_LABELS = 2\n\n# model\nTF_HIDDEN = 768\nFULL_FINETUNING = True\nLR = 3e-5\nOPTIMIZER = 'AdamW'\nCRITERION = 'CrossEntropyLoss'\nSAVE_BEST_MODEL = True\nEPOCHS = 1","7a0175f7":"df_train = pd.read_csv(DATA_DIR + \"train.tsv\", sep='\\t')\ndf_test = pd.read_csv(DATA_DIR + \"test.tsv\", sep='\\t')","68dc392d":"df_train","0850c0d0":"def get_df_text(df, is_test=False):\n    lem = WordNetLemmatizer()\n    stop_words = set(stopwords.words('english'))\n\n    def clean_text(s):\n        tokens = s.split()\n        no_stop = [tok for tok in tokens if tok.lower() not in stop_words]\n        lemmas = [lem.lemmatize(tok) for tok in no_stop]\n        return ' '.join(lemmas)\n    \n    def foo(s, keyword):\n        fin = \"\"\n        di = json.loads(s)\n        if keyword in di.keys() and di[keyword]:\n            fin = di[keyword]\n            \n        if fin == \"\":\n            if keyword == \"title\":\n                if \"url\" in di.keys() and di[\"url\"]:\n                    fin = di[\"url\"]\n                    fin = clean_text(fin)\n                else:\n                    fin = \"None\"\n            else:\n                fin = \"None\"\n        else:\n            fin = clean_text(fin)\n            \n        return fin\n\n    df_text = pd.DataFrame()\n    df_text[\"urlid\"] = df[\"urlid\"]\n    df_text[\"title\"] = df[\"boilerplate\"].apply(lambda x: foo(x, \"title\"))\n    df_text[\"body\"] = df[\"boilerplate\"].apply(lambda x: foo(x, \"body\"))\n    \n    if not is_test:\n        df_text[\"label\"] = df[\"label\"]\n    \n    return df_text","a9cb7e08":"df_train_text = get_df_text(df_train)\ndf_test_text = get_df_text(df_test, is_test=True)\ndf_train_text","9cd22d7c":"class TransformerDataset(torch.utils.data.Dataset):\n    def __init__(self, df, is_test=False):\n        super(TransformerDataset, self).__init__()\n        \n        self.titles = df[\"title\"].values\n        self.bodies = df[\"body\"].values\n        self.is_test = is_test\n        if not self.is_test:\n            self.labels = df[\"label\"].values\n\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(TF_MODEL_PATH)\n\n    def __len__(self):\n        return len(self.titles)\n    \n    def __getitem__(self, index):\n        title_tokenized = self.tokenizer.encode_plus(\n            str(self.titles[index]),\n            max_length=TITLE_MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n        body_tokenized = self.tokenizer.encode_plus(\n            str(self.bodies[index]),\n            max_length=BODY_MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n\n        if not self.is_test:\n            return {\n                \"title\": {\n                    \"input_ids\": title_tokenized[\"input_ids\"].squeeze().long(),\n                    \"attention_mask\": title_tokenized[\"attention_mask\"].squeeze().long()\n                },\n                \"body\": {\n                    \"input_ids\": body_tokenized[\"input_ids\"].squeeze().long(),\n                    \"attention_mask\": body_tokenized[\"attention_mask\"].squeeze().long()\n                },\n                \"labels\": torch.Tensor([self.labels[index]]).long().squeeze()\n            }\n\n        return {\n            \"title\": {\n                    \"input_ids\": title_tokenized[\"input_ids\"].squeeze().long(),\n                    \"attention_mask\": title_tokenized[\"attention_mask\"].squeeze().long()\n                },\n            \"body\": {\n                \"input_ids\": body_tokenized[\"input_ids\"].squeeze().long(),\n                \"attention_mask\": body_tokenized[\"attention_mask\"].squeeze().long()\n            },\n        }","1ecab7a1":"class TFDualHeadModel(torch.nn.Module):\n    def __init__(self):\n        super(TFDualHeadModel, self).__init__()\n\n        self.tf = transformers.AutoModel.from_pretrained(TF_MODEL_PATH)\n        self.dropout = torch.nn.Dropout(p=0.3)\n        self.output = torch.nn.Linear(TF_HIDDEN * 2, N_LABELS)\n\n    def forward(\n        self,\n        title_input_ids,\n        title_attention_mask,\n        body_input_ids,\n        body_attention_mask \n        ):\n\n        title_tf_out = self.tf(\n            input_ids=title_input_ids,\n            attention_mask=title_attention_mask\n        )\n        \n        title_drop = self.dropout(title_tf_out.pooler_output)\n        \n        body_tf_out = self.tf(\n            input_ids=body_input_ids,\n            attention_mask=body_attention_mask\n        )\n        \n        body_drop = self.dropout(body_tf_out.pooler_output)\n        \n        combined = torch.cat([title_drop, body_drop], dim=1)\n        x = self.output(combined)\n        \n        return x","ac1159ec":"def get_model(model_path):\n    print(\"\\n-- Loading model\")\n    model = TFDualHeadModel()\n    accelerator = Accelerator()\n    \n    unwrapped_model = accelerator.unwrap_model(model)\n    \n    chkpt = torch.load(model_path)\n    \n    unwrapped_model.load_state_dict(chkpt[\"state_dict\"]) \n    roc = chkpt[\"roc\"]\n    return unwrapped_model, roc\n\n\ndef load_data(df):\n    dataset_size = len(df)\n    indices = list(range(dataset_size))\n    split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n    \n    np.random.seed(SEED)\n    np.random.shuffle(indices)\n\n    train_indices, val_indices = indices[split:], indices[:split]\n    \n    df_train = df.iloc[train_indices]\n    df_val = df.iloc[val_indices]\n    \n    return df_train, df_val\n\n\ndef run(model_path=None, checkpoint=None):\n    torch.manual_seed(SEED)\n\n    # Initialize accelerator\n    accelerator = Accelerator()\n\n    df_train, df_val = load_data(df_train_text)\n    train_data = TransformerDataset(df_train)\n    val_data = TransformerDataset(df_val)\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_data, \n        batch_size=TRAIN_BATCH_SIZE,\n        drop_last=True\n    )\n\n    val_dataloader = torch.utils.data.DataLoader(\n        val_data, \n        batch_size=VAL_BATCH_SIZE,\n    )\n\n    # init model\n    if model_path:\n        model = get_model(model_path, checkpoint)\n    else:\n        accelerator.print('\\n-- Initializing Model')\n        model = TFDualHeadModel()\n    \n    criterion = getattr(torch.nn, CRITERION)()\n\n    # define the parameters to be optmized -\n    # - and add regularization\n    if FULL_FINETUNING:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = getattr(torch.optim, OPTIMIZER)(optimizer_parameters, lr=LR)\n\n    model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, val_dataloader\n    )\n\n    num_training_steps = len(train_dataloader) * EPOCHS\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    #######################\n    # training & validation\n    accelerator.print(\"\\n-- Training\")\n\n    max_val_roc = float('-inf')\n    for epoch in range(EPOCHS):\n        progress_bar = tqdm(\n            range(len(train_dataloader)), \n            desc=\"Epoch \" + str(epoch),\n            disable=not accelerator.is_main_process\n        )\n\n        # Training\n        train_loss = 0\n        for step, batch in enumerate(train_dataloader):\n            # set model.eval() every time during training\n            model.train()\n            \n            # unpack the batch contents and push them to the DEVICE (cuda or cpu).\n            b_title_input_ids = batch[\"title\"]['input_ids']\n            b_title_attention_mask = batch[\"title\"]['attention_mask']\n            b_body_input_ids = batch[\"body\"]['input_ids']\n            b_body_attention_mask = batch[\"body\"]['attention_mask']\n            b_labels = batch['labels']\n\n            # clear accumulated gradients\n            optimizer.zero_grad()\n\n            # forward pass\n            logits = model(\n                b_title_input_ids,\n                b_title_attention_mask,\n                b_body_input_ids,\n                b_body_attention_mask\n            )\n\n            # calculate loss\n            loss = criterion(logits, b_labels)\n            train_loss += loss.item()\n\n            # backward pass\n            accelerator.backward(loss)\n\n            # update weights\n            optimizer.step()\n            \n            # update scheduler\n            scheduler.step()\n\n            progress_bar.update(1)\n            progress_bar.set_postfix({\"loss\": loss.item()})\n        \n        avg_train_loss = train_loss \/ len(train_dataloader)\n        accelerator.print('Training loss:', avg_train_loss)\n\n        # Validation\n        val_loss = 0\n        preds = []\n        labels = []\n        \n        # set model.eval() every time during evaluation\n        model.eval()\n        \n        for step, batch in enumerate(val_dataloader):\n            b_title_input_ids = batch[\"title\"]['input_ids']\n            b_title_attention_mask = batch[\"title\"]['attention_mask']\n            b_body_input_ids = batch[\"body\"]['input_ids']\n            b_body_attention_mask = batch[\"body\"]['attention_mask']\n            b_labels = batch['labels']\n\n            with torch.no_grad():\n                logits = model(\n                    b_title_input_ids,\n                    b_title_attention_mask,\n                    b_body_input_ids,\n                    b_body_attention_mask\n                )\n\n            loss = criterion(logits, b_labels)\n            val_loss += loss.item()\n\n            b_pred = torch.argmax(logits, dim=1)\n\n            preds.append(accelerator.gather(b_pred))\n            labels.append(accelerator.gather(b_labels))\n\n        preds = torch.cat(preds)[:len(val_data)]\n        labels = torch.cat(labels)[:len(val_data)]\n\n        avg_val_loss = val_loss \/ len(val_dataloader)\n        accelerator.print('Val loss:', avg_val_loss)\n        val_roc = roc_auc_score(labels, preds)\n        accelerator.print('Val roc-auc:', val_roc)\n\n        if val_roc > max_val_roc:\n            accelerator.print(f\"-- Best Model. Val loss: {max_val_roc} -> {val_roc}\")\n            max_val_roc = val_roc\n            if SAVE_BEST_MODEL:\n                accelerator.print(\"-- Saving model.\")\n                accelerator.wait_for_everyone()\n                unwrapped_model = accelerator.unwrap_model(model)\n                accelerator.save(\n                    {\n                        \"state_dict\": unwrapped_model.state_dict(), \n                        \"roc\": max_val_roc\n                    },\n                    TRAINED_MODELS_PATH\n                )","adbdb32f":"try:\n    notebook_launcher(run())\nexcept ValueError:\n    pass","264ad201":"def predict(model, roc):\n    accelerator = Accelerator()\n    \n    test_data = TransformerDataset(df_test_text, is_test=True)\n    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=VAL_BATCH_SIZE)\n    \n    model, test_dataloader = accelerator.prepare(\n        model, test_dataloader\n    )\n    \n    preds = []\n    model.eval()\n    \n    print(\"\\n-- Predicting\")\n    tq = tqdm(test_dataloader, total=len(test_dataloader))     \n    for step, batch in enumerate(tq):\n        b_title_input_ids = batch[\"title\"]['input_ids']\n        b_title_attention_mask = batch[\"title\"]['attention_mask']\n        b_body_input_ids = batch[\"body\"]['input_ids']\n        b_body_attention_mask = batch[\"body\"]['attention_mask']\n\n        with torch.no_grad():\n            logits = model(\n                b_title_input_ids,\n                b_title_attention_mask,\n                b_body_input_ids,\n                b_body_attention_mask\n            )\n\n            b_pred = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            preds.extend(b_pred)\n\n    urlids = df_test_text[\"urlid\"]\n    submission = pd.DataFrame({\"urlid\": urlids, \"label\": preds})\n    roc = round(roc * 100, 2)\n    submission.to_csv(f\"{TF_MODEL_PATH}_titlebody_roc{roc}.csv\", index=False)","1ccdbded":"model, roc = get_model(TRAINED_MODELS_PATH)\npredict(model, roc)","4aa5fca7":"# Config","3b268e41":"# Model","2a9d4a4f":"# Imports","29a6d72c":"# Data Preprocessing","936be187":"# Dataset","fb7fa95b":"# Run","5a06839d":"# Inference & Submission"}}