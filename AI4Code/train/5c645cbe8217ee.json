{"cell_type":{"62a174ad":"code","6bafa6ba":"code","f9c20e2c":"code","bcb23fdf":"code","55ee2aba":"code","24dd9517":"code","6bf36c17":"code","acfc4920":"code","6e523291":"code","28e0e77b":"code","6b50dbde":"code","b8e928c7":"code","a9c2dd46":"code","f02a02fb":"code","57cd9ac1":"code","9f0efc70":"code","0532c0dd":"code","0e35ae14":"code","9e28cea9":"code","4bcfaa97":"code","18832c17":"code","89e7f8e5":"code","37185882":"code","8a904b9a":"code","0ae18f3f":"code","6e350801":"code","acd49f23":"code","f7c323b8":"code","bd78b929":"code","adcf7ad0":"code","47e2024c":"code","b49dbb2a":"code","b3e5bcef":"code","969855e0":"code","65b944b8":"code","ff6b1ded":"code","cd63a8b5":"code","1c18a652":"code","9a7937cd":"code","e0d253cc":"code","ab5a0a66":"code","66d985c3":"code","9be3996e":"code","790683a6":"code","c3d24f54":"code","1fe4e035":"code","9cdfb68d":"code","df19da5c":"code","5c529c0d":"code","5cd551a1":"code","683570aa":"markdown","d21385e4":"markdown","ab9c8fdc":"markdown","c40a7c8f":"markdown","d1b7de87":"markdown","18702e55":"markdown","8fca6a98":"markdown","816980f2":"markdown","82287e1d":"markdown","31f1cb9b":"markdown","c4d03de7":"markdown","76987763":"markdown","c9b6e90e":"markdown","709d1145":"markdown","af2a2e62":"markdown","940d4a66":"markdown","6e45763c":"markdown","7a235589":"markdown","fab98a46":"markdown","baed86dd":"markdown","0de4ce4b":"markdown"},"source":{"62a174ad":"# libraries\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# sklearn\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# LightGBM\nfrom lightgbm import LGBMRegressor, log_evaluation, early_stopping\n\n# Hyperparams tuning\nimport optuna","6bafa6ba":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\ndf_train.head(n=5)","f9c20e2c":"# train and test data sizes\n\nprint(\"Size of the train data: \", df_train.shape)\nprint(\"Size of the test data: \", df_test.shape)","bcb23fdf":"df_train.columns","55ee2aba":"df_train.describe()","24dd9517":"# Missing values\n# percentage\/ratio of the missing values by columns\n\nfor col, missing_ratio in (df_train.isnull().sum()\/df_train.shape[0]).to_dict().items():\n    if missing_ratio > 0:\n        print(col, \":\\t\", round(missing_ratio, 3))","6bf36c17":"# columns with more than one third of them have missing values\ndrop_columns = list(df_train.columns[df_train.isnull().sum()\/df_train.shape[0] > 0.33])\ndrop_columns","acfc4920":"df_train = df_train.drop(columns=drop_columns)\ndf_test = df_test.drop(columns=drop_columns)","6e523291":"# correlation of LotFrontage with other features: top 5\nfor k, v in df_train.corr()[\"LotFrontage\"].to_dict().items():\n    if v > 0.35 and v < 1.0:\n        print(k, \":\\t\", v)","28e0e77b":"# This function predicts the LotFrontage of the missing data values using a Linear Regression model \n# Build a linear regression model with known LotArea and LotFrontage and predicts the LotFrontage for \n# the data with missing values\n\ndef regression_coeffs(X_train, y_train):\n    X_train = X_train.reshape(len(X_train), 1)\n    y_train = y_train.reshape(len(y_train), 1)\n    reg = linear_model.LinearRegression()\n    reg.fit(X_train, y_train)\n    return reg.coef_[0][0], reg.intercept_[0] \n\n# linear reg coeffs\ntmp = df_train[[\"LotArea\", \"LotFrontage\"]].dropna()\nw, intercept = regression_coeffs(tmp.LotArea.values, tmp.LotFrontage.values)\nprint(f\"Regression params: weight={w}, intercept={intercept}\")\n\nfor i in range(len(df_train[\"LotFrontage\"])):\n    if pd.isnull(df_train.loc[i, \"LotFrontage\"]):\n        df_train.loc[i, \"LotFrontage\"] = df_train.loc[i, \"LotArea\"]*w + intercept","6b50dbde":"# No NaN values in the LotFrontage column left\ndf_train[\"LotFrontage\"].isnull().sum()","b8e928c7":"# Imputing in the Test set as well\nfor i in range(df_test[\"LotFrontage\"].shape[0]):\n    if pd.isnull(df_test.loc[i, \"LotFrontage\"]):\n        df_test.loc[i, \"LotFrontage\"] = df_test.loc[i, \"LotArea\"]*w + intercept","a9c2dd46":"# Let us see the distribution of \"MasVnrType\" in the data\ndf_train[\"MasVnrType\"].describe()","f02a02fb":"for i in range(len(df_train[\"MasVnrType\"])):  \n    if pd.isnull(df_train.loc[i, \"MasVnrType\"]) and pd.isnull(df_train.loc[i, \"MasVnrArea\"]):\n        df_train.loc[i, \"MasVnrType\"] = \"None\"\n        df_train.loc[i, \"MasVnrArea\"] = 0\n        \n# NaNs values in both MasVnrType and MasVnrArea are now removed\ndf_train[\"MasVnrType\"].isnull().sum(), df_train[\"MasVnrArea\"].isnull().sum()","57cd9ac1":"# same for test set as well\nfor i in range(len(df_test[\"MasVnrType\"])):  \n    if pd.isnull(df_test.loc[i, \"MasVnrType\"]) and pd.isnull(df_test.loc[i, \"MasVnrArea\"]):\n        df_test.loc[i, \"MasVnrType\"] = \"None\"\n        df_test.loc[i, \"MasVnrArea\"] = 0","9f0efc70":"# Imputation can be done for each columns manually like this - however, we will use boosting model that will be able to impute missing value \n# itself with reasonable effectiveness","0532c0dd":"df_train.dtypes","0e35ae14":"# numerical and categorical columns\nnumerical_vars = []\ncategorical_vars = []\n\nfor col in df_train.columns:\n    if df_train[col].dtype == \"object\":\n        categorical_vars.append(col)\n    else:\n        numerical_vars.append(col)\n\n# number of numerical and categorical features left\nlen(numerical_vars), len(categorical_vars)","9e28cea9":"interesting_cols = [\"OverallCond\", \"GrLivArea\", \"GarageCars\", \"YearBuilt\", \"LotArea\", \"SalePrice\"]\n\nplt.figure(figsize=(14,10))\nsns.pairplot(df_train[interesting_cols], dropna=True);\ndel interesting_cols;","4bcfaa97":"sns.histplot(x=\"YearBuilt\", data=df_train, bins=40);","18832c17":"# YearBuilt in test not included in train\nfor yr in df_test.YearBuilt.unique():\n    if yr not in df_train.YearBuilt.unique():\n        print(yr)","89e7f8e5":"# Sold Year\ndf_train.YrSold.unique(), df_test.YrSold.unique()","37185882":"# to categorical variable\ndf_train.YrSold = df_train.YrSold.astype(\"object\")\ndf_test.YrSold = df_test.YrSold.astype(\"object\")","8a904b9a":"# YearSold in test not included in train\nfor yr in df_test.YrSold.unique():\n    if yr not in df_train.YrSold.unique():\n        print(yr)","0ae18f3f":"df_train.YearBuilt.isnull().sum(), df_test.YearBuilt.isnull().sum()","6e350801":"# YearBuilt and YrSold: Calculate house Age from above info\ndf_train[\"Age\"] = df_train.YrSold - df_train.YearBuilt\ndf_test[\"Age\"] = df_test.YrSold - df_test.YearBuilt","acd49f23":"# Creating a categorical buckets for Year Built: will be use in addition to the YrSold as categorical features\ndef year_built_category(year):\n    \n    if year < 1900:\n        return \"1800s\"\n    decade = f\"{str(math.floor(year\/10)*10)}s\"\n    return decade\n        \n# df_train.YearBuilt = df_train.YearBuilt.apply(lambda x: year_built_category(x))\n# df_test.YearBuilt = df_test.YearBuilt.apply(lambda x: year_built_category(x))\n\ndf_train = df_train.drop(columns=[\"YearBuilt\"])\ndf_test = df_test.drop(columns=[\"YearBuilt\"])","f7c323b8":"# OverallQual: Boxplot\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=df_train);","bd78b929":"# Data Normalaization\n# Normalizing the right skewed SalePrice\n# Note: prediction wil be LogSalePrice --> need conversion back before submission\n\ndf_train[\"LogSalePrice\"] = df_train.SalePrice.apply(lambda x: math.log10(x))\n\nfig, ax =plt.subplots(1,2, figsize=(10, 3))\nsns.histplot(x='SalePrice', data=df_train, bins=70, kde=True, ax=ax[0])\nsns.histplot(x='LogSalePrice', data=df_train, bins=70, kde=True, ax=ax[1]);","adcf7ad0":"# Plottng the LotArea - SalePrice graph\n\nplt.scatter(df_train[\"LotArea\"], df_train[\"SalePrice\"])\nplt.xlabel(\"Lot Area\")\nplt.ylabel(\"Sale Price\")\nplt.show()","47e2024c":"corr_with_SalePrice = df_train.drop([\"Id\"], axis=1).corr()\nplot_data = corr_with_SalePrice[\"SalePrice\"].sort_values(ascending=True)\nplt.figure(figsize=(12,6))\nplot_data.plot.bar()\nplt.title(\"Correlations with the Sale Price\")\nplt.show()\ndel plot_data","b49dbb2a":"# Removing few columns (low correlation with SalePrice)?\ndrop_columns = [\"LowQualFinSF\", \"MiscVal\", \"BsmtHalfBath\", \"BsmtFinSF2\"]\n\ndf_train = df_train.drop(columns=drop_columns)\ndf_test = df_test.drop(columns=drop_columns)","b3e5bcef":"# categorical variable and unique item by variable\nfor col in df_train.columns:\n    if df_train[col].dtype == \"object\":\n        print(col, \":\\t\", df_train[col].nunique())","969855e0":"# violinplot: for all columns\/decorations in the categorical column list\n\nfew_cat_variables = ['KitchenQual', 'BsmtQual', 'Heating', 'ExterQual', 'LandSlope', 'HeatingQC', 'Foundation', 'Electrical', \\\n                     'LandContour', 'LotShape', 'CentralAir', 'SaleType']\n# categorical_list => plotted all the variables in this list before showing only few of them in the above list\nfor i in range(len(few_cat_variables)):\n    sns.violinplot(x=few_cat_variables[i], y='SalePrice', data=df_train)\n    plt.show()","65b944b8":"df_train.Neighborhood.value_counts()","ff6b1ded":"plt.figure(figsize=(10, 4))\nsns.histplot(x=\"SalePrice\", hue=\"Neighborhood\", data=df_train, bins=30);\nplt.xticks(rotation=90);","cd63a8b5":"df_train.shape, df_test.shape","1c18a652":"y = df_train.LogSalePrice\nX = df_train.drop(columns=[\"Id\", \"SalePrice\", \"LogSalePrice\"])\n\n# test data\nX_test = df_test.drop(columns=[\"Id\"])","9a7937cd":"categorical_features = []\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        categorical_features.append(col)","e0d253cc":"# Label encoding\nfor col in categorical_features:\n    encoder = LabelEncoder()\n    X[col] = X[col].astype(str)\n    X_test[col] = X_test[col].astype(str)\n    \n    encoder.fit(pd.concat([X[col], X_test[col]]))\n    \n    X[col] = encoder.transform(X[col])\n    X_test[col] = encoder.transform(X_test[col])","ab5a0a66":"# Cat feature indices on X\ncat_indices = []\nfor c in categorical_features:\n    if c in X.columns:\n        idx = list(X.columns).index(c)\n        cat_indices.append(idx)","66d985c3":"# Hyperparams tuning with Optuna\ndef objective(trial, data=X,target=y):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)\n    \n    params = {\n                'metric': 'rmse', \n                'random_state': 22,\n                'n_estimators': 20000,\n                'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"goss\"]),\n                'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n                'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n                'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n                'subsample': trial.suggest_categorical('subsample', [0.6, 0.7, 0.85, 1.0]),\n                'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.01, 0.02, 0.03, 0.05, 0.1]),\n                'max_depth': trial.suggest_int('max_depth', 2, 12, step=1),\n                'num_leaves' : trial.suggest_int('num_leaves', 13, 148, step=5),\n                'min_child_samples': trial.suggest_int('min_child_samples', 1, 96, step=5),\n            }\n    \n    reg = LGBMRegressor(**params)  \n    reg.fit(X_train ,y_train,\n            eval_set=[(X_valid, y_valid)],\n            #categorical_feature=cat_indices,\n            callbacks=[log_evaluation(period=1000), \n                       early_stopping(stopping_rounds=50)\n                      ],\n           )\n    \n    y_pred = reg.predict(X_valid)\n    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n    \n    return rmse","9be3996e":"params_search = True\n# # Optuna: run study trials\n\nif params_search:\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=120)","790683a6":"# Results from Hyperparameters tuning\nif params_search:\n    print('Totalnumber of trials: ', len(study.trials))\n    print(f\"Best RMSE score on validation data: {study.best_value}\")\n\n    print(\"-\"*30)\n    print('Best params:')\n    print(\"-\"*30)\n    for param, v in study.best_trial.params.items():\n        print(f\"{param} :\\t {v}\")","c3d24f54":"# K-FOLD Cross-validation training and Prediction on test data\n# Modeling with Best params\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\ncolumns = X.columns\nsplits = folds.split(X, y)\n\ny_preds = np.zeros(X_test.shape[0]) \ncv_score = 0\nfor fold_n, (train_idx, valid_idx) in enumerate(splits):\n    print(f\"FOLD: {fold_n}\")\n    \n    X_train, X_valid = X[columns].iloc[train_idx], X[columns].iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx] \n    \n    # further manually tune params from best params from Optuna\n    params = {\n             'n_estimators': 20000,\n             'boosting_type': \"gbdt\",\n             'reg_alpha': 1.0,\n             'reg_lambda': 2.0,\n             'colsample_bytree': 0.70,\n             'subsample': 1.0,\n             'learning_rate': 0.02,\n             'max_depth': 4,\n             'num_leaves': 65,\n             'min_child_samples': 3,\n             }\n    \n    reg = LGBMRegressor(**params) # **study.best_trial.params\n\n    reg.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[log_evaluation(period=100), \n                       early_stopping(stopping_rounds=100)\n                      ],\n           )\n    \n    # prediction on the test set\n    y_preds += reg.predict(X_test)\/NFOLDS   \n    # cross-validation score\n    cv_score += mean_squared_error(y_valid, reg.predict(X_valid), squared=False)\/NFOLDS","1fe4e035":"print(f\"Cross-validation mean RMSE score = {cv_score}\")","9cdfb68d":"sample_sub = pd.read_csv(\"\/kaggle\/input\/sample_submission.csv\")\nsample_sub.head(2)","df19da5c":"submission = pd.DataFrame(data={\"Id\": df_test.Id.values, \"SalePrice\": y_preds})\nsubmission.SalePrice = submission.SalePrice.apply(lambda x: round(10**x, 3))\nsubmission.head()","5c529c0d":"submission.to_csv(\"submission.csv\", index=False)","5cd551a1":"# Done!","683570aa":"### Regression Modelling and Prediction on Test Data\n\nNow, we take the cleaned data above, df_train, and carry out prediction analysis with different regression methods from sklearn-library. We will compare the accuracy of different regression methods with cross_val_score and mean squared error.","d21385e4":"The test data has one less column than that in train data as it doesn't have the regression target column \"SalePrice\".\n\n_Let us explore all the columns\/decorations in the train data in more vivid way:_","ab9c8fdc":"Let us now look at the 'SalePrice' variation on different categories of categorical variables\/columns. This will give us some idea about the columns that are important for us, and which will be considered further.","c40a7c8f":"- Plotting few important numerical columns ","d1b7de87":"### Correlation with 'SalePrice'\n\nLet us check the correlation of remaining numerical variables with 'SalePrice' once again now.","18702e55":"## Categorical columns\n\nWe looked into numerical columns\/decorations above and got some rough idea about their distribution and importance on the 'SalePrice' determination. The final\/important columns of them will be considered for the prediction model development later. There are also many columns that don't have numerical values, rather they have descriptive categorical values. Now, we will concentrate on those categorical columns below.","8fca6a98":"### House Built Year, Sold Year, and Age","816980f2":"Imputing the data with missing LotFrontage by using linear regression model. It seems that LotFrontage has somewhat linear relation with LotArea, therefore, we  can impute the missing LotFrontage data by the linear regression with LotArea as follow:","82287e1d":"#### Importing training and testing data","31f1cb9b":"The four data points on the far right side of the graph are outliers in the data set, based on LotArea\/SalePrice distribution, and we can remove these four data points from the dataset (not doing as non-linear model should be able to address this).","c4d03de7":"### Outliers! \n\nLet us look at some of the outliers data points in the most important variables to be used in the prediction model. We will drop such data points entirely from the train data depending upon their presence in the important columns\/decorators.  Here, we will are looking outliers in 'LotArea' and 'GrLivArea' variables. ","76987763":"\n### Correlation between the variables:\n\nLet us draw heatmap to study the correlations between different variables of SalePrice of the houses. We will then list the most important varibles looking at the heatmap. The variables with highest correlation with SalesPrice will be important for the further analysis and will be considered ahead.","c9b6e90e":"We can see the correlation of remaining numerical columns\/decorations on 'SalePrice'. The columns that have clear correlation (high positive or high negative) are important for the prediction model, but few of those with small (about zero) correlation will not have much effect on the 'SalePrice', therefore, we can still drop few of them.","709d1145":"***\n\n### HOUSE PRICES DATA : COMPREHENSIVE DATA ANALYSIS & PREDICTION MODELLING\n\n***\n\nStarting with looking into deeper in the dataset, we will try to understand the given train dataset in more detail. 'SalePrice' being the most important variable in the dataset, we will then explore the correlation of 'SalePrice' with other variables. In the first part of this notebook, we will basically, by using python - pandas and seaborn packagaes, try to understand the data in more deeper and visualize in appropriate ways to make it vivid. \n\n### LB ~ 0.12 [Top 20%], CV RMSE ~ 0.05\n\n\n#### *Content:*\n- Basic EDA & Visualizations\n- Optuna Hyperparameters tuning with LightGBMRegressor\n- K-Fold Cross-validation training and Test set prediction with LightGBMRegressor","af2a2e62":"##### 2008 Housing Crash\n- House price in 2006 vs 2010? There should be a singnificant effect because of the price correction. Age (will create a separate feature later) will not be enough to capture it.","940d4a66":"Looking at the above Boxplot, it can be seen that the average SalePrice is almost directly proportional to the OverallQual of the house. Therefore, OverallQual is very important variable to take into account for further calcualtions.","6e45763c":"Since most of the houses have \"MasVnrType\" values \"None\", let us replace remaining 8 values with \"None\" type and corresponding \"MasVnrArea\" value of 0","7a235589":"### Missing value imputation","fab98a46":"### Okay,\nThe categorical effect on 'SalePrice' for most of the columns\/decorations is not clearly conclusive. Still the categories can be ideally scaled into different prioritical numerical values looking on above plots. But we are not going to be that much precise, don't want to make it so complicated. Rather we will only concentrate on few of the variables which have comparatively clear effect on 'SalePrice' based on their categories. As just metioned, there might different preference but just going through above plots, I am going to take following columns into consideration. To decide, I have tried to consider both the mean and kernel density of the columns on different categories, definitely as allowed by my eyes and instinct in a quick going through!","baed86dd":"### \"MasVnrType\" and \"MasVnrArea\" Imputation\n\n***\nHere, we will see all the possible 'MasVnrType' and find out the most frequent type. Since only 8 of the data points have missing values, we will simply replace them with the most frequent type of 'MasVnrType', which is None type as obtained below, and the corresponding 'MasVnrArea' will be set to be 0.","0de4ce4b":"### Categorical and Numerical features:\n\nOut of the 66 remaining columns\/decorations left above, let us separate categorical and numerical variables."}}