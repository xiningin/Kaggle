{"cell_type":{"e80d2a09":"code","bf26ff4e":"code","9fdf9283":"code","155b36d8":"code","6ec51f03":"code","7d13c917":"code","05510952":"code","3737f277":"code","6bcbc113":"code","dce1b4da":"code","fac92cc2":"code","b8151d76":"code","c86e1327":"code","4fc289f8":"code","fe80d803":"code","a373ac0c":"code","eb7f33e1":"code","76a473eb":"code","2d23c629":"code","69ba6787":"code","3badc740":"code","378a74ea":"code","d6f2d145":"code","f81058c2":"code","a1a52e25":"code","8a0c22a1":"code","e0946858":"code","73e7483d":"code","1ba1e5d7":"code","84e33606":"markdown","2e700a80":"markdown","33cddded":"markdown"},"source":{"e80d2a09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf26ff4e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau\nimport cv2\nimport os","9fdf9283":"labels = ['PNEUMONIA', 'NORMAL']\nimg_size = 180\n\ndef get_training_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","155b36d8":"train = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')","6ec51f03":"# Train\ntrain_list = []\n\nfor i in train:\n    if(i[1] == 0):\n        train_list.append(\"Pneumonia\")\n    else:\n        train_list.append(\"Normal\")\nsns.set_style('darkgrid')\nsns.countplot(train_list) ","7d13c917":"# Val\nval_list = []\n\nfor i in val:\n    if(i[1] == 0):\n        val_list.append(\"Pneumonia\")\n    else:\n        val_list.append(\"Normal\")\nsns.set_style('darkgrid')\nsns.countplot(val_list) \n","05510952":"# Test\ntest_list = []\n\nfor i in test:\n    if(i[1] == 0):\n        test_list.append(\"Pneumonia\")\n    else:\n        test_list.append(\"Normal\")\nsns.set_style('darkgrid')\nsns.countplot(test_list) ","3737f277":"# Train\nplt.figure(figsize = (5,5))\nplt.imshow(train[0][0], cmap='gray')\nplt.title(labels[train[0][1]])\n\nplt.figure(figsize = (5,5))\nplt.imshow(train[-1][0], cmap='gray')\nplt.title(labels[train[-1][1]])","6bcbc113":"# Test\nplt.figure(figsize = (5,5))\nplt.imshow(test[0][0], cmap='gray')\nplt.title(labels[test[0][1]])\n\nplt.figure(figsize = (5,5))\nplt.imshow(test[-1][0], cmap='gray')\nplt.title(labels[test[-1][1]])","dce1b4da":"# val\nplt.figure(figsize = (5,5))\nplt.imshow(val[0][0], cmap='gray')\nplt.title(labels[val[0][1]])\n\nplt.figure(figsize = (5,5))\nplt.imshow(val[-1][0], cmap='gray')\nplt.title(labels[val[-1][1]])","fac92cc2":"x_train = []\ny_train = []\n\nx_val = []\ny_val = []\n\nx_test = []\ny_test = []\n\nfor feature, label in train:\n    x_train.append(feature)\n    y_train.append(label)\n\nfor feature, label in test:\n    x_test.append(feature)\n    y_test.append(label)\n    \nfor feature, label in val:\n    x_val.append(feature)\n    y_val.append(label)","b8151d76":"x_train[0]","c86e1327":"# Normalize the data\nx_train = np.array(x_train) \/ 255\nx_val = np.array(x_val) \/ 255\nx_test = np.array(x_test) \/ 255","4fc289f8":"x_train[0]","fe80d803":"# resize data for deep learning \nx_train = x_train.reshape(-1, img_size, img_size, 1)\ny_train = np.array(y_train)\n\nx_val = x_val.reshape(-1, img_size, img_size, 1)\ny_val = np.array(y_val)\n\nx_test = x_test.reshape(-1, img_size, img_size, 1)\ny_test = np.array(y_test)","a373ac0c":"x_train[0]","eb7f33e1":"# define generator\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    zoom_range = 0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\n# fit generator on our train features\ndatagen.fit(x_train)","76a473eb":"model = Sequential()\n\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (180,180,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units = 128 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\n\n\nmodel.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","2d23c629":"import tensorflow as tf\n\nlr_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                            patience = 3,\n                                            verbose=1,\n                                            factor=0.3,\n                                            min_lr=0.000001)\n\n\n\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\",\n                                                    save_best_only=True)\n","69ba6787":"epochs = 15\nbatch_size = 32\nhistory = model.fit(datagen.flow(x_train,y_train, \n                                 batch_size = batch_size),\n                    epochs = epochs , \n                    validation_data = datagen.flow(x_val, y_val) ,\n                    callbacks = [checkpoint_cb, lr_reduction])","3badc740":"print(\"Loss of the model is - \" , model.evaluate(x_test,y_test)[0])\nprint(\"Accuracy of the model is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n","378a74ea":"range_epochs = epochs\n\nepochs = [i for i in range(range_epochs)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\nax[0].set_title('Training & Validation Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Training & Validation Loss\")\nplt.show()","d6f2d145":"predictions = model.predict_classes(x_test)\npredictions = predictions.reshape(1,-1)[0]\npredictions[:15]","f81058c2":"print(classification_report(y_test, predictions, target_names = ['Pneumonia (Class 0)','Normal (Class 1)']))","a1a52e25":"cm = confusion_matrix(y_test,predictions)\ncm","8a0c22a1":"cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = labels,yticklabels = labels)","e0946858":"correct = np.nonzero(predictions == y_test)[0]\nincorrect = np.nonzero(predictions != y_test)[0]","73e7483d":"i = 0\nfor c in correct[:6]:\n    plt.subplot(3,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_test[c].reshape(180,180), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]))\n    plt.tight_layout()\n    i += 1","1ba1e5d7":"i = 0\nfor c in incorrect[:6]:\n    plt.subplot(3,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_test[c].reshape(180,180), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]))\n    plt.tight_layout()\n    i += 1","84e33606":"# Import Package","2e700a80":"# Loading Data","33cddded":"# Data Visualization & Preprocessing"}}