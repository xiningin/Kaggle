{"cell_type":{"76dced6e":"code","172d7903":"code","3bfdc5b2":"code","154ba2c3":"code","222e58c6":"code","e3a5079b":"code","9228e225":"code","085667b0":"code","53213e8e":"code","97d71418":"code","fae6f5ea":"code","ee2c95b3":"code","b0c2521d":"code","b38ae95b":"code","ae9c87fd":"code","5e35661c":"markdown","13895934":"markdown","4ccc5e8b":"markdown","703eb6fc":"markdown","01db6445":"markdown","88f7d7db":"markdown","f2c83727":"markdown","a3b654ac":"markdown","5d09aff7":"markdown","4138d4c2":"markdown","969e7dc4":"markdown","4fb728a6":"markdown"},"source":{"76dced6e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","172d7903":"train = pd.read_csv(r\"\/kaggle\/input\/now-mozilla-club-mpstme\/train.csv\")\ntest = pd.read_csv(r\"\/kaggle\/input\/now-mozilla-club-mpstme\/test.csv\")\n\ny = train[\"Happiness Score\"]\ntrain.drop([\"Happiness Score\", \"Country\"], axis = 1, inplace = True)","3bfdc5b2":"train.shape, test.shape","154ba2c3":"train.head()","222e58c6":"train.info()","e3a5079b":"train.describe()","9228e225":"test.info()","085667b0":"# Checking correlations\ntemp = train.copy()\ntemp[\"Happiness Score\"] = y.copy()\nax = sns.heatmap(temp.corr(), annot=True)\nplt.title(\"checking correlation\")\nplt.show()","53213e8e":"ax = train.hist(bins=50, figsize=(20, 15))\nplt.title(\"distributions\")\nplt.show()","97d71418":"from sklearn.preprocessing import StandardScaler\n\nclass prepare:\n\n    def __init__(self):\n        return\n\n    def transform(self, df):\n\n\n        df_ret = df.copy()\n\n        # fill na with median\n        df_ret[\"Trust\"] = df_ret[\"Trust\"].fillna(df_ret[\"Trust\"].median())\n\n        # scaling\n        from sklearn.preprocessing import StandardScaler\n        scaler = StandardScaler()\n        df_ret = scaler.fit_transform(df_ret)\n\n        return df_ret","fae6f5ea":"import random\n\nclass GDLinearRegressionModel():\n\n    def __init__(self, learning_rate = 0.01, iterations = 2000):\n        self.eta = learning_rate\n        self.iterations = iterations\n\n\n    def computeCostLinear(self, x, y, w, b, m):\n        sum = 0\n        for i, j in zip(x, y):\n            temp = i * w + b - j\n            sum += temp ** 2\n        return 1\/(2*m) * sum\n\n    def computeCostMulti(self, X, y, w, b, m):\n        J = 1\/(2*m) * np.sum((X.dot(w) + b - y)**2)\n        return J\n\n    def get_wb_linear(self, X, y, m):\n        self.history = []\n        w, b = 0, 0\n\n        for i in range(self.iterations):\n            temp = b - self.eta * 1\/m * np.sum(((w * X + b) - y))\n            a = (X.T.dot((w * X + b) - y))\n            w = w - self.eta * 1\/m * (X.T.dot((w * X + b) - y))\n            b = temp\n            self.history.append(self.computeCostLinear(X, y, w, b, m))\n        self.intercept, self.slope = b, w\n        return\n\n\n    def get_wb_multi(self, X, y, m):\n        b = np.array([0])\n        w = np.array([0] * len(X.T))\n        self.history = []\n        for i in range(self.iterations):\n            temp = b - self.eta * 1\/m * np.sum(((X.dot(w) + b) - y))\n            w = w - (self.eta * 1\/m * X.T.dot((X.dot(w) + b) - y))\n            b = temp\n            self.history.append(self.computeCostMulti(X, y, w, b, m))\n        self.intercept, self.slope = b, w\n        return\n\n    def fit(self, X, y):\n        m = len(y)\n        try:\n            lx = len(X[0])\n            try:\n                self.get_wb_multi(X.T, y, m)\n            except:\n                print(\"error in multi\")\n        except:\n            self.get_wb_linear(X, y, m)\n        return\n\n    def predict(self, X, w = None, b = None):\n        if w == None:\n            w = self.slope\n            b = self.intercept\n\n        return X.dot(w) + b\n","ee2c95b3":"# preprocessing\npre = prepare()\nX = pre.transform(train)","b0c2521d":"# fit model\nmodel = GDLinearRegressionModel(learning_rate = 0.001, iterations = 3000)\nmodel.fit(X.T, y)\n\nprint(\"Slope is\", model.slope)\nprint(\"Intercept is\", model.intercept)","b38ae95b":"# plot error\nfrom sklearn.metrics import mean_absolute_error\nprint(\"mae with predicted wights\", mean_absolute_error(y, model.predict(X)))\nplt.plot(model.history)\nplt.title(\"Cost vs iterations\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"cost\")\nplt.show()","ae9c87fd":"# Final predictions\ntest_countries = test[\"Country\"]\ntest.drop(\"Country\", axis = 1, inplace = True)\npre_test = pre.transform(test)\npreds = model.predict(pre_test)\nsubmission = pd.DataFrame({\"Country\": test_countries, \"Happiness Score\": preds})\nsubmission.to_csv(r\"submission.csv\", index = False)","5e35661c":"# Preprocessing","13895934":"## Visualization","4ccc5e8b":"# EDA","703eb6fc":"## Understanding data","01db6445":"### Importing data and important libraries","88f7d7db":"Whoa! The unkown feature, *X1*, does not have a strong correlation with *Happiness Score*.\nWe may choose to drop this. But, since I am using a Gradient Descent algorithm to find weights of the features, the *X1* feature will be taken care of. This way, we can use this feature in whatever little way it can contribute.","f2c83727":"### Things to note:\n\n* Less than 100 values in training set. That is way to less to fit fancy models. Since it is a regression task, we must go with the most simple type of models. This dataset, otherwise, will be severly prone to OVERFITTING. I would like to go with a basic linear regression model. I have my own Gradient Descent Algorithm. Let's hope it yields a satisfactory model.\n* There is an unkown feature X1, let's see how important that is later.\n* One missing value in *Trust*. We shall simply fill it with the median.","a3b654ac":"# Training Models\nAnd, here comes the interesting part: MACHINE LEARNING. ","5d09aff7":"Since this dataset is too small, no fancy modelling as well as no fancy preprocessing. I will be doing only the following things:\n* Filling NA with median for *Trust*\n* Scaling the features using StandarScaler (this is essential for the Gradient descent algorithm)\n\nP.S. - I tried Normalizer as well, just to explore. Doesn't give satisfactory results.","4138d4c2":"## training models","969e7dc4":"## Linear Regression model using Gradient Descent","4fb728a6":"# Takeaway from the competition:\n* Go for fancy models only after trying the models that work on fundamental concepts. \n* Public leaderboard is a scam, lol! Believe in <strike>yourself<\/strike> your model!!!"}}