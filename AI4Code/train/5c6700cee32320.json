{"cell_type":{"ca858b89":"code","9a213ef1":"code","d389adac":"code","64f21e9c":"code","5d21b1f2":"code","97ba4d40":"code","c6c1c924":"code","94dea4d1":"code","110f02b9":"code","eeb0f497":"code","34082155":"code","cfc9850c":"code","4bc10e3a":"code","c72b4ab9":"code","7348d27d":"code","2955911c":"code","869ef140":"code","a2996a0f":"code","60b2750d":"code","a6f440e6":"code","4d8f0ab0":"code","611961af":"code","236d8270":"code","9bd3a268":"code","29e04971":"code","fd2750c0":"code","333eaa92":"code","6b9e63e7":"code","40ff66f1":"code","81d842f2":"code","2b50bb57":"code","b34422d1":"code","cc51490b":"code","abede6ee":"code","334c0b3e":"code","f82d1aa3":"code","97c10d3f":"code","1a9946d2":"code","43be6a84":"code","dd80614e":"code","0e88e9da":"markdown","12fa78a1":"markdown","8e81d222":"markdown","0d3dcc4f":"markdown","1f0a037e":"markdown","829366d7":"markdown","67243ba8":"markdown","71d5e00c":"markdown","a345d0e0":"markdown","0d336364":"markdown","52cef678":"markdown","59b88f40":"markdown","ff2dab8f":"markdown","0b49160d":"markdown","6fe38f81":"markdown","680b6f34":"markdown","5a463fa7":"markdown","fbcc85ad":"markdown","cc195a3e":"markdown","124f34d5":"markdown","3ea7fbdb":"markdown","41942faf":"markdown","433d95d1":"markdown","3bbe91b0":"markdown","b7abf435":"markdown","ba25fd01":"markdown","5bfb36dd":"markdown","b63bea3d":"markdown","dd71333e":"markdown","413b59b8":"markdown","0a065c97":"markdown","d5a62ae8":"markdown","120b961b":"markdown","90a5267e":"markdown","8342fbfb":"markdown","13a1736c":"markdown","714feb85":"markdown"},"source":{"ca858b89":"import numpy as np\nimport pandas as pd\n\n# read in the csv into a DataFrame\ndf = pd.read_csv('..\/input\/train.csv')","9a213ef1":"# view the first 5 rows\ndf.head()","d389adac":"# view a summary of column names, counts, and data types\ndf.info()","64f21e9c":"# view statistical summary of numerical features\ndf.describe()","5d21b1f2":"import matplotlib.pyplot as plt\ndf.hist(bins=50, figsize = (30,20))\nplt.show()","97ba4d40":"housing = df.copy()","c6c1c924":"corr_matrix = housing.corr()","94dea4d1":"# view list of attributes correlating with the sales price\nattributes = corr_matrix['SalePrice'].sort_values(ascending = False)\nattributes","110f02b9":"# top 10 attributes\ntop10 = attributes[1:11]\nattributes[:11]","eeb0f497":"from pandas.plotting import scatter_matrix\n\n# scatter plot of the top 5 attributes\nscatter_matrix(housing[attributes[:6].index], figsize = (20,20));","34082155":"housing.plot(kind = 'scatter', x = 'GrLivArea', y = 'SalePrice', alpha = 0.1);","cfc9850c":"# copy the training data without the final sales price\nhousing = df.drop('SalePrice', axis = 1)\n\n# copy only the final sales price\nhousing_labels = df['SalePrice'].copy()","4bc10e3a":"housing.head()","c72b4ab9":"housing_labels.head()","7348d27d":"# subset of housing data with only the top 10 features\ntop10 = list(top10.index)\nhousing = housing[top10]","2955911c":"housing.describe()","869ef140":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Pipeline constructor\nfull_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy = 'median')),\n        ('std_scaler', StandardScaler()),\n    ])\n\n# call the pipeline, fit and transform our housing data\nhousing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared","a2996a0f":"# place the standardized housing data into a DataFrame with the same columns and indices as our unprepared data\npd.DataFrame(housing_prepared, columns = housing.columns, index = housing.index).head()","60b2750d":"# compare to non-standardized\nhousing.head()","a6f440e6":"from sklearn.linear_model import LinearRegression\n\n# Linear Regression constructor\nlin_reg = LinearRegression()\n\n# fit our training data\nlin_reg.fit(housing_prepared, housing_labels)","4d8f0ab0":"# retrieve the first 5 rows and first 5 labels\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\n\n# predict the first 5 labels\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint('Predictions: ', lin_reg.predict(some_data_prepared))\nprint('Labels: ', list(some_labels))","611961af":"from sklearn.metrics import mean_squared_error\n\n# use our model to predict the final sales price\nhousing_predictions = lin_reg.predict(housing_prepared)\n\n# calculate the mean squared error between our prediction and the label\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\n\n# take the square root of our error\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","236d8270":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","9bd3a268":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","29e04971":"from sklearn.model_selection import cross_val_score\n\n# use our decision tree model with the prepared data, the labels, and give us 10 evaluation scores\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\n\n# calculate our error\ntree_rmse_scores = np.sqrt(-scores)","fd2750c0":"# function to display scores\ndef display_scores(scores):\n    print('Scores: ', scores)\n    print('Mean: ', scores.mean())\n    print('Standard Deviation: ', scores.std())\n    \ndisplay_scores(tree_rmse_scores)","333eaa92":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\n\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","6b9e63e7":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = forest_reg.predict(housing_prepared)\n\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","40ff66f1":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","81d842f2":"from sklearn.model_selection import RandomizedSearchCV\n\n# number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# iterate to find the best hyperparameters\nforest_reg = RandomForestRegressor()\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions = random_grid, n_iter = 5, cv = 5, scoring = 'neg_mean_squared_error')\n\n# fit the data\nrnd_search.fit(housing_prepared, housing_labels)","2b50bb57":"negative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse","b34422d1":"rnd_search.best_estimator_","cc51490b":"# print every score and the corresponding parameters\ncvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","abede6ee":"feature_importances = rnd_search.best_estimator_.feature_importances_\nfeature_importances","334c0b3e":"sorted(zip(feature_importances, housing.columns), reverse = True)","f82d1aa3":"# get our best model\nfinal_model = rnd_search.best_estimator_\n\n# load in the test set\nX_test = pd.read_csv('..\/input\/test.csv')\n\n# extract the top 10 features\nX_test_top10 = X_test[top10]\n\n# call our transformation pipeline\nX_test_prepared = full_pipeline.transform(X_test_top10)\n\n# make our final predictions\nfinal_predictions = final_model.predict(X_test_prepared)","97c10d3f":"# create a download link for our predictions\n\nfrom IPython.display import HTML\nimport base64\n\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index = False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create submission DataFrame with the corresponding Id\nsubmission = pd.DataFrame()\nsubmission['Id'] = X_test['Id']\nsubmission['SalePrice'] = final_predictions\n\ncreate_download_link(submission)","1a9946d2":"from tpot import TPOTRegressor\n\n# pipeline caching\nfrom tempfile import mkdtemp\nfrom joblib import Memory\nfrom shutil import rmtree\n\ntpot = TPOTRegressor(generations = 100, population_size = 100, memory = 'auto', \n                     warm_start = True, verbosity = 2, periodic_checkpoint_folder = 'checkpoint',\n                     scoring = 'neg_mean_squared_error', max_time_mins = 240)\n\ntpot.fit(housing_prepared, housing_labels)\ntpot.export('tpot_housing_data.py')","43be6a84":"# import best TPOT model\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom tpot.builtins import StackingEstimator\nfrom xgboost import XGBRegressor\n\n# Average CV score on the training set was:-833332153.5462229\nexported_pipeline = make_pipeline(\n    StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.65, tol=0.001)),\n    StackingEstimator(estimator=LassoLarsCV(normalize=False)),\n    XGBRegressor(learning_rate=0.1, max_depth=8, min_child_weight=3, n_estimators=100, nthread=1, objective=\"reg:squarederror\", subsample=0.9000000000000001)\n)\n\nexported_pipeline.fit(housing_prepared, housing_labels)\nresults = exported_pipeline.predict(X_test_prepared)","dd80614e":"# download best TPOT model predictions\ntpot_submission = pd.DataFrame()\ntpot_submission['Id'] = X_test['Id']\ntpot_submission['SalePrice'] = results\n\ncreate_download_link(tpot_submission)","0e88e9da":"The [`data_description.txt`](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) gives us a what each column means, but for now let's breakdown some of our observations:\n- There are 81 columns\n- There are 1460 rows\n- Many features are missing entries\n- There are ints, floats, and object data types.\n\nNow that we have some understanding of the structure, let's look at the statistical summary of the numerical features.","12fa78a1":"# Prepare the Data for Machine Learning Algorithms\n\nInstead of preparing the data manually, we will write a few functions to do this for us.\n\nFirst, let's separate the label from the features.","8e81d222":"# Introduction\n\nIn this notebook, I apply my learnings from the 2nd edition of the book [*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 2: Concepts, Tools, and Techniques to Build Intelligent Systems* by Aur\u00e9lien G\u00e9ron](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=as_li_ss_tl?crid=1RIE6GMGYGEB7&keywords=hands+on+machine+learning+with+scikit-learn+and+tensorflow&qid=1564493355&s=gateway&sprefix=hands+on+machine+l,aps,424&sr=8-4&linkCode=sl1&tag=lejimmy00-20&linkId=752dc952853b9f472dddde246b504cfb&language=en_US).\n\n# Look at the Big Picture\n\n__1. What's the problem?__\n\nLet's assume we work for a company like Zillow where their mission is to empower consumers with data, inspiration and knowledge around the place they call home, and connect them with the best local professionals who can help.\n\nTo make up a problem... let's say we are getting many complaints from homeowners and real estate professionals that our home estimates are drastically different from the market price which has negatively impacted the sale of their home.\n\nThis may indicate our current model in production is rotting and that we may need to retrain our model to be redeployed.\n\n__2. What does the current solution look like?__\n\nThe Zestimate\u00ae home valuation model is Zillow\u2019s estimate of a home's market value. The Zestimate incorporates public and user-submitted data, taking into account home facts, location and market conditions.\n\nAs of June 19, 2019:\n\n| Type       | Median Error | Homes with Zestimates | Within 5% of Sale Price | Within 10% of Sale Price | Within 20% of Sale Price |\n|------------|--------------|-----------------------|-------------------------|--------------------------|--------------------------|\n| Off-Market | 7.7%         | 97.5M                 | 36.2%                   | 59.2%                    | 80.5%                    |\n|  On-Market | 1.9%         | 1.6M                  | 83.7%                   | 95.3%                    | 98.8%                    |\n<br>\nWe encourage our consumers that the Zestimate is simply a starting point and not an official appraisal. We encourage buyers, sellers and homeowners to supplement the Zestimate with other research such as visiting the home, getting a professional appraisal of the home, or requesting a comparative market analysis (CMA) from a real estate agent.\n\nHowever, empowering consumers with data is a core mission of ours such that we should strive to improve our Zestimates.\n\n__3. How will you frame the problem?__\n\nSince we are given the *labeled* training examples, this is clearly a typicaly supervised learning task.  Since we are expected to predict the final price of each home, it is also a typical *univariate* regression task as we are only trying to predict a single value for each home.  Also, since we do not have a continuous flow of data and the data is small enough to fit into memory, plain batch learning should be fine.","0d3dcc4f":"Now... it doesn't look so good.  This must mean our Decision Tree was overfitting to the training data.  The mean error of \\$39,000 is on par with our Linear Regression model from earlier.  The cross-validation also tells us that the precision of our model is within \\$5,200.  In other words, we can say our Decision Tree has a score of approximately 39,010 \u00b1 5,195.\n\nJust to be sure, let's run the same cross-validation on our Linear Regression model from earlier.","1f0a037e":"Let's take a quick note of some observations:\n- We have homes that were built from 1872 to 2010\n- There is a huge range in lot areas.  From 1300 square feet (a typical 2 bedroom apartment) to 215,000 square feet (about 4 football fields)\n- The sales prices range from \\$35k - 755k with a mean of \\$180k\n- All sales occured between then years of 2006 - 2010\n\nTo get a quick feel for the data, let's plot a histogram for each numerical value.  Since some of our features are actually categorical values encoded as numbers, these will also be plotted.  But we'll address this at a later time.","829366d7":"## Create a Test Set\nAt this time, if we did not have a separate test and training set, we would use Scikit-Learn's `train_test_split` function to separate our data.  Otherwise, we may be victim to the *data snooping bias*.\n\nLuckily for us, this data set is already split for us into the testing and training sets.","67243ba8":"New score of .15833!  Looks like AutoML can improve our score by a little, however its probably best to revisit the features themselves.","71d5e00c":"Our Linear Regression model has a score of approximately 37,686 \u00b1 9,907.  Compared to our Decision Tree model, the Linear Regression model has a lower score, however it is not as precise.\n\nLet's try one last model for now: the `RandomForestRegressor`.\n\nThe Random Forest Regressor will train many Decision Trees from random subsets of the features, then averaging out their predictions.  Building models on top of each other is called *Ensemble Learning*.","a345d0e0":"In the first row and column, we can indeed we can see many of these attributes have a linear correlation with the sales price.  Let's take a closer look at the living area in square feet as it is also a numerical attribute.","0d336364":"Let's take a closer look at the description for the attributes mostly correlated with the sales price:\n- OverallQual: Rates the overall material and finish of the house from 1 to 10.  1 being very poor and 10 being very excellent.\n- GrLivArea: The above grade (ground) living area in square feet\n- GarageCars: The size of garage in car capacity\n- GarageArea: The size of garage in square feet\n- TotalBsmtSF: The total square feet of basement area\n\nAs a real estate professional, these results are pretty much inline with my experience with buyers and sellers.  The typical features buyers and sellers are usually focused on are the general living space, number of bedrooms, number of bathrooms, and the quality of the kitchen\/bathrooms.\n\nHowever, I'm surprised to see the pool square footage and month in which the home sold not having much affect on the sales price.  This can be explored further at a later time.\n\nWe can use pandas' `scatter_matrix` function to plot each attribute against each other.  Let's go ahead and do this for the top 5 attributes.  Otherwise there will be too much on the screen to discern.","52cef678":"Now that we have our predictions, let's download them and prepare for submission!","59b88f40":"Since the cross-validation feature expects a utility function where higher is better and our application is being measured by a cost function (lower is better), we are inputting the negative scores before computing the square root.","ff2dab8f":"# Get the Data\n\nFirst, let's import the data into a pandas DataFrame.\n\nIn our Workspace we'll select our training file to view it's path: \n\n![](https:\/\/i.imgur.com\/vSTgbhZ.png)\n\nWe'll copy this file path into our `read_csv()` function.","0b49160d":"Done!  Depending on how many number of iterations and folds you set in the RandomizedSearch, it can take quite awhile.  There is certainly a tradeoff between the incremental improvements by tuning the hyperparameters and how long the search may take.\n\nLet's look at what our best score was:","6fe38f81":"# Take a Quick Look at the Data Structure\n\nPandas will provide us with some high level tools to explore the data.  Now, we'll do some quick exploration to build some intuition around the data.","680b6f34":"## Feature Scaling\n\nSince most Machine Learning algorithms don't perform well when the numerical input attributes have very different scales, we will utilize Scikit-Learn's transformer called `StandardScaler` for standardization.\n\nThe `StandardScaler` will subtract each attribute by the mean of the attribute and divide by its standard deviation.\n\nLuckily, the top 10 attributes were all numerical.  However, if we also had categorical attributes, we would have some additional preprocessing steps.  To prepare for this, we will set up the foundations of our transformation Pipeline, even though we only have one step here.\n\n*(Since the testing set is missing one row, we will also include a `SimpleImputer` to fill in missing values with the median before standardizing the values)*","5a463fa7":"# Results\n\n1. On our first submission we scored a 0.16 and fell within the top 70%!  This is incredible given that we were able to accomplish this in a few hours.\n\n## Next Steps\n\nFrom here, we can iterate through our process as we learn and gain more skills to implement!","fbcc85ad":"Even better!  Our Random Forests score is approximately 32,782 \u00b1 6,955.\n\nSince our training score is much lower than our validation score, our models are still overfitting the data.  To improve our score, we can simplify the model, constrain the model, or obtain more training data.","cc195a3e":"Done!  That was easy right?  Let's look at a few instances of our training set to see how we did.","124f34d5":"Whoa, only an error of \\$828?!  Either our model is nearly perfect or that it is overfitting the data.  How can we be sure?  Instead of trying our model on the testing set, let's use part of the training set for our model validation.\n\n## Better Evaluation Using Cross-Validation\n\nWe could use Scikit-Learn's `train_test_split` function, however, Scikit-Learn also has a *K-fold cross-validation* feature built just for evaluation.  Let's use this feature to split the training set into 10 distinct subsets, train, and evaluate the Decision Tree model 10 times.  It will picka different fold for evaluation every time and train on the remaining 9 folds.  This will give us an array of 10 evaluation scores.","3ea7fbdb":"Not bad for our first try!  It looks like our 1st entry was about \\$5,000 (3%) off while our 4th entry was off by about \\$50,000 (30%).  To get a better idea of how we did overall, let's use Scikit-Learn's `mean_squared_error` function:","41942faf":"# Fine-Tune the Model\n\nLet's look at a few ways we can fine-tune our Random Forests model.\n\n## Randomized Grid Search\n\nThough we could go through and tweak some of the hyperparameters of the model manually, we'll utilize Scikit-Learn's `RandomizedSearchCV` to do this for us.\n\nFirst we need to specify a range of values for the Random Grid Search.\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) has a great article on hyperparameter tuning.  We'll borrow their parameter grid to get us started.","433d95d1":"# Experimental: Trying out TPOT\nI'm not sure if this is included in the book or not, but I'm ingtrigue about AutoML so I'm using this area to try out the TPOT library.","3bbe91b0":"## Data Cleaning\n\nTo simplify our model, we will only look at the top 10 features correlated with the sales price.\n\nEven though earlier we noticed some missing features, our top 10 features all contain 1460 entries.  Otherwise we would have to either drop the missing rows, drop the entire columns, or fill in the missing rows using Scikit-Learn's `SimpleImputer` class.","b7abf435":"Since we are most interested in the final sales price, we'll take a look at which attributes correlate the most with the final sales price.","ba25fd01":"We can also view which features gave us the most improvements to our score.  This can give us some insights on how to narrow our grid search for future improvements.","5bfb36dd":"For future reference: RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=20,\n                      max_features='sqrt', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=5,\n                      min_weight_fraction_leaf=0.0, n_estimators=800,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)","b63bea3d":"Not bad for our first prediction!  The mean sales price is approximately \\$180k, we're right around 20% off.  Considering that approximately 83% of the time, the current production solution is within 5% of the sales price, I think we're on the right track.\n\nFor our next model, let's train a `DecisionTreeRegressor`.  The process will be similar to our previous model.","dd71333e":"Some additional observations from the plots:\n- Most homes sold between the months of June and July.\n- Many features have a peak at the minium value.  I believe this indicates the particular home does not have that feature.  *e.g. Miscellaneous Value, 2nd Floor Sq Ft, Year Remodel Added, etc.\n- There are a mix of distributions and scales across all features","413b59b8":"# Select and Train a Model\n\nNow that we have framed the problem, explored the data, and cleaned up our data - let's select and train a Machine Learning model.\n\n## Training and Evaluating on the Training Set\n\nLet's start with a simple Linear Regression model.","0a065c97":"## Looking for Correlations\n\nWe can compute the *standard correlation coefficient* or *Pearson's r* for every pair of attributes using the `corr()` method.  This will let us know which attributes have a positive or negative correlation with one another.","d5a62ae8":"Our standardized housing data is now in a NumPy array, to preview what it looks like as a DataFrame:","120b961b":"## Evaluate the System on the Test Set\n\nNow that we've tweaked our model to our heart's content, let's evaluate the final model on the test set.  We will get the predictors and the labels from the test set, run our transformation pipeline, and evaluate the final model on the test set.","90a5267e":"## Analyze the Best Models and Their Errors\nEarlier, we simplified our model by only taking a look at the top 10 features that were highly correlated with the final sale price.\n\nLet's look at our best model, and see which features were actually the most important for making accurate predictions:","8342fbfb":"# Discover and Visualize the Data to Gain Insights\n\nLet's go a bit more in depth to learn more about our data.  To make sure we're not manipulating our data during our exploration, let's make a copy of the training set.","13a1736c":"Our score improved by another 3%!  Let's look at which parameters gave us the best score:","714feb85":"Of our top 10 features, it looks like the Overall Quality and Above Ground Living Area contributed to over 44% of the final sales price!  We can iterate with more of the features and simplify our model by analyzing the feature imporances in the future."}}