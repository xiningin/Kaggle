{"cell_type":{"d2997695":"code","f65051bd":"code","d0e6b100":"code","402f8155":"code","361bfc26":"code","57075d1b":"code","3a0da2d0":"code","4a0bac1a":"code","f5984c30":"code","83ce250d":"code","673e03ea":"code","5b937e04":"code","bfa6413d":"code","ee09d0db":"code","98aec6e6":"code","dc9a19cc":"code","28f83caa":"code","d9fc8fec":"code","a79157e2":"code","a88aaece":"code","08af672b":"code","b1c8e3d3":"code","fb2a1ef9":"code","5f795a02":"code","f842c3f5":"code","7c0c303f":"code","098ca8f6":"code","7b7eda40":"code","eb44895c":"code","d9112daa":"code","1fa86e46":"code","d161fab0":"code","630889a4":"code","6df18bd8":"code","b3079701":"code","0c6700e6":"code","68ef4e7b":"code","83431cc4":"code","3d5d4606":"code","7bf47ff0":"code","d0fe3e8a":"code","cf421365":"code","b5fe2a0a":"code","1d52f5b9":"code","455f8fbf":"code","788a3c44":"markdown","a1771e4b":"markdown","6ea8ff07":"markdown","dba2b821":"markdown","b8b2ab01":"markdown","f388d91d":"markdown","7c77ba93":"markdown","c8e1a4cd":"markdown","e5faf48d":"markdown","d2cc3027":"markdown","5f210137":"markdown","6c2dffba":"markdown","0e3bd767":"markdown","bf523302":"markdown","3e9336ad":"markdown","b1e20fb8":"markdown"},"source":{"d2997695":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f65051bd":"#data = pd.read_csv(\"..\/input\/train.csv\",dtype={'Year':'str','Qrtr':'str','Date':'str'})\ndata = pd.read_csv(\"..\/input\/train.csv\",dtype={'YearBuilt':'str','YrSold':'str','GarageYrBlt':'str','YearRemodAdd':'str'})\ndata.shape","d0e6b100":"data.info(verbose=True)","402f8155":"data.describe()","361bfc26":"profile = pp.ProfileReport(data)\nprofile.to_file(\"HousingSales.html\")\npp.ProfileReport(data)","57075d1b":"missing_list = data.columns[data.isna().any()].tolist()\ndata.columns[data.isna().any()].tolist()","3a0da2d0":"data.shape","4a0bac1a":"data_org = data\ndata_org.shape\ndata.drop(['Alley','MasVnrArea','PoolQC','Fence','MiscFeature'], inplace=True, axis=1)\ndata.shape","f5984c30":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical = ['object']\n\nfor cols in list((data.select_dtypes(include=numerics)).columns.values):\n    data[cols] =  data[cols].replace(np.nan,data[cols].median())\n    \nfor cols in list((data.select_dtypes(include=categorical)).columns.values):\n    data[cols] =  data[cols].replace(np.nan,\"Not_Available\")    ","83ce250d":"# Checking to see if all missing values have been taken care of \ndata.columns[data.isna().any()].tolist()","673e03ea":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\ntrain_numeric = data.select_dtypes(include=numerics)\ntrain_numeric.shape","5b937e04":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfor i in range(0,33):\n    plt.figure(figsize=(45, 10))\n    b = sns.boxplot(data = train_numeric.iloc[:,i:i+3])\n    b.tick_params(labelsize= 30)\n    i = i+3 ","bfa6413d":"#varstobetreated = ['Age','avg_change_in_efficiency_6m','avg_spl_inc_amt_3M','avg_deviation_avg_training_peers_3M','avg_perc_deviation_incentive_amt_peers_3M','avg_peer_bktx_cases_ratio_3M','OD_EFFICIENCY_3M','CD_EFFICIENCY_3M','time_last_supervisor_change','incentive_amount_lag_1','incentive_amount_lag_2','incentive_amount_lag_3','incentive_amount_lag_4','incentive_amount_lag_5','incentive_amount_lag_6','peer_od_cases_ratio_lag_2','peer_od_cases_ratio_lag_3','peer_od_cases_ratio_lag_4','AVG_OB_CASES_3M','perc_resignation_under_supervisor_3M']\nvarstobetreated = list(train_numeric.columns)\nfor cols in varstobetreated:\n    Q1 = data[cols].quantile(0.25)\n    Q3 = data[cols].quantile(0.75)\n    IQR = Q3 - Q1\n    Upper_Limit = Q3 + 1.5*IQR\n    Lower_Limit = Q1 - 1.5*IQR\n    data[cols] = np.where(data[cols] > Upper_Limit,Upper_Limit,data[cols])\n    data[cols] = np.where(data[cols] < Lower_Limit,Lower_Limit,data[cols])","ee09d0db":"# Filtering out variables\ndata.drop(['BsmtHalfBath', 'CentralAir', 'Condition2', 'Heating', 'RoofMatl', 'Street', 'Utilities', 'Heating', 'PoolArea', 'YearRemodAdd', 'YearBuilt', 'GarageYrBlt'],inplace=True, axis=1)\ndata.shape","98aec6e6":"colstokeep = ['Id','MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape',\n       'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n       'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n       'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n       'HeatingQC', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n       'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional',\n       'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageCars',\n       'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal',\n       'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice']","dc9a19cc":"def getFeatures(df , run_id, tuple_cols_toKeep, cat_ResponseVariable):\n    #response = df[cat_ResponseVariable]  == 'Y'\n    response = df[cat_ResponseVariable]\n    if(df is None):\n        df = get_data_from_SQL()\n    #writeFrameToCSV(df,PROJECT2_HOME+run_id)\\n\",\n    if(tuple_cols_toKeep is None):\n        features = df\n    else:\n        df = df.drop(columns = cat_ResponseVariable)\n        features = pd.DataFrame(df, columns = tuple_cols_toKeep)\n        features = features.drop(columns = ['Id'])\n    features = pd.get_dummies(features,drop_first=True)\n    features[cat_ResponseVariable] = response\n    features.head()\n    return(features)","28f83caa":"features = getFeatures(data,'1020',colstokeep,'SalePrice')\nfeatures_new = pd.DataFrame(data, columns = colstokeep) \nfeatures_final = pd.concat([features,features_new['Id']], axis=1) \nfeatures_copy = features_final\nlabelkey = features_final['Id']\nlabels = features_final['SalePrice']\nfeature_list = list(features_final.columns)\nfeatures_final = features_final.drop(columns = ['SalePrice','BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch', '3SsnPorch','ScreenPorch', 'MiscVal', 'Electrical_Mix'])\nfeatures_final = features_final.drop(columns = ['Id'])\nfeatures_final.shape","d9fc8fec":"import numpy as np\nimport pandas as pd\nimport time\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor    \nfrom joblib import Parallel, delayed\n\n# Defining the function that you will run later\ndef calculate_vif_(X, thresh=5.0):\n    variables = [X.columns[i] for i in range(X.shape[1])]\n    dropped=True\n    while dropped:\n        dropped=False\n        #print(len(variables))\n        vif = Parallel(n_jobs=-1,verbose=5)(delayed(variance_inflation_factor)(X[variables].values, ix) for ix in range(len(variables)))\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            #print(time.ctime() + ' dropping \\'' + X[variables].columns[maxloc] + '\\' at index: ' + str(maxloc))\n            variables.pop(maxloc)\n            dropped=True\n\n    #print('Remaining variables:')\n    #print([variables])\n    return X[[i for i in variables]]\n","a79157e2":"X2 = calculate_vif_(features_final,5)\nX2.shape\n\nfeatures_final = X2","a88aaece":"# Libraries that sklearn provides:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.formula.api as sm","08af672b":"train_features, test_features, train_labels, test_labels, train_labelkey, test_labelkey = train_test_split(features_final, labels, labelkey, test_size = 0.15, random_state = 42)\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Training Label Key Shape:', train_labelkey.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\nprint('Testing Label Key Shape:', test_labelkey.shape)","b1c8e3d3":"rf = RandomForestRegressor(n_estimators = 50, random_state = 42 , max_depth= 5, min_samples_leaf=5)\nrf.fit(train_features, train_labels)","fb2a1ef9":"predicted_vals_train = rf.predict(train_features)\npredicted_vals_train","5f795a02":"labelkey_train = np.array(train_labelkey)\nlabels_train = np.array(train_labels)\npredicted_vals_train = rf.predict(train_features)\ndata_train = pd.concat([pd.DataFrame(labelkey_train), pd.DataFrame(labels_train), pd.DataFrame(predicted_vals_train)], axis=1) \ndata_train.shape\ndata_train.columns = ['Id','Actual','Predicted']\ndata_train['pred_error'] = (data_train['Actual']-data_train['Predicted']).abs()\ndata_train['pred_error_percent'] = data_train['pred_error']\/data_train['Actual']\nprint(data_train['pred_error_percent'].mean())","f842c3f5":"labelkey_test = np.array(test_labelkey)\nlabels_test = np.array(test_labels)\npredicted_vals_test = rf.predict(test_features)\ndata_test = pd.concat([pd.DataFrame(labelkey_test), pd.DataFrame(labels_test), pd.DataFrame(predicted_vals_test)], axis=1) \ndata_test.shape\ndata_test.columns = ['Id','Actual','Predicted']\ndata_test['pred_error'] = (data_test['Actual']-data_test['Predicted']).abs()\ndata_test['pred_error_percent'] = data_test['pred_error']\/data_test['Actual']\nprint(data_test['pred_error_percent'].mean())","7c0c303f":"feat_importances = pd.Series(rf.feature_importances_, index= train_features.columns)\nfeat_importances.nlargest(25).plot(kind='bar')","098ca8f6":"importances = rf.feature_importances_\nindices = np.argsort(importances)[::-1]\na = []\n\nprint(\"Feature ranking:\")\nfor f in range(train_features.shape[1]):\n    print(\"feature  %d (%f)\" % (indices[f], importances[indices[f]]))","7b7eda40":"SalePrice_CutOff = np.percentile(features_copy['SalePrice'],80)\nfeatures_copy['SalePrice_Classification'] =  np.where(features_copy['SalePrice'] >= SalePrice_CutOff,1,0)","eb44895c":"features_copy['SalePrice_Classification'].value_counts()","d9112daa":"labels_classification = features_copy['SalePrice_Classification']\nlabelkey_classification = features_copy['Id']\nfinal_colstokeep = list(features_final.columns)\nfeatures_final_classification = pd.DataFrame(features_copy,columns=final_colstokeep)\n#features_final_classification.reset_index(inplace = True)\nfeatures_final_classification.shape ","1fa86e46":"train_features_class, test_features_class, train_labels_class, test_labels_class,  train_labelkey_class, test_labelkey_class = train_test_split(features_final_classification, labels_classification, labelkey_classification,  test_size = 0.15, random_state = 42)\n#train_features_class, test_features_class, train_labels_class, test_labels_class,  train_labelkey_class, test_labelkey_class = train_test_split(features_copy, labels_classification, labelkey_classification,  test_size = 0.15, random_state = 42)\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Training Label Key Shape:', train_labelkey.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\nprint('Testing Label Key Shape:', test_labelkey.shape)","d161fab0":"xgb_class = XGBClassifier(gamma=0.01, learning_rate=0.1,max_depth=2,n_estimators=70,n_jobs=1,random_state=42)\nxgb_class.fit(train_features_class, train_labels_class)","630889a4":"labelkey_class_train = np.array(train_labelkey_class)\nlabels_class_train = np.array(train_labels_class)\npredicted_probs_train = xgb_class.predict_proba(train_features_class)\ndata_train = pd.concat([pd.DataFrame(labelkey_class_train),pd.DataFrame(labels_class_train), pd.DataFrame(predicted_probs_train,columns=['Col_0','Col_1'])], axis=1) \ndata_train.shape\ndata_train['prob_decile'] = pd.qcut(data_train['Col_1'], 10,labels=False)\ndata_train.head()","6df18bd8":"labelkey_class_test = np.array(test_labelkey_class)\nlabels_class_test = np.array(test_labels_class)\npredicted_probs_test = xgb_class.predict_proba(test_features_class)\ndata_test = pd.concat([pd.DataFrame(labelkey_class_test),pd.DataFrame(labels_class_test), pd.DataFrame(predicted_probs_test,columns=['Col_0','Col_1'])], axis=1) \ndata_test.shape\ndata_test['prob_decile'] = pd.qcut(data_test['Col_1'], 10,labels=False)\ndata_test.head()","b3079701":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(labels_class_train, predicted_probs_train[::,1], pos_label=1)\nimport matplotlib.pyplot as plt\nauc_train = metrics.roc_auc_score(labels_class_train, predicted_probs_train[::,1])\nprint(auc_train)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_train))\nplt.legend(loc=4)","0c6700e6":"fpr, tpr, thresholds = metrics.roc_curve(labels_class_test, predicted_probs_test[::,1], pos_label=1)\nimport matplotlib.pyplot as plt\nauc_test = metrics.roc_auc_score(labels_class_test, predicted_probs_test[::,1])\nprint(auc_test)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_test))\nplt.legend(loc=4)","68ef4e7b":"data_train.head()","83431cc4":"data_train_copy=data_train\ncolumns_names=['Id','TAG','Col_0','Col_1','prob_decile']\n#columns_names=['Id','TAG','Col_0','Col_1','prob_decile','prediction']\ndata_train.columns = columns_names\ndata_train.tail()\n\nBase_Considered = data_train.loc[(data_train['prob_decile'] >= 7)]\nset_prob = Base_Considered['Col_1'].min()\nprint(set_prob)\ndata_train['prediction']=np.where(data_train['Col_1'] >= set_prob ,1,0)\ndata_train.head()\n\ndt_pivot = pd.pivot_table(data_train, values= 'Id', index= 'prob_decile', columns= 'TAG', aggfunc= np.count_nonzero ,\n               margins= True)\ndt_pivot.index.name = None\ndt_pivot.columns.name = None\ndt_pivot.columns = ['Base', 'Responders', 'Total']\n\ntable = pd.pivot_table(data_train, values=['Id', 'Col_1'], index=['prob_decile'], aggfunc={'prob_decile': np.count_nonzero,'Col_1': [min, max]})\ntable['responders']=dt_pivot['Responders']\nBase_Considered=table.sort_index(ascending=False)\nBase_Considered['cumulative_responders'] = Base_Considered.responders.cumsum()\nBase_Considered['responders_perc'] = 100*Base_Considered.cumulative_responders\/Base_Considered.responders.sum()\nBase_Considered\n\ndt_pivot = pd.pivot_table(data_train, values= 'Id', index= 'TAG', columns= 'prediction', aggfunc= np.count_nonzero ,\n               margins= True)\n# print(dt_pivot)\nconfusion_matrix=dt_pivot\nconfusion_matrix.rename(index={0:'FALSE',1:'TRUE'}, columns={0:'FALSE',1:'TRUE'}, inplace=True)\nconfusion_matrix\n\nprint('Accuracy :')\nprint((confusion_matrix.iloc[0,0]+confusion_matrix.iloc[1,1])\/(confusion_matrix.iloc[2,2]))\nprint('precision:')\nprint((confusion_matrix.iloc[1,1])\/(confusion_matrix.iloc[2,1]))\nprint('Recall:')\nprint((confusion_matrix.iloc[1,1])\/(confusion_matrix.iloc[1,2]))\nprint('F1 Score:')\na=2*((confusion_matrix.iloc[1,1])\/(confusion_matrix.iloc[2,1])*(confusion_matrix.iloc[1,1])\/(confusion_matrix.iloc[1,2]))\nb=((confusion_matrix.iloc[1,1])\/(confusion_matrix.iloc[2,1]))+((confusion_matrix.iloc[1,1])\/(confusion_matrix.iloc[1,2]))\nprint(a\/b)","3d5d4606":"feat_importances = pd.Series(xgb_class.feature_importances_, index= train_features_class.columns)\nfeat_importances.nlargest(25).plot(kind='bar')","7bf47ff0":"importances = xgb_class.feature_importances_\nindices = np.argsort(importances)[::-1]\na = []\n\nprint(\"Feature ranking:\")\nfor f in range(train_features_class.shape[1]):\n    print(\"feature  %d (%f)\" % (indices[f], importances[indices[f]]))","d0fe3e8a":"#train_logit = pd.concat([train_labels_class,train_features_class], axis=1)\n#train_cols = train_logit.columns[1:]\n\nimport statsmodels.api as sm\nlogit = sm.Logit(train_labels_class,train_features_class)\n#logit = sm.Logit(train_logit['SalePrice_Classification'],train_logit[train_cols])\n\n# fit the model\nresult = logit.fit(method = 'bfgs')","cf421365":"result.summary2()","b5fe2a0a":"predicted_probs_train = result.predict(train_features_class)\ndata_train = pd.concat([pd.DataFrame(train_labelkey_class),pd.DataFrame(train_labels_class), pd.DataFrame(predicted_probs_train)], axis=1)\ndata_train.shape\ndata_train.columns = ['Id','OrginalFlag','PredictedProbability']\ndata_train['prob_decile'] = pd.qcut(data_train['PredictedProbability'], 10,labels=False)\ndata_train.head()\n\npredicted_probs_test = result.predict(test_features_class)\ndata_test = pd.concat([pd.DataFrame(test_labelkey_class),pd.DataFrame(test_labels_class), pd.DataFrame(predicted_probs_test)], axis=1) \ndata_test.shape\ndata_test.columns = ['Id','OrginalFlag','PredictedProbability']\ndata_test['prob_decile'] = pd.qcut(data_test['PredictedProbability'], 10,labels=False)\ndata_test.head()","1d52f5b9":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(train_labels_class, predicted_probs_train, pos_label=1)\nimport matplotlib.pyplot as plt\nauc_train = metrics.roc_auc_score(train_labels_class, predicted_probs_train)\nprint(auc_train)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_train))\nplt.legend(loc=4)","455f8fbf":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(test_labels_class, predicted_probs_test, pos_label=1)\nimport matplotlib.pyplot as plt\nauc_test = metrics.roc_auc_score(test_labels_class, predicted_probs_test)\nprint(auc_test)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_test))\nplt.legend(loc=4)","788a3c44":"**Preprocessing 3: Filtering Variables**\n\n**Multicollinearity**\n* This is not a deal-breaker while dealing with machine learning techniques like random forest and xgboost. However is essential while doing linear and logistic regression. \n* Multicollinearity is a statistical phenomenon in which predictor variables in a logistic regression model are highly correlated.It is common when there are a large number of predictors in the model.\n* Multicollinearity can cause unstable estimates and inaccurate variances which affects confidence intervals and hypothesis tests. The existence of collinearity inflates the variances of the parameter estimates, and consequently incorrect inferences about relationships between independant and response variables.\n* Using the correlation matrix in pandas profiling output, we can first filter out  independent variables that are highly correlated to one another.\n* VIF (Variance Inflation Factor) is one of the helpful metric to detect the multicollinearity. For moderate to large sample sizes, the approach to drop one of the correlated variables was established entirely satisfactory to reduce multicollinearity.\n* The loop to identify variables that cause inflation eliminates those variables on account of a particular threshold. We can then compare them to the original dataset and put back variables if needed.\n","a1771e4b":"**Database Split: Train and Test split**\n* There are two types of validations that are most commonly used for both Classification and Regression predictive models.\n    a) Out of Sample Validation\n    b) Out of Time Validation.\n* In the case of an Out of Time validation we have consider a seperate from a seperate time window that wasn't considered in the base that was used to train\n* In the case of an out of sample validation we will have to split the base to seperate the base to train and the base to validate. We will explore the out of sample option for validation in our case.","6ea8ff07":"** Preprocessing 2: Outlier Treatment**\n* It a pre-processing technique used to eliminate rare case scenarios in the case if numerical variables that can skew and distort the final result of the model.\n* However we need to keep the business context in mind while treating variables for outliers.As outlier treatment alters the distribution of the variable and there may be variables whose distribution needs to be preserved.\n* Box-plot and inter-quartile range is the most popular technique for outlier detection.","dba2b821":"**Problem statement and Data Load:**\n* We are going to use both classification and regression predictive models to predict the sale price of a house based on it's features.\n* We will be using pandas to help load the data that we will be building the model on.\n* To ensure that the date and time related variables are not loaded in a numeric format we will ensure to specify the format that it should be loaded in.","b8b2ab01":"**Model Build and Validation:**\n* As our problem is originally a Regression problem, we will build a Random Forest Regressor. However in the case Regression problems both XgBoost and Random Forest as they are part of the same library, have the same syntax for build and predict.\n* For Regression porblems we have two potential options for a validation metric \n    a) MAE (Mean Absolute Error).\n    b) MAPE (Mean Absolute Percentage Error)\n* We will calculate the validation metric for both train and test base, to ensure that model hasn't been overfit to the training base so as to not be able to generalise and achieve the same level of accuracy for the test base.","f388d91d":"**Alternate Package for Logistic Regression**\n\n* However in regards to logistic regression, Sklearn is not the only package under consideration.\n* Stats Model is also a package that offers to build logistic regression in python.","7c77ba93":"**Model Building and Evaluation**\n* The sklearn package in python carries Random Forest, XGBoost and Logistic Regression for solving a Classification Predictive Model,\n* We will look at one example with XGBoost Classifier, however the other techniques in the same package hold the same syntax\n* Classification Models have a few common evaluation metrics that we will consider.\n    a) AUC - Area Under the Curve.\n    b) Confusion Metric : Recall, Precision, F1-Score, Accuracy. ","c8e1a4cd":"**Data Pre-Processing**\n* We will be going through the a series of data pre-processing techniques that helps prepare the data that needs to go into the model. \n* We will be doing some cleaning exercises and some variable transformation techniques too.\n\n**Pre-processing 1: Missing Value Treatment**\n* We must first understand the proportion of missing values in a variable. We get this information for each variable pandas profiling output.\n* If the proportion of missing values is very high (More than 50%) then we should not consider the variable in the model building exercise as there will be no value add from this variable.\n* We have to replace the missing values of a numerical variable from one of the following options:\na) Mean of the variable.\nb) Median of the variable.\nc) Zero (0)\n* The recommended replacement is the median as it ensures that distribution of the variable is not disturbed.\n* For the replacement of missing values in categorical variables, we could have one of two options:\na) Mode (The category with the highest frequency).\nb) New category of \u201cNot Available\u201d.","e5faf48d":"**SKLEARN - XGBOOST**\n\n* Sklearn also has a provision for XGBoost while dealing with regression predictive problems.\n* Following on the similar syntax lines of Random Forest here is the code for XGBoost:\n\n*from xgboost import XGBRegressor*\n\n*xgb = XGBRegressor(n_estimators=50,random_state=421,max_depth=5,colsample_bytree=0.3)*","d2cc3027":"**SKLEARN - RF AND LOGISTIC REGRESSION**\n\n* Sklearn also has a provision for Random Forest and Logistic Regression while dealing with classification predictive problems.\n* Following on the similar syntax lines of XGBoost here is the code for Random Forest:\n\n**Random Forest**\n\n*from sklearn.ensemble import RandomForestClassifier*\n\n*rf_class = RandomForestClassifier(n_estimators= 70, random_state = 42,max_depth=2)*\n\n*rf_class.fit(train_features_class, train_labels_class)*\n\n**Logistic Regression**\n\n*from sklearn.linear_model import LogisticRegression*\n\n*logreg = LogisticRegression()*\n\n*logreg.fit(train_features_class, train_labels_class)*","5f210137":"**Preprocessing 3: Filtering Variables**\n\n**Categorical Variable Grouping**\n* Python only considers for numerical varaibles while building a predictive model So we have to convert independant categorical variables in to dummy variables. Dummy variables are numeric binary (1\/0) variables.\n* If a categorical variable has k different values then we should create (k-1) dummy (1\/0) variables for each (k-1) values of a variable. If all (k-1) dummy variable\u2019s value is 0 then it indicates kth value of that variable.\n* This indicates as the values of a categorical variable increases it will increase the number of independant variables which are getting added into the building of a model, this will make model more complex and may not have enough instances of every value in the categorical variable for the model to learn from.\n* So we should ensure to restrict the number of categories of a categorical variable, such that the values in consideration should cover more than 95% of the instances in the considered dataset.\n* From the Pandas profiling output we see that there are a few categorical variables with large number of distinct values and we will now attempt to group them.\n\n**Identifying High Cardinality Variables** \n* The Year Variables viz. **YearRemodAdd, YearBuilt, GarageYrBlt** have a lot of distinct values.\n* So in our initial iteration we will eliminate these variables (As they have over 50 distinct values), however based on the model's outcome we will consider grouping these variables.\n* In the case of date and time varaibles we have to consider the aspect of order but shouldn't ignore the distribution either while grouping them.\n\n**Identifying Low Variation Varaibles** \n* If a variable doesn't have enough variation it will not have any value in identifying the cases and values we want to identify with a predictive model.\n* We use Pandas Profiling to identify such varaibles both among numerical and categorical varaibles. We remove these varaibles from our database viz. **BsmtHalfBath, CentralAir, Condition2, Heating, RoofMatl, Street, Utilities** among the categorical variables and **Heating, PoolArea** among the numerical variabes\/","6c2dffba":"* We combine the list of variables that we see to have missing values and the pandas profiling output to see which variables should be eliminated.\n* We see that the variables 'Alley','MasVnrArea','PoolQC','Fence','MiscFeature' have far too many missing values to be considered in the process of model building and hence we will drop them from the data base in consideration and treat the other missing values. ","0e3bd767":"**Model Building**\n\nPython offers two libraries that is useful for model building.\n\n**Sckit-learn**\n* The sckit-learn library in python helps build all 3 techniques of model building that we are considering today which are Logistic Regression, Random Forest and XGB.\n* Sckit-learn provides for both Random Forest and XGB as both classifiers and regressors.\n* So the syntax for all three within this package stays the same.\n\n**Stats Model**\n* Logistic Regression can also be done using through the stats-model library instead of the sckit learn library.\n* The coefficients between the stats model and sckit learn models vary based on the regularization in the sckit-learn package.\n","bf523302":"**Classification Model Case Study :**\n* We will now convert the existing Regression problem to a Classification problem and understand how to solve the same.\n* Keeping the preprocessing the same, we will simply convert the dependent varaible to binary and understand how the model build and evaluation differs from that of a regression problem.","3e9336ad":"**Understanding the data set:**\n* In the process of model building, it is important for us to understand the business and the data set for which we are building a model.\n* This will help us filter out unnecessary varaibles that would add noise and not value to the prediction.\n* We can have two approaches to understand the distribution of the variables under consideration,using the describe command and then we get a more elaborate analysis using Pandas profiling","b1e20fb8":"**Understanding the contribution of variables**\n* Sklearn provides Feature importance for its models that gives us an understanding of the contribution of variables to the prediction.\n* This gives us a good sense of the health of the model.\n* It also helps in understanding important factors that gives the business good insights in decision making\n* If the contribution of any variable or 2 variables like in the case below is significantky higher than the other, it is a good idea to examine the creation and correlation of the variable with the dependant variable"}}