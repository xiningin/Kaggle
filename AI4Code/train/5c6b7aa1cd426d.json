{"cell_type":{"0eb84158":"code","30982a48":"code","e04df72c":"code","585183fa":"code","23d5adc4":"code","0a655718":"code","018896da":"code","8fc15cec":"code","defa38fd":"code","d47bb258":"code","77a7a655":"markdown","aebb2313":"markdown","49984a37":"markdown","b8ecbcef":"markdown","ba255269":"markdown","13ffccb5":"markdown","7dd18760":"markdown","7592be08":"markdown","99a2f29b":"markdown","c336a04e":"markdown","204ce0f1":"markdown","594ebb56":"markdown","9f588fce":"markdown","e181b31d":"markdown","e613be07":"markdown","d45d1940":"markdown","75d89bc0":"markdown","8de1fc9e":"markdown","915b0d91":"markdown","8cf3624a":"markdown","dae1fa9d":"markdown","4f70e0c6":"markdown","bc60fd11":"markdown","f53dc307":"markdown","4d5bdbab":"markdown","00e25ca9":"markdown","dafc60b5":"markdown","99b61275":"markdown","8bce0372":"markdown"},"source":{"0eb84158":"import logging\nimport time\n\nimport catboost as cb\nimport joblib\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.compose import (\n    ColumnTransformer,\n    make_column_selector,\n    make_column_transformer,\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n)\noptuna.logging.set_verbosity(optuna.logging.WARNING)","30982a48":"import datatable as dt  # pip install datatable\nimport pandas as pd","e04df72c":"%%time\ntps_dt = dt.fread(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\").to_pandas()\ntps_dt.head()","585183fa":"%%time\ntps_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntps_df.head()","23d5adc4":"memory_usage = tps_df.memory_usage(deep=True) \/ 1024 ** 2\n\nmemory_usage.head(7)","0a655718":"memory_usage.sum()","018896da":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","8fc15cec":"reduced_df = reduce_memory_usage(tps_df, verbose=True)","defa38fd":"sample_df = tps_df.sample(int(len(tps_df) * 0.2))\nsample_df.shape","d47bb258":"fig, ax = plt.subplots(figsize=(12, 9))\n\nsns.histplot(\n    data=tps_df, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n)\nsns.histplot(\n    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n)\n\nplt.legend()\nplt.show();","77a7a655":"# Use vectorization instead of loops","aebb2313":"Based on the minimum and maximum value of a *numeric* column and the above table, the function converts it to the smallest subtype possible. Let's use it on our data:","49984a37":"Up until this point, I mainly mentioned `pandas`. It might be slow, but the vast range of data manipulation functions gives it a mounting advantage over its competitors.\n\nBut what can its competitors do? Let's start with datatable (again).\n\n[`datatable`](https:\/\/datatable.readthedocs.io\/en\/latest\/start\/index-start.html) allows multi-threaded preprocessing of datasets sized up to 100 GBs. At such scales, `pandas` starts throwing memory errors while `datatable` humbly executes. You can read [this excellent article](https:\/\/towardsdatascience.com\/an-overview-of-pythons-datatable-package-5d3a97394ee9) by @parulpandey for an intro to the package.\n\nAnother alternative is [`cuDF`](https:\/\/docs.rapids.ai\/api\/cudf\/stable\/), developed by RAPIDS. This package has many dependencies and can be used in extreme cases (think hundreds of billions). It enables running preprocessing functions distributed over one or more GPUs, as is the requirement by most of today's data applications. Unlike `datatable`, its API is very similar to `pandas`. Read [this article](https:\/\/developer.nvidia.com\/blog\/pandas-dataframe-tutorial-beginners-guide-to-gpu-accelerated-dataframes-in-python\/) from the NVIDIA blog for more information.\n\nYou can also check out [Dask](https:\/\/dask.org\/) or [Vaex](https:\/\/vaex.io\/docs\/index.html) that offer similar functionalities.\n\nIf you are dead set on `pandas`, then read on to the next section.","b8ecbcef":"Machine learning is an iterative process. When dealing with large datasets, you have to make sure each iteration is as fast as possible. You want to build baselines, develop a validation strategy, check if different feature engineering ideas improve the baseline, and so on.\n\nAt this stage, don't use models in Sklearn because they are CPU-only. Choose from XGBoost, LightGBM or CatBoost. And here is the surprising fact\u200a-\u200aXGBoost is much slower than the other two, even on GPUs.\n\nIt is up to [10 times slower than LightGBM](https:\/\/towardsdatascience.com\/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997?source=your_stories_page-------------------------------------). CatBoost beats both libraries, and the speed difference grows rapidly as the dataset size gets bigger. It also regularly outperforms them in terms of accuracy.\n\nThese speed differences become much more pronounced when you are running multiple experiments, cross-validating, or hyperparameter tuning.","ba255269":"70% memory reduction is pretty impressive. However, please note that memory reduction won't speed up computation in most cases. If the memory size is not an issue, you can skip this step.\n\nRegarding non-numeric data types, never use the `object` datatype in Pandas as it consumes the most memory. Either use `str` or `category` if there are few unique values in the feature. In fact, using pd.Categorical data type can speed things up to 10 times while using[LightGBM's default categorical](https:\/\/towardsdatascience.com\/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997?source=your_stories_page-------------------------------------) handler.\n\nFor other data types like `datetime` or `timedelta`, use the native formats offered in `pandas` since they enable special manipulation functions.","13ffccb5":"Next, we have memory issues. Even a 200k row dataset may exhaust your 16GB RAM while doing complex computations.\n\nI have experienced this first-hand twice in the last month's TPS competition on Kaggle. The first one was when projecting the training data to 2D using UMAP\u200a-\u200aI ran out of RAM. The second was while computing the SHAP values with XGBoost for the test set\u200a-\u200aI ran out of GPU VRAM. What is shocking is that the training and test sets only had 250k and 150k rows with a hundred features, and I was using Kaggle kernels.\n\nThe dataset we are using today has ~960k rows with 120 features, so memory issues are much more likely:","7dd18760":"# Wrapping up...","7592be08":"# You might also be interested...\n\n- [Kaggler\u2019s Guide to LightGBM Hyperparameter Tuning with Optuna in 2021](https:\/\/towardsdatascience.com\/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)\n\n- [You Are Missing Out on LightGBM. It Crushes XGBoost in Every Aspect](https:\/\/towardsdatascience.com\/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997)\n\n- [Tired of Clich\u00e9 Datasets? Here are 18 Awesome Alternatives From All Domains](https:\/\/towardsdatascience.com\/tired-of-clich%C3%A9-datasets-here-are-18-awesome-alternatives-from-all-domains-196913161ec9)\n\n- [Love 3Blue1Brown Animations? Learn How to Create Your Own in Python in 10 Minutes](https:\/\/towardsdatascience.com\/love-3blue1brown-animations-learn-how-to-create-your-own-in-python-in-10-minutes-8e0430cf3a6d)\n\n- [Yes, These Unbelievable Masterpieces Are Created With Matplotlib](https:\/\/ibexorigin.medium.com\/yes-these-unbelievable-masterpieces-are-created-with-matplotlib-2256a4c54b12)","99a2f29b":"# Setup","c336a04e":"The first of your worries start when loading the data\u200a-\u200athe time it takes to read the dataset into your working environment can be as long as you train a model. At this stage, don't use pandas\u200a-\u200athere are much faster alternatives available. One of my favorites is the `datatable` package which can read data up to 10 times faster.\n\nAs an example, we will load ~1M row Kaggle TPS September 2021 dataset with both `datatable` and `pandas` and compare the speeds:","204ce0f1":"# Choose a data manipulation library","594ebb56":"# Miscellaneous tips","9f588fce":"Numbers next to the datatype refer to how many bits of memory a single data unit consumes when represented in that format. To reduce the memory as much as possible, choose the smallest NumPy data format. Here is a good table to understand this:\n\n![](https:\/\/miro.medium.com\/max\/1050\/1*f7kTFcscHI7dstMHZ1_eFg.png)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Source: https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/user\/basics.types.html\n    <\/strong>\n<\/figcaption>","e181b31d":"One of the difficult stages of my learning journey was about overcoming my fear of massive datasets. It wasn't easy because working with million-row datasets was nothing like the tiny, toy datasets the online courses continuously gave me.\n\nToday, I am here to share the concepts and tricks I have learned to handle the challenges of gigabyte-sized datasets with millions or even billions of rows. By the end, they will feel to you almost as natural as working with the Iris or Titanic.","e613be07":"As you can see, the distributions are roughly the same\u200a-\u200ayou can even compare the variances to check.\n\nNow, you can use this sample for rapid prototyping, experimenting, building a model validation strategy, and so on.","d45d1940":"# Reduce the memory size","75d89bc0":"Using the `memory_usage` method on a DataFrame with `deep=True`, we can get the exact estimate of how much RAM each feature is consuming - 7 MBs. Overall, it is close to 1GB.\n\nNow, there are certain tricks you can use to decrease memory usage up to 90%. These tricks have a lot to do with changing the data type of each feature to the smallest subtype possible.\n\nPython represents various data with unique types such as `int`, `float`, `str`, etc. In contrast, pandas has several NumPy alternatives for each of Python's:\n\n![](https:\/\/miro.medium.com\/max\/1050\/1*j9CH_6m1XrvuPz2DUGf5tQ.png)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Source: http:\/\/pbpython.com\/pandas_dtypes.html\n    <\/strong>\n<\/figcaption>","8de1fc9e":"In the above table, `uint` refers to unsigned, only positive integers. I have found this handy function that reduces the memory of pandas DataFrames based on the above table (shout out to [this Kaggle kernel](https:\/\/www.kaggle.com\/somang1418\/tuning-hyperparameters-under-10-minutes-lgbm?scriptVersionId=11067143&cellId=10)):","915b0d91":"Whenever you find yourself itching to use some looping function like `apply`, `applymap`, or `itertuples` - stop. Use vectorization instead.\n\nFirst, start thinking about DataFrame columns as giant n-dimensional vectors. As you know, vector operations affect each element in the vector simultaneously removing the need for loops in math. Vectorization is the process of executing operations on arrays rather than individual scalars.\n\nPandas has a large collection of vectorized functions. In fact, virtually any function and operator with the ability to affect each element in the array is vectorized in pandas. These functions are orders of magnitude faster than anything that loops.\n\nYou can also define custom vectorized preprocessing functions that accept whole DataFrame columns as vectors rather than scalars. The hairy details of this are beyond the scope of this article. Why don't you check out [this awesome guide](https:\/\/engineering.upside.com\/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6)?","8cf3624a":"# Choose a machine learning library for baseline models or prototypes","dae1fa9d":"Use Cython (C Python) - usually it is up to 100 times faster than pure Python. Check out [this section](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/enhancingperf.html#cython-writing-c-extensions-for-pandas) of the Pandas documetnation.\n\nIf you really have to loop, decorate your custom functions with `@numba.jit` function after installing Numba. JIT (just-in-time) complition converts pure Python to native machine instructions, enabling you to achieve C, C++ and Fortran-like speeds. Again, check [this section](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/enhancingperf.html#numba-jit-compilation) from the docs.\n\nSearch for alternatives other than CSV files for storage. File formats like feather, parquet, and jay are lighting fast - it only takes seconds to load billion-row datasets if they are stored in these.\n\nRead the Pandas documentation on [enhancing performance](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/enhancingperf.html) and [scaling to large datasets](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/scale.html).","4f70e0c6":"7 times speedup! The datatable API for manipulating data may not be as intuitive as pandas - so, call the to_pandas method after reading the data to convert it to a DataFrame.\n\nApart from datatable, there are Dask, Vaex, or cuDF, etc. that read data multiple times faster than pandas. If you want to see some of those in action, refer to [this notebook](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets) on reading large datasets by Kaggle Grandmaster Rohan Rao.","bc60fd11":"Here is a brief summary of the article:\n\n1. Load the data only using libraries like `datatable`, `cuDF` or `dask`. They are always faster than Pandas.\n2. Reduce the memory consumption by up to 90% by casting each column to the smallest subtype possible.\n3. Choose a data manipulation library you are comfortable with or based on what you need.\n4. Take a 10\u201320% sample of the data for rapid analysis and experimentation.\n5. Think in vectors and use vectorized functions.\n6. Choose a fast ML library like CatBoost for building baselines and doing feature engineering.\n\nThank you for reading!\n![](https:\/\/cdn-images-1.medium.com\/max\/1080\/1*KeMS7gxVGsgx8KC36rSTcg.gif)","f53dc307":"As proof, we can plot a histogram of a single feature from both the sample and the original data:","4d5bdbab":"# How to Work with Million-row Datasets Like a Pro\n## It is time to take off your training wheels\n![](https:\/\/cdn-images-1.medium.com\/max\/1620\/1*bUX-lSoJ4VMQz2YnmSwgFA.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@belart84?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Artem Beliaikin<\/a>\n        on \n        <a href='https:\/\/www.pexels.com\/photo\/aerial-photo-of-woman-standing-in-flower-field-1657974\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels.<\/a> All images are by the author unless specified otherwise.\n    <\/strong>\n<\/figcaption>","00e25ca9":"Regardless of any speed tricks or packages on GPU steroids, too much data, well, is too much. When you have millions of rows, there is a good chance you can sample them so that all feature distributions are preserved.\n\nThis is done mainly to speed up computation. Take a small sample instead of running experiments, feature engineering, and training baseline models on all the data. Typically, 10\u201320% is enough. Here is how it is done in pandas:","dafc60b5":"# Read in the massive dataset","99b61275":"# Introduction","8bce0372":"# Sample the data"}}