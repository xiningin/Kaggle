{"cell_type":{"4cacfbbd":"code","4f026f59":"code","0502fc31":"code","f67e00d3":"code","db5eb21d":"code","55cc1206":"code","03a266f1":"code","eec407f2":"code","e75dc881":"code","62ca1da1":"code","2596a3b8":"code","de47407a":"code","144ddd20":"code","caa78ece":"code","b488d6ec":"code","fd010244":"code","1bfadbe6":"code","6cb54850":"code","eda4fb6a":"code","7b29e326":"code","780ece2c":"code","b216e186":"code","a9fb66ed":"code","f6211bc0":"code","883ffa59":"code","67e032e9":"code","25d7af6f":"code","785a660e":"code","70d210eb":"code","ff82b453":"code","fa0d0f99":"code","18651b6b":"code","1245d3f6":"code","9411a980":"code","87d383bf":"code","048c558e":"code","e911a1f4":"code","3ce677cd":"code","acaf4def":"code","281518f6":"code","8528d33a":"code","3381ae8d":"code","72ef17e2":"code","a151d853":"code","af031208":"code","c3102c32":"code","dbb1748b":"code","0e9b746d":"code","f59fc91b":"code","8822da48":"code","a353853e":"code","65da9be7":"code","1525ebc8":"code","d8ea0930":"code","f3b09ca3":"code","d4c15522":"code","2ef2f4d8":"code","07bd3594":"code","04decb30":"code","4af6c65f":"code","0d2be561":"code","35973b1e":"code","8eff0769":"code","3856b24e":"code","862e498c":"code","6b69a5ec":"code","39108a03":"code","a4e61d71":"code","e6677c68":"code","597ddcc4":"code","6c1fa40a":"code","99c06834":"code","42963199":"code","e6a7a85d":"code","772dac17":"code","8f4c9d3d":"code","c6acf851":"code","70bbcc85":"code","afeab59b":"code","ed053866":"code","72f729e9":"code","297d710e":"code","99369a2c":"code","e08ad3db":"code","05923704":"code","78e22595":"code","1585a2f8":"code","f18cb714":"code","e548cc80":"code","bfecb853":"code","a4f15a40":"code","a581d94c":"code","cabaeee5":"code","55f48127":"code","bd813ab4":"code","422805ba":"code","84818519":"code","05c171a8":"code","4d5a5c29":"code","77bdca66":"code","421308e7":"code","320f80c8":"code","93eb4fd9":"code","edb5d65d":"code","94ca0850":"code","aa49dc27":"code","3e8cd4f1":"code","81183d43":"code","72286406":"code","06e8f219":"markdown","dbc671c6":"markdown","1b4f50ac":"markdown","49878433":"markdown","517efb86":"markdown","a1dfc0f9":"markdown","181a222a":"markdown","7aaf15c9":"markdown","a5287ab3":"markdown","6efedb17":"markdown","fb95f7e1":"markdown","08c14ffd":"markdown","52a4b820":"markdown"},"source":{"4cacfbbd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","4f026f59":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","0502fc31":"#gives the shape of the data \ndf.shape","f67e00d3":"#gives the first few values of the data \ndf.head()","db5eb21d":"# shows a heatmap of the data with null values separated out in yellow color\nsns.heatmap(df.isnull(),yticklabels=False,cbar=True)","55cc1206":"#gives info about the data - # of null records per column & type of data\ndf.info()","03a266f1":"# Lists column names by data type - Integars and Objects\ng = df.columns.to_series().groupby(df.dtypes).groups\nprint(g)","eec407f2":"#count the number of null values in each column\ndf.isnull().sum()","e75dc881":"# plots the columns with null values. Number of null values on y-axis and column headers in x-axis\nnull_counts = df.isnull().sum()\nplt.figure(figsize=(16,8))\nplt.xticks(np.arange(len(null_counts))+0.5,null_counts.index,rotation='vertical')\nplt.ylabel('fraction of rows with missing data')\nplt.bar(np.arange(len(null_counts)),null_counts)","62ca1da1":"# Replaxe null values with Mean of LotFrontage\ndf['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())\n#df.head()","2596a3b8":"df.isnull().sum()","de47407a":"#drops some columns - columns with too many null values - number of columns fall by 3 (from 81 to 78)\ndf.drop(['Alley', 'PoolQC', 'Fence'],axis=1,inplace=True)\ndf.shape","144ddd20":"#replaces all the null values with the mode since the data type is a categorical value\ndf['BsmtCond']=df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\ndf['BsmtQual']=df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])","caa78ece":"df['FireplaceQu']=df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\ndf['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0])","b488d6ec":"df['GarageFinish']=df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\ndf['GarageQual']=df['GarageQual'].fillna(df['GarageQual'].mode()[0])\ndf['GarageCond']=df['GarageCond'].fillna(df['GarageCond'].mode()[0])","fd010244":"df['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])      \ndf['BsmtFinType1']=df['BsmtFinType1'].fillna(df['BsmtFinType1'].mode()[0])  \ndf['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])   \ndf.shape","1bfadbe6":"# Fill missing values of GarageYrBuilt with corresponding values from YearBuilt\ndf['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt'])","6cb54850":"# axis=1 when you want to refer to a column. Drop another column with too many null values \ndf.drop(['MiscFeature'],axis=1,inplace=True)\ndf.shape","eda4fb6a":"df['Fireplaces']=df['Fireplaces'].fillna(df['Fireplaces'].mode()[0]) ","7b29e326":"df['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0]) ","780ece2c":"null_counts = df.isnull().sum()\nplt.figure(figsize=(16,8))\nplt.xticks(np.arange(len(null_counts))+0.5,null_counts.index,rotation='vertical')\nplt.ylabel('fraction of rows with missing data')\nplt.bar(np.arange(len(null_counts)),null_counts)","b216e186":"#gives info about the data - we are looking to see if null values remain\ndf.info()","a9fb66ed":"#drops the 'Id' column \ndf.drop(['Id'],axis=1,inplace=True)\ndf.shape","f6211bc0":"df.info()","883ffa59":"# Drop all rows with zero values - 1460 rows come down to 1451  \n#df = df.dropna(how ='any',axis=0) \n#df.shape","67e032e9":"#count and graphs the number of null values\nnull_counts = df.isnull().sum()\nplt.figure(figsize=(16,8))\nplt.xticks(np.arange(len(null_counts))+0.5,null_counts.index,rotation='vertical')\nplt.ylabel('fraction of rows with missing data')\nplt.bar(np.arange(len(null_counts)),null_counts)","25d7af6f":"sns.heatmap(df.isnull(),yticklabels=False,cbar=True, cmap='coolwarm')","785a660e":"#Handle Categorical Features\n#list(df.columns)\n#df.info()\n#df._get_numeric_data()\n#df.select_dtypes(include=['int']).dtypes\ncol = df.select_dtypes(include=['object'])\n#df.select_dtypes(include=['float'])\nlist(col)","70d210eb":"# Assigning column names with categorical features to a variable called \"columns'\ncolumns=['MSZoning',\n 'Street',\n 'LotShape',\n 'LandContour',\n 'Utilities',\n 'LotConfig',\n 'LandSlope',\n 'Neighborhood',\n 'Condition1',\n 'Condition2',\n 'BldgType',\n 'HouseStyle',\n 'RoofStyle',\n 'RoofMatl',\n 'Exterior1st',\n 'Exterior2nd',\n 'MasVnrType',\n 'ExterQual',\n 'ExterCond',\n 'Foundation',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtExposure',\n 'BsmtFinType1',\n 'BsmtFinType2',\n 'Heating',\n 'HeatingQC',\n 'CentralAir',\n 'Electrical',\n 'KitchenQual',\n 'Functional',\n 'FireplaceQu',\n 'GarageType',\n 'GarageFinish',\n 'GarageQual',\n 'GarageCond',\n 'PavedDrive',\n 'SaleType',\n 'SaleCondition']","ff82b453":"#tells me how many colums there are with categorical values \nlen(columns)","fa0d0f99":"# Defining a function to convert all categorical values into numerical values. \n# Using the method of One Hot Encoding, create dummy columns for each categorical value using pd.get_dummies function\n# Create \n\ndef category_onehot_multcols(multcolumns):\n    df_final=final_df\n    i=0\n    for fields in multcolumns:     \n        print(fields)\n        \n        df1=pd.get_dummies(final_df[fields],drop_first=True)\n        \n        final_df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df1.copy()\n        else:\n            \n            df_final=pd.concat([df_final,df1],axis=1)\n        i=i+1\n          \n    df_final=pd.concat([final_df,df_final],axis=1)\n        \n    return df_final","18651b6b":"# Creating a copy of df to ensure any changes can be tracked back\nmain_df=df.copy()","1245d3f6":"df.shape","9411a980":"main_df.shape\nmain_df.info()","87d383bf":"# Reading the test data file, that has been transformed and saved in kaggle notebook - kernel2dc1902644  - TEST DATA\ntest_df = pd.read_csv('..\/input\/retestdata0522\/reformulated_test.csv')","048c558e":"test_df.shape\n#test_df.head()","e911a1f4":"# Concatinating test and train date across rows - 1434 + 1451 = 2885 rows and 76 columns should be the result\nfinal_df=pd.concat([df,test_df],axis=0)","3ce677cd":"#tells me the number of rows and columns and the data \nfinal_df.shape\n#final_df.info()","acaf4def":"# Checking to see where test and train data merge \nfinal_df.iloc[1440:1470]","281518f6":"final_df.tail()","8528d33a":"# Checking 76 columns to confirm \"SalePrice\" is included. It should be part of train data but not test data\nfinal_df.columns","3381ae8d":"# Call the One Hot Multcols function and pass it all categorical columns listed earlier under variable \"columns\"\nfinal_df=category_onehot_multcols(columns)","72ef17e2":"# Final data frame now has 237 columns - one column for each categorical value of each column\nfinal_df.info()","a151d853":"# Remove any  duplicate columns \nfinal_df =final_df.loc[:,~final_df.columns.duplicated()]","af031208":"# # of columns come down to 177\nfinal_df.info()","c3102c32":"final_df\nfinal_df.iloc[1450:1470]","dbb1748b":"# Dividing the dataframe into training and test data frame\ndf_Train=final_df.iloc[:1460,:]\ndf_Test=final_df.iloc[1460:,:]","0e9b746d":"# Checking to see df_Test has 'SalePrice'with all null values - thereby confirming the right division of final_df into test and train\nnull_counts = df_Test.isnull().sum()\nplt.figure(figsize=(24,8))\nplt.xticks(np.arange(len(null_counts))+0.75,null_counts.index,rotation='vertical')\nplt.ylabel('rows with missing data')\nplt.bar(np.arange(len(null_counts)),null_counts)","f59fc91b":"# Checking to see df_Train.. clearly no column with 1400+ null values \nnull_counts = df_Train.isnull().sum()\nplt.figure(figsize=(24,8))\nplt.xticks(np.arange(len(null_counts))+0.75,null_counts.index,rotation='vertical')\nplt.ylabel('rows with missing data')\nplt.bar(np.arange(len(null_counts)),null_counts)","8822da48":"df_Train.head()","a353853e":"df_Train.tail()","65da9be7":"df_Test.head()","1525ebc8":"df_Test.columns","d8ea0930":"df_Train.columns","f3b09ca3":"len(df_Test.columns)","d4c15522":"len(df_Train.columns)","2ef2f4d8":"df_Test['SalePrice']","07bd3594":"df_Train['SalePrice']","04decb30":"df_Test.tail()","4af6c65f":"df_Test.shape","0d2be561":"df_Train.shape","35973b1e":"# Dropping the SalePrice column in df_Test as all values are null\ndf_Test.drop(['SalePrice'],axis=1,inplace=True)","8eff0769":"df_Test.shape","3856b24e":"df_Train.shape","862e498c":"# Dividing df_Train into X_train and Y_Train.\n# In X_train, we drop SalesPrice and Y_Train has only SalePrice as a column\nX_train=df_Train.drop(['SalePrice'],axis=1)\ny_train=df_Train['SalePrice']","6b69a5ec":"# Importing the XGBoost algorithm and using it to fit X_train and y_train\nimport xgboost\nclassifier=xgboost.XGBRegressor()\nclassifier.fit(X_train,y_train)","39108a03":"df_Test.shape","a4e61d71":"# Using the trained claasifier to predict SalePrice for df_TEst\ny_pred=classifier.predict(df_Test)","e6677c68":"y_pred","597ddcc4":"y_pred.shape","6c1fa40a":"# Creating submission file. File will be in Kaggle \/ Data \/ Output\n\npred=pd.DataFrame(y_pred)\nsub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets\n#datasets = datasets.iloc[:1459,:]\n#datasets['Id'] = datasets['Id'].astype('int32') \ndatasets.to_csv('Seventh_sample_submission.csv',index=False)\n#datasets.tail(30)","99c06834":"datasets.shape","42963199":"# Import RandomizedSearchCV to find the best Regressor for the data set.\nfrom sklearn.model_selection import RandomizedSearchCV","e6a7a85d":"regressor=xgboost.XGBRegressor()","772dac17":"# Using the regressor with the parameters derived by RandomCV\nregressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=1, missing=None, monotone_constraints=None,\n             n_estimators=900, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)","8f4c9d3d":"regressor.fit(X_train,y_train)","c6acf851":"import pickle\nfilename = 'finalized_model.pkl'\npickle.dump(classifier, open(filename, 'wb'))","70bbcc85":"df_Test.shape","afeab59b":"# Using regressor to predict df_Test \ny_pred=regressor.predict(df_Test)","ed053866":"y_pred","72f729e9":"pred=pd.DataFrame(y_pred)\nsub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\n#datasets = datasets.iloc[:1459,:]\n#datasets['Id'] = datasets['Id'].astype('int32') \ndatasets.to_csv('Tuned_Fifth_sample_submission.csv',index=False)\npred","297d710e":"# Assign SalePrice as name of column\npred.columns=['SalePrice']\npred","99369a2c":"df_Train.columns","e08ad3db":"# temp_df copied with values from df_train (SalePrice)\ntemp_df=df_Train['SalePrice'].copy()","05923704":"temp_df.head()","78e22595":"# Assign column name \"SalePrice\"\ntemp_df.column=['SalePrice']","1585a2f8":"temp_df.head","f18cb714":"temp_df.shape","e548cc80":"df_Train.head()","bfecb853":"df_Test.head()","a4f15a40":"df_Train.drop(['SalePrice'],axis=1,inplace=True)","a581d94c":"df_Train.shape","cabaeee5":"# Add temp_df to df_Train - ie. add SalePrice column to df_Train\ndf_Train=pd.concat([df_Train,temp_df],axis=1)","55f48127":"df_Train['SalePrice']","bd813ab4":"df_Train.shape","422805ba":"df_Test.shape","84818519":"# Add predicted values of SalePrice to df_Test\ndf_Test=pd.concat([df_Test,pred],axis=1)","05c171a8":"df_Test.shape","4d5a5c29":"# Create df_Train combining df_Train and df_Test. Df_Train now has 2919 rows for model fitment\ndf_Train2=pd.concat([df_Train,df_Test],axis=0)","77bdca66":"df_Train2.shape","421308e7":"# Create X_train and Y_Train for training model - this time with 2919 rows \nX_train=df_Train2.drop(['SalePrice'],axis=1)\ny_train=df_Train2['SalePrice']","320f80c8":"X_train.shape","93eb4fd9":"y_train","edb5d65d":"# Using CVbest Estimator for regressor \nregressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=1, missing=None, monotone_constraints=None,\n             n_estimators=900, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)","94ca0850":"# Using Regressor to fit X_train and Y-Train\nregressor.fit(X_train,y_train)","aa49dc27":"df_Test.drop(['SalePrice'],axis=1,inplace=True)","3e8cd4f1":"df_Test.shape","81183d43":"# Using regressor to predict SalePrice values of df_Test\ny_pred=regressor.predict(df_Test)","72286406":"pred=pd.DataFrame(y_pred)\nsub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\n#datasets = datasets.iloc[:1459,:]\n#datasets['Id'] = datasets['Id'].astype('int32') \ndatasets.to_csv('Mashup_sample_submission.csv',index=False)\ndatasets","06e8f219":"random_cv.fit(X_train,y_train)","dbc671c6":"HYPERPARAMETER TUNING*****","1b4f50ac":"from keras import backend as K\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))","49878433":"**PREDICTION MODELING USING KERAS**","517efb86":"df_pred = ann_pred","a1dfc0f9":"n_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }","181a222a":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU,PReLU,ELU\nfrom keras.layers import Dropout\n\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 176))\n\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n\n# Adding the third hidden layer\nclassifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n\n# Compiling the ANN\nclassifier.compile(loss= 'mean_squared_error', optimizer='Adamax')\n\n# Fitting the ANN to the Training set\nmodel_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)","7aaf15c9":"ann_pred=classifier.predict(df_Test)","a5287ab3":"pred=pd.DataFrame(ann_pred)\nsub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets = datasets.iloc[:1459,:]\ndatasets['Id'] = datasets['Id'].astype('int32') \ndatasets.to_csv('Tuned_First_ANN_sample_submission.csv',index=False)\ndatasets.head(30)","6efedb17":"booster=['gbtree','gblinear']\nbase_score=[0.25,0.5,0.75,1]","fb95f7e1":"df_pred.shape","08c14ffd":"random_cv = RandomizedSearchCV (estimator=regressor,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter= 70,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)","52a4b820":"random_cv.best_estimator_"}}