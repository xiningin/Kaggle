{"cell_type":{"9369ecc0":"code","a0dec646":"code","cd471845":"code","feb69b54":"code","fbe847a8":"code","4a311951":"code","104c57bd":"code","9b2d0c58":"code","34e7e4b8":"code","12574ab4":"code","62d67858":"code","48fde14f":"code","13c00297":"code","a54c4461":"code","d83ae4bb":"code","97777eab":"code","df99b368":"code","761dfffe":"code","baf4149e":"code","97d83f31":"code","b17e385e":"code","a025eb55":"markdown","842a14ea":"markdown","57982380":"markdown","eee9dc65":"markdown","a48719b7":"markdown","52fd0ada":"markdown","477eb5de":"markdown","76156549":"markdown","0a72312b":"markdown","c2c599d4":"markdown","1ec87df8":"markdown"},"source":{"9369ecc0":"import pandas as pd \nimport numpy as np \nimport json \nfrom ast import literal_eval","a0dec646":"df = pd.read_csv('..\/input\/train_v2.csv', nrows=10000)","cd471845":"df.head()","feb69b54":"json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\ndef parse_json_col(raw_str):\n    return pd.Series(json.loads(raw_str))\n\nfor col in json_cols:\n    parsed_df = df[col].apply(parse_json_col)\n    parsed_df.columns = [f'{col}_{x}' for x in parsed_df.columns]\n    df = pd.concat([df, parsed_df], axis=1)\n    df.drop(col, axis=1, inplace=True)","fbe847a8":"df.shape","4a311951":"df.filter(regex='.*_.*', axis=1).head()","104c57bd":"trafficSource_adwordsClickInfo_df = df.trafficSource_adwordsClickInfo.apply(pd.Series)\ntrafficSource_adwordsClickInfo_df.columns = [f'trafficSource_adwordsClickInfo_{x}' for x in trafficSource_adwordsClickInfo_df.columns]\ndf = pd.concat([df, trafficSource_adwordsClickInfo_df], axis=1)\ndf.drop('trafficSource_adwordsClickInfo', axis=1, inplace=True)","9b2d0c58":"df.shape","34e7e4b8":"df.filter(regex='trafficSource_adwordsClickInfo_.*', axis=1).head()","12574ab4":"# for customDimensions and hits columns\ndef parse_special_col(raw_str):\n    lst = literal_eval(raw_str)\n    if isinstance(lst, list) and lst:\n        return pd.Series(lst[0])\n    else:\n        return pd.Series({})","62d67858":"customDimensions_df = df.customDimensions.apply(parse_special_col)\ncustomDimensions_df.columns = [f'customDimensions_{x}' for x in customDimensions_df.columns]\ndf = pd.concat([df, customDimensions_df], axis=1)\ndf.drop('customDimensions', axis=1, inplace=True)","48fde14f":"df.shape","13c00297":"df.filter(regex='customDimensions_.*', axis=1).head()","a54c4461":"hits_df = df.hits.apply(parse_special_col)\nhits_df.columns = [f'hits_{x}' for x in hits_df.columns]\ndf = pd.concat([df, hits_df], axis=1)\ndf.drop('hits', axis=1, inplace=True)","d83ae4bb":"df.shape","97777eab":"df.filter(regex='hits_.*', axis=1).head()","df99b368":"df.drop(['hits_experiment', 'hits_customVariables', 'hits_customMetrics', 'hits_publisher_infos', 'hits_customDimensions'], axis=1, inplace=True)","761dfffe":"dict_cols = ['hits_page', 'hits_transaction', 'hits_item', 'hits_appInfo', \n        'hits_exceptionInfo', 'hits_eCommerceAction', 'hits_social', 'hits_contentGroup', 'hits_promotionActionInfo']\nfor col in dict_cols:\n    parsed_df = hits_df[col].apply(pd.Series)\n    parsed_df.columns = [f'{col}_{x}' for x in parsed_df.columns]\n    df = pd.concat([df, parsed_df], axis=1)\n    df.drop(col, axis=1, inplace=True)","baf4149e":"def parse_list(x):\n    if isinstance(x, list) and x:\n        return pd.Series(x[0])\n    else:\n        return pd.Series({})\n    \nfor col in ['hits_product', 'hits_promotion']:\n    parsed_df = hits_df[col].apply(parse_list)\n    parsed_df.columns = [f'{col}_{x}' for x in parsed_df.columns]\n    df = pd.concat([df, parsed_df], axis=1)\n    df.drop(col, axis=1, inplace=True)","97d83f31":"df.shape","b17e385e":"def flatten(in_csv, out_csv, nrows=None):\n    df = pd.read_csv(in_csv, dtype=np.object, nrows=nrows)\n    # json columns\n    json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\n    def parse_json_col(raw_str):\n        return pd.Series(json.loads(raw_str))\n    \n    for col in json_cols:\n        parsed_df = df[col].apply(parse_json_col)\n        parsed_df.columns = [f'{col}_{x}' for x in parsed_df.columns]\n        df = pd.concat([df, parsed_df], axis=1)\n        df.drop(col, axis=1, inplace=True)\n    \n    # trafficSource_adwordsClickInfo\n    trafficSource_adwordsClickInfo_df = df.trafficSource_adwordsClickInfo.apply(pd.Series)\n    trafficSource_adwordsClickInfo_df.columns = [f'trafficSource_adwordsClickInfo_{x}' for x in trafficSource_adwordsClickInfo_df.columns]\n    df = pd.concat([df, trafficSource_adwordsClickInfo_df], axis=1)\n    df.drop('trafficSource_adwordsClickInfo', axis=1, inplace=True)\n\n    # customDimensions\n    def parse_customDimensions(raw_str):\n        lst = literal_eval(raw_str)\n        if isinstance(lst, list) and lst:\n            return pd.Series(lst[0])\n        else:\n            return pd.Series({})\n    \n    customDimensions_df = df.customDimensions.apply(parse_customDimensions)\n    customDimensions_df.columns = [f'customDimensions_{x}' for x in customDimensions_df.columns]\n    df = pd.concat([df, customDimensions_df], axis=1)\n    df.drop('customDimensions', axis=1, inplace=True)\n\n    # hits\n    def parse_hits(raw_str):\n        lst = literal_eval(raw_str)\n        if isinstance(lst, list) and lst:\n            return pd.Series(lst[0])\n        else:\n            return pd.Series({})\n    \n    hits_df = df.hits.apply(parse_hits)\n    hits_df.columns = [f'hits_{x}' for x in hits_df.columns]\n    df = pd.concat([df, hits_df], axis=1)\n    df.drop('hits', axis=1, inplace=True)\n\n    # 'hits_page', 'hits_transaction', 'hits_item', 'hits_appInfo',\n    # 'hits_exceptionInfo', 'hits_eCommerceAction', 'hits_social', 'hits_contentGroup', 'hits_promotionActionInfo'\n    dict_cols = ['hits_page', 'hits_transaction', 'hits_item', 'hits_appInfo', \n        'hits_exceptionInfo', 'hits_eCommerceAction', 'hits_social', 'hits_contentGroup', 'hits_promotionActionInfo']\n    for col in dict_cols:\n        parsed_df = hits_df[col].apply(pd.Series)\n        parsed_df.columns = [f'{col}_{x}' for x in parsed_df.columns]\n        df = pd.concat([df, parsed_df], axis=1)\n        df.drop(col, axis=1, inplace=True)\n    \n    # 'hits_experiment', 'hits_customVariables', 'hits_customMetrics', 'hits_publisher_infos', 'hits_customDimensions' are empty\n    df.drop(['hits_experiment', 'hits_customVariables', 'hits_customMetrics', 'hits_publisher_infos', 'hits_customDimensions'], axis=1, inplace=True)\n\n    # 'hits_product', 'hits_promotion'\n    def parse_list(x):\n        if isinstance(x, list) and x:\n            return pd.Series(x[0])\n        else:\n            return pd.Series({})\n    \n    for col in ['hits_product', 'hits_promotion']:\n        parsed_df = hits_df[col].apply(parse_list)\n        parsed_df.columns = [f'{col}_{x}' for x in parsed_df.columns]\n        df = pd.concat([df, parsed_df], axis=1)\n        df.drop(col, axis=1, inplace=True)\n\n    df.to_csv(out_csv, index=False)\n\n    return df.shape","a025eb55":"## hits","842a14ea":"`hits_product`, `hits_promotion` are python list, we should flatten it.","57982380":"It takes about 1~2h to flatten the train_v2.csv and test_v2.csv. Have fun!","eee9dc65":"`hits_page`, `hits_transaction`, `hits_item`, `hits_appInfo`, `hits_exceptionInfo`, `hits_eCommerceAction`, `hits_social`, `hits_contentGroup`, `hits_promotionActionInfo` are python dict, we can should flatten it.","a48719b7":"This notebook loads train_v2.csv\/test_v2.csv file and flatten the json fields. ","52fd0ada":"Let's check the new columns. We found that `trafficSource_adwordsClickInfo` is also a json column which should be flattened.","477eb5de":"## device, geoNetwork, totals and trafficSource","76156549":"## customDimensions","0a72312b":"## Pack it to a function\nI have put the code in one function so you can copy it easily.","c2c599d4":"`hits_experiment`, `hits_customVariables`, `hits_customMetrics`, `hits_publisher_infos`, `hits_customDimensions` are empty, we can drop it.","1ec87df8":"We have 6 special columns which should be flattened. `device`, `geoNetwork`, `totals`,`trafficSource` is standard json fields and can't be easily flattened by json module; `customDimensions` and `hit` columns can not be processed by json module, bu can be processed by ast.literal_eval. "}}