{"cell_type":{"de5b530f":"code","f0632ba7":"code","dc93fff4":"code","b9c326d9":"code","d029d3af":"code","a7e4b85e":"code","4b0184ae":"code","9ff80b6b":"code","9606a87d":"code","9de45a1a":"code","27500d35":"code","8e30675b":"code","83d176df":"code","bbe30909":"code","afa7fd13":"code","eb2d7f51":"code","39249f16":"code","37d48e41":"code","308ac1e5":"code","213fe8db":"code","c1f8b535":"code","08a2f778":"code","e72db8d1":"code","a925dc92":"code","ca010ab1":"code","55bcf72a":"code","2887f0e6":"code","964bf9ec":"code","0d7aa4f5":"code","aa964095":"code","b78e0bf0":"code","c1329e61":"code","bc358e3f":"code","23f8f2ac":"code","f00bc302":"code","645525c6":"code","7c65c5ec":"code","666fcfc6":"code","b2674b0a":"code","d30bd861":"code","a3a37030":"code","14bc6d9c":"code","45e06d43":"code","5d9e3d58":"code","68d9d5f0":"code","1ed6d373":"code","46f954a0":"code","a8b60ce8":"code","23e00726":"code","b2f0a24a":"code","fd904737":"code","30279e63":"code","7efda4e4":"code","89cc7fbf":"code","e217e9fd":"code","f7b1049c":"code","7e7c4c43":"code","e982c8be":"code","ed7b3f9d":"code","ee3da2b7":"code","784c2937":"code","7e52d063":"code","8b2364e1":"code","d238c3c1":"code","7d54984f":"code","71bb6ad9":"code","09cba17f":"code","1ecf9699":"markdown","dd552c40":"markdown","a9aa8e88":"markdown","1c1390de":"markdown","bce2f8ce":"markdown","d9e77e22":"markdown","fe2e6abc":"markdown","e14fc7bd":"markdown","9eeffba9":"markdown","1ad14a1f":"markdown","2fe2fd3e":"markdown","5e812508":"markdown","a9c477ed":"markdown","36cfc896":"markdown","0e2df923":"markdown","83709e87":"markdown","f5a33b2c":"markdown","82c21d86":"markdown","21b83895":"markdown","1161d3b4":"markdown","281d5bdd":"markdown","85f83cc5":"markdown","b100449f":"markdown","33a005f6":"markdown","22281928":"markdown","eb63f9dd":"markdown","fc071628":"markdown","f690c367":"markdown","b72d4679":"markdown","d783e2da":"markdown","eaadfe4c":"markdown","9bd46781":"markdown","86ebf2e7":"markdown","99ddf766":"markdown","41c3c776":"markdown","e50cc80a":"markdown","0f0d62c8":"markdown","53095ce4":"markdown","302caba0":"markdown","5bf23320":"markdown","7e6dc4a1":"markdown","2ca7a744":"markdown","012733d2":"markdown","1f1bfb0b":"markdown","1f8098f2":"markdown","8b917046":"markdown"},"source":{"de5b530f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport math, time, datetime\n# from math import sqrt\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import colors\nimport missingno as msno\n%matplotlib inline\n\nfrom sklearn.model_selection import (train_test_split, GridSearchCV)\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, scale\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (confusion_matrix, mean_squared_error, r2_score, classification_report,\n                            roc_auc_score, roc_curve, precision_recall_curve, auc, log_loss)\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import mutual_info_classif","f0632ba7":"df_first = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test_first = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_gender_sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndf_first.head()","dc93fff4":"fig, ax = plt.subplots(figsize = (12, 2))\nax.barh(df_first['Survived'].unique(), df_first['Survived'].value_counts(), align='center', color=['red', 'green'])\nax.text(530, 0, df_first['Survived'].value_counts()[0], ha='center', va='center', color='w', size=20)\nax.text(320, 1, df_first['Survived'].value_counts()[1], ha='center', va='center', color='w', size=20)\nax.set_yticks(df_first['Survived'].unique())\nax.set_yticklabels(df_first['Survived'].unique())\nax.invert_yaxis()\nax.set_ylabel('Survived')\nax.set_title('How many people survived?')\n\nplt.show()","b9c326d9":"df_first.info()\nprint(\"----------------------------\")\ndf_test_first.info()","d029d3af":"## plot graphic of missing values\nmsno.matrix(df_first, figsize=(12, 6))","a7e4b85e":"# plot graphic of missing values\nmsno.matrix(df_test_first, figsize=(12, 6))","4b0184ae":"# drop unnecessary columns, which won't be used in analysis and prediction\ndf = df_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)\ndf_test = df_test_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)","9ff80b6b":"# Checking for missing values in train data\ndf.isnull().sum()","9606a87d":"# Checking for missing values in test data\ndf_test.isnull().sum()","9de45a1a":"# getting the Pclass of the Fare missing value\nPclass_fare_mis = df_test[df_test['Fare'].isnull()]['Pclass'].values[0]\nprint(Pclass_fare_mis)\nprint(\"----------------------------\")\n# getting the mean of Fare for the Pclass 3\ndf_Pclass_mean = df_test.groupby(['Pclass']).mean().loc[Pclass_fare_mis,'Fare']\n\n# fill NaN values in Fare column with mean\ndf_test.loc[152, 'Fare'] = df_Pclass_mean\n\ndf_test.isnull().sum()","27500d35":"# getting the Pclass of the Embarked missing value\nPclass_embarked_mis = df[df['Embarked'].isnull()]['Pclass'].values\nprint(Pclass_embarked_mis)\nprint(\"----------------------------\")\n# distribution in Embarked by their Pclass\nprint(df[df['Pclass'] == 1]['Embarked'].value_counts())\n\n# for Pclass = 1, the most occurring values are S and C. \n# Therefore, one is filled as 'S', and the other as 'C'. \ndf.loc[61, 'Embarked'] = 'S'\ndf.loc[829, 'Embarked'] = 'C'\n\ndf.isnull().sum()","8e30675b":"mis_val_female = df[df['Sex'] == 'female']['Age'].isna().sum()\nmis_val_male = df[df['Sex'] == 'male']['Age'].isna().sum()\n\nprint(f'Missing values in Age for female: {mis_val_female}')\nprint(f'Missing values in Age for male: {mis_val_male}')","83d176df":"def generate_random_numbers(df):\n    nan_count = df.isna().sum()\n    \n    ax = df.hist(bins=10, density=True, stacked=True, color='teal', alpha=0.6)\n    ax1 = df.plot(kind='kde', color='teal')\n    \n    # mean age\n    df_mean = df.mean(skipna=True)\n    # median age\n    df_median = df.median(skipna=True)\n    # std age\n    df_std = df.std(skipna=True)\n        \n    # getting density peak values\n    density = stats.gaussian_kde(df.dropna())\n    xs = np.linspace(df.min(),df.max(),200)\n    ys = density(xs)\n    index = np.argmax(ys)\n    max_y = ys[index]\n    max_x = xs[index]\n    \n    # density peak plot\n    ax.axvline(max_x, 0, 1, color='blue', label='peak(x): {:.2f}'.format(max_x))\n    \n    # once the median is closer than the mean to the x value of the density peak, median will be used.\n    # otherwise, the mean will be used to fill in the missing values by generated random numbers.\n    if abs(max_x - df_mean) <=  abs(max_x - df_median):\n        rand_numbers = np.random.randint(df_mean - df_std, df_mean + df_std, \n                                   size = nan_count)\n        ax.axvline(df_mean, 0, 1, color='red', label='mean: {:.2f}'.format(df_mean))\n        # we will generate random numbers using the mean {between (mean - std) & (mean + std)}\n        ax.axvspan(df_mean - df_std, df_mean + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n    else: \n        rand_numbers = np.random.randint(df_median - df_std, df_median + df_std, \n                                   size = nan_count)\n        ax.axvline(df_median, 0, 1, color='red', label='median: {:.2f}'.format(df_median))\n        \n        # we will generate random numbers using the median {between (median - std) & (median + std)}\n        ax.axvspan(df_median - df_std, df_median + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n\n    ax.set(xlabel='Age')\n    ax.legend(title='Female')\n    plt.show()\n\n    return (rand_numbers)","bbe30909":"df_female_age = df[df['Sex'] == 'female']['Age']\ndf_female_rand_numbers = generate_random_numbers(df_female_age)\n\ndf_female_null = (df[df['Age'].isnull()]['Sex'] == 'female')\ndf_female_ind_null = df_female_null[df_female_null].index\ndf.loc[df_female_ind_null, 'Age'] = df_female_rand_numbers\n\nprint(df.isnull().sum())","afa7fd13":"df_male_age = df[df['Sex'] == 'male']['Age']\ndf_male_rand_numbers = generate_random_numbers(df_male_age)\n\ndf_male_null = (df[df['Age'].isnull()]['Sex'] == 'male')\ndf_male_ind_null = df_male_null[df_male_null].index\ndf.loc[df_male_ind_null, 'Age'] = df_male_rand_numbers\n\nprint(df.isnull().sum())","eb2d7f51":"df_test_female_age = df_test[df_test['Sex'] == 'female']['Age']\ndf_test_female_rand_numbers = generate_random_numbers(df_test_female_age)\n\ndf_test_female_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'female')\ndf_test_female_ind_null = df_test_female_null[df_test_female_null].index\ndf_test.loc[df_test_female_ind_null, 'Age'] = df_test_female_rand_numbers\n\nprint(df_test.isnull().sum())","39249f16":"df_test_male_age = df_test[df_test['Sex'] == 'male']['Age']\ndf_test_male_rand_numbers = generate_random_numbers(df_test_male_age)\n\ndf_test_male_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'male')\ndf_test_male_ind_null = df_test_male_null[df_test_male_null].index\ndf_test.loc[df_test_male_ind_null, 'Age'] = df_test_male_rand_numbers\n\nprint(df_test.isnull().sum())","37d48e41":"## Create categorical variable for traveling alone\ndf['TravelAlone']=np.where((df[\"SibSp\"]+df[\"Parch\"])>0, 0, 1)\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\ndf.head()","308ac1e5":"## Create categorical variable for traveling alone\ndf_test['TravelAlone']=np.where((df_test[\"SibSp\"]+df_test[\"Parch\"])>0, 0, 1)\ndf_test.drop('SibSp', axis=1, inplace=True)\ndf_test.drop('Parch', axis=1, inplace=True)\ndf_test.head()","213fe8db":"col_order = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone', 'Survived']\ndf=df[col_order]\ndf.head()","c1f8b535":"for col in df:\n    unique_vals = np.unique(df[col])\n    nr_values = len(unique_vals)\n    if nr_values < 10:\n        print(f'The number of values for feature {col} :{nr_values} -- {unique_vals}')\n    else:\n        print(f'The number of values for feature {col} :{nr_values}')","08a2f778":"axes = pd.plotting.scatter_matrix(df, alpha=0.7, figsize=(14, 8), diagonal='kde')\nfor ax in axes.flatten():\n    ax.xaxis.label.set_rotation(0)\n    ax.yaxis.label.set_rotation(90)\n    ax.yaxis.label.set_ha('right')\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nplt.gcf().subplots_adjust(wspace=0, hspace=0)\nplt.show()","e72db8d1":"def subplot_graph(df, features):\n    f_count = len(features)\n    cols = f_count\n    if f_count < 4:\n        rows = 1\n    else:\n        rows = math.ceil(f_count\/3)\n        cols = 3\n                \n    # Set up the matplotlib figure\n    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n        \n    sub_rows = 0\n    sub_cols = 0    \n    for f in features:\n        if sub_cols > cols-1:\n            sub_cols = 0\n            sub_rows += 1\n            \n        unique_vals = len(np.unique(df[f]))\n        if unique_vals < 10:\n            df_sub = df.groupby(df.columns[-1])[f].value_counts().sort_index(level=0)\n            x_no = list(df_sub.xs(0, level=0).index)\n            y_no = list(df_sub.xs(0, level=0).values)\n            x_yes = list(df_sub.xs(1, level=0).index)\n            y_yes = list(df_sub.xs(1, level=0).values)\n            list_no = list(zip(x_no, y_no))\n            list_yes = list(zip(x_yes, y_yes))\n            df_no = pd.DataFrame(list_no)\n            df_yes = pd.DataFrame(list_yes)\n\n            df_plot = pd.merge(df_no, df_yes, on=df_no.columns[0], how=\"outer\")\n            df_plot = df_plot.fillna(0)\n            try:\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                   label='1', color='steelblue')\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n            except:\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                             label='1', color='steelblue')  \n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        else:    \n            df_3 = df[df[df.columns[-1]] == 0][f]\n            df_4 = df[df[df.columns[-1]] == 1][f]\n            try:\n                df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_xlim(xmin=0)\n            except:\n                df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n            \n        sub_cols += 1\n        \n    fig.tight_layout()\n    fig.show()\n\nfeatures = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone']\nsubplot_graph(df, features)","a925dc92":"f_count = len(features)\ncols = f_count\nif f_count < 4:\n    rows = 1\nelse:\n    rows = math.ceil(f_count\/3)\n    cols = 3\n    \n# Set up the matplotlib figure\nfig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n\n\nsub_rows = 0\nsub_cols = 0    \nfor f in features:\n    if sub_cols > cols-1:\n        sub_cols = 0\n        sub_rows += 1\n        \n    \n    unique_vals = len(np.unique(df[f]))\n    if unique_vals < 10:\n        df_yy = pd.DataFrame()\n        df1 = df.groupby([f])[df.columns[-1]].value_counts().sort_index(level=0)\n        df_uniq = df1.index.levels[0]\n        f_uniqs = []\n        sur_rates = []\n        for uniq in df_uniq:\n            try:\n                df2 = df1.xs(uniq, level=0)\n                df_sum = df2.sum()\n                df_lev_1_each = df2[1]\n                sur_rate = round((df_lev_1_each \/ df_sum)*100, 1)\n                sur_rates.append(sur_rate)\n                f_uniqs.append(uniq)\n            except:\n                pass\n\n        df_xx = pd.DataFrame(sur_rates, index=f_uniqs, columns=[f])\n        df_yy = pd.concat([df_yy, df_xx], axis=1)\n        \n        df_sub = df_yy[f].dropna()\n#         print(df_sub)\n\n        try:\n            axes[sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        except:\n            axes[sub_rows, sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_rows, sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n    else:    \n        df_3 = df[df[df.columns[-1]] == 0][f]\n        df_4 = df[df[df.columns[-1]] == 1][f]\n        try:\n            df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].legend(title=df.columns[-1])\n            axes[sub_cols].set_xlim(xmin=0)\n        except:\n            df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n            axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n        \n    sub_cols += 1\n    \nfig.tight_layout()\nfig.show()","ca010ab1":"def create_heatmap(hm, figsize=(7, 6)):\n    fig, ax = plt.subplots(figsize=figsize)\n\n    im = ax.imshow(hm, \n    #                vmin=0, vmax=10, \n                   cmap='viridis', aspect='auto')\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax)\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(hm.columns)))\n    ax.set_yticks(np.arange(len(hm.columns)))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels(hm.columns)\n    ax.set_yticklabels(hm.columns)\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n    ax.set_xticks(np.arange(hm.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(hm.shape[0]+1)-.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(hm.columns)):\n        for j in range(len(hm.columns)):\n            hm_val = round(hm.values[i, j], 2)\n            if hm_val > 0.85:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"black\", size=16)\n            else:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"w\", size=16)\n\n    fig.tight_layout()\n    plt.show()","55bcf72a":"hm = df.corr()\ncreate_heatmap(hm)","2887f0e6":"X = df[df.columns[:-1]]\ny = df[df.columns[-1]]\nX_kaggle = df_test.copy()\nprint(X.shape)\nprint(X_kaggle.shape)","964bf9ec":"# There is no categorical columns in the dataframe\ncategorical_cols = list(set(X.columns) - set(X._get_numeric_data().columns))\ncategorical_cols","0d7aa4f5":"numerical_cols = list(X._get_numeric_data().columns)\nnumerical_cols","aa964095":"categorical_cols + ['Pclass', 'TravelAlone']","b78e0bf0":"from sklearn.compose import make_column_transformer\n\ncolumn_trans = make_column_transformer((OneHotEncoder(), categorical_cols + ['Pclass', 'TravelAlone']),\n                                      remainder='passthrough')\nX_trans = column_trans.fit_transform(X)\nX_kaggle_trans = column_trans.fit_transform(X_kaggle)\n\ncols = []\nfor i in column_trans.transformers_[0][2]:\n    cols_enc = sorted(X[i].unique())\n    for col_enc in cols_enc:\n        col_name = str(i) + '_' + str(col_enc)\n        cols.append(col_name)\n        \nfor i in column_trans.transformers_[1][2]:\n    col_name = X.columns[i]\n    cols.append(col_name)\n    \nX_encoded = pd.DataFrame(X_trans, columns=cols)\nX_kaggle_encoded = pd.DataFrame(X_kaggle_trans, columns=cols)\nprint(X_encoded.shape)\nprint(X_kaggle_encoded.shape)\nX_encoded.head()","c1329e61":"X_kaggle_encoded.head()","bc358e3f":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_encoded)\nX_encoded.columns[var_thres.get_support()]\n\nconstant_columns = [column for column in X_encoded.columns\n                    if column not in X_encoded.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","23f8f2ac":"X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, train_size = 0.8, test_size=0.2, random_state=42)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","f00bc302":"X_train_scaled = scale(X_train)\nX_test_scaled = scale(X_test)","645525c6":"param_grid = [{\n    'kernel': ['rbf'],\n    'C': [0.1, 0.5, 1, 10, 100],\n    'gamma': ['auto', 'scale', 1, 0.1, 0.01, 0.001, 0.0001],\n},\n             ]\n\noptimal_param = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=0)\noptimal_param.fit(X_train_scaled, y_train)\nprint(optimal_param.best_params_)\n# print how our model looks after hyper-parameter tuning\nprint(optimal_param.best_estimator_)","7c65c5ec":"svm = SVC(kernel='rbf',C=0.5, gamma=0.1, random_state=42)\nsvm.fit(X_train_scaled, y_train)\ny_pred_train = svm.predict(X_train_scaled)\ny_pred_test = svm.predict(X_test_scaled)","666fcfc6":"print(\"Test Accuracy: {:.1%}\".format(svm.score(X_test_scaled,y_test)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_test, y_pred_test))\nprint('='*10)\nprint(classification_report(y_test, y_pred_test))","b2674b0a":"hm_X_train = X_train.corr()\ncreate_heatmap(hm_X_train, figsize=(10, 6))","d30bd861":"#  to select highly correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                \n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","a3a37030":"corr_features = correlation(X_train, 0.85)\ncorr_features","14bc6d9c":"X_train = X_train.drop(corr_features, axis=1)\nX_test = X_test.drop(corr_features, axis=1)\nX_kaggle_encoded = X_kaggle_encoded.drop(corr_features, axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_kaggle_encoded.shape)","45e06d43":"X_train_scaled = scale(X_train)\nX_test_scaled = scale(X_test)","5d9e3d58":"# Run a Tree-based estimators (i.e. decision trees & random forests)\ndt = DecisionTreeClassifier(random_state=15, criterion = 'entropy', max_depth = 10)\ndt.fit(X_train, y_train)\n\nfi_col = []\nfi = []\nfor i,column in enumerate(X_train):\n#     print('The feature importance for {} is: {:.2%}'.format(column, dt.feature_importances_[i]))    \n    fi_col.append(column)\n    fi.append(dt.feature_importances_[i])\n\n# Creating a Dataframe\nfi_df = zip(fi_col, fi)\nfi_df = pd.DataFrame(fi_df, columns = ['Feature','Feature Importance'])\n\n# Ordering the data\nfi_df = fi_df.sort_values('Feature Importance', ascending = False).reset_index(drop=True)\nfi_df","68d9d5f0":"# determine the mutual information\nmutual_info = mutual_info_classif(X_train.values, y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","1ed6d373":"def model_evaluation(cols):\n    X_train_scaled = scale(X_train[cols])\n    X_test_scaled = scale(X_test[cols])\n\n    svm = SVC(kernel='rbf',C=0.5, gamma=0.1, random_state=42)\n    svm.fit(X_train_scaled, y_train)\n    y_pred_train = svm.predict(X_train_scaled)\n    y_pred_test = svm.predict(X_test_scaled)\n\n#     print(\"Test Accuracy: {:.1%}\".format(svm.score(X_test_scaled,y_test)))\n#     print(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\n#     print(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\n#     print('='*10)\n\n    svc_acc = svm.score(X_test_scaled,y_test)\n    svc_mse = mean_squared_error(y_test, y_pred_test)\n    svc_r2 = r2_score(y_test, y_pred_test)\n    \n    return svc_acc, svc_mse, svc_r2","46f954a0":"import itertools\n\nstuff = X_train.columns\nbest_cols = []\nbest_acc = []\nbest_mse = []\nbest_r2 = []\nfor L in range(0, len(X_train)+1):\n    for subset in itertools.combinations(stuff, L):\n        sub_list = list(subset)\n        if len(sub_list) != 0:\n            svc_acc, svc_mse, svc_r2 = model_evaluation(sub_list)\n            best_cols.append(sub_list)\n            best_acc.append(svc_acc)\n            best_mse.append(svc_mse)\n            best_r2.append(svc_r2)","a8b60ce8":"best_zip = zip(best_cols, best_acc, best_mse, best_r2)\ndf_best_acc = pd.DataFrame(best_zip, columns=['columns', 'accuracy', 'mse', 'r2'])\ndf_best_acc = df_best_acc.sort_values('accuracy', ascending = False).reset_index(drop=True)\npd.set_option('max_colwidth', -1)\ndf_best_acc.head(10)","23e00726":"cols_final = ['Sex_female', 'Pclass_1', 'Pclass_2', 'Age', 'TravelAlone_0', 'Fare']\n\nX_train_final = X_train[cols_final]\nX_test_final = X_test[cols_final]\nX_kaggle_final = X_kaggle_encoded[cols_final]\n\nprint(X_train_final.shape)\nprint(X_test_final.shape)\nprint(X_kaggle_final.shape)","b2f0a24a":"X_train_scaled = scale(X_train_final)\nX_test_scaled = scale(X_test_final)\n\nsvm = SVC(kernel='rbf',C=0.5, gamma=0.1, random_state=42, probability=True)\nsvm.fit(X_train_scaled, y_train)\ny_pred_train = svm.predict(X_train_scaled)\npred_proba_train = svm.predict_proba(X_train_scaled)\n\nprint(\"Train Accuracy: {:.1%}\".format(svm.score(X_train_scaled,y_train)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_train, y_pred_train)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_train, y_pred_train)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_train, y_pred_train))\nprint('='*10)\nprint(classification_report(y_train, y_pred_train))","fd904737":"y_pred_test = svm.predict(X_test_scaled)\npred_proba_test = svm.predict_proba(X_test_scaled)\n\nprint(\"Test Accuracy: {:.1%}\".format(svm.score(X_test_scaled,y_test)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_test, y_pred_test))\nprint('='*10)\nprint(classification_report(y_test, y_pred_test))","30279e63":"def confusion_matrix_func(cm, cm_title):\n    fig, ax = plt.subplots(figsize=(4, 4))\n\n    # Plot the heatmap\n    im = ax.imshow(cm, interpolation='nearest', cmap='Reds', aspect='auto')\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(cm.tolist())))\n    ax.set_yticks(np.arange(len(cm.tolist())))\n\n\n    thresh = cm.max() \/ 1.5\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(cm.tolist())):\n        for j in range(len(cm.tolist())):\n            text = ax.text(j, i, cm.tolist()[i][j],\n                           ha=\"center\", va=\"center\", size=16,\n                           color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Let the horizontal axes labeling appear on top.\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n\n    plt.xlabel('Actual value', size=16)\n    plt.ylabel('Predicted value', size=16)\n    plt.title(cm_title, size=20, x=0.2, y=1.2)\n    plt.show()\n# https:\/\/matplotlib.org\/stable\/gallery\/images_contours_and_fields\/image_annotated_heatmap.html","7efda4e4":"def Confusion_matrix_metrics(TP, FP, FN, TN):\n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP \/ (TP + FN)\n    print('The True Positive Rate is: {:.2%}'.format(TPR))\n    # Specificity, selectivity or true negative rate (TNR)\n    TNR = TN \/ (TN + FP)\n    print('The True Negative Rate is: {:.2%}'.format(TNR))\n    print('='*10)\n\n    # accuracy (ACC)\n    ACC = (TP + TN) \/ (TP + TN + FP + FN)\n    print('The Accuracy is: {:.2%}'.format(ACC))\n    # balanced accuracy (BA)\n    BA = (TPR + TNR) \/ 2\n    print('The Balanced Accuracy is: {:.2%}'.format(BA))\n    print('='*10)\n\n    # Precision or positive predictive value\n    PPV = TP \/ (TP + FP)\n    print('The Precision is: {:.2%}'.format(PPV))\n    # negative predictive value (NPV)\n    NPV = TN \/ (TN + FN)\n    print('The Negative Predictive Value is: {:.2%}'.format(NPV))\n    # false discovery rate (FDR)\n    FDR = 1 - PPV\n    print('The False Discovery Rate is: {:.2%}'.format(FDR))\n    # false omission rate (FOR)\n    FOR = 1 - NPV\n    print('The False Omission Rate is: {:.2%}'.format(FOR))\n    print('='*10)\n\n    # prevalence threshold (PT)\n    PT = (math.sqrt(TPR*(1 - TNR)) + TNR - 1)\/(TPR + TNR - 1)\n    print('The Prevalence Threshold is: {:.2}'.format(PT))\n    # F1 score\n    F1 = 2*TP \/ (2*TP + FP + FN)\n    print('The F1 Score is: {:.2}'.format(F1))\n    # Matthews correlation coefficient (MCC) or phi coefficient\n    MCC = ((TP*TN) - (FP*FN)) \/ math.sqrt((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))\n    print('The Matthews Correlation Coefficient is: {:.2}'.format(MCC))\n    print('='*10)\n\n    # False positive rate or False alarm rate\n    FPR = FP \/ (FP + TN)\n    print('The False positive rate is: {:.2}'.format(FPR))\n    # False negative rate or Miss Rate\n    FNR = FN \/ (FN + TP)\n    print('The False Negative Rate is: {:.2%}'.format(FNR))","89cc7fbf":"cm = confusion_matrix(y_train, y_pred_train).T\nconfusion_matrix_func(cm, cm_title=\"Confusion Matrix_Train\")","e217e9fd":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","f7b1049c":"cm_test = confusion_matrix(y_test, y_pred_test).T\nconfusion_matrix_func(cm_test, cm_title=\"Confusion Matrix_Test\")","7e7c4c43":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm_test.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","e982c8be":"# TRAIN\n# calculate scores\nlr_auc = roc_auc_score(y_train, pred_proba_train[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc))\n# calculate roc curves\nlr_fpr, lr_tpr, thresholds = roc_curve(y_train, pred_proba_train[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc = pd.DataFrame({\n    'False Positive Rate': lr_fpr,\n    'True Positive Rate': lr_tpr\n}, index=thresholds)\ndf_roc.index.name = \"Thresholds\"\ndf_roc.columns.name = \"Rate\"\n\n\n# TEST\n# calculate scores\nlr_auc_test = roc_auc_score(y_test, pred_proba_test[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc_test))\n# calculate roc curves\nlr_fpr_test, lr_tpr_test, thresholds_test = roc_curve(y_test, pred_proba_test[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc_test = pd.DataFrame({\n    'False Positive Rate': lr_fpr_test,\n    'True Positive Rate': lr_tpr_test\n}, index=thresholds_test)\ndf_roc_test.index.name = \"Thresholds\"\ndf_roc_test.columns.name = \"Rate\"\n\n\n# GRAPH\n# Set up the matplotlib figure\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\n\n# row=0, col=0\naxes[0, 0].plot(df_roc.iloc[:,0], df_roc.iloc[:,1], color='red', linewidth=2, \n                label=f'AUC={lr_auc:.2f}')\naxes[0, 0].fill_between(df_roc.iloc[:,0], df_roc.iloc[:,1], 0, color='red', alpha=0.3)\naxes[0, 0].plot(df_roc_test.iloc[:,0], df_roc_test.iloc[:,1], color='black', linewidth=2, \n                label=f'AUC_test={lr_auc_test:.2f}')\naxes[0, 0].plot([0, 1], [0, 1], color='green', linestyle='--', linewidth=1,\n                label='No Skill')\n\n# index of the first threshold for which the sensibility > 0.90\nidx = np.min(np.where(lr_tpr > 0.90))\naxes[0, 0].plot([0,lr_fpr[idx]], [lr_tpr[idx],lr_tpr[idx]], 'k--', color='blue')\naxes[0, 0].plot([lr_fpr[idx],lr_fpr[idx]], [0,lr_tpr[idx]], 'k--', color='blue')\n# Annotation\naxes[0, 0].annotate('(%.2f, %.2f)'%(lr_fpr[idx], lr_tpr[idx]),\n            (lr_fpr[idx], lr_tpr[idx]), \n            xytext =(-2 * 50, -30),\n            textcoords ='offset points',\n            bbox = dict(boxstyle =\"round\", fc =\"0.8\"), \n            arrowprops = dict(arrowstyle = \"->\"))\n\naxes[0, 0].set_xlabel('False Positive Rate', size=12)\naxes[0, 0].set_ylabel('True Positive Rate (recall)', size=12)\naxes[0, 0].legend(title='kNN')\naxes[0, 0].set_title('ROC curve', color='red', size=14)\n\n# row=0, col=1\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"True Positive Rate\"][1:], color='blue', linewidth=2, \n                label='TPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"True Positive Rate\"][1:], color='black', linewidth=2, \n                label='TPR_test')\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"False Positive Rate\"][1:], color='orange', linewidth=2, \n                label='FPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"False Positive Rate\"][1:], color='black', linewidth=2, \n                label='FPR_test')\n\naxes[0, 1].set_xlabel('Threshold', size=12)\naxes[0, 1].legend()\naxes[0, 1].set_title('TPR and FPR at every threshold', color='red', size=14)\n\n# row=1, col=0\nprecision, recall, thresholds = precision_recall_curve(y_test, pred_proba_test[:, 1])\n\naxes[1, 0].plot(recall, precision, color='green', linewidth=2, \n                label=f'PR_Curve (AUC={auc(lr_fpr, lr_tpr):.2f})')\naxes[1, 0].fill_between(recall, precision, 0, color='green', alpha=0.3)\n\naxes[1, 0].set_xlabel('Recall', size=12)\naxes[1, 0].set_ylabel('Precision', size=12)\naxes[1, 0].legend()\naxes[1, 0].set_title('Precision-Recall Curve', color='red', size=14)\n\nfig.tight_layout()\nfig.show()","ed7b3f9d":"# Running Log loss on training\nprint('The Log Loss on Training is: {:.2}'.format(log_loss(y_train, pred_proba_train[:, 1])))\n\n# Running Log loss on testing\nprint('The Log Loss on Testing Dataset is: {:.2}'.format(log_loss(y_test, pred_proba_test[:, 1])))","ee3da2b7":"pca = PCA()\nX_train_pca = pca.fit_transform(X_train_scaled)\n\nper_var = np.round(pca.explained_variance_ratio_*100, decimals=1)\nlabels = ([str(x) for x in range(1, len(per_var) + 1)])\n\nplt.bar(x=range(1, len(per_var) + 1), height=per_var)\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Components')\nplt.title('Scree Plot')\nplt.show()","784c2937":"train_pc1_coords = X_train_pca[:, 0]   # pc1 contains the x-axis coordinates of the dat after PCA\ntrain_pc2_coords = X_train_pca[:, 1]   # pc2 contains the y-axis coordinates of the dat after PCA\n\n# Center and scale the PCs\npca_train_scaled = scale(np.column_stack((train_pc1_coords, train_pc2_coords)))\n\n# Optimize the SVM fit to the x and y-axis coordinates of the data after PCA dimension reduction.\nparam_grid = [{'C': [0.5, 1, 10, 100], \n              'gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']},\n             ]\n\noptimal_param = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=0)\noptimal_param.fit(X_train_scaled, y_train)\nprint(optimal_param.best_params_)","7e52d063":"svm_pca = SVC(random_state=42, C=1, gamma='scale')\nsvm_pca.fit(pca_train_scaled, y_train)\n\n# Transform the test dataset with the PCA\nX_test_pca = pca.transform(X_train_scaled)\ntrain_pc1_coords = X_train_pca[:, 0]\ntrain_pc2_coords = X_train_pca[:, 1]\ntest_pc1_coords = X_test_pca[:, 0]\ntest_pc2_coords = X_test_pca[:, 1]\n\n# Create a matrix of points that we can use to show the decision regions. \n# The matrix will be a little bit larger than the transformed PCA points so that we can plot\n# all of the points on it without them being on the edge. \n\nx_min = test_pc1_coords.min() - 1\nx_max = test_pc1_coords.max() + 1\ny_min = test_pc2_coords.min() - 1\ny_max = test_pc2_coords.max() + 1\n\nxx, yy = np.meshgrid(np.arange(start=x_min, stop=x_max, step=0.1),\n                     np.arange(start=y_min, stop=y_max, step=0.1)\n                    )\n\n# We will classify every point in that matrix with the SVM. \n# Points on one side of the classification boundary will get 0, points on the other side will get 1.\nZ = svm_pca.predict(np.column_stack((xx.ravel(), yy.ravel())))\n# Z is just a long array of lots of 0s and 1s, which reflect how each point in the mesh was classified.\n# We use reshape() so that each classification (0 or 1) correspond to a specific point in the matrix. \nZ = Z.reshape(xx.shape)\n\n# contourf() to draw a filled contour plot using the matrix values and classifications.\n# The contours will be filled according to the predicted classifications (0s and 1s) in Z\nfig, ax = plt.subplots(figsize=(10, 10))\nax.contourf(xx, yy, Z, alpha=0.1)\n\ncmap = colors.ListedColormap(['#e41a1c', '#4daf4a'])\n\nscatter = ax.scatter(test_pc1_coords, test_pc2_coords, c=y_train, cmap=cmap, s=100, \n                     edgecolors='k', alpha=0.7)\n\nlegend = ax.legend(scatter.legend_elements()[0],\n                   scatter.legend_elements()[1],\n                   loc='upper right'\n                  )\n\nax.set_ylabel('PC2')\nax.set_xlabel('PC1')\nax.set_title('Decision surface using the PCA transformed\/projected features')\nplt.show()","8b2364e1":"df_gender_sub.head()","d238c3c1":"submission = df_gender_sub.drop('Survived', axis=1)\nX_kaggle_final = scale(X_kaggle_final)\ny_pred_kaggle = svm.predict(X_kaggle_final)\nsubmission['Survived'] = y_pred_kaggle\nsubmission.head()","7d54984f":"# Are our test and submission dataframes the same length?\nif len(submission) == len(df_gender_sub):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","71bb6ad9":"# Convert submisison dataframe to csv for submission to csv for Kaggle submisison\nsubmission.to_csv('titanic_submission_svm.csv', index=False)\nprint('Submission CSV is ready!')","09cba17f":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"titanic_submission_svm.csv\")\nsubmissions_check.head()","1ecf9699":"<a id='41'><\/a>\n#### 4_8_1 Feature Selection - Drop Features Using Pearson Correlation","dd552c40":"<a id='45'><\/a>\n### 4_5 Separate the dataset into train and test","a9aa8e88":"<a id='2'><\/a>\n## 2 Read CSV train\/test files into DataFrame","1c1390de":"#### 3_2_2 Test data","bce2f8ce":"<a id='31'><\/a>\n### 3_1 Missing Values","d9e77e22":"<a id='5'><\/a>\n## 5. Submission","fe2e6abc":"![titanic_data_dict.png](attachment:titanic_data_dict.png)","e14fc7bd":"<a id='35'><\/a>\n### 3_5 Exploratory Data Analysis","9eeffba9":"##### 3_1_1_1 Missing values in test data","1ad14a1f":"## Index\n\n[1 Importing packages](#1)<br>\n[2 Read CSV train\/test files into DataFrame](#2)<br>\n[3 Data Preprocessing](#3)<br>\n    <ul>\n        <li>[3_1 Missing Values](#31)<\/li>\n            <ul><li>[3_1_1 Fare - Missing Values](#311)<\/li>\n            <li>[3_1_2 Embarked - Missing Values](#12)<\/li>\n            <li>[3_1_3 Age - Missing Values](#313)<\/li><\/ul>\n        <li>[3_2 Combine SibSp and Parch for Simplicity](#32)<\/li>\n        <li>[3_3 Move the dependent column to the end](#33)<\/li>\n        <li>[3_4 Investigate all the elements within each Feature](#34)<\/li>\n        <li>[3_5 Exploratory Data Analysis](#35)<\/li>\n    <\/ul>\n[4 Regressions and Results](#4)<br>\n    <ul>\n        <li>[4_1 Separate the dataset](#41)<\/li>\n        <li>[4_2 Check categorical columns](#42)<\/li>\n        <li>[4_3 One_Hot_Encoding](#43)<\/li>\n        <li>[4_4 Check zero variance features](#44)<\/li>\n        <li>[4_5 Separate the dataset into train and test](#45)<\/li>\n        <li>[4_6 Optimize parameters with Cross Validation and GridSearchCV()](#46)<\/li>\n        <li>[4_7 Support Vector Machine Model](#47)<\/li>\n        <li>[4_8 Feature Selection](#48)<\/li>\n            <ul><li>[4_8_1 Feature Selection - Drop Features Using Pearson Correlation](#481)<\/li>\n            <li>[4_8_2 Feature_Selection - Tree-based](#482)<\/li>\n            <li>[4_8_3 Feature_Selection - Mutual information](#483)<\/li>\n            <li>[4_8_4 Feature_Selection - Feature_Selection - Evaluation of all column combinations](#484)<\/li>\n            <li>[4_8_5 Feature_Selection - Final](#485)<\/li><\/ul>\n        <li>[4_9 Evaluating the Model](#49)<\/li>\n        <li>[4_10 Confusion matrix](#410)<\/li>\n        <li>[4_11 roc curve and auc](#411)<\/li>\n        <li>[4_12 Logarithmic loss](#412)<\/li>\n        <li>[4_13 PCA (Principal Component Analysis)](#413)<\/li>    \n[5. Submission](#5)<br>","2fe2fd3e":"<a id='1'><\/a>\n## 1 Importing packages","5e812508":"<a id='48'><\/a>\n### 4_8 Feature Selection","a9c477ed":"**The goal** is to predict the target variable(Survived) using logistic regression.","36cfc896":"<a id='485'><\/a>\n#### 4_8_5 Feature_Selection - Final","0e2df923":"<a id='483'><\/a>\n#### 4_8_3 Feature_Selection - Mutual information","83709e87":"<a id='410'><\/a>\n### 4.10. Confusion matrix","f5a33b2c":"<a id='46'><\/a>\n### 4_7 Support Vector Machine Model","82c21d86":"<a id='3'><\/a>\n## 3 Data Preprocessing","21b83895":"<a id='42'><\/a>\n### 4_2 Check categorical columns","1161d3b4":"<a id='482'><\/a>\n#### 4_8_2 Feature_Selection - Tree-based","281d5bdd":"<a id='32'><\/a>\n### 3_2 Combine SibSp and Parch for Simplicity","85f83cc5":"<a id='413'><\/a>\n### 4.13. PCA (Principal Component Analysis)","b100449f":"##### 3_1_3_2 Missing values in test data","33a005f6":"<a id='46'><\/a>\n### 4_6 Optimize parameters with Cross Validation and GridSearchCV()","22281928":"<a id='312'><\/a>\n#### 3_1_2 Embarked - Missing Values","eb63f9dd":"<a id='412'><\/a>\n### 4.12. Logarithmic loss","fc071628":"<a id='43'><\/a>\n### 4_3 One-Hot Encoding","f690c367":"<a id='321'><\/a>\n#### 3_1_1 Fare - Missing Values","b72d4679":"Mutual information (MI) measures the dependency between the variables. Higher values mean higher dependency.","d783e2da":"{'Sex_male'} {'Sex_female'} and {'TravelAlone_0'} {'TravelAlone_1'} are highly correlated features in the dataset.","eaadfe4c":"<a id='41'><\/a>\n### 4_1 Separate the dataset","9bd46781":"pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","86ebf2e7":"<a id='49'><\/a>\n### 4.9. Evaluating the Model","99ddf766":"##### 3_1_3_1 Missing values in train data","41c3c776":"The scree plot shows that the first principal component, PC1, accounts for a relatively large amount of variation in the raw data, and this means that it will be a good candidate for the x-axis in the 2-Dimensional graph. However, PC2 is not much different from PC3 or PC4, which does not bode well for dimensional reduction. Now we will draw the PCA graph.","e50cc80a":"<a id='411'><\/a>\n### 4.11. roc curve and auc","0f0d62c8":"#### 3_2_1 Train data","53095ce4":"##### 3_1_2_1 Missing values in train data","302caba0":"<a id='4'><\/a>\n## 4 Regressions and Results","5bf23320":"<a id='484'><\/a>\n#### 4_8_4 Feature_Selection - Evaluation of all column combinations","7e6dc4a1":"<a id='44'><\/a>\n### 4_4 Check zero variance features","2ca7a744":"REFERENCE: **StatQuest with Josh Starmer**<br>\nSupport Vector Machines in Python from Start to Finish.<br>\nhttps:\/\/www.youtube.com\/watch?v=8A7L0GsBiLQ","012733d2":"PLife -> Life is unfortunately not fair. If you're a 3rd class person in this world, it's a chance for you to survive.\n\nSex -> Positive discrimination has been made against women here as well.\n\nAge -> The age distribution for survivors and deceased is very similar except for children. The travelers strived for the survival of the children.\n\nFare -> Passengers who pay lower fare seem less likely to survive.\n\nEmbarked -> Although the number of boarders in Southhampton is higher than those in Cherbourg, the survival rate of Cherbourg is higher. This is probably related to the socioeconomic situation.","1f1bfb0b":"<a id='313'><\/a>\n#### 3_1_3 Age - Missing Values","1f8098f2":"<a id='33'><\/a>\n### 3_3 Move the dependent column to the end","8b917046":"<a id='34'><\/a>\n### 3_4 Investigate all the elements within each Feature "}}