{"cell_type":{"9885df1d":"code","cd43bcbe":"code","6bff4c13":"code","ecb312ab":"code","b63bfeca":"code","98927b17":"code","aec3b896":"code","68cc3395":"code","345e1a4d":"markdown","72d9027f":"markdown","168326d2":"markdown"},"source":{"9885df1d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","cd43bcbe":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\nX = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values\nX = (X - np.min(X))\/(np.max(X)-np.min(X))\nX_train, X_test, y_train, y_test = train_test_split(X,y)","6bff4c13":"def pipeline(X,y,X_test, y_test, alpha, max_iter, bs):\n    \"\"\"\n    Sklearn Sanity Check\n    \"\"\"\n    print(\"-\"*20,'Sklearn',\"-\"*20)\n    sk = LogisticRegression(penalty='none',max_iter=max_iter)\n    sk.fit(X,y)\n    sk_y = sk.predict(X_test)\n    print('Accuracy ',accuracy_score(y_test, sk_y))\n    \n    print(\"-\"*20,'Custom',\"-\"*20)\n    me = LogisticReg(learning_rate=alpha,max_iteration=max_iter,batch_size=bs)\n    me.fit(X,y)\n    yhat = me.predict(X_test)\n    me_score = accuracy_score(y_test, yhat)\n    print('Accuracy ',me_score)\n    me.plot()","ecb312ab":"class LogisticReg:\n    \n    def __init__(self, learning_rate=0.01,max_iteration=100, batch_size=10):\n        self.learning_rate = learning_rate\n        self.max_iter = max_iteration\n        self.batch_size = batch_size\n        \n    def sigmoid(self,x):\n        if isinstance(x, np.ndarray):\n            result = np.zeros((x.shape[0]))\n            for i in range(x.shape[0]):\n                result[i] = np.exp(x[i]) \/ (1+ np.exp(x[i]))\n            return result\n        else:\n            return np.exp(x) \/ (1+ np.exp(x))\n    \n    def sgd(self, X, y):\n        n_samples, n_features = X.shape\n        self.betas = np.zeros(n_features) # column vector, parameters\n        costs = []\n        for it in range(self.max_iter):\n            indices = np.random.randint(0,X.shape[0],size=self.batch_size)\n\n#             ----------------------\n#                 Forward Pass\n#             ----------------------\n            prediction = self.sigmoid(np.dot(X[indices, :],self.betas))\n            \n            error = y[indices] - prediction\n#             ----------------------\n#                 Backward Pass\n#             ----------------------\n            cost = (-1 \/ indices.shape[0]) * (y[indices] @ np.log(prediction) + (1 - y[indices]) @ np.log(1-prediction) )\n            gradient = (1 \/ indices.shape[0]) * (X[indices, :].T @ error)\n        \n            self.betas = self.betas - (self.learning_rate * -gradient)\n            costs.append(cost)\n            \n            if it % (self.max_iter \/ 10)==0:\n                accuracy = accuracy_score(y[indices],np.round(prediction))\n                print(f\"iteration: {it}, Cost: {cost}, Accuracy: {accuracy}\")\n            \n        self.history = costs\n            \n        \n    def plot(self):\n        fig, ax = plt.subplots(1,1,figsize=(20,10),facecolor='white')\n        ax.plot(range(self.max_iter),self.history)\n        plt.show()\n        \n    def fit(self, X, y):\n        \"\"\"\n        Fit logistic model using Stochastic Gradient Descent\n        \"\"\"\n        print(X.shape)\n        X = np.insert(X,0,1,axis=1) # add 1s for matrix multiplication\n        \n        self.sgd(X,y)\n    \n    def predict(self, X):\n        X = np.insert(X,0,1,axis=1)\n        yhat = np.dot(X,self.betas)\n        yhat = self.sigmoid(yhat)\n        return np.round(yhat)\n    \n    def score(self, X,y):\n        yhat = self.predict(X)\n        return accuracy_score(y,yhat)","b63bfeca":"pipeline(X_train, y_train, X_test, y_test, alpha=2,max_iter=1000, bs=250)","98927b17":"class LRGD:\n    \n    def __init__(self, learning_rate=0.01,max_iteration=100, batch_size=10):\n        self.learning_rate = learning_rate\n        self.max_iter = max_iteration\n        self.batch_size = batch_size\n        \n    def sigmoid(self,x):\n        if isinstance(x, np.ndarray):\n            result = np.zeros((x.shape[0]))\n            for i in range(x.shape[0]):\n                result[i] = np.exp(x[i]) \/ (1+ np.exp(x[i]))\n            return result\n        else:\n            return np.exp(x) \/ (1+ np.exp(x))\n    \n    def gd(self, X, y):\n        n_samples, n_features = X.shape\n        self.betas = np.zeros(n_features) # column vector, parameters\n        costs = []\n        for it in range(self.max_iter):\n            \n            indices = np.arange(0,n_samples,1)\n#             indices = np.random.randint(0,X.shape[0],size=self.batch_size)\n\n#             ----------------------\n#                 Forward Pass\n#             ----------------------\n            prediction = self.sigmoid(np.dot(X[indices, :],self.betas))\n            \n            error = y[indices] - prediction\n#             ----------------------\n#                 Backward Pass\n#             ----------------------\n            cost = (-1 \/ indices.shape[0]) * (y[indices] @ np.log(prediction) + (1 - y[indices]) @ np.log(1-prediction) )\n            gradient = (1 \/ indices.shape[0]) * (X[indices, :].T @ error)\n        \n            self.betas = self.betas - (self.learning_rate * -gradient)\n            costs.append(cost)\n            \n            if it % (self.max_iter \/ 10)==0:\n                accuracy = accuracy_score(y[indices],np.round(prediction))\n                print(f\"iteration: {it}, Cost: {cost}, Accuracy: {accuracy}\")\n            \n        self.history = costs\n            \n        \n    def plot(self):\n        fig, ax = plt.subplots(1,1,figsize=(20,10),facecolor='white')\n        ax.plot(range(self.max_iter),self.history)\n        plt.show()\n        \n    def fit(self, X, y):\n        \"\"\"\n        Fit logistic model using Stochastic Gradient Descent\n        \"\"\"\n        X = np.insert(X,0,1,axis=1) # add 1s for matrix multiplication\n        self.gd(X,y)\n    \n    def predict(self, X):\n        X = np.insert(X,0,1,axis=1)\n        yhat = np.dot(X,self.betas)\n        yhat = self.sigmoid(yhat)\n        return np.round(yhat)\n    \n    def score(self, X,y):\n        yhat = self.predict(X)\n        return accuracy_score(y,yhat)","aec3b896":"wgd = LRGD(learning_rate=2, max_iteration=5000, batch_size=50)\nwgd.fit(X_train,y_train)\nprint(wgd.score(X_test, y_test))\nwgd.plot()","68cc3395":"sk = LogisticRegression(max_iter=1000)\nsk.fit(X_train, y_train)\nsk.score(X_test,y_test)","345e1a4d":"**NOTE**: Most of what I'm writing below is from *The Elements of Statistical Learning* by Hastie, Tibshirani, Friedman.\n\nThe only reason logistic regression is called regression because it has partial linear regression similarity. The logistic regression models the posterior probabilities of $K$ classes  via linear transformation on $x$. The same linear transformation used in linear regression. The model equation is, \n\n$$\n\\log \\frac{\\Pr(G=K-1 \\mid X=x)}{Pr(G=K \\mid X=x} = \\beta_{0} + \\beta^{T}X\n$$\n\nThis model is specified in terms of $K-1$ log-odds (sum of all of them will be 1). Logistic regression generally designed for a two-class problem but can be extended for multi-class problem too. Here I've only shown the two-class problem, $K=2$. After a few calculations on the above equation, \n\n$$\nPr(Y=K_1 \\mid X=x) = \\frac{exp \\left(\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n \\right)}{1+exp \\left(\\beta_0 + \\beta_1  X_1 + ... + \\beta_n X_n \\right)}  \\label{eq1}\\tag{1} \\\\\nand, \\\\\n\\Pr(Y=k_2 \\mid X=x) = 1 - \\Pr(Y=K_1 \\mid X=x)\n$$\n\nRight hand side of $( \\ref{eq1} )$ is called sigmoid, $\\sigma$ function. A function, $f(X,\\beta) =\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n$ produces unbounded output as in, values can be any real numbers. $\\sigma$ function transform that to $[0,1]$ so it can be used as probability. The log-likelyhood can be written as, \n\n$$\n\\ell(\\beta) = \\sum_{i=1}^{N} y_i \\ log \\ p(x_i; \\beta) + (1-y_i) \\ log \\ (1-p(x_i;\\beta)) \\label{eq2}\\tag{2}\n$$\n\nHere, $p(x_i;\\beta)$ is the probability of a single sample, $Pr(y=K_1 \\mid X=x)$ given parameters, $\\beta$.  $\\ell(\\beta)$ is also known as cost function. There is no closed-form solution for this function, but we can optimize using optimizer such as Newton-Raphson, Gradient Descent etc. I'll be using Batch Gradient Descent. But to optimize for $\\beta$, we need the gradient of it. \n\n$$\n\\frac{\\partial \\ell(\\beta)}{\\partial \\beta} = \\sum_{i=1}^{N} (y_i - p(x_i; \\beta))x_i \\label{eq3}\\tag{3}\n$$\n\nFinally, update rule is,\n\n$$\n\\beta^{new} = \\beta^{old} - learning\\_rate * (- \\frac{\\partial \\ell(\\beta)}{\\partial \\beta} )\n$$\n\nThese equations were for a single iteration. Implementation is easier and faster matrix notation. Variables that we will need for gradient descent,\n\n|Variable|Dimension|Note|\n|-|-|-|\n|$X$|n_samples, n_features + 1|Input matrix, extra all 1s column will be added in zeroth position; it will be multiplied with intercept|\n|$y$|n_samples|output vector|\n|$\\beta$|n_feature+1|Parameters. $\\beta_0$ is intercept|\n|$learning\\_rate$|float|Step size in each iteration.|\n|$error$|n_samples|cost, aka, log-likelyhood function|\n|$gradient$|n_features + 1|Gradient of the cost function|\n\nand finally, in each iteration,\n\n$$\n------------- \\\\\nForward Propagation \\\\\n------------- \\\\\n\\hat{y} = \\sigma(X \\cdot \\beta) \\\\\nerror =  \\left(\\frac{-1}{n\\_samples}\\right) \\left(y \\cdot \\log(\\hat{y}) + (1-y) \\cdot \\log(1-\\hat{y})\\right) \\\\\n------------- \\\\\nBackward Propagation \\\\\n------------- \\\\\ngradient = \\left(\\frac{1}{n\\_samples}\\right) \\left( X^T \\cdot error \\right) \\\\\n\\beta = \\beta - learning\\_rate * \\left( - gradient \\right)\n$$\n","72d9027f":"# also, Gradient Descent","168326d2":"[![Saint Martin's Beach, Bangladesh](https:\/\/i.postimg.cc\/BZCw6y5z\/Group-1log-reg.png)](https:\/\/postimg.cc\/Z0CLswMP)\nSaint Martin's Island, Bangladesh. Image by Riyadh Razzaq"}}