{"cell_type":{"91c4c9cd":"code","2cada56e":"code","9240e160":"code","618f6ed9":"code","93710931":"code","94f397da":"code","aede9e4b":"code","014c83f8":"code","77a138c0":"code","4eb54f10":"code","579cbb6d":"code","631e30e9":"code","ac766b58":"code","fcbf4a09":"code","8ed115b7":"code","6ebbb9f7":"code","1e0bddd0":"code","17ca7463":"code","99e6998e":"code","15381dbf":"code","ccb990a5":"code","4cae032c":"code","32926746":"code","0ea6e794":"code","86e4d7c1":"code","91cd71a9":"code","03c1ce4c":"code","e61f3307":"code","cb1245fc":"markdown","e11c188d":"markdown","cb96cac3":"markdown","66d54906":"markdown","b6ea904f":"markdown","60fbb6e2":"markdown","d6bc2ecd":"markdown","3272108c":"markdown","de86b3a4":"markdown","19f919e6":"markdown","0760cd4a":"markdown","6b7e48b2":"markdown","62807a9e":"markdown","1f3d496a":"markdown","ffe1ee0d":"markdown","cda4be2a":"markdown"},"source":{"91c4c9cd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2cada56e":"train_raw = pd.read_csv(\"..\/input\/mnist_train.csv\")\ntest_raw = pd.read_csv(\"..\/input\/mnist_test.csv\")","9240e160":"train_raw.head()","618f6ed9":"train_raw.describe()","93710931":"train = train_raw[(train_raw[\"label\"] == 0) | (train_raw[\"label\"] == 1)]\ntest =  train_raw[(train_raw[\"label\"] == 0) | (train_raw[\"label\"] == 1)]\n\ntrain = train.head(1000)\ntest = test.head(500)","94f397da":"train.head()","aede9e4b":"test.head()","014c83f8":"y_train = train[\"label\"]\ny_test = test[\"label\"]\ndel train[\"label\"]\ndel test[\"label\"]\nX_train = train\nX_test = test","77a138c0":"X_train \/= 255\nX_test \/= 255","4eb54f10":"X_train.describe()","579cbb6d":"y_train = y_train.values\ny_test = y_test.values\ny_train = y_train.reshape(y_train.shape[0], 1)\ny_test = y_test.reshape(y_test.shape[0], 1)","631e30e9":"X_train = X_train.T\nX_test = X_test.T\ny_train = y_train.T\ny_test = y_test.T","ac766b58":"m = X_train.shape[1]","fcbf4a09":"def sigmoid(z):\n    output = 1 \/ (1+np.exp(-z))\n    return output\n\ndef relu(z):\n    return np.maximum(z, 0)\n\ndef tanh(z):\n    return (np.exp(z) - np.exp(-z)) \/ (np.exp(z) + np.exp(-z))","8ed115b7":"import matplotlib.pylab as plt\ndef plot_function(function, title=\"sigmoid\"):\n    x = np.arange(-7, 7, 0.01)\n    y = function(x)\n    \n    plt.plot(x, y)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(title)\n    plt.show()\n    \nplot_function(sigmoid, \"sigmoid\")    \nplot_function(tanh, \"tanh\")\nplot_function(relu, \"relu\")\n","6ebbb9f7":"def sigmoid_derivative(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\ndef relu_derivative(z):\n    z[z<=0] = 0\n    z[z>0] = 1\n    return z\n\ndef tanh_derivative(z):\n    return 1 - (np.power(tanh(z), 2))\n\ndef compute_derivative(z, function=\"sigmoid\"):\n    if function == \"sigmoid\":\n        return sigmoid_derivative(z)\n    elif function == \"relu\":\n        return relu_derivative(z)\n    elif function == \"tanh\":\n        return tanh_derivative(z)","1e0bddd0":"plot_function(sigmoid_derivative, \"sigmoid derivative\")\nplot_function(tanh_derivative, \"tanh derivative\")\nplot_function(relu_derivative, \"relu derivative\")","17ca7463":"def initialize_params():\n    W1 = np.random.randn(256, 784) * 0.01\n    b1 = np.zeros((256, 1))\n    W2 = np.random.randn(1, 256) * 0.01\n    b2 = np.zeros((1, 1))\n    return W1, b1, W2, b2","99e6998e":"W1, b1, W2, b2 = initialize_params()\nprint(\"y_train shape\", y_train.shape)\nprint(\"W1 shape\", W1.shape, \"X_train (A1) shape\", X_train.shape,  \"b1 shape\", b1.shape)\nA2 = np.dot(W1, X_train) + b1\nprint(\"W2 shape\", W2.shape, \"Z2 shape\", A2.shape, \"b2 shape\", b2.shape)\nprint(\"----------------------------------------------------------------------\")\nprint(\"We will doo W1 x X_train = Z1 \", W1.shape, \"x\" , X_train.shape, \"=\", W1.shape[0],\",\", X_train.shape[1])\nprint(\"\\tThen we apply a activation function to Z1, getting A1... A1 shape will be the same as Z1, so \", X_train.shape)\nprint(\"Finally we will do W2 x Z1\", W2.shape, \"x\", A2.shape, \"=\", W2.shape[0], \",\", A2.shape[1])\nprint(\"\\tThen we apply the sigmoid function to Z2, and we will get A2, which is our y_hat\")","15381dbf":"def forward_pass(W1, b1, W2, b2, X, m, activation=\"sigmoid\"):\n    Z1 = np.dot(W1, X) + b1\n    if activation == \"sigmoid\":\n        A1 = sigmoid(Z1)\n    elif activation == \"relu\":\n        A1 = relu(Z1)\n    elif activation == \"tanh\":\n        A1 = tanh(Z1)\n        \n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n\n    y_hat = A2\n\n    return y_hat, Z1, A1","ccb990a5":"def calculate_cost(y, y_hat):\n    m = y.shape[1]\n    log_probs = np.dot(y, np.log(y_hat.T)) + np.dot((1-y), np.log(1-y_hat.T))\n    cost = (-1\/m) * np.sum(log_probs)\n    cost = np.squeeze(cost)\n    return cost","4cae032c":"y_hat, Z1, A1 = forward_pass(W1, b1, W2, b2, X_train, m)\ncost = calculate_cost(y_train, y_hat)\nprint(\"Cost\", cost)","32926746":"def backward_pass(X, y, Z1, A1, W2, y_hat, activation=\"sigmoid\"):\n    m = y.shape[1]\n    dZ2 = y_hat - y\n    dW2 = (1\/m) * np.dot(dZ2, A1.T)\n    db2 = (1\/m) * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dZ1 = np.dot(W2.T, dZ2) * compute_derivative(Z1, function=activation)\n    dW1 = (1\/m) * np.dot(dZ1, X.T)\n    db1 = (1\/m) * np.sum(dZ1, axis=1, keepdims=True)\n    \n    return dW1, db1, dW2, db2","0ea6e794":"def model(X, y, iterations=100, activation=\"sigmoid\", lr=0.001):\n    W1, b1, W2, b2 = initialize_params()\n    costs = []\n    for epoch in range(0, iterations+1):\n        y_hat, Z1, A1 = forward_pass(W1, b1, W2, b2, X, m, activation=activation)\n        \n        \n        \n        dW1, db1, dW2, db2 = backward_pass(X, y, Z1, A1, W2, y_hat, activation=activation)\n        \n        W1 = W1 - lr * dW1\n        b1 = b1 - lr * db1\n        W2 = W2 - lr * dW2\n        b2 = b2 - lr * db2\n        \n        if epoch % 500 == 0:\n            current_cost = calculate_cost(y, y_hat)\n            costs.append(current_cost)\n            print(\"Cost at epoch\", epoch, \"is\", current_cost)\n    return costs","86e4d7c1":"sigmoid_costs = model(X_test, y_test, iterations = 6000, activation=\"sigmoid\")","91cd71a9":"tanh_costs = model(X_test, y_test, iterations = 6000, activation=\"tanh\")","03c1ce4c":"relu_costs = model(X_test, y_test, iterations = 6000, activation=\"relu\")","e61f3307":"x = np.arange(0, 6001, 500)\nplt.figure(figsize=(12, 7))\nplt.plot(x, sigmoid_costs)\nplt.plot(x, tanh_costs)\nplt.plot(x, relu_costs)\nplt.legend([ \"Sigmoid cost\", \"Tanh cost\", \"ReLU cost\"])\nplt.show()","cb1245fc":"## Now we want to ensure that our y values have shape (number_of_examples, 1) instead of (number_of_examples, )","e11c188d":"## A quick exploration of the dataset","cb96cac3":"## We need to be able to calculate our cost over time, this will be done using the following formula\n# $$ \\frac{-1}{m}  \\sum\\limits_{i = 0}^{m} [ y_i * log(\\hat{y_i}) + (1-y_i) * log(1-\\hat{y}_i)]$$","66d54906":"# Neural network and activation functions. A showcase of the differences between tanh, sigmoid and ReLU as activation functions.\n\n### In this notebook I am going to code a simple neural network entirely in numpy, note that normally you will never do this (the same way you will never code your own hash table), but It is an interesting way to understand what happens in neural networks.\n\n### It is worth noticing that both the training examples and the labels will be organized by COLUMNS, not rows, this means that each sample will be in one column, not in one row, the only reason to do so is to make calculations easier.\n\n### Also this is based in the notations of the course deeplearning.ai, by Andrew Ng, which I highly recommend.","b6ea904f":"## Remember, our columns will be our training examples, so we need to do the transpose of each matrix here","60fbb6e2":"## For the purposes of this notebook, I am only going to write a network which can identify the digits 0 and 1, so I will drop every other example, this is done just to simplify calculations.\n\n## Also, I will NOT be using mini batches, instead I will feed all the training examples in each epoch, so I will only keep 1000 training examples and 500 test samples. No validation data will be used (again, this is just to showcase the activation functions).","d6bc2ecd":"## We have to normalize our data, in the case of the MNIST dataset, this simply means dividing by 255, a more formal explanations is that \n\n## $$X = \\frac{X - min(x)}{max(x) - min(x)}$$","3272108c":"## Now, we just need to implement the forward pass, which is pretty much a prediction. Notice the ```activation``` argument, so later we can control it.","de86b3a4":"## We will use $m$ to denote our number of training examples","19f919e6":"## Have a closer look at the implications of this. Remember that in gradient descent, we will update our weights and biases as\n\n## $$ W_l = W_l -  \\alpha * \\frac{\\partial cost}{\\partial W_l} $$\n## $$ b_l = b_l -  \\alpha * \\frac{\\partial cost}{\\partial b_l} $$\n\n## Where $ l $ represents the layer number we want to calculate...  this essentially means that a small \n## $$ \\frac{\\partial cost}{\\partial b_l} $$\n## or a small \n## $$ \\frac{\\partial cost}{\\partial W_l}  $$ \n## means a slow learning, conversely, a large value of any of those derivatives will mean a quick learning... at this point you should already have an intuition about why the sigmoid function is not great for learning.\n\n","0760cd4a":"## Now we will provide another function to compute the derivative, and here is the whole deal. \n\n## The ReLU function will have normally quite a large derivative, because it is defined as 0 if x <= 0 and 1 otherwise... however in the case of the sigmoid function, the derivative is\n# $$ \\frac{e^z - e^{-z}}{e^z + e^{-z}} * (1 - \\frac{e^z - e^{-z}}{e^z + e^{-z}}) $$\n## Or in simpler terms\n# $$ sigmoid(x) * (1-sigmoid(x)) $$\n## Now, the problem with this is that the derivative of the sigmoid will never, ever be larger than 0.25... and this is going to make things... slow, because ultimately we will use the derivative to uptate our weights and biases, so if the derivative is small, the update will be small, and the learning will be slow.\n\n## Finally the tanh function derivative is \n# $$ 1 - (\\frac{e^z - e^{-z}}{e^z + e^{-z}})^ 2 $$\n## Or in simpler terms\n# $$ 1 - tanh^2(x) $$\n","6b7e48b2":"## Time to implement the backward pass, notice the ```activation``` function, of course this assumes that the ```activation``` argument will be the same one that was passed in the ```forward_pass``` function","62807a9e":"## And here is where the fun begins, we are going to define our three functions. \n\n## The sigmoid function is defined as  \n # $$sigmoid(z) = \\frac{1}{1 + e ^ {-z}}$$\n## The relu function is much simpler, as it is the maximum between the input and zero\n## The tanh function is defined as  \n#  $$tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$","1f3d496a":"## Finally, we plot the evolution of our cost with the different activation functions. Of course we want our cost to go down quickly and in an stable manner. ","ffe1ee0d":"## Now, it would be VERY interesting to plot the derivative function of each of our activation functions. Ultimately we are interested in seeing how the derivatives change their values.\n\n## Pay close attention to the y axis... you will notice that the sigmoid never actually goes over 0.25 while tanh reaches 1.0 in certain areas... ReLU on the other end, has a larger derivative","cda4be2a":"## It would be good to display our functions... lets plot them"}}