{"cell_type":{"f66efc23":"code","d607f3e0":"code","20fce62b":"code","d03670be":"code","24eda54d":"code","960e4d4b":"code","d6e2ee4a":"code","91be621c":"code","27383795":"code","fee10adc":"code","d0c2ac5a":"markdown","a0badbae":"markdown","df1f7b83":"markdown","8d6a5509":"markdown","8f54e8a4":"markdown","d1832122":"markdown","155f05db":"markdown","6784c66e":"markdown","c20b7f9e":"markdown","cf3855ff":"markdown"},"source":{"f66efc23":"# Global Variables \nK = 2 # number of components\nquery = 'nice good price'","d607f3e0":"import pandas as pd\nimport numpy as np\n\n# Data filename\ndataset_filename = \"..\/input\/Womens Clothing E-Commerce Reviews.csv\"\n\n# Loading dataset\ndata = pd.read_csv(dataset_filename, index_col=0)\n\n# We are reducing the size of our dataset to decrease the running time of code\ndatax = data.loc[data['Clothing ID'] == 1078 , :]\n\n\n# Delete missing observations for variables that we will be working with\nfor x in [\"Recommended IND\",\"Review Text\"]:\n    datax = datax[datax[x].notnull()]\n\n# Keeping only those features that we will explore\ndatax = datax[[\"Recommended IND\",\"Review Text\"]]\n\n# Resetting the index\ndatax.index = pd.Series(list(range(datax.shape[0])))\n    \nprint('Shape : ',datax.shape)\ndatax.head()","20fce62b":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nwordnet_lemmatizer = WordNetLemmatizer()\ntokenizer = RegexpTokenizer(r'[a-z]+')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess(document):\n    document = document.lower() # Convert to lowercase\n    words = tokenizer.tokenize(document) # Tokenize\n    words = [w for w in words if not w in stop_words] # Removing stopwords\n    # Lemmatizing\n    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n    return \" \".join(words)","d03670be":"datax['Processed Review'] = datax['Review Text'].apply(preprocess)\n\ndatax.head()","24eda54d":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nTF_IDF_matrix = vectorizer.fit_transform(datax['Processed Review'])\nTF_IDF_matrix = TF_IDF_matrix.T\n\nprint('Vocabulary Size : ', len(vectorizer.get_feature_names()))\nprint('Shape of Matrix : ', TF_IDF_matrix.shape)","960e4d4b":"import numpy as np\n\n# Applying SVD\nU, s, VT = np.linalg.svd(TF_IDF_matrix.toarray()) # .T is used to take transpose and .toarray() is used to convert sparse matrix to normal matrix\n\nTF_IDF_matrix_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n\n# Getting document and term representation\nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\ndocs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix ","d6e2ee4a":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=datax['Recommended IND'])\nplt.title(\"Document Representation\")\nplt.show()","91be621c":"plt.scatter(terms_rep[:,0], terms_rep[:,1])\nplt.title(\"Term Representation\")\nplt.show()","27383795":"# This is a function to generate query_rep\n\ndef lsa_query_rep(query):\n    query_rep = [vectorizer.vocabulary_[x] for x in preprocess(query).split()]\n    query_rep = np.mean(terms_rep[query_rep],axis=0)\n    return query_rep","fee10adc":"from scipy.spatial.distance import cosine\n\nquery_rep = lsa_query_rep(query)\n\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nprint_count = 0\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index],' Review : ', datax['Review Text'][sort_index])\n    if print_count == 4 :\n        break\n    else:\n        print_count += 1","d0c2ac5a":"## 3 Information Retreival Using LSA <a id=\"ir_lsa\"><\/a>","a0badbae":"### 1.1 Importing Data and Separating Data of Our Interest <a id=\"1.1\"><\/a>","df1f7b83":"### 2.1 Create Term and Document Representation  <a id=\"2.1\"><\/a>","8d6a5509":"##  1. Data Preprocessing <a id=\"data_preprocessing\"><\/a>","8f54e8a4":"## 2. Apply SVD to TF-IDF Matrix <a id=\"apply_svd\"><\/a>","d1832122":"## Table of Content <a id=\"toc\"><\/a>\n* [Global Variables](#gv)\n* [1. Data Preprocessing](#data_preprocessing)\n    * [1.1 Importing Data and Separating Data of Our Interest](#1.1)\n    * [1.2 Creating Preprocessing Function and Applying it on Our Data](#1.2)\n    * [1.3 Creating TF-IDF Matrix](#1.3)\n* [2. Apply SVD to TF-IDF Matrix](#apply_svd)\n    * [2.1 Create Term and Document Representation](#2.1)\n    * [2.2 Visulize Those Representation](#2.2)\n* [3 Information Retreival Using LSA](#ir_lsa)\n* [4 References](#references)","155f05db":"##  4. References <a id=\"references\"><\/a>\n* [Latent Semantic Analysis (Tutorial)](https:\/\/www.engr.uvic.ca\/~seng474\/svd.pdf)","6784c66e":"### 1.3 Creating TF-IDF Matrix <a id=\"1.3\"><\/a>","c20b7f9e":"### 1.2 Creating Preprocessing Function and Applying it on Our Data <a id=\"1.2\"><\/a>","cf3855ff":"### 2.2 Visulize Those Representation <a id=\"2.2\"><\/a>"}}