{"cell_type":{"dc5abf5f":"code","d3c7fced":"code","e6597618":"code","03428ea2":"code","e6c2b18b":"code","3bf57962":"code","b6b6e288":"code","26af34a5":"code","41988788":"code","31c8998a":"code","ffe211eb":"code","cb4dbdbd":"code","806ddda3":"code","17eecd5e":"code","9e012c19":"code","6ed064a3":"code","991e79cb":"code","8f20865c":"code","8811a3b5":"code","a3ec2a20":"code","6b0400d8":"code","f0573889":"code","3f85a325":"code","80565fe4":"code","3ac1751e":"code","8ade2007":"code","51a63f72":"code","d448caca":"code","bfc109f6":"code","f499c1de":"code","bb52d1de":"code","293699b7":"code","785d212f":"code","7580b2c0":"code","94d7cbba":"code","d5f2307f":"code","80ae50fd":"code","743f93f8":"code","6e03d735":"code","505c5bc3":"code","025a0ea9":"code","ba20cbeb":"code","31e0bd8c":"code","48a1834f":"code","fb4ff3b2":"code","84ba96b6":"markdown","dc723ebc":"markdown","cbc5ad0b":"markdown","d0f563b2":"markdown","ed45c450":"markdown","6efa34ab":"markdown","ae752671":"markdown","0ac5e210":"markdown","61e86f57":"markdown","04859911":"markdown","c96db0be":"markdown","28674ed1":"markdown","25f63665":"markdown","280990a3":"markdown","ed84cece":"markdown","174843a4":"markdown","40a1e01d":"markdown","bb7c69d6":"markdown","96ee4d2e":"markdown","a16f67b7":"markdown","624eca75":"markdown","81ff7a85":"markdown","f533f9fa":"markdown"},"source":{"dc5abf5f":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d3c7fced":"train_df = pd.read_csv(\"\/kaggle\/input\/Train.csv\")","e6597618":"train_df.columns","03428ea2":"train_df.head()","e6c2b18b":"train_df.describe()","3bf57962":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,10))\ncorr = train_df.corr()\nsns.heatmap(corr)","b6b6e288":"X = train_df.drop(\n    columns={'Employee_ID', 'Gender', 'Age',\n        'Hometown', 'Unit', 'Relationship_Status',\n         'Travel_Rate','Decision_skill_possess','Time_since_promotion',\n        'Post_Level', 'Compensation_and_Benefits','Education_Level',\n         'Attrition_rate','VAR1','VAR2','VAR3','VAR4','VAR5','VAR6','VAR7'})\ny = train_df['Attrition_rate']","26af34a5":"'''from sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\nX['Gender'] = labelencoder.fit_transform(X['Gender'])\nX['Relationship_Status'] = labelencoder.fit_transform(X['Relationship_Status'])\nX['Hometown'] = labelencoder.fit_transform(X['Hometown'])\nX['Unit'] = labelencoder.fit_transform(X['Unit'])\nX['Decision_skill_possess'] = labelencoder.fit_transform(X['Decision_skill_possess'])\nX['Compensation_and_Benefits'] = labelencoder.fit_transform(X['Compensation_and_Benefits'])'''","41988788":"X","31c8998a":"k=0\ncol = ['Time_of_service', 'growth_rate', 'Pay_Scale', 'Work_Life_balance']\nfig,ax = plt.subplots(2,2,figsize=(20,20))\nfor i in np.arange(2):\n    for j in np.arange(2):\n        chart = sns.countplot(x = X[col[k]],ax = ax[i][j],order = X[col[k]].value_counts().index)\n        chart.set_xticklabels(rotation=90,labels = chart.get_xticklabels())\n        k+=1\nplt.show()","ffe211eb":"X['Pay_Scale'].fillna((X['Pay_Scale'].median()), inplace=True)\nX['Time_of_service'].fillna((X['Time_of_service'].median()), inplace=True)\nX['Work_Life_balance'].fillna((X['Work_Life_balance'].median()), inplace=True)","cb4dbdbd":"'''X_train['Time_of_service'] = X_train['Time_of_service'].fillna(value=0)\nX_train['Work_Life_balance'] = X_train['Work_Life_balance'].fillna(value=0)\nX_train['Pay_Scale'] = X_train['Pay_Scale'].fillna(value=0)'''","806ddda3":"X['Time_of_service'].value_counts()","17eecd5e":"'''X['Time_of_service']=X['Time_of_service'].fillna(value=6.0)\nX['Work_Life_balance']=X['Work_Life_balance'].fillna(value=1.0)\nX['Pay_Scale']=X['Pay_Scale'].fillna(value=8.0)'''","9e012c19":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","6ed064a3":"X_train.info(),\nX_test.info()","991e79cb":"X_train.columns","8f20865c":"X_test.columns","8811a3b5":"X_train['Time_of_service'] = X_train['Time_of_service'].astype(np.int64)\nX_train['Pay_Scale'] = X_train['Pay_Scale'].astype(np.int64)\nX_train['Work_Life_balance'] = X_train['Work_Life_balance'].astype(np.int64)\n\nX_test['Time_of_service'] = X_test['Time_of_service'].astype(np.int64)\nX_test['Pay_Scale'] = X_test['Pay_Scale'].astype(np.int64)\nX_test['Work_Life_balance'] = X_test['Work_Life_balance'].astype(np.int64)","a3ec2a20":"X_train.info(),\nX_test.info()","6b0400d8":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\ny_test = y_test.ravel()\ny_train = y_train.ravel()","f0573889":"#ridge\nfrom sklearn.linear_model import Ridge\nreg = Ridge(alpha=1.0)\nreg.fit(X_train,y_train)\n\n# Predicting a new result\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\npred1 = reg.predict(X_test)\nscore1 = 100 * max(0,1 - sqrt(mean_squared_error(y_test,pred1)))\nprint('ridge regression:',score1)","3f85a325":"# random forest pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nmodel = RandomForestRegressor(n_jobs=-1, criterion=\"mse\", max_features='auto', min_samples_split=5, max_depth=3,\n                              random_state=0, min_samples_leaf=1090, bootstrap=True, verbose=0)\nestimators = 110\nmodel.set_params(n_estimators=estimators)\n\npipeline = Pipeline([('scaler2', StandardScaler()),\n                     ('RandomForestRegressor: ', model)])\npipeline.fit(X_train, y_train)\npred2 = pipeline.predict(X_test)\nscore2 = 100 * max(0,1 - sqrt(mean_squared_error(y_test, pred2)))\nprint('random forest:', score2)\n","80565fe4":"#BR\nfrom sklearn.linear_model import BayesianRidge\nreg2 = BayesianRidge(compute_score=True)\nreg2.fit(X_train,y_train)\n\n# Predicting a new result\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\npred3 = reg2.predict(X_test)\nscore3 = 100 * max(0,1 - sqrt(mean_squared_error(y_test,pred3)))\nprint('bayesian regression:',score3)\n\n","3ac1751e":"#fitting the decision tree regression to the dataset\nfrom sklearn.tree import DecisionTreeRegressor\nreg4 = DecisionTreeRegressor(random_state=0)\nreg4.fit(X_train,y_train)\n\n# Predicting a new result\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\npred4 = reg4.predict(X_test)\nscore4 = 100 * max(0,1 - sqrt(mean_squared_error(y_test,pred4)))\nprint('decision tree regression',score4)","8ade2007":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n# Initialising the ANN\nmodel = Sequential()\nmodel.add(Dense(output_dim=10, kernel_initializer='normal', activation='relu', input_dim=4))\nmodel.add(Dense(output_dim=10, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(output_dim=1, kernel_initializer='normal',activation='linear'))\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['mse','mae'])\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train, batch_size=5, nb_epoch=10)\npred5 = model.predict(X_test)\nscore5 = 100 * max(0, 1 - sqrt(mean_squared_error(y_test, pred5)))\nprint('KNN:', score5)","51a63f72":"#GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nreg5 = GradientBoostingRegressor(random_state=0)\nreg5.fit(X_train, y_train)\n\n# Predicting a new result\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\npred6 = reg5.predict(X_test)\nscore6 = 100 * max(0,1 - sqrt(mean_squared_error(y_test,pred6)))\nprint('GradientBoostingRegressor:',score6)\n","d448caca":"#fitting KNN regression to the training set\nfrom sklearn.neighbors import KNeighborsRegressor\nreg6 = KNeighborsRegressor(n_neighbors=5)\nreg6.fit(X_train,y_train)\n\n# Predicting a new result\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\npred7 = reg6.predict(X_test)\nscore7 = 100 * max(0,1 - sqrt(mean_squared_error(y_test,pred7)))\nprint('KNN regression',score7)","bfc109f6":"# lasso regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import linear_model\nclf2 = linear_model.Lasso(alpha=0.1, normalize=True,max_iter=1e5)\nfrom sklearn.pipeline import Pipeline\npipe2 = Pipeline([\n        ('scaler', StandardScaler()),\n        ('regressor', linear_model.Lasso())\n        ])\npipe2.fit(X_train, y_train)\npred8 = pipe2.predict(X_test)\n# Predicting a new result\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nscore8 = 100 * max(0, 1 - sqrt(mean_squared_error(y_test, pred8)))\nprint('lasso regression:', score8)","f499c1de":"#SVR\n#fitting svr to dataset\nfrom sklearn.svm import SVR\nreg7 = SVR(kernel='rbf')\nreg7.fit(X_train,y_train)\n\n# Predicting a new result\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\npred9 = reg7.predict(X_test)\nscore9 = 100 * max(0,1 - sqrt(mean_squared_error(y_test,pred9)))\nprint('SVR',score9)","bb52d1de":"print('ridge regression:',score1)\nprint('random forest:', score2)\nprint('bayesian regression:',score3)\nprint('decision tree regression',score4)\nprint('KNN:', score5)\nprint('GradientBoostingRegressor:',score6)\nprint('KNN regression',score7)\nprint('lasso regression:', score8)\nprint('SVR',score9)","293699b7":"# testing on actual data\ntest_df = pd.read_csv(\"\/kaggle\/input\/Test.csv\")\n\nEID = test_df['Employee_ID']","785d212f":"test_df = test_df.drop(\n    columns={'Employee_ID', 'Gender', 'Age',\n        'Hometown', 'Unit', 'Relationship_Status',\n         'Travel_Rate','Decision_skill_possess','Time_since_promotion',\n        'Post_Level', 'Compensation_and_Benefits','Education_Level',\n         'VAR1','VAR2','VAR3','VAR4','VAR5','VAR6','VAR7'})","7580b2c0":"test_df.info()","94d7cbba":"test_df.columns","d5f2307f":"test_df['Time_of_service'].fillna((test_df['Time_of_service'].median()), inplace=True)\ntest_df['Work_Life_balance'].fillna((test_df['Work_Life_balance'].median()), inplace=True)\ntest_df['Pay_Scale'].fillna((test_df['Pay_Scale'].median()), inplace=True)","80ae50fd":"# converting float variables columns in int64 training set\ntest_df['Time_of_service'] = test_df['Time_of_service'].astype(np.int64)\ntest_df['Work_Life_balance'] = test_df['Work_Life_balance'].astype(np.int64)\ntest_df['Pay_Scale'] = test_df['Pay_Scale'].astype(np.int64)","743f93f8":"test_df.info()","6e03d735":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntest_df = sc.fit_transform(test_df)","505c5bc3":"main_pred = reg.predict(test_df)\nmain_submission = pd.DataFrame({'Employee_ID': EID, 'Attrition_rate': main_pred})\n\nmain_submission.to_csv(\"submission.csv\", index=False)\nmain_submission.head()","025a0ea9":"w = train_df.drop(columns={'Employee_ID','Attrition_rate'})\n\nz = train_df['Attrition_rate']","ba20cbeb":"k=0\ncol = ['Hometown','Unit','Decision_skill_possess','Compensation_and_Benefits','Gender','Relationship_Status',\n      'Education_Level','Travel_Rate','Post_Level','Work_Life_balance']\nfig,ax = plt.subplots(5,2,figsize=(20,20))\nfor i in np.arange(5):\n    for j in np.arange(2):\n        chart = sns.countplot(x = w[col[k]],ax = ax[i][j],order = w[col[k]].value_counts().index)\n        chart.set_xticklabels(rotation=90,labels = chart.get_xticklabels())\n        k+=1\nplt.show()","31e0bd8c":"from sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\nw['Gender'] = labelencoder.fit_transform(w['Gender'])\nw['Relationship_Status'] = labelencoder.fit_transform(w['Relationship_Status'])\nw['Hometown'] = labelencoder.fit_transform(w['Hometown'])\nw['Unit'] = labelencoder.fit_transform(w['Unit'])\nw['Decision_skill_possess'] = labelencoder.fit_transform(w['Decision_skill_possess'])\nw['Compensation_and_Benefits'] = labelencoder.fit_transform(w['Compensation_and_Benefits'])\n\nw['Age'].fillna((w['Age'].median()), inplace=True)\nw['Time_of_service'].fillna((w['Time_of_service'].median()), inplace=True)\nw['Work_Life_balance'].fillna((w['Work_Life_balance'].median()), inplace=True)\nw['VAR2'].fillna((w['VAR2'].median()), inplace=True)\nw['VAR4'].fillna((w['VAR4'].median()), inplace=True)\nw['Pay_Scale'].fillna((w['Pay_Scale'].median()), inplace=True)\n","48a1834f":"w.corr()","fb4ff3b2":"#this code is use to check correlatoin between two features\nprint(w[[\"Age\",\"Time_of_service\"]].corr())\nprint(w[[\"Age\",\"Decision_skill_possess\"]].corr())\nprint(w[[\"Age\",\"Time_since_promotion\"]].corr())","84ba96b6":"as you can see on the above output the age , time_of_service are more correlated with each other and also time_since_pramotion,pay_scale somewhat correlated with age.","dc723ebc":"# 4)decision tree","cbc5ad0b":"the below code is used for converting the categorical variable into numeric one\nbut here i am only taking some columns which is alredy numeric so i am not applying labelencoder for this","d0f563b2":"# 7)KNN regressor","ed45c450":"# **NOW APPLYING ALL THE REGRESSION METHODS IN WHICH THE RIDGE REGRESSION IS THE BEST**","6efa34ab":"# 3)bayesian ridge","ae752671":"applying feature scaling and ravel method to convert series into ndarray","0ac5e210":"# 8)Lasso regression","61e86f57":"# 6)gradientboostingregressor","04859911":"as you see in above output the value 6 is encountered more times compare to other so i can add this value to fill missing values","c96db0be":"here i am adding all the methods with i used","28674ed1":"# **please do upvote if you like**","25f63665":"# 5)ANN","280990a3":"# 1)ridge regression","ed84cece":"i have used almost all regression techniques such as ANN,SVR,RF,Xgboost,ridgeregression,lasso,Decisiontree,gradientboosting etc","174843a4":"i am not using labelencoder but only for knowledge i am adding this code.","40a1e01d":"# 2)random forest","bb7c69d6":"Today the competition on the HackerEarth has finished at 7:30 A.M and this is my first competition. I am at the 109th position with 81.289 accuracy. I have gained moreknowledge on many things such as feature engineering, model selection, feature selection methods, and thinking from every possible angle to make the model better and better. I have also applied all the regression techniques in which random forest, xgboost, and ridge regression is best fit for this dataset. I gained this accuracy using ridge regression.\n\nIT WAS REALLY GOOD EXPERIENCE","96ee4d2e":"below code is used to fill missing values by 0.","a16f67b7":"below code is use to fill missing values you can also use 0 , mean() , mode() to fill missing values\nbut here i am applying median because this is good for my dataset","624eca75":"# 9)SVR","81ff7a85":"# **EXTRA WORK**","f533f9fa":"below code is use to put values which are max in particular column for example in age column 27 years encountered more times compare to other years"}}