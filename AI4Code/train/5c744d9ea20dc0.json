{"cell_type":{"9b7ab661":"code","efe5359c":"code","fadee338":"code","0356700c":"code","f6a483f2":"code","5b1d1f22":"code","90c25026":"code","58e60fd1":"code","0b8a0002":"code","113df2f9":"code","c749b208":"code","e61f800e":"code","e8859170":"code","5852ecc1":"code","8b7a3a79":"code","b3325639":"code","3a21a282":"code","1155cfd9":"code","a088c71b":"code","15ecea92":"code","c66644c5":"code","c6b2554b":"code","b4033bd5":"code","143b3a4b":"code","66a78361":"code","cdc1eb6c":"code","7997003a":"code","7fa44ee0":"code","87de7387":"code","2a67d976":"code","584ad74d":"code","903d0336":"code","e59a5f18":"code","9082e3c0":"code","ad5aed68":"code","7f8f05ef":"code","e7604b06":"code","a835b358":"markdown","86f8b9e0":"markdown","3f2053d6":"markdown","d6eb479c":"markdown","7c194cc2":"markdown","48a96695":"markdown"},"source":{"9b7ab661":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","efe5359c":"df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","fadee338":"df.head()","0356700c":"df.info()","f6a483f2":"df.describe()","5b1d1f22":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","90c25026":"sns.pairplot(df,hue=\"quality\");","58e60fd1":"plt.subplots(figsize =(10,10))\nsns.heatmap(df.corr(),annot=True,linewidths=0.2,cmap='plasma');","0b8a0002":"df.quality.value_counts()\nprint(df.quality.value_counts().plot(kind='bar'))","113df2f9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler ","c749b208":"X = df.drop(['quality'],axis=1)\ny = df.quality\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=38,stratify=y)","e61f800e":"numeric_features = df.describe().columns[:-1]","e8859170":"# Define preprocessing for numeric columns (normalize them so they're on the same scale)\n \nnumeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n        \n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('randomforest', RandomForestClassifier(max_depth=15,n_estimators=400))])\n\n\n# fit the pipeline to train a random forest classifier model on the training set\nrfc = pipeline.fit(X_train, (y_train))\n\npredictions = rfc.predict(X_test)","5852ecc1":"confusion_matrix(y_test,predictions)\npd.crosstab(y_test, predictions, rownames = ['Actual'], colnames =['Predicted'])","8b7a3a79":"print(classification_report(y_test,predictions))","b3325639":"df2 = df.copy()","3a21a282":"# create 3 bins for mediocre,medium and good wine\n\nbins = [2,5,6,np.inf]\nlabels = [\"mediocre\", \"medium\", \"good\"]\ndf2['quality'] = pd.cut(df2['quality'],bins=bins, labels=labels)","1155cfd9":"df2.quality.value_counts()","a088c71b":"# label encode quality\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ndf2['quality'] = label.fit_transform(df2['quality'])","15ecea92":"label.classes_","c66644c5":"X = df2.drop(['quality'],axis=1)\ny = df2.quality","c6b2554b":"from itertools import cycle\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_auc_score\n\n# Binarize the output\ny = label_binarize(y, classes=[0,1,2])\nn_classes = y.shape[1]\n\n# shuffle and split training and test sets\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=38,stratify=y)\n\n\n# Define preprocessing for numeric columns (normalize them so they're on the same scale)\n \nnumeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n        \n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('randomforest', RandomForestClassifier())])\n\n\n# fit the pipeline to train a random forest classifier model on the training set\n\nrfc = pipeline.fit(X_train, (y_train))\n\npredictions =rfc.predict(X_test)\n\nprint(classification_report(y_test,predictions))\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], predictions[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), predictions.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","b4033bd5":"# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.subplots(figsize=(15,10))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue','green','blue','red'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve for  {0} wine quality (area = {1:0.2f})'\n             ''.format(label.classes_[i], roc_auc[i]))\n    \nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","143b3a4b":"df3 = df.copy()","66a78361":"# create 2 bins for good and bad wine\n\nbins = [2, 6.5, 8]\nlabels = [\"bad\", \"good\"]\ndf3['quality'] = pd.cut(df3['quality'],bins=bins, labels=labels)","cdc1eb6c":"df3['quality'].value_counts()","7997003a":"# label encode quality\nlabel = LabelEncoder()\ndf3['quality'] = label.fit_transform(df3['quality'])","7fa44ee0":"label.classes_","87de7387":"X = df3.drop(['quality'],axis=1)\ny = df3.quality","2a67d976":"#shuffle and split training and test sets\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=38,stratify=y)\n\n\n# Define preprocessing for numeric columns (normalize them so they're on the same scale)\n \nnumeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n        \n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('randomforest', RandomForestClassifier(max_depth=15))])\n\n\n# fit the pipeline to train a random forest classifier model on the training set\n\nrfc = pipeline.fit(X_train, (y_train))\n\npredictions =rfc.predict(X_test)","584ad74d":"confusion_matrix(y_test,predictions)\npd.crosstab(y_test, predictions, rownames = ['Actual'], colnames =['Predicted'])","903d0336":"print(classification_report(y_test,predictions))","e59a5f18":"roc_auc_score(y_test, predictions)","9082e3c0":"def draw_roc_curve(y_pred_prob):\n    fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred_prob)\n    plt.subplots(figsize=(15,10))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.plot(fpr, tpr, label='Random Forest Classifier')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Random Forest Classifier ROC Curve')\n    plt.show();","ad5aed68":"y_pred_prob = rfc.predict_proba(X_test)[:,1]\ndraw_roc_curve(y_pred_prob)","7f8f05ef":"# set threshold to 0.35\n\nthreshold = 0.35\n\nfrom sklearn.preprocessing import binarize\n\ny_pred_prob = y_pred_prob.reshape(1,-1)\n\ny_pred_class = binarize(y_pred_prob,threshold=threshold)[0]\n\ny_pred_class = y_pred_class.astype('int')\n\nprint(confusion_matrix(y_test,y_pred_class))\n\nprint(classification_report(y_test,y_pred_class))\n\nprint(f'roc_auc_score : {roc_auc_score(y_test, y_pred_class)}')\n\ndraw_roc_curve(y_pred_class)","e7604b06":"# set threshold to 0.15\n\nthreshold = 0.15\n\ny_pred_prob = y_pred_prob.reshape(1,-1)\n\ny_pred_class = binarize(y_pred_prob,threshold=threshold)[0]\n\ny_pred_class = y_pred_class.astype('int')\n\nprint(confusion_matrix(y_test,y_pred_class))\n\nprint(classification_report(y_test,y_pred_class))\n\nprint(f'roc_auc_score : {roc_auc_score(y_test, y_pred_class)}')\n\ndraw_roc_curve(y_pred_class)","a835b358":"### Comment:\n\n* with 2 bins and lower threshold (i.e 0.15) , we have a good roc_auc_score : 0.85","86f8b9e0":"### Using Random Forest with the Red Wine datase.Model evaluation with ROC curve.","3f2053d6":"### create 2 bins","d6eb479c":"ref: https:\/\/www.wine-searcher.com\/wine-scores\n\nScore\tExplanation\n\n95\u2013100\tClassic: a great wine\n\n90\u201394\tOutstanding: a wine of superior character and style\n\n85\u201389\tVery good: a wine with special qualities 9-10\n\n80\u201384\tGood: a solid, well-made wine 6-8\n\n75\u201379\tMediocre: a drinkable wine that may have minor flaws 3-5\n\n50\u201374\tNot recommended  0-2\n\nUsing the guide above, here's the classification we will use for this dataset\n* Bad - 0-2\n* Mediocre - 3-5\n* Good - 6\n* Very Good - 7-8\n* Outstanding - 9-10","7c194cc2":"## Comment: \n* unable to predict quality 3,4,8 due to high imbalance of the data and less data for training.","48a96695":"### create 3 bins"}}