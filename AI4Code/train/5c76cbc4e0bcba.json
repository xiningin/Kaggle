{"cell_type":{"2e05cad1":"code","39fb3cd1":"code","25e56ed6":"code","c740f984":"code","e3be690d":"code","cb4af3d2":"code","8911ed2e":"code","e79f084c":"code","a5619059":"code","d32c54c0":"code","982d7571":"code","06f4f756":"code","88e4005f":"code","554f845f":"code","606a74af":"code","784b77da":"code","76bf0cf8":"code","c10b0ed8":"code","161ddcca":"code","60ab8470":"code","35d1cba7":"code","0cfa791a":"markdown","20214469":"markdown","2d16d64d":"markdown","3f29a09e":"markdown","5cdb3731":"markdown","67c34fe5":"markdown","f5d7a716":"markdown","4fac365c":"markdown","a91945dd":"markdown","05412ce3":"markdown","481e9080":"markdown","f9fc70a2":"markdown","a4f71e97":"markdown","d9245438":"markdown","04059af5":"markdown","5d651c4a":"markdown"},"source":{"2e05cad1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","39fb3cd1":"#Importing Some Packages\nimport string\nfrom string import punctuation\nimport nltk\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import preprocessing\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem import PorterStemmer , SnowballStemmer\nstemmer_s = SnowballStemmer('english')\nfrom string import punctuation\nstop_nltk=stopwords.words(\"english\")\nfrom nltk.tokenize import TweetTokenizer\ntw=TweetTokenizer()\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","25e56ed6":"data=pd.read_csv('\/kaggle\/input\/budget-txt\/budget_speech.txt',error_bad_lines=False, warn_bad_lines=False)","c740f984":"txt = \" \".join(data['Budget 2020-2021'].values)","e3be690d":"txt[:40]","cb4af3d2":"wordcloud = WordCloud().generate(txt)\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.show()","8911ed2e":"# A little Beautification.\nword_cloud = WordCloud(width=800,height=800,background_color='white',\n                      max_words=150).generate(txt)\nplt.figure(figsize=(15,10))\nplt.imshow(word_cloud)\nplt.show()","e79f084c":"#Getting words and its frequency in dict-key-values.\ny = {} \nfor i in txt.split(' '): \n    y[i] = y.get(i,0)+1\n#conversion:\nfreq = {'words':list(y.keys()) , 'freq':list(y.values())}\n\nmydata=pd.DataFrame(freq)\nmydata.sort_values(by='freq',ascending=False).head()","a5619059":"#Applying tokenizer\ntext=word_tokenize(txt.lower()) # Converting all words to lower case for Uniform Casing.\nprint(len(txt),len(text))","d32c54c0":"fdist = FreqDist(text)\nfdist.plot(30,cumulative=False)\nplt.show()","982d7571":"from string import punctuation\nstop_nltk=stopwords.words(\"english\")\nupd_stop = stop_nltk + ['department','government','proposed','under','centeral','will','ministry','provide','rate'] \ntxt_upd = [term for term in text if term not in upd_stop and \\\n               term not in list(punctuation) and len(term)>2]\nlen(set(txt_upd))","06f4f756":"newtxt=[lemm.lemmatize(word) for word in txt_upd]","88e4005f":"#New WordCloud\nmytxt=\" \".join(newtxt)\n#Initiating WordCloud\nword_cloud = WordCloud().generate(mytxt)\n#Beautifying\nword_cloud = WordCloud(width=800,height=800,background_color='white',\n                      max_words=150).generate(mytxt)\nplt.figure(figsize=(15,10))\nplt.imshow(word_cloud)\nplt.show()","554f845f":"#Barplot\nfdist = FreqDist(newtxt)\nfreq = {'words':list(fdist.keys()) , 'freq':list(fdist.values())}\ndf=pd.DataFrame(freq)\ndat=df.sort_values(by='freq',ascending=False).head(25)\nplt.figure(figsize=(10,12))\nsns.barplot(data=dat,x='freq',y='words')\nplt.show()","606a74af":"count_vect = CountVectorizer()\nX = count_vect.fit_transform(newtxt)\n#Getting the BOW:-\ncount_vect.get_feature_names()\n#Getting the DataFrame\nDTM = pd.DataFrame(X.toarray(),columns=count_vect.get_feature_names()) #datatermmatrix\n# TDM=term document matrix\nTDM = DTM.T\nTDM.head()","784b77da":"#Bigrams\ncount_vect_bg = CountVectorizer(ngram_range=(2,2),max_features=25)\nX_bg = count_vect_bg.fit_transform(newtxt)\nDTM_bg = pd.DataFrame(X_bg.toarray(),columns=count_vect_bg.get_feature_names())\nDTM_bg.drop(columns=['01 04','04 2020','19 20','2019 20','2020 21'],inplace=True)#Dont need date columns so dropping them.","76bf0cf8":"plt.figure(figsize=(10,7))\nx=DTM_bg.sum().sort_values(ascending=False).head(25)\ny=x.reset_index()\ny.head(1)\nplt.pie(y[0],autopct='%1.1f%%', startangle=90, pctdistance=0.85,shadow=True)\nplt.legend(y['index'],loc='lower right')\nplt.show()","c10b0ed8":"analyser = SentimentIntensityAnalyzer()\ndef get_vader_sentiment(sent):\n    return analyser.polarity_scores(sent)['compound']\nprint(get_vader_sentiment(mytxt))","161ddcca":"tfidf_vect =TfidfVectorizer()\nX=tfidf_vect.fit_transform(newtxt)\nX.toarray()","60ab8470":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4,random_state=0)\ny_means = kmeans.fit_predict(X.toarray())","35d1cba7":"BOW=tfidf_vect.get_feature_names()\nnum_clusters = 4\narr=kmeans.cluster_centers_\nordered_clu = arr.argsort()[:,::-1]\nfor i in range(num_clusters):\n    print('Topics :',i)\n    for i in ordered_clu[i,:5]:\n        print(BOW[i])","0cfa791a":"Here we can observe words spoken most are coming in bigger font and we have some words like department and others which have to be removed as we know the data is about budget and there words are gonna br common.","20214469":"## Clustering and analysis major topics of the budget.","2d16d64d":"Top 4 topics and top 5 terms in respective topics being used in the budget 2020.\n\nIf you find it interesting please do upvote.\n- Thank You","3f29a09e":"## Raw Wordcloud","5cdb3731":"## Final WordCloud","67c34fe5":"We get a positive .99 score which denotes a overall positive vibe throughout the speech.","f5d7a716":"## Step-2\/Removing StopWords","4fac365c":"- From Pie chart we can observe:\n    - Different sub sections being talked about\n    - Stress being given on 'Swach Bharat Abhiyaan' as anti dumping comes into picture.\n    - pm kisaan yojna and start-ups given importance","a91945dd":"After tokenizating we are left with 6119 words.","05412ce3":"- From this we can infer that \n    - good word is used - which is a positive sentiment\n    - tax is been talked about the most\n    - different schemes could have be launched or reviewed for different development purposes \n    - alse technology can be seen as one of mostly used word\n    ","481e9080":"## Getting Bigrams for better inference","f9fc70a2":"Still we have punctuations and ofcourse STOPWORDS which we will remove now.","a4f71e97":"## Step-1\/ Tokenizing","d9245438":"## Step-3\/Lemmatization","04059af5":"## Getting Sentiment-score of the speech.","5d651c4a":"Here we can see some \"STOPWORDS\" and space coming most times which has to be removed."}}