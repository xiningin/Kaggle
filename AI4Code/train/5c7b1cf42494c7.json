{"cell_type":{"72d74282":"code","8b946062":"code","5e652136":"code","e902e5a2":"code","d28cfb87":"code","472f79df":"code","e874cf76":"code","4b8e6dd6":"code","80e03334":"code","0d9eefca":"code","5fff9009":"code","b7bbef55":"code","f53fbfd3":"code","5d83a42c":"code","8eb0f4e8":"code","6ba29333":"code","4b2ac18f":"code","2a53cbbb":"code","5f395ef8":"code","f1152ff3":"code","881a02b5":"code","7fc19341":"code","c1c2eb10":"code","2e5605e9":"code","85eee428":"code","7acf2e78":"code","492b9a7d":"code","682add60":"code","6554f0bc":"code","41e61210":"code","5992fe3d":"code","767be452":"code","bc1dc1b0":"code","78824ad6":"code","f4e3c15f":"code","4e68c769":"code","1da8a8fc":"code","a20e5927":"code","9f436274":"code","d6828dc9":"code","2d00e6ae":"code","544a357f":"code","9bf1d929":"code","6f32c78a":"code","83b54f12":"code","0e621910":"code","ac84545d":"code","5f95a24f":"code","e3267ba8":"code","11d2ad70":"code","5134b221":"code","57c3ba41":"code","81e7f3f6":"code","47e4b875":"code","215644c4":"code","73be59a3":"code","23572e69":"code","8e4764d3":"code","6d1586d4":"code","b83c5220":"code","4b5d84dc":"code","82f9db06":"code","c5399495":"code","3c9c4e75":"code","fe223179":"code","786fad91":"code","ba124de7":"code","24d792a6":"code","43b89817":"code","da160829":"code","284be892":"code","d152f6dd":"code","3b7e2816":"code","3ff7554e":"code","027109fd":"code","a1eab38a":"code","0ad2ef09":"code","e19051c1":"code","efa7bd0f":"code","3c2a0d13":"code","5b6b0dd9":"code","a19a0845":"code","3e4a0aaa":"code","8acd8ae6":"code","edfd9f9c":"code","d4fae084":"code","6690133e":"code","4010cd48":"code","0c28dab2":"code","dd32aac2":"code","20408651":"code","d570b038":"code","35e1867e":"code","eb6605e5":"code","b1cee7f3":"code","1e6717ea":"code","1e824a5e":"code","0a125e0a":"code","2c793fac":"code","41807ab4":"code","1b0e6eff":"code","758227f6":"code","830034c4":"code","3a6a51d3":"code","ae6c6f06":"code","3c4ec07c":"code","f0800bb3":"code","56187d23":"code","5897e897":"code","bc4473d9":"code","0773556c":"code","c8756ee1":"code","a876aca5":"code","8f47426e":"code","9b6e0376":"code","d744a7ee":"code","5c0c17d7":"code","db09b642":"code","c0489a7d":"code","f79d514f":"code","f01b0d35":"code","57a971d2":"code","b829df3c":"code","73247ba3":"code","bf83845f":"code","016304fa":"code","359a6f8c":"code","14e49fb3":"code","cfa546f9":"code","46f6f189":"code","1e219f9b":"code","0fe2a93b":"code","0258779a":"code","66af4eb5":"code","0db2aa05":"code","daa82c70":"code","0c6e31d8":"code","e731233a":"code","96e2027a":"code","4d1ccbe1":"code","b7461aa9":"code","64f3fcbc":"code","0ff41e62":"code","c8ee0bc7":"code","e93f3d65":"code","261aa6ee":"code","4a6d2867":"code","27d57222":"markdown","ff912327":"markdown","62e0a493":"markdown"},"source":{"72d74282":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8b946062":"\ntrain = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')","5e652136":"train_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')","e902e5a2":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","d28cfb87":"def test_in_sub(test):\n    tgms = test.groupby('installation_id').last().game_session\n    tgms1 = tgms.reset_index()\n    test_ass = test[test.type == \"Assessment\"]\n    tgms1[\"title\"] = str(test_ass[test_ass.game_session == tgms1[\"game_session\"][0]].title.reset_index(drop=True)[0])\n    \n    for i in range(0,len(tgms1)):\n        tgms1[\"title\"][i] = str(test_ass[test_ass.game_session==tgms1[\"game_session\"][i]].title.reset_index(drop=True)[0])\n    return tgms1","472f79df":"def c_accuracy_group(df):\n    df[\"accuracy_group\"]=0\n    for i in range(0,len(df)):\n        acc = float(df[\"accuracy\"][i])\n        if (acc == float(0)):\n            df[\"accuracy_group\"][i]=0\n        elif (acc < float(0.5)):\n            df[\"accuracy_group\"][i]=1\n        elif (acc < float(1)):\n            df[\"accuracy_group\"][i]=2\n        elif (acc == float(1)):\n            df[\"accuracy_group\"][i]=3\n        else:\n            df[\"accuracy_group\"][i] = None\n    return df\n            ","e874cf76":"def test_to_label(test):\n    print(\"Converting to label format, as of submissions done in assessment\")\n    test_ass = test[test.type == \"Assessment\"]\n    test_ass_sub = test_ass[(((test_ass.event_code == 4100) & (test_ass.title != 'Bird Measurer (Assessment)'))) | (((test_ass.event_code == 4110) & (test_ass.title == 'Bird Measurer (Assessment)')))]\n    test_ass_sub_inf = test_ass_sub[[\"installation_id\",\"game_session\",\"timestamp\",\"title\",\"event_data\"]]\n    test_ass_sub_inf0 = test_ass_sub_inf\n    test_ass_sub_inf0[\"correct\"] = 0\n    test_ass_sub_inf0[\"incorrect\"] = 0\n    \n    for i in range(0,len(test_ass_sub_inf0)):\n        if \"\\\"correct\\\":true\" in test_ass_sub_inf0[\"event_data\"][test_ass_sub_inf0.index[i]]:\n            test_ass_sub_inf0[\"correct\"][test_ass_sub_inf0.index[i]] = 1\n        else:\n            test_ass_sub_inf0[\"incorrect\"][test_ass_sub_inf0.index[i]] = 1\n    test_ass_sub_inf1 = test_ass_sub_inf0.groupby(by=[\"installation_id\",\"game_session\",\"title\"],sort=False).sum()\n    test_ass_sub_inf2 = test_ass_sub_inf1\n    test_ass_sub_inf2 = test_ass_sub_inf2.reset_index()\n    test_ass_sub_inf2[\"accuracy\"] =float(0)\n    \n    for i in range(0,len(test_ass_sub_inf2)):\n        corr = test_ass_sub_inf2[\"correct\"][i]\n        incor = test_ass_sub_inf2[\"incorrect\"][i]\n        test_ass_sub_inf2[\"accuracy\"][i] = float(corr)\/(incor+corr)\n    \n    test_ass_sub_inf3 = test_ass_sub_inf2\n    test_ass_sub_inf3 = c_accuracy_group(test_ass_sub_inf3)\n    return test_ass_sub_inf3\n    ","4b8e6dd6":"def get_time_gm(train, train_labels):\n    print(\"Adding cumulative game played time for each session\")\n    train_data_1 = train[[\"installation_id\", \"game_session\", \"title\",\"game_time\"]]\n    train_time_god_2 = train[[\"installation_id\", \"game_session\", \"title\",\"game_time\"]]\n    ttg_time = train_time_god_2.groupby(by=['game_session'], sort=False).last().game_time.reset_index()\n    ttg_time0 = train.groupby(by = [\"installation_id\", \"game_session\", \"title\"],sort=False).size().reset_index().drop(columns=[0]).merge(ttg_time, on = 'game_session', how = 'left')\n    ttg_time00 = ttg_time0.groupby(by=['installation_id','game_session'], sort=False).sum().groupby(level=[0]).cumsum()\n    ttg_time1 = ttg_time00.reset_index()\n    ttg_time2 = ttg_time1[[\"game_session\",\"game_time\"]]\n    train_labels1 = train_labels\n    # join train with train labels\n    train_labels_t = train_labels1.merge(ttg_time2, on = 'game_session', how = 'left')\n\n    return train_labels_t\n","80e03334":"\ndef get_final_feat2(train, train_labels_derive_time):   \n    # train, train_labels_derive_time\n    print(\"Adding more time sensitive features such as: all correct, all incorrect,avg_corr_all,avg_incorr_all,avg_inacc_r, avg_acc_r,score, average score, corr to incorr ratio and vice versa until any game session\")\n    \n    train_edit_c = train[[\"installation_id\", \"game_session\", \"title\", \"event_data\"]]\n    trc_ic = train_edit_c[(train_edit_c.event_data.str.contains(\"\\\"correct\\\":true\") | train_edit_c.event_data.str.contains(\"\\\"correct\\\":false\")) ]\n    trc_ic1 = trc_ic.groupby([\"installation_id\", \"game_session\", \"title\"]).size().reset_index().drop(columns = [0])\n    \n    trc = train_edit_c[train_edit_c.event_data.str.contains(\"\\\"correct\\\":true\")]\n    trc_edit = trc[[\"installation_id\",\"game_session\"]]\n    trc_edit[\"correct_all\"] = 1\n    trc_edit_all1 = trc_edit.groupby(by=['installation_id','game_session'], sort = False).sum().groupby(level=[0]).cumsum()\n    trc_edit_all1 = trc_edit_all1.reset_index()\n    \n    tric = train_edit_c[train_edit_c.event_data.str.contains(\"\\\"correct\\\":false\")]\n    tric_edit = tric[[\"installation_id\",\"game_session\"]]\n    tric_edit[\"incorrect_all\"] = 1\n    tric_edit_all1 = tric_edit.groupby(by=['installation_id','game_session'], sort=False).sum().groupby(level=[0]).cumsum()\n    tric_edit_all1 = tric_edit_all1.reset_index()\n    \n    print(\"Adding correct all and incorrect all feature, later we might wanna add specific accuracy groups of titles\/assesssments, to record history of gameplay of user\")\n    # join train with train labels\n    train_c_1 = trc_ic1.merge(trc_edit_all1[[\"game_session\",\"correct_all\"]], on = 'game_session', how = 'left')\n    train_c_2 = train_c_1.merge(tric_edit_all1[[\"game_session\",\"incorrect_all\"]], on = 'game_session', how = 'left')\n    \n    # join train with train labels\n    train_c_2[\"correct_all\"].fillna(0, inplace=True)\n    train_c_2[\"incorrect_all\"].fillna(0, inplace=True)\n    to_get_acc = train_c_2 # contains all the gms with either true or false\n    \n    print(\"Adding score and score count\")\n    to_get_acc1 = to_get_acc\n    \n    to_get_acc1[\"score\"] = 0.000001\n    to_get_acc1[\"score_c\"] = 0\n    to_get_acc1[\"acc_r_single\"] = 0.000001\n    to_get_acc1[\"inacc_r_single\"] = 0.000001\n   # to_get_acc1[\"acc_r\"] = 0.000001\n    #to_get_acc1[\"inacc_r\"] = 0.000001\n    \n   \n    for i in range(0,len(to_get_acc1)):\n        acc = to_get_acc1[\"correct_all\"][i]\n        ina = to_get_acc1[\"incorrect_all\"][i]\n        if((acc == 0) and (ina) == 0):\n            to_get_acc1[\"score_c\"][i] = 0\n            to_get_acc1[\"score\"][i] = 0\n            to_get_acc1[\"acc_r_single\"][i] = 0\n            to_get_acc1[\"inacc_r_single\"][i] = 0\n         #   to_get_acc1[\"acc_r\"][i] = 0\n         #   to_get_acc1[\"inacc_r\"][i] = 0\n            continue\n        elif(acc == 0):\n            to_get_acc1[\"score\"][i] = round(float(ina),3)*(-5)\n            to_get_acc1[\"score_c\"][i] = 1\n            to_get_acc1[\"acc_r_single\"][i] = 0\n            to_get_acc1[\"inacc_r_single\"][i] = ina\n            #to_get_acc1[\"acc_r\"][i] = 0\n          #  to_get_acc1[\"inacc_r\"][i] = ina\n        elif(ina == 0):\n            to_get_acc1[\"score\"][i] = round(float(acc),3)*(5)\n            to_get_acc1[\"score_c\"][i] = 1\n            to_get_acc1[\"acc_r_single\"][i] = acc\n            to_get_acc1[\"inacc_r_single\"][i] = 0\n          #  to_get_acc1[\"acc_r\"][i] = acc\n          #  to_get_acc1[\"inacc_r\"][i] = 0\n        elif((ina != 0) and (acc != 0)):\n            to_get_acc1[\"score\"][i] = round((float(acc)),3)*3-round((float(ina)),3)*1\n            to_get_acc1[\"score_c\"][i] = 1\n            to_get_acc1[\"acc_r_single\"][i] = round(float(acc)\/ina,3)\n            to_get_acc1[\"inacc_r_single\"][i] = round(float(ina)\/acc,3)\n           # to_get_acc1[\"acc_r\"][i] = round(float(acc)\/ina,3)\n           # to_get_acc1[\"inacc_r\"][i] = round(float(ina)\/acc,3)\n            \n    #to_get_acc1\n    \n    train_copy = train[[\"installation_id\", \"game_session\", \"title\"]].groupby(by = [\"installation_id\",\"game_session\",\"title\"], sort=False).size().reset_index()\n    # join train with train labels\n    train_t_1 = train_copy.drop(columns=[0]).merge(to_get_acc1[[\"game_session\",\"correct_all\", \"incorrect_all\",\"acc_r_single\",\"inacc_r_single\", \"score\", \"score_c\"]], on = 'game_session', how = 'left')\n    train_t_2 = train_t_1\n    train_t_2[\"correct_all\"].fillna(0, inplace=True)\n    train_t_2[\"incorrect_all\"].fillna(0, inplace=True)\n    train_t_2[\"score\"].fillna(0, inplace=True)\n    train_t_2[\"score_c\"].fillna(0, inplace=True)\n    train_t_2[\"acc_r_single\"].fillna(0, inplace=True)\n    train_t_2[\"inacc_r_single\"].fillna(0, inplace=True)\n    \n    train_t_3 = train_t_2\n    train_t_3 = train_t_3.groupby(by=['installation_id','game_session','title'],sort=False).sum().groupby(level=[0]).cumsum()\n    train_t_3 = train_t_3.reset_index()\n    \n    print(\"Adding average score, avg acc ratio and avg inacc ratio\")\n    train_t_4 = train_t_3\n    # now count average score till that point, acc_r, inacc_r\n    train_t_4[\"average_score\"] = float(0)\n    train_t_4[\"acc_r\"] = float(0)\n    train_t_4[\"inacc_r\"] = float(0)\n    train_t_4[\"avg_acc_r\"] = float(0)\n    train_t_4[\"avg_inacc_r\"] = float(0)\n    for i in range(0,len(train_t_4)):\n        acc = train_t_4[\"correct_all\"][i]\n        inacc = train_t_4[\"incorrect_all\"][i]\n        score = round(float(train_t_4[\"score\"][i]))\n        count = train_t_4[\"score_c\"][i]\n        acc_r_single = train_t_4[\"acc_r_single\"][i]\n        inacc_r_single = train_t_4[\"inacc_r_single\"][i]\n        \n        if (count!=0):\n            train_t_4[\"average_score\"][i] = round(float(score)\/count,3)\n            train_t_4[\"avg_acc_r\"][i] = round(float(acc_r_single)\/count,3)\n            train_t_4[\"avg_inacc_r\"][i] = round(float(inacc_r_single)\/count,3)\n        else:\n            train_t_4[\"average_score\"][i] = 0\n            train_t_4[\"avg_acc_r\"][i] = 0\n            train_t_4[\"avg_inacc_r\"][i] = 0\n        if((inacc == 0)&(acc == 0)):\n            train_t_4[\"acc_r\"][i] = 0\n            train_t_4[\"inacc_r\"][i] = 0\n        elif(inacc == 0):\n            train_t_4[\"acc_r\"][i] = acc\n            train_t_4[\"inacc_r\"][i] = 0\n        elif(acc == 0):\n            train_t_4[\"acc_r\"][i] = 0\n            train_t_4[\"inacc_r\"][i] = inacc\n        elif((inacc != 0) & (acc != 0)):\n            train_t_4[\"acc_r\"][i] = round(float(acc)\/inacc,3)\n            train_t_4[\"inacc_r\"][i] = round(float(inacc)\/acc,3)\n                \n    train_t_5 = train_t_4        \n    \n    print(\"Almost done, combining all into label format\")\n    # join train with train labels\n    train_labels_derive_time_corr = train_labels_derive_time.merge(train_t_5[[\"game_session\",\"correct_all\",\"incorrect_all\",\"avg_acc_r\",\"avg_inacc_r\",\"acc_r_single\",\"inacc_r_single\",\"score\",\"score_c\",\"average_score\",\"acc_r\",\"inacc_r\"]], on = 'game_session', how = 'left')\n    return train_labels_derive_time_corr","0d9eefca":"def get_misses_all(train,train_all_other):\n    train_miss = train[train.event_data.str.contains(\"misses\")]\n    train_miss1 = train_miss.reset_index()\n    train_miss1[\"misses\"] = 0\n    for i in range(0,len(train_miss1)):\n        miss = eval(train_miss1.event_data[train_miss1.event_data.index[i]])[\"misses\"]\n        train_miss1[\"misses\"][i] = miss\n    train_miss2 = train_miss1[[\"installation_id\", \"game_session\",\"title\",\"world\",\"misses\"]]\n    train_miss3 = train_miss2\n    train_miss3c = train_miss3[train_miss3.world==\"CRYSTALCAVES\"]\n    train_miss3m = train_miss3[train_miss3.world==\"MAGMAPEAK\"]\n    train_miss3t = train_miss3[train_miss3.world==\"TREETOPCITY\"]\n    \n    train_miss3c[\"missc\"] = train_miss3c[\"misses\"]\n    train_miss3m[\"missm\"] = train_miss3m[\"misses\"]\n    train_miss3t[\"misst\"] = train_miss3t[\"misses\"]\n    \n    train_miss4c = train_miss3c.groupby(by = [\"installation_id\",\"game_session\"],sort=False).sum().reset_index()#.groupby(level=[0]).cumsum().reset_index()\n    train_miss4m = train_miss3m.groupby(by = [\"installation_id\",\"game_session\"],sort=False).sum().reset_index()#.groupby(level=[0]).cumsum().reset_index()\n    train_miss4t = train_miss3t.groupby(by = [\"installation_id\",\"game_session\"],sort=False).sum().reset_index()#.groupby(level=[0]).cumsum().reset_index()\n    train_miss4 = train_miss3.groupby(by = [\"installation_id\",\"game_session\"],sort=False).sum().reset_index()#.groupby(level=[0]).cumsum().reset_index()\n    train_data_1 = train.groupby(by = [\"installation_id\", \"game_session\", \"title\"],sort=False).size().reset_index().drop(columns=[0])\n    \n    train_missc = train_data_1.merge(train_miss4c[[\"game_session\", \"missc\"]], on = 'game_session', how = 'left')\n    train_misscm = train_missc.merge(train_miss4m[[\"game_session\", \"missm\"]], on = 'game_session', how = 'left')\n    train_misscmt = train_misscm.merge(train_miss4t[[\"game_session\", \"misst\"]], on = 'game_session', how = 'left')\n    train_missall = train_misscmt.merge(train_miss4[[\"game_session\", \"misses\"]], on = 'game_session', how = 'left')\n    train_missall[\"misses\"].fillna(0, inplace=True)\n    train_missall[\"missc\"].fillna(0, inplace=True)\n    train_missall[\"missm\"].fillna(0, inplace=True)\n    train_missall[\"misst\"].fillna(0, inplace=True)\n    \n    train_missall1 = train_missall.groupby(by=[\"installation_id\",\"game_session\"],sort=False).sum().groupby(level=[0]).cumsum().reset_index()\n    \n    train_all_other1 = train_all_other.merge(train_missall1[['game_session','misses','missc','missm','misst']], on = 'game_session', how = 'left')\n \n    return train_all_other1\n\n","5fff9009":"def get_all2(train):\n    train_labels_derive2 = test_to_label(train)\n    train_labels_derive_time2 = get_time_gm(train,train_labels_derive2)\n    get2 = get_final_feat2(train, train_labels_derive_time2)\n    get_miss = get_misses_all(train,get2)\n    return get_miss","b7bbef55":"def get_sub2(test):\n    test_labels_derive2 = test_in_sub(test)\n    test_labels_derive_time2 = get_time_gm(test,test_labels_derive2)\n    get2 = get_final_feat2(test, test_labels_derive_time2)\n    get_miss = get_misses_all(test,get2)\n    return get_miss","f53fbfd3":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport numpy as np\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.stats import mode\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\n#modeling\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n#LGB imports\nimport lightgbm as lgb","5d83a42c":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm","8eb0f4e8":"from sklearn.metrics import confusion_matrix\n# this function is the quadratic weighted kappa (the metric used for the competition submission)\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    # Calculate the percent each class was tagged each label\n    O = confusion_matrix(act,pred)\n    # normalize to sum 1\n    O = np.divide(O,np.sum(O))\n    \n    # create a new matrix of zeroes that match the size of the confusion matrix\n    # this matriz looks as a weight matrix that give more weight to the corrects\n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            # makes a weird matrix that is bigger in the corners top-right and botton-left (= 1)\n            W[i][j] = ((i-j)**2)\/((n-1)**2)\n            \n    # make two histograms of the categories real X prediction\n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    # multiply the two histograms using outer product\n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E)) # normalize to sum 1\n    \n    # apply the weights to the confusion matrix\n    num = np.sum(np.multiply(W,O))\n    # apply the weights to the histograms\n    den = np.sum(np.multiply(W,E))\n    \n    return 1-np.divide(num,den)\n    ","6ba29333":"# this function makes the model and sets the parameters\n# for configure others parameter consult the documentation below:\n# https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostclassifier.html\ndef make_classifier1():\n    clf1 = CatBoostClassifier(\n                               loss_function='MultiClass',\n                                # eval_metric=\"AUC\",\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n                               verbose=None,\n                               depth=8,\n                               early_stopping_rounds=50,\n                                #l2_leaf_reg=1,\n                                #border_count=96,\n                               random_seed=42\n                              )\n        \n    return clf1","4b2ac18f":"get2_test = get_all2(test)","2a53cbbb":"get2_test.to_csv(\"get2_test.csv\")","5f395ef8":"get2_train = get_all2(train)","f1152ff3":"get2_train.to_csv(\"get2_train.csv\")","881a02b5":"get2_sub = get_sub2(test)","7fc19341":"get2_sub.to_csv(\"get2_sub.csv\")","c1c2eb10":"get2A = get2_train.drop(columns = [\"installation_id\",\"game_session\"]).sample(frac=1,random_state = 42).reset_index(drop=True)","2e5605e9":"get2A_test = get2_test.drop(columns = [\"installation_id\",\"game_session\"]).sample(frac=1,random_state = 42).reset_index(drop=True)","85eee428":"get2A_sub = get2_sub.drop(columns = [\"installation_id\",\"game_session\"])#.sample(frac=1,random_state = 42).reset_index(drop=True)","7acf2e78":"get2A.to_csv(\"get2A_train.csv\")","492b9a7d":"labels_map = {\"Mushroom Sorter (Assessment)\":1,\"Bird Measurer (Assessment)\":2,\"Cauldron Filler (Assessment)\":3,\"Chest Sorter (Assessment)\":4,\"Cart Balancer (Assessment)\":5}","682add60":"world_map = {\"Mushroom Sorter (Assessment)\":1,\"Bird Measurer (Assessment)\":1,\"Cauldron Filler (Assessment)\":2,\"Chest Sorter (Assessment)\":3,\"Cart Balancer (Assessment)\":3}","6554f0bc":"get2A['world'] = get2A['title'].map(world_map)\nget2A_test['world'] = get2A_test['title'].map(world_map)\n\nget2A['title'] = get2A['title'].map(labels_map)\nget2A_test['title'] = get2A_test['title'].map(labels_map)\n","41e61210":"get2A_sub['world'] = get2A_sub['title'].map(world_map)\nget2A_sub['title'] = get2A_sub['title'].map(labels_map)\n","5992fe3d":"get2A.to_csv(\"get2A_train.csv\", index=None)","767be452":"get2A_test.to_csv(\"get2A_test.csv\")","bc1dc1b0":"get2A_sub.to_csv(\"get2A_sub.csv\")","78824ad6":"get2_train","f4e3c15f":"get2A","4e68c769":"get2A1_test = get2A_test.drop(columns = [\"accuracy_group\",\"correct\",\"incorrect\",\"accuracy\"])\nget2A1_test[\"accuracy_group\"] = get2A_test[\"accuracy_group\"]\nget2A1_test","1da8a8fc":"get2A1 = get2A.drop(columns = [\"accuracy_group\",\"correct\",\"incorrect\",\"accuracy\"])\nget2A1[\"accuracy_group\"] = get2A[\"accuracy_group\"]\nget2A1","a20e5927":"Xmod1 = get2A.drop(columns = [\"accuracy_group\",\"correct\",\"incorrect\",\"accuracy\"])\nYmod1 = get2A[\"accuracy_group\"]","9f436274":"Xmod1","d6828dc9":"# CART Classification\n#from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\ndataframe = get2A1\narray = dataframe.values\nX = array[:,0:18]\nY = array[:,18]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","2d00e6ae":"model.fit(X,Y)","544a357f":"y_pr = model.predict(X)\nqwk(Y,y_pr)","9bf1d929":"dataframe1 = get2A1_test\narray1 = dataframe1.values\nX1 = array1[:,0:18]\nY1 = array1[:,18]\ny_pr1 = model.predict(X1)\nqwk(Y1,y_pr1)","6f32c78a":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring='f1_macro')\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","83b54f12":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring='f1_micro')\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","0e621910":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(cohen_kappa_score))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","ac84545d":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CAT',CatBoostClassifier()))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(cohen_kappa_score))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","5f95a24f":"X1","e3267ba8":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CAT',CatBoostClassifier(verbose=999)))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(cohen_kappa_score))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","11d2ad70":"get2A1","5134b221":"get2A1_1 = get2A1[get2A1.title == 1]\nget2A1_2 = get2A1[get2A1.title == 2]\nget2A1_3 = get2A1[get2A1.title == 3]\nget2A1_4 = get2A1[get2A1.title == 4]\nget2A1_5 = get2A1[get2A1.title == 5]\ndf1 = get2A1_1.values\ndf2 = get2A1_2.values\ndf3 = get2A1_3.values\ndf4 = get2A1_4.values\ndf5 = get2A1_5.values\nX_1 = df1[:,1:18]\nY_1 = df1[:,18]\nX_2 = df2[:,1:18]\nY_2 = df2[:,18]\nX_3 = df3[:,1:18]\nY_3 = df3[:,18]\nX_4 = df4[:,1:18]\nY_4 = df4[:,18]\nX_5 = df5[:,1:18]\nY_5 = df5[:,18]\nget2A1_1","57c3ba41":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CAT',CatBoostClassifier(verbose=999,early_stopping_rounds=300)))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X_1, Y_1, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","81e7f3f6":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CAT',CatBoostClassifier(verbose=999,early_stopping_rounds=300)))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X_2, Y_2, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","47e4b875":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CAT',CatBoostClassifier(verbose=999,early_stopping_rounds=300)))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X_3, Y_3, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","215644c4":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CAT',CatBoostClassifier(verbose=999,early_stopping_rounds=300)))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X_4, Y_4, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","73be59a3":"# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CAT',CatBoostClassifier(verbose=999,early_stopping_rounds=300)))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X_5, Y_5, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","23572e69":"import lightgbm as lgb\nimport xgboost as xgb\n# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('LGB', lgb.LGBMClassifier(verbose=999)))\nmodels.append(('CAT',CatBoostClassifier(verbose=999)))\nmodels.append(('XGB', xgb.XGBClassifier(verbose=999)))\n\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","8e4764d3":"from sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('LGB', lgb.LGBMClassifier(verbose=999)))\nmodels.append(('CAT',CatBoostClassifier(verbose=999)))\nmodels.append(('XGB', xgb.XGBClassifier(verbose=999)))\n\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","6d1586d4":"from sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n# Compare Algorithms\n#from pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.scorer import make_scorer\n# load dataset\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('LGB', lgb.LGBMClassifier(verbose=999)))\nmodels.append(('CAT',CatBoostClassifier(verbose=999)))\nmodels.append(('XGB', xgb.XGBClassifier(verbose=999)))\n\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    #cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=scoring)\n    cv_results = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(qwk))\n    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","b83c5220":"# Create a pipeline that extracts features from the data then creates a model\n#from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n# load data\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# create feature union\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nprint(features)\nfeature_union = FeatureUnion(features)\n# create pipeline\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('random forest', RandomForestClassifier()))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(qwk))\nprint(results.mean())\nfeature_union","4b5d84dc":"# Create a pipeline that extracts features from the data then creates a model\n#from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n# load data\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# create feature union\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nprint(features)\nfeature_union = FeatureUnion(features)\n# create pipeline\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('Catboost', CatBoostClassifier(verbose=999)))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(qwk))\nprint(results.mean())\nfeatures","82f9db06":"# Create a pipeline that extracts features from the data then creates a model\n#from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n# load data\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# create feature union\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=8)))\nprint(features)\nfeature_union = FeatureUnion(features)\n# create pipeline\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('Catboost', CatBoostClassifier(verbose=999)))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X1, Y1, cv=kfold, scoring=make_scorer(qwk))\nprint(results.mean())\nfeatures","c5399495":"# Create a pipeline that extracts features from the data then creates a model\n#from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n# load data\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# create feature union\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=8)))\nprint(features)\nfeature_union = FeatureUnion(features)\n# create pipeline\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('Catboost', CatBoostClassifier(verbose=999)))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=make_scorer(qwk))\nprint(results.mean())\nfeatures","3c9c4e75":"dataframe1 = get2A2_test\narray1 = dataframe1.values\nX1 = array1[:,0:18]\nY1 = array1[:,18]\ny_pr1 = model.predict(X1)\nqwk(Y1,y_pr1)","fe223179":"Xmod2 = Xmod1.reset_index()\nXmod2","786fad91":"Xmod2.columns","ba124de7":"Xmod2.describe()","24d792a6":"from numba import jit \n\n@jit\ndef qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n    e = e \/ a1.shape[0]\n    return 1 - o \/ e","43b89817":"#define the parameters for lgbm.\n\nSEED = 42\nN_FOLD = 10\nparams = {\n    'min_child_weight': 10.0,\n    'objective': 'multi:softprob',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.5,\n    'num_class':4,\n    'learning_rate':0.05,\n    'n_estimators':2000,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'eval_metric':'mlogloss'\n    }\n\nfeatures = [i for i in final_train_df.columns if i not in ['accuracy_group']]","da160829":"X_train = Xmod1\ny_train = Ymod1\nfinal_test_df = Xmod1test","284be892":"def model(train_X,train_Y, test, params, n_splits=N_FOLD):\n    \n    #define KFold Strategy\n    folds = StratifiedKFold(n_splits=N_FOLD,shuffle=True, random_state=SEED)\n    scores = []\n    \n    #out of the fold \n    y_pre = np.zeros((len(test),4), dtype=float)\n    target = [\"accuracy_group\"]\n    #print(\"done\")\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, train_Y)):\n        print(\"------------------------ fold {} -------------------------\".format(fold_ + 1))\n        \n        X_train, X_valid = train_X.iloc[trn_idx], train_X.iloc[val_idx]\n        y_train, y_valid = train_Y.iloc[trn_idx], train_Y.iloc[val_idx]\n        \n        # Convert our data into XGBoost format\n        d_train = xgb.DMatrix(X_train, y_train)\n        d_valid = xgb.DMatrix(X_valid, y_valid)\n        \n        xgb_model = xgb.train(params,\n                      d_train,\n                      num_boost_round=1600,\n                      evals=[(d_train, 'train'), (d_valid, 'val')],\n                      verbose_eval=False,\n                      early_stopping_rounds=70\n                     )\n        \n        d_val = xgb.DMatrix(X_valid)\n        pred_val = [np.argmax(x) for x in xgb_model.predict(d_val)]\n        \n        #calculate cohen kappa score\n        score = cohen_kappa_score(pred_val,y_valid,weights='quadratic')\n        scores.append(score)\n\n        pred = xgb_model.predict(xgb.DMatrix(test))\n        #save predictions\n        y_pre += pred\n        \n        print(f'Fold: {fold_+1} quadratic weighted kappa score: {np.round(score,4)}')\n\n    pred = np.asarray([np.argmax(line) for line in pred])\n    print('Mean choen_kappa_score:',np.round(np.mean(scores),6))\n    \n    return xgb_model,pred\n","d152f6dd":"xgb_model,pred = model(X_train,y_train,final_test_df,params)","3b7e2816":"new_str = str(Xmod2['title'][0])+\" \"+str(Xmod2['game_time'][0])\nnew_str","3ff7554e":"Xmod2.index[0]","027109fd":"Xmod2_q = Xmod2[\"index\"].to_frame()\nXmod2_q","a1eab38a":"Xmod2_q[\"new_event\"] = \"a\"\nXmod2_q","0ad2ef09":"i=0\nnew_str1 = str(Xmod2['title'][i])+\" \"+str(Xmod2['game_time'][i])+\" \"+str(Xmod2['correct_all'][i])+\" \"+str(Xmod2['incorrect_all'][i])+\" \"+str(Xmod2['avg_acc_r'][i])+\" \"+str(Xmod2['avg_inacc_r'][i])+\" \"+str(Xmod2['acc_r_single'][i])+\" \"+str(Xmod2['inacc_r_single'][i])+\" \"+str(Xmod2['score'][i])+\" \"+str(Xmod2['score_c'][i])+\" \"+str(Xmod2['average_score'][i])+\" \"+str(Xmod2['acc_r'][i])+\" \"+str(Xmod2['inacc_r'][i])+\" \"+str(Xmod2['misses'][i])+\" \"+str(Xmod2['missc'][i])+\" \"+str(Xmod2['missm'][i])+\" \"+str(Xmod2['misst'][i])+\" \"+str(Xmod2['world'][i])\nnew_str1","e19051c1":"%%time\nfor i in range(0,len(Xmod2_q)):\n    Xmod2_q[\"new_event\"][i] = str(Xmod2['title'][i]))+\" \"+str(Xmod2['game_time'][i])+\" \"+str(Xmod2['correct_all'][i])+\" \"+str(Xmod2['incorrect_all'][i])+\" \"+str(Xmod2['avg_acc_r'][i])+\" \"+str(Xmod2['avg_inacc_r'][i])+\" \"+str(Xmod2['acc_r_single'][i])+\" \"+str(Xmod2['inacc_r_single'][i])+\" \"+str(Xmod2['score'][i])+\" \"+str(Xmod2['score_c'][i])+\" \"+str(Xmod2['average_score'][i])+\" \"+str(Xmod2['acc_r'][i])+\" \"+str(Xmod2['inacc_r'][i])+\" \"+str(Xmod2['misses'][i])+\" \"+str(Xmod2['missc'])+\" \"+str(Xmod2['missm'][i])+\" \"+str(Xmod2['misst'][i])+\" \"+str(Xmod2['world'][i])","efa7bd0f":"label = []\nfor i in range(0,len(Ymod1)):\n    if Ymod1[i] == 3:\n        label.append([0, 0,0,1])  # class 3\n    elif Ymod1[i] == 2:\n        label.append([0, 0,1,0])  # class 2\n    elif Ymod1[i] == 1:\n        label.append([0, 1,0,0])  # class 1\n    elif Ymod1[i] == 0:\n        label.append([1, 0,0,0])  # class 0\nlabel","3c2a0d13":"len(label)","5b6b0dd9":"Xmod1test = get2A_test.drop(columns = [\"accuracy_group\",\"correct\",\"incorrect\",\"accuracy\"])\nYmod1test = get2A_test[\"accuracy_group\"]","a19a0845":"labelt = []\nfor i in range(0,len(Ymod1test)):\n    if Ymod1test[i] == 3:\n        labelt.append([0, 0,0,1])  # class 3\n    elif Ymod1test[i] == 2:\n        labelt.append([0, 0,1,0])  # class 2\n    elif Ymod1test[i] == 1:\n        labelt.append([0, 1,0,0])  # class 1\n    elif Ymod1test[i] == 0:\n        labelt.append([1, 0,0,0])  # class 0\nlabelt","3e4a0aaa":"len(labelt)","8acd8ae6":"Xmod1sub = get2A_sub","edfd9f9c":"import pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","d4fae084":"get2A1 = get2A.drop(columns = [\"accuracy\", \"correct\",\"incorrect\",\"accuracy_group\"])\nget2A1[\"accuracy_group\"] = get2A[\"accuracy_group\"]\nget2A1","6690133e":"get2A1.to_csv(\"daat.csv\",header=False,index=None) #df.to_csv('file.csv', header=False, index=False)","4010cd48":"\ndaat = numpy.loadtxt(\"daat.csv\", delimiter=\",\")\ndaat","0c28dab2":"Xk = daat[:,0:18]\nYk = daat[:,18]\nXk","dd32aac2":"Yk","20408651":"Xmod1.to_csv(\"Xmod1.csv\",header=False,index=None) #df.to_csv('file.csv', header=False, index=False)","d570b038":"\ndataf = numpy.loadtxt(\"dataf.csv\", delimiter=\",\")\ndataf","35e1867e":"\ndataset = numpy.loadtxt(\"Xmod1.csv\", delimiter=\",\")\ndataset","eb6605e5":"Xmod1","b1cee7f3":"# create model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=18, init= 'uniform' , activation= 'relu' ))\n#model.add(Dense(8, init= 'uniform' , activation= 'relu' ))\nmodel.add(Dense(1, init= 'uniform' , activation= 'sigmoid' ))","1e6717ea":"# Compile model\nmodel.compile(loss= 'mean_squared_error' , optimizer= 'adam' , metrics=[ 'accuracy' ])","1e824a5e":"# Fit the model\nmodel.fit(dataset, Ymod1, nb_epoch=100,validation_split=0.2, batch_size=64)","0a125e0a":"%%time\nimport numpy as np\nimport catboost\nfrom catboost import CatBoostClassifier, Pool\n\n# train model on all data once\npoolmod2 = Pool(Xmod1, Ymod1,cat_features=['title','world'], feature_names=list(Xmod1.columns))\nclfmode2 = make_classifier1()\nclfmode2.fit(poolmod2, plot=True)","2c793fac":"predictedmod2 = clfmode2.predict(Xmod1test)\nfrom sklearn.metrics import classification_report\nreportcatmod2 = classification_report(Ymod1test, predictedmod2)\nprint(reportcatmod2)","41807ab4":"predictedmodtr = clfmode2.predict(Xmod1)\nfrom sklearn.metrics import classification_report\nreportcatmodtr = classification_report(Ymod1, predictedmodtr)\nprint(reportcatmodtr)","1b0e6eff":"clfmode2.get_feature_importance()","758227f6":"Xmod1test.columns","830034c4":"# oof is an zeroed array of the same size of the input dataset\nprint('OOF QWK:', qwk(Ymod1test, predictedmod2))","3a6a51d3":"# oof is an zeroed array of the same size of the input dataset\nprint('OOF QWK:', qwk(Ymod1, predictedmodtr))","ae6c6f06":"X","3c4ec07c":"all_features = Xmod1.columns\nall_features","f0800bb3":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats","56187d23":"%%time\n# CV\nX = Xmod1\ny = Ymod1\ncat_features = [\"title\"]\nfrom sklearn.model_selection import KFold\n# oof is an zeroed array of the same size of the input dataset\noof = np.zeros(len(X))\nNFOLDS = 5\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits\/folds, less the test will be impacted by randomness\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\ntraining_start_time = time()\nmodels = []\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n    # each iteration of folds.split returns an array of indexes of the new training data and validation data\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # creates the model\n    clfm = make_classifier1()\n    # fits the model using .loc at the full dataset to select the splits indexes and features used\n    clfm.fit(X.loc[trn_idx, all_features], y.loc[trn_idx], eval_set=(X.loc[test_idx, all_features], y.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)\n    \n    # then, the predictions of each split is inserted into the oof array\n    oof[test_idx] = clfm.predict(X.loc[test_idx, all_features]).reshape(len(test_idx))\n    models.append(clfm)\n    oof1 = np.zeros(len(X.loc[trn_idx, all_features]))\n    oof1 = clfm.predict(X.loc[trn_idx, all_features])\n    print('OOF validation QWK:', qwk(y.loc[test_idx], oof[test_idx]))\n    print('OOF training QWK:', qwk(y.loc[trn_idx], oof1))\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    print('____________________________________________________________________________________________\\n')\n    #break\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(y, oof))\nprint('-' * 30)","5897e897":"from sklearn.metrics import classification_report\nreportcatmodo = classification_report(y, oof)\nprint(reportcatmodo)","bc4473d9":"# make predictions on test set once\npredictions = []\nfor model in models:\n    predictions.append(model.predict(Xmod1test))\npredictions = np.concatenate(predictions, axis=1)\nprint(predictions.shape)\npredictions = stats.mode(predictions, axis=1)[0].reshape(-1)\nprint(predictions.shape)\n#del X_test","0773556c":"predictions","c8756ee1":"# oof is an zeroed array of the same size of the input dataset\nprint('OOF QWK:', qwk(Ymod1test, predictions))","a876aca5":"from sklearn.metrics import classification_report\nreportcatmodo1 = classification_report(Ymod1test, predictions)\nprint(reportcatmodo1)","8f47426e":"oof","9b6e0376":"len(oof)","d744a7ee":"train_labels_exp = train_labels\n","5c0c17d7":"train_labels_exp","db09b642":"for i in range(0, len(train_labels_exp)):\n    train_labels_exp[\"accuracy_group\"][i] = oof[i]\ntrain_labels_exp.groupby([\"accuracy_group\"]).size()","c0489a7d":"train_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')","f79d514f":"train_labels.groupby([\"accuracy_group\"]).size()","f01b0d35":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n\nmodel = Sequential()\n\n# Embedding layer\nmodel.add(\n    Embedding(input_dim=Xmod1test,\n              input_length = training_length,\n              output_dim=len,\n              weights=[embedding_matrix],\n              trainable=False,\n              mask_zero=True))\n\n# Masking layer for pre-trained embeddings\nmodel.add(Masking(mask_value=0.0))\n\n# Recurrent layer\nmodel.add(LSTM(64, return_sequences=False, \n               dropout=0.1, recurrent_dropout=0.1))\n\n# Fully connected layer\nmodel.add(Dense(64, activation='relu'))\n\n# Dropout for regularization\nmodel.add(Dropout(0.5))\n\n# Output layer\nmodel.add(Dense(num_words, activation='softmax'))\n\n# Compile the model\nmodel.compile(\n    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","57a971d2":"predictedmod2120 = clfmode2.predict(Xmod1sub)","b829df3c":"sample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')","73247ba3":"sample_submission2120 = sample_submission","bf83845f":"for i in range(0,len(sample_submission2120)):\n    sample_submission2120[\"accuracy_group\"][i] = int(predictedmod2120[i])","016304fa":"sample_submission2120.to_csv(\"submission.csv\", index=None)","359a6f8c":"sample_submission2120.groupby(\"accuracy_group\").size()","14e49fb3":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport lightgbm as lgb\nimport scipy as sp\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm","cfa546f9":"tqdm.pandas()","46f6f189":"def cv_train(X, y, cv, **kwargs):\n    \"\"\"\n    Author: https:\/\/www.kaggle.com\/xhlulu\/\n    Source: https:\/\/www.kaggle.com\/xhlulu\/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    models = []\n    \n    kf = KFold(n_splits=cv, random_state=2019)\n    \n    for train, test in kf.split(X):\n        x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n        \n        train_set = lgb.Dataset(x_train, y_train)\n        val_set = lgb.Dataset(x_val, y_val)\n        \n        model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], **kwargs)\n        models.append(model)\n        \n        if kwargs.get(\"verbose_eval\"):\n            print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    return models\n\n","1e219f9b":"def cv_predict(models, X):\n    \"\"\"\n    Author: https:\/\/www.kaggle.com\/xhlulu\/\n    Source: https:\/\/www.kaggle.com\/xhlulu\/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    return np.mean([model.predict(X) for model in models], axis=0)","0fe2a93b":"Xmod1\n","0258779a":"Ymod1\n","66af4eb5":"%%time\nX = Xmod1.values\ny = Ymod1.values\nparams = {\n    'learning_rate': 0.01,\n    'bagging_fraction': 0.95,\n    'feature_fraction': 0.2,\n    'max_height': 10,\n    'lambda_l1': 10,\n    'lambda_l2': 10,\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'random_state': 2019\n}\nmodels11 = cv_train(X, y, cv=10, params=params, num_boost_round=5000,\n                  early_stopping_rounds=100, verbose_eval=250)\n","0db2aa05":"X_test11 = Xmod1test.values\ntest_pred11 = cv_predict(models=models11, X=X_test11).argmax(axis=1)","daa82c70":"from sklearn.metrics import classification_report\nreportcatmod2f11 = classification_report(Ymod1test,test_pred11)\nprint(reportcatmod2f11)","0c6e31d8":"# oof is an zeroed array of the same size of the input dataset\nprint('OOF QWK:', qwk(Ymod1test, test_pred11))","e731233a":"test_pred1b1 = cv_predict(models=models11, X=X).argmax(axis=1)","96e2027a":"from sklearn.metrics import classification_report\nreportcatmod2f1b1 = classification_report(y,test_pred1b1)\nprint(reportcatmod2f1b1)","4d1ccbe1":"# oof is an zeroed array of the same size of the input dataset\nprint('OOF QWK:', qwk(y, test_pred1b1))","b7461aa9":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(Xmod1)\nx_test = sc.transform(Xmod1test)","64f3fcbc":"import lightgbm as lgb\nd_train = lgb.Dataset(x_train, label=Ymod1)\nparams = {\n    'learning_rate': 0.01,\n    'bagging_fraction': 0.95,\n    'feature_fraction': 0.2,\n    'max_height': 3,\n    'lambda_l1': 10,\n    'lambda_l2': 10,\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'random_state': 2019\n}\n#params = {}\n#params['learning_rate'] = 0.003\n#params['boosting_type'] = 'gbdt'\n#params['objective'] = 'binary'\n#params['metric'] = 'binary_logloss'\n#params['sub_feature'] = 0.5\n#params['num_leaves'] = 10\n#params['min_data'] = 50\n#params['max_depth'] = 10\nclflgb = lgb.train(params, d_train, 100)","0ff41e62":"#Prediction\ny_pred1=clflgb.predict(x_test)\ny_pred1","c8ee0bc7":"len(Ymod1test)","e93f3d65":"for i in range(0,len(Ymod1test)):\n    if (y_pred1[i][0]>=.25):\n        Y_pred1.append(0)\n    elif (y_pred1[i][1]>=.25):\n        Y_pred1.append(1)\n    elif (y_pred1[i][2]>=.25):\n        Y_pred1.append(2)\n    elif (y_pred1[i][3]>=.25):\n        Y_pred1.append(3)","261aa6ee":"Y_pred1","4a6d2867":"from sklearn.metrics import classification_report\nreportcatmod2l = classification_report(Ymod1test, Y_pred1)\nprint(reportcatmod2l)","27d57222":"## inspired from https:\/\/www.kaggle.com\/xhlulu\/dsb-2019-simple-lgbm-using-aggregated-data","ff912327":"## Simplest","62e0a493":"## LightGBM"}}