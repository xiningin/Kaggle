{"cell_type":{"1f408596":"code","9b68340d":"code","5a0bf522":"code","46f8e756":"code","8aa02304":"code","b22397e5":"code","4f6c2a05":"code","96400d5c":"code","3b350f96":"code","9e924253":"code","e4b5a6b7":"code","5b0000a8":"code","06e70e2a":"code","f906d1aa":"markdown","de3f90b9":"markdown","d7bb63f0":"markdown","df4d1f41":"markdown","1d820c0d":"markdown","b709a9db":"markdown","b4e7c0e5":"markdown","107a48a6":"markdown","4edf56ff":"markdown","f5b09c75":"markdown","f5625cdd":"markdown"},"source":{"1f408596":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b68340d":"from torchvision import datasets, transforms\nroot_dir = \"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\"\nflower_transform = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\nflower_dataset = datasets.ImageFolder(root_dir, transform=flower_transform)","5a0bf522":"### Train Test Parameters ###\n\nbatch_size = 32\nvalid_split = 0.3\nshuffle_dataset = True\nrandom_seed = 42\nflower_dataset.classes","46f8e756":"### Split dataset into train and test ###\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import DataLoader\nimport PIL\n\ndataset_size = len(flower_dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(valid_split * dataset_size))\n\nif shuffle_dataset:\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:],  indices[:split]\n#len(train_indices) #3459\n#len(val_indices) #864\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\n# Data augmentation on training set\ntransform_train = transforms.Compose([\n    transforms.Resize((256,256)),\n    \n    ### Augmentation\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation((30,60), resample=PIL.Image.BILINEAR),\n    transforms.RandomRotation((-60,-30), resample=PIL.Image.BILINEAR),\n    transforms.RandomAffine(degrees=0, translate=(0.1,0.1), scale=None, shear=None, resample=False, fillcolor=0),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5, interpolation=3, fill=0),\n    ###\n    \n    transforms.ToTensor(), \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_loader = DataLoader(datasets.ImageFolder(root_dir, transform=transform_train), batch_size = batch_size, sampler = train_sampler) #109\nvalid_loader = DataLoader(flower_dataset, batch_size = batch_size, sampler = valid_sampler) #27 ","8aa02304":"### Visualize Images in Train and Test ###\nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\n\ntrain_batch, label_train = next(iter(train_loader))\nvalid_batch, label_val = next(iter(train_loader))\n\ndef img_plotter(batch, rows=8, cols=8):\n    fig,axs = plt.subplots(nrows=rows, ncols=cols, figsize=(30,30))\n    for i in range(rows):\n        for j in range(cols):\n            axs[i,j].imshow(batch[rows*i+j].permute(1,2,0))","b22397e5":"### Visualize training images ###\nimg_plotter(train_batch, rows=4, cols=4)","4f6c2a05":"### Visualize validation images ###\nimg_plotter(valid_batch, rows=4, cols=4)","96400d5c":"import torchvision\nfrom torchvision.models import vgg16\nvgg = torchvision.models.vgg16_bn(pretrained=True)","3b350f96":"import torch.nn as nn\nimport torch.nn.functional as F\n    \n# Freeze feature detection parameters so we don't backprop through them\nfor param in vgg.parameters():\n    param.requires_grad = False\n    \n# Change the classification layers  \nvgg.classifier[6] = nn.Linear(in_features=4096,out_features=5,bias=True)\nvgg","9e924253":"from torch.optim import Adam\nimport torch\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nvgg.to(device)\n\nfrom torch.optim import Adam,SGD\ncriterion = nn.CrossEntropyLoss()\noptim = Adam(vgg.parameters(),lr=1e-3,weight_decay=3e-3)","e4b5a6b7":"### Training Loop ####\nn_epochs = 150\nmax_acc=0\ntrain_acc = []\nval_acc = []\n\nfor epoch in range(n_epochs):\n    train_loss = 0\n    val_loss = 0\n    acc = 0.0\n    print(\"Training....\")\n    vgg.train()\n    \n    for batch_num,(batch,labels) in enumerate(train_loader):\n        inp,target = batch.to(device),labels.to(device)\n        optim.zero_grad()\n        output = vgg.forward(inp)\n        \n        op = F.softmax(output,dim=1)\n        \n        final_op = torch.argmax(op,dim=1)\n        \n        acc += torch.sum(final_op==target).item()\/len(target)\n        loss = criterion(output,target)\n        \n        loss.backward()\n        optim.step()\n        \n        train_loss+=(loss.item()\/len(batch))\n        if batch_num%50 ==0 and batch_num!=0:\n            print(\"TARGET: \",target)\n            print(\"OUTPUT: \",final_op)\n            print(\"Accuracy after \",batch_num,\"steps: \",acc\/batch_num)\n        \n    \n    acc = acc\/len(train_loader)\n    train_acc.append(acc)\n    print(\"Epoch: \",epoch,\"Loss: \",train_loss,\" Accuracy: \",acc)\n    \n    eval_acc = 0\n         \n    # set model to evaluation mode\n    \n    # Turn off gradients for validation, saves memory and computations\n    with torch.no_grad():\n        vgg.eval()\n        print(\"Validating.....\")\n\n        for batch in valid_loader:\n            inp,target = batch[0].to(device),batch[1].to(device)\n            op = F.softmax(vgg.forward(inp))\n            final_op = torch.argmax(op,dim=1)\n\n            eval_acc += np.sum(final_op.detach().cpu().numpy()==target.detach().cpu().numpy())\/len(target)\n        \n    print(\"Validation accuracy: \",eval_acc\/len(valid_loader))\n    val_acc.append(eval_acc\/len(valid_loader))\n    if eval_acc>max_acc:\n        max_acc = eval_acc\n        torch.save(vgg,\"vgg2.pt\")\n    #print(\"FOP\",final_op)\n    #print(\"TARGET\",target)","5b0000a8":" print(\"MAX ACCURACY:\", max_acc\/len(valid_loader))","06e70e2a":"from numpy import random\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nfigure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\n\nx = list(range(1, n_epochs+1))\n\nplt.plot(x, train_acc, label=\"Training Accuracy\")\n\nplt.plot(x, val_acc, label=\"Validation Accuracy\")\n\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.title('VGG16-Net (Pretrained=True)')\n\nplt.legend()\n\nplt.show()","f906d1aa":"# 3. Define parameters","de3f90b9":"# 7. Train VGG from scratch","d7bb63f0":"# 6. Plot available images","df4d1f41":"Define parameters","1d820c0d":"# 4. Define samplers for sampling data little by little","b709a9db":"# 1. Import required libraries","b4e7c0e5":"# 8. Plot the result","107a48a6":"Train the vgg model","4edf56ff":"Create a model","f5b09c75":"# 5. Define image plotting fucntion","f5625cdd":"# 2. Make Pytorch dataset for fetching images"}}