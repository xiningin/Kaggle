{"cell_type":{"8e7d865e":"code","1c6d8a22":"code","416db792":"code","0385efec":"code","7c0d19b4":"code","2c5366a3":"code","a503bbfc":"code","c734fc41":"code","43a3691b":"code","7abf868d":"code","910a313e":"code","fce36f9d":"code","1fc63581":"code","990a605b":"code","2601532b":"code","6d91422b":"code","a04886b5":"code","d11f17b6":"code","e9688ea2":"code","91f314a8":"code","598edd09":"code","55b48e6b":"code","fdae6ade":"code","63bc9452":"code","d45f4688":"code","e4360655":"code","0a282353":"code","d9599bba":"code","0e66382d":"code","f7811dac":"code","8bfd0831":"code","dd61ce1b":"code","76c20217":"code","f1a862ef":"code","21dd1409":"code","487b673d":"code","34467602":"code","b85d0ee1":"code","b1c5be1c":"code","58a54250":"code","efae002e":"code","874b9eb3":"code","788af1e6":"code","7dadff48":"code","47771763":"code","7449f340":"code","88238acc":"code","e5593ddf":"code","3cfb5478":"code","f01be366":"code","a3dce2de":"code","7cac80a1":"code","7c89e2c5":"code","3a1b0ef3":"code","25e5e4e1":"code","59afbddd":"code","4c2c104e":"code","d0baee31":"code","d06ec8f7":"code","b2482e71":"code","6bbabf7b":"code","ce765edc":"code","00776b6f":"code","658f2e89":"code","d4182b80":"code","95ab8972":"code","b8cc8198":"code","527325a7":"code","0296daf7":"code","aa9e4dcd":"code","5175b767":"code","793d1716":"code","992b4e81":"code","22a27d82":"code","f1c75983":"code","480ecbd8":"code","779d4d9d":"code","508470a5":"code","2d534643":"code","9bdc1e43":"code","05279746":"code","1c60d550":"code","03d100bc":"code","2f2a5190":"code","477aaa63":"code","4442ba77":"code","b3cb9eaf":"code","7855f5b8":"code","40ec2696":"code","a51a8ed5":"code","628dcb36":"code","3a9cdafb":"code","d332ef69":"code","7c719465":"code","d53171c9":"code","79902e6d":"markdown","abefc835":"markdown","71f8b158":"markdown","928dd292":"markdown","edf48d24":"markdown","d54413a5":"markdown","945d53a9":"markdown","4bd2725d":"markdown","aeed4bfb":"markdown","5c8030f1":"markdown","86836943":"markdown","c63fcc7a":"markdown","9f36277e":"markdown","0a362034":"markdown","c27f30e5":"markdown","09595bd1":"markdown","14e0ad8a":"markdown","4dd53a29":"markdown","5729b95c":"markdown","e98a6e49":"markdown","a21f3d5b":"markdown","90771188":"markdown","cf7bbe46":"markdown","80fda7d5":"markdown","06f10485":"markdown","bc232107":"markdown","dc4e70be":"markdown","75c1f05b":"markdown","a06f8ecf":"markdown","7f5a9d80":"markdown","20f9865e":"markdown","b766dabc":"markdown","2f7f4347":"markdown","0b248e3c":"markdown","4472e043":"markdown","018d7fac":"markdown","e0a24815":"markdown"},"source":{"8e7d865e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c6d8a22":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport missingno as msno\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport math\nfrom scipy import interpolate\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport time\nfrom statistics import mean\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFECV\nfrom sklearn import model_selection","416db792":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","0385efec":"# take look our data frame\ntrain_df.head()","7c0d19b4":"# let's look from bottom\ntrain_df.tail()","2c5366a3":"#Check data type\ntrain_df.dtypes","a503bbfc":"msno.bar(train_df,color=\"dodgerblue\", sort=\"ascending\", figsize=(10,5), fontsize=12)","c734fc41":"#let's look at list if sum of null values\ntrain_df.isnull().sum()","43a3691b":"# we can look at our missing value with matris that is provide us to understand those missing values are rondom or not\nmsno.matrix(train_df)\n","7abf868d":"# Before the look correlations for missing values we should convert the a few columns that are not numerical and might have more corretion with missings.\n\ntrain_df['Sex'] = train_df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n\ntrain_df.head()\n\n","910a313e":"#Let's look at correlation between ages.\ntrain_df.corr()","fce36f9d":"# Filling of Embarked Column\ntrain_df[train_df['Embarked'].isna()] #Passengers travelling together\n\nsub_embarked = train_df[(train_df['Fare'] > 79) & (train_df['Fare'] < 81) & (train_df['Pclass'] == 1)]\nfill_mode = sub_embarked[\"Embarked\"].mode()[0]\n\n\ntrain_df = train_df.fillna({'Embarked': fill_mode})","1fc63581":"# Drop cabin column\ntrain_df = train_df.drop(['Cabin'], axis=1)\n# Also we can drop ticket column now because we will not use future analyza and predict.\ntrain_df = train_df.drop([\"Ticket\"], axis=1)","990a605b":"# Let's create one matrix for guess based on Sex and Pclass column.\nags = np.zeros((2,3))\nags","2601532b":"# We created function we will use also test data in Applying Several ML Algorithms section.\ndef find_ages(df):\n    for i in range(0, 2):\n        for j in range(0, 3):\n            g_df = df[(df['Sex'] == i) & (df['Pclass'] == j+1)]['Age'].dropna()\n\n\n            guess = g_df.median()\n\n            # Convert random age float to nearest .5 age\n            ags[i,j] = int( guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            df.loc[ (df.Age.isnull()) & (df.Sex == i) & (df.Pclass == j+1),'Age'] = ags[i,j]\n\n    df['Age'] = df['Age'].astype(int)\n","6d91422b":"#Call function with train data set\nfind_ages(train_df)","a04886b5":"# Check to missing values again\ntrain_df.info()","d11f17b6":"pd.crosstab([train_df.Embarked,train_df.Pclass],[train_df.Sex,train_df.Survived],margins=True).style.background_gradient(cmap='summer_r')","e9688ea2":"pd.crosstab([train_df.Sex,train_df.Survived],train_df.Pclass,margins=True).style.background_gradient(cmap='summer_r')","91f314a8":"def age_buckets(x): \n    if x < 18: return '0-18' \n    elif x < 30: return '18-29'\n    elif x < 40: return '30-39' \n    elif x < 50: return '40-49' \n    elif x < 60: return '50-59' \n    elif x < 70: return '60-69' \n    elif x >=70: return '70+' \n    else: return 'other'\n    \n    \ntrain_df[\"Age\"] = train_df[\"Age\"].astype(int)\n\ntrain_df[\"Age_Range\"] = train_df[\"Age\"].apply(lambda x: age_buckets(x))\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,8))  \n\ngrouped_by_age_female = train_df[train_df[\"Sex\"] == 1].groupby([\"Age_Range\"])[\"Survived\"].value_counts().unstack()\ngrouped_by_age_female.plot.bar(stacked=True, color=['#99CCFF', '#BCE2C8'], rot=0,ax=ax1, title=\"Number of female survived\/drowned passengers age group\")\nax1.legend(('Drowned', 'Survived'))\n\ngrouped_by_age_men = train_df[train_df[\"Sex\"] == 0].groupby([\"Age_Range\"])[\"Survived\"].value_counts().unstack()\ngrouped_by_age_men.plot.bar(stacked=True, color=['#99CCFF', '#BCE2C8'], rot=0,ax=ax2, title=\"Number of male survived\/drowned passengers per age group\")\nax2.legend(('Drowned', 'Survived'))","598edd09":"fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,8)) \n\ngrouped_by_family = train_df.groupby([\"Parch\"])[\"Survived\"].value_counts()\ngrouped_by_family.unstack().plot.bar(stacked=True, color=['#99CCFF', '#BCE2C8'], rot=0,ax=ax1, title=\"Number of survived\/drowned passengers per number of parents\/children on board\")\nplt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\nax1.legend(('Drowned', 'Survived'))\n\ngrouped_by_family_norm = train_df.groupby([\"Parch\"])[\"Survived\"].value_counts(normalize=True)\ngrouped_by_family_norm.unstack().plot.bar(stacked=True, color=['#99CCFF', '#BCE2C8'], rot=0,ax=ax2, title=\"Proportion of survived\/drowned passengers per number of parents\/children on board\")\nax2.legend(('Drowned', 'Survived'))","55b48e6b":"grouped_by_sibsp = train_df.groupby('SibSp')['Survived'].value_counts(normalize=True).unstack()\ngrouped_by_sibsp.plot(kind='bar', color=[\"#99CCFF\", \"#BCE2C8\"], stacked=True, rot=0, figsize=(10,8), title=\"Number of survived\/drowned passengers per number of siblings\/spouses on board\")\nplt.legend(( 'Drowned', 'Survived'))\n\nplt.xlabel('Number of siblings\/spouses')\nplt.ylabel('%')\nplt.show()","fdae6ade":"#Sex and Embarked columns to check if there's any correlation between them and the Survived data\n\ntrain_df['Sex_data'] = train_df['Sex'].map({'male': 1,'female': 0})\ntrain_df['Embarked_data'] = train_df['Embarked'].map({'S': 0,'C': 1, 'Q': 2})\ntrain_df.corr()","63bc9452":"sns.heatmap(train_df.corr(), cmap='icefire')\nplt.title('Correlation', fontsize=24)\n#In our analysis we see a low correlation between embarked and sex.","d45f4688":"#Visualization of 'Survived' (Target column)\ntrain_df.Survived.value_counts()","e4360655":"train = train_df.Survived.value_counts().plot(kind='bar')\ntrain.set_xlabel('Survived or not')\ntrain.set_ylabel('Passenger Count')\n","0a282353":"train = train_df[['Pclass', 'Survived']].groupby('Pclass').mean().Survived.plot(kind='bar')\ntrain.set_xlabel('Pclass')\ntrain.set_ylabel('Survival Probability')","d9599bba":"#Survival per Age\/Sex\ngrouped_by_sex = train_df.groupby([\"Sex\"])[\"Survived\"].value_counts()\ngrouped_by_sex.unstack().plot.bar(stacked=True, color=['#99CCFF', '#BCE2C8'], rot=0)","0e66382d":"# Let's read Test Data and look our train data that we edited in EDA section.\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_df.head()","f7811dac":"train_df","8bfd0831":"# Delete some columns that are we will not use\ntrain_df = train_df.drop(['Sex_data'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)","dd61ce1b":"# Let's look at missings columns for test data \ntest_df.info()","76c20217":"# Call our function to fill age column as the same way\ntest_df['Sex'] = test_df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\nfind_ages(test_df)","f1a862ef":"# Get the null value line to know which class that passenger have.\ntest_df[test_df.isna().any(axis=1)].head()","21dd1409":"# Fill fare column that we had one missing value.\ndf_new = test_df[test_df[\"Pclass\"] == 3]\nfare_median = df_new[\"Fare\"].median()\n\ntest_df[\"Fare\"] = test_df[\"Fare\"].fillna(fare_median) ","487b673d":"# Look at null value as we can see we handled it.\ntest_df.info()","34467602":"# Add to title columns it will effect our model\ndef add_title(df):\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n\nadd_title(train_df)\nadd_title(test_df)\n","b85d0ee1":"# Make numerical value for title \ndef num_title(df):\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\n    df['Title'] = df['Title'].map(title_mapping)\n    df['Title'] = df['Title'].fillna(0)\n\nnum_title(train_df)\nnum_title(test_df)","b1c5be1c":"#Let's look at corr between title and survived\ntrain_df.corr()\n\n# As we can see there is too much corr with it.\n","58a54250":"# Now we can delete name column\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","efae002e":"# We should creat age band get better predict.\n\ndef age_range(df):\n    df.loc[ df['Age'] <= 16, 'Age'] = 0\n    df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1\n    df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2\n    df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3\n    df.loc[ df['Age'] > 64, 'Age']\n\nage_range(train_df)\nage_range(test_df)\ntrain_df = train_df.drop(['Age_Range'], axis=1)\n","874b9eb3":"train_df.head()\n","788af1e6":"test_df.head()","7dadff48":"# We are preparing our both data to apply ML models.\ntrain_df = train_df.drop([\"Embarked\"], axis=1)","47771763":"# Let's make embarked numerical value for test data set\ntest_df['Embarked_data'] = test_df['Embarked'].map({'S': 0,'C': 1, 'Q': 2})","7449f340":"test_df = test_df.drop([\"Embarked\"], axis=1)","88238acc":"# We also do not need passengerid in train data for predict \ntrain_df = train_df.drop([\"PassengerId\"], axis=1)\n","e5593ddf":"# We can create fareband before to create range we sholud look how many band we can create?\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n# I tried to 4,5,6 ranges but after for survived rate near to similar to others so we should choose 4 bands.","3cfb5478":"# We wrote function that create new\ndef create_fare(df):\n    df.loc[ df['Fare'] <= 7.91, 'Fare'] = 0\n    df.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1\n    df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare']   = 2\n    df.loc[ df['Fare'] > 31, 'Fare'] = 3\n    df['Fare'] = df['Fare'].astype(int)\n\ncreate_fare(train_df)\ncreate_fare(test_df)\ntrain_df = train_df.drop(['FareBand'], axis=1)\n\n    \ntrain_df.head(10)","f01be366":"train_df.corr()","a3dce2de":"test_df.head()","7cac80a1":"X = train_df.drop(\"Survived\", axis=1)\nY = train_df[\"Survived\"]","7c89e2c5":"\nX_tr, X_ts, Y_tr, Y_ts =  train_test_split(X,Y, test_size=0.2, random_state=1) \n#Feature scaling\nsc_X = StandardScaler()\nX_tr = pd.DataFrame(sc_X.fit_transform(X_tr),columns = X_tr.columns)\nX_ts = pd.DataFrame(sc_X.fit_transform(X_ts),columns = X_ts.columns)\n","3a1b0ef3":"# To find the best value for K we iterated through the range 1 to 40 \n# We choose K = 3 that gave us the highest accuracy.\nlist_neighbors = {}\n\nfor i in range(1,40):\n\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_tr, Y_tr)\n    accuracy_knn = round(knn.score(X_tr, Y_tr) * 100, 2)\n    list_neighbors[i] = accuracy_knn\n    \n    \nmax_key = max(list_neighbors, key=list_neighbors.get)\nall_values = list_neighbors.values()\nmax_value_knn = max(all_values)\nY_pred_knn = knn.predict(X_ts)\nprint(\"The max accuracy is % \" + str(max_value_knn) + \" with \" + str(max_key) + \" neighbors.\")\n\n","25e5e4e1":"# Plot accuracy according to K-values.\nplt.style.use('ggplot')\nplt.subplots(figsize = (15,8))\naccuracies = []\nfor k in range(1,40):\n    accuracies.append(list_neighbors[k])\nplt.plot(range(1,40), accuracies , linewidth=3 , linestyle = 'dashdot', color = 'black', marker = 'p',markersize=9, markerfacecolor = 'red')\n\nplt.xlabel('K-value',fontsize=14)\nplt.ylabel('Accuracy',fontsize=14)\nplt.title('Accuracy vs K-value', fontsize=20)","59afbddd":"# Calculating the accuracy score of our training dataset by using the Logistic regression Model.\nlogreg = LogisticRegression()\nlogreg.fit(X_tr, Y_tr)\naccuracy_log = round(logreg.score(X_tr, Y_tr) * 100, 2)\nY_pred_logreg = logreg.predict(X_ts)\nprint(\"The accuracy is: \" + str(accuracy_log))","4c2c104e":"# Calculating the accuracy score of our training dataset by using the Logistic regression Model.\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_tr, Y_tr)\naccuracy_lda = round(lda.score(X_tr, Y_tr) * 100, 2)\nY_pred_lda = lda.predict(X_ts)\nprint(\"The accuracy is: \" + str(accuracy_lda))\n","d0baee31":"# We used plot-bar to see compare different obtain accuracy.\nlabel = ['KNN','Logreg','Lda']\nlist_accuracy = [max_value_knn,accuracy_log,accuracy_lda]\ndef plot_bar_x():\n    # this is for plotting purpose\n    index = np.arange(len(label))\n    plt.bar(index, list_accuracy)\n    plt.xlabel('Models', fontsize=9)\n    plt.ylabel('Percentage % ', fontsize=9)\n    plt.xticks(index, label, fontsize=10, rotation=30)\n    plt.title('Compare Models')\n    plt.show()\n    \nplot_bar_x()","d06ec8f7":"# Confusion Matrix\ndef conf_mat(Y_predict,model_name):\n    ax = plt.axes()\n    cf = pd.crosstab(Y_ts, Y_predict, rownames=['Actual'], colnames=['Predicted'])\n    sns.heatmap(cf, annot=True)\n    ax.set_title(str(model_name)+' Confusion Matrix')\n    plt.show()\n","b2482e71":"conf_mat(Y_pred_knn,\"KNN\")","6bbabf7b":"conf_mat(Y_pred_logreg,\"Logreg\")","ce765edc":"conf_mat(Y_pred_lda,\"Lda\")","00776b6f":"list_neighbors_train = {}\nlist_neighbors_val = {}\n\nfor i in range(1,40):\n\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_tr, Y_tr)\n    \n    #IMPORATNT training and validation data\n    y_pred_knn_train = knn.predict(X_tr)\n    accuracy_knn_train = accuracy_score(Y_tr,y_pred_knn_train)\n    \n    y_pred_knn_val = knn.predict(X_ts)\n    accuracy_knn_val = accuracy_score(Y_ts,y_pred_knn_val)\n        \n    #accuracy_knn = round(knn.score(X_tr, Y_tr) * 100, 2)\n    list_neighbors_train[i] = accuracy_knn_train\n    list_neighbors_val[i] = accuracy_knn_val\n    \n    \nmax_key_train = max(list_neighbors_train, key=list_neighbors_train.get)\nall_values_train = list_neighbors_train.values()\nmax_value_knn_train = max(all_values_train)\nprint(\"The max training accuracy is % \" + str(max_value_knn_train) + \" with \" + str(max_key_train) + \" neighbors.\")\n\nmax_key_val = max(list_neighbors_val, key=list_neighbors_val.get)\nall_values_val = list_neighbors_val.values()\nmax_value_knn_val = max(all_values_val)\nprint(\"The max validation accuracy is % \" + str(max_value_knn_val) + \" with \" + str(max_key_val) + \" neighbors.\")","658f2e89":"logreg = LogisticRegression()\nlogreg.fit(X_tr, Y_tr)\n\n#IMPORATNT training and testing data\ny_pred_logreg_train = logreg.predict(X_tr)\naccuracy_logreg_train = accuracy_score(Y_tr,y_pred_logreg_train)\n\ny_pred_logreg_val = logreg.predict(X_ts)\naccuracy_logreg_val = accuracy_score(Y_ts,y_pred_logreg_val)\n    \nprint(\"The training accuracy is % \" + str(accuracy_logreg_train))\nprint(\"The validation accuracy is % \" + str(accuracy_logreg_val))","d4182b80":"lda = LinearDiscriminantAnalysis()\nlda.fit(X_tr, Y_tr)\n\n#IMPORATNT training and testing data\ny_pred_lda_train = lda.predict(X_tr)\naccuracy_lda_train = accuracy_score(Y_tr,y_pred_lda_train)\n\ny_pred_lda_val = lda.predict(X_ts)\naccuracy_lda_val = accuracy_score(Y_ts,y_pred_lda_val)\n    \nprint(\"The training accuracy is % \" + str(accuracy_lda_train))\nprint(\"The validation accuracy is % \" + str(accuracy_lda_val))","95ab8972":"label = ['KNN_Train','KNN_Val','Logreg_Train','Logreg_Val','Lda_Train','Lda_Val']\nlist_accuracy = [accuracy_knn_train,accuracy_knn_val,accuracy_logreg_train,accuracy_logreg_val,accuracy_lda_train,accuracy_lda_val]\ndef plot_bar_x():\n    # this is for plotting purpose\n    index = np.arange(len(label))\n    plt.bar(index, list_accuracy)\n    plt.xlabel('Models', fontsize=9)\n    plt.ylabel('Percentage % ', fontsize=9)\n    plt.xticks(index, label, fontsize=10, rotation=30)\n    plt.title('Compare Models with Test Data and Train Data')\n    plt.show()\n    \nplot_bar_x()","b8cc8198":"# We changed size of test from %10 to %90 and we got accuracies for each diffrent sample size.\nknn_accs=[]\nlog_accs=[]\nlda_accs=[]\nfor j in range(9):\n    size = (j+1)\/10\n    X_tr, X_ts, Y_tr, Y_ts =  train_test_split(X,Y, test_size=size, random_state=1) \n    #Feature scaling\n    sc_X = StandardScaler()\n    X_tr = pd.DataFrame(sc_X.fit_transform(X_tr),columns = X_tr.columns)\n    X_ts = pd.DataFrame(sc_X.fit_transform(X_ts),columns = X_ts.columns)\n    #Knn\n    knn = KNeighborsClassifier(n_neighbors = 3)\n    knn.fit(X_tr, Y_tr)\n    Y_pred_knn = knn.predict(X_ts)\n    accuracy_knn = round(knn.score(X_tr, Y_tr) * 100, 2)\n    knn_accs.append(accuracy_knn)\n    print(\"Sample size of test for KNN  :\" + str(size)+ \", Accuracy is : \" + str(accuracy_knn)  )\n    #Logreg\n    logreg = LogisticRegression()\n    logreg.fit(X_tr, Y_tr)\n    Y_pred = logreg.predict(X_ts)\n    accuracy_log = round(logreg.score(X_tr, Y_tr) * 100, 2)\n    log_accs.append(accuracy_log)\n    print(\"Sample size of test for Log  :\" + str(size)+ \", Accuracy is : \" + str(accuracy_log)  )\n    #Lda\n    lda = LinearDiscriminantAnalysis()\n    lda.fit(X_tr, Y_tr)\n    Y_pred = lda.predict(X_ts)\n    accuracy_lda= round(lda.score(X_tr, Y_tr) * 100, 2)\n    lda_accs.append(accuracy_lda)\n    print(\"Sample size of test for Lda  :\" + str(size)+ \", Accuracy is : \" + str(accuracy_lda)  )","527325a7":"# This Function is plotting according to model for to see sample size effects.\n\ndef plot_sample(model,accs):\n    \n    plt.style.use('ggplot')\n    plt.subplots(figsize = (15,8))\n    sample = [ ]\n    for i in range(9):\n        sample.append((i+1)\/10)\n\n    plt.plot(sample, accs , linewidth=3 , linestyle = 'dashed', color = 'gray', marker = 'D',markersize=9, markerfacecolor = 'blue')\n    title = 'Accuracy vs Sample Size on '+ model\n    plt.xlabel('Sample Size',fontsize=14)\n    plt.ylabel('Accuracy',fontsize=14)\n    plt.title(title, fontsize=20)","0296daf7":"plot_sample(\"KNN\",knn_accs)","aa9e4dcd":"plot_sample(\"Logreg\",log_accs)","5175b767":"plot_sample(\"Lda\",lda_accs)","793d1716":"# We removed survived column to look other features.\ncolumns = []\ntrain_df_new = train_df.copy()\nfor col in train_df_new.columns: \n    columns.append(col)\ncolumns.pop(0)\ncolumns","992b4e81":" \na_knn = {}\na_log = {}\na_lda = {}\nfor i in columns:\n    train_df_1 = train_df.copy()\n    i = str(i)\n    train_df_1.drop(i, axis=1, inplace=True )\n    X = train_df_1.drop(\"Survived\", axis=1)\n    Y = train_df_1[\"Survived\"]\n    \n    X_tr, X_ts, Y_tr, Y_ts =  train_test_split(X,Y, test_size=0.2, random_state=1) \n    #Feature scaling\n    sc_X = StandardScaler()\n    X_tr = pd.DataFrame(sc_X.fit_transform(X_tr),columns = X_tr.columns)\n    X_ts = pd.DataFrame(sc_X.fit_transform(X_ts),columns = X_ts.columns)\n    \n    #Knn\n    knn = KNeighborsClassifier(n_neighbors = 3)\n    knn.fit(X_tr, Y_tr)\n    accuracy_knn = round(knn.score(X_tr, Y_tr) * 100, 2)\n    a_knn[i]=(87.22-accuracy_knn)\n    #Logreg\n    logreg = LogisticRegression()\n    logreg.fit(X_tr, Y_tr)\n    accuracy_log = round(logreg.score(X_tr, Y_tr) * 100, 2)\n    a_log[i] = (80.62-accuracy_log)\n    #Lda\n    lda = LinearDiscriminantAnalysis()\n    lda.fit(X_tr, Y_tr)\n    accuracy_lda= round(lda.score(X_tr, Y_tr) * 100, 2)\n    a_lda[i] = (80.48-accuracy_lda)\n","22a27d82":"# Ploting accuracy of models with respect to the features.\n\nfig, ((ax1,ax2,ax3)) = plt.subplots(1, 3,figsize=(25, 5), sharex=True, sharey=True)\n\nax1.bar(a_knn.keys(), a_knn.values())\nax2.bar(a_log.keys(), a_log.values())\nax3.bar(a_lda.keys(), a_lda.values())\nax1.title.set_text('KNN Indicators')\nax2.title.set_text('Logreg Indicators')\nax3.title.set_text('Lda Indicators')\n\nplt.show()","f1c75983":"# As we can see at below our indicators are four column: Pclass, Sex, Sibsp, and Title.\n# Now we will look at accuracy just with 4 columns.\n\ntrain_df_2 = train_df.copy()\ntrain_df_2.drop([\"Age\"], axis=1, inplace=True )\ntrain_df_2.drop([\"Parch\"], axis=1, inplace=True )\ntrain_df_2.drop([\"Fare\"], axis=1, inplace=True )\ntrain_df_2.drop([\"Embarked_data\"], axis=1, inplace=True )\nX = train_df_2.drop(\"Survived\", axis=1)\nY = train_df_2[\"Survived\"]\n#KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_tr, Y_tr)\nY_pred_knn = knn.predict(X_ts)\naccuracy_knn = round(knn.score(X_tr, Y_tr) * 100, 2)\n#Logreg\nlogreg = LogisticRegression()\nlogreg.fit(X_tr, Y_tr)\nY_pred = logreg.predict(X_ts)\naccuracy_log = round(logreg.score(X_tr, Y_tr) * 100, 2)\n#Lda\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_tr, Y_tr)\nY_pred = lda.predict(X_ts)\naccuracy_lda= round(lda.score(X_tr, Y_tr) * 100, 2)\n\n\nprint(\"Knn accuracy score on the train data: \" + str(accuracy_knn))\nprint(\"Logreg accuracy score on the train data: \" + str(accuracy_log))\nprint(\"Lda accuracy score on the train data: \" + str(accuracy_lda))","480ecbd8":"\nfrom sklearn.model_selection import learning_curve\ndef plot_curve(clf,title):\n\n    train_sizes,train_scores,test_scores = learning_curve(clf,X,Y,random_state = 1,cv = 5)\n\n    plt.figure()\n    plt.title(title)\n\n    ylim = (0.7, 1.01)\n    if ylim is not None:\n        plt.ylim(*ylim)\n\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                train_scores_mean + train_scores_std, alpha=0.1,\n                color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n        label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n        label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.show()\n\nplot_curve(LogisticRegression(),'Learning Curve of Logistic Regression')\nplot_curve(KNeighborsClassifier(),'Learning Curve of KNN')\nplot_curve(LinearDiscriminantAnalysis(),'Learning Curve of LinearDiscriminantAnalysis')\n","779d4d9d":"#Recursive Feature Elimination\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFECV\n\nrfe = RFE(estimator=logreg,step=1)\nrfe = rfe.fit(X_tr,Y_tr)\n\nselected_rfe_features = pd.DataFrame({'Feature':list(X_tr.columns), 'Ranking': rfe.ranking_})\nselected_rfe_features.sort_values(by='Ranking')\nprint(selected_rfe_features)\n\n\n# Redefine  X and Y RFE\nX_train_rfe = rfe.transform(X_tr)\nX_test_rfe = rfe.transform(X_ts)\n\n\n#Logistic Regression RFE\nlogreg_rfe_model = logreg.fit(X_train_rfe,Y_tr)\naccuracy_log_rfe = round(accuracy_score(Y_ts,logreg_rfe_model.predict(X_test_rfe))* 100, 2)\nprint(\"Log Accuracy: \" +str(accuracy_log_rfe))\n\n\n\n#KNN RFE\nknn_rfe_model = knn.fit(X_train_rfe,Y_tr)\naccuracy_knn_rfe = round(accuracy_score(Y_ts,knn_rfe_model.predict(X_test_rfe))* 100, 2)\nprint(\"KNN Accuracy: \" +str(accuracy_knn_rfe))\n\n\n#LDA RFE\nlda_rfe_model = lda.fit(X_train_rfe,Y_tr)\naccuracy_lda_rfe = round(accuracy_score(Y_ts,lda_rfe_model.predict(X_test_rfe))* 100, 2)\nprint(\"LDA Accuracy: \" +str(accuracy_lda_rfe))","508470a5":"results = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression','LDA'],\n    'Score': [accuracy_knn, accuracy_log, accuracy_lda],\n    'Score_rfe': [accuracy_knn_rfe, accuracy_log_rfe, accuracy_lda_rfe]})\n\nresults.head(9)","2d534643":"# Decision Tree\ndef dtree_scores(dtree):\n    dtree.fit(X_tr,Y_tr)\n    y_dtree_train_pred = dtree.predict(X_tr)\n    y_dtree_test_pred = dtree.predict(X_ts)\n\n    print(f'Train score {accuracy_score(y_dtree_train_pred,Y_tr)}')\n    print(f'Test score {accuracy_score(y_dtree_test_pred,Y_ts)}')\n\n    plot_confusionmatrix(y_dtree_train_pred,Y_tr,dom='Train')\n    plot_confusionmatrix(y_dtree_test_pred,Y_ts,dom='Test')","9bdc1e43":"# Helper Function Decicion Tree\ndef plot_decicion_tree(model_tree):\n    plt.figure(figsize=(20,20))\n    features = list(train_df.drop(['Survived'], axis=1))\n    classes = ['Died','Survived']\n    tree.plot_tree(model_tree,feature_names=features,class_names=classes,filled=True)\n    plt.show()","05279746":"# Helper Function\ndef plot_confusionmatrix(y_train_pred,Y_tr,dom):\n    print(f'{dom} Confusion matrix')\n    cf = confusion_matrix(y_train_pred,Y_tr)\n    sns.heatmap(cf,annot=True,cmap='Blues', fmt='g')\n    plt.tight_layout()\n    plt.show()","1c60d550":"dtree = DecisionTreeClassifier()\ndtree_scores(dtree)","03d100bc":"# Plot decision tree\nplot_decicion_tree(dtree)","2f2a5190":"## Decision Tree + Pre-pruning\n\n\nparams = {'max_depth': [2,4,6],   ##maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n         'min_samples_split': [2,3,4,5,6],\n         'min_samples_leaf': [1,2,3,4]}\n\nclf = tree.DecisionTreeClassifier()\ngcv = GridSearchCV(estimator=clf,param_grid=params)\ngrid_result = gcv.fit(X_tr,Y_tr)\n# show best params chosen\nbest_params = grid_result.best_params_\nprint(best_params)","477aaa63":"dtree_pre_pruned = gcv.best_estimator_\ndtree_scores(dtree_pre_pruned)","4442ba77":"## Plot decision tree pre pruning\nplot_decicion_tree(dtree_pre_pruned)","b3cb9eaf":"## Cost complexity aka weakest link pruning\npath = clf.cost_complexity_pruning_path(X_tr, Y_tr)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nprint(ccp_alphas)","7855f5b8":"# For each alpha we will append our model to a list\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_tr, Y_tr)\n    clfs.append(clf)","40ec2696":"# We will remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nplt.scatter(ccp_alphas,node_counts)\nplt.scatter(ccp_alphas,depth)\nplt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle=\"steps-post\")\nplt.plot(ccp_alphas,depth,label='depth',drawstyle=\"steps-post\")\nplt.legend()\nplt.show()","a51a8ed5":"dtree_post_pruned = DecisionTreeClassifier(random_state=0,ccp_alpha=0.005)\n\ndtree_scores(dtree_post_pruned)","628dcb36":"## Plot decision tree post pruning\nplot_decicion_tree(dtree_post_pruned)","3a9cdafb":"\nX = train_df.drop(\"Survived\", axis=1)\nY = train_df[\"Survived\"]\n\ndef sample_size_time():\n    avg_list_time = []\n    avg_list_accs = []\n    total_time = 0 \n    total_accuracy = 0 \n    for h in range(9):\n        total_accuracy = 0\n        total_time = 0 \n        for i in range(10):\n            size = (h+1)\/10\n            X_tr, X_ts, Y_tr, Y_ts =  train_test_split(X,Y, test_size=size, random_state=1) \n            sc_X = StandardScaler()\n            X_tr = pd.DataFrame(sc_X.fit_transform(X_tr),columns = X_tr.columns)\n            X_ts = pd.DataFrame(sc_X.fit_transform(X_ts),columns = X_ts.columns)\n            start = time.time()\n            gaussian = GaussianNB()\n            end = time.time()\n            total_time += (end-start)\n\n\n            gaussian.fit(X_tr, Y_tr)\n            Y_pred = gaussian.predict(X_ts)\n            accuracy_gaussian = round(gaussian.score(X_tr, Y_tr) * 100, 2)\n            total_accuracy +=  accuracy_gaussian\n\n            \n        avg_list_time.append((total_time\/10))\n        avg_list_accs.append(((total_accuracy)\/10))\n\n    #Ploting....\n    print(avg_list_time)\n    fig, ax1 = plt.subplots(figsize = (15,8))\n    plt.style.use('ggplot')\n    plt.title(' Time and Accuracy Chart According to Sample Size ', fontsize=22)\n    sample = [ ]\n    for i in range(9):\n        sample.append((i+1)\/10)\n    color = 'tab:red'\n    ax1.set_xlabel('Sample Size',fontsize=18)\n    ax1.set_ylabel('Computing Time',fontsize=18, color=color)\n    ax1.plot(sample, avg_list_time, linewidth=4 , linestyle = 'dashed', color = color, marker = 'v',markersize=10, markerfacecolor = 'white')\n    ax1.tick_params(axis='y', labelcolor=color)\n\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\n\n    ax2.set_ylabel('Accuracy', color='green',fontsize=18)  # we already handled the x-label with ax1\n    ax2.plot(sample, avg_list_accs ,  linewidth=4 , linestyle = 'dashed', color = 'green', marker = '^',markersize=10, markerfacecolor = 'white')\n    ax2.tick_params(axis='y', labelcolor='green')\n\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    plt.show()\nsample_size_time()","d332ef69":"# Let's get same grapgh with our indicators.<We have already created new df for our indicators>\nX = train_df_2.drop(\"Survived\", axis=1)\nY = train_df_2[\"Survived\"]","7c719465":"sample_size_time()","d53171c9":"# Compare Algorithms\n\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DECTREE', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n\n# evaluate each model in turn\n\nscoring = 'accuracy'\nresults = []\nnames = []\nfor name, model in models:\n\n    kfold = model_selection.KFold(n_splits=10, random_state=1, shuffle = False)\n    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\n\nplt.show()","79902e6d":"## Starting ML modeling ","abefc835":"*Based on the information we have, we can see that only 342 people out of 891 passengers survived on our graph.","71f8b158":"### Final Decide for Missing Values\n#### Embarked :\n> Looking at the two missing values for 'Embarked', we can see that the passengers traveled together, as they have the same ticket number. We then look at the price they have paid for the ticket and compare it with a range of passengers who paid a similar fee and traveled in the same class. From those passengers, we calculate the mode and use this value as our two missing Embarked values.\n#### Age : \n> This could be a case of MAR(Missing at random) as we cannot directly observe the reason for missingness of data in these columns. We should fill them. There are various approach to fill it. We will figure out this missing values with find median of combination with columns(Gender and Class) which have more correlation between age column.\n#### Cabin : \n>There are a lot of missing in this column we are not going to use in our analyze therefore,we can drop that column.\n","928dd292":"### Question 5: What is the proportion of survivors and non-survivors among siblings and spouse(except mistresses and fianc\u00e9s )?","edf48d24":"### Naive Bayes (Gaussian)","d54413a5":"#### Question 4 Comment :\n>Almost all the women (% 97) in the 1st class survived.\n>\n>Although the number of passengers in the 2nd class is less than the number of passengers in the 1st class, the number of survivors in the 1st class is higher.","945d53a9":"#### Compering ML Models","4bd2725d":"> We calculated computing time and accuracy according to sample size with kaggle cpu. \n>\n> However,computing time vary based on your device specs.\n>\n> You should this code with your computer cpu. If possible write your device specs and computing time in comments.","aeed4bfb":"#### Question 4 Comment:\n> We can see that the majority of the passengers traveled without children\/parents. If traveling with children\/parents, the chances of surviving have not been significantly higher.","5c8030f1":"### We will now analyze which features are our most relevant indicators.","86836943":"### KNN","c63fcc7a":"### Now we will apply two additional models for classifaction.Those are Decesion Tree and Naive Bayes.","9f36277e":"### Linear Discriminant Analysis","0a362034":"> According to the average accuracy rate and standard deviation, we observe that is  our best model as Decision Tree.","c27f30e5":"#### Question 3 Comment:\n>We can conclude that the number of female survivors per age group is higher than the number of females per age group that drowned.\n>\n>When looking at the number of male survivors, we can observe that male passenger were more likely to drown in all age groups.","09595bd1":"### Which questions will we find answers in the Exploratory Data Analysis section?\n\n> 1. What is the number of survivors and non-survivors among those who boarded from which port by gender and in what class?\n>\n> 2. What are the Passengers' survival percentages by class and gender?\n>\n> 3. What are the number  of survived\/drowned passengers  by age and gender?\n>\n> 4. What are the Passengers' survival\/drowned distribution for passengers with parents \/ children on board?\n> \n> 5. What is the proportion of survivors and non-survivors among siblings and spouse(except mistresses and fianc\u00e9s )?\n>\n","14e0ad8a":"#### Question 5 Comment:\n>We can observe that the percentage of survivors that traveled with siblings\/spouses of size 1 and 2 is bigger compared to single travelers. On the contrary when traveling with starting 3 siblings\/spouses the percentage of surviving decreases.","4dd53a29":"### Before the pass Conclusion Section we will compare all 5 models.","5729b95c":"#### Question 1 Comment:\n>The highest number of women and men who lost their lives are 3rd class passengers at Southampton Embarkation port.\n>\n>The number of people who lost their lives was in the 3rd class most in total.","e98a6e49":"#### We will now start by splitting our dataset into our training (80%) and validation dataset (20%)","a21f3d5b":"<a id='eda'><\/a>\n## Exploratory Data Analysis\n\n> \n> Now our data is clean and ready to analyze !\n\n### Question 1: What is the number of survivors and non-survivors among those who boarded from which port by gender and in what class?","90771188":"### Logistic Regression","cf7bbe46":"### Question 3 : Number  of survived\/drowned passengers  by age and gender?","80fda7d5":"<a id='intro'><\/a>\n## Introduction\n\n\n\n>The sinking of the Titanic is one of the most infamous shipwrecks in history.\n>On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n> ### Data Dictionary\n\n>1. survival = Survival \/ 0 = No, 1 = Yes\n2. pclass = Ticket class \/ 1 = 1st, 2 = 2nd, 3 = 3rd\n3. sex = Sex \t\n4. Age = Age in years \t\n5. sibsp = of siblings \/ spouses aboard the Titanic \t\n6. parch = of parents \/ children aboard the Titanic \t\n7. ticket = Ticket number \t\n8. fare = Passenger fare \t\n9. cabin = Cabin number \t\n10. embarked = Port of Embarkation \/ C = Cherbourg, Q = Queenstown, S = Southampton\n\n> ### Variable Notes\n\n>pclass: A proxy for socio-economic status (SES)\n\/1st = Upper\n\/2nd = Middle\n\/3rd = Lower\n\n>age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n>sibsp: The dataset defines family relations in this way...\n\/Sibling = brother, sister, stepbrother, stepsister\n\/Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n>parch: The dataset defines family relations in this way...\n\/Parent = mother, father\n\/Child = daughter, son, stepdaughter, stepson\n\/Some children travelled only with a nanny, therefore parch=0 for them.\n","06f10485":"### Question 2: What are the Passengers' survival percentages by class and gender?","bc232107":"### Before the start Ml apply we will change the columns into the numerical values. And we will edit test data to get better predict.","dc4e70be":"#### Explanation of code in the below\n>To find the difference in accuracy, \n>\n>We did this calculation: the rate of accuracy we found in the train data - the rate of accuracy we get by dropping each feature.\n>\n>And so we found the effect of every feature.\n>\n>We performed for all 3 models.\n","75c1f05b":"### Decesion Tree","a06f8ecf":"#### Learning Curve\n[Learning Curve Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Learning_curve_(machine_learning))\n> We can brief here :\n>\n> A learning curve is a graph that shows the progress on the experience of a particular learning-related metric during the training of a machine learning model. \n>\n> So we implement graph for all 3 models.\n\n\n","7f5a9d80":"<a id='wrangling'><\/a>\n## Data Wrangling\n\n> After uploading the data, We will perform the clearing, deleting, editing and explore operations in this section in order to get it ready for analysis. Therefore, We need some important libraries which are has been imported at below.","20f9865e":"\n# Titanic EDA to Predict\n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#wrangling\">Data Wrangling<\/a><\/li>\n<li><a href=\"#eda\">Exploratory Data Analysis<\/a><\/li>\n<li><a href=\"#ml\">Applying Several ML Algorithms<\/a><\/li>\n<li><a href=\"#conclusion\">Conclusion and Final Comments<\/a><\/li>\n\n<\/ul>","b766dabc":"### Now we will take diffrent sample size from train data set to look our effects of sample.\n","2f7f4347":"> <a id='ml'><\/a>\n## Apply Several ML Algorithms","0b248e3c":"*Based on the information we have, we can see that first class has the most survived passengers.","4472e043":"### Visualization","018d7fac":"### Question 4 : What are the Passengers' survival\/drowned distribution for passengers with parents \/ children on board?","e0a24815":"> We will now compare the accuracy score of our training and validation dataset on the KNN, Logereg and lda Model "}}