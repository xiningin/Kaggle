{"cell_type":{"c37f69dd":"code","668f8b7a":"code","4b0444eb":"code","577fd114":"code","7cf6d8da":"code","aa8a0073":"code","5bbb87ce":"code","c66f78d2":"code","cfc5c32d":"code","c93c6c30":"code","68974d30":"code","948a63ad":"code","bcccc795":"code","1465d67f":"code","64972b45":"code","0e43a307":"code","007785a6":"code","10d3db18":"code","598fec91":"code","33839c51":"code","1790bdcc":"code","526662ed":"markdown","c243f96e":"markdown","6467fc12":"markdown","b9c38846":"markdown","6e6eb34a":"markdown","820cbc62":"markdown","ea12f5b6":"markdown","60551575":"markdown","a141a869":"markdown","de531c27":"markdown","15197543":"markdown","1dc85004":"markdown","30362acc":"markdown","ecb7fb07":"markdown","03796e8f":"markdown","7b1fddda":"markdown","4822a2cb":"markdown","f29d4c57":"markdown","ddf2abe6":"markdown","2c917409":"markdown","ac1ea75b":"markdown","367394f8":"markdown","c27eb3d3":"markdown"},"source":{"c37f69dd":"import numpy as np\nimport pandas as pd","668f8b7a":"# Load and prepare Titanic data\ntitanic_train = pd.read_csv(\"..\/input\/train.csv\")    # Read the data\n\n# Impute median Age for NA Age values\nnew_age_var = np.where(titanic_train[\"Age\"].isnull(), # Logical check\n                       28,                       # Value if check is true\n                       titanic_train[\"Age\"])     # Value if check is false\n\ntitanic_train[\"Age\"] = new_age_var ","4b0444eb":"from sklearn import tree\nfrom sklearn import preprocessing","577fd114":"# Initialize label encoder\nlabel_encoder = preprocessing.LabelEncoder()\n\n# Convert Sex variable to numeric\nencoded_sex = label_encoder.fit_transform(titanic_train[\"Sex\"])\n\n# Initialize model\ntree_model = tree.DecisionTreeClassifier()\n\n# Train the model\ntree_model.fit(X = pd.DataFrame(encoded_sex), \n               y = titanic_train[\"Survived\"])","7cf6d8da":"import graphviz\n\n# Save tree as dot file\ndot_data = tree.export_graphviz(tree_model, out_file=None) \ngraph = graphviz.Source(dot_data)  \ngraph ","aa8a0073":"# Get survival probability\npreds = tree_model.predict_proba(X = pd.DataFrame(encoded_sex))\n\npd.crosstab(preds[:,0], titanic_train[\"Sex\"])","5bbb87ce":"# Make data frame of predictors\npredictors = pd.DataFrame([encoded_sex, titanic_train[\"Pclass\"]]).T\n\n# Train the model\ntree_model.fit(X = predictors, \n               y = titanic_train[\"Survived\"])","c66f78d2":"# Save tree as dot file\ndot_data = tree.export_graphviz(tree_model, out_file=None) \ngraph = graphviz.Source(dot_data)  \ngraph ","cfc5c32d":"# Get survival probability\npreds = tree_model.predict_proba(X = predictors)\n\n# Create a table of predictions by sex and class\npd.crosstab(preds[:,0], columns = [titanic_train[\"Pclass\"], \n                                   titanic_train[\"Sex\"]])","c93c6c30":"predictors = pd.DataFrame([encoded_sex,\n                           titanic_train[\"Pclass\"],\n                           titanic_train[\"Age\"],\n                           titanic_train[\"Fare\"]]).T\n\n# Initialize model with maximum tree depth set to 8\ntree_model = tree.DecisionTreeClassifier(max_depth = 8)\n\ntree_model.fit(X = predictors, \n               y = titanic_train[\"Survived\"])","68974d30":"# Save tree as dot file\ndot_data = tree.export_graphviz(tree_model, out_file=None) \ngraph = graphviz.Source(dot_data)  \ngraph ","948a63ad":"tree_model.score(X = predictors, \n                 y = titanic_train[\"Survived\"])","bcccc795":"# Read and prepare test data\ntitanic_test = pd.read_csv(\"..\/input\/test.csv\")    # Read the data\n\n# Impute median Age for NA Age values\nnew_age_var = np.where(titanic_test[\"Age\"].isnull(), # Logical check\n                       28,                       # Value if check is true\n                       titanic_test[\"Age\"])      # Value if check is false\n\nnew_fare_var = np.where(titanic_test[\"Fare\"].isnull(), # Logical check\n                       50,                       # Value if check is true\n                       titanic_test[\"Fare\"])      # Value if check is false\n\ntitanic_test[\"Age\"] = new_age_var \ntitanic_test[\"Fare\"] = new_fare_var","1465d67f":"# Convert test variables to match model features\nencoded_sex_test = label_encoder.fit_transform(titanic_test[\"Sex\"])\n\ntest_features = pd.DataFrame([encoded_sex_test,\n                              titanic_test[\"Pclass\"],\n                              titanic_test[\"Age\"],\n                              titanic_test[\"Fare\"]]).T","64972b45":"# Make test set predictions\ntest_preds = tree_model.predict(X=test_features)\n\n# Create a submission for Kaggle\nsubmission = pd.DataFrame({\"PassengerId\":titanic_test[\"PassengerId\"],\n                           \"Survived\":test_preds})\n\n# Save submission to CSV\nsubmission.to_csv(\"tutorial_dectree_submission.csv\", \n                  index=False)        # Do not save index values","0e43a307":"from sklearn.model_selection import train_test_split","007785a6":"v_train, v_test = train_test_split(titanic_train,     # Data set to split\n                                   test_size = 0.25,  # Split ratio\n                                   random_state=1,    # Set random seed\n                                   stratify = titanic_train[\"Survived\"]) #*\n\n# Training set size for validation\nprint(v_train.shape)\n# Test set size for validation\nprint(v_test.shape)","10d3db18":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=10, random_state=12)\nkf.get_n_splits(titanic_train)","598fec91":"fold_accuracy = []\n\ntitanic_train[\"Sex\"] = encoded_sex\n\nfor train_fold, valid_fold in kf.split(titanic_train):\n    train = titanic_train.loc[train_fold] # Extract train data with cv indices\n    valid = titanic_train.loc[valid_fold] # Extract valid data with cv indices\n    \n    model = tree_model.fit(X = train[[\"Sex\",\"Pclass\",\"Age\",\"Fare\"]], \n                           y = train[\"Survived\"])\n    valid_acc = model.score(X = valid[[\"Sex\",\"Pclass\",\"Age\",\"Fare\"]], \n                            y = valid[\"Survived\"])\n    fold_accuracy.append(valid_acc)    \n\nprint(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\nprint(\"Average accuracy: \", sum(fold_accuracy)\/len(fold_accuracy))","33839c51":"from sklearn.model_selection import cross_val_score","1790bdcc":"scores = cross_val_score(estimator= tree_model,     # Model to test\n                X= titanic_train[[\"Sex\",\"Pclass\",   # Train Data\n                                  \"Age\",\"Fare\"]],  \n                y = titanic_train[\"Survived\"],      # Target variable\n                scoring = \"accuracy\",               # Scoring metric    \n                cv=10)                              # Cross validation folds\n\nprint(\"Accuracy per fold: \")\nprint(scores)\nprint(\"Average accuracy: \", scores.mean())","526662ed":"Now let's look at the graph of the new decision tree model:","c243f96e":"Next, we need to load and initialize scikit-learn's the decision tree model and then train the model using the sex variable:","6467fc12":"Note the list of default arguments included in the model above. Read more about them here.\n\nNow let's view a visualization of the tree the model created. We can do this with the \"graphviz\" library:","b9c38846":"# Holdout Validation and Cross Validation","6e6eb34a":"If you've ever had to diagnose a problem with an appliance, car or computer, there's a good chance you've encountered a troubleshooting flowchart. A flowchart is a tree-like structure of yes\/no questions that guides you through some process based on your specific situation. A decision tree is essentially a flow chart for deciding how to classify an observation: it consists of a series of yes\/no or if\/else decisions that ultimately assign each observation to a certain probability or class. The series of yes\/no decisions can be depicted as a series of branches that lead decisions or \"leaves\" at the bottom of the tree.\n\nWhen working with the Titanic survival prediction data last time, we suggested a simple model that classifies all women as survivors and all men as non-survivors. This model is an example of a simple decision tree with only one branch or split.\n\nLet's create the gender-based model on the Titanic training set using decision trees in Python. First we'll load some libraries and preprocess the Titanic data:","820cbc62":"Notice that the more complex model still predicts a higher survival rate for women than men, but women in third class only have a 50% predicted death probability while women in first class are predicted to die less than 5% of the time.\n\nThe more variables you add to a decision tree, the more yes\/no decisions it can make, resulting in a deeper, more complex tree. Adding too much complexity to a decision tree, however, makes it prone to overfitting the training data, which can lead to poor generalization to unseen data. Let's investigate by creating a larger tree with a few more variables:","ea12f5b6":"# Decision Trees","60551575":"# Python for Data 29: Decision Trees\n[back to index](https:\/\/www.kaggle.com\/hamelg\/python-for-data-analysis-index)","a141a869":"Notice that by adding one more variable, the tree is considerably more complex. It now has 6 decision nodes, 6 leaf nodes and a maximum depth of 3.\nLet's make predictions and view a table of the results:","de531c27":"The table shows that the decision tree managed to create the simple gender-based model where all females survive and all males perish.\n\nLet's create a new decision tree that adds the passenger class variable and see how it changes the resulting predictions:","15197543":"The image above illustrates how complex decision trees can become when you start adding more explanatory variables. You can control the complexity of the tree by altering some of the decision tree function's default parameters. For example, when we made the tree above, we set max_depth = 8, which limited the tree to a depth of 8 (if we hadn't done this the tree would have been much larger!).\n\nFor interest's sake, let's check the accuracy of this decision tree model on the training data:","1dc85004":"# Wrap Up","30362acc":"After creating a cross validation object, you can loop over each fold and train and evaluate a your model on each one:","ecb7fb07":"# Next Lesson: [Python for Data 30: Random Forests](https:\/\/www.kaggle.com\/hamelg\/python-for-data-30-random-forests)\n[back to index](https:\/\/www.kaggle.com\/hamelg\/python-for-data-analysis-index)","03796e8f":"Notice that the average accuracy across each fold is higher than the non-stratified K-fold example. The cross_val_score function is useful for testing models and tuning model parameters (finding optimal values for arguments like maximum tree depth that affect model performance).","7b1fddda":"Upon submission the model scores 0.78468 accuracy, which is slightly better than the simple gender-based model, but far worse than the accuracy the model achieved on the training data itself. This underscores the fact that predictive performance on the training data is a poor barometer of predictive performance on new data.","4822a2cb":"*Note: When performing classification, it is desirable for each class in the target variable to have roughly the same proportion across each split of the data. The stratify argument lets you specify a target variable to spread evenly across the train and test splits.*\n\nThe output above shows that we successfully created a new training set with roughly 75% of the original data and a validation test set with 25% of the data. We could proceed by building models with this new training set and making predictions on the validation set to assess the models.\n\nCross validation is a popular alternative to holdout validation that involves splitting the training data into two or more partitions and creating a model for each partition where the partition acts as the validation set and the remaining data acts as the training data. A common form of cross validation is \"k-fold\" cross validation, which randomly splits data into some number k (a user specified parameter) partitions and then creates k models, each tested against one of the partitions. Each of the k models are then combined into one aggregate final model.\n\nThe primary advantage of cross validation is it uses all the training data to build and assess the final model. The main drawback is that building and testing several models can be computationally expensive, so it tends to take much longer than holdout validation. You can create K cross validation splits of the data using the Kfold() function in sklearn's model_selection library:","f29d4c57":"In the last lesson we introduced logistic regression as a predictive modeling technique for classification tasks. While logistic regression can serve as a low variance baseline model, other models often yield better predictive performance. Decision trees are another relatively simple classification model that have more expressive power than logistic regression.","ddf2abe6":"Model accuracy can vary significantly from one fold to the next, especially with small data sets, but the average accuracy across the folds gives you an idea of how the model might perform on unseen data.\n\nAs with holdout validation, we'd like the target variable's classes to have roughly the same proportion across each fold when performing cross validation for a classification problem. To perform stratified cross validation, use the StratifiedKFold() function instead of KFold().\n\nYou use can score a model with stratified cross validation with a single function call with the cross_val_score() function:","2c917409":"The model is almost 89% accurate on the training data, but how does it do on unseen data? Let's load the test data, make some predictions submit them to Kaggle to find out:","ac1ea75b":"The tree's graph show us that it consists of only one decision node that splits the data on the variable sex (the first variable, X[0]). All 314 females end up in one leaf node and all 577 males end up in a different leaf node.\n\nLet's make predictions with this tree and view a table of the results:","367394f8":"When creating a predictive model, we'd like to get an accurate sense of its ability to generalize to unseen data before actually going out and using it on unseen data. As we saw earlier, generating predictions on the training data itself to check the model's accuracy does not work very well: a complex model may fit the training data extremely closely but fail to generalize to new, unseen data. We can get a better sense of a model's expected performance on unseen data by setting a portion of our training data aside when creating a model, and then using that set aside data to evaluate the model's performance. This technique of setting aside some of the training data to assess a model's ability to generalize is known as validation.\n\nHoldout validation and cross validation are two common methods for assessing a model before using it on test data. Holdout validation involves splitting the training data into two parts, a training set and a validation set, building a model with the training set and then assessing performance with the validation set. In theory, model performance on the hold-out validation set should roughly mirror the performance you'd expect to see on unseen test data. In practice, holdout validation is fast and it can work well, especially on large data sets, but it has some pitfalls.\n\nReserving a portion of the training data for a holdout set means you aren't using all the data at your disposal to build your model in the validation phase. This can lead to suboptimal performance, especially in situations where you don't have much data to work with. In addition, if you use the same holdout validation set to assess too many different models, you may end up finding a model that fits the validation set well due to chance that won't necessarily generalize well to unseen data. Despite these shortcomings, it is worth learning how to use a holdout validation set in Python.\n\nYou can create a holdout validation set using the train_test_split() in sklearn's cross_validation library:","c27eb3d3":"Decision trees are an easily interpretable yet surprisingly expressive form of predictive model. A decision tree of limited depth can provide a good starting point for classification tasks and model complexity is easy adjustable. For our final lesson, we'll learn about random forests, an extension of decision trees that preform very well on a wide range of classification tasks."}}