{"cell_type":{"44567287":"code","f63c7723":"code","19c45e98":"code","0025935a":"code","0ce4effa":"code","fee5011c":"code","83c39571":"code","77b3cee1":"code","a7a0c307":"code","3ec901c7":"code","394dc741":"code","57a8a7b7":"code","44a7571b":"code","16276b0f":"code","dd1c6056":"code","353a5480":"code","c8c6cef6":"markdown","825c2855":"markdown","8e00a919":"markdown","01562897":"markdown","33695bc1":"markdown","2902d7d3":"markdown","4a69065d":"markdown","01f7157a":"markdown","3729a0c4":"markdown","7ebce87b":"markdown","4076a694":"markdown","3ba62d33":"markdown"},"source":{"44567287":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom kaggle.competitions import twosigmanews\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom sys import getsizeof\nprint(os.listdir(\"..\/input\"))\nenv = twosigmanews.make_env()\nprint('Done!')\n\nisTestCode = True\n\nprint('Is a test?\\t', isTestCode)\n# Any results you write to the current directory are saved as output.","f63c7723":"sample_frac = 0.1\nmarket_data, news_data = env.get_training_data()\nmarket_data = market_data.sample(frac=sample_frac, random_state=2018)\nnews_data = news_data.sample(frac=sample_frac, random_state=2018)\ngc.collect()\nprint('shape of market_data:\\t', market_data.shape)\nprint('shape of news_data:\\t', news_data.shape)","19c45e98":"print('memory(market_data):\\t', getsizeof(market_data))\nprint('memory(news_data):\\t', getsizeof(news_data))","0025935a":"if isTestCode:\n    market_data = market_data.tail(10000)\n    news_data = news_data.tail(50000)","0ce4effa":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","fee5011c":"\ndef market_news_join(market_data, news_data, news_cols_agg=news_cols_agg, agg_day=7):\n    news_cols = ['time', 'assetName'] + sorted(news_cols_agg.keys())\n    \n    # delete the unnecessary data\n    news_data = news_data[news_data.time<=market_data.time.max()]\n    news_data = news_data[news_data.time>=(market_data.time.min()-np.timedelta64(agg_day,'D'))]\n    \n#     print('shape(news in joinFun):\\t', news_data.shape)\n#     print('memory(news in joinFun):\\t', getsizeof(news_data))\n    \n#     print('max_time:\\t', news_data.time.max())\n#     print('min_time:\\t', news_data.time.min())\n    \n    merge_data = pd.merge(market_data, news_data[news_cols], how='inner', on='assetName', suffixes=(['_market','_news']))\n    \n    # window limit\n    merge_data = merge_data[np.array((merge_data.time_market-merge_data.time_news).dt.days<agg_day)&np.array((merge_data.time_market-merge_data.time_news).dt.days>=0)]\n    group_data = merge_data.groupby(by=['assetName']).agg(news_cols_agg)\n    \n    group_data.columns = ['_'.join(col).strip() for col in group_data.columns.values]\n    \n    result = market_data.join(group_data, on=['assetName'])\n    \n    # release memory\n    del merge_data\n    del group_data\n#     print(gc.collect())\n    \n    return result","83c39571":"# sorted by time\nnews_data = news_data[['time', 'assetName']+sorted(news_cols_agg.keys())]\nnews_data = news_data.sort_values(by=['time'])\nmarket_data = market_data.sort_values(by=['time'])\nprint(gc.collect())","77b3cee1":"slice_list = []\nslice_size = 100000\nfor i in range(int(market_data.shape[0]\/slice_size)+1):\n    slice_data = market_news_join(market_data.iloc[(i*slice_size):((i+1)*slice_size)], news_data)\n    slice_list.append(slice_data)\n    \ndel market_data\ndel news_data\nprint('release memory:\\t', gc.collect())\ndataTrain = pd.concat(slice_list)","a7a0c307":"feature_used_to_train = dataTrain.columns.tolist()\nfeature_used_to_train.remove('time')\nfeature_used_to_train.remove('assetName')\nfeature_used_to_train.remove('assetCode')\nfeature_used_to_train.remove('returnsOpenNextMktres10')\nfeature_used_to_train.remove('universe')","3ec901c7":"def metricFun(test_pre, test_data):\n    data_score = pd.DataFrame({'time':test_data.time, 'val':test_data.returnsOpenNextMktres10 * test_pre})\n    day_sum = data_score.groupby(by=['time']).sum()\n    score = day_sum.val.mean()\/day_sum.val.std()\n    return score","394dc741":"kf = KFold(n_splits=5, shuffle=True, random_state=2018)\nscorelist = []\nmodellist = []\nfor train_index, test_index in kf.split(dataTrain):\n    train_x = dataTrain.iloc[train_index][feature_used_to_train]\n    train_y = dataTrain.iloc[train_index].returnsOpenNextMktres10\n    \n    test_x = dataTrain.iloc[test_index][feature_used_to_train]\n    test_y = dataTrain.iloc[test_index]\n    \n    # the rows whose universe==1 are selected\n    test_x = test_x[test_y.universe==1.0]\n    test_y = test_y[test_y.universe==1.0]\n    \n    model = lgb.LGBMClassifier(num_leaves=127, random_state=2018)\n    train_y = 2*(train_y>0).astype(int)-1\n    model.fit(train_x, train_y)\n    \n    test_pre = model.predict_proba(test_x)[:, 1]\n    test_pre = 2*test_pre-1\n    \n    score = metricFun(test_pre, test_y)\n    \n    print('-'*50)\n    print('accuracy:\\t', (np.sum((test_pre>0) == (test_y.returnsOpenNextMktres10>0)))\/test_y.shape[0] )\n    print('score:\\t', score)\n    scorelist.append(score)\n    modellist.append(model)\n    \nprint('mean of scores:\\t', np.array(scorelist).mean())\nprint('std of scores:\\t', np.array(scorelist).std())","57a8a7b7":"import matplotlib.pyplot as plt\nfeature_importances = np.sum([model.feature_importances_ for model in modellist], axis=0)\nfeature_importances = pd.Series(feature_importances)\nfeature_importances.index = feature_used_to_train\nplt.figure(figsize=(20, 5))\nfeature_importances.sort_values(ascending=False).plot(kind='bar')\nplt.show()","44a7571b":"del train_x\ndel train_y\n\ndel test_x\ndel test_y\n\ndel dataTrain\n\nprint('release:\\t', gc.collect())","16276b0f":"days = env.get_prediction_days()","dd1c6056":"count=0\nfor (market_data_pre, news_data_pre, template) in days:\n    prelist = []\n    \n    data_pre = market_news_join(market_data_pre, news_data_pre)[feature_used_to_train]\n    \n    for model in modellist:\n        out_pre = model.predict_proba(data_pre.apply(np.float64))[:, 1]\n        out_pre = 2*out_pre-1\n        \n#         out_pre = model.predict(test_x)\n        \n        \n        prelist.append(out_pre)\n    \n    template.confidenceValue=np.clip(np.mean(np.array(prelist), axis=0), -1, 1)\n    env.predict(template)\n    count += 1\n    print(count, end=' ')\nprint('Done')","353a5480":"env.write_submission_file()","c8c6cef6":"The raw data are divided into some slices to reduce memory usage","825c2855":"Agg features\nrefer to **A simple model - using the market and news data**\n\n[https:\/\/www.kaggle.com\/bguberfain\/a-simple-model-using-the-market-and-news-data](http:\/\/)","8e00a919":"plot feature importances","01562897":"A metric function\nparam:\n\n    test_pre: array\n    test_data: pd.DataFrame","33695bc1":"A function to join market data and news data\nparam:\n\n    market_data: data of market\n    news_data: data of news\n    news_cols_agg: the features \n    agg_day: the size of window\n    ","2902d7d3":"In this kernel, the rows of train data is **10000**.\n\nIt is interesting to find the smaller train data has better performance on LB.\n\nif the rows of train data exceed 40w, LB would be less than 0.1 in this kernel.","4a69065d":"In order to reduce memory usage, the market data and the news data are **downsampled**. ","01f7157a":"generate output file","3729a0c4":"delete some features","7ebce87b":"5-fold and trainning\n\nlocal cv is very bad","4076a694":"reduce memory usage","3ba62d33":"Only **1w** rows are selected."}}