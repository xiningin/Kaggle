{"cell_type":{"c28ad2b0":"code","9e008919":"code","0f43d51d":"code","42879338":"code","ac756560":"code","f80f3a60":"code","898eadee":"code","dcebd2a4":"code","3b6a677a":"code","6a6a0402":"markdown","ea835d3a":"markdown","41325bf4":"markdown","007f8c6c":"markdown","4c34b691":"markdown","915ef5fe":"markdown","2548b7ad":"markdown","6e0710e9":"markdown"},"source":{"c28ad2b0":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nimport glob\nimport cv2\nimport annoy\nimport time\n\nfrom tqdm import tqdm\nfrom numpy.linalg import norm\nfrom scipy.spatial import distance","9e008919":"train_folder_dir = '..\/input\/fashionfull\/fashion_image_resized\/fashion_image_resized\/train\/'\ntest_folder_dir = '..\/input\/fashionfull\/fashion_image_resized\/fashion_image_resized\/test\/'\n\ndf_train = pd.read_csv('..\/input\/fashionfull\/fashion.csv')\ndf_test = pd.read_csv('..\/input\/fashionfull\/fashion_test.csv')\n\ntrain_image_paths = df_train['image_name'].values\ntest_image_paths = df_test['image_name'].values","0f43d51d":"def image_generator(image_paths, base_dir):\n    for path in image_paths:\n        img = cv2.imread(base_dir + path)\n        resized = cv2.resize(img, (25,25))\n        yield resized.flatten()","42879338":"plt.figure(figsize=(20,12))\n\nimage1 = cv2.imread(train_folder_dir + train_image_paths[3])\nresized1 = cv2.resize(image1, (25,25))\n\nimage2 = cv2.imread(train_folder_dir + train_image_paths[300])\nresized2 = cv2.resize(image2, (25,25))\n\n\nplt.subplot(1, 4, 1)\nplt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image 1',fontsize=16)\n\nplt.subplot(1, 4, 2)\nplt.imshow(cv2.cvtColor(resized1, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Resized Image 1',fontsize=16)\n\nplt.subplot(1, 4, 3)\nplt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image 2',fontsize=16)\n\nplt.subplot(1, 4, 4)\nplt.imshow(cv2.cvtColor(resized2, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Resized Image 2',fontsize=16)\n\nplt.tight_layout()\nplt.show()","ac756560":"start_time = time.time()\n\nvector_length = 25*25*3\n\nt = annoy.AnnoyIndex(vector_length)\n\nfor i, v in enumerate(image_generator(train_image_paths, train_folder_dir)):\n    t.add_item(i, v)\n\nprint(f'Time taken : {(time.time() - start_time) \/ 60:.2f} mins')","f80f3a60":"start_time = time.time()\n\nNtrees = 100\nt.build(Ntrees)\n\nprint(f'Time taken : {(time.time() - start_time) \/ 60:.2f} mins')","898eadee":"def plot_neighbours(query_vector, n = 4):\n    \n    n_indices = t.get_nns_by_vector(query_vector, n)\n    n_vectors = [np.array(query_vector, dtype=np.uint8).reshape((25,25,3))]\n    n_distances = [0]\n    \n    for i in n_indices:\n        n_vec = t.get_item_vector(i)\n        n_arr = np.array(n_vec, dtype=np.uint8).reshape((25,25,3))\n        n_vectors.append(n_arr)\n        n_distances.append(np.abs(distance.cosine(n_vectors[0].ravel(), n_arr.ravel())))\n        \n    rows = n \/\/ 5 + 1\n    \n    plt.figure(figsize=(20, rows * 4))\n    \n    for i, n in enumerate(n_vectors):\n        plt.subplot(rows, 5, i + 1)\n        plt.imshow(cv2.cvtColor(n, cv2.COLOR_BGR2RGB))\n        plt.axis('off')\n        \n        if i == 0:\n            plt.title('Query Image', fontsize=16)\n        else:\n            plt.title(f'N{i}, Cosine_dist = {n_distances[i]:.2f}', fontsize=16)\n        \n    plt.tight_layout()\n    plt.show()","dcebd2a4":"for image in image_generator(test_image_paths[0:10], test_folder_dir):\n    plot_neighbours(image, n=4)","3b6a677a":"t.save('fashion_100trees.ann')\n\n# To load it next time, just run the following line with the appropriate file path\n# t.load('..\/input\/image-matching-fashion-v1\/fashion_100trees.ann')","6a6a0402":"# **We need an image generator that generates resized images.**\n\n### This function will resize the image to 25 x 25 x 3, mainly to deal with the memory constraint of Kaggle kernels. You are free to use higher resolution images (but make sure to standardize the size across all images) if you have monstrous RAM (probably 32GB or more for this dataset) or found a way to read directly from disk without loading them in RAM.","ea835d3a":"# **Hope it was useful!** \n\n### Closing thought : No data is totally useless. Always strive to find patterns and use cases within the available data and squeeze out every last bit of available information from the data.","41325bf4":"# **Next, create and map the indices of all images and build the ANN (trees)**\n\n### Higher number of trees = higher accuracy & longer building time.","007f8c6c":"# **First, load the files.**","4c34b691":"### Just to give you an understanding of how much quality difference we are talking about:","915ef5fe":"# **Image Matching using Approximate Nearest Neighbours**\n\n### After browsing through the image datasets, you will occasionally see images that are carbon copies of each other. A simple strategy to classify the test data is to find the images that look similar to your query image (input test data). \n\n### To identify similar images, we can measure the distance between each of the pixels of 2 images (color image are just 3 layers of pixels, with each layer corresponding to Red, Green and Blue channel), and look for the image sets which are closest to each other. The images similar to our query image is known as the nearest neighbours. We can then decide the sub-category of our query image by looking at its neighbours' labels. This is a lazy-learning technique known as **K - Nearest Neighbour (KNN)**.\n\n### However, we have some constraints. Traditional KNN is computationally and memory expensive (imagine we got to store all the training images, and for each image query, we do a blind search on these training images). Thus, in this notebook, I will show you a proof of concept of image matching using **Approximate Nearest Neighbours (ANN)** instead, which is theoretically faster with decent search accuracy. I will use **[Annoy](https:\/\/github.com\/spotify\/annoy)** package for the ANN, because it is **~~annoying~~** straight forward to use.\n\n### As for the dataset, I will use the **[Fashion dataset](https:\/\/www.kaggle.com\/burn874\/fashionfull)** which is kindly prepared and shared by **[Chua Cheng Hong](https:\/\/www.kaggle.com\/burn874)**. ","2548b7ad":"# **Now lets check out the ANN using some test data.**\n\n### We will look for the 4 nearest neighbours to the query image and print out the neighbours.","6e0710e9":"# **Hmmm ok.**\n\n### Not too bad. Note that by default, Annoy uses cosine distance to look for the query image's nearest neighbour, if you would like to build a model on a different distance measure, you have to specify it in the first step where you map the Annoy Index. \n\n### For ideal results, you will likely require a combinations of distance measures, each with a fine-tuned threshold. You can experiment yourself and figure them out.\n\n### Also, there will be some images which are not duplicated within the dataset, and thus, you won't be able to find any similar matches (though Annoy will still return you the nearest neighbours). For these images, you will need to rely on the text data to classify them instead.\n\n# **Lastly, remember to save the ANN model, so that you can load it quickly (like in seconds) next time.**"}}