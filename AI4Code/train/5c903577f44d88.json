{"cell_type":{"6ebc9395":"code","773c4b5d":"code","89b22a5b":"code","5c1bd689":"code","fac8089b":"code","ec32e6e7":"code","a111e03c":"code","b7d1e77e":"code","127a6341":"code","d9f3dcc5":"code","60ecd265":"code","ec773604":"code","480a61e4":"code","6349798f":"code","8880893b":"code","a7fc95b3":"code","a4e618dd":"code","eec7da3a":"code","4c964cf7":"code","77992909":"code","22da163c":"code","eb533dcb":"code","04860e84":"code","cdbbd63e":"markdown","a078b2f0":"markdown","0a2181eb":"markdown","77441860":"markdown"},"source":{"6ebc9395":"# -*- coding: utf-8 -*-\n\n\"\"\"\nTitanic Dataset Classification\n\nhttps:\/\/www.kaggle.com\/c\/titanic\/data\n\nThis notebook features:\n    detecting running on Kaggle or not, and adjusting file names.\n    data preprocessing functions to apply to both train and test.\n    applying a list of models.\n    automatically choosing the best model from the list.\n    applying GridSearchCV to the best model.\n\n```\nColumns:\nsurvival | Survival | 0 = No, 1 = Yes\npclass   | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd\nsex      | Sex                            |\nAge      | Age in years                   |\nsibsp    | # of siblings \/ spouses aboard |\nparch    | # of parents \/ children aboard |\nticket   | Ticket number                  |\nfare     | Passenger fare                 |\ncabin    | Cabin number                   |\nembarked | Port of Embarkation\n    | C = Cherbourg, Q = Queenstown, S = Southampton\n```\n\"\"\"\n","773c4b5d":"# import\nimport os\nimport pandas as pd\nimport numpy as np\n# import matplotlib as plt\n# import seaborn as sns\nfrom sklearn import ensemble\nfrom sklearn import tree\nfrom sklearn import neighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n","89b22a5b":"# read files\n# import os\nif os.path.isdir(\"\/kaggle\/input\"):\n    # we are on Kaggle.\n    file_name_train = \"\/kaggle\/input\/titanic\/train.csv\"\n    file_name_test = \"\/kaggle\/input\/titanic\/test.csv\"\n    file_name_submission = \"\/kaggle\/input\/titanic\/gender_submission.csv\"\n    file_name_output = \"\/kaggle\/input\/titanic\/my_submission.csv\"\n    # import os\n    # for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #     for filename in filenames:\n    #         print(os.path.join(dirname, filename))\nelse:\n    # we are not on Kaggle, local machine possibly.\n    file_name_train = \"train.csv\"\n    file_name_test = \"test.csv\"\n    file_name_submission = \"gender_submission.csv\"\n    file_name_output = \"my_submission.csv\"\n\ndf = pd.read_csv(file_name_train)\ndf2 = pd.read_csv(file_name_test)\nids = df2[\"PassengerId\"]\ndf0 = df.copy()\n","5c1bd689":"dfdesc = df.describe()\ndfdesc\n","fac8089b":"df.info()\n","ec32e6e7":"# checking nulls of the train dataset, before any preprocessing:\ndfnulls = pd.isnull(df).sum()\ndfnulls\n","a111e03c":"# checking nulls of the test dataset:\ndfnulls = pd.isnull(df2).sum()\ndfnulls\n","b7d1e77e":"# fill Embarked\ndf[\"Embarked\"].fillna(\"S\", inplace=True)\ndf2[\"Embarked\"].fillna(\"S\", inplace=True)\n","127a6341":"# fill Fare\ndf[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf2[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\n","d9f3dcc5":"# drop columns\ndef drop_columns(df, column_names=None):\n    if not column_names:\n        # cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Fare', 'Embarked']\n        cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n    df = df.drop(cols_to_drop, axis=1)\n    return df\n\n\ndf = drop_columns(df)\ndf2 = drop_columns(df2)\n","60ecd265":"# get_dummies\n# all of the data should be numerical before applying machine learning algorithms.\n# we already have columns that include categorical values.\n# let's convert them to columns.\ndef prepare_dummies(df, cols_to_dummy=None):\n    if not cols_to_dummy:\n        cols_to_dummy = ['Sex', 'Embarked']\n    dummies = []\n    for col in cols_to_dummy:\n        if col in df:\n            dummies.append(pd.get_dummies(df[col], prefix=col))\n        else:\n            continue\n\n    # dummies is now a list.\n    # Let's convert it to a DataFrame:\n    titanic_dummies = pd.concat(dummies, axis=1)\n\n    # we concatenate to the original dataframe columnwise\n    df = pd.concat((df, titanic_dummies), axis=1)\n\n    # let's drop the columns which we have applied get_dummies:\n\n    # we are not using this\n    # since some of the columns may have already been dropped:\n    # df = df.drop(cols_for_dummies, axis=1)\n\n    for col in cols_to_dummy:\n        if col in df:\n            df = df.drop(col, axis=1)\n\n    return df\n\n\ndf = prepare_dummies(df)\ndf2 = prepare_dummies(df2)\n","ec773604":"# checking nulls again:\ndfnulls = pd.isnull(df).sum()\ndfnulls\n","480a61e4":"# Age has nan values, fill them:\ndef handle_nans(df):\n    r\"\"\"\n    # how does interpolate work?\n    # https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.interpolate.html\n    # # x = ((x-1) + (x+1)) \/ 2\n    # data1 = {}\n    # data1[\"age\"] = [20, 20, np.nan, 40, np.nan, 42]\n    # df1 = pd.DataFrame(data1)\n    # df1[\"age2\"] = df1[\"age\"].interpolate()\n    \"\"\"\n    # dfnulls1 = pd.isnull(df).sum()  # before (for checking)\n    # df['Age'] = df['Age'].interpolate().astype(\"int64\")\n    df['Age'] = df['Age'].interpolate()\n    # dfnulls2 = pd.isnull(df).sum()  # after (for checking)\n    return df\n\n\ndf = handle_nans(df)\ndf2 = handle_nans(df2)\n","6349798f":"# handle family\ndef handle_family(df):\n    df[\"family_size\"] = df[\"SibSp\"] + df[\"Parch\"]\n    df = df.drop(columns=[\"SibSp\", \"Parch\"], axis=1)\n    return df\n\n\ndf = handle_family(df)\ndf2 = handle_family(df2)\n","8880893b":"# set age groups:\ndef set_age_group(age):\n    # groups = list(range(0, 100, 5))\n    groups = list(range(0, 100, 10))\n    for i in groups:\n        if i >= age:\n            group = i\n            break\n    return group\n\n\ndef set_age_groups(df):\n    df[\"AgeGroup\"] = df[\"Age\"].apply(set_age_group)\n    df = df.drop(\"Age\", axis=1)\n    return df\n\n\ndf = set_age_groups(df)\ndf2 = set_age_groups(df2)\n","a7fc95b3":"# let's note the columns before modelling:\nprint(f\"columns in df: {df.columns}\")\n","a4e618dd":"# data split\n# our target column is: \"Survived\"\n\n# X_train -> y_train\n# X_test  -> ?\n\ny = df['Survived'].values  # makes a copy\nX = df.drop(columns=[\"Survived\"]).values  # makes another copy\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=19)\n","eec7da3a":"# model objects\nmodels = []\nmodels.append(tree.DecisionTreeClassifier(max_depth=5, random_state=19))\n# models.append(ensemble.RandomForestClassifier(n_estimators=100, random_state=19))\nmodels.append(ensemble.GradientBoostingClassifier(\n    n_estimators=100, random_state=19))\n# models.append(neighbors.KNeighborsClassifier())\n","4c964cf7":"# apply all the models and find the best model:\nbest_score = 0\nbest_model = None\nfor model in models:\n    print(\"_\" * 10, type(model))\n    model.fit(X_train, y_train)\n    model_score = model.score(X_test, y_test)\n    print(\"  \", model_score)\n\n    if model_score > best_score:\n        best_score = model_score\n        best_model = model\n\nprint(\"best model:\", best_model)\nbest_model_str = str(type(best_model))\n","77992909":"# tune the best model using GridSearchCV\n# from sklearn.model_selection import GridSearchCV\n\nif \"DecisionTreeClassifier\" in best_model_str:\n    # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n    params = {\n        \"max_depth\": range(2, 11, 1),\n        \"random_state\": [19, 42],\n    }\n    gs = GridSearchCV(tree.DecisionTreeClassifier(), params, n_jobs=-1)\nelif \"RandomForestClassifier\" in best_model_str:\n    # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n    params = {\n        \"n_estimators\": range(50, 550, 50),\n        \"random_state\": [19, 42],\n    }\n    gs = GridSearchCV(ensemble.RandomForestClassifier(), params, n_jobs=-1)\nelif \"GradientBoostingClassifier\" in best_model_str:\n    # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html\n    params = {\n        \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n        \"n_estimators\": range(50, 550, 50),\n        \"random_state\": [19, 42],\n    }\n    gs = GridSearchCV(ensemble.GradientBoostingClassifier(), params, n_jobs=-1)\nelif \"KNeighborsClassifier\" in best_model_str:\n    # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n    params = {\n        \"leaf_size\": [10, 20, 30, 40, 50],\n    }\n    gs = GridSearchCV(neighbors.KNeighborsClassifier(), params, n_jobs=-1)\n\n\ngs_results = gs.fit(X_train, y_train)\n# print(\"GS results\", gs_results)\nprint(\"best score: \", gs_results.best_score_)\nprint(\"best parameters: \", gs_results.best_params_)\nbest_model_of_gs = gs_results.best_estimator_\n","22da163c":"# prediction\ny_pred = best_model_of_gs.predict(df2)\n","eb533dcb":"# prepare submission\ndfout = ids.to_frame()\ndfout[\"Survived\"] = y_pred\n\ndfout.to_csv(file_name_output, index=False)\n","04860e84":"# last words\nprint(\"end.\")\n","cdbbd63e":" consider dropping columns:\n - `PassengerId` is the ID of data, and should be dropped to prevent overfitting.\n - `Ticket`, is a string such as ticket number, we should drop it.\n   - sample data: `A\/5 21171` or `113803`\n Let's drop the columns.\n\n\n","a078b2f0":" Now we will apply all these operations to real test file.\n\n\n","0a2181eb":"# end\n","77441860":" examine ages for survived females:\n dffemale = df[(df[\"Sex_female\"] == 1) & (df[\"Survived\"] == 1)]\n sns.distplot(dffemale[\"Age\"], bins=20)\n plt.show()\n\n\n"}}