{"cell_type":{"360098bf":"code","4aa66bfc":"code","04ea6a2a":"code","bc0a478e":"code","881cf460":"code","0d90a415":"code","d26c1c18":"code","dea127f9":"code","7ce75e1f":"code","8e074b82":"code","bce7c69d":"code","8e300d93":"code","7410b53a":"code","752fca26":"code","4fd71d6d":"code","02d2087d":"code","ea924e09":"code","9127fdec":"code","4a332e08":"code","de8e0a43":"code","9a997a83":"code","9fe3b4f6":"code","145dead3":"code","6072f412":"code","7837f7ac":"code","566e602c":"code","943e846f":"markdown","d6ae8988":"markdown","e161e1cf":"markdown","c8ab6cf9":"markdown","2a17ffb3":"markdown","865c9264":"markdown","ef62969c":"markdown","dd1c58af":"markdown","9bf77e83":"markdown","6bc6d9a4":"markdown","4f8de4e7":"markdown","58a25466":"markdown","b5da25b8":"markdown","5b66750f":"markdown","d70e5baf":"markdown","ad38941f":"markdown","1d97f2a8":"markdown","bdfea3b3":"markdown","4e27a7a3":"markdown","79ebb9e0":"markdown","d24264b9":"markdown","37612dc6":"markdown","28f712a6":"markdown","15ff4073":"markdown","1be84059":"markdown","cc36c428":"markdown","a58cfb1c":"markdown","afb8880c":"markdown","88486e26":"markdown","cca00d6b":"markdown","3ece6558":"markdown","34028a3b":"markdown","dbbab7f9":"markdown","efd5f348":"markdown","343732df":"markdown","3c23fe2b":"markdown","9a186f6e":"markdown"},"source":{"360098bf":"import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([2.0, 1.1, 3.2, 5.2, 7.0, 2.3, 4.0, 5, 6, 4.5])\ny = np.array([3.5, 2.3, 5.1, 7.1, 6.5, 3.2, 4.5, 5.4, 7, 5.5])\n\n# y_cap = w*x + b\ny_cap = 0.8 * x + 1.76 \n\nplt.plot(x, y, 'o')\nplt.plot(x, y_cap, '-')\nplt.show()\n","4aa66bfc":"mean_squared_error = np.mean((y_cap - y)**2)\nmean_squared_error","04ea6a2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc0a478e":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_df.head()","881cf460":"train_df.shape","0d90a415":"train_df = train_df.drop('Id', axis=1)\ntrain_df.shape","d26c1c18":"train_df.info()","dea127f9":"target = 'SalePrice'","7ce75e1f":"categorical_features = []\nnumeric_features = []\nfeatures = train_df.columns.values.tolist()\nfor col in features:\n    if train_df[col].dtype != 'object': \n        if col != target:\n            numeric_features.append(col)\n    else:\n        categorical_features.append(col)","8e074b82":"for col in numeric_features:\n    mean = train_df[col].mean()\n    train_df[col] = train_df[col].fillna(mean)","bce7c69d":"for col in categorical_features:\n    train_df[col] = train_df[col].fillna('None')","8e300d93":"train_df.info()","7410b53a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nsigma = train_df.SalePrice.std()\nmu = train_df.SalePrice.mean()\nmed = train_df.SalePrice.median()\nmode = train_df.SalePrice.mode().to_numpy()\n\nplt.title(f'Untransformed Data, Skew: {stats.skew(train_df.SalePrice):.3f}')\nsns.distplot(train_df.SalePrice)\nplt.axvline(mode, linestyle='--', color='green', label='mode')\nplt.axvline(med, linestyle='--', color='blue', label='median')\nplt.axvline(mu, linestyle='--', color='red', label='mean')\nplt.legend()","752fca26":"plt.title(f'Log transform, Skew: {stats.skew(np.log1p(train_df.SalePrice)):.3f}')\nsns.distplot(np.log1p(train_df.SalePrice))","4fd71d6d":"train_df.corr()['SalePrice'].sort_values(ascending=False)","02d2087d":"top_6_corr=['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', '1stFlrSF', 'FullBath']\nprows = 2\npcols = 3\nfig, axs = plt.subplots(prows, pcols, figsize=(pcols*3.5, prows*3))\nfor r in range(0,prows):\n    for c in range(0,pcols):  \n        i = r*pcols+c\n        col=top_6_corr[i]\n        sns.regplot(x=train_df[col], y=train_df['SalePrice'], ax = axs[r][c])\n","ea924e09":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])","9127fdec":"from scipy.stats import skew\nskewed_feats = train_df[numeric_features].apply(lambda x: skew(x)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\nskewed_feats","4a332e08":"train_df[skewed_feats] = np.log1p(train_df[skewed_feats])","de8e0a43":"from sklearn.preprocessing import LabelEncoder\n# Encoding categorical features\nfor col in categorical_features:\n    le = LabelEncoder()\n    le.fit(list(train_df[col].astype(str).values))\n    train_df[col] = le.transform(list(train_df[col].astype(str).values))","9a997a83":"y = train_df['SalePrice']\nX = train_df.drop('SalePrice', axis=1)","9fe3b4f6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","145dead3":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)","6072f412":"linreg.coef_","7837f7ac":"index = []\nfor c in top_6_corr:\n    i = train_df.columns.get_loc(c)\n    index.append(i)\nlinreg.coef_[index]","566e602c":"print(\"Training set score: {:.2f}\".format(linreg.score(X_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(linreg.score(X_test, y_test)))","943e846f":"Generalized prediction for target value y^ are made by plugging the value of x into the fitted model (the line of best fit).\n\nThe difference between actual observed value of y and predicted value y^ is refered as residuals.\n\nParameter 'w' represents the slop of line and 'b' represents the y-axis intercept. \n\nIf we have more features, w represents the slopes along each feature axis.\n\nWhen we have two features, instead of the 'line of best fit', there is a 'plane of best fit'. ","d6ae8988":"![1.png](attachment:8c05a2f5-9f29-40f1-91ee-6f588366af7f.png)","e161e1cf":"## Check Normality assumption","c8ab6cf9":"R Square measures how much variability in target variable can be explained by the model. It is the square of the Correlation Coefficient(R) and that is why it is called R Square.","2a17ffb3":"If we have more than two features the prediction is represented by hyperplane of best fit.","865c9264":"## Evaluate the model","ef62969c":"Before we apply Linear regression model on the dataset we make four assumptions about the features(X) and target value (Y).\n\n<b>Linearity:<\/b> The relationship between X and the mean of Y is linear.\n\n<b>Homoscedasticity:<\/b> The variance of residual is the same for any value of X.\n\n<b>Independence:<\/b> Observations are independent of each other.\n\n<b>Normality:<\/b> For any fixed value of X, Y is normally distributed.","dd1c58af":"Check the coef_ of top 6 correlated features with target variable.","9bf77e83":"Check if target variable SalePrice can be represented as linear function of input features.","6bc6d9a4":"We are using Kaggle - House Prices - Advanced Regression Techniques Dataset.\n\nThis is modernized and expanded version of the often cited Boston Housing dataset. \n\nThis dataset consists of 79 explanatory variables describing various aspects of residential homes in Ames, Iowa. The objective is to predict the final price of home given its features.","4f8de4e7":"When comparing training set and test set scores, we find that we predict very accurately on the training set, but the R Square on the test set has scope for improvement.","58a25466":"In this notebook we will look into Linear regression.\n\nLinear models are a class of models that are widely used in practice and have been studied extensively in the last few decades, with roots going back to 18th century.\n\nLinear models make a prediction using a linear function of the input features,\n\n\u0177 = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n\nx[0] to x[p] - denotes the features of a single data point\n\np - number of features.\n\nw and b - are the parameters that the model learns. \nVector w[0]...w[p]  is designated as coef_ and 'b' as intercept_ in the model object.\n\n\u0177 - is the prediction the model makes.","b5da25b8":"'Id' is a non-informative column, lets drop it.","5b66750f":"Linear Regression is simple to implement and easier to interpret the output coefficients.\n\nBest to use when it is known that features and target variables have linear relationship.\n\nLinear regression has no parameters, which is a benefit, but it also has no way to control model complexity.\n\nLinear Regression is susceptible to over-fitting.\n\nOutliers can have huge impact on accuracy of the model.\n\nMaking an assumption that there is a straight line relationship between features and target variables is a strong assumption to make.","d70e5baf":"## Split train data ","ad38941f":"## Check Linearity assumption","1d97f2a8":"# Linear Regression","bdfea3b3":"Weights for coef_ for top correlated features seems to be higher than others.","4e27a7a3":"The relationship between a single target variable y and single feature x is modeled with simple linear regression equation, \n\ny^ = w*x + b\n\nThat is, the target value of y^ is a straight-line function of X. ","79ebb9e0":"### R-Square","d24264b9":"## Preprocess Data","37612dc6":"## Build Linear Regression model","28f712a6":"The target variable SalePrice is right-skewed, meaning that the mean is biased towards a higher price than the median. Below we see that log(y+1) provides a nice distribution. ","15ff4073":"R Square is calculated by the sum of squared of prediction error divided by the total sum of the square which replaces the calculated prediction with mean. R Square value is between 0 to 1 and a bigger value indicates a better fit between prediction and actual value.","1be84059":"### check estimated coefficients","cc36c428":"### Fill in missing values","a58cfb1c":"### Load data","afb8880c":"#### Segregate numeric and categorical features","88486e26":"##  Four assumptions associated with a Linear Regression Model","cca00d6b":"Here we have hardcoded the values of w and b, w=0.8 and b=1.76 to fit a straight line. It will be the job of the model to learn these parameters.\n\nThe model selects parameters w and b by choosing the line that  minimizes the squared distance between each 'y' value and the line of best fit.","3ece6558":"## Dataset with a single feature","34028a3b":"Let us plot top 6 correlated features against target variable SalePrice.","dbbab7f9":"## Meet the Data - Housing Price Prediction","efd5f348":"## Pros and Cons of Linear Regression","343732df":"#### currently filling missing numeric feature values with mean value and categorical value with None - negative value. ","3c23fe2b":"![2.png](attachment:3b04d2a4-0460-4389-ad4d-bf89e7813118.png)","9a186f6e":"The main purpose of transforming the target variable is in hopes of assisting the training of a machine learning algorithm."}}