{"cell_type":{"7747e5d5":"code","1b1bb4a0":"code","aa1a71c1":"code","14e742ed":"code","7b2958a0":"code","00f35d4a":"code","0444244b":"code","b9d7dc02":"code","4684bb52":"code","41b1172a":"code","47fcb19b":"code","72942ccf":"code","2cd2b788":"code","6df9f6d7":"code","419c15bf":"code","d67b227d":"code","f1b0813d":"code","a0645d1c":"code","2aaa9fe3":"code","656f8ba2":"code","611fb780":"code","6f78372c":"code","220df126":"code","96b7fd4e":"code","856b32dc":"code","8bf07063":"code","d77005f7":"code","8bf4cbbd":"code","dfddd3a1":"code","477e6739":"code","07f13a9c":"code","7ee709cb":"code","deed25b6":"code","3fc39c8e":"code","bb935ad3":"code","fe1d6ad1":"code","91b522a9":"code","0563303b":"code","42b50106":"code","0804aeb9":"code","cdeab3c8":"code","808b208e":"code","50c26275":"code","42d88ce0":"code","9bfb87f6":"code","a3ba01a6":"code","1cf63189":"code","3f0d4412":"code","443a3c84":"code","afef08ab":"code","ba38ec33":"code","5b79d1b0":"code","9abdcdbf":"code","e3c28928":"code","49ce753f":"markdown","676a4839":"markdown","f1207332":"markdown","f9f4eb1b":"markdown","64e2a5ae":"markdown","8c0373f7":"markdown","fa316101":"markdown","345eb904":"markdown","570969ab":"markdown","4509a5e2":"markdown","e1976e4d":"markdown","6e38a60d":"markdown","a7029f40":"markdown","acfa435b":"markdown","3acd0482":"markdown","8f4eb31b":"markdown"},"source":{"7747e5d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b1bb4a0":"df = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv', parse_dates=['Date'])","aa1a71c1":"df.head()","14e742ed":"df.info()","7b2958a0":"df.describe()","00f35d4a":"df.isna().sum() \/ len(df)","0444244b":"df['Year'] = df.Date.dt.year\ndf['Month'] = df.Date.dt.month\ndf['Day'] = df.Date.dt.day\ndf['Dayofweek'] = df.Date.dt.dayofweek\ndf['Dayofyear'] = df.Date.dt.dayofyear\n\n# Drop original columns Date\ndf.drop('Date', axis=1, inplace=True)","b9d7dc02":"# Explore the data\n\ndf.head()","4684bb52":"# Check which columns have string:\n\nfor label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","41b1172a":"# This will turn all of the string values into category values\n\nfor label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        df[label] = content.astype('category').cat.as_ordered()","47fcb19b":"# Exploration\n\ndf.info()","72942ccf":"for label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","2cd2b788":"# Check which columns have a null values\n\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","6df9f6d7":"# Fill numeric rows with the median\n\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            df[label+'_is_missing'] = pd.isnull(content)\n            df[label] = content.fillna(content.median())","419c15bf":"# Check again\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","d67b227d":"df.head()","f1b0813d":"df.isna().sum() \/ len(df)","a0645d1c":"df.dropna(inplace=True)","2aaa9fe3":"df.isna().sum() \/ len(df)","656f8ba2":"df.info()","611fb780":"for label, content in df.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        df[label+'_is_missing'] = pd.isnull(content)\n        df[label] = pd.Categorical(content).codes+1\n\n# Why codes+1 ? Because pandas encodes missing categories as -1","6f78372c":"# Explore our data\n\ndf.info()","220df126":"df.head()","96b7fd4e":"df_tmp = df.copy()","856b32dc":"# Correlation matrix\n\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(30,20))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                 linewidth=0.5,\n                 fmt='.2f',\n                 cmap='YlGnBu')","8bf07063":"# Train_test_split\n\nfrom sklearn.model_selection import train_test_split\n\nfeatures = df.drop('RainTomorrow',axis=1)  # Our X set\ntarget = df['RainTomorrow']  # Our y set\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","d77005f7":"# Logistic Regression\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nprint(accuracy_score(y_test, lr_predict))\nprint(classification_report(y_test, lr_predict))","8bf4cbbd":"# KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_predict = knn.predict(X_test)\nprint(accuracy_score(y_test, knn_predict))\nprint(classification_report(y_test, knn_predict))","dfddd3a1":"# Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=7)\ntree.fit(X_train, y_train)\ntree_predict = tree.predict(X_test)\nprint(accuracy_score(y_test, tree_predict))\nprint(classification_report(y_test, tree_predict))","477e6739":"# Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(random_state=7)\nforest.fit(X_train, y_train)\nforest_predict = forest.predict(X_test)\nprint(accuracy_score(y_test, forest_predict))\nprint(classification_report(y_test, forest_predict))","07f13a9c":"# Naive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\n\nbayes = GaussianNB()\nbayes.fit(X_train, y_train)\nbayes_predict = bayes.predict(X_test)\nprint(accuracy_score(y_test, bayes_predict))\nprint(classification_report(y_test, bayes_predict))","7ee709cb":"# XGBoost\n\nimport xgboost as xgb\n\nxgb = xgb.XGBClassifier()\nxgb.fit(X_train, y_train)\nxgb_predict = xgb.predict(X_test)\nprint(accuracy_score(y_test, xgb_predict))\nprint(classification_report(y_test, xgb_predict))","deed25b6":"models_default_scores = {\n    'Logistic Regression' : lr.score(X_test, y_test),\n    'KNearest Neighbors' : knn.score(X_test, y_test),\n    'Decision Tree' : tree.score(X_test, y_test),\n    'Random Forest Classifier' : forest.score(X_test, y_test),\n    'Naive Bayes GNB' : bayes.score(X_test, y_test),\n    'XGBoost' : xgb.score(X_test, y_test)\n}","3fc39c8e":"models_default_scores","bb935ad3":"default_models_compare = pd.DataFrame(models_default_scores, index=['accuracy'])\ndefault_models_compare.T.plot.bar()","fe1d6ad1":"# Logistic Regression Grid\nlr_grid = {'C' : np.logspace(-4,4,20),\n           'solver' : ['liblinear', 'saga']}\n\n# Random Forest Classifier Grid\nforest_grid = {'n_estimators' : np.arange(10,600,10),\n               'max_depth' : np.arange(1,12,1),\n               'min_samples_leaf' : np.arange(2,14,2),\n               'min_samples_split' : np.arange(2,14,2)}\n\n# Decision Tree Grid\n\ntree_grid = {'max_depth' : np.arange(1,9,1),\n             'max_features' : np.arange(1,12,1),\n             'min_samples_leaf' : np.arange(1,9,1),\n             'criterion' : ['gini','entropy']}","91b522a9":"from sklearn.model_selection import RandomizedSearchCV\n\nnp.random.seed(7)\n\nlr_cv = RandomizedSearchCV(LogisticRegression(),\n                           param_distributions=lr_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\nlr_cv.fit(X_train, y_train)","0563303b":"lr_cv.best_params_","42b50106":"lr_cv.score(X_test, y_test)","0804aeb9":"lr_y_preds = lr_cv.predict(X_test)","cdeab3c8":"np.random.seed(7)\n\nforest_cv = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1,\n                                                      max_samples=10000),\n                               param_distributions=forest_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n\nforest_cv.fit(X_train, y_train)","808b208e":"forest_cv.best_params_","50c26275":"forest_cv.score(X_test, y_test)","42d88ce0":"forest_y_preds = forest_cv.predict(X_test)","9bfb87f6":"np.random.seed(7)\n\ntree_cv = RandomizedSearchCV(DecisionTreeClassifier(),\n                             param_distributions=tree_grid,\n                             cv=5,\n                             n_iter=20,\n                             verbose=True)\n\ntree_cv.fit(X_train, y_train)","a3ba01a6":"tree_cv.score(X_test, y_test)","1cf63189":"tree_y_preds = tree_cv.predict(X_test)","3f0d4412":"updated_models_scores = {\n    'Logistic Regression' : lr_cv.score(X_test, y_test),\n    'KNearest Neighbors' : knn.score(X_test, y_test),\n    'Decision Tree' : tree_cv.score(X_test, y_test),\n    'Random Forest Classifier' : forest_cv.score(X_test, y_test),\n    'Naive Bayes GNB' : bayes.score(X_test, y_test),\n    'XGBoost' : xgb.score(X_test, y_test)\n}","443a3c84":"# Before\nmodels_default_scores","afef08ab":"# After\nupdated_models_scores","ba38ec33":"# As we can see, we could improve our Logistic Regression, Decision Tree and Random Forest Classifier","5b79d1b0":"# Best of best model is XGBoost\n\nfrom sklearn import metrics\n\nmetrics.plot_roc_curve(xgb, X_test, y_test)","9abdcdbf":"# Our AUC score is 0.90 ! It's good result.\n\nprint(metrics.confusion_matrix(y_test, xgb_predict))","e3c28928":"# Let's print classification report\n\nprint(classification_report(y_test, xgb_predict))","49ce753f":"**Okay, 6 objects have a string type**","676a4839":"# Convert strings (objects) to categories\n**We'll use API pandas**","f1207332":"# Hyperparameter tuning with RandomizedSearchCV","f9f4eb1b":"# What's we could understand ? \n* We have 145460 entries and 23 columns\n* We have some NA values but not more than 50% of column's\n* In data have 6 object's type, other is float64, and 1 is datetime64","64e2a5ae":"# Let's see correlation matrix","8c0373f7":"**First at all, let's convert our date columns to make more data about dates and split this column**","fa316101":"# Predicting Modelling\n**We'll use:**\n* Logistic Regression\n* KNearest Neighbors\n* Decision Trees\n* Random Forest\n* Naive Bayes","345eb904":"# Finally, our model have 86% of accuracy, \n# this is a very good result. Please, if you like my work, you can rate it and leave a comment, I will be very pleased.","570969ab":"# Tune Random Forest Classifier with RandomizedSearchCV","4509a5e2":"# Tune Logistic Regression with RandomizedSearchCV","e1976e4d":"# Now, time for tune categorical values into numerical","6e38a60d":"# 1. **We need to explore the data in more details**","a7029f40":"# Tune Decision Tree Classifier with Randomized Search","acfa435b":"# Now, time for fill missing values\n* We must know, that all our data must be numerical\n* There can't be any missing values","3acd0482":"Let's use dropna() for remove rows with NaN.\n\nThe big advantage is that we won't lose a lot of data.","8f4eb31b":"But, before we start, me must to do train_test_split for split our data on X and y"}}