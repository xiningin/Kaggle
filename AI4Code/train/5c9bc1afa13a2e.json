{"cell_type":{"0834edeb":"code","8f5469e6":"code","b51f668a":"code","0650d966":"code","e0fc5a26":"code","107a18ae":"code","31297e57":"code","c72aedb6":"code","731ac438":"code","b17302bb":"code","b0d8f5fb":"code","07080716":"code","874c2160":"code","5bb76ed6":"code","ad9443c9":"code","82fdec69":"code","01799b53":"code","7fc13395":"code","0c77ecea":"code","3bf63a8d":"code","c199601c":"code","3a0597e0":"code","8612fe30":"code","129bf29b":"code","bc80e44e":"code","a1d20f42":"code","7f7f377d":"code","9d5122a0":"code","a38112ee":"code","9287efc4":"code","ae99ea56":"code","b12b6000":"code","102613b2":"code","de7ea3a2":"code","730d7093":"code","aa4a140d":"code","8542d9ad":"code","38cd829c":"code","88cfd8cf":"code","0510bc46":"markdown","f2d39a70":"markdown","84b39901":"markdown","c3722e42":"markdown","3ade53d2":"markdown","56ce6967":"markdown"},"source":{"0834edeb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom inspect import signature\n\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import *\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","8f5469e6":"# Carregando o DataSet\ndf_treino = pd.read_csv(\"..\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_treino.csv\", index_col = 0)\ndf_teste  = pd.read_csv(\"..\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_teste.csv\", index_col = 0)\n\n#Visualizando as 10 primeiras linhas do dataset\ndf_treino.head(10)","b51f668a":"# examinando os tipos de dados e estat\u00edsticas descritivas \n\nprint (df_treino.info ()) \nprint (df_treino.describe ())","0650d966":"# funcao para avaliar distribui\u00e7\u00e3o dos dados missings em cada atributo\ndef checa_missing(df):\n    for col in df.columns:\n        if df[col].isnull().sum() > 0:\n            print(col, df[col].isnull().sum())","e0fc5a26":"checa_missing(df_treino)","107a18ae":"# fun\u00e7\u00e3o para Avaliar a distribui\u00e7\u00e3o dos dados em cada atributo\ndef checa_distribuicao(df):\n    for col in df.columns:\n        if df[col].dtype == object:\n            print(df.groupby([col])[col].count())\n            print('')","31297e57":"checa_distribuicao(df_treino)","c72aedb6":"df_teste.head()","731ac438":"df_treino.shape","b17302bb":"# Visualizando a distribui\u00e7\u00e3o de classes\n\nprint(\"Class Counts\")\nprint(df_treino[\"target\"].value_counts(), end=\"\\n\\n\")\nprint(\"Class Proportions\")\nprint(df_treino[\"target\"].value_counts()\/len(df_treino[\"target\"]))","b0d8f5fb":"sns.set(style=\"whitegrid\")\n\n#Usando um gr\u00e1fico de barras para mostrar a distribui\u00e7\u00e3o das classes: ativado e n\u00e3o-ativado\nbp = sns.countplot(x=df_treino[\"target\"])\nplt.title(\"Distribui\u00e7\u00e3o de classe do conjunto de dados\")\nbp.set_xticklabels([\"n\u00e3o ativado\",\"ativado\"])\nplt.show()","07080716":"# funcao para transformar os dados categ\u00f3ricos\ndef tranforma(df):\n    labelencoder_X=LabelEncoder()\n    for col in df.columns:\n        if df[col].dtype == object:\n            df[col] = labelencoder_X.fit_transform(df[col].astype(str))\n    return df       ","874c2160":"# Aplicando transforma\u00e7\u00e3o nos dados categ\u00f3ricos\ndf_tratado = tranforma(df_treino)\ndft_tratado = tranforma(df_teste)","5bb76ed6":"# Imputando recursos multivariados para os valores missings\n\nX_tratado = df_tratado.iloc[:,1:]\nimp = IterativeImputer(max_iter=10, initial_strategy='median', random_state=0)\nimp.fit(X_tratado)\nX_tratado = imp.transform(X_tratado)","ad9443c9":"# Aplicando algoritmo de Regress\u00e3o Logistica para sele\u00e7\u00e3o autom\u00e1tica de recursos para o aprendizado\nlsvc = logisticRegr = LogisticRegression(C=10, l1_ratio=0.25, max_iter=800, solver='saga',penalty=\"elasticnet\",fit_intercept=True,multi_class='ovr').fit(X_tratado,  df_tratado[\"target\"])\nmodel = SelectFromModel(lsvc, prefit=True)\nX_svc = model.transform(X_tratado)\nfeature_idx = model.get_support()\nX_svc.shape","82fdec69":"# Mantendo apenas as vari\u00e1veis mais importantes para aplicar no modelo\ncolumns = []\nfor i in range(feature_idx.shape[0]):\n    if feature_idx[i] == True:\n        columns.append(df_tratado.columns[i+1])","01799b53":"# Avalia\u00e7\u00e3o da importancia de cadas vari\u00e1vel no conjunto de dados em rela\u00e7\u00e3o a vari\u00e1vel alvo\n\nfrom sklearn.feature_selection import mutual_info_classif, GenericUnivariateSelect\ndf_mutual_information = mutual_info_classif(X_svc, df_treino[\"target\"])\n\nplt.subplots(1, figsize=(26, 1))\nsns.heatmap(df_mutual_information[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\nplt.yticks([], [])\nplt.gca().set_xticklabels(columns, rotation=90, ha='right', fontsize=12)\nplt.suptitle(\"Verificando a importancia de cada vari\u00e1vel  (mutual_info_classif)\", fontsize=18, y=1.2)\nplt.gcf().subplots_adjust(wspace=0.2)\npass","7fc13395":"# Gerando os dados de treino e de teste para o modelo\nX_feature_train, X_feature_test, y_train, y_test = train_test_split(X_svc, df_treino[\"target\"], test_size=0.2, random_state=42)","0c77ecea":"from xgboost import XGBClassifier\nimport scipy.stats as st\nxgb = XGBClassifier(nthread=1,\n                    silent=False, \n                    scale_pos_weight=0.761199,\n                    scale_neg_weight=0.238801,\n                    learning_rate=0.01,  \n                    colsample_bytree = 0.4,\n                    subsample = 0.33,\n                    rate_drop=0.4,\n                    objective='binary:logistic', \n                    eval_metric='logloss',\n                    n_estimators=600, \n                    reg_alpha = 0.4,\n                    max_depth=7, \n                    gamma=10)\neval_set = [(X_feature_train, y_train), (X_feature_test, y_test)]\nxgb.fit(X_feature_train, y_train,early_stopping_rounds=15, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n#generate predicted classes for test data\npred = xgb.predict(X_feature_test)\n#generate predicted probabilites for test data\npred_prob = xgb.predict_proba(X_feature_test)","3bf63a8d":"#from xgboost import XGBClassifier\n#import scipy.stats as st\n#from sklearn.model_selection import RandomizedSearchCV\n#xgb = XGBClassifier(silent=False, \n#                    scale_pos_weight=1,\n#                    learning_rate=0.01,  \n#                    colsample_bytree = 0.4,\n#                    subsample = 0.8,\n#                    objective='binary:logistic', \n#                    n_estimators=1000, \n#                    reg_alpha = 0.4,\n#                    max_depth=7, \n#                    gamma=10)\n\n#one_to_left = st.beta(10, 1)  \n#from_zero_positive = st.expon(0, 50)\n\n#params = {  \n#    \"n_estimators\": st.randint(900, 1000),\n#    \"max_depth\": st.randint(3, 10),\n#    \"learning_rate\": st.uniform(0.05, 0.4),\n#    \"colsample_bytree\": one_to_left,\n#    \"subsample\": one_to_left,\n#    \"objective\": 'binary:logistic',\n#    \"gamma\": st.uniform(0, 10),\n#    'reg_alpha': from_zero_positive,\n#    \"min_child_weight\": from_zero_positive,\n#}\n# 16\/12\/2019 as 23:40 log-loss 0.47062557673195327\n#xgb = XGBClassifier(silent=False, \n#                    scale_pos_weight=1,\n#                    learning_rate=0.01,  \n#                    colsample_bytree = 0.4,\n#                    subsample = 0.8,\n#                    objective='binary:logistic', \n#                    eval_metric='logloss',\n#                    n_estimators=1000, \n#                    reg_alpha = 0.4,\n#                    max_depth=7, \n#                    gamma=10)\n# 16\/12\/2019 as 23:48 log-loss 0.47062557673195327\n#xgb = XGBClassifier(silent=False, \n#                    scale_pos_weight=1,\n#                    learning_rate=0.01,  \n#                    colsample_bytree = 0.4,\n#                    subsample = 0.8,\n#                    objective='binary:logistic', \n#                    eval_metric='logloss',\n#                    n_estimators=1000, \n#                    reg_alpha = 0.4,\n#                    max_depth=10, \n#                    gamma=10)\n# 0.469607022417657\n#xgb = XGBClassifier(silent=False, \n#                    scale_pos_weight=1,\n#                    learning_rate=0.01,  \n#                    colsample_bytree = 0.4,\n#                    subsample = 0.8,\n#                    objective='binary:logistic', \n#                    eval_metric='logloss',\n#                    n_estimators=1000, \n#                    reg_alpha = 0.4,\n#                    max_depth=12, \n#                    gamma=10)\n# 0.4692065531642619 -> max_depth=15, 0.4691725650704019 -> max_depth=17\n#xgb = XGBClassifier(silent=False, \n#                    scale_pos_weight=0.9,\n#                    learning_rate=0.01,  \n#                    colsample_bytree = 0.4,\n#                    subsample = 0.8,\n#                    objective='binary:logistic', \n#                    eval_metric='logloss',\n#                    n_estimators=1000, \n#                    reg_alpha = 0.4,\n#                    max_depth=10, \n#                    gamma=10)\n#####################################################\n#xgb = XGBClassifier(silent=False, \n#                    scale_pos_weight=1,\n#                    learning_rate=0.01,  \n#                    colsample_bytree = 0.4,\n#                    subsample = 0.8,\n#                    objective='binary:logistic', \n#                    eval_metric='logloss',\n#                    n_estimators=1200, \n#                    reg_alpha = 0.4,\n#                    max_depth=10, \n#                    gamma=10)\n#xgb = XGBClassifier()\n#gs = RandomizedSearchCV(xgb, params, n_jobs=1, verbose=1)  \n#gs.fit(X_feature_train, y_train)  \n#xgb = XGBClassifier()\n#use logistic model to fit training data\n#eval_set = [(X_feature_train, y_train), (X_feature_test, y_test)]\n#xgb.fit(X_feature_train, y_train,early_stopping_rounds=15, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n#generate predicted classes for test data\n#pred = xgb.predict(X_feature_test)\n#generate predicted probabilites for test data\n#pred_prob = xgb.predict_proba(X_feature_test)","c199601c":"# Avalia\u00e7\u00e3o das m\u00e9tricas de desempenho\nresults = xgb.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","3a0597e0":"\n#generate confusion matrix\ncm_xgb = confusion_matrix(y_test, pred)\n#put it into a dataframe\ncm_xgb_df = pd.DataFrame(cm_xgb)\n\n#plot CM\nfig, ax = plt.subplots(figsize = (7,7))\nsns.heatmap(pd.DataFrame(cm_xgb_df.T), annot=True, annot_kws={\"size\": 15}, cmap=\"Purples\", vmin=0, vmax=500, fmt='.0f', linewidths=1, linecolor=\"white\", cbar=False,\n           xticklabels=[\"n\u00e3o ativado\",\"ativado\"], yticklabels=[\"n\u00e3o ativado\",\"ativado\"])\nplt.ylabel(\"Predicted\", fontsize=15)\nplt.xlabel(\"Actual\", fontsize=15)\nax.set_xticklabels([\"n\u00e3o ativado\",\"ativado\"], fontsize=13)\nax.set_yticklabels([\"n\u00e3o ativado\",\"ativado\"], fontsize=13)\nplt.title(\"Confusion Matrix for Logistic Classifier (Threshold = 0.5) - Counts\", fontsize=15)\nplt.show()","8612fe30":"#Generating a Confusion matrix of proportions for logistic model\n\n#converting counts to proportions\ncm_xgb = cm_xgb.astype('float') \/ cm_xgb.sum(axis=1)[:, np.newaxis]\n\ncm_xgb_df = pd.DataFrame(cm_xgb)\nfig, ax = plt.subplots(figsize = (7,7))\nsns.heatmap(pd.DataFrame(cm_xgb_df.T), annot=True, annot_kws={\"size\": 15}, cmap=\"Purples\", vmin=0, vmax=1, fmt='.3f', linewidths=1, linecolor=\"white\", cbar=False,\n           xticklabels=[\"n\u00e3o ativado\",\"ativado\"], yticklabels=[\"n\u00e3o ativado\",\"ativado\"])\nplt.ylabel(\"Predicted\", fontsize=15)\nplt.xlabel(\"Actual\", fontsize=15)\nax.set_xticklabels([\"n\u00e3o ativado\",\"ativado\"], fontsize=13)\nax.set_yticklabels([\"n\u00e3o ativado\",\"ativado\"], fontsize=13)\nplt.title(\"Confusion Matrix for Logistic Classifier (Threshold = 0.5) - Proportions\", fontsize=15)\n\nplt.show()","129bf29b":"#generating a report to extract the measure of interest using built-in sklearn function\nprint(classification_report(y_test,pred))","bc80e44e":"#Plotting the ROC curve\n\n#Generating points to plot on ROC curve (logistic model)\nfpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, pred_prob[:,1])\n\n\nfig, ax = plt.subplots(figsize = (10,7))\n#plotting the \"guessing\" model\nplt.plot([0, 1], [0, 1], 'k--')\n#plotting the logistic model\nplt.plot(fpr_xgb, tpr_xgb)\nplt.fill_between(fpr_xgb, tpr_xgb, alpha=0.2, color='b')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve: AUC={0:0.3f}'.format(roc_auc_score(y_test,pred_prob[:,1])))\nplt.show()","a1d20f42":"#Plot PR curve\n\n#Generating points to plot on recall precision curve\nprecision, recall, thresholds = precision_recall_curve(y_test, pred_prob[:,1])\naverage_precision = average_precision_score(y_test, pred_prob[:,1])\n\n#its a step function so plotting is different \nfig, ax = plt.subplots(figsize = (10,7))\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='orange', alpha=1,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='orange', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall curve: Average Precision={0:0.3f}'.format(average_precision))\nplt.show()","7f7f377d":"print(log_loss(y_test, pred_prob))","9d5122a0":"print(accuracy_score(y_test,pred))","a38112ee":"array_tratado = imp.transform(dft_tratado)","9287efc4":"columns = dft_tratado.columns\nindexs  = dft_tratado.index","ae99ea56":"dft_tratado = pd.DataFrame(array_tratado, columns=columns, index=indexs)","b12b6000":"dft_tratado = dft_tratado.iloc[:, feature_idx]","102613b2":"dft_tratado.head()","de7ea3a2":"dft_tratado.shape","730d7093":"pred_prob = xgb.predict_proba(dft_tratado.values)[:,1]","aa4a140d":"pred_prob","8542d9ad":"sub = pd.read_csv(\"..\/input\/competicao-dsa-machine-learning-dec-2019\/sample_submission.csv\", sep=',',\n                    parse_dates = True, low_memory = False)","38cd829c":"sub['PredictedProb'] = pred_prob","88cfd8cf":"# Salvando \nsub.to_csv(\"sample_submission.csv\", sep=',', index=False)","0510bc46":"## An\u00e1lise Explorat\u00f3ria","f2d39a70":"## Preparando os dados para submiss\u00e3o","84b39901":"## Avaliando a performance do modelo","c3722e42":"## Aplicando o Modelo aos dados de teste","3ade53d2":"## Limpeza e Transforma\u00e7\u00e3o dos dados","56ce6967":"# Solu\u00e7\u00e3o Proposta para Competi\u00e7\u00e3o DSA de Machine Learning\n# Edi\u00e7\u00e3o Dezembro-2019\n\n### Aluno: Aonildo Santos"}}