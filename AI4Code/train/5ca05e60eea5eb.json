{"cell_type":{"a30ecb0d":"code","a9e42e1b":"code","e66a2fef":"code","fb016cb5":"code","9a80f4a8":"code","7315392c":"code","73a84e23":"code","6e7b588d":"code","8f742bf6":"code","e725889f":"code","7cb09558":"code","4cc730a3":"code","e131e91e":"code","26cf5fb0":"code","e389268d":"code","42dcb16e":"code","7838b29f":"code","146a1696":"code","b19d88b1":"code","ca0379be":"code","63a5e099":"code","803dfb30":"markdown","443a4d26":"markdown","a3b0bdc0":"markdown","38a71678":"markdown","91d0b1e7":"markdown","441ca776":"markdown","f3a3751c":"markdown","76618978":"markdown","9aaf1c23":"markdown","1257d827":"markdown","829f14c7":"markdown","bd2c2c3f":"markdown","31a3908d":"markdown"},"source":{"a30ecb0d":"import pandas as pd","a9e42e1b":"dataset_train1 = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv', index_col='id')\ndataset_test1 = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv', index_col='id')\ny = dataset_train1.target\ny = pd.DataFrame(y)\ndataset_train = dataset_train1.drop(['target'], axis=1)","e66a2fef":"for col in dataset_train.columns:\n    if dataset_train[col].dtype == 'float64':\n        dataset_train[col] = dataset_train[col].astype('float32')\n        dataset_test1[col] = dataset_test1[col].astype('float32')\n    if dataset_train[col].dtype == 'int64':\n        dataset_train[col] = dataset_train[col].astype('int8')\n        dataset_test1[col] = dataset_test1[col].astype('int8')","fb016cb5":"#dataset_train['nan_count'] = dataset_train.isnull().sum(axis=1)","9a80f4a8":"#dataset_test1['nan_count'] = dataset_test1.isnull().sum(axis=1)","7315392c":"import tensorflow as tf","73a84e23":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","6e7b588d":"from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer, PowerTransformer","8f742bf6":"qt = QuantileTransformer(n_quantiles=300, output_distribution='uniform')\ndataset_train = qt.fit_transform(dataset_train)\ndataset_test = qt.transform(dataset_test1)","e725889f":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(dataset_train)\ndataset_test_sc = x_scaler.transform(dataset_test)","7cb09558":"from tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\n#from tensorflow.keras.engine.input_layer import Input\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization","4cc730a3":"def model_builder(lr):\n    \"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0434\u043b\u044f \u0430\u0433\u0435\u043d\u0442\u0430\"\"\"\n    inputA = keras.Input(shape=(285))\n    line = Reshape((285,1))(inputA)\n    line = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.1)(line)\n\n    line = Conv1D(filters=16, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.3)(line)\n    \n    line = Conv1D(filters=32, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Conv1D(filters=32, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    line = Conv1D(filters=64, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    line = Conv1D(filters=64, kernel_size=3, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Flatten()(line)\n    line = Dense(64, activation='relu')(line)\n    line = Dropout(0.5)(line)\n    #line = Dense(64, activation='relu')(line)\n    outputA = Dense(units=1, activation=\"sigmoid\", kernel_regularizer=keras.regularizers.l1(0.01))(line)\n    model = Model(inputs=inputA, outputs=outputA)\n    #model = keras.models.load_model('models\/model2')\n    model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr=lr), metrics=[tf.keras.metrics.AUC(name='auc'), 'binary_accuracy'],)\n    return model","e131e91e":"lr=0.0001\nwith strategy.scope():\n    model = model_builder(lr)","26cf5fb0":"model.summary()","e389268d":"checkpoint_filepath = 'pseudo_best.h5'\nsave_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_auc',\n    mode='max',\n    verbose=1,\n    save_best_only=True)","42dcb16e":"model = load_model('..\/input\/tps1021\/best_85221.h5')","7838b29f":"N_split = int(0.2 * len(dataset_train_sc))\ndataset_sc_TRAIN = dataset_train_sc[:-N_split, :]\ndataset_sc_VAL = dataset_train_sc[-N_split:, :]\ny_TRAIN = y[:-N_split]\ny_VAL = y[-N_split:]","146a1696":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=1, min_lr=0.0001, verbose=1, mode='max')","b19d88b1":"#EPOCHS = 200\n#EPOCHS = 1\n#model.fit(\n#    dataset_sc_TRAIN, y_TRAIN,\n#    validation_data=(dataset_sc_VAL, y_VAL), epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], batch_size=4096, shuffle=True)","ca0379be":"#model = load_model('pseudo_best.h5')","63a5e099":"preds = model.predict(dataset_test_sc)\noutput = pd.DataFrame({'Id': dataset_test1.index,'target': preds[:,0]})\npath = 'sample_submission.csv'\noutput.to_csv(path, index=False)\noutput ","803dfb30":"preds = model.predict(dataset_test_sc)","443a4d26":"import numpy as np\nbig_dataset_X = np.concatenate([dataset_sc_TRAIN, dataset_test_sc], axis=0)\nlen(big_dataset_X)","a3b0bdc0":"repeat few times 2 cells above","38a71678":"big_dataset_y = np.concatenate([y_TRAIN, preds], axis=0)\nlen(big_dataset_y)","91d0b1e7":"#pt = PowerTransformer()\n#dataset_train = pt.fit_transform(dataset_train)\n#dataset_test = pt.transform(dataset_test)","441ca776":"EPOCHS = 50\n#EPOCHS = 1\nmodel.fit(\n    big_dataset_X, big_dataset_y,\n    validation_data=(dataset_sc_VAL, y_VAL), epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], batch_size=512, shuffle=True)","f3a3751c":"for i in range(len(preds)):\n    preds[i] = 1 if preds[i] >= 0.5 else 0","76618978":"N = len(preds)\/(len(preds)+len(y_TRAIN))\nN","9aaf1c23":"reduce memory usage","1257d827":"real training","829f14c7":"pseudo training","bd2c2c3f":"repeated pseudo training","31a3908d":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer()\ndataset_train = imputer.fit_transform(dataset_train)\ndataset_test = imputer.transform(dataset_test1)"}}