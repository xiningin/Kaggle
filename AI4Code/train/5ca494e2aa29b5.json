{"cell_type":{"e2fea646":"code","2792dc99":"code","2c279e8d":"code","ff055067":"code","19d3568d":"code","12dcab5c":"code","47a37ac2":"code","529bc873":"code","5efefdfa":"code","5076fac3":"code","d8d0f7ea":"code","e347e754":"markdown","a471b4fb":"markdown","2f1da457":"markdown","30c8d4f5":"markdown","a6f84874":"markdown","148c26f1":"markdown","4856e422":"markdown","2f6150b2":"markdown","a660248b":"markdown","3301a309":"markdown"},"source":{"e2fea646":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2792dc99":"import cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\ntrain_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","2c279e8d":"train_y = np.array(train_df['label'])\ntrain_df.drop(['label'], axis = 1)\ntrain_x = []\nfor i in range(len(train_df)):\n    img = np.array(list(train_df.loc[i]))\n    img = np.resize(img, (28,28,1))\n    train_x.append(img)\ntrain_x = np.array(train_x)\nprint('[INFO] All images read from training data.')\n\ntest_x = []\nfor i in range(len(test_df)):\n    img = np.array(list(test_df.loc[i]))\n    img = np.resize(img, (28,28,1))\n    test_x.append(img)\ntest_x = np.array(test_x)\nprint('[INFO] All images read from testing data.')","ff055067":"train_samples = [train_x[0], train_x[88], train_x[6857], train_x[1365]]\ntest_samples = [test_x[3556], test_x[410], test_x[17], test_x[87]]\n\nfig = plt.figure(figsize = (15,15))\nfor i in range(1,5):\n    ax = fig.add_subplot(1,4,i)\n    ax.imshow(train_samples[i-1], cmap = 'gray')\n    ax.set_title('Training sample '+ str(i))\n    ax.axis('off')\n\nfig = plt.figure(figsize = (15,15))\nfor i in range(1,5):\n    ax = fig.add_subplot(1,4,i)\n    ax.imshow(test_samples[i-1], cmap = 'gray')\n    ax.set_title('Testing sample '+ str(i))\n    ax.axis('off')","19d3568d":"print('Sample label before transformation:',train_y[10])\nTrain_y = keras.utils.to_categorical(train_y, num_classes = 10)\nprint('Sample label after transformation:',Train_y[10])\nprint(Train_y.shape)","12dcab5c":"model = keras.Sequential([\n    keras.layers.Conv2D(32, kernel_size = (3,3), strides = 1, activation = 'relu', padding = 'same', input_shape = (28,28,1), kernel_regularizer = keras.regularizers.l2( l=0.04)),\n    keras.layers.MaxPooling2D(pool_size = (2,2), padding = 'same'),\n    keras.layers.Conv2D(16, kernel_size = (3,3), strides = 1, activation = 'relu', padding = 'same', kernel_regularizer = keras.regularizers.l2( l=0.04)),\n    keras.layers.MaxPooling2D(pool_size = (2,2), padding = 'same'),\n    keras.layers.Conv2D(8, kernel_size = (3,3), strides = 1, activation = 'relu',  padding = 'same', kernel_regularizer = keras.regularizers.l2( l=0.04)),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(256, activation = 'relu'),\n    keras.layers.Dense(10, activation = 'softmax')\n])","47a37ac2":"# To see the model architecture details\nmodel.summary()","529bc873":"#Compile the model\nmodel.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])","5efefdfa":"desired_acc = 0.99\nclass myCallback(keras.callbacks.Callback): \n    def on_epoch_end(self, epoch, logs={}): \n        if(logs.get('accuracy') > desired_acc):   \n            print(\"Reached 99% accuracy, so stopping training.\")   \n            self.model.stop_training = True\n#instantiate the callback\ncallbacks = myCallback()","5076fac3":"model.fit(train_x, Train_y, epochs = 20, batch_size = 16, callbacks=[callbacks])","d8d0f7ea":"test_y = model.predict(test_x)\n# The argmax function fromthe numpy library is used to convert the predicted one-hot vectors into single-digit labels\ny_pred = np.argmax(test_y, axis = 1)\n#Build a data frame and convert it into a csv file\nId = list(range(len(test_df)))\nimageId = [(i+1) for i in Id]\nsubmission_dict =  {'ImageId':imageId, 'label':y_pred}\nsub = pd.DataFrame(submission_dict)\nsub.head()\nsub.to_csv('submission.csv', index = False)","e347e754":"# Test Score\n\nUsing this simple CNN model, I was able to acheive a score of 0.960 on the test data.","a471b4fb":"# 3.) Build the model using Keras Sequential API\n\nNow it is time to build a simple CNN that accepts an input of shape (28,28,1) and outputs a (10,1) one-hot vector to classify the input digit image. \nThe first 6 layers consist of pairs of Convolutional and Max pooling Layers.\nThe output of the last Max Polling layers if flattened and connected to a Fully Connected Layer (Dense Layers) that contains 256 neurons which is then connected to the output layer.","2f1da457":"# 2. Prepare train and test data by reshaping the pixel array into 28X28X1 images","30c8d4f5":"# 5.) Predicting labels for test data","a6f84874":"# Digit Recognizer with a simple CNN (No pre-trained model)\n\n**The aim of my work in this notebook is to use a simple Convolutional Neural Network model built from scratch since we are dealing with images. The input data in the form of 784 pixel values can be reshaped into an image of size 28 X 28 X 1 (1 in the 3rd dimension because we have black and white images, not color images). Furthermore, I believe there is no need to use a pre-trained model or use pre-trained weights for images of such small size.**\n\nHere is a brief overview of the steps performed in the notebook:\n\n1. Import necessary libraries and load the train and test data by reading the csv files.\n\n2. a.) Prepare the train and test data by reshaping the 784 pixel value array into 28X28 image \n\n   b.) Convert the labels in to categorical type.\n   \n3. Build the model using Keras Sequential API.\n\n4. Compile and train the model on the train data.\n\n5. Use the model to predict labels for test data.\n\nI will provide further explanation wherever required alongwith the respective code cells.","148c26f1":"**Let us now visualize a few random images from the train and test data.**","4856e422":"# 4.) Compile and Train the model","2f6150b2":"# 1. Import necessary libraries and load the train and test data .","a660248b":"# 2.b) Convert the labels in to categorical type.\n\nThe labels for the train images are given in the form of a single digit denoting the number in the image (from 0 to 9).\nHowever, as we are using a CNN for multi-class classification, the output layer (last layer) will consist of 10 neurons (each using the softmax activation function). Therefore, the output of the CNN will be a one-hot vector of shape (10,1).\n\nConsequently, to train the model the labels must be converted into one-hot vectors. This is done by using the ***to_categorical*** function from the keras API.\n\nThe code cell below shows a sample label value before and after one-hot vector transformation. It can be seen that all the elements of the one-hot vector are zeros except the one at the index equal to the label value. \n\nIn the CNN it essentially shows that the 8th Neuron should fire-up in the last Dense layer to indicate that the sample input image is a digit 9.\n\nIt is important to note that since we have converted the labels into one-hot vectors, I will use the ***categorical crossentropy*** loss function to compile the model and train it.","3301a309":"# What are Callbacks? Why use them?\n\nA callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc). For more information refer: https:\/\/keras.io\/api\/callbacks\/\n\nCallbacks are usually used for various purposes, some of which are early stopping and learning rate adjustment. I will use callbacks here for early stopping. \n\nEarly Stopping: After each epoch of training, the callback function checks accuracy acheived by the model. If the model has acheived an accuracy value greater than or equal to the **desired accuracy**, then model training is stopped at the same time.\n"}}