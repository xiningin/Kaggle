{"cell_type":{"5e9e5009":"code","0fc40c09":"code","3e0dd0ef":"code","1976d065":"code","4c12d029":"code","d34964a8":"code","8ca1f186":"code","bfdd4a91":"code","16fa3ca5":"code","795455d7":"code","be1de041":"code","e0fc8d3e":"code","700c6967":"code","1f290304":"code","88998dea":"code","6952bd81":"code","e1136640":"code","cb1b21dc":"code","34c7ee2f":"code","be786242":"code","a027dac8":"code","95fdd51f":"code","751f7d31":"code","8a2d8c6d":"code","0d654948":"code","5683a280":"code","e9859a15":"code","80c43cb7":"code","4c4c0c14":"code","88c7869d":"code","c64ffe1c":"code","6ffe4046":"code","537e7982":"code","9c67aa1e":"code","457c82d5":"code","71b51842":"code","d7f1acf7":"markdown","4685ba7f":"markdown","f67d271e":"markdown","dd235108":"markdown"},"source":{"5e9e5009":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom torchvision import datasets, transforms, models \nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nfrom torch.utils.data.dataloader import DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline","0fc40c09":"train_dir = '..\/input\/vegetable-image-dataset\/Vegetable Images\/train'\nval_dir = '..\/input\/vegetable-image-dataset\/Vegetable Images\/validation'\ntest_dir = '..\/input\/vegetable-image-dataset\/Vegetable Images\/test'\nclasses = os.listdir(train_dir)","3e0dd0ef":"train_transform=transforms.Compose([\n        transforms.RandomRotation(10),      # rotate +\/- 10 degrees\n        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n        transforms.Resize(40),             # resize shortest side\n        transforms.CenterCrop(40),         # crop longest side\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n])","1976d065":"trainset = ImageFolder(train_dir, transform=train_transform)\nvalset = ImageFolder(val_dir, transform=train_transform)\ntestset = ImageFolder(test_dir, transform=train_transform)\nprint('Size of training dataset :', len(trainset))","4c12d029":"# view one image shape of the dataset.\nimg, label = trainset[100]\nprint(img.shape)","d34964a8":"# function for the showing the image.\ndef show_image(img, label):\n    print('Label: ', trainset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1,2,0))","8ca1f186":"show_image(*trainset[200])","bfdd4a91":"show_image(*trainset[1000])","16fa3ca5":"#train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\ntrain_ds =trainset\nval_ds =valset\ntest_ds =testset\nlen(train_ds), len(val_ds),len(test_ds)   ","795455d7":"batch_size = 64\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)","be1de041":"for images, labels in train_loader:\n    fig, ax = plt.subplots(figsize=(18,10))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n    break","e0fc8d3e":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","700c6967":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","1f290304":"torch.cuda.is_available()","88998dea":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","6952bd81":"device = get_default_device()\ndevice","e1136640":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","cb1b21dc":"input_size = 3*40*40\noutput_size = 3","34c7ee2f":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                   # Generate predictions\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","be786242":"class CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 100, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(100, 150, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), \n\n            nn.Conv2d(150, 200, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(200, 200, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), \n\n            nn.Conv2d(200, 250, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(250, 250, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), \n\n            nn.Flatten(), \n            nn.Linear(6250, 256),  \n            nn.ReLU(),            \n            nn.Linear(256, 128),  \n            nn.ReLU(),            \n            nn.Linear(128, 64),           \n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(32, len(classes)))\n        \n    def forward(self, xb):\n        return self.network(xb)","a027dac8":"model = CnnModel()\nmodel.cuda()","95fdd51f":"model","751f7d31":"for images, labels in train_loader:\n    out = model(images)\n    print('images.shape:', images.shape)    \n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","8a2d8c6d":"device = get_default_device()\ndevice","0d654948":"train_dl = DeviceDataLoader(train_loader, device)\nval_dl = DeviceDataLoader(val_loader, device)\nto_device(model, device)","5683a280":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","e9859a15":"model = to_device(CnnModel(), device)","80c43cb7":"history=[evaluate(model, val_loader)]\nhistory","4c4c0c14":"num_epochs = 10\nopt_func = torch.optim.Adam\nlr = 0.001","88c7869d":"history+= fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","c64ffe1c":"#history+= fit(num_epochs, lr\/10, model, train_dl, val_dl, opt_func)","6ffe4046":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    plt.show()\n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs')\n    plt.show()","537e7982":"plot_accuracies(history)","9c67aa1e":"plot_losses(history)","457c82d5":"evaluate(model, test_loader)","71b51842":"from sklearn.metrics import classification_report\n\npred = []\nY = []\nfor i, (x,y) in enumerate(test_loader):\n    with torch.no_grad():\n        outputs = model(x)\n    pred += [int(op.argmax()) for op in outputs]\n    Y += [int(yi) for yi in y]\n\nprint(classification_report(Y, pred))","d7f1acf7":"# Vegetable Image Torch Conv2d","4685ba7f":"torch.manual_seed(10)\nval_size = len(dataset)\/\/10\ntest_size = len(dataset)\/\/5\ntrain_size = len(dataset) - val_size - test_size","f67d271e":"# Prediction and classification_report","dd235108":"# Conv2d Model"}}