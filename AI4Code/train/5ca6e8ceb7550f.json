{"cell_type":{"662f125d":"code","d1b76548":"code","511b44f6":"code","ddacdcdf":"code","1ac1b473":"code","ec59ead4":"code","a6ac3784":"code","6cfe414a":"code","873ee88f":"code","fb5d3923":"code","f0bb0143":"code","a480bf82":"code","fa9d5cee":"code","7bd54abb":"code","daa9a5aa":"code","c1237753":"code","6f4c848d":"code","75230b20":"code","937c05be":"code","d21196d7":"code","844c750f":"code","3c1920cc":"code","e5e44cfe":"code","f1eae9e6":"code","36a6d538":"code","9b221dbd":"code","b4c77109":"code","14386eff":"code","22e4ff2a":"code","d3acf25d":"code","761be47b":"code","71117f98":"code","8b7c0050":"code","90becaf7":"code","c559d164":"code","5c264d76":"code","afe6569f":"code","8e0d5e41":"code","eba5a0e1":"code","9bf878ee":"code","75651ebe":"code","ad78a2b8":"code","ca51060c":"code","c5961e2b":"code","d086d048":"code","72825340":"code","572b840a":"code","d56c6095":"code","e5bdad5e":"code","b93835f6":"code","6d8e2131":"code","6496d3aa":"code","97a0e7c0":"code","6a7cfb34":"code","bb9b6e8a":"code","45a5e151":"code","f1f1193a":"code","c771c5fa":"code","30bd72c6":"code","e59fa15b":"code","661bbe52":"code","fdf0cf07":"code","0bdf0f37":"markdown","96b5da76":"markdown","078704f2":"markdown","55c6cd6a":"markdown","050ec94f":"markdown","1d6158e0":"markdown","119a0957":"markdown","d72be689":"markdown","48fe6f5d":"markdown","1654a933":"markdown","ebdea678":"markdown","3f5e6d3b":"markdown","085bd028":"markdown","82329525":"markdown","422d3d23":"markdown","23530bb1":"markdown","0e154913":"markdown","c973db16":"markdown","484ec2c9":"markdown","c211f587":"markdown","35784013":"markdown","c9053b7d":"markdown","c4248890":"markdown","7b099b82":"markdown","1b329fcf":"markdown","03fd0229":"markdown","0527992e":"markdown","7ac4d7dc":"markdown","3dd937df":"markdown","5c5e7725":"markdown","9cbc597f":"markdown","e700c02d":"markdown","482c1d90":"markdown","0ade837f":"markdown","6ccf3284":"markdown","d1ce03c2":"markdown","4dc2ccc2":"markdown","50687903":"markdown"},"source":{"662f125d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1b76548":"!pip install urduhack","511b44f6":"import pandas as pd\nimport numpy as np\n\n# Import Plotting Libararies\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Import Data Preprocessing Libraries \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Machine Learning Models\nfrom sklearn import svm  \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\n\n# Model Evaluation Libraries\nfrom sklearn.metrics import classification_report, confusion_matrix","ddacdcdf":"import urduhack\nurduhack.download()\nfrom urduhack.normalization import normalize\nfrom urduhack.preprocessing import normalize_whitespace, remove_punctuation, remove_accents, replace_urls, replace_emails, replace_numbers, replace_currency_symbols, remove_english_alphabets","1ac1b473":"train_data = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-translated-urdu-reviews\/imdb_urdu_reviews_train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-translated-urdu-reviews\/imdb_urdu_reviews_test.csv\")\n\ntrain_data.head(), test_data.head()","ec59ead4":"# Combine Both Files to Preprocess \ndata =  pd.concat([train_data, test_data]).reset_index(drop=True)\nprint(data.shape)\n","a6ac3784":"# Make copy of a dataset\ndf =  data.copy()\ndf","6cfe414a":"sns.countplot( x = 'sentiment', data = df );","873ee88f":"# Encode the labels\nle = LabelEncoder()\nle.fit(df['sentiment'])\ndf['encoded_sentiments'] = le.transform(df['sentiment'])","fb5d3923":"df['review'] = df['review'].apply(normalize) # To normalize some text, all you need to do pass unicode text. It will return a str with normalized characters both single and combined, proper spaces after digits and punctuations and diacritics(Zabar - Paish) removed.\n# df['review'] = df['review'].apply(remove_punctuation) # Remove punctuation from text by removing all instances of marks. marks=',;:'\ndf['review'] = df['review'].apply(remove_accents) # Remove accents from any accented unicode characters in text str, either by transforming them into ascii equivalents or removing them entirely.\ndf['review'] = df['review'].apply(replace_urls) # Replace all URLs in text str with replace_with str.\ndf['review'] = df['review'].apply(replace_emails) # Replace all emails in text str with replace_with str.\n# df['review'] = df['review'].apply(replace_numbers) # Replace all numbers in text str with replace_with str.\ndf['review'] = df['review'].apply(replace_currency_symbols) # Replace all currency symbols in text str with string specified by replace_with str.\n# df['review'] = df['review'].apply(remove_english_alphabets) # Removes English words and digits from a text\ndf['review'] = df['review'].apply(normalize_whitespace) ## Given text str, replace one or more spacings with a single space, and one or more linebreaks with a single newline. Also strip leading\/trailing whitespace.","f0bb0143":"# Remove stop words from text\nfrom typing import FrozenSet\n\n# Urdu Language Stop words list\nSTOP_WORDS: FrozenSet[str] = frozenset(\"\"\"\n \u0622 \u0622\u0626\u06cc \u0622\u0626\u06cc\u06ba \u0622\u0626\u06d2 \u0622\u062a\u0627 \u0622\u062a\u06cc \u0622\u062a\u06d2 \u0622\u0633 \u0622\u0645\u062f\u06cc\u062f \u0622\u0646\u0627 \u0622\u0646\u0633\u06c1 \u0622\u0646\u06cc \u0622\u0646\u06d2 \u0622\u067e \u0622\u06af\u06d2 \u0622\u06c1 \u0622\u06c1\u0627 \u0622\u06cc\u0627 \u0627\u0628 \u0627\u0628\u06be\u06cc \u0627\u0628\u06d2\n \u0627\u0631\u06d2 \u0627\u0633 \u0627\u0633\u06a9\u0627 \u0627\u0633\u06a9\u06cc \u0627\u0633\u06a9\u06d2 \u0627\u0633\u06cc \u0627\u0633\u06d2 \u0627\u0641 \u0627\u0641\u0648\u06c1 \u0627\u0644\u0628\u062a\u06c1 \u0627\u0644\u0641 \u0627\u0646 \u0627\u0646\u062f\u0631 \u0627\u0646\u06a9\u0627 \u0627\u0646\u06a9\u06cc \u0627\u0646\u06a9\u06d2 \u0627\u0646\u06c1\u0648\u06ba \u0627\u0646\u06c1\u06cc \u0627\u0646\u06c1\u06cc\u06ba \u0627\u0648\u0626\u06d2 \u0627\u0648\u0631 \u0627\u0648\u067e\u0631\n \u0627\u0648\u06c1\u0648 \u0627\u067e \u0627\u067e\u0646\u0627 \u0627\u067e\u0646\u0648\u06ba \u0627\u067e\u0646\u06cc \u0627\u067e\u0646\u06d2 \u0627\u067e\u0646\u06d2\u0622\u067e \u0627\u06a9\u062b\u0631 \u0627\u06af\u0631 \u0627\u06af\u0631\u0686\u06c1 \u0627\u06c1\u0627\u06c1\u0627 \u0627\u06cc\u0633\u0627 \u0627\u06cc\u0633\u06cc \u0627\u06cc\u0633\u06d2 \u0627\u06cc\u06a9 \u0628\u0627\u0626\u06cc\u06ba \u0628\u0627\u0631 \u0628\u0627\u0631\u06d2 \u0628\u0627\u0644\u06a9\u0644 \u0628\u0627\u0648\u062c\u0648\u062f \u0628\u0627\u06c1\u0631\n \u0628\u062c \u0628\u062c\u06d2 \u0628\u062e\u06cc\u0631 \u0628\u0634\u0631\u0637\u06cc\u06a9\u06c1 \u0628\u0639\u062f \u0628\u0639\u0636 \u0628\u063a\u06cc\u0631 \u0628\u0644\u06a9\u06c1 \u0628\u0646 \u0628\u0646\u0627 \u0628\u0646\u0627\u0624 \u0628\u0646\u062f \u0628\u0691\u06cc \u0628\u06be\u0631 \u0628\u06be\u0631\u06cc\u06ba \u0628\u06be\u06cc \u0628\u06c1\u062a \u0628\u06c1\u062a\u0631 \u062a\u0627\u06a9\u06c1 \u062a\u0627\u06c1\u0645 \u062a\u0628 \u062a\u062c\u06be\n \u062a\u062c\u06be\u06cc \u062a\u062c\u06be\u06d2 \u062a\u0631\u0627 \u062a\u0631\u06cc \u062a\u0644\u06a9 \u062a\u0645 \u062a\u0645\u0627\u0645 \u062a\u0645\u06c1\u0627\u0631\u0627 \u062a\u0645\u06c1\u0627\u0631\u0648\u06ba \u062a\u0645\u06c1\u0627\u0631\u06cc \u062a\u0645\u06c1\u0627\u0631\u06d2 \u062a\u0645\u06c1\u06cc\u06ba \u062a\u0648 \u062a\u06a9 \u062a\u06be\u0627 \u062a\u06be\u06cc \u062a\u06be\u06cc\u06ba \u062a\u06be\u06d2 \u062a\u06cc\u0631\u0627 \u062a\u06cc\u0631\u06cc \u062a\u06cc\u0631\u06d2\n \u062c\u0627 \u062c\u0627\u0624 \u062c\u0627\u0626\u06cc\u06ba \u062c\u0627\u0626\u06d2 \u062c\u0627\u062a\u0627 \u062c\u0627\u062a\u06cc \u062c\u0627\u062a\u06d2 \u062c\u0627\u0646\u06cc \u062c\u0627\u0646\u06d2 \u062c\u0628 \u062c\u0628\u06a9\u06c1 \u062c\u062f\u06be\u0631 \u062c\u0633 \u062c\u0633\u06d2 \u062c\u0646 \u062c\u0646\u0627\u0628 \u062c\u0646\u06c1\u0648\u06ba \u062c\u0646\u06c1\u06cc\u06ba \u062c\u0648 \u062c\u06c1\u0627\u06ba \u062c\u06cc \u062c\u06cc\u0633\u0627\n \u062c\u06cc\u0633\u0648\u06ba \u062c\u06cc\u0633\u06cc \u062c\u06cc\u0633\u06d2 \u062d\u0627\u0644\u0627\u0646\u06a9\u06c1 \u062d\u0627\u0644\u0627\u06ba \u062d\u0635\u06c1 \u062d\u0636\u0631\u062a \u062e\u0627\u0637\u0631 \u062e\u0627\u0644\u06cc \u062e\u0648\u0627\u06c1 \u062e\u0648\u0628 \u062e\u0648\u062f \u062f\u0627\u0626\u06cc\u06ba \u062f\u0631\u0645\u06cc\u0627\u0646 \u062f\u0631\u06cc\u06ba \u062f\u0648 \u062f\u0648\u0631\u0627\u0646 \u062f\u0648\u0633\u0631\u0627 \u062f\u0648\u0633\u0631\u0648\u06ba \u062f\u0648\u0633\u0631\u06cc \u062f\u0648\u06ba\n \u062f\u06a9\u06be\u0627\u0626\u06cc\u06ba \u062f\u06cc \u062f\u06cc\u0626\u06d2 \u062f\u06cc\u0627 \u062f\u06cc\u062a\u0627 \u062f\u06cc\u062a\u06cc \u062f\u06cc\u062a\u06d2 \u062f\u06cc\u0631 \u062f\u06cc\u0646\u0627 \u062f\u06cc\u0646\u06cc \u062f\u06cc\u0646\u06d2 \u062f\u06cc\u06a9\u06be\u0648 \u062f\u06cc\u06ba \u062f\u06cc\u06d2 \u062f\u06d2 \u0630\u0631\u06cc\u0639\u06d2 \u0631\u06a9\u06be\u0627 \u0631\u06a9\u06be\u062a\u0627 \u0631\u06a9\u06be\u062a\u06cc \u0631\u06a9\u06be\u062a\u06d2 \u0631\u06a9\u06be\u0646\u0627 \u0631\u06a9\u06be\u0646\u06cc\n \u0631\u06a9\u06be\u0646\u06d2 \u0631\u06a9\u06be\u0648 \u0631\u06a9\u06be\u06cc \u0631\u06a9\u06be\u06d2 \u0631\u06c1 \u0631\u06c1\u0627 \u0631\u06c1\u062a\u0627 \u0631\u06c1\u062a\u06cc \u0631\u06c1\u062a\u06d2 \u0631\u06c1\u0646\u0627 \u0631\u06c1\u0646\u06cc \u0631\u06c1\u0646\u06d2 \u0631\u06c1\u0648 \u0631\u06c1\u06cc \u0631\u06c1\u06cc\u06ba \u0631\u06c1\u06d2 \u0633\u0627\u062a\u06be \u0633\u0627\u0645\u0646\u06d2 \u0633\u0627\u0691\u06be\u06d2 \u0633\u0628 \u0633\u0628\u06be\u06cc\n \u0633\u0631\u0627\u0633\u0631 \u0633\u0645\u06cc\u062a \u0633\u0648\u0627 \u0633\u0648\u0627\u0626\u06d2 \u0633\u06a9\u0627 \u0633\u06a9\u062a\u0627 \u0633\u06a9\u062a\u06d2 \u0633\u06c1 \u0633\u06c1\u06cc \u0633\u06cc \u0633\u06d2 \u0634\u0627\u06cc\u062f \u0634\u06a9\u0631\u06cc\u06c1 \u0635\u0627\u062d\u0628 \u0635\u0627\u062d\u0628\u06c1 \u0635\u0631\u0641 \u0636\u0631\u0648\u0631 \u0637\u0631\u062d \u0637\u0631\u0641 \u0637\u0648\u0631 \u0639\u0644\u0627\u0648\u06c1 \u0639\u06cc\u0646\n \u0641\u0642\u0637 \u0641\u0644\u0627\u06ba \u0641\u06cc \u0642\u0628\u0644 \u0642\u0637\u0627 \u0644\u0626\u06d2 \u0644\u0627\u0626\u06cc \u0644\u0627\u0626\u06d2 \u0644\u0627\u062a\u0627 \u0644\u0627\u062a\u06cc \u0644\u0627\u062a\u06d2 \u0644\u0627\u0646\u0627 \u0644\u0627\u0646\u06cc \u0644\u0627\u0646\u06d2 \u0644\u0627\u06cc\u0627 \u0644\u0648 \u0644\u0648\u062c\u06cc \u0644\u0648\u06af\u0648\u06ba \u0644\u06af \u0644\u06af\u0627 \u0644\u06af\u062a\u0627\n \u0644\u06af\u062a\u06cc \u0644\u06af\u06cc \u0644\u06af\u06cc\u06ba \u0644\u06af\u06d2 \u0644\u06c1\u0630\u0627 \u0644\u06cc \u0644\u06cc\u0627 \u0644\u06cc\u062a\u0627 \u0644\u06cc\u062a\u06cc \u0644\u06cc\u062a\u06d2 \u0644\u06cc\u06a9\u0646 \u0644\u06cc\u06ba \u0644\u06cc\u06d2 \u0644\u06d2 \u0645\u0627\u0633\u0648\u0627 \u0645\u062a \u0645\u062c\u06be \u0645\u062c\u06be\u06cc \u0645\u062c\u06be\u06d2 \u0645\u062d\u062a\u0631\u0645 \u0645\u062d\u062a\u0631\u0645\u06c1 \u0645\u062d\u0636\n \u0645\u0631\u0627 \u0645\u0631\u062d\u0628\u0627 \u0645\u0631\u06cc \u0645\u0631\u06d2 \u0645\u0632\u06cc\u062f \u0645\u0633 \u0645\u0633\u0632 \u0645\u0633\u0679\u0631 \u0645\u0637\u0627\u0628\u0642 \u0645\u0644 \u0645\u06a9\u0631\u0645\u06cc \u0645\u06af\u0631 \u0645\u06af\u06be\u0631 \u0645\u06c1\u0631\u0628\u0627\u0646\u06cc \u0645\u06cc\u0631\u0627 \u0645\u06cc\u0631\u0648\u06ba \u0645\u06cc\u0631\u06cc \u0645\u06cc\u0631\u06d2 \u0645\u06cc\u06ba \u0646\u0627 \u0646\u0632\u062f\u06cc\u06a9\n \u0646\u0645\u0627 \u0646\u06c1 \u0646\u06c1\u06cc\u06ba \u0646\u06cc\u0632 \u0646\u06cc\u0686\u06d2 \u0646\u06d2 \u0648 \u0648\u0627\u0631 \u0648\u0627\u0633\u0637\u06d2 \u0648\u0627\u0642\u0639\u06cc \u0648\u0627\u0644\u0627 \u0648\u0627\u0644\u0648\u06ba \u0648\u0627\u0644\u06cc \u0648\u0627\u0644\u06d2 \u0648\u0627\u06c1 \u0648\u062c\u06c1 \u0648\u0631\u0646\u06c1 \u0648\u063a\u06cc\u0631\u06c1 \u0648\u0644\u06d2 \u0648\u06af\u0631\u0646\u06c1 \u0648\u06c1 \u0648\u06c1\u0627\u06ba\n \u0648\u06c1\u06cc \u0648\u06c1\u06cc\u06ba \u0648\u06cc\u0633\u0627 \u0648\u06cc\u0633\u06d2 \u0648\u06cc\u06ba \u067e\u0627\u0633 \u067e\u0627\u06cc\u0627 \u067e\u0631 \u067e\u0633 \u067e\u0644\u06cc\u0632 \u067e\u0648\u0646 \u067e\u0648\u0646\u06cc \u067e\u0648\u0646\u06d2 \u067e\u06be\u0631 \u067e\u06c1 \u067e\u06c1\u0644\u0627 \u067e\u06c1\u0644\u06cc \u067e\u06c1\u0644\u06d2 \u067e\u06cc\u0631 \u067e\u06cc\u0686\u06be\u06d2 \u0686\u0627\u06c1\u0626\u06d2\n \u0686\u0627\u06c1\u062a\u06d2 \u0686\u0627\u06c1\u06cc\u0626\u06d2 \u0686\u0627\u06c1\u06d2 \u0686\u0644\u0627 \u0686\u0644\u0648 \u0686\u0644\u06cc\u06ba \u0686\u0644\u06d2 \u0686\u0646\u0627\u0686\u06c1 \u0686\u0646\u062f \u0686\u0648\u0646\u06a9\u06c1 \u0686\u06a9\u06cc \u0686\u06a9\u06cc\u06ba \u0686\u06a9\u06d2 \u0688\u0627\u0644\u0646\u0627 \u0688\u0627\u0644\u0646\u06cc \u0688\u0627\u0644\u0646\u06d2 \u0688\u0627\u0644\u06d2 \u06a9\u0626\u06d2 \u06a9\u0627 \u06a9\u0627\u0634 \u06a9\u0628 \u06a9\u0628\u06be\u06cc\n \u06a9\u062f\u06be\u0631 \u06a9\u0631 \u06a9\u0631\u062a\u0627 \u06a9\u0631\u062a\u06cc \u06a9\u0631\u062a\u06d2 \u06a9\u0631\u0645 \u06a9\u0631\u0646\u0627 \u06a9\u0631\u0646\u06d2 \u06a9\u0631\u0648 \u06a9\u0631\u06cc\u06ba \u06a9\u0631\u06d2 \u06a9\u0633 \u06a9\u0633\u06cc \u06a9\u0633\u06d2 \u06a9\u0645 \u06a9\u0646 \u06a9\u0646\u06c1\u06cc\u06ba \u06a9\u0648 \u06a9\u0648\u0626\u06cc \u06a9\u0648\u0646 \u06a9\u0648\u0646\u0633\u0627\n \u06a9\u0648\u0646\u0633\u06d2 \u06a9\u0686\u06be \u06a9\u06c1 \u06a9\u06c1\u0627 \u06a9\u06c1\u0627\u06ba \u06a9\u06c1\u06c1 \u06a9\u06c1\u06cc \u06a9\u06c1\u06cc\u06ba \u06a9\u06c1\u06d2 \u06a9\u06cc \u06a9\u06cc\u0627 \u06a9\u06cc\u0633\u0627 \u06a9\u06cc\u0633\u06d2 \u06a9\u06cc\u0648\u0646\u06a9\u0631 \u06a9\u06cc\u0648\u0646\u06a9\u06c1 \u06a9\u06cc\u0648\u06ba \u06a9\u06cc\u06d2 \u06a9\u06d2 \u06af\u0626\u06cc \u06af\u0626\u06d2 \u06af\u0627 \u06af\u0646\u0627\n \u06af\u0648 \u06af\u0648\u06cc\u0627 \u06af\u06cc \u06af\u06cc\u0627 \u06c1\u0627\u0626\u06cc\u06ba \u06c1\u0627\u0626\u06d2 \u06c1\u0627\u06ba \u06c1\u0631 \u06c1\u0631\u0686\u0646\u062f \u06c1\u0631\u06af\u0632 \u06c1\u0645 \u06c1\u0645\u0627\u0631\u0627 \u06c1\u0645\u0627\u0631\u06cc \u06c1\u0645\u0627\u0631\u06d2 \u06c1\u0645\u06cc \u06c1\u0645\u06cc\u06ba \u06c1\u0648 \u06c1\u0648\u0626\u06cc \u06c1\u0648\u0626\u06cc\u06ba \u06c1\u0648\u0626\u06d2 \u06c1\u0648\u0627\n \u06c1\u0648\u0628\u06c1\u0648 \u06c1\u0648\u062a\u0627 \u06c1\u0648\u062a\u06cc \u06c1\u0648\u062a\u06cc\u06ba \u06c1\u0648\u062a\u06d2 \u06c1\u0648\u0646\u0627 \u06c1\u0648\u0646\u06af\u06d2 \u06c1\u0648\u0646\u06cc \u06c1\u0648\u0646\u06d2 \u06c1\u0648\u06ba \u06c1\u06cc \u06c1\u06cc\u0644\u0648 \u06c1\u06cc\u06ba \u06c1\u06d2 \u06cc\u0627 \u06cc\u0627\u062a \u06cc\u0639\u0646\u06cc \u06cc\u06a9 \u06cc\u06c1 \u06cc\u06c1\u0627\u06ba \u06cc\u06c1\u06cc \u06cc\u06c1\u06cc\u06ba\n\"\"\".split())\n\n\ndef remove_stopwords(text: str):\n    return \" \".join(word for word in text.split() if word not in STOP_WORDS)","a480bf82":"len(STOP_WORDS)","fa9d5cee":"df[['review']].head(10)","7bd54abb":"from urduhack.models.lemmatizer import lemmatizer\ndef lemitizeStr(str):\n    lemme_str = \"\"\n    temp = lemmatizer.lemma_lookup(str)\n    for t in temp:\n        lemme_str += t[0] + \" \"\n    \n    return lemme_str","daa9a5aa":"df['review'] =  df['review'].apply(remove_stopwords)","c1237753":"df['lemmatized_text'] = df['review'].apply(lemitizeStr)","6f4c848d":"df['review'][2], df['lemmatized_text'][2]","75230b20":"df[['review', 'lemmatized_text']].head(10)","937c05be":"df.head()","d21196d7":"X_train, X_test, Y_train, Y_test = train_test_split(df['lemmatized_text'], df['encoded_sentiments'], test_size = 0.30, random_state = 7, shuffle = True)","844c750f":"print('Shape of X_train', X_train.shape)\nprint('Shape of X_test', X_test.shape)\nprint('Shape of Y_train', Y_train.shape)\nprint('Shape of Y_test', Y_test.shape)","3c1920cc":"max_feature_num = 50000\nvectorizer = TfidfVectorizer(max_features=max_feature_num)\ntrain_vecs = vectorizer.fit_transform(X_train)\ntest_vecs = TfidfVectorizer(max_features=max_feature_num, vocabulary=vectorizer.vocabulary_).fit_transform(X_test)","e5e44cfe":"# check the dimensions of feature vectors\ntrain_vecs.shape, test_vecs.shape","f1eae9e6":"def SVM_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    SVM = svm.LinearSVC(max_iter=100)\n    SVM.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionSVM = SVM.predict(test_vecs)\n    return classification_report(test_predictionSVM, Y_test), confusion_matrix(test_predictionSVM, Y_test)","36a6d538":"def LR_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    LR = LogisticRegression()\n    LR.fit(train_vecs, Y_train)\n\n    # testing\n    test_predictionLR = LR.predict(test_vecs)\n    return classification_report(test_predictionLR, Y_test) , confusion_matrix(test_predictionLR, Y_test)","9b221dbd":"def DT_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    DT = DecisionTreeClassifier(max_depth = 9, random_state = 23 )\n    DT.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionDT = DT.predict(test_vecs)\n    return classification_report(test_predictionDT, Y_test), confusion_matrix(test_predictionDT, Y_test) ","b4c77109":"def XGB_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    XGB = xgb.XGBClassifier(colsample_bytree = 0.2, learning_rate = 0.01, n_estimators = 100)\n    XGB.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionXGB = XGB.predict(test_vecs)\n    return classification_report(test_predictionXGB, Y_test), confusion_matrix(test_predictionXGB, Y_test)  \n","14386eff":"def RF_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    RF = RandomForestClassifier(n_estimators = 450, max_depth=9, random_state=43)\n    RF.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionRF = RF.predict(test_vecs)\n    return classification_report(test_predictionRF, Y_test), confusion_matrix(test_predictionRF, Y_test)","22e4ff2a":"class_report , conf_matrix = SVM_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of SVM CLASSIFIER on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)\n","d3acf25d":"class_report , conf_matrix = LR_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Logistic Regression Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","761be47b":"class_report , conf_matrix = DT_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Decision Tree Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","71117f98":"class_report , conf_matrix = XGB_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Xgboost Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","8b7c0050":"class_report , conf_matrix = RF_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Random Forest Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","90becaf7":"# Make copy of Dataset to prepare for Word2Vector\ndf_w2v = df.copy() ","c559d164":"df_w2v.head()","5c264d76":"import spacy\ndef tokenizer(str):\n    nlp = spacy.blank('ur')\n    doc = nlp.tokenizer(str)\n    return [i.text for i in doc]\ndf_w2v[\"tokens\"] = df_w2v[\"lemmatized_text\"].apply(tokenizer)","afe6569f":"import gensim\n\nmodel_word2vec = gensim.models.Word2Vec(sentences=df_w2v[\"tokens\"], size=128, window=5, workers=10, min_count = 1)","8e0d5e41":"model_word2vec.wv.most_similar(\"\u0645\u0631\u062f\")","eba5a0e1":"model_word2vec.wv.most_similar(\"\u0639\u0648\u0631\u062a\")","9bf878ee":"model_word2vec.wv.most_similar(\"\u062e\u0648\u0641\u0646\u0627\u06a9\")","75651ebe":"model_word2vec.wv.most_similar(\"\u0641\u0644\u0645\")","ad78a2b8":"model_word2vec.wv.most_similar(\"\u06a9\u0627\u0631\u0679\u0648\u0646\")","ca51060c":"VOCAB_SIZE = len(model_word2vec.wv.vocab)\nDIMENSIONS = 128\nMAX_LEN = max([len(x) for x in df_w2v[\"tokens\"]])","c5961e2b":"VOCAB_SIZE, DIMENSIONS, MAX_LEN","d086d048":"from keras.preprocessing.text import Tokenizer\ntoken = Tokenizer()\ntoken.fit_on_texts(df_w2v[\"tokens\"])\nencoded = token.texts_to_sequences(df_w2v[\"tokens\"])","72825340":"words2vec_matrix = np.zeros((VOCAB_SIZE+1,DIMENSIONS))\nfor word, index in token.word_index.items():\n    try:\n        words2vec_matrix[index] = model_word2vec.wv[word]\n    except:\n        print(index, word)","572b840a":"import tensorflow as tf\ntrain_vectors = tf.keras.preprocessing.sequence.pad_sequences(encoded,padding='post',dtype=int)","d56c6095":"train_label = df_w2v.encoded_sentiments","e5bdad5e":"type(train_label[0])","b93835f6":"(train_sentences,test_sentences, train_tags, test_tags) = train_test_split(train_vectors, train_label, test_size=0.2, shuffle = True)","6d8e2131":"train_sentences","6496d3aa":"import tensorflow.keras.layers as Layers\nimport tensorflow.keras.activations as Actications\nimport tensorflow.keras.models as Models\nfrom tensorflow.keras.optimizers import Adam, Optimizer, SGD\nimport tensorflow.keras.initializers as Init\nfrom tensorflow.keras import regularizers","97a0e7c0":"model = Models.Sequential()\n\nmodel.add(Layers.Embedding(VOCAB_SIZE+1,DIMENSIONS,\n                           embeddings_initializer = Init.Constant(words2vec_matrix),\n                           input_length=MAX_LEN, trainable=False ))\n\nmodel.add(Layers.Conv1D(512, 5, activation=\"relu\"))\nmodel.add(Layers.MaxPooling1D(5))\n\nmodel.add(Layers.Conv1D(256, 5, activation=\"relu\"))\nmodel.add(Layers.MaxPooling1D(5))\n\nmodel.add(Layers.Conv1D(128, 5, activation=\"relu\"))\nmodel.add(Layers.Dropout(0.3))\nmodel.add(Layers.MaxPooling1D(3))\n\nmodel.add(Layers.Conv1D(64, 3, activation=\"relu\"))\nmodel.add(Layers.Dropout(0.3))\nmodel.add(Layers.MaxPooling1D(3))\n\nmodel.add(Layers.Conv1D(32, 3, activation=\"relu\"))\nmodel.add(Layers.Dropout(0.3))\n\nmodel.add(Layers.Flatten())\n\nmodel.add(Layers.Dense(32, activation='relu', kernel_regularizer = regularizers.l2(1e-4)))\nmodel.add(Layers.Dropout(0.6))\n\nmodel.add(Layers.Dense(1,activation='sigmoid'))\n\nmodel.summary()\n\nmodel.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])","6a7cfb34":"CONV_NET = model.fit( train_sentences, train_tags, epochs=10, validation_split=0.20 )","bb9b6e8a":"plt.plot(CONV_NET.history['accuracy'])\nplt.plot(CONV_NET.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","45a5e151":"plt.plot(CONV_NET.history['loss'])\nplt.plot(CONV_NET.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","f1f1193a":"print(classification_report(model.predict(test_sentences).round(), test_tags))","c771c5fa":"lstm = Models.Sequential()\n\nlstm.add(Layers.Embedding(VOCAB_SIZE+1,DIMENSIONS,\n                          embeddings_initializer = Init.Constant(words2vec_matrix),\n                          input_length=MAX_LEN, trainable=False ))\n\nlstm.add(Layers.Bidirectional(Layers.LSTM(256, activation='tanh')))\n\nlstm.add(Layers.Dense(128, activation='tanh'))\nlstm.add(Layers.Dropout(0.3))\n\nlstm.add(Layers.Dense(64, activation='tanh'))\nlstm.add(Layers.Dropout(0.3))\n\nlstm.add(Layers.Dense(1, activation='sigmoid'))\n\nlstm.summary()","30bd72c6":"from keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n \n\nlstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nes_callback = EarlyStopping(monitor='val_loss', patience=3) \nLSTM_NET = lstm.fit(train_sentences, train_tags, epochs=10, validation_split=0.2, callbacks=[es_callback], shuffle=False)","e59fa15b":"plt.plot(LSTM_NET.history['accuracy'])\nplt.plot(LSTM_NET.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","661bbe52":"plt.plot(LSTM_NET.history['loss'])\nplt.plot(LSTM_NET.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","fdf0cf07":"print(classification_report(lstm.predict(test_sentences).round(), test_tags))","0bdf0f37":"### 4.1 Label Encoding of Target Variable\n\nEncode the target label sentiment.","96b5da76":"## 5. Machine Learning Modeling\n\nWe will apply multiple machine learning models and compare the accuracies.","078704f2":"### 5.3 Decision Tree Classifier","55c6cd6a":"### 4.2 Apply urduhack preprocessing\nNow we will apply text cleaning modules from Urdu Hack Library","050ec94f":" ## 7. Sentiment Analysis using Word to Vector and Deep Learning ","1d6158e0":"The Validation Loss starts increasing after 5 epochs.","119a0957":"## 3. Load Dataset\nDataset is available in 2 sets:\n* Training\n* Testing","d72be689":"### 7.2 Word to Vector Model\nWord2Vec consists of models for generating word embedding. These models are shallow two layer neural networks having one input layer, one hidden layer and one output layer.","48fe6f5d":"## 6. Results - UrduHack and TF-IDF Vectorizer ","1654a933":"### 7.5 Long Short Term Memory\nAs LSTM models works good on text data, as they considered the sequences of inputs to make predictions. We will try LSTM Model on this data. ","ebdea678":"### 2.1 Urduhack\n\nUrduhack is a NLP library for urdu language. It comes with a lot of battery included features to help you process Urdu data in the easiest way possible.\n \nhttps:\/\/docs.urduhack.com\/en\/stable\/\n\n","3f5e6d3b":"### 7.4 1D Convolutional Network\nWe will build convolutional network to build the model. Conv layer are used to extract the feature from data.","085bd028":"We have used spacy tokenizer for urdu text. Urduhack library also provides tokenizer but its bit slow so we used this.","82329525":"### 5.1 Support Vector Machine Classifier","422d3d23":"## 1. Install  Required Libraries","23530bb1":"## 2. Import Libraries","0e154913":"We can see that there are only two classes in our dataset:\n\n* Positive means the review holds positive sentiment.\n* Negative means the review holds negative sentiment.\n\nAlso the class is very balanced. So, it will be easy for us to build any model.","c973db16":"### 7.3 Embedding Layer Preparation","484ec2c9":"## 9. Future Work\n\nIn coming days, I will also build Glove Embeddings for this data and train Deep Learning Models.","c211f587":"* The VOCAB_SIZE shows the size of vocabulary.\n* We set the size of dimension to 128 to reduce the dimensions of data.\n* The MAX_LEN represents the Maximum length of Sentence in dataset.","35784013":"Lets see the distribution of label column which is sentiment.","c9053b7d":"We can see that the model start overfitting after 5 epochs.","c4248890":"## 8. Conclusion\n\nWe have preprocessed the text data using urduhack library and applied multiple algorithms on this dataset and achieved maximum accuracy of 87 percent using Logistic Regression. In the previos notebook i have acieved 88 percent accuracy using SVM but after preprocessing using urduhack the accuracy falls down. So we can conclude that the urduhack text processing preprocess the text quite well but also affects the accuracy.\n\n\nThen we build the Word to Vector embeddings and build deep learning models. We build 2 models, LSTM and Convolutional Network and compare their results and noticed that the LSTM works good on this data and achieved the accuracy of 83 percent. We also achieved the same accuracy with ConvNet but LSTM is more stable in results.","7b099b82":"The lstm model shows same trend, validation loss starts increasing after 5 epochs.","1b329fcf":"Using publically available set of Urdu Text Stopwords we will remove stop words from our text.","03fd0229":"Split the dataset into train and test data.","0527992e":"Lets Check the quality of embeddings generated.","7ac4d7dc":"## 4. Data Preprocessing","3dd937df":"### 4.4 TF - IDF Vectorization\n\nTF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","5c5e7725":"### 5.2 Logistic Regression Classifier","9cbc597f":"### 7.1 Tokenize the Text using Spacy Urdu Tokenizer","e700c02d":"### 5.4 Xgboost Classifier","482c1d90":"### 5.5 Random Forest Classifier","0ade837f":"We used Keras Tokenizer to tokenize the data and made text sequences.","6ccf3284":"### 4.3 Train Test Split","d1ce03c2":"We have now 50,000 records available in our dataset. The size of dataset is good and we can build very good predictive model using this data.","4dc2ccc2":"Make Copy of dataset so we dont have to load again and again","50687903":"Now we have cleansed text in lemmatized_text and encoded version of sentiment column as encoded_sentiments.\n\nData is prepared for the Modeling."}}