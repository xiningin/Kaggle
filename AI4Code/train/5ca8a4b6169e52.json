{"cell_type":{"796e8ec3":"code","7e98b183":"code","efdd7625":"code","6cff7ebd":"code","49935b07":"code","a2e68c09":"code","543685f0":"code","054f77e6":"code","66aa74ef":"code","88753dfc":"code","5aedcca1":"code","b3870190":"code","72eb36b4":"code","e052b892":"code","98596f50":"code","34bc8b20":"code","34bace59":"code","ac61efcc":"code","f9132db5":"markdown","7e0d9a4b":"markdown","36ece6d2":"markdown","0e3c9cae":"markdown","72d8bc85":"markdown","1a2467e1":"markdown","7568c076":"markdown","e7190795":"markdown","519f96a0":"markdown","b6d99950":"markdown","bbe3e07b":"markdown","ccb214a6":"markdown","2c40cb81":"markdown","78144408":"markdown"},"source":{"796e8ec3":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\nimport matplotlib.pyplot as plt","7e98b183":"data_dir = '..\/input\/kvasirdatasetv2'","efdd7625":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.3,\n                                                              subset='training',seed=123,image_size=(305,305),\n                                                              batch_size=16)","6cff7ebd":"valid_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.2,\n                                                              subset='validation',seed=123,image_size=(305,305),\n                                                              batch_size=16)","49935b07":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.1,\n                                                              subset='validation',seed=123,image_size=(266,266),\n                                                              batch_size=16)","a2e68c09":"plt.figure(figsize = (10,10))\nclass_names = train_ds.class_names\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i+1)\n        plt.imshow(images[i].numpy().astype('uint8'))\n        plt.title(class_names[labels[i]])\n        plt.axis('off')","543685f0":"data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Normalization(mean=0.485, variance=0.229),\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n])\ntrain_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))","054f77e6":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.3,\n                                                              subset='training',seed=123,image_size=(230,230),\n                                                              batch_size=16)","66aa74ef":"valid_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.2,\n                                                              subset='validation',seed=123,image_size=(230,230),\n                                                              batch_size=16)","88753dfc":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.1,\n                                                              subset='validation',seed=123,image_size=(230,230),\n                                                              batch_size=16)","5aedcca1":"resnet = tf.keras.applications.ResNet50(weights='imagenet', input_shape=(224,224,3))\n\n# In the prediction layer, we would also be using the kernel regularizer part to reduce the squared error\n# Regularizer has been used to reduce the chances of overfitting\n# Dropout has been used to reduce the chances of overfitting\n# Batch Normalization has been used to reduce error\n\nprediction_layer = tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer='l2')\nbatch_norm = tf.keras.layers.BatchNormalization()\nmodel = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Normalization(mean=0.485, variance=0.229),\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n    tf.keras.layers.Conv2D(filters=3, kernel_size=7,strides=(1,1), padding='valid'),\n    resnet,\n    tf.keras.layers.Dropout(0.5),\n    batch_norm,\n    prediction_layer])\n\nlr = 1e-4\nopt = Adam(lr)\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), tf.keras.metrics.Precision()])\n\ncheckpoint = ModelCheckpoint(\"resnet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\nearlystop = EarlyStopping(monitor=\"val_accuracy\",patience=5,mode=\"auto\",verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, \n                              patience = 2, min_delta = 0.001, \n                              mode = 'auto', verbose = 1)\n\nhistory = model.fit(train_ds,epochs=15,validation_data = valid_ds,\n                     callbacks=[checkpoint,earlystop,reduce_lr])","b3870190":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","72eb36b4":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","e052b892":"preds = model.evaluate(test_ds, batch_size=16)\nprint(preds)","98596f50":"mobilenet = tf.keras.applications.MobileNet(weights='imagenet', input_shape=(224,224,3))\n\nprediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\nbatch_norm = tf.keras.layers.BatchNormalization()\nmodel = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Normalization(mean=0.485, variance=0.229),\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n    tf.keras.layers.Conv2D(filters=3, kernel_size=7,strides=(1,1), padding='valid'),\n    mobilenet,\n    tf.keras.layers.Dropout(0.5),\n    batch_norm, \n    prediction_layer])\n\nlr = 1e-4\nopt = Adam(lr)\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), tf.keras.metrics.Precision()])\n\ncheckpoint = ModelCheckpoint(\"mobilenet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\nearlystop = EarlyStopping(monitor=\"val_accuracy\",patience=5,mode=\"auto\",verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, \n                              patience = 2, min_delta = 0.001, \n                              mode = 'auto', verbose = 1)\n\nhistory = model.fit(train_ds,epochs=15,validation_data = valid_ds,\n                     callbacks=[checkpoint,earlystop,reduce_lr])","34bc8b20":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","34bace59":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","ac61efcc":"preds = model.evaluate(test_ds, batch_size=16)\nprint(preds)","f9132db5":"# **Augmentations**\nAugmentations are necessary in order to make the dataset robust. These would ensure better classifications since the images are subjected to several modifications based on shear, flips and normalization. Hence this would make the model more robust to the changes in the images","7e0d9a4b":"Since we have used ResNet50 here, we would be requiring an image dataset of different frame size (224,224,3). Again, since we have implemented Conv2D layer before the introduction of the ResNet50 model, we have to change the frame size to (230,230,6) for the model to read the dataset. In the next lines, the conversion would be made.","36ece6d2":"# **Plotting**\nPlots have been made that would show the changes in the training and the validation dataset in terms of accuracy and loss. This has been plotted against the number of epochs that we move forward to.","0e3c9cae":"Since we have used MobileNet here, we would be requiring an image dataset of different frame size (224,224,3). Again, since we have implemented Conv2D layer before the introduction of the ResNet50 model, we have to change the frame size to (230,230,6) for the model to read the dataset. In the next lines, the conversion would be made.","72d8bc85":"# **Results**\nNow we have to test the model with our test dataset. The following lines would make the same possible.","1a2467e1":"# **Results**\nNow we have to test the model with our test dataset. The following lines would make the same possible.","7568c076":"# **Model [MobileNet]**\nHere, MobileNet model has been displayed. It has to be considered while making the model that there is sufficient room for the applications of the same in mobile (low-powered) devices and hence models have to be chosen in such a way that they would be implementable in such devices without making the OS killing their processes.","e7190795":"# **Importing all the modules**\nAll the necessary modules are imported. These modules would be used later in the notebook and would help in the determination of many features in the total image dataset","519f96a0":"# **Visualize**\nThe below cell would give a view of the dataset in arrangement with therir corresponding classes.","b6d99950":"# **Data Directory**\nThe data directory has been mentioned in the below code cell","bbe3e07b":"Training the dataset in our prepared ResNet50 base model","ccb214a6":"Training, validation and the Testing datasets have been prepared from the current dataset and the have been resized on the basis of the requirements of each model that has been implemented","2c40cb81":"# **Model [ResNet50]**\nHere, ResNet50 model has been displayed. It has to be considered while making the model that there is sufficient room for the applications of the same in mobile (low-powered) devices and hence models have to be chosen in such a way that they would be implementable in such devices without making the OS killing their processes.","78144408":"# **Plotting**\nPlots have been made that would show the changes in the training and the validation dataset in terms of accuracy and loss. This has been plotted against the number of epochs that we move forward to."}}