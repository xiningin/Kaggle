{"cell_type":{"4ee735e9":"code","8abc0952":"code","636c00d7":"code","12fcfd7e":"code","828a440e":"code","ff1cfbf9":"code","93079adb":"code","0e0e3e40":"code","ac438852":"code","0fdf2957":"code","6d91b8db":"code","1777ecd2":"code","b1fb4d0e":"code","a5540da2":"code","e865a197":"code","e0b8d69c":"code","b1185beb":"code","4648cefe":"code","eb04ecd6":"code","767a3385":"code","ab77b68a":"markdown","62437513":"markdown","100fc726":"markdown","4309f5b2":"markdown","b8141945":"markdown","1830fab3":"markdown"},"source":{"4ee735e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8abc0952":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import r2_score,mean_squared_error","636c00d7":"df = pd.read_csv('..\/input\/pizza-price-prediction\/pizza_v2.csv')","12fcfd7e":"df.dtypes","828a440e":"df.isnull().sum()","ff1cfbf9":"df.info()","93079adb":"df.describe()","0e0e3e40":"def preprocessing(df):\n    df_copy = df.copy()\n    \n    X = df_copy\n    \n    X['price_rupiah'] = X.price_rupiah.str.replace('Rp', '')\n    X['price_rupiah'] = X.price_rupiah.str.replace(',', '').astype(\"int\")\n    X['diameter'] = X.diameter.str.replace(' inch', '').astype(\"float\")\n    \n    y = X['price_rupiah']\n        \n    return X,y","ac438852":"X ,y= preprocessing(df)","0fdf2957":"cat_features = X.iloc[:,3:].columns.tolist()\ncat_features","6d91b8db":"sns.countplot(x=\"company\", data=X)","1777ecd2":"RP=plt.figure(figsize=(15,18))\nfor i, feature in enumerate(cat_features):\n    r=RP.add_subplot(5,2,i+1)\n    sns.countplot(x=feature, data=X)\n    r.set_title(feature+\" Histogram Plot\",color='DarkRed')\nRP.tight_layout()  ","b1fb4d0e":"RP=plt.figure(figsize=(15,18))\nfor i, feature in enumerate(cat_features):\n    r=RP.add_subplot(5,2,i+1)\n    sns.boxplot(x=X[feature], y =X[\"price_rupiah\"])\n    r.set_title(feature+\" Histogram Plot\",color='DarkRed')\nRP.tight_layout()  ","a5540da2":"fig = px.scatter(X, x=\"diameter\", y=\"price_rupiah\", color='company')\nfig.show()","e865a197":"X.groupby(['company'])['price_rupiah'].mean().sort_values(ascending=False).head(30).plot(kind='bar') ","e0b8d69c":"def label_encoded(feat):\n    le = LabelEncoder()\n    le.fit(feat)\n    return le.transform(feat)","b1185beb":"X['company'] = label_encoded(X['company'])\nX['topping'] = label_encoded(X['topping'])\nX['variant'] = label_encoded(X['variant'])\nX['size'] = label_encoded(X['size'])\nX['extra_sauce'] = label_encoded(X['extra_sauce'])\nX['extra_cheese'] = label_encoded(X['extra_cheese'])\nX['extra_mushrooms'] = label_encoded(X['extra_mushrooms'])","4648cefe":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state =42)","eb04ecd6":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","767a3385":"models=[('LR',LinearRegression()),('DT',DecisionTreeRegressor()),('RF',RandomForestRegressor()),('XGB',XGBRegressor())\n        ,('LOG R',LogisticRegression(solver='liblinear')),('SVR',SVR())]\nscores=[]\n\nfor name,model in models:\n    model.fit(X_train,y_train)\n    print(model.score(X_test, y_test))\n    forest_test_pred=model.predict(X_test)\n    forest_train_pred = model.predict(X_train)\n    \n    score=explained_variance_score(forest_test_pred,y_test)\n    scores.append([name,model,model.score(X_test, y_test)])\n    \n    print('MSE train data: %.3f, MSE test data: %.3f' % (\n    mean_squared_error(y_train,forest_train_pred),\n    mean_squared_error(y_test,forest_test_pred)))\n    print('R2 train data: %.3f, R2 test data: %.3f \\n' % (\n    r2_score(y_train,forest_train_pred),\n    r2_score(y_test,forest_test_pred)))\n\nscores_df=pd.DataFrame(scores,columns=['Name','Model','Score'])\nscores_df.sort_values('Score',ascending=False)","ab77b68a":"<font size=\"8\">**EDA**<\/font>","62437513":"There are outliners in some datas , but I ignore these values this time , beacause total datas is not much as a whole.\nSo I keep this data distribution so far .","100fc726":"**<font size=\"8\">Building a model<\/font>**","4309f5b2":"<font size=\"8\">**Preprocessing**<\/font>","b8141945":"**<font size=\"8\">LabelEncoding and split the data<\/font>**","1830fab3":"<font size=\"5\">each company is almost equivalent .<\/font>"}}