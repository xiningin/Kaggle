{"cell_type":{"edb5f01b":"code","a8728fbb":"code","93602c05":"code","b89da8ab":"code","30506810":"code","8a0d79b2":"code","fcf3f7db":"code","73ea7f50":"code","c4e5fc99":"code","1c220e4c":"code","104f0a07":"markdown","0bcac501":"markdown","66218c51":"markdown","f8d3f8a1":"markdown","4e332d7b":"markdown","ad4e1c26":"markdown","6bc91b79":"markdown","ac1feed6":"markdown","7b6f8c26":"markdown","bb811b61":"markdown","443bd943":"markdown","e2eacbe3":"markdown","9f04c3b0":"markdown","c5d71795":"markdown","004fa841":"markdown","3c2fe2bc":"markdown","3c490e04":"markdown","9b613305":"markdown","1f43b387":"markdown","c78e20b8":"markdown"},"source":{"edb5f01b":"# import the Azure ML libs.\n!pip install azureml\n!pip install azureml.core\n!pip install azureml.widgets\n!pip install azureml.train\n!pip install azureml.dataprep\n\nimport azureml.core\nimport azureml.widgets \nprint(\"Ready to use Azure ML\", azureml.core.VERSION)\nfrom azureml.core import Workspace","a8728fbb":"## In this segment you should replace the 3-parameters values according to the workspace available in the subscription\n## ths experiment will not work beyond this point if these values are not appropriatly inserted.\n## HENCE, THE Notebook Execution will terminate\n\n## Example - \n    ## ws = Workspace.get(name=\"<<MLSERVICENAME>>\", subscription_id='<<GUID - ML Service ID>>', resource_group='<<Hosting Azure Resource Group>>')\n\n# Pulling values from Kaggle Secrets\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nMLServiceName = user_secrets.get_secret(\"MLServiceName\")\naz_resource_grp  = user_secrets.get_secret(\"az_resource_grp\")\nsub_id = user_secrets.get_secret(\"sub_id\")\n## Instanciating the Workspace object.\nws = Workspace.get(name=MLServiceName, subscription_id=sub_id, resource_group=az_resource_grp)\nprint(ws.name, \"loaded\")","93602c05":"from azureml.core import Dataset\ndefault_ds = ws.get_default_datastore()\nif 'flower tab ds' not in ws.datasets:\n    default_ds.upload_files(files=['..\/input\/iris-flower-dataset\/IRIS.csv'],\n                 target_path='flower_data\/',\n                 overwrite=True, show_progress=True)\n    \n    # Creating tabular dataset from files in datastore.\n    tab_dataset = Dataset.Tabular.from_delimited_files(path=(default_ds,'flower_data\/*.csv'))\n    try:\n        tab_dataset = tab_dataset.register(workspace=ws, name='flower tab ds', description='Iris flower Dataset in tabular format', tags={'format':'CSV'}, create_new_version=True)\n    except Exception as ex:\n        print(ex)\nelse:\n    print('Dataset already registered')","b89da8ab":"import os\n\ntab_ds_experiment = \"tab_dataset_experiment\"\nos.makedirs(tab_ds_experiment,exist_ok=True)\nprint(tab_ds_experiment, 'created')","30506810":"%%writefile $tab_ds_experiment\/iris_simple_DTexperiment.py\nfrom azureml.core import Run\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport joblib\nimport os\n\n# Get the experiment run context -  we are going to pass this configuration later\nrun = Run.get_context()\n\n# load the data from a dataset -  passed as an \"inputs\" to the script\ndata = run.input_datasets['flower_ds'].to_pandas_dataframe()\nX = data[['sepal_length', 'sepal_width','petal_length','petal_width']].values\nY= data[['species']].values\n\n#Split data into train and test set\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.25, random_state=1234)\n# fit the model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=1234).fit(X_train,Y_train)\n\nY_pred = model.predict(X_test)\naccuracy = np.average(Y_test == Y_pred)\nprint(\"accuracy: \" + str(accuracy))\nrun.log(\"Accuracy\", np.float(accuracy))\n\n# Save the trained model in the \"outputs\" folder. The \"outputs\" folder is standard output folder for AML.\nos.makedirs(\"outputs\", exist_ok=True)\njoblib.dump(value=model, filename='outputs\/iris_simple_DTmodel.pkl')\n# Complete the run\nrun.complete()","8a0d79b2":"from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\niris_env = Environment(\"iris_trn_environment\")\niris_env.python.user_managed_dependencies = False\niris_env.docker.enabled = False\n\niris_deps = CondaDependencies.create(conda_packages=[\"scikit-learn\",\"pandas\",\"numpy\"],\n                               pip_packages=[\"azureml-defaults\",'azureml-dataprep[pandas]'])\niris_env.python.conda_dependencies = iris_deps","fcf3f7db":"iris_env.register(workspace=ws)\n# carefully notice the json returened ","73ea7f50":"## Get the list of already registered and custom environments by using - \n\nfor env_name in Environment.list(workspace=ws):\n    print(\"Environment Name\",env_name)","c4e5fc99":"from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\ncluster_name = 'aml-cluster'\ntry:\n    trn_cluster = ComputeTarget(workspace=ws, name=cluster_name)\nexcept ComputeTargetException:\n    compute_config=AmlCompute.provisioning_configuration(vm_size='STANDARD_D1', max_nodes=4)\n    trn_cluster=ComputeTarget.create(ws,cluster_name,compute_config)\ntrn_cluster.wait_for_completion(show_output=True)","1c220e4c":"from azureml.train.sklearn import SKLearn ## note - directly using SKLearn as estimator, hence avoid using conda_packages\nfrom azureml.core import Experiment, Dataset, Environment\nfrom azureml.widgets import RunDetails\n\n# Get the previously registered Environmnt.\nreg_env = Environment.get(ws,'iris_trn_environment')\n\n# Get the previously registered tabular flower dataset\ntab_dataset = ws.datasets.get('flower tab ds')\n\n# Create an estimator, look into the 'compute_target' & 'environment_definition' additional parameters and their values\nestimator = SKLearn(source_directory=tab_ds_experiment, ## pointing to the correct experiment folder for this context\n                      entry_script='iris_simple_DTexperiment.py',\n                      compute_target=cluster_name, ## Pass the value of cluster created as above\n                      environment_definition = reg_env, ## Pass the value of registered  Environment as created already\n                      use_docker=False,\n                      inputs=[tab_dataset.as_named_input('flower_ds')] ## pass the 'tab_dataset' as input to the experiment\n                      )\n\n# Create an experiment\nexperiment_name = 'iris-compute-experiment'\nexperiment = Experiment(workspace = ws, name = experiment_name)\n\n# Run the experiment based on the estimator\nrun = experiment.submit(config=estimator)\n\n# Get Run Details\nRunDetails(run).show()\n\n# Wait to complete the experiment. In the Azure Portal we will find the experiment state as preparing --> finished.\nrun.wait_for_completion(show_output=True)","104f0a07":"## Create & Manage Compute Targets\n* Azure ML Compute Targets are physical or virtual computers on which experiments are run\n* In General -  Code can be developed and tested on low cost 'local' compute. Trained on high-end GPU machines. Deployed and inferenced on low-cost CPU machines.\n* Start\/Stop Scale in\/out on demand with cloud capabilities.\n* Types of **'Compute Targets'**:\n    * Local -  This is used to run the experiment on the same compute target as the code used to initiate the experiment.\n    * Training Clusters - for high scalable training requirements - distributed computes, CPU\/GPU are enabled and scaled on-demand.\n    * Inference Clusters - containerized clusters to deploy the inference of the trained model as an overall application module.\n    * Attached Compute -  to attach already acquired Azure ML VM or Databricks machine.\n    \n![image.png](attachment:image.png)","0bcac501":"![image.png](attachment:image.png)","66218c51":"## Recap: Prepare Data","f8d3f8a1":"## Recap: Azure Machine Learning Service\n#### Expand the section below to learn from the previous Notebooks and quick recap of the concepts covered so far.","4e332d7b":"## What Next?\nIn the next notebook I will show to create E2E Machine Learning project pipeline which can later be utilized for the model deployment on Inference clusters. Stay Tuned...","ad4e1c26":"![image.png](attachment:image.png)","6bc91b79":"## Import AML libraries & workspace\nThe following (hidden) section will load required AML libraries and workspace required to run the experiment.","ac1feed6":"The code below provision the **Training Cluster  -> 'aml-cluster'** used for training purpose. \nDefine size of the VM, max nodes and regestering the same with the workspace.","7b6f8c26":"<u>**IMPORTANT NOTE**<\/u>\n\n>Please proceed with this example **iff** you are familier with foundation of Microsoft Azure public cloud. In this notebook, the basics of Microsoft Azure and its development methodology is not covered. As it will be beyond the scope of this notebook.\n\n* [PART 1: Azure Machine Learning service - Introduction](https:\/\/www.kaggle.com\/pankaj1234\/azure-machine-learning-introduction): in the first notebook I have already discussed about the nitty-gritty of Azure ML service.\n    * Creating an instance of Azure ML service\n    * Downloading libraries\/dependencies (in Kaggle environment)\n    * Various methods to connect to Azure ML service workspace: using config file and using get() method.\n    * Simple ML experiment - for data exploration. Capture the details from the experiment, logging and preserving the run details from the experiment.\n    * Overview of Azure ML Service Dashboard. Experiment Dashboard.\n    \n* [PART 2: Azure Machine Learning service - Introduction II](https:\/\/www.kaggle.com\/pankaj1234\/azure-machine-learning-introduction-ii): I did deep-dive into the introduction and covered some more topics:\n    * Create and run the experiment using <u>Custom Script<\/u> file. Implementing simple Logistic Regression model on IRIS dataset.\n    * RunConfiguration and ScriptRunConfiguration, these classes were used to define the runtime environment for the custom script.\n    * Output the model and run details to the external folder for future referencing.\n\n* [PART 3: Azure Machine Learning service - Model Training](https:\/\/www.kaggle.com\/pankaj1234\/azure-machine-learning-model-training): I discussed about: \n    * Create and run the experiment using <u>Parameterized Custom Script<\/u> file. Passing a parameter value uning argparse allowed me to run the experiment using different settings.\n    * **ESTIMATOR** object was used to encapsulate both RunConfiguration and ScriptRunConfiguration. Also dicussed about Generic estimator and frame-work specific estimators such as SKLearn, TensorFlow, etc... \n    * Register the model, model versions and its metadata. It is the foundation of any model deployment over various compute with its dependencies (this i will discuss much later). \n* [PART 4: Azure Machine Learning - Working with Data](https:\/\/www.kaggle.com\/pankaj1234\/azure-machine-learning-working-with-data): I discussed about: \n    * Describe cloud **DATASTORES** and ways to use and manage them in a ML project. \n    * Enable remote **DataSets** to train any registered models. they are a versioned reference to a specific set of data that we want to use in an experiment","bb811b61":"## Create Environment\nThe experiment which we were running so far, we were running it through default Coda environment via 'local' compute. For production like scenarios to have better control on the execution environment AML service provide an abstraction to define the custom environment specific to the experiment's need. This environment definition then can repeatedly applied to any execution environment.\n\nThe code below define such environment by instantiating from 'CondadepenDependencies' object and then passing 'conda_packages' and 'pip_packages' required by experiment to run.\n\n> Additional Info: There are many other ways to create and manage package in AzureML see this [link](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments)","443bd943":"#### Provisioning of the Node\n![image.png](attachment:image.png)","e2eacbe3":"## Register Environment\nOnce the environment definition is created we need to register it in the workspace so that it can be reused later.","9f04c3b0":"## Training Using Environment & Managed Compute\nBelow is the crux of the entire experiment  - using the environment and compute targets as defined above, run the experiment with the estimator.\n> Note: The experiment will take quite a lot longer because a container image must be built with the conda environment, and then the cluster nodes must be started and the image deployed before the script can be run.","c5d71795":"## Recap: Create Training Script","004fa841":"![image.png](attachment:image.png)","3c2fe2bc":"# Azure Machine Learning Service - Execution Environments & Compute\nIn this fifth part from the same Azure Machine Learning Service (**AML**) series I will do a experiment to :\n* Describe AML **Environment** provisioning options. The environment definition and configuration allow us to deploy model consistently across various compute platform thus reducing the overhead of maintaining separately.\n* Describe various types of AML **Compute Targets** options and ways to use and manage them in a ML project. \n* The Compute Targets allows us to provision an execution environment according to the experiments training, validation and deployment needs.","3c490e04":"#### Cluster Provisioning\n![image.png](attachment:image.png)","9b613305":"![image.png](attachment:image.png)","1f43b387":"## Registered Environments\nFollowing is the list of all registered environments including the 'Default' environments already available with AMS service workspace.","c78e20b8":"#### Experiment submitted and running on the cluster\n![image.png](attachment:image.png)"}}