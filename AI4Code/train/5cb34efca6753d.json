{"cell_type":{"a384e15f":"code","04dc5084":"code","1a266654":"code","e0b35176":"code","9179e0f7":"code","2f6e4e58":"code","d16e7252":"code","d6f7d2ce":"code","c678cd2c":"code","43d5434c":"code","3725681a":"code","53b50531":"code","38860adc":"code","e555f49f":"code","500fe82a":"code","609e6307":"code","03a48838":"code","29c4a1fb":"code","7d2d3e3f":"code","9f21dfbf":"code","4a6e85d1":"code","9edcd075":"code","10d23bd7":"code","937314cb":"code","3d5ab467":"code","6a54429d":"code","4c557f6b":"code","823e5044":"code","7ba73eeb":"code","f81f2ec5":"code","0c088f70":"code","b3f4bb3c":"code","35224b68":"code","d9a40c7e":"code","26758ebc":"code","58d21405":"code","cf9f5170":"code","b99bf24d":"code","0872b520":"code","08cf659f":"code","26635846":"code","946584af":"code","7bc73870":"code","4b110dbc":"code","b3745b05":"code","e13a2556":"code","b3e2fb75":"code","119435bb":"code","0e103020":"code","237694b3":"code","ed90bc3f":"code","5768e1f2":"code","d947b6f3":"code","9fb84b5f":"code","2eef1366":"code","ed4f1ec9":"markdown","bef94d27":"markdown","2db516a8":"markdown","644b8090":"markdown","40272ac6":"markdown","67fa24b6":"markdown","d5e3a1c6":"markdown","55c9e96e":"markdown","03eb77e6":"markdown","229028a2":"markdown","30c13246":"markdown","1f5174b2":"markdown","bddeb5da":"markdown","2f6e7cb8":"markdown","1cc27724":"markdown","4badd3da":"markdown","af77a6a3":"markdown","c02fac03":"markdown","b6204a21":"markdown","1b133dc0":"markdown","5bec04a0":"markdown","2fba4cf3":"markdown","ff2c9fa9":"markdown","5471f88f":"markdown","5f69634a":"markdown","54bfd869":"markdown"},"source":{"a384e15f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04dc5084":"train = pd.read_csv('..\/input\/pump-it-up-data-mining-the-water-table\/Train.csv')\ntest = pd.read_csv('..\/input\/pump-it-up-data-mining-the-water-table\/Test.csv')","1a266654":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","e0b35176":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom wordcloud import wordcloud\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Loading required libraries\n%matplotlib inline\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, plot_confusion_matrix, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network.multilayer_perceptron import MLPClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb","9179e0f7":"train.shape","2f6e4e58":"train.isnull().sum().sort_values(ascending=False)","d16e7252":"plt.figure(figsize=(12,5))\nsns.countplot(train.status_group, palette = \"Set3\")\ntrain.status_group.value_counts()\n\ncounts = train.status_group.value_counts()\nFunctional = counts['functional']\nRepair = counts['functional needs repair']\nShutdown = counts['non functional']\nperc_functional = (Functional\/(Shutdown+Repair+Functional))*100\nperc_repair = (Repair\/(Shutdown+Repair+Functional))*100\nperc_shutdown = (Shutdown\/(Shutdown+Repair+Functional))*100\n\nprint('There were {} Shutdown or {:.3f}%, {} Repair or {:.3f}% and {} Functional or {:.3f}%.'.format(Shutdown, perc_shutdown, Repair, perc_repair, Functional, perc_functional))","d6f7d2ce":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.status_group\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['status_group'],axis=1, inplace=True)\nprint(\"all_data size is: {}\".format(all_data.shape))","c678cd2c":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:100]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","43d5434c":"all_data.head(3)","3725681a":"all_data.info()","53b50531":"all_data.isnull().sum().sort_values(ascending=False)","38860adc":"missed = [\"scheme_management\", \"installer\", \"funder\", \"public_meeting\", \"permit\", \"subvillage\"]\nfor m in missed:\n    all_data[m].fillna(\"other\",inplace=True)","e555f49f":"all_data.shape","500fe82a":"all_data['date_recorded'] = pd.to_datetime(all_data['date_recorded'])\nall_data.date_recorded.head(3)\nprint(all_data.date_recorded.dt.year.head(3))\nprint(all_data.construction_year.head(3))\nall_data['operational_year'] = all_data.date_recorded.dt.year - all_data.construction_year\nall_data.operational_year.head(3)","609e6307":"all_data['operation_region'] = all_data.amount_tsh * all_data.region_code\nall_data['operation_population'] = all_data.amount_tsh * all_data.population\nall_data['operation_year'] = all_data.amount_tsh * all_data.construction_year","03a48838":"all_data.drop(['id','scheme_name','recorded_by','date_recorded'], axis = 1, inplace = True)","29c4a1fb":"plt.figure(figsize=(14,6))\nsns.countplot(data=train,x='water_quality',hue='status_group')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\ntrain.water_quality.value_counts()","7d2d3e3f":"plt.figure(figsize=(24, 9))\nsns.countplot(data=train,x='region',hue='status_group')","9f21dfbf":"plt.figure(figsize=(24, 9))\nsns.countplot(data=train,x='public_meeting',hue='status_group')","4a6e85d1":"sns.FacetGrid(data=train,hue='status_group',size=14).map(sns.kdeplot, 'population', shade=True).add_legend()","9edcd075":"sns.FacetGrid(data=train,hue='status_group',size=10).map(sns.kdeplot, 'gps_height', shade=True).add_legend()","10d23bd7":"sns.FacetGrid(data=train,hue='status_group',size=10).map(sns.kdeplot, 'construction_year', shade=True).add_legend()","937314cb":"sns.FacetGrid(data=train, hue='status_group',size=10).map(sns.kdeplot, 'latitude', shade=True).add_legend()","3d5ab467":"plt.figure(figsize=(24, 9))\nsns.countplot(data=train,x='waterpoint_type',hue='status_group')","6a54429d":"plt.figure(figsize=(24, 9))\n\nsns.countplot(data=train,x='quantity',hue='status_group')","4c557f6b":"# create a list of our conditions\nconditions = [\n    (train['status_group'] == \"functional\"),\n    (train['status_group'] == \"non functional\"),\n    (train['status_group'] == \"functional needs repair\")]\n# create a list of the values we want to assign for each condition\nvalues = [1, 0, 2]\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntrain['Operation'] = np.select(conditions, values)","823e5044":"all_data.head(5)","7ba73eeb":"from scipy.stats import norm, skew\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew","f81f2ec5":"for feature in high_skew.index:\n    all_data[feature] = np.log1p(np.abs(all_data[feature]))","0c088f70":"all_data.dtypes[all_data.dtypes == 'object'].index","b3f4bb3c":"cats = all_data.dtypes[all_data.dtypes == 'object'].index\nfor cat in cats:\n    all_data[cat] = all_data[cat].astype('category').cat.codes","35224b68":"all_data.head(3)","d9a40c7e":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,30))\n## Plotting heatmap. # Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(all_data.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncorr = all_data.corr()\nsns.heatmap(corr, cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0);\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","26758ebc":"all_data.head(3)","58d21405":"X = all_data[:ntrain].values\nZ = all_data[ntrain:].values\ny = train['Operation'].values","cf9f5170":"def SMOTEs(X, y):\n    # borderline-SMOTE for imbalanced dataset\n    from collections import Counter\n    from sklearn.model_selection import train_test_split\n    from sklearn.datasets import make_classification\n    from imblearn.over_sampling import SMOTE\n    from matplotlib import pyplot\n    from numpy import where\n    \n# summarize class distribution\n    counter = Counter(y)\n    print(counter)\n# transform the dataset\n    #smt = SMOTE(sampling_strategy='minority')\n    smt = SMOTE(random_state=0)\n    X, y = smt.fit_sample(X, y) \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n# scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    pyplot.show()\n    return X_train, X_test, y_train, y_test","b99bf24d":"def counts(input):\n    zero = 0\n    one = 0\n    two = 0\n    for ele in input:\n        if (ele == 0):\n            zero = zero + 1\n        elif (ele == 1):\n            one = one +1\n        else:\n            two = two +1\n    return zero, one, two\n\ndef Models(models, X_train, X_test, y_train, y_test, X, y, title):\n    model = models\n    model.fit(X_train,y_train)\n    \n    train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=['Actual'], colnames=['Predicted'])    \n    test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=['Actual'], colnames=['Predicted'])\n    matrix = pd.crosstab(y, model.predict(X), rownames=['Actual'], colnames=['Predicted'])\n    actr = accuracy_score((model.predict(X_train)), y_train)\n    acte = accuracy_score((model.predict(X_test)), y_test)\n    acto = accuracy_score((model.predict(X)), y)\n    \n    f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(20, 4))\n    g1 = sns.heatmap(train_matrix, annot=True, fmt=\".1f\", cbar=False,annot_kws={\"size\": 18},ax=ax1)\n    g1.set_title(title)\n    g1.set_ylabel('{} | {} |   {}'.format(round(counts(y_train)[0],0), round(counts(y_train)[1],0),round(counts(y_train)[2],0)), fontsize=14, rotation=90)\n    g1.set_xlabel('Accuracy score Trainset: {}'.format(actr))\n    \n    g2 = sns.heatmap(test_matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax2)\n    g2.set_ylabel('{} | {} |   {}'.format(round(counts(y_test)[0],0), round(counts(y_test)[1],0),round(counts(y_test)[2],0)), fontsize=14, rotation=90)\n    g2.set_xlabel('Accuracy score Testingset: {}'.format(acte))\n    \n    g3 = sns.heatmap(matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax3)\n    g3.set_ylabel('{} | {} |   {}'.format(round(counts(y)[0],0), round(counts(y)[1],0),round(counts(y)[2],0)), fontsize=14, rotation=90)\n    g3.set_xlabel('Accuracy score Totalset: {}'.format(acto))\n    \n    plt.show()\n    return actr, acte, acto\n\ndef Featureimportances(models):\n    model = models\n    model.fit(X_train,y_train)\n    importances = model.feature_importances_\n    features = df_test.columns[:9]\n    imp = pd.DataFrame({'Features': ftest, 'Importance': importances})\n    imp['Sum Importance'] = imp['Importance'].cumsum()\n    imp = imp.sort_values(by = 'Importance')\n    return imp","0872b520":"X_train, X_test, y_train, y_test = SMOTEs(X, y)","08cf659f":"Acc = pd.DataFrame(index=None, columns=['model','Score trainset','Score testset', 'Score total'])\nClassifiers = [['DecisionTreeClassifier',DecisionTreeClassifier(max_depth=22, splitter='best')],\n               ['ExtraTreeClassifier',ExtraTreesClassifier(min_samples_split= 3, n_estimators= 160)],\n               ['RandomForestClassifier',RandomForestClassifier(max_depth= 22,n_estimators= 500)],\n              ['XGBClassifier',XGBClassifier(max_depth= 20, n_estimators= 160)]]\nfor mod in Classifiers:\n    name = mod[0]\n    model = mod[1]\n    \n    model.fit(X_train,y_train)\n    actr = accuracy_score(y_train, model.predict(X_train))\n    acte = accuracy_score(y_test, model.predict(X_test))\n    acov = accuracy_score(y, model.predict(X))\n    Acc = Acc.append(pd.Series({'model':name,'Score trainset':actr,'Score testset':acte, 'Score total':acov}),ignore_index=True )\nAcc.sort_values(by='Score testset')","26635846":"RF = RandomForestClassifier(n_estimators=500,max_depth=22,random_state=0)\nModels(RF, X_train, X_test, y_train, y_test, X, y, \"RandomForestClassifier\")\nresult1 = RF.predict(Z)","946584af":"accuracy = dict()\nf1 = dict()\n\naccuracy[\"RandomForestClassifier\"] = accuracy_score(y_test, RF.predict(X_test))\nf1[\"RandomForestClassifier\"] = f1_score(y_test, RF.predict(X_test), average=\"macro\")\n\nprint(classification_report(y_train, RF.predict(X_train)))\nprint(classification_report(y_test, RF.predict(X_test)))","7bc73870":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\n\n# Metrics dictionary\naccuracy = dict()\nprecision = dict()\nrecall = dict()\nf1 = dict()\nfpr = dict()\ntpr = dict()\n\nY = label_binarize(y_test, classes=[0, 1, 2])\ny_score = RF.predict_proba(X_test)\n\nprecision[\"RandomForestClassifier\"], recall[\"RandomForestClassifier\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"RandomForestClassifier\"], tpr[\"RandomForestClassifier\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(10, 5))\nplt.step(recall[\"RandomForestClassifier\"], precision[\"RandomForestClassifier\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - RandomForestClassifier\")\nplt.show()","4b110dbc":"ETC = ExtraTreesClassifier(min_samples_split= 3, n_estimators= 100)\nModels(ETC, X_train, X_test, y_train, y_test, X, y, \"ExtraTreesClassifier\")\nresult2 = ETC.predict(Z)","b3745b05":"ETC = ExtraTreesClassifier(min_samples_split= 3, n_estimators= 120)\nModels(ETC, X_train, X_test, y_train, y_test, X, y, \"ExtraTreesClassifier\")\nresult2 = ETC.predict(Z)","e13a2556":"accuracy = dict()\nf1 = dict()\n\naccuracy[\"ExtraTreesClassifier\"] = accuracy_score(y_test, ETC.predict(X_test))\nf1[\"ExtraTreesClassifier\"] = f1_score(y_test, ETC.predict(X_test), average=\"macro\")\n\nprint(classification_report(y_train, ETC.predict(X_train)))\nprint(classification_report(y_test, ETC.predict(X_test)))","b3e2fb75":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\n\n# Metrics dictionary\naccuracy = dict()\nprecision = dict()\nrecall = dict()\nf1 = dict()\nfpr = dict()\ntpr = dict()\n\nY = label_binarize(y_test, classes=[0, 1, 2])\ny_score = ETC.predict_proba(X_test)\n\nprecision[\"ExtraTreesClassifier\"], recall[\"ExtraTreesClassifier\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"ExtraTreesClassifier\"], tpr[\"ExtraTreesClassifier\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(10, 5))\nplt.step(recall[\"ExtraTreesClassifier\"], precision[\"ExtraTreesClassifier\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - ExtraTreesClassifier\")\nplt.show()","119435bb":"XGC = XGBClassifier(max_depth= 15, n_estimators= 160)\nModels(XGC, X_train, X_test, y_train, y_test, X, y, \"XGBClassifier\")\nresult3 = XGC.predict(Z)","0e103020":"XGC = XGBClassifier(max_depth= 15, n_estimators= 150)\nModels(XGC, X_train, X_test, y_train, y_test, X, y, \"XGBClassifier\")\nresult3 = XGC.predict(Z)","237694b3":"accuracy = dict()\nf1 = dict()\n\naccuracy[\"XGBClassifier\"] = accuracy_score(y_test, XGC.predict(X_test))\nf1[\"XGBClassifier\"] = f1_score(y_test, XGC.predict(X_test), average=\"macro\")\n\nprint(classification_report(y_train, XGC.predict(X_train)))\nprint(classification_report(y_test, XGC.predict(X_test)))","ed90bc3f":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\n\n# Metrics dictionary\naccuracy = dict()\nprecision = dict()\nrecall = dict()\nf1 = dict()\nfpr = dict()\ntpr = dict()\n\nY = label_binarize(y_test, classes=[0, 1, 2])\ny_score = XGC.predict_proba(X_test)\n\nprecision[\"XGBClassifier\"], recall[\"XGBClassifier\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"XGBClassifier\"], tpr[\"XGBClassifier\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(10, 5))\nplt.step(recall[\"XGBClassifier\"], precision[\"XGBClassifier\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - XGBClassifier\")\nplt.show()","5768e1f2":"GBC = GradientBoostingClassifier(n_estimators=100, max_depth=12, verbose=False)\nModels(GBC, X_train, X_test, y_train, y_test, X, y, \"GradientBoostingClassifier\")\nresult4 = GBC.predict(Z)","d947b6f3":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\n\n# Metrics dictionary\naccuracy = dict()\nprecision = dict()\nrecall = dict()\nf1 = dict()\nfpr = dict()\ntpr = dict()\n\nY = label_binarize(y_test, classes=[0, 1, 2])\ny_score = GBC.predict_proba(X_test)\n\nprecision[\"GradientBoostingClassifier\"], recall[\"GradientBoostingClassifier\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"GradientBoostingClassifier\"], tpr[\"GradientBoostingClassifier\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(12, 6))\nplt.step(recall[\"GradientBoostingClassifier\"], precision[\"GradientBoostingClassifier\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - GradientBoostingClassifier\")\nplt.show()","9fb84b5f":"accuracy = dict()\nf1 = dict()\n\naccuracy[\"GradientBoostingClassifier\"] = accuracy_score(y_test, GBC.predict(X_test))\nf1[\"GradientBoostingClassifier\"] = f1_score(y_test, GBC.predict(X_test), average=\"macro\")\n\nprint(classification_report(y_train, GBC.predict(X_train)))\nprint(classification_report(y_test, GBC.predict(X_test)))","2eef1366":"result = result3\nconditions = [(result == 1), (result == 0), (result == 2)]\nvalues = [\"functional\",\"non functional\",\"functional needs repair\"]\n\n# create a new column and use np.select to assign values to it using our lists as arguments\nres = np.select(conditions, values)\n\nsub = pd.DataFrame()\nsub = pd.DataFrame({\"Id\":test.id, \"status_group\":res})\nsub.to_csv(\"my_submission.csv\",index=False)\nsub.head(3)","ed4f1ec9":"<a href=\"https:\/\/ibb.co\/vLNGwm1\"><img src=\"https:\/\/i.ibb.co\/SVk8s0N\/Result4.jpg\" alt=\"Result4\" border=\"0\"><\/a>","bef94d27":"Check to finalize and make sure before implement modeling ...","2db516a8":"Also, we creat some other features:","644b8090":"# Classification modeling\nAfter running multioutput regressors we will now be running classification models","40272ac6":"Now, let's check which column is not numerical category?","67fa24b6":"In order to avoid outliers and some possible invalid values nonlinar transformation was applied as follows:","d5e3a1c6":"Influence of water quality on the pump operation condition.","55c9e96e":"And looking at regions ...","03eb77e6":"# Numerical features:\n\n+ amount_tsh: the amount of water: it contains 0s as invaild values. nonlinear transformation does not help to remove the 0 values\n+ gps_height: the values look ok no invaild values (it not possible to tell if the 0s are invaild values)\n+ longitude: there are some 0 values as invalid values and they should be replaced\n+ latitude: No sign of invalid values\n+ population: there are some zeros as invalid values\n\n# Categorical features:\n\n+ region_code: there are some outliers with log transformation the problem can be solved\n+ district_code: there are some outliers but the log transform it can solved\n+ funder: 3635 missing values, a lot of categories\n+ installer: 3655 missing values, a lot of categories\n+ wpt_name: a lot of categories there are some None values\n+ payment: limited categories no sign for missing values\n+ payment_type: limited categories no sign for missing values (there are some 0s)\n+ water_quality:limited categories no sign for missing values (there are some 0s)\n+ quality_group:limited categories no sign for missing values (there are some 0s)\n+ source:limited categories no sign for missing values (there are some 0s)\n+ source_type:limited categories no sign for missing values (there are some 0s) (high correlated with soruce)\n+ source_class: alot of 0s-->unknown\n+ waterpoint_type:limited categories no sign for missing values (there are some 0s)\n+ waterpoint_type_group:limited categories no sign for missing values (there are some 0s)\n+ basin:limited categories no sign for missing values\n+ subvillage: 371 missing values, a lot of categories there are no sign of missing values\n+ region: limited categories no sign for missing values\n+ lga:a lot of categories there are no sign of missing values\n+ ward:a lot of categories there are no sign of missing values\n+ public_meeting: 3334 missing values:limited categories there are some signs for missing values (there are some 0s and -1)\n+ recorded_by: one value no variation\n+ scheme_management: 3877 missing values limited categories\n+ scheme_name: 28166 missing values lot of categories\n+ permit:3056 missing value two categories\n+ extraction_type:limited categories no sign for missing values\n+ extraction_type_group:limited categories no sign for missing values\n+ extraction_type_class:limited categories no sign for missing values\n+ management:limited categories no sign for missing values\n+ management_group:limited categories no sign for missing values\n+ quantity:limited categories no sign for missing values\n+ quantity_group:limited categories no sign for missing values\n\n# Labels:\nThere are three labels status_group that should be predicted:\n\n+ functional\n+ non functional\n+ functional needs repair.","229028a2":"# Challenges\nThere were 41 features in the dataset which were too many to handle and accurately use.\n+ Many of the features have many missing values, with some important features like scheme_name having as much as 28,166 missing values out of 59400 data points.\n+ Many fetures had too many sub features like \u2018sub village\u2019 etc.","30c13246":"# Data cleaning:\n+ To ensure the same processing on both the training and testing datasets both are merged in one dataset all_data. They will be splited again without any problems. \n+ The first step is to change the text values in categorical features to numerical values so the correlation between all features can be calculated.","1f5174b2":"Import necessary Librairies first ...","bddeb5da":"To this step, we tried to check if there is any variable possessing high skew value?","2f6e7cb8":"The goal of this project is to build a predictive model using data from Taarifa and the Tanzanian Ministry of Water to predict which pumps are:\n+ functional, \n+ which need some repairs, and \n+ which don't work at all. \nPredict one of these three classes is based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n\nData are taken from: [www.drivendata.org](https:\/\/www.drivendata.org\/competitions\/7\/pump-it-up-data-mining-the-water-table\/)","1cc27724":"<h1 align=\"center\"> Predict which water pumps are faulty? <\/h1>\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/kHq95Dh\/pump-it-up.jpg\" alt=\"pump-it-up\" border=\"0\"><\/a>","4badd3da":"<a href=\"https:\/\/ibb.co\/ynqZCSg\"><img src=\"https:\/\/i.ibb.co\/1fTPN6R\/Result3.jpg\" alt=\"Result3\" border=\"0\"><\/a>","af77a6a3":"# Introduction\nThis project is a multi-class classification problem. To solve this problem the data will loaded and explored to address its qualilty and apply data cleaning method to get tidy data. The data will be used to train a classifier. Finally the model will be tested on a cross validation dataset before prediction the classes of the given testing dataset that should be submitted to the competition website.","c02fac03":"<a href=\"https:\/\/ibb.co\/qY8DV1Z\"><img src=\"https:\/\/i.ibb.co\/XxPWMJw\/Score.jpg\" alt=\"Score\" border=\"0\"><\/a>","b6204a21":"We converted these \"high skew\" columns to \"normal\" by taking logarith:","1b133dc0":"If water point is old, it is highly likely to be non functional or needs repair.\nNew features were created namely number of days and number of months water point is operational.","5bec04a0":"<a href=\"https:\/\/ibb.co\/XtC298X\"><img src=\"https:\/\/i.ibb.co\/tDJmSXB\/Result2.jpg\" alt=\"Result2\" border=\"0\"><\/a>","2fba4cf3":"Now, we convert the operation status of pump into numeric category, by applying a simple numpy function to creat a new column named \"Operation\", as following:","ff2c9fa9":"NOT BAD ! Now, we start to run each model, submit the result and check the acuracy & score !","5471f88f":"# Approach\nMy first approach was to find those features which would be important in predicting the result:\n+ We know that the location and the population around any particular water point would play vital role in the status of that water point.\n+ But there was problems as features like gps_height,population,latitude and longitude had many missing data points, so I missed filled those missing data points with the mean and median(as required) of the respective feature in that particular district in which it was lying.\n\nNow, let's check first the Label - target values: Operation status ","5f69634a":"And, simply convert it into numerical data using cat.codes:","54bfd869":"<a href=\"https:\/\/ibb.co\/DLRyjmk\"><img src=\"https:\/\/i.ibb.co\/M9C4JKp\/Result1.jpg\" alt=\"Result1\" border=\"0\"><\/a>"}}