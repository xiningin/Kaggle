{"cell_type":{"699538e3":"code","35294fe0":"code","deee82ab":"code","4fc9ab57":"code","17de1cae":"code","de46ef3f":"code","1d2bb8cd":"code","43e2cfee":"code","2977f7a1":"code","f9c613a0":"code","29ff9f2b":"code","2f44d16d":"code","2c0faae7":"code","306e79da":"code","7d0d7cfa":"code","fbfe8cbc":"code","caff23b9":"code","1f2539d3":"code","daaa94a2":"code","1d2ddb92":"code","443155e0":"code","43a93017":"code","e2cb3eb6":"code","5e5f4a04":"code","257ddc63":"code","f7a1e051":"code","4c58d4b4":"code","7409f539":"code","bc30927b":"code","ed882fc0":"code","2a3ba6ba":"code","0e20d1c9":"code","4cbdb22c":"code","adced0d6":"code","2b53bb2e":"code","c6a61d30":"code","e3bc4edf":"code","e98c7d3f":"code","f3bf7b36":"code","3e3872ce":"code","3d57b79f":"code","3ab3cc3e":"code","18f221d4":"markdown","acd203b0":"markdown","4536e8cd":"markdown","f73239de":"markdown","e541e7bc":"markdown","d46d9c0d":"markdown","1462302f":"markdown","ede783c3":"markdown","99befd8f":"markdown","002e7746":"markdown","0f813ea4":"markdown","f3cec82b":"markdown","3e66ea3d":"markdown"},"source":{"699538e3":"# Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport statistics\n\n# Data based libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Model libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression","35294fe0":"# Import the data\ndia = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndia.head(10)","deee82ab":"# Shape of the dataset\ndia.shape","4fc9ab57":"# Description\ndia.describe()","17de1cae":"# Checking for null values\ndia.isna().sum()","de46ef3f":"# Checking the datatypes\ndia.dtypes","1d2bb8cd":"# Number of positive cases vs negative cases\ndia[\"Outcome\"].value_counts().plot(kind = 'bar',color=[\"green\",\"red\"]);","43e2cfee":"# Finding Correlation between the columns\ndia.corr()","2977f7a1":"# Visualising the correlation\ncorr_mat = dia.corr()\nfig,ax = plt.subplots(figsize = (13,8))\nax = sns.heatmap(corr_mat,annot=True,fmt='.2f',linewidths=0.5,cmap=\"YlGnBu\")","f9c613a0":"# BMI and Age \n\nplt.figure(figsize=(10,6))\n\n# Positive cases\nplt.scatter(dia.Age[dia.Outcome == 1],\n            dia.BMI[dia.Outcome == 1],\n            c=\"red\")\n\n# Negative cases\nplt.scatter(dia.Age[dia.Outcome == 0],\n            dia.BMI[dia.Outcome == 0],\n            c=\"green\")\n\n# Labeling\nplt.title(\"Age & BMI: Positive vs Negative\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"BMI\")\nplt.legend([\"Positive\",\"Negative\"]);","29ff9f2b":"# Age Distribution\ndia.Age.plot.hist();","2f44d16d":"dia.BMI.plot.hist();","2c0faae7":"# Splitting the data\n\nx = dia.drop(\"Outcome\",axis=1)\ny = dia[\"Outcome\"]\n","306e79da":"np.random.seed(42) \n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)","7d0d7cfa":"models = {\n          \"Logistic Regression\": LogisticRegression(solver='liblinear'), \n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier()\n          }\n\n# Create function to fit and score models\ndef fit_and_score(models, x_train, x_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(x_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(x_test, y_test)\n    return model_scores    ","fbfe8cbc":"model_scores = fit_and_score(models = models,\n                             x_train = x_train,\n                             x_test = x_test,\n                             y_train = y_train,\n                             y_test = y_test)\n\nmodel_scores","caff23b9":"# comparing the model\nmodel_compare = pd.DataFrame(model_scores,index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","1f2539d3":"# Tunnig KNN\ntrain_score = []\ntest_score = []\n\nneighbors = range(1,21)\nknn = KNeighborsClassifier()\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    knn.fit(x_train,y_train)\n    train_score.append(knn.score(x_train,y_train))\n    test_score.append(knn.score(x_test,y_test))","daaa94a2":"train_score","1d2ddb92":"test_score","443155e0":" statistics.mean(test_score)","43a93017":"plt.plot(neighbors, train_score, label = \"Train Score\")\nplt.plot(neighbors, test_score, label = \"Test Score\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model Score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_score)*100:.2f}%\")","e2cb3eb6":"# Logistic Regression Grid\n\nL_grid = {\"C\" : np.logspace(-4,4,20),\n          \"solver\" : [\"liblinear\"]}\n\n# RandomForest Grid\n\nR_grid = {\"n_estimators\" : np.arange(10, 1000, 50),\n           \"max_depth\" : [None, 3, 5, 10],\n           \"min_samples_split\" : np.arange(2, 20, 2),\n           \"min_samples_leaf\" : np.arange(1, 20, 2)}\n\n# KNN\n\nKNN_grid = {'n_neighbors' : [3,5,11,19],\n            'weights' : ['uniform','distance'],\n            'metric' : ['euclidean','manhattan']}","5e5f4a04":"# For Logistic Regression\n\nnp.random.seed(42)\n\nRS_log = RandomizedSearchCV(LogisticRegression(),\n                            param_distributions=L_grid,\n                            cv = 5,\n                            n_iter = 20,\n                            verbose = True)\n\nRS_log.fit(x_train,y_train)","257ddc63":"RS_log.best_params_","f7a1e051":"RS_log.best_score_","4c58d4b4":"# RandomForest\n\nnp.random.seed(42)\n\nRS_RF = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions = R_grid,\n                           cv = 5,\n                           n_iter = 20,\n                           verbose = True)\n\nRS_RF.fit(x_train,y_train)","7409f539":"RS_RF.best_params_","bc30927b":"RS_RF.best_score_","ed882fc0":"# KNN\n\nnp.random.seed(42)\n\nRS_knn = RandomizedSearchCV(KNeighborsClassifier(),\n                           param_distributions = KNN_grid,\n                           cv = 5,\n                           n_iter = 16,\n                           verbose = True)\n\nRS_knn.fit(x_train,y_train)","2a3ba6ba":"RS_knn.best_params_","0e20d1c9":"RS_knn.best_score_","4cbdb22c":"# For Logistic Regression\n\nnp.random.seed(42)\n\nGS_log = GridSearchCV(LogisticRegression(),\n                            param_grid = L_grid,\n                            cv = 5,\n                            verbose = True)\n\nGS_log.fit(x_train,y_train)","adced0d6":"GS_log.best_params_","2b53bb2e":"GS_log.best_score_","c6a61d30":"# RandomForest\n\nR1_grid = {\"n_estimators\" : np.arange(10, 1000, 200),\n           \"max_depth\" : [None, 10],\n           \"min_samples_split\" : np.arange(2, 20, 5),\n           \"min_samples_leaf\" : np.arange(1, 20, 5)}\n\nnp.random.seed(42)\n\nGS_RF = GridSearchCV(RandomForestClassifier(),\n                           param_grid = R1_grid,\n                           cv = 2,\n                           verbose = True)\n\nGS_RF.fit(x_train,y_train)","e3bc4edf":"GS_RF.best_params_","e98c7d3f":"GS_RF.best_score_","f3bf7b36":"# KNN\n\nnp.random.seed(42)\n\nGS_knn = GridSearchCV(KNeighborsClassifier(),\n                           param_grid = KNN_grid,\n                           cv = 5,\n                           verbose = True)\n\nGS_knn.fit(x_train,y_train)","3e3872ce":"GS_knn.best_params_","3d57b79f":"GS_knn.best_score_","3ab3cc3e":"print(\"Best Scores\")\nprint(\"RandomForest : \",RS_RF.best_score_)\nprint(\"Logistic Regression : \",GS_log.best_score_)\nprint(\"KNN : \",max(test_score))","18f221d4":"# Diabetes Prediction\n\n> Problem Statement\n\nWith the given information of a person we have to predict whether a person has diabetes or not","acd203b0":"#### Data modeling","4536e8cd":"There is slight improvement","f73239de":"#### Data Analysis","e541e7bc":"## RandomizedSearchCV","d46d9c0d":"## GridSearchCV","1462302f":"## Model fitting\n* KNN\n* Logistic Regression\n* RandomForestClassifier","ede783c3":"There are no null values","99befd8f":"Sligth improvement","002e7746":"All are numeric","0f813ea4":"### Data\n\n> Attributes\n\n* Pregnancies - Number of times pregnant \n\n\n* Glucose - Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n\n* BloodPressure - Diastolic blood pressure (mm Hg)\n\n\n* SkinThickness - Triceps skin fold thickness (mm)\n\n\n* Insulin - 2-Hour serum insulin (mu U\/ml)\n\n\n* BMI - Body mass index (weight in kg\/(height in m)^2)\n\n\n* DiabetesPedigreeFunction - Diabetes pedigree function\n\n\n* Age - Age (years)\n\n\n> Result\n\n* Outcome - Class variable (0 or 1)","f3cec82b":"### Improving the model\n\n* Hyperparameter tunuing","3e66ea3d":"# Final Results"}}