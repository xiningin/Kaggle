{"cell_type":{"898a8898":"code","e23312c6":"code","4010a178":"code","fcc5e5d8":"code","357e3ae1":"code","55e1aaab":"code","83549000":"code","a5cc79de":"code","c6b47f9a":"code","796afa8b":"code","019c498a":"code","8cc1ea0a":"code","866695a9":"code","f671ced9":"code","995b597e":"code","558b2eec":"code","85c68ec4":"code","3451e376":"code","a0db584b":"code","bac712ed":"code","fcc0bd35":"code","8a15c0a1":"code","cc1379fd":"code","49667f5c":"code","50314c70":"code","c644785f":"code","027c5c9c":"code","bad16a9b":"code","6e6409e4":"markdown","0e68f672":"markdown","8735fad8":"markdown","38cf3699":"markdown","71f1e14a":"markdown","c048a2b7":"markdown","4cecf3a0":"markdown","dfd160ab":"markdown","cebb6291":"markdown","fe79c895":"markdown","a325b1bd":"markdown","3c7544e7":"markdown","70c5ac2a":"markdown","a4952200":"markdown","b18322e2":"markdown","ae5d4c9b":"markdown","c9ac48ef":"markdown","bb008cdd":"markdown","7e677647":"markdown"},"source":{"898a8898":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nimport xgboost as xgb\n\n%matplotlib inline\nsns.set(style='ticks')\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500\n\ndata = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","e23312c6":"data.info()\ndata.tail()","4010a178":"data.describe()","fcc5e5d8":"# Target contains imbalanced classes\ndata.Class.value_counts()","357e3ae1":"# Boxplot of variables V1 - V28\nplt.figure(figsize=(20,5))\nsns.boxplot(data=data.drop(['Time', 'Amount', 'Class'], axis=1))\nplt.xticks(rotation=90)\nplt.title('Boxplots of features V1 - V26')\nplt.xlabel('Feature Name')\nplt.ylabel('Value')\nplt.show()\n\n#datelist=pd.date_range(\"00:00:00\", \"23:59:59\", freq=\"S\")\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,5))\nax1.scatter(data.index, data['Time']\/(60*60))\nax1.set_xlabel('Index')\nax1.set_ylabel('Time feature (in hours)')\nax1.set_title('Scatterplot of Time feature')\nax2.boxplot(data['Amount'])\nax2.set_xlabel('Amount')\nax2.set_title('Boxplot of Amount feature')","55e1aaab":"# Scaling 'Time' and 'Amount'\nstd_scaler = StandardScaler()\n\ndata['scaled_amount'] = std_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = std_scaler.fit_transform(data['Time'].values.reshape(-1,1))","83549000":"# Split data into training, holdout & test set\nX_train, X_rest, y_train, y_rest= train_test_split(data.drop(['Class','Time','Amount'], axis=1), data.Class, \n                                                   test_size=0.5, random_state=123)\nX_hold, X_test, y_hold, y_test= train_test_split(X_rest, y_rest, \n                                                   test_size=0.5, random_state=123)","a5cc79de":"clf = xgb.XGBClassifier(n_estimators=150, seed=123)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_hold)\naccuracy = accuracy_score(y_hold, y_pred)\nprint(\"accuracy: %f\" % (accuracy))\nprint('As 99.828% of the data is class 0, the 99.927% heavily biased accuracy does not tell us much about the quality of our model .')\n\nf1 = f1_score(y_hold, y_pred)\nprint(\"f1 score is a better metric: %f\" % (f1))\n\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","c6b47f9a":"y_0=y_train[y_train==0]\ny_1=y_train[y_train==1]\ny_0_under=y_0.sample(n=len(y_1), random_state=123)\ny_under=pd.concat([y_0_under,y_1]).sample(frac=1, random_state=123)\nX_under=X_train.reindex(y_under.index)","796afa8b":"# Target is no longer imbalanced. \npd.DataFrame(y_under)['Class'].value_counts()","019c498a":"# Target is no longer imbalanced. \npd.DataFrame(y_under)['Class'].value_counts()\n\nclf = xgb.XGBClassifier(n_estimators=150, seed=123)\nclf.fit(X_under, y_under)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","8cc1ea0a":"\nmethod = RandomOverSampler()\nX_over, y_over = method.fit_resample(X_train,y_train)\nX_over = pd.DataFrame(X_over, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'scaled_amount',\n       'scaled_time'])","866695a9":"# Target is no longer imbalanced. \npd.DataFrame(y_over)['Class'].value_counts()","f671ced9":"clf = xgb.XGBClassifier(n_estimators=150, seed=123)\nclf.fit(X_over, y_over)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","995b597e":"method = SMOTE()\nX_over, y_over = method.fit_resample(X_train,y_train)\nX_over = pd.DataFrame(X_over, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'scaled_amount',\n       'scaled_time'])","558b2eec":"# Target is no longer imbalanced. \npd.DataFrame(y_over)['Class'].value_counts()","85c68ec4":"clf = xgb.XGBClassifier(n_estimators=150, seed=123)\nclf.fit(X_over, y_over)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","3451e376":"# Below code has been commented out because it takes time a long time (~30 mins with. 8 cores, GPU acceleration) \n# to run on every commit. So it was run once and commented out.\n# defining the space for hyperparameter tuning\n'''\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),'gamma': hp.uniform ('gamma', 1,9),\n       'reg_alpha' : hp.quniform('reg_alpha', 1,180,1),\n       'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n       'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n       'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1)\n       }\n\ndef hyperparameter_tuning(space):\n    clf=xgb.XGBClassifier(max_depth = int(space['max_depth']), gamma = space['gamma'],\n                         reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n                         colsample_bytree=space['colsample_bytree'],eta= 0.8, nthread=-1, \n                         scale_pos_weight = np.sqrt(sum(y_train==0)\/sum(y_train==1)),\n                          n_estimators=150, random_state=123)\n    evaluation = [( X_train, y_train), ( X_hold, y_hold)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n\n    y_pred = clf.predict(X_hold)\n    recall = recall_score(y_hold, y_pred)\n    print(classification_report(y_hold, y_pred))\n    # We want to ensure that every fraud case is reported. \n    # Even if we tradeoff a large number of false positives. \n    # So we will maximize recall (ie. minimize 1-recall)\n    print (\"Recall:\", recall)\n    return {'loss': 1-recall, 'status': STATUS_OK }\n\n\n# run the hyper paramter tuning\ntrials = Trials()\n\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=100)\n\nprint (best)\n'''","a0db584b":"# Model with best parameters \nclf = xgb.XGBClassifier(colsample_bytree = 0.9797504331289699, \n                        gamma = 8.736312595289913, max_depth = 7, \n                        min_child_weight = 9, n_estimators = 150, \n                        reg_alpha = 62, \n                        reg_lambda = 0.1083378611307041,\n                        scale_pos_weight = np.sqrt(sum(y_train==0)\/sum(y_train==1)),\n                        eta= 0.8, nthread=-1, seed=123)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))\n\n# Default parameters in baseline model (80% recall, 82% f1 score)\n'''\ncolsample_bytree= 1,\n gamma = 0,\n max_depth = 6,\n min_child_weight = 1,\n reg_alpha = 0,\n reg_lambda = 1\n'''","bac712ed":"# Checking model performance by 3-fold cross-validation\nclf = xgb.XGBClassifier(colsample_bytree = 0.9797504331289699, \n                        gamma = 8.736312595289913, max_depth = 7, \n                        min_child_weight = 9, n_estimators = 150, \n                        reg_alpha = 62, \n                        reg_lambda = 0.1083378611307041,\n                        scale_pos_weight = np.sqrt(sum(y_train==0)\/sum(y_train==1)),\n                        eta= 0.8, nthread=-1)\nprint(cross_val_score(clf, X_train, y_train, cv=3, scoring='recall'))","fcc0bd35":"# Plot precision recall curve for current best model. \nprecisions, recalls, thresholds = precision_recall_curve(y_hold, y_pred)\nplt.plot(recalls, precisions, \"b-\", linewidth=2)\nplt.xlabel(\"Recall\", fontsize=16)\nplt.ylabel(\"Precision\", fontsize=16)\nplt.axis([0, 1, 0, 1])\nplt.grid(True)\nplt.show()","8a15c0a1":"# Plot mean Amounts of non-fraud (label 0) and fraud (label 1) transactions\nsns.catplot(x='Class', y='Amount', data=data, kind='bar', ci=None)\nplt.title('Mean Amounts for non-fraud (label 0) and fraud (label 1) transactions' )","cc1379fd":"clf = LogisticRegression(class_weight='balanced',max_iter=200, random_state=123)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","49667f5c":"param_grid = {'C' : [0.1, 1, 10, 100]}\n\nclf = LogisticRegression(class_weight='balanced', max_iter=200, random_state=123, n_jobs=-1)\nsearch = GridSearchCV(clf, param_grid, cv=3, scoring='recall')\nsearch.fit(X_train, y_train)\ny_pred = search.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))\nprint('Best parameters: ', search.best_params_)\n","50314c70":"* This model has lower scores but does better after crosss-validation.","c644785f":"rf = RandomForestClassifier(max_depth=110, min_samples_split=10, random_state = 123, n_jobs=-1)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","027c5c9c":"xgb_cf = xgb.XGBClassifier(colsample_bytree = 0.9797504331289699, \n                        gamma = 8.736312595289913, max_depth = 7, \n                        min_child_weight = 9, n_estimators = 150, \n                        reg_alpha = 62, \n                        reg_lambda = 0.1083378611307041,\n                        scale_pos_weight = np.sqrt(sum(y_train==0)\/sum(y_train==1)),\n                        eta= 0.8, nthread=-1, n_jobs=-1)\nlr_cf = LogisticRegression(C= 0.1, class_weight='balanced', n_jobs=-1)\nrf_cf = RandomForestClassifier(max_depth=110, min_samples_split=10, n_jobs=-1)\n\n\nvote = VotingClassifier (estimators = [('xgb_cf', xgb_cf), ('lr_cf', lr_cf), ('rf_cf', rf_cf)], \n                         voting='hard', n_jobs=-1)\nvote.fit(X_train,y_train)\ny_pred = vote.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","bad16a9b":"prediction = vote.predict(X_test)\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","6e6409e4":"## Logistic Regression","0e68f672":"## Test set performance","8735fad8":"### XGBoostClassifier does better on unaltered data. Under\/oversampling techniques decrease performance. \n\n* This is an interesting result. \n\n* Random Forests and Logistic Regression methods are commonly used for imbalanced class data. These do well with varius sampling strategies. Autoencoder Neural Network work very well to model imbalanced classes. \n\n* XGBoostClassifier does better with the original data. It is faster than most ML algorithms AND needs less preprocessing! \n\n* Let's improve out XGBoostClassifier through hyperparameter tuning. ","38cf3699":"* Random Forests generally do not benefit much from hypertuning. ","71f1e14a":"# Explore data","c048a2b7":"# Dealing with imbalanced classes. \n\n* I use **hyperopt** to tune parameters of an **XGBoost-Classifier**. I then build a Voting CLassifier that combines votes from a Logistic Regression model, a Random Forest model and the XGBoostClassifier. \n\n\n* Using the Kaggle **Credit Card Fraud Detection** dataset. \n\n\n* Quoting from Kaggle: \n\n    - The datasets contains transactions made by credit cards in September 2013 by european cardholders.This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n    \n    - Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n    \n    - Features V1-V28 are anonymized. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount.\n    \n    \n* XGBoost \n\n    - is known for its excellent speed & performance (operations parallizable) on a variety on Machine Learning problems. \n    - Is an Ensemble algoritm. It uses CART decision trees as base learners. \n    \n    - It is preferred for use when we have a large number of training samples. Number of training samples must be greater than the number of features. \n    \n    - The complete parameter list can be found at https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.rst\n    \n    \n* Hyperopt\n\n    - Manual search, grid search and random search are common algorithms used for hyperparameter tuning. \n    - All these previous methods are un-informed: ie. they scan through the entire\/randomly selected set of hyperparameters to select the best one. \n    - Hyperopt is a Bayasian optimization algorithm, it is classified as an 'informed search' algorithm as the score from the previous round of search, 'informs' the choice of a better set of hyperparameters. \n \n* Another extremely cool advantage of hyperopt is: \n    - There are a lot of machine learning algorithms. And many different configuration for preprocessing\/ feature engineering. Hyperopt allows us to use an algorithm to search this large set of other algorithms for the best, without additional input on our part. \n    - The 'algo' parameter is customizable. I will only use the inbuilt tpe.suggest algo. It is known to work well for most use cases. \n    - My dataset does not need much pre-processing. \n    - So I will only try out hyperparameter tuning for XGBoost today. ","4cecf3a0":"* #### Undersampling : Precision very low, Recall is high. ","dfd160ab":"* The tuned model improves recall by 7%. f1 score reduces from 82% to 80%: \n    - The tuned model is better able to classify the fraud cases\n    - it has alpha & lambda regularization as well as a higher gamma value (higher gamma = greater gain threshold for maintaining nodes, and thus, more pruning). \n    - A fraction of the features are subsampled in the tuned model (colsample_bytree) . We thus make the model run faster (and introduce regularization) without losing out on performance. \n    - The tuned model builds deeper trees . A greater number of interactions can be captured by the tuned model. \n    - A higher weight threshold is used to allow\/deny subdivision of node, in the tuned model. \n    \n    \n* Further tuning with a lower learning rate (eta) can be done to narrow down on a better model. \n\n* Finally. the precision-recall threshold should be changed to ensure better recall, if possible. We see in the current precision - recall curve, there is a sharp drop in precision if we require >80% recall. But we also see that the mean amount of 492 fraud transactions is higher than the mean of 284315 non-fraud transactions. Considering the higher losses, the bank may want to increase recall to ~99% even if it means more calls to customers to check on legitimate transctions flagged as fraud.       ","cebb6291":"### Sampling & Models : Decreases performance\n\n* **OVERSAMPLING OR UNDERSAMPLING SHOULD ONLY BE APPLIED TO TRAIN.** ","fe79c895":"### Using hyperopt for XGBoostClassifier hyperparameter tuning \n\n* Parameter spaces can be built from manually entering values or selecting from one of these distrbutions.\n    - hp.choice(label, options) \u2014 Returns one of the options, which should be a list or tuple.\n    - hp.randint(label, upper) \u2014 Returns a random integer between the range (0, upper).\n    - hp.uniform(label, low, high) \u2014 Returns a value uniformly between low and high.\n    - hp.quniform(label, low, high, q) \u2014 Returns a value round(uniform(low, high) \/ q) * q, i.e it rounds the decimal values and returns an integer\n    - hp.normal(label, mean, std) \u2014 Returns a real value that\u2019s normally-distributed with mean and standard deviation sigma.\n    \n    \n* The function optimized by hyperopt always minimizes - so we have to return 1-f1_score, in order to maximize this metric of our choice. \n\n\n* An odd feature: parameter names have to be provided twice in the parameter selection space. \n\n\n* XGBoost uses a special matrix type called a DMatrix. xgb.XGBClassifier automatically groups the data in a DMatrix, but when using cross validation or more complicated code, we must explicitly convert data to a DMatrix first. \n\n\n* the Trials() function stores data as the hyperopt algorithm progresses. It allows us to learn a few details about the internal working of the hyperopt algorithm. Running the Trials() function is optional. \n\n","a325b1bd":"# Preprocessing","3c7544e7":"## Random Forest","70c5ac2a":"### Baseline model using XGBClassifier.","a4952200":"#### Synthetic Minority Oversampling: Precision very low, Recall high.\n\n* SMOTE is an improved version of Random Oversampling. But this is useful only when all undersampled cases are similar.","b18322e2":"* 30 feature columns in dataset. \n    - We don't know what V1-V26 are, but we know they have been scaled (boxplots show a similar range of values, to confirm).\n    - Additionally,  'Time' in seconds, over a period of 2 days, is available. \n    - Majority of transactions are of a smaller 'Amount', mean is USD 88, range is ~ USD 0 - USD 25,691 range.\n    \n    \n* No null values in dataset. \n\n\n* Imbalanced classes can be seen below. ","ae5d4c9b":"# Models\n\n## XGBoost","c9ac48ef":"## Voting Classifier\n\n* We now have three models with diverse performance measures. \n\n\n* Logistic Regression model shows highest recall, but very low precion & f1 score. \n* XGBoostClassifier shows a good recall, intermediate precision and f1 score. \n* Random Forest model shows highest precision, recall and f1 scores are good too. \n\n\n* A voting classifier will support stronger detection of fraud cases, than by using the models individually and thus protect banks from large losses. ","bb008cdd":"#### Random Oversampling : Precision very low, Recall high","7e677647":"# Imports & read file"}}