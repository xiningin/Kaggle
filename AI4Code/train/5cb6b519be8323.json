{"cell_type":{"538870f0":"code","4b58c904":"code","09aaa735":"code","1c7b9853":"code","bba665b0":"code","b2f28bb6":"code","457b291a":"code","b5d09706":"code","1ecf009f":"code","02762667":"code","b539517c":"code","aadc9838":"code","ba3ad847":"code","85468a77":"code","75143651":"code","3ac95527":"code","4bc11b8f":"code","3407272e":"code","8eb91af5":"code","9a22bd6d":"code","e8cc83c2":"code","e989c48d":"code","5676f300":"code","97ceffaa":"code","4ce651f7":"code","1ce3b33c":"code","864adb35":"code","5aa462d9":"code","f58e292b":"code","02e76723":"code","22d01631":"code","cd400848":"code","a8c437da":"code","b22c9475":"code","231d3bea":"code","4baec6f2":"code","3b04553e":"code","f7c9ebbb":"code","fcbf2dbc":"code","d4c75c81":"code","7be52224":"code","5f40fdc5":"code","92c34a03":"code","ccfe0767":"code","b36bf4e5":"code","c094f44a":"code","10186448":"code","84f774cc":"code","d010d4db":"markdown","53b734b8":"markdown","57256594":"markdown","59efcd3c":"markdown","5f43eaa0":"markdown","067ed7d7":"markdown","d1b07284":"markdown","c0a80f41":"markdown","4b94bd9d":"markdown","9b87d046":"markdown","a3f45516":"markdown","0f9a908c":"markdown","397d3b2f":"markdown","ac51b709":"markdown","ab186778":"markdown","b234bfe1":"markdown","260bec64":"markdown","eaface88":"markdown"},"source":{"538870f0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport sklearn\nimport re\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\n\nimport os\nprint(os.listdir(\"..\/input\"))","4b58c904":"stopwords = stopwords.words('english')\nstemmer = SnowballStemmer(\"english\")","09aaa735":"# Utility functions\n\n# takes raw data, converts to sentences, and removes tokens that aren't words or digits\ndef clean_sentences(text):\n    tokens = [sent for sent in nltk.sent_tokenize(text)]\n\n    sent_list = []\n    for sent in tokens:\n        sent_str = ''\n        for i, word in enumerate(nltk.word_tokenize(sent)):\n            # nltk doesn't handle apostrophes correctly\n            if word[0] == \"'\":\n                sent_str = sent_str[:-1]\n            \n            # only adds words and digits\n            if re.search('[a-zA-Z0-9]', word):\n                sent_str += word.lower() + ' '\n        sent_list.append(sent_str.strip())\n\n    return sent_list\n\n# takes list of clean sentences and converts to list of tokens\ndef tokens_only(text):\n    tokens = []\n    \n    for sentence in text:\n        tokens.extend(sentence.split(\" \"))\n    \n    return tokens\n\n# takes in text, cleans it, and returns lemma only\ndef lemma_tokens(text):\n    tokens = tokens_only(clean_sentences(text))\n    \n    return [stemmer.stem(token) for token in tokens]","1c7b9853":"# loading files\nfilenames = ['against_the_gods', 'battle_through_the_heavens', 'desolate_era', 'emperors_domination', 'martial_god_asura', 'martial_world', 'overgeared', 'praise_the_orc', 'sovereign_of_the_three_realms', 'wu_dong_qian_kun']\nraw_files = []\n\nfor filename in filenames:\n    with open('..\/input\/' + filename + '.txt', encoding='utf-8') as myfile:\n        raw_files.append(myfile.read())","bba665b0":"# use extend so it's a big flat list of vocab\nall_lemma = []\nall_tokens = []\nall_sentences = []\nall_sentences_label = []\n\nfor i, doc in enumerate(raw_files):\n    \n    # clean sentences    \n    tmp_list= clean_sentences(doc)\n    all_sentences.extend(tmp_list)\n    for j in range(len(tmp_list)):\n        all_sentences_label.append(filenames[i])\n    \n    # convert list of clean sentences to tokens\n    tmp_list = tokens_only(tmp_list)\n    all_tokens.extend(tmp_list)\n    \n    # gets root word for tokens in document\n    all_lemma.extend(lemma_tokens(doc))","b2f28bb6":"vocab_df = pd.DataFrame({'words': all_tokens}, index = all_lemma)\nprint ('there are', vocab_df.shape[0], 'words and', len(all_sentences), 'sentences')","457b291a":"vocab_df.head()","b5d09706":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer","1ecf009f":"vectorizer = TfidfVectorizer(max_df=0.75, # drop words that occur in more than 3\/4 of the sentence\n                             min_df=2, # only use words that appear at least twice\n                             stop_words='english', \n                             lowercase=False,\n                             use_idf=True, # use inverse document frequencies in our weighting\n                             norm=u'l2', # Apply a correction factor so that longer sentences and shorter sentences get treated equally\n                             smooth_idf=True, # Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n                             tokenizer=lemma_tokens,\n                             ngram_range=(1,3)                             \n                            )\n\n# Apply the vectorizer\ntfidf_matrix = vectorizer.fit_transform(raw_files)\nprint(tfidf_matrix.shape)","02762667":"terms = vectorizer.get_feature_names()","b539517c":"from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, SpectralClustering, AffinityPropagation\n\nnum_clusters = 5","aadc9838":"km = KMeans(n_clusters=num_clusters)\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()","ba3ad847":"novels = { 'title': filenames, 'text': raw_files, 'cluster': clusters }\ndf = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])","85468a77":"print(\"Clusters:\")\nprint('-'*40)\n\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    if i != 0:\n        print('\\n')\n    print(\"Cluster %d words: \" % i, end='')\n    \n    for j, ind in enumerate(order_centroids[i, :10]):\n        if (j == 0):\n            print('%s' % vocab_df.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n        else:\n            print(', %s' % vocab_df.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n    print()\n    \n    print(\"Cluster %d titles: \" % i, end='')\n    for j, title in enumerate(df.loc[i]['title'].values.tolist()):\n        if (j == 0):\n            print('%s' % title, end='')\n        else:\n            print(', %s' % title, end='')","75143651":"ms = MeanShift()\nms.fit(tfidf_matrix.toarray())\n\nn_clusters_ = len(np.unique(ms.labels_))\nprint(\"Number of estimated clusters: {}\".format(n_clusters_))","3ac95527":"sc = SpectralClustering(n_clusters=num_clusters)\nsc.fit(tfidf_matrix)\nclusters = sc.labels_.tolist()","4bc11b8f":"sc_df = pd.DataFrame(sc.affinity_matrix_, index=filenames, columns=filenames)\nprint(sns.heatmap(sc_df.corr()))\nsc_df","3407272e":"novels = { 'title': filenames, 'text': raw_files, 'cluster': clusters }\n\ndf = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])","8eb91af5":"print(\"Clusters:\")\nprint('-'*40)\n\nfor i in range(num_clusters):\n    if i != 0:\n        print('\\n')\n\n    print(\"Cluster %d titles: \" % i, end='')\n    for j, title in enumerate(df.loc[i]['title'].values.tolist()):\n        if (j == 0):\n            print('%s' % title, end='')\n        else:\n            print(', %s' % title, end='')","9a22bd6d":"num_clusters = 6\nsc = SpectralClustering(n_clusters=num_clusters)\nsc.fit(tfidf_matrix)\nclusters = sc.labels_.tolist()","e8cc83c2":"novels = { 'title': filenames, 'text': raw_files, 'cluster': clusters }\n\ndf = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])","e989c48d":"print(\"Clusters:\")\nprint('-'*40)\n\nfor i in range(num_clusters):\n    if i != 0:\n        print('\\n')\n\n    print(\"Cluster %d titles: \" % i, end='')\n    for j, title in enumerate(df.loc[i]['title'].values.tolist()):\n        if (j == 0):\n            print('%s' % title, end='')\n        else:\n            print(', %s' % title, end='')","5676f300":"af = AffinityPropagation()\naf.fit(tfidf_matrix)\n\naf_clusters = len(af.cluster_centers_indices_)\nprint(\"Number of estimated clusters: {}\".format(af_clusters))","97ceffaa":"clusters = af.labels_","4ce651f7":"novels = { 'title': filenames, 'text': raw_files, 'cluster': clusters }\n\ndf = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])","1ce3b33c":"print(\"Clusters:\")\nprint('-'*40)\n\nfor i in range(af_clusters):\n    if i != 0:\n        print('\\n')\n\n    print(\"Cluster %d titles: \" % i, end='')\n    for j, title in enumerate(df.loc[i]['title'].values.tolist()):\n        if (j == 0):\n            print('%s' % title, end='')\n        else:\n            print(', %s' % title, end='')","864adb35":"num_clusters = 4\nkm = KMeans(n_clusters=num_clusters)\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()","5aa462d9":"novels = { 'title': filenames, 'text': raw_files, 'cluster': clusters }\n\ndf = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])","f58e292b":"print(\"Clusters:\")\nprint('-'*40)\n\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    if i != 0:\n        print('\\n')\n    print(\"Cluster %d words: \" % i, end='')\n    \n    for j, ind in enumerate(order_centroids[i, :10]):\n        if (j == 0):\n            print('%s' % vocab_df.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n        else:\n            print(', %s' % vocab_df.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n    print()\n    \n    print(\"Cluster %d titles: \" % i, end='')\n    for j, title in enumerate(df.loc[i]['title'].values.tolist()):\n        if (j == 0):\n            print('%s' % title, end='')\n        else:\n            print(', %s' % title, end='')","02e76723":"vectorizer = TfidfVectorizer(max_df=0.75, # drop words that occur in more than 3\/4 of the sentence\n                             min_df=2, # only use words that appear at least twice\n                             stop_words='english', \n                             lowercase=False,\n                             use_idf=True, # use inverse document frequencies in our weighting\n                             norm=u'l2', # Apply a correction factor so that longer sentences and shorter sentences get treated equally\n                             smooth_idf=True, # Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n                             tokenizer=lemma_tokens                        \n                            )\n\n# Apply the vectorizer\ntfidf_matrix = vectorizer.fit_transform(all_sentences)\nprint(\"Number of features: %d\" % tfidf_matrix.get_shape()[1])","22d01631":"terms = vectorizer.get_feature_names()","cd400848":"# splitting into training and test sets\n# keeps sentence structure\nX_train, X_test, y_train, y_test = train_test_split(all_sentences, all_sentences_label, test_size=0.25, random_state=0)\n# scores of tfidf\nX_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_matrix, all_sentences_label, test_size=0.25, random_state=0)\n\n# force output into compressed sparse row if it isn't already; readable format\nX_train_tfidf_csr = X_train_tfidf.tocsr()","a8c437da":"n = X_train_tfidf_csr.shape[0]\nterms = vectorizer.get_feature_names()\n\n# create empty list of dictionary, per sentence\nsents_tfidf = [{} for _ in range(0,n)]\n\n# for each sentence, list feature words and tf-idf score\nfor i, j in zip(*X_train_tfidf_csr.nonzero()):\n    sents_tfidf[i][terms[j]] = X_train_tfidf_csr[i, j]","b22c9475":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\n# SVD data reducer.  Reduces the number of features from 5252 to 950.\nsvd= TruncatedSVD(950)\nlsa = make_pipeline(svd, Normalizer(copy=False))\n# Run SVD on the training data, then project the training data.\nlsa_all_sents = lsa.fit_transform(tfidf_matrix)\n\nvariance_explained=svd.explained_variance_ratio_\ntotal_variance = variance_explained.sum()\nprint(\"Percent variance captured by all components:\",total_variance*100)\n\n#Looking at what sorts of sentences solution considers similar, for the first three identified topics\nsents_by_component=pd.DataFrame(lsa_all_sents,index=all_sentences)\nfor i in range(3):\n    print('Component {}:'.format(i))\n    print(sents_by_component.loc[:,i].sort_values(ascending=False)[0:10])","231d3bea":"X_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(sents_by_component, all_sentences_label, test_size=0.25, random_state=0)","4baec6f2":"from sklearn import ensemble\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","3b04553e":"# logistic regression with tf idf\nlr = LogisticRegression()\n\nlr.fit(X_train_tfidf, y_train_tfidf)\nprint('Training set score:', lr.score(X_train_tfidf, y_train_tfidf))\nprint('Test set score:', lr.score(X_test_tfidf, y_test_tfidf))","f7c9ebbb":"# logistic regression with SVD\nlr.fit(X_train_svd, y_train_svd)\nprint('Training set score:', lr.score(X_train_svd, y_train_svd))\nprint('Test set score:', lr.score(X_test_svd, y_test_svd))","fcbf2dbc":"# random forest with tf idf\nrfc = ensemble.RandomForestClassifier()\n\nrfc.fit(X_train_tfidf, y_train_tfidf)\nprint('Training set score:', rfc.score(X_train_tfidf, y_train_tfidf))\nprint('Test set score:', rfc.score(X_test_tfidf, y_test_tfidf))","d4c75c81":"# random forest with SVD\nrfc.fit(X_train_svd, y_train_svd)\nprint('Training set score:', rfc.score(X_train_svd, y_train_svd))\nprint('Test set score:', rfc.score(X_test_svd, y_test_svd))","7be52224":"# support vector classification with tf idf\nsvc = SVC(kernel='linear', gamma='auto')\n\nsvc.fit(X_train_tfidf, y_train_tfidf)\nprint('Training set score:', svc.score(X_train_tfidf, y_train_tfidf))\nprint('Test set score:', svc.score(X_test_tfidf, y_test_tfidf))","5f40fdc5":"# support vector classification with svd\nsvc.fit(X_train_svd, y_train_svd)\nprint('Training set score:', svc.score(X_train_svd, y_train_svd))\nprint('Test set score:', svc.score(X_test_svd, y_test_svd))","92c34a03":"test_list = []\nfor title in filenames:\n    tmp_list = \"\"\n    for i, sentence_label in enumerate(y_test):\n        if sentence_label == title:\n            tmp_list += X_test[i] + '. '\n    test_list.append(tmp_list)        ","ccfe0767":"vectorizer = TfidfVectorizer(max_df=0.75, # drop words that occur in more than 3\/4 of the sentence\n                             min_df=2, # only use words that appear at least twice\n                             stop_words='english', \n                             lowercase=False,\n                             use_idf=True, # use inverse document frequencies in our weighting\n                             norm=u'l2', # Apply a correction factor so that longer sentences and shorter sentences get treated equally\n                             smooth_idf=True, # Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n                             tokenizer=lemma_tokens,\n                             ngram_range=(1,3)                             \n                            )\n\n# Apply the vectorizer\ntfidf_matrix = vectorizer.fit_transform(test_list)\nprint(tfidf_matrix.shape)","b36bf4e5":"terms = vectorizer.get_feature_names()","c094f44a":"km = KMeans(n_clusters=num_clusters)\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()","10186448":"novels = { 'title': filenames, 'text': test_list, 'cluster': clusters }\ndf = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])","84f774cc":"print(\"Clusters:\")\nprint('-'*40)\n\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    if i != 0:\n        print('\\n')\n    print(\"Cluster %d words: \" % i, end='')\n    \n    for j, ind in enumerate(order_centroids[i, :10]):\n        if (j == 0):\n            print('%s' % vocab_df.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n        else:\n            print(', %s' % vocab_df.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n    print()\n    \n    print(\"Cluster %d titles: \" % i, end='')\n    for j, title in enumerate(df.loc[i]['title'].values.tolist()):\n        if (j == 0):\n            print('%s' % title, end='')\n        else:\n            print(', %s' % title, end='')","d010d4db":"## Affinity Propagation\nLike Mean Shift, the number of clusters does not have to be specified for Affinity Propagation. Instead, it determines the number of clusters by finding tokens that are so similar between novels that it is possible that the words can be used in the other novels in the clusters.","53b734b8":"After creating a list of tokens and lemma, I put them into a dataframe to easily reference them again later ","57256594":"Next, let's see how well the texts get clustered again with only 25% of the texts.","59efcd3c":"As the number of clusters that Mean Shift clustering has found is only 1 (all novels are alike), it is not a good clustering method for this scenario.","5f43eaa0":"Out of all the clustering methods applied, the clustering method that appears to best represent the data is KMeans. While KMeans and Affinity Propagation were able to group the clusters identically when the same number of clusters were specified, KMeans can take it a step further and figure out which exact words were the most similar that caused the novels to be grouped together.","067ed7d7":"## Spectral Clustering\nSpectral Clustering groups tokens that are similar based on the eigenvalues of the similarity matrix. Put another way, tokens that show up in similar contexts will determine which novels are grouped together.","d1b07284":"# Grouping Similar Web Novels\nIn this unsupervised learning project, I will create a model that groups novels that are similar together.","c0a80f41":"# Clustering\nUsing the data available, it will be run through four different clustering methods to see which method best represents the data. Those methods are KMeans, Mean Shift, Spectral Clustering, and Affinity Propagation","4b94bd9d":"# Supervised Modeling","9b87d046":"Unfortunately, it appears that these supervised learning models aren't very accurate.  These models appear to highly overfit on the training data sets. However, out of all of these, it appears that logistic regression with SVD, while not the most accurate, does not overfit as much as the other models.","a3f45516":"# Classifying Texts\nLet's keep the clustering method in the backburner for a bit and see if we can correctly identify which sentence originated from which novel. To do this, we will push all the sentences into a vectorizer which breaks each sentence down to words and gives it a tf-idf score, which shows how frequent the word appears in that sentence compared to total sentences. The tf-idf scores for all sentences will then be stored in a matrix, which will be used by supervised learning models. In addition, because the number of tokens generated is high, the tokens will also be checked and attempt to combine tokens that are most similar together, like Spectral Clustering.","0f9a908c":"Stopwords are a list of the most common words that may not be useful in determining whether it belongs to one group or another. Words such as *and, else, her, or, their*.\nStemmer is used to reduce words to their basic word. An example would be *fighting* would be reduced to *fight*","397d3b2f":"Interestingly enough, Affinity Propagation figured that 4 clusters is enough to classify these novels. If I ran KMeans again with 4 clusters, would it group these works similarily?","ac51b709":"In the above affinity matrix, the higher the number is, represents how similar the novel is compared to another novel. When a novel is compared against itself, it is 1 because the two works are identical. Looking at this, we can see that against_the_gods and battle_through_the_heavens are most similar together. praise_the_orc, is most similar to overgeared and desolate_era is also slightly less similar.  However, depending on the number of clusters set, it may or may not be in a category by itself. Emperors_domination isn't similar to any other work so it is likely that it will be in it's own cluster. Martial_god_asura is most similar to sovereign_of_the_three_realms, and martial_world is most similar to wu_dong_qian_kun. Let's see how accurate the predictions are.","ab186778":"## Mean Shift\nMean Shift clustering finds vectors that are most similar to the data points. It then shifts the data points onto those vectors, which determines how many clusters are available. The number of clusters is not a parameter. Instead, Mean Shift will let us know how many clusters it has found.","b234bfe1":"## K Means\nThis clustering method splits the tokens into *k* clusters, with each token belonging to the cluster with the nearest mean.","260bec64":"Interestingly enough, it appears that the clusters still group the novels together in the same way!","eaface88":"Looks like the predictions aren't too far off. The only difference was that desolate_era and emperors_domination were grouped together. If there were six clusters however, would they both have their own individual cluster?"}}