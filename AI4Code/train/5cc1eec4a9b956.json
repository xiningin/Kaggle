{"cell_type":{"0235773c":"code","1cfc8b04":"code","0234fde7":"code","bae52f57":"code","07d19a75":"code","68727494":"code","01017006":"code","b9575568":"code","7456b7cb":"code","53d5a86b":"code","ca20b26b":"code","f2c90ccc":"code","0e4e25d7":"code","abdc55c9":"code","b29560d5":"code","522c71bd":"code","9e3f76f1":"code","ead56dc7":"code","3a64523c":"code","186f3905":"code","ec6381da":"code","4395c46d":"code","48f54a26":"code","45f6f069":"code","f995cc56":"code","5ace9e1a":"markdown","b391f817":"markdown","f528d3d3":"markdown","193038e2":"markdown","e5af9369":"markdown","83963233":"markdown","dba7f29e":"markdown","f0f4f9b3":"markdown","319f47e5":"markdown","a4085cc8":"markdown","6c44ccaf":"markdown","4f9e2516":"markdown","fc5d7406":"markdown","f8ba0ff0":"markdown","54837da2":"markdown","d3ef4eca":"markdown","9d16cd4a":"markdown","eea666f6":"markdown","03627233":"markdown","cee7ac82":"markdown","5855c34f":"markdown"},"source":{"0235773c":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport random\nimport time\nimport os\nimport copy\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport torchvision\nimport torchvision.transforms as transform\nfrom torchvision.utils import make_grid\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","1cfc8b04":"train_pd = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntrain_pd.head(5)","0234fde7":"sorted(train_pd.label.unique())","bae52f57":"fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(10,8))\n\ni = 0\nfor row in range(3):\n  for col in range(4):\n    axs[row][col].imshow(train_pd.iloc[i, 1:].values.reshape(28,28), cmap='gray')\n    axs[row][col].set_title(str(train_pd.iloc[i, 0]), fontsize=20)\n    axs[row][col].axis('off')\n    i+=1\n\nplt.suptitle('Data Samples', fontsize=30)\nplt.show()","07d19a75":"train_samplesize = pd.DataFrame({str(x): train_pd['label'].value_counts()[x] for x in range(10)}, index=[0])\n\nsns.barplot(data=train_samplesize).set_title('Training Set Data Imbalance', fontsize=20)\nplt.show()","68727494":"def plotHist(img):\n  img = train_pd.iloc[img, 1:].values.reshape(28,28)\n  plt.figure(figsize=(10,5))\n  plt.subplot(1,2,1)\n  plt.imshow(img, cmap='gray')\n  plt.axis('off')\n  histo = plt.subplot(1,2,2)\n  histo.set_ylabel('Count')\n  histo.set_xlabel('Pixel Intensity')\n  plt.hist(img.flatten(), bins=10, lw=0, color='r', alpha=0.5)\n\nplotHist(2021)","01017006":"class MyDataset(Dataset):\n  def __init__(self, features, labels, Transform):\n    self.x = features\n    self.y = labels\n    self.transform = Transform\n\n  def __len__(self):\n    return len(self.x)\n\n  def __getitem__(self, index):\n    return self.transform(self.x[index]), self.y[index]","b9575568":"def GetDf(df, Transform):\n  x_features = df.iloc[:, 1:].values\n  y_labels = df.label.values\n  x_features = x_features.reshape(-1, 1, 28, 28)\n  x_features = np.uint8(x_features)\n  x_features = torch.from_numpy(x_features)\n  y_labels = torch.from_numpy(y_labels)\n  return MyDataset(x_features, y_labels, Transform)","7456b7cb":"transformer = {\n '0': transform.Compose([\n                           transform.ToPILImage(),\n                           transform.Resize(94),\n                           transform.Grayscale(num_output_channels=3), \n                           transform.ToTensor(),\n                           transform.Normalize(\n                                    [0.13097111880779266, 0.13097111880779266, 0.13097111880779266],\n                                    [0.30848443508148193, 0.30848443508148193, 0.30848443508148193])\n]),\n\n    '1': transform.Compose([\n                           transform.ToPILImage(),\n                           transform.Resize(94),\n                           transform.Grayscale(num_output_channels=3),\n                           transform.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n                           transform.RandomRotation(5),\n                           transform.RandomAffine(degrees=11, translate=(0.1,0.1), scale=(0.8,0.8)),\n                           transform.ToTensor(),\n                           transform.Normalize(\n                                    [0.13097111880779266, 0.13097111880779266, 0.13097111880779266],\n                                    [0.30848443508148193, 0.30848443508148193, 0.30848443508148193])\n]),\n    'val': transform.Compose([\n                           transform.ToPILImage(),\n                           transform.Resize(94),\n                           transform.Grayscale(num_output_channels=3),\n                           transform.ToTensor(),\n                           transform.Normalize(\n                                  [0.13141274452209473, 0.13141274452209473, 0.13141274452209473],\n                                  [0.30904173851013184, 0.30904173851013184, 0.30904173851013184])\n    ])\n}","53d5a86b":"exampleset = GetDf(train_pd, Transform=transform.Compose([\n                           transform.ToPILImage(),\n                           transform.Grayscale(num_output_channels=3),\n                           transform.ToTensor()\n    ]))\n\nx, y = next(iter(torch.utils.data.DataLoader(exampleset)))\n\nchannels = ['Red', 'Green', 'Blue']\ncmaps = [plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n\nfig, ax = plt.subplots(1, 4, figsize=(15, 10))\n\nfor i, axs in enumerate(fig.axes[:3]):\n    axs.imshow(x[0][i,:,:], cmap=cmaps[i])\n    axs.set_title(f'{channels[i]} Channel')\n    axs.set_xticks([])\n    axs.set_yticks([])\n    \nax[3].imshow(x[0].permute(1,2,0), cmap='gray')\nax[3].set_title('Three Channels')\nax[3].set_xticks([])\nax[3].set_yticks([]);","ca20b26b":"# Get the device\nuse_cuda = torch.cuda.is_available()\nprint(use_cuda)","f2c90ccc":"class TestDataset(Dataset):\n    def __init__(self, features,transform=transform.Compose([\n                              transform.ToPILImage(),\n        transform.Resize(94),\n        transform.Grayscale(num_output_channels=3),\n                              transform.ToTensor(),\n                              transform.Normalize(\n                                  [0.13141274452209473, 0.13141274452209473, 0.13141274452209473],\n                                  [0.30904173851013184, 0.30904173851013184, 0.30904173851013184])\n    ])):\n        self.features = features.values.reshape((-1,28,28)).astype(np.uint8)\n        self.targets = None\n        self.transform=transform\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        return self.transform(self.features[idx])","0e4e25d7":"def create_dataloaders(seed, test_size=0.1, df=train_pd, batch_size=50):\n    \n    # Create training set and validation set\n    train_data, valid_data = train_test_split(df,\n                                              test_size=test_size,\n                                              random_state=seed)\n    \n    # Create Datasets\n    train_dataset_0 = GetDf(train_data, Transform=transformer['0'])\n    train_dataset_1 = GetDf(train_data, Transform=transformer['1'])\n    \n    train_dataset = ConcatDataset([train_dataset_0, train_dataset_1])\n\n    valid_dataset = GetDf(valid_data, Transform=transformer['val'])\n    \n    # Create Dataloaders\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=4)\n\n    train_size = len(train_dataset)\n    val_size = len(valid_dataset)\n\n    return train_loader, valid_loader, train_size, val_size","abdc55c9":"losses = {'train':[], 'val':[]}\naccuracies = {'train':[], 'val':[]}","b29560d5":"def train(seed, epochs, model):\n\n  # Train and valid dataloaders\n  print('Creating new dataloaders...')\n    \n  train_loader, valid_loader, train_size, val_size = create_dataloaders(seed=seed)\n\n  loaders = {'train': train_loader, 'val': valid_loader}\n\n  dataset_sizes = {'train': train_size, 'val': val_size}\n\n  print('Creating a model {}...'.format(seed))\n\n  model.to(device)  \n  criterion = nn.CrossEntropyLoss()\n  if seed==2 or seed==3:\n    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n  else:\n    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n#   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=3, verbose=True)\n\n  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)\n  since = time.time()\n  best_model = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n  for epoch in range(epochs):\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n      \n      running_loss = 0.0\n      running_corrects = 0.0\n\n      for inputs, labels in loaders[phase]:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase=='train'):\n          outp = model(inputs)\n          _, pred = torch.max(outp, 1)\n          loss = criterion(outp, labels)\n        \n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n            \n\n        running_loss += loss.item()*inputs.size(0)\n        running_corrects += torch.sum(pred == labels.data)\n\n#       if phase == 'train':\n#           acc = 100. * running_corrects.double() \/ dataset_sizes[phase]\n#           scheduler.step(acc)\n\n      epoch_loss = running_loss \/ dataset_sizes[phase]\n      epoch_acc = running_corrects.double()\/dataset_sizes[phase]\n      losses[phase].append(epoch_loss)\n      accuracies[phase].append(epoch_acc)\n      if phase == 'train':\n        print('Epoch: {}\/{}'.format(epoch+1, epochs))\n      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n    \n      if phase == 'val':\n        print('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n        print('=='*31)\n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model = copy.deepcopy(model.state_dict())\n    scheduler.step() \n  time_elapsed = time.time() - since\n  print('CLASSIFIER TRAINING TIME {}m {}s'.format(time_elapsed\/\/60, time_elapsed%60))\n  print('=='*31)\n\n\n  model.load_state_dict(best_model)\n\n  for param in model.parameters():\n        param.requires_grad=True\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)  \n#   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=3, verbose=True)\n  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)\n  for epoch in range(epochs):\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n      \n      running_loss = 0.0\n      running_corrects = 0.0\n\n      for inputs, labels in loaders[phase]:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase=='train'):\n          outp = model(inputs)\n          _, pred = torch.max(outp, 1)\n          loss = criterion(outp, labels)\n        \n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n        \n        running_loss += loss.item()*inputs.size(0)\n        running_corrects += torch.sum(pred == labels.data)\n\n#       if phase == 'train':\n#         acc = 100. * running_corrects.double() \/ dataset_sizes[phase]\n#         scheduler.step(acc)\n\n      epoch_loss = running_loss \/ dataset_sizes[phase]\n      epoch_acc = running_corrects.double()\/dataset_sizes[phase]\n      losses[phase].append(epoch_loss)\n      accuracies[phase].append(epoch_acc)\n      if phase == 'train':\n        print('Epoch: {}\/{}'.format(epoch+1, epochs))\n      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n    \n      if phase == 'val':\n        print('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n        print('=='*31)    \n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model = copy.deepcopy(model.state_dict())\n    scheduler.step() \n  time_elapsed = time.time() - since\n  print('ALL NET TRAINING TIME {}m {}s'.format(time_elapsed\/\/60, time_elapsed%60))\n  print('=='*31)\n\n  model.load_state_dict(best_model)\n    \n  model.eval() # Evaluation mode -> Turn off dropout\n  test_pred = torch.LongTensor()\n    \n  if use_cuda:\n    test_pred = test_pred.cuda()\n        \n  with torch.no_grad(): # Turn off gradients for prediction, saves memory and computations\n    for features in test_loader:\n        \n        if use_cuda:\n            features = features.cuda()\n\n            # Get the softmax probabilities\n        outputs = model(features)\n            # Get the prediction of the batch\n        _, predicted = torch.max(outputs, 1)\n            # Concatenate the prediction\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    \n  model_name = 'model_' + str(seed + 1)\n  ensemble_df[model_name] = test_pred.cpu().numpy()\n  print('Prediction Saved! \\n') ","522c71bd":"densenet121_0 = torchvision.models.densenet121(pretrained=True)\nfor param in densenet121_0.parameters():\n  param.requires_grad=False\n\ndensenet121_0.classifier = nn.Linear(in_features=densenet121_0.classifier.in_features, out_features=10, bias=True)","9e3f76f1":"densenet121_1 = torchvision.models.densenet121(pretrained=True)\nfor param in densenet121_1.parameters():\n  param.requires_grad=False\n\ndensenet121_1.classifier = nn.Linear(in_features=densenet121_1.classifier.in_features, out_features=10, bias=True)","ead56dc7":"googlenet = torchvision.models.googlenet(pretrained=True)\nfor param in googlenet.parameters():\n  param.grad_requires = False\n\ngooglenet.fc = nn.Linear(in_features=googlenet.fc.in_features, out_features=10, bias=True)","3a64523c":"resnet101 = torchvision.models.resnet101(pretrained=True)\nfor param in resnet101.parameters():\n  param.grad_requires = False\n\nresnet101.fc = nn.Linear(in_features=resnet101.fc.in_features, out_features=10, bias=True)","186f3905":"vgg19_bn = torchvision.models.vgg19_bn(pretrained=True)\nfor param in vgg19_bn.parameters():\n  param.grad_requires = False\n\nvgg19_bn.classifier[6] = nn.Linear(4096, 10, bias=True)","ec6381da":"# Create test_loader\nsubmit_df = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntest_dataset = TestDataset(test_df)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nensemble_df = submit_df.copy()\n\nnum_models = 5\nnum_epochs = 10\n\nmodels = [densenet121_0, densenet121_1, googlenet, resnet101, vgg19_bn]\n\nfor seed in range(num_models):\n   train(seed=seed, epochs=num_epochs, model=models[seed])","4395c46d":"ensemble_df.head()","48f54a26":"# Final prediction\nfinal_pred = ensemble_df.iloc[:,2:].mode(axis=1).iloc[:,0]\nsubmit_df.Label = final_pred.astype(int)\nsubmit_df.head()","45f6f069":"# Create a submission file\nsubmit_df.to_csv('submission0.csv', index=False)","f995cc56":"fig, ax = plt.subplots(5, 2, figsize=(15, 15))\nmodelname = ['DenseNet_0', 'DenseNet_1', 'GooglNet', 'ResNet101', 'VGG16 with BN']\n\nepochs=10\n\ni=0\n\nfor row in range(5):\n\n  epoch_list = list(range(1,epochs*2+1))\n\n  ax[row][0].plot(epoch_list, accuracies['train'][i:20+i], label='Train Accuracy')\n  ax[row][0].plot(epoch_list, accuracies['val'][i:20+i], label='Validation Accuracy')\n  ax[row][0].set_xticks(np.arange(0, epochs*2+1, 5))\n  ax[row][0].set_ylabel('Accuracy Value')\n  ax[row][0].set_xlabel('Epoch')\n  ax[row][0].set_title('Accuracy {}'.format(modelname[row]))\n\n  ax[row][1].plot(epoch_list, losses['train'][i:20+i], label='Train Loss')\n  ax[row][1].plot(epoch_list, losses['val'][i:20+i], label='Validation Loss')\n  ax[row][1].set_xticks(np.arange(0, epochs*2+1, 5))\n  ax[row][1].set_ylabel('Loss Value')\n  ax[row][1].set_xlabel('Epoch')\n  ax[row][1].set_title('Loss {}'.format(modelname[row]))\n  fig.tight_layout()\n  fig.subplots_adjust(top=1.5, wspace=0.3)\n  i+=20","5ace9e1a":"**Here we will save the history to make a visualization at the end**","b391f817":"**Launching training**","f528d3d3":"**Uploading models**","193038e2":"# **5. Learning Visualization**","e5af9369":"# **3. Training and Test**\n**I will use an ensemble of pre-trained models, the idea is this: I first train only the classifier on 10 epochs, then unfreeze the network and train all together for another 10 epochs. After that, the model makes predictions on the test data**","83963233":"# **0. Importing Libraries**","dba7f29e":"**Before calling the class, you need to prepare the data, that is, break the dataframe into features and labels, make a reshape, and convert it to tensors. All this is done by the function below**","f0f4f9b3":"**Let's write a function that will create its own datasets for each model**","319f47e5":"Train function structure:\n\n1. **Classifier Training**\n1. **Network-wide Training**\n1. **Predictions**","a4085cc8":"**Now we know that there are only 10 classes from 0 to 9**","6c44ccaf":"**Augmentation. There is nothing unusual here, I would just like to draw your attention to the fact that we are converting images to three-channel images, since they will be fed to the input models that were pre-trained on three-channel images**","4f9e2516":"***I am always happy to receive any feedback. What do you think can be changed and what can be removed?***","fc5d7406":"**Let's write our own dataset so that we can apply augmentations**","f8ba0ff0":"**This is a custom dataset for the test part of the data**","54837da2":"**Let's see what the pictures look like**","d3ef4eca":"# **4. Submit Preparing**","9d16cd4a":"**Here we can see that there is no strong imbalance, so it is not worth fighting it**","eea666f6":"# **2. Data preparation**","03627233":"# **1. Data Loading**\n**Open the data in the pandas table**","cee7ac82":"**As you can see, the idea of defrosting feature extractor worked and we see a sharp increase in accuracy**","5855c34f":"**Sometimes the data is normalized in advance, but as you can see in the graph, this is not the case, so the data will have to be normalized**"}}