{"cell_type":{"bbdf4a3f":"code","d034cec8":"code","45399c0b":"code","62e04104":"code","a95273e7":"code","529cfee7":"code","873bb359":"code","77ad540c":"code","4638a517":"code","1552801c":"code","c050d543":"code","544f0488":"code","6259f55d":"code","af6d0b9c":"code","af4e1a96":"code","53578d7d":"markdown","c357bd10":"markdown","9b948ff0":"markdown","a9062442":"markdown","14a5ba42":"markdown","0ee7021e":"markdown","da124ee3":"markdown","120fed42":"markdown","c7cbeec6":"markdown"},"source":{"bbdf4a3f":"import os\nimport pandas as pd\nfrom kaggle.competitions import nflrush\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nimport pickle\nimport tqdm","d034cec8":"env = nflrush.make_env()\ntrain_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)","45399c0b":"train_df.iloc[0, :]","62e04104":"train_df.head()","a95273e7":"unused_columns = [\"GameId\",\"PlayId\",\"Team\",\"Yards\",\"TimeHandoff\",\"TimeSnap\"]","529cfee7":"unique_columns = []\nfor c in train_df.columns:\n    if c not in unused_columns+[\"PlayerBirthDate\"] and len(set(train_df[c][:11]))!= 1:\n        unique_columns.append(c)\n        print(c,\" is unique\")\nunique_columns+=[\"BirthY\"]","873bb359":"ok = True\nfor i in range(0,509762,22):\n    p=train_df[\"PlayId\"][i]\n    for j in range(1,22):\n        if(p!=train_df[\"PlayId\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by PlayId.\" if ok else \"train data is not sorted by PlayId.\")\nok = True\nfor i in range(0,509762,11):\n    p=train_df[\"Team\"][i]\n    for j in range(1,11):\n        if(p!=train_df[\"Team\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by Team.\" if ok else \"train data is not sorted by Team.\")","77ad540c":"all_columns = []\nfor c in train_df.columns:\n    if c not in unique_columns + unused_columns+[\"DefensePersonnel\",\"GameClock\",\"PlayerBirthDate\"]:\n        all_columns.append(c)\nall_columns.append(\"DL\")\nall_columns.append(\"LB\")    \nall_columns.append(\"DB\")\nall_columns.append(\"GameHour\")   \nfor c in unique_columns:\n    for i in range(22):\n        all_columns.append(c+str(i))","4638a517":"lbl_dict = {}\nfor c in train_df.columns:\n    if c == \"DefensePersonnel\":\n        arr = [[int(s[0]) for s in t.split(\", \")] for t in train_df[\"DefensePersonnel\"]]\n        train_df[\"DL\"] = pd.Series([a[0] for a in arr])\n        train_df[\"LB\"] = pd.Series([a[1] for a in arr])\n        train_df[\"DB\"] = pd.Series([a[2] for a in arr])\n    elif c == \"GameClock\":\n        arr = [[int(s[0]) for s in t.split(\":\")] for t in train_df[\"GameClock\"]]\n        train_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n    elif c == \"PlayerBirthDate\":\n        arr = [[int(s[0]) for s in t.split(\"\/\")] for t in train_df[\"PlayerBirthDate\"]]\n        train_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n    elif train_df[c].dtype=='object' and c not in unused_columns: \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[c].values))\n        lbl_dict[c] = lbl\n        train_df[c] = lbl.transform(list(train_df[c].values))","1552801c":"train_data=np.zeros((509762\/\/22,len(all_columns)))\nfor i in tqdm.tqdm(range(0,509762,22)):\n    count=0\n    for c in all_columns:\n        if c in train_df:\n            train_data[i\/\/22][count] = train_df[c][i]\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            train_data[i\/\/22][count] = train_df[c][i+j]\n            count+=1        ","c050d543":"y_train_ = np.array([train_df[\"Yards\"][i] for i in range(0,509762,22)])\nX_train = pd.DataFrame(data=train_data,columns=all_columns)","544f0488":"data = [0 for i in range(199)]\nfor y in y_train_:\n    data[int(y+99)]+=1\nplt.plot([i-99 for i in range(199)],data)","6259f55d":"y_train = np.zeros(len(y_train_),dtype=np.float)\nfor i in range(len(y_train)):\n    y_train[i]=(y_train_[i])\n\nscaler = preprocessing.StandardScaler()\nscaler.fit([[y] for y in y_train])\ny_train = np.array([y[0] for y in scaler.transform([[y] for y in y_train])])","af6d0b9c":"folds = 50\nseed = 222\nkf = KFold(n_splits = folds, shuffle = True, random_state=seed)\ny_valid_pred = np.zeros(X_train.shape[0])\nmodels = []\n\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.datasets import make_classification\n\n\nfor tr_idx, val_idx in kf.split(X_train, y_train):\n    tr_x, tr_y = X_train.iloc[tr_idx,:], y_train[tr_idx]\n    vl_x, vl_y = X_train.iloc[val_idx,:], y_train[val_idx]\n            \n    print(len(tr_x),len(vl_x))\n    tr_data = lgb.Dataset(tr_x, label=tr_y)\n    vl_data = lgb.Dataset(vl_x, label=vl_y)  \n    #clf = RandomForestClassifier(n_estimators=700, max_depth=20, random_state=0)\n    clf = lgb.LGBMRegressor( n_estimators=100, learning_rate=0.01, max_depth=2)\n    clf.fit(tr_x, tr_y,\n        eval_set=[(vl_x, vl_y)],\n        early_stopping_rounds=20,\n        verbose=False)\n    y_valid_pred[val_idx] += clf.predict(vl_x, num_iteration=clf.best_iteration_)\n    models.append(clf)\n\ngc.collect()","af4e1a96":"index = 0\nfor (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n    for c in test_df.columns:\n        if c == \"DefensePersonnel\":\n            try:\n                arr = [[int(s[0]) for s in t.split(\", \")] for t in test_df[\"DefensePersonnel\"]]\n                test_df[\"DL\"] = [a[0] for a in arr]\n                test_df[\"LB\"] = [a[1] for a in arr]\n                test_df[\"DB\"] = [a[2] for a in arr]\n            except:\n                test_df[\"DL\"] = [np.nan for i in range(22)]\n                test_df[\"LB\"] = [np.nan for i in range(22)]\n                test_df[\"DB\"] = [np.nan for i in range(22)]\n        elif c == \"GameClock\":\n            try:\n                arr = [[int(s[0]) for s in t.split(\":\")] for t in test_df[\"GameClock\"]]\n                test_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n            except:\n                test_df[\"GameHour\"] = [np.nan for i in range(22)]\n        elif c == \"PlayerBirthDate\":\n            try:\n                arr = [[int(s[0]) for s in t.split(\"\/\")] for t in test_df[\"PlayerBirthDate\"]]\n                test_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n            except:\n                test_df[\"BirthY\"] = [np.nan for i in range(22)]\n        elif c in lbl_dict and test_df[c].dtype=='object'and c not in unused_columns\\\n            and not pd.isnull(test_df[c]).any():\n            try:\n                test_df[c] = lbl_dict[c].transform(list(test_df[c].values))\n            except:\n                test_df[c] = [np.nan for i in range(22)]\n    count=0\n    test_data = np.zeros((1,len(all_columns)))\n\n    for c in all_columns:\n        if c in test_df:\n            test_data[0][count] = test_df[c][index]\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            test_data[0][count] = test_df[c][index + j]\n            count+=1        \n    y_pred = np.zeros(199)        \n    y_pred_p = np.sum(np.round(scaler.inverse_transform(\n        [model.predict(test_data)[0] for model in models])))\/folds\n    y_pred_p += 99\n    for j in range(199):\n        if j>=y_pred_p+10:\n            y_pred[j]=1.0\n        elif j>=y_pred_p-10:\n            y_pred[j]=(j+10-y_pred_p)*0.05\n    env.predict(pd.DataFrame(data=[y_pred],columns=sample_prediction_df.columns))\n    index += 22\nenv.write_submission_file()","53578d7d":"Since the training data was sorted, preprocessing can be done easily.","c357bd10":"Since the variance is small, I standardized the objective variable.","9b948ff0":"## import\nLoad the necessary libraries.","a9062442":"It is a fork of https:\/\/www.kaggle.com\/hukuda222\/nfl-simple-model-using-lightgbm\n\nMany thanks to hukuda222: https:\/\/www.kaggle.com\/hukuda222\n\n## Introduction\nI will introduce a simple method using lightGBM as a starter.\nfor the original notebook\ncredit and many thanks to hukuda222","14a5ba42":"The organizers seemed to expect to predict one by one, so I did. \nHowever, it seems that it is likely to be faster to predict at once after all the evaluation data is acquired by dummy input.\n\n\nThis model is a simple one that has not been tuned, so I think we can still expect a better score.\nPlease let me know if you have any opinions or advice.","0ee7021e":"## make submission\n\nWhen there is a label that does not exist in the training data, it is handled as nan.\nIf you can check the error one by one and complement it, you will get better score.","da124ee3":"## train\nI used LGBMRegressor.\nI wanted to use multi-class classification, but the number of datasets was small and it was difficult to split them including all labels.","120fed42":"## train data\nThe shape of train data is 509762 \u00d7 49.\nBut, since one set consists of 22 lines, the actual number of data is 23171.\nI converted it to a format that is easy to use.","c7cbeec6":"## evaluation\nContinuous Ranked Probability Score (CRPS) is derived based on the predicted scalar value.\nThe CRPS is computed as follows:\n$$\nC=\\frac{1}{199N}\\sum_{m=1}^N\\sum_{n=-99}^{99}(P(y\\geq n)-H(n-Y_m))^2\n$$\n$H(x)=1$ if $x\\geq 0$ else $0$"}}