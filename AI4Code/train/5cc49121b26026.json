{"cell_type":{"dc2a58cf":"code","179a03bb":"code","8c4daf46":"code","51104fd1":"code","9ff5a097":"code","e9812a90":"code","75f3b571":"code","c4248ca5":"code","5e4a24c1":"code","a45be29b":"code","040896fe":"code","b26fd775":"code","9dc8f8e0":"code","f428fbf1":"code","bf4b3d24":"code","1a3052e7":"code","aac5b580":"code","a84604cf":"code","b6f6c038":"code","ec9a847c":"code","1bf0b98c":"code","c3ef5527":"code","62330356":"code","fbd53883":"code","fef708f8":"code","4f10d2d5":"code","c37bbb63":"code","d3ae7416":"code","519adcb4":"code","ad855bb6":"code","255dd4d3":"code","0680f445":"code","b7815e5a":"code","1d8603fa":"code","f6f321cb":"code","7909eedb":"code","d77e30df":"code","aab88fdf":"code","8b163334":"code","7a2e0f74":"code","a5d824ad":"code","a63d6e82":"code","0adcc0d0":"code","a4510f48":"code","77bc315e":"code","a262435d":"markdown","a6d00447":"markdown","8b377574":"markdown","8efd8b78":"markdown","cf6e7417":"markdown","d651c3f8":"markdown","457dd3ef":"markdown","f50b3aa7":"markdown","7e3c12aa":"markdown","5b71cc7c":"markdown","f80c6cb5":"markdown","47358fc6":"markdown","e2638e3f":"markdown","785d1121":"markdown","274cab1f":"markdown","df579681":"markdown","ac2ec7ea":"markdown","91e2ca73":"markdown","1c3d2e5c":"markdown","34df49fd":"markdown","f6606371":"markdown","5477cc45":"markdown","f4e645f2":"markdown","9aaf3c63":"markdown","065656e8":"markdown","126aaf5c":"markdown","a86c0d68":"markdown"},"source":{"dc2a58cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","179a03bb":"#Importing necessary libraries for initial analysis\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.special import boxcox1p\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","8c4daf46":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","51104fd1":"df_train.head()\nId = df_test['Id']\nprint(df_train.shape)","9ff5a097":"df_train['SalePrice'].describe()","e9812a90":"#Skewness\nprint('Sale Price Skewness = ',df_train['SalePrice'].skew())\n#Kurtosis\nprint('Sale Price Kurtosis = ',df_train['SalePrice'].kurtosis())","75f3b571":"plt.figure(figsize=(15,10))\nsns.distplot(df_train.SalePrice)","c4248ca5":"# Scatter plot Saleprice and GrliveArea\nsns.scatterplot(x = 'GrLivArea', y = 'SalePrice' , data = df_train)","5e4a24c1":"# Scatter plot Saleprice and TotalBsmtSF\nsns.scatterplot(x = 'TotalBsmtSF', y = 'SalePrice' , data = df_train)","a45be29b":"plt.figure(figsize=(15,10))\nsns.boxplot(y = 'SalePrice', x = 'OverallQual', data = df_train)","040896fe":"plt.figure(figsize=(15,10))\nsns.boxplot(y = 'SalePrice', x = 'YearBuilt', data = df_train)\nplt.xticks(rotation=90)\nplt.axis(ymin=0, y=800000)","b26fd775":"#Correlation with Sale Price \ncorr = df_train.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr,vmax=.8, square=True)","9dc8f8e0":"#saleprice correlation matrix\nplt.figure(figsize=(15,10))\nk = 10 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()\n","f428fbf1":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', \n        'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols])","bf4b3d24":"#Normalizing Sale price \ndf_train.SalePrice  = np.log1p(df_train.SalePrice)\nplt.figure(figsize=(15,10))\nsns.distplot(df_train.SalePrice)","1a3052e7":"#Outlier in GrlivArea\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000)&(df_train['SalePrice']<12.5)].index)\n","aac5b580":"plt.figure(figsize=(15,10))\nsns.scatterplot(x = 'GrLivArea', y = 'SalePrice' , data = df_train)","a84604cf":"#Missing Train data\nmissing_total = df_train.isnull().sum().sort_values(ascending=False)\nmiss_pre = ((df_train.isnull().sum() \/ df_train.isnull().count()) * 100).sort_values(ascending=False)\nmissing = pd.concat([missing_total,miss_pre], axis=1, keys=['Total', 'Percent'])\nmissing.head(20)","b6f6c038":"missing = missing.head(20)\nplt.figure(figsize=(15,10))\nmissing['Percent'].plot(kind=\"bar\", fontsize = 10)\nplt.xlabel('Columns')\nplt.ylabel('Percentage')\nplt.title('Missing Train Data')","ec9a847c":"#Missing Test data\nmissing_total = df_test.isnull().sum().sort_values(ascending=False)\nmiss_pre_test = ((df_test.isnull().sum() \/ df_test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_test = pd.concat([missing_total,miss_pre], axis=1, keys=['Total', 'Percent'])\nmissing_test.head(20)","1bf0b98c":"#Test data\nmissing_test = missing_test.head(20)\nplt.figure(figsize=(15,10))\nmissing_test['Percent'].plot(kind=\"bar\", fontsize = 10)\nplt.xlabel('Columns')\nplt.ylabel('Percentage')\nplt.title('Missing Test Data')","c3ef5527":"# ID drop Train\ndf_train = df_train.drop('Id',1)\nprint(df_train.shape)\n# ID drop Test\ndf_test = df_test.drop(['Id'],1)\nprint(df_test.shape)","62330356":"#combine Data to increase amount of data for analysis\ncombine = pd.concat([df_train,df_test], ignore_index=True)\ncombine.shape","fbd53883":"# Separate target variable \nY = df_train['SalePrice']\ndf = combine.drop(['SalePrice'], axis = 1)\nprint(Y.shape)\nprint(df.shape)","fef708f8":"#Missing Test data\nmissing_total = df.isnull().sum().sort_values(ascending=False)\nmiss_pre_test = ((df.isnull().sum() \/ df.isnull().count()) * 100).sort_values(ascending=False)\nmissing_test = pd.concat([missing_total,miss_pre], axis=1, keys=['Total', 'Percent'])\nmissing_test.head(20)","4f10d2d5":"#40% > column removal \ndf = df.drop(['PoolQC', 'MiscFeature','Alley', 'Fence','FireplaceQu'], 1)\ndf.shape","c37bbb63":"#Find the types of data in column \ndf.dtypes.value_counts()","d3ae7416":"df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)","519adcb4":"#Fill missing values with None\ncols1 = [\"GarageQual\",\"GarageCond\", \"GarageFinish\", \"GarageType\", \"BsmtExposure\", \n         \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\",\"MasVnrType\",\"GarageYrBlt\"]\nfor col in cols1:\n    df[col].fillna(\"None\", inplace=True)","ad855bb6":"#Fill these values with 0 as already many present\ncols=[\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \n      \"BsmtFinSF1\", \"GarageArea\"]\nfor col in cols:\n    df[col].fillna(0, inplace=True)","255dd4d3":"# fill in with mode\ncols2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\", \n         \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\nfor col in cols2:\n    df[col].fillna(df[col].mode()[0], inplace=True)","0680f445":"#For numeric data:\ndf.select_dtypes(include = ['int','float']).isnull().sum().sort_values(ascending=False)","b7815e5a":"# Now for lot area. We assume that each neighbourhood has same lot area \ndf['LotFrontage'] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","1d8603fa":"#Conversion numeric to string\nNum_Str = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\n           \"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\n           \"GarageYrBlt\"]\nfor col in Num_Str:\n    df[col]=df[col].astype(str)","f6f321cb":"#All clean\nprint(df.shape)","7909eedb":"#Getting dummies variable \ndf = pd.get_dummies(df)\ndf.shape ","d77e30df":"#Applying PCA \npca = PCA(whiten = True)\npca.fit(df)\nvariance = pca.explained_variance_ratio_\ncumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\ncumulative_explained_variance ","aab88fdf":"# Visualize\nplt.plot(range(len(cumulative_explained_variance)), cumulative_explained_variance)","8b163334":"#PCA Apply on 12 featues that explain all the variance \npca = PCA(n_components = 12)\nX_transform = pca.fit_transform(df)\nprint(X_transform.shape)","7a2e0f74":"X = pd.DataFrame(X_transform)\npca_dict = []\n\nfor i in range(12):\n    pca_name = 'PCA ' + str(i + 1)\n    pca_dict.append(pca_name)\nX.columns = pca_dict\nX.index = df.index\nX.head()","a5d824ad":"X_train = X[:1458]\nX_test = X[1458:]\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y.shape)","a63d6e82":"LR = LinearRegression()\nLR.fit(X_train,Y)\nY_predict = LR.predict(X_train)\nprint( 'R_2 is :', (r2_score(Y,Y_predict))*100)\nprint( 'MAE:', (mean_absolute_error(Y,Y_predict))*100)","0adcc0d0":"RF = RandomForestRegressor(n_estimators=100, max_depth = 10 )\nRF.fit(X_train, Y)\nY_predict = RF.predict(X_train)\nprint( 'R_2 is :', (r2_score(Y,Y_predict))*100)\nprint( 'MAE:', (mean_absolute_error(Y,Y_predict))*100)","a4510f48":"Y_predict = RF.predict(X_test)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': np.exp(Y_predict)})","77bc315e":"submission.to_csv('SimpleAvg.csv', index=False)","a262435d":"Revmoving outlier in GrlivArea and TotalBSMT\n","a6d00447":"Lot Area is divided on the basis of neighbourhood","8b377574":"# Further Exploration ","8efd8b78":"Moving on lets see the Sale Price relationship with \n* 'GrLivArea' \n* 'TotalBsmtSF'","cf6e7417":"Year Vs Sale Price ","d651c3f8":"Data in Tabulated form","457dd3ef":"From the above visulaization it is clear that:-\n1. GrliveArea have a linear relationship with Sale price with some outliers \n2. TotalBsmtSF have a Exponential relationship with Sale price with some outliers ","f50b3aa7":"Remove the id column in both test and train as is irrelevant to calculation","7e3c12aa":"# Feature Engineering","5b71cc7c":"Linear Regression","f80c6cb5":"# Data Cleaning","47358fc6":"Random Forest Regressor","e2638e3f":"Lets first normalize the sales price which is skewed using log.\n","785d1121":"The above pair plot give us a conclusive understanding of the variable that are most correlated with the sale price.","274cab1f":"# Data Exploration ","df579681":"Remove column having < 40%  null data.","ac2ec7ea":"# Training the model ","91e2ca73":"Now lets get the dummies variables and apply PCA.","1c3d2e5c":"From the distplot it seem that salesprice is left skewed and needs to be normalized","34df49fd":"Now we combine all the data for further preprocessing ","f6606371":"Now we will convert numeric column to strings that appears to be categorical in nature and values.","5477cc45":"Above mentioned variable looks to be affecting most the total sale price but still it doesnt mean we can reject the other variable. ","f4e645f2":"Now lets see what % of data is missing in columns.","9aaf3c63":"Lets find the correlation of different variables with Sale price ","065656e8":"OverallQual Vs SalePrice ","126aaf5c":"Lets first see how sales price is distributed","a86c0d68":"Now further analyze the largest effecting variable on the total sale price "}}