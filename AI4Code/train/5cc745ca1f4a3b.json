{"cell_type":{"289b57a2":"code","4a0f651d":"code","a1b3eb23":"code","ba97b952":"code","af468242":"code","4791f118":"code","aad08506":"code","8b5563ed":"code","c261972c":"code","879db7af":"code","1bb47a55":"code","18e0c6a7":"code","07011e2b":"code","118d40f5":"code","c30a46c0":"code","d407e224":"code","d8a9c094":"code","664f3313":"code","c6c8ad8f":"code","5451a5bf":"code","e2d71867":"code","d4e5587f":"code","d561e69c":"code","bd779dfe":"code","e03ca4d0":"code","4a315354":"code","b9678aae":"code","995c2ebd":"code","273f6409":"code","83bd14ac":"code","c21a3701":"code","48da60e5":"markdown","cacba1f5":"markdown","e67bc2e5":"markdown","5f319635":"markdown","2ea7c4b1":"markdown","cc0ef47d":"markdown","a3822885":"markdown","92edf678":"markdown","1ecc7cc2":"markdown","cf627891":"markdown"},"source":{"289b57a2":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv', index_col = 'id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv', index_col = 'id')\ntrain","4a0f651d":"test","a1b3eb23":"train.shape","ba97b952":"test.shape","af468242":"train.info()","4791f118":"test.info()","aad08506":"target = train.target.copy()\ntarget","8b5563ed":"target.describe()","c261972c":"train.drop('target', axis = 1, inplace = True)\ntrain","879db7af":"(train.columns).equals(test.columns)","1bb47a55":"train.describe().T.style.bar(subset = ['mean'], color = 'royalblue').background_gradient(subset = ['std'], cmap = 'Blues_r')","18e0c6a7":"pd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nsns.set_style('whitegrid')","07011e2b":"plt.figure(figsize = (16, 6))\ntarget_order = sorted(target.unique())\nsns.barplot(x = target.value_counts().index, y = target.value_counts(), order = target_order, palette = 'Blues_r')","118d40f5":"def plot_grid(data, fig_size, grid_size, plot_type, target = ''):\n    \"\"\"\n    Custom function for plotting grid of plots.\n    It takes: DataFrame of data, size of a grid, type of plots, string name of target variable;\n    And it outputs: grid of plots.\n    \"\"\"\n    fig = plt.figure(figsize = fig_size)\n    if plot_type == 'histplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.histplot(data[column_name], kde = True, color = 'royalblue', stat = 'count')\n    if plot_type == 'boxplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.boxplot(x = data[column_name], color = 'royalblue')\n    if plot_type == 'barplot':\n        target = data[target]\n        target_order = sorted(target.unique())\n        for i, column_name in enumerate(data.drop(target.name, axis = 1).columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            new_data = data[[column_name, target.name]].groupby(target.name).mean()\n            plot = sns.barplot(x = new_data.index, y = new_data[column_name], palette = 'Blues_r', order = target_order)\n    plt.tight_layout()","c30a46c0":"plot_grid(train, (16, 36), (17, 3), 'histplot')","d407e224":"plot_grid(train, fig_size = (16, 36), grid_size = (17, 3), plot_type = 'boxplot')","d8a9c094":"plot_grid(pd.concat([train, target], axis = 1), (16, 36), (17, 3), 'barplot', 'target')","664f3313":"plt.figure(figsize = (16, 16))\nsns.heatmap(train.corr(),\n#             annot = True,\n#             fmt = '.2f',\n            square = True,\n            cmap = 'Blues_r',\n            cbar = False,\n            mask = np.triu(train.corr()))","c6c8ad8f":"zeroes = pd.DataFrame()\nfor i, column in enumerate(train.columns):\n    zeroes.loc[i, 'ColumnName'] = column\n    zeroes.loc[i, 'PercentOfZeroes'] = train.loc[train[column] == 0, column].count() \/ train.shape[0]\n#     print(f'{column} = {train.loc[train[column] == 0, column].count() \/ train.shape[0]}')\nzeroes.sort_values(by = 'PercentOfZeroes', ascending = False).style.background_gradient('Blues')","5451a5bf":"train_test = pd.concat([train, test], keys = ['train', 'test'], axis = 0)\ntrain_test","e2d71867":"train_test = (train_test - train_test.mean()) \/ train_test.std()\ntrain = train_test.xs('train').copy()\ntest = train_test.xs('test').copy()\ntrain","d4e5587f":"class_map = {\n    'Class_1': 0,\n    'Class_2': 1,\n    'Class_3': 2,\n    'Class_4': 3,\n}\n\ntarget = target.map(class_map).astype('int')\n\ntarget","d561e69c":"from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","bd779dfe":"def test_estimators(X, y, estimators, labels, cv):\n    ''' \n    A function for testing multiple estimators.\n    It takes: full train data and target, list of estimators, \n              list of labels or names of estimators,\n              cross validation splitting strategy;\n    And it returns: a DataFrame of table with results of tests\n    '''\n    result_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        est_name = label\n        result_table.loc[row_index, 'Model Name'] = est_name\n        \n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv = cv,\n                                    scoring = 'neg_log_loss',\n                                    n_jobs = -1)\n\n        result_table.loc[row_index, 'Test log loss'] = -cv_results['test_score'].mean()\n        result_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        result_table.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    result_table.sort_values(by = ['Test log loss'], ascending = True, inplace = True)\n\n    return result_table","e03ca4d0":"X_train, X_valid, y_train, y_valid = train_test_split(train, \n                                                      target, \n                                                      stratify = target,\n                                                      train_size = 0.1,\n                                                      random_state = 1)\ny_train","4a315354":"logreg = LogisticRegression()\ndt = DecisionTreeClassifier(random_state = 1)\nrf = RandomForestClassifier()\nxgb = XGBClassifier()\nlgbm = LGBMClassifier()\ncb = CatBoostClassifier(allow_writing_files = False, logging_level = 'Silent')\nsvc = SVC(probability = True)\ngnb = GaussianNB()\n\nestimators = [logreg,\n              dt,\n              rf,\n              lgbm, \n              cb,\n              svc,\n              gnb,]\n#               xgb]\n\nlabels = ['LogRegression',\n          'DecisionTree',\n          'RandomForest',\n          'LGBM',\n          'CatBoost',\n          'SVC',\n          'GNB',]\n#           'XGB']\n\nresults = test_estimators(X_train, y_train, estimators, labels, cv = StratifiedKFold(n_splits = 5))\nresults.style.background_gradient(cmap = 'Blues')","b9678aae":"import optuna\nfrom optuna.trial import TrialState\n\nfrom catboost import Pool, cv\n\ndef objective(trial, model, X_train_full, y_train_full):\n    if (model == 'cb'):\n        train_set = Pool(X_train_full, label = y_train_full)\n        \n        params = {\n            \"objective\": 'MultiClass',\n            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n            \"depth\": trial.suggest_int(\"depth\", 1, 12),\n            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n            \"bootstrap_type\": trial.suggest_categorical(\n                \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n            ),\n\n            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1e-1, log = True),\n            'min_child_samples': trial.suggest_int('min_child_samples', 2, 20),\n            'random_strength': trial.suggest_float('random_strength', 0.05, 1, log = True)\n        }\n\n        if params[\"bootstrap_type\"] == \"Bayesian\":\n            params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n        elif params[\"bootstrap_type\"] == \"Bernoulli\":\n            params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n        \n        k = 5\n        cb_cv_results = cv(\n            params = params,\n            pool = train_set,\n            num_boost_round = 4000,\n            nfold = k,\n            stratified = True,\n            early_stopping_rounds = 100,\n            verbose_eval = False,\n        )\n        # Set n_estimators as a trial attribute; Accessible via study.trials_dataframe().\n        trial.set_user_attr(\"n_estimators\", len(cb_cv_results['test-MultiClass-mean']))\n        # Extract the best score.\n        best_score = cb_cv_results['test-MultiClass-mean'].iloc[-1]\n        return best_score","995c2ebd":"study_cb = optuna.create_study(direction = 'minimize')\nstudy_cb.optimize(lambda trial: objective(trial, 'cb', train, target), n_trials = 100, timeout = 3600 * 7)","273f6409":"print(\"Number of finished trials: \", len(study_cb.trials))\nprint(\"Best trial:\")\ntrial = study_cb.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\nprint(\"  Number of estimators: {}\".format(trial.user_attrs[\"n_estimators\"]))","83bd14ac":"cb = CatBoostClassifier(allow_writing_files = False, \n                        logging_level = 'Silent', \n                        n_estimators = trial.user_attrs[\"n_estimators\"], \n                        **study_cb.best_params)\ncb.fit(train, target)\npredictions = cb.predict_proba(test)\npredictions","c21a3701":"submission = pd.DataFrame({'id': test.index,\n                           'Class_1': predictions[:, 0],\n                           'Class_2': predictions[:, 1],\n                           'Class_3': predictions[:, 2],\n                           'Class_4': predictions[:, 3],})\n\nsubmission.to_csv('submission.csv', index = False)","48da60e5":"It seems that there are a few features that consist almost entirely out of zeroes. Let's look into that.","cacba1f5":"# 1. Meeting our data","e67bc2e5":"# Introduction\nGreetings!\ud83d\udc4b\n\nIn this kernel you will find my data science approach to \"Tabular Playground Series - May 2021\" competition using CatBoost and Optuna.\n\nAs always, any feedback Is very much appreciated! :)","5f319635":"# 2. Creating visualizations","2ea7c4b1":"Taking a sample to save some time.","cc0ef47d":"# 5. Parameter tuning with Optuna","a3822885":"# 3. Doing a bit of preprocessing","92edf678":"# Table of contents:\n\n1. Meeting our data\n\n2. Creating visualizations\n\n3. Doing a bit of preprocessing\n\n4. Creating and evaluating models\n\n5. Parameter tuning with Optuna\n\n6. Creating a final model and submitting results","1ecc7cc2":"# 4. Creating and evaluating models","cf627891":"# 6. Creating a final model and submitting results"}}