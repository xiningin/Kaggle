{"cell_type":{"ff4cb120":"code","a49dbe0a":"code","56d2a9f4":"code","f911513a":"code","979fa852":"code","ec9318f9":"code","ae3e45ac":"code","ab1acf1f":"code","30934104":"code","8ae1e9db":"code","1484add6":"code","fa48d007":"code","e995866c":"code","8f2a54a5":"code","bdbf10c0":"code","49f9360b":"code","724d081b":"code","a650fbc2":"code","3908837c":"code","7ad77f04":"code","081f3374":"code","a39dd62a":"markdown","2a3f9ca4":"markdown","2405f8e2":"markdown","9f447acb":"markdown","a0e59d48":"markdown","4e690fb3":"markdown","93bbb5ec":"markdown","103bbae4":"markdown","8592431d":"markdown","fa4be447":"markdown","73e6f7a2":"markdown","275f3d94":"markdown","9c492225":"markdown","4961faef":"markdown","01302a3a":"markdown","a803de18":"markdown","addc7bd7":"markdown","962be924":"markdown","5d05f361":"markdown","0dd8cead":"markdown","31a8c363":"markdown","4039b9b2":"markdown","2659f494":"markdown"},"source":{"ff4cb120":"# Importing the important libraries.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","a49dbe0a":"ds = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv') # reading the dataset\nds","56d2a9f4":"loc = np.where(ds['stalk-root'] == '?') # finding the location where '?' are present\nnp.shape(loc) # total number of rows where '?' is present","f911513a":"ds['stalk-root'].replace('?', np.nan, inplace = True) # replacing the '?' values with NaN values.\nds['stalk-root'].isnull().sum()","979fa852":"from sklearn.impute import SimpleImputer\nsi = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")\nds['stalk-root'] = si.fit_transform(ds['stalk-root'].values.reshape(-1,1))\nds['stalk-root'].isnull().sum() # checking for the null values again.","ec9318f9":"clist = ds.columns.values\nfor i in range(0, len(clist)):\n    print(f\" {i + 1} The unique values in feature {clist[i]} are {ds[clist[i]].unique()}\") \n# Here we have all the unique values in every feature of dataset.","ae3e45ac":"from sklearn.preprocessing import LabelEncoder\n# Encoding the categorical values to numerical values.\nle = LabelEncoder()\nfor i in range(len(clist)): # this loop will encode values in the features of the dataset one by one \n    ds[clist[i]] = le.fit_transform(ds[clist[i]])\n    print(f\" {i + 1} The unique values in feature {clist[i]} are {ds[clist[i]].unique()}\") \n    # printing the final unique values after encoding","ab1acf1f":"x = ds # making a copy of dataset\nx0 = x[x['class'] > 0] # Values from the class which are only 1 i.e Poisonous\nx1 = x[x['class'] == 0] # Values from the class which are only 0 i.e Edible\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_traces(go.Histogram(x = x0['class'], name='Poisonous', xbins = dict(size=0.5),\n                            marker_color='darkred', opacity=0.75))\nfig.add_traces(go.Histogram(x = x1['class'], name='Edible', xbins = dict(size=0.5),\n                            marker_color='forestgreen', opacity=0.75))\nfig.update_layout(title_text=\"Mushroom's Class\", xaxis_title_text='Value', yaxis_title_text='Count')\nfig.show()","30934104":"ds.var() # variance of the dataset","8ae1e9db":"ds.drop(['veil-type'], axis = 1, inplace = True) # removing from the dataset.\nds.shape # Checking the dimensions of th edataset after dropping the dataset.","1484add6":"cor_mat = ds.corr()\ncor_mat\nplt.figure(figsize = (20, 12)) # changing the figure size so that we can analyse better\nsb.heatmap(cor_mat, annot = True)","fa48d007":"x = ds.loc[:, 'cap-shape':'habitat'] # Features\ny = ds.loc[:, 'class'] # Target\nprint(x.shape, y.shape) # Dimensions of Features and Target","e995866c":"# Importing all the important models, meathods and classes.\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","8f2a54a5":"from sklearn.model_selection import train_test_split","bdbf10c0":"max_accuracy = 0\nbest_rs = 0\nfor i in range(1, 200):\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = i)\n    lg = LogisticRegression()\n    lg.fit(x_train, y_train)\n    pred = lg.predict(x_test)\n    acc = accuracy_score(y_test, pred)\n    if acc > max_accuracy: # after each iteration, acc is replace by the best possible accuracy\n        max_accuracy = acc\n        best_rs = i\nprint(f\"Best accuracy is {max_accuracy} and best random state is {best_rs}\")","49f9360b":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 54)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape) # The dimensions of training and testing data","724d081b":"rfc = RandomForestClassifier(n_estimators = 100) # making the instance of RandomForestClassifier class\nrfc.fit(x_train, y_train) # fitting the model\npred_rfc = rfc.predict(x_test) # predicting the values\nprint(\"Accuracy Score of RFC model is\", accuracy_score(y_test, pred_rfc))\n\ndtc = DecisionTreeClassifier() # making the instance of DecisionTreeClassifier class\ndtc.fit(x_train, y_train) # fitting the model\npred_dtc = dtc.predict(x_test) # predicting the values\nprint(\"Accuracy Score of DTC model is\", accuracy_score(y_test, pred_dtc))\n\nnb = MultinomialNB() # making the Multinomial Naive Bayes class\nnb.fit(x_train, y_train) # fitting the model\npred_nb = nb.predict(x_test) # predicting the values\nprint(\"Accuracy Score of MNB model is\", accuracy_score(y_test, pred_nb))\n\nknc = KNeighborsClassifier(n_neighbors = 5) # making the K-Nearest Neighbor Classifier class. By default value of nn is 5\nknc.fit(x_train, y_train) # fitting the model\npred_knc = knc.predict(x_test) # predicting the values\nprint(\"Accuracy Score of KNN model is\", accuracy_score(y_test, pred_knc))\n\nsvc = SVC(kernel = 'rbf') # making the Support Vector Machine Classifier class. By default, the kernel is set to RBF.\nsvc.fit(x_train, y_train) # fitting the model\npred_svc = svc.predict(x_test) # predicting the values\nprint(\"Accuracy Score of svc model is\", accuracy_score(y_test, pred_svc))\n\nada= AdaBoostClassifier()\nada.fit(x_train, y_train) # fitting the model\npred_ada = ada.predict(x_test) # predicting the values\nprint(\"Accuracy Score of ADA model is\", accuracy_score(y_test, pred_ada))\n\nlg.fit(x_train, y_train)\npred_lg = lg.predict(x_test) # predicting the values\nprint(\"Accuracy Score of LG model is\", accuracy_score(y_test, pred_lg))","a650fbc2":"from sklearn.model_selection import cross_val_score\nrfc_scores = cross_val_score(rfc, x, y, cv = 5) # cross validating the model\nprint(f\"Mean of accuracy for RFC is{rfc_scores.mean()}\")\n\ndtc_scores = cross_val_score(dtc, x, y, cv = 5) # cross validating the model\nprint(f\"Mean of accuracy for DTC is {dtc_scores.mean()}\")\n\nnb_scores = cross_val_score(nb, x, y, cv = 5) # cross validating the model\nprint(f\"Mean of accuracy for NB is {nb_scores.mean()}\")\n\nknc_scores = cross_val_score(knc, x, y, cv = 5) # cross validating the model\nprint(f\"Mean of accuracy for KNC is {knc_scores.mean()}\")\n\nsvc_scores = cross_val_score(svc, x, y, cv = 5) # cross validating the model\nprint(f\"Mean of accuracy for SVC is {svc_scores.mean()}\")\n\nada_scores = cross_val_score(ada, x, y, cv = 5) # cross validating the model\nprint(f\"Mean of accuracy for ADA is {ada_scores.mean()}\")\n\nlg_scores = cross_val_score(lg, x, y, cv = 5) # cross validating the model\nprint(f\"Mean of accuracy for ADA is {lg_scores.mean()}\")","3908837c":"from sklearn.model_selection import GridSearchCV\ndtc = DecisionTreeClassifier() # making the instance of class.\n# defining the parameters in the dictionary.\nparameters = { 'criterion' : ['gini', 'entropy'], 'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,15,16,17,18,19,20]}\ngs = GridSearchCV(estimator = dtc, param_grid = parameters, scoring = 'f1', cv = 5)\ngs.fit(x_train, y_train)\nprint(f\"The best possible score after tuning is : {gs.best_score_}\")\nprint(f\"The best parameters for the model given after tuning are : {gs.best_params_}\")","7ad77f04":"dtc = DecisionTreeClassifier(criterion = 'gini', max_depth = 7)\ndtc.fit(x_train, y_train) # fitting the model\nprint(f\"Learning score of model : {dtc.score(x_train, y_train)}\") #calculating that how much data have been learned\npred_dtc = dtc.predict(x_test) # predicting the values\n# Now performing some metrics to test the fitted model.\nprint(\"Accuracy Score of Decision Tree Classifier model is\", accuracy_score(y_test, pred_dtc))\nprint(\"Confusion matrix for Decision Tree Classifier model Model is : \")\nprint(confusion_matrix(y_test, pred_dtc))\nprint(\"Classification Report of the Decision Tree Classifier model Model is\")\nprint(classification_report(y_test, pred_dtc))","081f3374":"import joblib\njoblib.dump(dtc, 'mushroom prediction model.obj')","a39dd62a":"From the above correlation matrix, we can check the between class and all the other features of the dataset.\n\nSince the *values were all categorical*, so **we donot check for the outliers and skewness of the features**.\n\nWe can now proceed with spliting the data and fitting it into various models to check its performance and selecting the best model for the dataset.","2a3f9ca4":"From the above plot, it is clear that the the **dataset is balanced**. *Poisonous class* of mushrooms have *3916* values and *Edible class* has values *4208*. So the sampling of the data is not required.","2405f8e2":"**Model Fitting and Tuning**","9f447acb":"Now that we have the *best possible parameters* for the model. We fit the model with final parameters, and perform all the metrics on that model.\n\n**Final Fitting of the model** -","a0e59d48":"*Dealing with Null values in 'stalk-root' column*\n\nIn the column 'stalk-root' of the dataset, there are 2480 missing values which are replaced with '?' sign. So, we replace these values with the most frequent values or mode for that column such that no missing values are present in ther dataset.","4e690fb3":"**Encoding the Categorical Values**\n\nNow that we have checked all the null values and unique values of the features, We now convert the categorical data into the numeric data using Label Encoder, so that the model can learn and predict the target.","93bbb5ec":"From the above metrics we have found out that the **f1-score of our model is 1.0 or 100%**. Also the **precision and recall for the model is 1.0**. It means that the *model is perfectly fitted and predicting all the values accurately*.\n\n*We can now succesfully predict on any new given data for Mushrooms that they are either Edible (0) or Poisonous (1)*","103bbae4":"We now check the variance of all the features of the dataset to check if there are any columns with low variance. The feature which consist of only one value or constant values and which is low on variance will be useless for the learning of the model.","8592431d":"From the above cross validation scores, we find out that the accuracy mean for Decision Tree Classifier Model is 0.92 and Accuracy given by fitting is 1. So it has the least difference between original accuracy and mean accuracy. Hence, we select the Decision Tree Classifier as our model.\n\n**Hyper Parameter Tuning** - \n\nNow that we have selected our model as decision tree classifier, we now select the best parameters to give the best possible metrics possible. We use *GridSearchCV for the tuning of the model*.","fa4be447":"As from above, we see that the best possible random state will be 54 so we use it for the splitting of the model. Now, we split the data into training and testing with the best random state calculated","73e6f7a2":"In this project we, have to predict that if the mushroom is edible or if it is poisnous. We have 22 features and 1 target variable named as 'class'. Depending on the data, we will train our model to predict the class to which the mushroom belongs.","275f3d94":"**Cross Validating - Checking Underfiting or Overfiting**\n\nNow we cross validate different models and check the difference between the accuracy score to find out that which model is actually giving the best result. This will help us to find out if the model accuracy is real or is the model *Underfiting or Overfiting*","9c492225":"**EDA**\n\nNow we perform some visualisations to get the insights about the data. First we check if the data is balance or inbalanced","4961faef":"Now, what we have with us is a classification problem with all the features containing categorical dataset.\nWe can check for the unique values in all the features of the dataset. ","01302a3a":"Now we see that all the values have been encoded to give us the int datatype such that we can proceed with the learning.","a803de18":"We first start with splititng the dataset into input variables or features and target variable.","addc7bd7":"**Checking the Correlation - Features and Class**\n\nWe check the correlation of all the columns with the target feature.","962be924":"From the above values, we see that variance of column 'veil-type' is 0. So, all the values in this column are same. So this column can be dropped from the dataset as it will not help in the learning of the model.","5d05f361":"**Mushroom Classification - *UCI Machine Learning Problem***","0dd8cead":"As we can see that the '?' values have been replaced with the NaN values in the stalk-root column. We can now replace these null values with the mode of the column using simple imputer.","31a8c363":"We first start with checking the best possible random state for the model fitting. For this we can use any model as later we will check the performance of all the models anyways.","4039b9b2":"**Serialisation - Saving the model.**\n\nWe will save the project such that predictions can be carried out on the different types of mushrooms and one can safely find out the class of mushrooms.","2659f494":"**Finding the best Model** - \n\nNow, we fit the training and testin gdata into different classification models and check their respective accuracies. If the accuracy is low, it can mean that the model is underfitting, or if the accuracy is very high then it means that the model is overfitting. We will check the underfitting and overfitting of the model using the cross validation"}}