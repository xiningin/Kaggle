{"cell_type":{"8b82d358":"code","515c712e":"code","ed9d8c28":"code","4b9b1997":"code","7e139718":"code","0cab38e8":"code","20d183e8":"code","4c07fc6c":"code","96048ec7":"code","e6e01d38":"code","fd4eb016":"code","c02fc9db":"code","ec594f1b":"code","065ad14c":"code","a532327c":"code","e075ce6b":"code","5fde7d31":"code","bbbcaf98":"markdown","77e43bf3":"markdown","3b945517":"markdown","875971a1":"markdown","5672d49b":"markdown","7a44e2ef":"markdown","f0a0056a":"markdown","3e0d8d2f":"markdown","5abcacaf":"markdown","a23b4779":"markdown","93e8dae9":"markdown","63bd0bd1":"markdown","3ed7b965":"markdown","f7a03628":"markdown","c7e2053d":"markdown","17dad0fb":"markdown","1bf4a8c2":"markdown","5e335651":"markdown","51bcf247":"markdown","055c962e":"markdown","0f8807be":"markdown","8065d10a":"markdown"},"source":{"8b82d358":"import tensorflow as tf\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","515c712e":"pip install -q tensorflow-datasets","ed9d8c28":"import tensorflow_datasets as tfds\nfrom kaggle_datasets import KaggleDatasets\n\nif tpu:\n    DATA_DIR = KaggleDatasets().get_gcs_path('tensorflow-flowers')\nelse:\n    DATA_DIR = '\/kaggle\/input\/tensorflow-flowers'","4b9b1997":"ds, ds_info = tfds.load(\n    'tf_flowers',\n    split='train',\n    with_info=True,\n    download=False,\n    data_dir=DATA_DIR\n)","7e139718":"print(ds)","0cab38e8":"for example in ds.take(4):\n    print(\"Shape:\", example['image'].shape, \"Label:\", example['label'])","20d183e8":"ds_iter = iter(ds)\n\nexample_1 = next(ds_iter)\nprint(example_1['image'].shape, example_1['label'])\nexample_2 = next(ds_iter)\nprint(example_2['image'].shape, example_2['label'])","4c07fc6c":"import matplotlib.pyplot as plt\n\nds_np_iter = ds.as_numpy_iterator()\nexample = next(ds_np_iter)\nimage = example['image']\nlabel = example['label']\n\nplt.imshow(image)\nplt.title(\"Label: {}\\n Shape: {}\".format(label, image.shape))\nplt.axis('off')\nplt.show();","96048ec7":"ds_info","e6e01d38":"print(ds_info.splits)\nprint(ds_info.splits['train'])\nprint(ds_info.splits['train'].num_examples)\nprint(ds_info.features['label'])\nprint(ds_info.features['label'].int2str(3))","fd4eb016":"import matplotlib.pyplot as plt\n\nROWS = 3\nCOLS = 4\n\nplt.figure(figsize=(12, 12))\nfor i, example in enumerate(ds.take(ROWS*COLS)):\n    image = example['image']\n    label = example['label']\n    name = ds_info.features['label'].int2str(label)\n    plt.subplot(ROWS, COLS, i+1)\n    plt.title(\"{} ({})\".format(name, label))\n    plt.axis('off')\n    plt.imshow(image)","c02fc9db":"ds_train_, ds_valid_ = tfds.load(\n    'tf_flowers',\n    as_supervised=True,\n    split=['train[:75%]', 'train[75%:]'],\n    data_dir=DATA_DIR,\n)\n\nfor (image, label) in ds_train_.take(5):\n    print(image.shape, label)","ec594f1b":"SIZE = [640, 640]\n\ndef preprocess(image, label):\n    image = tf.image.resize(image, size=SIZE)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\ndef augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    return image, label","065ad14c":"AUTO = tf.data.experimental.AUTOTUNE # TensorFlow can automatically apply optimizations to some parts of the pipeline\nNUM_TRAINING_IMAGES = ds_info.splits['train'].num_examples\nSHUFFLE_BUFFER =  NUM_TRAINING_IMAGES \/\/ 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\nds_train = (ds_train_\n            .map(preprocess, AUTO) # do any (non-random) preprocessing first\n            .cache() # and then keep the images in memory cache\n            .shuffle(SHUFFLE_BUFFER) # randomize image order while training\n            .batch(BATCH_SIZE)\n            .map(augment, AUTO) # put (random) augmentation after batching\n            .prefetch(AUTO) # use CPU to load data while TPU is working\n)\n\nds_valid = (ds_valid_\n            .map(preprocess, AUTO)\n            .batch(BATCH_SIZE)\n            .cache()\n            .prefetch(AUTO)\n)","a532327c":"import tensorflow.keras.layers as layers\n\nNUM_CLASSES = ds_info.features['label'].num_classes\n\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        layers.Conv2D(64, kernel_size=5, activation='relu'),\n        layers.MaxPool2D(),\n        layers.Conv2D(128, kernel_size=3, activation='relu'),\n        layers.MaxPool2D(),\n        layers.Conv2D(256, kernel_size=3, activation='relu'),\n        layers.Conv2D(256, kernel_size=3, activation='relu'),\n        layers.MaxPool2D(),\n        layers.Conv2D(512, kernel_size=3, activation='relu'),\n        layers.Conv2D(512, kernel_size=3, activation='relu'),\n        layers.Conv2D(512, kernel_size=3, activation='relu'),\n        layers.MaxPool2D(),\n        layers.Conv2D(1024, kernel_size=3, activation='relu'),\n        layers.Conv2D(1024, kernel_size=3, activation='relu'),\n        layers.Conv2D(1024, kernel_size=3, activation='relu'),\n        layers.MaxPool2D(),\n        layers.Flatten(),\n        layers.Dense(16),\n        layers.Dense(16),\n        layers.Dense(NUM_CLASSES, activation='softmax')\n])\n    \nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n    loss='sparse_categorical_crossentropy', # labels are integers 0-4\n    metrics=['accuracy']\n)","e075ce6b":"EPOCHS = 20\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=EPOCHS,\n)","5fde7d31":"import pandas as pd\n\nhistory_frame = pd.DataFrame(history.history)\n\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();","bbbcaf98":"Use the methods of the `Dataset` instance to manipulate the data stream. See [the API docs](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset) for what's available.","77e43bf3":"Now let's look at our training curves.","3b945517":"## Create a Pipeline ##\n\nWe can also load the dataset in \"supervised\" format, which creates a stream of `(image, label)` tuples. A dataset in this format we can pass directly to a Keras model's fit method.","875971a1":"## Introduction ##\n\nIn this notebook, we will demonstrate how to interact with a dataset of TFRecords through the [TensorFlow Datsets](https:\/\/www.tensorflow.org\/datasets\/overview) API and how to create a data pipeline with `tensorflow.data.Dataset`.\n\nTFRecords are TensorFlow's data serialization format for high-performance data streaming. This format is ideal for applications requiring high data-throughput, like distributed neural network training with GPU or TPU acceleration. In this notebook, we will demonstrate distributed training by building an image classifier with Keras.","5672d49b":"## Load Data ##\nWe'll need to install the `tensorflow-datasets` package, since it's not currently available in the Kaggle environment. This should just take a moment.","7a44e2ef":"By default, `tfds.load` will create a dataset of dictionaries containing feature\/value pairs, in this case, an image with a label.","f0a0056a":"The `strategy` object contains a [context manager](https:\/\/docs.python.org\/3\/reference\/compound_stmts.html#with) which will later apply it to our model using a `with` statement.","3e0d8d2f":"## Explore Data ##","5abcacaf":"We'll map transformations like preprocessing and data augmentation over the `(image, label)` pairs as part of the pipeline.","a23b4779":"## Access Data ##","93e8dae9":"The `ds_info` object contains metadata about the dataset.","63bd0bd1":"The data loader needs to know:\n- the **name** of the dataset (`tf_flowers`)\n- the **split** desired (`tf_flowers` contains only a `train` split)\n- whether to return also a metadata object (`ds_info`)\n- where the dataset is located (`data_dir`)\n\nThe TensorFlow Flowers dataset is part of the [TFDS catalog](https:\/\/www.tensorflow.org\/datasets\/catalog\/tf_flowers). Follow the link to see more about this dataset.","3ed7b965":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","f7a03628":"When training with TPU, we need to instruct Kaggle to send the dataset to a data bucket accessible to the TPU host machine. Otherwise, we can use Kaggle's local copy of the dataset.","c7e2053d":"## Define Model ##","17dad0fb":"Or convert it to an iterator and pull out examples one-by-one.","1bf4a8c2":"## Distribution Strategy ##\n\nSeveral tasks later on will depend on what distribution strategy we are using, that is, whether we will be training with CPU(None), GPU, or TPU. (See the **Accelerator** option in the **Settings** menu to the right.)\n\nOn Kaggle, only training by TPU requires special instruction. Running the code cell below will detect whether this is the case and return an appropriate distribution strategy.","5e335651":"The objects in the dataset are tensors, but you can convert the dataset to a stream of numpy objects as well.","51bcf247":" A `tf.data.Dataset` represents a stream of data records. You can enumerate over its contents.","055c962e":"We'll create a simple CNN image classifier using Keras.","0f8807be":"Which we can access as attributes.","8065d10a":"Epochs after the first will be faster since the data has already been cached. You can check your resource utilization by clicking the bars in the upper right."}}