{"cell_type":{"94a1aef9":"code","ab74a113":"code","6f688d55":"code","ef5f40ec":"code","2893c833":"code","7355984e":"code","3f1b592f":"code","8fb30bed":"code","8ead4c97":"code","04a65985":"code","792896f5":"code","9fc1765e":"code","16f4d343":"code","caaac64f":"code","b3f78afa":"code","3af52527":"code","553bdb20":"code","d58797f6":"code","68b73ec0":"code","1f5eae2e":"code","175b9de5":"code","da7f5b59":"code","aef8123a":"code","fcd91a97":"code","0aaca7af":"code","60de6141":"code","a538864e":"code","c4c8cc1b":"code","603aa0be":"code","d67d3d86":"code","696a7009":"code","cc82f805":"code","fe144dcc":"code","bf8d780d":"code","f5cbf022":"code","a32e774f":"code","d0c87931":"code","7ad67227":"code","cbd4e1a6":"code","ce479ee6":"code","72a59586":"code","cbbc9fe6":"code","3a37dca8":"code","77da6c56":"code","8f99ed4b":"code","9c22a835":"code","30712774":"code","dc35cb90":"code","0518a408":"code","411fb75a":"code","6ba4c775":"code","1f1b44f9":"code","5e4b855c":"code","5f6e7027":"code","466ddcd6":"code","1d7e5584":"code","7cf2b4b7":"code","318ab719":"code","42c628f0":"code","ae910422":"code","a50f61f2":"code","e507cf28":"code","f89f5ce8":"code","5213095f":"code","4329c2b5":"code","0481e65f":"code","a56d30b0":"code","2e6f8699":"code","8b909c11":"code","6f5a8499":"code","1b430fb1":"code","89fa6567":"code","48190d1f":"code","3ec56f6b":"code","1417126a":"code","99fb08fa":"code","09573164":"code","8c59ef7b":"code","744f14ed":"code","525e2a9e":"code","4c83e391":"code","b86ea04a":"code","d8a000c2":"code","9b10abed":"code","67b51c80":"code","8125a8b2":"code","31eeca64":"code","29f3ac99":"code","e0b4d203":"code","edec42fc":"code","fb636e32":"markdown","bced5a0b":"markdown","4e033303":"markdown"},"source":{"94a1aef9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n#matplot and seaborn for visualization \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","ab74a113":"#read the train csv file\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n#print first 5 rows of dataset\ntrain.head()","6f688d55":"#read the test csv file\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n#print first 5 rows of dataset\ntest.head()","ef5f40ec":"#dimesnions of train data\ntrain.shape","2893c833":"#Describes the summary of train dataset in statistical info\ntrain.describe()","7355984e":"#dimensions of test data\ntest.shape","3f1b592f":"##Describes the summary of test dataset in statistical info\ntest.describe()","8fb30bed":"#Missing value","8ead4c97":"#Count number of Nan values in train dataset\ntrain_nullvalue_count=train.isnull().sum()\ntrain_nullvalue_count","04a65985":"#Counts number of Nan values in test dataset\ntest_nullvalue_count=test.isnull().sum()\ntest_nullvalue_count","792896f5":"#percentage of Nan values in train data\nnull_train=train.isnull().sum()\/len(train)*100\nnull_train","9fc1765e":"#Percentage of Nan values in test data\nnull_test=test.isnull().sum()\/len(test)*100\nnull_test","16f4d343":"#Complete view of Nan values in count and percentage\ndf = pd.DataFrame({'Train_null_count':train_nullvalue_count,'Train_Null_percent':null_train,'Test_Null_count':test_nullvalue_count,'Test_Null_percent_test':null_test})\ndf","caaac64f":"#Passengers Survived and not survived in a ship \n#1 - Not survived 2- Survived\nsns.factorplot('Survived',data=train,kind='count',palette='Blues')","b3f78afa":"#We can see the passengers survived in Different Classes\nsns.countplot('Survived',hue='Pclass',data=train,palette='coolwarm')","3af52527":"sns.countplot(x='Survived',hue='Sex',data=train,palette='coolwarm')","553bdb20":"sns.countplot(x='Survived',hue='Embarked',data=train,palette='coolwarm')","d58797f6":"sns.barplot(x='Sex',y='Survived',hue='Pclass',data=train,palette='GnBu_d')","68b73ec0":"sns.boxplot(x='Survived',y='Age',data=train,palette='Blues')\nsns.stripplot(x='Survived',y='Age',jitter=True,data=train)","1f5eae2e":"train['Embarked'].value_counts().plot(kind='bar')\nplt.title('Boarding Places')\nplt.show()","175b9de5":"sns.factorplot(x='Pclass',y='Survived',data=train,color='r')","da7f5b59":"sns.factorplot(x='Embarked',y='Survived',data=train,color='r')","aef8123a":"#Correlation amoung features\ncorr=train.corr()\nf, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(corr,square=True,annot=True, linewidths=.5, ax=ax)","fcd91a97":"#Values missing in train dataset\n#age values were missing in train data\n#we compute with mean and fill those Nan values with the mean\nmean_age = train['Age'].mean() \ntrain['Age'] = train['Age'].fillna(value=mean_age)","0aaca7af":"#Embarked feature is a char column, so we normalize the feature and check which category has large scale of percent values \ntrain['Embarked'].value_counts(normalize=True)","60de6141":"#Fill the Nan value with 'S' category because it is highest repeated category value in Embarked feature\ntrain = train.fillna({'Embarked':'S'})","a538864e":"#Replace the Nan values of age with average age value in test data \nmean_test_age = test['Age'].mean()\ntest['Age'] = test['Age'].fillna(value=mean_test_age)","c4c8cc1b":"#Replace the Nan values of Fare with average Fare value in test data\nmean_fare = test['Fare'].mean()\ntest['Fare'] = test['Fare'].fillna(value=mean_fare)","603aa0be":"#Embarked feature is a char column, so we normalize the feature and check which category has large scale of percent values\ntest['Embarked'].value_counts(normalize=True)","d67d3d86":"#Fill the Nan value with 'S' category because it is highest repeated category value in Embarked feature\ntest = test.fillna({'Embarked':'S'})","696a7009":"#visualizate the train dataset by heatmap, to see the where the data was missing\nsns.heatmap(train.isnull())","cc82f805":"#visualize the test dataset by heatmap, to see the where we the data was missing \nsns.heatmap(test.isnull())","fe144dcc":"#drop the unwanted columns \ntrain = train.drop(['Name','Cabin','Ticket'],axis=1)","bf8d780d":"test = test.drop(['Cabin','Name','Ticket'],axis=1)","f5cbf022":"train.head()","a32e774f":"test.head()","d0c87931":"#create the category feature with one hot encoding \nsex = pd.get_dummies(train['Sex'],drop_first=True)","7ad67227":"embarked = pd.get_dummies(train['Embarked'],drop_first=True)","cbd4e1a6":"#After encoding the features drop the repective column \ntrain = train.drop(['Sex','Embarked'],axis=1)","ce479ee6":"#concat the encoding columns into train dataset\ntrain = pd.concat([train,sex,embarked],axis=1)","72a59586":"train.head()","cbbc9fe6":"#create the category feature with one hot encoding\nsex_test = pd.get_dummies(test['Sex'],drop_first=True)","3a37dca8":"embarked_test = pd.get_dummies(test['Embarked'],drop_first=True)","77da6c56":"#After encoding the features drop the repective column\ntest = test.drop(['Sex','Embarked'],axis=1)","8f99ed4b":"#concat the encoding columns into train dataset\ntest = pd.concat([test,sex_test,embarked_test],axis=1)","9c22a835":"test.head()","30712774":"#Feature Scaling\n#We can see that Age, Fare are measured on different scales, so we need to do Feature Scaling first before we proceed with predictions. \n#If we dont scale down the feature then these columns will have high importance while modeling, to overcome this we use standardscaler, \n#so all columns will have equal importance","dc35cb90":"from sklearn.preprocessing import StandardScaler","0518a408":"std_scale = StandardScaler()\ntrain[['Age','Fare']] = std_scale.fit_transform(train[['Age','Fare']])","411fb75a":"#Split the features into predictors and target variable","6ba4c775":"x = train.drop(['Survived','PassengerId'],axis=1)\ny = train['Survived']","1f1b44f9":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=45)","5e4b855c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score, log_loss, precision_score, recall_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","5f6e7027":"lr = LogisticRegression()\nlr.fit(x_train,y_train)","466ddcd6":"pred_lr = lr.predict(x_test)","1d7e5584":"lr_score=cross_val_score(lr,x_train,y_train,scoring='accuracy',cv=10)\nlr_score","7cf2b4b7":"lr_score.mean()","318ab719":"lr_accuracy=accuracy_score(y_test,pred_lr)\nlr_accuracy","42c628f0":"print(classification_report(y_test,pred_lr))","ae910422":"#76% of area is under the curve and it is quite good in this model","a50f61f2":"roc_auc_score(y_test,pred_lr)","e507cf28":"gbc = GradientBoostingClassifier(n_estimators=250,verbose=-1)\ngbc.fit(x_train,y_train)","f89f5ce8":"pred_gbc = gbc.predict(x_test)","5213095f":"gbc_score=cross_val_score(gbc,x_train,y_train,scoring='accuracy',cv=10)\ngbc_score","4329c2b5":"gbc_score.mean()","0481e65f":"gbc_accuracy=accuracy_score(y_test,pred_gbc)\ngbc_accuracy","a56d30b0":"print(classification_report(y_test,pred_gbc))","2e6f8699":"#80% of area is under the curve and it is quite good in this model","8b909c11":"roc_auc_score(y_test,pred_gbc)","6f5a8499":"abc = AdaBoostClassifier()\nabc.fit(x_train,y_train)","1b430fb1":"pred_abc = abc.predict(x_test)","89fa6567":"abc_score=cross_val_score(abc,x_train,y_train,scoring='accuracy',cv=10)\nabc_score","48190d1f":"abc_score.mean()","3ec56f6b":"abc_accuracy=accuracy_score(y_test,pred_abc)\nabc_accuracy","1417126a":"#78% of area is under the curve and it is quite good in this model","99fb08fa":"roc_auc_score(y_test,pred_abc)","09573164":"print(classification_report(y_test,pred_abc))","8c59ef7b":"rfc = RandomForestClassifier()\nrfc.fit(x_train,y_train)","744f14ed":"pred_rfc = rfc.predict(x_test)","525e2a9e":"rfc_score=cross_val_score(rfc,x_train,y_train,scoring='accuracy',cv=10)\nrfc_score","4c83e391":"#80% of area is under the curve and it is quite good in this model","b86ea04a":"rfc_score.mean()","d8a000c2":"rfc_accuracy=accuracy_score(y_test,pred_rfc)\nrfc_accuracy","9b10abed":"roc_auc_score(y_test,pred_rfc)","67b51c80":"print(classification_report(y_test,pred_rfc))","8125a8b2":"#Accuracy for the models","31eeca64":"score = pd.DataFrame({'Models':['LogesticRegression_score','GradientBoostingClassifier_score','AdaBoostClassifier_score','RandomForestClassifier'],'Scores':[lr_accuracy,gbc_accuracy,abc_accuracy,rfc_accuracy]})\nscore.sort_values('Scores', ascending=False)","29f3ac99":"test_pred = pd.DataFrame(pred_gbc, columns= ['Survived'])","e0b4d203":"new_test = pd.concat([test, test_pred], axis=1, join='inner')","edec42fc":"submission = pd.DataFrame({\n        'PassengerId' : test['PassengerId'],\n        'Survived' : test_pred['Survived']})\n\nsubmission.to_csv('titanic_submission.csv', index=False)","fb636e32":"*Visualization of data*","bced5a0b":"> Titanic : Machine Learning From Disaster","4e033303":"Variable \nPassengerId: Id Number\nSurvived: Survive(1)  Died(0)\nPclass: Class 1, Class 2, Class 3\nName: Name \nSex: Gender of passenger\nAge : Age of passenger\nSibSp: Siblings\/Spouses count\nParch: Parent\/Children count\nTicket: Ticket number\nFare: Ticket amount\nCabin: Dock category\nEmbarked: Passenger Boarding (C=Cherbourg, Q=Queenstone, S=Southhampton)\n\n"}}