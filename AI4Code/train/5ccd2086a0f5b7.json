{"cell_type":{"89083601":"code","4310df0a":"code","97072212":"code","af55339f":"code","84878984":"code","834c9107":"code","21995f70":"code","450082df":"code","f4e61ffa":"code","cfab1a51":"code","67902d94":"code","70e610de":"code","47eaae4e":"code","a09d4030":"code","df1d8ddb":"code","f652c041":"code","8270188c":"code","1b21c92d":"code","16edb6ee":"code","150f47ec":"code","125f9b3e":"code","7cc99a99":"code","2148b811":"code","165de812":"code","04310c3a":"code","00c03cb7":"code","9c77e865":"code","d16cb41c":"code","5959fb4f":"code","c132cdc9":"code","047e9d99":"code","d9e4cc29":"code","7774a911":"code","357064eb":"code","9721cf4d":"code","faf7a3d2":"code","ebd0136b":"code","eb0287b9":"code","31d6015c":"code","d14ba826":"code","7a0ca281":"code","7e67f67c":"code","3563d11e":"code","0000fe72":"code","38636c80":"code","866ff469":"code","8bb3d76f":"code","466b0226":"code","78c0f9b2":"code","2403d266":"code","4888b91d":"code","fb44a607":"code","e0cd4abb":"code","b7d610c0":"code","63c0494a":"code","00250e09":"code","6a315a9d":"code","3795f8fc":"code","373a5b02":"code","80de970a":"code","1f3fd0d8":"code","05e99bd6":"code","ae1dc71d":"code","587511bd":"code","b2705a63":"code","cec5688b":"code","dffa414f":"code","c8a4659e":"code","c7bb95a5":"code","1fc9c934":"code","c1b49255":"code","a9f3db63":"code","a9a42b33":"code","dd19c14d":"code","a97e4881":"code","163c7bed":"code","cf47f076":"code","19d40aa5":"code","c6961894":"code","bd6df7a6":"code","86e0dbf9":"code","74365ab9":"code","5477c6e7":"code","9f814086":"code","2209c0a1":"code","47d4e58a":"code","34f22cbb":"code","248c4371":"code","102012b0":"code","76d7eac2":"code","6a6902a4":"code","734333b5":"code","0d814c0d":"code","f3c46da2":"code","02283988":"markdown","ca148e4d":"markdown","20e94c6c":"markdown","3e3fb640":"markdown","7fe7ce6e":"markdown","0e59e791":"markdown","607bb217":"markdown","71404dbe":"markdown","bd4f81b3":"markdown","adde1e16":"markdown","919a97f7":"markdown","aefb6fa7":"markdown","c8bec3d1":"markdown","ca4cc874":"markdown","21b75ec6":"markdown","a0acbea9":"markdown","9191fb87":"markdown","cff5cb57":"markdown","4f6e1abf":"markdown","f921fae4":"markdown","f3037dc4":"markdown","7b7faca2":"markdown","b645534f":"markdown","29a5e76d":"markdown","29631356":"markdown","634f68c9":"markdown","46b60737":"markdown","1f6c332b":"markdown","187cc76c":"markdown","38f6f174":"markdown","30909c41":"markdown","ccc7ee53":"markdown","dd73539f":"markdown","e1a005d1":"markdown","efbe328a":"markdown","9d9ab922":"markdown","241184ac":"markdown","a1afee29":"markdown","9bd67bdf":"markdown","c7c7bd72":"markdown","7d90c77b":"markdown","14b1597d":"markdown","b25bb65d":"markdown","07da86ad":"markdown","68a39e06":"markdown","a820474f":"markdown","2d5dab0d":"markdown","52c9130f":"markdown","a6682904":"markdown","00dc7b55":"markdown","d52e5b01":"markdown","2603c8bb":"markdown","e24aec08":"markdown","7bb46538":"markdown","eea343f9":"markdown","2e0b8e34":"markdown","e7e7bdfe":"markdown","b3677203":"markdown","3b14497b":"markdown","cfd9b2d7":"markdown","9c3763d0":"markdown"},"source":{"89083601":"# Importing the libraries required\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nimport matplotlib.ticker as mticker\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport shap\n\n# Importing the sklearn modules required\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Importing the required libraries\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Importing catboost\nfrom catboost import Pool, CatBoostClassifier\n\n# Setting pandas options\npd.set_option('display.max_colwidth', 500, 'display.max_rows', 20, 'display.max_columns', 100)\npd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n\nsns.color_palette(\"bright\")\nsns.set(style='darkgrid', palette='bright', font_scale=1.2)\n\n# Plotting options\n%matplotlib inline","4310df0a":"loans = pd.read_csv(\"..\/input\/lending-club\/accepted_2007_to_2018Q4.csv.gz\", low_memory=False)","97072212":"loans.shape","af55339f":"missing_data = loans.isnull().mean().sort_values(ascending=False)*100\nmissing_data","84878984":"# Set the font options for the x\/y axes labels:\nfont = FontProperties(size=14)\nfont.set_style('italic')\n\n# Setting the formatting of the figure:\nplt.figure(figsize=(12,4))\nplt.title('Histogram of missing data\\n', fontsize=16, fontweight='bold')\nplt.xlabel('Proportion of data missing (%)', fontproperties=font)\nplt.ylabel('Count', fontproperties=font)\n\n# Setting the figure up:\nmissing_data.plot.hist(bins=30, fontsize=12);","834c9107":"columns_to_drop = sorted(list(missing_data[missing_data > 30].index))\nprint(\"Columns with less than 70% of the data included:\\n\"+str(columns_to_drop))","21995f70":"len(columns_to_drop)","450082df":"loans.drop(labels=columns_to_drop, axis=1, inplace=True)","f4e61ffa":"# Removing the features that prospective lenders would not know:\nexcluded_features = ['acc_now_delinq',  # won't know at time of the loan\n                     'acc_open_past_24mths',  # Lending Club metric\n                     'application_type',  # Not relevant\n                     'bc_open_to_buy',  # Not relevant\n                     'chargeoff_within_12_mths',  # Lending Club metric\n                     'collection_recovery_fee',  # won't know at time of the loan\n                     'debt_settlement_flag',  # won't know at time of the loan\n                     'disbursement_method',  # irrelevant\n                     'earliest_cr_line',  # relevance?\n                     'fico_range_low',  # Credit score - defeats purpose of the exercise!\n                     'fico_range_high',  # Credit score - defeats purpose of the exercise!\n                     'funded_amnt',  # won't know at time of the loan\n                     'funded_amnt_inv',  # won't know at time of the loan\n                     'hardship_flag',  # won't know at time of the loan\n                     'id',  # irrelevant\n                     'initial_list_status',  # irrelevant\n                     'last_credit_pull_d',  # won't know at time of loan\n                     'last_fico_range_high',  # won't know at time of loan\n                     'last_fico_range_low',  # won't know at time of loan\n                     'last_pymnt_d',  # irrelevant\n                     'last_pymnt_amnt',  # irrelevant\n                     'policy_code',  # irrelevant\n                     'pymnt_plan',  # won't know at time of loan\n                     'recoveries',  # won't know at time of loan\n                     'out_prncp_inv',  # won't know at time of loan\n                     'out_prncp',  # won't know at time of loan\n                     'tot_hi_cred_lim',  # definition?\n                     'title',  # irrelevant\n                     'total_pymnt',  # won't know at time of loan\n                     'total_pymnt_inv',  # won't know at time of loan\n                     'total_rec_int',  # won't know at time of loan\n                     'total_rec_late_fee',  # won't know at time of loan\n                     'total_rec_prncp',  # won't know at time of loan\n                     'total_rev_hi_lim',  # definition?\n                     'url',  # irrelevant\n                     'collections_12_mths_ex_med'  # won't know at time of application\n                    ]\nlen(excluded_features)","cfab1a51":"loans.drop(labels=excluded_features, axis=1, inplace=True)","67902d94":"loans['loan_status'].value_counts()","70e610de":"loans = loans.loc[loans['loan_status'].isin(['Fully Paid','Charged Off'])]\nprint(\"Number of loans: \"+\"{:,.0f}\".format(len(loans)))","47eaae4e":"loans['loan_status'].value_counts(dropna=False)","a09d4030":"# Calculating the charge off rate:\nchargeoffrate = loans['loan_status'].value_counts(normalize=True,dropna=False)\n\n# Showing it visually in a pie chart; creating the labels for the chart:\nlabels = []\nfor i in chargeoffrate:\n    labels.append(str(round(i*100,0))+\"%\")\n\n# Creating the pie chart\nplt.figure(figsize=(12,6))\nplt.title('Loan status', fontsize=16, fontweight='bold');\nplt.pie(chargeoffrate,labels=labels,startangle=90);\nplt.legend(['Fully paid','Charged off'], loc='best', bbox_to_anchor=(1, 0.6));","df1d8ddb":"chargeoffamount = loans.groupby('loan_status').loan_amnt.sum().sort_values(ascending=False)\n\n# Showing the amounts in $bn \nprint(\"Fully paid loans amount to $\"+str(round(chargeoffamount[0]\/1e9,1))+\"bn\")\nprint(\"Charged off loans amount to $\"+str(round(chargeoffamount[1]\/1e9,1))+\"bn\")\n\n# What is this as a percentage of the charge off rate, i.e. the cost of risk (CoR)? \nTotal_CoR = chargeoffamount[1]\/loans['loan_amnt'].sum()\nprint(\"Life time cost of risk (CoR) is \"+str(round(Total_CoR*100,1))+\"%\")","f652c041":"# Showing visually\n# Creating the labels for the pie chart:\nlabels = []\nfor i in chargeoffamount:\n    labels.append('$'+str(round(i\/1e9,1))+\"bn\")\n\n# Creating the pie chart\nplt.figure(figsize=(12,6))\nplt.title('Loan amounts', fontsize=16, fontweight='bold');\nplt.pie(chargeoffrate,labels=labels,startangle=90);\nplt.legend(['Fully paid','Charged off'], loc='best', bbox_to_anchor=(1, 0.6));","8270188c":"loans['int_amnt'] = loans['int_rate']*loans['loan_amnt']\nwgt_av_int_rate = loans['int_amnt'].sum()\/loans['loan_amnt'].sum()\n\nprint('Average interest rate of the book is: '+str(round(loans['int_rate'].mean(),2))+\"%\")\nprint('Weighted av. rate of the book is: '+str(round(wgt_av_int_rate,2))+\"%\")","1b21c92d":"# Function to visualise impact on success rate depending on whether it is a continuous or discrete variable\n\ndef plot_var(col_name, full_name, df=loans, continuous=True, rotation=0, alignment='center'):\n    \"\"\"\n    Visualises the distribution of a variable and assess its impact on success rate.\n    - col_name (variable): variable name in the dataframe\n    - full_name (string): full variable name for presentation purposes \n    - df (df): name of the dataframe\n    - continuous (bool): True if the variable is continuous, and is set as True as default\n    - rotation (int): rotation of the x axis labels \n    - alignment (string): alignment of the x axis labels and should be 'right' if rotating the labels\n    - returns: two charts analysing the loan portfolio\n    \"\"\"\n    \n    # Set up figure space:\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,4), dpi=90)\n    sns.set(palette='bright')\n    \n    # Set the font options for the x\/y axes labels:\n    font = FontProperties(size=12)\n    font.set_style('italic')\n    \n    # Plot distribution of the variable without showing charge off rate\n    if continuous:\n        # Show a histogram to visualise variable\n        sns.histplot(df.loc[df[col_name].notnull(), col_name], kde=False, ax=ax1, bins=30)\n    else:\n        x_labels = df[col_name].sort_values().unique()\n        sns.countplot(x=df[col_name], order=sorted(df[col_name].unique()), ax=ax1).set_xticklabels(x_labels, rotation=rotation, horizontalalignment=alignment)\n    \n    # Formatting the x labels\n    ax1.set_xlabel(full_name, fontproperties=font)\n    \n    # Formatting the y labels\n    y_labels_1 = ['{:,.0f}'.format(x) + 'K' for x in ax1.get_yticks()\/1000]\n    ax1.yaxis.set_major_locator(mticker.FixedLocator(ax1.get_yticks().tolist()))\n    ax1.set_yticklabels(y_labels_1)\n    ax1.set_ylabel('Count', fontproperties=font)\n    \n    # Formatting the title\n    ax1.set_title(full_name+\"\\n\", fontweight='bold')\n\n    # Plot distribution of variable split by loan status \n    if continuous:\n        sns.boxplot(x=col_name, y='loan_status', data=df, ax=ax2)\n        ax2.set_ylabel('Loan status', fontproperties=font)\n        ax2.set_title(full_name + ' by loan status'+\"\\n\", fontweight='bold')\n    else:\n        charge_off_rates = df.groupby(col_name)['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']\n        sns.barplot(x=charge_off_rates.index, y=charge_off_rates.values, ax=ax2)\n        \n        # Formatting the x-axis\n        ax2.set_xticklabels(charge_off_rates.index,rotation=rotation, horizontalalignment=alignment)\n        \n        # Formatting the y-axis\n        y_labels_2 = ['{:,.0f}'.format(x) + '%' for x in ax2.get_yticks()*100]\n        ax2.yaxis.set_major_locator(mticker.FixedLocator(ax2.get_yticks().tolist()))\n        ax2.set_yticklabels(y_labels_2)\n        ax2.set_ylabel('Proportion of loans charged off', fontproperties=font)\n\n        # Formatting the title \n        ax2.set_title('Charge off rate by ' + full_name+\"\\n\", fontweight='bold')\n    \n    ax2.set_xlabel(full_name, fontproperties=font)\n    \n    plt.tight_layout()","16edb6ee":"plot_var('int_rate', 'Interest rate', loans) ","150f47ec":"plot_var('grade','Investment grade',continuous=False)","125f9b3e":"plot_var('sub_grade','Sub-investment grade',continuous=False)","7cc99a99":"plot_var('loan_amnt', 'Loan amount')","2148b811":"loans['term'] = loans.term.map({' 36 months': 36, ' 60 months': 60})","165de812":"plot_var('term', 'Term', continuous=False)","04310c3a":"loans['term_amnt'] = loans['term']*loans['loan_amnt']\nwgt_av_term = loans['term_amnt'].sum()\/loans['loan_amnt'].sum()\n\nprint('Average term of the book is: '+str(round(loans['term'].mean(),2))+\" months\")\nprint('Weighted av. term of the book is: '+str(round(wgt_av_term,2))+\" months\")","00c03cb7":"missing_fractions = loans.isnull().mean().sort_values(ascending=False)\nprint(missing_fractions[missing_fractions>0])","9c77e865":"# Removing NaN in the remaining columns as they're likely zero anyway:\nloans.fillna(0, inplace=True)\n\n# Cleaning home ownership:\nloans[\"home_ownership\"].replace([\"NONE\", \"ANY\"], \"OTHER\", inplace=True)\n\n# Dropping dependent variables like grade and installment:\ndep_vars = [\"grade\", \"sub_grade\", \"installment\"]\nloans.drop(columns=dep_vars, axis=1, inplace=True)\n\n# After analysis, dropping columns with no impact on creditworthiness:\nno_indict = ['emp_title', 'emp_length', 'zip_code']\nloans.drop(columns=no_indict, axis=1, inplace=True)\n\n# Removing outliers on DTI column:\nloans['dti'].values[loans['dti'].values > 40] = 40\nloans['dti'].values[loans['dti'].values < 0] = 0\n\n# Removing outliers in open accounts column:\nloans['open_acc'].values[loans['open_acc'].values > 30] = 30\n\n# Removing outliers (>100) in revolver utilisation rates:\nloans['revol_util'].values[loans['revol_util'].values > 100] = 100\n\n# Removing the tail (36m+) of months since recent revolver opened:\nloans['mo_sin_rcnt_rev_tl_op'].values[loans['mo_sin_rcnt_rev_tl_op'].values > 36] = 36\n\n# Removing the tail (36m+) of months since recent account opened:\nloans['mo_sin_rcnt_tl'].values[loans['mo_sin_rcnt_tl'].values > 36] = 36\n\n# Swapping the charge off and fully paid strings for 1\/0\nloans.insert(1, 'charge_off_rate', loans.loan_status.map({'Charged Off': 1, 'Fully Paid': 0}))\n\n# Creating two columns useful for calculations later:\nloans['term_amnt'] = loans['term'] * loans['loan_amnt']\nloans['int_rate_amnt'] = loans['int_rate'] * loans['loan_amnt']","d16cb41c":"# Creating the dummy variables\ncat_features = ['home_ownership','verification_status', 'purpose']\nloans_cb = loans.copy()\nloans = pd.get_dummies(loans, columns=cat_features, drop_first=False)","5959fb4f":"# Defining the independent variables \nindependent_var = ['loan_amnt', 'term', 'annual_inc', 'dti', 'delinq_2yrs',\n                   'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_util', 'avg_cur_bal',\n                   'delinq_amnt', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n                   'num_tl_120dpd_2m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq',\n                   'percent_bc_gt_75', 'pub_rec_bankruptcies', 'total_bal_ex_mort',\n                   'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT',\n                   'verification_status_Source Verified', 'verification_status_Verified',\n                   'purpose_credit_card', 'purpose_debt_consolidation', 'purpose_educational',\n                   'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase',\n                   'purpose_medical', 'purpose_moving', 'purpose_other',\n                   'purpose_renewable_energy', 'purpose_small_business',\n                   'purpose_vacation', 'purpose_wedding',\n                   ]\n\n# Defining the features for the catboost model, i.e. without one-hot encoding, and \n# explicitly calling out the categorical features\nindependent_var_cb = ['loan_amnt', 'term', 'annual_inc', 'dti', 'delinq_2yrs',\n                   'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_util', 'avg_cur_bal',\n                   'delinq_amnt', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n                   'num_tl_120dpd_2m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq',\n                   'percent_bc_gt_75', 'pub_rec_bankruptcies', 'total_bal_ex_mort',\n                   'home_ownership','verification_status', 'purpose']\n\n# And finally the target feature\ndependent_var = ['charge_off_rate']","c132cdc9":"# Defining which year our testing set should start in:\ntesting_years_start = 2016","047e9d99":"# Creating a 'year' column so we can split the data\nloans['year'] = pd.DatetimeIndex(loans['issue_d']).year\nloans_train = loans.copy().loc[loans['year'] <= (testing_years_start-1)]\nloans_test = loans.copy().loc[loans['year'] >= testing_years_start]\n\n# Creating our training variables\nX_train = loans_train[independent_var]\ny_train = loans_train[dependent_var]\n\n# Creating our training variables for Catboost\nloans_cb['year'] = pd.DatetimeIndex(loans_cb['issue_d']).year\nloans_train_cb = loans_cb.copy().loc[loans_cb['year'] <= (testing_years_start-1)]\nloans_test_cb = loans_cb.copy().loc[loans_cb['year'] >= testing_years_start]\nX_train_cb = loans_train_cb[independent_var_cb]\nX_test_cb = loans_test_cb[independent_var_cb]\n\n# Creating our test variables\nX_test = loans_test[independent_var]\ny_test = loans_test[dependent_var]","d9e4cc29":"# Create data for graph:\namnt_by_year = loans.groupby('year').loan_amnt.sum()\/1e6\namount = list(amnt_by_year.values)\nyear = list(amnt_by_year.index)\ngroup = []\nfor i in range(loans_cb.year.min(),loans_cb.year.max()+1):\n    if i >= testing_years_start: group.append('Test')\n    else: group.append('Train')\n\ngraph_data = pd.DataFrame({'Year': year,'Loan amount': amount,'Group': group})\n\n# Bar plot\nfigsize = (12, 4)\nfig, ax = plt.subplots(figsize=figsize)\nsns.barplot(x = 'Year', y = 'Loan amount', hue='Group', data=graph_data, palette = 'bright', dodge = False);\n\n# Set the font options for the x\/y axes labels:\nfont = FontProperties(size=14)\nfont.set_style('italic')\n\nplt.title('Loans by year\\n', fontsize=16, fontweight='bold')\nplt.ylabel('Loan amount ($bn)', fontsize=14, fontproperties=font)\nylabels = ['{:,.1f}'.format(y) for y in ax.get_yticks()\/1000]\nax.yaxis.set_major_locator(mticker.FixedLocator(ax.get_yticks().tolist()))\nax.set_yticklabels(ylabels, fontsize=12)\nplt.xlabel('Year', fontsize=14, fontproperties=font)\nax.set_xticklabels(labels=year, fontsize=12);","7774a911":"n = y_train.value_counts()\nclass_weights = {0:1- n[0]\/n.sum(), 1:1- n[1]\/n.sum()}","357064eb":"# Here are our switches for undersampling\nundersample = False\ntarget_FP_proportion = 0.5","9721cf4d":"# Changing the training subset to the desired proportion of Fully Paid loans\nif undersample:\n    # Creating a new dataframe with only the fully paid loans:\n    loans_train_FP = loans_train[loans_train['charge_off_rate'] == 0]\n\n    # Creating a new dataframe with only the charged off loans:\n    loans_train_CO = loans_train[loans_train['charge_off_rate'] == 1]\n\n    # Creating a function to remove the requisite proportion of these at random\n    target_FP = int(len(loans_train_CO) * target_FP_proportion \/ (1 - target_FP_proportion))\n    remove_n = len(loans_train_FP) - target_FP\n    drop_indices = np.random.choice(loans_train_FP.index, remove_n, replace=False)\n    loans_FP_subset = loans_train_FP.drop(drop_indices)\n\n    # Combining the two dataframes to create a new loans loans_train\n    loans_train_subset = loans_FP_subset.append(loans_train_CO, ignore_index=True)\n\nelse:\n    loans_train_subset = loans_train\n\n# Redefining our test variables:\nX_train_subset = loans_train_subset[independent_var]\ny_train_subset = loans_train_subset[dependent_var]\n    \n# Checking our new charge off rate\ncharge_off_rate_train_subset = loans_train_subset['charge_off_rate'].value_counts(normalize=True,dropna=False)\nprint(\"Fully paid proportion of training set is: \"+str(round(charge_off_rate_train_subset[0]*100,1))+\"%\")","faf7a3d2":"probability_threshold = 0.5","ebd0136b":"# Creating a short function to calculate the basic book stats:\ndef book_stats(df, title):\n    \"\"\"\n    Function returns the key stats for the dataframe of loans\n    - df (df): dataframe we'll input to calculate the metrics from\n    - title (string): title of the output\n    - returns a number of financial stats as floats\n    \"\"\"\n    \n    # Creating some variables for outputs later:\n    wgt_av_term = df['term_amnt'].sum() \/ df['loan_amnt'].sum()\n    wgt_av_int_rate = df['int_rate_amnt'].sum() \/ df['loan_amnt'].sum()\n    charge_off_rate = df['loan_status'].value_counts(normalize=True, dropna=False)\n    charge_off_amount = df.groupby('loan_status').loan_amnt.sum().sort_values(ascending=False)\n    total_CoR = charge_off_amount[1] \/ df['loan_amnt'].sum()\n    \n    # And printing them out:\n    print(title)\n    print(\"  - Wgt. av. term is: \" + str(round(wgt_av_term, 2)) + \" months\")\n    print(\"  - Wgt. av. interest rate is: \" + str(round(wgt_av_int_rate, 2)) + \"%\")\n    print(\"  - Total cost of risk is: \" + str(round(total_CoR * 100, 2)) + \"%\")\n\n    return wgt_av_term, wgt_av_int_rate, charge_off_rate, charge_off_amount, total_CoR","eb0287b9":"wgt_av_term, wgt_av_int_rate, charge_off_rate, charge_off_amount, total_CoR = book_stats(loans, 'Total book')\nwgt_av_term_train, wgt_av_int_rate_train, charge_off_rate_train, charge_off_amount_train, total_CoR_train = book_stats(loans_train, 'Training set')\nwgt_av_term_test, wgt_av_int_rate_test, charge_off_rate_test, charge_off_amount_test, total_CoR_test = book_stats(loans_test, 'Test set')","31d6015c":"CoF = 3 # Cost of funds as a percentage (%)","d14ba826":"# Defining a function to return our book stats:\ndef return_stats(charge_off_amount, wgt_av_int_rate, wgt_av_term, CoF):\n    \"\"\"\n    Function to return all of the financial returns data for comparing the models\n    - charge_off_amount (array): the $ amount fully paid and charged off\n    - wgt_av_int_rate (float): the wgt average interest rate of the book\n    - wgt_av_term (float): the wgt average term of the book\n    - returns (float): the total book return on equity in %\n    \"\"\"\n\n    # Taking the values from the charged off calculations above:\n    loans_approved = charge_off_amount.sum()\n    loans_charged_off = charge_off_amount[1]\n\n    # And therefore, the RoE:\n    loans_returned = loans_approved - loans_charged_off\n    interest_earned = loans_returned * (wgt_av_int_rate) \/ 100 * (wgt_av_term \/ 12)\n    interest_paid = loans_approved * CoF \/ 100 * (wgt_av_term \/ 12)\n    banking_profit = interest_earned - interest_paid - loans_charged_off\n    total_RoE = banking_profit \/ loans_approved\n\n    print('Total loan book is: $' + str(f'{round(loans_approved \/ 1e6, 0):,.0f}') + \"m\")\n    print('Interest earned is: $' + str(f'{round(interest_earned \/ 1e6, 0):,.0f}') + \"m\")\n    print('Interest paid is: $' + str(f'{round(interest_paid \/ 1e6, 0):,.0f}') + \"m\")\n    print('Banking profit is: $' + str(f'{round(banking_profit \/ 1e6, 0):,.0f}') + \"m\")\n    print(\"Total book RoE: \" + str(round(total_RoE * 100, 2)) + \"%\")\n\n    return total_RoE","7a0ca281":"print('Calculating the return stats for the entire book:')\ntotal_book_RoE = return_stats(charge_off_amount, wgt_av_int_rate, wgt_av_term, CoF)","7e67f67c":"print('Calculating the return stats for the TRAIN book:')\ntrain_book_RoE = return_stats(charge_off_amount_train, wgt_av_int_rate_train, wgt_av_term_train, CoF)","3563d11e":"print('Calculating the return stats for the TEST book:')\ntest_book_RoE = return_stats(charge_off_amount_test, wgt_av_int_rate_test, wgt_av_term_test, CoF)","0000fe72":"# Building a function to create a heatmap\n\ndef heatmap(matrix,title,size=(8,6)):\n    \"\"\"\n    Function to create a heatmap chart of a confusion matrix.\n    - Matrix is the confusion matrix to be represented\n    - Title (string) is the title of the graph\n    - Size (tuple) is the size of the chart, set at 6x4 as default\n    - Returns a heatmap chart \n    \"\"\"\n    \n    # Setting the axis labels:\n    xticks = yticks = ['Fully Paid', 'Charged Off']\n\n    # Setting the heatmap labels:\n    annot=matrix\n    \"\"\"\n    If True, write the data value in each cell. If an array-like with the same shape as data, then use this \n    to annotate the heatmap instead of the data. Note that DataFrames will match on position, not index.\n    \"\"\"\n    \n    # Set the font options:\n    font = FontProperties(size=16)\n    font.set_style('italic')\n    \n    #Build the chart\n    plt.rcParams['figure.figsize'] = size\n    fig = sns.heatmap(matrix,annot=annot,cmap='rainbow', linewidths=1, linecolor='white',\n                      xticklabels=xticks, yticklabels=yticks, fmt=\",\", annot_kws={\"size\":14},\n                      cbar_kws={'label': '\\nNumber of applications'})\n    fig.figure.axes[-1].yaxis.label.set_size(14)\n    \n    # Setting the font size for the colorbar \n    cbar = fig.collections[0].colorbar\n    cbar.ax.tick_params(labelsize=14)\n    \n    # X-axis and tick marks labels\n    fig.set_xlabel('Predicted condition', fontproperties=font)\n    plt.xticks(fontsize=14)\n    \n    # y-axis and tick marks label\n    fig.set_ylabel('True condition', horizontalalignment='center', fontproperties=font)\n    plt.yticks(fontsize=14, verticalalignment='center')\n    \n    # Title settings \n    fig.set_title(title+\"\\n\", fontsize=20, fontweight=\"bold\");\n    \n    return fig","38636c80":"def matrix_stats(matrix):\n    \"\"\"\n    Function to tell us the precision and recall of our confusion matrix as well as what %age of loans we're approving\n    - Matrix: confusion matrix\n    - Returns: A number of stats as floats\n    \"\"\"\n\n    # Setting the true\/false positive\/negatives\n    TP = matrix[1][1]\n    FP = matrix[0][1]\n    FN = matrix[1][0]\n    TN = matrix[0][0]\n\n    # Calculating the outputs\n    accuracy = (TP + TN) \/ (TP + TN + FP + FN)\n    precision = TP \/ (TP + FP)\n    recall = TP \/ (TP + FN)\n    F1_score = 2*(precision*recall)\/(precision+recall) \n    approval_rate = (matrix[0][0] + matrix[1][0]) \/ (matrix[0].sum() + matrix[1].sum())\n\n    print(\"Accuracy: \" + str(round(accuracy, 3)))\n    print(\"Precision: \" + str(round(precision, 3)))\n    print(\"Recall: \" + str(round(recall, 3)))\n    print(\"F1 score: \" + str(round(F1_score, 3)))\n    print(\"Approval rate: \" + str(round(approval_rate * 100, 1)) + \"%\")\n    \n    return precision, recall, F1_score, approval_rate","866ff469":"# Let's define a function to give us the performance of the modelled portfolio:\ndef return_book_performance(y_pred, column, total_CoR_test, df, CoF):\n    \"\"\"\n    Function returns a number of metrics to assess the performance of the modelled portfolio\n    versus the test book\n    - y_pred (series): the model prediction series\n    - column (string): name of the column we add to the df with the prediction in y_pred\n    - total_CoR_test (df): the CoR of the test dataframe that we're comparing our model performance to\n    - df (df): the test dataframe\n    - CoF (variable): the assumed CoF\n    - Returns: interest rate, cost of risk and RoE of the book in % (floats)\n    \"\"\"\n\n    # Adding new column with the predicted charge off rate\n    df[column] = y_pred\n\n    # Create a dataframe with just the approved loans:\n    y_test_approved = df.loc[df[column].isin([0])]\n\n    # What was the performance of the approved loans? Creating a small array with that info\n    approved_performance = y_test_approved.groupby('charge_off_rate').loan_amnt.sum()\n\n    # CoR is the proportion that was charged off:\n    loans_approved = approved_performance.sum()\n    loans_charged_off = approved_performance[1]\n    CoR = loans_charged_off \/ loans_approved\n\n    # Total saving vs. LC models:\n    saving = df['loan_amnt'].sum() * (total_CoR_test - CoR)\n\n    # Calculate the interest rate for the modelled portfolio\n    wgt_av_int_rate = y_test_approved['int_rate_amnt'].sum() \/ y_test_approved['loan_amnt'].sum()\n\n    # What is the average term of the new portfolio?\n    wgt_av_term = y_test_approved['term_amnt'].sum() \/ y_test_approved['loan_amnt'].sum()\n\n    # And finally, the RoE:\n    loans_returned = loans_approved - loans_charged_off\n    interest_earned = loans_returned * wgt_av_int_rate \/ 100 * (wgt_av_term \/ 12)\n    interest_paid = loans_approved * CoF \/ 100 * (wgt_av_term \/ 12)\n    banking_profit = interest_earned - interest_paid - loans_charged_off\n    RoE = banking_profit \/ loans_approved\n\n    print('Total loans approved = $' + str(round(loans_approved \/ 1e9, 3)) + 'bn')\n    print('Total loans charged off = $' + str(round(loans_charged_off \/ 1e9, 3)) + 'bn')\n    print('Modelled cost of risk is: ' + str(round(CoR * 100, 2)) + \"%\")\n    print(\"Total saving = $\" + str(round(saving \/ 1e6, 1)) + \"m\\n\")\n    print('Weighted av. rate of the book is: ' + str(round(wgt_av_int_rate, 2)) + \"%\")\n    print('Weighted av. term of the book is: ' + str(round(wgt_av_term, 2)) + \" months\\n\")\n    print('Interest earned: $' + str(round(interest_earned \/ 1e6, 0)) + \"m\")\n    print('Interest paid: $' + str(round(interest_paid \/ 1e6, 0)) + \"m\")\n    print('Banking profit is: $' + str(round(banking_profit \/ 1e6, 0)) + \"m\")\n    print('Return on equity is: ' + str(round(RoE * 100, 2)) + \"%\")\n\n    return wgt_av_int_rate, CoR, RoE","8bb3d76f":"# Fit a logistic regression model and store the class predictions.\nlogreg = LogisticRegression(max_iter=10000, class_weight=class_weights)\n\n# Train the model\nlogreg.fit(X_train_subset, y_train_subset.values.ravel())\n\n# Create an output for our predictions\nlogreg_y_pred = (logreg.predict_proba(X_test)[:,1]>probability_threshold).astype(\"int32\")\n\n# Create our confusion matrix\nlogreg_conf_matrix = confusion_matrix(y_test, logreg_y_pred)","466b0226":"heatmap(logreg_conf_matrix,'Logistic regression performance');","78c0f9b2":"logreg_precision, logreg_recall, logreg_F1_score, logreg_approval_rate = matrix_stats(logreg_conf_matrix)","2403d266":"logreg_int_rate, logreg_CoR, logreg_RoE = return_book_performance(logreg_y_pred,'logreg_CO_pred', total_CoR_test, loans_test, CoF)","4888b91d":"neighbours = 3","fb44a607":"def KNN_model_build(neighbours, X_train_subset, y_train_subset, X_test, y_test):\n    \"\"\"\n    Function to build the KNN model \n    - neighbours (int): K, the number of near-neighbours for the model\n    - X_train_subset (df): the training set\n    - y_train_subset (df): the training dependent variable\n    - X_test (df): test set\n    - y_test: dependent variable\n    - returns: confusion matrix, the predicted variable series\n    \"\"\"\n    \n    KNN = KNeighborsClassifier(n_neighbors=neighbours)\n    \n    # Train the model using the training sets\n    KNN.fit(X_train_subset, y_train_subset.values.ravel())\n    \n    # Predict output using our model:\n    KNN_y_pred = (KNN.predict_proba(X_test)[:,1] > probability_threshold).astype('int32')\n\n    # Calculate the confusion matrix:\n    KNN_conf_matrix = confusion_matrix(y_test, KNN_y_pred)\n    \n    return KNN_conf_matrix, KNN_y_pred","e0cd4abb":"KNN_conf_matrix, KNN_y_pred = KNN_model_build(neighbours, X_train, y_train, X_test, y_test)","b7d610c0":"heatmap(KNN_conf_matrix,'KNN performance');","63c0494a":"KNN_precision, KNN_recall, KNN_F1_score, KNN_approval_rate = matrix_stats(KNN_conf_matrix)","00250e09":"KNN_int_rate, KNN_CoR, KNN_RoE = return_book_performance(KNN_y_pred,'KNN_y_pred', total_CoR_test, loans_test, CoF)","6a315a9d":"%%time\n# Number of trees to build:\ntrees = 10\n\n# Building the pipeline\npipeline_rfc = Pipeline([('imputer', SimpleImputer(copy=False)),\n                         ('model', RandomForestClassifier(class_weight=class_weights, random_state=42))])\nparam_grid_rfc = {'model__n_estimators': [trees]}  # The number of randomized trees to build\ngrid_rfc = GridSearchCV(estimator=pipeline_rfc, param_grid=param_grid_rfc, scoring='roc_auc', n_jobs=1,\n                        pre_dispatch=1, cv=5, verbose=0, return_train_score=False)\n\n# Train the model\ngrid_rfc.fit(X_train_subset, y_train_subset.values.ravel())\n\n# Predict classes for the full dataframe:\nrfc_y_pred = (grid_rfc.predict_proba(X_test)[:,1]>probability_threshold).astype('int32')\n\n# Create the confusion matrix\nrfc_conf_matrix = confusion_matrix(y_test, rfc_y_pred)","3795f8fc":"heatmap(rfc_conf_matrix,'Random forest classifier performance');","373a5b02":"rfc_precision, rfc_recall, rfc_F1_score, rfc_approval_rate = matrix_stats(rfc_conf_matrix)","80de970a":"rfc_int_rate, rfc_CoR, rfc_RoE = return_book_performance(rfc_y_pred,'rfc_y_pred', total_CoR_test, loans_test, CoF)","1f3fd0d8":"# Using the train:test split to create a validation set:\nX_train_subset, X_val, y_train_subset, y_val = train_test_split(X_train_subset, y_train_subset, test_size=0.2, random_state=42)","05e99bd6":"# We scale the feature data. To prevent data leakage from the test set, we only fit our scaler to the \n# training set\nscaler = MinMaxScaler()\n\n# Scaling the data\nX_train_subset_ann = scaler.fit_transform(X_train_subset)\nX_val_ann = scaler.transform(X_val)\n\n# So how many neurons do we need on the input layer?\nneurons = len(independent_var)\n\n# How much drop out do we want to prevent overfitting?\ndropout = 0.2\n\n# Define the model\nann_model = Sequential()\n\n# input layer\nann_model.add(Dense(neurons, activation='relu'))\nann_model.add(Dropout(dropout))\n\n# hidden layer 1\nann_model.add(Dense(neurons, activation='relu'))\nann_model.add(Dropout(dropout))\n\n# hidden layer 2\nann_model.add(Dense(neurons, activation='relu'))\nann_model.add(Dropout(dropout))\n\n# hidden layer 3\nann_model.add(Dense(neurons, activation='relu'))\nann_model.add(Dropout(dropout))\n\n# output layer\nann_model.add(Dense(1, activation='sigmoid'))\n\n# compile model\nann_model.compile(optimizer=\"adam\", loss='binary_crossentropy')\n\n# Building an early stop:\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n\ntf.keras.backend.set_floatx('float64')\nann_model.fit(x=X_train_subset_ann,\n              y=y_train_subset,\n              epochs=400,\n              verbose=2,\n              class_weight=class_weights,\n              batch_size=256,\n              validation_data=(X_val_ann, y_val),\n              callbacks=[early_stop])\n\n# Graphing the loss progression:\nlosses = pd.DataFrame(ann_model.history.history)\n\nplt.figure(figsize=(15, 5))\nsns.lineplot(data=losses, lw=3)\nplt.xlabel('Epochs')\nplt.ylabel('')\nplt.title('Training Loss per Epoch')\nsns.despine()","ae1dc71d":"# Scaling the test data:\nX_test_ann = scaler.fit_transform(X_test)\n\n# Using the ANN model to predict the outcome of the test set...\nann_y_pred = (ann_model.predict(X_test_ann) > probability_threshold).astype(\"int32\")\n\n# ...and checking the outcomes in a confusion matrix\nann_conf_matrix = confusion_matrix(y_test, ann_y_pred)","587511bd":"heatmap(ann_conf_matrix,'Neural network performance');","b2705a63":"ann_precision, ann_recall, ann_F1_score, ann_approval_rate = matrix_stats(ann_conf_matrix)","cec5688b":"ann_int_rate, ann_CoR, ann_RoE = return_book_performance(ann_y_pred,'ann_y_pred', total_CoR_test, loans_test, CoF)","dffa414f":"# Setting the key parameters:\niterations = 1000\nlearning_rate = 0.1\ndepth = 6\nboosting_type = 'Plain'\nloss_function = 'AUC'","c8a4659e":"# Using the train:test split to create a validation set:\nX_train_cb, X_val_cb, y_train, y_val = train_test_split(X_train_cb, y_train, test_size=0.2, random_state=1)","c7bb95a5":"# Setting the pools for training and testing\npool_train = Pool(X_train_cb, y_train, cat_features=cat_features)\npool_val = Pool(X_val_cb, y_val, cat_features=cat_features)\n\n# Set n to allow us to calculate the class weights - this ensures that successful applications proportionally \n# represented in all the training classes:\nn = y_train.value_counts()\n\n# Create the model\ncatboost_model = CatBoostClassifier(learning_rate=learning_rate,\n                                    iterations=iterations,\n                                    depth=depth,\n                                    early_stopping_rounds=100,\n                                    use_best_model=True,\n                                    class_weights=class_weights,\n                                    boosting_type=boosting_type,\n                                    verbose=False,\n                                    custom_loss=loss_function,\n                                    eval_metric=loss_function,\n                                    random_state=42)\n\n# Fit the model:\ncatboost_model.fit(pool_train, eval_set=pool_val, plot=True)\n\n# Make the prediction:\ncatboost_y_pred = (catboost_model.predict_proba(X_test_cb)[:,1]>probability_threshold).astype(\"int32\")\n\n# Create the confusion matrix:\ncatboost_conf_matrix = confusion_matrix(y_test, catboost_y_pred)","1fc9c934":"heatmap(catboost_conf_matrix,'Catboost model performance');","c1b49255":"catboost_precision, catboost_recall, catboost_F1_score, catboost_approval_rate = matrix_stats(catboost_conf_matrix)","a9f3db63":"catboost_int_rate, catboost_CoR, catboost_RoE = return_book_performance(catboost_y_pred,'catboost_y_pred', total_CoR_test, loans_test, CoF)","a9a42b33":"# Defining a function to add values to my bar graphs:\n\ndef add_value_labels(ax, spacing=1):\n    \"\"\"\n    Functions to add labels to the end of each bar in a bar chart.\n    - ax (matplotlib.axes.Axes): The matplotlib object containing the axes of the plot to annotate.\n    - spacing (int): The distance between the labels and the bars\n    \"\"\"\n\n    # For each bar: Place a label\n    for rect in ax.patches:\n        # Get X and Y placement of label from rect.\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        # Number of points between bar and label:\n        space = spacing\n        # Vertical alignment for positive values:\n        va = 'bottom'\n\n        # If value of bar is negative, place label below bar\n        if y_value < 0:\n            # Invert space to place label below\n            space *= -1\n            # Vertically align label at top\n            va = 'top'\n\n        # Use Y value as label and format number with 2 decimal places\n        label = \"{:.2f}\".format(y_value)\n\n        # Create annotation\n        ax.annotate(\n            label,                          # Use `label` as label\n            (x_value, y_value),             # Place label at end of the bar\n            xytext=(0, space),              # Vertically shift label by `space`\n            textcoords=\"offset points\",     # Interpret `xytext` as offset in points\n            fontsize=14,\n            ha='center',                    # Horizontally center label\n            va=va)                          # Vertically align label","dd19c14d":"# Defining a function to produce graphs of performance: \n\ndef graph_model_performance(names, values, y_axis_title, y_axis_scale, x_label='Model', percent=False, ranked=False, rotation=0, alignment='center'):\n    \"\"\"\n    Returns a bar chart of the models' performance\n    - values (list): the scores in a list\n    - names (list of strings): the x-axis labels\n    - y_axes_title (string): the y-axis title\n    - y_axis_scale (tuple): the scale of the y axis\n    - percent (bool): boolean on whether to multiply the numbers by 100 to ensure percentages can be seen\n    - ranked (bool): boolean on whether to rank the output\n    - Returns: seaborn bar chart\n    \"\"\"\n    \n    # Multiplying the numbers by 100 to scale percentages properly...\n    if percent:\n        new_values = []\n        for i in values:\n            new_values.append(i * 100)\n    # ...or leave it 'as is'\n    else:\n        new_values = values\n\n    if ranked:\n        # Create a dictionary of the names and values\n        x = dict(zip(names, new_values))\n        # Sort the diction by value\n        all_model_values = {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n        # Create our axis values in a list\n        output_names = list(all_model_values.keys())\n        output_values = list(all_model_values.values())\n    else:\n        output_names = names\n        output_values = new_values\n    \n    # Set the figure up\n    figsize = (16, 4)\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Set the font options:\n    font = FontProperties(size=14)\n    font.set_style('italic')\n    \n    # Set the title\n    fig.suptitle(y_axis_title, fontsize=18, fontweight=\"bold\")\n    \n    # Set x-axis options\n    plt.xticks(rotation=rotation, fontsize=14, horizontalalignment=alignment)\n    plt.xlabel(x_label, fontproperties=font)\n    \n    # Set the y-axid options\n    plt.yticks(fontsize=14)\n    plt.ylabel(y_axis_title, fontproperties=font)\n    ax.set_ylim(y_axis_scale)\n    \n    # Plot the chart\n    sns.barplot(x=output_names, y=output_values, saturation=1, palette='bright')\n    add_value_labels(ax, 5)","a97e4881":"names = ['Test set', 'Logistic regression', 'Random forest', 'K-nearest neighbours', 'Neural network', 'Catboost']\napproval_rate_values = [1, logreg_approval_rate, rfc_approval_rate, KNN_approval_rate, ann_approval_rate, catboost_approval_rate]\nint_rate_values = [wgt_av_int_rate_test, logreg_int_rate, rfc_int_rate, KNN_int_rate, ann_int_rate, catboost_int_rate]\nCoR_values = [total_CoR_test, logreg_CoR, rfc_CoR, KNN_CoR, ann_CoR, catboost_CoR]\nRoE_values = [test_book_RoE, logreg_RoE, rfc_RoE, KNN_RoE, ann_RoE, catboost_RoE]","163c7bed":"graph_model_performance(names, approval_rate_values, 'Approval rate (%)', (50,100), x_label='Model', percent=True, ranked=False, rotation=0, alignment='center')","cf47f076":"graph_model_performance(names, int_rate_values, 'Weighted average interest rate (%)', (10,16), x_label='Model', percent=False, ranked=False, rotation=0, alignment='center')","19d40aa5":"graph_model_performance(names, CoR_values, 'Cost of risk (%)', (10,30), x_label='Model', percent=True, ranked=False, rotation=0, alignment='center')","c6961894":"graph_model_performance(names, RoE_values, 'Return on equity (%)', (2,7), x_label='Model', percent=True, ranked=False, rotation=0, alignment='center')","bd6df7a6":"probability_threshold_sens = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\npts_x_axis = [str(int(i*100))+\"%\" for i in probability_threshold_sens]","86e0dbf9":"def probability_threshold_sensitivity(probability_threshold_sens, model, X_test, y_test, df, column):\n    \"\"\"\n    Function to sensitise the probability threshold\n    - probability_threshold_sens (list of floats): the values to flex the probability threshold\n    - model: the predictive model to use\n    - X_test (df): our test set\n    - y_test (df): dependent variable linked ot the test set\n    - df (dataframe): test dataframe \n    - column (string): name of the new column to be connected to the \n    - Returns: the predictions (df), interest rate (list), CoR (list), RoE (list), banking profit (list)\n    \"\"\"\n    \n    # Setting up the blank outputs:\n    int_rate_sens = []\n    CoR_sens = []\n    RoE_sens = []\n    banking_profit_sens = []\n    \n    # Let's cycle through the probability thresholds\n    for i in probability_threshold_sens:\n        \n        # Calculating the ANN and catboost outcomes as per revised thresholds:\n        if model == ann_model:\n            y_pred = (model.predict(X_test) > i).astype(\"int32\")\n        else:\n            y_pred = (model.predict_proba(X_test)[:,1] > i).astype(\"int32\")\n        \n        # Calculate the confusion matrices\n        conf_matrix = confusion_matrix(y_test, y_pred)\n        \n        # Creating a new column:\n        df[column] = y_pred\n\n        # Create a dataframe with just the approved loans:\n        y_test_approved = df.loc[df[column].isin([0])]\n\n        # What was the performance of the approved loans? Creating a small array with that info\n        approved_performance = y_test_approved.groupby('charge_off_rate').loan_amnt.sum()\n\n        # CoR is the proportion that was charged off:\n        loans_approved = approved_performance.sum()\n        loans_charged_off = approved_performance[1]\n        CoR = loans_charged_off \/ loans_approved\n\n        # Calculate the interest rate for the modelled portfolio\n        wgt_av_int_rate = y_test_approved['int_rate_amnt'].sum() \/ y_test_approved['loan_amnt'].sum()\n\n        # What is the average term of the new portfolio?\n        wgt_av_term = y_test_approved['term_amnt'].sum() \/ y_test_approved['loan_amnt'].sum()\n\n        # And finally, the RoE:\n        loans_returned = loans_approved - loans_charged_off\n        interest_earned = loans_returned * wgt_av_int_rate \/ 100 * (wgt_av_term \/ 12)\n        interest_paid = loans_approved * CoF \/ 100 * (wgt_av_term \/ 12)\n        banking_profit = interest_earned - interest_paid - loans_charged_off\n        RoE = banking_profit \/ loans_approved\n        \n        # Appending the sensitivities:\n        int_rate_sens.append(wgt_av_int_rate)\n        CoR_sens.append(CoR)\n        RoE_sens.append(RoE)\n        banking_profit_sens.append(banking_profit)\n        \n        banking_profit_m_sens = [i\/1e6 for i in banking_profit_sens]\n    \n    return y_pred, int_rate_sens, CoR_sens, RoE_sens, banking_profit_m_sens","74365ab9":"y_pred_cb, int_rate_sens_cb, CoR_sens_cb, RoE_sens_cb, profit_cb = probability_threshold_sensitivity(probability_threshold_sens, catboost_model, X_test_cb, y_test, loans_test, 'catboost_pred')\ny_pred_ann, int_rate_sens_ann, CoR_sens_ann, RoE_sens_ann, profit_ann = probability_threshold_sensitivity(probability_threshold_sens, ann_model, X_test_ann, y_test, loans_test, 'ann_pred')","5477c6e7":"def lineplots_builder(x_axis_labels, model_output_1, model_name_1, position_1, model_output_2, model_name_2, position_2, y_axis_title, x_axis_title, y_axis_scale, percent=False, text_format='{:.2f}'):\n    \"\"\"\n    Builds a line chart comparing the sensitivity of two models given a change in the threshold probability\n    - x_axis_labels (list): the x-axis tick labels\n    - model_output_1 (list): the variable to be compared from the first model\n    - model_name_1 (string): name of the first model\n    - position_1 (tuple, list): first element is either 'above' or 'below' and second is the distance from the marker\n    - model_output_2 (list): the variable to be compared from the second model\n    - model_name_2 (string): name of the second model\n    - position_2 (tuple, list): first element is either 'above' or 'below' and second is the distance from the marker\n    - y_axis_title (string): y-axis title\n    - x_axis_title (string): x-axis title\n    - y_axis_scale (tuple): low\/high scale of the y-axis\n    - percent (bool): whether the y variables are %ages or not\n    - text_format: format for the chart labels\n    - Returns: a line plot of the two models' performance\n    \"\"\"\n    \n    # Multiplying numbers by 100 if percentages\n    if percent:\n        model_output_1 = [i*100 for i in model_output_1]\n        model_output_2 = [i*100 for i in model_output_2]\n    \n    # Setting label positions\n    if position_1[0] == 'above':\n        label_offset_1 = position_1[1]\n    if position_1[0] == 'below':\n        label_offset_1 = -position_1[1]\n    if position_2[0] == 'above':\n        label_offset_2 = position_2[1]\n    if position_2[0] == 'below':\n        label_offset_2 = -position_2[1]\n    \n    # Setting up the dataframe for the modelling\n    df = pd.DataFrame(list(zip(x_axis_labels, model_output_1, model_output_2)),\n                      columns = ['Probability threshold', model_name_1, model_name_2]\n                     ).set_index('Probability threshold')\n    \n    # Setting our palette\n    pal = sns.color_palette('bright', n_colors=2)\n    \n    # Set the figure up\n    figsize = (16, 6)\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Set the font options:\n    font = FontProperties(size=14)\n    font.set_style('italic')\n    \n    # Set the title\n    fig.suptitle(y_axis_title, fontsize=18, fontweight=\"bold\")\n    \n    # Set x-axis options\n    plt.xticks(fontsize=14)\n    plt.xlabel(x_axis_title, fontproperties=font)\n    \n    # Set the y-axis options\n    plt.yticks(fontsize=14)\n    plt.ylabel(y_axis_title, fontproperties=font)\n    ax.set_ylim(y_axis_scale)\n\n    # label points on the plot for first line\n    for x, y in zip(df.index, df[model_name_1]):\n        # the position of the data label relative to the data point can be adjusted by adding\/subtracting a value from the x &\/ y coordinates\n        plt.text(x = x,  # x-coordinate position of data label\n        y = y + label_offset_1,  # y-coordinate position of data label, adjusted to be above the data point\n        s = text_format.format(y),  # data label, formatted to ignore decimals\n        color = pal[0],  # set colour of line\n        fontsize=14,  # Set fontsize\n        horizontalalignment='center') \n    \n    # label points on the plot for first line\n    for x, y in zip(df.index, df[model_name_2]):\n        # the position of the data label relative to the data point can be adjusted by adding\/subtracting a value from the x &\/ y coordinates\n        plt.text(x = x, # x-coordinate position of data label\n        y = y + label_offset_2, # y-coordinate position of data label, adjusted to be above the data point\n        s = text_format.format(y), # data label, formatted to ignore decimals\n        color = pal[1], # set colour of line\n        fontsize=14,\n        horizontalalignment='center') \n    \n    # Show the chart\n    sns.lineplot(data=df, markers=['o','o'], dashes=False, lw=5, markersize=15)\n\n    # Legend sizing\n    plt.legend(fontsize=14, loc='upper left')","9f814086":"lineplots_builder(pts_x_axis, int_rate_sens_cb, 'Catboost', ('below', 0.9), \n                  int_rate_sens_ann,'Neural network', ('above', 0.6), \n                  'Interest rate (%)', 'Threshold probability', (5,15));","2209c0a1":"lineplots_builder(pts_x_axis, CoR_sens_cb, 'Catboost', ['below', 2],\n                  CoR_sens_ann, 'Neural network', ['above', 1],\n                  'Cost of risk (%)', 'Threshold probability', (0,25), percent=True);","47d4e58a":"lineplots_builder(pts_x_axis, RoE_sens_cb, 'Catboost', ('above', 0.35),\n                  RoE_sens_ann,'Neural network', ('below', 0.5),\n                  'Return on equity (%)', 'Threshold probability', (3,10), percent=True);","34f22cbb":"lineplots_builder(pts_x_axis, profit_cb, 'Catboost', ('above', 20),\n                  profit_ann,'Neural network', ('below', 30),\n                  'Banking profit ($m)', 'Threshold probability', (0,350), text_format='{:.0f}');","248c4371":"# Calculating the shap values for each cell\npool1 = Pool(data=X_test_cb, label=y_test, cat_features=cat_features)\nshap_info = catboost_model.get_feature_importance(data=pool1, type='ShapValues', verbose=0)\nshap_values = shap_info[:, : -1]\nbase_values = shap_info[:, -1]","102012b0":"# Visualisation showing all the factors that drive the Catboost model decisioning:\nshap.summary_plot(shap_values, X_test_cb)","76d7eac2":"# What are the model's most important features? \nimportances = catboost_model.get_feature_importance(prettified=True)\ngraph_model_performance(importances['Feature Id'][0:11], importances['Importances'][0:11], 'Importance score', [0,30], ranked=True, rotation=30, alignment='right')","6a6902a4":"# Switch for checking individual applications:\napp_num = 3","734333b5":"# Printing the probabilities of the loan application:\ntest_objects = [X_test_cb.iloc[app_num:app_num + 1, :]]\nfor obj in test_objects:\n    print('Formula raw prediction = {:.2f}'.format(catboost_model.predict(obj, prediction_type='RawFormulaVal')[0]))\n    print('Probability of loan charging off = {:.2f}'.format(catboost_model.predict_proba(obj)[0][1] * 100) + \"%\")","0d814c0d":"# The shap visualisation tool also shows how which variables affected the model's decision for \n# the application in question:\nshap.initjs()\nshap.force_plot(base_value=base_values[0], shap_values=shap_values[app_num], features=X_test_cb.iloc[app_num])","f3c46da2":"#shap_explainer = shap.TreeExplainer(catboost_model)\n#shap_values_all = shap_explainer.shap_values(X_test_cb)\n#shap.force_plot(shap_explainer.expected_value, shap_values_all, X_test_cb)","02283988":"# 8. Model performance summary\nI have built a number of credit models to correctly identify loans that will \"charge off\" after drawdown. I will therefore compare the cost of risk (CoR) of each of the books of loans that the models would have approved. However, as discussed above, the true test of performance is the overall return on equity (RoE) which balances minimising cost of risk (as its first priority) with maximising the interest earned. ","ca148e4d":"## 2.4 Refining our dependent variable \n\nI am interested in predicting which loans are likely to be fully repaid versus those which are 'charged off', i.e. the lender assumes that the loan will not be repaid. I need to assess which loans in the dataset fall into the above two categories.","20e94c6c":"## 7.2. K-Nearest neighbours","3e3fb640":"# 1. Problem statement\n**Can we build a dynamic pricing model for personal loans while minimising the amount of information we require customers to present during the loan application?** \n\nLendingClub was an American peer-to-peer lending company, headquartered in San Francisco, California, offering unsecured personal loans between USD1,000 and USD40,000. Investors were able to view the loan book on LendingClub website and complete their own analysis to determine the quality of the book based on the information supplied about the borrower, amount of loan, loan grade, and loan purpose. \n\nI plan to build a credit model which will identify which loans are likely to charge off, with the aim of reducing the cost of risk (CoR) of the resulting portfolio. However, I will also be keeping a close eye on the return on equity (RoE) of the book, which seeks to maximise the balance between risk (charge off loans) and reward (interest earned). I will also analyse the link between risk appetite and profitability by flexing the probablity of charge off at which loans are accepted\/rejected. \n\nIn addition, I will explore which models represent the best option for lenders who require very clear decision making processes when lending, i.e. any model must be extremely transparent in its function. ","7fe7ce6e":"There are also a number of features which are not applicable, or are expected to be outputs of our modelling. For example, `expDefaultRate` and `intRate` will be outputs of our modelling. However, keeping both elements will be useful sense checks later in the process. \n\nSo, dropping the unknown features from our data set leaves us with fewer columns with which to analyse the creditworthiness of the applicant. ","0e59e791":"## 7.3. Random forest classifier\nNext I will try a random forest classifier. Obviously, every tree I add to our forest will increase precision but likely increase recall and lead to overfitting as well as additional training time. I've used 25 trees as a \"starter for 10\". ","607bb217":"# Building an AI-driven credit model - Lending Club loan data","71404dbe":"Charge off rate increases as the applicant's grade falls, meaning that Lending Club's underwriting models are clearly a good predictor of creditworthiness!","bd4f81b3":"## 7.1. Logistic regression \nLet's start by fitting a logistic regression and returning the confusion matrix:","adde1e16":"![Picture](https:\/\/i.imgur.com\/oa6OuU8.png)","919a97f7":"## 5.4. Probability threshold\nAs I set out in section 6, 2\/3rds of the costs of running the book of loans come from loans that have charged off. As a result, to maximise RoE, we should prioritise minimising CoR or, in modelling parlance, I will prioritise minimising false negatives over false positives.\n\nTo do this, we can flex our probability threshold for classifying which loans are predicted to charge off, so I have included a variable for this purpose below. ","aefb6fa7":"## 2.2 Reducing the number of columns\n\nI need to reduce the number of items that we will analyse by checking:\n\n1. which columns are empty (or largely empty); and\n2. which columns will a prospective lender not have at the time the loan is written.\n\nLet's have a look how many of the columns are empty or largely empty. To do this I will sort the columns by the percentage of data that is missing from each of them.","c8bec3d1":"## 3.2. Dependent\/Lending Club variables\nWe cannot use certain of the columns for prediction as they are either a: \n1. dependent variable of our final model, e.g. the interest rate; or\n2. metric created by Lending Club that would not be available to a 3rd party lender. \n\nHowever, these metrics are useful for sense-checking our model once it is built, so it is therefore useful to analyse these variables for reference later. I will review a few of these metrics below to aid in my understanding of the underlying loan portfolio.","ca4cc874":"Unsurprisingly, loan amount is a key driver of charge off rate. ","21b75ec6":"So I will now drop all 58 columns which are missing a significant proportion (i.e. >30%) of their data.","a0acbea9":"## 7.4. Artificial neural network\nTo ensure that we construct a robust neural network model, we need to complete two actions:\n1. split our training data into a train:validate 80:20 split; and\n2. scale the feature data. Given the use of small weights in the model and the use of error between predictions and expected values, the scale of inputs and outputs used to train the model are an important factor. Unscaled input variables can result in a slow or unstable learning process.","9191fb87":"<img src=\"https:\/\/i.imgur.com\/DNnB5iw.png\" width=\"500px\">","cff5cb57":"### 3.2.1. Interest rate","4f6e1abf":"# 6. Properties of the loan book\nBelow beginning modelling we will calculate some of the key properties of the loan book.","f921fae4":"In short, the visualisation above highlights that the key features driving approval of the loan: term, number of accounts opened in the last 12m, loan amount, and annual income are the most important factors.  \n\nA simplified visualisation below shows the rank of the individual importances and reinforces the result above. ","f3037dc4":"### 3.3.2. Term\nFirst I need to convert the term column into integers, rather than string.\n","7b7faca2":"####\u00a0Reviewing individual decisions\nThe Catboost algorithm also allows us to check the drivers of each individual decision. We will therefore include a switch to check each individual application (see below).","b645534f":"For each of these metrics, it is appropriate to replace NaN with zero, as the missing data most likely indicate that the applicant does not, for example, have any accounts in delinquency, and\/or it is more conservative to assume that they have very low (i.e. zero) current balances. We should also:\n\n1. Remove outliers from DTI, open accounts, revolver utilisation, months since last revolver opened, and months since recent account opened. \n2. Swapping the charge off and fully paid strings for 1\/0 for incoporation into the model.\n3. Create two columns which are the product of amount and: i) term; and ii) interest rate, for calculations later. ","29a5e76d":"There are a large number of columns with almost no data, and also many with complete data. Let's build a visualisation to better understand this dynamic.","29631356":"## 6.1. Term, interest rate, and cost of risk\nIt would be useful to understand how the training and testing set differs in its key metrics, e.g. term, interest rate, and CoR.","634f68c9":"# 10. Analysing Catboost's performance","46b60737":"## 5.2. Class weights\nAs the fully paid loans greatly outnumber the charged off loans, our dataset is imbalanced. To ensure that our models do not overfit to fully paid loans, we define the class weights and use the in-built functionality in the ML models to correct for this imbalance. ","1f6c332b":"### 3.2.2. Investment grade and sub-grade","187cc76c":"# 2. Cleaning the data\nLet's load the Lending Club loans data.","38f6f174":"**Almost USD4.2bn has been lost to customers not repaying their loans!**\n\nI should also calculate the average and weighted average interest rate of the book so that we can compare this to the performance of our modelled books later.","30909c41":"## 2.3 Removing information that prospective lenders would not know\n\nI have reviewed the list of remaining columns and built a list of features that prospective lenders would not have access to when deciding whether to approve the loan application, or those that are not relevent to this project. \n\nThis information can be viewed here: https:\/\/resources.lendingclub.com\/LCDataDictionary.xlsx\n\nIn short, there are four main reasons not to use variables as model features, the metric in question:\n\n1. will not be available at the time the loan is applied for, e.g. change in FICO score; \n2. is an assessment of creditworthiness from Lending Club or another body, e.g. grade, sub-grade, FICO score, etc.;\n3. is irrelevant for assessing the loan application, e.g. loan ID, disbursement method; and \n4. has a definition which is unclear\/ambiguous, and thus I cannot rely on it as an indicator.\n\nI have gone through all the 93 metrics and excluded those listed below, for the reasons stated:","ccc7ee53":"## 5.1. Defining the test data\nBefore that, I need to take the loans from 2017 onwards and put them 'in the fridge' for testing later.","dd73539f":"Above we can see that the factors driving the rejection of this loan are primarily the fact that the applicant has opened 5 credit accounts in the last 12 months, they borrowed USD 30,000 and have an annual income of USD 57,000, meaning the loan is nearly 28x their monthly income. Mitigating these factors are the fact that they only wish to borrow over 3 years, and have been approved for a significant outstanding debt of over USD 62,000.","e1a005d1":"### 3.3.1. Loan amount","efbe328a":"# 5. Undersampling the fully paid loans to prevent oversampling later\nOne significant risk of having a single class dominating is that our models are likely to overfit to the fully paid loans and grossly under-represent the charged off loans. That will lead to them always approving loan applications, rather than rejecting those which are more risky. \n\nTo train the models on a modified subset of the data, I need to take the following steps: \n\n1. **Train test split**:  Split the loans dataframe into so that loans in 2017 and 2018 will be our test set. The training data (2007-2016 inclusive) will be undersampled (see step 2).\n2. **Undersample the training data:** Remove (at random) enough of the fully paid loans so that there is a 50:50 split of Fully Paid:Charged Off loans in the 2007-16 training set. This will be called the train_subset.\n3. **Train the models:** train all the models on the undersampled 2007-17 training subset.\n4. **Test the models:** on the 2017-18 test dataset.","9d9ab922":"## 3.3. Independent variables\nThe remaining columns are those that we should seek to investigate to understand which will be important indicators of creditworthiness.","241184ac":"# 3. Initial analysis of the data\nI now need to cycle through all of the outstanding metrics and assess which of these are worthy of further investigation, i.e. identifying those that could be used as independent variables in our model. \n## 3.1. Function to present initial findings for each variables\nFirst, I will build a function that visualises each of the variables against the charge off rate. ","a1afee29":"Or as a percentage:","9bd67bdf":"## 5.3. Undersampling the data\nAn alternative to weighting our models by class, would be to build a function to remove, at random, a number of the fully paid loans from the test data to deliver a target charge off rate in the test set. This will prevent our models overfitting to the fully paid loans. \n\n**This function is currently switched off in this version of the sheet, but could be turned on should my models overfit to Fully Paid loans.**","c7c7bd72":"There are 2.2m loans and 151 variables to choose from. ","7d90c77b":"We also need to define an arbitrary cost of funds which defines the profitability of the book:","14b1597d":"# 4. Defining model features\n\nThere are three columns with categorical variables that I would like to include as features in each of our models. I will need to convert these via one-hot encoding for use later (except in the case of Catboost where the model takes care of the OHE).","b25bb65d":"So how many columns will we drop if we remove these? ","07da86ad":"## 6.2. Returns performance of the book\nNow we need to understand the returns performance of the book so we have a comparison against which to test our models.","68a39e06":"4\/5ths of the loans have been paid off and 1\/5th have been charged off. This is a significant proportion of the accounts, representing over 268K loans. How much money (in billions of dollars) does that represent? And what does this represent as a perentage, AKA life time cost of risk (CoR)?","a820474f":"# 11. Conclusions","2d5dab0d":"This reduces the number of loans in the dataset to 1.3m (down from 2.2m). \n\nNext I need to check how many of the loans fall into each category:","52c9130f":"## 7.5. Catboost\nCatboost can handle both categorical variables and unbalanced datasets, so I have used different model features (i.e. without one hot encoding) and the full dataset but with clearly defined class weights.\n\nGiven the size of the dataset, I have picked hyperparameters that prioritise speed of training, so there is additional performance available for those that wish to tuned further. ","a6682904":"<img src=\"https:\/\/i.imgur.com\/ptcsu8D.png\" width=\"1000px\">","00dc7b55":"There are a large number of current loans which are not relevant to our analysis, so I need to remove these from the dataset. ","d52e5b01":"As we can see from the above, by sensitising the threshold of probability of default (i.e. the predicted probability at which we decline loan applications) we can dramatically drop the cost of risk. However, at the margin this partly comes at the expense of return on equity as the higher interest rate loans are rejected. \n\nThe biggest impact, however, comes on the absolute profitability of the book, with a more cautious approach to underwriting resulting in a smaller profit pool in dollar terms. \n\nInterestingly, Catboost outperforms the neural network when adopting a more cautious underwriting strategy, while the neural network outperforms Catboost when the risk appetite is increased. ","2603c8bb":"I will build a short function to output the raw formula output from the model and the probability of success derived from the model that the formula output corresponds to.","e24aec08":"Unsurprisingly, the charge off rate increases based on the interest rate offered to the borrower, as the interest rate is driven by Lending Club's assessment of applicant risk (i.e. their underwriting models). Interestingly, thre are a number of fully paid loans with extremely high interest rates (25%+) reflecting the level of risk within the book.","7bb46538":"Catboost and the neural network were the standout performers in minimising the cost of risk of the test portfolio. However, the neural network slightly outperformed on a RoE basis, as the Catboost model's most important feature (by a factor of >2x) was term, which significantly reduced the weighted average term of the Catboost portfolio, and hence overall profitability. \n\nHowever, Catboost has two important advantages over the neural network:\n1. It outperforms on a CoR and RoE basis when the probability threshold for rejecting loan applications is lower, i.e. when the risk appetite is lower; and \n2. Its decisions are completely transparent, which is of critical importance in a regulated industry like banking. \n\n","eea343f9":"One of the benefits of the Catboost algorithm is the transparency of the loan decisions. Below are set out the drivers of the Catboost model in its decisions.","2e0b8e34":"## 3.4. Final data cleaning\nNow that we have our variables to analyse, we need to clean the data to remove NaN metrics to ensure that we can completed a range of modelling tasks to predict creditworthiness. First we need to calculate the propotion of data missing for each variable:","e7e7bdfe":"# 7. Modelling\nI will now trial a number of different models that are useful classifiers, namely:\n1. Logistic regression\n2. K-nearest neighbours\n3. Random forrest\n4. Artficial neural network\n5. Catboost\n\nBefore doing so, let's define a few simple functions to draw our heatmaps and output the book performance.","b3677203":"Term is also a key driver of risk. This is due to a longer term leaving a longer period for the borrower to fall into financial difficulty, and also indicative of stretch affordability, i.e. a longer term has lower installments, meaning that applicants borrowing a larger proportion of their incomes are more likely to need a longer term. \n\nIt will be useful to have the average and the weighted average term of the book, so I have calculated them below:","3b14497b":"So there is a clear dicotomy between columns which have nearly all the data (<20% missing) and those that have a large proportion of the data missing. As it is difficult to infer meaning from columns with significant missing data (especially when utilising train\/test splits) we should drop the columns that have over 30% missing data. \n\nBuilding a list of the columns with missing data:","cfd9b2d7":"# 9. Varying the probability threshold\nNow that our models are trained we can vary the sensitivity threshold to optimise for RoE. Setting out a process for this below:","9c3763d0":"I can now define my model features as follows:"}}