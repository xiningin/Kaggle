{"cell_type":{"676876c8":"code","14f0c699":"code","64efe9d4":"code","d47727e2":"code","ffa00ea5":"code","e8d0ad31":"code","9ddf7b82":"code","e844a319":"code","52b05fe8":"code","f37fa633":"code","3dcbb35e":"code","dea1ab05":"code","b79dea1f":"code","93a6d042":"markdown","9469b770":"markdown","a4ab25cd":"markdown","0fb4c443":"markdown","f191e7a9":"markdown","dffa1488":"markdown","4519feaa":"markdown","ec3e67b2":"markdown","4b9821db":"markdown"},"source":{"676876c8":"import operator as op\nimport random\nrandom.seed(123)\n\nimport numpy as np \nimport pandas as pd \n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as skm\n\n%matplotlib inline\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12,8)})\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","14f0c699":"# Loading the data\ndf = pd.read_csv('\/kaggle\/input\/others\/MBA_ADMISSIONS.csv')\ndf.head()","64efe9d4":"# Encoding \ncat_vars = ['Gender','STATE', 'Previous_Degree','Marital_status',\n            'Place_you_belong_to', 'perceived#Job#Skill', 'Specialization']\nfor i in cat_vars:\n    df[i+\"_cat\"] = df[i].astype('category').cat.codes\ndf.head()","d47727e2":"# Masking to show only one side of the matrix\ncorr = np.corrcoef(df.corr())                        \nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n# Axtual Correlation matrix as a heatmap\nsns.heatmap(df.corr(), annot=True, mask=mask)","ffa00ea5":"#Distribution of the target variable\nsns.histplot(data=df, x='Age_in_years', hue='Gender')","e8d0ad31":"# Distribution of all independent variables by Gender\nfig = plt.figure()\ngs = fig.add_gridspec(2, 3, hspace=0.4, wspace=0.3)\n(ax1, ax2, ax3), (ax4, ax5, ax6) = gs.subplots(sharex=False, sharey=False)\nfig.suptitle('Histograms for School Performance')\n\nsns.histplot(ax= ax1, data=df, x ='pre_score', hue='Gender', \n             bins=10, palette=[\"green\", \"red\"], stat='count')\nax1.set_title(\"Pre-Score\")\n\nsns.histplot(ax= ax2, data=df, x ='post_score', hue='Gender',  \n             bins=10, palette=[\"green\", \"red\"], stat='count')\nax2.set_title(\"Post-Score\")\n\nsns.histplot(ax= ax3, data=df, x ='Percentage_in_10_Class', hue='Gender',  \n             bins=10, palette=[\"green\", \"red\"], stat='count')\nax3.set_title(\"% in 10 classes\")\n\nsns.histplot(ax= ax4, data=df, x ='Percentage_in_12_Class', hue='Gender', \n             bins=10, palette=[\"green\", \"red\"], stat='count')\nax4.set_title(\"% in 12 classes\")\n\nsns.histplot(ax= ax5, data=df, x ='Percentage_in_Under_Graduate', hue='Gender',  \n             bins=10, palette=[\"green\", \"red\"], stat='count')\nax5.set_title(\"% in UndergGrad\")\n\nsns.histplot(ax= ax6, data=df, x ='percentage_MBA', hue='Gender',  \n             bins=10, palette=[\"green\", \"red\"], stat='count')\nax6.set_title(\"% in MBA\")\n\n\nplt.show()","9ddf7b82":"# Scatterplots between % in Undergrad in key performance metrics by Genders\nfig = plt.figure()\ngs = fig.add_gridspec(3, 1, hspace=0.1, wspace=0.4)\n(ax1), (ax2), (ax3) = gs.subplots(sharex=True, sharey=False)\nfig.suptitle(\"Scatterplots with '% in Undergrad' as predictor and Gender as Category\")\nsns.scatterplot(ax=ax1, data=df, x='Percentage_in_Under_Graduate', y='pre_score', hue='Gender')\n\nsns.scatterplot(ax=ax2, data=df, x='Percentage_in_Under_Graduate', y='post_score', hue='Gender')\n\nsns.scatterplot(ax=ax3, data=df, x='Percentage_in_Under_Graduate', y='percentage_MBA', hue='Gender')\n\nplt.show()","e844a319":"# Barplots by Ages\nfig = plt.figure()\ngs = fig.add_gridspec(3, 1, hspace=0.05, wspace=0.4)\n(ax1), (ax2), (ax3) = gs.subplots(sharex=True, sharey=False)\nfig.suptitle(\"Scatterplots with '% in Undergrad' as predictor and Gender as Category\")\nsns.barplot(ax=ax1, data=df, x='Age_in_years', y='pre_score', hue='Gender')\n\nsns.barplot(ax=ax2, data=df, x='Age_in_years', y='post_score', hue='Gender')\n\nsns.barplot(ax=ax3, data=df, x='Age_in_years', y='percentage_MBA', hue='Gender')\n\nplt.show()","52b05fe8":"# Barplots by Ages\nfig = plt.figure()\ngs = fig.add_gridspec(1, 2, hspace=0.4, wspace=0.1)\n(ax1, ax2) = gs.subplots(sharex=False, sharey=True)\n\nsns.violinplot(ax=ax1, data=df, x='Marital_status', y='post_score', \n               order=['Single', 'Married'], hue='Gender', split=True)\nax1.set_title(\"Post_score by Marital Status\")\n\nsns.violinplot(ax=ax2, data=df, x='Place_you_belong_to',y='post_score', \n               order=['Urban', 'Semi Urban', 'Rural'], hue='Gender', split=True)\nax2.set_title(\"Post_score by Location\")\nfor ax in fig.get_axes():\n    ax.label_outer()\nplt.show()","f37fa633":"sns.boxplot(data=df, x=\"post_score\", y=\"Previous_Degree\", hue='Gender')\nplt.show()","3dcbb35e":"# Models for all categorical variables\nclass clf_models():\n    def __init__(self, iters=None):\n        self.iters = iters\n        \n    def fit(self, X_train, Y_train, X_test, Y_test):\n        self.X_train = X_train\n        self.Y_train = Y_train\n        self.X_test = X_test\n        self.Y_test = Y_test\n\n    def predict_knn(self):\n    \n        # Using Kbest to build kNN\n        kbest = self.elbow_method(self.iters)\n        model_knn = KNeighborsClassifier(n_neighbors=kbest)\n        model_knn.fit(self.X_train, self.Y_train)\n        pred_knn = model_knn.predict(self.X_test)\n        \n        return pred_knn\n    \n    def predict_nb(self):\n        # Naive Bayes\n        model_nb = GaussianNB()\n        model_nb.fit(self.X_train, self.Y_train)\n        pred_nb = model_nb.predict(self.X_test)\n    \n        return pred_nb\n    \n    def accuracy(self, Y_pred):\n        return skm.accuracy_score(self.Y_test, Y_pred)\n    \n    def elbow_method(self, iters):\n        # Elbow Method to find k-best\n        # May rewrite it using a while loop and a margin\n        errors = {}    \n        for i in range(1, iters):\n            model_i = KNeighborsClassifier(n_neighbors=i)\n            model_i.fit(self.X_train, self.Y_train)\n            pred_i = model_i.predict(self.X_test)\n            errors[i] = np.mean(pred_i != self.Y_test)\n        return min(errors.items(), key=op.itemgetter(1))[0]\n    ","dea1ab05":"# Visualizing for the binary categories\ndef visualize(ax, var, Y_test, pred_knn, pred_NB):\n    # Benchmark for prediction of only 1s (only men)\n    bm_pred = [0 for _ in range(len(Y_test))]\n    bm_auc  = roc_auc_score(Y_test, bm_pred)\n    bm_fpr, bm_tpr, _ = roc_curve(Y_test, bm_pred)\n\n    knn_auc = roc_auc_score(Y_test, pred_knn)\n    NB_auc = roc_auc_score(Y_test, pred_NB)\n    knn_fpr, knn_tpr, _ = roc_curve(Y_test, pred_knn)\n    NB_fpr, NB_tpr, _ = roc_curve(Y_test, pred_NB)\n    \n    ax.plot(bm_fpr, bm_tpr, linestyle='--', label='Benchmark')\n    ax.plot(NB_fpr, NB_tpr, marker='.', label='Naive Bayes')\n    ax.plot(knn_fpr, knn_tpr, marker='.', label='kNN')\n    text = ': BM =%.2f, K-NN =%.2f, Naive Bayes =%.2f'\n    ax.set_title(var+text % (bm_auc,knn_auc, NB_auc))\n    ax.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n    for ax in fig.get_axes():\n        ax.label_outer()","b79dea1f":"if __name__ == \"__main__\":\n    \n    targets = ['Gender_cat', 'Marital_status_cat']\n    scores = {}\n    \n    vars, Y_tests, preds_knn, preds_nb = [], [], [], []\n    \n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 2, hspace=0.5, wspace=0.3)\n    (ax1, ax2) = gs.subplots(sharex='all', sharey='all')\n    \n    for t in targets:\n        X = df.drop(cat_vars+[t], axis=1).values\n        Y = df[t].values\n\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                                            test_size = 0.2, \n                                                            random_state = 0)\n\n        model_knn = clf_models(iters=10)\n        model_knn.fit(X_train, Y_train, X_test, Y_test)\n        pred_knn = model_knn.predict_knn()\n        knn_acc = model_knn.accuracy(pred_knn)\n        \n        model_nb = clf_models(iters=None)\n        model_nb.fit(X_train, Y_train, X_test, Y_test)\n        pred_nb = model_nb.predict_nb()\n        nb_acc = model_nb.accuracy(pred_nb)\n        scores[t[:-4]] = [round(knn_acc, 4), round(nb_acc, 4)]\n        \n        vars.append(t[:-4])\n        Y_tests.append(Y_test)\n        preds_knn.append(pred_knn)\n        preds_nb.append(pred_nb) \n        \n    \n    ax1 = visualize(ax1, vars[0], Y_tests[0], preds_knn[0], preds_nb[0])\n    ax2 = visualize(ax2, vars[1], Y_tests[1], preds_knn[1], preds_nb[1])\n    for ax in fig.get_axes():\n        ax.label_outer()\n    \n    plt.legend()\n    plt.show()\n\n    scores_df = pd.DataFrame.from_dict(scores, orient='index', \n                                  columns = ['accuracy_knn', 'accuracy_nb'])\n    print(scores_df)","93a6d042":"## 3. How do men and women differ in their MBA performance?","9469b770":"## 6. How does MBA performance differ by marital status and location?","a4ab25cd":"# Exploratory Data Analysis","0fb4c443":"## 5. Does maturity, denoted by age, impact performance in MBA?","f191e7a9":"## 1. What's the correlation between all of our variables?","dffa1488":"# Model Building & Evaluation","4519feaa":"## 7. Does a more technical undergrad background ensure MBA success?","ec3e67b2":"## 2. What are the demographics of this  MBA class?","4b9821db":"## 4. Does undergrad grades predict future performance in MBA?"}}