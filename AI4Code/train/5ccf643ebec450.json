{"cell_type":{"f1052534":"code","3ef4495e":"code","662d18a5":"code","465b525d":"code","d88fb826":"code","1c835b99":"code","d7322c6f":"code","4fceba64":"code","f3f11be0":"code","fe5b7101":"code","c6b4f6b2":"code","e0fdfa2c":"code","10168fb0":"code","ca0b6d6b":"code","f3831d7a":"code","24b96a47":"code","2bfd051a":"code","7d4b987d":"code","d0bdfae1":"code","dfc243f0":"code","37bd9611":"code","719ec87c":"code","379f6d38":"code","bf6377b5":"code","c382e0f5":"code","6f257cef":"code","4e5ddf87":"code","07cf1042":"code","c52135dd":"code","eb97d0d9":"code","a22bb1e6":"code","eed52e5d":"code","d218d438":"code","0bff8ffd":"code","4274715e":"code","0709df4f":"code","8e6a0212":"code","0a831e71":"code","9fd57edc":"code","d99a132c":"code","cef60312":"code","34ec9722":"code","2d3c3bcc":"code","0ec49a3b":"code","ba2de7d6":"code","865d0589":"code","66aed66b":"code","a4b9f265":"code","793c50ba":"code","a53fcdef":"code","f3f50a87":"code","dd1994d8":"code","c5161709":"code","fab2ea9a":"markdown","bef4c3fe":"markdown","b0cc6d2b":"markdown","cf9c7f59":"markdown","0ddcb67a":"markdown","5de22399":"markdown","13317a12":"markdown","ee1e321c":"markdown","77d1bdce":"markdown","77a5d28d":"markdown"},"source":{"f1052534":"#!pip install xgboost\n#!pip install lightgbm\n#!pip install catboost","3ef4495e":"#close warnings\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n#import libraries for linear models\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#import libraries for non-linear models (additional to linear models)\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","662d18a5":"#read dataset\ndf_hitters=pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")","465b525d":"#copy dataset in case of reloading dataset immediately\ndf=df_hitters.copy()","d88fb826":"df.head()","1c835b99":"df.shape","d7322c6f":"df.info()","4fceba64":"#get a summary of descriptive statistics\ndf.describe([0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).T","f3f11be0":"#check number of missing values\ndf.isnull().sum().sort_values(ascending=False).head()","fe5b7101":"#get columns names according to variable types >> int64, float64, object\n#it helps us to use fancy indexes, manipulate different types separately and relatively reduce memory usage instead of creating two dataframes\ncat_cols=[col for col in df.columns if df[col].dtype=='object']\nnum_cols=[col for col in df.columns if col not in cat_cols]","c6b4f6b2":"#as a summary, there are 20 variables and 'Salary' is target variable. \n#rest 19 variables (16 are numerical, 3 are categorical\/object) are independent variables.\n#there are 59 null values only in target variable. ","e0fdfa2c":"#detect outliers for target variable\nsns.boxplot(x=df['Salary'])","10168fb0":"#before filling missing values in salary, trim\/correct them slightly\nupper_limit=df['Salary'].quantile(0.95)\noutliers_upper=df[df[\"Salary\"] > upper_limit]\ndf.loc[df[\"Salary\"] > upper_limit, \"Salary\"] = upper_limit","ca0b6d6b":"#check outliers again\nsns.boxplot(x=df['Salary'])","f3831d7a":"#make a decision about handling NaN values: 1. Drop them 2. Fill them.\n#there are 59 missing values in 322 rows. 18% is not a small ratio and can affect results. Filling seems a better way.\n#if dropping is being preferred, code: df.dropna(inplace=True)","24b96a47":"#fill missing values according to categorical variables and mean\ndf['Salary']=df.groupby(['League','Division'])['Salary'].transform(lambda x: x.fillna(x.mean()))","2bfd051a":"#no missing values:)\ndf.isnull().sum().sort_values(ascending=False).head()","7d4b987d":"#check categorical variables and number of subcategories\nprint(df['League'].value_counts())\nprint(df['Division'].value_counts())\nprint(df['NewLeague'].value_counts())","d0bdfae1":"#all categorical variables consist of two subcategories. Then, use Label Encoding (LE).\n#LE assigns values as 0-1 (means to model: coequal variables)\nle_League=LabelEncoder()\nle_Division=LabelEncoder()\nle_NewLeague=LabelEncoder()","dfc243f0":"df['League']=le_League.fit_transform(df['League'])\ndf['Division']=le_Division.fit_transform(df['Division'])\ndf['NewLeague']=le_NewLeague.fit_transform(df['NewLeague'])","37bd9611":"#to get original categorical values below inverse code can be used\n#le_League.inverse_transform(df['League'])\n#le_Division.inverse_transform(df['League'])\n#le_NewLeague.inverse_transform(df['League'])","719ec87c":"df.head()","379f6d38":"#Feature Scaling: Normalization of Numerical Variables (Except from target variable)\n#First remove Salary variable from numerical columns - you don't want to normalize it\nnum_cols.remove('Salary')\nnorm_num_df=preprocessing.normalize(df[num_cols])\nnorm_num_df=pd.DataFrame(norm_num_df, columns=num_cols)\nnorm_num_df.head()","bf6377b5":"#change dataframe with normalized variables\ndf=pd.concat([norm_num_df, df[cat_cols], df['Salary']], axis=1)\ndf.head()","c382e0f5":"#get dependent and independent values\ny=df[['Salary']] #dependent\/target variable\nX=df.drop(['Salary'], axis=1)  #independent variable","6f257cef":"#divide dataset to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","4e5ddf87":"#for linear models, according to models' documentation and logic of penalty, it is advised that alpha, l1_ratio values are between 0 and 1.\n#if you define alpha=0 it turns models to linear regression w\/o penalties:)","07cf1042":"#Lineer: Primitive Model\nlin_reg=LinearRegression().fit(X_train, y_train)\ny_pred_lin_reg=lin_reg.predict(X_test)\nlin_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_lin_reg))\nlin_reg_rmse","c52135dd":"#Ridge: Primitive Model\nrid_reg=Ridge().fit(X_train, y_train)\ny_pred_rid_reg=rid_reg.predict(X_test)\nrid_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rid_reg))\nprint(rid_reg_rmse)\n\n#Ridge: CV Model\nalpha_rid = [0.001, 0.005, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1]\nrid_reg_cv=RidgeCV(alphas = alpha_rid, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nrid_reg_cv.fit(X_train, y_train)\nprint(rid_reg_cv.alpha_)\n\n#Ridge: Tuned Model\nrid_reg_tuned=Ridge(rid_reg_cv.alpha_).fit(X_train, y_train)\ny_pred_rid_reg_tuned=rid_reg_tuned.predict(X_test)\nrid_reg_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rid_reg_tuned))\nrid_reg_tuned_rmse","eb97d0d9":"#Lasso: Primitive Model\nlas_reg=Lasso().fit(X_train, y_train)\ny_pred_las_reg=las_reg.predict(X_test)\nlas_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_las_reg))\nprint(las_reg_rmse)\n\n#Lasso: CV Model\nalpha_las = [0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.5, 1, 1.1]\nlas_reg_cv = LassoCV(alphas = alpha_las, cv = 10, normalize = True)\nlas_reg_cv.fit(X_train, y_train)\nprint(las_reg_cv.alpha_)\n\n#Lasso: Tuned Model\nlas_reg_tuned = Lasso(alpha = las_reg_cv.alpha_).fit(X_train,y_train)\ny_pred_las_reg_tuned = las_reg_tuned.predict(X_test)\nlas_reg_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_las_reg_tuned))\nlas_reg_tuned_rmse","a22bb1e6":"#ElasticNet: Primitive Model\nenet_reg=ElasticNet().fit(X_train, y_train)\ny_pred_enet_reg=enet_reg.predict(X_test)\nenet_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_enet_reg))\nprint(enet_reg_rmse)\n\n#ElasticNet: CV Model\nenet_reg_params = {\"l1_ratio\": [0.001, 0.01, 0.1, 0.5, 0.9, 1, 1.1],\n              \"alpha\":[0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1, 1.1]}\nenet_reg_cv = GridSearchCV(enet_reg, enet_reg_params, cv = 10).fit(X, y)\nprint(enet_reg_cv.best_params_)\n\n#ElasticNet: Tuned Model\nenet_reg_tuned = ElasticNet(**enet_reg_cv.best_params_).fit(X_train,y_train)\ny_pred_enet_reg_tuned = enet_reg_tuned.predict(X_test)\nenet_reg_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred_enet_reg_tuned))\nenet_reg_tuned_rmse","eed52e5d":"#KNN: Primitive Model\nknn_model=KNeighborsRegressor().fit(X_train, y_train)\nprint(knn_model)\ny_pred_knn_model=knn_model.predict(X_test)\ny_pred_knn_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_knn_model))\ny_pred_knn_model_rmse","d218d438":"#KNN: CV Model\nknn_params={\"n_neighbors\": np.arange(2,20,1)}\nknn_cv_model=GridSearchCV(knn_model, knn_params, cv=10, n_jobs=-1, verbose=2).fit(X_train, y_train)\nprint(knn_cv_model.best_params_)\n#KNN: Tuned Model\nknn_tuned=KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train, y_train)\ny_pred_knn_tuned=knn_tuned.predict(X_test)\ny_pred_knn_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_knn_tuned))\ny_pred_knn_tuned_rmse","0bff8ffd":"#SVR: Primitive Model\nsvr_model=SVR().fit(X_train, y_train)\nprint(svr_model)\ny_pred_svr_model=svr_model.predict(X_test)\ny_pred_svr_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_svr_model))\ny_pred_svr_model_rmse","4274715e":"#SVR: CV Model\nsvr_params={\"C\": (0.01, 0.1, 0.5, 0.9, 1),\n           \"kernel\": ('rbf', 'linear')}\nsvr_cv_model=GridSearchCV(svr_model, svr_params, cv=10, n_jobs=-1, verbose=2).fit(X_train, y_train)\nprint(svr_cv_model.best_params_)\n#SVR: Tuned Model\nsvr_tuned=SVR(**svr_cv_model.best_params_).fit(X_train, y_train)\ny_pred_svr_tuned=svr_tuned.predict(X_test)\ny_pred_svr_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_svr_tuned))\ny_pred_svr_tuned_rmse","0709df4f":"#ANN: Primitive Model\n#independent variables were already scaled\/normalized, then no need to be scaled again\nann_model=MLPRegressor(random_state=42).fit(X_train, y_train)\nprint(ann_model)\ny_pred_ann_model=ann_model.predict(X_test)\ny_pred_ann_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_ann_model))\ny_pred_ann_model_rmse","8e6a0212":"#ANN: CV Model\nann_params = {\"alpha\": [0.001, 0.01, 0.1, 0.2, 0.3, 0.5], \n             \"hidden_layer_sizes\": [(5,5), (10,10), (20,20), (100,100)],\n             \"solver\": ['lbfgs', 'sgd', 'adam']}\nann_cv_model=GridSearchCV(ann_model, ann_params, cv=10, n_jobs=-1, verbose=2).fit(X_train, y_train)\nprint(ann_cv_model.best_params_)\n#ANN: Tuned Model\nann_tuned=MLPRegressor(**ann_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_ann_tuned=ann_tuned.predict(X_test)\ny_pred_ann_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_ann_tuned))\ny_pred_ann_tuned_rmse","0a831e71":"#CART: Primitive Model\ncart_model = DecisionTreeRegressor(random_state=42).fit(X_train,y_train)\nprint(cart_model)\ny_pred_cart_model=cart_model.predict(X_test)\ny_pred_cart_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_cart_model))\ny_pred_cart_model_rmse","9fd57edc":"#CART: CV Model\ncart_params = {\"max_depth\": [2, 3, 4, 5, 10, None],\n              \"min_samples_split\": [2, 5, 10, 12, 20]}\ncart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(cart_cv_model.best_params_)\n\n#CART: Tuned Model\ncart_tuned=DecisionTreeRegressor(**cart_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_cart_tuned=cart_tuned.predict(X_test)\ny_pred_cart_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_cart_tuned))\ny_pred_cart_tuned_rmse","d99a132c":"#RF: Primitive Model\nrf_model = RandomForestRegressor(random_state=42).fit(X_train,y_train)\nprint(rf_model)\ny_pred_rf_model=rf_model.predict(X_test)\ny_pred_rf_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rf_model))\ny_pred_rf_model_rmse","cef60312":"#RF: CV Model\nrf_params = {\"max_depth\": [5, 8, 10, None],\n             \"max_features\": [3, 5, 10, 15, 17],\n             \"min_samples_split\": [2, 3, 5, 10],\n             \"n_estimators\": [100, 200, 500]}\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(rf_cv_model.best_params_)\n\n#RF: Tuned Model\nrf_tuned=RandomForestRegressor(**rf_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_rf_tuned=rf_tuned.predict(X_test)\ny_pred_rf_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rf_tuned))\ny_pred_rf_tuned_rmse","34ec9722":"#GBM: Primitive Model\ngbm_model = GradientBoostingRegressor(random_state=42).fit(X_train,y_train)\nprint(gbm_model)\ny_pred_gbm_model=gbm_model.predict(X_test)\ny_pred_gbm_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_gbm_model))\ny_pred_gbm_model_rmse","2d3c3bcc":"#GBM: CV Model\ngbm_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n             \"max_depth\": [2, 3, 4],\n             \"n_estimators\": [1000, 1500, 2000],\n             \"subsample\": [0.2, 0.3, 0.5],\n             \"loss\": [\"ls\",\"lad\",\"quantile\"]}\ngbm_cv_model = GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(gbm_cv_model.best_params_)\n\n#GBM: Tuned Model\ngbm_tuned=GradientBoostingRegressor(**gbm_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_gbm_tuned=gbm_tuned.predict(X_test)\ny_pred_gbm_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_gbm_tuned))\ny_pred_gbm_tuned_rmse","0ec49a3b":"#XGB: Primitive Model\nxgb_model = XGBRegressor(random_state=42).fit(X_train,y_train)\nprint(xgb_model)\ny_pred_xgb_model=xgb_model.predict(X_test)\ny_pred_xgb_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_xgb_model))\ny_pred_xgb_model_rmse","ba2de7d6":"#XGB: CV Model\nxgb_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n             \"max_depth\": [2, 3, 5, 8],\n             \"n_estimators\": [100, 200, 1000],\n             \"colsample_bytree\": [0.5, 0.8, 1]}\nxgb_cv_model = GridSearchCV(xgb_model, xgb_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(xgb_cv_model.best_params_)\n\n#XGB: Tuned Model\nxgb_tuned=XGBRegressor(**xgb_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_xgb_tuned=xgb_tuned.predict(X_test)\ny_pred_xgb_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\ny_pred_xgb_tuned_rmse","865d0589":"#LGBM: Primitive Model\nlgbm_model = LGBMRegressor(random_state=42).fit(X_train,y_train)\nprint(lgbm_model)\ny_pred_lgbm_model=lgbm_model.predict(X_test)\ny_pred_lgbm_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_lgbm_model))\ny_pred_lgbm_model_rmse","66aed66b":"#LGBM: CV Model\nlgbm_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n             \"max_depth\": [2, 3, 4, 5],\n             \"n_estimators\": [200, 500, 700, 1000],\n             \"colsample_bytree\": [0.6, 0.7, 0.8, 1]}\nlgbm_cv_model = GridSearchCV(lgbm_model, lgbm_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(lgbm_cv_model.best_params_)\n\n#LGBM: Tuned Model\nlgbm_tuned=LGBMRegressor(**lgbm_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_lgbm_tuned=lgbm_tuned.predict(X_test)\ny_pred_lgbm_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_lgbm_tuned))\ny_pred_lgbm_tuned_rmse","a4b9f265":"#CATB: Primitive Model\ncatb_model = CatBoostRegressor(verbose=False, random_state=42).fit(X_train,y_train)\nprint(catb_model)\ny_pred_catb_model=catb_model.predict(X_test)\ny_pred_catb_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_catb_model))\ny_pred_catb_model_rmse","793c50ba":"#CATB: CV Model\ncatb_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n               \"iterations\": [100, 200, 500],\n              \"depth\": [3, 5, 8]}\ncatb_cv_model = GridSearchCV(catb_model, catb_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(catb_cv_model.best_params_)\n\n#CATB: Tuned Model\ncatb_tuned=CatBoostRegressor(**catb_cv_model.best_params_, verbose=False, random_state=42).fit(X_train, y_train)\ny_pred_catb_tuned=lgbm_tuned.predict(X_test)\ny_pred_catb_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_catb_tuned))\ny_pred_catb_tuned_rmse","a53fcdef":"pd.set_option('display.max_colwidth', -1)\nresults = pd.DataFrame({\"Model Name\": [\"Primitive Test Errors\", \"Tuning Params\", \"Tuned Test Errors\"],\n                        \"Linear Reg\": [lin_reg_rmse, np.nan, np.nan],\n                        \"Ridge Reg\": [rid_reg_rmse, rid_reg_cv.alpha_, rid_reg_tuned_rmse],\n                        \"Lasso Reg\": [las_reg_rmse, las_reg_cv.alpha_, las_reg_tuned_rmse],\n                        \"ElasticNet Reg\": [enet_reg_rmse, enet_reg_cv.best_params_, las_reg_tuned_rmse],\n                        \"KNN\": [y_pred_knn_model_rmse, knn_cv_model.best_params_, y_pred_knn_tuned_rmse],\n                        \"SVR\": [y_pred_svr_model_rmse, svr_cv_model.best_params_, y_pred_svr_tuned_rmse],\n                        \"ANN\": [y_pred_ann_model_rmse, ann_cv_model.best_params_, y_pred_ann_tuned_rmse],\n                        \"CART\": [y_pred_cart_model_rmse, cart_cv_model.best_params_, y_pred_cart_tuned_rmse],\n                        \"RF\": [y_pred_rf_model_rmse, rf_cv_model.best_params_, y_pred_rf_tuned_rmse],\n                        \"GBM\": [y_pred_gbm_model_rmse, gbm_cv_model.best_params_, y_pred_gbm_tuned_rmse],\n                        \"XGB\": [y_pred_xgb_model_rmse, xgb_cv_model.best_params_, y_pred_xgb_tuned_rmse],\n                        \"LGBM\": [y_pred_lgbm_model_rmse, lgbm_cv_model.best_params_, y_pred_lgbm_tuned_rmse],\n                        \"CATB\": [y_pred_catb_model_rmse, catb_cv_model.best_params_, y_pred_catb_tuned_rmse]\n                        })\n\nresults.set_index(\"Model Name\", inplace=True)\nresults.T.sort_values(by=\"Tuned Test Errors\", ascending=True)","f3f50a87":"#GBM: Feature Importances & Visualization\nimportance=pd.DataFrame({'importance': gbm_tuned.feature_importances_ * 100},\n                       index=X_train.columns)\n\nimportance.sort_values(by='importance', axis=0, ascending=True). plot(kind='barh', color='g')\n\nplt.xlabel('Variable Importances')\nplt.gca().legend_=None","dd1994d8":"#RF: Feature Importances & Visualization\nimportance=pd.DataFrame({'importance': rf_tuned.feature_importances_ * 100},\n                       index=X_train.columns)\n\nimportance.sort_values(by='importance', axis=0, ascending=True). plot(kind='barh', color='g')\n\nplt.xlabel('Variable Importances')\nplt.gca().legend_=None","c5161709":"#XGB: Feature Importances & Visualization\nimportance=pd.DataFrame({'importance': xgb_tuned.feature_importances_ * 100},\n                       index=X_train.columns)\n\nimportance.sort_values(by='importance', axis=0, ascending=True). plot(kind='barh', color='g')\n\nplt.xlabel('Variable Importances')\nplt.gca().legend_=None","fab2ea9a":"# DATA UNDERSTANDING","bef4c3fe":"# EVALUATING","b0cc6d2b":"# MODELING (NON-LINEAR MODELS)","cf9c7f59":"# OVERVIEW OF STUDY","0ddcb67a":"# MODELING (LINEAR MODELS)","5de22399":"### As a conlusion, best model seems like GBM. Top models' test errors are similar to each other.\n\n### When we look at feature importances of top 3 models (GBM, RF, XGB), we see that \"AtBat, Years, Chits\" features are important. \n### We can back to data understanding and preparation steps and focus on important features of top models (AtBat, Years, Chits).","13317a12":"The **GOAL** of this study is building linear and non-linear models by using Hitters dataset. \nAll built models will be tuned to minimize prediction errors - test dataset errors- by RMSE metric.\nHold-out method will be used to split dataset for validation. \n\n**Content of Study:**\n1. Business Understanding\n1. Data Understanding\n2. Data Preparation\n    - Checking Outliers (Quantile Method, Suppressing)\n    - Missing Values (Filling Missing Values)\n    - Feature Engineering (Label Encoding)\n    - Feature Scaling (Normalization)\n3. Modeling (Linear, Non-Linear Modeling and Tuning)\n    - Linear Regression Models: Linear, Ridge, Lasso, ElasticNet\n    - Non-Linear Models: KNN, SVR, ANN, CART, RF, GBM, XGB, LGBM, CatBoost\n4. Evaluation & Proposals","ee1e321c":"* Dataset: Hitters\n* Description: Major League Baseball Data from the 1986 and 1987 seasons.\n* Source: This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n* A data frame with 322 observations of major league players on the following 20 variables.\n----\n* AtBat: Number of times at bat in 1986\n* Hits: Number of hits in 1986\n* HmRun: Number of home runs in 1986\n* Runs: Number of runs in 1986\n* RBI: Number of runs batted in in 1986\n* Walks: Number of walks in 1986\n----\n* PutOuts: Number of put outs in 1986\n* Assists: Number of assists in 1986\n* Errors: Number of errors in 1986\n---- \n* CAtBat: Number of times at bat during his career\n* CHits: Number of hits during his career\n* CHmRun: Number of home runs during his career\n* CRuns: Number of runs during his career\n* CRBI: Number of runs batted in during his career\n* CWalks: Number of walks during his career\n---- \n* Years:Number of years in the major leagues\n* League: A factor with levels A and N indicating player's league at the end of 1986\n* Division: A factor with levels E and W indicating player's division at the end of 1986\n* NewLeague: A factor with levels A and N indicating player's league at the beginning of 1987\n* Salary: 1987 annual salary on opening day in thousands of dollars\n----","77d1bdce":"# BUSINESS UNDERSTANDING","77a5d28d":"# DATA PREPARATION"}}