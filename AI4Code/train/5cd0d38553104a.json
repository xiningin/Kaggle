{"cell_type":{"e8392c71":"code","1046b3de":"code","48975afd":"code","abb7cd36":"code","08ea269c":"code","8084835d":"code","12d8633a":"code","27093332":"code","c99ac688":"code","5cb97331":"code","72bf837e":"code","e751046b":"code","6a81fcb3":"code","b442fda7":"code","01660399":"code","89078d16":"code","79ff4ebb":"code","5e6c35a8":"code","a1bb76a7":"code","74d2ab62":"code","ab9f64e4":"code","8931ef98":"code","352ec7f7":"code","1117289b":"code","5f841ece":"code","3ca0c239":"code","712178f0":"code","f0de839d":"code","1dbdcaa0":"markdown","e9e8cfeb":"markdown","75cb3dfc":"markdown","0d93a5aa":"markdown","f011aa25":"markdown","8286d92e":"markdown","ada9c364":"markdown","d1740fe7":"markdown","67c5982e":"markdown","2d008615":"markdown","e0857e5a":"markdown","f9cb9060":"markdown","61b2b24d":"markdown","bf55e556":"markdown","1520e972":"markdown","3598bac9":"markdown","ca0b1a9d":"markdown","e4c93033":"markdown","f7c10c29":"markdown","544c5f2a":"markdown","e6e4a37d":"markdown","6abeb316":"markdown","872a45cd":"markdown","6e70917e":"markdown","0c43ddf5":"markdown","45f84d6d":"markdown"},"source":{"e8392c71":"import os\nprint(os.listdir('..\/input\/flowers-recognition\/flowers'))\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\nfrom keras.layers import Dropout, Flatten,Activation\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n \nimport tensorflow as tf\nimport random as rn\nimport cv2                  \nimport numpy as np  \nfrom tqdm import tqdm\nimport os                   \nfrom random import shuffle  \nfrom zipfile import ZipFile\nfrom PIL import Image","1046b3de":"X=[]\nZ=[]\nIMG_SIZE=150\nFLOWER_DAISY_DIR='..\/input\/flowers-recognition\/flowers\/daisy'\nFLOWER_SUNFLOWER_DIR='..\/input\/flowers-recognition\/flowers\/sunflower'\nFLOWER_TULIP_DIR='..\/input\/flowers-recognition\/flowers\/tulip'\nFLOWER_DANDI_DIR='..\/input\/flowers-recognition\/flowers\/dandelion'\nFLOWER_ROSE_DIR='..\/input\/flowers-recognition\/flowers\/rose'\ndef assign_label(img,flower_type):\n    return flower_type\ndef make_train_data(flower_type,DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label=assign_label(img,flower_type)\n        path = os.path.join(DIR,img)\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        if img is None:\n            continue\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        \n        X.append(np.array(img))\n        Z.append(str(label))\nmake_train_data('Daisy',FLOWER_DAISY_DIR)\nmake_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\nmake_train_data('Tulip',FLOWER_TULIP_DIR)\nmake_train_data('Dandelion',FLOWER_DANDI_DIR)\nmake_train_data('Rose',FLOWER_ROSE_DIR)\nprint(len(X))","48975afd":"fig,ax=plt.subplots(5,2)\nfig.set_size_inches(15,15)\nfor i in range(5):\n    for j in range (2):\n        l=rn.randint(0,len(Z))\n        ax[i,j].imshow(X[l])\n        ax[i,j].set_title('Flower: '+Z[l])\n        \nplt.tight_layout()\n        ","abb7cd36":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nY=le.fit_transform(Z)\nY=to_categorical(Y,5)\nX=np.array(X)\nX=X\/255","08ea269c":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)\nnp.random.seed(42)\nrn.seed(42)\ntf.random.set_seed(42)","8084835d":"# # modelling starts using a CNN.\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(5, activation = \"softmax\"))","12d8633a":"batch_size=128\nepochs=50\n\nfrom keras.callbacks import ReduceLROnPlateau\nred_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)","27093332":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","c99ac688":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","5cb97331":"History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size)","72bf837e":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","e751046b":"plt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","6a81fcb3":"# getting predictions on val set.\npred=model.predict(x_test)\npred_digits=np.argmax(pred,axis=1)","b442fda7":"# now storing some properly as well as misclassified indexes'.\ni=0\nprop_class=[]\nmis_class=[]\n\nfor i in range(len(y_test)):\n    if(np.argmax(y_test[i])==pred_digits[i]):\n        prop_class.append(i)\n    if(len(prop_class)==8):\n        break\n\ni=0\nfor i in range(len(y_test)):\n    if(not np.argmax(y_test[i])==pred_digits[i]):\n        mis_class.append(i)\n    if(len(mis_class)==8):\n        break","01660399":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\ncount=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range (4):\n    for j in range (2):\n        ax[i,j].imshow(x_test[prop_class[count]])\n        ax[i,j].set_title(\"Predicted Flower : \"+\\\n                          str(le.inverse_transform(np.array([pred_digits[prop_class[count]]])))+\\\n                          \"\\n\"+\"Actual Flower : \"+\\\n                          str(le.inverse_transform(np.array([np.argmax([y_test[prop_class[count]]])]))))\n        plt.tight_layout()\n        count+=1","89078d16":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\ncount=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range (4):\n    for j in range (2):\n        ax[i,j].imshow(x_test[mis_class[count]])\n        ax[i,j].set_title(\"Predicted Flower : \"+\\\n                          str(le.inverse_transform(np.array([pred_digits[mis_class[count]]])))\\\n                          +\"\\n\"+\"Actual Flower : \"+\\\n                          str(le.inverse_transform(np.array([np.argmax([y_test[mis_class[count]]])]))))\n        plt.tight_layout()\n        count+=1","79ff4ebb":"\nmodel = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Conv2D(filters = 118, kernel_size = (3,3), padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Conv2D(filters = 118, kernel_size = (3,3), padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Conv2D(filters = 192, kernel_size = (3,3), strides=(3,3), padding = 'Same',activation ='relu'))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(5, activation = \"softmax\"))","5e6c35a8":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","a1bb76a7":"batch_size=128\nepochs=50\n\nfrom keras.callbacks import ReduceLROnPlateau\nred_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)","74d2ab62":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","ab9f64e4":"History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size)","8931ef98":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","352ec7f7":"plt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","1117289b":"\nmodel = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Conv2D(filters = 256, kernel_size = (4,4),padding = 'Same',strides=(4,4), activation ='relu'))\n \nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(5, activation = \"softmax\"))","5f841ece":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","3ca0c239":"batch_size=128\nepochs=50\n\nfrom keras.callbacks import ReduceLROnPlateau\nred_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)","712178f0":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","f0de839d":"History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size)","1dbdcaa0":"# With acceptable accuracy of 88.37%, the model seems to do well, however we want better and In 2014, Springenber et al. published a paper entitled Striving for Simplicity: The All Convolutional Net which demonstrated that replacing pooling layers with strided convolutions can increase accuracy in some situations.\n# So i'll be adding a couple of extra layers and a final layer while doubling the filter and increasing stride size to 3*3 to compensate","e9e8cfeb":"# 1.EDA","75cb3dfc":"# lets load the feature images.","0d93a5aa":"# Another trial increasing the layers and the kernel size to try to increase the accuracy further","f011aa25":"Wrongly classified","8286d92e":"# An accuracy of 88% for the 4 million learnable parameters.","ada9c364":"# Fitting and predicting","d1740fe7":"# Kera compilation","67c5982e":"# The learnable parameters for this design is half the first design's and 3 times the second","2d008615":"# convNet model","e0857e5a":"# The images are of flowers of different types with no common background.\n# It also seems that most of the images have grass and other plants in them.","f9cb9060":"# A total of 4 million learnable parameters for the designed layers, this might be costly in memory and processing power comparing to other designs.\n# it will need further analysis and comparisons with other designs.","61b2b24d":"As per the above analysis of wrongly classified and correctly classified it seems that some images with unclear edges are misclassified.","bf55e556":"# With less than quarter the learnable parameters the new design had a 88.73% accuracy which is statistically equals to the first design but with a big advantage in computational cost.","1520e972":"# data augmentation","3598bac9":"# A total of 692,000 learnable parameters for the designed layers, this ,comparing to the first design, is almost 1\/8 the parameters and less than quarter the computational cost.\n# Let's proceed to compare the accuracy.","ca0b1a9d":"# 3.Modeling","e4c93033":"![](http:\/\/)","f7c10c29":"# Why choose Relu over sigmoid or tanh ?\n# sigmoid and tanh are not zero centered which would make the gradient unidirectional, i.e it would move only in one direction(the positive direction),also the exponential calculation used in using the sigmoid or tanhis costly to calculate\n# unlike Relu which although might be not zero centered is is alot less costly in calculation\n\n# Which relu to use though?\n# the literature doesn't show a trend on accuracy with different Relu versions, so no specific Relu is recommended; we'll use the simplest form","544c5f2a":"Correctly classified","e6e4a37d":"# The accuracy of this design is still around the 88% mark which is similar to the first and the second designs.\n# So according to our trials the seconed design with the least numbers of trainable parameters and similar accuracy result might be our best candidate, further more, it seems that blindingly increasing the number of layers does not result in a significant increase in accuracy, hyper parameters tuning, image rotation, random scaling, and whitening should be tried to test if they have effect on accuracy","6abeb316":"# 4.Evaluation and analysis","872a45cd":"# 2.Extracting features","6e70917e":"# Let's visualize some of the images:","0c43ddf5":"# split into training and testing sets with 75%, 25% ration","45f84d6d":"# Encoding labels(targets)"}}