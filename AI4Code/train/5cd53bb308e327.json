{"cell_type":{"2541dff6":"code","f9993b55":"code","1c3ac2f5":"code","2ae65c5a":"code","ecaa4db5":"code","f5713387":"code","beb1de5a":"code","f3b8cf83":"code","d5ad1a69":"code","566de2ca":"code","40a4848a":"code","8a497b20":"code","a628b5fa":"code","acc405c8":"code","e87a1533":"code","7a326f25":"code","b5d93294":"code","a8169c3d":"code","12071bf9":"code","c0e8f88f":"code","5123c309":"code","28042212":"code","b04a55f5":"code","dad06107":"code","9fec783a":"code","28216dac":"code","f1fb05ea":"code","b566391c":"code","8456225d":"code","7dac46a9":"code","baa144d3":"code","978de776":"code","4f4e3f4d":"code","2f48e963":"code","42e17530":"code","acf8af05":"code","18d48965":"code","260563d6":"code","aedc7908":"code","e380c7b8":"code","71ee2765":"code","62f7c161":"code","c8f925c8":"code","03af5900":"markdown","d3e0e3c8":"markdown","0e344844":"markdown","b6cdb690":"markdown","0ad05094":"markdown"},"source":{"2541dff6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom haversine import haversine\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f9993b55":"# Importation du fichier CSV (en indiquant que la colonne IDENTITY est la colonne id du dataset)\ndata = pd.read_csv('..\/input\/train.csv') #, index_col = 0)","1c3ac2f5":"test = pd.read_csv('..\/input\/test.csv') #, index_col = 0)","2ae65c5a":"test.shape","ecaa4db5":"test.head()","f5713387":"data.head()","beb1de5a":"data.info","f3b8cf83":"test.shape","d5ad1a69":"# Convertir les dates de timestamp en datetime afin d'extraire d'autres d\u00e9tails importants de la date\ndata['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\ndata['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])\n\n# Convertir les dates de timestamp en datetime afin d'extraire d'autres d\u00e9tails importants de la date\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])","566de2ca":"# Extraction, Calcul et affectation des nouvelles donn\u00e9es relative \u00e0 la pickup_date dans le dataset\ndata['weekday'] = data.pickup_datetime.dt.weekday_name\ndata['month'] = data.pickup_datetime.dt.month\ndata['weekday_num'] = data.pickup_datetime.dt.weekday\ndata['pickup_hour'] = data.pickup_datetime.dt.hour\n\n# Extraction, Calcul et affectation des nouvelles donn\u00e9es relative \u00e0 la pickup_date dans le dataset\ntest['weekday'] = test.pickup_datetime.dt.weekday_name\ntest['month'] = test.pickup_datetime.dt.month\ntest['weekday_num'] = test.pickup_datetime.dt.weekday\ntest['pickup_hour'] = test.pickup_datetime.dt.hour","40a4848a":"# Fonction de calcul de distance entre les points de d\u00e9parts et les points d'arriv\u00e9es\n# Elle prend en param\u00e8tre le dataset, et renvoie un vecteur contenant les distances entre ces points\n# Elle applique la m\u00e9thode de Haversine pour le calcul des distances entre deux coordonn\u00e9es\ndef calcul_distance(df):\n    pickedup = (df['pickup_latitude'], df['pickup_longitude'])\n    dropoff = (df['dropoff_latitude'], df['dropoff_longitude'])\n    return haversine(pickedup, dropoff)","8a497b20":"# Calcul des distances entre les points de d\u00e9parts et les points d'arriv\u00e9es\n# et les mettant dans une nouvelle colonne distance\ndata['distance'] = data.apply(lambda x : calcul_distance(x), axis = 1)","a628b5fa":"test['distance'] = test.apply(lambda x : calcul_distance(x), axis = 1)","acc405c8":"data.dtypes.reset_index()","e87a1533":"test.dtypes.reset_index()","7a326f25":"# D\u00e9couper les features cat\u00e9goriques en plusieurs variables num\u00e9riques \/ indicatrices\n\ndummy = pd.get_dummies(data.store_and_fwd_flag, prefix='flag')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.vendor_id, prefix='vendor_id')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.month, prefix='month')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.weekday_num, prefix='weekday_num')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.pickup_hour, prefix='pickup_hour')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.passenger_count, prefix='passenger_count')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ndata = pd.concat([data,dummy], axis = 1)","b5d93294":"test.head()","a8169c3d":"# D\u00e9couper les features cat\u00e9goriques en plusieurs variables num\u00e9riques \/ indicatrices\n\ndummy = pd.get_dummies(test.store_and_fwd_flag, prefix='flag')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ntest = pd.concat([test,dummy], axis = 1)\n\ndummy = pd.get_dummies(test.vendor_id, prefix='vendor_id')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ntest = pd.concat([test,dummy], axis = 1)\n\ndummy = pd.get_dummies(test.month, prefix='month')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ntest = pd.concat([test,dummy], axis = 1)\n\ndummy = pd.get_dummies(test.weekday_num, prefix='weekday_num')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ntest = pd.concat([test,dummy], axis = 1)\n\ndummy = pd.get_dummies(test.pickup_hour, prefix='pickup_hour')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ntest = pd.concat([test,dummy], axis = 1)\n\ndummy = pd.get_dummies(test.passenger_count, prefix='passenger_count')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #enlever la premi\u00e8re colonne qui est l'index\ntest = pd.concat([test,dummy], axis = 1)","12071bf9":"test.shape","c0e8f88f":"data.head()","5123c309":"data.shape","28042212":"test.head()","b04a55f5":"pd.options.display.float_format = '{:.2f}'.format #Basculer l'affichage des floats en format scientifique","dad06107":"data.passenger_count.value_counts()","9fec783a":"test.passenger_count.value_counts()","28216dac":"print(data.passenger_count.describe())\nprint(f'median = {data.passenger_count.median()}')\n# On remarque que la moyenne, la m\u00e9diane et les modes sont presque \u00e9gaux \u00e0 1","f1fb05ea":"# Alors on remplace le passenger_count 0 par 1\ndata['passenger_count'] = data.passenger_count.map(lambda x: 1 if x == 0 else x)","b566391c":"test['passenger_count'] = test.passenger_count.map(lambda x: 1 if x == 0 else x)","8456225d":"test.shape","7dac46a9":"data.passenger_count.value_counts()","baa144d3":"test.passenger_count.value_counts()","978de776":"#Nombre de courses par nombre de passagers\nsns.countplot(data.passenger_count)\nplt.show()","4f4e3f4d":"data.dtypes.reset_index()","2f48e963":"test.dtypes.reset_index()","42e17530":"# Distribution des horaires de d\u00e9parts des courses sur 24 heures\nsns.countplot(data.pickup_hour)\nplt.show()","acf8af05":"data.head()","18d48965":"#V\u00e9rifiez d'abord l'index des features et le label\nlist(zip( range(0,len(data.columns)),data.columns))","260563d6":"SELECTED_COLUMNS = ['vendor_id', 'vendor_id_2', \n                    'flag_Y', \n                    'pickup_hour', 'distance', \n                    'month','weekday_num',\n                    'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n                    'weekday_num_1', 'weekday_num_2', 'weekday_num_3', 'weekday_num_4', 'weekday_num_5', 'weekday_num_6',\n                    'passenger_count_1', 'passenger_count_2', 'passenger_count_3', 'passenger_count_4', 'passenger_count_5', 'passenger_count_6',\n                    'pickup_hour', 'pickup_hour_1', 'pickup_hour_2','pickup_hour_3', 'pickup_hour_4', 'pickup_hour_5','pickup_hour_6',  'pickup_hour_7', 'pickup_hour_8', \n                    'pickup_hour_9', 'pickup_hour_10', 'pickup_hour_11', 'pickup_hour_12', 'pickup_hour_13', 'pickup_hour_14', 'pickup_hour_15', 'pickup_hour_16', \n                    'pickup_hour_17', 'pickup_hour_18', 'pickup_hour_19', 'pickup_hour_20', 'pickup_hour_21', 'pickup_hour_22', 'pickup_hour_23' ]\nX_many_features = data[SELECTED_COLUMNS]\nX_many_features.head()\ny_many_features = np.log1p(data['trip_duration'])\n","aedc7908":"X_many_features.shape, y_many_features.shape","e380c7b8":"rf = RandomForestRegressor(random_state=42)\nrf.fit(X_many_features, y_many_features)","71ee2765":"cv_scores = -cross_val_score(rf, X_many_features, y_many_features, cv=3, scoring='neg_mean_squared_error')\ncv_scores","62f7c161":"cv_scores.mean()","c8f925c8":"X_test = test[SELECTED_COLUMNS]\npredictions = np.exp(rf.predict(X_test))-np.ones(len(X_test))\n\nX_test.shape\npred = pd.DataFrame(predictions, index=test['id'])\npred.columns = ['trip_duration']\npred.to_csv(\"submission_.csv\")\n\npd.read_csv('submission_.csv')","03af5900":"# 3 Preprocessing","d3e0e3c8":"# 1 Importation des donn\u00e9es","0e344844":"# 4 Analyse univari\u00e9e\n#### On va analyser et \u00e9tudier les variables une par une","b6cdb690":"# 2 Analyse des donn\u00e9es","0ad05094":"# 5 S\u00e9l\u00e9ction des features"}}