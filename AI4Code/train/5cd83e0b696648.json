{"cell_type":{"28968a25":"code","036ce600":"code","517b43d0":"code","9098da82":"code","6a9b99a2":"code","e47b5f58":"code","ae7e7512":"code","34d489d1":"code","d0bb73d6":"code","34679738":"code","5855a715":"code","f6868b62":"code","282730c2":"code","e2f974e4":"code","4e72961a":"code","6a43caa6":"code","7e1c4f66":"code","d00f52f5":"code","821fa70b":"code","60a3e036":"code","c79da712":"code","f5738874":"code","dc358823":"code","506d82cc":"code","58a38556":"code","28a6f2d1":"code","490d696e":"code","ca073b2c":"code","77af61fd":"code","246dc281":"code","2db30fc8":"code","ba798483":"code","43b31783":"code","578df622":"code","b36bf7fb":"code","283b2840":"code","64342b34":"code","22ff9edf":"markdown"},"source":{"28968a25":"import torch\nimport torchvision\nimport cv2\nimport numpy as np\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom random import choice, choices, shuffle\nimport re\nfrom ipywidgets import IntProgress\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensor, ToTensorV2\n\n\nBOX_COLOR = (0, 0, 255)\nTEXT_COLOR = (255, 255, 255)\nTRAIN_IMG_DIR = \"..\/input\/global-wheat-detection\/train\"","036ce600":"#show 1 \u1ea3nh\ndef plot_img(img, size=(7,7), is_rgb=False):\n    plt.figure(figsize=size)\n    if is_rgb:\n        plt.imshow(img)\n    else:\n        plt.imshow(img[:,:,::-1])\n    plt.show()\n    \n    \n#show nhi\u1ec1u \u1ea3nh\ndef plot_imgs(imgs, cols=5, size=7, is_rgb=False):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        fig.add_subplot(rows, cols, i+1)\n        if is_rgb:\n            plt.imshow(img)\n        else:\n            plt.imshow(img[:,:,::-1])\n    plt.show()\n    \n    \n# v\u1ebd bounding box l\u00ean \u1ea3nh\ndef visualize_bbox(img, boxes, thickness=3, color=BOX_COLOR):\n    img_copy = img.copy()\n    for box in boxes:\n        img_copy = cv2.rectangle(\n            img_copy,\n            (int(box[0]), int(box[1])),\n            (int(box[2]), int(box[3])),\n            color, thickness)\n    return img_copy\n\n\n# v\u1ebd bounding box l\u00ean \u1ea3nh\ndef load_img(img_id, folder=TRAIN_IMG_DIR):\n    img_fn = f\"{folder}\/{img_id}.jpg\"\n    img = cv2.imread(img_fn).astype(np.float32)\n    img \/= 255.0\n    return img","517b43d0":"#chuy\u1ec3n \u0111\u1ed5i c\u1eb7p [imgs, targets] sang d\u1ea1ng tensor theo device cpu\/gpu\ndef data_to_device(images, targets, device=torch.device(\"cuda\")):\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    return images, targets\n\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\n\n# \u0111\u1ecdc data t\u1eeb file csv\n# output l\u00e0 1 list ch\u1ee9a th\u00f4ng tin v\u1ec1 c\u00e1c \u1ea3nh\n# m\u1ed7i ph\u1ea7n t\u1eed bao g\u1ed3m 1 image_id v\u00e0 1 list c\u00e1c bounding box\ndef read_data_in_csv(csv_path=\".\/wheat-dataset\/train.csv\"):\n    df = pd.read_csv(csv_path)\n    df['x'], df['y'],  df['w'], df['h'] = -1, -1, -1, -1\n    df[['x', 'y', 'w', 'h']] = np.stack(df['bbox'].apply(lambda x: expand_bbox(x)))\n    df.drop(columns=['bbox'], inplace=True)\n    df['x'] = df['x'].astype(np.float)\n    df['y'] = df['y'].astype(np.float)\n    df['w'] = df['w'].astype(np.float)\n    df['h'] = df['h'].astype(np.float)\n    objs = []\n    img_ids = set(df[\"image_id\"])\n    \n    for img_id in tqdm(img_ids):\n        records = df[df[\"image_id\"] == img_id]\n        boxes = records[['x', 'y', 'w', 'h']].values\n        area = boxes[:,2]*boxes[:,3]\n        boxes[:,2] = boxes[:,0] + boxes[:,2]\n        boxes[:,3] = boxes[:,1] + boxes[:,3]\n\n        obj = {\n            \"img_id\": img_id,\n            \"boxes\": boxes,\n            \"area\":area\n        }\n        objs.append(obj)\n    return objs\n\n\nclass WheatDataset(Dataset):\n    def __init__(self, data, img_dir ,transform=None):\n        self.data = data\n        self.img_dir = img_dir\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        img_data = self.data[idx]\n        bboxes = img_data[\"boxes\"]\n        box_nb = len(bboxes)\n        labels = torch.ones((box_nb,), dtype=torch.int64)\n        iscrowd = torch.zeros((box_nb,), dtype=torch.int64)\n        img = load_img(img_data[\"img_id\"], self.img_dir)\n        area = img_data[\"area\"]\n        if self.transform is not None:\n            sample = {\n                \"image\":img,\n                \"bboxes\": bboxes,\n                \"labels\": labels,\n                \"area\": area\n            }\n            sample = self.transform(**sample)\n            img = sample['image']\n            area = sample[\"area\"]\n            bboxes = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        target = {}\n        target['boxes'] = bboxes.type(torch.float32)\n        target['labels'] = labels\n        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n        target['iscrowd'] = iscrowd\n        target[\"image_id\"] = torch.tensor([idx])\n        return img, target\n        \n    def __len__(self):\n        return len(self.data)\n    \n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","9098da82":"# #load data form csv file\n# data = read_data_in_csv('..\/input\/global-wheat-detection\/train.csv')\n# shuffle(data)\n# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# # t\u1ea1o transform cho dataset - c\u00e1c bi\u1ebfn \u0111\u1ed5i \u0111\u1ec3 augmentation data\n# train_transform = A.Compose(\n#     [A.Flip(0.5), ToTensorV2(p=1.0)],\n#     bbox_params={\n#         \"format\":\"pascal_voc\",\n#         'label_fields': ['labels']\n# })\n\n# # kh\u1edfi t\u1ea1o Dataset v\u00e0 Dataloader\n# train_dataset = WheatDataset(data, img_dir=TRAIN_IMG_DIR, transform=train_transform)\n# train_loader = DataLoader(\n#     train_dataset,\n#     batch_size=8,\n#     shuffle=True,\n#     num_workers=2,\n#     collate_fn=collate_fn)\n","6a9b99a2":"# from torch.optim.lr_scheduler import _LRScheduler\n# from torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\n# class GradualWarmupScheduler(_LRScheduler):\n#     def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n#         self.multiplier = multiplier\n#         if self.multiplier < 1.:\n#             raise ValueError('multiplier should be greater thant or equal to 1.')\n#         self.total_epoch = total_epoch\n#         self.after_scheduler = after_scheduler\n#         self.finished = False\n#         super(GradualWarmupScheduler, self).__init__(optimizer)\n\n#     def get_lr(self):\n#         if self.last_epoch > self.total_epoch:\n#             if self.after_scheduler:\n#                 if not self.finished:\n#                     self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n#                     self.finished = True\n#                 return self.after_scheduler.get_last_lr()\n#             return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n#         if self.multiplier == 1.0:\n#             return [base_lr * (float(self.last_epoch) \/ self.total_epoch) for base_lr in self.base_lrs]\n#         else:\n#             return [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]\n\n#     def step_ReduceLROnPlateau(self, metrics, epoch=None):\n#         if epoch is None:\n#             epoch = self.last_epoch + 1\n#         self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n#         if self.last_epoch <= self.total_epoch:\n#             warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]\n#             for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n#                 param_group['lr'] = lr\n#         else:\n#             if epoch is None:\n#                 self.after_scheduler.step(metrics, None)\n#             else:\n#                 self.after_scheduler.step(metrics, epoch - self.total_epoch)\n\n#     def step(self, epoch=None, metrics=None):\n#         if type(self.after_scheduler) != ReduceLROnPlateau:\n#             if self.finished and self.after_scheduler:\n#                 if epoch is None:\n#                     self.after_scheduler.step(None)\n#                 else:\n#                     self.after_scheduler.step(epoch - self.total_epoch)\n#                 self._last_lr = self.after_scheduler.get_last_lr()\n#             else:\n#                 return super(GradualWarmupScheduler, self).step(epoch)\n#         else:\n#             self.step_ReduceLROnPlateau(metrics, epoch)","e47b5f58":"\n\n# num_classes = 2\n# num_epochs = 5\n# iters = 1\n# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, progress=False)\n# in_features = model.roi_heads.box_predictor.cls_score.in_features\n# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# params = [p for p in model.parameters() if p.requires_grad]\n# optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.9, weight_decay=0.0005)\n\n# scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs-1)\n# scheduler = GradualWarmupScheduler(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n\n# model.to(device)","ae7e7512":"# # ti\u1ebfn h\u00e0nh train model\n# for epoch in range(num_epochs):\n#     scheduler.step(epoch)\n#     model.train()\n#     for images, targets in train_loader:\n#         images, targets = data_to_device(images, targets)\n#         loss_dict = model(images, targets)\n#         losses = sum(loss for loss in loss_dict.values())\n#         loss_value = losses.item()\n        \n#         optimizer.zero_grad()\n#         losses.backward()\n#         optimizer.step()\n        \n#         iters += 1\n#         # show loss per 30 iteration\n#         if iters%30 == 0:\n#             print(f\"Iteration #{iters} loss: {loss_value}\")\n            \n# #         # \u0111\u1ec3 \u0111\u01a1n gi\u1ea3n, ta save model m\u1ed7i 90 iteration\n# #         if iters%90 == 0:\n# #             model_path = f\".\/saved_model\/model_{iters}_{round(loss_value, 2)}.pth\"\n# #             torch.save(model.state_dict(), model_path)\n# #             model.train()","34d489d1":"# torch.save(model.state_dict(), '.\/model_lr_5epoch.h5')","d0bb73d6":"class WheatTestDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","34679738":"def get_test_transform():\n    return A.Compose([\n        # A.Resize(512, 512),\n        ToTensorV2(p=1.0)\n    ])","5855a715":"# load a model; pre-trained on COCO\n# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)","f6868b62":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# num_classes = 2  # 1 class (wheat) + background\n\n# get number of input features for the classifier\n# in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\n# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the trained weights\n# model.load_state_dict(torch.load('..\/input\/fatercnnresnet152102410epochfold1\/fastercnnresnet152_10ep_fold1_ap60.h5'))\nmodel = torch.load('..\/input\/new-gwd2021-resnet50\/model_ep-7_ap-0.6110084455701318')\n\nmodel.eval()\n\nx = model.to(device)","282730c2":"# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n# from torchvision.models.detection.faster_rcnn import FasterRCNN\n# from torchvision.models.detection.backbone_utils import BackboneWithFPN\n\n# def fasterrcnn_resnet_fpn(backbone_name='resnet152', progress=True, num_classes=91, pretrained=True, pretrained_backbone=True, **kwargs):\n#     if backbone_name == 'resnet50':\n#         model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n#     elif backbone_name in ['resnet101', 'resnet152']:\n#         backbone = resnet_fpn_backbone(backbone_name, pretrained_backbone)\n#     else:\n#         backbone = my_resnet_fpn_backbone(backbone_name, pretrained_backbone)\n#     model = FasterRCNN(backbone, num_classes, **kwargs)\n#     return model","e2f974e4":"# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# # model = fasterrcnn_resnet_fpn(backbone_name='resnet152', pretrained=True, pretrained_backbone=True)\n# # in_features = model.roi_heads.box_predictor.cls_score.in_features\n# # model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n# # model.load_state_dict(torch.load('..\/input\/testmodelfastercnndungnb\/model-frcnn-dungnb.h5'))\n\n# model1 = torch.load('..\/input\/fasterrcnn20epfull1024folds1\/model_ep-19_trainloss-1.0128233870248047')\n# model1.eval()\n\n# model2 = torch.load('..\/input\/testdungnbnew\/model-frcnn-dungnb-new.h5')\n# model2.eval()\n\n# model1 = model1.to(device)\n# model2 = model2.to(device)","4e72961a":"# \u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a33","6a43caa6":"# detection_threshold = 0.5\n# results = []\n\n# for images, image_ids in test_data_loader:\n\n#     images = list(image.to(device) for image in images)\n#     output1 = model1(images)\n#     output2 = model2(images)\n#     break\n#     for i, image in enumerate(images):\n\n#         boxes = outputs[i]['boxes'].data.cpu().numpy()\n#         scores = outputs[i]['scores'].data.cpu().numpy()\n        \n#         boxes = boxes[scores >= detection_threshold].astype(np.int32)\n#         scores = scores[scores >= detection_threshold]\n#         image_id = image_ids[i]\n        \n#         boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n#         boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n#         result = {\n#             'image_id': image_id,\n#             'PredictionString': format_prediction_string(boxes, scores)\n#         }\n\n        \n#         results.append(result)","7e1c4f66":"# ! pip install ensemble-boxes==1.0.4","d00f52f5":"# from ensemble_boxes import weighted_boxes_fusion\n","821fa70b":"# boxe1 = output1[0]['boxes'].cpu().detach().numpy()\n# boxe2 = output2[0]['boxes'].cpu().detach().numpy()","60a3e036":"# temp1 = pd.DataFrame(boxe1)\n# temp2 = pd.DataFrame(boxe2)","c79da712":"# temp[0][3]","f5738874":"# boxes = []\n# for i in range(len(temp1)):\n#     boxes.append([temp1[i][0], temp1[i][1], temp1[i][2], temp1[i][3]])\n# len(boxes)","dc358823":"# boxes, scores, labels","506d82cc":"# boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=0.5, skip_box_thr=0.32)\n# boxes = np.array(boxes)\n# scores = np.array(scores)","58a38556":"# \u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3\u00a3","28a6f2d1":"test_df = pd.read_csv('..\/input\/global-wheat-detection\/sample_submission.csv')\ntest_df.shape","490d696e":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = WheatTestDataset(test_df, '..\/input\/global-wheat-detection\/test', get_test_transform())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","ca073b2c":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","77af61fd":"detection_threshold = 0.5\nresults = []\n\nfor images, image_ids in test_data_loader:\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        \n        results.append(result)","246dc281":"# results[0:2]\n","2db30fc8":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","ba798483":"test_df.to_csv('.\/submission.csv',index=False)","43b31783":"# sample = images[2].permute(1,2,0).cpu().numpy()\n# boxes = outputs[2]['boxes'].data.cpu().numpy()\n# scores = outputs[2]['scores'].data.cpu().numpy()\n\n# boxes = boxes[scores >= detection_threshold].astype(np.int32)\n\n\n# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n# for box in boxes:\n#     cv2.rectangle(sample,\n#                   (box[0], box[1]),\n#                   (box[2], box[3]),\n#                   (220, 0, 0), 2)\n    \n# ax.set_axis_off()\n# ax.imshow(sample)","578df622":"# torch.save(model,'.\/model.h5')","b36bf7fb":"# model1 = torch.load('..\/input\/testmodelfastercnndungnb\/model-frcnn-dungnb.h5')","283b2840":"# model1","64342b34":"# model1 = torch.load('..\/input\/testmodel\/model.h5')","22ff9edf":"========================================================================="}}