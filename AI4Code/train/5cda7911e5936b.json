{"cell_type":{"e54e65b0":"code","eb91e4fe":"code","0a0b9eb2":"code","334e0791":"code","24eab051":"code","e5a3d944":"code","75a9c6af":"code","012d6394":"code","41ae531e":"code","4fc608d5":"code","3afa9554":"code","64486d08":"code","63bc3fce":"code","d2bf561e":"code","105bffe5":"code","2ef25bb9":"code","5974b9ad":"code","01cff040":"code","66fab615":"code","42eb2aa2":"code","22e467f8":"code","5dd0ca7f":"code","7214fb3a":"code","d8a5a066":"code","982bdd8f":"code","00a066dd":"code","f4a57e05":"code","e25b4d4a":"code","854c8ff8":"code","7ea4e34e":"code","511dffc3":"code","ef67398a":"code","ea27bfb2":"code","8bd4c4de":"code","194c232c":"code","8ad7fc9a":"code","500a8768":"code","b5a95c14":"code","bda6d584":"code","30588959":"code","5c122245":"code","c8359143":"code","8084def4":"code","49de8276":"code","c3d714d1":"code","3df790cc":"markdown","40772f1b":"markdown","eb9885e4":"markdown","c5e012c4":"markdown","6d172e0b":"markdown","867f8cad":"markdown","a6ee95aa":"markdown","8da7049e":"markdown","d8fe3db0":"markdown","02be7410":"markdown","c21d6526":"markdown","942c59cd":"markdown","f18d64eb":"markdown","de06bb0d":"markdown","0bc17f2c":"markdown","39b1e23d":"markdown","d83b2733":"markdown","c662e6cc":"markdown","09eae8ae":"markdown","5755ed37":"markdown"},"source":{"e54e65b0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample, shuffle\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, learning_curve, train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix ,accuracy_score\nfrom sklearn.metrics import precision_score, recall_score","eb91e4fe":"data = pd.read_csv('..\/input\/KaggleV2-May-2016.csv')\ndata.head()","0a0b9eb2":"n_rows = data.shape[0]\nn_cols = data.shape[1]\nprint('The dataset has', n_rows, 'rows and', n_cols, 'features')","334e0791":"data['No-show'] = data['No-show'].replace({'No':0, 'Yes':1})","24eab051":"# engineer feature missed_appointment before\nmissed_appointment = data.groupby('PatientId')['No-show'].sum()\nmissed_appointment = missed_appointment.to_dict()\ndata['missed_appointment_before'] = data.PatientId.map(lambda x: 1 if missed_appointment[x]>0 else 0)\ndata['missed_appointment_before'].corr(data['No-show'])","e5a3d944":"patients = data.drop_duplicates(subset=['PatientId'])\nn_patients = patients.shape[0]\ncount = patients.Gender.value_counts()\nn_women = count.values[0]\nn_men = count.values[1]\nprint('Proportion of the dataset that are women: {0:2.1f}%'.format(100*n_women\/n_patients))\nprint('Proportion of the dataset that are men: {0:2.1f}%'.format(100*n_men\/n_patients))","75a9c6af":"#separate in 2 groups\nshow = data[data['No-show']==0]\nno_show = data[data['No-show']==1]\nn_show = show.shape[0]\nn_no_show = no_show.shape[0]\nprint('Percentage of persons that do not miss appointments:{0: 2.2f}%'.format(100*n_show\/n_rows))\nprint('Percentage of persons that miss appointments:{0: 2.2f}%'.format(100*n_no_show\/n_rows))","012d6394":"def map_waiting_interval_to_days(x):\n    '''\n    Receives an integer representing the number of days until an appointment and\n    returns the category it is in.\n    '''\n    if x ==0 :\n        return 'Less than 15 days'\n    elif x > 0 and x <= 2:\n        return 'Between 1 day and 2 days'\n    elif x > 2 and x <= 7:\n        return 'Between 3 days and 7 days'\n    elif x > 7 and x <= 31:\n        return 'Between 7 days and 31 days'\n    else:\n        return 'More than 1 month'","41ae531e":"d = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\ndata['mapped_AppointmentDay'] = data['AppointmentDay'].map(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\"))\ndata['mapped_ScheduledDay'] = data['ScheduledDay'].map(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\"))\ndata['waiting_interval'] = abs(data['mapped_ScheduledDay'] - data['mapped_AppointmentDay'])\ndata['waiting_interval_seconds'] = data['waiting_interval'].map(lambda x: x.seconds)\ndata['waiting_interval_days'] = data['waiting_interval'].map(lambda x: x.days)\ndata['waiting_interval_days'] = data['waiting_interval_days'].map(lambda x: map_waiting_interval_to_days(x))\n\ndata['ScheduledDay_month'] = data['mapped_ScheduledDay'].map(lambda x: x.month)\ndata['ScheduledDay_day'] = data['mapped_ScheduledDay'].map(lambda x: x.day)\ndata['ScheduledDay_weekday'] = data['mapped_ScheduledDay'].map(lambda x: x.weekday())\n\ndata['AppointmentDay_month'] = data['mapped_AppointmentDay'].map(lambda x: x.month)\ndata['AppointmentDay_day'] = data['mapped_AppointmentDay'].map(lambda x: x.day)\ndata['AppointmentDay_weekday'] = data['mapped_AppointmentDay'].map(lambda x: x.weekday())\ndata['AppointmentDay_weekday'] = data['AppointmentDay_weekday'].replace(d)\n\n# separate in 2 groups\nshow = data[data['No-show']==0]\nno_show = data[data['No-show']==1]\nn_show = show.shape[0]\nn_no_show = no_show.shape[0]","4fc608d5":"levels = ['Less than 15 days','Between 1 day and 2 days','Between 3 days and 7 days',\n'Between 7 days and 31 days','More than 1 month']\n\ngrouped = show.groupby(by='waiting_interval_days')\ncount_days1 = grouped.waiting_interval_days.count().reindex(index = levels)\ncount_days1 = 100*count_days1\/show.shape[0]\n\ngrouped = no_show.groupby(by='waiting_interval_days')\ncount_days2 = grouped.waiting_interval_days.count().reindex(index = levels)\ncount_days2 = 100*count_days2\/no_show.shape[0]\n\nsns.set_style(\"whitegrid\")\nf, ax = plt.subplots(1, 2,figsize=(12, 4),sharey=True)\ng1 = sns.barplot(x=count_days1.index, y=count_days1.values, \n                color='lightblue',ax=ax[0])\ng1.set_xticklabels(levels, rotation=90);\ng1.set_xlabel('');\ng1.set_title('Show');\ng2 = sns.barplot(x=count_days2.index, y=count_days2.values, \n                color='salmon',ax=ax[1])\ng2.set_xticklabels(levels, rotation=90);\ng2.set_xlabel('');\ng2.set_title('No-Show');","3afa9554":"def map_age(x):\n    '''\n    Receives an integer and returns the age category that this age is in.\n    '''\n    if x < 12:\n        return 'Child'\n    elif x > 12 and x < 18:\n        return 'Teenager'\n    elif x>=20 and x<25:\n        return 'Young Adult'\n    elif x>=25 and x<60:\n        return 'Adult'\n    else:\n        return 'Senior'\ndata['mapped_Age'] = data['Age'].map(lambda x: map_age(x))\npatients['mapped_Age'] = patients['Age'].map(lambda x: map_age(x))","64486d08":"ages = ['Child','Teenager','Young Adult','Adult','Senior']\nn_patients = patients.shape[0]\ngrouped = patients.groupby(by='mapped_Age')\ncount_ages = grouped.Age.count().reindex(index = ages)\ng = sns.barplot(x=count_ages.index, y=count_ages.values*(100\/n_patients), color='lightblue');\ng.set_title('Age percentage');\ng.set_xlabel('');","63bc3fce":"show = data[data['No-show']==0]\nno_show = data[data['No-show']==1]\nn_show = show.shape[0]\nn_no_show = no_show.shape[0]\nages = ['Child','Teenager','Young Adult','Adult','Senior']\n\n# count ages for group which didnt miss appointment\ngrouped = show.groupby(by='mapped_Age')\ncount_ages1 = grouped.Age.count().reindex(index = ages)\ncount_ages1 = count_ages1*(100\/show.shape[0])\n\n# count ages for group which missed appointment\ngrouped = no_show.groupby(by='mapped_Age')\ncount_ages2 = grouped.Age.count().reindex(index = ages)\ncount_ages2 = count_ages2*(100\/no_show.shape[0])\n\nsns.set_style(\"whitegrid\")\nf, ax = plt.subplots(1, 2,figsize=(12, 4),sharey=True)\ng1 = sns.barplot(x=count_ages1.index, y=count_ages1.values, \n            color='lightblue',ax=ax[0])\ng1.set_xlabel('');\ng1.set_title('Show');\ng2 = sns.barplot(x=count_ages2.index, y=count_ages2.values, \n            color='salmon',ax=ax[1]);\ng2.set_xlabel('');\ng2.set_title('No-Show');","d2bf561e":"patients = data.drop_duplicates(subset=['PatientId'])\npatients[['Hipertension','Diabetes','Alcoholism','Handcap']].sum(axis=0)\/patients.shape[0]","105bffe5":"data['haveDisease'] = data.Alcoholism | data.Handcap | data.Diabetes | data.Hipertension","2ef25bb9":"fig, ax = plt.subplots(figsize=[15,10])\ndata = data.drop(columns=['AppointmentID', 'PatientId'])\ncor=data.corr()\nmask = np.zeros_like(cor)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nax = sns.heatmap(cor, xticklabels=cor.columns, yticklabels=cor.columns, \n            annot=True, cmap=cmap, mask=mask);","5974b9ad":"# Check for missing values\ndata.isnull().sum().any()","01cff040":"def process_data(data):\n    '''\n    Receives the dataset, clean data and engineer new features. \n    Return cleaned dataset with features that will be used for training model.\n    '''\n    d = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n    data['mapped_AppointmentDay'] = data['AppointmentDay'].map(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\"))\n    data['mapped_ScheduledDay'] = data['ScheduledDay'].map(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\"))\n    data['waiting_interval'] = abs(data['mapped_ScheduledDay'] - data['mapped_AppointmentDay'])\n    data['waiting_interval_days'] = data['waiting_interval'].map(lambda x: x.days)\n    data['waiting_interval_days'] = data['waiting_interval_days'].map(lambda x: map_waiting_interval_to_days(x))\n    \n    data['ScheduledDay_month'] = data['mapped_ScheduledDay'].map(lambda x: x.month)\n    data['ScheduledDay_day'] = data['mapped_ScheduledDay'].map(lambda x: x.day)\n    data['ScheduledDay_weekday'] = data['mapped_ScheduledDay'].map(lambda x: x.weekday())\n    data['ScheduledDay_weekday'] = data['ScheduledDay_weekday'].replace(d)\n\n    data['AppointmentDay_month'] = data['mapped_AppointmentDay'].map(lambda x: x.month)\n    data['AppointmentDay_day'] = data['mapped_AppointmentDay'].map(lambda x: x.day)\n    data['AppointmentDay_weekday'] = data['mapped_AppointmentDay'].map(lambda x: x.weekday())\n    data['AppointmentDay_weekday'] = data['AppointmentDay_weekday'].replace(d)\n    \n    data['No-show'] = data['No-show'].replace({'Yes':1, 'No':0})\n   \n    missed_appointment = data.groupby('PatientId')['No-show'].sum()\n    missed_appointment = missed_appointment.to_dict()\n    data['missed_appointment_before'] = data.PatientId.map(lambda x: 1 if missed_appointment[x]>0 else 0)\n    data['mapped_Age'] = data['Age'].map(lambda x: map_age(x))\n    data['Gender'] = data['Gender'].replace({'F':0, 'M':1})\n    data['haveDisease'] = data.Alcoholism | data.Handcap | data.Diabetes | data.Hipertension\n\n    data = data.drop(columns=['waiting_interval', 'AppointmentDay', 'ScheduledDay',\n                             'PatientId','Age', 'mapped_ScheduledDay',\n                             'mapped_AppointmentDay', 'AppointmentID', \n                              'Alcoholism','Handcap','Diabetes','Hipertension'])\n    \n    return data","66fab615":"def one_hot_encode(data):\n    return pd.get_dummies(data)","42eb2aa2":"data = pd.read_csv('..\/input\/KaggleV2-May-2016.csv')\nprocessed_data = process_data(data)\nprocessed_data.head()","22e467f8":"encoded_data = one_hot_encode(processed_data)\nencoded_data.head()","5dd0ca7f":"print('All data - Naive predictor accuracy: {:2.2f}%'.format(100 - (100*encoded_data['No-show'].sum()\/encoded_data.shape[0])))","7214fb3a":"# row: true label ; columns: predictions\ntn, fp, fn, tp = confusion_matrix(encoded_data['No-show'], np.zeros(encoded_data.shape[0])).ravel()\n(tn, fp, fn, tp)","d8a5a066":"X = encoded_data.drop(columns='No-show')\ny = encoded_data['No-show']","982bdd8f":"scaler = MinMaxScaler()\nX_std = scaler.fit_transform(X)","00a066dd":"X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.30, random_state=42)\n\nprint('Train size:{}'.format(X_train.shape))\nprint('Test size:{}'.format(X_test.shape))","f4a57e05":"clf1 = RandomForestClassifier(random_state = 0)\nclf1.fit(X_train, y_train)\ny_preds = clf1.predict(X_test)","e25b4d4a":"print('RF - Accuracy: {:2.2f}%'.format(accuracy_score(y_test, y_preds) * 100))\nprint('RF - Precision score: {:2.2f}%'.format(precision_score(y_test, y_preds)*100))\nprint('RF - Recall score: {:2.2f}%'.format(recall_score(y_test, y_preds)*100))\nprint('RF - F1-score: {:2.2f}%'.format(f1_score(y_test, y_preds) * 100))","854c8ff8":"tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()\nconfusion_matrix(y_test, y_preds)","7ea4e34e":"clf1 = GradientBoostingClassifier(random_state = 0)\nclf1.fit(X_train, y_train)\ny_preds = clf1.predict(X_test)\nprint('GB - Accuracy: {:2.2f}%'.format(accuracy_score(y_test, y_preds) * 100))\nprint('GB - Precision score: {:2.2f}%'.format(precision_score(y_test, y_preds)*100))\nprint('GB - Recall score: {:2.2f}%'.format(recall_score(y_test, y_preds)))\nprint('GB - F1-score: {:2.2f}%'.format(f1_score(y_test, y_preds) * 100))","511dffc3":"tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()\nconfusion_matrix(y_test, y_preds)","ef67398a":"print(processed_data.shape)\nprint(encoded_data.shape)\n\n# separates data in two groups\nshow = data[data['No-show']==0]\nno_show = data[data['No-show']==1]\nn_show = show.shape[0]\nn_no_show = no_show.shape[0]\nprint('Proportion of minority class: {:2.2f}%'.format(100*no_show.shape[0]\/n_rows))\nprint('Proportion of majority class: {:2.2f}%'.format(100*show.shape[0]\/n_rows))","ea27bfb2":"no_show.shape[0]\/show.shape[0] # 1 positive no-show for 4 negatives no-show","8bd4c4de":"# Calculates how many samples we resample from the majority class\ndownsampling_factor = 4\nn_samples = round(show.shape[0]\/downsampling_factor)\nn_samples","194c232c":"# separate classes\ndf_majority = encoded_data[encoded_data['No-show']==0]\ndf_minority = encoded_data[encoded_data['No-show']==1]\n \n# downsample without replacement majority class\ndf_majority_downsampled = resample(df_majority, replace=False, n_samples=n_samples, random_state=0)  \n \n# save the index to use when perform the upweight of the samples\nresampled_index = df_majority_downsampled.index\n\n# combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n\n# shuffle dataframe\ndf_downsampled = shuffle(df_downsampled)\n\n# take a look into proportion after resampling\ndf_majority = df_downsampled[df_downsampled['No-show']==0]\ndf_minority = df_downsampled[df_downsampled['No-show']==1]\n\nprint('Proportion of minority class after downsampling: {:2.2f}%'.format(100*df_minority.shape[0]\/df_downsampled.shape[0]))\nprint('Proportion of majority class after downsampling: {:2.2f}%'.format(100*df_majority.shape[0]\/df_downsampled.shape[0]))","8ad7fc9a":"# create column of original index before resampling\ndf_downsampled = df_downsampled.reset_index()\n\n# get the weights\nweight_factor = round(downsampling_factor)\nweights = np.ones(df_downsampled.shape[0])\n\n# iforiginal index is in resampled_index change weight\ncols = df_downsampled.columns\ndf_downsampled.columns = ['original_index'] + list(cols)[1:]\ndf_downsampled['weight'] = df_downsampled.original_index.map(lambda x: weight_factor if x in resampled_index else 1).values","500a8768":"X = df_downsampled.drop(columns='No-show')\ny = df_downsampled['No-show']","b5a95c14":"scaler = MinMaxScaler()\nX_to_scale = X.drop(columns=['original_index', 'weight'])\ncols = X_to_scale.columns\nX_std = scaler.fit_transform(X_to_scale)\nX_std = pd.DataFrame(X_std)\n\n# reappend 'original_index', 'weight' to the dataframe\nX_std = X_std.join(X[['original_index', 'weight']])\nX_std.columns = list(cols) + ['original_index', 'weight']","bda6d584":"X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.30, random_state=0)\n\nprint('Train size:{}'.format(X_train.shape))\nprint('Test size:{}'.format(X_test.shape))","30588959":"weights = X_train.weight\nX_train = X_train.drop(columns=['original_index','weight'])\nX_test = X_test.drop(columns=['original_index','weight'])","5c122245":"clf1 = RandomForestClassifier(random_state = 0)\nclf1.fit(X_train, y_train, sample_weight=weights)\ny_preds = clf1.predict(X_test)\nprint('RF - Accuracy: {:2.2f}%'.format(accuracy_score(y_test, y_preds) * 100))\nprint('RF - Precision score: {:2.2f}%'.format(precision_score(y_test, y_preds)*100))\nprint('RF - Recall score: {:2.2f}%'.format(recall_score(y_test, y_preds)*100))\nprint('RF - F1-score: {:2.2f}%'.format(f1_score(y_test, y_preds) * 100))","c8359143":"tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()\nconfusion_matrix(y_test, y_preds)","8084def4":"clf = GradientBoostingClassifier(random_state=0)\nclf.fit(X_train, y_train)\ny_preds = clf.predict(X_test)\nacc = accuracy_score(y_test, y_preds) * 100\nprecision = precision_score(y_test, y_preds)*100\nrecall = recall_score(y_test, y_preds)*100\nf_score = f1_score(y_test, y_preds) * 100\n\nprint('GB - Accuracy: {:2.2f}%'.format(acc))\nprint('GB - Precision score: {:2.2f}%'.format(precision))\nprint('GB - Recall score: {:2.2f}%'.format(recall))\nprint('GB - F1-score: {:2.2f}%'.format(f_score))","49de8276":"tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()\nconfusion_matrix(y_test, y_preds)","c3d714d1":"#http:\/\/benalexkeen.com\/bar-charts-in-matplotlib\/\n%matplotlib inline\n\nx = ['Acc', 'Precision','Recall', 'F1']\nmetrics = [acc, precision, recall, f_score]\nplt.gca().yaxis.grid(True)\nx_pos = [i for i, _ in enumerate(x)]\nplt.bar(x_pos, metrics, color='lightblue')\nplt.xlabel(\"Metrics\")\nplt.ylabel(\"Percentage\")\nplt.xticks(x_pos, x)\nplt.show()","3df790cc":"### Downsampling and upweighting approach\nFollow the steps of https:\/\/developers.google.com\/machine-learning\/data-prep\/construct\/sampling-splitting\/imbalanced-data before using RF (image from link)\n![](https:\/\/developers.google.com\/machine-learning\/data-prep\/images\/downsampling-upweighting-v5.svg)    ","40772f1b":"**Does the waiting time until the appointment matter?**\n    - use features ScheduledDay and AppointmentDay","eb9885e4":"### Data understanding","c5e012c4":"#### Using dataset without dealing with the class imbalance","6d172e0b":"- The accuracy is 86%, which is small improvement from the 79.81% of the naive classifier (predict 0 for every entry). Since accuracy can be misleading, so we take a look in other models\n\n- The F1-score is at 65%, which is low. Lets try a model more robust to class imbalance.","867f8cad":"- We can see that the naive model does not classify correctly any of out positivies entries. The 22319 false negatives (this model have poor recall)\n\n- We want to retrieve individues that will miss the appointment (our True Positives) and also avoid classify persons that will show up as a no-show. We need a metric that takes precision and recall into consideration, therefore F1 score seems a good choice.","a6ee95aa":"### Model","8da7049e":"- Seems an improvement compared with the previous F1-score: 65.04% of RF without downsampling","d8fe3db0":"**Does age matter?**\t","02be7410":"- This mean that for each sample with no-show==1 there is 4 samples with no-show==0.\n- The downsampling factor of 4 is used, then after downsampling the classes proportion will be almost the same","c21d6526":"-  The F1-score for this model is at 72% which was an improve from the RF model.\n- In the next section we will try some approaches to deal with the imbalance of the dataset and see if we can improve this.","942c59cd":"### Data preparation\n    - check for missing data\n    - create new features and process data\n    - one hot encode categorical features","f18d64eb":"**1. Naive predictor:**\n    - predict class with majority (No-show==0) for all cases","de06bb0d":"**Scale data and split data into training and testing**\n    - scale all features except 'original_index', 'weight' which are used to calculate the weights to pass as sample_weight parameter of Random Forest method .fit()","0bc17f2c":"**Does miss an appointment before influences on missing again?**\n","39b1e23d":"### RF","d83b2733":"**Check correlation of the features**","c662e6cc":"**Is gender important?**","09eae8ae":"**Hipertension, Diabetes, Alcoholism, Handcap**","5755ed37":"**Upweight the samples**\n    - {example_weight} = {original_weight} x {downsampling_factor}"}}