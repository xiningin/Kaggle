{"cell_type":{"8d2bff23":"code","7d21f67d":"code","1b4b0cd7":"code","9c8f6f3f":"code","65e04d60":"code","9f6487ae":"code","2ef0eb98":"code","cf4b0992":"code","19b773dd":"code","db4a5927":"code","13f9ff02":"code","d11467d2":"code","cf1851fd":"code","07b0c712":"code","e04f2664":"code","7df71df1":"code","7efaba2d":"code","ce9067c1":"code","167d865a":"code","2c56c53e":"code","f6799735":"code","945bc171":"code","35f025d6":"code","c20b8070":"code","2319690d":"code","f7dc7003":"code","23a7d4b4":"markdown","d83cc32f":"markdown","354294b7":"markdown","c3a63908":"markdown","8d0e02b2":"markdown","152b1f25":"markdown","20c53dc3":"markdown","11bbcae0":"markdown","d57a7b43":"markdown","11ee7f36":"markdown","61327691":"markdown","87511da3":"markdown","fbb225e9":"markdown","5a5e608e":"markdown","5b0c5515":"markdown","de738543":"markdown","915834a2":"markdown","57a74618":"markdown","ec4c7c8f":"markdown","0600c107":"markdown","6416635e":"markdown","07fd4660":"markdown","df38fd30":"markdown","b335c045":"markdown","dd6cc523":"markdown"},"source":{"8d2bff23":"import numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom pytorch_zoo import *","7d21f67d":"# Fake data of examples of different lengths, padded with zeros\ndata = [[2, 3, 44, 21, 89, 0, 0, 0],\n           [45, 22, 89, 0, 0, 0, 0, 0],\n           [21, 67, 43, 76, 28, 29, 90, 32],\n           [45, 22, 89, 0, 0, 0, 0, 0],\n           [45, 22, 62, 80, 89, 0, 0, 0],\n           [21, 67, 43, 76, 28, 29, 90, 32]]\ndata = np.array(data).astype(float)\ndata = torch.from_numpy(data)","1b4b0cd7":"# Create the dataset\ntrain_dataset = torch.utils.data.TensorDataset(data)\n\n# Create the dynamic sampler\nsampler = torch.utils.data.RandomSampler(train_dataset)\n\nsampler = DynamicSampler(sampler, batch_size=2, drop_last=False)\n\n# Create the dataloader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler)\n\nfor epoch in range(10):\n    for batch in train_loader:\n        batch = trim_tensors(batch)\n\n        # train_batch(...)","9c8f6f3f":"# Fake data\nlogits = torch.rand((4, 3, 128, 128))\nlabels = torch.ones((4, 3, 128, 128))","65e04d60":"loss = lovasz_hinge(logits, labels)\nprint(loss)","9f6487ae":"criterion = DiceLoss()\nloss = criterion(logits, labels)\nprint(loss)","2ef0eb98":"class Model(nn.Module):    \n    def __init__(self, in_ch, out_ch):\n        super(Model, self).__init__()\n        \n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1)\n        self.SE = SqueezeAndExcitation(out_ch, r=16)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.SE(x)\n        \n        return x","cf4b0992":"# Fake data\nx = torch.rand((1, 3, 128, 128))\n\n# Create the model\nmodel = Model(3, 64)\n\nout = model(x)\n\nprint(out.shape)","19b773dd":"class Model(nn.Module):    \n    def __init__(self, in_ch, out_ch):\n        super(Model, self).__init__()\n        \n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1)\n        self.sSE = ChannelSqueezeAndSpatialExcitation(out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.sSE(x)\n        \n        return x","db4a5927":"# Fake data\nx = torch.rand((1, 3, 128, 128))\n\n# Create the model\nmodel = Model(3, 64)\n\nout = model(x)\n\nprint(out.shape)","13f9ff02":"class Model(nn.Module):    \n    def __init__(self, in_ch, out_ch):\n        super(Model, self).__init__()\n        \n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1)\n        self.scSE = ConcurrentSpatialAndChannelSqueezeAndChannelExcitation(out_ch, r=16)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.scSE(x)\n        \n        return x","d11467d2":"# Fake data\nx = torch.rand((1, 3, 128, 128))\n\n# Create the model\nmodel = Model(3, 64)\n\nout = model(x)\n\nprint(out.shape)","cf1851fd":"class Model(nn.Module):    \n    def __init__(self, in_ch, out_ch):\n        super(Model, self).__init__()\n        \n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1)\n        self.gaussian_noise = GaussianNoise(0.1)\n\n    def forward(self, x):\n        x = self.conv(x)\n\n        if self.training:\n            x = self.gaussian_noise(x)\n        \n        return x","07b0c712":"# Fake data\nx = torch.rand((1, 3, 128, 128))\n\n# Create the model\nmodel = Model(3, 64)\n\nout = model(x)\n\nprint(out.shape)","e04f2664":"optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\nscheduler = CyclicMomentum(optimizer)\n\ntrain_dataset = torch.utils.data.TensorDataset(data)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2)\n\nfor epoch in range(10):\n    for batch in train_loader:\n        scheduler.batch_step()\n\n        # train_batch(...)","7df71df1":"notify({'value1': 'Notification title', 'value2': 'Notification body'}, key=[YOUR_PRIVATE_KEY_HERE])","7efaba2d":"seed_environment(42)","ce9067c1":"gpu_usage(device, digits=4)","167d865a":"print(n_params(model))","2c56c53e":"save_model(model, fold=0)","f6799735":"model = load_model(model, fold=0)","945bc171":"save(data, 'data.pkl')","35f025d6":"data = load('data.pkl')","c20b8070":"logits = torch.rand((1, 256))\nmask = torch.ones((1, 256))","2319690d":"out = masked_softmax(logits, mask, dim=-1)\nprint(out.shape)","f7dc7003":"out = masked_log_softmax(logits, mask, dim=-1)\nprint(out.shape)","23a7d4b4":"### seed_environment\n\nSet random seeds for python, numpy, and pytorch to ensure reproducible research.\n\n_Arguments_:  \n`seed` (int): The random seed to set.","d83cc32f":"### save_model()\n\nSave a trained pytorch model on a particular cross-validation fold to disk.\n\nImplementation adapted from https:\/\/github.com\/floydhub\/save-and-resume.\n\n_Arguments_:  \n`model` (nn.Module): The model to save.  \n`fold` (int): The cross-validation fold the model was trained on.\n","354294b7":"### load()\n\nLoad an object saved to disk with `save`.\n\n_Arguments_:  \n`path` (String): The path to the saved object.\n\n_Returns_:  \n(Object): The loaded object.","c3a63908":"### ConcurrentSpatialAndChannelSqueezeAndChannelExcitation()\n\nThe scSE (Concurrent Spatial and Channel Squeeze and Channel Excitation) block from the [Concurrent Spatial and Channel \u2018Squeeze & Excitation\u2019 in Fully Convolutional Networks](https:\/\/arxiv.org\/abs\/1803.02579) paper.\n\nImplementation adapted from https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/66178\n\n_Arguments_:  \n`in_ch` (int): The number of channels in the feature map of the input.  \n`r` (int): The reduction ratio of the intermidiate channels. Default: 16.\n\n_Shape_:\n\n-   Input: (batch, channels, height, width)\n-   Output: (batch, channels, height, width) (same shape as input)","8d0e02b2":"### n_params()\n\nReturn the number of parameters in a pytorch model.\n\n_Arguments_:  \n`model` (nn.Module): The model to analyze.\n\n_Returns_:  \n(int): The number of parameters in the model.\n","152b1f25":"### DiceLoss()\n\nThe dice loss for semantic segmentation\n\nImplementation adapted from https:\/\/www.kaggle.com\/soulmachine\/siim-deeplabv3\n\n_Shape_:\n\n-   Input:\n    -   logits: (batch, \\*)\n    -   targets: (batch, \\*) _same as logits_\n-   Output: (1)\n\n_Returns_:  \n(torch.tensor): The dice loss","20c53dc3":"### save()\n\nSave an object to disk.\n\n_Arguments_:  \n`obj` (Object): The object to save.  \n`filename` (String): The name of the file to save the object to.","11bbcae0":"### lovasz_hinge() Loss\n\nThe binary Lovasz Hinge loss for semantic segmentation.\n\nImplementation adapted from https:\/\/github.com\/bermanmaxim\/LovaszSoftmax\n\n_Arguments_:  \n`logits` (torch.tensor): Logits at each pixel (between -\\infty and +\\infty).  \n`labels` (torch.tensor): Binary ground truth masks (0 or 1).  \n`per_image` (bool, optional): Compute the loss per image instead of per batch. Defaults to True.\n\n_Shape_:\n\n-   Input:\n    -   logits: (batch, height, width)\n    -   labels: (batch, height, width)\n-   Output: (batch)\n\n_Returns_:  \n(torch.tensor): The lovasz hinge loss","d57a7b43":"## Importing\n\nImport `pytorch_zoo` into your kernel by adding it as a utility script from the file menu, then import it as shown below.","11ee7f36":"# Losses","61327691":"### gpu_usage()\n\nPrints the amount of GPU memory currently allocated in GB.\n\n_Arguments_:  \n`device` (torch.device, optional): The device you want to check. Defaults to device.  \n`digits` (int, optional): The number of digits of precision. Defaults to 4.","87511da3":"### GaussianNoise()\n\nA gaussian noise module.\n\n_Arguments_:  \n`stddev` (float): The standard deviation of the normal distribution. Default: 0.1.\n\n_Shape_:\n\n-   Input: (batch, \\*)\n-   Output: (batch, \\*) (same shape as input)","fbb225e9":"# Schedulers","5a5e608e":"# Modules","5b0c5515":"### notify()\n\nSend a notification to your phone with IFTTT\n\nSetup a IFTTT webhook with https:\/\/medium.com\/datadriveninvestor\/monitor-progress-of-your-training-remotely-f9404d71b720\n\n_Arguments_:  \n`obj` (Object): Object to send to IFTTT  \n`key` ([type]): IFTTT webhook key","de738543":"### masked_log_softmax()\n\nA masked log-softmax module to correctly implement attention in Pytorch.\n\nImplementation adapted from: https:\/\/github.com\/allenai\/allennlp\/blob\/master\/allennlp\/nn\/util.py\n\n_Arguments_:  \n`vector` (torch.tensor): The tensor to log-softmax.  \n`mask` (torch.tensor): The tensor to indicate which indices are to be masked and not included in the log-softmax operation.  \n`dim` (int, optional): The dimension to log-softmax over. Defaults to -1.\n\n_Returns_:  \n(torch.tensor): The masked log-softmaxed output\n","915834a2":"### load_model()\n\nLoad a trained pytorch model saved to disk using `save_model`.\n\n_Arguments_:\n`model` (nn.Module): The model to save.  \n`fold` (int): Which saved model fold to load.\n\n_Returns_:  \n(nn.Module): The same model that was passed in, but with the pretrained weights loaded.","57a74618":"### masked_softmax()\n\nA masked softmax module to correctly implement attention in Pytorch.\n\nImplementation adapted from: https:\/\/github.com\/allenai\/allennlp\/blob\/master\/allennlp\/nn\/util.py\n\n_Arguments_:  \n`vector` (torch.tensor): The tensor to softmax.  \n`mask` (torch.tensor): The tensor to indicate which indices are to be masked and not included in the softmax operation.  \n`dim` (int, optional): The dimension to softmax over. Defaults to -1.  \n`memory_efficient` (bool, optional): Whether to use a less precise, but more memory efficient implementation of masked softmax. Defaults to False.  \n`mask_fill_value` ([type], optional): The value to fill masked values with if `memory_efficient` is `True`. Defaults to -1e32.\n\n_Returns_:  \n(torch.tensor): The masked softmaxed output\n\n","ec4c7c8f":"# Utils","0600c107":"### SqueezeAndExcitation()\n\nThe channel-wise SE (Squeeze and Excitation) block from the [Squeeze-and-Excitation Networks](https:\/\/arxiv.org\/abs\/1709.01507) paper.\n\nImplementation adapted from https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/65939 and https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/66178\n\n_Arguments_:  \n`in_ch` (int): The number of channels in the feature map of the input.  \n`r` (int): The reduction ratio of the intermidiate channels. Default: 16.\n\n_Shape_:\n\n-   Input: (batch, channels, height, width)\n-   Output: (batch, channels, height, width) (same shape as input)\n","6416635e":"### ChannelSqueezeAndSpatialExcitation()\n\nThe sSE (Channel Squeeze and Spatial Excitation) block from the [Concurrent Spatial and Channel \u2018Squeeze & Excitation\u2019 in Fully Convolutional Networks](https:\/\/arxiv.org\/abs\/1803.02579) paper.\n\nImplementation adapted from https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/66178\n\n```python\n# in __init__()\nself.sSE = ChannelSqueezeAndSpatialExcitation(in_ch)\n\n# in forward()\nx = self.sSE(x)\n```\n\n_Arguments_:  \n`in_ch` (int): The number of channels in the feature map of the input.\n\n_Shape_:\n\n-   Input: (batch, channels, height, width)\n-   Output: (batch, channels, height, width) (same shape as input)","07fd4660":"### Dynamic batches\n\nSpeed up training NLP models with dynamic batch sizes. \n\nImplementation adapted from https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/94779 and https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/utils\/data\/sampler.py\n\n_Arguments_:  \n`sampler` (torch.utils.data.Sampler): Base sampler.  \n`batch_size` (int): Size of minibatch.  \n`drop_last` (bool): If `True`, the sampler will drop the last batch if its size would be less than `batch_size`.","df38fd30":"### CyclicalMomentum()\n\nPytorch's [cyclical learning rates](https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/optim\/lr_scheduler.py), but for momentum, which leads to better results when used with cyclic learning rates, as shown in [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https:\/\/arxiv.org\/abs\/1803.09820).\n\n_Arguments_:  \n`optimizer` (Optimizer): Wrapped optimizer.  \n`base_momentum` (float or list): Initial momentum which is the lower boundary in the cycle for each param groups. Default: 0.8  \n`max_momentum` (float or list): Upper boundaries in the cycle for each parameter group. scaling function. Default: 0.9  \n`step_size` (int): Number of training iterations per half cycle. Authors suggest setting step_size 2-8 x training iterations in epoch. Default: 2000  \n`mode` (str): One of {triangular, triangular2, exp_range}. Default: 'triangular'  \n`gamma` (float): Constant in 'exp_range' scaling function. Default: 1.0  \n`scale_fn` (function): Custom scaling policy defined by a single argument lambda function. Mode paramater is ignored Default: None  \n`scale_mode` (str): {'cycle', 'iterations'}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default: 'cycle'  \n`last_batch_iteration` (int): The index of the last batch. Default: -1","b335c045":"# Data","dd6cc523":"<h1 align='center'>\n    Pytorch Zoo \u2022 \ud83d\udd25\n<\/h1>\n\n<h4 align='center'>\n    A collection of useful modules and utilities for kaggle not available in <a href=\"https:\/\/pytorch.org\">Pytorch<\/a>\n<\/h4>\n\n-----\n\nThis is the demo notebook to showcase my python package [pytorch_zoo](https:\/\/pypi.org\/project\/pytorch-zoo) and how to use it. The utility script that I made for the [competition](https:\/\/www.kaggle.com\/general\/109651) can be found [here](https:\/\/www.kaggle.com\/bkkaggle\/pytorch-zoo)."}}