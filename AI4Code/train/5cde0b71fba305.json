{"cell_type":{"cf33aaf5":"code","8141ec80":"code","63c30a99":"code","240437c0":"code","bd50fa47":"code","5b8e6840":"code","44d764f3":"code","9ad34b1b":"code","ceb0210d":"code","5c97cd12":"code","de564d55":"code","fe365a5a":"code","f6de68d1":"code","982644a7":"code","15f7a756":"code","cbc71579":"code","4de6fcab":"code","fc2f875d":"code","ef1280a7":"code","fa69c0af":"code","5174faf6":"code","c6b7e8cd":"code","0ed23722":"code","513be5b3":"code","894f02ef":"code","863b3ee4":"code","ed97c91b":"code","b13e3f84":"code","b7d38c18":"code","3c7d1383":"markdown","a36a70bd":"markdown","3f22ecfc":"markdown","30855e84":"markdown","ef5bc441":"markdown","08ebd5bf":"markdown","645c5b91":"markdown","d9e685a2":"markdown","ddcff60e":"markdown","d746f237":"markdown","a5f12fbb":"markdown","8edaa9f7":"markdown","77eeeec0":"markdown","4011611e":"markdown","607adf6b":"markdown","97fad73c":"markdown","2db95c69":"markdown"},"source":{"cf33aaf5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split # trian and test\nfrom sklearn import metrics\nfrom sklearn import preprocessing \nfrom sklearn.metrics import classification_report\nwarnings.filterwarnings('ignore')","8141ec80":"bankrupt = pd.read_csv(\"..\/input\/banknote-authentication-uci\/BankNoteAuthentication.csv\")\nbankrupt","63c30a99":"bankrupt.describe()","240437c0":"print(bankrupt.info())\nprint(bankrupt.shape)","bd50fa47":"bankrupt.isnull().sum()","5b8e6840":"bankrupt_new = bankrupt.iloc[:,:]\nbankrupt_new","44d764f3":"bankrupt.corr()","9ad34b1b":"sns.heatmap(bankrupt.corr(), vmin = -1, vmax = 1, annot = True)","ceb0210d":"sns.countplot(x = 'class', data = bankrupt, palette = 'hls')","5c97cd12":"np.shape(bankrupt)","de564d55":"# Input\nx = bankrupt.iloc[:,:-1]\n\n# Target variable\n\ny = bankrupt.iloc[:,-1]","fe365a5a":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state = 0)","f6de68d1":"from sklearn.linear_model import LogisticRegression\n\nlogisticlassifier = LogisticRegression() ","982644a7":"logisticlassifier.fit(x_train, y_train)\n\nlogisticlassifier.coef_ # coefficients of features","15f7a756":"y_pred = logisticlassifier.predict(x_test)\ny_pred","cbc71579":"from sklearn.metrics import confusion_matrix\n\nconfusion_logist = confusion_matrix(y_test, y_pred)\n\nconfusion_logist","4de6fcab":"#Accuracy of a Model\n# Train Accuracy\n\ntrain_acc_logist = np.mean(logisticlassifier.predict(x_train)== y_train)\ntrain_acc_logist","fc2f875d":"# Test Accuracy\n\ntest_acc_logist = np.mean(logisticlassifier.predict(x_test)== y_test)\ntest_acc_logist","ef1280a7":"from sklearn.metrics import accuracy_score\n\nlogistic_acc = accuracy_score(y_test, y_pred)\nlogistic_acc","fa69c0af":"logisticlassifier.fit(x, y)\n\nlogisticlassifier.coef_ # coefficients of features","5174faf6":"y_pred = logisticlassifier.predict(x)\n\nconfusion_matrix = confusion_matrix(y, y_pred)\nconfusion_matrix","c6b7e8cd":"acc = accuracy_score(y, y_pred)\nacc","0ed23722":"logisticlassifier.score(x_test, y_test)","513be5b3":"logisticlassifier.score(x_train, y_train)","894f02ef":"from sklearn import linear_model\n\nlasso_reg = linear_model.Lasso(alpha = 50, max_iter = 100, tol =0.1)\n\nlasso_reg.fit(x_train, y_train)","863b3ee4":"lasso_reg.score(x_test, y_test)","ed97c91b":"from sklearn.linear_model import Ridge\n\nridge_reg = Ridge(alpha = 50, max_iter = 100, tol = 0.1)\n\nridge_reg.fit(x_train, y_train)","b13e3f84":"ridge_reg.score(x_test, y_test)","b7d38c18":"ridge_reg.score(x_train, y_train)","3c7d1383":"<header><h2><p style=\"background-color:#FFFFF;font-family:Georgia;color:#000000;font-size:90%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><b>Information is the oil of the 21st century, and analytics is the combustion engine.<\/b><\/p><\/h2><\/header>\n<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:60%;text-align:center;border-radius:10px 10px\"><\/p>","a36a70bd":"\n<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>As we can see the other variables are not correlated to each other<\/b><\/p>","3f22ecfc":"# L2 = Ridge regularization ","30855e84":"<h1 style=\"color:#4cd5e5;font-size:70px;font-family:Georgia;text-align:center;\"><strong>WELCOME <strong\nstyle=\"color:#f8de31;font-size:60px;font-family:Georgia;\">TO <strong style=\"color:#4cd5e5;font-size:70px;font-\nfamily:Georgia;\">Bank <strong style=\"color:#f8de31;font-size:60px;font-family:Georgia;\">Note <strong\nstyle=\"color:#4cd5e5;font-size:70px;font-family:Georgia;\">Authentication <strong style=\"color:#4cd5e5;font-size:60px;font\n-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/strong><\/strong><\/h1>\n","ef5bc441":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Basic statistics:-<\/b><\/p>","08ebd5bf":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>EDA:-<\/b><\/p>","645c5b91":"# As we can see No false positive & no false negative.","d9e685a2":"<header><p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>1. Logistic Regression<\/b><\/p><\/header> ","ddcff60e":"<h1 style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;text-align:center;\"><strong>General <strong style=\"color:#f8de31;font-size:50px;font-family:Georgia;\">:- <\/strong><\/strong><\/h1>","d746f237":"\n<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Preparing models<\/b><\/p>","a5f12fbb":"# After training the model we are trying model on test data","8edaa9f7":"# L1 Lasso","77eeeec0":"\n<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>Sliptting the data into train and test.<\/b><\/p>","4011611e":"# Accuracy of overall model","607adf6b":"From the accuracy we can say that the model is overfitted to avoid overfit problem we use Regularozation method\nhere we have L1, L2 regularization\nIt turns out they have different but equally useful properties. From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero.\nL1 = lasso regularization","97fad73c":"# let's test the performance of our model - confusion matrix","2db95c69":"<h1 style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;text-align:center;\"><strong>What <strong style=\"color:#f8de31;font-size:50px;font-family:Georgia;\">We <strong style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;\">Are <strong style=\"color:#f8de31;font-size:50px;font-family:Georgia;\">Doing <strong style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;\">In <strong style=\"color:#4cd5e5;font-size:50px;font-family:Georgia;\">This <strong style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;\">Kernel <strong style=\"color:#4cd5e5;font-size:50px;font-family:Georgia;\">? <\/strong><\/strong><\/strong><\/strong><\/strong><\/strong><\/strong><\/strong><\/h1>\n\n\n<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:60%;text-align:center;border-radius:10px 10px\"><b>--Understanding the problem statement.<br>-About the dataset.<br>-Examine the dataset.<br>--General:-<br>\n    <br>\n   1.First of 5 rows.<br>\n   2.Number of rows and columns.<br>\n   3.Column name.<br>\n   4.Basic Statistic on continous features.<br> \n   5.Finding the missing values and its treatment.<br>\n       6.Unique values in each feature.<br>\n       7.Detecting the duplicates if any.<br>\n       8.Dropping the unwanted features to reduce dimensionality and model complexity.<br>\n       9.Replacing the values.<br>\n       10.Rearranging the order of the features.<br>\n-Univariate Analysis on Discrete or continous Features.<br>-Bi-variate Analysis on Discrete or continous Features.<br>-Feature Generation.<br>-Feature & Target correlation.<br>\n    -Label encoding and scaling the dataset.<br>-Finding the number of clusters.<br>\n    -Used K means cluster to visualize the clusters.<\/b><\/p>"}}