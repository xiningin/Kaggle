{"cell_type":{"9cd00bd4":"code","4db865d2":"code","47449f35":"code","5ead14e1":"code","2ad30a6f":"code","62e8e509":"code","820239f3":"code","2554c589":"code","85a416f7":"code","2ea01ec2":"code","1b3fa448":"code","dad8f22a":"code","d8c8a436":"code","b8e87272":"code","0742314b":"code","524f8f71":"code","97cef5ec":"code","66d41a95":"code","f04eb04d":"code","5744f2e2":"code","d6d11ad6":"code","9bf28f91":"code","be5b3a80":"code","118e749e":"code","0a255d7d":"code","bcc56d93":"code","0898010f":"code","55483d23":"code","0ebd6ea4":"code","08e9d2f5":"code","dbb88b36":"code","8f296a7e":"code","12b0ca95":"code","70a8fb51":"code","41b92df7":"code","3c21135a":"code","c45fd5e3":"code","5f013a37":"code","90a59914":"code","bf471cd7":"code","14bd7a9c":"code","ab5f1c0e":"code","8da5259d":"code","4ac50733":"code","22a90bfc":"code","48c0cc0b":"code","0ca63a32":"code","3dcf68e5":"code","bc2d9106":"code","5ff1b291":"code","b486b39c":"code","379092c4":"code","24aef271":"code","773f8d0e":"code","ea5d48dc":"code","01b35fb9":"code","9df7d92c":"code","83cce358":"code","4a5538a3":"code","a4000077":"markdown","ac9fa396":"markdown","8ed1ae45":"markdown","7bd65e60":"markdown","00159632":"markdown","7b186b85":"markdown","503162f8":"markdown","248a1fbc":"markdown","a96e118b":"markdown","5dbac4fe":"markdown","0859f8ab":"markdown","5fa60038":"markdown","bff27246":"markdown","05cf73f0":"markdown","56488b3e":"markdown","f885f0f4":"markdown","6ab89378":"markdown","b889c833":"markdown","dac36273":"markdown","ff94f0d4":"markdown","651693ff":"markdown","5cce39a6":"markdown","310c280d":"markdown","2538348b":"markdown","846ab85f":"markdown","f3b0e2c8":"markdown","3129ae4b":"markdown","d10c5f4b":"markdown","afa9d07d":"markdown","596d721f":"markdown","f12df186":"markdown","274ebe07":"markdown","de1480a9":"markdown","67b57925":"markdown"},"source":{"9cd00bd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4db865d2":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","47449f35":"data.shape","5ead14e1":"data.describe()","2ad30a6f":"data.Time.hist(bins=48)\nplt.show()","62e8e509":"data['Hour'] = data['Time'].map(lambda x : x \/\/ 3600 % 24)\ndata['Day'] = data['Time'].map(lambda x: x \/\/ 3600 \/\/ 24 + 1)","820239f3":"data.head()","2554c589":"data.Hour.hist(bins=24)\nplt.show()","85a416f7":"data.Amount.value_counts().head(10)","2ea01ec2":"data.Amount.hist(bins=50)\nplt.show()","1b3fa448":"data.corr().Class.sort_values()","dad8f22a":"data_fraud = data[data.Class==1]\ndata_not = data[data.Class==0]","d8c8a436":"data_fraud.describe()","b8e87272":"data_fraud.Hour.hist(bins=24)\nplt.show()","0742314b":"data_fraud.Amount.value_counts().head(10)","524f8f71":"data_fraud.Amount.sort_values(ascending=False).head(10)","97cef5ec":"data_fraud.Amount.hist(bins=22)\nplt.show()","66d41a95":"data_fraud.describe() - data.describe()","f04eb04d":"(data_fraud.describe() - data.describe()).loc['mean'].sort_values()","5744f2e2":"(data_fraud.describe() - data.describe()).loc['std'].sort_values()","d6d11ad6":"X = data[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n       'Hour']]\ny = data['Class']","9bf28f91":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","be5b3a80":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nlog_clf = LogisticRegression()\n\nparam_grid = [{\n    'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n    'class_weight': ['balanced', None]\n}]\ngrid_search = GridSearchCV(log_clf, param_grid, cv=3, scoring='roc_auc')\ngrid_search.fit(X_scaled, y)","118e749e":"grid_search.best_params_","0a255d7d":"grid_search.best_score_","bcc56d93":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_auc_score\n\nlog_clf = LogisticRegression()\nfor i in range(1, 11, 1):\n    log_clf.C = i\/10000\n    y_score = cross_val_predict(log_clf, X_scaled, y, method='decision_function', cv=5, n_jobs=-1)\n    print('C={:.4f}'.format(log_clf.C), roc_auc_score(y, y_score))","0898010f":"log_clf = LogisticRegression()\nfor i in range(1, 11, 1):\n    log_clf.C = i\/10000 + 0.001\n    y_score = cross_val_predict(log_clf, X_scaled, y, cv=5, method='decision_function', n_jobs=-1)\n    print('C={:.4f}'.format(log_clf.C), roc_auc_score(y, y_score))","55483d23":"log_clf = LogisticRegression(C=0.0017)\ny_scores = cross_val_predict(log_clf, X_scaled, y, cv=5, method='decision_function', n_jobs=-1)","0ebd6ea4":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y, y_scores)","08e9d2f5":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)\n    plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n    plt.grid(True)","dbb88b36":"plt.figure(figsize=(8, 6))\nplot_roc_curve(fpr, tpr)\nplt.show()","8f296a7e":"from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier()","12b0ca95":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\ny_probas_rnd = cross_val_predict(rnd_clf, X_scaled, y, cv=3, method='predict_proba', n_jobs=-1)\ny_scores_rnd = y_probas_rnd[:, 1]\nfpr_rnd, tpr_rnd, thresholds_rnd = roc_curve(y, y_scores_rnd)\nroc_auc_score(y, y_scores_rnd)","70a8fb51":"rnd_clf.fit(X_scaled, y)\nfor name, score in zip(X.columns, rnd_clf.feature_importances_):\n    print(name, score)","41b92df7":"fig = plt.figure(figsize=(12, 5))\nax1 = fig.add_subplot(121, projection='3d', title='Data')\nax1.scatter(X.V17, X.V12, X.V14, c=y)\nax1.set_xlabel('V17')\nax1.set_ylabel('V12')\nax1.set_zlabel('V14')\n\n\nax2 = fig.add_subplot(122, projection='3d', title='Fraud Data Only')\nax2.scatter(X.V17[y==1], X.V12[y==1], X.V14[y==1], c='y')\nax2.set_xlabel('V17')\nax2.set_ylabel('V12')\nax2.set_zlabel('V14')\n\nplt.show()","3c21135a":"plt.plot(X.V17[y==1], X.V14[y==1], 'y.')\nplt.plot([5, -5], [-9, 2])\nplt.xlim([-30, 10])\nplt.ylim([-20, 5])\nplt.show()","c45fd5e3":"from sklearn.ensemble import IsolationForest\niso_clf = IsolationForest(n_estimators=500, random_state=42)\niso_clf.fit(X_scaled)","5f013a37":"y_pred = iso_clf.predict(X_scaled)","90a59914":"y_pred.sum()","bf471cd7":"from sklearn.svm import SVC\nsvm_clf = SVC()\ny_score = cross_val_predict(svm_clf, X_scaled, y, \n                            method='decision_function', cv=5, n_jobs=-1)\nroc_auc_score(y, y_score)","14bd7a9c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(), n_estimators=400,\n    algorithm='SAMME.R', learning_rate=0.6)\ny_score = cross_val_predict(ada_clf, X_scaled, y, \n                            method='decision_function', cv=5, n_jobs=-1)\nroc_auc_score(y, y_score)","ab5f1c0e":"import xgboost\nxgb_clf = xgboost.XGBClassifier()\nxgb_clf.fit(X_scaled, y)","8da5259d":"y_xgb_pred = xgb_clf.predict(X_scaled)","4ac50733":"from sklearn.model_selection import cross_val_score\ny_score = cross_val_score(xgb_clf, X_scaled, y, \n                            scoring='roc_auc', cv=5, n_jobs=-1)\ny_score","22a90bfc":"y_score.mean()","48c0cc0b":"xgb_clf = xgboost.XGBClassifier(tree_method = 'hist')\nfrom sklearn.model_selection import cross_val_score\ny_score = cross_val_score(xgb_clf, X_scaled, y, \n                            scoring='roc_auc', cv=5, n_jobs=-1)\ny_score","0ca63a32":"y_score.mean()","3dcf68e5":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX2D = pca.fit_transform(X_scaled)","bc2d9106":"plt.figure(figsize=(8, 6))\nplt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], 'b.')\nplt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], 'r.')\nplt.show()","5ff1b291":"from sklearn.cluster import KMeans\nk = 5\nkmeans = KMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X_scaled)","b486b39c":"fig = plt.figure(figsize=(12, 5))\nax1 = fig.add_subplot(121, projection='3d', title='Clustered Data')\nax1.scatter(X.V17, X.V12, X.V14, c=y_pred, cmap='rainbow')\nax1.set_xlabel('V17')\nax1.set_ylabel('V12')\nax1.set_zlabel('V14')\n\nax2 = fig.add_subplot(122, projection='3d', title='Data')\nax2.scatter(X.V17, X.V12, X.V14, c=y)\nax2.set_xlabel('V17')\nax2.set_ylabel('V12')\nax2.set_zlabel('V14')\n\nplt.show()","379092c4":"from sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=3, n_init=10)\ngm.fit(X_scaled)","24aef271":"FPR = []\nTPR = []\nreal_Negative = 284807 - 492\nreal_Positive = 492\ndensities = gm.score_samples(X_scaled)\nfor i in range(10000):\n    threshold = (i + 1) \/ 100\n    density_threshold = np.percentile(densities, threshold)\n    anomalies = data[densities < density_threshold]\n    TP = anomalies.Class.sum()\n    FP = anomalies.Class.count() - TP\n    FPR.append(FP \/ real_Negative)\n    TPR.append(TP \/ real_Positive)","773f8d0e":"plot_roc_curve(FPR, TPR)","ea5d48dc":"score = 0\nfor i in range(10000-1):\n    score += (TPR[i+1] + TPR[i]) * (FPR[i+1] - FPR[i]) \/ 2\nscore","01b35fb9":"from sklearn.mixture import BayesianGaussianMixture\nbgm = BayesianGaussianMixture(n_components=3, n_init=10)\nbgm.fit(X_scaled)","9df7d92c":"FPR = []\nTPR = []\nreal_Negative = 284807 - 492\nreal_Positive = 492\ndensities = bgm.score_samples(X_scaled)\nfor i in range(10000):\n    threshold = (i + 1) \/ 100\n    density_threshold = np.percentile(densities, threshold)\n    anomalies = data[densities < density_threshold]\n    TP = anomalies.Class.sum()\n    FP = anomalies.Class.count() - TP\n    FPR.append(FP \/ real_Negative)\n    TPR.append(TP \/ real_Positive)","83cce358":"plot_roc_curve(FPR, TPR)","4a5538a3":"score = 0\nfor i in range(10000-1):\n    score += (TPR[i+1] + TPR[i]) * (FPR[i+1] - FPR[i]) \/ 2\nscore","a4000077":"## XGBoost","ac9fa396":"Compare the two pictures above:\n1. Most fraud data are seperated from majority of normal data \n2. Some fraud data seem 'inside' normal data, and they are seperated from other fraud data. We can see a gap in the top left of the right picture.  ","8ed1ae45":"Top three fraud transaction amounts are 1, 0 and 99.99","7bd65e60":"Maybe it's a logarithmic normal distribution","00159632":"roc_auc_score=0.9356093000020445","7b186b85":"Seems not very good.","503162f8":"## PCA","248a1fbc":"0.9319914295505969","a96e118b":"## Logistic","5dbac4fe":"Fraud data's features are more widely spread especially for amount, V17 and V7 ","0859f8ab":"#### Conclusion:\n\n1. Logistic and xgboost are two best models with roc_auc_score above 0.97.\n2. Random Forest, SVM, Gaussian Mixtures and Bayesian Gaussian Mixture Models have roc_auc_score around 0.94, not bad.\n3. Isolation Forest, AdaBoosting, KMeans Clustering are not very good models.\n4. V17, V12, V14 are three most important features in Random Forest model and their means and std show significant differences between fraud and normal data.","5fa60038":"The mean of fraud data's V3 is much smaller than whole data, while the mean of fraud amount is higher","bff27246":"## Credit Card Data","05cf73f0":"### Anomaly Detection Using Gaussian Mixtures","56488b3e":"## AdaBoost","f885f0f4":"C=0.0017 \n\nroc_auc_score=0.9779556812415635","6ab89378":"According to description, the dataset contains two days' transactions.","b889c833":"roc_auc_score=0.9438909687225709","dac36273":"## SVM","ff94f0d4":"0.9728999017275853, our second best score, not bad","651693ff":"Fraud data shows a different frequency type. It's more likely to happen during 2am to 3am and 10am to 11am.","5cce39a6":"What? Almost all points are isolations?","310c280d":"1. V17, V12, V14 are three most impotant features in Random Forest model. About 45% importancs conbained.\n2. These three features are also top three negative correlated with Class.\n3. The means of these three features of fraud data are top 4 on fruad-normal data mean difference table.\n4. The stds of these three features of fraud data are 3 or 6 larger than those of whole data's. \n5. Since V1 to V28 are principal coponents of original data, I assume V1 to V3 contain most information, but they are not important features here.\n6. It is suprising that Amount and Hour are not so important.","2538348b":"### isolation Forest","846ab85f":"Top ten transaction amounts are all below 15, and most of them are below 2!","f3b0e2c8":"## Random Forest","3129ae4b":"0.9754203060136369. A little improvement","d10c5f4b":"V17, V14, V12, V10 are negative corralated with Class, which means the smaller these values are, the greater probability of fraud","afa9d07d":"We can see that credit card transactions are more frequent from 9 am to 23 pm, and less frequent from 1am to 7am.","596d721f":"Fraud data shows different types of means and stds from chart above","f12df186":"## Clustering","274ebe07":"## Fraud Data","de1480a9":"0.9329426746556106. Not bad.","67b57925":"### Bayesian Gaussian Mixture Models"}}