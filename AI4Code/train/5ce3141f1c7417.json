{"cell_type":{"850ed477":"code","e4cad7a4":"code","1c8aa564":"code","839e5450":"code","fb12fbdc":"code","9b14d8db":"code","9fae214a":"code","819f1de9":"code","8c08492d":"code","16f28148":"code","43bf5c75":"code","ff3d4c19":"code","6222b27a":"code","5947ab54":"code","7eb64787":"code","e7083c9a":"code","688b1117":"code","10af413f":"code","d976bd1d":"code","a4b00af7":"code","7068fc7f":"code","e55ce790":"code","b6990395":"code","1d1331fd":"code","4b31564f":"code","ec28917e":"code","72f725bc":"code","ee0771ce":"code","13dc58d5":"code","83ab115f":"code","4600aaa1":"code","fd291d7e":"markdown","53da781b":"markdown","ab29cefb":"markdown","1e89df43":"markdown","f757e334":"markdown","36591135":"markdown","75364545":"markdown","6fc4342c":"markdown","9a8e3c64":"markdown","d61c845f":"markdown","353f817b":"markdown","53a36f05":"markdown","37ce66f3":"markdown","902b9dce":"markdown","5f209419":"markdown","da272cc0":"markdown","7a539967":"markdown","ee5e1ba1":"markdown"},"source":{"850ed477":"import tensorflow as tf\n\nimport numpy as np\nimport os\nimport time","e4cad7a4":"path_to_file = '..\/input\/lyrics-generation\/lyrics_dataset.txt'","1c8aa564":"# Opening the text file in read mode and standard encoding it\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n\n# Length of text is the number of characters in it\nprint ('Length of text: {} characters'.format(len(text)))","839e5450":"# A look at the first 250 characters in text\nprint(text[:100])","fb12fbdc":"# The unique characters in the file\nvocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab)))","9b14d8db":"# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","9fae214a":"print('{ ===========>')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n==========>}')","819f1de9":"# Show how the first 20 characters from the text are mapped to integers\nprint ('{} ==> characters mapped to int ==> {}'.format(repr(text[:20]), text_as_int[:20]))","8c08492d":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n  print(idx2char[i.numpy()] , end = \"\")","16f28148":"# Using batch method converted individual characters to sequences of desired size\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n  print(repr(''.join(idx2char[item.numpy()])))","43bf5c75":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","ff3d4c19":"for input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","6222b27a":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","5947ab54":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units \nrnn_units = 1500 # keep between (1024 -> 1800) for best results ","7eb64787":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n                               \n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n  \n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n\n    tf.keras.layers.Dense(vocab_size,activation='relu'),\n    \n    tf.keras.layers.Dropout(0.2),\n  ])\n  return model","e7083c9a":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","688b1117":"for input_example_batch, target_example_batch in dataset.take(1):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","10af413f":"model.summary()","d976bd1d":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","a4b00af7":"sampled_indices","7068fc7f":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","e55ce790":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","b6990395":"model.compile(optimizer='adam', loss=loss)","1d1331fd":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","4b31564f":"# Configure it according to the loss you get at the end.\n# Ensure that the loss is between 1.4 to 1.1 for best meaningful text generation ensuring that it is always new lyrics\/text , NOT same as in the training set\n# If the loss becomes less than 1 , then a lot of same text would be generated\n# Obviously we don't want the same , but something new\n\nEPOCHS=25 ","ec28917e":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","72f725bc":"tf.train.latest_checkpoint(checkpoint_dir)","ee0771ce":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","13dc58d5":"model.summary()","83ab115f":"def generate_text(model, chars_to_generate , temp , start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = chars_to_generate\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = temp\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions \/ temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","4600aaa1":"from numpy import arange\n\n# Number of characters to generate (keep between 250 to 500)\nchars_to_generate = 500 \n\n# Printing the generated text\n# Temperature 1.0 gives the craziest output and 0.1 gives the lowest varience\n# Keeping the temperature 0.35 gives best meaningful \/ coherent text.\n\n# Give the seed string as the first word of generate text\nprint(generate_text(model , chars_to_generate , 0.35 , start_string=u\"Love \"))\n\n# Uncomment below to check the variences ==>\n\n# for i in arange(0.1,1.1,0.1):\n#   print(\"==============\")\n#   print(\"FOR TEMP : {} \".format(i))\n#   print(\"==============\")\n#   print(generate_text(model , chars_to_generate , i , start_string=u\"Love \"))\n#   print()","fd291d7e":"#### **Generating text**","53da781b":"#### **Configuring the checkpoints , ensuring that the checkpoints are saved during training**","ab29cefb":"#### **Preprocessing of text i.e from strings to numerical representation**\n##### Creating lookup-tables for char->num and num->char","1e89df43":"# **PLEASE UPVOTE THE KERNEL IF YOU LIKE IT !** \u270c","f757e334":"# **Lyrics Generation using RNN**\n### (can be generalised as coherent \/ meaningful text generation)","36591135":"#### **Creating training batches for splitting text into manageable sequences**","75364545":"#### **Generation (finally :))**","6fc4342c":"#### **Specifying the path of the text file to process**","9a8e3c64":"#### **Attaching an optimizer, and a loss function**\n###### **tf.keras.losses.sparse_categorical_crossentropy** loss function works in this case because it is applied across the last dimension of the predictions.","d61c845f":"#### **Creating training examples and targets**\nBroke the text into chunks of seq_length + 1 , if text is => \"Alright\" \n\nThen input sequence becomes => \"Alrigh\"\n\nThe output sequence becomes => \"lright\"","353f817b":"#### **Rounds of training -> EPOCHS**","53a36f05":"#### **Model Summary**","37ce66f3":"#### **Prediction Loop : the code block that generates the text**","902b9dce":"### **Buliding the Model**\n#### **Four layers were used :**\n######  **1. Embedding layer :** The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions\n######  **2. GRU layer :** A type of RNN with size units=rnn_units (LSTM could also be used here.)\n###### **3. Dense layer :** The output layer, with vocab_size outputs and 'RELU' as the activation fuction \n###### **4. Dropout layer :** Benifits regularisation and prevents overfitting  \n","5f209419":"#### **Importing TensorFlow and other libraries**","da272cc0":"#### **Opening the text file in read mode**","7a539967":"#### **Adam optimiser gives the best result hands down**","ee5e1ba1":"#### **Mapping function**"}}