{"cell_type":{"ac2d4f78":"code","75b5dd05":"code","89a166e4":"code","49b33b19":"code","7af6200d":"code","f6d19bcd":"code","649bf61f":"code","d0332f79":"code","f5bd9f21":"code","62e0e146":"code","4c76ee80":"code","1f9462e0":"code","4fe7a82b":"code","6b2386e9":"code","ad73cb8b":"markdown","d2f943b7":"markdown","d1df0e90":"markdown","418459d5":"markdown","f4d651dd":"markdown","c27499d1":"markdown","9879d9b9":"markdown","e232c5dc":"markdown","a9d35421":"markdown"},"source":{"ac2d4f78":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport SimpSOM as sps # Kohohen maps\nfrom sklearn.preprocessing import minmax_scale","75b5dd05":"df = pd.read_csv(\"..\/input\/creditcard.csv\")\ndf_array =  df.values\ndf_normalised = minmax_scale(df_array)","89a166e4":"net = sps.somNet(20, 20, df_normalised, PBC=True)\nnet.train(0.01, 10000)\nnet.nodes_graph(colnum=30)\nnet.diff_graph()","49b33b19":"from sklearn.model_selection import train_test_split, KFold","7af6200d":"X = df_array[:,0:29]\ny = df_array[:,30]\n\nX.shape","f6d19bcd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify= y)","649bf61f":"from sklearn.ensemble import IsolationForest\nfrom sklearn.metrics  import average_precision_score\nimport matplotlib.pyplot as plt","d0332f79":"kf = KFold(n_splits=3, random_state=42, shuffle=False)","f5bd9f21":"resultats = {}\n\nfor t in  [50,70,80,100,120,140]:\n    for psi in [250,300,400,600,800,1200,2000,5000]:\n        clf = IsolationForest(n_estimators = t, max_samples= psi, behaviour = \"new\", contamination = \"auto\", random_state = 42)\n        result = []\n        for train_index, test_index in kf.split(X_train):\n            clf.fit(X_train[train_index])\n            \n            score = abs(clf.score_samples(X_train[test_index]))\n            \n            result.append(average_precision_score(y_train[test_index],score))\n            \n        resultats[\"t_\"+str(t)+\"psi_\"+str(psi)] = {\n            \"t\": t,\n            \"psi\": psi,\n            \"score\": np.mean(result)\n        }","62e0e146":"## Let's find out which is the best model we trained\n\nscore_max, ind_best = (0,None)\n\nfor model in resultats :\n    if resultats[model][\"score\"] > score_max :\n        score_max  = resultats[model][\"score\"]\n        ind_best = model\n        ","4c76ee80":"## And the best model is .. : \n\nprint(\"t : \", resultats[ind_best][\"t\"], \"psi : \", resultats[ind_best][\"psi\"], \"score :\",resultats[ind_best][\"score\"])\n\nifor = IsolationForest(n_estimators = resultats[ind_best][\"t\"], max_samples= resultats[ind_best][\"psi\"], behaviour = \"new\",\n                contamination = \"auto\")\n\nifor.fit(X_train)","1f9462e0":"score  = abs(ifor.score_samples(X_test)) ","4fe7a82b":"from sklearn.metrics import precision_score, recall_score\nfrom matplotlib import pyplot as plt\n\nscore  = abs(ifor.score_samples(X_test))\n\ndef precisifun(i,X_test,y_test):\n    ind = np.argpartition(score, -int((i\/100)*len(X_test)))[-int((i\/100)*len(X_test)):]\n    pred = np.zeros(len(y_test))\n    pred[ind] = 1\n    return precision_score(y_test, pred)\n    \ndef recallfun(i,X_test,y_test):\n    ind = np.argpartition(score, -int((i\/100)*len(X_test)))[-int((i\/100)*len(X_test)):]\n    pred = np.zeros(len(y_test))\n    pred[ind] = 1\n    return recall_score(y_test, pred)\n    \nprecision = []\nrecall = []\nfor i in np.concatenate( ([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],range(1,100))):\n    precision.append(precisifun(i,X_test,y_test))\n    recall.append(recallfun(i,X_test,y_test))\n    \nplt.plot(recall, precision)\n","6b2386e9":"def precisifun(i,X_test,y_test):\n    ind = np.argpartition(score, -int((i\/100)*len(X_test)))[-int((i\/100)*len(X_test)):]\n    pred = np.zeros(len(y_test))\n    pred[ind] = 1\n    return precision_score(y_test, pred)\n    \ndef recallfun(i,X_test,y_test):\n    ind = np.argpartition(score, -int((i\/100)*len(X_test)))[-int((i\/100)*len(X_test)):]\n    pred = np.zeros(len(y_test))\n    pred[ind] = 1\n    return recall_score(y_test, pred)\n    \nprecision = []\nrecall = []\nfor i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n    precision.append(precisifun(i,X_test,y_test))\n    recall.append(recallfun(i,X_test,y_test))\n    \nplt.plot(recall, precision)","ad73cb8b":"Not Bad ! We could find a third of the fraudster using anomaly detection, for the rest of them, we may use SVM, but that'for another kernel ! ","d2f943b7":"## Cross Validation : ","d1df0e90":"## Try Anomaly Detection : Isolation Forest","418459d5":"## Precision Recall Curve : ","f4d651dd":"Were good to go ! ","c27499d1":"We don't need to normalize data with Isolation Forest. ","9879d9b9":"We have built the Kohohen maps using the target variable which is not ideal, but it is not a huge deal and we can interpret the Kohohen heatmap.\n\nAs we can see, the fraulent transactions seems concentrated in a small part of the dataset in a area quite separated from the other observations. \n\nThere is enough elements to justify the use of anomaly detection. \n\n","e232c5dc":"> As they are very few fraudulent transactions (0.17%), we should look at the performance of our model below 1% precision and recall","a9d35421":"we have normalised the data using minmax scaling because it is a good method of normalisation when we want the outliers to continue to be outliers. \n\nWe will see how the Kohohen Maps of the dataset is looking, "}}