{"cell_type":{"a9eb6f56":"code","141b7421":"code","5c2039d3":"code","41609a48":"code","6865954d":"code","9ec106af":"code","f0b76d33":"code","4b12e959":"code","83939750":"code","8358d4e9":"code","b36f499c":"code","65d15532":"code","f3e86ae5":"code","fff42b52":"code","5667c4e4":"code","7ee04afa":"code","88e3e444":"code","46375489":"code","3f05c93e":"code","c4c86584":"code","41e751c8":"code","fa3b29d4":"code","d51ba823":"code","2873894f":"code","29f3c13a":"code","3dc47b60":"code","bbb3c4a3":"code","aae326fb":"code","c05731e4":"code","2a552b7c":"code","3e5a937c":"code","0af52149":"code","71feea01":"code","cd51dd96":"code","8ab81719":"code","5910cf34":"code","11335566":"code","0bea9e1c":"code","70db850c":"code","963c2305":"code","fa0983b2":"code","c3e03301":"code","282e1ba9":"code","b97ea8d1":"code","7c73d66a":"code","2484b6ec":"code","bd1615d7":"code","7bdb8a0f":"code","3f4f4966":"code","4524c366":"code","e299796d":"code","8bad41e0":"code","1e1669f6":"code","22bfed24":"code","c53dc28b":"code","96fc9651":"code","e468a199":"code","45772ce2":"code","569d3467":"code","053a6a15":"code","c97b7181":"code","3d4c06bf":"code","05f7c727":"code","2d42e2a4":"code","54a549f2":"code","285993b2":"markdown","30f2d964":"markdown","ceebb418":"markdown","c3cfe58a":"markdown","cd545223":"markdown"},"source":{"a9eb6f56":"!ls ..\/input","141b7421":"# basic\nimport os\nimport sys\nfrom typing import Optional, List, Dict\nfrom pathlib import Path\nimport json\nimport pickle\n\n# data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5c2039d3":"datadir_name = 'tfqaconverteddata'\ndatadir = Path('..\/input\/') \/ datadir_name\noutputdir = Path('..\/output\/')\noutputdir.mkdir()\ntrain_file = 'simplified-nq-train.jsonl'\ntest_file = 'simplified-nq-test.jsonl'","41609a48":"%%time\ntrain_df = pd.read_csv(datadir \/ 'light_train.csv')\ntrain_df.shape","6865954d":"%%time\ntest_df = pd.read_json(datadir \/ test_file, orient='records', lines=True)\ntest_df.shape","9ec106af":"train_df.head()","f0b76d33":"question_texts = train_df['question_text'].values.tolist()","4b12e959":"question_texts[:5]","83939750":"# check length","8358d4e9":"word_lens_list = []\nchar_lens_list = []\n\nfor text in question_texts:\n    word_lens_list.append(len(text.split()))\n    char_lens_list.append(len(text))\nword_lens = np.array(word_lens_list)\nchar_lens = np.array(char_lens_list)","b36f499c":"word_lens.max(), word_lens.min(), word_lens.mean(), word_lens.std()","65d15532":"# word_lens histgram: most of questions are shorter than 15\nsns.distplot(word_lens, bins=26, kde=False)","f3e86ae5":"char_lens.max(), char_lens.min(), char_lens.mean(), char_lens.std()","fff42b52":"# char_lens histgram: most sentences are around 50\nsns.distplot(char_lens, bins=85, kde=False)","5667c4e4":"# some data have just 100 char length, check this\nlong_questions = np.array(question_texts)[char_lens == 100]\nlen(long_questions)","7ee04afa":"long_questions[:10]","88e3e444":"# check words\nfrom collections import Counter","46375489":"# make all word array\nwords = []\nfor text in question_texts:\n    words.extend(text.split())\nall_count = Counter(words).most_common()","3f05c93e":"# see top 20\nall_count[:20]","c4c86584":"# see bottom 50, some are misspelling and special symbols and abbreviation\nall_count[-50:]","41e751c8":"# make all word array\nhead_words = []\nfor text in question_texts:\n    head_words.append(text.split()[0])\nhead_count = Counter(head_words).most_common()","fa3b29d4":"# see top 20\nhead_count[:20]","d51ba823":"total_question = 307373\naccm = []\ncounts = [c for _, c in head_count]\nfor i, (w, cnt) in enumerate(head_count):\n    accm.append(cnt + sum(counts[:i]))\naccm = np.array(accm) \/ np.sum(counts)","2873894f":"plt.title('Accumulate Ratio for word count')\nplt.plot(range(len(head_count[:1000])), accm[:1000])","29f3c13a":"# top 20 get 80% of head words\nlist(zip(head_count[:20], accm[:20]))\n# who, what, when occupy 50%, and about 70% by where, how ","3dc47b60":"# - ['who', 'who\\'s', 'what', 'when', 'where', 'how', 'which', 'why', 'what\\'s'] or not","bbb3c4a3":"train_df['annotations'].iloc[0]","aae326fb":"# one record is a str of list of json dict\n# there are keys:\nkeys = ['yes_no_answer', 'long_answer', 'short_answers', 'annotation_id']\n# long_answer has keys:\nlong_answer_keys = ['start_token', 'candidate_index', 'end_token']\n# short_answers is a list of dict and keys:\nshort_answers_keys = ['start_token', 'end_token']","c05731e4":"# make DataFrame for annotation records\ndef annotations2value(x: str, key: str) -> Optional[str]:\n    if isinstance(x, str):\n        x = x.replace('\\'', '\"')\n    x = json.loads(x)\n    if isinstance(x, list):\n        assert len(x) == 1, f'{x} has multi answers'\n        x = x[0]\n    v = x[key] if key in x else None\n    return v\n\n\ndef short2value(x: List[Dict[str, int]], key: str) -> Optional[List[int]]:\n    if len(x) == 0:\n        return ''\n    return ','.join([str(xx[key]) for xx in x])\n\nfor k in keys:\n    train_df[k] = train_df['annotations'].apply(lambda x: annotations2value(x, k))\n    \nfor k in long_answer_keys:\n    train_df['long_' + k] = train_df['long_answer'].apply(lambda x: x[k])\n    \nfor k in short_answers_keys:\n    train_df['short_' + k] = train_df['short_answers'].apply(lambda x: short2value(x, k))\n    \ndef get_short_count(x: str):\n    if x == '':\n        return 0\n    x = x.split(',')\n    return len(x)\n\ntrain_df['short_answer_count'] = train_df['short_start_token'].apply(get_short_count)","2a552b7c":"# del train_df['annotations'], train_df['long_answer'], train_df['short_answers']\ntrain_df.head()\n# there are some some values -1 in long","3e5a937c":"# save annotation DataFrame\n# train_df.to_csv(outputdir \/ 'annotations_detail.csv', index=False)\nannotation_df = train_df\ndel train_df","0af52149":"annotation_df = pd.read_csv(outputdir \/ 'annotations_detail.csv')\nannotation_df.shape","71feea01":"# check yes_no_answer\nsns.countplot(x=annotation_df['yes_no_answer'])","cd51dd96":"# most of answer is None, check only yes_no\nsns.countplot(x=annotation_df[annotation_df['yes_no_answer'] != 'NONE']['yes_no_answer'])","8ab81719":"# next check the number of short answer\nprint(annotation_df['short_answer_count'].describe())\nsns.countplot(x=annotation_df['short_answer_count'])\n# short answer has 25 answers at max but most of them are 1 or 2, or 0.\n# 2\/3 of short answers are None.\n# what means for multiple short answer? check it later.","5910cf34":"# what value is in short_start\/end when answer is yes\/no?\nsns.countplot(x=annotation_df[annotation_df['yes_no_answer'] != 'NONE']['short_answer_count'])\n# it refer to zero (None). if the answer is yes\/no, there are no short_answer by token index.","11335566":"# how many records are zero short answer?\nannotation_df.query(\"yes_no_answer == 'NONE' and short_answer_count == 0\")['example_id'].count()","0bea9e1c":"# check length of short answer","70db850c":"# check short_start_token if or not it has anomaly","963c2305":"short_start_tokens = []\nshort_end_tokens = []\nshort_token_lengths = []\nfor start_token, end_token in annotation_df[['short_start_token', 'short_end_token']].values:\n    if start_token is np.nan and end_token is np.nan:\n        continue\n    short_start_tokens.extend([int(t) for t in start_token.split(',')])\n    short_end_tokens.extend([int(t) for t in end_token.split(',')])\nshort_start_tokens = np.array(short_start_tokens)\nshort_end_tokens = np.array(short_end_tokens)\nshort_token_lengths = short_end_tokens - short_start_tokens","fa0983b2":"short_df = pd.DataFrame(\n    zip(short_start_tokens, short_end_tokens, short_token_lengths),\n    columns=['short_start_tokens', 'short_end_tokens', 'short_token_lengths']\n)\nprint(short_df['short_start_tokens'].describe())\nprint(short_df['short_end_tokens'].describe())\nprint(short_df['short_token_lengths'].describe())\nsns.countplot(x=short_df[short_df['short_token_lengths'] < 20]['short_token_lengths'])","c3e03301":"# max length of short answer is 250, is it anomaly value?\nprint('more than 20:', short_df[short_df['short_token_lengths'] >= 20]['short_token_lengths'].count())\nprint('more than 50:', short_df[short_df['short_token_lengths'] >= 50]['short_token_lengths'].count())\nprint('more than 100:', short_df[short_df['short_token_lengths'] >= 100]['short_token_lengths'].count())\nprint('more than 150:', short_df[short_df['short_token_lengths'] >= 150]['short_token_lengths'].count())\nprint('more than 200:', short_df[short_df['short_token_lengths'] >= 200]['short_token_lengths'].count())\nprint('more than 220:', short_df[short_df['short_token_lengths'] >= 220]['short_token_lengths'].count())\n# There are 35 samples whose length are longer than 100, what is short\n# check this not short short answer later","282e1ba9":"# multiple short answer is same phrase?\n# check length of each multiple short answer\nnot_same_count = 0\nfor start_token, end_token in annotation_df[['short_start_token', 'short_end_token']].values:\n    if start_token is np.nan and end_token is np.nan:\n        continue\n    starts = np.array([int(t) for t in start_token.split(',')])\n    ends = np.array([int(t) for t in end_token.split(',')])\n    if len(starts) <= 1:\n        continue\n    diff = ends - starts\n    is_all_same = (diff == diff[0]).all()\n    if not is_all_same:\n        not_same_count += 1\n\nprint(not_same_count)\n\n# almost multiple answers are not same, I have to check the text later.","b97ea8d1":"# long answer\nannotation_df = pd.read_csv(outputdir \/ 'annotations_detail.csv')\nannotation_df.head()","7c73d66a":"# long answers have candidates and one of them are correct\n# addition to that, it has 'top_level' column which indicate the answer is included in another answer.\nannotation_df['long_answer_candidates'].iloc[0]","2484b6ec":"# firstly, check the number of candidates\nla_keys = ['start_token', 'end_token', 'top_level']\ndef la2value(key, x):\n    if isinstance(x, str):\n        x = x.replace('\\'', '\"')\n        x = x.replace('True', '1')\n        x = x.replace('False', '0')\n    x = json.loads(x)\n    values = []\n    for candidate in x:\n        values.append(int(candidate[key]))\n    values = ','.join([str(v) for v in values])\n    return values\nfor key in la_keys:\n    print('start:', key)\n    annotation_df['long_' + key + 's'] = annotation_df['long_answer_candidates'].apply(lambda x: la2value(key, x))","bd1615d7":"annotation_df.to_csv(outputdir \/ 'annotations_detail.csv', index=False)\nannotation_df.head()","7bdb8a0f":"# firstly check long answer candidates num\nannotation_df['long_candidates_num'] = annotation_df['long_start_tokens'].apply(lambda x: len(x.split(',')))\nprint(annotation_df['long_candidates_num'].describe())\n_ = plt.figure(figsize=(20, 10))\n# sns.countplot(x=annotation_df['long_candidates_num'])\nsns.countplot(x=annotation_df[annotation_df['long_candidates_num'] < 150]['long_candidates_num'])","3f4f4966":"# check the record that has no long answer. token == -1 means no long answer\nannotation_df['no_long_answer'] = annotation_df['long_start_token'] == -1\nprint(annotation_df['no_long_answer'].describe())\nsns.countplot(x=annotation_df['no_long_answer'])\n# half of data have no long answer...","4524c366":"# in case there is no long answer, there shoud be no short answer. check this.\nannotation_df[annotation_df['no_long_answer']]['short_answer_count'].describe()","e299796d":"# so then, there are\n# - 307373 record\n# - 196649 have no short answer\n# - 155225 have no long answer (off course no short answer if no long answer)","8bad41e0":"# check top_level\nannotation_df['long_top_level_1_num'] = annotation_df['long_top_levels'].apply(lambda x: sum([int(xx) for xx in x.split(',')]))\nannotation_df['long_top_level_0_num'] = annotation_df['long_top_levels'].apply(lambda x: len(x.split(',')) - sum([int(xx) for xx in x.split(',')]))","1e1669f6":"_ = plt.figure(figsize=(18, 4))\n# sns.countplot(x=annotation_df['long_top_level_0_num'])\nsns.countplot(x=annotation_df[annotation_df['long_top_level_0_num'] <= 100]['long_top_level_0_num'])\n_ = plt.figure(figsize=(18, 4))\n# sns.countplot(x=annotation_df['long_top_level_1_num'])\nsns.countplot(x=annotation_df[annotation_df['long_top_level_1_num'] <= 100]['long_top_level_1_num'])","22bfed24":"# by histgram\nsns.distplot(annotation_df[annotation_df['long_top_level_0_num'] <= 100]['long_top_level_0_num'], bins=100)\nsns.distplot(annotation_df[annotation_df['long_top_level_1_num'] <= 100]['long_top_level_1_num'], bins=100)","c53dc28b":"num0 = annotation_df['long_top_level_0_num'].sum()\nnum1 = annotation_df['long_top_level_1_num'].sum()\nsns.barplot(y=[num0, num1], x=['0', '1'])\n# about 2\/3 data are top_level = 0","96fc9651":"# annotation_df['long_correct_top_level']\nlong_correct_top_level = np.zeros(annotation_df.values.shape[0])\nfor i, target in enumerate(annotation_df['long_candidate_index'].values):\n    long_correct_top_level[i] = annotation_df['long_top_levels'].iloc[i].split(',')[target]\nannotation_df['long_correct_top_level'] = long_correct_top_level","e468a199":"sns.countplot(x=annotation_df['long_correct_top_level'])\n# most of data are top_level = 1 for only correct data\n# even though 2\/3 data are top_level = 0 plotted above","45772ce2":"# next, check long answer length\nannotation_df['long_correct_length'] = annotation_df['long_end_token'] - annotation_df['long_start_token']","569d3467":"print(annotation_df[annotation_df['long_correct_length'] >= 1]['long_correct_length'].describe())\n_ = plt.figure(figsize=(20, 6))\n_ = sns.distplot(annotation_df.query('long_correct_length > 1 & long_correct_length < 1000')['long_correct_length'], bins=100)","053a6a15":"# all long candidates\nlong_start_tokens = []\nlong_end_tokens = []\nlong_token_lengths = []\nfor start_token, end_token in annotation_df[['long_start_tokens', 'long_end_tokens']].values:\n    long_start_tokens.extend([int(t) for t in start_token.split(',')])\n    long_end_tokens.extend([int(t) for t in end_token.split(',')])\nlong_start_tokens = np.array(long_start_tokens)\nlong_end_tokens = np.array(long_end_tokens)\nlong_token_lengths = long_end_tokens - long_start_tokens","c97b7181":"long_df = pd.DataFrame(\n    long_token_lengths,\n    columns=['long_token_lengths']\n)\ndel long_start_tokens, long_end_tokens, long_token_lengths","3d4c06bf":"# print(long_df['long_start_tokens'].describe())\n# print(long_df['long_end_tokens'].describe())\nprint(long_df['long_token_lengths'].describe())","05f7c727":"_ = plt.figure(figsize=(20, 6))\n_ = sns.distplot(\n    long_df.query('long_token_lengths < 1000')['long_token_lengths'],\n    bins=100,\n    label='all_long_candidate_answer'\n)\n_ = sns.distplot(\n    annotation_df.query('long_correct_length > 1 & long_correct_length < 1000')['long_correct_length'],\n    bins=100,\n    label='correct_long_answer'\n)\n_ = plt.legend()\n# big difference in length distribution of candidates and correct answer ","2d42e2a4":"annotation_df.to_csv(outputdir \/ 'annotations_detail.csv', index=False)","54a549f2":"# todo\n# - multiple short answer text\n# - not short short answer\n# - answer check by question type","285993b2":"I check below:\n\n- question_text\n- annotations\n  - short answer\n  - long answer\n\nI use whole 'light_train.csv' data which exclude document_text because it's too large.\nthe 'light_train.csv' is available at https:\/\/www.kaggle.com\/kentaronakanishi\/tfqaconverteddata","30f2d964":"## check annotation\nWhat I check:\n- annotations column detail\n- short answers\n  - yes\/no answer\n  - short answers count\n    - statistics\n    - when yes\/no answer exists\n  - length of short answers\n  - whether multiple short answer is same or not\n- long answers\n  - candidates num\n  - target long answer existing num\n  - long answers length\n  - top level count\n  - correct top level count\n  - all answers count","ceebb418":"## check long answers\n\n- candidates num\n- target long answer existing num\n- long answers length\n- top level count\n- correct top level count","c3cfe58a":"## check short answer\n- yes\/no answer\n- short answers count\n- statistics\n- when yes\/no answer exists\n- length of short answers\n- whether multiple short answer is same or not","cd545223":"## check question_text\nWhat I check:\n- word length statistics of question text\n- char length statistics of question text\n- word count in all question text\n  - frequent words\n  - frequent head words (important for question)\n  - head word count accumulation rate"}}