{"cell_type":{"762cedf7":"code","9972dcdb":"code","e9fb4c68":"code","4a4252c5":"code","c0049aea":"code","68dd010b":"code","3f4bb1aa":"code","44baa649":"code","1897a3dd":"code","273fc229":"code","043a1cd5":"code","b215b585":"code","0c1ec236":"code","b3da9352":"code","cded5079":"code","27dacbff":"code","73ccabd1":"code","4aec4765":"code","4b58e03f":"code","5a7a507a":"code","458b4470":"code","cb1243bc":"code","50c5cabf":"code","34c2558e":"code","0faac47f":"code","77157250":"code","ee444f37":"code","856a759c":"code","2459c542":"markdown","a0a75865":"markdown","eb71cfbe":"markdown","4068c845":"markdown","6b5f2c07":"markdown","8385e685":"markdown","e620e24b":"markdown","0c501001":"markdown","4dd3a3db":"markdown","cdca4c8b":"markdown","302947c7":"markdown","38ed54af":"markdown","9b735d40":"markdown","ddf8ff60":"markdown","e11f75f2":"markdown"},"source":{"762cedf7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9972dcdb":"data = pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\",index_col = 0)\ndata.head()","e9fb4c68":"data.describe()","4a4252c5":"\ndata.shape","c0049aea":"temp = list(data.apply(np.mean,axis = 0))\ntemp2 = list(data.apply(np.median,axis = 0))\n\nmnm = pd.DataFrame([temp,temp2],columns=data.columns,index=[\"mean\",\"median\"])\nmnm","68dd010b":"import matplotlib.pyplot as plt","3f4bb1aa":"data.columns","44baa649":"plt.bar(data[\"SOP\"],data[\"Chance of Admit \"])\nplt.xlabel(\"SOP\")\nplt.ylabel(\"Chances of Admit\")\nplt.title(\"SOP Comparision\")\nplt.show()","1897a3dd":"plt.scatter(data[\"GRE Score\"],data[\"Chance of Admit \"])\nplt.show()","273fc229":"plt.hist(data[\"Chance of Admit \"],bins = 5,color = \"red\",orientation = \"horizontal\",cumulative =1)\nplt.ylabel(\"Chance of Admit\")\nplt.xlabel(\"Applications\")\nplt.show()","043a1cd5":"gr = data.groupby(\"Research\")[\"Chance of Admit \"].mean()\nlist(gr)","b215b585":"plt.bar(gr.index,list(gr))\nplt.xlabel(\"Research\")\nplt.xticks([0,1])\nplt.ylabel(\"Chances of Admit\")\nplt.title(\"Do research matter?\")\nplt.show()","0c1ec236":"X = data.iloc[:,:-1]\ny = data[[\"Chance of Admit \"]]\ny","b3da9352":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX.shape","cded5079":"def cost_function(X,y,B):\n    m = X.shape[0]\n    J = np.sum((np.dot(X,B)-y)**2)\/(2*m)\n    return J","27dacbff":"def gradient_decent(X,y,B,iterations,alpha=0.001):\n    history = [0]*iterations\n    m = X.shape[0]\n    for iteration in range(iterations):\n        h = np.dot(X,B)\n        loss = h - y\n        der = np.dot(loss,X)\/m\n        B = B - alpha*der\n        cost = cost_function(X,y,B)\n        print(f\"Iteration : {iteration}; Cost : {cost}\")\n        history[iteration] = cost\n    return B,history\n        ","73ccabd1":"X = np.c_[np.ones(len(X),dtype='int64'),X] \n","4aec4765":"m = int(X.shape[0]-(X.shape[0]*0.20))\nX_train = X[:m,:]\n\ny_train = y.iloc[:m,:].to_numpy()\ny_train= y_train.flatten()\n\nprint(X_train)\nprint(y_train)\n\nX_test = X[m:,:]\ny_test = y[m:]\n\n","4b58e03f":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n","5a7a507a":"B = np.zeros(X_train.shape[1])\nalpha = 0.005\niteratn = 1500\nupdatedB,hist = gradient_decent(X_train,y_train,B,iteratn,alpha)","458b4470":"epochs = [x for x in range(1,1501)]\nplt.plot(epochs,hist)\nplt.show","cb1243bc":"def pred(x_test, newB):\n    return x_test.dot(newB)","50c5cabf":"y_pred = pred(X_test,updatedB)\n","34c2558e":"y_pred","0faac47f":"import sklearn.metrics as sm","77157250":"print(sm.r2_score(y_test,y_pred))\nprint(y_pred.shape,y_test.shape)\n\ny_test","ee444f37":"vals = y_test.assign(predictions = y_pred)\n","856a759c":"vals","2459c542":"# Train-Test Split","a0a75865":"# Predictions","eb71cfbe":"## Visualizing the loss over iterations","4068c845":"### Cost Function","6b5f2c07":"# Model Training","8385e685":"## Gradient Descent","e620e24b":"## Standardize the data","0c501001":"# Import Dataset","4dd3a3db":">## Plotting Practice","cdca4c8b":"# Calculating R2 Score","302947c7":"# Preparing data for training","38ed54af":"### Adding Intercept","9b735d40":"## **Insights**","ddf8ff60":"### *Data Shape*","e11f75f2":"*Just some pandas practice*"}}