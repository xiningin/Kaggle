{"cell_type":{"d7815688":"code","d5abb4ca":"code","3c381f65":"code","ca664794":"code","c62b9561":"code","805923ab":"code","a7bb49aa":"code","0d3a5b3a":"code","0f943059":"code","83a6800a":"code","df9115ff":"code","7468c082":"code","a3d79dd5":"code","0f955103":"code","5b17a9ae":"code","d3908aee":"code","f3150546":"code","dd7614c9":"code","4d0acd31":"code","99805190":"code","265e201d":"code","bb7bbc44":"code","7968ea31":"markdown","41f55e71":"markdown","ef8bbea1":"markdown","7de10437":"markdown","315d169a":"markdown","6de72e48":"markdown","02dcc96f":"markdown","29b1d65d":"markdown","37798954":"markdown","df693213":"markdown"},"source":{"d7815688":"# Importar os principais pacotes\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm import tqdm_notebook as tqdm\nimport re\nimport random as rd\nimport os\nimport codecs\nimport time\nimport datetime\nimport gc\nfrom numba import jit\nfrom collections import Counter\nimport copy\nfrom typing import Any\n\nseed = 12345\nnp.random.seed(seed)\nrd.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\n\n# Evitar que aparece os warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Seta algumas op\u00e7\u00f5es no Jupyter para exibi\u00e7\u00e3o dos datasets\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)\n\n# Variavel para controlar o treinamento no Kaggle\nTRAIN_OFFLINE = False","d5abb4ca":"# Importa os pacotes de algoritmos\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nimport lightgbm as lgb\nimport xgboost as xgb \n\n# Importa os pacotes de algoritmos de redes neurais (Keras)\nimport keras\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\nfrom keras.layers import Activation\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import Callback,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras import optimizers\nfrom keras.utils import np_utils\n\n# Importa pacotes do sklearn\nfrom sklearn import preprocessing\nimport sklearn.metrics as mtr\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, log_loss, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.utils import class_weight\n\nfrom sklearn.preprocessing import StandardScaler as SS\nfrom sklearn.model_selection import train_test_split as TTS\n\nfrom keras.callbacks import ReduceLROnPlateau as RLRP\nfrom keras.callbacks import EarlyStopping as ES","3c381f65":"def read_data():\n    \n    if TRAIN_OFFLINE:\n        print('Carregando arquivo dataset_treino.csv....')\n        train = pd.read_csv('..\/dataset\/dataset_treino.csv')\n        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n\n        print('Carregando arquivo dataset_teste.csv....')\n        test = pd.read_csv('..\/dataset\/dataset_teste.csv')\n        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n\n        \n    else:\n        print('Carregando arquivo dataset_treino.csv....')\n        train = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_treino.csv')\n        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n        \n        print('Carregando arquivo dataset_treino.csv....')\n        test = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_teste.csv')\n        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n    \n    return train, test","ca664794":"# Leitura dos dados\ntrain, test = read_data()","c62b9561":"# Removendo as top 5 colunas com mais dados missing\ndrop_columns = ['v30', 'v113', 'v56', 'v3', 'v31']\ntrain = train.drop(drop_columns, axis=1)\ntest = test.drop(drop_columns, axis=1)\ntrain.shape, test.shape","805923ab":"# Label encoder nas features categoricas\nfor c in train.columns[train.dtypes == 'object']:\n    train[c] = train[c].factorize()[0]\n    \nfor c in test.columns[test.dtypes == 'object']:\n    test[c] = test[c].factorize()[0]","a7bb49aa":"# Preenche os dados missing com a media\ntrain.fillna(train.mean(),inplace=True)\ntest.fillna(train.mean(),inplace=True)","0d3a5b3a":"from boruta import BorutaPy\n\n# Separando features preditoras e target\nX = train.drop(['ID', 'target'], axis=1)\ny = train['target']\n\nX = X.values\ny = y.values\ny = y.ravel()\n\n# Define o classificador Random Forest\nrf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\nrf.fit(X, y)\n\n# Define o metodo de feature selection\nfeat_selector = BorutaPy(rf, n_estimators=100, verbose=2, random_state=1)\n\n# Procura por todas as features relevantes\nfeat_selector.fit(X, y)\n\n# Check as features selecionadas\nfeat_selector.support_\n\n# Check o ranking das features\nfeat_selector.ranking_\n\n# Chama o call transform() nos dados de treino para filtrar as features\nX_filtered = feat_selector.transform(X)\n\n# Mostra no final o shape do dataset\nX_filtered.shape","0f943059":"# Separando features preditoras e target\ntrain_x = X_filtered.copy()\ntrain_y = train['target']\ntrain_y = to_categorical(train_y)\n\n# Padronizando os dados\nscaler = StandardScaler()\ntrain_x = scaler.fit_transform(train_x)\n\n# Verificando o shape dos datasets depois dos ajustes\n# Neste momento est\u00e1 pronto para ser usado pelo treinamento\ntrain_x.shape, train_y.shape","83a6800a":"# Limpeza da mem\u00f3ria\ngc.collect()","df9115ff":"# Criando fun\u00e7\u00e3o para treinar a rede neural\ndef get_nn(x_tr,y_tr,x_val,y_val,shape):\n    K.clear_session()\n    \n    # Cria a estrutura da rede neural com 3 camadas ocultas\n    inp = Input(shape = (x_tr.shape[1],))\n\n    x = Dense(512, input_dim=x_tr.shape[1], activation='relu')(inp)\n    x = Dropout(0.5)(x)    \n    x = BatchNormalization()(x)\n\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)    \n    x = BatchNormalization()(x)\n    \n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.1)(x)    \n    x = BatchNormalization()(x)\n    \n    x = Dense(16, activation='relu')(x)\n    x = Dropout(.25)(x)\n    x = BatchNormalization()(x)\n    \n    out = Dense(2, activation='softmax')(x)\n    model = Model(inp,out)\n    \n    model.compile(optimizer = 'Adam',\n                  loss='categorical_crossentropy', \n                  metrics=['categorical_accuracy'])\n    \n    # Realiza a parada mais cedo quando percebe overfitting\n    es = EarlyStopping(monitor='val_loss', \n                       mode='min',\n                       restore_best_weights=True, \n                       verbose=1, \n                       patience=20)\n\n    # Realiza checkpoint durante o treinamento\n    mc = ModelCheckpoint('best_model.h5',\n                         monitor='val_loss',\n                         mode='min',\n                         save_best_only=True, \n                         verbose=1, \n                         save_weights_only=True)\n\n    # Realize o ajuste na Learning Rate durante o treinamento\n    rl = ReduceLROnPlateau(monitor='val_loss', \n                           factor=0.1, \n                           patience=10, \n                           verbose=1, \n                           epsilon=1e-4, \n                           mode='min')\n\n    # Realiza o fit do modelo\n    model.fit(x_tr, y_tr,\n              validation_data=[x_val, y_val],\n              callbacks=[es,mc,rl],\n              epochs=250, \n              batch_size=1024,\n              verbose=1,\n              shuffle=True)\n    \n    # Carrega os melhores pesos\n    model.load_weights(\"best_model.h5\")\n    \n    # Realiza as previs\u00f5es\n    y_pred = model.predict(x_val)\n    y_valid = y_val\n             \n    # Calcula o log loss\n    logloss = log_loss(y_valid, y_pred, eps=1e-15)\n\n    return model, logloss","7468c082":"%%time\n\n# Bloco para executar a rede neural a cada passada do KFold\n# Vamos realizar 2 loops com 5 kfolds e apurar a m\u00e9dia\nloop = 2\nfold = 5\n\n# Definindo listas que ser\u00e3o preenchidas durante o loop for\noof_nn = np.zeros([loop, train_y.shape[0], train_y.shape[1]])\nmodels_nn = []\nlogloss_csv_nn = []\n\n# Treinando o modelo\nfor k in range(loop):\n    kfold = KFold(fold, random_state = 42 + k, shuffle = True)\n    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(train_y)):\n        print(\"-----------\")\n        print(f'Loop {k+1}\/{loop}' + f' Fold {k_fold+1}\/{fold}')\n        print(\"-----------\")\n        \n        tr_x, tr_y = train_x[tr_inds], train_y[tr_inds]\n        val_x, val_y = train_x[val_inds], train_y[val_inds]\n        \n        # Train NN\n        nn, logloss_nn = get_nn(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n        models_nn.append(nn)\n        print(\"the %d fold Log-Loss (NN) is %f\"%((k_fold+1), logloss_nn))\n        logloss_csv_nn.append(logloss_nn)\n        \n        #Predict OOF\n        oof_nn[k, val_inds, :] = nn.predict(val_x)\n        \n    print(\"PARTIAL: mean Log-Loss (NN) is %f\"%np.mean(logloss_csv_nn))        ","a3d79dd5":"# Verificando o resultado m\u00e9dio do Log Loss para cada passada do Kfold\nloss_oof_nn = []\n\nfor k in range(loop):\n    loss_oof_nn.append(log_loss(train_y, oof_nn[k,...], eps=1e-15))\n    \nprint(\"M\u00e9dia log-loss:  %f\"%np.mean(logloss_csv_nn))\nprint(\"M\u00e9dia OOF log-loss: %f\"%np.mean(loss_oof_nn))","0f955103":"# Apenas para acompanhar o resultado visual\n# Exibir o treinamento somente do primeiro kfold\nplt.figure(figsize=(18, 12))\nplt.subplot(2, 1, 1)\nplt.plot(models_nn[0].history.history[\"loss\"], \"o-\", alpha=.9, label=\"loss\")\nplt.plot(models_nn[0].history.history[\"val_loss\"], \"o-\", alpha=.9, label=\"val_loss\")\nplt.axhline(1, linestyle=\"--\", c=\"C2\")\nplt.legend()\nplt.subplot(2, 1, 2)\nplt.plot(models_nn[0].history.history[\"categorical_accuracy\"], \"o-\", alpha=.9, label=\"accuracy\")\nplt.plot(models_nn[0].history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.9, label=\"val_accuracy\")\nplt.axhline(.7, linestyle=\"--\", c=\"C2\")\nplt.legend()\nplt.show()","5b17a9ae":"# Preparando os dados de teste\nnew_test = test.drop(['ID'], axis=1).values\ntest_filtered = feat_selector.transform(new_test)\ntest_filtered = scaler.fit_transform(test_filtered)\ntest_filtered.shape","d3908aee":"# Funcao para realizar as previsoes baseado em todos os modelos do Kfold\ndef predict_proba(model, x, batch_size=32, verbose=0):\n    preds = model.predict(x, batch_size, verbose)\n    if preds.min() < 0. or preds.max() > 1.:\n        warnings.warn('Network returning invalid probability values.')\n    return preds\n\ndef predict(x_te, models_nn):\n    model_num_nn = len(models_nn)\n\n    for k,m in enumerate(models_nn):\n        if k==0:\n            y_pred_nn = predict_proba(m, x_te, batch_size=1024)\n        else:\n            y_pred_nn += predict_proba(m, x_te, batch_size=1024)\n            \n    y_pred_nn = y_pred_nn \/ model_num_nn\n    return y_pred_nn","f3150546":"# Realizando as previs\u00f5es no dataset de teste\ntest_pred = predict(test_filtered, models_nn)\ntest_pred[:,1]","dd7614c9":"# Carrega o dataset de exemplo de submission e carrega as previs\u00f5es das probabilidades\nsubmission = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/sample_submission.csv')\nsubmission['PredictedProb'] = test_pred[:,1]\nprint(submission.shape)\nsubmission.head()","4d0acd31":"# Gera o arquivo de sa\u00edda para submeter no Kaggle\nsubmission.to_csv('submission_nn_v1.0.2.csv', index=False)","99805190":"# Apenas para visualizar a distribui\u00e7\u00e3o das previs\u00f5es\nsubmission['PredictedProb'].value_counts(normalize=True)","265e201d":"# Histograma com as previs\u00f5es\nplt.hist(submission.PredictedProb)\nplt.show()","bb7bbc44":"### Continua....","7968ea31":"**Vers\u00e3o 1.0.1**\n- modelo: NN com 3 camadas ocultas\n- dados missing: removido colunas com mais de 50% de NA e as demais usei a m\u00e9dia\n- features categoricas: label encoder\n- feature engineering: usando pacote Boruta (dica do Allyson)\n\n**Melhorias:**\n- Explorar os dados missing e vari\u00e1veis categ\u00f3ricas\n- Testar algumas configura\u00e7\u00f5es na quantidade de neur\u00f4nios\n- Explorar outras formas de otimiza\u00e7\u00e3o (usei o Adam)\n- Testar outros valores para Kfold e Loop (pode demorar muito o processamento)\n- Entre outros...","41f55e71":"# Submiss\u00e3o","ef8bbea1":"# Carregando os dados de treino e teste","7de10437":"# Kaggle\n## Competi\u00e7\u00e3o DSA de Machine Learning - Dezembro 2019\n\n- Teste com redes neurais multicamadas (MLP)\n- Se gostou ou achou \u00fatil, up-vote!! :)","315d169a":"# Resultado","6de72e48":"# Algoritmo Neural Network MLP","02dcc96f":"# Feature Selection","29b1d65d":"# Importando as bibliotecas","37798954":"# Previs\u00f5es","df693213":"# Feature Engineering"}}