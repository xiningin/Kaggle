{"cell_type":{"44369019":"code","8e18b4c8":"code","adbf6361":"code","decc73c2":"code","86cc5467":"code","e9b64491":"code","05f6a9f8":"code","107b6f3e":"code","04f82711":"code","8364a794":"code","d740bdcb":"code","ed408b19":"code","0892fd24":"code","73ee5596":"code","69d8bf24":"code","d4c6b03a":"code","fe472eb7":"code","a87ca09c":"code","1facac48":"markdown","177ddcff":"markdown","b853cdd0":"markdown","d1ccd82e":"markdown","12035cff":"markdown","031cc18c":"markdown","696040c6":"markdown","7f3aead6":"markdown","b4e1f768":"markdown","8f4aead4":"markdown","be5ccaa3":"markdown","ebb08963":"markdown","7aa49518":"markdown","1f79b808":"markdown","4062aa49":"markdown","0c75ff8b":"markdown","0d52506a":"markdown","9b3a2b4d":"markdown","a80b9bb7":"markdown","9c61c2ba":"markdown","71331c07":"markdown"},"source":{"44369019":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nplt.style.use('fivethirtyeight')\n\nimport re\nimport string\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\n\nimport warnings\nwarnings.filterwarnings('ignore')","8e18b4c8":"data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","adbf6361":"data.head()","decc73c2":"data.drop(columns=['keyword','location'], inplace=True)","86cc5467":"data.head()","e9b64491":"values = data['target'].value_counts().values\nfig = go.Figure(data=[go.Pie(labels=['Count 0','Count 1',], values=values)])\nfig.update_layout(template=\"plotly_dark\",title={'text': \"Count of Type\",'y':0.9,\n                                                'x':0.45,'xanchor': 'center','yanchor': 'top'},\n                  font=dict(size=18, color='white', family=\"Courier New, monospace\"))\nfig.show()","05f6a9f8":"data['message_len'] = data['text'].apply(lambda x: len(x.split(' ')))\ndata.head()","107b6f3e":"fig = px.histogram(data, x='message_len')\nfig.update_layout(template=\"plotly_dark\",title={'text': \"Phrase Length\",'y':0.9,\n                                                'x':0.45,'xanchor': 'center','yanchor': 'top'},\n                  font=dict(size=18, color='white', family=\"Courier New, monospace\"))\nfig.show()","04f82711":"def text_clear(data):\n    tx = data.apply(lambda x: re.sub(\"http\\S+\", '', str(x)))\n    tx = tx.apply(lambda x: re.sub(u'[^a-zA-Z0-9\u00e1\u00e9\u00ed\u00f3\u00fa\u00c1\u00c9\u00cd\u00d3\u00da\u00e2\u00ea\u00ee\u00f4\u00c2\u00ca\u00ce\u00d4\u00e3\u00f5\u00c3\u00d5\u00e7\u00c7: ]', '',x))\n    tx = tx.apply(lambda x: re.sub(' +', ' ', x)) # remover espa\u00e7os em brancos\n    tx = tx.apply(lambda x: re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', x)) # remover as hashtag\n    tx = tx.apply(lambda x: re.sub('(@[A-Za-z]+[A-za-z0-9-_]+)', '', x)) # remover os @usuario\n    tx = tx.apply(lambda x: re.sub('rt', '', x)) # remover os rt\n    tx = tx.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n    return tx","8364a794":"data['text'] = text_clear(data['text'])\ndata.head()","d740bdcb":"tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef bert_encode(data, maximum_length) :\n    input_ids = []\n    attention_masks = []\n\n    for text in data:\n        encoded = tokenizer.encode_plus(\n            text, \n            add_special_tokens=True,\n            max_length=maximum_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","ed408b19":"texts = data['text']\ntarget = data['target']\n\ntrain_input_ids, train_attention_masks = bert_encode(texts,30)","0892fd24":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","73ee5596":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_model(bert_model):\n    \n    input_ids = tf.keras.Input(shape=(30,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(30,),dtype='int32')\n\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","69d8bf24":"model = create_model(bert_model)","d4c6b03a":"stoped = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001)\nredutor = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)","fe472eb7":"history = model.fit([train_input_ids, train_attention_masks],\n    target, validation_split=0.2, epochs=30, batch_size=16, callbacks=[stoped, redutor])","a87ca09c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\naxes[0].plot(history.history['accuracy'])\naxes[0].plot(history.history['val_accuracy'])\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Accuracy')\naxes[0].legend(['Accuracy in Train','accuracy in Test'])\naxes[0].grid(True)\n\naxes[1].plot(history.history['loss'])\naxes[1].plot(history.history['val_loss'])\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('Erro')\naxes[1].legend(['Erro in Train','Erro in Test'])\naxes[1].grid(True)","1facac48":"The input embedding sub-layer converts the input tokens to vectors of dimension using learned embeddings in the original Transformer model.\n\nThe Transformer contains a learned embedding sub-layer. Many embedding methods can be applied to the tokenized input.","177ddcff":"# <p style=\"background-color:#80ccff; font-family:newtimeroman; font-size:150%; text-align:center; border-radius:  80px 5px; padding-top:8px; padding-bottom:8px;\">Import<\/p>","b853cdd0":"# Attention Is All You Need","d1ccd82e":"## Attention Layers\n\nThe Transformer is an autoregressive model. It uses the previous output sequences as an additional input. The multi-head attention layers of the decoder use the same process as the encoder.\n\nHowever, the masked multi-head attentio sub-layer 1 only lets attention apply to the postions up to and including the current position. The future words ate hidden from the Transformer, and this forces it to learn how to predict. ","12035cff":"## Transformer","031cc18c":"# <p style=\"background-color:#80ccff; font-family:newtimeroman; font-size:150%; text-align:center; border-radius:  80px 5px; padding-top:8px; padding-bottom:8px;\">BERT - Bidirectional Encoder Representations from Transformers<\/p>","696040c6":"* EarlyStopping: Stop training when a monitored metric has stopped improving.\n\n* ReduceLROnPlateau :Reduce learning rate when a metric has stopped improving.","7f3aead6":"The attention mechanism will provide a deeper relationship between words and produce better results.\n\nFor each attention sub-layer, the original Transformer model run not on but eight attention mechanisms in parallel to spedd up the calculations. This process is named \"multi-head attention\".","b4e1f768":"<img src=\"https:\/\/iq.opengenus.org\/content\/images\/2020\/06\/encoder-1.png\">","8f4aead4":"## Decoder\n\nThe structure of the decoder layers remanins the same as the encoder for all the N=6 layers of the Transformer model. Each layer contains threee sub-layers: a multi-headed masked attention mechanism, a multi-headed attention mechanism, and a fully connceted position-wise feedforard network. \n\nThe decoder has third main sub-layer, which is the masked multi-head attention mechanism. In this sub-layer output, at a given position, the following words ate masked so the the Transformer bases its assumptions on tis inferences without seeing the rest of the sequence. That way, in this model, it cannot see future parts of the sequence.","be5ccaa3":"# <p style=\"background-color:#80ccff; font-family:newtimeroman; font-size:150%; text-align:center; border-radius:  80px 5px; padding-top:8px; padding-bottom:8px;\">Model<\/p>","ebb08963":"# <p style=\"background-color:#80ccff; font-family:newtimeroman; font-size:150%; text-align:center; border-radius:  80px 5px; padding-top:8px; padding-bottom:8px;\">Preprocessing<\/p>","7aa49518":"### Function to clear text and prepare to tokenizer","1f79b808":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*X92uPDSMofn49e3oyOwf-Q.png\">","4062aa49":"<img src=\"https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/Transformer-network.png?resize=400%2C549&ssl=1\">","0c75ff8b":"# <p style=\"background-color:#80ccff; font-family:newtimeroman; font-size:150%; text-align:center; border-radius:  80px 5px; padding-top:8px; padding-bottom:8px;\">EDA<\/p>","0d52506a":"## Multi-Head Attention\n\nThe multi-head attention sub-layer contains eight heads and is followed by post-layer normalization, which will add residual connections to the output of the sub-layers an normalize it.\n\nThe input of multi-attention sub-layer of the first layer of the encoder stack is a vector that contains the embedding and the positional encoding of each word. ","9b3a2b4d":"## Input Embedding","a80b9bb7":"The original Transformr model is a stack of 6 layers. The output of layers *l* is the input of *l+1* until the \nfinal prediction is reached. There is a 6 layers encoder on the left ans a 6 layer decoder stack on the right. \n\nOn the left, the inputs enter the encoder side of the Transformer through an attention sub-layer and FeedForward Network (FNN) sub-layer. On the right, the target outputs go into the decoder side of the Transformer through two attention sub-layers ans a FFN sub-layer. We immediately notice thar there is no  RNN, LSTM, or CNN. Recurrence has been abandoned. \n\nThe attention mechanism is a \"word-to-word\" operation. The attention mechanism will fing how each word is realted to all other words in a sequence, including the word being analyzed itself. ","9c61c2ba":"### Token, Attention Mask and Padding","71331c07":"BERT is Google's deep learning algorithm for NLP.\n\nThe main technical innovation is the application of bidirectional training that traverses the text from left to right and from right to left. The result is a deeper trained model than a single-drive model, yielding better results"}}