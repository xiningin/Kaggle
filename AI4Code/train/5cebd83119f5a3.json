{"cell_type":{"08b53ac5":"code","f338baca":"code","ccd01470":"code","a3302251":"code","2910c5ed":"code","cfece681":"code","c4409e88":"code","d63bd6a3":"code","002ab172":"code","f2e25443":"code","73474493":"code","71460cd7":"code","834173a6":"code","19152b8a":"code","8f76263f":"code","a219e55a":"code","a86e173a":"code","2485d399":"code","dec733e2":"code","72f05978":"code","4439a9eb":"code","3251034b":"code","acbab3c8":"code","a0e9744d":"code","510953ef":"code","b028c773":"code","3354ed17":"code","c56c9ce0":"code","c16ac65e":"code","3ca248fe":"code","0ac39f20":"code","c3c6d73c":"code","65b75155":"code","1293161c":"code","e1f35cef":"code","21dab4f3":"code","d7084b89":"code","6aaae48d":"code","2747437a":"code","e3a8fb5b":"code","b609d3fb":"code","80173d4c":"code","821f4a5f":"code","25881e53":"code","94f3bf05":"code","b7702a23":"code","128cdc50":"code","b3ec1aae":"code","3748d795":"code","58ac5135":"code","bbd63ca6":"code","85b691b6":"code","d294d029":"code","8b0b3846":"code","5a01056b":"code","f94b0df7":"code","14d3d6d8":"code","5fd1d133":"code","d029290c":"code","53445e56":"code","cfbddd4f":"code","386a940b":"code","6e8ce485":"code","bb65733b":"code","448b23db":"code","bb722cc1":"code","ff057f1d":"code","2f39d4b7":"code","e9ad53ae":"code","8ba8f4cb":"code","b2bee564":"code","de2c86e8":"code","9eb67554":"code","f7967cec":"code","d7e26c8a":"code","806dd712":"code","1e423fa9":"markdown","e01ab3b9":"markdown","cd603d7a":"markdown","c14bffd2":"markdown","887b4cbb":"markdown","3bbffc52":"markdown","a7cfa892":"markdown","7f08ca99":"markdown","aafb7257":"markdown","a90fca7a":"markdown","8d9a79b9":"markdown","eb69da02":"markdown","b3a48c85":"markdown","a70bdd77":"markdown","c79ca179":"markdown","88c68df2":"markdown","2a92fadc":"markdown","a044d60b":"markdown","68031cd6":"markdown","6e115cad":"markdown","fbd4adae":"markdown","02a6b653":"markdown","c522a842":"markdown","7d406900":"markdown","203b6f5a":"markdown","872a1e70":"markdown","567665f2":"markdown","d62b9c8f":"markdown","20447efc":"markdown","cbde500e":"markdown","6e0e0d08":"markdown"},"source":{"08b53ac5":"# basic imports \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # visualization\nsns.set()","f338baca":"# image manipulation for word cloud\nfrom PIL import Image \nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nSTOP_WORDS = set(stopwords.words('english'))","ccd01470":"# neural networks\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization","a3302251":"# getting image from url\nfrom io import BytesIO \nimport requests","2910c5ed":"#  Vectorizer for text data - Counts and Tfidf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","cfece681":"from sklearn.decomposition import PCA","c4409e88":"# Importing re for text preprocessing\nimport re","d63bd6a3":"# spacy for text preprocessing (lemmatization, tokenization, NER, POS)\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")","002ab172":"# Extreme Gradient Boosting Models\nimport xgboost as xgb","f2e25443":"# Imports from sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline","73474493":"# data paths\ntrain_path = '..\/input\/commonlitreadabilityprize\/train.csv'\ntest_path = '..\/input\/commonlitreadabilityprize\/test.csv'","71460cd7":"# images to use as mask in wordclouds generator\nbook_path = 'https:\/\/cdn.pixabay.com\/photo\/2013\/04\/01\/21\/30\/book-99132_960_720.png'\nbook_path_2 = 'https:\/\/cdn.icon-icons.com\/icons2\/2622\/PNG\/512\/book_icon_158035.png'","834173a6":"# Dictionary of POS Tags and NER Tags along with explanation used by spacy\nGLOSSARY_POS = {\n    # POS tags\n    # Universal POS Tags\n    # http:\/\/universaldependencies.org\/u\/pos\/\n    \"ADJ\": \"adjective\",\n    \"ADP\": \"adposition\",\n    \"ADV\": \"adverb\",\n    \"AUX\": \"auxiliary\",\n    \"CONJ\": \"conjunction\",\n    \"CCONJ\": \"coordinating conjunction\",\n    \"DET\": \"determiner\",\n    \"INTJ\": \"interjection\",\n    \"NOUN\": \"noun\",\n    \"NUM\": \"numeral\",\n    \"PART\": \"particle\",\n    \"PRON\": \"pronoun\",\n    \"PROPN\": \"proper noun\",\n    \"PUNCT\": \"punctuation\",\n    \"SCONJ\": \"subordinating conjunction\",\n    \"SYM\": \"symbol\",\n    \"VERB\": \"verb\",\n    \"X\": \"other\",\n    \"EOL\": \"end of line\",\n    \"SPACE\": \"space\"}\n\nGLOSSARY_NER = {\n    # Named Entity Recognition\n    # OntoNotes 5\n    # https:\/\/catalog.ldc.upenn.edu\/docs\/LDC2013T19\/OntoNotes-Release-5.0.pdf\n    \"PERSON\": \"People, including fictional\",\n    \"NORP\": \"Nationalities or religious or political groups\",\n    \"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",\n    \"FAC\": \"Buildings, airports, highways, bridges, etc.\",\n    \"ORG\": \"Companies, agencies, institutions, etc.\",\n    \"GPE\": \"Countries, cities, states\",\n    \"LOC\": \"Non-GPE locations, mountain ranges, bodies of water\",\n    \"PRODUCT\": \"Objects, vehicles, foods, etc. (not services)\",\n    \"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n    \"WORK_OF_ART\": \"Titles of books, songs, etc.\",\n    \"LAW\": \"Named documents made into laws.\",\n    \"LANGUAGE\": \"Any named language\",\n    \"DATE\": \"Absolute or relative dates or periods\",\n    \"TIME\": \"Times smaller than a day\",\n    \"PERCENT\": 'Percentage, including \"%\"',\n    \"MONEY\": \"Monetary values, including unit\",\n    \"QUANTITY\": \"Measurements, as of weight or distance\",\n    \"ORDINAL\": '\"first\", \"second\", etc.',\n    \"CARDINAL\": \"Numerals that do not fall under another type\",\n}","19152b8a":"#dictionary with english contractions like don't isn't for function explanding contractions\ncontractions = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"I'd\": \"I had \/ I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall \/ I will\",\n\"I'll've\": \"I shall have \/ I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n","8f76263f":"# Retrieving website address from url_legal column\ndef clean_link(link):\n    \"\"\"Function that retrieves main website address from the link\"\"\"\n    if pd.isnull(link):\n        return link\n    \n    link = link.replace(\"https:\/\/\",'')\n    link = link.replace(\"http:\/\/\",'')\n    link = link.split('\/')\n    if isinstance(link,list):\n        return link[0]\n    else:\n        return link","a219e55a":"# Reading image from url path for wordcloud generation\ndef read_img_from_url(url):\n    \"\"\"Returns np.array from url leading to image\"\"\"\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    img_matrix = np.array(img)\n    return img_matrix","a86e173a":"# constants and functions that allow counting syllables in the word\n\nVOWEL_RUNS = re.compile(\"[aeiouy]+\", flags=re.I)\nEXCEPTIONS = re.compile(\n    # fixes trailing e issues:\n    # smite, scared\n    \"[^aeiou]e[sd]?$|\"\n    # fixes adverbs:\n    # nicely\n    + \"[^e]ely$\",\n    flags=re.I\n)\nADDITIONAL = re.compile(\n    # fixes incorrect subtractions from exceptions:\n    # smile, scarred, raises, fated\n    \"[^aeioulr][lr]e[sd]?$|[csgz]es$|[td]ed$|\"\n    # fixes miscellaneous issues:\n    # flying, piano, video, prism, fire, evaluate\n    + \".y[aeiou]|ia(?!n$)|eo|ism$|[^aeiou]ire$|[^gq]ua\",\n    flags=re.I\n)\n\ndef count_syllables(word):\n    \"\"\"Returns number of syllables in the word based on string\"\"\"\n    vowel_runs = len(VOWEL_RUNS.findall(word))\n    exceptions = len(EXCEPTIONS.findall(word))\n    additional = len(ADDITIONAL.findall(word))\n    return max(1, vowel_runs - exceptions + additional)","2485d399":"# Cleaning text\ndef lemma_txt(doc):\n    \"Function that returns lowercase, lemmatized text without punctuation\"\n    lemma_list = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ != '-PRON-']\n    return ' '.join(lemma_list)\n\n# Calculating basic text statistics\ndef counter(doc):    \n    \"\"\" Function that returns dictionary with text statistics:\n        - count of chars\n        - count of words\n        - count of sentences\n        - count of syllables\n        - avg. count of words per sentence\n        - avg. count of syllables per word\n        - count of difficult words (with more than 2 syllables)\n        - percentage of words with more than two syllables in the text\"\"\"\n    char_list = np.array([len(token.text) for token in doc if token.is_alpha])\n    syll_list = np.array([count_syllables(token.text) for token in doc if token.is_alpha])\n    word_count = len(char_list)\n    sent_count = len(list(doc.sents))\n    char_count = char_list.sum()\n    diff_word_count = np.sum(syll_list>3)\n    syll_count = syll_list.sum()\n    diff_word_perc = round(diff_word_count\/word_count,2)*100\n    syll_per_word = round(syll_count\/word_count,2)\n    word_per_sent = round(word_count\/sent_count,2)\n    \n    counter_dict = {\n        'Words' : word_count,\n        'Sentences' : sent_count,\n        'Chars' : char_count,\n        'Syllables' : syll_count,\n        'Diff_Words' : diff_word_count,\n        'Diff_Words_Perc' : diff_word_perc,\n        'Words_Per_Sent' : word_per_sent,\n        'Syll_Per_Word' : syll_per_word,\n    }\n    \n    return counter_dict\n\n# Conting different Parts of Speech in text\ndef pos_counter(doc):\n    \"\"\"Functions that returns dictionary with count of different parts of speech in the text.\n    - POS tags based on spacy package\"\"\"\n    counts_dict = doc.count_by(spacy.attrs.IDS['TAG'])\n\n    pos_dict = {}\n    \n    for i in nlp.tokenizer.vocab.morphology.tag_map.keys():\n            if i != '_SP':\n                pos_dict[spacy.explain(i)]=0\n    \n    # Create dict with the human readable part of speech tags\n    for pos, count in counts_dict.items():\n        tag = spacy.explain(doc.vocab[pos].text)\n        pos_dict[tag] = count\n        \n    return pos_dict\n\n# Counting number of different named entities in text\ndef ner_counter(doc):\n    \"\"\"Functions that returns dictionary with count of different named entities in the text.\n    - NER tags based on spacy package\"\"\"\n    ner_dict = {}\n    for ner in GLOSSARY_NER:\n        ner_dict[ner]=0\n    \n    for ent in doc.ents:\n        ner_dict[ent.label_] += 1\n    \n    return ner_dict","dec733e2":"#define function to expand contractions and showcase\ndef expand_contractions(s, contractions = contractions):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, s)\n","72f05978":"# Function that expands dataframe including text column with text statistics, POS counts, NER counts and \"Clean Text\"\ndef preprocess_text(df_data, txt_col='excerpt'):\n    \"\"\"Functions that preprocess text and expands DataFrame with relevant columns.\"\"\"\n    docs = nlp.pipe(df_data[txt_col].tolist())\n    df_data['DOCS']=[doc for doc in list(docs)]\n    df_data['Clean_Text']= df_data['DOCS'].apply(lemma_txt)\n    \n    df_data['Text_Stats']= df_data['DOCS'].apply(counter)\n    df_data['POS_Stats'] = df_data['DOCS'].apply(pos_counter)\n    df_data['NER_Stats'] = df_data['DOCS'].apply(ner_counter)\n    \n    dfs = [df_data.drop(labels=['Text_Stats','POS_Stats','NER_Stats','DOCS'],axis=1),\n           pd.DataFrame(df_data['Text_Stats'].tolist()),\n           pd.DataFrame(df_data['POS_Stats'].tolist()),\n           pd.DataFrame(df_data['NER_Stats'].tolist())]\n    \n    return pd.concat(dfs, axis=1)","4439a9eb":"# Lists with columns created using preprocess_text function\nbasic_stats = ['Chars','Words','Syllables','Sentences','Syll_Per_Word','Words_Per_Sent','Diff_Words']\nPOS_stats = [spacy.explain(i) for i in nlp.tokenizer.vocab.morphology.tag_map.keys() if i != '_SP']\nNER_stats = list(GLOSSARY_NER.keys())\n\nall_stats = basic_stats + POS_stats + NER_stats","3251034b":"# Loading data\ndata_train = pd.read_csv(train_path)\ndisplay(data_train.head())","acbab3c8":"# Checking size of data\nprint('TRAIN DATA')\nprint('Samples:',data_train.shape[0])\nprint('Columns:',data_train.shape[1])","a0e9744d":"# Checking data types and amount of nulls\ndisplay(data_train.info())","510953ef":"# Loading test data and showing first 5 rows\ndata_test = pd.read_csv(test_path)\ndisplay(data_test.head())","b028c773":"# Checking size of test data\nprint(\"TEST DATA:\")\nprint('Samples:',data_test.shape[0])\nprint('Columns:',data_test.shape[1])","3354ed17":"# cleaning the text and preparing different statistics\ndata_train = preprocess_text(data_train)\n\n# Retrieving main website from url link\ndata_train['website'] = data_train['url_legal'].apply(clean_link)","c56c9ce0":"# Count of source websites and license for texts in training data\nfig, axis = plt.subplots(2, figsize=(10,12))\n\nsns.countplot(y='website',hue='website', data=data_train, dodge=False, ax=axis[0])\naxis[0].set_title('Source Website Count',fontsize=16)\naxis[0].get_legend().remove()\n\nsns.countplot(y='license', hue='license',data=data_train, dodge=False, ax=axis[1])\naxis[1].set_title('License Count',fontsize=16)\naxis[1].get_legend().remove()\n\nfig.tight_layout()\nplt.show()","c16ac65e":"# Inspecting targer variable and standard error\ndisplay(data_train[['target','standard_error']].describe())","3ca248fe":"# Visualizing Target and Standard Error\nfig, axis = plt.subplots(1,3,figsize=(14,5))\n\nsns.histplot(x='target',kde=True, data=data_train,bins=100, ax=axis[0])\naxis[0].set_title('Target Distribution', fontsize=16)\n\nsns.histplot(x='standard_error',kde=True, data=data_train, ax=axis[1], color='darkred')\naxis[1].set_title('Standard Error Distribution', fontsize=16)\n\nsns.histplot(x='standard_error',kde=True, data=data_train.query('standard_error > 0.01'), ax=axis[2], color='darkred')\naxis[2].set_title('Standard Error Distribution', fontsize=16)\n\nplt.show()","0ac39f20":"print('Std error above 0:',data_train[data_train['standard_error']>0].shape[0], 'samples')\nprint('Std error equal or below 0:',data_train[data_train['standard_error']<=0].shape[0], 'samples')","c3c6d73c":"# Plot of standard_error versus target - points colored by length of text\nfig, ax = plt.subplots(figsize=(7,7))\nsns.scatterplot(x='target', y='standard_error',hue='Chars', data=data_train,\n                alpha=0.5, ax=ax, palette='viridis_r')\nax.set_title('Standard Error vs Target', fontsize=16)\nax.set_ylim([0.4,0.7])\nplt.show()","65b75155":"# The most difficult to read text\ndisplay(data_train.sort_values(by='target')[['target','excerpt']].head(1).values)","1293161c":"# The easiest to read text\ndisplay(data_train.sort_values(by='target', ascending=False)[['target','excerpt']].head(1).values)","e1f35cef":"# Correlation between POS stats and target\nfig,ax = plt.subplots(figsize=(4,8))\nax.set_title('Correlation of Text Stats and Target', fontsize=16)\nsns.heatmap(data_train[basic_stats+['target']].corr()[['target']].sort_values(by='target'),\n            annot=True, fmt='.2f',\n            vmin=-1, vmax=1,\n            cmap='RdBu_r',\n            ax=ax)\nfig.tight_layout()\nplt.show()","21dab4f3":"# Correlation between POS stats and target\nfig,ax = plt.subplots(figsize=(3,20))\nax.set_title('Correlation of POS and Target', fontsize=16)\nsns.heatmap(data_train[POS_stats+['target']].corr()[['target']].sort_values(by='target'),\n            annot=True, fmt='.2f',\n            vmin=-1, vmax=1,\n            cmap='RdBu_r',\n            ax=ax)\nplt.show()","d7084b89":"# Correlation between NER stats and target\nfig,ax = plt.subplots(figsize=(3,10))\nax.set_title('Correlation of NER and Target', fontsize=16)\nsns.heatmap(data_train[NER_stats+['target']].corr()[['target']].sort_values(by='target'),\n            annot=True, fmt='.2f',\n            vmin=-1, vmax=1,\n            cmap='RdBu_r',\n            ax=ax)\nplt.show()","6aaae48d":"# Plot of text length versus target \nfig, ax = plt.subplots( figsize=(5,5))\n\nfig.suptitle(\"Text Statistics\", fontsize=18)\n\nsns.scatterplot(y='Words_Per_Sent', x='Syll_Per_Word',hue='target',\n                data=data_train, ax=ax)\nax.set_title('Avg. Word Length vs Avg Sentence Length', fontsize=16)\n\nfig.tight_layout()\nplt.show()","2747437a":"# statistics of text data\ndisplay(data_train[basic_stats].describe())","e3a8fb5b":"# Variance over mean - index of dispersion\n(data_train[basic_stats].var()\/data_train[basic_stats].mean()).abs()","b609d3fb":"# Variance over mean - index of dispersion\nfig, ax = plt.subplots()\nsns.barplot(x=(data_train[basic_stats].var()\/data_train[basic_stats].mean()).abs(),ax=ax,\n                  y=basic_stats)\nfig.suptitle(\"Index of Dispersion - Variance Over Mean\", fontsize=16)\nax.set_yticklabels(labels=basic_stats)\nplt.show()","80173d4c":"fig,ax = plt.subplots(1,2, figsize=(10,5))\n\nfig.suptitle('Distribution of Avg. Word and Sentence Length in Texts', fontsize=18)\nax[0].set_title('Avg. Sentence Length', fontsize=16)\nsns.histplot(x='Words_Per_Sent', data=data_train, bins=100,kde=True, ax=ax[0], color='green')\n\nax[1].set_title('Avg. Word Length', fontsize=16)\nsns.histplot(x='Syll_Per_Word', data=data_train,bins=100,kde=True, ax=ax[1], color='blue')\n\nfig.tight_layout()\nplt.show()","821f4a5f":"fig,ax = plt.subplots(1,4, figsize=(20,5))\n\nfig.suptitle('Distribution of Chars, Words and Sentences Count in Texts', fontsize=18)\n\nax[0].set_title('Word Count', fontsize=16)\nsns.histplot(x='Words', data=data_train, bins=40,kde=True, ax=ax[0], color='red')\n\nax[1].set_title('Sentence Count', fontsize=16)\nsns.histplot(x='Sentences', data=data_train, bins=40,kde=True, ax=ax[1], color='yellow')\n\nax[2].set_title('Syllable Count', fontsize=16)\nsns.histplot(x='Syllables', data=data_train, bins=40,kde=True, ax=ax[2], color='violet')\n\nax[3].set_title('Char Count', fontsize=16)\nsns.histplot(x='Chars', data=data_train, bins=40,kde=True, ax=ax[3], color='blue')\n\nfig.tight_layout()\nplt.show()","25881e53":"# Joining whole corpus to generate wordcloud\nwc_data = ' '.join(data_train['Clean_Text'].tolist()).upper()","94f3bf05":"# instantiate a word cloud object\nexcerpt_cloud = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords.words('english'),\n)\n# generate the word cloud\nexcerpt_cloud.generate(wc_data);","b7702a23":"# display the word cloud\nfig, ax = plt.subplots(figsize=(14,7))\nfig.suptitle('Word Cloud with Most Frequent Words', fontsize=20)\nplt.imshow(excerpt_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","128cdc50":"# Calculating word frequency - top 20 unigrams, bigrams and trigrams\n\n# Unigrams\nCV = CountVectorizer(stop_words=stopwords.words('english'),\n                    ngram_range=(1,1),\n                    max_features=20)\nCV_excerpt = CV.fit_transform([wc_data])\ntop_uni = pd.DataFrame({'unigram':CV.get_feature_names(),'count':CV_excerpt.toarray()[0]})\n\n# Bigrams\nCV = CountVectorizer(stop_words=stopwords.words('english'),\n                    ngram_range=(2,2),\n                    max_features=20)\nCV_excerpt = CV.fit_transform([wc_data])\ntop_bi = pd.DataFrame({'bigram':CV.get_feature_names(),'count':CV_excerpt.toarray()[0]})\n\n# Trigrams\nCV = CountVectorizer(stop_words=stopwords.words('english'),\n                    ngram_range=(3,3),\n                    max_features=20)\nCV_excerpt = CV.fit_transform([wc_data])\ntop_tri = pd.DataFrame({'trigram':CV.get_feature_names(),'count':CV_excerpt.toarray()[0]})","b3ec1aae":"fig, ax = plt.subplots(1,3, figsize=(16,7))\nfig.suptitle('Most Frequent n-grams', fontsize=20)\n\nsns.barplot(y='unigram',x='count', data=top_uni.sort_values(by='count'), palette='viridis', ax=ax[0])\nax[0].set_title('Unigrams', fontsize=16)\nsns.barplot(y='bigram',x='count', data=top_bi.sort_values(by='count'), palette='magma', ax=ax[1])\nax[1].set_title('Bigrams', fontsize=16)\nsns.barplot(y='trigram',x='count', data=top_tri.sort_values(by='count'), palette='inferno', ax=ax[2])\nax[2].set_title('Trigrams', fontsize=16)\n\nfig.tight_layout()\nplt.show()","3748d795":"# Display POS Statistics for dataset\nfig, ax = plt.subplots(figsize=(10,14))\nax.set_title('Part of Speech Statistics for Training Data', fontsize=16)\ndata_train[POS_stats].sum().sort_values().plot.barh(ax=ax)\nplt.show()","58ac5135":"# Display NER Statistics for dataset\nfig, ax = plt.subplots(figsize=(10,12))\nfig.suptitle('Part of Speech Statistics for Training Data')\ndata_train[NER_stats].sum().sort_values().plot.barh(ax=ax)\nplt.show()","bbd63ca6":"# Let's calculate sample weights based on standard error\ndata_train['sample_weight']= 1.6 - data_train['standard_error']\n\nsns.scatterplot(y='sample_weight', x='standard_error', data=data_train)\nplt.show()","85b691b6":"# Preparing Tfidf Vectorizer\nTfidf = TfidfVectorizer(stop_words = stopwords.words('english'), max_df=0.995, min_df=0.005)","d294d029":"# Splitting the data into train and test\nX = data_train[basic_stats+NER_stats+POS_stats+['Clean_Text']]\ny = data_train[['target','sample_weight']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8b0b3846":"# Fitting vectorizer\ntfidf_train = Tfidf.fit_transform(X_train['Clean_Text'])\ntfidf_test = Tfidf.transform(X_test['Clean_Text'])\n\n# Preparing dataframes with features\ntf_train_df = pd.concat([X_train.drop(labels='Clean_Text', axis=1),\n                         pd.DataFrame(tfidf_train.toarray(),index=X_train.index, columns=Tfidf.get_feature_names())],\n                         ignore_index=True,axis=1)\n\ntf_test_df = pd.concat([X_test.drop(labels='Clean_Text', axis=1),\n                        pd.DataFrame(tfidf_test.toarray(), index=X_test.index, columns=Tfidf.get_feature_names())],\n                       ignore_index=True, axis=1)","5a01056b":"scale = StandardScaler()\npca = PCA(n_components=0.99)","f94b0df7":"train_sc = scale.fit_transform(X_train[all_stats])\ntrain_pca = pca.fit_transform(train_sc)","14d3d6d8":"test_sc = scale.transform(X_test[all_stats])\ntest_pca = pca.transform(test_sc)","5fd1d133":"my_train = np.hstack([train_pca, tfidf_train.toarray()])\nmy_test = np.hstack([test_pca, tfidf_test.toarray()])","d029290c":"plt.bar(x=range(pca.explained_variance_ratio_.shape[0]), height=pca.explained_variance_ratio_)\nplt.show()","53445e56":"# Building simple Sequential NN model\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(my_train.shape[1],)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1))","cfbddd4f":"model.summary()","386a940b":"# Compiling the model with adam optimizer and huber loss\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n              loss=tf.keras.losses.huber,\n              metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])","6e8ce485":"# This callback will reduce learning rate if the model will get stuck\ncallbacks = [tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)]","bb65733b":"my_train.shape","448b23db":"# Training the model\nhist = model.fit(my_train, y_train['target'],\n          batch_size=200,\n          validation_data=(my_test, y_test['target']),\n          sample_weight= y_train['sample_weight'],\n          callbacks=callbacks,\n          verbose=1,\n          epochs=200)","bb722cc1":"metrics_v = pd.DataFrame(hist.history)[['rmse','val_rmse']]\nmetrics_v.plot()\nplt.show()","ff057f1d":"loss_v = pd.DataFrame(hist.history)[['loss','val_loss']]\nloss_v.plot()\nplt.show()","2f39d4b7":"y_pred = model.predict(my_test).reshape(-1)\nRMSE = np.sqrt(mean_squared_error(y_test['target'], y_pred))\nprint('RMSE: ',RMSE)","e9ad53ae":"fig,ax = plt.subplots()\nsns.scatterplot(x=y_test['target'],y=y_pred, ax =ax , alpha=0.5)\nax.plot([-3.5,0,1.75],[-3.5,0,1.75],color='darkred')\nax.set_xlim([-4,2])\nax.set_ylim([-4,2])\nplt.axis('Equal')\nplt.show()","8ba8f4cb":"sns.jointplot(x=y_test['target'],y=y_pred, kind='resid')\nplt.show()","b2bee564":"tr_mean = y_train['target'].mean()\ny_brute_pred = np.ones_like(y_pred)*tr_mean","de2c86e8":"RMSE = np.sqrt(mean_squared_error(y_brute_pred, y_pred))\nprint('RMSE: ',RMSE)","9eb67554":"# Loading test data\ndata_test = pd.read_csv(test_path)","f7967cec":"# predicting based on test set\n\n# Text preprocessing\ndata_test = preprocess_text(data_test)\n\n# Generating tfidf\nX_new = data_test[basic_stats+NER_stats+POS_stats+['Clean_Text']]\nnew_test = Tfidf.transform(X_new['Clean_Text'])\n\n# Preparing dataframes with features\nnew_sc = scale.transform(X_new[all_stats])\nnew_pca = pca.transform(new_sc)\n\nnew_feat = np.hstack([new_pca, new_test.toarray()])\n\npreds = model.predict(new_feat).reshape(-1)","d7e26c8a":"# Generating submission file\ndata_test['target']  = np.round(preds,2)\ndata_test[['id','target']].to_csv(\"submission.csv\", index=False)","806dd712":"data_test[['id','target']]","1e423fa9":"#### I will start the exploaratory analysis by reviewing the source website and license of available texts","e01ab3b9":"#### Let's load the test data and also take a look in first few rows\n","cd603d7a":"## Word Clouds with word frequency","c14bffd2":"#### For comparison I created dummy regressor which predicts mean","887b4cbb":"#### Let's review relationship between target and std error. It seems like the biggest std error is on the extremes.","3bbffc52":"#### Preparing functions that will make text processing and data exploration easier","a7cfa892":"# NEURAL NETWORK","7f08ca99":"# EXPLORATORY DATA ANALYSIS","aafb7257":"# TEXT CONTENT","a90fca7a":"It seems like in standard error there is one untypical observation equal 0 and rest of them lays between 0.4 and 0.7. <br> \nTarget have distribution close to normal with mean -1 and is slightly skewed towards lower values","8d9a79b9":"#### Then I will explore target distribution and standard error","eb69da02":"# FUNCTIONS","b3a48c85":"## Part of Speech Statistics","a70bdd77":"#### Let's calculate frequency for different ngrams","c79ca179":"# CONSTANTS","88c68df2":"# LOADING DATA","2a92fadc":"# GENERATING SUBMISSION FILES","a044d60b":"Word Cloud is useful and pretty way to see most common words","68031cd6":"# IMPORTS","6e115cad":"#### Let's plot relationship between basic text stats and target and review their distribution","fbd4adae":"#### Preprocessing the train and creating columns with text statistics","02a6b653":"#### Let's load the training data and take a look at first few rows. I will also check the data shape and if we are dealing with any nulls. ","c522a842":"#### Building basic neural network that will consider text stats and Tfidf Vectors","7d406900":"# INFO ABOUT COMPETITION\n![books](https:\/\/cdn.pixabay.com\/photo\/2016\/08\/24\/16\/20\/books-1617327_960_720.jpg)\n<br>Image by Marisa Sias from Pixabay\n\n### Goal \nThe goal is to build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. \n\n### Data\nFiles <br>\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format<br>\n\nColumns<br>\n* id - unique ID for excerpt\n* url_legal - URL of source - this is blank in the test set.\n* license - license of source material - this is blank in the test set.\n* excerpt - text to predict reading ease of\n* target - reading ease\n* standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.<br>\n\nNote: \nData includes excerpts from several time periods and a wide range of reading ease scores.<br>\nTest set includes a slightly larger proportion of modern texts (the type of texts model should generalize to) than the training set.<br>\nWhile licensing information is provided for the public test set, the hidden private test set includes only blank license\/legal information.\n\n### Evaluation\nSubmissions are scored on the RMSE - root mean squared error. <br>\n* example of submission file:<br>\nid,target<br>\neaf8e7355,0.0<br>\n60ecc9777,0.5<br>\nc0f722661,-2.0<br>\netc.<br>","203b6f5a":"# DUMMY REGRESSOR","872a1e70":"### Word Frequency","567665f2":"#### Now I'm gonna take closer look at content of texts - calculating word frequency and POS, NER frequency for whole corpus","d62b9c8f":"#### Let's review the examples for the lowest and highest values of target ","20447efc":"## Named Entity Statistics","cbde500e":"#### Paths to files with data and image for mask in wordcloud. Dictionary with NER and POS Tags for Spacy","6e0e0d08":"#### Let's take a look at correlation between target and statistics "}}