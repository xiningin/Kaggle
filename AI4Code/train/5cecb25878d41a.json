{"cell_type":{"f4bcd5fd":"code","c8c299ee":"code","812eba32":"code","d4d0c5d9":"code","4372e20c":"code","061725be":"code","5339efb6":"code","30efb3ac":"code","a6b7e577":"code","b8562e7f":"code","1ea1dd7d":"code","0a2c94ad":"code","da043122":"code","8979b640":"code","3158fca8":"code","7a2d9c0e":"code","3d6541f8":"code","40d83e13":"code","6a4e9b0c":"code","02673222":"code","24187d7f":"code","06fe907b":"code","c69c379b":"code","0440a6a9":"code","ee819997":"code","2df95828":"code","2bd688a5":"code","b24dbab0":"code","5a39d7b2":"code","4121d2c3":"code","eca35962":"code","ccd9a2a6":"code","9569315d":"code","a18377a1":"code","8799ace8":"code","0bc878b8":"code","d0f887af":"code","1c712428":"code","d03c1241":"code","20fd7bb8":"code","6ac0bfd4":"code","54de10c0":"code","640bab1b":"code","4ce1aa23":"code","0c72d264":"code","13f00e0c":"code","8cb48e5f":"code","29cde2ff":"code","1b363890":"markdown","4c87587e":"markdown","5aa84d8b":"markdown","19899688":"markdown","815eaa2d":"markdown","7378d518":"markdown","fcc2b064":"markdown","c9381918":"markdown","9e41b00d":"markdown","97ce790b":"markdown","fe7d2b5d":"markdown","fcda5030":"markdown","a2fe1ec6":"markdown","ac72a189":"markdown","ccbee4d7":"markdown","799841c9":"markdown","da98d45a":"markdown","98ab5179":"markdown","679f007e":"markdown","5d52b132":"markdown","1123a833":"markdown","84c7278c":"markdown","62ad396d":"markdown","743d2d6c":"markdown","5380add0":"markdown","38f13b2f":"markdown","daf0a15c":"markdown","c8fef795":"markdown","8b0e757a":"markdown","ef6706a9":"markdown","e1a245a9":"markdown","4200b749":"markdown","5d26b1dd":"markdown","be6bf984":"markdown","ed4e2c6d":"markdown","c5fec5a8":"markdown","f18dc51a":"markdown","6f7cbda5":"markdown","73325aa6":"markdown","1c28badf":"markdown","df88fb58":"markdown","dd18e0be":"markdown","f23c23e1":"markdown","9ff3babb":"markdown","07fe8165":"markdown","e3767422":"markdown","7b280a61":"markdown","679ceb7a":"markdown","764f2018":"markdown","f67da88d":"markdown","6b120aa3":"markdown","565358c2":"markdown","45f01acd":"markdown","bf8256a0":"markdown","8a064859":"markdown","4d2ab84c":"markdown","1c5f93db":"markdown","3ddc4391":"markdown","e729c9c6":"markdown","946972b1":"markdown","49565d1d":"markdown","fbbb5f54":"markdown"},"source":{"f4bcd5fd":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","c8c299ee":"# Road training and test set\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\ntrain.head()","812eba32":"train.info()","d4d0c5d9":"# Correlation matrix\nsns.heatmap(train.corr(), annot = True, cmap = 'RdYlGn', linewidth = 0.2)\nfig = plt.gcf()\nfig.set_size_inches(10, 8)\nplt.show()","4372e20c":"# Count subplots functions for category features\n# Count plots a feature's number without Survival and feature's number with Survival\n\ndef count_subplots(data, feature1, hue = 'Survived', ylim = None, xlim = None):\n    f, ax = plt.subplots(2, figsize = (18, 15))\n    sns.countplot(feature1,  data = data, ax = ax[0])\n    ax[0].set_title('{} Count Plot'.format(feature1), size = 20)\n    ax[0].set_xlabel(feature1, size = 15)\n    ax[0].set_ylabel('Count', size = 15)\n    ax[0].tick_params(labelsize = 15)\n    \n    sns.countplot(feature1, hue = hue, data = data, ax = ax[1])\n    ax[1].set_title('{} Count Plot'.format(feature1), size = 20)\n    ax[1].set_xlabel(feature1, size = 15)\n    ax[1].set_ylabel('Count')\n    ax[1].tick_params(labelsize = 15)\n    if hue == 'Survived':\n        ax[1].legend(['Not Survived', 'Survived'], loc = 'upper right', prop = {'size' : 15})\n        \n    if ylim != None:\n        plt.ylim(ylim)\n    if xlim != None:\n        plt.xlim(xlim)    \n    \n    plt.show()\n\ncount_subplots(train, 'Sex')","061725be":"count_subplots(train, 'Pclass')","5339efb6":"print('SibSp Count Plot', end = '\\n{}\\n\\n'.format('-'*100))\ncount_subplots(train, 'SibSp')\nprint('Parch Count Plot', end = '\\n{}\\n\\n'.format('-'*100))\ncount_subplots(train, 'Parch')","30efb3ac":"count_subplots(train, 'Embarked')","a6b7e577":"# Factor plot function for comparing each features. \n\ndef factor_plots(data, feature1, feature2 = None, col = None, hue = None, kind = 'point', ylim = None, xlim = None):\n        g = sns.factorplot(feature1, feature2, col = col, hue = hue, kind = kind, data = data)\n        #if feature2 != None:\n        #    plt.title(\"{} and {}'s {} plot\".format(feature1, feature2, kind))\n        #else:\n        #    plt.title(\"{}'s {} plot\".format(feature1, kind))\n        fig = plt.gcf()\n        fig.set_size_inches(13, 4)\n        \n        if ylim != None:\n            plt.ylim(ylim)\n        if xlim != None:\n            plt.xlim(xlim)\n            \n        plt.show()\n\nfactor_plots(train, 'Embarked',  kind = 'count', hue = 'Survived', col = 'Pclass')","b8562e7f":"factor_plots(train, 'Pclass', 'Survived', hue = 'Sex',  kind = 'point', col = 'Embarked')","1ea1dd7d":"factor_plots(train, 'Survived', 'Age', hue = 'Sex', kind = 'violin')","0a2c94ad":"factor_plots(train, 'Pclass', 'Fare', hue = 'Survived', kind = 'bar')","da043122":"f, ax = plt.subplots(2, figsize = (18, 8))\nsns.distplot(train['Age'], label = 'Age', hist = False, ax = ax[0])\nsns.distplot(train['Fare'], label = 'Fare', hist = False, ax = ax[1])\nplt.show()","8979b640":"df = pd.concat(objs = [train, test], axis = 0).reset_index(drop = True)\ndf.head(-5)","3158fca8":"# Check null data\ndf.isnull().sum()","7a2d9c0e":"age_by_pclass_sex = df.groupby(['Sex', 'Pclass'])['Age'].median()\nprint(age_by_pclass_sex)\n\ndf['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","3d6541f8":"df['Age'].isnull().sum()","40d83e13":"# Extracting Initial consonant\ndf['Cabin_Initial'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\nfactor_plots(df, 'Pclass', hue = 'Cabin_Initial', kind = 'count', ylim = [0, 100])","6a4e9b0c":"df['Cabin_Initial'] = df['Cabin_Initial'].replace(['A', 'B', 'C', 'T'], 'ABCT')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['D', 'E'], 'DE')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['F', 'G'], 'FG')\ndf['Cabin_Initial'].value_counts()","02673222":"df['Cabin_Initial'].isnull().sum()","24187d7f":"df.loc[df['Embarked'].isnull()]","06fe907b":"factor_plots(df, 'Cabin_Initial', kind = 'count', ylim = [0, 50],\n             hue = 'Sex', col = 'Embarked')","c69c379b":"df['Embarked'] = df['Embarked'].fillna('S')\ndf['Embarked'].isnull().sum()","0440a6a9":"df.iloc[df['Fare'].loc[df['Fare'].isnull()].index]","ee819997":"df['Fare'] = df['Fare'].fillna(df.groupby(['Pclass', 'Embarked'])['Fare'].median()[3]['S'])\ndf['Fare'].isnull().sum()","2df95828":"# binding numeric feature function\n# We have to do this process twice(Age, Fare). So I think that creating function is more efficient.\n\n\ndef binding_band(column, binnum):\n    df[column + '_band'] = pd.qcut(df[column].map(int), binnum)\n\n    for i in range(len(df[column + '_band'].value_counts().index)):\n        print('{}_band {} :'.format(column, i), df[column + '_band'].value_counts().index.sort_values(ascending = True)[i])\n        df[column + '_band'] = df[column + '_band'].replace(df[column + '_band'].value_counts().index.sort_values(ascending = True)[i], int(i))\n        \n    df[column + '_band'] = df[column + '_band'].astype(int)    \n    \n    return df.head()\n\nbinding_band('Age',8)\n\n\n\"\"\"\nCorrelation Check\nte = []\ntemp = 0\nfor j in range(80):\n    for i in range(len(df.groupby('Age_band').Survived.mean())):\n        temp += df.groupby('Age_band').Survived.mean()[i]\n    te.append(temp \/ len(df.groupby('Age_band').Survived.mean()))\n\"\"\"","2bd688a5":"binding_band('Fare', 6)","b24dbab0":"df['Initial'] = 0\nfor i in range(len(df['Name'])):\n    df['Initial'].iloc[i] = df['Name'][i].split(',')[1].split('.')[0].strip()\n\ndf['Initial'].value_counts().index","5a39d7b2":"count_subplots(df, 'Initial', ylim = [0, 100])","4121d2c3":"Mrs_Miss_Master = []\nOthers = []\n\nfor i in range(len(df.groupby('Initial')['Survived'].mean().index)):\n    if df.groupby('Initial')['Survived'].mean()[i] > 0.5:\n        Mrs_Miss_Master.append(df.groupby('Initial')['Survived'].mean().index[i])\n    elif df.groupby('Initial')['Survived'].mean().index[i] != 'Mr':\n        Others.append(df.groupby('Initial')['Survived'].mean().index[i])\n    \ndf['Initial'] = df['Initial'].replace(Mrs_Miss_Master, 'Mrs\/Miss\/Master')\ndf['Initial'] = df['Initial'].replace(Others, 'Others')\n\ncount_subplots(df, 'Initial')","eca35962":"df['Alone'] = 0\ndf['Alone'].loc[(df['SibSp'] + df['Parch']) == 0] = 1\n\ncount_subplots(df, 'Alone')","ccd9a2a6":"# Ticket_Size : Number of same ticket name\ndf['Ticket_Number'] = df['Ticket'].replace(df['Ticket'].value_counts().index, df['Ticket'].value_counts())\ncount_subplots(df, 'Ticket_Number')","9569315d":"# Family_Size : Number of family members on aboared Titanic\ndf['Family_Size'] = df['Parch'] + df['SibSp'] + 1\ncount_subplots(df, 'Family_Size')","a18377a1":"df.loc[:,('Family_Size', 'Ticket_Number')].corr()","8799ace8":"# Create Companion_Survival_Rate feature\ndf['Companion_Survival_Rate'] = 0\nfor i, j in df.groupby(['Family_Size', 'Ticket_Number'])['Survived'].mean().index:\n    df['Companion_Survival_Rate'].loc[(df['Family_Size'] == i) & (df['Ticket_Number'] == j)] = df.groupby(['Family_Size', 'Ticket_Number'])[\"Survived\"].mean()[i, j]\n    \ndf.loc[df['Companion_Survival_Rate'].isnull()]","0bc878b8":"# Calculation means of combination of Family_Size 5 and Ticket_Number 3.\n\n# It is too long to write down at once, so calculated by dividing the variables\ncomb_sum = df.loc[df['Family_Size'] == 5]['Survived'].sum() + df.loc[df['Ticket_Number'] == 3]['Survived'].sum()\ncomb_counts = df.loc[df['Family_Size'] == 5]['Survived'].count() + df.loc[df['Ticket_Number'] == 3]['Survived'].count()\nmean = comb_sum \/ comb_counts\n\ndf['Companion_Survival_Rate'] = df['Companion_Survival_Rate'].fillna(mean)\ndf['Companion_Survival_Rate'].isnull().sum()","d0f887af":"from sklearn.preprocessing import OneHotEncoder\n\n# Select categorical features\ncate_col = []\nfor i in [4, 11, 12, 15]:\n    cate_col.append(df.columns[i])\n\ncate_df = pd.get_dummies(df.loc[:,(cate_col)], drop_first = True)\ndf = pd.concat(objs = [df, cate_df], axis = 1).reset_index(drop = True)","1c712428":"df = df.drop(['Name', 'Sex', 'Age', 'Ticket', 'Fare', 'Embarked', 'Cabin_Initial', 'SibSp', 'Parch',\n              'Cabin', 'Initial', 'Ticket_Number', 'Family_Size'], axis = 1)","d03c1241":"sns.heatmap(df.drop(['PassengerId'], axis = 1).corr(), annot = True, cmap = 'RdYlGn', linewidth = 0.2)\nfig = plt.gcf()\nfig.set_size_inches(15, 10)\nplt.show()","20fd7bb8":"# Importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression # logistic regression\nfrom sklearn import svm # support vector machine\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier # KNN\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.model_selection import train_test_split # training and testing data split\n#from sklearn import metrics # accuracy measure\nfrom sklearn.model_selection import KFold # for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score # score evaluation\nfrom sklearn.model_selection import cross_val_predict # prediction\nfrom sklearn.preprocessing import StandardScaler # StandardScaler\n\n# Split data\ndf = df.astype(float)\ntrain = df[:891]\ntest = df[891:]\n\n\"\"\"\nval_train, val_test = train_test_split(train, test_size = 0.3, random_state = 1, stratify = train['Survived'])\nval_train_X = val_train[val_train.columns[1:]]\nval_train_Y = val_train[val_train.columns[:1]]\nval_test_X = val_test[val_test.columns[1:]]\nval_test_Y = val_test[val_test.columns[:1]]\n\"\"\"\n\ntrain_X = StandardScaler().fit_transform(train.drop(columns = ['PassengerId', 'Survived']))\ntrain_Y = train['Survived']\ntest_X = StandardScaler().fit_transform(test.drop(columns = ['PassengerId', 'Survived']))\n","6ac0bfd4":"\"\"\"estimators = []\nparams = []\n\n# SVM parameter\nC = np.arange(0.1, 1.1, 0.1)\ngamma = np.arange(0.1, 1.1, 0.1)\nkernel = ['rbf', 'linear']\nSVM_params = {'kernel' : kernel, 'gamma' : gamma, 'C' : C}\n\nestimators.append(svm.SVC())\nparams.append(SVM_params)\n\n\n# RandomForestClassifier parameter\nn_estimators = np.arange(800, 1800, 200)\n#max_depth = range(3, 7)\n#min_samples_split = range(3, 7)\n#min_samples_leaf = range(3, 7)\n#RF_params = {'n_estimators' : n_estimators, 'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n#            'min_samples_leaf' : min_samples_leaf}\n#RF_params = {'n_estimators' : n_estimators}\n\nestimators.append(RandomForestClassifier())\nparams.append(RF_params)\n\n\n# KNeighborsClassifier parameter\nn_neighbors = range(1, 11)\nKN_params = {'n_neighbors' : n_neighbors}\n\nestimators.append(KNeighborsClassifier())\nparams.append(KN_params)\n\n\n# DecisionTreeCalssifier parameter\ncriterion = ['gini', 'entropy']\nmax_depth = range(10, 100, 10)\n#min_samples_leaf = range(2, 5)\n#max_leaf_nodes = range(1, 10)\n#DT_params = {'criterion' : criterion, 'max_depth' : max_depth, 'min_samples_splits' : min_samples_splits,\n#            'min_samples_leaf' : min_samples_leaf, 'max_leaf_nodes': max_leaf_nodes}\nDT_params = {'criterion' : criterion, 'max_depth' : max_depth}\n\nestimators.append(DecisionTreeClassifier())\nparams.append(DT_params)\n\n# LogisticeRegression parameter\nC = np.arange(0.1, 1.1, 0.1)\nLR_params = {'C' : C}\n\nestimators.append(LogisticRegression())\nparams.append(LR_params)\n\n# Run GridSearchCV() to select best parameter by each model.\nfor i in range(len(estimators)):\n    gd = GridSearchCV(estimator = estimators[i], param_grid = params[i], verbose = True, cv = 2)\n    gd.fit(train_X, train_Y)\n    print(\"{}'s best score :\".format(estimators[i]), gd.best_score_)\n    print(\"{}'s best parameters :\".format(estimators[i]), gd.best_params_)\n\"\"\"","54de10c0":"base_estimators = [svm.SVC(kernel = 'rbf', C = 0.6, gamma = 0.1),\n                   RandomForestClassifier(max_depth = 3, min_samples_leaf = 3, min_samples_split = 6, n_estimators = 1000),\n                   KNeighborsClassifier(n_neighbors = 9),\n                   DecisionTreeClassifier(criterion = 'gini', max_depth = 80),\n                   LogisticRegression(C = 0.3)]\nsingle_models_predict = []\nsingle_models_score = []\nsingle_models = {}\n\nfor i in range(len(base_estimators)):\n    base_estimators[i].fit(train_X, train_Y)\n    single_models['{}'.format(str(base_estimators[i]).split('(')[0])] = base_estimators[i]\n    single_models_predict.append(base_estimators[i].predict(test_X))\n    single_models_score.append((cross_val_score(base_estimators[i], train_X, train_Y, cv = 3, scoring = 'accuracy')).mean())","640bab1b":"single_model = ['SVM', 'RandomForest', 'Kneighbors', 'DecisionTree', 'LogisticRegression']\nsingle_model_dataframe = pd.DataFrame({'accuracy' : single_models_score}, index = single_model).sort_values('accuracy')\n\nsingle_model_dataframe.plot.barh(width = 0.8, color = 'limegreen')\nplt.title('Single model Accuracy', size = 15)\nfig = plt.gcf()\nfig.set_size_inches(8, 5)\nplt.show()","4ce1aa23":"import xgboost as xg\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nmodels = {}\nmodels_ensemble_name = []\nmodels_score = []\nerror_ = []\n\nbase_estimators = [svm.SVC(kernel = 'rbf', C = 0.6, gamma = 0.1),\n#                   RandomForestClassifier(max_depth = 3, min_samples_leaf = 3, min_samples_split = 6, n_estimators = 1000),\n                   KNeighborsClassifier(n_neighbors = 9),\n                   DecisionTreeClassifier(criterion = 'gini', max_depth = 80),\n                   LogisticRegression(C = 0.3)]\nfor i in range(len(base_estimators)):\n\n    boosts = []\n    boosts.append(xg.XGBClassifier(base_estimator = base_estimators[i]))\n    boosts.append(GradientBoostingClassifier())\n    boosts.append(AdaBoostClassifier(base_estimator = base_estimators[i]))\n\n    n_estimators = range(100, 600, 100)\n    learning_rate = np.arange(0.1, 1.1, 0.1)\n    params = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate}\n\n    for j in range(len(boosts)):\n        try:\n            if not(i != 0 and j == 1):\n                pass\n                gd = GridSearchCV(estimator = boosts[j], param_grid = params, verbose = True, cv = 2)\n                gd.fit(train_X, train_Y)\n                # print(\"{}'s best score :\".format(str(boosts[j])[:3]), gd.best_score_)\n                # print(\"{}'s best parameters :\".format(str(boosts[j])[:3]), gd.best_params_)\n                models['{}_{}'.format(str(base_estimators[i]).split('(')[0], str(boosts[j])[:3])] = gd.best_estimator_\n                models_ensemble_name.append('{}_{}'.format(str(base_estimators[i]).split('(')[0], str(boosts[j])[:3]))\n                models_score.append(gd.best_score_)\n        except TypeError as TE:\n            error_.append(str(base_estimators[i]).split('(')[0] + str(boosts[j])[:3] + '_TE')\n        except ValueError as VE:\n            error_.append(str(base_estimators[i]).split('(')[0] + str(boosts[j])[:3] + '_VE')     \n\n            \n# AdaBoost for RandomForest\n# Parameter : learning_rate = 0.1, n_estimators = 100\n\nRF_Ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth = 3, min_samples_leaf = 3, min_samples_split = 6, n_estimators = 1000), learning_rate = 0.1, n_estimators = 100)\nmodels_score.append((cross_val_score(RF_Ada, train_X, train_Y, cv = 2, scoring = 'accuracy')).mean())\nRF_Ada = RF_Ada.fit(train_X, train_Y)\nmodels['RF_Ada'] =  RF_Ada\nmodels_ensemble_name.append('RF_Ada')","0c72d264":"from sklearn.ensemble import BaggingClassifier\n\n\n\n\"\"\"\nbase_estimators = [svm.SVC(kernel = 'rbf', C = 0.6, gamma = 0.1),\n#                   RandomForestClassifier(max_depth = 3, min_samples_leaf = 3, min_samples_split = 6, n_estimators = 1000),\n                   KNeighborsClassifier(n_neighbors = 9),\n                   DecisionTreeClassifier(criterion = 'gini', max_depth = 80),\n                   LogisticRegression(C = 0.3)]\n\"\"\"\n\nparam ={'n_estimators' : range(100, 600, 100)}\n\nfor i in range(len(base_estimators)):\n    try:\n        gd = GridSearchCV(estimator = BaggingClassifier(base_estimator = base_estimators[i]), param_grid = param, verbose = True, cv = 3)\n        gd.fit(train_X, train_Y)\n        models['{}_Bagging'.format(str(base_estimators[i]).split('(')[0])] = gd.best_estimator_\n        models_ensemble_name.append('{}_Bagging'.format(str(base_estimators[i]).split('(')[0]))\n        models_score.append(gd.best_score_)\n\n    except TypeError:\n        error_.append(str(base_estimators[i]).split('(')[0] + '_Bagging')\n    ","13f00e0c":"from sklearn.ensemble import VotingClassifier\n\nVC = VotingClassifier(estimators = [('SVM', svm.SVC(kernel = 'linear', C = 0.4, gamma = 0.1, probability = True)),\n                                   ('RF', RandomForestClassifier(max_depth = 5, min_samples_leaf = 3, min_samples_split = 3, n_estimators = 1600)),\n                                   ('KN', KNeighborsClassifier(n_neighbors = 10)),\n                                   ('DT', DecisionTreeClassifier(criterion = 'entropy', max_depth = 40)),\n                                   ('LR', LogisticRegression(C = 0.3))],\n                      voting = 'soft')\n\nmodels_score.append((cross_val_score(VC, train_X, train_Y, cv = 3, scoring = 'accuracy')).mean())\nVC = VC.fit(train_X, train_Y)\nmodels['Voting'] = VC\nmodels_ensemble_name.append('Voting')","8cb48e5f":"scores = models_score + single_models_score\nmodels_name = list(models.keys()) + single_model\ndic_model = {}\nfor i in range(len(scores)):\n    dic_model[models_name[i]] = scores[i]\ndic_model = {k: v for k, v in sorted(dic_model.items(), key = lambda item: item[1])}\nscores = list(dic_model.values())\nmodels_name = list(dic_model.keys())\n\n\n# Accuracy barh plot\nind = np.arange(len(models_name))\nwidth = 0.8\nfig, ax = plt.subplots(figsize = (20, 15))\n\nfor i in range(len(models_name)):\n    if '_' in models_name[i] or  models_name[i] == 'Voting':\n        ax.barh(ind[i], scores[i], width, color='skyblue')\n    else:\n        ax.barh(ind[i], scores[i], width, color='limegreen')\n\nax.tick_params(labelsize = 17)\nax.set_yticklabels(models_name, size = 17)\nax.set_yticks(ind)\nax.set_xlabel('Accuracy', size = 25)\nax.legend(labels = ['Single Model', 'Ensemble Model'], loc = 'upper right', prop = {'size' : 15})\nax.set_title('Ensemble Model Accuracy vs Single Model Accuracy', size = 30)\n\nplt.show()","29cde2ff":"Fanal_model = ['LogisticRegression', 'LogisticRegression_Bagging', 'Voting', 'RF_Ada', 'SVC']\nPredict = pd.Series(np.zeros((418)))\nfor i in range(len(Fanal_model)):\n    if Fanal_model[i] in models:\n        Predict += pd.Series(models[Fanal_model[i]].predict(test_X))\n    else:\n        Predict += pd.Series(single_models[Fanal_model[i]].predict(test_X))\n\n'''\n# Ensemble model prediction\n\nPredict = pd.Series(np.zeros((418)))\nfor i in range(len(models)):\n    Predict += pd.Series(models[models_ensemble_name[i]].predict(test_X))\n'''\n'''\n# Single model prediction with Ensemble\n\nfor i in range(len(single_models_predict)):\n    Predict += pd.Series(single_models_predict[i])\n'''\n\nPredict = Predict \/ len(Fanal_model)\nPredict[Predict >= 0.5] = 1\nPredict[Predict != 1] = 0\n\n# Submission csv\nsubmission = pd.DataFrame(columns = ['PassengerId', 'Survived'])\nsubmission['PassengerId'] = df['PassengerId'][891:].map(int)\nsubmission['Survived'] = Predict.map(int).values\nsubmission.to_csv('my_submission.csv', header = True, index = False)\nsubmission.head(10)","1b363890":"We can see that passenger who boarded from Cherbourg has many chance to survive than other. \n\n<b>So can we consider that feature is a important feature to predict survival?<\/b>","4c87587e":"##### Correlation between Family_Size and Ticket_number","5aa84d8b":"## Heatmap after engineering features","19899688":"Because of its high correlation, if learning a model using two features together, they are can lead to problems with multicollinearity. \n\n<b>So we will find survival rates according to the two features respectively and average the two values and use them as new feature('Companion_Survival_Rate')<\/b>","815eaa2d":"### 3.2.3 Voting","7378d518":"### Boosting Parameter\n- SVC_XGB : learning_rate = 0.1, n_estimators = 100\n- KNeighborsClassifier_XGB : learning_rate = 0.1, n_estimators = 100\n- DecisionTreeClassifier_XGB : learning_rate = 0.1, n_estimators = 100\n- DecisionTreeClassifier_Ada : learning_rate = 0.6, n_estimators = 400\n- LogisticRegression_XGB : learning_rate = 0.1, n_estimators = 100 \n- LogisticRegression_Ada : learning_rate = 0.4, n_estimators = 500\n- RF_Ada : learning_rate = 0.1, n_estimators = 100","fcc2b064":"### 1.2.6 Pclass, Sex and Embarked with Survived feature","c9381918":"## 2.3 Creating some features\nWe can create more features based on EDA and domain knowledge, and it will make model more accurate. I will create several features:\n- <b>Initial<\/b> : It has very many information like marriage. It could be important feature.\n- <b>Alone<\/b> : As we saw before, the survival rate of those who aboarded alone was lower than others. It can be new feature.\n- <b>Companion_Survival_Rate<\/b>: We can think that the situation at the time of the Titanic's sinking is very important. So if we can find companion, it make model more accurate. We will find it by median of survival rate of Ticket, Family Size(Parch + SibSp + 1).\n","9e41b00d":"### 2.1.4 Filling null-values in Fare\nWe will fill it by Pclass and Embarked","97ce790b":"## 1.2 EDA Features","fe7d2b5d":"- Mrs, Miss, Master, Mme, Ms, Lady, Sir, Mlle and Countess has very higher survival rate.(more than half) <b>So it will be group as  Mrs\/Miss\/Master.<\/b>\n- Most are 'Mr' but most of them are dead. <b>So it doesn't need to be grouped.<\/b>\n- The rest will be grouped under the name of  <b>Others<\/b> in a timely manner","fcda5030":"# Submission","a2fe1ec6":"#### Correlation Heatmap\n\nWe can see feature correlation between numeric features. There are some noticeable correlation between numeric features.\n- Fare and Pclass has negative correlation. We can guess because Pclass is higher(1 than 2, 3), then Fare is higher\n- Survived and Pclass has negative correlation. We will check this relation by making some plots.\n- Age and Pclass has negative correlation. Although Pclass has very negative correlation with Survived, Age has no correlation with Survived in this heatmap. So we also check that later.\n- SibSp and Parch has positive correlation. We can guess because there are many families on the Titanic. So We can handle these features.\n- Age with SibSp and Parch has negative correlation. \n- Other than that, there are some noticeable correlation between Fare and Survived and Parch and Fare, ect.\n\nWe will see that by making some plots.","ac72a189":"There are same Cabin between two data. So we can consider they are companion. So we tried to input values in Embarked by Cabin and Sex. However it is very similar between 'S' and 'C'.\n\n<b>So I searched them name and found that they boarded from Southampton.<\/b>\n\nSometimes it is helpful to search that we don't know.","ccbee4d7":"# Summary or results\n\nIt's score is about <b>'0.76794'<\/b>, not such a good score. On the contrary, when using only one or two ensemble technique, higher accuracy was given. We can deduce several reasons for this :\n<b>\n1. Selecting features problem\n2. Selecting models and using ensemble techniques without considering data sets\n3. Treating models equally without considering model specific characteristics when bringing them together and predict survival rate.\n<\/b><br>\n\n\n\n<b>However, the goal of this Titanic challenge was to learn the overall data analysis process. <\/b> Therefore, we wanted to approach the problem as diverse as possible, and as a result, we achieved this goal through the analysis of Titanic data. If I have a chance in the future, I will try to write a notebook with a focus on improving the accuracy of predicting Titanic survivors.\n\n<b>If the notebook was helpful, please press comments and recommendations. Or if you have any advice or questions about my laptop, you can leave a comment anytime. It could make me passionate.<\/b>\n","799841c9":"#### 2.3.3 Creating Companion's survival rate","da98d45a":"### Bagging parameter\n- SVM_Bagging : n_estimators = 200\n- KNeighborsClassifier_Bagging = 400\n- DecisionTreeClassifier_Bagging = 300\n- LogisticRegression_Bagging = 200","98ab5179":"# 3. Modeling\n\nIt is time to predict survival rate. we will use some models and ensemble methods. Various parameters are required for each model. We will use ensemble techniques to learn different models, so we will get the best parameters through hyperparameters about this dataset. We will also use models with adjusted parameters to apply ensemble techniques and submit final models.\n\nWe have two steps:\n<b>\n1. Hyper parameter tuning\n2. Ensembling\n<\/b>","679f007e":"## Checking Feature\n\nBefore we start to analyze Titanic problem, we have to consider each features in this data set. If we don't work this process, we can't perform to analyze very well. Because we will handle features by combining, deleting, it in Feature engineering stage. \n\n### Feature description\n- PassengerId : PassnegerId \n- Survived : Survival (0 : not survived, 1 : survived)\n- Pclass : Ticket class (1 : 1st, 2 : 2nd, 3 : 3rd)\n- Name : Passeger's name\n- Sex : Passenger's Sex\n- Age : Passenger's Age\n- SibSp : Passenger's siblings or spouses aboard the Titanic\n- Parch : Passenger's parents or chidren aboard the Titanic\n- Ticket : Ticket number\n- fare : Passenger fare\n- Cabin : Cabin number\n- Embarked : Port of Embarkation (C : Cherbourg, Q : Queenstown, S : Southampton)\n\nNow we can classify this features by description and type.\n### Input Feature\n\n<b>Categorical Feature<\/b> : Name, Sex, Ticket, Cabin, Embarked\n\n<b>Ordinal Feature<\/b> : Pclass\n\n<b>Numeric Feature<\/b> : PassengerId, Age, SibSp, Parch, Fare\n    \n### Target Feature\n- Survived\n\n","5d52b132":"### 1.2.8 Fare and Pclass with Survived feature","1123a833":"# Titanic Survival Prediction","84c7278c":"### 2.3.2 Creating Alone feature","62ad396d":"## 2.1 Filling null-values\n\nBefore we start, we have to combining training and test set. If we pass this step, we can't predict survival because of differences of number of columns and column's information.","743d2d6c":"This is my first notebook in Kaggle. Before starting to analyze Titanic Prediction, I refer to some notebook for Titanic solution. I have studied by making a transcription directly and tried to understand their notebook. I'm jonior majored in Business Administration in Korea. I want to became data scientist, So I will study in Kaggle. Although I can't using English very well, I will try to convey my thought.\n\nThis notebook have Three steps for solve this problem(predict to survive):\n1. Checking Features by EDA\n2. Feature engineering\n3. Modeling\n\nThis notebook is writed for understading overall data analysis process. First, we will check this analysis's goal, <b>to predict if a passenger survived the of the Titanic or not.<b> While we are working on this process, we have to keep in mind about this goal.","5380add0":"### 1.2.4 Embarked and Survived features","38f13b2f":"## 3.1 HyperParameter Tuning for selecting parameter\n- <b>Hyperparameter techniques will be used to select parameters.<\/b>\n\nEspecially RandomForest's hyperparameter tuning process takes a very long time. So. someone who run this notebook should keep in mind about this. If you don't have enough time, I recommend you to pass this step, and check the parameter under this cell.","daf0a15c":"## 1.1 Road data set and check features roughly.","c8fef795":"# 1. Checking Features by EDA","8b0e757a":"### 1.2.3 SibSp and Parch with Survived feature","ef6706a9":"### 2.1.1 Filling null-values in Age\nHow can we fill null-values in Age? It can be done by filling median values without null-values. However Age is important feature to predict Survived, we have to be careful with filling it. It can make noise. Then how can we do this?\n\nWhy don't we check Age's correlation? Pclass has the highest correlation with Age. So we will fill it by Pclass and Sex. I refer to this notebook, [Titanic-Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial)","e1a245a9":"### 1.2.1 Sex with Survived feature","4200b749":"### 2.3.1 Creating Initial feature","5d26b1dd":"### Summary of model accuracy\n\n##### Several features can be seen when graphed based on the accuracy of the models concerned.\n- <b>The accuracy of the XGBoost is very low compared to other models.<\/b> It can be seen that the accuracy is very poor compared to single models. This may mean that XGBoost does not fit well with the data set.\n\n- <b>For single models with high accuracy (Logistic Regression, SVM) the accuracy was higher than that of ensemble techniques.<\/b> However, the variance of individual models is higher than that of ensemble techniques, so it is not known which scores will be higher when submitting the results.\n\n#### To make more accurate predictions, we will check the predicts for the top five models and submit them as final results.\n- LogisticRegression\n- LogisticRegression_Bagging\n- Voting\n- RF_Ada\n- SVM","be6bf984":"### 2.1.2 Filling null-values in Cabin\n\nThere are many null-values in Cabin. if we fill null-values roughly, it leads us to make wrong model. So we have to be careful with handle this feature. Then first, check whether it is worth to bear the risk.\n\nCabin means cabin number, so I think it can be related to Pclass and Survived. So we will extract it's Initial consonant.","ed4e2c6d":"#### Parameter\n##### SVM\n- kernel = 'rbf', C = 0.6, gamma = 0.1\n\n##### RandomForestClassifier\n- max_depth = 3, min_samples_leaf = 3, min_samples_split = 6, n_estimators = 1000\n\n##### KNeighborsClassifier\n- n_neighbors = 9\n\n##### DecisionTreeClassifier\n- criterion = 'gini', max_depth = 80\n\n##### LogisticRegression\n- C = 0.3","c5fec5a8":"we can think that if Fare is higher, Pclass is higher. So we create plot that xaix is Pclass, yaxis is Fare and hue is Survived. Our though is somewhat right.","f18dc51a":"There are some null-values:\n- Survived\n- Age\n- Cabin\n- Embarked\n- Fare\n\nSurvived is due to test set. <b>So we have to fill null-values in Age, Cabin and Embarked<\/b>","6f7cbda5":"We can see that male's survival rate is lower than female's. Although number of male is about twice than number of female on the Titanic. However number of survived female is about twice than number of survived male. \n\n<b>So we can see that Sex feature is important feature to predict survival<\/b>","73325aa6":"There are one null-value in Companion_Survival_Rate. This is because there is a combination of Family_Size and Ticket_Number in the test set that is not in the training set.\n\n<b>So it will be replaced by mean survival rate between the combination that the value belongs to.<\/b>(Family_Size = 5, Ticket_Size = 3)","1c28badf":"# 2. Feature Engineering\n\nWe checked features. Now we will engineer features for modeling. Before we engineer features, we have to think of which model we will use for predicting Survived roughly. Because every model has comparatively each advantages and shortages. For example, regression analysis is weak for outliers, But tree based classification is not. It caused by different classification criteria.\n\nWe will use tree based classification and Logistic Regression model. So we have to engineer Fare feature by binding because it has positive skew that can make noise in Regression model.\n\nAnd there are some null data in input features. So we have to fill it correctly. It should be done after filling null-values in features.\n\nWe can create some features by combining some features. It makes our model more correctly.\n\nSo we have some steps:\n<b>\n1. Filling null-values\n2. Binding numeric features\n3. Creating some features\n4. One-Hot Encoding.\n5. Delete useless features.\n<\/b>","df88fb58":"## Ensemble Model Accuracy vs Single Model Accuracy","dd18e0be":"We can see that shape of plot between SibSp and Survived is very similar to shape of plot between Parch and Survived. We can guess that if we combine thier features, we can create number of family's member and it can be important feature.(SibSp + Parch + 1)\n\n<b>So we will combine their features<\/b>","f23c23e1":"### 1.2.7 Age, Sex with survived feature","9ff3babb":"### 3.2.2 Bagging","07fe8165":"It shows a fairly good accuracy! <b>We can create models from this single model by ensemble!<\/b>","e3767422":"## 2.2 binding numeric features\n- Category feature ->> Ordinary feature.","7b280a61":"## 2.4 Encoding Categorical Features \n- <b> It should be done about categorical features.<\/b> Except for the ordinal feature, which is an ordered variable, one-hot-encoding must be carried out to properly modelling. If one-hot-encoding is not carried out, the model learns that there is a meaning between the value differences of the category variable.\n\n- <b>We will also proceed with n-1 encoding to solve the multicollinearity problem.<\/b>","679ceb7a":"Now we can see why Embarked 'C' has higher survival rate than others. There are many 1st class passengers from Cherbourg","764f2018":"### 1.2.2 Pclass with Survived feature","f67da88d":"There are some correlation between Pclass and Cabin_Initial. Some Cabin_Initial is concentrated on 1st Class. So we will use this feature. Then we will be grouped this features by class.\n- A, B, C and T are 1st class passengers. So it will be grouped by ABCT.\n- D and E are similar passenger class distribution. So it will be grouped by DE.\n- F and G are similar passenger class distribution. Sp it will be grouped by FG.\n- M is very different from others. So it doesn't need to be grouped.","6b120aa3":"## 3.2 Ensembling\n### 3.2.1 Boosting\n<b> This step takes a very long time. (About 20 minutes on my computer) Accordingly, I recommend that you check only the parameter values and move on. Especially RandomForest's boost take a very long time. So we will pass at random only.<\/b>","565358c2":"### 2.1.3 Filling null-values in Embarked\nFirst we will find two null-data's common features","45f01acd":"We can see that regardless of aboarded port(Embarked) 1st, 2nd passengers has higher survival rate than 3rd passengers without men who aboarded from Queenstown(Embarked = Q). And we can see one more that female has higher survival rate than male.\n\n<b>We can see one more that Sex, Pclass are important features to predict survival.<\/b>","bf8256a0":"## Single model Accuracy","8a064859":"## 2.5 Delete useless features","4d2ab84c":"### 2.2.1 Age_band\nAs we saw bofore, We can guess relation between Age and Survival is almost <b>quadratic function shape.<\/b> (youngest -> 1 \/ middle -> 0 \/ oldest -> 1) So correlation between Age and Survival has to be very low.\n\nBefore I choosed the number to divide, I would divide it several times and choose number to divide based on correlation mean. However It is difficult to choose number to divide based of correlation mean. Because number to divide is higher, correlation mean is also higher. So I couldn't choose this method. Finally I choosed the number arbiratily.\n\n\n<b>We will divide them by 8.<\/b>","1c5f93db":"When we checked the correlation heatmap, we saw that there are no correlation between <b>Age<\/b> and <b>Survived<\/b>. However we can see that the youngest and oldest passengers is higher to survive than others. That's why there are no correlation between Age and Survived. the youngest passengers has higher survival rate than middle-aged passengers. And middle-aged passengers has lower survival rate than the oldest passengers. So it can't be checked correlation between them.\n\n<b> So if we engineer Age feature, we can use it to predict survival<\/b>","3ddc4391":"We can see that Fare is positively skewed. As we confirmed before, Fare and Pclass has some correlation with Survived. However as we will engineer Age feature, we will engineer Fare feature too.\n\n<b>So we will engineer Fare feature.<\/b>","e729c9c6":"### 2.2.2 Fare_band\n\n<b>We will divide it 6.<\/b>","946972b1":"### 1.2.5 Embarked and Pclass feature","49565d1d":"We can see that number of 3rd class's passengers is half of passenger on the Titanic. Also almost of them account for not surviving. On the other hand, more than half of 1st class's passengers is survived.\n\n<b>It shows that Pclass feature is also important features in this practice.<\/b>","fbbb5f54":"### 1.2.9 Age and Fare distplot\n##### Checking the numerical features about skewness"}}