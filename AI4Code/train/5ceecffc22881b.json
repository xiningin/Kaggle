{"cell_type":{"691c4f68":"code","a18f2f66":"code","32ce5a7e":"code","626521f7":"code","8d324a11":"code","4a8e5038":"code","48bec29b":"code","ee38e8df":"code","9db4cb2f":"code","7882b9eb":"code","f70d1eca":"code","22dae69d":"code","79a28061":"code","501b7c25":"code","5d28c67a":"code","83786588":"code","8068a185":"code","d22b33e0":"code","45528f5f":"code","5be88e61":"code","ee939146":"code","300b5e8d":"code","dadcae60":"code","847d10e7":"code","dfaac25c":"code","57790eff":"code","2acb2028":"code","f5e129f1":"code","4595def5":"code","abccf1dd":"code","de2ac343":"code","497ed1ae":"code","863a698f":"code","380212ae":"markdown","3c850b97":"markdown","be497b8b":"markdown","63f9bb38":"markdown","1f805392":"markdown","576257ff":"markdown","d8e9dd71":"markdown","528ca13e":"markdown","6608dd97":"markdown","59ceb8df":"markdown","89976b74":"markdown","090feab5":"markdown","dbbe45b1":"markdown","9813ba60":"markdown","e8572602":"markdown"},"source":{"691c4f68":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import auc, classification_report, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\npd.set_option('max_colwidth', 500)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","a18f2f66":"# Load files\ntrain = pd.read_csv('..\/input\/umojahack-financial\/Train (3).csv')\ntest = pd.read_csv('..\/input\/umojahack-financial\/Test (3).csv')\nsamplesubmission = pd.read_csv('..\/input\/submission\/SampleSubmission (2).csv')\nvariable_definations = pd.read_csv('..\/input\/variables\/VariableDefinitions.csv')","32ce5a7e":"variable_definations","626521f7":"train.target","8d324a11":"# Preview the first five rows of the train set\ntrain.head()","4a8e5038":"# Preview the first five rows of the test set\ntest.head()","48bec29b":"# Preview the first five rows of the sample submission file\nsamplesubmission","ee38e8df":"# Preview variable definations\nvariable_definations","9db4cb2f":"# Check the shape of the train and test sets\nprint(f'The shape of the train set is: {train.shape}\\nThe shape of the test set is: {test.shape}')","7882b9eb":"# Check if there any missing values in train set\nax = train.isna().sum().sort_values().plot(kind = 'barh', figsize = (9, 10))\nplt.title('Percentage of Missing Values Per Column in Train Set', fontdict={'size':15})\nfor p in ax.patches:\n    percentage ='{:,.0f}%'.format((p.get_width()\/train.shape[0])*100)\n    width, height =p.get_width(),p.get_height()\n    x=p.get_x()+width+0.02\n    y=p.get_y()+height\/2\n    ax.annotate(percentage,(x,y))","f70d1eca":"# Check if there missing values in test set\nax = test.isna().sum().sort_values().plot(kind = 'barh', figsize = (9, 10))\nplt.title('Percentage of Missing Values Per Column in Test Set', fontdict={'size':15})\n\nfor p in ax.patches:\n    percentage ='{:,.1f}%'.format((p.get_width()\/test.shape[0])*100)\n    width, height =p.get_width(),p.get_height()\n    x=p.get_x()+width+0.02\n    y=p.get_y()+height\/2\n    ax.annotate(percentage,(x,y))","22dae69d":"# Check for duplicates\ntrain.duplicated().any(), test.duplicated().any()","79a28061":"plt.figure(figsize=(7, 6))\nsns.countplot(train.target)\nplt.title('Target Variable Distribution');","501b7c25":"# Q1 - Has ATM\/debit card\nplt.figure(figsize=(8, 7))\nax =sns.countplot(train.Q1)\nax.set_xticklabels(['Yes', 'No', 'Don\"t Know', 'Refused to answer'], rotation=45 )\nplt.title('Distribution of Q1 - Has ATM\/debit card', fontdict = {'size': 15});","5d28c67a":"# Combine train and test set\nntrain = train.shape[0] # to be used to split train and test set from the combined dataframe\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nprint(f'The shape of the combined dataframe is: {all_data.shape}')","83786588":"# Check the column names and datatypes\nall_data.info()","8068a185":"# Category columns\ncat_cols = ['country',\t'region', 'owns_mobile'] + [x for x in all_data.columns if x.startswith('Q')]\nnum_cols = ['age', 'population']\n\n# Change columns to their respective datatypes\nall_data[cat_cols] = all_data[cat_cols].astype('category')\n\n# Confirm whether the changes have been successful\nall_data.info()","d22b33e0":"# Check unique values for each categorical column\nfor col in cat_cols:\n  print(col, all_data[col].nunique())","45528f5f":"train.isnull()","5be88e61":"train = train.fillna(value = 3,axis=1)\ntest = test.fillna(value = 3,axis=1)\n\n","ee939146":"train.isnull().sum()","300b5e8d":"train.head()","dadcae60":"train.head()","847d10e7":"# Fill in missing values\n# For cat cols and date cols fill in with mode and for num cols fill in with 9999\nfor col in all_data.columns:\n  if col in cat_cols:\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n  elif col in num_cols:\n    all_data[col] = all_data[col].fillna(all_data[col].fillna(9999))\n\n# Confirm that there aren't any missing values\nall_data[all_data.columns.difference(['target'])].isna().sum().any()","dfaac25c":"train.head()","57790eff":"# Shape of data before encoding\nall_data.shape","2acb2028":"# Use one hot encoding to turn categorical features to numerical features\n# Encode categorical features\nall_data = pd.get_dummies(data = all_data, columns = cat_cols)\nall_data.head()","f5e129f1":"# Shape of data after encoding\nall_data.shape","4595def5":"# Separate train and test data from the combined dataframe\ntrain_df = all_data[:ntrain]\ntest_df = all_data[ntrain:]\n\n# Check the shapes of the split dataset\ntrain_df.shape, test_df.shape","abccf1dd":"# Select main columns to be used in training\nmain_cols = all_data.columns.difference(['ID', 'target'])\nX = train_df[main_cols]\ny = train_df.target.astype(int)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n\n# Train model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict_proba(X_test)[:, 1]\n\n# Check the auc score of the model\nprint(f'RandomForest AUC score on the X_test is: {roc_auc_score(y_test, y_pred)}\\n')\n\n# print classification report\nprint(classification_report(y_test, [1 if x >= 0.5 else 0 for x in y_pred]))","de2ac343":"# Train model\nmodel = LGBMClassifier()\nmodel.fit(X_train.to_numpy(), y_train)\n\n# Make predictions\ny_pred = model.predict_proba(X_test.to_numpy())[:, 1]\n\n# Check the auc score of the model\nprint(f'LGBM AUC score on the X_test is: {roc_auc_score(y_test, y_pred)}\\n')\n\n# print classification report\nprint(classification_report(y_test, [1 if x >= 0.5 else 0 for x in y_pred]))","497ed1ae":"# Make prediction on the test set\ntest_df = test_df[main_cols]\npredictions = model.predict_proba(test_df)[:, 1]\n\n# # Create a submission file\nsub_file = samplesubmission.copy()\nsub_file.target = predictions\n\n# # Check the distribution of your predictions\nsns.countplot([1 if x >= 0.5 else 0 for x in sub_file.target])\nplt.title('Predicted Variable Distribution');","863a698f":"# Create a csv file and upload to zindi \nsub_file.to_csv('Baseline.csv', index = False)\nsub_file.head()","380212ae":"### Training and making predictions\n\n- Is lgbm the best model for this challenge?\n- Parameter tuning\n  - Grid search, random search, perhaps bayesian search works better...","3c850b97":"## Combine train and test set for easy preprocessing ","be497b8b":"### Number of unique values per categorical column","63f9bb38":"### Train different model and compare results","1f805392":"### Making predictions of the test set and creating a submission file","576257ff":"- There is a light improvement when using LGBMClassifier\n\n[More on AUC score](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc#:~:text=AUC%20represents%20the%20probability%20that,has%20an%20AUC%20of%201.0.)","d8e9dd71":"This shows us that the target (Can you make a payment if you were in an emergency) is fairly balanced. The majority class in this dataset are people who can make a payment incase of an emergency","528ca13e":"### Feature Engineering\n#### Try different strategies of dealing with categorical variables\n - One hot encoding\n - Label encoding\n - Target encoding\n - Reduce the number of unique values...","6608dd97":"## Fill in missing values\nMissing values can be filled using different strategies\n - Mean\n - Max\n - Min\n - for categorical variables - mode\n - [sklearn SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html)\n - Others... do more reasearch","59ceb8df":"This shows that most people do not own a debit or ATM card","89976b74":"###More Tips\n- Thorough EDA and domain knowledge sourcing\n- Re-group Categorical features \n- More Feature Engineering \n- Dataset balancing - oversampling, undersampling, SMOTE...\n- Ensembling of models \n- Cross-validation: Group folds, Stratified...","090feab5":"# ******************* GOOD LUCK!!! ***************************","dbbe45b1":"## Distribution of the target variable","9813ba60":"### Check for missing values","e8572602":"## UmojaHack Africa 2021 #3: Financial Resilience Challenge (BEGINNER) by UmojaHack Africa\n\nCan you predict if an individual will be able to make a payment in an emergency situation?\n\nThe objective of this challenge is to build a machine learning model to predict which individuals across Africa and around the world are most likely to be financially resilient.\n\n\n![Umoja Hack](https:\/\/zindpublic.blob.core.windows.net\/public\/uploads\/competition\/image\/151\/thumb_cdf71374-9857-44bb-8216-5f8c67afb51f.png)\n\nThis is a simple Python starter notebook to get you started with the Financial Resilience Challenge.\n\nThis notebook covers:\n- Loading the data\n- Simple EDA and an example of feature enginnering\n- Data preprocessing and data wrangling\n- Creating a simple model\n- Making a submission\n- Some tips for improving your score"}}