{"cell_type":{"daf1389b":"code","dc42fe3a":"code","a92b797c":"code","80f1d77f":"code","09bffc7a":"code","6540c4be":"code","8ab5f03d":"code","4581208f":"code","cb31ebef":"code","55e79a9c":"code","41848852":"code","ef6ad617":"code","e37f5e10":"code","983b2f9c":"code","1153fdbd":"code","e0938adf":"code","00460bac":"code","a02e9270":"code","df72e364":"code","a85d3a65":"code","3b5f0b61":"code","930ee880":"code","2cb212b0":"code","4d1559f2":"code","15e0101d":"code","6234b32c":"code","0de2aba0":"code","d03637a9":"code","1ce55ef5":"code","078fd0aa":"code","3e2495e7":"code","f94149cc":"code","2ddf062b":"code","3c98a557":"code","a07225f0":"code","a667cb26":"code","fcfa8e2b":"code","bd024c8b":"code","48601f4d":"code","8d3b8efc":"code","61578c3c":"code","f9accabe":"code","3682e978":"code","463c45b3":"code","f7307f6c":"code","f7488055":"code","399b5dab":"code","be63d73e":"markdown","3429fded":"markdown","7aa6f9c6":"markdown","34ff5b9c":"markdown","3bece091":"markdown","dddc2449":"markdown","2a92b53b":"markdown","3c140177":"markdown","ac64ab5c":"markdown","b97ccc8c":"markdown","97e66855":"markdown","70f0b3a7":"markdown","f047a0c1":"markdown","895196e8":"markdown","7db0fa84":"markdown","bb7511fe":"markdown","fad15646":"markdown","fb28633b":"markdown","a93ef70d":"markdown","72bf16be":"markdown","050bef09":"markdown"},"source":{"daf1389b":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","dc42fe3a":"from fastai.tabular.all import *","a92b797c":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.head()","80f1d77f":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest.head()","09bffc7a":"from sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold","6540c4be":"Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train[\"Survived\"])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\nprint(train.groupby(['fold', \"Survived\"]).size())","8ab5f03d":"seed=42\nset_seed(seed, reproducible=True)","4581208f":"train.head()","cb31ebef":"catlist = [\"Sex\",\"Embarked\"] # define string ( categorical data ). \u6587\u5b57\u5217\u306e\u5217\u540d\ncontlist = [\"Pclass\",\"Age\",\"SibSp\",\"Parch\"] # define continuous data. \u6570\u5b57\u306e\u5217\u540d\nTARGET = \"Survived\" # target . \u4e88\u60f3\u3059\u308b\u3082\u306e","55e79a9c":"validind = train[train[\"fold\"]==0].index # validation\u306eindex\nvalidind[:3]","41848852":"BATCH_SIZE = 256","ef6ad617":"dls = TabularDataLoaders.from_df(train, # dataframe\n                                 path='.',  \n                                 procs=[ Categorify,FillMissing,Normalize], # \u6587\u5b57\u5217\u306e\u6570\u5b57\u5909\u63db\u3001\u4e2d\u592e\u5024\u3067fillna\u3001\u5e73\u5747\u6e1b\u7b97\u6a19\u6e96\u504f\u5dee\u3067\u5272\u3063\u3066\u6b63\u898f\u5316 \n                                 cat_names=catlist, # \u6587\u5b57\u5217\u30ea\u30b9\u30c8\u3002\u6587\u5b57\u5217\u306a\u3044\u5834\u5408\u306fNone\u3067OK\n                                 cont_names=contlist, # \u6570\u5b57\u5217\u30ea\u30b9\u30c8\u3002\u306a\u3044\u5834\u5408\u306fNone\u3067OK\n                                 y_names=TARGET, # target\n                                 y_block=CategoryBlock, # CategoryBlock, MultiCategoryBlock, RegressionBlock\u304b\u3089\u9078\u3076 none\u3067\u3082\u53ef \n                                 valid_idx=validind, # Validation\u306eindex\n                                 bs=BATCH_SIZE, # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n                                 shuffle_train=True, # \u5b66\u7fd2\u6642\u306b\u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bfshuffle\u3059\u308b\u304b\n                                 shuffle=False,\n                                 val_shuffle=False,\n                                 n=None, \n                                 device=None, # gpu\u4f7f\u7528\u6642\u306f\"cuda\"\n                                 drop_last=None, # dataloader\u3067batchsize\u306b\u4f59\u308a\u304c\u3067\u305f\u3089\u3001\u5207\u308b\u304b\u3002\n                                 val_bs=BATCH_SIZE * 2 # validation\u306ebatch size\n                                 )","e37f5e10":"dls.show_batch()","983b2f9c":"learn = tabular_learner(dls, # dataset.\u5148\u307b\u3069\u4f5c\u6210\n                        layers=[1000,500,200],  # neuralnet\u306e\u4e2d\u9593\u5c64\n                        emb_szs=None,  \n                        config=None, \n                        n_out=None, # \u51fa\u529b\u3059\u308b\u6570.None\u3060\u3068\u81ea\u52d5\u8a8d\u8b58\u3057\u3066\u304f\u308c\u308b\n                        y_range=None, \n                        loss_func=CrossEntropyLossFlat(), # \u2191\u306e\u30ea\u30f3\u30af\u53c2\u7167\n                        opt_func=Adam,  # optimizer\n                        lr=0.001, # \u5b66\u7fd2\u7387\n                        splitter=trainable_params, \n                        \n                        \n                        cbs=[\n                            \n                            SaveModelCallback(monitor=\"accuracy\",comp=np.greater), # \u5b66\u7fd2\u5f8c\u306b\u4e00\u756a\u826f\u304b\u3063\u305f\u30e2\u30c7\u30ebsave\u3068load \n                          #   EarlyStoppingCallback(monitor=\"accuracy\",comp=np.greater, patience=30), # \u30e2\u30cb\u30bf\u30fc\u3057\u3066patience\u66f4\u65b0\u3057\u306a\u304b\u3063\u305f\u3089\u3084\u3081\u308b\n                          #   GradientClip, # \u91cd\u307f\u3065\u3051\u3092\u3044\u304d\u3059\u304e\u306a\u3044\u3088\u3046\u306b\n                          #   ReduceLROnPlateau(monitor='accuracy',comp=np.greater, patience=10,factor = 10) # \u5b66\u7fd2\u7387\u30e2\u30cb\u30bf\u30fc\u3057\u306a\u304c\u3089\u5909\u3048\u305f\u3044\u4eba\u306f\u3053\u308c\u3092\u5165\u308c\u308b\n                           ],\n\n\n                        metrics=accuracy, # metric \n                        path=None, \n                        model_dir='models', # savepath\u3060\u3068\u601d\u3046\u304c\u3001\u5f8c\u3067\u81ea\u5206\u3067\u3084\u308b\u304b\u3089\u3044\u3044\u3002 \n                        wd=None, \n                        wd_bn_bias=False,\n                        train_bn=True, \n                        moms=(0.95, 0.85, 0.95)\n                        )","1153fdbd":"learn.model","e0938adf":"learn.lr_find()","00460bac":"learn.fit(200,1e-4) # epoch\u6570\u3001\u5b66\u7fd2\u7387\n#learn.fit_one_cycle(200,2e-4) # epoch\u6570\u3001\u5b66\u7fd2\u7387\n","a02e9270":"learn.recorder.plot_loss()","df72e364":"learn.show_results()","a85d3a65":"#learn.save(f\"fastai_fold{0}\") # save pth\n\nlearn.export(f\"fastai_fold{0}.pkl\") # pickle\u3067save.\u5bb9\u91cf\u304c\u6291\u3048\u3089\u308c\u308b\u306e\u3067\u3001\u753b\u50cf\u51e6\u7406\u3068\u304b\u3067\u306f\u3053\u3061\u3089\u304c\u826f\u3044\u3002\n\n# torch.save(learn.model,f\"fastai_fold{0}.pth\") # learn\u3068\u3057\u3066save\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001pytorch model\u3068\u3057\u3066save","3b5f0b61":"validdf = train[train[\"fold\"]==0].reset_index(drop=True)\nvaliddf.head()","930ee880":"test_dl = dls.test_dl(validdf)","2cb212b0":"preds,_ = learn.get_preds(dl=test_dl)\n# preds,_ = learn.get_preds(1) # \u3053\u308c\u3067\u3082ok\npreds[:3] # softmax\u304c\u81ea\u52d5\u3067\u304b\u304b\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u610f","4d1559f2":"preds = np.array(preds)\n\npreds = [s.argmax() for s in preds]\npreds[:3]","15e0101d":"validdf[\"preds\"] = preds","6234b32c":"validdf.head()","0de2aba0":"score = np.sum(validdf[\"Survived\"] == validdf[\"preds\"]) \/ len(validdf)\nscore","d03637a9":"test.head()","1ce55ef5":"test_dl = dls.test_dl(test)","078fd0aa":"preds,_ = learn.get_preds(dl=test_dl)\npreds[:3] # softmax\u304c\u81ea\u52d5\u3067\u304b\u304b\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u610f","3e2495e7":"preds = np.array(preds)\n\npreds = [s.argmax() for s in preds]\npreds[:3]","f94149cc":"sample = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nsample.head()","2ddf062b":"sample[\"Survived\"] = preds","3c98a557":"sample.to_csv(\"submission1.csv\",index=False)","a07225f0":"set_seed(seed, reproducible=True)\n\nvalidind = train[train[\"fold\"]==0].index # validation\u306eindex\ndls = TabularDataLoaders.from_df(train, # dataframe\n                                 path='.',  \n                                 procs=[ Categorify,FillMissing,Normalize], # \u6587\u5b57\u5217\u306e\u6570\u5b57\u5909\u63db\u3001\u4e2d\u592e\u5024\u3067fillna\u3001\u5e73\u5747\u6e1b\u7b97\u6a19\u6e96\u504f\u5dee\u3067\u5272\u3063\u3066\u6b63\u898f\u5316 \n                                 cat_names=catlist, # \u6587\u5b57\u5217\u30ea\u30b9\u30c8\u3002\u6587\u5b57\u5217\u306a\u3044\u5834\u5408\u306fNone\u3067OK\n                                 cont_names=contlist, # \u6570\u5b57\u5217\u30ea\u30b9\u30c8\u3002\u306a\u3044\u5834\u5408\u306fNone\u3067OK\n                                 y_names=TARGET, # target\n                                 y_block=CategoryBlock, # CategoryBlock, MultiCategoryBlock, RegressionBlock\u304b\u3089\u9078\u3076 none\u3067\u3082\u53ef \n                                 valid_idx=validind, # Validation\u306eindex\n                                 bs=BATCH_SIZE, # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n                                 shuffle_train=True, # \u5b66\u7fd2\u6642\u306b\u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bfshuffle\u3059\u308b\u304b\n                                 shuffle=False,\n                                 val_shuffle=False,\n                                 n=None, \n                                 device=None, # gpu\u4f7f\u7528\u6642\u306f\"cuda\"\n                                 drop_last=None, # dataloader\u3067batchsize\u306b\u4f59\u308a\u304c\u3067\u305f\u3089\u3001\u5207\u308b\u304b\u3002\n                                 val_bs=BATCH_SIZE * 2 # validation\u306ebatch size\n                                 )\n\nlearn = tabular_learner(dls, # dataset.\u5148\u307b\u3069\u4f5c\u6210\n                        layers=[1000,500,200],  # neuralnet\u306e\u4e2d\u9593\u5c64\n                        emb_szs=None,  \n                        config=None, \n                        n_out=None, # \u51fa\u529b\u3059\u308b\u6570.None\u3060\u3068\u81ea\u52d5\u8a8d\u8b58\u3057\u3066\u304f\u308c\u308b\n                        y_range=None, \n                        loss_func=CrossEntropyLossFlat(), # \u2191\u306e\u30ea\u30f3\u30af\u53c2\u7167\n                        opt_func=Adam,  # optimizer\n                        lr=0.001, # \u5b66\u7fd2\u7387\n                        splitter=trainable_params, \n                        \n                        \n                        cbs=[\n                            \n                            SaveModelCallback(monitor=\"accuracy\",comp=np.greater), # \u5b66\u7fd2\u5f8c\u306b\u4e00\u756a\u826f\u304b\u3063\u305f\u30e2\u30c7\u30ebsave\u3068load \n                          #   EarlyStoppingCallback(monitor=\"accuracy\",comp=np.greater, patience=30), # \u30e2\u30cb\u30bf\u30fc\u3057\u3066patience\u66f4\u65b0\u3057\u306a\u304b\u3063\u305f\u3089\u3084\u3081\u308b\n                          #   GradientClip, # \u91cd\u307f\u3065\u3051\u3092\u3044\u304d\u3059\u304e\u306a\u3044\u3088\u3046\u306b\n                          #   ReduceLROnPlateau(monitor='accuracy',comp=np.greater, patience=10,factor = 10) # \u5b66\u7fd2\u7387\u30e2\u30cb\u30bf\u30fc\u3057\u306a\u304c\u3089\u5909\u3048\u305f\u3044\u4eba\u306f\u3053\u308c\u3092\u5165\u308c\u308b\n                           ],\n\n\n                        metrics=accuracy, # metric \n                        path=None, \n                        model_dir='models', # savepath\u3060\u3068\u601d\u3046\u304c\u3001\u5f8c\u3067\u81ea\u5206\u3067\u3084\u308b\u304b\u3089\u3044\u3044\u3002 \n                        wd=None, \n                        wd_bn_bias=False,\n                        train_bn=True, \n                        moms=(0.95, 0.85, 0.95)\n                        )\n\n\nlearn.fit(200,1e-4) # epoch\u6570\u3001\u5b66\u7fd2\u7387\n#learn.fit_one_cycle(200,2e-4) # epoch\u6570\u3001\u5b66\u7fd2\u7387\n\n\nlearn.export(f\"fastai_fold{0}.pkl\")\n\nvaliddf = train[train[\"fold\"]==0].reset_index(drop=True)\ntest_dl = dls.test_dl(validdf)\n\npreds,_ = learn.get_preds(dl=test_dl)\npreds = np.array(preds)\n\npreds = [s.argmax() for s in preds]\npreds[:3]\n\nvaliddf[\"preds\"] = preds\n\nscore = np.sum(validdf[\"Survived\"] == validdf[\"preds\"]) \/ len(validdf)\nscore\n","a667cb26":"allvaliddf = pd.DataFrame()\nscores = []\n\nfor a in range(5):\n    \n\n    set_seed(seed, reproducible=True)\n\n    validind = train[train[\"fold\"]==a].index # validation\u306eindex\n    dls = TabularDataLoaders.from_df(train, # dataframe\n                                     path='.',  \n                                     procs=[ Categorify,FillMissing,Normalize], # \u6587\u5b57\u5217\u306e\u6570\u5b57\u5909\u63db\u3001\u4e2d\u592e\u5024\u3067fillna\u3001\u5e73\u5747\u6e1b\u7b97\u6a19\u6e96\u504f\u5dee\u3067\u5272\u3063\u3066\u6b63\u898f\u5316 \n                                     cat_names=catlist, # \u6587\u5b57\u5217\u30ea\u30b9\u30c8\u3002\u6587\u5b57\u5217\u306a\u3044\u5834\u5408\u306fNone\u3067OK\n                                     cont_names=contlist, # \u6570\u5b57\u5217\u30ea\u30b9\u30c8\u3002\u306a\u3044\u5834\u5408\u306fNone\u3067OK\n                                     y_names=TARGET, # target\n                                     y_block=CategoryBlock, # CategoryBlock, MultiCategoryBlock, RegressionBlock\u304b\u3089\u9078\u3076 none\u3067\u3082\u53ef \n                                     valid_idx=validind, # Validation\u306eindex\n                                     bs=BATCH_SIZE, # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n                                     shuffle_train=True, # \u5b66\u7fd2\u6642\u306b\u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bfshuffle\u3059\u308b\u304b\n                                     shuffle=False,\n                                     val_shuffle=False,\n                                     n=None, \n                                     device=None, # gpu\u4f7f\u7528\u6642\u306f\"cuda\"\n                                     drop_last=None, # dataloader\u3067batchsize\u306b\u4f59\u308a\u304c\u3067\u305f\u3089\u3001\u5207\u308b\u304b\u3002\n                                     val_bs=BATCH_SIZE * 2 # validation\u306ebatch size\n                                     )\n\n    learn = tabular_learner(dls, # dataset.\u5148\u307b\u3069\u4f5c\u6210\n                            layers=[1000,500,200],  # neuralnet\u306e\u4e2d\u9593\u5c64\n                            emb_szs=None,  \n                            config=None, \n                            n_out=None, # \u51fa\u529b\u3059\u308b\u6570.None\u3060\u3068\u81ea\u52d5\u8a8d\u8b58\u3057\u3066\u304f\u308c\u308b\n                            y_range=None, \n                            loss_func=CrossEntropyLossFlat(), # \u2191\u306e\u30ea\u30f3\u30af\u53c2\u7167\n                            opt_func=Adam,  # optimizer\n                            lr=0.001, # \u5b66\u7fd2\u7387\n                            splitter=trainable_params, \n\n\n                            cbs=[\n\n                                SaveModelCallback(monitor=\"accuracy\",comp=np.greater), # \u5b66\u7fd2\u5f8c\u306b\u4e00\u756a\u826f\u304b\u3063\u305f\u30e2\u30c7\u30ebsave\u3068load \n                            #     EarlyStoppingCallback(monitor=\"accuracy\",comp=np.greater, patience=30), # \u30e2\u30cb\u30bf\u30fc\u3057\u3066patience\u66f4\u65b0\u3057\u306a\u304b\u3063\u305f\u3089\u3084\u3081\u308b\n                            #     GradientClip, # \u91cd\u307f\u3065\u3051\u3092\u3044\u304d\u3059\u304e\u306a\u3044\u3088\u3046\u306b\n                            #     ReduceLROnPlateau(monitor='accuracy',comp=np.greater, patience=10,factor = 10) # \u5b66\u7fd2\u7387\u30e2\u30cb\u30bf\u30fc\u3057\u306a\u304c\u3089\u5909\u3048\u305f\u3044\u4eba\u306f\u3053\u308c\u3092\u5165\u308c\u308b\n                               ],\n\n\n                            metrics=accuracy, # metric \n                            path=None, \n                            model_dir='models', # savepath\u3060\u3068\u601d\u3046\u304c\u3001\u5f8c\u3067\u81ea\u5206\u3067\u3084\u308b\u304b\u3089\u3044\u3044\u3002 \n                            wd=None, \n                            wd_bn_bias=False,\n                            train_bn=True, \n                            moms=(0.95, 0.85, 0.95)\n                            )\n\n\n    learn.fit(200,1e-4) # epoch\u6570\u3001\u5b66\u7fd2\u7387\n    #learn.fit_one_cycle(200,2e-4) # epoch\u6570\u3001\u5b66\u7fd2\u7387\n\n\n    learn.export(f\"fastai_fold{a}.pkl\")\n\n    validdf = train[train[\"fold\"]==a].reset_index(drop=True)\n    test_dl = dls.test_dl(validdf)\n\n    preds,_ = learn.get_preds(dl=test_dl)\n    preds = np.array(preds)\n\n    preds = [s.argmax() for s in preds]\n    preds[:3]\n\n    validdf[\"preds\"] = preds\n    \n    allvaliddf = pd.concat([allvaliddf,validdf]) # \u8ffd\u52a0\n\n    score = np.sum(validdf[\"Survived\"] == validdf[\"preds\"]) \/ len(validdf)\n    scores.append(score)\n","fcfa8e2b":"allvaliddf.head()","bd024c8b":"np.sum(allvaliddf[\"Survived\"] == allvaliddf[\"preds\"]) \/ len(allvaliddf)","48601f4d":"scores","8d3b8efc":"np.mean(scores)","61578c3c":"learn= load_learner(f\"fastai_fold{0}.pkl\",cpu=True) # gpu\u306e\u3068\u304d\u306fcpu=False","f9accabe":"test_dl = dls.test_dl(test)\nallpreds = []\n\nfor a in range(5):\n    learn= load_learner(f\"fastai_fold{a}.pkl\",cpu=True) # gpu\u306e\u3068\u304d\u306fcpu=False\n    preds,_ = learn.get_preds(dl=test_dl)\n    preds = np.array(preds)\n    allpreds.append(preds)\n\n","3682e978":"allpreds = np.mean(allpreds,axis=0)","463c45b3":"allpreds = [s.argmax() for s in allpreds]","f7307f6c":"sample[\"Survived\"] = allpreds","f7488055":"sample.to_csv(\"submission2.csv\",index=False)","399b5dab":"sample","be63d73e":"################  Try one fold (\u307e\u305a\u306f\u4e00\u3064\u306efold\u3060\u3051\u3084\u3063\u3066\u307f\u307e\u3059)\u3002 ######################\n\nThe writing method is slightly different between the classification problem and the regression problem. Here, it is regarded as a classification problem of 0 and 1.\n\n\u5206\u985e\u554f\u984c\u3068\u56de\u5e30\u554f\u984c\u3067\u66f8\u304d\u65b9\u304c\u5c11\u3057\u7570\u306a\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f0,1\u306e\u5206\u985e\u554f\u984c\u3068\u3068\u3089\u3048\u307e\u3059\u3002\n\n\n# 3. Fastai \n## 3.1 Define feature","3429fded":"###### chapter3\u3067\u5fc5\u8981\u306a\u3068\u3053\u308d\u3060\u3051\u307e\u3068\u3081\u308b","7aa6f9c6":"########### calculation out of fold score ################","34ff5b9c":"###### \u4e2d\u8eab\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u306f\u4ee5\u4e0b\u3092\u898b\u308b","3bece091":"# 1.Load data","dddc2449":"############# learn.fit : \u5b66\u7fd2\u7387\u56fa\u5b9a. learn.fit_one_cycle : \u5b66\u7fd2\u7387\u5909\u5316 ##############","2a92b53b":"#### \u901a\u5e38\u306f\u3001\u3053\u3053\u3067\u3001\u6587\u5b57\u5217\u3092\u6570\u5b57\u306b\u5909\u3048\u305f\u308a\u3001NaN\u30c7\u30fc\u30bf\u3092\u57cb\u3081\u305f\u308a\u3057\u307e\u3059\u304c\u3001fastai\u306f\u8a2d\u5b9a\u3059\u308b\u3068\u81ea\u52d5\u3067\u3084\u3063\u3066\u304f\u308c\u307e\u3059\u306e\u3067\u3001\u30d1\u30b9\u3057\u307e\u3059\u3002","3c140177":"## 3.3 make model(dataset + model = learner)","ac64ab5c":"#### for\u6587\u3067\u56de\u3059","b97ccc8c":"## 3.5 inference","97e66855":"# 4. kfold","70f0b3a7":"##### \u5225\u306enotebook\u3067load\u3059\u308b\u3068\u304d\u306f\u3001dls,learn\u304c\u540c\u3058\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u306e\u304c\u3001fastai\u3060\u3068\u3081\u3093\u3069\u304f\u3055\u3044\u3002\n##### torch.save\u3067save\u3057\u305f\u5834\u5408\u306f\u3001\u3067\u304d\u308b\u6c17\u304c\u3059\u308b\u304c\u3001\u6587\u5b57\u5217\u64cd\u4f5c\u306a\u3069\u304c\u308f\u304b\u3063\u3066\u3044\u307e\u305b\u3093\u3002","f047a0c1":"## 3.4 make out of folds","895196e8":"# 2. Kfold","7db0fa84":"######### model\u306e\u4e2d\u8eab\u78ba\u8a8d ###############","bb7511fe":"## 3.2 make dataset\/dataloader","fad15646":"# 5 inference of kfold result","fb28633b":"#### loss\u4e00\u89a7 \n\nhttps:\/\/docs.fast.ai\/losses.html","a93ef70d":"# About this notebook\n\n\u30fb\u3000\u3084\u3063\u3071\u308akeras\u3084pytorch\u306e\u30b3\u30fc\u30c9\u304c\u9577\u304f\u3066\u96e3\u3057\u3044\u3068\u3044\u3046\u65b9\u306b\n\u3000fastai\u3068\u3044\u3046\u30e9\u30a4\u30d6\u30e9\u30ea\u30fc\u3092\u4f7f\u3063\u3066\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7c21\u5358\u306b\u66f8\u304f\u65b9\u6cd5\u3067\u3059\u3002","72bf16be":"## 3.6 submit","050bef09":"###### \u6700\u9069\u5b66\u7fd2\u7387\u306e\u30b5\u30fc\u30c1 ##########"}}