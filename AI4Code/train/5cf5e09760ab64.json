{"cell_type":{"8ebeb8ca":"code","8fa061cb":"code","2874e953":"code","3c2b6cbd":"code","91997527":"code","0bdb63f9":"code","d8139825":"code","3fff3f9c":"code","3055018e":"code","355a8325":"code","3a02ceb7":"code","ac916758":"code","3cdd6e28":"code","14301cb9":"code","62369201":"code","7dcf1d39":"code","14c51b29":"code","ecce92df":"code","857c48b8":"code","87a924ad":"code","2605a533":"code","b9cdcd24":"code","9f741fce":"code","1e7f18b9":"code","66ebf44f":"code","b654fdc9":"code","dae264ee":"code","aed0f0c9":"code","407023c8":"code","f9369f96":"code","4afe22aa":"code","3008d10b":"code","0b141710":"code","3a3d284a":"code","e3ed8e29":"code","41886752":"code","feeafac8":"code","35af62d7":"code","ca51a856":"code","3af56e4d":"code","24c510f3":"code","231a4d73":"code","88634894":"code","af4da76e":"code","278d4835":"code","2490ad6e":"code","23ae335a":"code","c6e333b3":"code","e31edb95":"code","940261dd":"code","d1a8664f":"code","37842b90":"code","0c4e5c69":"code","25e4b5f8":"code","ed229915":"code","4cf00b0d":"code","e13ebb14":"code","49af5d67":"code","ad018ca5":"code","5a6ed17d":"code","dbd96aa0":"code","3b1425d0":"code","7be737be":"code","178caca2":"code","6ae17164":"code","e238f9b0":"code","f0f05b05":"code","6ef4b18b":"code","24cbaf92":"code","858c97b6":"code","cfc7f8ed":"code","3df3a96d":"code","71e718c5":"code","12267f9e":"code","ee8c8968":"code","dfa1bd47":"code","76b4c4b4":"code","afdd6461":"code","99a430a4":"code","be840b94":"code","93926c7f":"code","af18b22a":"code","0e268215":"code","72e88e32":"code","981de9bf":"markdown","601da5e0":"markdown","71314ea4":"markdown","9b8d51f2":"markdown","d54bbd82":"markdown","11d90f09":"markdown","3e764606":"markdown","7d7eb23c":"markdown","cfe74e2d":"markdown","4da38315":"markdown","aab75767":"markdown","aae9cd6c":"markdown","7bdf561c":"markdown","ab22f8d1":"markdown","aaf24ce3":"markdown","36018619":"markdown","e7a6efb7":"markdown","5ea946b0":"markdown","40a9f1d9":"markdown","e628494c":"markdown","de0a0089":"markdown","b6a96a72":"markdown","085db623":"markdown","960cc51c":"markdown","6a0213da":"markdown","399c6e17":"markdown","96b10e89":"markdown","f36d74e0":"markdown","cd2f62e6":"markdown","59c47364":"markdown","366a5fea":"markdown","14bdfad3":"markdown","561b448f":"markdown","eb9e1f39":"markdown","5c9ccc84":"markdown","f04c0926":"markdown","cfa22f00":"markdown","86da4e66":"markdown","55ef4a2a":"markdown","7538e416":"markdown","86aef00c":"markdown","eec1fa41":"markdown","95d812e9":"markdown","ca33b5a9":"markdown","c1c1364c":"markdown","790f76e6":"markdown","ce3677a8":"markdown","2226803c":"markdown","cde4413d":"markdown","52e38643":"markdown","5702ec63":"markdown","0db635cd":"markdown","30a8867b":"markdown","a003865e":"markdown","0332a25a":"markdown","d2d43407":"markdown","c9b3a5e8":"markdown","5615a8e4":"markdown","9a254685":"markdown","63282e72":"markdown","9b8108c5":"markdown","50de053a":"markdown","6abfd643":"markdown","81ad336b":"markdown","ba1cbf02":"markdown","c7a5fddb":"markdown","14785a51":"markdown","e9d801ed":"markdown","c501ea3e":"markdown","7e3473c9":"markdown","575b9ee0":"markdown","a57727c1":"markdown","9542f805":"markdown","f33757e2":"markdown","328d9638":"markdown","19790550":"markdown","aac3a7df":"markdown","3cb17151":"markdown","f866dc08":"markdown","db42290d":"markdown","138400ba":"markdown"},"source":{"8ebeb8ca":"!pip install lightgbm","8fa061cb":"!pip install chart_studio\n!sudo pip install chart_studio","2874e953":"!pip install xgboost","3c2b6cbd":"!pip install plotly==3.10.0","91997527":"!pip install git+https:\/\/github.com\/hyperopt\/hyperopt.git","0bdb63f9":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 200})\n%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nfrom numpy import array\nfrom matplotlib import cm\nfrom sklearn import preprocessing\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\n\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))\n","d8139825":"warnings.simplefilter(\"ignore\")\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","3fff3f9c":"df_sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\ndf_test_identity = pd.read_csv(\"..\/input\/test_identity.csv\")\ndf_test_transaction = pd.read_csv(\"..\/input\/test_transaction.csv\")\ndf_train_identity = pd.read_csv(\"..\/input\/train_identity.csv\")\ndf_train_transaction = pd.read_csv(\"..\/input\/train_transaction.csv\")","3055018e":"print('Size of df_sample_submission data', df_sample_submission.shape)\nprint('Size of df_test_identity data', df_test_identity.shape)\nprint('Size of df_test_transaction data', df_test_transaction.shape)\nprint('Size of df_train_identity data', df_train_identity.shape)\nprint('Size of df_train_transaction data', df_train_transaction.shape)","355a8325":"df_test_identity.head(5)","3a02ceb7":"df_test_transaction.head()","ac916758":"df_train_identity.head()","3cdd6e28":"df_train_transaction.head()","14301cb9":"df_sample_submission.head(10)","62369201":"x= df_train_transaction['isFraud'].value_counts().values\nsns.barplot([0,1],x)\nplt.title('Target variable count')","7dcf1d39":"Ccols= df_train_transaction.columns[df_train_transaction.columns.str.startswith('C')]\ndf_train_transaction[Ccols].describe()","14c51b29":"missing_values_count = df_train_transaction.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(df_train_transaction.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","ecce92df":"missing_values_count = df_train_identity.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(df_train_identity.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","857c48b8":"df_id = pd.read_csv(\"..\/input\/train_identity.csv\")\ndf_trans = pd.read_csv(\"..\/input\/train_transaction.csv\")","87a924ad":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng = sns.distplot(df_trans[df_trans['TransactionAmt'] <= 1000]['TransactionAmt'])\ng.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng1 = sns.distplot(np.log(df_trans['TransactionAmt']))\ng1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\n\nplt.subplot(212)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                 label='Fraud', alpha=.2)\ng4= plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\", fontsize=18)\ng4 = plt.xlabel(\"Index\")\ng4 = plt.ylabel(\"Amount Distribution\", fontsize=15)\ng4 = plt.legend()\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(321)\ng = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]), \n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                label='isFraud', alpha=.4)\nplt.title(\"FRAUD - Transaction Amount ECDF\", fontsize=18)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Amount Distribution\", fontsize=12)\n\nplt.subplot(322)\ng1 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng1= plt.title(\"NO FRAUD - Transaction Amount ECDF\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n\nplt.suptitle('Individual ECDF Distribution', fontsize=22)\n\nplt.show()","2605a533":"df_train_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ndf_test_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()","b9cdcd24":"ax = df_train_transaction.plot(x='TransactionDT',\n                       y='TransactionAmt',\n                       kind='scatter',\n                       alpha=0.01,\n                       label='TransactionAmt-train',\n                       title='Train and test Transaction Ammounts by Time (TransactionDT)',\n                       ylim=(0, 5000),\n                       figsize=(15, 5))\ndf_test_transaction.plot(x='TransactionDT',\n                      y='TransactionAmt',\n                      kind='scatter',\n                      label='TransactionAmt-test',\n                      alpha=0.01,\n                      color=color_pal[1],\n                       ylim=(0, 5000),\n                      ax=ax)\n# Plot Fraud as Orange\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 1] \\\n    .plot(x='TransactionDT',\n         y='TransactionAmt',\n         kind='scatter',\n         alpha=0.01,\n         label='TransactionAmt-train',\n         title='Train and test Transaction Ammounts by Time (TransactionDT)',\n         ylim=(0, 5000),\n         color='orange',\n         figsize=(15, 5),\n         ax=ax)\nplt.show()","9f741fce":"print('  {:.4f}% of Transactions that are fraud in train '.format(df_train_transaction['isFraud'].mean() * 100))","1e7f18b9":"df_train_transaction.groupby('isFraud') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribution of Target in Train',\n          figsize=(15, 3))\nplt.show()","66ebf44f":"df_train_transaction['TransactionAmt'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Log Transaction Amt')\nplt.show()","b654fdc9":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 6))\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt.Gunlk islm kay - Fraud.Dolandiricilik',\n          color=color_pal[1],\n          xlim=(-3, 10),\n         ax= ax1)\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Not Fraud',\n          color=color_pal[2],\n          xlim=(-3, 10),\n         ax=ax2)\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Fraud',\n          color=color_pal[1],\n         ax= ax3)\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt.\u0130\u015flem Tutar\u0131 - Not Fraud.Sahtekarl\u0131k De\u011fil',\n          color=color_pal[2],\n         ax=ax4)\nplt.show()","dae264ee":"print('Mean transaction amt for fraud is {:.4f}'.format(df_train_transaction.loc[df_train_transaction['isFraud'] == 1]['TransactionAmt'].mean()))\nprint('Mean transaction amt for non-fraud is {:.4f}'.format(df_train_transaction.loc[df_train_transaction['isFraud'] == 0]['TransactionAmt'].mean()))","aed0f0c9":"df_train_transaction.groupby('ProductCD') \\\n    ['TransactionID'].count() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Count of Observations by ProductCD. Urun gozlem say\u0131lar\u0131')\nplt.show()","407023c8":"df_train_transaction.groupby('ProductCD')['isFraud'] \\\n    .mean() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Percentage of Fraud by ProductCD.Sahtekarlik %')\nplt.show()","f9369f96":"card_cols = [c for c in df_train_transaction.columns if 'card' in c]\ndf_train_transaction[card_cols].head()","4afe22aa":"color_idx = 0\nfor c in card_cols:\n    if df_train_transaction[c].dtype in ['float64','int64']:\n        df_train_transaction[c].plot(kind='hist',\n                                      title=c,\n                                      bins=50,\n                                      figsize=(15, 2),\n                                      color=color_pal[color_idx])\n    color_idx += 1\n    plt.show()","3008d10b":"df_train_transaction_fr = df_train_transaction.loc[df_train_transaction['isFraud'] == 1]\ndf_train_transaction_nofr = df_train_transaction.loc[df_train_transaction['isFraud'] == 0]\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\ndf_train_transaction_fr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax1, title='Count of card4 fraud')\ndf_train_transaction_nofr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax2, title='Count of card4 non-fraud')\ndf_train_transaction_fr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax3, title='Count of card6 fraud')\ndf_train_transaction_nofr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax4, title='Count of card6 non-fraud')\nplt.show()","0b141710":"print(' addr1 - has {} NA values'.format(df_train_transaction['addr1'].isna().sum()))\nprint(' addr2 - has {} NA values'.format(df_train_transaction['addr2'].isna().sum()))","3a3d284a":"df_train_transaction['addr1'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr1 distribution dagilimi')\nplt.show()\ndf_train_transaction['addr2'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr2 distribution dagilimi')\nplt.show()","e3ed8e29":"df_train_transaction['dist1'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist1 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()\ndf_train_transaction['dist2'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist2 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()","41886752":"df_id = pd.read_csv(\"..\/input\/train_identity.csv\")\ndf_trans = pd.read_csv(\"..\/input\/train_transaction.csv\")","feeafac8":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) \/ len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","35af62d7":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\ntotal = len(df_trans)\ntotal_amt = df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(16,6))\n\nplt.subplot(121)\ng = sns.countplot(x='isFraud', data=df_trans, )\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15) \n\nperc_amt = (df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\nperc_amt = perc_amt.reset_index()\nplt.subplot(122)\ng1 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=perc_amt)\ng1.set_title(\"% Total Amount in Transaction Amt \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng1.set_xlabel(\"Is fraud?\", fontsize=18)\ng1.set_ylabel('Total Transaction Amount Scalar', fontsize=18)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total_amt * 100),\n            ha=\"center\", fontsize=15) \n    \nplt.show()","ca51a856":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\nprint(\"Transaction Amounts Quantiles:\")\nprint(df_trans['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","3af56e4d":"print(pd.concat([df_trans[df_trans['isFraud'] == 1]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index(), \n                 df_trans[df_trans['isFraud'] == 0]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index()],\n                axis=1, keys=['Fraud', \"No Fraud\"]))","24c510f3":"CalcOutliers(df_trans['TransactionAmt'])","231a4d73":"tmp = pd.crosstab(df_trans['ProductCD'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('ProductCD Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='ProductCD', data=df_trans)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\ng.set_title(\"ProductCD Distribution\", fontsize=19)\ng.set_xlabel(\"ProductCD Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\ng.set_ylim(0,500000)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \n\n    plt.subplot(222)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=df_trans)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='ProductCD', y='Fraud', data=tmp, color='black', order=['W', 'H',\"C\", \"S\", \"R\"], legend=False)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\ng1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"ProductCD Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"ProductCD Name\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","88634894":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    df_trans[col] = df_trans[col].fillna(\"Miss\")\n    \ndef ploting_dist_ratio(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{col} Distributions ', fontsize=22)\n\n    plt.subplot(121)\n    g = sns.countplot(x=col, data=df, order=list(tmp[col].values))\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n    g.set_title(f\"{col} Distribution\\nCound and %Fraud by each category\", fontsize=18)\n    g.set_ylim(0,400000)\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,20)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    for p in gt.patches:\n        height = p.get_height()\n        gt.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=14) \n        \n    perc_amt = (df_trans.groupby(['isFraud',col])['TransactionAmt'].sum() \/ total_amt * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.subplot(122)\n    g1 = sns.boxplot(x=col, y='TransactionAmt', hue='isFraud', \n                     data=df[df['TransactionAmt'] <= lim], order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,5)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g1.set_title(f\"{col} by Transactions dist\", fontsize=18)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Amount(U$)\", fontsize=16)\n        \n    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n    \n    plt.show()\n","af4da76e":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    ploting_dist_ratio(df_trans, col, lim=2500)","278d4835":"print(\"Card Features Quantiles: \")\nprint(df_trans[['addr1', 'addr2']].quantile([0.01, .025, .1, .25, .5, .75, .90,.975, .99]))","2490ad6e":"df_trans.loc[df_trans.addr1.isin(df_trans.addr1.value_counts()[df_trans.addr1.value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ndf_trans.loc[df_trans.addr2.isin(df_trans.addr2.value_counts()[df_trans.addr2.value_counts() <= 50 ].index), 'addr2'] = \"Others\"","23ae335a":" def ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                \/ df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()\n    \nploting_cnt_amt(df_trans, 'addr1')","c6e333b3":"ploting_cnt_amt(df_trans, 'addr2')","e31edb95":"df_trans.loc[df_trans['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.P_emaildomain.isin(df_trans.P_emaildomain\\\n                                         .value_counts()[df_trans.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ndf_trans.P_emaildomain.fillna(\"NoInf\", inplace=True)","940261dd":"ploting_cnt_amt(df_trans, 'P_emaildomain')","d1a8664f":"df_trans.loc[df_trans['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.R_emaildomain.isin(df_trans.R_emaildomain\\\n                                         .value_counts()[df_trans.R_emaildomain.value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ndf_trans.R_emaildomain.fillna(\"NoInf\", inplace=True)","37842b90":"ploting_cnt_amt(df_trans, 'R_emaildomain')","0c4e5c69":"## REducing memory\ndf_trans = reduce_mem_usage(df_trans)\ndf_id = reduce_mem_usage(df_id)","25e4b5f8":"df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n                      'C9', 'C10', 'C11', 'C12', 'C13', 'C14']].describe()","ed229915":"df_trans.loc[df_trans.C1.isin(df_trans.C1\\\n                              .value_counts()[df_trans.C1.value_counts() <= 400 ]\\\n                              .index), 'C1'] = \"Others\"","4cf00b0d":"ploting_cnt_amt(df_trans, 'C1')","e13ebb14":"df_trans.loc[df_trans.C2.isin(df_trans.C2\\\n                              .value_counts()[df_trans.C2.value_counts() <= 350 ]\\\n                              .index), 'C2'] = \"Others\"","49af5d67":"ploting_cnt_amt(df_trans, 'C2')","ad018ca5":"# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100400#latest-579480\nimport datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_trans[\"Date\"] = df_trans['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek\ndf_trans['_Hours'] = df_trans['Date'].dt.hour\ndf_trans['_Days'] = df_trans['Date'].dt.day","5a6ed17d":"from functools import partial\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))","dbd96aa0":"ploting_cnt_amt(df_trans, '_Days')","3b1425d0":"ploting_cnt_amt(df_trans, '_Weekdays')","7be737be":"ploting_cnt_amt(df_trans, '_Hours')","178caca2":"df_id[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']].describe(include='all')","6ae17164":"df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True)","e238f9b0":"def cat_feat_ploting(df, col):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(14,10))\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n\n    plt.subplot(221)\n    g = sns.countplot(x=col, data=df, order=tmp[col].values)\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\n    g.set_title(f\"{col} Distribution\", fontsize=19)\n    g.set_xlabel(f\"{col} Name\", fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    # g.set_ylim(0,500000)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=14) \n\n    plt.subplot(222)\n    g1 = sns.countplot(x=col, hue='isFraud', data=df, order=tmp[col].values)\n    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, color='black', order=tmp[col].values, legend=False)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\n    g1.set_title(f\"{col} by Target(isFraud)\", fontsize=19)\n    g1.set_xlabel(f\"{col} Name\", fontsize=17)\n    g1.set_ylabel(\"Count\", fontsize=17)\n\n    plt.subplot(212)\n    g3 = sns.boxenplot(x=col, y='TransactionAmt', hue='isFraud', \n                       data=df[df['TransactionAmt'] <= 2000], order=tmp[col].values )\n    g3.set_title(\"Transaction Amount Distribuition by ProductCD and Target-\", fontsize=20)\n    g3.set_xlabel(\"ProductCD Name\", fontsize=17)\n    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n\n    plt.subplots_adjust(hspace = 0.4, top = 0.85)\n\n    plt.show()","f0f05b05":"for col in ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n    df_train[col] = df_train[col].fillna('NaN')\n    cat_feat_ploting(df_train, col)","6ef4b18b":"df_trans = pd.read_csv('..\/input\/train_transaction.csv')\ndf_test_trans = pd.read_csv('..\/input\/test_transaction.csv')\n\ndf_id = pd.read_csv('..\/input\/train_identity.csv')\ndf_test_id = pd.read_csv('..\/input\/test_identity.csv')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n# y_train = df_train['isFraud'].copy()\ndel df_trans, df_id, df_test_trans, df_test_id\n","24cbaf92":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    df_train[c + '_bin'] = df_train[c].map(emails)\n    df_test[c + '_bin'] = df_test[c].map(emails)\n    \n    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","858c97b6":"!pip install catboost","cfc7f8ed":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\nimport gc\nimport time\nfrom contextlib import contextmanager","3df3a96d":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))","71e718c5":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train,test,verbose=False):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations, train_df, test_df, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,train,test):\n    nm = col1+'_'+col2\n    train[nm] = train[col1].astype(str)+'_'+train[col2].astype(str)\n    test[nm] = test[col1].astype(str)+'_'+test[col2].astype(str) \n    encode_LE(nm,train,test)\n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df, test_df):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","12267f9e":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","ee8c8968":"def comb_mails(emails,us_emails,train,test):\n    for c in ['P_emaildomain', 'R_emaildomain']:\n                train[c + '_bin'] = train[c].map(emails)\n                test[c + '_bin'] = test[c].map(emails)\n\n                train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n                test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n\n                train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n                test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","dfa1bd47":"!pip install future","76b4c4b4":"def love(data):\n    print('\\n'.join([''.join([(' I_Love_Data_Science_'[(x-y) % len('I_Love_Data_Science_')] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 <= 0 else ' ') for x in range(-30, 30)]) for y in range(15, -15, -1)]))","afdd6461":"        train_id= pd.read_csv(\"..\/input\/train_identity.csv\")\n        test_id = pd.read_csv(\"..\/input\/test_identity.csv\")\n        train_tr = pd.read_csv(\"..\/input\/train_transaction.csv\")\n        test_tr = pd.read_csv(\"..\/input\/test_transaction.csv\")","99a430a4":"def combine():\n     with timer('Combining :'):   \n        print('Combining Start...')\n        # Read train and test data with pd.read_csv():\n        train_id= pd.read_csv(\"..\/input\/train_identity.csv\")\n        test_id = pd.read_csv(\"..\/input\/test_identity.csv\")\n        train_tr = pd.read_csv(\"..\/input\/train_transaction.csv\")\n        test_tr = pd.read_csv(\"..\/input\/test_transaction.csv\")\n        train=pd.merge(train_tr, train_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\n        test=pd.merge(test_tr, test_id, on = \"TransactionID\",how=\"left\",left_index=True, right_index=True)\n        del train_id, train_tr, test_id, test_tr\n        test.columns=train.columns.drop(\"isFraud\")\n    \n        \n        \n\n        return train,test","be840b94":"test_tr.head()","93926c7f":"def pre_processing_and_feature_engineering():\n    train,test=combine()   \n    \n    with timer('Preprocessing and Feature Engineering'):\n        print('-' * 30)\n        print('Preprocessing and Feature Engineering start...')\n        print('-' * 10)\n        \n        emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n              'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n              'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',     \n              'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n              'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n              'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n              'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n              'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n              'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n              'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n              'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n              'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n              'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n              'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n              'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n              'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n              'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n              'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n              'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\n        us_emails = ['gmail', 'net', 'edu']\n        comb_mails(emails,us_emails,train,test)\n        print(\"Creating New Features...\")\n        numeric=['TransactionAmt','D4','D9','D10','D15', 'C1' , 'C2' ,\n               'C3' , 'C4' , 'C5' , 'C6' , 'C7' , 'C8' , 'C9' , 'C10' , \n                'C11' , 'C12' , 'C13' , 'C14','V95' ,'V279' , 'V281',\n                'V284' ,'V285' , 'V286' , 'V288' , 'V290', 'V296',\n                 'V297',\"C11\",\"D14\", \"id_05\",\"id_06\",\"C2\", \"id_13\",\"D3\",\"D11\",\n                \"C14\",\"D5\", \"D9\", \"C1\", \"dist2\", \"D1\", \"D2\", \"C13\", \"id_20\", \"id_19\", \"D4\",\n                \"D8\", \"dist1\", \"id_02\", \"D15\", \"addr1\", \"card2\", \"day\", \"id_19\", \"id_02\"]\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        encode_CB('card1','addr1',train,test)\n        train['day'] = train.TransactionDT \/ (24*60*60)\n        train['uid'] = train.card1_addr1.astype(str)+'_'+np.floor(train.day-train.D1).astype(str)\n\n        test['day'] = test.TransactionDT \/ (24*60*60)\n        test['uid'] = test.card1_addr1.astype(str)+'_'+np.floor(test.day-test.D1).astype(str)\n        encode_FE(train,test,['uid'])\n        encode_AG(numeric, ['uid'],['mean',\"std\"], train, test)\n        encode_AG2(['P_emaildomain','dist1','id_02',\"M4\",\"P_emaildomain\", \"M5\", \"id_31\"], ['uid'], train, test)\n        # FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        # COMBINE COLUMNS CARD1+ADDR1+P_EMAILDOMAIN\n        encode_CB('card1_addr1','P_emaildomain',train, test)\n        # FREQUENCY ENOCDE\n        encode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n        # GROUP AGGREGATE\n        encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],train,test,usena=True)\n        del train['uid'], test['uid']\n        print(\"Creating New Features Finished\")\n        print('-' * 10)\n        print('Label Coding and One Hot Encoding Start...')\n        train_cat = train.select_dtypes(include=['object'])\n        train_cat_columns=train_cat.columns\n        train_columns=train.columns\n        test_cat = test.select_dtypes(include=['object'])\n        test_cat_columns=test_cat.columns\n        del test_cat, train_cat\n        from sklearn import preprocessing\n        for i in train_cat_columns: \n            lbe=preprocessing.LabelEncoder()\n            train[i]=lbe.fit_transform(train[i].astype(str))\n        for i in test_cat_columns:    \n            test[i]=lbe.fit_transform(test[i].astype(str))\n        for i in train_cat_columns:\n            if (test[i].max()== train[i].max())&(train[i].max()<6):\n                    test = pd.get_dummies(test, columns = [i])\n                    train=pd.get_dummies(train, columns = [i])\n          \n        \n        import datetime\n        START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n        train['DT_M'] = train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        train['DT_M'] = (train['DT_M'].dt.year-2017)*12 + train['DT_M'].dt.month \n\n        test['DT_M'] = test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        test['DT_M'] = (test['DT_M'].dt.year-2017)*12 + test['DT_M'].dt.month \n         \n        print('-' * 10)\n        return train,test","af18b22a":"def pre_processing_and_feature_engineering():\n    train,test=combine()   \n    \n    with timer('Preprocessing and Feature Engineering'):\n        print('-' * 30)\n        print('Preprocessing and Feature Engineering start...')\n        print('-' * 10)\n        \n        emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n              'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n              'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',     \n              'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n              'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n              'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n              'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n              'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n              'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n              'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n              'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n              'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n              'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n              'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n              'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n              'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n              'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n              'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n              'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\n        us_emails = ['gmail', 'net', 'edu']\n        comb_mails(emails,us_emails,train,test)\n        print(\"Creating New Features...\")\n        numeric=['TransactionAmt','D4','D9','D10','D15', 'C1' , 'C2' ,\n               'C3' , 'C4' , 'C5' , 'C6' , 'C7' , 'C8' , 'C9' , 'C10' , \n                'C11' , 'C12' , 'C13' , 'C14','V95' ,'V279' , 'V281',\n                'V284' ,'V285' , 'V286' , 'V288' , 'V290', 'V296',\n                 'V297',\"C11\",\"D14\", \"id_05\",\"id_06\",\"C2\", \"id_13\",\"D3\",\"D11\",\n                \"C14\",\"D5\", \"D9\", \"C1\", \"dist2\", \"D1\", \"D2\", \"C13\", \"id_20\", \"id_19\", \"D4\",\n                \"D8\", \"dist1\", \"id_02\", \"D15\", \"addr1\", \"card2\", \"day\", \"id_19\", \"id_02\"]\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        encode_CB('card1','addr1',train,test)\n        train['day'] = train.TransactionDT \/ (24*60*60)\n        train['uid'] = train.card1_addr1.astype(str)+'_'+np.floor(train.day-train.D1).astype(str)\n\n        test['day'] = test.TransactionDT \/ (24*60*60)\n        test['uid'] = test.card1_addr1.astype(str)+'_'+np.floor(test.day-test.D1).astype(str)\n        encode_FE(train,test,['uid'])\n        encode_AG(numeric, ['uid'],['mean',\"std\"], train, test)\n        encode_AG2(['P_emaildomain','dist1','id_02',\"M4\",\"P_emaildomain\", \"M5\", \"id_31\"], ['uid'], train, test)\n        # FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        # COMBINE COLUMNS CARD1+ADDR1+P_EMAILDOMAIN\n        encode_CB('card1_addr1','P_emaildomain',train, test)\n        # FREQUENCY ENOCDE\n        encode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n        # GROUP AGGREGATE\n        encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],train,test,usena=True)\n        del train['uid'], test['uid']\n        print(\"Creating New Features Finished\")\n        print('-' * 10)\n        print('Label Coding and One Hot Encoding Start...')\n        train_cat = train.select_dtypes(include=['object'])\n        train_cat_columns=train_cat.columns\n        train_columns=train.columns\n        test_cat = test.select_dtypes(include=['object'])\n        test_cat_columns=test_cat.columns\n        del test_cat, train_cat\n        from sklearn import preprocessing\n        for i in train_cat_columns: \n            lbe=preprocessing.LabelEncoder()\n            train[i]=lbe.fit_transform(train[i].astype(str))\n        for i in test_cat_columns:    \n            test[i]=lbe.fit_transform(test[i].astype(str))\n        for i in train_cat_columns:\n            if (test[i].max()== train[i].max())&(train[i].max()<6):\n                    test = pd.get_dummies(test, columns = [i])\n                    train=pd.get_dummies(train, columns = [i])\n          \n        \n        import datetime\n        START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n        train['DT_M'] = train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        train['DT_M'] = (train['DT_M'].dt.year-2017)*12 + train['DT_M'].dt.month \n\n        test['DT_M'] = test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        test['DT_M'] = (test['DT_M'].dt.year-2017)*12 + test['DT_M'].dt.month \n         \n        print('-' * 10)\n        return train,test","0e268215":"def main():\n    with timer('Full Model Run '):\n        print(\"Full Model Run Start...\")\n        print('-' * 50)\n        data=feature_importance()\n        love(data)   \n        print('-' * 50)","72e88e32":"if __name__ == \"__main__\":\n     main()","981de9bf":"## Libraries","601da5e0":"- I will group all e-mail domains by the respective enterprises.\n- Also, I will set as \"Others\" all values with less than 500 entries.\n\n- T\u00fcm e-posta alan adlar\u0131n\u0131 ilgili kurulu\u015flara g\u00f6re grupland\u0131raca\u011f\u0131m.\n- Ayr\u0131ca, 500'den az giri\u015fi olan t\u00fcm de\u011ferleri 'Di\u011ferleri' olarak ayarlayaca\u011f\u0131m.","71314ea4":"#### The graph below shows number of unique values in each of the C Columns as blue bars.Orange bar shows the number of unique values in 96.5% of the data in each of the columns. The difference between the two bars is a measure of how distributed the data is across the range of unique values in the column.Red line is the percentage of missing values in the columns.\n\n* **A\u015fa\u011f\u0131daki grafik, C S\u00fctunlar\u0131n\u0131n her birindeki benzersiz de\u011ferlerin say\u0131s\u0131n\u0131 mavi \u00e7ubuklar olarak g\u00f6sterir. Turuncu \u00e7ubuk, her bir s\u00fctundaki verilerin% 96.5'indeki benzersiz de\u011ferlerin say\u0131s\u0131n\u0131 g\u00f6sterir. \u0130ki \u00e7ubuk aras\u0131ndaki fark, verilerin s\u00fctundaki benzersiz de\u011ferler aral\u0131\u011f\u0131nda ne kadar da\u011f\u0131ld\u0131\u011f\u0131n\u0131n bir \u00f6l\u00e7\u00fcs\u00fcd\u00fcr.K\u0131rm\u0131z\u0131 \u00e7izgi, s\u00fctunlardaki eksik de\u011ferlerin y\u00fczdesidir.**","9b8d51f2":"# Importing necessary libraries","d54bbd82":"### Seeing the Quantiles of Fraud and No Fraud Transactions-Doland\u0131r\u0131c\u0131l\u0131k Miktarlar\u0131n\u0131 G\u00f6rme ve Doland\u0131r\u0131c\u0131l\u0131k \u0130\u015flemleri Yok","11d90f09":"## Ploting Transaction Amount Values Distribution \n## \u0130\u015flem Tutar\u0131 De\u011ferleri Da\u011f\u0131l\u0131m\u0131","3e764606":"* The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp). One early discovery about the data is that the train and test appear to be split by time. There is a slight gap inbetween, but otherwise the training set is from an earlier period of time and test is from a later period of time. This will impact which cross validation techniques should be used.\n\n* We will look into this more when reviewing differences in distribution of features between train and test.\n\n* TransactionDT \u00f6zelli\u011fi, belirli bir referans tarih saatinden (ger\u00e7ek bir zaman damgas\u0131 de\u011fil) bir zaman \u00e7izelgesidir. Verilerle ilgili erken bir ke\u015fif, tren ve testin zamana b\u00f6l\u00fcnm\u00fc\u015f gibi g\u00f6r\u00fcnmesidir. Arada hafif bir bo\u015fluk var, ancak aksi takdirde e\u011fitim seti daha \u00f6nceki bir zamandan ve test daha sonraki bir zaman diliminden. Bu, hangi \u00e7apraz validasyon tekniklerinin kullan\u0131lmas\u0131 gerekti\u011fini etkileyecektir.\n\n* \u00d6zelliklerin tren ve test aras\u0131ndaki da\u011f\u0131l\u0131m\u0131ndaki farkl\u0131l\u0131klar\u0131 incelerken bunu daha ayr\u0131nt\u0131l\u0131 inceleyece\u011fiz.","7d7eb23c":"# TransactionAmt","cfe74e2d":"- It's considering outlier values that are highest than 3 times the std from the mean\n\n- Ortalamadan 3 kat daha y\u00fcksek ayk\u0131r\u0131 de\u011ferleri dikkate al\u0131yor","4da38315":"#### Ploting columns with few unique values - Birka\u00e7 benzersiz de\u011fere sahip s\u00fctunlar\u0131 \u00e7izme","aab75767":"### ProductCD\n- For now we don't know exactly what these values represent.\n- `W` has the most number of observations, `C` the least.\n- ProductCD `C` has the most fraud with >11%\n- ProductCD `W` has the least with ~2%\n\n### ProductCD \n* \u015eimdilik bu de\u011ferlerin tam olarak neyi temsil etti\u011fini bilmiyoruz. \n* ProductCD W en fazla g\u00f6zlem, C en az g\u00f6zlem i\u00e7erir. \n* ProductCD  C>% 11 ile en fazla doland\u0131r\u0131c\u0131l\u0131\u011fa sahiptir \n* ProductCDCD W ~% 2 ile en d\u00fc\u015f\u00fck de\u011fere sahiptir","aae9cd6c":"## addr1 & addr2","7bdf561c":"#### Now, let's known the Product Feature\n* Distribution Products\n* Distribution of Frauds by Product\n* Has Difference between Transaction Amounts in Products?\n\n \n* Da\u011f\u0131t\u0131m \u00dcr\u00fcnleri\n* Sahtekarl\u0131klar\u0131n \u00dcr\u00fcne G\u00f6re Da\u011f\u0131l\u0131m\u0131\n* \u00dcr\u00fcnlerde \u0130\u015flem Tutar\u0131 Aras\u0131ndaki Fark Nedir?","ab22f8d1":"### Ploting P-Email Domain","aaf24ce3":"### M distributions:  Count, %Fraud and Transaction Amount distribution","36018619":"- **We have 3.5% of Fraud transactions in our dataset.I think that it would be interesting to see if the amount percentual is higher or lower than 3.5% of total. I will see it later.We have the same % when considering the Total Transactions Amount by Fraudand No Fraud.Let's explore the Transaction amount further below.**\n\n* **Veri setimizde Sahtekarl\u0131k i\u015flemlerinin% 3,5'ine sahibiz. Y\u00fczdelik miktar\u0131n toplam\u0131n% 3,5'inden y\u00fcksek veya d\u00fc\u015f\u00fck olup olmad\u0131\u011f\u0131n\u0131 g\u00f6rmek ilgin\u00e7 olaca\u011f\u0131n\u0131 d\u00fc\u015f\u00fcn\u00fcyorum. Daha sonra g\u00f6rece\u011fim. Sahtekarl\u0131k ve Sahtekarl\u0131k Olmadan Toplam \u0130\u015flem Tutar\u0131 g\u00f6z \u00f6n\u00fcne al\u0131nd\u0131\u011f\u0131nda ayn\u0131% oran\u0131na sahibiz. \u0130\u015flem tutar\u0131n\u0131 a\u015fa\u011f\u0131da daha ayr\u0131nt\u0131l\u0131 inceleyelim.**","e7a6efb7":"#### C1 Distribution Plot","5ea946b0":"## Categorical Features - Transaction\n\n* ProductCD\n* emaildomain\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* _emaildomain\n* M1 - M9","40a9f1d9":"![fraud](https:\/\/abcountrywide.com.au\/wp-content\/uploads\/2018\/04\/fraud.jpg)Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren\u2019t thinking about the data science that determined your fate.\n\nEmbarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. \u201cPress 1 if you really tried to spend $500 on cheddar cheese.\u201d\n\nWhile perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle.\n\nIEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.\n\nIn this competition, you\u2019ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.\n\nIf successful, you\u2019ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.\n\nAcknowledgements:\n\n","e628494c":"### Mapping emails","de0a0089":"## Target Distribution","b6a96a72":"TransactionDT \u00f6zelli\u011fi, belirli bir referans tarih saatinden (ger\u00e7ek bir zaman damgas\u0131 de\u011fil) bir zaman \u00e7izelgesidir.","085db623":"- **Transaction Amount Distribuition by ProductCD and Target**\n- **\u00dcr\u00fcn CD'si ve Hedefe G\u00f6re \u0130\u015flem Tutar\u0131 Da\u011f\u0131l\u0131m\u0131**","960cc51c":"#### Ploting Hours Distributions - \u00c7izim Saatleri Da\u011f\u0131l\u0131mlar\u0131","6a0213da":"* There is clearly a class imbalace problem.\n* We will look into methods of solving this issue later in this notebook.\n\n* A\u00e7\u0131k\u00e7a bir s\u0131n\u0131f dengesizli\u011fi sorunu var.\n* Bu sorunu daha sonra bu not defterinde \u00e7\u00f6zme y\u00f6ntemlerine bakaca\u011f\u0131z.","399c6e17":"## Fraud by card types . Kart cinslerine g\u00f6re sahtecilik","96b10e89":"#### Ploting WeekDays Distributions - Haftai\u00e7i Da\u011f\u0131l\u0131mlar","f36d74e0":"### Addr2 Distributions","cd2f62e6":"### Converting to Total Days, Weekdays and Hours","59c47364":"- We can see a very similar distribution in both email domain features.\n- It's interesting that we have high values in google and icloud frauds\n\n- Her iki e-posta alan \u00f6zelli\u011finde de benzer bir da\u011f\u0131l\u0131m g\u00f6rebiliriz.\n- Google ve icloud sahtekarl\u0131klar\u0131nda y\u00fcksek de\u011ferlere sahip olmam\u0131z ilgin\u00e7","366a5fea":"* We can note interesting patterns on Addr1.\n* Addr1'de ilgin\u00e7 desenler not edebiliriz.","14bdfad3":"# Distribution of Target in Training Set","561b448f":"* **This graphs give us many interesting intuition about the M features.**\n* **Only in M4 the Missing values haven't the highest % of Fraud.**\n** **Bu grafikler bize M \u00f6zellikleri hakk\u0131nda bir\u00e7ok ilgin\u00e7 sezgi veriyor.**\n** **Sadece M4'te Eksik de\u011ferler Sahtekarl\u0131k'\u0131n en y\u00fcksek y\u00fczdesine sahip de\u011fildir.**","eb9e1f39":"## Sorular","5c9ccc84":"![](https:\/\/www.xenonstack.com\/wp-content\/uploads\/xenonstack-credit-card-fraud-detection.png)","f04c0926":"# Now, let's known the Product Feature \u015eimdi \u00dcr\u00fcn \u00d6zelli\u011fini bilelim\n- Distribution Products\n- Distribution of Frauds by Product\n- Has Difference between Transaction Amounts in Products? \n\n\n- Da\u011f\u0131t\u0131m \u00dcr\u00fcnleri\n- Sahtekarl\u0131klar\u0131n \u00dcr\u00fcne G\u00f6re Da\u011f\u0131l\u0131m\u0131\n- \u00dcr\u00fcnlerde \u0130\u015flem Tutarlar\u0131 Aras\u0131nda Fark Var m\u0131?","cfa22f00":"We are told in the data description that the following transaction columns are categorical:\nVeri a\u00e7\u0131klamas\u0131nda a\u015fa\u011f\u0131daki i\u015flem s\u00fctunlar\u0131n\u0131n kategoriktir.    \n\n* ProductCD\n* emaildomain\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9","86da4e66":"- Let's understand what this features are.\n- What's the distributions? \n\n Bu \u00f6zelliklerin ne oldu\u011funu anlayal\u0131m.\n- Da\u011f\u0131l\u0131mlar nedir?","55ef4a2a":"#### Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios\n* **Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios**","7538e416":"> # Preprocessing and Feature Engineering","86aef00c":"##  Questions","eec1fa41":"### Ploting Transaction Amount Values Distribution- \u0130\u015flem Tutar\u0131 De\u011ferleri Da\u011f\u0131l\u0131m\u0131","95d812e9":"### Addr1 Distributions","ca33b5a9":"## Doland\u0131r\u0131c\u0131l\u0131k i\u015flemleri oran 3.4990% ","c1c1364c":"* **Of the 394 features\/columns in the train_transaction data 15 columns begin in C . The officaila explanation of these columns is.**\n* **C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked**\n* **All C columns are of the numeric data type and summary is as below**\n\n\n* **train_transaction verilerindeki 394 \u00f6zellik \/ s\u00fctundan 15 s\u00fctun C ile ba\u015flar. Bu s\u00fctunlar\u0131n resmi a\u00e7\u0131klamas\u0131d\u0131r.**\n* **C1-C14: \u00f6deme kart\u0131yla ili\u015fkilendirilen ka\u00e7 adres gibi say\u0131m, vb. Ger\u00e7ek anlam maskelenir**\n* **T\u00fcm C s\u00fctunlar\u0131 say\u0131sal veri t\u00fcr\u00fcndedir ve \u00f6zet a\u015fa\u011f\u0131daki gibidir**","790f76e6":"# IEEE-CIS Fraud Detection","ce3677a8":"## Competition Objective is to detect fraud in transactions;","2226803c":"### Encoding categorical features","cde4413d":"## Train vs Test are Time Series Split","52e38643":"### Transaction Amount Outliers - \u0130\u015flem Tutar\u0131 Ayk\u0131r\u0131 De\u011ferleri","5702ec63":"### Top Days with highest Total Transaction Amount - Toplam \u0130\u015flem Tutar\u0131 en y\u00fcksek olan \u0130lk G\u00fcnler","0db635cd":"* We are told these are all categorical, even though some appear numeric.\n* Baz\u0131lar\u0131n\u0131n say\u0131sal g\u00f6r\u00fcnse de bunlar\u0131n hepsinin kategorik oldu\u011fu s\u00f6yleniyor.","30a8867b":"#### The ammount of transaction. I've taken a log transform in some of these plots to better show the distribution- otherwise the few, very large transactions skew the distribution. Because of the log transfrom, any values between 0 and 1 will appear to be negative.\n\n#### \u0130\u015flem miktar\u0131. Da\u011f\u0131l\u0131m\u0131n\u0131 daha iyi g\u00f6stermek i\u00e7in bu parsellerden baz\u0131lar\u0131nda g\u00fcnl\u00fck d\u00f6n\u00fc\u015f\u00fcm\u00fc yapt\u0131m, aksi halde birka\u00e7 b\u00fcy\u00fck i\u015flem da\u011f\u0131t\u0131m\u0131 \u00e7arp\u0131t\u0131r. G\u00fcnl\u00fck aktar\u0131m\u0131 nedeniyle, 0 ile 1 aras\u0131ndaki t\u00fcm de\u011ferler negatif g\u00f6r\u00fcnecektir.","a003865e":"The data description states that these are categorical even though they look numeric. Could they be the address value?\n \n* **Veri a\u00e7\u0131klamas\u0131, say\u0131sal g\u00f6r\u00fcnseler bile bunlar\u0131n kategorik oldu\u011funu belirtir. Adres de\u011feri olabilir mi?**","0332a25a":"* **Let's see if the frauds have some specific hour that has highest % of frauds**","d2d43407":"## Kategorik \u00d6zellikler - \u0130\u015flem","c9b3a5e8":"## dist1 & dist2","5615a8e4":"## R-Email Domain plot distribution\n- I will group all e-mail domains by the respective enterprises.\n- I will set as \"Others\" all values with less than 300 entries.\n\n- T\u00fcm e-posta alan adlar\u0131n\u0131 ilgili kurulu\u015flara g\u00f6re grupland\u0131raca\u011f\u0131m.\n- 300'den az giri\u015fi olan t\u00fcm de\u011ferleri 'Di\u011ferleri' olarak ayarlayaca\u011f\u0131m.","9a254685":"> # main","63282e72":"* **In discussions tab I read an excellent solution to Timedelta column, I will set the link below;\nWe will use the first date as 2017-12-01 and use the delta time to compute datetime features**\n\n* **Tart\u0131\u015fmalar sekmesinde Timedelta s\u00fctununa m\u00fckemmel bir \u00e7\u00f6z\u00fcm okudum, a\u015fa\u011f\u0131daki ba\u011flant\u0131y\u0131 ayarlayaca\u011f\u0131m;\u0130lk tarihi 2017-12-01 olarak kullanaca\u011f\u0131z ve datetime \u00f6zelliklerini hesaplamak i\u00e7in delta zaman\u0131n\u0131 kullanaca\u011f\u0131z**","9b8108c5":"## Categorical Features - Identity","50de053a":"* Yaln\u0131zca = 0 ila 800 aras\u0131ndaki de\u011ferleri dikkate al\u0131rsak, ayk\u0131r\u0131 de\u011ferlerden ka\u00e7\u0131n\u0131r\u0131z ve da\u011f\u0131t\u0131m\u0131m\u0131za daha fazla g\u00fcveniriz.\n* Toplam sat\u0131rlar\u0131n% 1.74'\u00fcn\u00fc temsil eden ayk\u0131r\u0131 de\u011ferlere sahip 10 bin sat\u0131r\u0131m\u0131z var.","6abfd643":"# dist1 & dist2\nPlotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home\/work address. This is just a guess.\n# dist1 & dist2\nDa\u011f\u0131t\u0131m\u0131 daha iyi g\u00f6stermek i\u00e7in logx ile \u00e7izim. Muhtemelen bu i\u015flemin kart sahibinin ev \/ i\u015f adresine olan uzakl\u0131\u011f\u0131 olabilir. Bu sadece bir tahmin.","81ad336b":"## Categorical Features - Transaction Kategorik \u00d6zellikler - \u0130\u015flem\n","ba1cbf02":"* Tan\u0131mlanm\u0131\u015f en d\u00fc\u015f\u00fck ayk\u0131r\u0131 de\u011ferler: 0\n* Tan\u0131mlanan \u00fcst ayk\u0131r\u0131 de\u011ferler: 10093\n* Toplam ayk\u0131r\u0131 g\u00f6zlemler: 10093\n* Ayk\u0131r\u0131 olmayan g\u00f6zlemler: 580447\n* Ayk\u0131r\u0131 de\u011ferlerin toplam y\u00fczdesi: 1.7388","c7a5fddb":"#### Exploring M1-M9 Features- M1-M9 \u00d6zelliklerini Ke\u015ffetme","14785a51":"# Helper Functions","e9d801ed":"* DeviceType\n* DeviceInfo\n* id_12 - id_38","c501ea3e":"Kategorik \u00d6zelliklere ve \u0130\u015flem Tutarlar\u0131na g\u00f6re ke\u015ffetmeye ba\u015flayaca\u011f\u0131m. Ama\u00e7, a\u015fa\u011f\u0131daki gibi baz\u0131 sorular\u0131 cevaplamakt\u0131r:\n\n* Verilerimizde ne t\u00fcr veriler var?\n* Ka\u00e7 tane k\u00f6m\u00fcr, s\u0131ra, eksik de\u011fer var?\n* Hedef da\u011f\u0131l\u0131m\u0131 nedir?\n* Doland\u0131r\u0131c\u0131l\u0131k i\u015flemlerinin doland\u0131r\u0131c\u0131l\u0131k da\u011f\u0131l\u0131m\u0131na de\u011fer verdi\u011fi ve doland\u0131r\u0131c\u0131l\u0131k i\u015flemlerinin olmad\u0131\u011f\u0131 nedir?\n* Bask\u0131n hileli \u00fcr\u00fcnlerimiz var m\u0131?\n* Hangi \u00f6zellikler veya hedefler baz\u0131 ilgin\u00e7 desenler g\u00f6steriyor?\n* Ve ke\u015ffi art\u0131racak daha bir\u00e7ok soru.","7e3473c9":"## \u00c7al\u0131\u015fman\u0131n Hedefi, i\u015flemlerde sahtekarl\u0131\u011f\u0131 tespit etmektir;","575b9ee0":"### TimeDelta Feature","a57727c1":"# Combine","9542f805":"#### Addr1 and Addr2","f33757e2":"### Modelling","328d9638":"### card1 - card6","19790550":"### C1-C14 features","aac3a7df":"* Fraudulent charges appear to have a higher average transaction ammount\n* Hileli \u00fccretlerin ortalama i\u015flem tutar\u0131 daha y\u00fcksek gibi g\u00f6r\u00fcn\u00fcyor","3cb17151":"* Almost all entries in Addr2 are in the same value.\n* Interestingly in the value 65 , the percent of frauds are almost 60%\n* Altought the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts\n\n\n* Addr2'deki hemen hemen t\u00fcm giri\u015fler ayn\u0131 de\u011ferdedir. \n* \u0130lgin\u00e7 bir \u015fekilde, 65 de\u011ferinde, doland\u0131r\u0131c\u0131l\u0131klar\u0131n y\u00fczdesi neredeyse% 60't\u0131r, ancak 87 de\u011ferinin toplam girdilerin% 88'ine sahip olmas\u0131na ra\u011fmen, \n* Toplam \u0130\u015flem Tutarlar\u0131n\u0131n% 96's\u0131na sahiptir.","f866dc08":"I will set all values in Addr1 that has less than 5000 entries to \"Others\"\nIn Addr2 I will set as \"Others\" all values with less than 50 entries\n\nAddr1'de 5000'den az giri\u015fi olan t\u00fcm de\u011ferleri 'Di\u011ferleri' olarak ayarlayaca\u011f\u0131m\nAddr2'de 50'den az giri\u015fi olan t\u00fcm de\u011ferleri 'Di\u011ferleri' olarak ayarlayaca\u011f\u0131m","db42290d":"### emaildomain Distributions","138400ba":"I will start exploring based on Categorical Features and Transaction Amounts. The aim is answer some questions like:\n\n* What type of data we have on our data?\n* How many cols, rows, missing values we have?\n* Whats the target distribution?\n* What's the Transactions values distribution of fraud and no fraud transactions?\n* We have predominant fraudulent products?\n* What features or target shows some interesting patterns?\n* And a lot of more questions that will raise trought the exploration."}}