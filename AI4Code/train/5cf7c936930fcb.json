{"cell_type":{"3156fee7":"code","c4e8d9c4":"code","da74e683":"code","b1fa7381":"code","e4a26206":"code","9543cb56":"code","cb43f8bb":"code","11a9d90e":"code","b0f9dd7d":"code","e0397b41":"code","428fac82":"code","3c0cc765":"code","c377a031":"code","11e5f488":"code","f672cea6":"code","d914274d":"code","8676e69b":"code","eb009b89":"code","2266cd62":"code","1593b494":"code","5612b209":"code","a2fb622e":"code","15938831":"code","ecaba500":"code","236f8a0b":"code","6df3ce00":"code","6728a7fc":"code","7d171b05":"code","3c8adb7a":"code","68490bad":"code","4d30c4e2":"code","75be410b":"code","9783b247":"code","2cc00e9d":"code","927d523e":"markdown","d6df52f2":"markdown","aa0406fc":"markdown","a66da82f":"markdown","f60133c4":"markdown","22956a02":"markdown"},"source":{"3156fee7":"# # Load packages\n\n# Ignore warnings\nimport warnings\n\ndef warn(*args, **kwargs):\n    pass\n\nwarnings.warn = warn\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\n\nimport keras\nfrom keras import *\nfrom keras import layers\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import Callback\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom io import StringIO","c4e8d9c4":"class RocCallback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n    \n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_train = self.model.predict_proba(self.x)\n        roc_train = roc_auc_score(self.y, y_pred_train)\n        y_pred_val = self.model.predict_proba(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc_train: %s - roc-auc_val: %s' % (str(round(roc_train,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n\n\n    \nPLOT_FONT_SIZE = 10    #font size for axis of plots\n\n#define helper function for confusion matrix\n\ndef displayConfusionMatrix(confusionMatrix):\n    \"\"\"Confusion matrix plot\"\"\"\n    \n    confusionMatrix = np.transpose(confusionMatrix)\n    \n    ## calculate class level precision and recall from confusion matrix\n    precisionLow = round((confusionMatrix[0][0] \/ (confusionMatrix[0][0] + confusionMatrix[0][1]))*100, 1)\n    precisionHigh = round((confusionMatrix[1][1] \/ (confusionMatrix[1][0] + confusionMatrix[1][1]))*100, 1)\n    recallLow = round((confusionMatrix[0][0] \/ (confusionMatrix[0][0] + confusionMatrix[1][0]))*100, 1)\n    recallHigh = round((confusionMatrix[1][1] \/ (confusionMatrix[0][1] + confusionMatrix[1][1]))*100, 1)\n\n    ## show heatmap\n    plt.imshow(confusionMatrix, interpolation='nearest',cmap=plt.cm.Blues,vmin=0, vmax=100)\n    \n    ## axis labeling\n    xticks = np.array([0,1])\n    plt.gca().set_xticks(xticks)\n    plt.gca().set_yticks(xticks)\n    plt.gca().set_xticklabels([\"Not Toxic \\n Recall=\" + str(recallLow), \"Toxic \\n Recall=\" + str(recallHigh)], fontsize=PLOT_FONT_SIZE)\n    plt.gca().set_yticklabels([\"Not Toxic \\n Precision=\" + str(precisionLow), \"Toxic \\n Precision=\" + str(precisionHigh)], fontsize=PLOT_FONT_SIZE)\n    plt.ylabel(\"Predicted Class\", fontsize=PLOT_FONT_SIZE)\n    plt.xlabel(\"Actual Class\", fontsize=PLOT_FONT_SIZE)\n        \n    ## add text in heatmap boxes\n    addText(xticks, xticks, confusionMatrix)\n    \ndef addText(xticks, yticks, results):\n    \"\"\"Add text in the plot\"\"\"\n    for i in range(len(yticks)):\n        for j in range(len(xticks)):\n            text = plt.text(j, i, results[i][j], ha=\"center\", va=\"center\", color=\"white\", size=PLOT_FONT_SIZE) ### size here is the size of text inside a single box in the heatmap","da74e683":"def lemmetize_data(data,field):\n    cleaned_texts = []\n    for text in data[field]: # Loop through the tokens (the words or symbols) \n        cleaned_text = text.lower()  # Convert the text to lower case\n        cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stopset])  # Keep only words that are not stopwords.\n        cleaned_text = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='n') for word in cleaned_text.split()])  # Keep each noun's lemma.\n        cleaned_text = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in cleaned_text.split()])  # Keep each verb's lemma.\n        cleaned_text = re.sub(\"[^a-zA-Z]\",\" \", cleaned_text)  # Remove numbers and punctuation.\n        cleaned_text = ' '.join(cleaned_text.split())  # Remove white space.\n        cleaned_texts.append(cleaned_text) \n    data['cleanText'] = cleaned_texts","b1fa7381":"nltk.download('stopwords')\nnltk.download('wordnet')\n\nwordnet_lemmatizer = WordNetLemmatizer()\nstopset = list(set(stopwords.words('english')))","e4a26206":"# load training data 1\ntrain_comment=pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\nlemmetize_data(train_comment,'comment_text')\ntrain_comment","9543cb56":"# load validation data 1\nvalidGoogle=pd.read_csv('..\/input\/val-en-df\/validation_en.csv')\nlemmetize_data(validGoogle,'comment_text_en')","cb43f8bb":"# load validation data 2\nvalidYandex=pd.read_csv('..\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\nlemmetize_data(validYandex,'translated')","11a9d90e":"# append, when we calculate AUC it will reflect the average\nvalid = validGoogle.append(validYandex)","b0f9dd7d":"# load testing data (Translated via Google)\ntest_google=pd.read_csv('..\/input\/test-en-df\/test_en.csv')\nlemmetize_data(test_google,'content_en')","e0397b41":"# load testing data (Translated via Yandex)\ntest_yandex=pd.read_csv('..\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv')\nlemmetize_data(test_yandex,'translated')","428fac82":"def plotCases(data):\n    cases_count = data.value_counts(dropna=False)\n\n    # Plot  results \n    plt.figure(figsize=(6,6))\n    sns.barplot(x=cases_count.index, y=cases_count.values)\n    plt.ylabel('Texts', fontsize=12)\n    plt.xticks(range(len(cases_count.index)), ['Not', 'Toxic'])","3c0cc765":"train_labels = train_comment['toxic']\nvalid_labels = valid['toxic']","c377a031":"plotCases(train_labels)","11e5f488":"# the numbers here are doubled from the append above\nplotCases(valid_labels)","f672cea6":"# lemmetize\ntrain_texts = train_comment['cleanText']\n\nvalid_texts = valid['cleanText']\n\ntest_textsGoogle = test_google['cleanText']\ntest_textsYandex = test_yandex['cleanText']","d914274d":"# Define vocabulary size (you can tune this parameter and evaluate model performance)\nVOCABULARY_SIZE = 15000","8676e69b":"# Create input feature arrays\ntokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\ntokenizer.fit_on_texts(train_texts)","eb009b89":"# Convert words into word ids\nmeanLengthTrain = np.mean([len(item.split(\" \")) for item in train_texts])\nmeanLengthValid = np.mean([len(item.split(\" \")) for item in valid_texts])\nmeanLengthTestGoogle = np.mean([len(item.split(\" \")) for item in test_textsGoogle])\nmeanLengthTestYandex = np.mean([len(item.split(\" \")) for item in test_textsYandex])\n\nprint('Average length - Train:',meanLengthTrain,'Valid:',meanLengthValid,'TestGoogle:',meanLengthTestGoogle,'TestYandex:',meanLengthTestYandex)","2266cd62":"MAX_SENTENCE_LENGTH = int(meanLengthTrain + 20) # we let a text go 20 words longer than the mean text length (you can also tune this parameter).\n\n# Convert train, validation, and test text into lists with word ids\ntrainFeatures = tokenizer.texts_to_sequences(train_texts)\ntrainFeatures = pad_sequences(trainFeatures, MAX_SENTENCE_LENGTH, padding='post')\ntrainLabels = train_labels.values\n\nvalidFeatures = tokenizer.texts_to_sequences(valid_texts)\nvalidFeatures = pad_sequences(validFeatures, MAX_SENTENCE_LENGTH, padding='post')\nvalidLabels = valid_labels.values\n\ntestFeaturesGoogleLSTMwith = tokenizer.texts_to_sequences(test_textsGoogle)\ntestFeaturesGoogleLSTMwith = pad_sequences(testFeaturesGoogleLSTMwith, MAX_SENTENCE_LENGTH, padding='post')\n\ntestFeaturesYandexLSTMwith = tokenizer.texts_to_sequences(test_textsYandex)\ntestFeaturesYandexLSTMwith = pad_sequences(testFeaturesYandexLSTMwith, MAX_SENTENCE_LENGTH, padding='post')","1593b494":"#from: https:\/\/www.kaggle.com\/bertcarremans\/using-word-embeddings-for-sentiment-analysis\/data\nEMBEDDING_FILE='..\/input\/glove-twitter\/glove.twitter.27B.25d.txt'\nemb_dict = {}\nglove = open(EMBEDDING_FILE)\nfor line in glove:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1:], dtype='float32')\n    emb_dict[word] = vector\nglove.close()","5612b209":"#from: https:\/\/www.kaggle.com\/bertcarremans\/using-word-embeddings-for-sentiment-analysis\/data\nembedding_matrix = np.zeros((VOCABULARY_SIZE, 25))\n\nfor w, i in tokenizer.word_index.items():\n    # The word_index contains a token for all words of the training data so we need to limit that\n    if i < VOCABULARY_SIZE:\n        vect = emb_dict.get(w)\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros\n        if vect is not None:\n            embedding_matrix[i] = vect\n    else:\n        break","a2fb622e":"embedding_matrix.shape","15938831":"# Hyperparameters for model tuning\nLEARNING_RATE = 0.001\nBATCH_SIZE = 128\nEPOCHS = 9","ecaba500":"#LSTM\nLSTMwith = Sequential()\n\n# We use pre-trained embeddings from GloVe. These are fed in as a layer of our network and the weights do not update during the training process.\nLSTMwith.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False, input_length=len(trainFeatures[0])))\nLSTMwith.add(Bidirectional(LSTM(24)))\nLSTMwith.add(Dropout(0.6))\nLSTMwith.add(Dense(12, activation='relu'))\nLSTMwith.add(Dropout(0.5))\nLSTMwith.add(Dense(1, activation='sigmoid'))\n            \noptimizer = optimizers.Adam(lr=LEARNING_RATE)\nLSTMwith.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n\nprint(LSTMwith.summary())","236f8a0b":"# ratio of non-toxic to toxic in training:\n200000\/25000","6df3ce00":"# We have a class imbalance, upweight the toxic comments\nclass_weights = {0: 1,\n                 1: 8}","6728a7fc":"roc = RocCallback(training_data=(trainFeatures, trainLabels),\n                  validation_data=(validFeatures, validLabels))","7d171b05":"# train model\nstart = time.time()    \nhistory = LSTMwith.fit(trainFeatures, trainLabels, validation_data = (validFeatures, validLabels), batch_size=BATCH_SIZE, epochs=EPOCHS, class_weight=class_weights, callbacks=[roc])\nprint(\"Training took %d seconds\" % (time.time() - start))  ","3c8adb7a":"pred_valid_LSTMwith = pd.DataFrame(LSTMwith.predict(validFeatures))","68490bad":"roc_auc_score(validLabels, pred_valid_LSTMwith)","4d30c4e2":"validPred = pred_valid_LSTMwith\npred_valid_binary = round(validPred)\n\nconfusionMatrix = None\nconfusionMatrix = confusion_matrix(validLabels, pred_valid_binary)\n\nplt.rcParams['figure.figsize'] = [3, 3] ## plot size\ndisplayConfusionMatrix(confusionMatrix)\nplt.title(\"Confusion Matrix\", fontsize=PLOT_FONT_SIZE)\nplt.show()","75be410b":"# make test predictions (average both translations)\npredictionsGoogle = pd.DataFrame(LSTMwith.predict(testFeaturesGoogleLSTMwith))\npredictionsYandex = pd.DataFrame(LSTMwith.predict(testFeaturesYandexLSTMwith))\npredictions = (predictionsGoogle+predictionsYandex)\/2\npredictions","9783b247":"# prep for submission\nsample = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv\")\nsample['toxic'] = predictions\nsample","2cc00e9d":"# make submission\nsample.to_csv(\"submission.csv\", index=False)","927d523e":"# Load data","d6df52f2":"Prepare data:","aa0406fc":"# Make predictions","a66da82f":"# LSTM with pre-trained GloVe embeddings \nOnly using one of the training sets. Relies on translated test and validation data from @bamps53 and @kashnitsky","f60133c4":"# Helper functions","22956a02":"# LSTM with pretrained"}}