{"cell_type":{"c15b51a8":"code","b0ca15cc":"code","d85a7c5f":"code","e3a6875c":"code","b17721fb":"code","5ee43c2a":"code","8f6eb3b2":"code","b3cf0cc7":"code","7c2a9755":"code","4fcd499e":"code","b20656a4":"code","4e7250a8":"code","dc67bd2d":"code","1edaa12c":"code","39740e63":"markdown","b1bf78ee":"markdown"},"source":{"c15b51a8":"# Installing the most recent version of skopt directly from Github\n!pip install git+https:\/\/github.com\/scikit-optimize\/scikit-optimize.git","b0ca15cc":"# Assuring you have the most recent CatBoost release\n!pip install catboost -U","d85a7c5f":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifiers\nfrom catboost import CatBoostClassifier\n\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\nfrom skopt.space import Real, Categorical, Integer","e3a6875c":"# Loading data directly from CatBoost\nfrom catboost.datasets import amazon\n\nX, Xt = amazon()\n\ny = X[\"ACTION\"].apply(lambda x: 1 if x == 1 else 0)\nX.drop([\"ACTION\"], axis=1, inplace=True)","b17721fb":"X.head()","5ee43c2a":"Xt.head()","8f6eb3b2":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title, callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    if callbacks:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                  len(optimizer.cv_results_['params']),\n                                  best_score,\n                                  best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","b3cf0cc7":"# Converting roc-auc score into a scorer suitable for model selection\nroc_auc = make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True)","7c2a9755":"# Setting a 5-fold stratified cross-validation (note: shuffle=True)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)","4fcd499e":"# Initializing a CatBoostClassifier\nclf = CatBoostClassifier(thread_count=2,\n                         loss_function='Logloss',\n                         verbose = False)","b20656a4":"# Defining your search space\nsearch_spaces = {'iterations': Integer(10, 300),\n                 'depth': Integer(1, 8),\n                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n                 'bagging_temperature': Real(0.0, 1.0),\n                 'border_count': Integer(1, 255),\n                 'ctr_border_count': Integer(1, 255),\n                 'l2_leaf_reg': Integer(2, 30),\n                 'scale_pos_weight':Real(0.01, 1.0, 'uniform')}","4e7250a8":"# Setting up BayesSearchCV\nopt = BayesSearchCV(clf,\n                    search_spaces,\n                    scoring=roc_auc,\n                    cv=skf,\n                    n_iter=20,\n                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=42)","dc67bd2d":"# Running the optimization\nbest_params = report_perf(opt, X, y,'CatBoost', \n                          callbacks=[VerboseCallback(20), \n                                     DeadlineStopper(60*30)])","1edaa12c":"# Using optimized BayesSearchCV for predictions\nsubmission = pd.DataFrame(Xt.id)\nsubmission['Action'] = opt.predict_proba(Xt)[:,1]\nsubmission.to_csv(\"amazon_submission.csv\", index=False)","39740e63":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3338\/media\/gate.png)","b1bf78ee":"## Amazon.com - Employee Access Challenge\n\nWhen an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read\/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery\/recovery cycle wastes a nontrivial amount of time and money.\n\nThere is a considerable amount of data regarding an employee\u2019s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access."}}