{"cell_type":{"5bb37f8d":"code","07b29446":"code","f8168283":"code","b59acf64":"code","8562de71":"code","3be10a63":"code","7239d659":"code","4cba68cb":"code","ca7a341a":"code","6fac514d":"code","95875ca3":"code","550c8f48":"code","b953e547":"code","d058c480":"code","3c4c231e":"code","269c42d9":"code","36493df2":"code","b8888811":"code","8accd210":"code","5121bb93":"code","8d760a6d":"code","59ec0c0e":"code","65baa43f":"code","b109a8dd":"markdown","81808018":"markdown","d883ab9a":"markdown","0053dc20":"markdown","e7d37203":"markdown","bd3bfcec":"markdown","61f139db":"markdown","5e61ebfa":"markdown","3a58b2e1":"markdown","7b835321":"markdown","c705807e":"markdown","7e97b978":"markdown","50145a1d":"markdown","57dceef6":"markdown","332d3970":"markdown","3de94c41":"markdown","3afce177":"markdown","70b8ce01":"markdown","6ebbd927":"markdown","f5d4d212":"markdown","2245f94b":"markdown","c95cc6ba":"markdown","3d0eb6d5":"markdown","acdaa790":"markdown","ccaadbc1":"markdown"},"source":{"5bb37f8d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","07b29446":"## Load Data (train.csv)\nhouse_df_org = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse_df = house_df_org.copy()\nhouse_df.head()","f8168283":"## Datasets shape\nprint('SHAPE: ', house_df.shape)\n\n## Features Type\nprint('\\nFeatures type\\n', house_df.dtypes.value_counts())\n\n## Null values\nisnull_series = house_df.isnull().sum()\nprint('\\nNull Columns & Counts\\n', isnull_series[isnull_series > 0].sort_values(ascending=False))","b59acf64":"## Target Value Distribution\nplt.title('Original Sale Price Histogram')\nplt.xticks(rotation=45)\nsns.distplot(house_df['SalePrice'])","8562de71":"## After applying Log Transformation using Numpy log1p\nplt.title('Log Transformed Sale Price Histogram')\nlog_SalePrice = np.log1p(house_df['SalePrice'])\nsns.distplot(log_SalePrice)","3be10a63":"# SalePrice Log Transformation\noriginal_SalePrice = house_df['SalePrice']\nhouse_df['SalePrice'] = np.log1p(house_df['SalePrice'])\n\n# Drop Columns with Many Null Values\nhouse_df.drop(['Id', 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1, inplace=True)\n\n# Replace Null To Avg\nhouse_df.fillna(house_df.mean(), inplace=True)","7239d659":"## Print Feature & Type that have null values.\nnull_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0]\nprint('### Null Feature Type : \\n', house_df.dtypes[null_column_count.index])","4cba68cb":"# One Hot Encoding\nhouse_df_ohe = pd.get_dummies(house_df)\n\n## Check Null Values \nnull_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]\nprint('### Null Feature Type : \\n', house_df_ohe.dtypes[null_column_count.index])","ca7a341a":"## evaluation function\nfrom sklearn.metrics import mean_squared_error\n\ndef get_rmse(model) :\n    pred = model.predict(x_test)\n    mse = mean_squared_error(y_test, pred)\n    rmse = np.sqrt(mse)\n    print(model.__class__.__name__, ' log converted RMSE: ', np.round(rmse, 3))\n    return rmse\n\ndef get_rmses(models) :\n    rmses = []\n    for model in models :\n        rmse = get_rmse(model)\n        rmses.append(rmse)\n    return rmses","6fac514d":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ny_target = house_df_ohe['SalePrice']\nx_features = house_df_ohe.drop('SalePrice', axis=1, inplace=False)\n\nx_train, x_test, y_train, y_test = train_test_split(x_features, y_target, test_size=0.2)\n\n# LinearRegression\nlr_reg = LinearRegression()\nlr_reg.fit(x_train, y_train)\n\n# Ridge\nridge_reg = Ridge()\nridge_reg.fit(x_train, y_train)\n\n# Lasson\nlasso_reg = Lasso()\nlasso_reg.fit(x_train, y_train)\n\n# Evaluation\nmodels = [lr_reg, ridge_reg, lasso_reg]\nget_rmses(models)","95875ca3":"def get_top_bottom_coef(model, n=10) :\n    coef = pd.Series(model.coef_, index=x_features.columns)\n    \n    # + top 10, - bottom 10 regression coefficients\n    coef_high = coef.sort_values(ascending=False).head(n)\n    coef_low = coef.sort_values(ascending=False).tail(n)\n    return coef_high, coef_low","550c8f48":"# Visualize Function\ndef visualize_coefficient(models) :\n    fig, axs = plt.subplots(figsize=(24, 10), nrows=1, ncols=3)\n    fig.tight_layout()\n    \n    for i_num, model in enumerate(models) :\n        ## Merge Top 10, Bottom 10 regression coefficient using Pandas concat\n        coef_high, coef_low = get_top_bottom_coef(model)\n        coef_concat = pd.concat([coef_high, coef_low])\n        ## Visualize\n        axs[i_num].set_title(model.__class__.__name__ + 'Coeffiecents', size=25)\n        axs[i_num].tick_params(axis='y', direction='in', pad=-120)\n        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()) :\n            label.set_fontsize(22)\n        sns.barplot(x=coef_concat.values, y=coef_concat.index, ax=axs[i_num])\n\nmodels = [lr_reg, ridge_reg, lasso_reg]\nvisualize_coefficient(models)","b953e547":"## Cross Validation Fold\nfrom sklearn.model_selection import cross_val_score\n\ndef get_avg_rmse_cv(models) :\n    for model in models :\n        rmse_list = np.sqrt(-cross_val_score(model, x_features, y_target, scoring='neg_mean_squared_error', cv=5))\n        rmse_avg = np.mean(rmse_list)\n        print('\\n{0} CV RMSE VALUE LIST: {1}'.format(model.__class__.__name__, np.round(rmse_list, 3)))\n        print('{0} CV AVERAGE RMSE VALUE: {1}'.format(model.__class__.__name__, np.round(rmse_avg, 3)))\n        \nget_avg_rmse_cv(models)","d058c480":"## Hyper Parameters Tuning Function\nfrom sklearn.model_selection import GridSearchCV\n\ndef print_best_params(model, params) :\n    grid_model = GridSearchCV(model, param_grid=params, scoring='neg_mean_squared_error', cv=5)\n    grid_model.fit(x_features, y_target)\n    rmse = np.sqrt(-1 * grid_model.best_score_)\n    print('{0} 5 CV AVG RMSE VALUE: {1}, best alpha: {2}'.format(model.__class__.__name__, np.round(rmse, 4), grid_model.best_params_))\n\nridge_params = {'alpha': [0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}\nlasso_params = {'alpha': [0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1, 5, 10]}\nprint_best_params(ridge_reg, ridge_params)\nprint_best_params(lasso_reg, lasso_params)","3c4c231e":"# LinearRegression\nlr_reg = LinearRegression()\nlr_reg.fit(x_train, y_train)\n\n# Ridge\nridge_reg_alpha12 = Ridge(alpha=12)\nridge_reg_alpha12.fit(x_train, y_train)\n\n# Lasson\nlasso_reg_alpha0001 = Lasso(alpha=0.001)\nlasso_reg_alpha0001.fit(x_train, y_train)\n\n# Evaluation\nmodels_alpha = [lr_reg, ridge_reg_alpha12, lasso_reg_alpha0001]\nget_rmses(models_alpha)\n\n# Visualize\nvisualize_coefficient(models_alpha)","269c42d9":"from scipy.stats import skew\n\n# Not Object.ONLY NUMERIC FEATURES\nfeatures_index = house_df.dtypes[house_df.dtypes != 'object'].index\n\n# Apply `skew` for check data distribution\nskew_features = house_df[features_index].apply(lambda x: skew(x))\n\n# Extract only columns with skew of 1 or more\nskew_features_top = skew_features[skew_features > 1]\nprint(skew_features_top.sort_values(ascending=False))","36493df2":"house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index])","b8888811":"house_df_ohe = pd.get_dummies(house_df)\n\ny_target = house_df_ohe['SalePrice']\nx_features = house_df_ohe.drop('SalePrice', axis=1, inplace=False)\nx_train, x_test, y_train, y_test = train_test_split(x_features, y_target, test_size=0.2)\n\nridge_params = {'alpha': [0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}\nlasso_params = {'alpha': [0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1, 5, 10]}\n\nprint_best_params(ridge_reg, ridge_params)\nprint_best_params(lasso_reg, lasso_params)","8accd210":"# LinearRegression\nlr_reg = LinearRegression()\nlr_reg.fit(x_train, y_train)\n\n# Ridge\nridge_reg_alpha10 = Ridge(alpha=10)\nridge_reg_alpha10.fit(x_train, y_train)\n\n# Lasson\nlasso_reg_alpha0001 = Lasso(alpha=0.001)\nlasso_reg_alpha0001.fit(x_train, y_train)\n\n# Evaluation\nmodels_alpha = [lr_reg, ridge_reg_alpha10, lasso_reg_alpha0001]\nget_rmses(models_alpha)\n\n# Visualize\nvisualize_coefficient(models_alpha)","5121bb93":"plt.scatter(x=house_df_org['GrLivArea'], y=house_df_org['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","8d760a6d":"cond1 = house_df_ohe['GrLivArea'] > np.log1p(4000)\ncond2 = house_df_ohe['SalePrice'] < np.log1p(500000)\noutlier_index = house_df_ohe[cond1 & cond2].index\n\nhouse_df_ohe.drop(outlier_index, axis=0, inplace=True)","59ec0c0e":"y_target = house_df_ohe['SalePrice']\nx_features = house_df_ohe.drop('SalePrice', axis=1, inplace=False)\nx_train, x_test, y_train, y_test = train_test_split(x_features, y_target, test_size=0.2)\n\nridge_params = {'alpha': [0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}\nlasso_params = {'alpha': [0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1, 5, 10]}\n\nprint_best_params(ridge_reg, ridge_params)\nprint_best_params(lasso_reg, lasso_params)","65baa43f":"# LinearRegression\nlr_reg = LinearRegression()\nlr_reg.fit(x_train, y_train)\n\n# Ridge\nridge_reg_alpha8 = Ridge(alpha=8)\nridge_reg_alpha8.fit(x_train, y_train)\n\n# Lasson\nlasso_reg_alpha0001 = Lasso(alpha=0.001)\nlasso_reg_alpha0001.fit(x_train, y_train)\n\n# Evaluation\nmodels_alpha = [lr_reg, ridge_reg_alpha8, lasso_reg_alpha0001]\nget_rmses(models_alpha)\n\n# Visualize\nvisualize_coefficient(models_alpha)","b109a8dd":"#### The datasets consists of 1460 records and 81 features, and feature types include many object types as well as numeric types.\n#### Some features have more null values \u200b\u200bthan the amount of data. Columns with too many null values \u200b\u200bwill be dropped.\n\n### Before applying the regression model, check whether the distribution of target values \u200b\u200bis normally distributed. ","81808018":"#### Let's visualize regression coefficients for each model using `get_top_bottom_coef`","d883ab9a":"Let's recreate the feature data and target data based on the **updated** `house_df_ohe`, and perform model optimization using the `print_best_params()` function.","0053dc20":"#### Create a regression model using `linear regression`, `Ridge`, and `Lasso` of `sklearn`.\n#### Predictive evaluation uses *RMSLE*. However, SalePrice has already been log converted. The predicted value is also predicted based on the log transformed value, so it is the log transformed value of the original SalePrice predicted value. That is, if only *RMSE* is applied to the result error, *RMSLE* is automatically measured.","e7d37203":"# *Data Preprocessing*","bd3bfcec":"Even in the case of feature data, if excessively distorted features exist, regression prediction performance may be degraded. Check the data distribution for all numeric features to see how much the distribution is distorted.","61f139db":"## *Data distribution diagram*","5e61ebfa":"Visualize the relationship between `GrLivArea` and `SalePrice` in the original data set, `house_df_org`, and handle outlier values.","3a58b2e1":"## *Handle Outlier Values*","7b835321":"#### `Lasso` seems to need hyper parameter tuning compared to other regressions. Let's do the alpha optimization later. First, let's visualize the regression coefficients for each feature and check the feature's regression coefficients for each model.\n\n#### Create a function to check the top 10 and bottom 10 regression coefficients","c705807e":"#### There is no NULL value except for the object type. The object type applies one hot encoding.","7e97b978":"> Looking at the regression coefficients for each model, OLS-based Linear Regression and Ridge have similar regression coefficients, but Lasso has very small coefficients overall. If there is a problem with the partitioning of the training data, let's measure the average RMSE by dividing the entire data into a set of 5 cross-validation folds without dividing it with train_test_split.","50145a1d":"#### It can be seen that the result value is distributed in the form of a standard distribution, and now, after log conversion of SalePrice, it is reflected in the DataFrame.\n\n#### PoolQC, MiscFeature, Alley, Fence, and FireplaceQu, which are features with many null values, are deleted, and ID, which is a simple identifier, is also deleted. LotFrontage is relatively large with 259 nulls, but it is replaced with an average value.","57dceef6":"> Let's further process the dataset to further tune the model. The first is the data distribution diagram of the feature dataset, and the second is the outlier data processing.","332d3970":"Since the features of `house_df` are partially log converted, `house_df_ohe` is created with one hot encoding applied again. And let's print the optimal alpha and RMSE values \u200b\u200busing the `print_best_params()` function.","3de94c41":"#### **Target** Value is *SalePrice*. Let's check datasets shape, type and Null Values.","3afce177":"# *Configuration*","70b8ce01":"#### Even after training with a set of 5 folds and evaluating, the Lasso case still performs poorly. Optimization proceeds through the alpha hyperparameter tuning. Since the optimization hyperparameter work is repeatedly performed for each model, a separate function is created for this.","6ebbd927":"# *Linear Regression Model Training\/Prediction\/Evaluation*","f5d4d212":"For the Ridge model, the alpha value was changed from 12 to 10, and the RMSE value was improved. The visualization results show that all three models have GrLivArea, that is, the size of the living space, which has a high regression coefficient.","2245f94b":"The extracted features with high distortion are log transformed.","c95cc6ba":"The two data existing in the lower right are processed.","3d0eb6d5":"> As can be seen in the figure, the distribution of the data values \u200b\u200bis skewed to the left and is out of the normal distribution. \n\n*Thus, WE NEED TO APPLY LOG TRANSFORMATION FOR STANDARD DISTRIBUTION*","acdaa790":"> Only two outliers data were removed, and the predicted results improved significantly. In the case of the Ridge model, the optimal alpha value was changed to 8, and the average RMSE was all improved. The effect of the `GrLivArea` property on the regression model was large, and improving this outlier gave great significance to the performance improvement.","ccaadbc1":"> After optimizing the alpha value, the prediction performance improved. The regression coefficients for each model have also changed a lot. This time in Ridge and Lasso, similar features have high regression coefficients."}}