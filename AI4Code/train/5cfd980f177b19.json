{"cell_type":{"b46640ba":"code","527fd6a5":"code","34fc8ed7":"code","395dfe00":"code","ca6a2af8":"code","6b2cb0da":"code","30c98597":"code","da53b4e5":"code","8cc7b08e":"code","30b5eb22":"code","e91b41e4":"code","7a18bc43":"code","6c0d05e1":"code","3c7356b4":"code","8ca67648":"code","d2b1683a":"code","fb06f08d":"code","829da2f9":"code","c80f42e5":"code","529759cd":"code","0319828c":"code","2f556052":"code","bac7d180":"code","0baf529f":"code","0208a9c6":"code","d4105252":"code","675eafa9":"code","d598037a":"code","227a2e29":"code","5d9cea30":"code","7e018f60":"code","b0904f88":"code","ff0125a5":"code","059f828b":"code","94170fe2":"code","f2076605":"code","37f1011b":"code","150fbabc":"code","7e554d09":"code","8807a0ae":"code","e3698f83":"code","cdb2640c":"code","91308162":"code","45392417":"code","7ae4019e":"code","05c4ad8c":"code","af6cd220":"code","3f4dbfbe":"code","610fd9df":"code","dfedd533":"code","efea55ed":"code","82e05f29":"code","2810e9be":"code","cbef4b09":"code","8361795b":"code","7fc6881d":"code","3804e28b":"code","5ccb596b":"code","cc31577e":"code","af18405f":"code","321be6ca":"code","9ad072b2":"code","3c54ee1c":"code","9a488fb4":"code","c210524c":"code","58e0f3d7":"code","1edd05d0":"code","cdfc4964":"code","e1fdb465":"code","f2c039d8":"code","f4d05c82":"code","b260625a":"code","4cd1e6d5":"code","f7671d90":"code","06719666":"code","57e05bbb":"code","421f6890":"code","a3dc7498":"code","50fbe636":"code","96a345de":"code","b5ba9ac8":"code","31b20b4c":"code","040506dc":"code","c9045e98":"code","39631fb0":"code","e707c994":"code","a5bed01a":"code","2c13558c":"code","c32f7fd1":"code","3044e6f0":"code","f625ba69":"code","62264e56":"code","98ffd38b":"markdown","1da2afba":"markdown","f3fb403d":"markdown","e6bc598f":"markdown","9b34b134":"markdown","b33e6c97":"markdown","0b6131b2":"markdown","67da24e7":"markdown","3ea5150b":"markdown","461b28a7":"markdown","46b4068c":"markdown","191637bf":"markdown","e8d7855a":"markdown","3e94d91c":"markdown","88979a6a":"markdown","ebd6764f":"markdown","19c2fd92":"markdown","4725a2df":"markdown","7228838f":"markdown","2d31d059":"markdown","b9f5144f":"markdown","79d23c5b":"markdown","368ae632":"markdown","a11f1ae2":"markdown","f82086f5":"markdown","5defd38c":"markdown","185b4056":"markdown","2e1c9c26":"markdown","f4e406de":"markdown","735fcb1f":"markdown","f56c3490":"markdown","a5ad9552":"markdown","7316a350":"markdown","2c67bf2b":"markdown","a0d214a5":"markdown","543cae73":"markdown","8dc20e5a":"markdown","7233549e":"markdown","d4fa10f7":"markdown","d541dba2":"markdown","fb496b6f":"markdown","5b6e870f":"markdown","b54266bb":"markdown","70bf745c":"markdown","2dde0512":"markdown","b7c22529":"markdown","2ed6e58c":"markdown","ef77026e":"markdown","3c61bbb1":"markdown","446cee4d":"markdown","7b8d1236":"markdown","34f0d82c":"markdown","ef5b1358":"markdown","d8b288ad":"markdown","2e3b30c0":"markdown","1f17891c":"markdown"},"source":{"b46640ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","527fd6a5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams","34fc8ed7":"data = pd.read_csv(\"..\/input\/iris-flower-dataset\/IRIS.csv\")","395dfe00":"data.columns","ca6a2af8":"data.describe()","6b2cb0da":"data['species'].value_counts()","30c98597":"data.head()","da53b4e5":"sepal_length = data['sepal_length']\npetal_length = data['petal_length']\nsepal_width=data['sepal_width']\npetal_width=data['petal_width']\n","8cc7b08e":"#density distribution of petal length, petal width, sepal length, sepal width of Iris-setosa\n\niris_setosa=data[data['species'].str.contains('Iris-setosa')]\niris_versicolor=data[data['species'].str.contains('Iris-versicolor')]\niris_virginica=data[data['species'].str.contains('Iris-virginica')]\n","30b5eb22":"sns.kdeplot(iris_setosa['sepal_length'],iris_setosa['sepal_width'], \n            color='r', shade=True, Label='Iris_Setosa', \n            cmap=\"Reds\", shade_lowest=False).set_title('Density distribution of Sepal Length and Sepal Width of Iris Setosa')","e91b41e4":"ax = sns.kdeplot(iris_setosa['petal_length'],iris_setosa['petal_width'], \n            color='g', shade=True, Label='Iris_Setosa', \n            cmap=\"Greens\", shade_lowest=False)\nax = sns.kdeplot(iris_versicolor['petal_length'],iris_versicolor['petal_width'], \n            color='b', shade=True, Label='Iris_Versicolor', \n            cmap=\"Reds\", shade_lowest=False).set_title('Density distribution of Petal length and Petal Width of Iris-Versicolor and Iris-Setosa')","7a18bc43":"ax=sns.kdeplot(data['sepal_length'],shade=True,color=\"g\").set_title('Density distribution of Sepal Length')","6c0d05e1":"iris_setosa.plot.density(title = 'Density distribution plot of Iris Setosa')","3c7356b4":"data.plot.density(title='Density distribution of all the flowers',grid='true')","8ca67648":"sns.distplot(data['petal_length'],kde = False).set_title('Histogram for petal length')","d2b1683a":"ax = sns.countplot(x=\"petal_length\",data=data).set_title('Petal length distribution of all flowers')\nrcParams['figure.figsize'] = 20,8.27\n","fb06f08d":"sns.barplot(x=\"species\",y=\"sepal_length\",data=data).set_title(\"Sepal Length of three species\")\nrcParams['figure.figsize'] = 10,8.27","829da2f9":"sns.boxplot(data=data).set_title(\"Distribution of Sepal_length, Sepal_width, petal_length and petal_width of 3 flowers\")","c80f42e5":"sns.boxplot(data=data,x=\"species\",y=\"sepal_width\").set_title(\"sepal_width distribution of three flowers\")","529759cd":"sns.scatterplot(x=data.sepal_length,y=data.sepal_width,hue=data.species).set_title(\"Sepal length and Sepal width distribution of three flowers\")","0319828c":"cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\nax = sns.scatterplot(x=\"petal_length\", y=\"petal_width\",hue=\"species\",size=\"species\",sizes=(20,200),legend=\"full\",data=data)","2f556052":"sns.violinplot(x=\"species\", y = \"sepal_width\",data=data, palette=\"muted\").set_title(\"sepal width of 3 species\")","bac7d180":"sns.violinplot(x=\"species\", y = \"petal_width\",data=data, palette=\"muted\").set_title(\"petal width of 3 species\")","0baf529f":"sns.lineplot(x=\"petal_length\", y=\"petal_width\", hue=\"species\",\n                  data=data).set_title(\"Distribution of petal length and petal width of the 3 species\")","0208a9c6":"g = sns.pairplot(data, hue=\"species\", palette=\"husl\")","d4105252":"sns.pairplot(data, vars=[\"sepal_width\", \"sepal_length\"],diag_kind=\"kde\")","675eafa9":"sns.pairplot(data,x_vars=[\"sepal_width\", \"sepal_length\"],y_vars=[\"petal_width\", \"petal_length\"])","d598037a":"data.corr()\nsns.heatmap(data.corr(),center=0).set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","227a2e29":"sns.heatmap(data.corr(), annot=True, fmt=\"f\").set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","5d9cea30":"sns.heatmap(data.corr(), cmap=\"YlGnBu\").set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","7e018f60":"#corr = np.corrcoef(np.random.randn(10, 200))\nmask = np.zeros_like(data.corr())\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(7, 5))\n    ax = sns.heatmap(data.corr(), mask=mask, vmax=.3, square=True).set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","b0904f88":"X=data.iloc[:,0:4].values\ny=data.iloc[:,4].values","ff0125a5":"#Train and Test split\nfrom sklearn.model_selection import KFold,train_test_split,cross_val_score\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\ny_test.shape","059f828b":"#Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test)\n","94170fe2":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlg_class=LogisticRegression(random_state=0)\nlg_class.fit(X_train,y_train)","f2076605":"y_pred_logit=lg_class.predict(X_test)\n","37f1011b":"ldf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_logit.flatten()})","150fbabc":"plt.plot(y_test.flatten(),y_pred_logit.flatten())\nplt.show()","7e554d09":"from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nlogreg = LogisticRegression(C=1e5)\n\n# Create an instance of Logistic Regression Classifier and fit the data.\nlogreg.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","8807a0ae":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_logit)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(lg_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","e3698f83":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_logistic=accuracy_score(y_test, y_pred_logit)\nacc_logistic","cdb2640c":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_logit, target_names=target_names))","91308162":"#KNN Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics as metrics\na_index = list(range(1,11))\na = pd.Series()\nx = [1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    k_class=KNeighborsClassifier(n_neighbors=5) \n    k_class.fit(X_train,y_train)\n    y_pred_neigh=k_class.predict(X_test)\n    a=a.append(pd.Series(metrics.accuracy_score(y_pred_neigh,y_test)))\nplt.plot(a_index, a)\nplt.title(\"KNN Prediction\")\nplt.xticks(x)\n    ","45392417":"#KNN \nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()","7ae4019e":"kdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_neigh.flatten()})","05c4ad8c":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_neigh)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(k_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","af6cd220":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_knn=metrics.accuracy_score(y_test, y_pred_neigh)\nacc_knn","3f4dbfbe":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_neigh, target_names=target_names))","610fd9df":"from sklearn.svm import SVC\nsvm_class=SVC(kernel='linear',random_state=0)\nsvm_class.fit(X_train,y_train)","dfedd533":"y_pred_svc=svm_class.predict(X_test)","efea55ed":"svdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_svc.flatten()})","82e05f29":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_svc)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(svm_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","2810e9be":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_svm=metrics.accuracy_score(y_test, y_pred_svc)","cbef4b09":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_svc, target_names=target_names))","8361795b":"from sklearn import svm, datasets\n\n\ndef make_meshgrid(x, y, h=.02):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\n# import some data to play with\niris = datasets.load_iris()\n# Take the first two features. We could avoid this by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (svm.SVC(kernel='linear', C=C),\n          svm.LinearSVC(C=C, max_iter=10000),\n          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\nmodels = (clf.fit(X, y) for clf in models)\n\n# title for the plots\ntitles = ('SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel')\n\n# Set-up 2x2 grid for plotting.\nfig, sub = plt.subplots(2, 2)\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\n\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,\n                  cmap=plt.cm.coolwarm, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel('Sepal length')\n    ax.set_ylabel('Sepal width')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\n\nplt.show()","7fc6881d":"from sklearn.naive_bayes import GaussianNB\nn_class=GaussianNB()\nn_class.fit(X_train,y_train)","3804e28b":"y_pred_bayes=n_class.predict(X_test)","5ccb596b":"bdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_bayes.flatten()})","cc31577e":"plt.plot(y_test.flatten(),y_pred_bayes.flatten())\nplt.show()","af18405f":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_bayes)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(n_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","321be6ca":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_bayes=metrics.accuracy_score(y_test, y_pred_bayes)\nacc_bayes","9ad072b2":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_bayes, target_names=target_names))","3c54ee1c":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nd_class=DecisionTreeClassifier(criterion='entropy')\nmodel_all_params = d_class.fit(X_train,y_train)\nplt.figure(figsize = (20,10))\nplot_tree(model_all_params,filled=True)\nplt.show()","9a488fb4":"y_pred_tree=d_class.predict(X_test)","c210524c":"import matplotlib.pyplot as plt\n# import the needed dataset.\nfrom sklearn.datasets import load_iris\n# Import the model and an additional visualization tool.\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n# Define a variable to establish three classes\/species.\nclass_count = 3\n# Define standard RGB color scheme for visualizing ternary classification in order to match the color map used later.\nplot_colors = 'brg'\n# Define marker options for plotting class assignments of training data.\nmarkers = 'ovs'\n# We also need to establish a resolution for plotting.  I favor clean powers of ten, but this is not by any means a hard and fast rule.\nplot_res = 0.01\n\n# Load the iris dataset from scikit-learn (note the use of from [library] import [function] above)\niris = load_iris()\n\n# Set the size of the figure used to contain the subplots to be generated.\nplt.figure(figsize=(20,10))\n\n# Create an empty list of models to store the results of each pairwise model fit.\nmodels = []\n\n# Use enumerate() to define the possible pairs of features available and iterate over each pair.\nfor pair_index, pair in enumerate([[0, 1], [0, 2], [0, 3], \n                                           [1, 2], [1, 3], \n                                                   [2, 3] ]):\n\n    # We only take the two features corresponding to the pair in question...\n    X, y = iris.data[:, pair] , iris.target\n    \n    # ... to fit the decision tree classifier model.\n    model = DecisionTreeClassifier().fit(X, y)\n    \n    # Append the results to the models list\n    models.append(model)\n    \n    # Establish a two row by three column subplot array for plotting.\n    plt.subplot(2, 3, pair_index + 1)\n    \n    # Define appropriate x and y ranges for each plot...\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    # ... and use each range to define a meshgrid to use as the plotting area.\n    xx, yy = np.meshgrid(np.arange(x_min, \n                                   x_max, \n                                   plot_res),\n                         np.arange(y_min, \n                                   y_max, \n                                   plot_res) )\n    # Use plt.tight_layout() to establish spacing of the subplots.\n    plt.tight_layout(h_pad = 0.5, \n                     w_pad = 0.5, \n                       pad = 4.0 )\n    \n    # Predict the classification of each point in the meshgrid based on the calculated model above.\n    # The numpy methods .c_() and .ravel() reshape our meshgrid values into a format compatible with our model.predict() method,\n    Z = model.predict(np.c_[xx.ravel(), \n                            yy.ravel() ])\n    # Reshape the predictions to match xx...\n    Z = Z.reshape(xx.shape)\n    # ... and prepare a contour plot that reflects the predictions .\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.brg)\n    \n    # Define the subplot axis labels after title casing while preserving case on the unit of measure \n    plt.xlabel(iris.feature_names[pair[0]].title()[0:-4] + iris.feature_names[pair[0]][-4:])\n    plt.ylabel(iris.feature_names[pair[1]].title()[0:-4] + iris.feature_names[pair[1]][-4:])\n    \n    # Plot the training points for each species in turn\n    for i, color, marker in zip(range(class_count), plot_colors, markers):\n        # Subset the data to the class in question with the np.where() method\n        index = np.where(y == i)\n        # Plot the class in question on the subplot\n        plt.scatter(X[index, 0], \n                    X[index, 1], \n                    c = color,\n                    marker = marker,\n                    label = iris.target_names[i],\n                    cmap = plt.cm.brg, \n                    edgecolor = 'black', \n                    s = 15                       )\n\n# Define a title for the overall collection of subplots after each subplot is fully defined\nplt.suptitle('Decision Surface of a Decision Tree Using Paired Features',\n             size = 24                                                   )\n\n# Define the legend for the subplot collection\nplt.legend(loc = 'lower right',\n           fontsize = 16,\n           borderpad = 0.1, \n           handletextpad = 0.1 )\n\n# Set limits just large enough to show everything cleanly\nplt.axis(\"tight\")\n","58e0f3d7":"decdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_tree.flatten()})","1edd05d0":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_tree)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(d_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","cdfc4964":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_dec=accuracy_score(y_test, y_pred_tree)\nacc_dec","e1fdb465":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_tree, target_names=target_names))","f2c039d8":"from sklearn.ensemble import RandomForestClassifier\nran_class=RandomForestClassifier(n_estimators=10,criterion='entropy')\nran_class.fit(X_train,y_train)","f4d05c82":"y_pred_forest=ran_class.predict(X_test)","b260625a":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_forest)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(ran_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","4cd1e6d5":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_ran=metrics.accuracy_score(y_test, y_pred_forest)\nacc_ran","f7671d90":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_forest, target_names=target_names))","06719666":"randf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_forest.flatten()})","57e05bbb":"from matplotlib.colors import ListedColormap\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nn_estimators = 30\ncmap = plt.cm.RdYlBu\nplot_step = 0.02  # fine step width for decision surface contours\nplot_step_coarser = 0.5  # step widths for coarse classifier guesses\nRANDOM_SEED = 13  # fix the seed on each iteration\n\n# Load data\niris = load_iris()\n\nplot_idx = 1\n\nmodels = [DecisionTreeClassifier(max_depth=None),\n          RandomForestClassifier(n_estimators=n_estimators),\n          ExtraTreesClassifier(n_estimators=n_estimators),\n          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n                             n_estimators=n_estimators)]\n\nfor pair in ([0, 1], [0, 2], [2, 3]):\n    for model in models:\n        # We only take the two corresponding features\n        X = iris.data[:, pair]\n        y = iris.target\n\n        # Shuffle\n        idx = np.arange(X.shape[0])\n        np.random.seed(RANDOM_SEED)\n        np.random.shuffle(idx)\n        X = X[idx]\n        y = y[idx]\n\n        # Standardize\n        mean = X.mean(axis=0)\n        std = X.std(axis=0)\n        X = (X - mean) \/ std\n\n        # Train\n        model.fit(X, y)\n\n        scores = model.score(X, y)\n        # Create a title for each column and the console by using str() and\n        # slicing away useless parts of the string\n        model_title = str(type(model)).split(\n            \".\")[-1][:-2][:-len(\"Classifier\")]\n\n        model_details = model_title\n        if hasattr(model, \"estimators_\"):\n            model_details += \" with {} estimators\".format(\n                len(model.estimators_))\n        print(model_details + \" with features\", pair,\n              \"has a score of\", scores)\n\n        plt.subplot(3, 4, plot_idx)\n        if plot_idx <= len(models):\n            # Add a title at the top of each column\n            plt.title(model_title, fontsize=9)\n\n        # Now plot the decision boundary using a fine mesh as input to a\n        # filled contour plot\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                             np.arange(y_min, y_max, plot_step))\n\n        # Plot either a single DecisionTreeClassifier or alpha blend the\n        # decision surfaces of the ensemble of classifiers\n        if isinstance(model, DecisionTreeClassifier):\n            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n        else:\n            # Choose alpha blend level with respect to the number\n            # of estimators\n            # that are in use (noting that AdaBoost can use fewer estimators\n            # than its maximum if it achieves a good enough fit early on)\n            estimator_alpha = 1.0 \/ len(model.estimators_)\n            for tree in model.estimators_:\n                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n\n        # Build a coarser grid to plot a set of ensemble classifications\n        # to show how these are different to what we see in the decision\n        # surfaces. These points are regularly space and do not have a\n        # black outline\n        xx_coarser, yy_coarser = np.meshgrid(\n            np.arange(x_min, x_max, plot_step_coarser),\n            np.arange(y_min, y_max, plot_step_coarser))\n        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n                                         yy_coarser.ravel()]\n                                         ).reshape(xx_coarser.shape)\n        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n                                c=Z_points_coarser, cmap=cmap,\n                                edgecolors=\"none\")\n\n        # Plot the training points, these are clustered together and have a\n        # black outline\n        plt.scatter(X[:, 0], X[:, 1], c=y,\n                    cmap=ListedColormap(['r', 'y', 'b']),\n                    edgecolor='k', s=20)\n        plot_idx += 1  # move on to the next plot in sequence\n\nplt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\nplt.axis(\"tight\")\nplt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\nplt.show()","421f6890":"dict={'Logistic_Regression' : [acc_logistic],\n     'KNN' : [acc_knn],\n     'SVM' : [acc_svm],\n     'Naive_Bayes' : [acc_bayes],\n     'Decision_Tree' : [acc_dec],\n     'Random_Forest' : [acc_ran]\n     }\nmodels = pd.DataFrame.from_dict(dict,orient='index')\nmodels.transpose()\n\n\n","a3dc7498":"models.to_csv('mycsvfile.csv',index=False)\n\n\n","50fbe636":"print(\"Classification_Report of Logistic Regression : \\n\",classification_report(y_test, y_pred_forest, target_names=target_names))\nprint(\"Classification_Report of SVM : \\n\",classification_report(y_test, y_pred_svc, target_names=target_names))\nprint(\"Classification_Report of KNN : \\n\",classification_report(y_test, y_pred_neigh, target_names=target_names))\nprint(\"Classification_Report of Naive Bayes : \\n\",classification_report(y_test, y_pred_bayes, target_names=target_names))\nprint(\"Classification_Report of Decision Tree : \\n\",classification_report(y_test, y_pred_tree, target_names=target_names))\nprint(\"Classification_Report of Random Forest : \\n\",classification_report(y_test, y_pred_forest, target_names=target_names))","96a345de":"\nprint(\"Logistic Regression: \")\nprint(cross_val_score(lg_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(lg_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression train set is: \" , accuracy)\nprint(cross_val_score(lg_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(lg_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression test set is: \" , accuracy_test)","b5ba9ac8":"\nprint(\"KNN: \")\nprint(cross_val_score(k_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(k_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of KNN train set is: \" , accuracy)\nprint(cross_val_score(k_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(k_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression test set is: \" , accuracy_test)","31b20b4c":"\nprint(\"SVM: \")\nprint(cross_val_score(svm_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(svm_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of SVM train set is: \" , accuracy)\nprint(cross_val_score(svm_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(svm_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of SVM test set is: \" , accuracy_test)","040506dc":"\nprint(\"Naive Bayes: \")\nprint(cross_val_score(n_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(n_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Naive Bayes train set is: \" , accuracy)\nprint(cross_val_score(n_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(n_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Naive Bayes test set is: \" , accuracy_test)","c9045e98":"\nprint(\"Decision Tree: \")\nprint(cross_val_score(d_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(d_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Decision Tree train set is: \" , accuracy)\nprint(cross_val_score(d_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(d_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Decision Tree test set is: \" , accuracy_test)","39631fb0":"\nprint(\"Random Forest Classification: \")\nprint(cross_val_score(ran_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(ran_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression train set is: \" , accuracy)\nprint(cross_val_score(ran_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(ran_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression test set is: \" , accuracy_test)","e707c994":"from sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\nscores = []\nbest_svr = SVR(kernel='rbf')\ncv = KFold(n_splits=10, random_state=42, shuffle=False)\nfor train_index, test_index in cv.split(X):\n    print(\"Train Index: \", train_index, \"\\n\")\n    print(\"Test Index: \", test_index)\n\n    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n    best_svr.fit(X_train, y_train)\n    scores.append(best_svr.score(X_test, y_test))\n    \n","a5bed01a":"print(np.mean(scores))","2c13558c":"d = data.iloc[:,0:4].values\nd\n#X=data.iloc[:,0:4].values\n#y=data.iloc[:,4].values","c32f7fd1":"from sklearn.cluster import KMeans\nwcss={}\nfor i in range(1,10):\n    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=1000).fit(d)\n    wcss[i]=kmeans.inertia_\nplt.figure()\nplt.plot(list(wcss.keys()),list(wcss.values()))\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","3044e6f0":"kmeans = KMeans(n_clusters=3).fit(d)\ncentroids = kmeans.cluster_centers_\ny_kmeans=kmeans.fit_predict(d)\nprint(centroids)\nplt.scatter(d[y_kmeans==0,0], d[y_kmeans==0,1], c= 'red',s=50, label='Cluster1')\nplt.scatter(d[y_kmeans==1,0], d[y_kmeans==1,1], c= 'blue',s=50, label='Cluster2')\nplt.scatter(d[y_kmeans==2,0], d[y_kmeans==2,1], c= 'green',s=50, label='Cluster3')\nplt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=50,label='centroids')\nplt.title('Cluster of flowers')\nplt.legend()\nplt.show()","f625ba69":"\n#from scipy.cluster.hierarchy import dendrogram, linkage\n\nimport scipy.cluster.hierarchy as shc\nplt.figure(figsize=(30, 17))  \nplt.title(\"Dendrograms\")\ndend = shc.dendrogram(shc.linkage(d, method='ward'))","62264e56":"from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \ny_hc=cluster.fit_predict(d)\nplt.scatter(d[y_hc==0,0], d[y_hc==0,1], c= 'red',s=100, label='Cluster1')\nplt.scatter(d[y_hc==1,0], d[y_hc==1,1], c= 'blue',s=100, label='Cluster2')\nplt.scatter(d[y_hc==2,0], d[y_hc==2,1], c= 'green',s=100, label='Cluster3')\nplt.title('Cluster of flowers')\nplt.legend()\nplt.show()\n","98ffd38b":"Classification Report - Metrics evaluation","1da2afba":"# Petal length and width of 3 species","f3fb403d":"Accuracy Score - metrics evaluation","e6bc598f":"Confusion Matrix - metrics evaluation","9b34b134":"Classification Report metrics","b33e6c97":"Confusion Matrix","0b6131b2":"# Corelation Matrix of all attributes","67da24e7":"# Sepal Length and width","3ea5150b":"Classification Report metrics evaluation","461b28a7":"# Density distribution of Sepal Length","46b4068c":"# ML Algorithms - Classification","191637bf":"Confusion Matrix","e8d7855a":"# Sepal width of 3 species","3e94d91c":"#  Density Plot for Sepal Length and Sepal Width","88979a6a":"# Cross_val_score of ML Models","ebd6764f":"# Accuracy score output","19c2fd92":"# Density Plot for Petal Length and Petal Width","4725a2df":"Plot decision surface boundary for decision tree in Iris dataset","7228838f":"# Density distribution of Iris Setosa","2d31d059":"Decision boundary plot of SVM Classification","b9f5144f":"Accuracy score - metrics evaluation","79d23c5b":"# Pairplot","368ae632":"Decision Boundary of Neighbour classifier","a11f1ae2":"# Petal length of all flowers","f82086f5":"# SVM Classification","5defd38c":"# Petal length and width","185b4056":"# Feature Scaling","2e1c9c26":"# Sepal Length and width, Petal length and width of 3 flowers","f4e406de":"# Naive Bayes","735fcb1f":"# Model Selection - KFold for SVM","f56c3490":"# Corelation of attributes","a5ad9552":"# Plot decision boundary for Logistic Regression classifier","7316a350":"# Histogram for Petal Length","2c67bf2b":"# K-Means Clustering","a0d214a5":"# Pairplot - Sepal Length and width","543cae73":"# Sepal Length of 3 species","8dc20e5a":"# Petal width of 3 species","7233549e":"Confusion Matrix","d4fa10f7":"Classification Report - Metrics Evaluation","d541dba2":"Confusion Matrix - metrics evaluation","fb496b6f":"# Random Forest","5b6e870f":"Classification Report - Metrics Evaluation","b54266bb":"# Hierarchical Clustering","70bf745c":"Plot decision surface boundary for Random Forest","2dde0512":"Classification Report","b7c22529":"Accuracy score - Metrics evaluation","2ed6e58c":"# Clustering algorithms","ef77026e":"# Density distribution of all flowers with all 4 attributes","3c61bbb1":"Accuracy Score","446cee4d":"# Classification report of all the ML Models","7b8d1236":"# KNN Classification","34f0d82c":"Accuracy score metrics","ef5b1358":"Confusion Matrix - Metrics evaluation","d8b288ad":"Accuracy score - metrics evaluation","2e3b30c0":"# Decision Tree","1f17891c":"# Sepal width of 3 flowers"}}