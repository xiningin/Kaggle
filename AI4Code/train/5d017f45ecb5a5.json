{"cell_type":{"4b6c18d6":"code","659f165b":"code","bf233780":"code","019edc2a":"code","14e97469":"code","4de666ef":"code","9df4301e":"code","27eef034":"code","e8b3bda4":"code","2d99946c":"code","8c59c389":"code","a39abd4b":"code","7f9afd60":"code","15eca291":"code","8e9f7dfc":"code","ca01858e":"code","0fb32b1c":"code","aa661829":"code","8f286443":"code","6a38556b":"code","20a9c8f2":"code","72720a92":"code","8a245993":"code","5be59a4b":"code","f7171495":"code","e5de5b4e":"code","7fc16e77":"code","fa96bd36":"code","4537b001":"code","9550e442":"code","e1899168":"code","ffc6fdfc":"code","260948d2":"code","5768f5e4":"code","281829d6":"code","58556101":"code","f8aa3224":"code","18eea73c":"code","6fc53f3b":"code","1ff608e4":"code","7ed7fcea":"code","59ede100":"code","6729524c":"code","40f080a9":"code","4d76a784":"code","fb69d285":"code","20c0b90a":"code","31151fa1":"code","01f6ad04":"code","ac72c1e6":"code","766f27c9":"code","d586cde9":"code","4c0a510d":"code","b83ea4d1":"code","9dbd8208":"code","a6985f5f":"code","31430637":"code","98be54aa":"code","352b9b83":"code","2a7d0f10":"code","31bdfb43":"code","d47ecf82":"code","713d3150":"code","2d2327ef":"code","1854f99f":"code","34fb8e3f":"code","5cc2d6b1":"code","fb05e50f":"code","9f62dc58":"code","86b8e371":"code","3596b5b6":"code","bb02d804":"markdown","2df749b2":"markdown","dd76cd8b":"markdown","b062ca9c":"markdown","f391fbb3":"markdown","5e1156d8":"markdown","f17bb224":"markdown","26aedbe3":"markdown","08b443e0":"markdown","fb367061":"markdown","ca2e1969":"markdown","e2ee80d9":"markdown","aaf2650f":"markdown","43b3f6aa":"markdown","db62b0c4":"markdown","16b59c49":"markdown","ac1b4687":"markdown","97f38a71":"markdown","968f4f9b":"markdown","fdbc0900":"markdown","81e66628":"markdown","e1603a5b":"markdown","d1a95479":"markdown","617bfa6f":"markdown","4670d32a":"markdown","6ce2c011":"markdown","669f6170":"markdown","f8961934":"markdown","db33f852":"markdown","15c937c8":"markdown","d1501d23":"markdown","640db22a":"markdown","ca9c9692":"markdown","e3f8dd48":"markdown","fdb45a8e":"markdown","8c11267f":"markdown","ab789917":"markdown","d6ef902d":"markdown","63b7ef01":"markdown","d6bf3229":"markdown","4fe4984b":"markdown","7c3f8a04":"markdown","ca930dc9":"markdown","172cebf9":"markdown","95a15ade":"markdown","e18e4f0d":"markdown","e49c7c61":"markdown","ea66415c":"markdown","a6e90cf6":"markdown","d9d8ac6d":"markdown","7e615a85":"markdown","17006ec4":"markdown","9183cd11":"markdown","a6a7f173":"markdown","8fb0b6fa":"markdown","af09b5ef":"markdown","17501a2e":"markdown","3ec87b99":"markdown","2a8a2e8b":"markdown","d334b003":"markdown","7ed95f75":"markdown","7f7cdd4b":"markdown","f47d920f":"markdown","0c947893":"markdown","cec8c179":"markdown","dd5bcf1c":"markdown","ca5adfe0":"markdown","7bfe5d4e":"markdown","268c5ed6":"markdown","44ed9a22":"markdown","bce7d6d9":"markdown","d07d55ad":"markdown","8776c985":"markdown","fc9e4ff4":"markdown","c2a04937":"markdown","ebcd5ac2":"markdown","7411fbbf":"markdown","8839f354":"markdown","aafd8d27":"markdown","b9493f39":"markdown","93540b04":"markdown","b6d435da":"markdown","0e85a111":"markdown","37f8ee72":"markdown","e5098b32":"markdown","5723e2b5":"markdown","cb2b39b6":"markdown","70b336ba":"markdown","7eedadf3":"markdown"},"source":{"4b6c18d6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set(context=\"notebook\", style = 'darkgrid', font_scale = 1.5, color_codes=True)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm","659f165b":"df = pd.read_csv('..\/input\/diamonds\/diamonds.csv')","bf233780":"df.head()","019edc2a":"df.info()","14e97469":"df.describe()","4de666ef":"# Plots by Cut\nfig,ax=plt.subplots(ncols=3, figsize = (18,6))\nfig.suptitle('Diamonds by Cut')\ng1=sns.countplot(ax=ax[0],x=\"cut\", data=df,order=['Fair','Good','Very Good','Premium','Ideal'])\ng1.set(xlabel=None)\ng1.tick_params(labelrotation=45)\ng2=sns.boxplot(x=\"cut\", y=\"price\", order=['Fair','Good','Very Good','Premium','Ideal'], data=df, ax=ax[1])\ng2.set(xlabel=None)\ng2.tick_params(labelrotation=45)\ng3=sns.violinplot(ax=ax[2],x=\"cut\", y=\"price\", order=['Fair','Good','Very Good','Premium','Ideal'], data=df,scale='width',legend=False)\ng3.set(xlabel=None)\ng3.tick_params(labelrotation=45)","9df4301e":"# Plots by Color\nfig,ax=plt.subplots(ncols=3, figsize = (18,6))\nfig.suptitle('Diamonds by Color')\ng1=sns.countplot(ax=ax[0],x=\"color\", data=df.sort_values(by=['color'],ascending=False))\ng1.set(xlabel=None)\ng2=sns.boxplot(x=\"color\", y=\"price\", data=df.sort_values(by=['color'],ascending=False), ax=ax[1])\ng2.set(xlabel=None)\ng3=sns.violinplot(ax=ax[2],x=\"color\", y=\"price\", data=df.sort_values(by=['color'],ascending=False),scale='width',legend=False)\ng3.set(xlabel=None)","27eef034":"# Plots by Clarity\nfig,ax=plt.subplots(ncols=3, figsize = (18,6))\nfig.suptitle('Diamonds by Clarity')\ng1=sns.countplot(ax=ax[0],x=\"clarity\", data=df.sort_values(by=['clarity']))\ng1.set(xlabel=None)\ng1.tick_params(labelrotation=45)\ng2=sns.boxplot(x=\"clarity\", y=\"price\", data=df.sort_values(by=['clarity']), ax=ax[1])\ng2.set(xlabel=None)\ng2.tick_params(labelrotation=45)\ng3=sns.violinplot(ax=ax[2],x=\"clarity\", y=\"price\", data=df.sort_values(by=['clarity']),scale='width',legend=False)\ng3.set(xlabel=None)\ng3.tick_params(labelrotation=45)","e8b3bda4":"sns.set(rc={'figure.figsize':(20,14)})\nsns.catplot(x=\"clarity\", y=\"price\", hue=\"color\", kind=\"point\", col ='cut', data=df.sort_values(by=['clarity']),col_wrap=3)","2d99946c":"sns.set(rc={'figure.figsize':(20,14)})\np=df.hist(bins=50)","8c59c389":"df.drop(['Unnamed: 0'], axis=1, inplace=True)","a39abd4b":"plt.figure(figsize = (3,3))\nsns.kdeplot(df['price'])\nplt.show()","7f9afd60":"f, ax = plt.subplots(2, 3)\n#Dataset price distribution\ng1=sns.kdeplot(df['price'],ax=ax[0,0], legend = False)\ng1.text(s='Price distribution', x=5000, y=0.0003, fontsize=16, multialignment='center')\ng1.text(6000, 0.00027, r'$Skew$=%.2f' % (round(df.price.skew(),2)), fontsize=14)\n\n#Log transformation\ndf['price_t'] = np.log( df['price'])\ng2=sns.kdeplot(df['price_t'],ax=ax[0,1], legend = False)\ng2.text(s='Log Transform', x=7, y=0.38, fontsize=16, multialignment='center')\ng2.text(7.2, 0.36, r'$Skew$=%.2f' % (round(df.price_t.skew(),2)), fontsize=14)\n\n#Log 10 transformation\ndf['price_t'] = np.sin( df['price'])\ng3=sns.kdeplot(df['price_t'],ax=ax[0,2], legend = False)\ng3.text(s='Sen Transform', x=-0.6, y=0.75, fontsize=16, multialignment='center')\ng3.text(-0.5, 0.7, r'$Skew$=%.2f' % (round(df.price_t.skew(),2)), fontsize=14)\n\n#Square Root Transformation\ndf['price_t'] = np.sqrt(df.price)\ng4=sns.kdeplot(df['price_t'],ax=ax[1,0], legend = False)\ng4.text(s='Square Root Transform', x=60, y=0.023, fontsize=16, multialignment='center')\ng4.text(70, 0.020, r'$Skew$=%.2f' % (round(df.price_t.skew(),2)), fontsize=14)\n\n#Cubic root Transformation\ndf['price_t'] = np.cbrt(df.price)\ng5=sns.kdeplot(df['price_t'],ax=ax[1,1], legend = False)\ng5.text(s='Cubic Root Transform', x=13, y=0.11, fontsize=16, multialignment='center')\ng5.text(15, 0.10, r'$Skew$=%.2f' % (round(df.price_t.skew(),2)), fontsize=14)\n\n#Exponential Squared Transformation\ndf['price_t'] = (df.price)**2\ng6=sns.kdeplot(df['price_t'],ax=ax[1,2], legend = False)\ng6.text(s='Exponential Transform', x=1e8, y=3e-8, fontsize=16, multialignment='center')\ng6.text(1.4e8, 2.6e-8, r'$Skew$=%.2f' % (round(df.price_t.skew(),2)), fontsize=14)","15eca291":"df2 = df.copy()\ndf2['price_t'] = np.log(df['price'])\ndf2.drop(['price'], axis=1, inplace=True)","8e9f7dfc":"# Identify the 0 z value rows\nlen(df[(df['x']==0) | (df['y']==0) | (df['z']==0)])","ca01858e":"df2 = df2[df2.z != 0]\ndf2.describe()","0fb32b1c":"sns.pairplot(df2[['price_t','carat','depth','table','x','y','z']])","aa661829":"df2.loc[df2.z > 6].sort_values(by=['z']).head(15)","8f286443":"df2.z.replace(df2.z.max(), np.nan, inplace=True)\ndf2.z.replace(np.nan, df2.z.max(), inplace=True)","6a38556b":"df2.loc[df2.y > 9.5].sort_values(by=['y']).head(15)","20a9c8f2":"df2.y.replace(df2.y.max(), np.nan, inplace=True)   # 1st\ndf2.y.replace(df2.y.max(), np.nan, inplace=True)   # 2nd\ndf2.y.replace(np.nan, df2.y.max(), inplace=True)","72720a92":"df2.loc[df2.table > 75].sort_values(by=['table']).head(15)","8a245993":"# Replace the table outlier value with the next highest value\ndf2.table.replace(df2.table.max(), np.nan, inplace=True)\ndf2.table.replace(np.nan, df2.table.max(), inplace=True)","5be59a4b":"sns.pairplot(df2[['price_t','carat','depth','table','x','y','z']])","f7171495":"# Dropping the first column from the categorical features in the dataset\ncut = pd.get_dummies(df2.cut, drop_first = True)\ncolor = pd.get_dummies(df2.color, drop_first = True)\nclarity = pd.get_dummies(df2.clarity, drop_first = True)\n\n# Add the cut, color and clarity datasets to the main dataset\ndf2 = pd.concat([df2, cut, color, clarity], axis = 1)\n\n# Drop the cut, color and clarity features as have created the dummies for the features\ndf2.drop(['cut', 'color', 'clarity'], axis = 1, inplace = True)","e5de5b4e":"# Move the target 'price_t' column to the first column\nprice_col = df2.pop('price_t')\ndf2.insert(0, price_col.name, price_col)\npd.options.display.max_columns = None\ndf2.head()","7fc16e77":"sns.set(rc={'figure.figsize':(20,14)})\nsns.heatmap(df2.corr(), cmap=\"seismic\", annot=True, vmin=-1, vmax=1)","fa96bd36":"# Running an OLS as a baseline model\nnp.random.seed(0)\ny = df2.price_t\nX = df2.drop(['price_t'], axis=1)\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X)\nresults = model.fit()\nresults.summary()","4537b001":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\ndf2_train, df2_test = train_test_split(df2, train_size = 0.7, test_size = 0.3, random_state = 22)","9550e442":"from sklearn.preprocessing import MinMaxScaler\nscalar = MinMaxScaler()\nnum_vars = ['carat','depth','table','x','y','z','price_t']\ndf2_train[num_vars] = scalar.fit_transform(df2_train[num_vars])","e1899168":"df2_train.describe()","ffc6fdfc":"np.random.seed(0)\ny = df2_train.price_t\nX = df2_train.drop(['price_t'], axis=1)\n\ny_val = df2_test.price_t\nX_val = df2_test.drop(['price_t'], axis=1)","260948d2":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures","5768f5e4":"np.random.seed(0)\nX_lm = sm.add_constant(X)\nmodel = sm.OLS(y, X_lm)\nresults = model.fit()\nresults.summary()","281829d6":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","58556101":"X.drop(['z'], axis=1, inplace=True)","f8aa3224":"# Run the OLS model\nnp.random.seed(0)\nX_lm = sm.add_constant(X)\nmodel = sm.OLS(y, X_lm)\nresults = model.fit()\nresults.summary()","18eea73c":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6fc53f3b":"X.drop(['y'], axis=1, inplace=True)","1ff608e4":"# Run the OLS model\nnp.random.seed(0)\nX_lm = sm.add_constant(X)\nmodel = sm.OLS(y, X_lm)\nresults = model.fit()\nresults.summary()","7ed7fcea":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","59ede100":"X.drop(['x'], axis=1, inplace=True)","6729524c":"# Run the OLS model\nnp.random.seed(0)\nX_lm = sm.add_constant(X)\nmodel = sm.OLS(y, X_lm)\nresults = model.fit()\nresults.summary()","40f080a9":"X.drop(['depth'], axis=1, inplace=True)","4d76a784":"# Run the OLS model\nnp.random.seed(0)\nX_lm = sm.add_constant(X)\nmodel = sm.OLS(y, X_lm)\nresults = model.fit()\nresults.summary()","fb69d285":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","20c0b90a":"X.drop(['table'], axis=1, inplace=True)","31151fa1":"# Run the OLS model\nnp.random.seed(0)\nX_lm = sm.add_constant(X)\nmodel = sm.OLS(y, X_lm)\nresults = model.fit()\nresults.summary()","01f6ad04":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ac72c1e6":"X.drop(['Ideal'], axis=1, inplace=True)","766f27c9":"# Run the OLS model\nnp.random.seed(0)\nX_lm = sm.add_constant(X)\nmodel = sm.OLS(y, X_lm)\nresults = model.fit()\nresults.summary()","d586cde9":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4c0a510d":"import seaborn as sns\nimport matplotlib.pyplot as plt\ny_train_price = results.predict(X_lm)\n\n# Plot the histogram of the error terms\nfig = plt.figure(figsize=(5,5))\nsns.distplot((y - y_train_price), bins = 40)\nfig.suptitle('Error Terms', fontsize = 20)                  \nplt.xlabel('Errors', fontsize = 18)  ","b83ea4d1":"from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\n# Comparing the model against the Test dataset and making predictions\n# 1. Scale the test data\ndf2_test_copy = df2_test.copy()\nnum_vars = ['carat','depth','table','x','y','z','price_t']\ndf2_test_copy[num_vars] = scalar.fit_transform(df2_test_copy[num_vars])\n\n# 2. Run the model against the test features.\ny_test = df2_test_copy['price_t']\nX_test = df2_test_copy.drop(['price_t'], axis=1)\n\nX_test_lm = sm.add_constant(X_test)\nX_test_lm.drop(['x','y','z','depth','table','Ideal'], axis=1, inplace=True)\n\ny_pred = results.predict(X_test_lm)\n\n# Output the R2 score for the test data.\nprint(\"Mean Absolute Error: \"+ str(round(mean_absolute_error(y_test, y_pred),4)))\nprint('R Squared: ' + str(round(r2_score(y_test, y_pred),4)) )\nprint('Mean Squared Error: ' + str(round(mean_squared_error(y_test, y_pred),4)))","9dbd8208":"from sklearn.metrics import median_absolute_error, r2_score\nf, (ax0, ax1) = plt.subplots(1, 2,)\ny_pred = results.predict(X_test_lm)\ny_val2 = pd.DataFrame(y_test)\ny_pred2 = pd.DataFrame(y_pred)\n\n\nax0.scatter(y_val2, y_pred2, s=8)\nax0.plot([0, 1], [0, 1], '--k')\nax0.set_ylabel('True target')\nax0.set_xlabel('Predicted target')\nax0.text(s='OLS \\n with target transformation', x=0,\n            y=1, fontsize=14, multialignment='center')\nax0.text(0, 0.9, r'$R^2$=%.2f, MAE=%.2f' % (\n    r2_score(y_val2, y_pred2), median_absolute_error(y_val2, y_pred2)))\nax0.ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(0, 0))\n\n\nax1.scatter(y_pred2, (y_pred2.values - y_val2.values), s=8)\nax1.set_ylabel('Residual')\nax1.set_xlabel('Predicted target')\nax1.ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(0, 0))\n\nplt.show()","a6985f5f":"# Importing libraries\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures","31430637":"# Using df2 which is the cleaned dataset and already split into df_train and df_test\ny = df2['price_t']\nX = df2.drop(['price_t'], axis=1)","98be54aa":"np.random.seed(0)\n# Train is normalised\ny_train = df2_train.price_t\nX_train = df2_train.drop(['price_t'], axis=1)\n\n# Test is not normalised\ny_val = df2_test.price_t\nX_val = df2_test.drop(['price_t'], axis=1)","352b9b83":"num_vars = ['carat','depth','table','x','y','z','price_t']\ndf2_test_scaled = df2_test.copy()\ndf2_test_scaled[num_vars] = scalar.fit_transform(df2_test_scaled[num_vars])\n\ny_val_scaled = df2_test_scaled.price_t\nX_val_scaled = df2_test_scaled.drop(['price_t'], axis=1)","2a7d0f10":"alphavec = 10**np.linspace(-3,3,200)   # alpha varies from 0.001 to 1000\nlasso_model = LassoCV(alphas = alphavec, cv=5)\n#lasso_model.fit(X_train_scaled, y_train)\nlasso_model.fit(X_train, y_train)\n\n# This is the best alpha value found\nlasso_model.alpha_ ","31bdfb43":"list(zip(X_train.columns, lasso_model.coef_))","d47ecf82":"print(\"Accuracy: \"+ str(round(lasso_model.score(X_train, y_train),4)*100) + '%')\nprint(\"Mean Absolute Error: \"+ str(round(mean_absolute_error(y_val_scaled, lasso_model.predict(X_val_scaled)),4)))\nprint('R Squared: ' + str(round(r2_score(y_val_scaled, lasso_model.predict(X_val_scaled)),4)) )\nprint('Mean Squared Error: ' + str(round(mean_squared_error(y_val_scaled, lasso_model.predict(X_val_scaled)),4)))","713d3150":"alphavec = 10**np.linspace(-3,3,200)   # alpha varies from 0.001 to 1000\nridge_model = RidgeCV(alphas = alphavec, cv=5)\nridge_model.fit(X_train, y_train)\n# This is the best alpha value found\nridge_model.alpha_","2d2327ef":"# display all coefficients in the model with optimal alpha\nlist(zip(X_train.columns, ridge_model.coef_))","1854f99f":"print(\"Accuracy: \"+ str(round(ridge_model.score(X_train, y_train),4)*100) + '%')\nprint(\"Mean Absolute Error: \"+ str(round(mean_absolute_error(y_val_scaled, ridge_model.predict(X_val_scaled)),4)))\nprint('R Squared: ' + str(round(r2_score(y_val_scaled, ridge_model.predict(X_val_scaled)),4)) )\nprint('Mean Squared Error: ' + str(round(mean_squared_error(y_val_scaled, ridge_model.predict(X_val_scaled)),4)))","34fb8e3f":"alphavec = 10**np.linspace(-3,3,200)   # alpha varies from 0.001 to 1000\nelasticnet_model = ElasticNetCV(alphas = alphavec, cv=5)\nelasticnet_model.fit(X_train, y_train)\n# This is the best alpha value it found\nelasticnet_model.alpha_","5cc2d6b1":"# display all coefficients in the model with optimal alpha\nlist(zip(X_train.columns, ridge_model.coef_))","fb05e50f":"print(\"Accuracy: \"+ str(round(elasticnet_model.score(X_train, y_train),4)*100) + '%')\nprint(\"Mean Absolute Error: \"+ str(round(mean_absolute_error(y_val_scaled, elasticnet_model.predict(X_val_scaled)),4)))\nprint('R Squared: ' + str(round(r2_score(y_val_scaled, elasticnet_model.predict(X_val_scaled)),4)) )\nprint('Mean Squared Error: ' + str(round(mean_squared_error(y_val_scaled, elasticnet_model.predict(X_val_scaled)),4)))","9f62dc58":"## cross validation using KFold (on the 100% dataset, without manually splitting)\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\n# Creating Linear Regression Model\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n#print(f'Linear regression val R^2: {lm.score(X_val_scaled, y_val_scaled):.3f}')\n\n# Combine scaled X and y\nX_scaled = X_train.copy().append(X_val_scaled)\ny_scaled = y_train.copy().append(y_val_scaled)\n\n# Split data to k-folds and impemement K-fold validation\nkf = KFold(n_splits=10, shuffle=True, random_state = 1)\n\n# Linear model with selected features\ncvs_lm = cross_val_score(lm, X_scaled.drop(['x','y','z','table','depth','Good'],axis=1), y_scaled, cv=kf, scoring='r2')\nprint(cvs_lm)\nprint('Linear regression cv R^2:', round(np.mean(cvs_lm),3), '+-', round(np.std(cvs_lm),3) )\n\ncvs_ridge = cross_val_score(ridge_model, X_scaled, y_scaled, cv=kf, scoring='r2')\nprint(cvs_ridge)\nprint('Ridge regression cv R^2:', round(np.mean(cvs_ridge),3), '+-', round(np.std(cvs_ridge),3) )\n\ncvs_lasso = cross_val_score(lasso_model, X_scaled, y_scaled, cv=kf, scoring='r2')\nprint(cvs_lasso)\nprint('Lasso regression cv R^2:', round(np.mean(cvs_lasso),3), '+-', round(np.std(cvs_lasso),3) )\n\ncvs_elasticnet = cross_val_score(elasticnet_model, X_scaled, y_scaled, cv=kf, scoring='r2')\nprint(cvs_elasticnet)\nprint('ElasticNet regression cv R^2:', round(np.mean(cvs_elasticnet),3), '+-', round(np.std(cvs_elasticnet),3) )","86b8e371":"X_scaled = X_train.copy().append(X_val_scaled)\ny_scaled = y_train.copy().append(y_val_scaled)\nfrom sklearn.metrics import median_absolute_error, r2_score\nf, (ax0, ax1) = plt.subplots(1, 2,)\ny_pred = ridge_model.predict(X_val_scaled)\ny_val2 = pd.DataFrame(y_val_scaled)\ny_pred2 = pd.DataFrame(y_pred)\n\nax0.scatter(y_val2, y_pred2, s=8)\nax0.set_ylabel('True target')\nax0.set_xlabel('Predicted target')\nax0.text(s='Ridge regression', x=0,\n            y=1, fontsize=16, multialignment='center')\nax0.text(0, 0.95, r'$R^2$=%.2f, MAE=%.2f' % (\n    r2_score(y_val2, y_pred2), median_absolute_error(y_val2, y_pred2)), fontsize=14)\nax0.ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(0, 0))\n\nax1.scatter(y_pred2, (y_pred2.values - y_val2.values), s=8)\nax1.set_ylabel('Residual')\nax1.set_xlabel('Predicted target')\nax1.ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(0, 0))\n\nplt.show()","3596b5b6":"import scipy.stats as stats\nfit = ridge_model.fit(X,y) # for later use\nstats.probplot(fit.predict(X), dist=\"norm\", plot=plt)\nplt.title(\"Normal Q-Q plot\")\nplt.figure(figsize=(4,4))\nplt.show()","bb02d804":"<div id=\"section_4_5\"><\/div>\n\n### 4.5) OLS Regression Model\n\n<div id=\"section_4_5_1\"><\/div>\n\n#### 4.5.1) Feature Selection\nNow that we have a normalised training dataset. We need to perform feature selection to identify the most significant independent variables. We used the VIF and p-values to reduce the features by eliminating a single feature with each iterations until all VIF scores are < 5 and the p-values are <= 0.05. ","2df749b2":"Checking the summary statistics.","dd76cd8b":"<div id=\"section_2_4_2\"><\/div>\n\n#### 2.4.2) Continuous variables","b062ca9c":"# Diamond Price Prediction - In depth analysis to apply different Regression Models\n\nThis notebook was created by: Alyosha Pulle,  Pedro Vasconcelos and Wenjuan Zhao","f391fbb3":"As highlighted in the Data Understanding section, we can see that the price has an exponential distribution and requires to be transformed into a normal distribution for use in regression modelling.","5e1156d8":"<div id=\"section_5_1\"><\/div>\n\n## 5.1) Lasso Regression","f17bb224":"We now see that there are no more p-values > 0..05 and all VIF scores are < 5. Therefore these variables are the set of selected features for the model.  \nFrom the R2 we can see that we are getting a score of 0.887. We next need to check the Error Terms of the model to verify whether the distribution is a normal distribution.","26aedbe3":"<div id=\"section_5_1_1\"><\/div>\n\n### 5.1.1) Achieved Lasso model","08b443e0":"We did a clamp transformation by replacing the two y outlier values with the next highest value of 10.54.","fb367061":"# <div id=\"section_7\">7. Conclusion<\/div>","ca2e1969":"Our R^2 = 0.86 and we got a U-shaped Residual plot which is suggesting a better fit for a non-linear model, which will explore in the next section.","e2ee80d9":"As indentifed in the Data Understanding we have diamond dimensions (x, y, z) of size 0.","aaf2650f":"<div id=\"section_4_5_2\"><\/div>\n\n#### 4.5.2) OLS Error Terms","43b3f6aa":"Can see collinearity between carat, x, y and z.","db62b0c4":"<div id=\"section_5_2_1\"><\/div>\n\n### 5.2.1) Achieved Ridge model","16b59c49":"<div id=\"section_3_3\"><\/div>\n\n### 3.3) Identifiy and Resolve Bad Data","ac1b4687":"Here we can see that the p-values are below 0.05 and the feature with the highest VIF score > 5 is Ideal. Therefore Ideal can be dropped.","97f38a71":"From above, decided to go with the log transformation as it was the closest we could get to a normal distribution and it gives a very low skew.","968f4f9b":"<div id=\"section_3_2_1\"><\/div>\n\n#### 3.2.1) Different Transformations\nWe tried various transformations on the price target feature to convert into a normal distribution.  ","fdbc0900":"<div id=\"section_4_2\"><\/div>\n\n### 4.2) Split Into Train\/Test Datasets","81e66628":"Compared to the total number of rows (53,940) the percentage is very small. So can drop these 20 rows.","e1603a5b":"# <div id=\"section_6\">6. K-folder Cross Validation<\/div>","d1a95479":"We did a clamp transformation by replacing the table outlier value with the next highest value of 79.","617bfa6f":"## <div id=\"section_2\">2. Data Understanding<\/div>\nAs a first step we need to understand the features of the dataset.\n<div id=\"section_2_1\"><\/div>\n\n### 2.1)Importing The Dataset \n#### Data source: https:\/\/www.kaggle.com\/shivam2503\/diamonds","4670d32a":"<div id=\"section_5_3_1\"><\/div>\n\n### 5.3.1) Achieved Elastic Net model","6ce2c011":"We now see a better distribution of the features.","669f6170":"To help the evaluation of our models we will be using k-fold cross validation. \nWe have chosen k=10.\nWith this method we will be splitting our data into 10 groups and test their fit into each one.\n\n![image.png](attachment:image.png)","f8961934":"## <div id=\"section_4\">4. Data Modelling- OLS<\/div>\n\n<div id=\"section_4_1\"><\/div>\n\n### 4.1) OLS Baseline Regression Model\nWe ran a baseline OLS regression model with the cleaned data.","db33f852":"Run the cross validation, find the best alpha, refit the model on all the data with that alpha.","15c937c8":"* <b>Clarity<\/b> : Diamond Clarity refers to the absence of the Inclusions and Blemishes.\n(In order from Best to Worst, FL = flawless, I3= level 3 inclusions) FL, IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3 \n![DiamondClarityChart.jpeg](attachment:DiamondClarityChart.jpeg)","d1501d23":"<div id=\"section_3_6\"><\/div>\n\n### 3.6) Correlations Heatmap","640db22a":"Regenerating the pairplots...","ca9c9692":"Running the OLS model and the VIF scores on the normalised training dataset....","e3f8dd48":"* <b>Depth<\/b> : The Height of a Diamond, measured from the Culet to the table, divided by its average Girdle Diameter. \n\n* <b>Table<\/b> : The Width of the Diamond's Table expressed as a Percentage of its Average Diameter. \n\n* <b>X<\/b> : Length of the Diamond in mm. \n\n* <b>Y<\/b> : Width of the Diamond in mm. \n\n* <b>Z<\/b> : Height of the Diamond in mm. \n\n* <b>Target Variable - Price<\/b> : the Price of the Diamond. \n![DiamondGIA.jpeg](attachment:DiamondGIA.jpeg)","fdb45a8e":"The selected model is performing well on predicting our target variable. \nFrom our residuals plot we can see it is centred around 0.","8c11267f":"As the dataset is now cleaned and prepared to run the regression models we will check the correllation with all features.","ab789917":"Unnamed is an index of the rows as in a uniform distribution.  \nTable and depth look to be normally distributed.  \nThe price is exponentially distributed and will need to be transformed into a normal distribution for use in regression modelling.  \nThe x and y distributions are very similar in shape.","d6ef902d":"Can see now that all the 0 min values are removed for x, y and z.\nThe max is very high, relative to mean and std for the y and z and indicates that we have outliers.  \n","63b7ef01":"Here we can see that the p-values are below 0.05 and the feature with the highest VIF score > 5 is x. Therefore x can be dropped.","d6bf3229":"<b>Diamonds Clarity by Cut and Price<b>","4fe4984b":"* <b>Carat<\/b> : Carat weight of the Diamond. \n* <b>Cut<\/b> : Describe cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal \n* <b>Color<\/b> : Color of the Diamond. With D being the best and J the worst. \n![DiamondColorChart.jpeg](attachment:DiamondColorChart.jpeg)","7c3f8a04":"![Diamonds4Cs.jpeg](attachment:Diamonds4Cs.jpeg)","ca930dc9":"<div id=\"section_5_3\"><\/div>\n\n## 5.3) Elastic Net Regression","172cebf9":"<div id=\"section_7_2\"><\/div>\n\n## 7.2) Prediction Plot and Residuals","95a15ade":"<div id=\"section_3_5\"><\/div>\n\n### 3.5) Conversion of Categorical columns\nTo fit a regression line we need all features to be numeric.\nThe cut, color and clarity features are the categorical columns which need to be converted.","e18e4f0d":"We can see the max z (31.8mm) is much larger than the next highest of 8.06.  \nWe did a clamp transformation by replacing the z outlier with the next highest value.","e49c7c61":"<div id=\"section_4_4\"><\/div>\n\n### 4.4) Train\/Test X and Y datasets","ea66415c":"<div id=\"section_2_2\"><\/div>\n\n### 2.2)  Data Features","a6e90cf6":"<br\/>\nThe model gives an R2 of 0.98 and AIC and BIC is high. The model is run on all the features and is not normalised so is cannot be relied on.  \n","d9d8ac6d":"Our QQ-Plot is lightly tailed and does not follow a normal distribution.","7e615a85":"In preparation of creating the Ridge, Lasso and Elastic Net models we created the y and X for the entire cleaned dataset.  \nAlso created datasets for the scaled validation dataset.","17006ec4":"<div id=\"section_2_4\"><\/div>\n\n### 2.4) Data distributions","9183cd11":"![WhatsApp%20Image%202021-04-18%20at%2016.49.07.jpeg](attachment:WhatsApp%20Image%202021-04-18%20at%2016.49.07.jpeg)\n<center>Image by <a href=\"https:\/\/pixabay.com\/users\/11754907-11754907\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4040800\">Cherie Vilneff<\/a> from <a href=\"https:\/\/pixabay.com\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4040800\">Pixabay<\/a><\/center>","a6a7f173":"Compare the continuous variables for collinearity.","8fb0b6fa":"<div id=\"section_7_3\"><\/div>\n\n## 7.3) QQ Plot for residual analysis","af09b5ef":"<div id=\"section_2_3\"><\/div>\n\n### 2.3) Missing values","17501a2e":"Here we can see that the p-value for \"Depth\" is above 0.05. Working off the p-value we decided to drop \"Depth\".  \nAs the first preference is to the p-value we do not need to run the VIF in this instance.","3ec87b99":"We can see that carat, x, y and z are highly correllated (dark red regions) with values of 1 or near to 1.\nWe will investigate removing the highly correllated features in the Data Modelling section.","2a8a2e8b":"<div id=\"section_3_4\"><\/div>\n\n### 3.4) Collinearity and Outliers","d334b003":"<div id=\"section_4_5_3\"><\/div>\n\n#### 4.5.3) OLS Summary","7ed95f75":"Run the cross validation, find the best alpha, refit the model on all the data with that alpha.","7f7cdd4b":"After having built our four different models and run the k-fold cross validation can now select our final model.\n\nRegression Model Name | Best alpha | Accuracy | Mean Absolute Error | R Square (model build) | Mean Squared Error | 10-fold cross validation\n-------------|------|------|------|------|------|------ \nOLS  | - | - | - | 0.982 | -|0.981 +\/- 0.001\nOLS With FR*  | - | -| 0.065 | 0.8493 | 0.0094|0.881 +\/- 0.003\nLasso Regression | 0.001 | 95.71% | 0.0429| 0.9433 | 0.0035|0.953 +\/- 0.002\nRidge Regression  | 0.148 | 98.2% | 0.0552 | 0.936 | 0.004|0.98 +\/- 0.001\nElastic Net Regression  | 0.001 | 96.33% | 0.0464 | 0.938 | 0.0038|0.958 +\/- 0.001\n\n*=Feature Reduction\n\nWe got very good results from all of them, with R^2 > 0.85. The Ridge, Lasso and Elastic Net are performing very similarly and they would be very effictive if applied.\n\nWe do not advise the use of the Linear Regression (OLS) because it is including all the features, with high VIF scores as explained in section 4.5.  \nAs we can see in the \"OLS With FR\" which applied feature reduction the R^2 dropped to 0.881.  \n<br\/>\n<b>Since the R^2 of the Ridge Regression after the k-fold cross validation is the highest (0.98) and the MEA is very low (0.0552) we will select it as our Final Model<\/b>.","f47d920f":"The Error Terms are very close to a normal distribution which is a requirement for a regression model. The reason for the left skew could be due to the target \"price_t\" feature not following a perfect normal distribution.  ","0c947893":"Here we can see that the p-values are below 0.05 and the feature with the highest VIF score > 5 is y. Therefore y can be dropped.","cec8c179":"There are 53,940 rows and 11 columns in the dataset. And you can see that there are no missing values in the dataset.","dd5bcf1c":"## <div id=\"section_1\">1. Business Understanding<\/div>\n\n### About Diamond\nDiamonds were formed billions of years ago and are extremely rare because so few are able to survive the difficult journey from the pits of the earth to reach the earth\u2019s surface. From the diamonds that are being mined today, only about 50 percent are thought to be high enough quality to be sold on the diamond market. \n### Diamond Cost and Quality\nEach diamond is unique and has specific qualities that establish its value. Diamond pricing is much more complicated than gold. While every ounce of gold is the same, diamonds must be graded to determine their value. Even though the 4Cs is the basic standard for grading, a myriad other factors influence diamond value.\nAccording to [GIA](https:\/\/4cs.gia.edu\/en-us\/4cs-diamond-quality\/), \nglobally accepted standard for describing diamonds: Colour, Clarity, Cut and Carat Weight.  \n### Our Goal  \nTo analyse diamonds dataset by their cut, colour, clarity, price, and other attributes and find the best regression model to predict diamond price.\n","ca5adfe0":"Here we can see that the p-values are below 0.05 and the feature with the highest VIF score > 5 is table. Therefore table can be dropped.","7bfe5d4e":"<div id=\"section_3_1\"><\/div>\n\n### 3.1) Drop the index column.","268c5ed6":"Before using the data for modelling we need to split the dataset into training and test datasets.  \nTraining to create the models and test to validate the same.  \nWe chose to split with a 70:30 ratio.","44ed9a22":"<div id=\"section_5_2\"><\/div>\n\n## 5.2) Ridge Regression","bce7d6d9":"Here we can see that the p-values are below 0.05 and the feature with the highest VIF score > 5 is z. Therefore z can be dropped.","d07d55ad":"From the scatter plots, can see that there are large outliers in the table, y and z columns as highlighted in below snippet from the plots. These will need to be investigated and adjusted or removed from the dataset.\n![DiamondsOutliers.png](attachment:DiamondsOutliers.png)","8776c985":"<div id=\"section_3_4_3\"><\/div>\n\n#### 3.4.3) Inspect the max table value rows.","fc9e4ff4":"\n<div id=\"section_4_3\"><\/div>\n\n### 4.3) Normalizing the Training Dataset","c2a04937":"### Table of contents\n1. [Business Understanding](#section_1)\n2. [Data Understanding](#section_2)\n     * 2.1) [Importing the Dataset](#section_2_1)\n     * 2.2) [Data Features](#section_2_2)\n     * 2.3) [Missing Values](#section_2_3)\n     * 2.4) [Data Distributions](#section_2_4)\n         * 2.4.1) [Categorical Variables](#section_2_4_1)\n         * 2.4.2) [Continuos Variables](#section_2_4_2)\n3. [Data Preparation](#section_3)\n     * 3.1) [Drop index column](#section_3_1)\n     * 3.2) [Normalizing target variable](#section_3_2)\n         * 3.2.1) [Different transformations](#section_3_2_1)\n     * 3.3) [Identify and Resolve Bad Data](#section_3_3)\n     * 3.4) [Colinearity and Outliers](#section_3_4)\n         * 3.4.1) [Inspect \"z\" outliers](#section_3_4_1)\n         * 3.4.2) [Inspect \"y\" outliers](#section_3_4_2)\n         * 3.4.3) [Inspect \"table\" outliers](#section_3_4_3)\n     * 3.5) [Conversion of Categorical columns](#section_3_5)\n     * 3.6) [Correlations Heatmap](#section_3_6)\n4. [Data Modelling - OLS](#section_4)\n    * 4.1) [OLS Baseline Regression Model](#section_4_1)\n    * 4.2) [Split into Train\/Test Datasets](#section_4_2)\n    * 4.3) [Normalizing the Training Dataset](#section_4_3)\n    * 4.4) [Tran\/Test X and Y](#section_4_4)\n    * 4.5) [OLS Regression Model](#section_4_5)\n         * 4.5.1) [Feature Selection](#section_4_5_1)\n         * 4.5.2) [OLS Error Terms](#section_4_5_2)\n         * 4.5.3) [OLS Summary](#section_4_5_3)\n         * 4.5.3) [OLS Model Performance and Residuals](#section_4_5_4)\n5. [Other Regression Models](#section_5)\n    * 5.1) [Lasso Regression](#section_5_1)\n         * 5.1.1) [Achieved Lasso model](#section_5_1_1)\n    * 5.2) [Ridge Regression](#section_5_2)\n         * 5.2.1) [Achieved Ridge model](#section_5_2_1)\n    * 5.3) [Elastic Net Regression](#section_5_3)\n         * 5.3.1) [Achieved Ridge model](#section_5_2_1)\n6. [K-Fold Cross Validation](#section_6)\n7. [Conclusion](#section_7)\n    * 7.1) [Fianl Model](#section_7_1)\n    * 7.2) [Prediction Plot and Residuals](#section_7_2)\n    * 7.3) [QQ Plot](#section_7_)","ebcd5ac2":"<div id=\"section_3_4_1\"><\/div>\n\n#### 3.4.1) Inspect the max z value row.","7411fbbf":"<div id=\"section_4_5_4\"><\/div>\n\n#### 4.5.4) OLS model Performance and Residuals","8839f354":"## <div id=\"section_3\">3. Data Preparation<\/div>\nNow that we understand the data we need to prepare the data for modelling by transforming the price target feature distribution, investigating the 0 values, adjusting\/removing outliers and converting categorical features into numeric.","aafd8d27":"Finally running the model against the Test dataset for predictive capability.","b9493f39":"We can see that most diamonds in the dataset has Ideal cut, and the Premium cut is most expensive, following by the Very Good cut. The price of most Good cut, Very Good cut, Premium cut and ideal cut are all around 1600.","93540b04":"<div id=\"section_3_2\"><\/div>\n\n### 3.2) Normalizing target variable","b6d435da":"Most diamonds are in colour G, following by colour E, F.  \nMost expensive colour are J and I, they have similar price. The next expensive colours are G and H.  \nThe price of most diamonds of every colour are similar, around 1500.","0e85a111":"<div id=\"section_2_4_1\"><\/div>\n\n#### 2.4.1) Categorical variables","37f8ee72":"<div id=\"section_7_1\"><\/div>\n\n## 7.1) Our Final Model","e5098b32":"Run the cross validation, find the best alpha, refit the model on all the data with that alpha.","5723e2b5":"The most diamonds has SI1 clarity.  \nThe most expensive diamonds are with VS1 and VS2 clarity, and they have similar affect on the diamond price.  \nThe distribution of clarity VS1, VS2, VVS1 and VVS2 are similar.","cb2b39b6":"We can see that 'Unnamed: 0'  is a row index and can be dropped, we will drop it later in data preparation. Also the min value of x, y, z are 0, and the max value of x, y, z are quite high comparing to the 3rd quantile and min value, therefore, we need to investigate that later.","70b336ba":"# <div id=\"section_5\">5. Other Regression Models<\/div>","7eedadf3":"<div id=\"section_3_4_2\"><\/div>\n\n#### 3.4.2) Inspect the max y value rows."}}