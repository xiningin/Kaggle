{"cell_type":{"df501ca5":"code","8096a984":"code","e5db87dd":"code","a666377c":"code","d9d6ef36":"code","ce5d3b4a":"code","0ec1c20e":"code","3c8bd897":"code","db9348d4":"code","aef56b57":"code","261c9b1e":"code","5158d5b3":"code","4b4b5460":"code","691e9e51":"code","eef2b8dd":"code","7585d5fa":"code","cc630b41":"code","cc322da4":"code","7b12c515":"code","04c140f7":"code","c07efe4c":"code","6209354e":"code","ba12911f":"code","e934ea41":"code","5d81c64e":"code","3c1835c8":"code","04711a96":"code","01c79377":"code","fba5be29":"code","f4ec8c2a":"code","0308d4c2":"code","a58e6851":"markdown","00ce46c2":"markdown","6b996e79":"markdown","dd3c802c":"markdown","51624a5f":"markdown"},"source":{"df501ca5":"import warnings\nwarnings.filterwarnings(action='once')\n\nimport os\nfrom pathlib import Path\nimport random\nimport multiprocessing\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm_notebook, tqdm\n\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet.gluon import Block\nfrom mxnet.gluon import nn\nimport gluonnlp as nlp\nfrom gluonnlp.model import get_bert_model\nfrom gluonnlp.data import BERTTokenizer, BERTSentenceTransform","8096a984":"path = Path('..\/input\/jigsaw-unintended-bias-in-toxicity-classification')\nos.listdir(path)","e5db87dd":"train_csv = path\/'train.csv'\ntest_csv = path\/'test.csv'\nsample_csv = path\/'sample_submission.csv'","a666377c":"test_size=90000","d9d6ef36":"train_df = pd.read_csv(train_csv)\ntrain_df = train_df[:test_size]","ce5d3b4a":"test_df = pd.read_csv(test_csv)\ntest_df.head()","0ec1c20e":"sample_df = pd.read_csv(sample_csv)\nsample_df.head()","3c8bd897":"class BERTClassifier(Block):\n    \"\"\"Model for sentence (pair) classification task with BERT.\n\n    The model feeds token ids and token type ids into BERT to get the\n    pooled BERT sequence representation, then apply a Dense layer for\n    classification.\n\n    Parameters\n    ----------\n    bert: BERTModel\n        Bidirectional encoder with transformer.\n    num_classes : int, default is 2\n        The number of target classes.\n    dropout : float or None, default 0.0.\n        Dropout probability for the bert output.\n    prefix : str or None\n        See document of `mx.gluon.Block`.\n    params : ParameterDict or None\n        See document of `mx.gluon.Block`.\n    \"\"\"\n\n    def __init__(self,\n                 bert,\n                 num_classes=2,\n                 dropout=0.0,\n                 prefix=None,\n                 params=None):\n        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n        self.bert = bert\n        with self.name_scope():\n            self.classifier = nn.HybridSequential(prefix=prefix)\n            if dropout:\n                self.classifier.add(nn.Dropout(rate=dropout))\n            self.classifier.add(nn.Dense(units=num_classes))\n\n    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n        \"\"\"Generate the unnormalized score for the given the input sequences.\n\n        Parameters\n        ----------\n        inputs : NDArray, shape (batch_size, seq_length)\n            Input words for the sequences.\n        token_types : NDArray, shape (batch_size, seq_length)\n            Token types for the sequences, used to indicate whether the word belongs to the\n            first sentence or the second one.\n        valid_length : NDArray or None, shape (batch_size)\n            Valid length of the sequence. This is used to mask the padded tokens.\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, num_classes)\n        \"\"\"\n        _, pooler_out = self.bert(inputs, token_types, valid_length)\n        return self.classifier(pooler_out)","db9348d4":"class BERTDatasetTransform(object):\n    \"\"\"Dataset Transformation for BERT-style Sentence Classification or Regression.\n\n    Parameters\n    ----------\n    tokenizer : BERTTokenizer.\n        Tokenizer for the sentences.\n    max_seq_length : int.\n        Maximum sequence length of the sentences.\n    labels : list of int , float or None. defaults None\n        List of all label ids for the classification task and regressing task.\n        If labels is None, the default task is regression\n    pad : bool, default True\n        Whether to pad the sentences to maximum length.\n    pair : bool, default True\n        Whether to transform sentences or sentence pairs.\n    label_dtype: int32 or float32, default float32\n        label_dtype = int32 for classification task\n        label_dtype = float32 for regression task\n    \"\"\"\n\n    def __init__(self,\n                 tokenizer,\n                 max_seq_length,\n                 class_labels=None,\n                 pad=True,\n                 pair=True,\n                 has_label=True):\n        self.class_labels = class_labels\n        self.has_label = has_label\n        self._label_dtype = 'int32' if class_labels else 'float32'\n        if has_label and class_labels:\n            self._label_map = {}\n            for (i, label) in enumerate(class_labels):\n                self._label_map[label] = i\n        self._bert_xform = BERTSentenceTransform(\n            tokenizer, max_seq_length, pad=pad, pair=pair)\n\n    def __call__(self, line):\n        \"\"\"Perform transformation for sequence pairs or single sequences.\n\n        The transformation is processed in the following steps:\n        - tokenize the input sequences\n        - insert [CLS], [SEP] as necessary\n        - generate type ids to indicate whether a token belongs to the first\n          sequence or the second sequence.\n        - generate valid length\n\n        For sequence pairs, the input is a tuple of 3 strings:\n        text_a, text_b and label.\n\n        Inputs:\n            text_a: 'is this jacksonville ?'\n            text_b: 'no it is not'\n            label: '0'\n        Tokenization:\n            text_a: 'is this jack ##son ##ville ?'\n            text_b: 'no it is not .'\n        Processed:\n            tokens:  '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n            valid_length: 14\n            label: 0\n\n        For single sequences, the input is a tuple of 2 strings: text_a and label.\n        Inputs:\n            text_a: 'the dog is hairy .'\n            label: '1'\n        Tokenization:\n            text_a: 'the dog is hairy .'\n        Processed:\n            text_a:  '[CLS] the dog is hairy . [SEP]'\n            type_ids: 0     0   0   0  0     0 0\n            valid_length: 7\n            label: 1\n\n        Parameters\n        ----------\n        line: tuple of str\n            Input strings. For sequence pairs, the input is a tuple of 3 strings:\n            (text_a, text_b, label). For single sequences, the input is a tuple\n            of 2 strings: (text_a, label).\n\n        Returns\n        -------\n        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n        np.array: valid length in 'int32', shape (batch_size,)\n        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n        np.array: classification task: label id in 'int32', shape (batch_size, 1),\n            regression task: label in 'float32', shape (batch_size, 1)\n        \"\"\"\n        if self.has_label:\n            input_ids, valid_length, segment_ids = self._bert_xform(line[:-1])\n            label = line[-1]\n            # map to int if class labels are available\n            if self.class_labels:\n                label = self._label_map[label]\n            label = np.array([label], dtype=self._label_dtype)\n            return input_ids, valid_length, segment_ids, label\n        else:\n            return self._bert_xform(line)\n","aef56b57":"ctx = mx.gpu(0)","261c9b1e":"max_len = 128\npad = True\npair = False\n\nepochs = 1\nbatch_size = 32\n#optimizer='bertadam'\nclass_labels=[0,1]\nlr = 1e-6\nepsilon = 1e-06\nwarmup_ratio = 0.1\nlog_interval = 1000\naccumulate = None","5158d5b3":"bert_base, vocabulary = nlp.model.get_model('bert_12_768_12', \n                                             dataset_name='book_corpus_wiki_en_uncased',\n                                             pretrained=True, ctx=ctx, use_pooler=True,\n                                             use_decoder=False, use_classifier=False,\n                                             root='..\/input\/bertmx\/bert-mx')","4b4b5460":"model = BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n# only need to initialize the classifier layer.\nmodel.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\nmodel.hybridize(static_alloc=True)\n\n# softmax cross entropy loss for classification\nloss_function = gluon.loss.SoftmaxCELoss()\nloss_function.hybridize(static_alloc=True)\n\nmetric = mx.metric.Accuracy()","691e9e51":"tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)","eef2b8dd":"train_df['comment_text'] = train_df['comment_text'].astype(str)\ntrain_df['target']=(train_df['target']>=0.5).astype(int)","7585d5fa":"train_data_raw = train_df[['comment_text', 'target']].values","cc630b41":"pool = multiprocessing.Pool()\n\n# transformation for data train and dev\n#label_dtype = 'float32' if not task.class_labels else 'int32'\nlabel_dtype='int32'\ntrans = BERTDatasetTransform(tokenizer, max_len,\n                             class_labels=class_labels,\n                             pad=pad, pair=pair,\n                             has_label=True)\n\n# data train\n# task.dataset_train returns (segment_name, dataset)\n#train_tsv = task.dataset_train()[1]\n#data_train = mx.gluon.data.SimpleDataset(pool.map(trans, train_data_raw))\ndata_train = mx.gluon.data.SimpleDataset(train_data_raw)\ndata_train = data_train.transform(trans)\ndata_train_len = data_train.transform(\n    lambda input_id, length, segment_id, label_id: length, lazy=False)\n# bucket sampler for training\nbatchify_fn = nlp.data.batchify.Tuple(\n    nlp.data.batchify.Pad(axis=0), nlp.data.batchify.Stack(),\n    nlp.data.batchify.Pad(axis=0), nlp.data.batchify.Stack(label_dtype))\nbatch_sampler = nlp.data.sampler.FixedBucketSampler(\n    data_train_len,\n    batch_size=batch_size,\n    #num_buckets=10,\n    ratio=0,\n    shuffle=True)\n# data loader for training\ntrain_data = gluon.data.DataLoader(\n    dataset=data_train,\n    num_workers=4,\n    batch_sampler=batch_sampler,\n    batchify_fn=batchify_fn)\nnum_train_examples = len(data_train)","cc322da4":"print('vocabulary used for tokenization = \\n%s'%vocabulary)\nprint('[PAD] token id = %s'%(vocabulary['[PAD]']))\nprint('[CLS] token id = %s'%(vocabulary['[CLS]']))\nprint('[SEP] token id = %s'%(vocabulary['[SEP]']))\nprint('token ids = \\n%s'%data_train[4][0])\nprint('valid length = \\n%s'%data_train[4][1])\nprint('segment ids = \\n%s'%data_train[4][2])\nprint('label = \\n%s'%data_train[4][3])","7b12c515":"optimizer = mx.optimizer.create('adam', multi_precision=True, learning_rate=lr, epsilon=epsilon)","04c140f7":"all_model_params = model.collect_params()\noptimizer_params = {'learning_rate': lr, 'epsilon': epsilon, 'wd': 0.01}\n\ntry:\n    trainer = gluon.Trainer(all_model_params, optimizer,\n                            update_on_kvstore=False)\nexcept ValueError as e:\n    print(e)\n    warnings.warn(\n        'AdamW optimizer is not found. Please consider upgrading to '\n        'mxnet>=1.5.0. Now the original Adam optimizer is used instead.')\n    trainer = gluon.Trainer(all_model_params, 'adam',\n                            optimizer_params, update_on_kvstore=False)","c07efe4c":"model.load_parameters('..\/input\/mxbert06151\/j-20190616k', ctx=ctx)","6209354e":"step_size = batch_size * accumulate if accumulate else batch_size\nnum_train_steps = int(num_train_examples \/ step_size * epochs)\nnum_warmup_steps = int(num_train_steps * warmup_ratio)\nstep_num = 0\n\n# Do not apply weight decay on LayerNorm and bias terms\nfor _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n    v.wd_mult = 0.0\n# Collect differentiable parameters\nparams = [p for p in all_model_params.values() if p.grad_req != 'null']\n\n# Set grad_req if gradient accumulation is required\nif accumulate:\n    for p in params:\n        p.grad_req = 'add'\n\n#tic = time.time()\nfor epoch_id in range(epochs):\n    metric.reset()\n    step_loss = 0\n    #tic = time.time()\n    all_model_params.zero_grad()\n\n    for batch_id, seqs in enumerate(train_data):\n        # learning rate schedule\n        if step_num < num_warmup_steps:\n            new_lr = lr * step_num \/ num_warmup_steps\n        else:\n            non_warmup_steps = step_num - num_warmup_steps\n            offset = non_warmup_steps \/ (num_train_steps - num_warmup_steps)\n            new_lr = lr - offset * lr\n        optimizer.set_learning_rate(new_lr)\n\n        # forward and backward\n        with mx.autograd.record():\n            input_ids, valid_length, type_ids, label = seqs\n            out = model(\n                input_ids.as_in_context(ctx), type_ids.as_in_context(ctx),\n                valid_length.astype('float32').as_in_context(ctx))\n            ls = loss_function(out, label.as_in_context(ctx)).mean()\n        ls.backward()\n\n        # update\n        if not accumulate or (batch_id + 1) % accumulate == 0:\n            trainer.allreduce_grads()\n            nlp.utils.clip_grad_global_norm(params, 1)\n            trainer.update(accumulate if accumulate else 1)\n            # set grad to zero for gradient accumulation\n            all_model_params.zero_grad()\n            step_num += 1\n\n        step_loss += ls.asscalar()\n        metric.update([label], [out])\n        if (batch_id + 1) % (log_interval) == 0:\n            #log_train(batch_id, len(train_data), metric, step_loss, log_interval,\n                      #epoch_id, trainer.learning_rate)\n            print('[Epoch {} Batch {}\/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n                  .format(epoch_id, batch_id + 1, len(train_data),\n                 step_loss \/ log_interval,\n                 optimizer.learning_rate, metric.get()[1]))\n            step_loss = 0\n    mx.nd.waitall()","ba12911f":"model.save_parameters('j-20190616k')","e934ea41":"test_df['comment_text'] = test_df['comment_text'].astype(str)\ntest_data_raw = test_df[['comment_text']].values","5d81c64e":"# batchify for data test\ntest_batchify_fn = nlp.data.batchify.Tuple(\n    nlp.data.batchify.Pad(axis=0), nlp.data.batchify.Stack(),\n    nlp.data.batchify.Pad(axis=0))\n\n# transform for data test\ntest_trans = BERTDatasetTransform(tokenizer, max_len,\n                                  class_labels=None,\n                                  pad=pad, pair=pair,\n                                  has_label=False)\n\ndata_test = mx.gluon.data.SimpleDataset(pool.map(test_trans, test_data_raw))\nloader_test = mx.gluon.data.DataLoader(\n    data_test,\n    batch_size=batch_size,\n    num_workers=4,\n    shuffle=False,\n    batchify_fn=test_batchify_fn)","3c1835c8":"data_test[0]","04711a96":"#value_list = []\n#index_list = []\nresults = []\nfor _, seqs in enumerate(loader_test):\n    input_ids, valid_length, type_ids = seqs\n    out = model(input_ids.as_in_context(ctx),\n                type_ids.as_in_context(ctx),\n                valid_length.astype('float32').as_in_context(ctx))\n    results.extend([o for o in out.asnumpy()])\n    #values, indices = mx.nd.topk(out, k=1, ret_typ='both')\n    #value_list.extend(values.asnumpy().reshape(-1).tolist())\n    #index_list.extend(indices.asnumpy().reshape(-1).tolist())\n\nmx.nd.waitall()\n\n","01c79377":"results[24]","fba5be29":"predictions = [mx.nd.array(result).softmax().asnumpy()[1] for result in results]","f4ec8c2a":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': predictions\n})","0308d4c2":"submission.to_csv('submission.csv', index=False)","a58e6851":"### preprocessing","00ce46c2":"### training","6b996e79":"### prediction","dd3c802c":"I haven't seen a public kernel using Gluon NLP, so here is my simple example.  Adapted from https:\/\/gluon-nlp.mxnet.io\/examples\/sentence_embedding\/bert.html","51624a5f":"### classes"}}