{"cell_type":{"9829ce65":"code","d80e86a3":"code","3579a3b4":"code","f3497e63":"code","cddf83a7":"code","25051846":"code","82c5c3a6":"code","8ab09367":"code","faae0a32":"code","53fc8a7d":"code","b14e17c1":"code","ac6d1411":"code","bff9f93d":"code","c7a2436d":"code","41cf4a6e":"code","efad4d2e":"code","edf1dc5a":"code","b28ca588":"code","45a7192d":"code","d79b7129":"code","193139d7":"code","0d46af62":"code","97000943":"code","19324a45":"code","88a05480":"code","59855c34":"code","0300d1a4":"code","caca40d1":"code","9b8134d5":"code","0d5c5db2":"code","d58233db":"code","c3835043":"code","f5723901":"code","4cc44a98":"code","7afd967a":"code","b76c7580":"code","514c1d0b":"markdown","57c51b7c":"markdown","632b99c5":"markdown","3de3727d":"markdown","f6863971":"markdown","58068117":"markdown","e529801b":"markdown","e37da72f":"markdown","56591df7":"markdown","497db478":"markdown","fe893338":"markdown","5519b524":"markdown","f8e6be44":"markdown","1c241a76":"markdown","98502dbb":"markdown","449a9d34":"markdown","c15bb2b3":"markdown","b630a1e0":"markdown","8d0be706":"markdown","5f1672f0":"markdown","a20d1597":"markdown","ddd0d057":"markdown","934a7a3b":"markdown","cac95b50":"markdown","d3c6aefc":"markdown","8316e8bc":"markdown","8f4197b4":"markdown","213f10f3":"markdown","1c9a5cac":"markdown","267ea27b":"markdown"},"source":{"9829ce65":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn import preprocessing\nimport csv\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm\n#import parameters_housing\nfrom sklearn.preprocessing import LabelBinarizer\nfrom catboost import CatBoostClassifier\nfrom catboost import CatBoostRegressor\n\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","d80e86a3":"train_data = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntrain_data.shape","3579a3b4":"test_data = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")\ntest_data.shape","f3497e63":"train_data.columns","cddf83a7":"y = train_data['SalePrice']\ny.shape","25051846":"train_data.drop(['SalePrice'], axis=1,inplace=True)\ntrain_data.shape","82c5c3a6":"def get_list_of_columns_of_type(X,col_type):\n    s = (X.dtypes == 'object')     #default categorical\n    if col_type == 'categorical':\n        s = (X.dtypes == 'object')\n    elif col_type == 'numerical':\n        s = (X.dtypes != 'object')\n    cols = list(s[s].index)\n    return cols","8ab09367":"categorical_cols = get_list_of_columns_of_type(train_data, 'categorical')\nprint(len(categorical_cols))\ncategorical_cols","faae0a32":"numerical_cols = get_list_of_columns_of_type(train_data, 'numerical')\nprint(len(numerical_cols))\nnumerical_cols","53fc8a7d":"missing_val_count_num_train = (train_data[numerical_cols].isnull().sum())\nmissing_val_count_num_train.sort_values(ascending=False,inplace=True)\nmissing_val_count_num_train[missing_val_count_num_train > 0]","b14e17c1":"missing_val_count_cat_train = (train_data[categorical_cols].isnull().sum())\nmissing_val_count_cat_train.sort_values(ascending=False,inplace=True)\nmissing_val_count_cat_train[missing_val_count_cat_train > 0]","ac6d1411":"missing_val_count_num_test = (test_data[numerical_cols].isnull().sum())\nmissing_val_count_num_test.sort_values(ascending=False,inplace=True)\nmissing_val_count_num_test[missing_val_count_num_test > 0]","bff9f93d":"missing_val_count_cat_test = (test_data[categorical_cols].isnull().sum())\nmissing_val_count_cat_test.sort_values(ascending=False,inplace=True)\nmissing_val_count_cat_test[missing_val_count_cat_test > 0]","c7a2436d":"train_data_count = train_data.shape[0]\nmerged_data = train_data.append(test_data)\nmerged_data.shape","41cf4a6e":"missing_val_count_num_merged = (merged_data[numerical_cols].isnull().sum())\nmissing_val_count_num_merged.sort_values(ascending=False,inplace=True)\nmissing_val_count_num_merged[missing_val_count_num_merged > 0]","efad4d2e":"missing_val_count_cat_merged = (merged_data[categorical_cols].isnull().sum())\nmissing_val_count_cat_merged.sort_values(ascending=False,inplace=True)\nmissing_val_count_cat_merged[missing_val_count_cat_merged > 0]","edf1dc5a":"missing_val_count_cat_merged[missing_val_count_cat_merged > (merged_data.shape[0]\/2)]","b28ca588":"merged_data.drop(['PoolQC','MiscFeature','Alley','Fence'], axis=1,inplace=True)\nmerged_data.shape","45a7192d":"numerical_cols = get_list_of_columns_of_type(merged_data,'numerical')\ncategorical_cols = get_list_of_columns_of_type(merged_data,'categorical')\nnumerical_cols","d79b7129":"for col in numerical_cols:\n        merged_data[col] = merged_data[col].fillna(merged_data[col].mean())","193139d7":"missing_val_count_num_merged = (merged_data[numerical_cols].isnull().sum())\nmissing_val_count_num_merged.sort_values(ascending=False,inplace=True)\nmissing_val_count_num_merged[missing_val_count_num_merged > 0]","0d46af62":"for col in categorical_cols:\n        merged_data[col] = merged_data[col].fillna(\"Unknown\")","97000943":"missing_val_count_cat_merged = (merged_data[categorical_cols].isnull().sum())\nmissing_val_count_cat_merged.sort_values(ascending=False,inplace=True)\nmissing_val_count_cat_merged[missing_val_count_cat_merged > 0]","19324a45":"label_encoder = LabelEncoder()\nfor col in categorical_cols:\n    merged_data[col] = label_encoder.fit_transform(merged_data[col])","88a05480":"categorical_cols = get_list_of_columns_of_type(merged_data,'categorical')\ncategorical_cols","59855c34":"train_data = merged_data[0:train_data_count]\ntrain_data.shape","0300d1a4":"test_data = merged_data[train_data_count:]\ntest_data.shape","caca40d1":"X_train, X_valid, y_train, y_valid = train_test_split(train_data, y, train_size=0.8, test_size=0.2,random_state=0)\nprint(\" Training Data Shape : {}\\n Training Target Shape : {}\\n Validation Data Shape : {}\\n Validation Target Shape : {}\".format(X_train.shape,y_train.shape,X_valid.shape,y_valid.shape))","9b8134d5":"my_model = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42) ","0d5c5db2":"my_model.fit(X_train, y_train)","d58233db":"preds = my_model.predict(X_train)\nmean_absolute_error(y_train, preds)","c3835043":"preds = my_model.predict(X_valid)\nmean_absolute_error(y_valid, preds)","f5723901":"my_model.fit(train_data,y)","4cc44a98":"preds = my_model.predict(test_data)\npreds.shape","7afd967a":"submission = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv\")\nsubmission['SalePrice'] = preds","b76c7580":"submission.to_csv(\"submission.csv\", index=False)","514c1d0b":"Let's check if there are any categorical columns left with missing values","57c51b7c":"Let's look into missing value counts in categorical columns in training data","632b99c5":"Let's check how our model behaves on the training data itself","3de3727d":"The list of numerical and categorical columns might have changed after removing those columns. Let's update them.","f6863971":"Let's first analyze missing value counts in numerical columns in training data","58068117":"Note : Some of these imports were imported just for tuning\/testing purpose and might be not used as such in this notebook.\n\nNow let's load our train and test data. Please change path to file, if required.","e529801b":"As we can see the missing values in all the numerical columns have been filled.\n\nNow, let's come to categorical columns.\n\nFor simplicity. we can fill missing values in categorical columns with constant \"Unknown\"","e37da72f":"Now, let's test the above function and get lists of categorical columns in our training data.","56591df7":"Now let's print missing value counts for numerical and categorical columns for merged data","497db478":"Let's fit this model ONLY on X_train and y_train, as of now","fe893338":"Let's check if there are any numerical columns left with missing values","5519b524":"Let's remove these columns from merged data","f8e6be44":"Let's separate our target variable 'SalePrice' from training data and store in a variable y","1c241a76":"There are many ways to fill missing values in numerical columns:-\n1. Fill with 0\n2. Fill with mean\n3. Fill with median\n4. Fill with mode\n\nFor simplicity, let's fill the missing values by mean","98502dbb":"Now, we need to drop 'SalePrice' column from training data","449a9d34":"As we can see the list is empty. It means all columns have been converted into numerical columns.\n\nNow we can feed this data to our model. But before that let's de-merge our training and test data","c15bb2b3":"Now, we can feed this data to our model. But before that let's split our training data into training and validation data. So that we can test our model on unseen data for any biased-ness.","b630a1e0":"Let's print submission to a .csv file for submission","8d0be706":"As we can see, this value is too low as expected. Model has predicted already seen data quite correctly. \n\nLet's test in on validation data which is unseen by the model","5f1672f0":"Let's print the list of columns in our training data","a20d1597":"Let's use a GradientBoostRegresssor model with parameters(already tuned to best values) : n_estimators=4000, learning_rate=0.05, max_depth=4","ddd0d057":"As we can see the columns having missing values in test data is not same as columns having missing values in train data. It generally happens in real-time data, where we never know which all columns will be having missing values in future.\n\nFor this particular problem, as our train and test data is fixed. We can do this:-\n1. Merge train and test data \n2. Fill missing in the merged set\n3. Again de-merge into train and test data ","934a7a3b":"As we can see from training data shape, the number of columns is reduced by 1.\n\nMostly, the models that we train do not work if any columns is having a missing value. So we need to clean our data first. The missing values will be filled based on their data type : numerical or categorical.\n\nLet's define a function to get list of a specific category of columns.","cac95b50":"As we can see, there are some columns having too many missing values. These columns are not might not be providing much information to predict our target variable. \nWe can remove those columns from our data by setting some threshold of say 50%.\n\nLet's print the list of those columns","d3c6aefc":"Let's check if there is any still any categorical column","8316e8bc":"As we can see the missing values in all the categorical columns have also been filled.\n\nThe models that we train do not understand categorical values in the form of strings as such. So, we need to encode them into 'int' or 'float'. \n\nTwo most commonly used techniques are:-\n1. Label Encoding : Assigns a number to each distinct value in a column starting from 0\n2. One Hot Encoding : Replace that column with \"number of unique values in that column\" columns, where value will be 1 if that particular unique value was there in the original column in that row, and 0 otherwise\n\nFor simplicity, let's do Label Encoding","8f4197b4":"Let's load our submission format and fill 'SalePrice' columns with our predictions data","213f10f3":"Let us see how's the scenario of missing value counts of numerical data in test data","1c9a5cac":"As we can see there is large difference between Training Error and Validation Error. It means there is a certain biased-ness of our model towards training data.\n\nAnyways, let's move on to make predictions for test data for submission. \n\nBut before that we can make our model a little better by training it again on the entire training set which was there with us before train and valid split. So that we can get better results for our test data.","267ea27b":"Let's print list of numerical columns"}}