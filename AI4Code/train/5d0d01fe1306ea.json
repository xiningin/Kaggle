{"cell_type":{"c3745643":"code","7a7c0d2b":"code","96d0803e":"code","a3f2fd2f":"code","b913ed7c":"code","e81ed6d1":"code","82515d67":"code","8ef2bf9f":"code","5dd0be93":"code","93ba9ce4":"code","54fb6bb1":"code","55ea718c":"code","e64ef966":"code","a90b93bc":"code","c6c284c8":"code","fe03d1b8":"code","f4aff9ea":"code","3f378209":"code","2b8b80ef":"code","65c07261":"code","92335b15":"code","67262df2":"code","1a1b18db":"code","aa68bd7e":"code","74ab1e57":"code","05959d71":"code","d140a413":"code","ad8edc83":"code","a2de28cd":"code","90367c7c":"code","25c64310":"code","c2bbebd8":"code","97805d68":"code","63c8bdc0":"code","10ceea53":"code","9cc72b35":"code","5d35677b":"code","71317035":"code","21cbde63":"code","cd1fa53c":"markdown","3f573a08":"markdown","b3d7f7be":"markdown","d120423f":"markdown","e6923fb8":"markdown","4eb8c027":"markdown","e33070d3":"markdown","7f2c2c2f":"markdown","960ccb84":"markdown","3b54c0b0":"markdown","70b82ff9":"markdown","84204faa":"markdown","6bc10024":"markdown","83615228":"markdown","f98d3172":"markdown","c69c7cbb":"markdown"},"source":{"c3745643":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import svm\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import plot_tree\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.model_selection import cross_val_score\nsns.set()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a7c0d2b":"data_audi = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv')\ndata_audi['brand'] = 'Audi'\ndata_bmw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\ndata_bmw['brand'] = 'BMW'\ndata_ford = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')\ndata_ford['brand'] = 'Ford'\ndata_hyundi=pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/hyundi.csv')\ndata_hyundi['brand']='Hyundi'\ndata_hyundi = data_hyundi.rename(columns={'tax(\u00a3)': 'tax'})\ndata_merc =pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv')\ndata_merc['brand'] = 'Merc'\ndata_skoda = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv')\ndata_skoda['brand'] = 'Skoda'\ndata_toyota = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv')\ndata_toyota['brand']= 'Toyota'\ndata_vauxhall = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vauxhall.csv')\ndata_vauxhall['brand'] = 'Vauxhall'\ndata_vw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\ndata_vw['brand'] = 'VW'","96d0803e":"data = pd.concat([data_audi, data_bmw, data_ford, data_hyundi,\n                  data_merc, data_skoda, data_toyota, data_vauxhall, data_vw], ignore_index=True)","a3f2fd2f":"data","b913ed7c":"data.describe(include='all')","e81ed6d1":"data.info()","82515d67":"# reducing the size of the data set\n\ndata[\"model\"] = data[\"model\"].astype(\"category\")\ndata[\"transmission\"] = data[\"transmission\"].astype(\"category\")\ndata[\"fuelType\"] = data[\"fuelType\"].astype(\"category\")\ndata[\"brand\"] = data[\"brand\"].astype(\"category\")\n\n# Downcasting\ndata[[\"year\", \"price\", \"mileage\", \"tax\", \"mpg\"]] = data[[\"year\", \"price\", \"mileage\", \"tax\", \"mpg\"]].apply(pd.to_numeric, downcast=\"unsigned\")","8ef2bf9f":"data.info()","5dd0be93":"data.isnull().sum()","93ba9ce4":"corr = data.corr()\n\nsns.heatmap(corr, annot=True)","54fb6bb1":"plt.figure(figsize=(18,6))\nplt.subplot(2,5,1)\nplt.scatter(data['year'],data['price'])\nplt.title('price and year')\nplt.subplot(2,5,2)\nplt.scatter(data['engineSize'],data['price'])\nplt.title('price and engineSize')\nplt.subplot(2,5,3)\nplt.scatter(data['mileage'],data['price'])\nplt.title('price and mileage')\nplt.subplot(2,5,4)\nplt.scatter(data['mpg'],data['price'])\nplt.title('price and mpg')\nplt.subplot(2,5,5)\nplt.scatter(data['tax'],data['price'])\nplt.title('price and tax')\n\n\nplt.show()","55ea718c":"sns.distplot(data['price'])","e64ef966":"data['log_price'] = np.log(data['price']) ","a90b93bc":"plt.figure(figsize=(18,6))\nplt.subplot(2,5,1)\nplt.scatter(data['year'],data['log_price'])\nplt.title('log_price and year')\nplt.subplot(2,5,2)\nplt.scatter(data['engineSize'],data['log_price'])\nplt.title('log_price and engineSize')\nplt.subplot(2,5,3)\nplt.scatter(data['mileage'],data['log_price'])\nplt.title('log_price and mileage')\nplt.subplot(2,5,4)\nplt.scatter(data['mpg'],data['log_price'])\nplt.title('log_price and mpg')\nplt.subplot(2,5,5)\nplt.scatter(data['tax'],data['log_price'])\nplt.title('log_price and tax')\n\n\nplt.show()","c6c284c8":"plt.figure(figsize=(12,8))\ndata.drop(['price'], axis = 1).boxplot()","fe03d1b8":"# scalling\nfrom sklearn.preprocessing import StandardScaler\nfeatures = ['year', 'mileage', 'mpg', 'engineSize', 'tax']\ndummies = ['transmission', 'fuelType', 'brand']\nX = data[features]\ndata_scaler = StandardScaler()\n\ndata_scaler.fit(X)\n\ndata_trans=data_scaler.transform(X)\n\ndata_scaled = pd.DataFrame(data_trans, columns=[features])\ndata_scaled","f4aff9ea":"data_transform = pd.concat([data_scaled,data[dummies]], axis =1)\ndata_transform","3f378209":"cols = ['year','mileage','mpg','engineSize','tax','transmission','fuelType', 'brand']\n\ndata_transform.columns = cols\ndata_transform.head()","2b8b80ef":"plt.figure(figsize=(12,8))\ndata_scaled.boxplot()","65c07261":"data_dummies = pd.get_dummies(data_transform, drop_first= True)\n\ndata_dummies","92335b15":"X = data_dummies\ny= data['log_price']","67262df2":"def test_models(models, X,y, iterations = 100):\n    results = {}\n    for i in models:\n        r2_train = []\n        r2_test = []\n        for j in range(iterations):\n            x_train, x_test, y_train, y_test = train_test_split(X,y,test_size= 0.2)\n            models[i].fit(x_train,y_train).predict(x_train)\n            r2_train.append(models[i].fit(x_train,y_train).score(x_train, y_train))\n            models[i].fit(x_train,y_train).predict(x_test)\n            r2_test.append(models[i].fit(x_train,y_train).score(x_test, y_test))\n            \n            results[i] = [np.mean(r2_train), np.mean(r2_test)]\n           \n    return pd.DataFrame(results)","1a1b18db":"models = {'OLS': LinearRegression(),'Lasso': Lasso(),'Ridge': Ridge()}\n\ntest_models(models, X, y)","aa68bd7e":"\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)","74ab1e57":"boosting = GradientBoostingRegressor(n_estimators= 100, learning_rate= 0.1, max_depth=10)\nboosting.fit(x_train, y_train)\ny_pred_train = boosting.predict(x_train)\ny_pred_test =boosting.predict(x_test)","05959d71":"mse_gb_train= mean_squared_error(np.exp(y_train), np.exp(y_pred_train))\nmse_gb_test= mean_squared_error(np.exp(y_test), np.exp(y_pred_test))\nmse_abs_gb_train = mean_absolute_error(np.exp(y_train), np.exp(y_pred_train))\nmse_abs_gb_test = mean_absolute_error(np.exp(y_test), np.exp(y_pred_test))                                    \n\nrmse_gb_train = np.sqrt(mse_gb_train)\nrmse_gb_test = np.sqrt(mse_gb_test)\n\nprint(\"Train:\", mse_gb_train)\nprint(\"Test:\", mse_gb_test)\nprint(\"Train:\", mse_abs_gb_train)\nprint(\"Test:\", mse_abs_gb_test)\nprint(\"Train:\", rmse_gb_train)\nprint(\"Test:\", rmse_gb_test)","d140a413":"r2_boosting_test = boosting.score(x_test,y_test )\nr2_boosting_train = boosting.score(x_train,y_train )\nprint(r2_boosting_train)\nr2_boosting_test","ad8edc83":"scores_boosting = pd.DataFrame(data = np.round([r2_boosting_train, r2_boosting_test, mse_gb_train, mse_gb_test, rmse_gb_train, rmse_gb_test, mse_abs_gb_train, mse_abs_gb_test],2), index =['r2_train', 'r2_test', 'mse_train', 'mse_test', 'rmse_train', 'rmse_test', 'mse_abs_train', 'mse_abs_test'], columns=['scores'])\nscores_boosting","a2de28cd":"score_b= cross_val_score(boosting, X, y, cv=20, n_jobs=-1)\nscore_b","90367c7c":"random_forest_reg = RandomForestRegressor(max_depth = 20, max_features = 'auto', n_estimators = 100)\nrandom_forest_reg.fit(x_train, y_train)\n\ny_pred_train = random_forest_reg.predict(x_train)\ny_pred_test = random_forest_reg.predict(x_test)","25c64310":"r2_rfr_test = random_forest_reg.score(x_test, y_test)\nr2_rfr_train = random_forest_reg.score(x_train, y_train)\nr2_rfr_test, r2_rfr_train","c2bbebd8":"mse_rfg_train= mean_squared_error(np.exp(y_train), np.exp(y_pred_train))\nmse_rfg_test= mean_squared_error(np.exp(y_test), np.exp(y_pred_test))\nmse_abs_rfg_train = mean_absolute_error(np.exp(y_train), np.exp(y_pred_train))\nmse_abs_rfg_test = mean_absolute_error(np.exp(y_test), np.exp(y_pred_test))                                    \n\nrmse_rfg_train = np.sqrt(mse_rfg_train)\nrmse_rfg_test = np.sqrt(mse_rfg_test)\n\nprint(\"Train:\", mse_rfg_train)\nprint(\"Test:\", mse_rfg_test)\nprint(\"Train:\", mse_abs_rfg_train)\nprint(\"Test:\", mse_abs_rfg_test)\nprint(\"Train:\", rmse_rfg_train)\nprint(\"Test:\", rmse_rfg_test)","97805d68":"scores_randomforest = pd.DataFrame(data = np.round([r2_rfr_train, r2_rfr_test, mse_rfg_train, mse_rfg_test, rmse_rfg_train, rmse_rfg_test, mse_abs_rfg_train, mse_abs_rfg_test],2), index =['r2_train', 'r2_test', 'mse_train', 'mse_test', 'rmse_train', 'rmse_test', 'mse_abs_train', 'mse_abs_test'], columns=['scores'])\nscores_randomforest","63c8bdc0":"tree_regressor = DecisionTreeRegressor(max_depth=15)\ntree_regressor.fit(x_train, y_train)\ny_pred_train = tree_regressor.predict(x_train)\ny_pred_test = tree_regressor.predict(x_test)","10ceea53":"mse_dtr_train= mean_squared_error(np.exp(y_train), np.exp(y_pred_train))\nmse_dtr_test= mean_squared_error(np.exp(y_test), np.exp(y_pred_test))\nmse_abs_dtr_train = mean_absolute_error(np.exp(y_train), np.exp(y_pred_train))\nmse_abs_dtr_test = mean_absolute_error(np.exp(y_test), np.exp(y_pred_test))                                    \n\nrmse_dtr_train = np.sqrt(mse_dtr_train)\nrmse_dtr_test = np.sqrt(mse_dtr_test)\n\nprint(\"Train:\", mse_dtr_train)\nprint(\"Test:\", mse_dtr_test)\nprint(\"Train:\", mse_abs_dtr_train)\nprint(\"Test:\", mse_abs_dtr_test)\nprint(\"Train:\", rmse_dtr_train)\nprint(\"Test:\", rmse_dtr_test)","9cc72b35":"r2_dt_test = tree_regressor.score(x_test,y_test )\nr2_dt_train = tree_regressor.score(x_train,y_train )\nprint(r2_dt_train)\nr2_dt_test","5d35677b":"scores_DT = pd.DataFrame(data = np.round([r2_dt_train, r2_dt_test, mse_dtr_train, mse_dtr_test, rmse_dtr_train, rmse_dtr_test, mse_abs_dtr_train, mse_abs_dtr_test], 2), index =['r2_train', 'r2_test', 'mse_train', 'mse_test', 'rmse_train', 'rmse_test', 'mse_abs_train', 'mse_abs_test'], columns=['scores'])\nscores_DT","71317035":"models_with_scores= pd.concat([scores_DT, scores_boosting, scores_randomforest], axis =1)\n\ncols = ['DecisionTrees', 'GradiantBoosting', 'RandomForest']\n\nmodels_with_scores.columns = cols\n","21cbde63":"models_with_scores","cd1fa53c":"After log transformation of price its looks linear in most of the graphs","3f573a08":"### Train test split","b3d7f7be":"### 3.1) Plotted the graphs with numerical features vs price\n","d120423f":"Price has big tail at the end of the distribution so its better to apply log scale for price","e6923fb8":"### Merging and preprocessing","4eb8c027":"## Create the dummie variables","e33070d3":"## Scaling the inputs(features)","7f2c2c2f":"### Overview of models","960ccb84":"## correlation scores with Heatmap","3b54c0b0":"#### As per the box plot its important to do scaling","70b82ff9":"### 6.2) GradientBoostingRegressor model","84204faa":"### RandomForestRegressor Model","6bc10024":"### Testing the Linear regression models (OLS, Lasso, Ridge)\n","83615228":"### Decision Trees for Regression","f98d3172":"### 3.4)Declaring the X and y variables \n","c69c7cbb":"From the above results I choose RandomForest model for future predictions. "}}