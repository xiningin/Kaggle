{"cell_type":{"1a4a3fe0":"code","043c563a":"code","cce1b786":"code","c2b61639":"code","0c668a9a":"code","5a8ecff4":"code","d665d93f":"code","e8baffec":"code","c9af0304":"code","2c2d4325":"code","a914678b":"code","f8b6d1d7":"code","ed41d058":"code","06d2df6c":"code","f8a9cb3c":"code","f3a43ca6":"code","ee347bff":"code","8c1ed1db":"code","2eadba5a":"code","88d15431":"code","da04942e":"code","e010d547":"code","f3828938":"code","dadb08cc":"code","9cb199c5":"code","e96755a7":"code","ad45f094":"code","6114ff9f":"code","31acfbc3":"markdown","7ab91f52":"markdown","fe7c89af":"markdown","3081b7ab":"markdown","e289b549":"markdown","e7c5ffac":"markdown","b00ee2d4":"markdown","60b43d02":"markdown","71cef789":"markdown","9a6ba0ae":"markdown","582df628":"markdown","684818ef":"markdown","dfb66bf7":"markdown","f3fce09e":"markdown","23550492":"markdown","8f4846d2":"markdown","2c92012d":"markdown","636963a0":"markdown","2bfad708":"markdown","d1d06b0c":"markdown","a0aeed66":"markdown"},"source":{"1a4a3fe0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","043c563a":"dataset_train = pd.read_csv('..\/input\/stockprice-train\/Stock_Price_Train.csv')","cce1b786":"dataset_train.head()","c2b61639":"#open'\u0131 kullan\u0131caz sadece:\ntrain = dataset_train.loc[:, ['Open']].values #array'e \u00e7evirdik\ntrain","0c668a9a":"from sklearn.preprocessing import MinMaxScaler #bununla, 0-1 aras\u0131na scale ettik\nscaler = MinMaxScaler(feature_range = (0, 1))\ntrain_scaled = scaler.fit_transform(train)\ntrain_scaled","5a8ecff4":"plt.plot(train_scaled)","d665d93f":"#ilk 1-50 yi al\u0131p X_train'e, 51. data point'i de y_train'e,\n#2-51'i al\u0131p X_train'e, 52'yi y_train'e ...olacak \u015fekilde data frame i olu\u015fturuyoruz:\nX_train = []\ny_train = []\ntimesteps = 50\n\nfor i in range(timesteps, 1250):\n    X_train.append(train_scaled[i - timesteps:i, 0])\n    y_train.append(train_scaled[i, 0])\n    \nX_train, y_train = np.array(X_train), np.array(y_train)","e8baffec":"#Reshaping:\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))","c9af0304":"#import libraries and packages:\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout\n\n#Initialize RNN:\nregressor = Sequential()\n\n#Adding the first RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True, input_shape= (X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n\n#Adding the second RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True))\nregressor.add(Dropout(0.2))\n\n#Adding the third RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True))\nregressor.add(Dropout(0.2))\n\n#Adding the fourth RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\n\n#Adding the output layer\nregressor.add(Dense(units = 1))\n\n#Compile the RNN\nregressor.compile(optimizer='adam', loss='mean_squared_error')\n\n#Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs=100, batch_size=32)\n","2c2d4325":"dataset_test = pd.read_csv('..\/input\/stockprice-test\/Stock_Price_Test.csv')\ndataset_test.head()","a914678b":"real_stock_price = dataset_test.loc[:, ['Open']].values\nreal_stock_price","f8b6d1d7":"#Getting the predicted stock price\ndataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis=0)\ninputs = dataset_total[len(dataset_total)-len(dataset_test) - timesteps:].values.reshape(-1,1)\ninputs = scaler.transform(inputs) #minmax scaler\ninputs","ed41d058":"X_test = []\nfor i in range(timesteps, 70):\n    X_test.append(inputs[i-timesteps:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n#inverse_transform ile, scale edildikten sonra predict edilen de\u011ferleri ger\u00e7ek de\u011fer aral\u0131\u011f\u0131na \u00e7ekiyoruz","06d2df6c":"plt.plot(real_stock_price, color='red', label='Real Google Stock Price')\nplt.plot(predicted_stock_price, color='blue', label='Predicted Google Stock Price')\nplt.title('Google Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('Google Stock Price')\nplt.legend()\nplt.show()","f8a9cb3c":"data = pd.read_csv('..\/input\/international-airline-passengers\/international-airline-passengers.csv')\ndata.head()","f3a43ca6":"dataset = data.iloc[:, 1].values\nplt.plot(dataset)\nplt.xlabel('time')\nplt.ylabel('number of passengers (in thousands)')\nplt.title('Passengers')\nplt.show()","ee347bff":"dataset = dataset.reshape(-1,1) #(145, ) iken (145,1)e \u00e7evirdik\ndataset = dataset.astype('float32')\ndataset.shape","8c1ed1db":"scaler = MinMaxScaler(feature_range= (0,1))\ndataset = scaler.fit_transform(dataset)","2eadba5a":"train_size = int(len(dataset)*0.5)\ntest_size = len(dataset)- train_size\n\ntrain = dataset[0:train_size, :]\ntest = dataset[train_size:len(dataset), :]\n\nprint('train size: {}, test size: {}'.format(len(train), len(test)))","88d15431":"dataX = []\ndatay = []\ntimestemp = 10\n\nfor i in range(len(train)- timestemp -1):\n    a = train[i:(i+timestemp), 0]\n    dataX.append(a)\n    datay.append(train[i + timestemp, 0])\n\n    \ntrainX, trainy = np.array(dataX), np.array(datay)","da04942e":"dataX = []\ndatay = []\nfor i in range(len(test)- timestemp -1):\n    a = test[i:(i+timestemp), 0]\n    dataX.append(a)\n    datay.append(test[i + timestemp, 0])\n\n    \ntestX, testy = np.array(dataX), np.array(datay)","e010d547":"trainX.shape","f3828938":"trainX = np.reshape(trainX, (trainX.shape[0],1,  trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0],1,  testX.shape[1]))","dadb08cc":"trainX.shape","9cb199c5":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","e96755a7":"# model\nmodel = Sequential()\nmodel.add(LSTM(10, input_shape=(1, timestemp))) # 10 lstm neuron(block)\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainy, epochs=50, batch_size=1)","ad45f094":"#make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainy = scaler.inverse_transform([trainy])\ntestPredict = scaler.inverse_transform(testPredict)\ntesty = scaler.inverse_transform([testy])\n\nimport math\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainy[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testy[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","6114ff9f":"# shifting train\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[timestemp:len(trainPredict)+timestemp, :] = trainPredict\n# shifting test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(timestemp*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","31acfbc3":"<a id=\"12\"><\/a>\n**Implementing LSTM with KERAS**","7ab91f52":"<a id=\"8\"><\/a>\n**Reshape:**","fe7c89af":"<a id=\"2\"><\/a>\n**Recurrent Neural Network (RNN)**\n\n* RNN is able to remember important things about the input received, which enables them to be very precise in predicting what's coming next.\n\n* This is the reason why they are preferred algorithm for sequential data like time series, speech, text, financial data, audio, video because they can perform a much deeper understanding of a sequence and its content compared to the other algorithms.\n\n* Not only feeds output but also gives feed backs into itself. Because RNN has internal memory. How? RNN has hidden layers which have temporal loop (kendini besleyen zamansal d\u00f6ng\u00fcye sahiptir ve ge\u00e7ici belle\u011fe sahiptir). \n\n![temporal%20loop.jpg](attachment:temporal%20loop.jpg)\n\n* \u00d6rnek RNN Yap\u0131lar\u0131:\n\n    * One to Many: input image, output bunla ilgili bir c\u00fcmle\n    \n    ![one%20to%20many.jpg](attachment:one%20to%20many.jpg)\n    \n    * Many to one: input c\u00fcmle, output bu c\u00fcmleyle ilgili bir duygu \n    \n    ![many%20to%20one.jpg](attachment:many%20to%20one.jpg)\n    \n    * Many to many: mesela google translate kullanarak bir c\u00fcmle translate etmek\n    \n    ![many%20to%20many.jpg](attachment:many%20to%20many.jpg)\n    \n    \n    \n* Not: RNN short term memory'ye sahip, ancak LSTM long term memory'ye sahip olabilir. \n\n* Exploiding Gradients: Gradient'in \u00e7ok b\u00fcy\u00fck olmas\u0131 ve gereksiz yere weight'lere \u00f6nem kazand\u0131rmas\u0131 durumu.\n\n* Vanishing Gradients: Gradient'in \u00e7ok k\u00fc\u00e7\u00fck olmas\u0131 ve yava\u015f \u00f6\u011frenme durumu\n\n* Gradient: Weight'lerdeki cost'a g\u00f6re de\u011fi\u015fim","3081b7ab":"<a id=\"14\"><\/a>\n**Preprocessing Data**","e289b549":"<a id=\"6\"><\/a>\n**Feature Scaling:**","e7c5ffac":"<a id=\"1\"><\/a>\n**Sequence Models**\n\n* Sequence models plays an over time.\n\n* Speech recognition, NLP, music generation (Apple siri, Goggle's voice search...)\n\n* Sentiment classification (duygu s\u0131n\u0131fland\u0131rma). \u00d6rne\u011fin, bir c\u00fcmlenin olumlu - olumsuz olma durumunu anlama\n\n* \u00d6zetle, Sequence models time series modeldir.","b00ee2d4":"<a id=\"5\"><\/a>\n**Loading Data:**","60b43d02":"<a id=\"13\"><\/a>\n**Loading Data**","71cef789":"<a id=\"4\"><\/a>\n**Importing and Preprocessind Data**","9a6ba0ae":"* Epochs art\u0131r\u0131lbilir\n* Batch size de\u011fi\u015ftirilebilir\n* Activation functionlar de\u011fi\u015ftirilebilir\n* Layer say\u0131s\u0131 art\u0131r\u0131labilir","582df628":"<a id=\"18\"><\/a>\n**Create LSTM Model**","684818ef":"<a id=\"19\"><\/a>\n**Prediction**","dfb66bf7":"<a id=\"16\"><\/a>\n**Scaling:**","f3fce09e":"<a id=\"9\"><\/a>\n**Create RNN Model**","23550492":"<a id=\"11\"><\/a>\n**Long Short Term Memory (LSTM)**\n\n* LSTM, RNN'in bir t\u00fcr\u00fcd\u00fcr.\n\n* RNN'den farkl\u0131 olarak long term memory'den bahsedebiliriz.\n\n* LSTM architecture:\n\n    * x: Scaling of information (x=0 ise 1'den gelen bilgi Ct-1 okuna dahil olamayacak)\n    \n    * +: Adding information (+da gelen bilginin dahil olamamas\u0131 gibi bir durum s\u00f6z konusu de\u011fil, +'n\u0131n a\u015fa\u011f\u0131s\u0131ndaki oktan gelen bilgi her zaman eklenecek)\n    \n    * Sigmoid layer: Sigmoid memory'den bir\u015feyi hat\u0131rlamak i\u00e7in ya da unutmak i\u00e7in kullan\u0131l\u0131r.  1 ya da 0'd\u0131r. gate olarak adland\u0131r\u0131l\u0131r.\n    \n    * tanh: Activation function olarak kullan\u0131l\u0131r \u00e7\u00fcnk\u00fc t\u00fcrevi hemen 0'a inmez, bu da vanishing gradient(yava\u015f \u00f6\u011frenme) problemini \u00e7\u00f6zer.\n    \n    * h(t-1): Output of LSTM unit (bir \u00f6nceki layer'dan gelen output).\n    \n    * c(t-1): Memory from previous LSTM unit\n    \n    * X(t): input\n    \n    * c(t): new updated memory\n    \n    * h(t): output\n    \n    * From c(t-1) to c(t) is memory pipeline\n    \n    * Oklar vekt\u00f6rel de\u011ferler\n    \n![lstm.jpg](attachment:lstm.jpg)\n\n\n* Yap\u0131lar (g\u00f6rselde 1, 2, 3)\n    * 1-Forget Gate: Input olarak X(t) ve h(t-1) al\u0131r. Gelen bilginin unutulup unutulmayaca\u011f\u0131na karar verir.\n    \n    * 2-Input Gate: Hangi bilginin memoryde depolan\u0131p depolanmayaca\u011f\u0131na karar verir. 2 numaral\u0131 oka gelen bilgi sigmoid'den ge\u00e7erek 0 ya da 1 de\u011ferini alarak tanh ile aktivasyona giriyor. Yani, yukar\u0131daki gibi sigmoid varsa 0-1.\n    \n    * 3-Output Gate: Hangi bilginin output olup olmayaca\u011f\u0131na karar veriyor.","8f4846d2":"<a id=\"17\"><\/a>\n**Train Test Split:**","2c92012d":"<a id=\"3\"><\/a>\n**Implementing RNN with KERAS**","636963a0":"<a id=\"7\"><\/a>\n**Create Data Structure:**","2bfad708":"<a id=\"10\"><\/a>\n**Prediction and Visualization of RNN Model**","d1d06b0c":"<a id=\"15\"><\/a>\n**Reshape:**","a0aeed66":"## Content\n* [Sequence Models](#1)\n* [Recurrent Neural Network (RNN)](#2)\n    * [Implementing Recurrent Neural Network with Keras](#3)\n        * [Importing and Preprocessing Data](#4)\n            * [Loading Data](#5)\n            * [Feature Scaling](#6)\n            * [Create Data Structure](#7)\n            * [Reshape](#8)\n        * [Create RNN Model](#9)\n        * [Prediction and Visualization of RNN Model](#10)\n* [Long Short Term Memory (LSTM)](#11)\n    * [Implementing Long Short Term Memory with Keras](#12)\n        * [Loading Data](#13)\n        * [Preprocessing Data](#14)\n            * [Reshape](#15)\n            * [Scaling](#16)\n            * [Train Test Split](#17)\n        * [Create LSTM Model](#18)\n        * [Predictions and Visualising LSTM Model](#19)\n"}}