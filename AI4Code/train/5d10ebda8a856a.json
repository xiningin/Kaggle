{"cell_type":{"8fb5050f":"code","59957924":"code","61cd9f8e":"code","1e8a502c":"code","ac4b148c":"code","a66d06d5":"code","97d81457":"code","cc67e940":"code","1334141a":"code","a5061728":"code","f5f9be93":"code","8caf0c7a":"markdown","7404ca10":"markdown","6b58a3cd":"markdown","2267d765":"markdown","fb3d0662":"markdown","3ad91240":"markdown","c2d94c57":"markdown","ab267e27":"markdown","d3c14799":"markdown","3a34890f":"markdown"},"source":{"8fb5050f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/if4074-praktikum-1-cnn\/P1_dataset\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59957924":"import numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","61cd9f8e":"batch_size = 32\nimage_height = 256\nimage_width = 256\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\ndirectory='..\/input\/if4074-praktikum-1-cnn\/P1_dataset\/train',\nvalidation_split=0.2,\nsubset=\"training\",\nseed=123,\nlabels='inferred',\nlabel_mode='categorical',\ncolor_mode= 'rgb',\nbatch_size=batch_size,\nimage_size=(image_height, image_width))\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\ndirectory='..\/input\/if4074-praktikum-1-cnn\/P1_dataset\/train',\nvalidation_split=0.2,\nseed=123,\nsubset=\"validation\",\nlabels='inferred',\nlabel_mode='categorical',\ncolor_mode= 'rgb',\nbatch_size=batch_size,\nimage_size=(image_height, image_width))\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\ndirectory='..\/input\/testing-dataset\/test',\ncolor_mode= 'rgb',\nlabels='inferred',\nshuffle = False,\nbatch_size = 1,\nlabel_mode='categorical',\nimage_size=(image_height, image_width))","1e8a502c":"num_classes = 4\n\nmodel = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(image_height, image_width, 3)),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n  layers.Flatten(),\n  layers.Dense(512, activation='relu'),\n  layers.Dense(512, activation='relu'),\n  layers.Dense(num_classes)\n])\n\nmodel.summary()","ac4b148c":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","a66d06d5":"epochs=10\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","97d81457":"num_classes = 4\n\ndata_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(image_height, \n                                                              image_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)\n\nripVGGChanged = Sequential([\n  data_augmentation,\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(image_height, image_width, 3)),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(512, activation='relu'),\n  layers.Dense(512, activation='relu'),\n  layers.Dense(num_classes)\n])\n\nripVGGChanged.summary()","cc67e940":"ripVGGChanged.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","1334141a":"epochs=50\nhistory = ripVGGChanged.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","a5061728":"test_filenames = []\nfor _, _, filenames in os.walk('..\/input\/testing-dataset\/test'):\n    for files in filenames:\n        test_filenames.append(files)\n\n\n\nnum = 0\ncorrect = 0\nprediction_label = []\nclass_name = [0,1,2,3]\n\nfor batch, labels in test_ds:\n    predictions = ripVGGChanged.predict(batch)\n    score = tf.nn.softmax(predictions[0])\n    prediction_label.append(class_name[np.argmax(score)])\n    if(np.argmax(labels) == class_name[np.argmax(score)]):\n        correct+=1\n    \nprint('Accuracy on test data: ', correct\/len(test_filenames))\n\ntest_filenames = sorted(test_filenames)\n\ndf = pd.DataFrame({'id': test_filenames,\n                   'label': prediction_label})\n\n","f5f9be93":"print(df)\ndf.to_csv(r'.\/submission.csv', index=False)","8caf0c7a":"## Pengubahan Hasil Prediksi Menjadi File CSV","7404ca10":"## Peningkatan Epoch\nKami juga mencoba meningkatkan akurasi dari model kami dengan meningkatkan epoch yang kami lakukan.","6b58a3cd":"## Train Model\nBerikut adalah proses pelatihan model yang telah kami buat sebelumnya. Kami menggunakan 10 epoch dalam melatih model ini.","2267d765":"## Import Library yang Diperlukan\nBerikut adalah proses import library yang akan kami gunakan.","fb3d0662":"## Analisis Hasil Train\nDari hasil train model ini dapat dilihat bahwa terdapat perbedaan yang cukup jauh antara validation accuracy dan train accuracy. Perbedaan yang cukup jauh ini merupakan salah satu gejala dari **overfitting**. Untuk mengatasi permasalahan ini kamu melakukan modifikasi pada pemodelan di bawah.","3ad91240":"## Pembentukan Dataset\nBerikut adalah proses pembentukan dataset dari gambar yang akan digunakan pada praktikum ini. Kami menggunakan validation split 80-20 dalam membentuk training dan validation data. Selain training dan validation dataset, dataset yang akan kami gunakan dalam testing juga kami bentuk di sini.","c2d94c57":"## Prediksi Data Test\nSetelah dataset untuk testing dibentuk di awal, kami melakukan prediksinya pada bagian ini. Berdasarkan percobaan yang kami lakukan, kami mendapatkan akurasi sebesar 0.86776.","ab267e27":"## Pembentukan Model Berdasarkan Arsitektur RipVGG\nBerikut adalah proses pembentukan model berdasarkan arsitektur ripVGG yang diminta di spek.","d3c14799":"## Hasil Modifikasi\nDengan menerapkan **dropout** dan **data augmentation**, kami berhasil menyelesaikan permasalahan **overfitting** yang kami temukan. Hal ini dapat dilihat dengan kecilnya selisih antara akurasi training dan validation.","3a34890f":"## Pemodelan Ulang dengan Augmentation Layer dan Dropout Layer\nUntuk mengatasi **overfitting** kami mencoba menerapkan **data augmentation** dan **dropout**. \n\n**Data Augementation** adalah proses menghasilkan data tambahan dari data yang ada dengan cara mentransformasi data tersebut. Transformasi yang dilakukan itu seperti *flip*, rotasi, dan *zoom*.\n\n**Dropout** adalah suatu bentuk *regularization*. Cara kerjanya adalah saat **dropout** diterapkan pada suatu layer maka ia akan membuang output unit secara acak. Masukan dari **dropout** yang kami gunakan adalah sebesar 0.2, ini berarti output unit yang secara acak dibuang itu sebesar 20% pada layer tersebut."}}