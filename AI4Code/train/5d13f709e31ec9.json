{"cell_type":{"eab2dc1a":"code","b6eac5bd":"code","020474fb":"code","d83ebdf9":"code","3b2f848b":"code","227090e5":"code","83166bb3":"code","f5f08f60":"code","61286c09":"code","117a20c6":"code","18859259":"code","37475ee9":"code","61a2b525":"code","631eb35a":"code","0085d2df":"code","5826dbf7":"code","8791be3f":"code","279b8479":"code","921e3375":"code","dae58aa6":"code","c55c264c":"code","276f0a33":"code","01e89212":"code","bcdc67b9":"code","f8d902b4":"code","0e4fa56e":"code","1ab96a22":"code","f47dc10f":"code","265b181a":"code","bb657532":"code","bf1ea127":"code","971d7bc3":"code","a9d2a755":"code","30265ffd":"code","26e4ba78":"code","8a6ba673":"code","19289f12":"code","e0f3dfbb":"code","63014ac3":"code","2f9ded0d":"code","f84afe1a":"code","946fdce4":"code","7d359a80":"code","c600c93a":"code","5a108a60":"code","ca48b3ae":"code","fb8af1fe":"code","8372c213":"code","420ed1d2":"code","bafdb18b":"code","2b19c86a":"code","b055be63":"code","0a358c37":"code","47098d5d":"code","46631790":"code","33729c15":"code","79be0f92":"code","4d827488":"code","d8bc47b7":"code","119faf12":"code","d5107e7c":"code","e065ddd2":"code","473d6425":"code","c14d2bb7":"code","91af5898":"code","66cf0e14":"code","87ec7440":"code","13d687c1":"code","1d5583c4":"code","c4c5aacd":"code","d96e778a":"code","084a489d":"code","719b2f69":"code","66e6f8b4":"code","d210f016":"code","91dc7884":"code","b88dd0ed":"code","869b91dd":"code","00cc23d1":"code","a95941d9":"code","ea5b203f":"code","81c50482":"code","cb0174fd":"code","e9cdb330":"code","6ff4db07":"code","a5debb0c":"code","be28a5f0":"code","93574b83":"code","ff19367b":"code","10bbcce7":"code","a1670078":"code","c8dee5c9":"code","c751cf34":"code","21c76a45":"code","81a5b177":"code","5ddfbfd0":"markdown","24793bd9":"markdown","fee654f2":"markdown","3219e3c9":"markdown","5547b130":"markdown","18eba98a":"markdown","1ef47502":"markdown"},"source":{"eab2dc1a":"import re\nimport gc\ngc.enable()\n\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n%matplotlib inline","b6eac5bd":"# Function to reduce memory usage.  From: https:\/\/www.kaggle.com\/fabiendaniel\/detecting-malwares-with-lgbm\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","020474fb":"train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv'))\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\ntrain.head()","d83ebdf9":"train.info()","3b2f848b":"# Revenues are not uniformally distributed\ntrain['revenue'].hist(bins=25)","227090e5":"# When comparing the listed revenues with their actual values found online, \n# it's clear the values given here are not accurate.\ntrain.sort_values('revenue').head()","83166bb3":"# Budget is also skewed.\ntrain['budget'].hist(bins=25)","f5f08f60":"# $0 budget for some movies?\ntrain['budget'].describe()","61286c09":"print('Movies with 0$ Budget:', len(train[train['budget'] == 0]))\ntrain[train['budget'] == 0].head()","117a20c6":"# Create columns for year, month, and day of week\ntrain['release_date'] = pd.to_datetime(train['release_date'], infer_datetime_format=True)\ntrain['release_day'] = train['release_date'].apply(lambda t: t.day)\ntrain['release_weekday'] = train['release_date'].apply(lambda t: t.weekday())\ntrain['release_month'] = train['release_date'].apply(lambda t: t.month)\n# Year was being interpreted as future dates in some cases so I had to adjust some values\ntrain['release_year'] = train['release_date'].apply(lambda t: t.year if t.year < 2018 else t.year -100)\n\n#train.drop('release_date', inplace=True)","18859259":"train['runtime'].hist(bins=25)","37475ee9":"len(train[train['runtime'] == 0])","61a2b525":"# I'll write a function that will map the average runtime for each year to movies with 0 runtie.\nfrom collections import defaultdict\ndef map_runtime(df):\n    df['runtime'].fillna(0)\n    \n    run = df[(df['runtime'].notnull()) & (df['runtime'] != 0)]\n    year_mean = run.groupby(['release_year'])['runtime'].agg('mean')\n    d = dict(year_mean)\n    \n    for i in df[df['runtime'] == 0]:\n        df['runtime'] = df.loc[:, 'release_year'].map(d)\n    \n    return df","631eb35a":"train = map_runtime(train)\ntrain.runtime.describe()","0085d2df":"train['homepage'].head()","5826dbf7":"# For homepage, I'll change it to 0 for NaN and 1 if a page is listed.\ntrain['homepage'].fillna(0, inplace=True)\ntrain.loc[train['homepage'] != 0, 'homepage'] = 1","8791be3f":"train['poster_path'].head()","279b8479":"# For poster_path, I'll change it to 0 for NaN and 1 if a path is listed.\ntrain['poster_path'].fillna(0, inplace=True)\ntrain.loc[train['poster_path'] != 0, 'poster_path'] = 1","921e3375":"train['genres'].describe()","dae58aa6":"# For genres, I'll fill Na values with drama (most common).  Likely a better approach available.\ntrain.genres = train.genres.fillna('18')","c55c264c":"# To fill in zero budget data points, I'll try to use correlated values as predictors\nX = train[train['budget'] != 0]\nfor i in X.select_dtypes(include='number', exclude='datetime'):\n    print(i, stats.pearsonr(X.budget, X[i]))","276f0a33":"# release_year and popularity correlate most strongly with budget\ndef map_budget(df):\n    d = defaultdict()\n    #df['budget'] = df['budget'].fillna(0)\n    X = df[df['budget'] != 0]\n    \n    year_mean = pd.Series(X.groupby(['release_year'])['budget'].agg('mean'))\n    d = dict(year_mean)\n    \n    for i in df[df['budget'] == 0]:\n        df['budget'] = df.loc[:, 'release_year'].map(d)\n    \n    # In a few cases, there are only 1 or 2 movies provided from a given year and are filled with Na values\n    df.budget = df.sort_values(by='release_year').budget.fillna(method='ffill')\n    \n    return df","01e89212":"train = map_budget(train)\ntrain.budget.describe()","bcdc67b9":"train['belongs_to_collection'].head()","f8d902b4":"# belongs_to_collection NaN values can be replaced with 'none'\ntrain['belongs_to_collection'] = train['belongs_to_collection'].fillna('none')","0e4fa56e":"train['spoken_languages'].head()","1ab96a22":"train.spoken_languages.value_counts(dropna=False)","f47dc10f":"# For spoken_languages I'll fill Na values with [{'iso_639_1': 'en', 'name': 'English'}]\ntrain.spoken_languages = train.spoken_languages.fillna(\"[{'iso_639_1': 'en', 'name': 'English'}]\")","265b181a":"train['overview'].head()","bb657532":"# For overview, I'll fill Na values with 'none'\ntrain.overview = train.overview.fillna('none')","bf1ea127":"train['Keywords'].head()","971d7bc3":"# For Keywords, I'll fill Na values with 'none'\ntrain.Keywords = train.Keywords.fillna('none')","a9d2a755":"train.production_countries.describe()","30265ffd":"# For production_countries, I'll fill Na with the most common value\ntrain.production_countries = train.production_countries.fillna(\"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\")","26e4ba78":"train.production_companies.value_counts()","8a6ba673":"# Create a columns for title length\ntitle_len = []\nfor i in train['title']:\n    title_len.append(len(i.split()))\ntitle_len = pd.Series(title_len, name='title_length')\ntrain = pd.concat([train,title_len], axis=1)\n\ntrain['title_length'].describe()","19289f12":"# For genres, I'll make a new column counting the number of listed genre types\n# This will strip out all characters except for numbers, and return this as an array\ngenre_ids = []\nfor i in train['genres']:\n    i = re.findall('\\d+', i)\n    genre_ids.append(i)\ngenre_ids = pd.Series(genre_ids, name='genre_ids').astype(str)\n\n# This will count the number of genres listed for each film\nnum_genre_types = []\nfor i in genre_ids:\n    num_genre_types.append(len(i.split()))\nnum_genre_types = pd.Series(num_genre_types, name='num_genre_types').astype(int)\ntrain = pd.concat([train, genre_ids, num_genre_types], axis=1)\n\ntrain['num_genre_types'].describe()","e0f3dfbb":"# Create column for sequels\nis_sequel = []\nfor i in train['Keywords']:\n    if 'sequel' in str(i):\n        is_sequel.append(1)\n    else:\n        is_sequel.append(0)\nis_sequel = pd.Series(is_sequel, name='is_sequel')\ntrain = pd.concat([train, is_sequel], axis=1)\n\ntrain['is_sequel'].describe()","63014ac3":"keyword_words = []\nfor i in train['Keywords']:\n    i = re.findall('[a-zA-Z \\t]+', i)\n    stopwords = ['id', 'name', ' ']\n    i = [word for word in i if word not in stopwords]\n    keyword_words.append(i)\nkeyword_words = pd.Series(keyword_words, name='keyword_words').astype(str)\ntrain = pd.concat([train, keyword_words], axis=1)\n\n# This will count the number of Keywords listed for each film\nnum_keywords = []\nfor i in keyword_words:\n    num_keywords.append(len(i.split(',')))\nnum_keywords = pd.Series(num_keywords, name='num_keywords').astype(int)\ntrain = pd.concat([train, num_keywords], axis=1)\n\ntrain['num_keywords'].describe()","2f9ded0d":"# could use the numbers from the categories, sum them up, and then convert them to a category to target incode\nkeyword_ids = []\nfor i in train['Keywords']:\n    i = re.findall('[0-9]+', i)\n    keyword_ids.append(i)\nkeyword_ids = pd.Series(keyword_ids, name='keyword_ids')\ntrain = pd.concat([keyword_ids, train], axis=1)\ntrain.keyword_ids.head()","f84afe1a":"train.belongs_to_collection.head()","946fdce4":"# Extract number from belongs to collection\ncollection_id = []\nfor i in train['belongs_to_collection']:\n    i = re.findall('[0-9]+', i)\n    collection_id.append(i[:1])\ncollection_id = pd.Series(collection_id, name='collection_id').apply(lambda x: ''.join([str(i) for i in x]))\n\n# Fill in blank values with 'No Collection'\nfor i in collection_id[collection_id == ''].index:\n    collection_id.loc[i] = 'No Collection'\n\ntrain = pd.concat([train, collection_id], axis=1)\n\ntrain['collection_id'].describe()","7d359a80":"# Add column with 1 for movies in a collection and 0 if not\nis_in_collection = []\nfor i in train['collection_id']:\n    if i != 'No Collection':\n        is_in_collection.append(1)\n    else:\n        is_in_collection.append(0)\n\nis_in_collection = pd.Series(is_in_collection, name='is_in_collection')\ntrain = pd.concat([train, is_in_collection], axis=1)\n\ntrain['is_in_collection'].describe()","c600c93a":"train['production_countries'].head()","5a108a60":"# Create a column for production country (1 for US, 0 for rest of world)\n# It would be helpful if countries had different codes, but they all appear to be the same so it's difficult to work with\nUS_prod_country = []\nfor i in train['production_countries']:\n    if 'US' in str(i):\n        US_prod_country.append(1)\n    else:\n        US_prod_country.append(0)\nUS_prod_country = pd.Series(US_prod_country, name='US_prod_country')\ntrain = pd.concat([train, US_prod_country], axis=1)\n\ntrain['US_prod_country'].describe()","ca48b3ae":"# Create column for number of production countries\nnum_production_countries = []\nfor i in train['production_countries']:\n    i = re.findall('[a-zA-Z \\t]+', str(i))\n    num_production_countries.append(str(i).count('name'))\nnum_production_countries = pd.Series(num_production_countries, name='num_production_countries')\ntrain = pd.concat([train, num_production_countries], axis=1)\n\ntrain['num_production_countries'].describe()","fb8af1fe":"# Create a column for each production company name and a column for the number of companies\nproduction_company_names = []\nnum_production_companies = []\nfor i in train['production_companies']:\n    i = re.findall('[a-zA-Z \\t]+', str(i))\n    stopwords = ['id', 'name', ' ']\n    production_company_names.append([word for word in i if word not in stopwords])\n    num_production_companies.append(str(i).count('name'))\n\nproduction_company_1 = []\nproduction_company_2 = []\nproduction_company_3 = []\nproduction_company_4 = []\nproduction_company_5 = []\nproduction_company_6 = []\nproduction_company_7 = []\nproduction_company_8 = []\n\nfor i in production_company_names:\n    try:\n        production_company_1.append(i[:][0:1])\n        production_company_2.append(i[:][1:2])\n        production_company_3.append(i[:][2:3])\n        production_company_4.append(i[:][3:4])\n        production_company_5.append(i[:][4:5])\n        production_company_6.append(i[:][5:6])\n        production_company_7.append(i[:][6:7])\n        production_company_8.append(i[:][7:8])\n    except:\n        production_company_1.append('none')\n        production_company_2.append('none')\n        production_company_3.append('none')\n        production_company_4.append('none')\n        production_company_5.append('none')\n        production_company_6.append('none')\n        production_company_7.append('none')\n        production_company_8.append('none')\n\nnum_production_companies = pd.Series(num_production_companies, name='num_production_companies')\nproduction_company_1 = pd.Series(production_company_1, name='production_company_1').apply(''.join)\nfor i in production_company_1[production_company_1 == ''].index:\n    production_company_1.iloc[i] = False\nproduction_company_2 = pd.Series(production_company_2, name='production_company_2').apply(''.join)\nfor i in production_company_2[production_company_2 == ''].index:\n    production_company_2.iloc[i] = False\nproduction_company_3 = pd.Series(production_company_3, name='production_company_3').apply(''.join)\nfor i in production_company_3[production_company_3 == ''].index:\n    production_company_3.iloc[i] = False\nproduction_company_4 = pd.Series(production_company_4, name='production_company_4').apply(''.join)\nfor i in production_company_4[production_company_4 == ''].index:\n    production_company_4.iloc[i] = False\nproduction_company_5 = pd.Series(production_company_5, name='production_company_5').apply(''.join)\nfor i in production_company_5[production_company_5 == ''].index:\n    production_company_5.iloc[i] = False\nproduction_company_6 = pd.Series(production_company_6, name='production_company_6').apply(''.join)\nfor i in production_company_6[production_company_6 == ''].index:\n    production_company_6.iloc[i] = False\nproduction_company_7 = pd.Series(production_company_7, name='production_company_7').apply(''.join)\nfor i in production_company_7[production_company_7 == ''].index:\n    production_company_7.iloc[i] = False\nproduction_company_8 = pd.Series(production_company_8, name='production_company_8').apply(''.join)\nfor i in production_company_8[production_company_8 == ''].index:\n    production_company_8.iloc[i] = False\ntrain = pd.concat([train, num_production_companies, production_company_1, production_company_2,\n              production_company_3, production_company_4, production_company_5, production_company_6,\n              production_company_7, production_company_8], axis=1)\n\ntrain.production_company_8.head()","8372c213":"# Create a column for number of spoken languages\nnum_spoken_languages = []\nfor i in train['spoken_languages']:\n    a = str(i).split()\n    num_spoken_languages.append(a.count(\"'name':\"))\nnum_spoken_languages = pd.Series(num_spoken_languages, name = 'num_spoken_languages')\ntrain = pd.concat([train, num_spoken_languages], axis=1)\n\ntrain['num_spoken_languages'].describe()","420ed1d2":"# Create column for release status\nstatus_is_released = []\nfor i in train['status']:\n    if i == 'Released':\n        status_is_released.append(1)\n    else:\n        status_is_released.append(0)\nstatus_is_released = pd.Series(status_is_released, name = 'status_is_released')\ntrain = pd.concat([train, status_is_released], axis=1)\ntrain['status_is_released'].describe()","bafdb18b":"def data_processing(df):\n    # Create columns for year, month, and day of week\n    df['release_date'] = df['release_date'].fillna(method='ffill')\n    df['release_date'] = pd.to_datetime(df['release_date'], infer_datetime_format=True)\n    df['release_day'] = df['release_date'].apply(lambda t: t.day)\n    df['release_weekday'] = df['release_date'].apply(lambda t: t.weekday())\n    df['release_month'] = df['release_date'].apply(lambda t: t.month)\n    # Year was being interpreted as future dates in some cases so I had to adjust some values\n    df['release_year'] = df['release_date'].apply(lambda t: t.year if t.year < 2018 else t.year -100)\n    \n    # Function that will map the average runtime for each year to movies with 0 runtie.\n    def map_runtime(df):\n        df['runtime'].fillna(0)\n    \n        run = df[(df['runtime'].notnull()) & (df['runtime'] != 0)]\n        year_mean = run.groupby(['release_year'])['runtime'].agg('mean')\n        d = dict(year_mean)\n    \n        for i in df[df['runtime'] == 0]:\n            df['runtime'] = df.loc[:, 'release_year'].map(d)\n        return df\n    df = map_runtime(df)\n    \n    # For homepage, I'll change it to 0 for NaN and 1 if a page is listed.\n    df['homepage'].fillna(0, inplace=True)\n    df.loc[df['homepage'] != 0, 'homepage'] = 1\n    \n    # For poster_path, I'll change it to 0 for NaN and 1 if a path is listed.\n    df['poster_path'].fillna(0, inplace=True)\n    df.loc[df['poster_path'] != 0, 'poster_path'] = 1\n    \n    # release_year correlates strongly with budget, so I'll use that to estimate the null values\n    def map_budget(df):\n        d = defaultdict()\n        X = df[df['budget'] != 0]\n        year_mean = pd.Series(X.groupby(['release_year'])['budget'].agg('mean'))\n        d = dict(year_mean)\n    \n        for i in df[df['budget'] == 0]:\n            df['budget'] = df.loc[:, 'release_year'].map(d)\n    \n        # In a few cases, there are only 1 or 2 movies provided from a given year and are filled with Na values\n        df.budget = df.sort_values(by='release_year').budget.fillna(method='ffill')\n        return df\n    df = map_budget(df)\n    \n    # Fill remaining Na values\n    df['belongs_to_collection'] = df['belongs_to_collection'].fillna('none')\n    df.spoken_languages = df.spoken_languages.fillna(\"[{'iso_639_1': 'en', 'name': 'English'}]\")\n    df.overview = df.overview.fillna('none')\n    df.Keywords = df.Keywords.fillna('none')\n    df.production_countries = df.production_countries.fillna(\n        \"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\")\n    df.genres = df.genres.fillna('18')\n    \n    ############ Feature Engineering ############\n    \n    # Create a columns for title length\n    title_len = []\n    for i in df['title']:\n        title_len.append(len(str(i).split()))\n    title_len = pd.Series(title_len, name='title_length')\n    df = pd.concat([df, title_len], axis=1)\n    \n    # Create columns for genres id's and for number of genres listed\n    genre_id = []\n    num_genre_types = []\n    for i in df['genres']:\n        i = re.findall('\\d+', str(i))\n        genre_id.append(i)\n    genre_id = pd.Series(genre_id, name='genre_id') #.apply(lambda x: ''.join([str(i) for i in x]))\n    \n    genre_id_1 = []\n    genre_id_2 = []\n    genre_id_3 = []\n    genre_id_4 = []\n    genre_id_5 = []\n    genre_id_6 = []\n    genre_id_7 = []\n\n    for i in genre_id:\n        try:\n            genre_id_1.append(i[:][0:1])\n            genre_id_2.append(i[:][1:2])\n            genre_id_3.append(i[:][2:3])\n            genre_id_4.append(i[:][3:4])\n            genre_id_5.append(i[:][4:5])\n            genre_id_6.append(i[:][5:6])\n            genre_id_7.append(i[:][6:7])\n        except:\n            genre_id_1.append('none')\n            genre_id_2.append('none')\n            genre_id_3.append('none')\n            genre_id_4.append('none')\n            genre_id_5.append('none')\n            genre_id_6.append('none')\n            genre_id_7.append('none')\n            \n    genre_id_1 = pd.Series(genre_id_1, name='genre_id_1').apply(''.join)\n    for i in genre_id_1[genre_id_1 == ''].index:\n        genre_id_1.iloc[i] = 'none'\n    genre_id_2 = pd.Series(genre_id_2, name='genre_id_2').apply(''.join)\n    for i in genre_id_2[genre_id_2 == ''].index:\n        genre_id_2.iloc[i] = 'none'\n    genre_id_3 = pd.Series(genre_id_3, name='genre_id_3').apply(''.join)\n    for i in genre_id_3[genre_id_3 == ''].index:\n        genre_id_3.iloc[i] = 'none'\n    genre_id_4 = pd.Series(genre_id_4, name='genre_id_4').apply(''.join)\n    for i in genre_id_4[genre_id_4 == ''].index:\n        genre_id_4.iloc[i] = 'none'\n    genre_id_5 = pd.Series(genre_id_5, name='genre_id_5').apply(''.join)\n    for i in genre_id_5[genre_id_5 == ''].index:\n        genre_id_5.iloc[i] = 'none'\n    genre_id_6 = pd.Series(genre_id_6, name='genre_id_6').apply(''.join)\n    for i in genre_id_6[genre_id_6 == ''].index:\n        genre_id_6.iloc[i] = 'none'\n    genre_id_7 = pd.Series(genre_id_7, name='genre_id_7').apply(''.join)\n    for i in genre_id_7[genre_id_7 == ''].index:\n        genre_id_7.iloc[i] = 'none'\n    \n    for i in genre_id.astype(str):\n        num_genre_types.append(len(i.split(',')))\n    num_genre_types = pd.Series(num_genre_types, name='num_genre_types').astype(int)\n    df = pd.concat([df, genre_id_1, genre_id_2, genre_id_3, genre_id_4, genre_id_5, \n                    genre_id_6, genre_id_7, num_genre_types], axis=1)\n    \n    # Create column for sequels\n    is_sequel = []\n    for i in df['Keywords']:\n        if 'sequel' in str(i):\n            is_sequel.append(1)\n        else:\n            is_sequel.append(0)\n    is_sequel = pd.Series(is_sequel, name='is_sequel')\n    df = pd.concat([df, is_sequel], axis=1)\n    \n    keyword_words = []\n    for i in df['Keywords']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        stopwords = ['id', 'name', ' ']\n        i = [word for word in i if word not in stopwords]\n        keyword_words.append(i)\n    keyword_words = pd.Series(keyword_words, name='keyword_words')\n    df = pd.concat([df, keyword_words], axis=1)\n\n    # This will count the number of Keywords listed for each film\n    num_keywords = []\n    for i in keyword_words:\n        num_keywords.append(len(str(i).split(',')))\n    num_keywords = pd.Series(num_keywords, name='num_keywords').astype(int)\n    df = pd.concat([df, num_keywords], axis=1)\n    \n    # Create column for Keyword Id numbers\n    keyword_ids = []\n    for i in df['Keywords']:\n        i = re.findall('[0-9]+', str(i))\n        keyword_ids.append(i)\n    keyword_ids = pd.Series(keyword_ids, name='keyword_ids')\n    #df = pd.concat([keyword_ids, df], axis=1)\n    \n    # Extract number from belongs to collection\n    collection_id = []\n    for i in df['belongs_to_collection']:\n        i = re.findall('[0-9]+', str(i))\n        collection_id.append(i[:1])\n    collection_id= pd.Series(collection_id, name='collection_id').apply(lambda x: ''.join([str(i) for i in x]))\n\n    # Fill in blank values with 'No Collection'\n    for i in collection_id[collection_id == ''].index:\n        collection_id.loc[i] = 'no collection'\n    collection_id = collection_id\n    df = pd.concat([df, collection_id], axis=1)\n    \n    # Add column with 1 for movies in a collection and 0 if not\n    is_in_collection = []\n    for i in df['collection_id']:\n        if i != 'no collection':\n            is_in_collection.append(1)\n        else:\n            is_in_collection.append(0)\n    is_in_collection = pd.Series(is_in_collection, name='is_in_collection').astype(int)\n    df = pd.concat([is_in_collection, df], axis=1)\n    \n    # Create a column for production country (1 for US, 0 for rest of world)\n    # It would be helpful if countries had different codes, but they all appear to be the same so it's difficult to work with\n    US_prod_country = []\n    for i in df['production_countries']:\n        if 'US' in str(i):\n            US_prod_country.append(1)\n        else:\n            US_prod_country.append(0)\n    US_prod_country = pd.Series(US_prod_country, name='US_prod_country')\n    df = pd.concat([df, US_prod_country], axis=1)\n    \n    # Create column for number of production countries\n    num_prod_countries = []\n    for i in df['production_countries']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        num_prod_countries.append(str(i).count('name'))\n    num_prod_countries = pd.Series(num_prod_countries, name='num_production_countries')\n    df = pd.concat([df, num_prod_countries], axis=1)\n    \n    # Create a column for each production company name and a column for the number of companies\n    production_company_names = []\n    num_production_companies = []\n    for i in df['production_companies']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        stopwords = ['id', 'name', ' ']\n        production_company_names.append([word for word in i if word not in stopwords])\n        num_production_companies.append(str(i).count('name'))\n\n    production_company_1 = []\n    production_company_2 = []\n    production_company_3 = []\n    production_company_4 = []\n    production_company_5 = []\n    production_company_6 = []\n    production_company_7 = []\n    production_company_8 = []\n\n    for i in production_company_names:\n        try:\n            production_company_1.append(i[:][0:1])\n            production_company_2.append(i[:][1:2])\n            production_company_3.append(i[:][2:3])\n            production_company_4.append(i[:][3:4])\n            production_company_5.append(i[:][4:5])\n            production_company_6.append(i[:][5:6])\n            production_company_7.append(i[:][6:7])\n            production_company_8.append(i[:][7:8])\n        except:\n            production_company_1.append('none')\n            production_company_2.append('none')\n            production_company_3.append('none')\n            production_company_4.append('none')\n            production_company_5.append('none')\n            production_company_6.append('none')\n            production_company_7.append('none')\n            production_company_8.append('none')\n\n    num_production_companies = pd.Series(num_production_companies, name='num_production_companies')\n    production_company_1 = pd.Series(production_company_1, name='production_company_1').apply(''.join)\n    for i in production_company_1[production_company_1 == ''].index:\n        production_company_1.iloc[i] = 'none'\n    production_company_2 = pd.Series(production_company_2, name='production_company_2').apply(''.join)\n    for i in production_company_2[production_company_2 == ''].index:\n        production_company_2.iloc[i] = 'none'\n    production_company_3 = pd.Series(production_company_3, name='production_company_3').apply(''.join)\n    for i in production_company_3[production_company_3 == ''].index:\n        production_company_3.iloc[i] = 'none'\n    production_company_4 = pd.Series(production_company_4, name='production_company_4').apply(''.join)\n    for i in production_company_4[production_company_4 == ''].index:\n        production_company_4.iloc[i] = 'none'\n    production_company_5 = pd.Series(production_company_5, name='production_company_5').apply(''.join)\n    for i in production_company_5[production_company_5 == ''].index:\n        production_company_5.iloc[i] = 'none'\n    production_company_6 = pd.Series(production_company_6, name='production_company_6').apply(''.join)\n    for i in production_company_6[production_company_6 == ''].index:\n        production_company_6.iloc[i] = 'none'\n    production_company_7 = pd.Series(production_company_7, name='production_company_7').apply(''.join)\n    for i in production_company_7[production_company_7 == ''].index:\n        production_company_7.iloc[i] = 'none'\n    production_company_8 = pd.Series(production_company_8, name='production_company_8').apply(''.join)\n    for i in production_company_8[production_company_8 == ''].index:\n        production_company_8.iloc[i] = 'none'\n    df = pd.concat([df, num_production_companies, production_company_1, production_company_2,\n              production_company_3, production_company_4, production_company_5, production_company_6,\n              production_company_7, production_company_8], axis=1)\n    \n    # Create a column for number of spoken languages\n    num_spoken_languages=[]\n    for i in df['spoken_languages']:\n        a = str(i).split()\n        num_spoken_languages.append(a.count(\"'name':\"))\n    num_spoken_languages = pd.Series(num_spoken_languages, name = 'num_spoken_languages')\n    df = pd.concat([df, num_spoken_languages], axis=1)\n        \n    # Create column for release status\n    status_is_released = []\n    for i in df['status']:\n        if i == 'Released':\n            status_is_released.append(1)\n        else:\n            status_is_released.append(0)\n    status_is_released = pd.Series(status_is_released, name = 'status_is_released')\n    df = pd.concat([df, status_is_released], axis=1)\n    \n    # Drop columns that have been engineered\n    df = df.drop(['belongs_to_collection', 'genres', 'Keywords', 'belongs_to_collection', 'homepage', 'imdb_id', \n                 'original_title', 'overview', 'poster_path', 'production_companies', 'production_countries',\n                 'release_date', 'spoken_languages', 'status', 'tagline', 'title', 'cast', 'crew'], axis=1)\n    # Drop 'keyword_words' column for now.  Can work with it later.\n    df = df.drop(['keyword_words'], axis=1)\n    return reduce_mem_usage(df)","2b19c86a":"# Reload the data fresh and apply the processing function\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ntrain = data_processing(train)\ntest = data_processing(test)","b055be63":"# There are 13 object columns that will need to be converted to numeric\ntrain.info()","0a358c37":"def train_target_encoded_year(df, cols):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the average revenue for each unique value from a given year.\"\"\"\n    for i in cols:\n        d = df.groupby(['release_year', i]).agg({'revenue':'mean'})\n        df = df.set_index(['release_year', i], drop=False)\n        df[i] = d.revenue\n        df = df.reset_index(drop=True)\n    return df","47098d5d":"def test_target_encoded_year(df_train, df_test, cols):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the unique average revenue per year generated from the training dataframe.\"\"\"\n    for i in cols:\n        d = df_train.groupby(['release_year', i]).agg({'revenue':'mean'})\n        df_test = df_test.set_index(['release_year', i], drop=False)\n        df_test[i] = d.revenue\n        df_test = df_test.reset_index(drop=True)\n    return df_test","46631790":"def target_encode(df, target_feature, m = 300): \n    d = defaultdict()\n    target_mean = df[target_feature].mean()\n    \n    # Map values and create dictionary   \n    for cat_feature in df.select_dtypes(include='category'):\n        group_target_mean = df.groupby([cat_feature])[target_feature].agg('mean')\n        group_target_count = df.groupby([cat_feature])[target_feature].agg('count')\n        smooth = (group_target_count * group_target_mean + m * target_mean) \/ (group_target_count + m)\n        k = pd.Series(df[cat_feature])\n        v = df[cat_feature].map(smooth)\n        d[cat_feature] = dict(zip(k, v))\n        df[cat_feature] = df[cat_feature].map(smooth)\n        \n    return df, d\n\nfor i in df_test[df_test.isnull()]: #df_test[df_test.isnull()].index\n    if i['release_year'] in d: # df_test.iloc[i]['release_year'] in d:\n        X[i] = X[i].map(d[i]) # df_test[]","33729c15":"def test_target_encoded_year(df_train, df_test):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the unique average revenue per year generated from the training dataframe.\"\"\"\n    cols = df_test.select_dtypes(include='object').columns\n    for col in cols:\n        d = df_train.groupby(['release_year', col]).agg({'revenue':'mean'})\n        df_test = df_test.set_index(['release_year', col], drop=False)\n        df_test[col] = d.revenue\n        df_test = df_test.reset_index(drop=True)\n        \n    # There are a numerous missing values in the test set after processing so I'll fill them with the yearly avg.\n    for col in cols:\n        #d = defaultdict()\n        X = df_test[df_test[col].notnull()]\n        year_mean = pd.Series(X.groupby(['release_year'])[col].agg('mean'))\n        d = dict(year_mean)\n    \n        for i in df_test[df_test['budget'].isnull()]:\n            df_test[col] = df_test.loc[:, 'release_year'].map(d)\n    \n    return reduce_mem_usage(df_test)","79be0f92":"# The numeric columns look okay, but budget may need normalization as the st. dev is quite large\ntrain.describe()","4d827488":"# Budget normalization - Didn't improve model accuracy for linear regression (remained the same)\n#train.budget = (train.budget - train.budget.mean()) \/ (train.budget.max() - train.budget.min())\n#train.head()","d8bc47b7":"from category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder","119faf12":"# Make complete list of genre ids\ngenre_ids = train['genre_id_1']\nfor i in train.loc[:, 'genre_id_2': 'genre_id_7'].columns:\n    genre_ids = pd.concat([genre_ids, train[i]], axis=0)\n\nle = LabelEncoder()\nlab_enc = le.fit_transform(genre_ids)\ngenre_ids_dict = dict(zip(genre_ids, lab_enc))\n\n# Map genre_ids_dict to genre_id columns\nfor i in train.loc[:, 'genre_id_1': 'genre_id_7'].columns:\n    train[i] = train[i].map(genre_ids_dict)","d5107e7c":"train.loc[:, 'production_company_2': 'production_company_8'].head()","e065ddd2":"# Make complete list of production companies\nprod_companies = train['production_company_1']\nfor i in train.loc[:, 'production_company_2': 'production_company_8'].columns:\n    prod_companies = pd.concat([prod_companies, train[i]], axis=0)\n\nle = LabelEncoder()\nlab_enc = le.fit_transform(prod_companies)\nprod_companies_dict = dict(zip(prod_companies, lab_enc))\n\n# Map genre_ids_dict to genre_id columns\nfor i in train.loc[:, 'production_company_1': 'production_company_8'].columns:\n    train[i] = train[i].map(prod_companies_dict)","473d6425":"le = LabelEncoder()\ntrain['collection_id'] = le.fit_transform(train['collection_id'])\ntrain['original_language'] = le.fit_transform(train['original_language'])\ntrain.info()","c14d2bb7":"from sklearn.preprocessing import LabelEncoder\ndef cat_encode(df):\n    le = LabelEncoder()\n    \n    # Make complete list of genre ids\n    genre_ids = df['genre_id_1']\n    for i in df.loc[:, 'genre_id_2': 'genre_id_7'].columns:\n        genre_ids = pd.concat([genre_ids, df[i]], axis=0)\n\n    lab_enc_genres = le.fit_transform(genre_ids)\n    genre_ids_dict = dict(zip(genre_ids, lab_enc_genres))\n\n    # Map genre_ids_dict to genre_id columns\n    for i in df.loc[:, 'genre_id_1': 'genre_id_7'].columns:\n        df[i] = df[i].map(genre_ids_dict)\n\n    # Make complete list of production companies\n    prod_companies = df['production_company_1']\n    for i in df.loc[:, 'production_company_2': 'production_company_8'].columns:\n        prod_companies = pd.concat([prod_companies, df[i]], axis=0)\n\n    lab_enc_comp = le.fit_transform(prod_companies)\n    prod_companies_dict = dict(zip(prod_companies, lab_enc_comp))\n\n    # Map genre_ids_dict to genre_id columns\n    for i in df.loc[:, 'production_company_1': 'production_company_8'].columns:\n        df[i] = df[i].map(prod_companies_dict)\n        \n    df['collection_id'] = le.fit_transform(df['collection_id'])\n    df['original_language'] = le.fit_transform(df['original_language'])\n    \n    return reduce_mem_usage(df)","91af5898":"train = cat_encode(train)\ntest = cat_encode(test)\ntrain.info()","66cf0e14":"# Get an idea of what correlates most strongly with revenue\nfor i in train.columns:\n    print(i, stats.pearsonr(train[i], train['revenue']))","87ec7440":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","13d687c1":"X = train.drop(['id', 'revenue'], axis=1)\ny = train['revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","1d5583c4":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)","c4c5aacd":"sns.distplot((y_test-pred),bins=50)","d96e778a":"def rmsle(y_true, y_pred):\n    return 'rmsle', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False","084a489d":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint('RMSLE:', rmsle(y_test, pred))","719b2f69":"from lightgbm import LGBMRegressor\nlr = LGBMRegressor(boosting_type='dart', random_state=101)","66e6f8b4":"from sklearn.model_selection import GridSearchCV,StratifiedKFold","d210f016":"# grid_1\nparams_1 = {'num_leaves': [20, 40, 60, 80, 100], #20 is best\n          'max_depth': [-1, 2, 4, 6, 8], # -1 is best\n          'min_data_in_leaf': [20, 50, 100, 200], #20 is best\n          #'learning_rate': [0.05, 0.1, 0.15, 0.2],\n          #'n_estimators': [100, 500, 1000],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          #'subsample': [0.5, 0.75, 1.0],\n          'subsample_freq': [0],\n          #'colsample_bytree': [0.5, 0.75, 1.0],\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","91dc7884":"grid_1 = GridSearchCV(lr, param_grid=params_1, scoring='neg_mean_squared_error', cv=5)\ngrid_1.fit(X_train, y_train)","b88dd0ed":"print(grid_1.best_params_)\nprint(grid_1.best_score_)\nprint(grid_1.best_estimator_)","869b91dd":"# grid_2\nparams_2 = {'num_leaves': [10, 15, 20], # 20 is best\n          'max_depth': [-1],\n          'min_data_in_leaf': [10, 15, 20], # 20 is best\n          #'learning_rate': [0.05, 0.1, 0.15, 0.2],\n          #'n_estimators': [100, 500, 1000],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          #'subsample': [0.5, 0.75, 1.0],\n          'subsample_freq': [0],\n          #'colsample_bytree': [0.5, 0.75, 1.0],\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","00cc23d1":"grid_2 = GridSearchCV(lr, param_grid=params_2, scoring='neg_mean_squared_error', cv=5)\ngrid_2.fit(X_train, y_train)","a95941d9":"print(grid_2.best_params_)\nprint(grid_2.best_score_)\nprint(grid_2.best_estimator_)","ea5b203f":"# grid_3\nparams_3 = {'num_leaves': [20],\n          'max_depth': [-1],\n          'min_data_in_leaf': [20],\n          'learning_rate': [0.05, 0.1, 0.15, 0.2], # 0.2 is best\n          'n_estimators': [100, 250, 500, 1000], # 500 is best\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          #'subsample': [0.5, 0.75, 1.0],\n          'subsample_freq': [0],\n          #'colsample_bytree': [0.5, 0.75, 1.0],\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","81c50482":"grid_3 = GridSearchCV(lr, param_grid=params_3, scoring='neg_mean_squared_error', cv=5)\ngrid_3.fit(X_train, y_train)","cb0174fd":"print(grid_3.best_params_)\nprint(grid_3.best_score_)\nprint(grid_3.best_estimator_)","e9cdb330":"# grid_4\nparams_4 = {'num_leaves': [20],\n          'max_depth': [-1],\n          'min_data_in_leaf': [20],\n          'learning_rate': [0.2], \n          'n_estimators': [500],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          'subsample': [0.1, 0.25, 0.5, 0.75, 1.0], # 0.1 is best\n          'subsample_freq': [0],\n          'colsample_bytree': [0.1, 0.25, 0.5, 0.75, 1.0], # 0.75 is best\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","6ff4db07":"grid_4 = GridSearchCV(lr, param_grid=params_4, scoring='neg_mean_squared_error', cv=5)\ngrid_4.fit(X_train, y_train)","a5debb0c":"print(grid_4.best_params_)\nprint(grid_4.best_score_)\nprint(grid_4.best_estimator_)","be28a5f0":"# grid_5\nparams_5 = {'num_leaves': [20],\n          'max_depth': [-1],\n          'min_data_in_leaf': [20],\n          'learning_rate': [0.2], \n          'n_estimators': [500],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          'subsample': [0.1],\n          'subsample_freq': [0],\n          'colsample_bytree': [0.75],\n          'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1], # 0 is best\n          'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1], # 0 is best\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","93574b83":"grid_5 = GridSearchCV(lr, param_grid=params_4, scoring='neg_mean_squared_error', cv=5)\ngrid_5.fit(X_train, y_train)","ff19367b":"print(grid_5.best_params_)\nprint(grid_5.best_score_)\nprint(grid_5.best_estimator_)","10bbcce7":"lr = LGBMRegressor(boosting_type='dart',\n                   num_leaves=20,\n                   max_depth=-1,\n                   min_data_in_leaf=20, \n                   learning_rate=0.2,\n                   n_estimators=500,\n                   subsample_for_bin=200000,\n                   #objective='regression',\n                   class_weight=None,\n                   min_split_gain=0.0,\n                   min_child_weight=0.001,\n                   subsample=0.1,\n                   subsample_freq=0,\n                   colsample_bytree=0.75,\n                   reg_alpha=0.0,\n                   reg_lambda=0.0,\n                   random_state=101,\n                   n_jobs=-1)","a1670078":"lr.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric=rmsle,\n        early_stopping_rounds=500)","c8dee5c9":"pred = lr.predict(X_test, num_iteration=lr.best_iteration_)","c751cf34":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint('RMSLE:', rmsle(y_test, pred))","21c76a45":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['revenue'] = lr.predict(test.drop('id', axis=1), num_iteration=lr.best_iteration_)","81a5b177":"submission.to_csv('TMDB_test_predictions.csv', index=False)","5ddfbfd0":"## EDA\nIt's likely features like budget, popularity, and release date will correlate strongly with revenue.  By contrast, features like poster path might not be helpful without extensive analysis.","24793bd9":"#### Model Building and Parameter Tuning","fee654f2":"## Feature Engineering","3219e3c9":"I think it's useful to first use a basic linear regression model.  We can make a more complex model later.","5547b130":"I'll come back to budget and update the values using a linear regression approach.  But it will be helpful to have as much information as possible for other features like runtime as this might affect the total budget.","18eba98a":"I'll label encode the category columns using sklearn.","1ef47502":"#### Dealing with categorical columns"}}