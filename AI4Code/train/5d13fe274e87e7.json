{"cell_type":{"135a496f":"code","e563087c":"code","821f05bf":"code","86d55c9c":"code","7b955f96":"code","a3d4d36c":"code","8255869a":"code","3f2e36c4":"code","e0cf494c":"code","ac0d0f7b":"code","dabc1df7":"code","56b10b53":"code","3b938730":"code","a726c9f7":"code","182ede61":"code","a3a25eda":"code","cafed20d":"code","da37b451":"code","ad14f647":"code","19aa89dc":"code","2e08895d":"code","8bd1aaa3":"code","0f2d797e":"code","aa27c338":"code","d2f99b72":"code","1550db10":"code","4467da02":"code","27e0dec7":"code","5e7f7309":"code","482b3c70":"code","9bb1ad47":"code","816eeafc":"code","b56a76f1":"code","d6db87bd":"code","62da569e":"code","549d7e09":"code","9975e36f":"code","47a0c266":"code","da441da7":"code","6d21df96":"code","dfc3604f":"code","f4387a2b":"code","172027e5":"code","78b75b2b":"code","347f10ec":"code","dfc312f3":"code","c229889e":"code","c9a90f9a":"code","6b9c0580":"code","d33350be":"code","cbe257ef":"code","c138fbc6":"code","22d07b2f":"code","be64568e":"code","f0b12182":"code","f72a91a8":"code","07646703":"code","97c4d029":"code","f1c353e9":"code","2dbbe640":"code","b9750d99":"markdown","baf30fd3":"markdown","0755ca42":"markdown","f46269ba":"markdown","f006a486":"markdown","7f59e04d":"markdown","79b52141":"markdown","b060180c":"markdown","9b85e535":"markdown","01772c79":"markdown","7a42ba5f":"markdown","bb0227e2":"markdown","5cbc6bb0":"markdown","f53b2492":"markdown","43d1a449":"markdown","927e15ba":"markdown","1a731749":"markdown","b4f6c724":"markdown","421cd124":"markdown","3014c689":"markdown","0786c7d6":"markdown","11e85eb9":"markdown","0f7ac19a":"markdown","875a2aa0":"markdown","4102940d":"markdown","d218bab6":"markdown","f3d5cf3b":"markdown","5a9ab64f":"markdown","460e46b2":"markdown","ffdeff99":"markdown","fe90edac":"markdown","1ccc2585":"markdown","3eecfb53":"markdown","6c1bbe36":"markdown","9d933fc1":"markdown","0f4bb60b":"markdown","7994c4e0":"markdown","8e16e5de":"markdown","af37fccf":"markdown","b172f0b8":"markdown","e54c7f57":"markdown","143ac2b2":"markdown","a30cf39b":"markdown","e9341bc4":"markdown","38d82f43":"markdown","2dde91f6":"markdown","bcae536d":"markdown","33ac29be":"markdown","959ba146":"markdown","a8d73398":"markdown","7977baad":"markdown","81d58cd1":"markdown","85003633":"markdown","653ba32d":"markdown","2bddc4fa":"markdown","5db15333":"markdown","d8ff2ddb":"markdown","7e5f81a9":"markdown","9bc26525":"markdown","88643a06":"markdown","8847d7ca":"markdown"},"source":{"135a496f":"# import all important libararies\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pandas import Series\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","e563087c":"# load the data\ntrain = pd.read_csv(\"..\/input\/time-series-analysis\/Train_SU63ISt.csv\")\ntest = pd.read_csv(\"..\/input\/time-series-analysis\/Test_0qrQsBZ.csv\")","821f05bf":"# make a copy of train and test data\ntrain_original = train.copy()\ntest_original = test.copy()","86d55c9c":"train.columns,test.columns","7b955f96":"# Check data data types\ntrain.dtypes,test.dtypes","a3d4d36c":"# Shape of the data\ntrain.shape, test.shape","8255869a":"# Change the datatype to datetime\ntrain['Datetime'] = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M')\ntest['Datetime'] = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M')\ntrain_original['Datetime'] = pd.to_datetime(train_original.Datetime,format='%d-%m-%Y %H:%M')\ntest_original['Datetime'] = pd.to_datetime(test_original.Datetime,format='%d-%m-%Y %H:%M')","3f2e36c4":"for i in (train,test,train_original,test_original):\n    i['year']=i.Datetime.dt.year\n    i['month']=i.Datetime.dt.month\n    i['day']=i.Datetime.dt.day\n    i['Hour']=i.Datetime.dt.hour","e0cf494c":"train['day of week'] = train['Datetime'].dt.dayofweek\ntemp = train['Datetime']","ac0d0f7b":"# Let\u2019s assign 1 if the day of week is a weekend and 0 if the day of week in not a weekend.\ndef applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0","dabc1df7":"temp2 = train['Datetime'].apply(applyer)\ntrain['Weekend'] = temp2","56b10b53":"# Let\u2019s look at the time series.\ntrain.index = train['Datetime'] # indexing the Datetime to get the time period on the x-axis. \ndf=train.drop('ID',1) # drop ID variable to get only the Datetime on x-axis.\nts = df['Count']\nplt.figure(figsize=(16,8)) \nplt.plot(ts, label='Passenger Count') \nplt.title('Time Series') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best')","3b938730":"# Validate hypothesis\n# first hypothesis was traffic will increase as the years pass\ntrain.groupby('year')['Count'].mean().plot.bar()","a726c9f7":"# second hypothesis was about increase in traffic from May to October\ntrain.groupby('month')['Count'].mean().plot.bar()","182ede61":"temp = train.groupby(['year','month'])['Count'].mean()\ntemp.plot(figsize = (15,5),title = \"Passenger Count(Monthwise)\",fontsize=14)","a3a25eda":"# Let\u2019s look at the daily mean of passenger count.\ntrain.groupby('day')['Count'].mean().plot.bar()","cafed20d":"#  let\u2019s see the mean of hourly passenger count.\ntrain.groupby('Hour')['Count'].mean().plot.bar()","da37b451":"# Let\u2019s try to validate our hypothesis in which we assumed that the traffic will be more on weekdays.\ntrain.groupby('Weekend')['Count'].mean().plot.bar()","ad14f647":"# Note - 0 is the starting of the week, i.e., 0 is Monday and 6 is Sunday.\ntrain.groupby('day of week')['Count'].mean().plot.bar()","19aa89dc":"# Drop ID, as it has no influence\ntrain=train.drop('ID',1)","2e08895d":"train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M')\ntrain.index = train.Timestamp","8bd1aaa3":"# Hourly time series \nhourly = train.resample('H').mean()\n\n# Converting to daily mean \ndaily = train.resample('D').mean() \n\n# Converting to weekly mean \nweekly = train.resample('W').mean() \n\n# Converting to monthly mean \nmonthly = train.resample('M').mean()","0f2d797e":"# Let\u2019s look at the hourly, daily, weekly and monthly time series.\n\nfig, axs = plt.subplots(4,1) \nhourly.Count.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0]) \ndaily.Count.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1]) \nweekly.Count.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2]) \nmonthly.Count.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3]) \n\nplt.show()","aa27c338":"test.Timestamp = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M') \ntest.index = test.Timestamp\n\n# Converting to daily mean \ntest = test.resample('D').mean()","d2f99b72":"train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \n\n# Converting to daily mean \ntrain = train.resample('D').mean()","1550db10":"Train=train.loc['2012-08-25':'2014-06-24'] \nvalid=train.loc['2014-06-25':'2014-09-25']","4467da02":"# how the train and validaiton part has been divided\n\nTrain.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='train') \nvalid.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='valid') \nplt.xlabel(\"Datetime\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best') \nplt.show()","27e0dec7":"# Predictions using navie approach\n\ndd= np.asarray(Train.Count) \ny_hat = valid.copy() \ny_hat['naive'] = dd[len(dd)-1] \nplt.figure(figsize=(12,8)) \nplt.plot(Train.index, Train['Count'], label='Train') \nplt.plot(valid.index,valid['Count'], label='Valid') \nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast') \nplt.legend(loc='best') \nplt.title(\"Naive Forecast\") \nplt.show()","5e7f7309":"# We will now calculate RMSE to check the accuracy of our model on validation data set.\n\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt \nrms = sqrt(mean_squared_error(valid.Count, y_hat.naive)) \nprint(rms)","482b3c70":"# Lets try the rolling mean for last 10, 20, 50 days and visualize the results.\n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(10).mean().iloc[-1] # average of last 10 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 10 observations') \nplt.legend(loc='best') \nplt.show() \n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(20).mean().iloc[-1] # average of last 20 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 20 observations') \nplt.legend(loc='best') \nplt.show() \n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(50).mean().iloc[-1] # average of last 50 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 50 observations') \nplt.legend(loc='best') \nplt.show()","9bb1ad47":"# RMS\nrms = sqrt(mean_squared_error(valid.Count, y_hat_avg.moving_avg_forecast)) \nprint(rms)","816eeafc":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\ny_hat_avg = valid.copy()\nfit2 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=0.6,optimized=False)\ny_hat_avg['SES'] = fit2.forecast(len(valid))\nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['SES'], label='SES') \nplt.legend(loc='best') \nplt.show()","b56a76f1":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SES)) \nprint(rms)","d6db87bd":"# Lets visualize all these parts.\n\nimport statsmodels.api as sm \nsm.tsa.seasonal_decompose(Train.Count).plot()\nresult = sm.tsa.stattools.adfuller(train.Count) \nplt.show()","62da569e":"y_hat_avg = valid.copy()\nfit1 = Holt(np.asarray(Train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(valid))\nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear') \nplt.legend(loc='best') \nplt.show()","549d7e09":"# Let\u2019s calculate the rmse of the model.\nrms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_linear)) \nprint(rms)","9975e36f":"y_hat_avg = valid.copy() \nfit1 = ExponentialSmoothing(np.asarray(Train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit() \ny_hat_avg['Holt_Winter'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot( Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter') \nplt.legend(loc='best') \nplt.show()","47a0c266":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_Winter)) \nprint(rms)","da441da7":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(train_original):\n        #Determing rolling statistics\n    rolmean = train_original.rolling(24).mean() # 24 hours on each day\n    rolstd = train_original.rolling(24).std()\n        #Plot rolling statistics:\n    orig = plt.plot(train_original, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n        #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(train_original, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","6d21df96":"from matplotlib.pylab import rcParams \nrcParams['figure.figsize'] = 20,10\n\ntest_stationarity(train_original['Count'])","dfc3604f":"Train_log = np.log(Train['Count']) \nvalid_log = np.log(valid['Count'])","f4387a2b":"moving_avg = Train_log.rolling(24).mean()\nplt.plot(Train_log)\nplt.plot(moving_avg, color = 'red')\nplt.show()","172027e5":"train_log_moving_avg_diff = Train_log - moving_avg\ntrain_log_moving_avg_diff.dropna(inplace = True) \ntest_stationarity(train_log_moving_avg_diff)","78b75b2b":"train_log_diff = Train_log - Train_log.shift(1) \ntest_stationarity(train_log_diff.dropna())","347f10ec":"from statsmodels.tsa.seasonal import seasonal_decompose \ndecomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values, freq = 24) \n\ntrend = decomposition.trend \nseasonal = decomposition.seasonal \nresidual = decomposition.resid \n\nplt.subplot(411) \nplt.plot(Train_log, label='Original') \nplt.legend(loc='best') \nplt.subplot(412) \nplt.plot(trend, label='Trend') \nplt.legend(loc='best') \nplt.subplot(413) \nplt.plot(seasonal,label='Seasonality') \nplt.legend(loc='best') \nplt.subplot(414) \nplt.plot(residual, label='Residuals') \nplt.legend(loc='best') \nplt.tight_layout() \nplt.show()","dfc312f3":"# Let\u2019s check stationarity of residuals.\n\ntrain_log_decompose = pd.DataFrame(residual) \ntrain_log_decompose['date'] = Train_log.index \ntrain_log_decompose.set_index('date', inplace = True) \ntrain_log_decompose.dropna(inplace=True) \ntest_stationarity(train_log_decompose[0])","c229889e":"from statsmodels.tsa.stattools import acf, pacf \nlag_acf = acf(train_log_diff.dropna(), nlags=25) \nlag_pacf = pacf(train_log_diff.dropna(), nlags=25, method='ols')","c9a90f9a":"plt.plot(lag_acf) \nplt.axhline(y=0,linestyle='--',color='gray') \nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.title('Autocorrelation Function') \nplt.show() \nplt.plot(lag_pacf) \nplt.axhline(y=0,linestyle='--',color='gray') \nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.title('Partial Autocorrelation Function') \nplt.show()","6b9c0580":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(Train_log, order=(2, 1, 0))  # here the q value is zero since it is just the AR model \nresults_AR = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original') \nplt.plot(results_AR.fittedvalues, color='red', label='predictions') \nplt.legend(loc='best') \nplt.show()","d33350be":"AR_predict=results_AR.predict(start=\"2014-06-25\", end=\"2014-09-25\") \nAR_predict=AR_predict.cumsum().shift().fillna(0) \nAR_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index) \nAR_predict1=AR_predict1.add(AR_predict,fill_value=0) \nAR_predict = np.exp(AR_predict1)","cbe257ef":"plt.plot(valid['Count'], label = \"Valid\") \nplt.plot(AR_predict, color = 'red', label = \"Predict\") \nplt.legend(loc= 'best') \nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, valid['Count']))\/valid.shape[0])) \nplt.show()","c138fbc6":"model = ARIMA(Train_log, order=(0, 1, 2))  # here the p value is zero since it is just the MA model \nresults_MA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original') \nplt.plot(results_MA.fittedvalues, color='red', label='prediction') \nplt.legend(loc='best') \nplt.show()","22d07b2f":"MA_predict=results_MA.predict(start=\"2014-06-25\", end=\"2014-09-25\") \nMA_predict=MA_predict.cumsum().shift().fillna(0) \nMA_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index) \nMA_predict1=MA_predict1.add(MA_predict,fill_value=0) \nMA_predict = np.exp(MA_predict1)","be64568e":"plt.plot(valid['Count'], label = \"Valid\") \nplt.plot(MA_predict, color = 'red', label = \"Predict\") \nplt.legend(loc= 'best') \nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, valid['Count']))\/valid.shape[0])) \nplt.show()","f0b12182":"model = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='original') \nplt.plot(results_ARIMA.fittedvalues, color='red', label='predicted') \nplt.legend(loc='best') \nplt.show()","f72a91a8":"def check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Count'])[0], index = given_set.index)\n    predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))\/given_set.shape[0]))\n    plt.show()","07646703":"\ndef check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n \n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))\/given_set.shape[0]))\n    plt.show()","97c4d029":"# Predict the Values for validation set\nARIMA_predict_diff=results_ARIMA.predict(start=\"2014-06-25\", end=\"2014-09-25\")\ncheck_prediction_diff(ARIMA_predict_diff, valid)","f1c353e9":"import statsmodels.api as sm\n\ny_hat_avg = valid.copy() \nfit1 = sm.tsa.statespace.SARIMAX(Train.Count, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit() \ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-6-25\", end=\"2014-9-25\", dynamic=True) \nplt.figure(figsize=(16,8)) \nplt.plot( Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['SARIMA'], label='SARIMA') \nplt.legend(loc='best') \nplt.show()","2dbbe640":"## Let\u2019s check the rmse value for the validation part.\n\nrms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SARIMA)) \nprint(rms)","b9750d99":"## 3. Holt\u2019s Linear Trend Model\n\n- It is an extension of simple exponential smoothing to allow forecasting of data with a trend.\n- This method takes into account the trend of the dataset. The forecast function in this method is a function of level and trend.\n\nFirst of all let us visualize the trend, seasonality and error in the series.\n\nWe can decompose the time series in four parts.\n\n- Observed, which is the original time series.\n- Trend, which shows the trend in the time series, i.e., increasing or decreasing behaviour of the time series.\n- Seasonal, which tells us about the seasonality in the time series.\n> - Residual, which is obtained by removing any trend or seasonality in the time series.","baf30fd3":"*_As we have seen that there is a lot of noise in the hourly time series, we will aggregate the hourly time series to daily, weekly, and monthly time series to reduce the noise and make it more stable and hence would be easier for a model to learn._*","0755ca42":"### 4. Holt winter\u2019s model on daily time series\n\n   - Datasets which show a similar set of pattern after fixed intervals of a time period suffer from seasonality.\n   \n - The above mentioned models don\u2019t take into account the seasonality of the dataset while forecasting. Hence we need a method that takes into account both trend and seasonality to forecast future prices.\n\n - One such algorithm that we can use in such a scenario is Holt\u2019s Winter method. The idea behind Holt\u2019s Winter is to apply exponential smoothing to the seasonal components in addition to level and trend.","f46269ba":"- It can be inferred from the above plot that the traffic is more on weekdays as compared to weekends which validates our hypothesis.","f006a486":"### 1. Hypothesis Generation\n\n1. There will be an increase in the traffic as the years pass by.\n   - Explanation - Population has a general upward trend with time, so I can expect more people to travel by JetRail. Also, generally companies expand their businesses over time leading to more customers travelling through JetRail.\n\n\n2. The traffic will be high from May to October.\n   - Explanation - Tourist visits generally increases during this time perion.\n\n\n3. Traffic on weekdays will be more as compared to weekends\/holidays.\n   - Explanation - People will go to office on weekdays and hence the traffic will be more\n\n\n4. Traffic during the peak hours will be high.\n   - Explanation - People will travel to work, college.","7f59e04d":"- We can see that the time series is becoming more and more stable when we are aggregating it on daily, weekly and monthly basis.\n\n- But it would be difficult to convert the monthly and weekly predictions to hourly predictions, as first we have to convert the monthly predictions to weekly, weekly to daily and daily to hourly predictions, which will become very expanded process. So, we will work on the daily time series.","79b52141":"- We have done time based validation here by selecting the last 3 months for the validation data and rest in the train data. If we would have done it randomly it may work well for the train dataset but will not work effectively on validation dataset.\n\n- Lets understand it in this way: If we choose the split randomly it will take some values from the starting and some from the last years as well. It is similar to predicting the old values based on the future values which is not the case in real scenario. So, this kind of split is used while working with time related problems.","b060180c":"We made a hypothesis for the traffic pattern on weekday and weekend as well. So, let\u2019s make a weekend variable to visualize the impact of weekend on traffic.\n\n  - We will first extract the day of week from Datetime and then based on the values we will assign whether the day is a weekend or not.\n\n  - Values of 5 and 6 represents that the days are weekend.","9b85e535":"**The statistics shows that the time series is stationary as Test Statistic < Critical value but we can see an increasing trend in the data. So, firstly we will try to make the data more stationary. For doing so, we need to remove the trend and seasonality from the data.**","01772c79":"**NOTE** - It is always a good practice to create a validation set that can be used to assess our models locally. If the validation metric(rmse) is changing in proportion to public leaderboard score, this would imply that we have chosen a stable validation technique.","7a42ba5f":"## 7. SARIMAX model on daily time series","bb0227e2":"- It can be interpreted from the results that the residuals are stationary.\n\n- Now we will forecast the time series using different models.","5cbc6bb0":"- We took the average of last 10, 20 and 50 observations and predicted based on that. This value can be changed in the above code in .rolling().mean() part. We can see that the predictions are getting weaker as we increase the number of observations.","f53b2492":"### 2.Getting the system ready & loading the data","43d1a449":"- We have 18288 different records for the Count of passengers in train set and 5112 in test set.","927e15ba":"### Removing Trend\n\n- A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear.\n\n- We see an increasing trend in the data so we can apply transformation which penalizes higher values more than smaller ones, for example log transformation.\n\n- We will take rolling average here to remove the trend. We will take the window size of 24 based on the fact that each day has 24 hours.","1a731749":"- Lets plot the validation curve for AR model.\n\n- We have to change the scale of the model to the original scale.\n\n- First step would be to store the predicted results as a separate series and observe it.","b4f6c724":"### 1.Splitting the data into training and validation part","421cd124":"### 4. Feature Extraction","3014c689":"- We see that the months 10, 11 and 12 are not present for the year 2014 and the mean value for these months in year 2012 is very less.\n\n- Since there is an increasing trend in our time series, the mean value for rest of the months will be more because of their larger passenger counts in year 2014 and we will get smaller value for these 3 months.\n\n- In the above line plot we can see an increasing trend in monthly passenger count and the growth is approximately exponential.","0786c7d6":"- We can see the trend, residuals and the seasonality clearly in the above graph. Seasonality shows a constant trend in counter.","11e85eb9":"##### Naive Approach\n\n- In this forecasting technique, we assume that the next expected point is equal to the last observed point. So we can expect a straight horizontal line as the prediction. Lets understand it with an example and an image:","0f7ac19a":"### Combine Model","875a2aa0":"Let\u2019s understand each feature first:.\n   - ID is the unique number given to each observation point.\n   - Datetime is the time of each observation.\n   - Count is the passenger count corresponding to each Datetime","4102940d":"- It can be inferred that the peak traffic is at 7 PM and then we see a decreasing trend till 5 AM.\n- After that the passenger count starts increasing again and peaks again between 11AM and 12 Noon.","d218bab6":"### 3. Dataset Structure and Content","f3d5cf3b":"### AR Model","5a9ab64f":"- We can see that the Test Statistic is very smaller as compared to the Critical Value. So, we can be confident that the trend is almost removed.","460e46b2":"### 6. Parameter tuning for ARIMA model\n\n\n#### Stationarity Check\n\n- We use Dickey Fuller test to check the stationarity of the series.\n- The intuition behind this test is that it determines how strongly a time series is defined by a trend.\n- The null hypothesis of the test is that time series is not stationary (has some time-dependent structure).\n- The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.","ffdeff99":"### Removing Seasonality\n\n- By seasonality, we mean periodic fluctuations. A seasonal pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\n- Seasonality is always of a fixed and known period.\n- We will use seasonal decompose to decompose the time series into trend, seasonality and residuals.","fe90edac":"- We can infer that the fit of the model has improved as the rmse value has reduced.","1ccc2585":"### MA Model","3eecfb53":"- Order in the above model represents the order of the autoregressive model(number of time lags), the degree of differencing(number of times the data have had past values subtracted) and the order of moving average model.\n\n- Seasonal order represents the order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity.\n\n- In our case the periodicity is 7 since it is daily time series and will repeat after every 7 days.","6c1bbe36":"### Forecasting the time series using ARIMA\n\n- First of all we will fit the ARIMA model on our time series for that we have to find the optimized values for the p,d,q parameters.\n\n- To find the optimized values of these parameters, we will use ACF(Autocorrelation Function) and PACF(Partial Autocorrelation Function) graph.\n\n- ACF is a measure of the correlation between the TimeSeries with a lagged version of itself.\n\n- PACF measures the correlation between the TimeSeries with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons.","9d933fc1":"   - ID and Count are in integer format while the Datetime is in object format for the train file.\n   - Id is in integer and Datetime is in object format for test file.","0f4bb60b":"Lets recall the hypothesis that we made earlier:\n\n  - Traffic will increase as the years pass by\n  - Traffic will be high from May to October\n  - Traffic on weekdays will be more\n  - Traffic during the peak hours will be high","7994c4e0":"- We are not getting much insights from day wise count of the passengers.","8e16e5de":"- We can calculate how accurate our predictions are using rmse(Root Mean Square Error).\n- rmse is the standard deviation of the residuals.\n- Residuals are a measure of how far from the regression line data points are.\n- The formula for rmse is:\n\n\n*rmse=sqrt\u2211i=1N1N(p\u2212a)2*","af37fccf":"#### 2.Moving Average\n\n- In this technique we will take the average of the passenger counts for last few time periods only.\n- Here the predictions are made on the basis of the average of last few points instead of taking all the previously known values.","b172f0b8":"We made some hypothesis for the effect of hour, day, month and year on the passenger count. So, let\u2019s extract the year, month, day and hour from the Datetime to validate our hypothesis.","e54c7f57":"- It can be inferred that the rmse value has decreased.","143ac2b2":"- We have done time based validation here by selecting the last 3 months for the validation data and rest in the train data. If we would have done it randomly it may work well for the train dataset but will not work effectively on validation dataset.\n\n- Lets understand it in this way: If we choose the split randomly it will take some values from the starting and some from the last years as well. It is similar to predicting the old values based on the future values which is not the case in real scenario. So, this kind of split is used while working with time related problems.","a30cf39b":"## <span style='background :yellow' > **Table of Contents** <\/span>","e9341bc4":" ### Its Time For - Much Anticipated ARIMA","38d82f43":"### ACF and PACF plot","2dde91f6":"- From the above bar plot, we can infer that the passenger count is less for saturday and sunday as compared to the other days of the week","bcae536d":"### 5. Introduction to ARIMA model\n\n- ARIMA stands for Auto Regression Integrated Moving Average. It is specified by three ordered parameters (p,d,q).\n\n- Here p is the order of the autoregressive model(number of time lags)\n    \n- d is the degree of differencing(number of times the data have had past values subtracted)\n\n- q is the order of moving average model. We will discuss more about these parameters in next section.\n\n\n**What is a stationary time series?**\n\nThere are three basic criterion for a series to be classified as stationary series :\n\n- The mean of the time series should not be a function of time. It should be constant.\n- The variance of the time series should not be a function of time.- \n- The covariance of the ith term and the (i+m)th term should not be a function of time.","33ac29be":"- p value is the lag value where the PACF chart crosses the upper confidence interval for the first time. It can be noticed that in this case p=1.\n\n- q value is the lag value where the ACF chart crosses the upper confidence interval for the first time. It can be noticed that in this case q=1.\n\n- Now we will make the ARIMA model as we have the p,q values. We will make the AR and MA model separately and then combine them together.","959ba146":"- Holt winters model produced rmse of 82.3793 ","a8d73398":"### 5.Exploratory Analysis","7977baad":"### 2. Modeling\n\n- i) Naive Approach\n- ii) Moving Average\n- iii) Simple Exponential Smoothing\n- iv) Holt\u2019s Linear Trend Model","81d58cd1":"## <span style='color:Blue'> Chapter - 1 \"Understanding Data\"  <\/span>\n\n1. **Hypothesis Generation**\n2. **Getting the system ready and loading the data**\n3. **Dataset Structure and Content**\n4. **Feature Extraction**\n5. **Exploratory Analysis**","85003633":"#### 3. Simple Exponential Smoothing\n\n- In this technique, we assign larger weights to more recent observations than to observations from the distant past.\n- The weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations.\n\n- *NOTE* - If we give the entire weight to the last observed value only, this method will be similar to the naive approach. So, we can say that naive approach is also a simple exponential smoothing technique where the entire weight is given to the last observed value.","653ba32d":"## <span style='color:Blue'> Chapter - 2 \"Forecasting using Multiple Modeling Techniques\" <\/span>","2bddc4fa":"- We can infer that this method is not suitable for datasets with high variability. We can reduce the rmse value by adopting different techniques.","5db15333":"### Thanks for reading..if you find this book interesting kindly upvote..Thank you!!","d8ff2ddb":"- Here the red line shows the prediction for the validation set. Let\u2019s build the MA model now.","7e5f81a9":"## <span style='color:Blue'> Chapter - 2 \"Forecasting using Multiple Modeling Techniques\" <\/span>\n\n1. **Splitting the data into training and validation part**\n2. **Modeling techniques**\n3. **Holt\u2019s Linear Trend Model on daily time series**\n4. **Holt Winter\u2019s Model on daily time series**\n5. **Introduction to ARIMA model**\n6. **Parameter tuning for ARIMA model**\n7. **SARIMAX model on daily time series**","9bc26525":"### Problem Statement - a problem well stated is half solved\n\nThis time you are helping out Unicorn Investors with your data hacking skills. They are considering making an investment in a new form of transportation - JetRail. JetRail uses Jet propulsion technology to run rails and move people at a high speed! While JetRail has mastered the technology and they hold the patent for their product, the investment would only make sense, if they can get more than 1 Million monthly users with in next 18 months.\n\nYou need to help Unicorn ventures with the decision. They usually invest in B2C start-ups less than 4 years old looking for pre-series A funding. In order to help Unicorn Ventures in their decision, you need to forecast the traffic on JetRail for the next 7 months. You are provided with traffic data of JetRail since inception in the test file.","88643a06":"- Here the blue part represents the train data and the orange part represents the validation data.\n\n- We will predict the traffic for the validation part and then visualize how accurate our predictions are. Finally we will make predictions for the test dataset.","8847d7ca":"- Differencing can help to make the series stable and eliminate the trend."}}