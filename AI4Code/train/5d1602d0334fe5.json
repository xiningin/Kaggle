{"cell_type":{"5771cb4e":"code","dd4ef9fb":"code","726c4ad0":"code","bce51c4a":"code","976be29d":"code","b9833cb4":"code","4fe4cc1d":"code","ed5dd315":"code","89ce3ee1":"code","cf14c730":"code","33f31432":"code","303aaa32":"code","429c883d":"code","54aacd23":"code","d342ba8e":"code","51428a3b":"code","23b75692":"code","e9271092":"code","5e8ea9d9":"markdown","4c25429f":"markdown","e06c4c1e":"markdown","d9b71d6f":"markdown","1c2ef3aa":"markdown","d4ef5836":"markdown","68a84474":"markdown","2eaf56a1":"markdown","de8bd257":"markdown","489beef0":"markdown","9ab90a70":"markdown","4da6244f":"markdown","7e0c5428":"markdown"},"source":{"5771cb4e":"import numpy as np\nimport pandas as pd\nimport os, datetime\nimport tensorflow as tf\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","dd4ef9fb":"batch_size = 32\nseq_len = 128\n\nd_k = 256\nd_v = 256\nn_heads = 12\nff_dim = 256\n","726c4ad0":"df = pd.read_csv('\/kaggle\/input\/IBM.csv', delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n\n# Replace 0 to avoid dividing by 0 later on\ndf['Volume'].replace(to_replace=0, method='ffill', inplace=True) \ndf.sort_values('Date', inplace=True)\ndf.tail(5)","bce51c4a":"fig = plt.figure(figsize=(15,10))\nst = fig.suptitle(\"IBM Close Price and Volume\", fontsize=20)\nst.set_y(0.92)\n\nax1 = fig.add_subplot(211)\nax1.plot(df['Close'], label='IBM Close Price')\nax1.set_xticks(range(0, df.shape[0], 1464))\nax1.set_xticklabels(df['Date'].loc[::1464])\nax1.set_ylabel('Close Price', fontsize=18)\nax1.legend(loc=\"upper left\", fontsize=12)\n\nax2 = fig.add_subplot(212)\nax2.plot(df['Volume'], label='IBM Volume')\nax2.set_xticks(range(0, df.shape[0], 1464))\nax2.set_xticklabels(df['Date'].loc[::1464])\nax2.set_ylabel('Volume', fontsize=18)\nax2.legend(loc=\"upper left\", fontsize=12)","976be29d":"'''Calculate percentage change'''\n\ndf['Open'] = df['Open'].pct_change() # Create arithmetic returns column\ndf['High'] = df['High'].pct_change() # Create arithmetic returns column\ndf['Low'] = df['Low'].pct_change() # Create arithmetic returns column\ndf['Close'] = df['Close'].pct_change() # Create arithmetic returns column\ndf['Volume'] = df['Volume'].pct_change()\n\ndf.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n\n###############################################################################\n'''Normalize price columns'''\n\nmin_return = min(df[['Open', 'High', 'Low', 'Close']].min(axis=0))\nmax_return = max(df[['Open', 'High', 'Low', 'Close']].max(axis=0))\n\n# Min-max normalize price columns (0-1 range)\ndf['Open'] = (df['Open'] - min_return) \/ (max_return - min_return)\ndf['High'] = (df['High'] - min_return) \/ (max_return - min_return)\ndf['Low'] = (df['Low'] - min_return) \/ (max_return - min_return)\ndf['Close'] = (df['Close'] - min_return) \/ (max_return - min_return)\n\n###############################################################################\n'''Normalize volume column'''\n\nmin_volume = df['Volume'].min(axis=0)\nmax_volume = df['Volume'].max(axis=0)\n\n# Min-max normalize volume columns (0-1 range)\ndf['Volume'] = (df['Volume'] - min_volume) \/ (max_volume - min_volume)\n\n\n'''Create training, validation and test split'''\n\ntimes = sorted(df.index.values)\nlast_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\nlast_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n\ndf_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\ndf_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\ndf_test = df[(df.index >= last_10pct)]\n\n# Remove date column\ndf_train.drop(columns=['Date'], inplace=True)\ndf_val.drop(columns=['Date'], inplace=True)\ndf_test.drop(columns=['Date'], inplace=True)\n\n# Convert pandas columns into arrays\ntrain_data = df_train.values\nval_data = df_val.values\ntest_data = df_test.values\nprint('Training data shape: {}'.format(train_data.shape))\nprint('Validation data shape: {}'.format(val_data.shape))\nprint('Test data shape: {}'.format(test_data.shape))\n\ndf_train.head(5)","b9833cb4":"class Time2Vector(Layer):\n  def __init__(self, seq_len, **kwargs):\n    super(Time2Vector, self).__init__()\n    self.seq_len = seq_len\n\n  def build(self, input_shape):\n    '''Initialize weights and biases with shape (batch, seq_len)'''\n    self.weights_linear = self.add_weight(name='weight_linear',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n    \n    self.bias_linear = self.add_weight(name='bias_linear',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n    \n    self.weights_periodic = self.add_weight(name='weight_periodic',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n\n    self.bias_periodic = self.add_weight(name='bias_periodic',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n\n  def call(self, x):\n    '''Calculate linear and periodic time features'''\n    x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n    \n    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n    time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n    return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n   \n  def get_config(self): # Needed for saving and loading model with custom layer\n    config = super().get_config().copy()\n    config.update({'seq_len': self.seq_len})\n    return config\n  ","4fe4cc1d":"class SingleAttention(Layer):\n  def __init__(self, d_k, d_v):\n    super(SingleAttention, self).__init__()\n    self.d_k = d_k\n    self.d_v = d_v\n\n  def build(self, input_shape):\n    self.query = Dense(self.d_k, \n                       input_shape=input_shape, \n                       kernel_initializer='glorot_uniform', \n                       bias_initializer='glorot_uniform')\n    \n    self.key = Dense(self.d_k, \n                     input_shape=input_shape, \n                     kernel_initializer='glorot_uniform', \n                     bias_initializer='glorot_uniform')\n    \n    self.value = Dense(self.d_v, \n                       input_shape=input_shape, \n                       kernel_initializer='glorot_uniform', \n                       bias_initializer='glorot_uniform')\n\n  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n    q = self.query(inputs[0])\n    k = self.key(inputs[1])\n\n    attn_weights = tf.matmul(q, k, transpose_b=True)\n    attn_weights = tf.map_fn(lambda x: x\/np.sqrt(self.d_k), attn_weights)\n    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n    \n    v = self.value(inputs[2])\n    attn_out = tf.matmul(attn_weights, v)\n    return attn_out    \n\n\n\nclass MultiAttention(Layer):\n  def __init__(self, d_k, d_v, n_heads):\n    super(MultiAttention, self).__init__()\n    self.d_k = d_k\n    self.d_v = d_v\n    self.n_heads = n_heads\n    self.attn_heads = list()\n\n  def build(self, input_shape):\n    for n in range(self.n_heads):\n      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n    \n    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n    self.linear = Dense(input_shape[0][-1], \n                        input_shape=input_shape, \n                        kernel_initializer='glorot_uniform', \n                        bias_initializer='glorot_uniform')\n\n  def call(self, inputs):\n    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n    concat_attn = tf.concat(attn, axis=-1)\n    multi_linear = self.linear(concat_attn)\n    return multi_linear   \n\n\n\n","ed5dd315":"class TransformerEncoder(Layer):\n  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n    super(TransformerEncoder, self).__init__()\n    self.d_k = d_k\n    self.d_v = d_v\n    self.n_heads = n_heads\n    self.ff_dim = ff_dim\n    self.attn_heads = list()\n    self.dropout_rate = dropout\n\n  def build(self, input_shape):\n    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n    self.attn_dropout = Dropout(self.dropout_rate)\n    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n\n    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n    self.ff_dropout = Dropout(self.dropout_rate)\n    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n  \n  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n    attn_layer = self.attn_multi(inputs)\n    attn_layer = self.attn_dropout(attn_layer)\n    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n\n    ff_layer = self.ff_conv1D_1(attn_layer)\n    ff_layer = self.ff_conv1D_2(ff_layer)\n    ff_layer = self.ff_dropout(ff_layer)\n    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n    return ff_layer \n\n  def get_config(self): # Needed for saving and loading model with custom layer\n    config = super().get_config().copy()\n    config.update({'d_k': self.d_k,\n                   'd_v': self.d_v,\n                   'n_heads': self.n_heads,\n                   'ff_dim': self.ff_dim,\n                   'attn_heads': self.attn_heads,\n                   'dropout_rate': self.dropout_rate})\n    return config          ","89ce3ee1":"\ndf = pd.read_csv('\/kaggle\/input\/IBM.csv', delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n\n# Replace 0 to avoid dividing by 0 later on\ndf['Volume'].replace(to_replace=0, method='ffill', inplace=True) \ndf.sort_values('Date', inplace=True)\n\n# Apply moving average with a window of 10 days to all columns\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].rolling(10).mean() \n\n# Drop all rows with NaN values\ndf.dropna(how='any', axis=0, inplace=True) \ndf.head()","cf14c730":"fig = plt.figure(figsize=(15,10))\nst = fig.suptitle(\"IBM Close Price and Volume\", fontsize=20)\nst.set_y(0.92)\n\nax1 = fig.add_subplot(211)\nax1.plot(df['Close'], label='IBM  Close Price')\nax1.set_xticks(range(0, df.shape[0], 1464))\nax1.set_xticklabels(df['Date'].loc[::1464])\nax1.set_ylabel('Close Price', fontsize=18)\nax1.legend(loc=\"upper left\", fontsize=12)\n\nax2 = fig.add_subplot(212)\nax2.plot(df['Volume'], label='IBM  Volume')\nax2.set_xticks(range(0, df.shape[0], 1464))\nax2.set_xticklabels(df['Date'].loc[::1464])\nax2.set_ylabel('Volume', fontsize=18)\nax2.legend(loc=\"upper left\", fontsize=12)","33f31432":"'''Calculate percentage change'''\n\ndf['Open'] = df['Open'].pct_change() # Create arithmetic returns column\ndf['High'] = df['High'].pct_change() # Create arithmetic returns column\ndf['Low'] = df['Low'].pct_change() # Create arithmetic returns column\ndf['Close'] = df['Close'].pct_change() # Create arithmetic returns column\ndf['Volume'] = df['Volume'].pct_change()\n\ndf.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n\n\n'''Normalize price columns'''\n\nmin_return = min(df[['Open', 'High', 'Low', 'Close']].min(axis=0))\nmax_return = max(df[['Open', 'High', 'Low', 'Close']].max(axis=0))\n\n# Min-max normalize price columns (0-1 range)\ndf['Open'] = (df['Open'] - min_return) \/ (max_return - min_return)\ndf['High'] = (df['High'] - min_return) \/ (max_return - min_return)\ndf['Low'] = (df['Low'] - min_return) \/ (max_return - min_return)\ndf['Close'] = (df['Close'] - min_return) \/ (max_return - min_return)\n\n\n'''Normalize volume column'''\n\nmin_volume = df['Volume'].min(axis=0)\nmax_volume = df['Volume'].max(axis=0)\n\n# Min-max normalize volume columns (0-1 range)\ndf['Volume'] = (df['Volume'] - min_volume) \/ (max_volume - min_volume)\n\n\n'''Create training, validation and test split'''\n\ntimes = sorted(df.index.values)\nlast_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\nlast_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n\ndf_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\ndf_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\ndf_test = df[(df.index >= last_10pct)]\n\n# Remove date column\ndf_train.drop(columns=['Date'], inplace=True)\ndf_val.drop(columns=['Date'], inplace=True)\ndf_test.drop(columns=['Date'], inplace=True)\n\n# Convert pandas columns into arrays\ntrain_data = df_train.values\nval_data = df_val.values\ntest_data = df_test.values\nprint('Training data shape: {}'.format(train_data.shape))\nprint('Validation data shape: {}'.format(val_data.shape))\nprint('Test data shape: {}'.format(test_data.shape))\n\ndf_train.head()","303aaa32":"fig = plt.figure(figsize=(15,12))\nst = fig.suptitle(\"Data Separation\", fontsize=20)\nst.set_y(0.95)\n\n\n\nax1 = fig.add_subplot(211)\nax1.plot(np.arange(train_data.shape[0]), df_train['Close'], label='Training data')\n\nax1.plot(np.arange(train_data.shape[0], \n                   train_data.shape[0]+val_data.shape[0]), df_val['Close'], label='Validation data')\n\nax1.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n                   train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Close'], label='Test data')\nax1.set_xlabel('Date')\nax1.set_ylabel('Normalized Closing Returns')\nax1.set_title(\"Close Price\", fontsize=18)\nax1.legend(loc=\"best\", fontsize=12)\n\n\n\nax2 = fig.add_subplot(212)\nax2.plot(np.arange(train_data.shape[0]), df_train['Volume'], label='Training data')\n\nax2.plot(np.arange(train_data.shape[0], \n                   train_data.shape[0]+val_data.shape[0]), df_val['Volume'], label='Validation data')\n\nax2.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n                   train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Volume'], label='Test data')\nax2.set_xlabel('Date')\nax2.set_ylabel('Normalized Volume Changes')\nax2.set_title(\"Volume\", fontsize=18)\nax2.legend(loc=\"best\", fontsize=12)","429c883d":"# Training data\nX_train, y_train = [], []\nfor i in range(seq_len, len(train_data)):\n  X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n  y_train.append(train_data[:, 3][i]) #Value of 4th column (Close Price) of df-row 128+1\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n###############################################################################\n\n# Validation data\nX_val, y_val = [], []\nfor i in range(seq_len, len(val_data)):\n    X_val.append(val_data[i-seq_len:i])\n    y_val.append(val_data[:, 3][i])\nX_val, y_val = np.array(X_val), np.array(y_val)\n\n###############################################################################\n\n# Test data\nX_test, y_test = [], []\nfor i in range(seq_len, len(test_data)):\n    X_test.append(test_data[i-seq_len:i])\n    y_test.append(test_data[:, 3][i])    \nX_test, y_test = np.array(X_test), np.array(y_test)\n\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","54aacd23":"def create_model():\n  '''Initialize time and transformer layers'''\n  time_embedding = Time2Vector(seq_len)\n  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n\n  '''Construct model'''\n  in_seq = Input(shape=(seq_len, 5))\n  x = time_embedding(in_seq)\n  x = Concatenate(axis=-1)([in_seq, x])\n  x = attn_layer1((x, x, x))\n  x = attn_layer2((x, x, x))\n  x = attn_layer3((x, x, x))\n  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n  x = Dropout(0.1)(x)\n  x = Dense(64, activation='relu')(x)\n  x = Dropout(0.1)(x)\n  out = Dense(1, activation='linear')(x)\n\n  model = Model(inputs=in_seq, outputs=out)\n  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n  return model\n\n\nmodel = create_model()\nmodel.summary()\n\n","d342ba8e":"callback = tf.keras.callbacks.ModelCheckpoint('Transformer+TimeEmbedding_avg.hdf5', \n                                              monitor='val_loss', \n                                              save_best_only=True, \n                                              verbose=1)\n\nhistory = model.fit(X_train, y_train, \n                    batch_size=batch_size, \n                    epochs=35, \n                    callbacks=[callback],\n                    validation_data=(X_val, y_val))  ","51428a3b":"model = tf.keras.models.load_model('Transformer+TimeEmbedding_avg.hdf5',\n                                   custom_objects={'Time2Vector': Time2Vector, \n                                                   'SingleAttention': SingleAttention,\n                                                   'MultiAttention': MultiAttention,\n                                                   'TransformerEncoder': TransformerEncoder})\n\n\n#Calculate predication for training, validation and test data\ntrain_pred = model.predict(X_train)\nval_pred = model.predict(X_val)\ntest_pred = model.predict(X_test)\n\n#Print evaluation metrics for all datasets\ntrain_eval = model.evaluate(X_train, y_train, verbose=1)\nval_eval = model.evaluate(X_val, y_val, verbose=1)\ntest_eval = model.evaluate(X_test, y_test, verbose=1)\nprint(' ')\nprint('Evaluation metrics')\nprint('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\nprint('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\nprint('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))\n\n","23b75692":"\n'''Display results'''\n\nfig = plt.figure(figsize=(15,20))\nst = fig.suptitle(\"Moving Average - Transformer + TimeEmbedding Model\", fontsize=22)\nst.set_y(0.92)\n\n#Plot training data results\nax11 = fig.add_subplot(311)\nax11.plot(train_data[:, 3], label='IBM  Closing Returns')\nax11.plot(np.arange(seq_len, train_pred.shape[0]+seq_len), train_pred, linewidth=3, label='Predicted Google Closing Returns')\nax11.set_title(\"Training Data\", fontsize=18)\nax11.set_xlabel('Date')\nax11.set_ylabel('IBM  Closing Returns')\nax11.legend(loc=\"best\", fontsize=12)\n\n#Plot validation data results\nax21 = fig.add_subplot(312)\nax21.plot(val_data[:, 3], label='IBM  Closing Returns')\nax21.plot(np.arange(seq_len, val_pred.shape[0]+seq_len), val_pred, linewidth=3, label='Predicted IBM Closing Returns')\nax21.set_title(\"Validation Data\", fontsize=18)\nax21.set_xlabel('Date')\nax21.set_ylabel('IBM  Closing Returns')\nax21.legend(loc=\"best\", fontsize=12)\n\n#Plot test data results\nax31 = fig.add_subplot(313)\nax31.plot(test_data[:, 3], label='IBM  Closing Returns')\nax31.plot(np.arange(seq_len, test_pred.shape[0]+seq_len), test_pred, linewidth=3, label='Predicted Google Closing Returns')\nax31.set_title(\"Test Data\", fontsize=18)\nax31.set_xlabel('Date')\nax31.set_ylabel('IBM  Closing Returns')\nax31.legend(loc=\"best\", fontsize=12)","e9271092":"'''Display model metrics'''\n\nfig = plt.figure(figsize=(15,20))\nst = fig.suptitle(\"Moving Average - Transformer + TimeEmbedding Model Metrics\", fontsize=22)\nst.set_y(0.92)\n\n#Plot model loss\nax1 = fig.add_subplot(311)\nax1.plot(history.history['loss'], label='Training loss (MSE)')\nax1.plot(history.history['val_loss'], label='Validation loss (MSE)')\nax1.set_title(\"Model loss\", fontsize=18)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss (MSE)')\nax1.legend(loc=\"best\", fontsize=12)\n\n#Plot MAE\nax2 = fig.add_subplot(312)\nax2.plot(history.history['mae'], label='Training MAE')\nax2.plot(history.history['val_mae'], label='Validation MAE')\nax2.set_title(\"Model metric - Mean average error (MAE)\", fontsize=18)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Mean average error (MAE)')\nax2.legend(loc=\"best\", fontsize=12)\n\n#Plot MAPE\nax3 = fig.add_subplot(313)\nax3.plot(history.history['mape'], label='Training MAPE')\nax3.plot(history.history['val_mape'], label='Validation MAPE')\nax3.set_title(\"Model metric - Mean average percentage error (MAPE)\", fontsize=18)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Mean average percentage error (MAPE)')\nax3.legend(loc=\"best\", fontsize=12)","5e8ea9d9":"# Moving Average","4c25429f":"## TimeToVector\nhttps:\/\/arxiv.org\/abs\/1907.05321","e06c4c1e":"https:\/\/www.kaggle.com\/ajax0564\/timeseriestransformer","d9b71d6f":"## Moving Average - Model metrics","1c2ef3aa":"## Moving Average - Plot daily changes of close price and volume","d4ef5836":"## Moving Average - Model","68a84474":"## Calculate normalized percentage change of all columns","2eaf56a1":"## Transformer\n1. Attention https:\/\/arxiv.org\/abs\/1706.03762\n2. glorot_uniform https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/initializers\/GlorotUniform\n3. Transformer  https:\/\/arxiv.org\/pdf\/2001.08317","de8bd257":"DATA:\n\nhttps:\/\/finance.yahoo.com\/quote\/IBM\/history?period1=950400&period2=1594512000&interval=1d&filter=history&frequency=1d","489beef0":" Plot daily  closing prices and volume","9ab90a70":"## Moving Average - Calculate normalized percentage change for all columns","4da6244f":"## Moving Average - Create chunks of training, validation, and test data","7e0c5428":"## Moving Average - Plot daily Google closing prices and volume"}}