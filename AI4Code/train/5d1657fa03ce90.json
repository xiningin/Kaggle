{"cell_type":{"a10298ee":"code","1c5dce91":"code","5818fa37":"code","50fafa00":"code","6a36353a":"code","a8aa18d0":"code","9bb150aa":"code","c6a9e544":"code","372c3fa8":"code","6e38221f":"code","98180329":"code","820002fa":"code","78dc76b7":"code","e5803319":"code","a76b965e":"code","30b36205":"code","a2a20931":"code","ae648a98":"code","e73fb0c0":"code","befc4d61":"code","ae9a31fc":"code","d316db2a":"code","719336b0":"code","7394528f":"code","9aa3bdd4":"code","c0644da2":"code","cf49f9c3":"code","6122bf0a":"code","0a3c0d43":"markdown","4f8a069b":"markdown","b9c3d9d2":"markdown","532cf3b8":"markdown","b7e2b674":"markdown","71240edc":"markdown","5a2c06a7":"markdown","42c854d5":"markdown","92ee1d9b":"markdown","d5c4958a":"markdown","0cd4871f":"markdown","1dfe101c":"markdown","1bfc3859":"markdown","0e4c4054":"markdown","901a492d":"markdown","3a24c02f":"markdown","cbbdd18a":"markdown","6b860ebe":"markdown","d876db22":"markdown","728c85e1":"markdown","93e123fc":"markdown","5a73ea3b":"markdown","45fca959":"markdown","7a1cb067":"markdown","2e1d4216":"markdown","03a5ebde":"markdown","cb607d21":"markdown","0e6104c7":"markdown","35a79ab8":"markdown","539af636":"markdown","e3f910b5":"markdown","1cdce0c7":"markdown","126222eb":"markdown","a9cc5142":"markdown","847934b0":"markdown"},"source":{"a10298ee":"!pip install --upgrade wandb","1c5dce91":"import os \nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import *\nimport tensorflow_addons as tfa\n\nimport wandb\nfrom wandb.keras import WandbCallback","5818fa37":"# Load the data\ndef load_data(path):\n\n    train = pd.read_csv(path+\"train.csv\")\n    test = pd.read_csv(path+\"test.csv\")\n    \n    x_tr = train.drop(labels=[\"label\"], axis=1)\n    y_tr = train[\"label\"]\n    \n    print(f'Train: we have {x_tr.shape[0]} images with {x_tr.shape[1]} features and {y_tr.nunique()} classes')\n    print(f'Test: we have {test.shape[0]} images with {test.shape[1]} features')\n    \n    return x_tr, y_tr, test\n\n\ndef seed_all(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","50fafa00":"DEBUG = False       # set to True in case of testing\/debugging\nDATA_AUGM = False   # set to True if you wish to add data augmentation \n\nBATCH_SIZE = 64\n\nif DEBUG:\n    EPOCHS = 3          \nelse: \n    EPOCHS = 40","6a36353a":"SEED = 26\nPATH = \"..\/input\/digit-recognizer\/\"\n\nseed_all(SEED)\n\nx_train, y_train, x_test = load_data(path=PATH)","a8aa18d0":"sns.countplot(y_train)\nfor i in range(9):\n    print(i, y_train[y_train==i].count().min())","9bb150aa":"# Check the data for NaNs\nx_train.isnull().sum().sum(), x_test.isnull().sum().sum()   # .describe()","c6a9e544":"# Normalize the data\nx_train = x_train \/ 255.0\nx_test = x_test \/ 255.0","372c3fa8":"# Define image sizes and reshape to a 4-dim tensor\n\nIMG_H, IMG_W = 28, 28\nNO_CHANNELS = 1           # for greyscale images\n\n# Reshape to a 3-dim tensor\nx_train = x_train.values.reshape(-1, IMG_H, IMG_W, NO_CHANNELS)\nx_test = x_test.values.reshape(-1, IMG_H, IMG_W, NO_CHANNELS)\n\n\n# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\ny_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=10)\n\nprint('Tensor shape (train): ', x_train.shape)\nprint('Tensor shape (test): ', x_test.shape)\nprint('Tensor shape (target ohe): ', y_train_ohe.shape)","6e38221f":"# Split the train and the validation set for the fitting\n\nx_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train_ohe, test_size=0.15, random_state=SEED)\n\nprint('Tensors shape (train):', x_tr.shape, y_tr.shape)\nprint('Tensors shape (valid):', x_val.shape, y_val.shape)","98180329":"# visualize some examples\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):  \n    plt.subplot(2, 5, i+1)\n    plt.imshow(x_train[i].reshape((28,28)), cmap=plt.cm.binary)\n    plt.axis('off')\nplt.subplots_adjust(wspace=-0.1, hspace=-0.1)\nplt.show()","820002fa":"# Build CNN model \n# CNN architecture: In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\n# see original: https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6 \n\n\ndef build_model():\n    \n    fs = config.filters         # 32\n    k1 = config.kernel_1        #[(5,5), (3,3)]\n    k2 = config.kernel_2        # [(5,5), (3,3)]\n    pad = config.padding\n    activ = config.activation   # 'relu'\n    pool = config.pooling       # (2,2)\n    dp = config.dropout         # 0.25\n    dp_out = config.dropout_f   # 0.5\n    dense_units = config.dense_units  # 256\n    \n    inp = Input(shape=(IMG_H, IMG_W, NO_CHANNELS))     # x_train.shape[1:]\n    \n    # layer-1:: CNN-CNN-(BN)-Pool-dp\n    x = Conv2D(filters=fs, kernel_size=k1, padding=pad, activation=activ)(inp)\n    x = Conv2D(filters=fs, kernel_size=k1, padding=pad, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(dp)(x)    \n    \n    # layer-2:: CNN-CNN-(BN)-Pool-dp\n    x = Conv2D(filters=fs*2, kernel_size=k2, padding=pad, activation=activ)(inp)\n    x = Conv2D(filters=fs*2, kernel_size=k2, padding=pad, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n    x = Dropout(dp)(x)  \n    \n    x = Flatten()(x)\n    #     x = GlobalAveragePooling2D()(x)\n    \n    # FC head\n    x = Dense(dense_units, activation=activ)(x)\n    x = Dropout(dp_out)(x)\n    \n    out = Dense(10, activation=\"softmax\")(x)\n    \n    model = tf.keras.models.Model(inp, out)\n    \n    print(model.summary())\n    return model\n\n","78dc76b7":"# you may also experiment with this arhitecture (see credits)\n# https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\/notebook#Train-15-CNNs\n\ndef build_lenet():\n    \n    fs = config.filters         # 32\n    #     k1 = config.kernel_1        #[(5,5), (3,3)]\n    #     k2 = config.kernel_2        # [(5,5), (3,3)]\n    #     pad = config.padding\n    activ = config.activation   # 'relu'\n    dp = config.dropout         # 0.25\n    \n    \n    inp = Input(shape=(28, 28, 1)) \n    \n    \n    \n    x = Conv2D(fs, kernel_size = 3, activation=activ)(inp)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs, kernel_size = 3, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs, kernel_size = 5, strides=2, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    \n    x = Conv2D(fs*2, kernel_size = 3, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs*2, kernel_size = 3, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs*2, kernel_size = 5, strides=2, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    \n    x = Conv2D(128, kernel_size = 4, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Flatten()(x)\n    x = Dropout(0.4)(x)\n    out = Dense(10, activation='softmax')(x)\n    \n    model = tf.keras.models.Model(inp, out)\n\n    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n    #     model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model","e5803319":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"API_key\")","a76b965e":"!wandb login $api_key","30b36205":"hyperparams = dict(\n    filters = 64,\n    kernel_1 = (5,5),\n    kernel_2 = (3,3),\n    padding = 'same',\n    pooling = (2,2),\n    lr = 0.001,\n    wd = 0.0,\n    lr_schedule = 'RLR',    # cos, cyclic, step decay\n    optimizer = 'Adam+SWA',     # RMS\n    dense_units = 256,\n    activation = 'elu',      # elu, LeakyRelu\n    dropout = 0.25,\n    dropout_f = 0.5,\n    batch_size = BATCH_SIZE,\n    epochs = EPOCHS,\n)","a2a20931":"wandb.init(project=\"kaggle-mnist\", config=hyperparams)\nconfig = wandb.config","ae648a98":"config.keys()","e73fb0c0":"model = build_model()","befc4d61":"# Define the optimizer\n\nLR = config.lr     # 0.001\n\nif config.optimizer=='Adam':\n    opt = Adam(LR)\nelif config.optimizer=='RMS':\n    opt = RMSprop(lr=LR, rho=0.9, epsilon=1e-08, decay=0.0)\nelif config.optimizer=='Adam+SWA':\n    opt = Adam(LR)\n    opt = tfa.optimizers.SWA(opt)\nelse: \n    opt = 'adam'    # native adam optimizer \n    \n    \n# Compile the model\nmodel.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","ae9a31fc":"# mdl_path = '..\/input\/output\/'\n\ncallbacks = [\n    EarlyStopping(monitor='val_accuracy', patience=15, verbose=1),\n    #    ModelCheckpoint(mdl_path+'best_model.h5')\n    ReduceLROnPlateau(monitor='val_accuracy', patience=10, verbose=1, factor=0.5, min_lr=1e-4), \n    WandbCallback(monitor='val_loss')\n]","d316db2a":"if DATA_AUGM:\n    \n    # copied data generator \n    \n    datagen = ImageDataGenerator(\n        featurewise_center=False,                  # set input mean to 0 over the dataset\n        samplewise_center=False,                   # set each sample mean to 0\n        featurewise_std_normalization=False,       # divide inputs by std of the dataset\n        samplewise_std_normalization=False,        # divide each input by its std\n        zca_whitening=False,                       # apply ZCA whitening\n        rotation_range=10,                         #  randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1,                          #  Randomly zoom image \n        width_shift_range=0.1,                     # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,                    # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,                     # randomly flip images\n        vertical_flip=False)                       # randomly flip images\n\n    datagen.fit(x_train)","719336b0":"# # Fit the model\n\n# if DATA_AUGM:\n#     # With data augmentation to prevent overfitting (accuracy: 0.99286)\n#     hist = model.fit_generator(datagen.flow(x_train, y_train_ohe, batch_size=BATCH_SIZE),\n#                                epochs=EPOCHS, \n#                                validation_data=(x_val, y_val),\n#                                verbose=1, \n#                                steps_per_epoch=x_train.shape[0] \/\/ BATCH_SIZE, \n#                                callbacks=callbacks)\n    \n# else: \n#     # Without data augmentation (accuracy: 0.98114)\n#     hist = model.fit(x_train, y_train_ohe, \n#                      batch_size=config.batch_size,    # BATCH_SIZE, \n#                      epochs=config.epochs,            # EPOCHS, \n#                      validation_data=(x_val, y_val), \n#                      callbacks=callbacks,\n#                      verbose=1) ","7394528f":"hist = model.fit(x_train, y_train_ohe, \n                     batch_size=config.batch_size,    # BATCH_SIZE, \n                     epochs=config.epochs,            # EPOCHS, \n                     validation_data=(x_val, y_val), \n                     callbacks=callbacks,\n                     verbose=1) ","9aa3bdd4":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(1,2, figsize=(12, 6))\nax[0].plot(hist.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(hist.history['val_loss'], color='r', label=\"Validation loss\")\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","c0644da2":"# Predict the values from the validation dataset\nY_pred = model.predict(x_val)\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred, axis=1) \n\n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val, axis=1) \n\n# compute the confusion matrix\ncm = confusion_matrix(Y_true, Y_pred_classes) \n\n# plot the confusion matrix\nplot_confusion_matrix(cm, classes=range(10)) ","cf49f9c3":"# Confusion Matrix\nwandb.sklearn.plot_confusion_matrix(Y_true, Y_pred_classes, labels=range(10))","6122bf0a":"# predict results\nresults = model.predict(x_test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"cnn_mnist.csv\",index=False)","0a3c0d43":"## Split Training and Validation set","4f8a069b":"# Configure WandB\n\n\n### 1. Start by setting up your free W&B account in Weights & Biases webpage (or sign-in if you already have one)\n\n\nHelpful commands to get started with W&B:\n\n- `import wandb` \u2013 Import the wandb library\n- `wandb.keras.WandbCallback()` \u2013 Fetch all model parameters and log them automatically to your W&B dashboard (for Keras\/TF)\n\n- `wandb.init()` - Initialize a new W&B run. Each run is single execution of the training script.\n\n- `wandb.log()` - Logs custom objects like images, videos, audio files, HTML, plots, point clouds etc.\n\n- `wandb.agent()` - start running an agent in your machine (used in sweep)\n\n- `wandb.config` \u2013 Save all your hyperparameters in a config object\n\n- `%%wandb` - Add this at the top of a cell to show model metrics live below the cell","b9c3d9d2":"#### [See Part 2 of the series on hyperparameter optimization using W&B here](https:\/\/www.kaggle.com\/imeintanis\/tutorial-cnn-keras-hyperparameter-opt-w-b-p2?scriptVersionId=40022482)","532cf3b8":"### 2. Make a New Project (or go to an existing one) and choose a framework (optional) e.g. Keras\n","b7e2b674":"# Master Params","71240edc":"## Reshape\n\nTrain and test images loaded as 1D vectors of 784 (28 x 28) values. We reshape all data to 4d-tensors of shape: Nx28x28x1","5a2c06a7":"## Data augmentation (optional)\n\nIn order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n\nFor example, the number is not centered The scale is not the same (some who write with big\/small numbers) The image is rotated...\n\nApproaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n\nBy applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.\n\n\nFor the data augmentation :\n\n- Randomly rotate some training images by 10 degrees\n- Randomly Zoom by 10% some training images\n- Randomly shift images horizontally by 10% of the width\n- Randomly shift images vertically by 10% of the height","42c854d5":"## Predict Test set & Submit\n\nAlthought the purpose of the kernel is for learning, let's test our model in the LB ","92ee1d9b":"## Set Callbacks","d5c4958a":"![image.png](attachment:image.png)","0cd4871f":"As we can see from the figure above the classes are quite balanced, that makes our problem and our lives bit easier :)","1dfe101c":"Lets's also create a confusion matrix with W&B so we can view it in our dashboard","1bfc3859":"# Evaluate model","0e4c4054":"# Tutorial: Track your Experiments with Weights & Biases (W&B)","901a492d":"# CNN Model\n\n\nThe model is a simple 2d-CNN build using the Keras Functional API (TF backend). The main building blocks and their usage are:\n\n* Convolutional (Conv2D) layer: It is like a set of learnable filters. I choosed to set 32 filters for the first two layers and 64 filters for the two last ones. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image. The CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n\n* Pooling (MaxPool2D) layer: This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n\n* Dropout: is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n\n* Activation function: 'relu' is the rectifier activation function, used to add non linearity to the model.\n\n* Flatten layer: is used to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional\/maxpool layers. It combines all the found local features of the previous convolutional layers.\n\n* Fully-connected (Dense) layers: to act as a (NN) classifier. Note that in the last layer (Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class.","3a24c02f":"# Introduction (purpose of kernel)","cbbdd18a":"# Helpers","6b860ebe":"### 4. Login to W&B\n\nFrom the command line or inside a nb cell run `!wandb login <your_API_key>` to login and initiate W&B\n\nor, use the `wandb.login()` function and copy and paste the API key when asked","d876db22":"Sicnce there are no NaN or missing values in the train and test datasets, we can safely proceed with the modelling...","728c85e1":"We can get a better sense by visualising some images..","93e123fc":"Let's check the class distribution before proceed..","5a73ea3b":"# Hyperparameter Optimization\n\n\n### [Go to Part 2: Hyperparameter Optimization](https:\/\/www.kaggle.com\/imeintanis\/tutorial-cnn-keras-hyperparameter-opt-w-b-p2?scriptVersionId=40022482)\n","45fca959":"# Resources\n\n- W&B tutorial: https:\/\/www.kaggle.com\/lavanyashukla01\/better-models-faster-with-weights-biases\/notebook\n- W&B docs sweep: https:\/\/docs.wandb.com\/sweeps\/python-api\n\n## Inspiring kernels \n- Chriss Deotte - [25 Million Images! [0.99757] MNIST](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist)\n\n- Yassine Ghouzam - [introduction-to-cnn-keras-0-997-top-6](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6)","7a1cb067":"### 5. Create the W&B configuration\n\nLet's make a dictionary with all the hyperparameters that we'd like to track! For CNN type models I usually keep track of the following params:\n\n- filters\n- kernel size\n- learning rate (init)\n- optimizer\n- dense units\n- dropout rate etc\n\nBut you can add as many others you like.","2e1d4216":"# Build and compile the model\n\nTo compile and eventually train our model, we need to set up\n\n- **a loss function** - to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\", which is an extension of BinaryCrossEntropy for n-classes.\n\n- **an optimisation algorithm (optimizer)** - that will try to minimize the loss function by iteratively changing the weights and biases of neurons\n\n- **a score metric (optional)** - \"accuracy\" is used is to evaluate the performance of our model (used only for evaluation).","03a5ebde":"![image.png](attachment:image.png)","cb607d21":"After you have your first run logged succesfully, you will be able to visualize the standard metrics in the wandb site\n\n![image.png](attachment:image.png)","0e6104c7":"## Normalisation","35a79ab8":"## Confusion Matrix","539af636":"From now on whenever we want to assign\/access a hp we'll use the config.variable accessor.\n\nLet's check that we have all the parameters defined...","e3f910b5":"### 3. Next, we select a framework to start working on. For our example here, we choose Keras.\n\n![image.png](attachment:image.png)\n\nNote: Keep aside your username and login token (API key)","1cdce0c7":"# Training model","126222eb":"W&B helps you visualize your model performance and predictions, find the best models fast and share insights learned from your models. Here are a few use cases in which W&B can be specially useful for Kagglers:\n\n* Track and compare models - track your experiments easily, test out hypotheses fast and iterate quickly to find the best model\n* Visualize model performance for debugging - see how models are performing in real time and debug them\n* Efficient hyperparameter search - find the best model faster using sweeps (distributed to many agents!)\n* Resource efficient model training - be efficient with model training and share key insights with my teammates and the Kaggle community\n\n\nAs an example we use the famous digits recognition problem (MNIST dataset) and a simple CNN model [Keras TF] (addapted by @yassineghouzam). After the standard data loading and preprocessing routines we will focus on the CNN model and how to build the configuration for the hyperparameter tuning.","a9cc5142":"# Data Preparation","847934b0":"### Please, if you found this notebook helpful or you just liked it, some upvotes would be very much appreciated :)"}}