{"cell_type":{"756d3f57":"code","aef6b4c0":"code","744e97f9":"code","31e84ee6":"code","79bb7fa3":"code","8484aa5c":"code","e058770f":"code","2d12595f":"code","b45f408f":"code","73ed54bc":"code","7d24eb3c":"code","e5d9880e":"code","d5a42ce9":"code","f183b95d":"code","4afba3a8":"code","92951504":"code","cd61b6b6":"code","fb811925":"markdown","1382b99e":"markdown"},"source":{"756d3f57":"# First, we install actual versions of TF and Keras\n!pip install --upgrade -q tensorflow\n!pip install -q keras","aef6b4c0":"# Importing necessary libs\nimport pandas as pd\nimport numpy as np\nimport hashlib\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom keras import models, layers, callbacks, metrics\nfrom keras.utils import np_utils","744e97f9":"# Let's read our data and take a look at it\n# We have 84 features with mixed dtypes and the Class target variable\ndata = pd.read_csv('..\/input\/adware-detection\/Adware_Multiclass_Classification.csv').drop(columns=['Unnamed: 0'])\nlen_data = len(data)\ndata","31e84ee6":"# Let's check our data for missing values and fill NAs with mode if missing values exist\nfor col in data.drop(columns=['Class']).columns:\n  if data[col].isna().any():\n    print(f\"Column {col} contains missing values\")\n    data[col] = data[col].fillna(data[col].mode().iloc[0])","79bb7fa3":"# We have many columns with Object dtype; let's apply one-hot encoding\n# (if the number of unique values is relatively small)\n# or hashing if there are many uniques\ncols_to_drop = []\n\nfor col in data.drop(columns=['Class']).columns:\n  if data[col].dtype == 'object':\n    print(f'Column {col} has {data[col].nunique()} values among {len_data}')\n\n    if data[col].nunique() < 25:\n      print(f'One-hot encoding of {col}')\n      one_hot_cols = pd.get_dummies(data[col])\n      for ohc in one_hot_cols.columns:\n        data[col + '_' + ohc] = one_hot_cols[ohc]\n    else:\n      print(f'Hashing of {col}')\n      data[col + '_hash'] = data[col].apply(lambda row: int(hashlib.sha1((col + \"_\" + str(row)).encode('utf-8')).hexdigest(), 16) % len_data)\n\n    cols_to_drop.append(col)","8484aa5c":"# Dropping non-hashed versions of columns from previous step\ndata = data.drop(columns=cols_to_drop)","e058770f":"# The target variable (Class) has 6 unique string values,\n# let's represent them with numerical labels\nclass_nums = {}\ni = 0\nfor cl in data.Class.unique():\n    class_nums[cl] = i\n    i += 1\n\ndata['Class_num'] = [class_nums[cl] for cl in data.Class.values]\ndata = data.drop(columns=['Class'])\ndata","2d12595f":"# We have quite many features, so there may be highly correlated ones;\n# having highly correlated features won't help our model, so let's drop them\n# We drop the features that have correlation coefficient between 0.9 and 1\n# We don't include 1 as every feature has CC=1 with itself\n# Code idea is taken from\n# https:\/\/stackoverflow.com\/questions\/17778394\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ncorr = data.drop(columns=[\"Class_num\"]).corr()\ncorr_top = corr.abs().unstack().sort_values(kind='quicksort')\ncorr_top = corr_top[corr_top > 0.9][corr_top < 1]\n\ncols_to_drop = [corr_top.index[i][0] for i in range(0, len(corr_top), 2)]\nprint(f\"Highly correlated features: {cols_to_drop}\")\ndata = data.drop(columns=cols_to_drop)\n\ndata","b45f408f":"# Neural network fits better on data with small range, so let's scale it\nss = StandardScaler()\ndata_scaled = pd.DataFrame(columns=data.drop(columns=['Class_num']).columns,\n                               data=ss.fit_transform(data.drop(columns=['Class_num']), data.Class_num))\ndata_scaled['Class_num'] = data.Class_num\ndata_scaled","73ed54bc":"# Separating the dataset into train, test and validation sets at the ratio of (70%\/15%\/15%)\nX_train, X_test, Y_train, Y_test = train_test_split(data_scaled.drop(columns=['Class_num']), data_scaled.Class_num,\n                                                   random_state=42, stratify=data_scaled.Class_num, train_size=0.7)\nX_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, random_state=42, stratify=Y_test, train_size=0.5)","7d24eb3c":"# Let's check the target distribution;\n# it's easy to notice that the '0' class samples prevail,\n# so it's reasonable to assign weigths to classes to compensate\n# the imbalance\ndata_scaled['Class_num'].value_counts()","e5d9880e":"class_weights = class_weight.compute_class_weight('balanced',\n                                                 classes=Y_train.unique(),\n                                                 y=Y_train.values)","d5a42ce9":"# Before fitting, we have to one-hot encode the target vectors as we have multiclass problem\nY_train = np_utils.to_categorical(Y_train, data_scaled.Class_num.nunique())\nY_test = np_utils.to_categorical(Y_test, data_scaled.Class_num.nunique())\nY_val = np_utils.to_categorical(Y_val, data_scaled.Class_num.nunique())","f183b95d":"# Let's construct our simple DNN with two hidden layers\nclf = models.Sequential()\nclf.add(layers.Dense(64, activation='relu', input_dim=data_scaled.shape[1]-1))\nclf.add(layers.Dense(64, activation='relu'))\nclf.add(layers.Dense(64, activation='relu'))\nclf.add(layers.Dense(data_scaled.Class_num.nunique(), activation='softmax'))\n\nclf.summary()","4afba3a8":"# The target metric is CategoricalAccuracy; it's accuracy\n# for multiclass data with one-hot encoded labels\nclf.compile(optimizer='adam',\n loss='categorical_crossentropy',\n metrics=[metrics.CategoricalAccuracy()])","92951504":"# Fitting the net with EarlyStopping to reduce the training time and avoid overfitting\nclf.fit(X_train, Y_train,\n        epochs=100, validation_data=(X_val, Y_val),\n        callbacks=[callbacks.EarlyStopping(monitor='val_categorical_accuracy',\n                                   patience=5,\n                                   verbose=1,\n                                   restore_best_weights=True,\n                                   mode='max')])","cd61b6b6":"# Finally evaluating our model on test data\nclf.evaluate(X_test, Y_test)","fb811925":"This is a simple solution for multiclass network samples classification based on Keras Perceptron.","1382b99e":"The accuracy of 77% is not very high, but the model does not predict majoritary class only (otherwise we would have accuracy=167304\/276095~=60.6%); we can further work on this model by adding layers and neurons, changing the activation functions, optimizer parameters, etc. Also, the field of feature engineering is open if you wish to dive deeper into the problem.\n\nThanks for your attention. Feel free to start the discussion :)"}}