{"cell_type":{"b2f1cf8f":"code","10fae1d7":"code","edb93dfd":"code","691fd080":"code","57352076":"code","477a4e1e":"code","158d4d89":"code","c8a713fa":"code","7a89d92e":"code","25920815":"code","74a08d6b":"code","50b805dd":"code","3b41cdb1":"code","6c52d906":"code","e21e94f8":"code","feb01f1e":"code","907e4001":"code","97571309":"code","dc98e3ed":"code","82986c20":"code","7add6bf0":"code","2e6b80ea":"code","8dd326b2":"code","08a2a63a":"code","cb870827":"code","2ddb7394":"code","cbf4f249":"code","353803c8":"code","79997ef4":"code","f1a6aa8f":"code","bc70a9c3":"code","84183bab":"code","235a068c":"code","1a7d1d79":"code","8ef70a70":"code","c7397abb":"code","a8294dd2":"code","55b3da8b":"code","483d9d6e":"code","ed6d5bc2":"markdown","5a0a6a8e":"markdown","aba13531":"markdown","8f9a65e4":"markdown","ef6c48e9":"markdown","edbc4738":"markdown","6dd99f16":"markdown","d709c625":"markdown","9a2e1d5f":"markdown","2a998ac3":"markdown","4b521cfe":"markdown","79272dd8":"markdown","8067f263":"markdown","762e5394":"markdown","39a6a107":"markdown","d6a4e0a5":"markdown","abbf1636":"markdown","fbc3a01d":"markdown","e1c43524":"markdown","49e6beca":"markdown","ba3e5ac7":"markdown","4dd2147c":"markdown","d68ece2c":"markdown","2a2481d7":"markdown","381039da":"markdown","b0317065":"markdown","3eccbe4f":"markdown","82d3fb88":"markdown","74aac02f":"markdown","8b0ff9e8":"markdown","a0f26dc0":"markdown","fcfb3d4f":"markdown","09dc7660":"markdown","253e9f80":"markdown","78d0914c":"markdown","74b01a8e":"markdown"},"source":{"b2f1cf8f":"!python -m spacy download en_core_web_lg","10fae1d7":"!pip install ptable","edb93dfd":"!pip install fuzzywuzzy","691fd080":"! pip install contractions","57352076":"import plotly.graph_objs as go\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.probability import FreqDist\nimport re\nimport contractions\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom fuzzywuzzy import fuzz\nimport string\nfrom wordcloud import WordCloud\nimport spacy\nfrom spacy.matcher import Matcher\nimport plotly.express as px\nfrom prettytable import PrettyTable\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport numpy as np # linear algebra\nimport pandas as pd \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","477a4e1e":"import nltk\nnltk.download(\"all\")\nfrom nltk.corpus import stopwords as sw\nfrom wordcloud import WordCloud, STOPWORDS","158d4d89":"episodes = pd.read_csv('..\/input\/chai-time-data-science\/Episodes.csv',parse_dates=['recording_date','release_date'])\nyoutube_thumbnails = pd.read_csv('..\/input\/chai-time-data-science\/YouTube Thumbnail Types.csv')\nanchor_thumbnails = pd.read_csv('..\/input\/chai-time-data-science\/Anchor Thumbnail Types.csv')\ndescription = pd.read_csv('..\/input\/chai-time-data-science\/Description.csv')","c8a713fa":"def conv_to_sec(time):\n    \"\"\" Time to seconds \"\"\"\n    \n    t_list = time.split(':')\n         \n    if(len(t_list) == 2):\n        m = t_list[0]\n        s = t_list[1]\n        time_sec = (int(m)*60 + int(s))\n    else:\n        h = t_list[0]\n        m = t_list[1]\n        s = t_list[2]\n        time_sec = (int(h)*60*60 + int(m)*60 + int(s)) \n    \n    return time_sec\n\n\ndef add_columns(sub, episode_id):\n    \"\"\" Transform the transcript of the given episode \"\"\"\n    \n    # create the time second feature that converts the time into the unified qty. of seconds\n    sub['Time_sec'] = sub['Time'].apply(conv_to_sec)\n    \n        \n    # providing an identity to each transcript\n    sub['Episode_Id'] = episode_id\n    sub = sub[['Episode_Id', 'Time', 'Time_sec',  'Speaker', 'Text']]\n    \n    return sub\n\ndef get_all_texts(path):\n    \"\"\" Combine all the 75 transcripts of the ML Heroes Interviews together as one dataframe \"\"\"\n    \n    episodes = []\n    for i in range(1,76):\n        name = 'E'+str(i)\n        csv_file = name +'.csv'\n        try:            \n            sub_epi = pd.read_csv(os.path.join(path, csv_file))            \n            sub_epi = add_columns(sub_epi, (name))            \n            episodes.append(sub_epi)            \n        except:            \n            continue\n            \n    return(pd.concat(episodes, ignore_index=True))\n\n# create the combined transcript dataset\npath = \"..\/input\/chai-time-data-science\/Cleaned Subtitles\"\nrecord_texts = get_all_texts(path)\nrecord_texts.head(5)","7a89d92e":"episodes.describe(include=\"all\").T #transpose is used so that we can view all columns","25920815":"#Separating the interview episodes and the learning series\ndescription_bool = description['episode_id'].str.startswith('E')\nepisode_bool = episodes['episode_id'].str.startswith('E')\nl_episode_bool = episodes['episode_id'].str.startswith('M')\n\ndesc = description[description_bool]\njust_episodes = episodes[episode_bool]\njust_learning_episodes = episodes[l_episode_bool]","74a08d6b":"#pre processing methods\ndef clean_the_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    # Expanding Contractions eg. i've to I have\n    text = contractions.fix(text)\n    text = re.sub(r\".*native.*\\n?\",\"\",text)    \n    text = re.sub('Hey.+science.', '', text)\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n          \n    text = re.sub(r\".*Audio.*\\n?\",\"\",text)    \n    text = re.sub(r\".*sanyambhutani.com.*\\n?\",\"\",text)\n    text = re.sub(r\".*Newsletter.*\\n?\",\"\",text)\n    text = re.sub(r\".*so much.*\\n?\",\"\",text)\n    text = re.sub(r\".*Sanyam Bhutani:.*\\n?\",\"\",text)\n    text = re.sub(r\".*Subscribe.*\\n?\",\"\",text)\n    text = re.sub(r\".*twitter.*\\n?\",\"\",text)\n    text = re.sub(r\".*Flow.*\\n?\",\"\",text)\n    text = re.sub(r\".*Follow.*\\n?\",\"\",text)\n    text = re.sub(r\".*Playlist.*\\n?\",\"\",text)\n    text = re.sub(r\".*support.*\\n?\",\"\",text)\n    text = re.sub(r\".*Linkedin.*\\n?\",\"\",text)\n    text = re.sub(r\".*Kaggle:.*\\n?\",\"\",text)\n    text = re.sub(r\".*hosted.*\\n?\",\"\",text)\n    text = re.sub(r\"interview[s|ed|ing]\",\"\",text,re.IGNORECASE)\n    text = re.sub(r\".*track.*\\n?\",\"\",text)\n    text = re.sub(r\".*available.*\\n?\",\"\",text)\n    text = re.sub(r\".*Blog.*\\n?\",\"\",text)\n    text = re.sub(r\".*Correction.*\\n?\",\"\",text)\n    text = re.sub(r\".*Note.*\\n?\",\"\",text)\n    text = re.sub(r\".*About.*\\n?\",\"\",text)\n    #text = re.sub(r\".*YouTube.*.?\",\"\",text)\n    text = re.sub(r\".*lazy.*\\n?\",\"\",text)\n   \n    #text = re.sub(r\".*find.*.?\",\"\",text)\n    text = re.sub(r\".*Link[s].*\\n?\",\"\",text)\n    text = re.sub(r\".*KaggleDaysMeetup.*\\n?\",\"\",text)\n    \n    text = re.sub('\\n', ' ', text)\n    text = text.lower()\n    text = re.sub(r\"enjoy\",\"\",text)\n    text = re.sub(r\"show\",\"\",text)\n    text = re.sub(r\"even\",\"\",text)\n    text = re.sub(r\"another\",\"\",text)\n    text = re.sub(r\"time\",\"\",text)\n    text = re.sub(r\"part\",\"\",text)\n    text = re.sub(r\"sanyam\",\"\",text)\n    text = re.sub(r\"episode\",\"\",text)\n    text = re.sub(r\"interview\",\"\",text)\n    text = re.sub(r\"bhutani\",\"\",text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_the_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    common_words = set(['sure','everyone','chai','us','also','hello','find','check','currently','two','description','welcome','podcast','many','like','get','lot','please','hi','heres','really','one'])\n    stop_wds = common_words.union(sw.words('english'))\n    remove_stopwords = [w for w in tokenized_text if w not in stop_wds]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","50b805dd":"# Cleaning description to include only the guest details and counting the words and plotting \ndesc['processed_description'] = desc['description'].apply(str).apply(lambda x: text_preprocessing(x))\ndesc[\"description_len\"] = desc['processed_description'].map(lambda x : len(x.split(\" \")))\n\njust_episodes[\"title_length\"] = just_episodes[\"episode_name\"].apply(lambda x: len(x.split()))\nfig, ax = plt.subplots()\n_ = ax.scatter(x=just_episodes['youtube_views'], y=just_episodes['title_length'],  edgecolors=\"#000000\", linewidths=0.5)\n_ = ax.set(xlabel=\"Youtube Views\", ylabel=\"Title Length (in words)\")\n\nresult = pd.merge(desc, just_episodes, on=['episode_id'])\nfig, ax = plt.subplots()\n_ = ax.scatter(x=result['youtube_views'], y=result['description_len'],  edgecolors=\"#000000\", linewidths=0.5)\n_ = ax.set(xlabel=\"Youtube Views\", ylabel=\"Description Length (in words)\")","3b41cdb1":"fig = make_subplots(rows=2, cols=1, subplot_titles=(\"<b>Description Length over episodes<\/b>\",\"<b><\/b>\"))\nfig.append_trace(go.Scatter(name='<b>Description Length<\/b>', x=desc.episode_id, y=desc.description_len, marker_color='rgb(0, 102, 57)'), row=1, col=1),\n#fig.append_trace(go.Scatter(name='<b>Episode Duration<\/b>', x=just_episodes.episode_id, y=just_episodes.youtube_views, marker_color='rgb(100, 17, 53)'), row=2, col=1),\nfig.update_layout(height=400, width=800, legend_orientation=\"h\", plot_bgcolor='rgb(255,255,255)')\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","6c52d906":"print(\"Mean episode duration is {} minutes.\".format(round(just_episodes['episode_duration'].mean()\/60)))\n\nsorted_episodes = just_episodes.sort_values('youtube_views', ascending=False)\nhigh_viewed_episodes = sorted_episodes[:10]\nhigh_viewed_episodes = high_viewed_episodes[['episode_id','heroes','heroes_gender', 'episode_duration','youtube_views']]\n#print(high_viewed_episodes)\nprint(\"Average Duration for highly viewed episodes is {} minutes \".format(round(high_viewed_episodes['episode_duration'].mean()\/(60))))\n\nless_viewed_episodes = sorted_episodes[-9:]\nless_viewed_episodes = less_viewed_episodes[['episode_id','heroes','heroes_gender', 'episode_duration','youtube_views']]\n#print(less_viewed_episodes)\nprint(\"Average Duration for less viewed episodes is {} minutes  \".format(round(less_viewed_episodes['episode_duration'].mean()\/(60))))","e21e94f8":"# We are counting specific words in the host subtitles\ndef humoring_words_count(all_host_text):\n    words = 0\n    humoring_words = ['Awesome','cool',' ha ','Got it','Great','fan','amazing']\n    for i in range(len(humoring_words)):\n        words += all_host_text.count(humoring_words[i])\n    return words    \n    \nepisodes = []\nnum_questions = []\nnum_conversations = []\nqc_percentage = []\nquestions_asked = []\nhumoring_words_used = []\nhosts_talk = []\nguests_talk = []\nguest_introduction = []\n\nfor episode_id in list(record_texts['Episode_Id'].unique()):        \n    if (episode_id !='E69' and episode_id !='E0' ):        \n        episode = record_texts[record_texts['Episode_Id'] == episode_id]        \n        host_talk = episode[episode['Speaker']=='Sanyam Bhutani']\n        guest_talk = episode[episode['Speaker']!='Sanyam Bhutani'] \n        no_of_conversations = host_talk.shape[0]\n        all_host_text = ' '.join(host_talk['Text'])\n        all_guest_text = ' '.join(guest_talk['Text'])\n        \n        episodes.append(episode_id)        \n        \n        no_of_questions = all_host_text.count('?')\n        num_questions.append(no_of_questions)\n        num_conversations.append(no_of_conversations)\n        qlist = re.findall(r'\\b([a-zA-Z][^.!]*[?])',all_host_text)\n        qstring = ' '.join(str(e) for e in qlist)\n        questions_asked.append(qstring)\n        qc_percentage.append(round((no_of_questions\/no_of_conversations )* 100))\n        humoring_words_used.append(humoring_words_count(all_host_text))\n        hosts_talk.append(all_host_text)\n        guests_talk.append(all_guest_text)        \n        guest_introduction.append(host_talk['Text'][0:2].values[0])\n        \n    else:\n        continue\n        \n\n# make the dataframe\ntalk = pd.DataFrame({\n    'episode_id': episodes,\n    'num_host_questions': num_questions,\n    'num_host_conversations': num_conversations,    \n    'qc_percentage': qc_percentage,\n    'questions_asked':questions_asked,\n    'guest_introduction': guest_introduction,\n    'humoring_words_used': humoring_words_used  ,\n    'host_talk': hosts_talk,\n    'guest_talk': guests_talk    \n})\n\ntalk['guest_introduction_processed'] = talk['guest_introduction'].apply(str).apply(lambda x: text_preprocessing(x))","feb01f1e":"print(\"Average Question-to-Conversation ratio is {}\".format(round(talk['qc_percentage'].mean())))\n\nsorted_talks = talk.sort_values('qc_percentage', ascending=False)\nsorted_talks.head(3)","907e4001":"fig = make_subplots(rows=3, cols=1, subplot_titles=(\"<b>Number of Questions Asked<\/b>\", \"<b>Episode Duration<\/b>\",\"<b>Number of Humouring Words used<\/b>\"))\n\nfig.append_trace(go.Scatter(name='<b>Questions Asked<\/b>', x=talk.episode_id, y=talk.num_host_questions, marker_color='rgb(0, 102, 57)'), row=1, col=1),\nfig.append_trace(go.Scatter(name='<b>Episode Duration<\/b>', x=just_episodes.episode_id, y=just_episodes.episode_duration, marker_color='rgb(100, 17, 53)'), row=2, col=1),\nfig.append_trace(go.Scatter(name='<b>Humouring Words<\/b>', x=talk.episode_id, y=talk.humoring_words_used, marker_color='rgb(0, 200, 157)'), row=3, col=1),\n\nfig.update_layout(height=800, width=900, legend_orientation=\"h\", plot_bgcolor='rgb(255,255,255)')\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","97571309":"# Cleaning the text\ntalk['guest_introduction_processed'] = talk['guest_introduction'].apply(str).apply(lambda x: text_preprocessing(x))\n\n# We will look at Frequency Distribution of words used in introduction\nguest_intro = []\nfor s in talk['guest_introduction_processed']:\n    guest_intro.append(sent_tokenize(s))   \n\ngi_string = ''.join(str(gi) for gi in guest_intro)\ngi_string = re.sub('[^a-zA-Z]', ' ', gi_string )\ngi_tokenized_word = nltk.word_tokenize(gi_string)\n\ngi_fdist = FreqDist(gi_tokenized_word)\nprint(gi_fdist.most_common(100))\n\n%matplotlib inline\ngi_fdist.plot(30, cumulative=False,title = \"Common words used in guest introduction\")\nplt.show()","dc98e3ed":"# We are using spacy to match words\nnlp = spacy.load('en_core_web_lg')\n\ndef clean_by_spacy(text):\n    url_reg  = r'[a-z]*[:.]+\\S+'\n    text   = re.sub(url_reg, '', text)\n    noise_reg = r'\\&amp'\n    text   = re.sub(noise_reg, '', text)\n    return text\n\ncleaned_text = nlp(clean_by_spacy(talk['guest_talk'][55]))#Mark Landry\tE57\nmatcher = Matcher(nlp.vocab)\nmatched_sents = [] # collect data of matched sentences to be visualized\n\ndef collect_sents(matcher, doc, i, matches, label='MATCH'):\n    \"\"\"\n    Function to help reformat data for displacy visualization\n    \"\"\"\n    match_id, start, end = matches[i]\n    span = doc[start : end]  # matched span\n    sent = span.sent  # sentence containing matched span\n    \n    # append mock entity for match in displaCy style to matched_sents\n    \n    if doc.vocab.strings[match_id] == 'ADVICE':  \n        match_ents = [{'start': span.start_char - sent.start_char,\n                   'end': span.end_char - sent.start_char,\n                   'label': 'ADVICE'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n       \n    elif doc.vocab.strings[match_id] == 'STORY':  \n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'STORY'}]        \n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n        \n    elif doc.vocab.strings[match_id] == 'KAGGLE': \n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'KAGGLE'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })        \n        \n    elif doc.vocab.strings[match_id] == 'IDEA':  \n        match_ents = [{'start': span.start_char - sent.start_char,\n                   'end': span.end_char - sent.start_char,\n                   'label': 'IDEA'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n        \n    elif doc.vocab.strings[match_id] == 'CHALLENGE':  \n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'CHALLENGE'}]        \n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n        \n    elif doc.vocab.strings[match_id] == 'PROJECT':  \n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'PROJECT'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })          \n    \n# declare different patterns\nadvice_pattern = [{'LOWER': 'advice'}, {\"IS_ALPHA\": True} ]\nstory_pattern = [{'LOWER': 'story'}, {\"IS_PUNCT\": True}]\nkaggle_pattern = [{'LOWER': 'kaggle'}, {\"IS_PUNCT\": True}]\nidea_pattern = [{'LOWER': 'idea'}, {\"IS_PUNCT\": True} ]\nchallenge_pattern = [{'LOWER': 'challenge'}, {\"IS_PUNCT\": True}]\nproject_pattern = [{'LOWER': 'project'}, {\"IS_PUNCT\": True}]\n\nmatcher.add('ADVICE', collect_sents, advice_pattern)  # add pattern\nmatcher.add('STORY', collect_sents, story_pattern)  # add pattern\nmatcher.add('KAGGLE', collect_sents, kaggle_pattern)  # add pattern\nmatcher.add('IDEA', collect_sents, idea_pattern)  # add pattern\nmatcher.add('CHALLENGE', collect_sents, challenge_pattern)  # add pattern\nmatcher.add('PROJECT', collect_sents, project_pattern)  # add pattern\n\nmatches = matcher(cleaned_text)\nspacy.displacy.render(matched_sents, style='ent', manual=True, jupyter=True,  \n                      options = {'colors': {'STORY': '#ded1e8', 'ADVICE': '#cc2936', 'KAGGLE':'#f2cd5d','IDEA': '#5290c8', 'CHALLENGE': '#cb9936', 'PROJECT':'#d27571'}})\nprint(\"Matched Sentences: \",matched_sents)","82986c20":"cleaned_text = nlp(clean_by_spacy(talk['guest_talk'][48]))#Walter Reade E50\nmatcher = Matcher(nlp.vocab)\nmatched_sents = [] # collect data of matched sentences to be visualized\n\ndef collect_sents(matcher, doc, i, matches, label='MATCH'):\n    \"\"\"\n    Function to help reformat data for displacy visualization\n    \"\"\"\n   \n    match_id, start, end = matches[i]\n    span = doc[start : end]  # matched span\n    sent = span.sent  # sentence containing matched span\n    \n    # append mock entity for match in displaCy style to matched_sents\n    \n    if doc.vocab.strings[match_id] == 'ADVICE':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n                   'end': span.end_char - sent.start_char,\n                   'label': 'ADVICE'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n       \n    elif doc.vocab.strings[match_id] == 'STORY':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'STORY'}]        \n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n        \n    elif doc.vocab.strings[match_id] == 'KAGGLE':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'KAGGLE'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })        \n        \n    elif doc.vocab.strings[match_id] == 'IDEA':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n                   'end': span.end_char - sent.start_char,\n                   'label': 'IDEA'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n        \n    elif doc.vocab.strings[match_id] == 'CHALLENGE':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'CHALLENGE'}]        \n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n        \n    elif doc.vocab.strings[match_id] == 'PROJECT':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'PROJECT'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })          \n    \n# declare different patterns\nadvice_pattern = [{'LOWER': 'advice'}, {\"IS_ALPHA\": True} ]\nstory_pattern = [{'LOWER': 'story'}, {\"IS_PUNCT\": True}]\nkaggle_pattern = [{'LOWER': 'kaggle'}, {\"IS_PUNCT\": True}]\nidea_pattern = [{'LOWER': 'idea'}, {\"IS_PUNCT\": True} ]\nchallenge_pattern = [{'LOWER': 'challenge'}, {\"IS_PUNCT\": True}]\nproject_pattern = [{'LOWER': 'project'}, {\"IS_PUNCT\": True}]\n\nmatcher.add('ADVICE', collect_sents, advice_pattern)  # add pattern\nmatcher.add('STORY', collect_sents, story_pattern)  # add pattern\nmatcher.add('KAGGLE', collect_sents, kaggle_pattern)  # add pattern\nmatcher.add('IDEA', collect_sents, idea_pattern)  # add pattern\nmatcher.add('CHALLENGE', collect_sents, challenge_pattern)  # add pattern\nmatcher.add('PROJECT', collect_sents, project_pattern)  # add pattern\n\nmatches = matcher(cleaned_text)\nspacy.displacy.render(matched_sents, style='ent', manual=True, jupyter=True,  \n                      options = {'colors': {'STORY': '#ded1e8', 'ADVICE': '#cc2936', 'KAGGLE':'#f2cd5d','IDEA': '#5290c8', 'CHALLENGE': '#cb9936', 'PROJECT':'#d27571'}})\nprint(matched_sents)","7add6bf0":"#https:\/\/medium.com\/analytics-vidhya\/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\ncorpus = talk['guest_talk'].values\n\nstop_words = set(sw.words('english'))\ncommon_words = ['people','amazing','lot','obviously','often','hours','yeah','think','may','hero','guess','super','gonna','link','also','say','couple','let','science','like','usually','really','kind','first','certain','um','episode','Length','podcast','chai','sanyam','bhutani','playlist','Name','audio','version','here','available','episod','list','description']\nall_stop_words = stop_words.union(set(common_words))\n\ncv = CountVectorizer(max_df=0.7,stop_words = all_stop_words, analyzer='word', min_df=1,max_features=None, ngram_range=(1,3))\nX = cv.fit_transform(corpus)\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True,sublinear_tf=False, norm=None)\ntfidf_transformer.fit(X)\n\n# get feature names\nfeature_names=cv.get_feature_names()\n \n# fetch document for which keywords needs to be extracted\ndoc = \" \".join(talk['guest_talk'])\n#print((doc))\n\n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n\n#Function for sorting tf_idf in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn] \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]    \n    return results\n\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n\n#extract only the top 30\nkeywords=extract_topn_from_vector(feature_names,sorted_items,30)\n \n# now print the results\nprint(\"\\nFollowing are 30 Keywords: \",\" \".join(keywords))\n#for k in keywords:\n    #print(k,keywords[k])    ","2e6b80ea":"print(\"Ratio of Women-to-Men heroes is \",just_episodes.groupby('heroes_gender')['episode_id'].count()[0]\/(just_episodes.groupby('heroes_gender')['episode_id'].count()[0] + just_episodes.groupby('heroes_gender')['episode_id'].count()[1]))\nprint(\"Ratio of Women-to-Men Youtube Views is\",just_episodes.groupby('heroes_gender')['youtube_views'].sum()[0]\/(just_episodes.groupby('heroes_gender')['youtube_views'].sum()[0]+just_episodes.groupby('heroes_gender')['youtube_views'].sum()[1]))\nprint(\"Ratio of Women-to-Men Youtube Subscribers is\",just_episodes.groupby('heroes_gender')['youtube_subscribers'].sum()[0]\/(just_episodes.groupby('heroes_gender')['youtube_subscribers'].sum()[0]+just_episodes.groupby('heroes_gender')['youtube_subscribers'].sum()[1]))\nprint(\"Ratio of Women-to-Men Youtube Non Impression Views is\",just_episodes.groupby('heroes_gender')['youtube_nonimpression_views'].sum()[0]\/(just_episodes.groupby('heroes_gender')['youtube_nonimpression_views'].sum()[0]+just_episodes.groupby('heroes_gender')['youtube_nonimpression_views'].sum()[1]))\nprint(\"Ratio of Women-to-Men Youtube Impression Views is\",just_episodes.groupby('heroes_gender')['youtube_impression_views'].sum()[0]\/(just_episodes.groupby('heroes_gender')['youtube_impression_views'].sum()[0]+just_episodes.groupby('heroes_gender')['youtube_impression_views'].sum()[1]))\nprint(\"Ratio of Women-to-Men Youtube Click-Through-Rate is\",just_episodes.groupby('heroes_gender')['youtube_ctr'].sum()[0]\/(just_episodes.groupby('heroes_gender')['youtube_ctr'].sum()[0]+just_episodes.groupby('heroes_gender')['youtube_ctr'].sum()[1]))\nprint(\"Ratio of Women-to-Men Apple listeners is\",just_episodes.groupby('heroes_gender')['apple_listeners'].sum()[0]\/(just_episodes.groupby('heroes_gender')['apple_listeners'].sum()[0]+just_episodes.groupby('heroes_gender')['apple_listeners'].sum()[1]))\nprint(\"Ratio of Women-to-Men Spotify listeners is\",just_episodes.groupby('heroes_gender')['spotify_listeners'].sum()[0]\/(just_episodes.groupby('heroes_gender')['spotify_listeners'].sum()[0]+just_episodes.groupby('heroes_gender')['spotify_listeners'].sum()[1]))","8dd326b2":"print(\"YT Subscribers that channel has earned - grouped by nationality \",just_episodes.groupby('heroes_nationality')['youtube_subscribers'].sum())\nprint(\"YT Views that episodes have garnered - grouped by nationality \",just_episodes.groupby('heroes_nationality')['youtube_views'].sum())\nprint(\"YT Subscribers that channel has earned - grouped by location \",just_episodes.groupby('heroes_location')['youtube_subscribers'].sum())\nprint(\"YT Views that episodes have garnered - grouped by location \",just_episodes.groupby('heroes_location')['youtube_views'].sum())","08a2a63a":"just_learning_episodes[['episode_id','release_date','youtube_subscribers','youtube_likes','youtube_comments','youtube_impression_views','youtube_views','youtube_avg_watch_duration','apple_avg_listen_duration','apple_listeners','spotify_listeners']]","cb870827":"#calculating most viewed and least viewed 10 episodes.\nsorted_episodes = just_episodes.sort_values('youtube_views', ascending=False)\nepisodes_top10 = sorted_episodes[:10]\nepisodes_top10 = episodes_top10[['episode_id','heroes','heroes_gender','heroes_location', 'heroes_nationality','youtube_views']]\ntop_episode_ids = episodes_top10['episode_id']\n\nbottom = sorted_episodes[-9:]\nbottom = bottom[['episode_id','heroes','heroes_gender','heroes_location', 'heroes_nationality','youtube_views']]\nlow_episode_ids = bottom['episode_id']    ","2ddb7394":"tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\nemotion = pd.read_csv('..\/input\/emodata\/Emotion-Lexicon.txt', sep='\\t')\nfirst_run = False # Each run takes 75 minutes, so the dataframe is in pickl files.\n\nif first_run == True:\n    joy_array = []\n    trust_array = []\n    anticipation_array = []\n    surprise_array = []\n    episode_array = []\n    token_array = []\n\n    elements = emotion['word'].values\n    sentences_emotions = []\n    for id in top_episode_ids:\n        print(\"-\"*60)\n        guest_talk = []\n        \n        sentences_emotions = []\n    \n        joy = 0\n        trust = 0\n        anticipation = 0\n        surprise = 0\n        tokens = 0\n \n        chosen_talk = talk[talk['episode_id'] == id]\n        guest_talk = \" \".join(chosen_talk['guest_talk'])\n        for word in tokenizer.tokenize(guest_talk.lower()):        \n            sentences_emotions.append(word)\n    \n        tokens = len(sentences_emotions)    \n        \n        for (i, wd) in enumerate(sentences_emotions):\n            for (j, element) in enumerate(elements):        \n                if fuzz.ratio(element, wd) >= 94:            \n                    if emotion['emotion'][j] == 'trust':\n                        trust += 1\n                    elif emotion['emotion'][j] == 'surprise':\n                        surprise += 1\n                    elif emotion['emotion'][j] == 'anticipation':\n                        anticipation += 1\n                    elif emotion['emotion'][j] == 'joy':\n                        joy += 1\n    \n        episode_array.append(id)\n        token_array.append(tokens)\n        anticipation_array.append(anticipation)\n        surprise_array.append(surprise)\n        trust_array.append(trust)\n        joy_array.append(joy)\n        print(\"Episodes: \",episode_array)\n        print(\"Num of Tokens: \",token_array) \n        print(\"Num of 'Anticipation': \",anticipation_array )\n        print(\"Num of 'Surprise': \",surprise_array )\n        print(\"Num of 'Trust': \",trust_array )\n        print(\"Num of 'Joy': \",joy_array )\n        print(\"=\"*60)\n\n    # make the dataframe\n    top_videos_emotions = pd.DataFrame({\n        'episodes_array': episode_array,\n        'num_tokens':token_array,\n        'num_anticipation': anticipation_array,\n        'num_surprise': surprise_array,\n        'num_trust': trust_array,\n        'num_joy': joy_array    \n    })\n\n    # display first 10 rows\n    top_videos_emotions.head(2)\n    pd.to_pickle(top_videos_emotions, \".\/top_videos_emotions.pkl\")","cbf4f249":"if first_run == True:    \n    joy_array = []\n    trust_array = []\n    anticipation_array = []\n    surprise_array = []\n    episode_array = []\n    token_array = []\n\n    elements = emotion['word'].values\n    sentences_emotions = []\n    for id in low_episode_ids:\n        print(\"-\"*60)\n        guest_talk = []        \n        sentences_emotions = []\n    \n        joy = 0\n        trust = 0\n        anticipation = 0\n        surprise = 0\n        tokens = 0\n \n        chosen_talk = talk[talk['episode_id'] == id]\n        guest_talk = \" \".join(chosen_talk['guest_talk'])\n        for word in tokenizer.tokenize(guest_talk.lower()):        \n            sentences_emotions.append(word)\n    \n        tokens = len(sentences_emotions)    \n        \n        for (i, wd) in enumerate(sentences_emotions):\n            for (j, element) in enumerate(elements):        \n                if fuzz.ratio(element, wd) >= 94:            \n                    if emotion['emotion'][j] == 'trust':\n                        trust += 1\n                    elif emotion['emotion'][j] == 'surprise':\n                        surprise += 1\n                    elif emotion['emotion'][j] == 'anticipation':\n                        anticipation += 1\n                    elif emotion['emotion'][j] == 'joy':\n                        joy += 1\n    \n        episode_array.append(id)\n        token_array.append(tokens)\n        anticipation_array.append(anticipation)\n        surprise_array.append(surprise)\n        trust_array.append(trust)\n        joy_array.append(joy)\n        print(\"Episodes: \",episode_array)\n        print(\"Num of Tokens: \",token_array) \n        print(\"Num of 'Anticipation': \",anticipation_array )\n        print(\"Num of 'Surprise': \",surprise_array )\n        print(\"Num of 'Trust': \",trust_array )\n        print(\"Num of 'Joy': \",joy_array )\n        print(\"=\"*60)\n\n    # make the dataframe\n    min_videos_emotions = pd.DataFrame({\n        'episodes_array': episode_array,\n        'num_tokens':token_array,\n        'num_anticipation': anticipation_array,\n        'num_surprise': surprise_array,\n        'num_trust': trust_array,\n        'num_joy': joy_array    \n    })\n\n    # display first 10 rows\n    min_videos_emotions.head(2)\n    pd.to_pickle(min_videos_emotions, \".\/min_videos_emotions.pkl\")","353803c8":"def plot_stacked_bar(data, series_labels, category_labels=None, \n                     show_values=False, value_format=\"{}\", y_label=None, \n                     colors=None, grid=True, reverse=False):\n    \"\"\"Plots a stacked bar chart with the data and labels provided.\n\n    Keyword arguments:\n    data            -- 2-dimensional numpy array or nested list\n                       containing data for each series in rows\n    series_labels   -- list of series labels (these appear in\n                       the legend)\n    category_labels -- list of category labels (these appear\n                       on the x-axis)\n    show_values     -- If True then numeric value labels will \n                       be shown on each bar\n    value_format    -- Format string for numeric value labels\n                       (default is \"{}\")\n    y_label         -- Label for y-axis (str)\n    colors          -- List of color labels\n    grid            -- If True display grid\n    reverse         -- If True reverse the order that the\n                       series are displayed (left-to-right\n                       or right-to-left)\n    \"\"\"\n\n    ny = len(data[0])\n    ind = list(range(ny))\n\n    axes = []\n    cum_size = np.zeros(ny)\n\n    data = np.array(data)\n\n    if reverse:\n        data = np.flip(data, axis=1)\n        category_labels = reversed(category_labels)\n\n    for i, row_data in enumerate(data):\n        color = colors[i] if colors is not None else None\n        axes.append(plt.bar(ind, row_data, bottom=cum_size, \n                            label=series_labels[i], color=color))\n        cum_size += row_data\n\n    if len(category_labels) != 0:\n        plt.xticks(ind, category_labels)\n\n    if y_label:\n        plt.ylabel(y_label)\n\n    plt.legend()\n\n    if grid:\n        plt.grid()\n\n    if show_values:\n        for axis in axes:\n            for bar in axis:\n                w, h = bar.get_width(), bar.get_height()\n                plt.text(bar.get_x() + w\/2, bar.get_y() + h\/2, \n                         value_format.format(h), ha=\"center\", \n                         va=\"center\")\n                \nplt.figure(figsize=(12, 10))\nseries_labels = ['Anticipation', 'Trust','Surprise','Joy']\n\ndata = [\n   unpickled_top_videos_emotions['num_anticipation'], \n    unpickled_top_videos_emotions['num_trust'],\n    unpickled_top_videos_emotions['num_surprise'],\n    unpickled_top_videos_emotions['num_joy']\n]\n\ncategory_labels = top_videos_emotions['episodes_array']\n\nplot_stacked_bar(\n    data, \n    series_labels, \n    category_labels=category_labels, \n    show_values=True, \n    value_format=\"{:.1f}\",\n    colors=['tab:orange', 'tab:green', 'tab:blue', 'tab:red'],\n    y_label=\"Number of words\"\n)\nplt.title('Emotional appeal of top episodes')\nplt.savefig('top_episodes_emotions.png')\nplt.show()","79997ef4":"plt.figure(figsize=(12, 10))\nseries_labels = ['Anticipation', 'Trust','Surprise','Joy']\n\ndata = [\n   unpickled_min_videos_emotions['num_anticipation'], \n    unpickled_min_videos_emotions['num_trust'], \n    unpickled_min_videos_emotions['num_surprise'],\n    unpickled_min_videos_emotions['num_joy']\n]\n\ncategory_labels = unpickled_min_videos_emotions['episodes_array']\n\nplot_stacked_bar(\n    data, \n    series_labels, \n    category_labels=category_labels, \n    show_values=True, \n    value_format=\"{:.1f}\",\n    colors=['tab:orange', 'tab:green', 'tab:blue', 'tab:red'],\n    y_label=\"Number of words\"\n)\n\nplt.savefig('bottom_episodes_emotions.png')\nplt.show()","f1a6aa8f":"\nif first_run != True:\n    unpickled_top_videos_emotions = pd.read_pickle(\"..\/input\/pklfiles\/top_videos_emotions.pkl\")\n    print(\"Most Viewed Videos emotions: \",unpickled_top_videos_emotions.head(2))\n    unpickled_min_videos_emotions = pd.read_pickle(\"..\/input\/minpklfiles\/min_videos_emotions.pkl\")\n    print(\"Least Viewed Videos emotions: \",unpickled_min_videos_emotions.head(2))","bc70a9c3":"table = PrettyTable() \nprint('Comparison table of emotions across Most viewed & Least viewed YouTube Episodes')\ntable.add_column('Emotions',['Anticipation','Trust','Surprise','Joy'], align='l', valign='t')\ntable.add_column('Most Viewed Episodes',[round(unpickled_top_videos_emotions['num_anticipation'].mean()),round(unpickled_top_videos_emotions['num_trust'].mean()), round(unpickled_top_videos_emotions['num_surprise'].mean()), round(unpickled_top_videos_emotions['num_joy'].mean())],valign='b')\ntable.add_column('Least Viewed Episodes',[round(unpickled_min_videos_emotions['num_anticipation'].mean()),round(unpickled_min_videos_emotions['num_trust'].mean()), round(unpickled_min_videos_emotions['num_surprise'].mean()), round(unpickled_min_videos_emotions['num_joy'].mean())],valign='m')\nprint(table)","84183bab":"blog = pd.read_csv('..\/input\/mediumblog1\/sb_blog.csv', encoding='ISO-8859-1', parse_dates=['release_date'])\nblog.head(5)","235a068c":"#preprocessing and counting word length of script and introduction\nblog['script'] = blog['transcript'].apply(str).apply(lambda x: text_preprocessing(x))\nblog['intro'] = blog['introduction'].apply(str).apply(lambda x: text_preprocessing(x))\nblog['script_len'] = blog['script'].apply(str).apply(lambda x: len(x.split()))\nblog['intro_len'] = blog['intro'].apply(str).apply(lambda x: len(x.split()))\nblog","1a7d1d79":"print(\"Category vs Claps: \", blog.groupby('category')['claps'].sum())\nprint(\"\\nRatio of Women-to-Men heroes is \",blog.groupby('heroes_gender')['heroes'].count()[0]\/(blog.groupby('heroes_gender')['heroes'].count()[0] + blog.groupby('heroes_gender')['heroes'].count()[1]))\n","8ef70a70":"# Sorted by claps in descending order \nsorted_blog = blog.sort_values('claps', ascending=False)\nplt.bar(sorted_blog['heroes'], sorted_blog['claps'])\nplt.xlabel('Heroes', fontsize=15)\nplt.ylabel('Claps', fontsize=15)\nplt.xticks(sorted_blog['heroes'], fontsize=9, rotation=90)\nplt.title('Heroes Vs Claps')\nplt.figure(figsize=(20,15))\nplt.show()","c7397abb":"#calculating the average word length of blog interviews and the range that they usually are in.\nprint(\"Average Word Length of blog interviews is {} words.\".format(int(round(np.percentile(blog['script_len'],50)))))\nlower_range = int(round(np.percentile(blog['script_len'],.1)))\nupper_range = int(round(np.percentile(blog['script_len'],95)))\nprint(\"Word Length of blog interviews are between {} and {} words.\".format(lower_range, upper_range))\nprint(\"Dr.Vladimir's interview is a long one (2564 words), which is standing below as an outlier.\")\nsns.set(style=\"whitegrid\")\nax = sns.boxplot(x=blog[\"script_len\"], linewidth=2.5)","a8294dd2":"sorted_blog[['heroes','claps','script_len','intro_len']]","55b3da8b":"# We will look at Frequency Distribution of words used in introduction\nblog_guest_intro = []\nfor s in blog['intro']:\n    blog_guest_intro.append(sent_tokenize(s))   \n\ngi_string = ''.join(str(gi) for gi in blog_guest_intro)\ngi_string = re.sub('[^a-zA-Z]', ' ', gi_string )\ngi_tokenized_word = nltk.word_tokenize(gi_string)\ngi_fdist = FreqDist(gi_tokenized_word)\nprint(gi_fdist.most_common(80))\n\n%matplotlib inline\ngi_fdist.plot(30, cumulative=False,title = \"Common words used in guest introduction\")\nplt.show()\n\nprint(\"Bigrams :\")\nbigrams = nltk.bigrams(gi_tokenized_word)\nprint(list(bigrams)[:35])","483d9d6e":"stopwords = set(STOPWORDS)\neng_stop_words = set(sw.words('english'))\nstop_list = ['pavel','pleskov','artur','kuzin','vladimir','iglovikov','making','everything','come','give','quite','want','long','much','well','able','tell','day','speaking','using','bunch','okay','yeah','person','anyone','anything','someone',\"year\",\n             \"thing\",\"trying\",'still','others','allow','without','definitely','instead','wanted','often','every', 'actually','thank','might',\"working\", \"lot\",\"kind\",\"world\",\"something\",\"listening\",\"someone\",\"etc\",\"need\",\"today\",'look','feel','better']\nstopwords.update(stop_list)\nstopwords.update(eng_stop_words)\n\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       stopwords= stopwords,\n                       colormap='PuRd', \n                       margin=0,\n                       max_words=300, # Maximum numbers of words we want to see \n                       min_word_length=5, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=20,  # Font size range\n                       background_color=\"black\").generate(\" \".join(sorted_blog['script'][:5]))\n\nplt.figure(figsize=(14, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.show()","ed6d5bc2":"* Episode - E0 is a short \"Introduction\" episode of 157 seconds. \n* Episode - E74 is an empty one. So the questions are 0.\n\n* Highest number of questions were asked in Rohan Rao's episode (54 questions), followed by E17 (48 questions).\n* Lowest number of questions were asked in Jean Francois Puget's episode (7 questions), Vladimir Iglovikov's episode (11 questions).\n* Shivam Bansal's episode had most number of humoring words used (30 times) and Julien Chaumond\tis the next (27 times). Surprisingly, Shivam Bansal's episode was the shortest episode with 17.5 minutes.\n* Pierre Stock's episode and Sergey Kolesnikov's episode had least number of humoring words used (just once).  Surprisingly, Pierre Stock's episode is 47 minutes long and Sergey's episode is 1.25 hours long (considering that mean episode duration is 59 minutes.)!\n* Question-to-conversation ratio ranges from 27% to whopping 132%, with a mean qc ratio at 67%. \n* Sanyam has been an extremely curious host in the following episodes - Rohan Rao (asking 37 questions in 28 times he spoke), David Foster (asking 40 questions in 31 interactions), Navdeep Gill (asking 31 questions in 27 interactions), Ines Montani (asking 40 questions in 36 interactions)\n* Sanyam was an avid listener (read he let the guest speak ) in the following episodes - Eugene Khvedchenya\tand Jean Francois Puget\t (at qc ratio of 27 %).\n* There is no direct correlation between number of questions asked or number of welcoming\/encouraging words used and youtube subscribers\/views counts.\n* Nevertheless, we will keep exploring and digging deeper.\n* Looking at the 'questions_asked' the standard questions are - \n   *  a. Could you tell us what got you interested in datascience? \n   * b. So what would lead you to first signing up for kaggle and getting into the first competition? \n   * c. How does Kaggle blend\/balance along with your full time job? \n   * d. How do you approach a new problem?\n   * e. What are your best tips for a beginner who\\'s just getting started and dreams of getting a Kaggle gold? \n   * f. Could you tell us what does a day look like for a researcher? \n   * g. What sort of challenges are you interested in today?\n   * h. What problems are you currently working on?\n   * i. What would be the best platforms to follow you and follow your work?\n\n\n","5a0a6a8e":"![image.png](attachment:image.png)","aba13531":"\n\nThis is a massive effort from Sanyam Bhutani and big kudos goes to CTDS for hosting such informative, unique podcast series with top ML\/Data science researchers across the world\n\nEncouraging this is Youtube\/Apple\/Spotify are garnering more viewership\/listenership gradually.\n\nAverage Podcasts listenership was far more than the average youtube viewing time. The reason is because of the style of the episodes that have settling and  soothing effect that has a good old 'radio' feel.\n\nYoutube videos - In this medium, the audience expects MOOCs, live coding classes or debugging sessions or how to approach a give problem. Description in the videos can be improved. We can have youtube teasers so that audience can set 'reminder on' and know whom to expect.\n\nYoutube non-impression views views are more than impressions, the reason could be Sanyam and his guests are affliated to many networks and run many groups that has driven the non-impression views. Organic growth is important to have exponential growth.\n\nMost viewed videos had better engagement with heroes as mentioned in above emotion analysis.\n\nMasala and Ginger flavour chai seem to give Sanyam all the vigour need to interview the heroes in the podcasts.\n\nWhat is going well (CONTINUE)-\n* Sanyam does his homework about the guest. He sets the stage for a relaxed conversation and steering conversations using Kaggle, fast.ai and common mentors that immediately puts the guest at ease as they discuss some shared bond. This familiarity can be reflected in the rest of the interview. \n* He listens intently and is prepared to take the conversation in new and unexpected directions.\n* I have been pleasantly surpised by the blog interviews - the introduction of the guest is flattering, content has an anecdotal style to it,and ending note is a strong takeaway.\n\nWhat should be kicked off (START) (read: Chai recipe needs these ingredients)-\n* Introduction can be impactful - it can be like bringing out a unique aspect of the guest. \n* Weaving the questions such that story telling comes into play, rathern than a question\/answer format.\n* Off roading - Questions like - Favorite food\/animal, morning routine will give some relief\n* Ask about personal quality of theirs that made them the legends they are- could be dedication\/'never give up' atttitude\n* Sprinkling a healthy does of humour either by speaker or guest. (Q: What was your silly\/stupid fail? )\n* Ask Areas of Expertise - This will help audience in knowing whom to reach out\n* Need guest diversity on various levels - gender\/age\/country\/career turnarounds\/parallel career\n* Ask for recommendation of books\n* Before an interview - some questions can be taken from the audience.\n* Interaction with the youtube\/medium blog audience - like responding back to comments builds a connect\n\n\nThis is my first Kaggle submission. I am thankful to Parul Pandey for suggesting this in our women's forums. \nI thoroughly enjoyed this whole experience.","8f9a65e4":"**We looked at Walter Reade and Mark Landry's responses, which are full of advice\/inspiration, emotions around stories. Such kind of content makes for a feel good podcast.**","ef6c48e9":"# Analysing Introduction\n\nWhy is an introduction is important?\n\n* It makes your audience excited for who they are about to see\n* It makes the speaker feel genuinely acknowledged for being invited to that event\n\n> Without a good introduction, the audience will not have a strong enough reason to be open to the speaker\u2019s ideas because they might not know the credibility of the speaker.","edbc4738":"# Analyzing Transcripts - Blog Interview Series\n* Let us look at important words in Top 5 applauded blog interviews.","6dd99f16":"**Points to Note**:\n* Vladimir, Pavel and Artur interviews have been received by audience well. Claps they received are above 1000.\n* 9 out of 24 guests are Phds.\n","d709c625":"# Preliminary Analysis","9a2e1d5f":"**ABOUT THE NEW DATASET - TALK**\n\nContains data about the host and guest interactions\n\n* episode_id : Episode ID\n* num_questions : Number of questions asked by the host\n* num_conversations : Number of times the host spoke\/interacted\n* qc_percentage : num_questions \/ num_conversations\n* questions_asked : Actual questions asked\n* guest_introduction : subtitle of guest_introduction\n* humoring_words_used : These words allow the guest to have a welcoming, easy going environment to open up and share stories. Humoring mean obliging, compliant, amiable, pampering, allowing. It is different from humor we know.\n* host_talk : host's conversations\n* guest_talk : guest's conversations\n* abstract : abstracts","2a998ac3":"# Case for Diversity\n\nPoints to ponder over:\n* How are women faring ?\n* How about including more interviewees from other nationalities ?\n* How about including more interviewees from newer nationalities ?\n* How about including more from African backgrounds ? ('Black Lives matter')","4b521cfe":"# Analyzing Episode title and description","79272dd8":"# 1. Installing required packages","8067f263":"Words - Career, money, beginner, experience, learnng, practice, odsai, project, pipeline, hardware, challenge, practice, years\nIf we wire this up, how their story\/journey started,  challenges encountered, learning from Kaggle, improved performance\/writing, leading to a job\/research and becoming a grandmaster or getting a desired break or writing a library.\n\nMost applauded blog interviews have been ones that talks about whole gamut of components that led to Kaggle title or py library fame. Of course, the fact that some guests popularity has helped in applauses here like - Fracois cholet, Sudalai Rajkumar, Rachel Tatman etc.","762e5394":"# Analyzing Introduction - Blog Interviews\nWe look at 2 things:\n* Frequency Distribution of most common words\n* Bi-grams","39a6a107":"Points to Note:\n* Guests from 'Industry'category have maximum applause (8.3K) followed by guests from 'Research' industry (2.5K) \n* Out of 24 interviews, only 2 were women. Hence the female-male guests ratio is very small at .083","d6a4e0a5":"# Reading all files","abbf1636":"**TF-IDF stands for \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the documents. This method is a widely used technique in Information Retrieval and Text Mining.\nLet us apply this to guest talks to find out what are rarer words that are important.**","fbc3a01d":"**One can see that all the above keywords were key points of discussion in the episodes.**","e1c43524":"**Points to Note:**\n\n**** Note: Blog interviews has 5 heroes who were interviewed for Podcast also - Tuatini Godard, Dr. Vladimir I. Iglovikov, Dr. Jean-Francois Puget, Mikel Bober-Irizar, Sylvain Gugger****\n\n* One blog series i.e Blog Interview #12 is not available\n* Diversity has been exemplified through these 24 blogs interviews be it- age, career transitions, parallel career, countries, work (full time\/freelancers)\n  * Age - Mikel is youngest grandmaster (17 years) and at the other extreme we have a very senior person - Mr.Leslie Smith as guest. \n  * Career transition - Christine McLeavey transitioned - From physics field to data science. \n  * Parallel careers  - Dr. Alexandre (Radiologist as well as Data Scientist)\n  * Heroes hail from various countries. \n  * Freelancers (ex: Tuatini Godard) and Full time job holders (ex: S Rajkumar)\n\n* Comments on blogs have been very low and there are no responses by \"Sanyam Bhutani\" to the comments.","49e6beca":"**Our Audience ranges from college graduates to experienced folks.\nWhy are these Emotions important in this context:\n* Anticipation - ML\/DS newbies\/experienced who want to how to ace Kaggle competitions and looking for a break in these fields, find these interviews give them hope and expectation\n* Surprise - Seeing how our heroes tackled issues and solved complex problems amazes us\n* Trust - When there is faith, that connect is formed, audience seems to follow advice, tips, suggestion to better themselves.\n* Joy - When we are enjoying what we are doing we don't seem to mind the time because we are in a \"flow\". Audience is glued to long interviews when heroes recount their truimphant, jubilant tales and that ecstacy is percolated down to the audience.**","ba3e5ac7":"**Observations:**\n* 9 fastai lecture recap(also takeaways and Jeremy's advice) was released on 07-Mar-2020. \n* The youtube_subscribers gained from  these videos are not much to talk about. \n* Apple average listening duration is more than the youtube watch duration. \n* But all the stats are low in general. \n* This is very niche in the sense - for fast.ai course learners","4dd2147c":"**Guests are introduced by their title, research areas, background, their work, current\/previous companies and of course their Kaggle achievements. This is a very complete introduction ... Good Job !!! **","d68ece2c":"# 2. Importing required packages","2a2481d7":"**We can infer that - Sanyam is setting a context for the interviews, that he will talk about Kaggle competitions competed, journey to data science, research in machine learning, experiences in this field and amazing community that we have.** ","381039da":"# Analyzing Medium Blog Data\n\n* I have collected data from the interview blogs and uploaded as csv. \n* I could not segregate the transcripts speaker wise due to paucity of time.\n* I am not very sure of segregation of 'category'.\n* Let us see what is the deal with the blog interviews. :)","b0317065":"#  Concluding Statements ","3eccbe4f":"# Analyzing Guest responses\/answers\n\nQuestions to ponder on:\n* How are the guests responding ? \n* Are they going off the tangent ? \n* Are they brief and concise? \n* Are they bringing personable\/relatable account?\n\nLet us look at how spacy library brings out the word matching. It would be disservice if spacy is not used in our EDA, especially because we have Ines Montani as one of our guests (Co_founder at Explosion - spaCY) :)","82d3fb88":"**Points to Note:**\n* Clearly, female heroes are bringing more traffic in youtube medium. \n* More heroes can be picked from countries such as India, Australia, France, Germany, Austria, UK, Russia, Ukraine, Brazil, Canada. These heroes would have been local heroes in their home countries, airing their heroes would add more viewers\/listeners to our youtube\/podcast numbers.\n* We can also include newer countries with high population like China, Indonesia, Pakistan, Nigeria, Bangladesh and Mexico.\n* Going by Kaggle Survey 2019 - Countries like Egypt, Iran and Romania has highest proportion of data scientists working in ML\/DL\n* We can also include different profiles like early bloomers (below 20 years of age), late starters (above 50 years of age)\n* We can consider profiles of career transitioners - dancer\/skater\/priest --> data scientist\n* We can consider profiles of turnarounds - from bad coder to grandmaster\n","74aac02f":"Observations:\nEpisode E12 - One can say it went \"off the road\" as discussion steered around Unemployment, Free lancing, Travelling continents, Visa issues, boredom, effect on relationships etc.","8b0ff9e8":"**Observations - There is no direct correlation between the liked blogs and script\/intro length.**","a0f26dc0":"# Analyzing Emotions\n\n* Emotion is more powerful than reason and emotions can be triggered. \n* The palette of human emotions can be assessed to attract the attention of your audience and make them want to learn more and perform the desired action \u2013 view, hear, like, comment, contribute, share or subscribe.\n* They are: joy, surprise, trust, fear, anticipation, anger, sadness, and disgust.\n* Ones relevant to us are - Trust, Anticipation, Surprise and Joy.\n","fcfb3d4f":"> Points to note:\n* Description is cleaned to not include links, and just include guest details\/introduction\n* We can see that there is not much effect on views based on title or description length\n* Description length has not been consistent over the episodes.\n* Suggestion - Description can be catchy, also verbose including more details about what was discussed in the interview. This will excite the viewer to watch the episode.\n* For example - Andrew Lukyaneko's episode description boils down to following keywords - [king, kaggle, kernels, grandmaster, andrew, lukyanenko, ranked, journey, data, science, talk, pipeline, writing, kernels\"] and James Dellinger's episode description boils down to following keywords - [james, dellinger, experiments, dali, library, nvidia, talk, performance, convenience, comparisons, frameworks, others]\n* Rohan Rao's episode had one of longest description - [kaggle, grand master, data scientist, rohan rao, rohan kaggle, grand master, competition, tier, data scientist, represented, india, data science, sudoku, puzzles, current, national sudoku champion first indian ranked top world championship, secured eighth position world rank secured, podium, finishes, asian, sudoku, championship, five national puzzle champion, current rank four current national sudoku champion talk journey data science data numbers, his journey, world competitive sports kaggle journey, discuss data science journey, kaggle, discuss approach kaggle changed years evolved current thoughts, platform tips advice discuss recent eighth gold medal, kaggle teams, second position finish, ashrae great energy, prediction, cpmp, pos solution, rohan rao]\n* Highly viewed episodes are longer in duration than the the less viewed episodes. There is a good 20 minutes difference between average high-viewed and less-viewed episodes. Does it mean the longer the episode, the more it is viewed ? We will look at further analysis.","09dc7660":"# Looking at Fast.ai Series","253e9f80":"> ***Points to Note***\n\n* 85 episodes - 74 Episodes with ML Heroes, 9 Learning sessions, 1 kick off episode and 1 birthday special. Robert Bracco, Edouard Harris and Shivam Bansal appeared twice. Two interviews were hosted with multiple heroes in a podcast.\n* Learning sessions start with M* series in episode_id, while the episodes start with E*.\n* First release date is 21-07-2019 and last release date is 18-06-2020 (as per the given dataset), which makes it 10 months and 29 days( or 334 days).\n* Most ML Heroes are currently located in USA and USA is top nationality for heroes.\n* Most heroes interviewed are from 'Industry' category.\n* Most of recording time is in the night, we can say right off the bat - it is very likely because most of heroes are from USA.\n* Youtube subscribers cumulative sum is around 1K, while on the actual Youtube the number is 1.9K (checked on 08-Jul). It is likely that the subscribers are added from other networking sources that Mr.Sanyam Bhtani has and other communities that he is part of.\n* \"Chaitimedatascience\" has been catering to multiple audience across multiple platforms\/channels which is a big plus, placed in a unique spot to reach greater reach and heights.\n* Produced 75.55 hours of viewing content (including the learning content) in a span of 334 days in 48 weeks (approx). This means 1.6 hour of watching content a week.\n* CTDS has been churning out 1.77 episodes per week. (85 episodes released in 48 weeks)\n* A cumsum of Youtube views is at 43616, Spotify streams is at 6720, Spotify listeners is at 5455, Apple listeners is at 1714. Viewership\/Listenership has reached around 55+K, which is a good celebratory mark.\n* Youtube seems to be way ahead of the pack, followed by Spotify and Apple.\n* Interestingly, the average apple listening time have been 28.63 minutes, while the average anchor plays has been 8.77 minutes and youtube average viewing time has been 5.29 minutes. Net-Net - 'Listening time' has been more than the 'Viewing time'.\n* Female to Male ratio is 12:88 (Male =65, Female =9). \n* Heroes from 21 nationalities were interviewed, which is a good diverse spread, however half of countries (ie. 11 countries) have only 1 hero interviewed from their country.\n* A bulk of heroes  (ie. 37 heroes) are currently located in US. Many heroes could have migrated to the US. It could be because US has far more research opportunities, is spearheading DL\/ML fields and has related job openings.\n* Flavor of Tea - Masala Chai, Ginger Chai seems to be topping the chart with 16 each.\n\n\n\n\n\n\n","78d0914c":"# Analyzing the Questions asked, number of questions","74b01a8e":"I am using dataset \"NRC-Emotion-Intensity\" from - \nhttps:\/\/saifmohammad.com\/WebPages\/AffectIntensity.htm\n\nThe words are lemma\/stem based, so I am using fuzzywuzzy( >= 94) to choose closest words and get a count of 4 above mentioned emotions - anticipation, joy, trust, surprise.\n\n**We are getting most viewed episodes and less viewed episodes and looking at their emotional appeal.**\n**Below is  Plutchik\u2019s Wheel of Emotions**"}}