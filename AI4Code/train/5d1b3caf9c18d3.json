{"cell_type":{"194ef91a":"code","a3a2ba04":"code","f7c971ea":"code","20bf879c":"code","d6735f90":"code","b8a4a355":"code","6c924900":"code","63d30c02":"code","908f750c":"code","158039b4":"code","2bc0b232":"code","30e3118c":"code","a138387b":"code","dfd33aff":"code","a41717bc":"code","6dcc493f":"code","3f1eaeb0":"code","caff67e7":"code","cf2163a2":"code","b97156fb":"code","a551a7eb":"code","9a99141f":"code","6d662a35":"code","2b975306":"code","c6898bad":"code","cdefa035":"code","274f2836":"code","90a9c6f1":"code","1c78259e":"code","c3b4a04c":"code","14d51133":"code","a186a169":"code","cb668777":"code","c7569de9":"code","90f56504":"code","bb8c882b":"code","f9f0ab33":"markdown","9ef6e7f8":"markdown","26293df3":"markdown","b691a7e7":"markdown","c3d60586":"markdown","d9f9390c":"markdown","6c2973c3":"markdown","c55486f1":"markdown","f1b01670":"markdown","9c4f010d":"markdown","7d04f944":"markdown","655b1495":"markdown","f41233ab":"markdown","2eb424c4":"markdown"},"source":{"194ef91a":"import pandas as pd\nimport numpy as np\n\nimport torch\nimport tensorflow as tf\nimport transformers\nfrom transformers import *\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport time\nimport datetime\nimport random\n\nprint('Transformers version: ', transformers.__version__)\nprint('Tensorflow version: ', tf.__version__)","a3a2ba04":"randomseed=42\nbertmodel='bert-large-uncased'#bert-base-uncased\ndata_dir = '\/kaggle\/input\/nlp-getting-started\/'\ntrain_df = pd.read_csv(data_dir+'train.csv')\ntest_df = pd.read_csv(data_dir+'test.csv')\ntrain_df = train_df.sample(n=len(train_df), random_state=randomseed)# random permutation of the sample\nsample_submission = pd.read_csv(data_dir+'sample_submission.csv')\n# print(train_df['target'].value_counts())\n# train_df.head(2)","f7c971ea":"x_train = train_df['text']\ny_train = train_df['target']\nx_test = test_df['text']","20bf879c":"tokenizer = transformers.BertTokenizer.from_pretrained(bertmodel, do_lower_case=True)","d6735f90":"def encode_tweet(df):\n    input_ids = []\n\n    for x in df:\n        encoded_x = tokenizer.encode(x,\n                                    add_special_tokens = True)\n        input_ids.append(encoded_x)\n    return input_ids","b8a4a355":"test_input = encode_tweet(x_test)\n# print('Original: ', x_test[0])\n# print('Encoded: ', test_input[0])","6c924900":"train_input = encode_tweet(x_train)","63d30c02":"# a=encode_tweet(x_train)\n# a=encode_tweet(x_train[0])\n# a=encode_tweet(['Forest fire near La Ronge Sask. Canada'])\n# a=tokenizer.encode(x_train[0],add_special_tokens = True)\n# a[0]\n# len(a)\n# a\n# x_train[1]\n# x_train[0]\n# x_train[0:3]\n# a[1]\n# len(a[0])\n# len(x_train)\n# len(x_train)\n# x_train.shape\n# help(x_train)\n# type(x_train)\n# x_train\n# df=x_train[1:2]\n# df\n# input_ids = []\n\n# for x in df:\n#     encoded_x = tokenizer.encode(x,add_special_tokens = True)\n#     input_ids.append(encoded_x)\n#     print(encoded_x)\n#     print(len(encoded_x))\n#     print(x)\n#     print(len(x))\n# encoded_x = tokenizer.encode('un-imaginable',add_special_tokens = True)\n# print(encoded_x)\n# print(len(encoded_x))","908f750c":"# print('Max sentence length in Train Ids: ', max([len(sen) for sen in train_input]))\n# print('Max sentence length in Test Ids: ', max([len(sen) for sen in test_input]))\n# train_input[1]","158039b4":"from keras.preprocessing.sequence import pad_sequences\nMAX_LEN = 84\n\ndef pad_tweets(df):\n    df = pad_sequences(df, maxlen=MAX_LEN, dtype=\"long\", \n                       value=0, truncating=\"post\", padding=\"post\")\n    return df","2bc0b232":"train_input = pad_tweets(train_input)\ntest_input = pad_tweets(test_input)","30e3118c":"def get_att_mask(df):\n    attention_masks = []\n\n    for tweet in df:\n        att_mask = [int(token_id > 0) for token_id in tweet]\n        attention_masks.append(att_mask)\n    return attention_masks","a138387b":"train_att = get_att_mask(train_input)\ntest_att = get_att_mask(test_input)","dfd33aff":"# test_att[1]","a41717bc":"from sklearn.model_selection import train_test_split\n\ntr_input, val_input, tr_label, val_label = train_test_split(train_input, y_train, \n                                                            random_state=2020, test_size=0.15)\n# Do the same for the masks.\ntr_mask, val_mask, _, _ = train_test_split(train_att, y_train,\n                                             random_state=2020, test_size=0.15)","6dcc493f":"# For Training and Validation data and masks\ntr_input = torch.tensor(tr_input)\nval_input = torch.tensor(val_input)\n\n#convert to np.array, otherwise throws a mysterious 'KeyError: 4' error\ntr_label = torch.tensor(np.array(tr_label))\nval_label = torch.tensor(np.array(val_label))\n\ntr_mask = torch.tensor(tr_mask)\nval_mask = torch.tensor(val_mask)\n\n# For Test data and mask\nte_input = torch.tensor(test_input)\nte_mask = torch.tensor(test_att)","3f1eaeb0":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nBATCH_SIZE = 32 \n\n# For training\ntrain_data = TensorDataset(tr_input, tr_mask, tr_label)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n\n# For validation\nval_data = TensorDataset(val_input, val_mask, val_label)\nval_sampler = RandomSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = BATCH_SIZE)","caff67e7":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n\nmodel = BertForSequenceClassification.from_pretrained(bertmodel,\n                                                      num_labels = 2,\n                                                      output_attentions = False,\n                                                      output_hidden_states = False)","cf2163a2":"model.cuda()","b97156fb":"# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","a551a7eb":"from transformers import get_linear_schedule_with_warmup\n\n# AdamW is a class from the Huggingface library\noptimizer = AdamW(model.parameters(),\n                  lr = 1e-5, # default is 5e-5\n                  eps = 1e-8 # default is 1e-8\n                )\nepochs = 1 # 1 epoch gave the best result\n\n# the number of batches times the number of epochs\ntotal_steps = len(train_dataloader) * epochs\n\n# the learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)","9a99141f":"# Accuracy helper function\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","6d662a35":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","2b975306":"if torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","c6898bad":"seed = 50\n\nrandom.seed(seed)\nnp.random.seed(seed)\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nloss_arr = []\n\nfor i in range(0, epochs):\n    \n    # ========= Training ==========\n    \n    print('====== Epoch {:} of {:}'.format(i+1, epochs))\n    print('Training...')\n    \n    t0 = time.time()\n    \n    total_loss = 0\n    # initialize training mode\n    model.train()\n    \n    for step, batch in enumerate(train_dataloader):\n        if step % 30 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n            \n            # Unpacking the training batch from dataloader and copying each tensor to the GPU\n            \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # pytorch doesn't clear previously calculated gradients\n        # before performing backward pass, so clearing here:\n        model.zero_grad()\n        \n        outputs = model(b_input_ids,\n                       token_type_ids = None, \n                       attention_mask = b_input_mask,\n                       labels = b_labels)\n        loss = outputs[0]\n        \n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        \n        #update the learning rate\n        scheduler.step()\n    \n    avg_train_loss = total_loss \/ len(train_dataloader)\n    \n    loss_arr.append(avg_train_loss)\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n    \n    # ========= Validation ==========\n    \n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    \n    for batch in val_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        with torch.no_grad():\n            \n            outputs = model(b_input_ids, \n                           token_type_ids = None, \n                           attention_mask = b_input_mask)\n            \n        logits = outputs[0]\n        # move logits to cpu\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        # get accuracy\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        eval_accuracy += tmp_eval_accuracy\n        \n        nb_eval_steps += 1\n    \n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n    \nprint(\"\")\nprint(\"Training complete!\")","cdefa035":"import matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(loss_arr, 'b-o')\n\n# Label the plot.\nplt.title(\"Training loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.show()","274f2836":"pred_labels = np.array(sample_submission['target'])","90a9c6f1":"# b_labels","1c78259e":"te_labels = torch.tensor(pred_labels)","c3b4a04c":"prediction_data = TensorDataset(te_input, te_mask, te_labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler = prediction_sampler, batch_size = BATCH_SIZE)","14d51133":"print('Predicting labels for {:,} test sentences...'.format(len(te_input)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions , true_labels = [], []\n\n# Predict \nfor batch in prediction_dataloader:\n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    b_labels = batch[2].to(device)\n    \n    # Telling the model not to compute or store gradients, saving memory and \n    # speeding up prediction\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n        outputs = model(b_input_ids, token_type_ids=None,\n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n  \n    # Store predictions and true labels\n    predictions.append(logits)\n    true_labels.append(label_ids)\n\nprint('    DONE.')","a186a169":"flat_predictions = [item for sublist in predictions for item in sublist]\n# flat_predictions\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","cb668777":"# predictions","c7569de9":"flat_predictions","90f56504":"sample_submission['target'] = flat_predictions\nsample_submission.to_csv('submission.csv', index = False)","bb8c882b":"# sample_submission","f9f0ab33":"### Training Loop","9ef6e7f8":"Converting to Torch Tensors","26293df3":"### Attention Masks","b691a7e7":"# Tokenization and Input Formatting\n### Sequence to IDs","c3d60586":"# Training the Model\nUsing the pre-trained model, documentation can be found here\nhttps:\/\/huggingface.co\/transformers\/v2.2.0\/main_classes\/model.html#transformers.PreTrainedModel.from_pretrained","d9f9390c":"### AdamW optimizer\n\nhttps:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L109","6c2973c3":"Displaying some of the model's parameters:","c55486f1":"# Importing Data","f1b01670":"### Padding","9c4f010d":"This is adpated from [this](https:\/\/www.kaggle.com\/ozdata\/bert-huggingface-pytorch), and a few changes are added to try different options. \n\nIt turns out higher number of epoch (5) or larger learning rates(X5 or X10) doesn't help(decrease the performance of both crossvalidation and final). One epoch with a little bit train is enough. bert-large-uncased not help as well","7d04f944":"# Testing","655b1495":"https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128","f41233ab":"some self test on bert and it's found some 'word' are encoded by more than one long value","2eb424c4":"Using helper classes in order to use batches for training. It creates an iterator, which should save on memory during training. The same must be repeated on the test set once we have prediction labels."}}