{"cell_type":{"c806e3f1":"code","e1458dc7":"code","3c0a8494":"code","c59030a6":"code","8aa424b4":"code","a0d8c2c4":"code","688ddb54":"code","2cd5817e":"code","b48575d9":"code","6954f085":"code","6b47741c":"code","fa01f60d":"code","8cb5f8e4":"code","24b33807":"code","6842759d":"code","6825697e":"code","c194a342":"code","2ce37135":"code","6c75f4e5":"code","98acb488":"code","2eff17e4":"code","9d8e18d5":"code","1e2ca1c4":"code","7b2be233":"code","c0114532":"code","a625a217":"code","e9e81ca8":"code","c3fd2e98":"code","9a1eb03f":"code","1982e25a":"code","e1efb12f":"code","84e687b1":"code","a434f086":"code","cddac63f":"code","fcd464fc":"code","e226cdc7":"code","fb2368b4":"code","d0e3ec01":"code","baad1390":"code","aff838b2":"code","aeb74c2b":"code","563f29aa":"code","77577e37":"code","8854340b":"code","40bae5a4":"code","61fd501a":"code","78d5f69a":"code","3dd0f892":"code","9b2d23dd":"code","9a102b48":"code","f6637634":"code","5cb14e96":"code","909c830e":"code","2fc41239":"code","1e662012":"code","b05c0510":"code","3acbd990":"code","418ee68f":"code","3e590cf3":"code","a892c9ad":"code","339fb731":"markdown","b80680cf":"markdown","e13908db":"markdown","4349f03c":"markdown","7fe14f88":"markdown","838888ef":"markdown","d648f36b":"markdown","6e322cc2":"markdown","5d031563":"markdown","ddb7469c":"markdown","309edf66":"markdown","d654e27e":"markdown","65f3894e":"markdown","673a7f9e":"markdown","1644ff7a":"markdown","a52b9150":"markdown","31ff1298":"markdown","05348927":"markdown","6c2fc2d4":"markdown","1910d656":"markdown","91ccb648":"markdown","c54e67b1":"markdown","327896e5":"markdown"},"source":{"c806e3f1":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import *\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport lightgbm as lgb\nimport xgboost as xgb","e1458dc7":"train_data=pd.read_csv(\"..\/input\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/test.csv\")","3c0a8494":"#To get the dimensions of data\ntrain_data.shape, test_data.shape","c59030a6":"# Top-5 rows of the dataset\ntrain_data.head()","8aa424b4":"test_data.head()","a0d8c2c4":"# Checking statistical properties of variables\ntrain_data.describe(include='all')","688ddb54":"test_data.describe(include='all')","2cd5817e":"# Checking datatype \ntrain_data.info()","b48575d9":"# Checking and Counting of missing values\ntrain_data.isnull().sum()","6954f085":"train_data.isna().sum().plot(kind=\"barh\", figsize=(20,10))\nfor i, v in enumerate(train_data.isna().sum()):\n    plt.text(v, i, str(v), fontweight='bold', fontsize = 15)\nplt.xlabel(\"Missing Value Count\")\nplt.ylabel(\"Features\")\nplt.title(\"Missing Value count By Features\")","6b47741c":"test_data.isnull().sum()","fa01f60d":"train_data['homepage']=train_data['homepage'].astype(str).apply(lambda x: 1 if x[0:4] == 'http'  else 0)\n\ntest_data['homepage']=test_data['homepage'].astype(str).apply(lambda x: 1 if x[0:4] == 'http'  else 0)","8cb5f8e4":"# Checking for no.of unique languages\ntrain_data['original_language'].unique(), test_data['original_language'].unique()","24b33807":"#looking for language-wise revenue\nplt.figure(figsize=(12,7))\nsns.barplot('original_language', 'revenue', data=train_data)\n","6842759d":"# To convert release date feature into seperate Date, Month, Year features\ndef date_features(df):\n    df['release_date'] = pd.to_datetime(df['release_date'])\n    df['release_year'] = df['release_date'].dt.year\n    df['release_month'] = df['release_date'].dt.month\n    df['release_day'] = df['release_date'].dt.day\n    df['release_quarter'] = df['release_date'].dt.quarter\n    df.drop(columns=['release_date'], inplace=True)\n    return df\n\ntrain_data=date_features(train_data)\ntest_data=date_features(test_data)","6825697e":"fig = plt.figure(figsize=(20,10))\n\n# Average revenue by day\nplt.subplot(221)\ntrain_data.groupby('release_day').agg('mean')['revenue'].plot(kind='bar')\nplt.ylabel('Revenue')\nplt.title('Average revenue by day')\n\n# Average revenue by month\nplt.subplot(222)\ntrain_data.groupby('release_month').agg('mean')['revenue'].plot(kind='bar')\nplt.ylabel('Revenue')\nplt.title('Average revenue by month')\nloc, labels = plt.xticks()\nloc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n\nplt.show()\n#plt.xticks(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")","c194a342":"plt.figure(figsize=(15,7))\ntrain_data.groupby('release_year').agg('mean')['revenue'].plot(kind='bar')","2ce37135":"train_data['release_year']=np.where(train_data['release_year']> 2019, train_data['release_year']-100, train_data['release_year'])\ntest_data['release_year']=np.where(test_data['release_year']> 2019, test_data['release_year']-100, test_data['release_year'])","6c75f4e5":"plt.subplots(figsize=(15,10))\nsns.heatmap(train_data.corr(),annot=True)","98acb488":"# Plotting budget vs revenue plot\nsns.scatterplot('budget', 'revenue',data= train_data)","2eff17e4":"# Converting Json Format Columns to Dictionary Format\n\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain_data = text_to_dict(train_data)\ntest_data = text_to_dict(test_data)\n","9d8e18d5":"train_data['collection_name'] = train_data['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntrain_data.drop('belongs_to_collection', axis=1, inplace=True)\n\n\ntest_data['collection_name'] = test_data['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntest_data.drop('belongs_to_collection', axis=1, inplace=True)","1e2ca1c4":"(train_data['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts().sort_index()).plot(kind='bar',)\n\nfor i, v in enumerate(train_data['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts().sort_index()):\n    plt.text(i, v, str(v))\n    \nplt.xlabel('No.of genres in a film')\nplt.ylabel('count')","7b2be233":"train_data['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()\nlist_of_genres = list(train_data['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nmost_common_genres=Counter([i for j in list_of_genres for i in j]).most_common(20)\nfig = plt.figure(figsize=(10, 6))\ndata=dict(most_common_genres)\nnames = list(data.keys())\nvalues = list(data.values())\n\nplt.barh(range(len(data)),values,tick_label=names,color='teal')\nplt.xlabel('Count')\nplt.title('Movie Genre Count')\nplt.show()","c0114532":"train_data['no.of_genres']=train_data['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain_data['genres'] = train_data['genres'].apply(lambda x: ', '.join(sorted([i['name'] for i in x])) if x != {} else '')\n\ntop_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common()]\nfor k in top_genres:\n    train_data['genre_' + k] = train_data['genres'].apply(lambda x: 1 if  k in x else 0)\n    \n    \n    \n    \ntest_data['no.of_genres']=test_data['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest_data['genres'] = test_data['genres'].apply(lambda x: ', '.join(sorted([i['name'] for i in x])) if x != {} else '')\n\ntop_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common()]\nfor k in top_genres:\n    test_data['genre_' + k] = test_data['genres'].apply(lambda x: 1 if  k in x else 0)    ","a625a217":"train_data.groupby('no.of_genres').agg('mean')['revenue'].plot(kind='bar')\nplt.ylabel('revenue')\nplt.title('Revenue vs #genres')","e9e81ca8":"# Counting the frequency of production company \nlist_of_companies = list(train_data['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nmost_common_companies=Counter([i for j in list_of_companies for i in j]).most_common(20)\n\ndata=dict(most_common_companies)\nnames = list(data.keys())\nvalues = list(data.values())\n\nfig = plt.figure(figsize=(10, 6))\nplt.barh(names,values,color='brown')\nplt.xlabel('Count')\nplt.title('Top 20 Production Company Count')\nplt.show()","c3fd2e98":"# Creating features from production_companies variable\n\ntrain_data['no.of_production_companies'] = train_data['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain_data['production_companies'] = train_data['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]\nfor g in top_companies:\n    train_data['production_company_' + g] = train_data['production_companies'].apply(lambda x: 1 if g in x else 0)\n    \n    \n    \n    \n    \ntest_data['no.of_production_companies'] = test_data['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest_data['production_companies'] = test_data['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]\nfor g in top_companies:\n    test_data['production_company_' + g] = test_data['production_companies'].apply(lambda x: 1 if g in x else 0)    ","9a1eb03f":"list_of_countries=list(train_data['production_countries'].apply(lambda x:[i['name'] for i in x] if x!={} else []).values)\nmost_commom_countries=Counter([i for j in list_of_countries for i in j]).most_common(30)\ndata=dict(most_commom_countries)\nnames=list(data.keys())\nvalues=list(data.values())\n\nplt.figure(figsize=(15,8))\nplt.barh(names, values)\nplt.xlabel('count')\nplt.title('country-wise movies count ')","1982e25a":"train_data['no.of_produc_countries']=train_data['production_countries'].apply(lambda x: len(x) if x!={} else 0)\ntrain_data['production_countries_names']=train_data['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x!={} else '')\n\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(30)]\nfor p in top_countries:\n    train_data['produ_country_' + p]=train_data['production_countries_names'].apply(lambda x:1 if p in x else 0)\n\n    \n    \n    \n    \ntest_data['no.of_produc_countries']=test_data['production_countries'].apply(lambda x: len(x) if x!={} else 0)\ntest_data['production_countries_names']=test_data['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x!={} else '')\n\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(30)]\nfor p in top_countries:\n    test_data['produ_country_' + p]=test_data['production_countries_names'].apply(lambda x:1 if p in x else 0)    ","e1efb12f":"list_of_languages = list(train_data['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nmost_common_languages=Counter([i for j in list_of_languages for i in j]).most_common(20)\n\ntrain_data['num_languages'] = train_data['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntrain_data['all_languages'] = train_data['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')\ntop_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\nfor g in top_languages:\n    train_data['language_' + g] = train_data['all_languages'].apply(lambda x: 1 if g in x else 0)\n\n    \n    \n    \ntest_data['num_languages'] = test_data['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntest_data['all_languages'] = test_data['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')\nfor g in top_languages:\n    test_data['language_' + g] = test_data['all_languages'].apply(lambda x: 1 if g in x else 0)\n","84e687b1":"train_data['runtime'].fillna(train_data['runtime'].mean(), inplace=True)\n\n\ntest_data['runtime'].fillna(test_data['runtime'].mean(), inplace=True)","a434f086":"train=train_data.drop(['id', 'genres','original_language', 'imdb_id', 'original_title', 'overview', 'poster_path', 'production_companies', 'production_countries', 'spoken_languages', 'status', 'tagline', 'title', 'Keywords', 'cast', 'crew', 'production_countries_names',  'all_languages','collection_name',\n 'no.of_genres' ], axis=1)\n\ntest=test_data.drop(['id', 'genres','original_language', 'imdb_id', 'original_title', 'overview', 'poster_path', 'production_companies', 'production_countries', 'spoken_languages', 'status', 'tagline', 'title', 'Keywords', 'cast', 'crew', 'production_countries_names',  'all_languages','collection_name',\n 'no.of_genres' ], axis=1)","cddac63f":"train['revenue']=np.log1p(train.revenue)\n\ntrain['budget']=np.log1p(train.budget)\ntest['budget']=np.log1p(test.budget)\n\n\ntrain['popularity']=np.log1p(train.popularity)\ntest['popularity']=np.log1p(test.popularity)\n\ntrain['runtime']=np.log1p(train.runtime)\ntest['runtime']=np.log1p(test.runtime)","fcd464fc":"train.shape, test.shape","e226cdc7":"train_x=train.drop('revenue', axis=1)\ntrain_y=train['revenue']","fb2368b4":"x_train, x_test, y_train, y_test=train_test_split(train_x, train_y, test_size=0.33)","d0e3ec01":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","baad1390":"model=LinearRegression()\nmodel.fit(x_train, y_train)","aff838b2":"predict=model.predict(x_test)\npredict_train=model.predict(x_train)","aeb74c2b":"print('RMSE test:', np.sqrt(np.mean((predict - y_test)**2)))\nprint('RMSE train:', np.sqrt(np.mean((predict_train - y_train)**2)))","563f29aa":"model_rf=RandomForestRegressor()\nmodel_rf.fit(x_train, y_train)","77577e37":"predict_rf=model_rf.predict(x_test)\npredict_rf_train=model_rf.predict(x_train)","8854340b":"print('Test RMSE RF:', np.sqrt(np.mean((predict_rf - y_test)**2)))\nprint('Train RMSE RF:', np.sqrt(np.mean((predict_rf_train - y_train)**2)))","40bae5a4":"Random_Search_Params ={\n    'max_features':[1,2,3,4,5,6,7,8,9,10,15,20,25,30,40,50],\n    \"max_depth\": list(range(1,train.shape[1])),\n    'n_estimators' : [1, 2, 4, 8, 50, 100,150, 200, 250, 300],\n    \"min_samples_leaf\": [5,10,15,20,25],\n    'random_state' : [42] \n    }","61fd501a":"random_search = RandomizedSearchCV(\n    estimator=RandomForestRegressor(), param_distributions= Random_Search_Params, \n    cv=3,\n    refit=True,\n    verbose=True)","78d5f69a":"random_search.fit(x_train, y_train)","3dd0f892":"random_search.best_params_","9b2d23dd":"model_rf_tune=RandomForestRegressor(random_state=42, \n                                    n_estimators=150, min_samples_leaf=15,\n                                    max_features=40, max_depth=86\n                                   )","9a102b48":"model_rf_tune.fit(x_train, y_train)","f6637634":"predict_rf_tune=model_rf_tune.predict(x_test)\n\npredict_rf_tune_train=model_rf_tune.predict(x_train)","5cb14e96":"print('Test RMSE RF_tune_:', np.sqrt(np.mean((predict_rf_tune - y_test)**2)))\nprint('Train RMSE RF_tune:', np.sqrt(np.mean((predict_rf_tune_train - y_train)**2)))","909c830e":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbrt\",\n         \"metric\": 'rmse'}\n\nlgb_model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\nlgb_model.fit(x_train, y_train, \n        eval_set=[(x_train, y_train), (x_test, y_test)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=1000)\n","2fc41239":"Random_Search_lgb_Params ={\n    \"max_depth\": [4,5,6],\n    \"min_data_in_leaf\": [15,20,25],\n    'learning_rate': [0.01,0.2,0.3,0.4,0.001,0.002,0.003,0.004,0.005],\n    'num_leaves': [25,30,35,40]  }\n\n\n\nrandom_search_lgb = RandomizedSearchCV(\n    estimator=lgb_model, param_distributions= Random_Search_lgb_Params, \n    cv=3,\n    refit=True,\n    random_state=42,\n    verbose=True)\n\nrandom_search_lgb.fit(x_train, y_train)\nprint('Best score reached: {} with params: {} '.format(random_search_lgb.best_score_, random_search_lgb.best_params_))","1e662012":"tuned_params = {'num_leaves': 35,\n         'min_data_in_leaf': 15,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbrt\",\n         \"metric\": 'rmse'}\n\n\n\nlgb_tune_model = lgb.LGBMRegressor(**tuned_params, n_estimators = 10000, nthread = 4, n_jobs = -1)\nlgb_tune_model.fit(x_train, y_train, \n        eval_set=[(x_train, y_train), (x_test, y_test)],\n        verbose=1000, early_stopping_rounds=1000)","b05c0510":"xgb_params = {'eta': 0.01,\n              'objective': 'reg:linear',\n              'max_depth': 6,\n              'min_child_weight': 3,\n              'subsample': 0.8,\n              \n              'eval_metric': 'rmse',\n              'seed': 11,\n              'silent': True}\n\nmodel_xgb = xgb.XGBRegressor() \nmodel_xgb.fit(x_train, y_train)","3acbd990":"trainPredict_xgb = model_xgb.predict(x_train)\ntestPredict_xgb = model_xgb.predict(x_test)","418ee68f":"print(\"xgb test RMSE:\", np.sqrt(mean_squared_error(y_test, testPredict_xgb)))\nprint(\"xgb train RMSE:\", np.sqrt( mean_squared_error(y_train, trainPredict_xgb)))","3e590cf3":"\nRandom_Search_xgb_Params = {'eta': [0.01,0.02,0.03,0.04,0.05],\n              'max_depth': [3,4,5,6,7,8,9,10,11,12,13,14,15,20,25,30,35,40,45,50,60,70,80,90,100],\n              'min_child_weight': [3,4,5,6,7,8,9,10,15,20,25],\n              'subsample': [0.4,0.5,0.6,0.7,0.8,0.9,1],\n              'colsample_bytree': [0.4,0.5,0.6,0.7,0.8,0.9,1],\n              }\n\n\nrandom_search_xgb = RandomizedSearchCV(\n    estimator=xgb.XGBRegressor(), param_distributions= Random_Search_xgb_Params, \n    cv=3,\n    refit=True,\n    random_state=42,\n    verbose=True)\n\nrandom_search_xgb.fit(x_train, y_train)\n\nrandom_search_xgb.best_params_","a892c9ad":"xgb_params = {'eta': 0.04,\n              'booster': 'gbtree',\n               'max_depth': 8,\n              'min_child_weight': 4,\n              'subsample': 0.7,\n              'colsample_bytree': 0.5,\n             'eval_metric': 'rmse'}\n\nmodel_xgb_tune = xgb.XGBRegressor( params=xgb_params) \nmodel_xgb_tune.fit(x_train, y_train)\n\ntrainPredict_xgb_tune = model_xgb_tune.predict(x_train)\ntestPredict_xgb_tune = model_xgb_tune.predict(x_test)\n\nprint(\"xgb_tune test RMSE:\", np.sqrt(mean_squared_error(y_test, testPredict_xgb_tune)))\nprint(\"xgb_tune train RMSE:\", np.sqrt( mean_squared_error(y_train, trainPredict_xgb_tune)))\n","339fb731":"Linear regression model","b80680cf":"Log scaling","e13908db":"Data pre-prcessing","4349f03c":"Splitting dataset into train and test sets","7fe14f88":"spoken_languages","838888ef":"Filling na's in runtime variable","d648f36b":"Dropping unwanted columns","6e322cc2":"production_countries","5d031563":"genres","ddb7469c":"![](http:\/\/)Randomforest Regresssor","309edf66":"HOME PAGE","d654e27e":"To check Correlation between numerical variables","65f3894e":"XGBoost model","673a7f9e":"Importing  data into the Kernel by using  pandas ","1644ff7a":"ORIGINAL LANGUAGE","a52b9150":" Importing Libraries required","31ff1298":"production_companies","05348927":"Release_date","6c2fc2d4":"> parameter_tuning in Randomforest Regresssor","1910d656":"collection name","91ccb648":"lgb tuning","c54e67b1":"lgb model","327896e5":"XGBoost model tuning"}}