{"cell_type":{"36c9172f":"code","9e945bd7":"code","7eb0c380":"code","25500a46":"code","050906e0":"code","260b5afc":"code","c089949c":"code","f5e35c73":"code","92bf6ed8":"code","38c2bdeb":"code","aeb6fcc5":"code","7df33def":"code","0f572d6d":"code","954e2d95":"code","ac912d3a":"code","48385fcb":"markdown","83ba10f7":"markdown","f2c1d8fa":"markdown","2161c39f":"markdown"},"source":{"36c9172f":"import pandas as pd\nimport numpy as np\nimport re","9e945bd7":"np.random.seed(2019)","7eb0c380":"class Pack():\n    special = [\n        '\u3008PAD\u3009',\n        '\u3008UNK\u3009',\n        '\u3008NUM\u3009',\n        '\u3008STR\u3009',\n        '\u3008EOS\u3009',\n        '\u3008START\u3009',\n    ]\n    \n    def __init__(self, filename):\n        self.word_index, self.vector, self.starts, self.ends = np.load(filename)\n        self.vector += len(Pack.special)\n        self.word_index = dict(\n            **{w: i for i, w in enumerate(Pack.special)},\n            **{w: i + len(Pack.special) for w, i in self.word_index.items()}\n        )\n        self.index_word = list(self.word_index.keys())\n    \n    def __len__(self):\n        return len(self.starts)\n    \n    def __getitem__(self, i):\n        return self.vector[self.starts[i] : self.ends[i]]\n\n    def pprint(self, item):\n        print(' '.join(map(list(self.word_index).__getitem__, item)))\n    \n    def relabel(self, f):\n        new_id = {}\n        for j, vj in enumerate(self.vector):\n            self.vector[j] = vj_ = f(vj)\n            if vj_ not in new_id and vj_ >= len(Pack.special):\n                new_id[vj_] = len(new_id) + len(Pack.special)\n        for j, vj in enumerate(self.vector):\n            if vj in new_id:\n                self.vector[j] = -new_id[vj]\n        self.vector[self.vector < 0] *= -1\n        self.word_index = dict((\n            *((w, i) for i, w in enumerate(Pack.special)),\n            *sorted(((w, new_id[i]) for w, i in self.word_index.items() if i in new_id), key=lambda tup: tup[1])\n        ))\n        self.index_word = list(self.word_index.keys())\n    \n    def relabel_on_freq(self, f_infreq, f_freq, count_on, top_n):\n        cnt = np.zeros(len(self.word_index), dtype=np.int64)\n        if count_on.dtype == np.bool:\n            count_on = np.where(count_on)[0]\n        for item in count_on:\n            for i in self[item]:\n                cnt[i] += 1\n        rank = sorted(range(len(self.word_index)), key=lambda i: cnt[i], reverse=True)\n        freq = set(rank[:top_n])\n        print('freq rate when counting', sum(cnt[i] for i in freq) \/ (1e-9 + cnt.sum()))\n        cf = [0]\n        ci = [1e-9]\n        def inc(c, x):\n            c[0] += 1\n            return x\n        self.relabel(lambda i: inc(cf, f_freq(i)) if i in freq else inc(ci, f_infreq(i)))\n        print('freq rate when relabelling', cf[0] \/ (1e-9 + cf[0] + ci[0]))\n    \n    # turns a function on string\/words to that on number\/indices\n    def conjugate(self, f):\n        return lambda i: self.word_index[f(self.index_word[i])]","25500a46":"class Dataset():\n    def __init__(self, src, dst):\n        self.src = src\n        self.dst = dst\n        self.eos = self.dst.word_index['\u3008EOS\u3009']\n        self.start = self.dst.word_index['\u3008START\u3009']\n\n    def __len__(self):\n        return len(self.src.starts)\n\n    def __getitem__(self, i):\n        return self.src[i], [self.start, *self.dst[i]], [*self.dst[i], self.eos]\n    \n    def pprint(self, pair):\n        print('>>> src')\n        self.src.pprint(pair[0])\n        print()\n        print('>>> dst_in')\n        self.dst.pprint(pair[1])\n        print()\n        print('>>> dst_out')\n        self.dst.pprint(pair[2])","050906e0":"ds = Dataset(Pack('..\/input\/codes.npy'), Pack('..\/input\/comms.npy'))\nprint('src\/dst num_tokens:', len(ds.src.vector), len(ds.dst.vector))","260b5afc":"df = pd.read_csv('..\/input\/code_id.csv')\ndf['src_len'] = ds.src.ends - ds.src.starts\ndf['dst_len'] = ds.dst.ends - ds.dst.starts\nempty = (df.src_len == 0) | (df.dst_len == 0)\nprint('Empty rate', np.mean(empty))","c089949c":"df['file_long_id'] = df.repo_id * 1000000000 + df.file_id","f5e35c73":"repos = df[~empty].repo_id.unique()\nfiles = df[~empty].file_long_id.unique()\nprint('# repos', len(repos))\nprint('# files', len(files))","92bf6ed8":"def rand(x):\n    rand = np.linspace(0, 1, len(x))\n    np.random.shuffle(rand)\n    return rand","38c2bdeb":"rand_repo = rand(repos)\nrand_file = rand(files)\nrand_pair = rand(df)\n\nsplit = 0.05\ntest_repos = set(repos[(split < rand_repo) & (rand_repo < 2 * split)])\nvalid_repos = set(repos[rand_repo < split])\nvalid_files = set(files[rand_file < split])\n\n# test set for both bleu scoring and human checking, so empty output is okay (check by hand)\ntest = empty | df.repo_id.map(test_repos.__contains__)\nvalid = ~empty & df.repo_id.map(valid_repos.__contains__)\ntrain = ~valid & ~test\n\n# different validation strategy\nvalid_same_repo = train & df.file_long_id.map(valid_files.__contains__)\nvalid_same_file = train & ~valid_same_repo & (rand_pair < split)\ntrain = train & ~valid_same_repo & ~valid_same_file\n\nprint('\\t'.join('train, test, valid, s_repo, s_file'.split(', ')))\nprint('\\t'.join(map(str, map(np.sum, (train, test, valid, valid_same_repo, valid_same_file)))))","aeb6fcc5":"unk = Pack.special.index('\u3008UNK\u3009')\nds.dst.relabel_on_freq(\n    f_infreq = lambda i: unk,\n    f_freq = lambda i: i,\n    count_on = train,\n    top_n = 30000,\n)","7df33def":"def chain(f, *fs):\n    if len(fs) == 0:\n        return f\n    else:\n        return lambda i: f(chain(*fs)(i))","0f572d6d":"string_re = re.compile('^\".*\"$')\nnumber_re = re.compile('^([0-9]+\\.?[0-9]*|[0-9]*\\.[0-9]+)([eE][0-9]+)?$')\nstring_sub = lambda s: '\u3008STR\u3009' if string_re.match(s) else s\nnumber_sub = lambda s: '\u3008NUM\u3009' if number_re.match(s) else s\n#slash_sub = lambda s: s.split('\/')[0]\nto_unk_sub = lambda s: '\u3008UNK\u3009' if s not in ('\u3008STR\u3009', '\u3008NUM\u3009') else s\nds.src.relabel_on_freq(\n    f_infreq = ds.src.conjugate(chain(to_unk_sub, string_sub, number_sub)),\n    f_freq = ds.src.conjugate(chain(string_sub, number_sub)),\n    count_on = train,\n    top_n = 30000,\n)","954e2d95":"pprint_sample = lambda s: ds.pprint(ds[df[s].sample().index[0]])\npprint_sample(train & (df.src_len < 80))\nprint(); print('-' * 80); print()\npprint_sample(valid & (df.src_len < 80))\nprint(); print('-' * 80); print()\npprint_sample(test & (df.src_len < 80))","ac912d3a":"np.save('bundle.npy', (ds, df, train, test, valid, valid_same_repo, valid_same_file))","48385fcb":"### Relabel infrequent \/ literal words","83ba10f7":"### Dataset classes","f2c1d8fa":"### Load data and split on repo_ids","2161c39f":"### Save & load point for later uses"}}