{"cell_type":{"9f1f7b7d":"code","81705440":"code","9872cfd6":"code","02558725":"code","b66f322a":"code","74d0cb09":"code","2d12002b":"code","662ea51b":"code","f995b09d":"code","18ff11cd":"code","375fe2f6":"code","f8a63cd8":"code","7e86c899":"code","db00d100":"code","6ecb462a":"code","9d68c576":"code","a647e387":"code","e039699b":"code","14ef1c02":"code","57797996":"code","47a6f1da":"code","45925a12":"code","34feb921":"code","637d03a5":"markdown","03b45f56":"markdown","26bd11d4":"markdown","b93cab52":"markdown","8c28634d":"markdown","48c1a0b8":"markdown","7747ff51":"markdown","2a3aa144":"markdown","1e5c2d3d":"markdown","61c19db0":"markdown","e712f1ee":"markdown","f52a96f9":"markdown","f2c96e12":"markdown","4289be16":"markdown","133ed316":"markdown","d2188800":"markdown","0b72cb0f":"markdown","747d7a6c":"markdown","b7b10d93":"markdown","a6e6aff9":"markdown","f181b8bb":"markdown","3bb46cef":"markdown","c47eacac":"markdown","709d7332":"markdown","4ec61eb0":"markdown","039b8720":"markdown","1f5bbb32":"markdown","b30fd5e5":"markdown","ae3dae43":"markdown"},"source":{"9f1f7b7d":"### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 linea de c\u00f3digo)\ntest = \n### TERMINE EL C\u00d3DIGO AQU\u00cd ###","81705440":"print (\"test: \" + test)","9872cfd6":"# FUNCI\u00d3N A CALIFICAR basic_sigmoid\n\nimport math\n\ndef basic_sigmoid(x):\n    \"\"\"\n    Calcula el sigmoide de x\n    Input:\n    x: scalar\n    Output:\n    s: sigmoid(x)\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 linea de c\u00f3digo)\n    s =\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return s","02558725":"basic_sigmoid(3)","b66f322a":"### Una raz\u00f3n para utilizar \"numpy\" en lugar de \"math\" en Deep Learning ###\nx = [1, 2, 3]\nbasic_sigmoid(x) # esto da error, porque x es un vector","74d0cb09":"import numpy as np\n\n# ejemplo de np.exp\nx = np.array([1, 2, 3])\nprint(np.exp(x)) ","2d12002b":"# ejemplo de una operaci\u00f3n vectorial\nx = np.array([1, 2, 3])\nprint (x + 3)","662ea51b":"# FUNCI\u00d3N A CALIFICAR: sigmoid\n\nimport numpy as np # esto permite acceder funciones numpy simplemente escribiendo np.function() en lugar de numpy.function()\n\ndef sigmoid(x):\n    \"\"\"\n    Calcule el sigmoide de x\n    Input:\n    x: un escalar o arreglo numpy de cualquier tama\u00f1o\n    Output:\n    s: sigmoid(x)\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 linea de c\u00f3digo)\n    s = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return s","f995b09d":"x = np.array([1, 2, 3])\nsigmoid(x)","18ff11cd":"# FUNCI\u00d3N A CALIFICAR: sigmoid_derivative\n\ndef sigmoid_derivative(x):\n    \"\"\"\n    Calcule el gradiente (o derivada) de la funci\u00f3n sigmoide con respecto al input x.\n    Puede guardar el output del sigmoide como variables y luego usarlo para calcular el gradiente.\n    Input:\n    x: un escalar o arrgelo numpy \n    Output:\n    ds: el gradiente calculado.\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 lineas de codigo)\n    s = \n    ds = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return ds","375fe2f6":"x = np.array([1, 2, 3])\nprint (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))","f8a63cd8":"# FUNCI\u00d3N A CALIFICAR: image2vector\n\ndef image2vector(image):\n    \"\"\"\n    Input:\n    image: un arreglo numpy con forma (longitud, altura, profundidad)\n    Output:\n    v: un vector con forma (longitud*altura*profundidad, 1)\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 linea de c\u00f3digo)\n    v = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return v","7e86c899":"# Este es un arreglo de 3 por 3 por 2, usualmente las imagenes son de (num_px_x, num_px_y,3) donde 3 representa los valores RGB\nimage = np.array([[[ 0.67826139,  0.29380381],\n        [ 0.90714982,  0.52835647],\n        [ 0.4215251 ,  0.45017551]],\n\n       [[ 0.92814219,  0.96677647],\n        [ 0.85304703,  0.52351845],\n        [ 0.19981397,  0.27417313]],\n\n       [[ 0.60659855,  0.00533165],\n        [ 0.10820313,  0.49978937],\n        [ 0.34144279,  0.94630077]]])\n\nprint (\"image2vector(image) = \" + str(image2vector(image)))","db00d100":"# FUNCI\u00d3N A CALIFICAR: normalizeRows\n\ndef normalizeRows(x):\n    \"\"\"\n    Implemente una funci\u00f3n que normalize cada fila de la matriz x (para que tenga longitud unitaria).\n    Input:\n    x: Un arreglo numpy con forma (n, m)\n    Output:\n    x: La matriz numpy normalizada por filas. \n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 lineas de c\u00f3digo)\n    # Compute x_norm como la norma 2 de x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = \n    \n    # Divida a x por su norma.\n    x = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n    return x","6ecb462a":"x = np.array([\n    [0, 3, 4],\n    [1, 6, 4]])\nprint(\"normalizeRows(x) = \" + str(normalizeRows(x)))","9d68c576":"# FUNCI\u00d3N A CALIFICAR: softmax\n\ndef softmax(x):\n    \"\"\"\n    Calcule el softmax para cada fila del input x.\n    El c\u00f3digo debe funcionar tanto para un vector fila como para matrices de tama\u00f1o (n, m).\n    Input:\n    x: un arreglo numpy con forma (n,m)\n    Output:\n    s: Una matriz numpy igual al softmax de x, de tama\u00f1o (n,m)\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 3 lineas de c\u00f3digo)\n    # Utilize exp() sobre cada elemento de x. Use np.exp(...).\n    x_exp = \n\n    # Defina el vector x_sum que sume cada fila de x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = \n    \n    # Compute softmax(x) dividiendo x_exp por x_sum. Deber\u00eda usar autom\u00e1ticamente numpy broadcasting.\n    s = \n\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return s","a647e387":"x = np.array([\n    [9, 2, 5, 0, 0],\n    [7, 5, 0, 0 ,0]])\nprint(\"softmax(x) = \" + str(softmax(x)))","e039699b":"import time\n\nx1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\nx2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n\n### IMPLEMENTACION CLASICA DEL PRODUCTO PUNTO ENTRE DOS VECTORES ###\ntic = time.process_time()\ndot = 0\nfor i in range(len(x1)):\n    dot+= x1[i]*x2[i]\ntoc = time.process_time()\nprint (\"interno = \" + str(dot) + \"\\n ----- Tiempo computacional = \" + str(1000*(toc - tic)) + \"ms\")\n\n### IMPLEMENTACION CL\u00c1SICA DEL PRODUCTO EXTERIOR ###\ntic = time.process_time()\nouter = np.zeros((len(x1),len(x2))) # matriz de ceros de tama\u00f1o len(x1)*len(x2)\nfor i in range(len(x1)):\n    for j in range(len(x2)):\n        outer[i,j] = x1[i]*x2[j]\ntoc = time.process_time()\nprint (\"externo = \" + str(outer) + \"\\n ----- Tiempo computacional = \" + str(1000*(toc - tic)) + \"ms\")\n\n### IMPLEMENTACION CL\u00c1SICA POR ELEMENTOS ###\ntic = time.process_time()\nmul = np.zeros(len(x1))\nfor i in range(len(x1)):\n    mul[i] = x1[i]*x2[i]\ntoc = time.process_time()\nprint (\"multiplicaci\u00f3n por elementos = \" + str(mul) + \"\\n ----- Tiempo computacional = \" + str(1000*(toc - tic)) + \"ms\")\n\n### IMPLEMENTACION CLASICA GENERAL DEL PRODUCTO PUNTO ###\nW = np.random.rand(3,len(x1)) # Arreglo numpy aleatorio de tama\u00f1o 3*len(x1) \ntic = time.process_time()\ngdot = np.zeros(W.shape[0])\nfor i in range(W.shape[0]):\n    for j in range(len(x1)):\n        gdot[i] += W[i,j]*x1[j]\ntoc = time.process_time()\nprint (\"g_interno = \" + str(gdot) + \"\\n ----- Tiempo computacional = \" + str(1000*(toc - tic)) + \"ms\")","14ef1c02":"x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\nx2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n\n### PRODUCTO INTERNO VECTORIZADO ###\ntic = time.process_time()\ndot = np.dot(x1,x2)\ntoc = time.process_time()\nprint (\"interno = \" + str(dot) + \"\\n ----- Tiempo computacional =  \" + str(1000*(toc - tic)) + \"ms\")\n\n### PRODUCTO EXTERNO VECTORIZADO ###\ntic = time.process_time()\nouter = np.outer(x1,x2)\ntoc = time.process_time()\nprint (\"externo = \" + str(outer) + \"\\n ----- Tiempo computacional =  \" + str(1000*(toc - tic)) + \"ms\")\n\n### MULTIPLICACION POR ELEMENTOS VECTORIZADA ###\ntic = time.process_time()\nmul = np.multiply(x1,x2)\ntoc = time.process_time()\nprint (\"multiplicaci\u00f3n por elementos = \" + str(mul) + \"\\n ----- Tiempo computacional =  \" + str(1000*(toc - tic)) + \"ms\")\n\n### PRODUCTO INTERNO GENERAL VECTORIZADO ###\ntic = time.process_time()\ndot = np.dot(W,x1)\ntoc = time.process_time()\nprint (\"g_interno = \" + str(dot) + \"\\n ----- Tiempo computacional =  \" + str(1000*(toc - tic)) + \"ms\")","57797996":"# FUNCI\u00d3N A CALIFICAR: L1\n\ndef L1(yhat, y):\n    \"\"\"\n    Input:\n    yhat: vector de tama\u00f1o m (etiquetas estimadas)\n    y: vector de tama\u00f1o m (etiquetas observadas)\n    Output:\n    loss: el valor de la p\u00e9rdida L1 definida arriba\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 linea de c\u00f3digo)\n    loss = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return loss","47a6f1da":"yhat = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(\"L1 = \" + str(L1(yhat,y)))","45925a12":"# FUNCI\u00d3N A CALIFICAR: L2\n\ndef L2(yhat, y):\n    \"\"\"\n    Input:\n    yhat: vector de tama\u00f1o m (etiquetas estimadas)\n    y: vector de tama\u00f1o m (etiquetas observadas)\n    Output:\n    loss: el valor de la p\u00e9rdida L2 definida arriba\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 linea de c\u00f3digo)\n    loss = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return loss","34feb921":"yhat = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(\"L2 = \" + str(L2(yhat,y)))","637d03a5":"**Nota**:\n- Si examina la forma de x_exp, de x_sum y de s, notar\u00e1 que x_sum es de tama\u00f1o (2,1) mientras que x_exp y s son de forma (2,5). **x_exp\/x_sum** funciona gracias a python broadcasting.\n\nAhora tiene un conocimiento b\u00e1sico de python numpy y ha implementado algunas funciones \u00fatiles que se utilizan en deep learning.","03b45f56":"## 2) Vectorizaci\u00f3n","26bd11d4":"De hecho, $ x = (x_1, x_2, ..., x_n)$ es un vector fila, donde $np.exp(x)$ aplica la funci\u00f3n exponencial a cada elemento de x. La salida ser\u00e1: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$","b93cab52":"**Salida esperada**:\n\n<table style=\"width:60%\">\n\n     <tr> \n       <td> **normalizeRows(x)** <\/td> \n       <td> [[ 0.          0.6         0.8       ]\n [ 0.13736056  0.82416338  0.54944226]]<\/td> \n     <\/tr>\n    \n   \n<\/table>","8c28634d":"### 1.4 - Normalizaci\u00f3n de filas\n\nOtra t\u00e9cnica muy utilizada en Machine Learning y Deep Learning es normalizar los datos. Usualmente lleva a una mejor desempe\u00f1o porque el descenso en la direcci\u00f3n del gradiente converge m\u00e1s r\u00e1pidamente tras la normalizaci\u00f3n. Por normalizaci\u00f3n nos referimos aqu\u00ed a transformar x de acuerdo con la expresi\u00f3n $ \\frac{x}{\\| x\\|} $ (dividiendo cada vector file de x por su norma).\n\nPor ejemplo, si $$x = \n\\begin{bmatrix}\n    0 & 3 & 4 \\\\\n    2 & 6 & 4 \\\\\n\\end{bmatrix}\\tag{3}$$ entonces $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n    5 \\\\\n    \\sqrt{56} \\\\\n\\end{bmatrix}\\tag{4} $$ y        $$ x\\_normalizado = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n\\end{bmatrix}\\tag{5}$$ N\u00f3tese que se pueden dividir matrices de distinto tama\u00f1o y funciona sin problema: esto es llamado \"broadcasting\" y lo veremos m\u00e1s adelante.\n\n\n**Ejercicio**: Implemente normalizeRows() para normalizar las filas de una matriz. Luego de aplicar esta funci\u00f3n sobre una matriz x, cada fila de x debe ser una vector de longitud unitaria (longitud=1). ","48c1a0b8":"## Sobre los Cuadernos iPython ##\n\nLos Cuadernos iPython son ambientes de programaci\u00f3n de c\u00f3digo interactivos montados en una p\u00e1gina web. En esta clase utilizaremos cuadernos iPython. S\u00f3lo necesita escribir c\u00f3digo entre los comentarios de ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### y ### TERMINE EL C\u00d3DIGO AQU\u00cd ###. Tras escribir el c\u00f3digo, puede ejecutar la celda presionando \"SHIFT\"+\"ENTER\" o haciendo click en \"Run\" (s\u00edmbolo de \"play\") en la barra superior del cuaderno.  \n\nSe especificar\u00e1 en los comentarios aproximadamente cuantas lineas de codigo necesita escribir \"(\u2248 X lineas de codigo)\". Es solo una gu\u00eda, no pasa nada si escribe menos o m\u00e1s lineas siempre que el codigo ahga lo que debe hacer.\n\n**Ejercicio**: Defina test como `\"Hola Mundo\"` en la celda de abajo para imprimir \"Hello World\" y ejecute las dos celdas abajo. ","7747ff51":"Como se puede ver, la implementaci\u00f3n vectorizada es m\u00e1s limpia y eficiente. Para vectores\/matrices m\u00e1s grandes, las diferencias en tiempo computacional ser\u00e1n mayores. \n\n**Nota:** \n`np.dot()` desarrolla una multiplicaci\u00f3n de matriz-matriz o matriz-vector. Esto es distinto a `np.multiply()` y el operador `*`, que aplica una multiplicaci\u00f3n por elementos.","2a3aa144":"**Ejercicio**: Implemente la versi\u00f3n numpy vectorizada de p\u00e9rdida L2. Hay varias maneras de implementarla, pero puede encontrar \u00fatil la funci\u00f3n np.dot(). Como ayuda, si $x = [x_1, x_2, ..., x_n]$, entoncesn `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n\n- La p\u00e9rdida L2 se define como $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$","1e5c2d3d":"### 1.5 - Broadcasting y la funci\u00f3n softmax ####\nUn concepto muy importante para entender numpy es el de \"broadcasting\". Es muy \u00fatil para implementar operaciones matem\u00e1ticas entre areglos de distintos tama\u00f1os. Para mayores detalles se puede ver la documentaci\u00f3n oficial [broadcasting](http:\/\/docs.scipy.org\/doc\/numpy\/user\/basics.broadcasting.html).","61c19db0":"**Salida esperada**: \n<table>\n    <tr> \n        <td> **sigmoid([1,2,3])**<\/td> \n        <td> array([ 0.73105858,  0.88079708,  0.95257413]) <\/td> \n    <\/tr>\n<\/table> \n","e712f1ee":"**Ejercicio**: Implemente la funci\u00f3n softmax utilizando numpy. Puede entender el softmax como una funci\u00f3n de normalizaci\u00f3n utilizada cuando su algoritmo necesita clasificar dos o m\u00e1s clases. Aprender\u00e1 m\u00e1s sobre softmax en ejercicios posteriores.\n\n**Instrucciones**:\n- $ \\text{Para } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n    x_1  &&\n    x_2 &&\n    ...  &&\n    x_n  \n\\end{bmatrix}) = \\begin{bmatrix}\n     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n    ...  &&\n    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n\\end{bmatrix} $ \n\n\n\n- $\\text{Para una matriz } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ mapea el elemento en la $i-{\u00e9sima}$ fila y la  $j-{\u00e9sima}$ columna de $x$, por lo que se obtiene: }$  \n\n\n$$softmax(x) = softmax\\begin{bmatrix}\n    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n\\end{bmatrix} = \\begin{bmatrix}\n    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n\\end{bmatrix} = \\begin{pmatrix}\n    softmax\\text{(primera fila de x)}  \\\\\n    softmax\\text{(segunda fila de x)} \\\\\n    ...  \\\\\n    softmax\\text{(\u00faltima fila de x)} \\\\\n\\end{pmatrix} $$","f52a96f9":"Si x es un vector, entonces la operaci\u00f3n de Python $s = x + 3$ o $s = \\frac{1}{x}$ obtiene como resultado s como un vector del mismo tama\u00f1o que x.","f2c96e12":"**Salida esperada**:\n\n\n<table>\n    <tr> \n        <td> **sigmoid_derivative([1,2,3])**<\/td> \n        <td> [ 0.19661193  0.10499359  0.04517666] <\/td> \n    <\/tr>\n<\/table> \n\n","4289be16":"Espero te haya gustado este Notebook.\nPor favor compartelo para que entre todos aprendamos juntos.\n\nPuedes Seguirme a mi cuenta en Twitter **[@andres_jejen](https:\/\/twitter.com\/andres_jejen)** Constantemente comparto noticias y contenido educativo sibre Machine Learning, Big Data y Data Science.","133ed316":"\nEn deep learning, se trabaja con grandes volumenes de datos. Por lo tanto, una funci\u00f3n computacionalmente ineficiente puede convertirse en un gran cuello de botella de su algoritmo y el modelo puede tardar demasiado en correr. Para asegurarse que su c\u00f3digo es computacionalmente eficiente, usaremos la vectorizaci\u00f3n. Por ejemplo, determine la diferencia entre las siguientes implementaciones del producto interno, externo entre matrices y la multiplicaci\u00f3n por elementos.","d2188800":"**Salida esperada**: \n\n\n<table style=\"width:100%\">\n     <tr> \n       <td> **image2vector(image)** <\/td> \n       <td> [[ 0.67826139]\n [ 0.29380381]\n [ 0.90714982]\n [ 0.52835647]\n [ 0.4215251 ]\n [ 0.45017551]\n [ 0.92814219]\n [ 0.96677647]\n [ 0.85304703]\n [ 0.52351845]\n [ 0.19981397]\n [ 0.27417313]\n [ 0.60659855]\n [ 0.00533165]\n [ 0.10820313]\n [ 0.49978937]\n [ 0.34144279]\n [ 0.94630077]]<\/td> \n     <\/tr>\n    \n   \n<\/table>","0b72cb0f":"**Salida esperada**:\n\n<table style=\"width:60%\">\n\n     <tr> \n       <td> **softmax(x)** <\/td> \n       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n    1.21052389e-04]\n [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n    8.01252314e-04]]<\/td> \n     <\/tr>\n<\/table>\n","747d7a6c":"En verdad la \"math\" library suele utilizarse poco en deep learning porque los inputs de las funciones son n\u00fameros reales. En deep learning su utilizan m\u00e1s que todo matrices y vectores. Por esto es que numpy es m\u00e1s \u00fatil. ","b7b10d93":"**Nota**:\nAl calcular x_norm, se obtiene la norma de cada fila de x. Luego, x_norm tiene el mismo n\u00famero de filas que normalizeRows(x) pero s\u00f3lo una columna. Al dividir x por x_norm, se est\u00e1 aplicando el \"broadcasting\", que veremos a continuaci\u00f3n.  ","a6e6aff9":"**Salida esperada**:\n<table style=\"width:20%\">\n     <tr> \n       <td> **L2** <\/td> \n       <td> 0.43 <\/td> \n     <\/tr>\n<\/table>","f181b8bb":"**Salida esperada**: \n<table style = \"width:40%\">\n    <tr>\n    <td>** basic_sigmoid(3) **<\/td> \n        <td>0.9525741268224334 <\/td> \n    <\/tr>\n\n<\/table>","3bb46cef":"# Taller 1. Introducci\u00f3n a Python con Numpy \n\nBienvenido al primer taller. Contiene ejercicios para una breve introducci\u00f3n a Python. Si ya ha utilizado Python antes, este taller le ayudar\u00e1  a familiarizarse con las funciones que necesitamos.  \n\n**Instrucciones:**\n- Se utilizar\u00e1 Python 3.\n- Evite utilizar bucles-for y bucles-while, a menos que expl\u00edcitamente se le pida hacerlo.\n- No modifique el comentario (# FUNCI\u00d3N A CALIFICAR [nombre de la funcion]) de algunas celdas. Es neceario para su calificaci\u00f3n. Cada celda que contenga ese comentario debe contener solo una funci\u00f3n.  \n- Tras codificar su funci\u00f3n, verifique que el resultado es correcto. \n\n**Tras este taller usted va a ser capaz de:**\n- Usar Cuadernos iPython \n- Utilizar funciones numpy y operaciones numpy sobre matrices\/vectores\n- Entender el concepto de \"broadcasting\"\n- Vectorizar el c\u00f3digo\n\nManos a la obra!!","c47eacac":"### 1.2 - Gradiente del sigmoide\n\nComo hemos visto, se debe calcular gradientes para optimizar las funciones de coste usando retro-propagaci\u00f3n. A continuaci\u00f3n se computa la funci\u00f3n del gradiente. \n\n**Ejercicio**: Implemente la funci\u00f3n sigmoid_grad() para computar el gradiente de la funci\u00f3n sigmoide con respecto a su input x. La f\u00f3rmula es: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\nEsta funci\u00f3n se puede programar en dos pasos:\n1. Defina s como el sigmoide de x. Puede utilizar la funcion sigmoid(x) ya programada.\n2. Compute $\\sigma'(x) = s(1-s)$","709d7332":"M\u00e1s informaci\u00f3n sobre la funci\u00f3n numpy [documentaci\u00f3n oficial](https:\/\/docs.scipy.org\/doc\/numpy-1.10.1\/reference\/generated\/numpy.exp.html). \n\nTambi\u00e9n puede escribir en una nueva celda `np.exp?` y acceder a la documentaci\u00f3n.\n\n**Ejercicio**: Implemente la funcion sigmoide utilizando numpy. \n\n**Instrucciones**: x puede ser o un n\u00famero real, un vector, o una matriz. A las estructuras de datos que se utilizan en numpy para representar estas formas (vectores, matrices,...) se les denominan arreglos numpy.\n\n$$ \\text{Para } x \\in \\mathbb{R}^n \\text{,     } sigmoide(x) = sigmoide\\begin{pmatrix}\n    x_1  \\\\\n    x_2  \\\\\n    ...  \\\\\n    x_n  \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n    \\frac{1}{1+e^{-x_1}}  \\\\\n    \\frac{1}{1+e^{-x_2}}  \\\\\n    ...  \\\\\n    \\frac{1}{1+e^{-x_n}}  \\\\\n\\end{pmatrix}\\tag{1} $$","4ec61eb0":"**Salida esperada**:\ntest: Hola Mundo","039b8720":"### 1.3 - Reformando arreglos ###\n\nDos funciones numpy comunes usadas en deep learning son [np.shape](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.ndarray.shape.html) y [np.reshape()](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.reshape.html). \n- X.shape es usado para obtener la forma (dimension) de una matriz\/vector X. \n- X.reshape(...) es usado para reformar X en alguna otra dimensi\u00f3n. \n\nPor ejemplo, en ciencia computacional, un aimagen es representada por un arreglo en 3D con forma $(longitud, altura, profundidad= 3)$. Sin embargo, cuando se lee una imagen como el input de un algoritmo, se convierte en un vector con forma $(longitud*altura*3, 1)$. En otras palabras, se desenrolla o reforma el arreglo 3D en un vector 1D.\n\n**Ejercicio**: Implemente la funci\u00f3n `image2vector()` que toma un input de forma (longitud, altura, 3) y devuelve un vector de form (longitud\\*altura\\*3, 1). Por ejemplo, si quisiera deformar un arreglo v con forma (a, b, c) en un vector con forma (a*b,c), se escribir\u00eda:\n``` python\nv = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n```\n- N\u00f3tese que cada imagen tiene sus propias dimensiones, las cuales se pueden averiguar mediante  `image.shape[0]`, etc. ","1f5bbb32":"## 1 - Construyendo funciones b\u00e1sicas con numpy ##\n\nNumpy es el paquete principal para la ciencia computacional en Python (www.numpy.org). En este ejercicio aprender\u00e1 algunas funciones numpy claves tal como np.exp, np.log, y np.reshape. Va a necesitar saber c\u00f3mo utilizar estas funciones para talleres futuros.\n\n### 1.1 - Funci\u00f3n sigmoide, np.exp() ###\n\nAntes de utilizar np.exp(), utilizaremos math.exp() para implementar la funci\u00f3n sigmoide. Entonce spodr\u00e1 ver porqu\u00e9 np.exp() es preferible a math.exp().\n\n**Ejercicio**: Construya una funci\u00f3n que devuelva el sigmoide de un n\u00famero real x. Utilize math.exp(x) para la funci\u00f3n exponencial.\n\n**Ayuda**:\n$sigmoid(x) = \\frac{1}{1+e^{-x}}$ se le conoce como la funci\u00f3n log\u00edstica. Es una funci\u00f3n no-lineal utilizada tanto en Machine Learning (Regresi\u00f3n Logistica), como en Deep Learning.\n\nPara referirse a una funci\u00f3n de cierto paquete, la puede llamar utilizando package_name.function(). Ejecute el c\u00f3digo abajo para trabajar con math.exp().","b30fd5e5":"**Salida esperada**:\n\n<table style=\"width:20%\">\n\n     <tr> \n       <td> **L1** <\/td> \n       <td> 1.1 <\/td> \n     <\/tr>\n<\/table>\n","ae3dae43":"### 2.1 Implementaci\u00f3n de funciones de coste L1 y L2\n\n**Ejercicio**: Implemente la versi\u00f3n numpy vectorizada de p\u00e9rdida L1. Puede utilizar la funci\u00f3n abs(x) (valor absoluto de x).\n\n**Ayuda**:\n- La p\u00e9rdida o funci\u00f3n de coste es utilizada para evaluar el desempe\u00f1o del modelo. Entre m\u00e1s grande la p\u00e9rdida, mayor ser\u00e1 la diferencia entre las predicciones ($ \\hat{y} $) y los valores observados ($y$). En deep learning, se utilizan algoritmos de optimizaci\u00f3n como Descenso en la direcci\u00f3n del gradiente (G.D.) para entrenar el modelo y minimizar la p\u00e9rdida.\n- La p\u00e9rdida L1 se define como:\n$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"}}