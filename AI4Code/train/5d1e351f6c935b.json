{"cell_type":{"ee091822":"code","1df9aa04":"code","0068e5c3":"code","34252a91":"code","2e869657":"code","5ee93d56":"code","7d127e16":"code","4d141ada":"code","a7bffd58":"code","67e62fd6":"code","160c0d55":"markdown","14bb7da4":"markdown","5e729425":"markdown","e8639a8e":"markdown","ecf8d440":"markdown","bd10a201":"markdown","64f16ded":"markdown","3ab6eb72":"markdown","7d17894e":"markdown","da5f61f1":"markdown","8693ee83":"markdown"},"source":{"ee091822":"library(tidyverse)\nlibrary(tidymodels)","1df9aa04":"\nloaded_df <- read_rds('..\/input\/efasvsentsoe-inflow20152019\/AT_EFAS-3.5_vs_ENTSOE-INFLOW_2015-2019.rds')\ndim(loaded_df)\n","0068e5c3":"\nglimpse(loaded_df %>% select(1:10))\n","34252a91":"all_df <- loaded_df %>% \n          select(-year, -week, -time_id, -area)","2e869657":"  base_recipe <- recipe(entsoe_inflow ~ ., x = all_df) %>%\n    step_nzv(all_numeric()) %>%\n    step_center(all_numeric()) %>%\n    step_scale(all_numeric()) %>%\n    step_corr(all_numeric(), \n              threshold = 0.95)\nprint(base_recipe)","5ee93d56":"corrplot::corrplot(\n    cor(\n        all_df %>% \n        select(-entsoe_inflow)\n    ), \n    order = 'hclust', # columns shown with hierarchical clustering order\n    addgrid.col = NA, \n    tl.pos = 'n'\n)","7d127e16":"rf_model <- rand_forest(mtry = tune(), trees = 500) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"regression\") \nprint(rf_model)","4d141ada":"  formula_res <-\n    rf_model %>% \n    tune_grid(\n      base_recipe,\n      resamples = vfold_cv(all_df, v = 10, repeats = 1), \n      grid = tibble(mtry = c(1, 5, 10, 15, 20)),\n      control = control_resamples(save_pred = TRUE)\n    )","a7bffd58":"  collect_metrics(formula_res)\n","67e62fd6":"collect_predictions(formula_res) %>% \ndplyr::filter(mtry == 10) %>%\nggplot(aes(x = entsoe_inflow, y = .pred)) +\ngeom_point(alpha = 0.5) +\ngeom_abline(slope = 1) +\nlabs(x = 'target', y = 'RF prediction') +\ntheme_light(base_size = 18)","160c0d55":"\nThe dataset has 261 weekly time-steps (roughly 5 years) and 189 columns. We will visualise the first 10. \n","14bb7da4":"The last step of the recipe is particularly important because some of the river discharges can be highly correlated due to their close location. We can plot a heatmap of the correlation among the inputs to see that we have many plants with very high correlation.","5e729425":"## Using hydrological models to reconstruct hydropower generation\n\n### Why?\n\nNowadays, meteorological phenomena play a very important role on power systems, especially in Europe where the [penetration of renewable sources is increasing drastically](https:\/\/ec.europa.eu\/eurostat\/statistics-explained\/index.php\/Renewable_energy_statistics#Share_of_renewable_energy_more_than_doubled_between_2004_and_2019). In fact, all the most important renewable sources of electricity are linked with weather variables: wind for wind turbines, sunshine with solar power and precipitation (rain and snow) for hydropower.\n\nHydropower is the second most important source of renewable energy in EU (348.6 TWh in 2018) and, thanks to its capacity to accumulate energy in the water reservoirs, it has the fundamental role of providing stability and flexibility to the system.\n\nModelling hydropower is then very important for the planning of future power systems and simulating their behaviour. Then, to introduce the impact of climate variability (and change) in the modelling of hydropower it is necessary to have data of the availability of water for several years (the more the better).\n\n### What?\n\nThis notebook shows an example of how to use time-series of river discharge to model observed weekly inflow (the energy available from water) in European countries.\n\nThe input data are the following:\n\n1.  River discharge data from the European Flood Awareness System (EFAS) provided by the Copernicus Climate Change Service (C3S). The data is [freely available in the Climate Data Store](https:\/\/cds.climate.copernicus.eu\/cdsapp#!\/dataset\/efas-historical?tab=overview). The data is available as hourly gridded data at the resolution of 5 km. Time-series of river discharge are extracted at the location of all the European hydropower plants, which coordinates are available in the...\n\n2.  ...JRC Hydropower plants database. This dataset contains all the coordinates and the characteristics of the European hydropower plants and [it is available with open license](https:\/\/data.jrc.ec.europa.eu\/dataset\/52b00441-d3e0-44e0-8281-fda86a63546d).\n\n3.  Observed inflow data obtained from the [ENTSO-E Transparency Platform](https:\/\/transparency.entsoe.eu\/). The platform does not provide directly the inflows but they are derived by the [actual generation of hydropowe](https:\/\/transparency.entsoe.eu\/generation\/r2\/actualGenerationPerProductionType\/show)r and the [weekly reservoir levels](https:\/\/transparency.entsoe.eu\/generation\/r2\/waterReservoirsAndHydroStoragePlants\/show).\n\n### How?\n\nThe inputs (river discharges) and the output (inflows) for each country are used to calibrate machine-learning models using the [`tidymodels` framework](https:\/\/www.tidymodels.org\/).\n\nThe final dataset is available as [JRC-EFAS-Hydropower](https:\/\/zenodo.org\/record\/4086004#.YCBQpXnTWt8) with open license. The dataset has been generated with a similar workflow [described in this article](https:\/\/eartharxiv.org\/repository\/view\/1735\/).\n\n## Workflow\n\nThis notebook will use tidyverse and tidymodels functions, so we can load both the metapackages.","e8639a8e":"We perform the grid search assessing the performance of each hyperparameter's value through a 10-fold cross-validation. We also save the predictions for later. ","ecf8d440":"We will show the RMSE and the R-squared computed in cross-validation. ","bd10a201":"We can define our first recipe:\n1. remove all the columns with near zero variance\n2. center and scale the columns\n3. remove the features highly correlated (correlation > 0.95)","64f16ded":"The target variable is `entsoe_inflow` and the columns `year` and `week` store the information about the year and the week of the data. The `time_id` column is just an identifier created merging year and week. Then we have all the columns with the average weekly river discharge for the hydro power plants in Austria (the column name is the ID of the hydro power plant in the [JRC Hydro-power database](https:\/\/github.com\/energy-modelling-toolkit\/hydro-power-database))","3ab6eb72":"Now we can start implementing the usual `tidymodels` workflow. Firstly, we remove the temporal columns in the data frame and also the `area` columns that simply store the area name (`AT` in this case). ","7d17894e":"We can then collect the predictions of the combination with the best performance and plot a scatterplot. ","da5f61f1":"\nThe data is available per country and it is already formatted in a data frame stored in Parquet files, one per country. Those files store the weekly sets of river discharges for the hydropower plants and the national generation of the reservoir-based hydropower (thus excluding run-of-river and pumping stations).\n\nWe will load the file for Austria (AT). Here we load the file in the R binary format (RDS) because the `arrow` package to read Parquet format cannot be installed in a Kaggle notebook. \n","8693ee83":"Now we define a Random Forest model to predict the inflow from the river discharge. The parameter `mtry` is not set because it will be chosen using a grid search."}}