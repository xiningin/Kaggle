{"cell_type":{"6cd81a5d":"code","d892668c":"code","bec0e699":"code","b563ef56":"code","64d28286":"code","45ba34b1":"code","0a79deb4":"code","f07f4e99":"code","d7349b65":"code","fbd1a8b3":"code","dcd1fa4f":"code","2adad0bc":"code","acb70516":"code","b5e42224":"code","ee993d21":"code","17edc231":"code","e7bf7b97":"code","f652fa19":"code","e79ad1a0":"code","fbdccd56":"code","6628a788":"code","feb45a3b":"code","2431dd1e":"code","b3a7355b":"code","730b74f4":"code","26450c95":"code","95e435d8":"code","d07907b8":"code","3b5c72b4":"code","eea63cd6":"code","d2533356":"code","1c453ac6":"code","76dce5ec":"code","ab9ec3b8":"code","13aefa72":"code","ca401567":"code","c96a6e6f":"code","a5e3f9bb":"code","0dfe53f8":"code","c1d865ac":"code","cc31bfd6":"code","323e8ace":"code","e6ac33e0":"code","da1f9fed":"code","a0eb403b":"code","ff85ac1a":"code","c2751aaa":"markdown","9b95aec8":"markdown","ffe09778":"markdown","60fdb4f6":"markdown","d1fa4c2c":"markdown","9708bcf0":"markdown","b90f6164":"markdown","7802a2cb":"markdown","e9743eca":"markdown","4c3049ee":"markdown","58f10825":"markdown","b9137e1a":"markdown","c9e381bf":"markdown","14995520":"markdown","52328e6a":"markdown","fd87f10c":"markdown","f067bac3":"markdown","3d9499b5":"markdown","68c6403b":"markdown","c0234475":"markdown","44931569":"markdown","41ca2472":"markdown"},"source":{"6cd81a5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d892668c":"import os\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# # Time series decomposition\n# from stldecompose import decompose\n\n# Chart drawing\nimport plotly as py\nimport plotly.io as pio\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n\n# Mute sklearn warnings\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\nsimplefilter(action='ignore', category=DeprecationWarning)\n\n# Show charts when running kernel\ninit_notebook_mode(connected=True)\n\n# Change default background color for all visualizations\nlayout=go.Layout(paper_bgcolor='rgba(255,255,2555,0.5)', plot_bgcolor='rgba(250,250,250,0.8)')\nfig = go.Figure(layout=layout)\ntemplated_fig = pio.to_templated(fig)\npio.templates['my_template'] = templated_fig.layout.template\npio.templates.default = 'my_template'","bec0e699":"ETF_NAME = 'CERN'\nETF_DIRECTORY = '\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/Stocks\/'\nYEAR = 2010\n\n\ndf = pd.read_csv(os.path.join(ETF_DIRECTORY, ETF_NAME.lower() + '.us.txt'), sep=',')\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df[(df['Date'].dt.year >= YEAR)].copy()\ndf.index = range(len(df))\n\ndf.head()","b563ef56":"fig = make_subplots(rows=2, cols=1)\n\nfig.add_trace(go.Ohlc(x=df.Date,\n                      open=df.Open,\n                      high=df.High,\n                      low=df.Low,\n                      close=df.Close,\n                      name='Price'), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=df.Date, y=df.Volume, name='Volume'), row=2, col=1)\n\nfig.update(layout_xaxis_rangeslider_visible=False)\nfig.show()","64d28286":"# Relative Strength Index\ndef compute_momentum(ser):\n  '''\n  Momentum_1D = P(t) - P(t-1)\n  '''\n  ser['Momentum_1D'] = (ser['Close'] - ser['Close'].shift(1)).fillna(0)\n  return ser\n\n\ndef rsi(values):\n  '''\n  Relative Strength Index\n  '''\n  up = values[values>0].mean()\n  down = -1*values[values<0].mean()\n  return 100 * up \/ (up + down)\n\n\ndef compute_rsi(ser, window=14):\n  df = compute_momentum(ser)\n  return df['Momentum_1D'].rolling(center=False, window=window).apply(rsi).fillna(0)\n\n\n# df['RSI_7'] = compute_rsi(df, 7).fillna(0)\n# df['RSI_14'] = compute_rsi(df, 14).fillna(0)\n\n# fig = go.Figure(go.Scatter(x=df.Date, y=df.RSI_14, name='RSI'))\n# fig.show()\n\n\n\n# MACD\ndef MACD(ser):\n    ser['EMA_26'] = ser['Close'].ewm(span=26,min_periods=0,adjust=True,ignore_na=False).mean()\n    ser['EMA_12'] = ser['Close'].ewm(span=12,min_periods=0,adjust=True,ignore_na=False).mean()\n    ser['MACD'] = ser['EMA_12'] - ser['EMA_26']\n    ser['MACD_signal'] = pd.Series(ser.MACD.ewm(span=9, min_periods=9).mean())\n    ser = ser.fillna(0)\n     \n\n\n# Bollinger Bands\ndef bbands(price, length=30, numsd=2):\n    \"\"\" returns average, upper band, and lower band\"\"\"\n    #ave = pd.stats.moments.rolling_mean(price,length)\n    ave = price.rolling(window = length, center = False).mean()\n    #sd = pd.stats.moments.rolling_std(price,length)\n    sd = price.rolling(window = length, center = False).std()\n    upband = ave + (sd*numsd)\n    dnband = ave - (sd*numsd)\n    return np.round(ave,3), np.round(upband,3), np.round(dnband,3)\n\n\ndef compute_BB(ser):\n  '''\n\n  '''\n  ser['BB_Middle_Band'], ser['BB_Upper_Band'], ser['BB_Lower_Band'] = bbands(ser['Close'], length=20, numsd=1)\n  ser['BB_Middle_Band'] = ser['BB_Middle_Band'].fillna(0)\n  ser['BB_Upper_Band'] = ser['BB_Upper_Band'].fillna(0)\n  ser['BB_Lower_Band'] = ser['BB_Lower_Band'].fillna(0)\n  #return ser \n\n\n\n# Triple Exponential MA \ndef compute_ema_and_tema(ser):\n  ser['EMA_3D'] = ser['Close'].ewm(span=3, min_periods=0, adjust=True, ignore_na=False).mean()\n  ser['EMA_9D'] = ser['Close'].ewm(span=9, min_periods=0, adjust=True, ignore_na=False).mean()\n  ser['diff_EMA'] = ser['EMA_3D'] - ser['EMA_9D']\n  ser['SMA_5'] = ser['Close'].rolling(5).mean().shift()\n  ser['TEMA'] = (3 * ser['EMA_3D'] - 3 * ser['EMA_3D'] * ser['EMA_3D']) + (ser['EMA_3D']*ser['EMA_3D']*ser['EMA_3D'])\n  return ser \n\n\n# Stochastic\ndef compute_stochastic(df, n):\n    df['ST_K'] = ((df['Close'] - df['Low'].rolling(window=n, center=False).mean()) \/ (df['High'].rolling(window=n, center=False).max() - df['Low'].rolling(window=n, center=False).min())) * 100\n    df['ST_D'] = df['ST_K'].rolling(window = 3, center=False).mean()\n    df = df.fillna(0)\n\n\n# set moving average\ndef set_MA(df):\n    df['EMA_3'] = df['Close'].ewm(3).mean().shift()\n    df['EMA_8'] = df['Close'].ewm(8).mean().shift()\n\n    df['SMA_5'] = df['Close'].rolling(5).mean().shift()\n    df['SMA_13'] = df['Close'].rolling(13).mean().shift()\n\n\n    \n\n\ndef set_indicator(df):\n    df['RSI_7D'] = compute_rsi(df, 7)\n    df['RSI_14D'] = compute_rsi(df, 14)\n    MACD(df)\n    \n    compute_stochastic(df, 4)\n    compute_BB(df)\n    set_MA(df)\n    return df\n    \n\nset_MA(df)\ndf = set_indicator(df)\ndf.tail()\n\n#compute_BB(df)","45ba34b1":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df.Date, y=df.EMA_3, name='EMA 9'))\nfig.add_trace(go.Scatter(x=df.Date, y=df.EMA_8, name='EMA 8'))\n\nfig.add_trace(go.Scatter(x=df.Date, y=df.SMA_5, name='SMA 5'))\nfig.add_trace(go.Scatter(x=df.Date, y=df.SMA_13, name='SMA 13'))\n\nfig.add_trace(go.Scatter(x=df.Date, y=df.Close, name='Close', opacity=0.2))\nfig.show()","0a79deb4":"fig = make_subplots(rows=2, cols=1)\nfig.add_trace(go.Scatter(x=df.Date, y=df.Close, name='Close'), row=1, col=1)\nfig.add_trace(go.Scatter(x=df.Date, y=df.EMA_12, name='EMA 12'), row=1, col=1)\nfig.add_trace(go.Scatter(x=df.Date, y=df.EMA_26, name='EMA 26'), row=1, col=1)\nfig.add_trace(go.Scatter(x=df.Date, y=df['MACD'], name='MACD'), row=2, col=1)\nfig.add_trace(go.Scatter(x=df.Date, y=df['MACD_signal'], name='Signal line'), row=2, col=1)\nfig.show()","f07f4e99":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df.index, y=df.Close, name='Close',  marker_color='rgb(0, 0, 150)'))\nfig.add_trace(go.Scatter(x=df.index, y=df.BB_Middle_Band, name='BB_mid' , marker_color='rgb(0, 150, 0)'))\n\nfig.add_trace(go.Scatter(x=df.index, y=df['BB_Lower_Band'], mode='lines', name='BB_lower', marker_color='rgb(205, 188, 199)')) \nfig.add_trace(go.Scatter(x=df.index, y=df['BB_Upper_Band'], fill='tonexty', name='BB_upper', marker_color='rgb(205, 188, 199)') )\n\nfig.show()","d7349b65":"# Params \nWINDOW = 13 # sequence length\n\nbatch_size = 512          # Batch size (you may try different values)\nepochs = 200               # Epoch (you may try different values)\n#seq_len = seq_len             # sequence data (Representing the last 30 days)\nloss='mean_squared_error' # Since the metric is MSE\/RMSE\noptimizer = 'rmsprop'     # Recommended optimizer for RNN\nactivation = 'relu'       # Linear activation\ninput_shape=(None,1)      # Input dimension\noutput_dim = 40           # Output dimension\n\n\n# optimizer = keras.optimizers.RMSprop(\n#     learning_rate=0.01,\n#     rho=0.9,\n#     momentum=0.0,\n#     epsilon=1e-07,\n#     centered=False,\n#     name=\"RMSprop\"\n# )\n\noptimizer='rmsprop'","fbd1a8b3":"def train_test_split(df, train_length=0.90):\n    '''\n    \/!\\ Dont forget WINDOW PARAMS\n    '''\n    row = int(np.floor(df.shape[0]*train_length))\n    train = df[:row]\n    valid = df[row:]\n    \n    test = valid[-(WINDOW*2):] # to change\n    valid = valid[:-(WINDOW*2)] # to change\n    print(\"Train : \", train.shape)\n    print(\"Test : \", test.shape)\n    print(\"Validation : \", valid.shape)\n    return train, test, valid, row\n\n\ndef create_timeseries(data, seq_len=30, stride=1):\n  '''\n  dataframe to timeseries. TimeseriesGenerator could be use instead\n  '''\n  price_matrix = []\n  for index in range(0, len(data)-seq_len+1, stride):\n      price_matrix.append(data.Close[index:(index+seq_len)]) # stride = 1\n  price_matrix = np.array(price_matrix)\n  return price_matrix\n  \n\ndef normalize_windows(window_data):\n    '''\n    It normalizes each value to reflect the percentage changes from starting point\n    '''\n    normalised_data = []\n    for window in window_data:\n        normalised_window = [((float(p) \/ float(window[0])) - 1) for p in window]\n        normalised_data.append(normalised_window)\n    return np.array(normalised_data)","dcd1fa4f":"train, test, valid, row = train_test_split(df, 0.80)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train.Date, y=train.Close, name='Training'))\nfig.add_trace(go.Scatter(x=test.Date, y=test.Close, name='Test'))\nfig.add_trace(go.Scatter(x=valid.Date, y=valid.Close, name='Validation'))\nfig.show()","2adad0bc":"train_ts = create_timeseries(train, WINDOW)\nvalid_ts = create_timeseries(valid, WINDOW)\n\nX_train = normalize_windows(train_ts)\nX_valid = normalize_windows(valid_ts)\n\n# We want to predict the j+1 price\nX_train, y_train = X_train[:row,:-1], X_train[:row,-1]\nX_valid, y_valid = X_valid[:row,:-1], X_valid[:row,-1]\n\n# reshaping [batch_size, n_steps, variability]\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))","acb70516":"from tensorflow import keras \nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation, GRU, Dropout, BatchNormalization\nimport time\n\nmodel = Sequential()\nmodel.add(LSTM(units=output_dim, return_sequences=True, input_shape=input_shape, dropout=0.2))\nmodel.add(Dense(units=64,activation=activation))\nmodel.add(LSTM(units=output_dim, return_sequences=False, dropout=0.2))\nmodel.add(Dense(units=1,activation=activation))\nmodel.compile(optimizer=optimizer,loss=loss)\n\n\nmodel1 = Sequential()\nmodel1.add(keras.layers.Conv1D(filters=20, kernel_size=7, strides=2, padding=\"valid\", input_shape=[None, 1]) )\nmodel1.add(GRU(units=output_dim, return_sequences=True, input_shape=input_shape, dropout=0.2))\nmodel1.add(Dense(units=64,activation=activation))\nmodel1.add(GRU(units=output_dim, return_sequences=False, dropout=0.2))\nmodel1.add(Dense(units=1,activation=activation))\nmodel1.compile(optimizer=optimizer,loss=loss)","b5e42224":"# Simple model fitting\nhistory = model.fit(X_train, y_train, \n                    validation_data=(X_valid, y_valid),\n                    batch_size=batch_size,\n                    epochs=epochs)\n\n\npd.DataFrame(history.history).plot()","ee993d21":"history1 = model1.fit(X_train, y_train, \n                    validation_data=(X_valid, y_valid),\n                    batch_size=batch_size,\n                    epochs=epochs)\n\npd.DataFrame(history1.history).plot()","17edc231":"model.evaluate(X_valid, y_valid)\nmodel1.evaluate(X_valid, y_valid)","e7bf7b97":"# Deserialize\n# dates = test.timestamp\ndef deserializer_pred(preds, df):\n  preds = np.reshape(preds, (preds.shape[0]))\n  preds_original = []\n  # print(preds)\n  for index in range(0, len(preds)):\n      pred = (preds[index]+1)* df[index][0]\n      preds_original.append(pred)\n  preds_original = np.array(preds_original)\n  return preds_original\n\n\n\ndef get_prediction_dataframe(df):\n    ts = create_timeseries(df, WINDOW)\n    df_normalized = normalize_windows(ts)\n    print(df_normalized.shape)\n    print(row)\n    #X, y = df_normalized[:row,:-1], df_normalized[:row,-1]\n    X, y = df_normalized[:,:-1], df_normalized[:,-1]\n    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n\n    print(\"X shape: \", X.shape)\n\n    preds = model.predict(X)\n    preds1 = model1.predict(X)\n\n    model.evaluate(X, y)\n    model1.evaluate(X, y)\n\n    preds_original = deserializer_pred(preds, ts)\n    preds1_original = deserializer_pred(preds1, ts)\n\n\n    res = pd.DataFrame({\n        'timestamp' : df.Date[-preds_original.shape[0]:],\n        'pred_nn' : preds_original, \n        'pred_conv' : preds1_original,\n        'price': df.Close[-preds_original.shape[0]:]\n    })\n    \n#     print(preds_original, ts[:row, -1])\n#     print(preds1_original, ts[:row, -1])\n    \n    return res","f652fa19":"res_test = get_prediction_dataframe(valid)\nres_test.head()","e79ad1a0":"res_valid = get_prediction_dataframe(valid)\n\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scatter(x=res_valid.timestamp, y=res_valid.pred_conv, name='Pred conv'), col=1, row=1)\nfig.add_trace(go.Scatter(x=res_valid.timestamp, y=res_valid.pred_nn, name='Pred_nn'), col=1, row=1)\nfig.add_trace(go.Scatter(x=res_valid.timestamp, y=res_valid.price, name='Real Prices'), col=1, row=1)\nfig.show()\n","fbdccd56":"res_train = get_prediction_dataframe(train)\n\nres=res_train\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scatter(x=res.timestamp, y=res.pred_conv, name='Pred conv'), col=1, row=1)\nfig.add_trace(go.Scatter(x=res.timestamp, y=res.pred_nn, name='Pred_nn'), col=1, row=1)\nfig.add_trace(go.Scatter(x=res.timestamp, y=res.price, name='Real Prices'), col=1, row=1)\nfig.show()\n","6628a788":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nfeatures = [ 'Open', 'High', 'Low', 'Close', 'Volume',\n       'EMA_3', 'EMA_8', 'SMA_5', 'SMA_13', 'Momentum_1D', 'RSI_7D', 'RSI_14D',\n       'EMA_26', 'EMA_12', 'MACD', 'MACD_signal', 'BB_Middle_Band',\n       'BB_Upper_Band', 'BB_Lower_Band'] #, 'pred_nn', 'pred_conv']\n\n# Date could be one hot encoded if needed. I don't think so , so far\n\n\n# Train \ntrain_xgb = train.copy().set_index('Date')\ntrain_xgb['price_tomorrow'] = train_xgb['Close'].shift(-1)\ntrain_xgb = train_xgb.dropna()\n#x_train, y_train = scaler.fit_transform(train_xgb[features]), train_xgb['price_tomorrow']\nx_train, y_train = train_xgb[features], train_xgb['price_tomorrow']\n\n\n# Validation \nvalid_xgb = valid.copy().set_index('Date')\nvalid_xgb['price_tomorrow'] = valid_xgb['Close'].shift(-1)\nvalid_xgb = valid_xgb.dropna()\n#x_valid, y_valid = scaler.transform(valid_xgb[features]), valid_xgb['price_tomorrow']\nx_valid, y_valid = valid_xgb[features], valid_xgb['price_tomorrow']\n\n\n\n# Test\ntest_xgb = test.copy().set_index('Date')\ntest_xgb['price_tomorrow'] = test_xgb['Close'].shift(-1)\ntest_xgb = test_xgb.dropna()\n#x_test, y_test = scaler.transform(test_xgb[features]), test_xgb['price_tomorrow']\nx_test, y_test = test_xgb[features], test_xgb['price_tomorrow']\n\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_valid.shape, y_valid.shape)\nprint(x_test.shape, y_test.shape)\n\n\n# train_xgb.isnull().sum().sort_values(ascending = False)","feb45a3b":"%%time\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {\n    'n_estimators': [100, 200, 300, 400],\n    'learning_rate': [0.02, 0.05, 0.1],\n    'max_depth': [5, 10, 12],\n    'gamma': [0.001, 0.1, 0.2],\n    'random_state': [42]\n}\n\neval_set = [(x_train, y_train), (x_valid, y_valid)]\nxgb = xgb.XGBRegressor(eval_set=eval_set, objective='reg:squarederror', verbose=True)\nclf = GridSearchCV(xgb, parameters, n_jobs=-1, verbose=5, scoring='r2')\n\n# clf.fit(x_train, y_train)\n# print('Best params: ', clf.best_params_)\n# print('Best validation score = ', clf.best_score_)","2431dd1e":"%%time\nfrom xgboost import XGBRegressor\n\n#model_xgb = XGBRegressor(**clf.best_params_, objective='reg:squarederror')\n\nmodel_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.001, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.02, max_delta_step=0, max_depth=5,\n             min_child_weight=1,monotone_constraints='()',\n             n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=42,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nmodel_xgb.fit(x_train, y_train, eval_set=eval_set, verbose=False)\n\nmodel_xgb","b3a7355b":"from xgboost import plot_importance, plot_tree\nplot_importance(model_xgb)","730b74f4":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ny_pred = model_xgb.predict(x_valid)\n\nprint(f'y_true = {np.array(y_valid)[:5]}')\nprint(f'y_pred = {y_valid[:5]}')\nprint(f'mean_squared_error = {mean_squared_error(y_valid, y_pred)}')\nprint(f'r2_score = {r2_score(y_valid, y_pred)}')","26450c95":"predicted_prices = valid_xgb.copy()\npredicted_prices['truth'] = valid_xgb['Close']\npredicted_prices['pred_xgb'] = model_xgb.predict(x_valid)\n\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scatter(x=predicted_prices.index, y=predicted_prices.truth,\n                         name='Truth',\n                         marker_color='green'), row=1, col=1)\n\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_xgb,\n                         name='xgb',\n                         marker_color='MediumPurple'), row=1, col=1)\n\nfig.show()","95e435d8":"predicted_prices.tail(10)[['pred_xgb', 'truth']]","d07907b8":"predicted_prices = test_xgb.copy()\npredicted_prices['truth'] = test_xgb['Close']\npredicted_prices['pred_xgb'] = model_xgb.predict(x_test)\n\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scatter(x=predicted_prices.index, y=predicted_prices.truth,\n                         name='Truth',\n                         marker_color='green'), row=1, col=1)\n\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_xgb,\n                         name='xgb',\n                         marker_color='MediumPurple'), row=1, col=1)\n\nfig.show()","3b5c72b4":"features_with_nn =  ['Open', 'High', 'Low', 'Close', 'Volume',\n       'EMA_3', 'EMA_8', 'SMA_5', 'SMA_13', 'Momentum_1D', 'RSI_7D', 'RSI_14D',\n       'EMA_26', 'EMA_12', 'MACD', 'MACD_signal', 'BB_Middle_Band',\n       'BB_Upper_Band', 'BB_Lower_Band', 'pred_nn', 'pred_conv']\n\nscaler1 = StandardScaler()\n\n# # Train \nres_train = get_prediction_dataframe(train)\ntrain_xgb = pd.merge(train, res_train, left_on='Date', right_on='timestamp', how='inner')\n\ntrain_xgb['price_tomorrow'] = train_xgb['Close'].shift(-1)\ntrain_xgb = train_xgb.dropna()\n# x_train, y_train = scaler1.fit_transform(train_xgb[features_with_nn]), train_xgb['price_tomorrow']\nx_train, y_train = train_xgb[features_with_nn], train_xgb['price_tomorrow']\n\n\n\n# # Validation \nres_valid = get_prediction_dataframe(valid)\nvalid_xgb = pd.merge(valid, res_valid, left_on='Date', right_on='timestamp', how='inner')\n\nvalid_xgb['price_tomorrow'] = valid_xgb['Close'].shift(-1)\nvalid_xgb = valid_xgb.dropna()\n#x_valid, y_valid = scaler1.transform(valid_xgb[features_with_nn]), valid_xgb['price_tomorrow']\nx_valid, y_valid = valid_xgb[features_with_nn], valid_xgb['price_tomorrow']\n\n\n# # Test \nres_test = get_prediction_dataframe(test)\ntest_xgb = pd.merge(test, res_test, left_on='Date', right_on='timestamp', how='inner')\n\ntest_xgb['price_tomorrow'] = test_xgb['Close'].shift(-1)\ntest_xgb = test_xgb.dropna()\n# x_test, y_test = scaler1.transform(test_xgb[features_with_nn]), test_xgb['price_tomorrow']\nx_test, y_test = test_xgb[features_with_nn], test_xgb['price_tomorrow']\n\n\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_valid.shape, y_valid.shape)\nprint(x_test.shape, y_test.shape)\n\n\ntest.head()","eea63cd6":"%%time\n\neval_set = [(x_train, y_train), (x_valid, y_valid)]\nxgb = XGBRegressor(eval_set=eval_set, objective='reg:squarederror', verbose=True)\nclf = GridSearchCV(xgb, parameters, n_jobs=-1, verbose=5, scoring='r2')\n\n\n# clf.fit(x_train, y_train)\n\n# print('Best params: ', clf.best_params_)\n# print('Best validation score = ', clf.best_score_)","d2533356":"%%time\nfrom xgboost import XGBRegressor\n\n#model_xgb1 = XGBRegressor(**clf.best_params_, objective='reg:squarederror')\nmodel_xgb1 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.001, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.02, max_delta_step=0, max_depth=5,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=42,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nmodel_xgb1.fit(x_train, y_train, eval_set=eval_set, verbose=False)\n\nmodel_xgb1","1c453ac6":"from xgboost import plot_importance, plot_tree\nplot_importance(model_xgb1)","76dce5ec":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ny_pred = model_xgb1.predict(x_valid)\nprint(f'y_true = {np.array(y_valid)[:5]}')\nprint(f'y_pred = {y_valid[:5]}')\nprint(f'mean_squared_error = {mean_squared_error(y_valid, y_pred)}')\nprint(f'r2_score = {r2_score(y_valid, y_pred)}')","ab9ec3b8":"predicted_prices = x_valid.copy()\npredicted_prices['truth'] = y_valid\npredicted_prices['pred_xgb'] = model_xgb1.predict(x_valid)\n\n# predicted_prices = predicted_prices.set_index('Date')\n\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scatter(x=predicted_prices.index, y=predicted_prices.truth,\n                         name='Truth',\n                         marker_color='green'), row=1, col=1)\n\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_xgb,\n                         name='xgb',\n                         marker_color='MediumPurple'), row=1, col=1)\n\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_conv,\n                         name='NN conv',\n                         marker_color='LightSkyBlue'), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_nn,\n                         name='NN',\n                         marker_color='blue', opacity=0.2), row=1, col=1)\n\nfig.show()","13aefa72":"predicted_prices = x_train.copy()\npredicted_prices['truth'] = y_train\npredicted_prices['pred_xgb'] = model_xgb1.predict(x_train)\n\n# predicted_prices = predicted_prices.set_index('Date')\n\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scatter(x=predicted_prices.index, y=predicted_prices.truth,\n                         name='Truth',\n                         marker_color='green'), row=1, col=1)\n\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_xgb,\n                         name='xgb',\n                         marker_color='MediumPurple'), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_conv,\n                         name='NN conv',\n                         marker_color='LightSkyBlue'), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=predicted_prices.index,\n                         y=predicted_prices.pred_nn,\n                         name='NN',\n                         marker_color='blue', opacity=0.2), row=1, col=1)\n\nfig.show()","ca401567":"test.shape","c96a6e6f":"predicted_prices.tail(10)[['pred_xgb', 'truth']]","a5e3f9bb":"predicted_prices = x_test.copy()\npredicted_prices['truth'] = y_test\npredicted_prices['pred_xgb'] = model_xgb1.predict(x_test)\n\npredicted_prices[['pred_xgb', 'truth', 'pred_nn', 'pred_conv']]","0dfe53f8":"! pip install yfinance","c1d865ac":"import yfinance as yf \nimport datetime\n\nstart = datetime.datetime(2010,2,1)  \nend = datetime.datetime(2020,11,1)\n\nstock = yf.download('cern',start=start, end=end, progress=False)\ndf = stock[['Open', 'High', 'Low', 'Close', 'Volume']]\ndf['Date'] = df.index\ndf = df.reset_index(drop=True)\n","cc31bfd6":"set_indicator(df)\ndf = df.dropna()\n\nrow = df.shape[0]-9\nprint(row)\n\ntrain = df.head(df.shape[0]-WINDOW+1)\ntest=df.tail(WINDOW+1)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train.Date, y=train.Close, name='Training'))\nfig.add_trace(go.Scatter(x=test.Date, y=test.Close, name='Test'))\nfig.show()","323e8ace":"test","e6ac33e0":"# # Train \nres_train = get_prediction_dataframe(train) # Without fitting the NN\ntrain_xgb = pd.merge(train, res_train, left_on='Date', right_on='timestamp', how='inner')\n\ntrain_xgb['price_tomorrow'] = train_xgb['Close'].shift(-1)\ntrain_xgb = train_xgb.dropna()\ntrain_xgb = train_xgb.set_index('Date')\nx_train, y_train = train_xgb[features_with_nn], train_xgb['price_tomorrow']\n\n\n# # # Test \nres_test = get_prediction_dataframe(test)\ntest_xgb = pd.merge(test, res_test, left_on='Date', right_on='timestamp', how='inner')\n\ntest_xgb['price_tomorrow'] = test_xgb['Close'].shift(-1)\ntest_xgb = test_xgb.dropna()\ntest_xgb = test_xgb.set_index('Date')\nx_test, y_test = test_xgb[features_with_nn], test_xgb['price_tomorrow']\n\n\n\nmodel_xgb.fit(x_train, y_train)\ny_pred = model_xgb.predict(x_test)\n\nprint(f'y_true = {np.array(y_test)}')\nprint(f'y_pred = {y_pred}')\nprint(f'mean_squared_error = {mean_squared_error(y_test, y_pred)}')\n#print(f'r2_score = {r2_score(y_test, y_pred)}')","da1f9fed":"x_test['pred_xgb'] = y_pred\nx_test[['pred_nn', 'pred_conv', 'pred_xgb']]","a0eb403b":"test['Date']","ff85ac1a":"res_test = get_prediction_dataframe(test)\ntest_xgb = pd.merge(test, res_test, left_on='Date', right_on='timestamp', how='inner')\ntest_xgb['price_tomorrow'] = test_xgb['Close'].shift(-1)\n# test_xgb = test_xgb.dropna()\ntest_xgb = test_xgb.set_index('Date')\nx_test, y_test = test_xgb[features_with_nn], test_xgb['price_tomorrow']\n\ny_pred = model_xgb.predict(x_test)\nx_test['pred_xgb'] = y_pred\nx_test[['pred_nn', 'pred_conv', 'pred_xgb']]","c2751aaa":"### Thats looks good !","9b95aec8":"# Real life example: By using yahoo finance API","ffe09778":"let's see in visualization","60fdb4f6":"### Observation\n\nThis approach helps xgb to be more accurate ","d1fa4c2c":"### Define Series","9708bcf0":"## Deserialization processing","b90f6164":"# Recurrent Neural Network approach","7802a2cb":"## Data Preparation: Time series conversion, normalization","e9743eca":"## Futur Prediction ","4c3049ee":"## Validation visualization","58f10825":"## Data preparation","b9137e1a":"## Train test splitting","c9e381bf":"\n## Beware : From what I understand xgboost is not very good at extrapolating unseen values and is a problem with all tree based...\n\n\nYou can see that by taking apple stock ( 'aapl') \n\nOne of problem causing \"the prediction way lower than the price\" is that tree-based algorithms (like XGBRegressor used here) cannot extrapolate data. If you take a look at the plot with data split, imagine that all what an algorithm sees when training is train set obviously. For the algorithm, train set bounds are total bounds for the rest of data.\n\nSo if your validation or test set reach higher than training set maximum value, tree-based classes cannot predict them correctly. They will predict the value as the maximum they are aware of.\n\nHere I fixed this just by picking another stock (when highest high value is within train set).","14995520":"# More to improve\n* Use Wrap to ease the using of NN\n* Ensemble learning could be a good alternative: voting or stacking \n* in the XGB case, always be carful of bounds of train , test and validation sets (use 'aapl' stock to see why ^^)\n* The Neural Networks are simples. It is possible to add layers or use hyperparameter optimizer such as hyperas\n* Hyperopt could be used too for the XGBs ","52328e6a":"## the models fail to predict in November 2017 on validation","fd87f10c":"# Data Loading","f067bac3":"## 1. Simple XGB","3d9499b5":"## Models\nWe're going to try two models, a simple RNN and a RNN with a convolution 1D ","68c6403b":"## Technical indicators","c0234475":"# XGB approach with nn prediction as feature","44931569":"## 2. XGB by using NN outputs as features ","41ca2472":"### Test Result"}}