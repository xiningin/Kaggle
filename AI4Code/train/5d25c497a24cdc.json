{"cell_type":{"5eaaee9b":"code","ef97a709":"code","399d7d65":"code","f44243dd":"code","cb23a12d":"code","c6302acb":"code","9eea839e":"code","099dbc6c":"code","f4689e83":"code","a1410770":"code","e11a31a1":"code","3653c9a3":"code","6e60e89d":"code","13da7226":"markdown","c3a14294":"markdown","a70e8b86":"markdown","1791a8b2":"markdown","050e51a1":"markdown","4ee7c147":"markdown","e5c7146f":"markdown","1a48ab8a":"markdown","dac63ef8":"markdown","12046350":"markdown","2a09f54a":"markdown","0f86a26f":"markdown"},"source":{"5eaaee9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# This is for storing data file path\nimportdata = '' # '' meaning for define data type(String) \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        importdata = os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef97a709":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom collections import Counter, deque\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score","399d7d65":"data = pd.read_csv(importdata)\ndata","f44243dd":"print(data.info())  # find Nan, number of columns object(5)\nprint(data.describe())  # check the values\npd.set_option('display.max_columns', 6)  # display all columns (object +1 = 6)\nprint(data.head())\nBottle = data.groupby(['Bottle_name']).size()\nprint(Bottle)","cb23a12d":"data['Brand'].value_counts().plot(kind='barh')\nplt.show()\n\nplt.subplot(1, 2, 1)\n# opt = [\"Yamazaki\"] # this is for isin(), if I need to specify more, I cam write opt = ['Yamazaki', 'banrd name']\nx = data.loc[data['Brand'].isin([\"Yamazaki\"])] # extract all values (rows) which specified by using isin() function\nx['Bottle_name'].value_counts().plot(kind='bar')\nplt.title(\"Yamazaki and Bottle_Name\")\n\nplt.subplot(1, 2, 2)\ny = data.loc[data['Brand'].isin([\"Hibiki\"])]\ny['Bottle_name'].value_counts().plot(kind='bar', color='green')\nplt.title(\"Hibiki and Bottle_Name\")\nplt.show()\n\nplt.subplot(1, 2, 1)\nz = data.loc[data['Brand'].isin([\"Nikka\"])]\nz['Bottle_name'].value_counts().plot(kind='bar', color='orange')\nplt.title(\"Kikka and Bottle_Name\")\n\nplt.subplot(1, 2, 2)\nw = data.loc[data['Brand'].isin([\"Hakushu\"])]\nw['Bottle_name'].value_counts().plot(kind='bar', color='red')\nplt.title(\"Hakushu and Bottle_Name\")\nplt.show()","c6302acb":"corpus = [] # corpus will only get in the end all the clean reviews\nfor i in range(len(data)):\n    review = re.sub('[^a-zA-Z0-9_]', ' ', data['Review_Content'][i]) # for  remove all punctuation\n    review = review.lower() # all the capital letters were turned into lowercase\n    # review = review.split() # split different words\n    words = word_tokenize(review) # split words\n    ps = PorterStemmer() # loved -> love remove ed\n    stop = set(stopwords.words('english'))\n    stop.update(('Yamazaki', 'hibiki', 'it', 'whisky', 'whiskey', 'bottle')) # update stop words\n    review = [ps.stem(word) for word in words if not word in stop]\n    review = ' '.join(review) # adding space between each word of review\n    corpus.append(review)\n\ncount = dict(Counter(word for sentence in corpus for word in sentence.split()))\ndf = list(count.items())  # count.items() to return a collection of the key-value pairs in count\n# list(obj) with this collection as obj to convert in to a list\n\narray = np.array(df)  # np.array(df) with this list as data to convert it to array (make 2D)\nprint(array)","9eea839e":"sid = SentimentIntensityAnalyzer()\npos_word_list = []\nneu_word_list = []\nneg_word_list = []\n\nfor word in count:\n    if (sid.polarity_scores(word)['compound']) >= 0.5:\n        pos_word_list.append(word)\n    elif (sid.polarity_scores(word)['compound']) <= -0.5:\n        neg_word_list.append(word)\n    else:\n        neu_word_list.append(word)\n\nprint('Positive :', pos_word_list)\nprint('Neutral :', neu_word_list)\nprint('Negative :', neg_word_list)","099dbc6c":"def prev_and_next(input_list):\n    CURRENT = input_list\n    PREV = deque(input_list)\n    PREV.rotate(-1)\n    PREV = list(PREV)\n    NEXT = deque(input_list)\n    NEXT.rotate(1)\n    NEXT = list(NEXT)\n    return zip(PREV, CURRENT, NEXT)","f4689e83":"data['Sentiment'] = 0\n\npositiveList = pd.read_csv('\/kaggle\/input\/positive-words\/positive.txt') # import all positive words from online\nnegativelist = pd.read_csv('\/kaggle\/input\/negative-words\/nega.txt')\n\npos = positiveList.iloc[:, 0].values # for lemmatizer use: converting all words to the single distinct list 'awesome','good'\nneg = negativelist.iloc[:, 0].values # converting all words to the single distinct list 'bad','hate'\n\nindex = 0 # accessing all rows from 1\n\nlemmatizer = WordNetLemmatizer() # modeling and lemmatization that convert plural to singular ex) Feet -> Foot\n\nps = PorterStemmer()  # loved -> love remove ed\nstop = set(stopwords.words('english'))\nstop.remove('not') # for checking Not + any positive words: Not Like\nstempos = [ps.stem(word) for word in pos if not word in stop] # for positiveList Ps use: this is for the review which will applied stemming in the loop\nstemnega = [ps.stem(word) for word in neg if not word in stop] # this is for the review which will applied stemming in the loop\n\ncorpus = [] # collecting all stemming and lemmatazed words\nfor i in data['Review_Content']:\n    review = re.sub('[^a-zA-Z]', ' ', i)  # for  remove all punctuation\n    review = review.lower()  # all the capital letters were turned into lowercase\n    words = word_tokenize(review) # split words\n\n    lemWords = [lemmatizer.lemmatize(w) for w in words] # for words lemmatizer\n\n    # This loop will stop when itr finds one of the positive or negative words (lemmatization) in a sentence.\n    for itr in lemWords: # check a word all in lemWords(h)\n        if itr in pos:   # check h in pos\n            data.at[index, 'Sentiment'] = 1 # found positive word first\n            break # finish loop\n        elif itr in neg: # check h in neg\n            data.at[index, 'Sentiment'] = 0 # found negative word first\n            break\n\n    review = [ps.stem(word) for word in lemWords if not word in stop] # for NLP by lemWords PorterStemmer\n\n    # I assumed that sentences that have any negative word first are already negative.\n    # This loop point is for finding 'Not + any positive words' such as \"NOT LIKE\"\n    if (data.at[index, 'Sentiment'] == 1): # for find next has negative word    'It's okay but I do not like it'\n        for previous, current, next in prev_and_next(review): # assume the next words are any positive words\n            if current == 'not':\n                if next in stempos: # stempos is stemmed as review needs to be stemmed for NLP\n                    data.at[index, 'Sentiment'] = 0\n                    break # no longer this loop needs to search\n\n    index += 1 # index is increasing\n    review = ' '.join(review)\n    corpus.append(review) # stores stemmed and lemmatized words here\n\nsentiment = data['Sentiment'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\nprint(sentiment)","a1410770":"count = dict(Counter(word for sentence in corpus for word in sentence.split()))\ndf = list(count.items())  # count.items() to return a collection of the key-value pairs in count\n# list(obj) with this collection as obj to convert in to a list\n\narray = np.array(df)  # np.array(df) with this list as data to convert it to array (make 2D)\narray1d = np.array(array[:, -1], dtype='i') # i = int\n\n# describe outlier\nprint(np.median(array1d))\nprint(np.mean(array1d))\nprint(np.std(array1d))  # for check the array type\nprint(array1d[(array1d>np.quantile(array1d, 0.1)) & (array1d<np.quantile(array1d, 0.9))].tolist())\n\n# visualizing outlier\nplt.boxplot(array1d)\nplt.show()","e11a31a1":"removewords = [word for word in count if (count[word] > 50) & (not word in stempos and not word in stemnega)]\n\nremoveIndex = []\nfor word in removewords:\n    if word in corpus:\n        removeIndex.append(corpus.index(word))\n        corpus.remove(word)\ndata = data.drop(removeIndex)","3653c9a3":"cv = CountVectorizer()\nx = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values\n\nmax_words = int(len(x[0] * 0.9))\ncv = CountVectorizer(max_features=max_words)\nx = cv.fit_transform(corpus).toarray()","6e60e89d":"# Splitting the dataset into the Training set and Test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n\n# Training the Naive Bayes model on the Training set\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)","13da7226":"# Import Data","c3a14294":"# NLP","a70e8b86":"# Data Analysis","1791a8b2":"# Create Positive, Negative, Neutral word list","050e51a1":"# Creating Removing Words from the Above Steps","4ee7c147":"# Adding a column ('Sentiment') for NLP","e5c7146f":"# Data Analysis for NLP","1a48ab8a":"# Applying NLP","dac63ef8":"# Import Libraries ","12046350":"# Cleaning the Texts","2a09f54a":"# Data Visualization","0f86a26f":"# Creating the Bag of Words model"}}