{"cell_type":{"bbe97c47":"code","51cb47ed":"code","dd6e61f0":"code","9402a3ec":"code","1f52dfda":"code","4729b5db":"code","dfd84472":"code","3bc06ab5":"code","060d5283":"code","21378a02":"code","47e8a651":"code","e8791734":"code","195d223e":"code","c471eb97":"code","6b6ee094":"code","0e5a8d4c":"code","b5d14645":"code","b0f999b6":"code","e9071934":"code","34000ee9":"code","0fd4db36":"code","59a5e0fb":"code","7e5f7b3b":"code","25e726ed":"code","c89ee5fe":"code","018fab64":"code","d0eb54b2":"code","cd5c5ba1":"code","a234d80e":"code","18420b1b":"code","321851ce":"code","d1e937b9":"code","02ebc2e7":"code","e0d245c1":"code","0fa72a8f":"code","1aa19e9e":"code","618092be":"code","fac4f4f4":"code","2cd10408":"code","7e9a6d70":"code","7d6264f1":"code","58d451fc":"code","5fb27d47":"code","9e35bc20":"markdown","9f04fa86":"markdown","5f0bc49e":"markdown","5f5fa607":"markdown","68103945":"markdown","44e97688":"markdown","e9a38814":"markdown","8dc207a3":"markdown"},"source":{"bbe97c47":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport nltk\nnltk.download('averaged_perceptron_tagger')","51cb47ed":"%cd ..\/input\/nlp-getting-started","dd6e61f0":"train_file_name = 'train.csv'\ndf_train = pd.read_csv(train_file_name)\ntrain_column_names = df_train.columns.values\nprint(df_train['keyword'].isnull().value_counts())\nprint(df_train['location'].isnull().value_counts())","9402a3ec":"df_train[~(df_train['location'].isnull())]","1f52dfda":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nactual_disasters = df_train.loc[df_train['target'] == 1, 'text']\nnot_disasters = df_train.loc[df_train['target'] == 0, 'text']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\nnum_words_ad = actual_disasters.str.len()\nax1.hist(num_words_ad, color = 'red')\nax1.set_title('Disaster Tweets')\nnum_words_nd = not_disasters.str.len()\nax2.hist(num_words_nd, color = 'yellow')\nax2.set_title('Non Disaster Tweets')","4729b5db":"nltk.download('punkt')\ndefault_st = nltk.word_tokenize\ntokenized_tweets = df_train['text'].map(lambda x: x.split()).map(lambda x: len(x))\nplt.hist(tokenized_tweets)","dfd84472":"first_text = actual_disasters[0]\nimport nltk\ntry:\n  print(nltk.pos_tag(first_text.split()))\nexcept LookupError:\n  nltk.download('averaged_perceptron_tagger')\n  print(nltk.pos_tag(first_text.split()))","3bc06ab5":"pos_tagged_sent = nltk.pos_tag(first_text.split())\ngrammar = '''\n            NP: {<DT>?<JJ>?<NN.*>}\n            ADJP: {<JJ>}\n            ADVP: {<RB.*>}\n            PP: {<IN>}\n            VP: {<MD>?<VB.*>+}\n          '''\nrp = nltk.RegexpParser(grammar)\nshallow_parsed_sent = rp.parse(pos_tagged_sent)\nprint(shallow_parsed_sent)","060d5283":"import string\nfrom collections import defaultdict\n\nspecial_chars = string.punctuation\nactual_disasters_list = actual_disasters.to_list()\nnot_disasters_list = not_disasters.to_list()\nall_texts = actual_disasters_list + not_disasters_list\nall_texts = [x.split() for x in all_texts]\nlen(all_texts), len(actual_disasters_list) + len(not_disasters_list)","21378a02":" def get_dict(lists):\n  dic = defaultdict(int)\n  for j in lists:\n    for i in j:\n      if i in special_chars:\n        dic[i] += 1\n  return dic","47e8a651":"dic_ad = get_dict(actual_disasters)\ndic_nd = get_dict(not_disasters)\nx_ad, y_ad = zip(*dic_ad.items())\nplt.bar(x_ad, y_ad, color = 'red')","e8791734":"import re \nstring = 'Go To https:\/\/www.mysite.com'\npattern = 'https:\/\/\\S+|www\\.\\S+'\nres = re.sub(pattern, '', string, re.IGNORECASE)\nprint(res)","195d223e":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\nword_count = defaultdict(int)\nfor tweets in all_texts:\n  for word in tweets:\n    if word in stop:\n      word_count[word] += 1\ntop=sorted(word_count.items(), key=lambda x:x[1],reverse=True)[:10]\nwords, counts = zip(*top)\nplt.bar(words, counts, color = 'blue')","c471eb97":"### CHECK FOR URLS\nimport re\ndef check_urls(texts):\n  linked_tweets = []\n  pattern1 = 'http:\/\/\\S+|www\\.\\S+'\n  pattern2 = 'https:\/\/\\S+|www\\.\\S+'\n  for x in texts:\n    if re.search(pattern1, x, re.IGNORECASE)!=None or  re.search(pattern2, x, re.IGNORECASE) != None:\n      linked_tweets.append(x)\n  return linked_tweets\nlinked_tweets = check_urls(not_disasters_list)\nprint(len(linked_tweets))","6b6ee094":"pattern = r\"<.*>\"\nlinked_tweets = []\nfor x in actual_disasters_list:\n  if re.search(pattern, x, re.IGNORECASE)!=None:\n    linked_tweets.append(x)\nprint(len(linked_tweets))","0e5a8d4c":"### REMOVE URLS ###\ndef remove_urls(x):\n  pattern1 = r'http?:\/\/\\S+|www\\.\\S+'\n  pattern2 = r'https?:\/\/\\S+|www\\.\\S+'\n  res = re.sub(pattern1, '', x, re.IGNORECASE)\n  res = re.sub(pattern2, '', x, re.IGNORECASE)\n  return res\n\ndf_train['text'] = df_train['text'].map(lambda x: remove_urls(x))\nlinked_tweets = check_urls(df_train['text'].to_list())\nprint(len(linked_tweets))","b5d14645":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\"  \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","b0f999b6":"df_train['text']=df_train['text'].apply(lambda x: remove_emoji(x))","e9071934":"def remove_punct(text):\n  pattern = r'[^a-zA-Z0-9\\s]'\n  text = re.sub(pattern, '', text)\n  return text\n\ndf_train['text'] = df_train['text'].map(lambda x: \n                                        remove_punct(x))","34000ee9":"dic = get_dict(df_train['text'].to_list())","0fd4db36":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train['text'])","59a5e0fb":"def remove_stopwords(text):\n  tokens = text.split()\n  filtered_tokens = [token for token in tokens\n                     if token.lower() not in stop]\n  filtered_text = ' '.join(filtered_tokens)\n  return filtered_text\n\ntest = \"The a again go into hell\"\nremove_stopwords(test)","7e5f7b3b":"df_train['text'] = df_train['text'].map(lambda x: remove_stopwords(x))","25e726ed":"sequences = tokenizer.texts_to_sequences(df_train['text'])","c89ee5fe":"len(tokenizer.word_index.values())","018fab64":"maxlen = max([len(x) for x in sequences])\npadded_sequences = pad_sequences(sequences, maxlen = maxlen)\nlabels = df_train['target'].to_list()","d0eb54b2":"df_test = pd.read_csv('test.csv')\ndf_test['text'] = df_test['text'].map(lambda x: remove_urls(x))\ndf_test['text'] = df_test['text'].map(lambda x: remove_emoji(x))\ndf_test['text'] = df_test['text'].map(lambda x: remove_punct(x))\ndf_test['text'] = df_test['text'].map(lambda x: remove_stopwords(x))","cd5c5ba1":"test_sequences = pad_sequences(tokenizer.texts_to_sequences(df_test['text']), maxlen = maxlen)","a234d80e":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size = 0.2, stratify = labels)\nx_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)","18420b1b":"input_dim = len(tokenizer.word_index.values())\nembedding_dim = 64\noptimizer = 'adam'\nloss = 'binary_crossentropy'\nmetrics = ['acc']","321851ce":"!echo `pwd`","d1e937b9":"%cd ..\/..\/working\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","02ebc2e7":"!unzip glove.6B.zip","e0d245c1":"!wget \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"","0fa72a8f":"from gensim.models.keyedvectors import KeyedVectors\nword_vectors = KeyedVectors.load_word2vec_format(\n          \"GoogleNews-vectors-negative300.bin.gz\", binary = True\n      )","1aa19e9e":"## BASE LINE MODEL\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Embedding, LSTM, SimpleRNN, Bidirectional, Dropout, BatchNormalization\nimport os\nimport tensorflow.keras \nclass MyModel:\n  def __init__(self):\n    pass\n  def Model(self, embedding_type = 'custom',\n            loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'], input_dim = 30, \n            embedding_dim = 64, tokenizer = None):\n    model = Sequential()\n    \n    if embedding_type == 'custom':\n      model.add(Embedding(input_dim + 1, embedding_dim))\n    elif embedding_type == 'glove':\n      embeddings_index = {}\n      f = open('glove.6B.100d.txt')\n      for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype = 'float32')\n        embeddings_index[word] = coefs\n      f.close()\n      word_index = tokenizer.word_index\n      embedding_dim = 100\n      embedding_matrix = np.zeros((input_dim + 1, embedding_dim))\n      for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word) \n        if i < input_dim + 1:\n          if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector \n      model.add(Embedding(input_dim + 1, embedding_dim))\n      model.layers[0].set_weights([embedding_matrix])\n      model.layers[0].trainable = False\n    elif embedding_type == 'word2vec':\n      from gensim.models.keyedvectors import KeyedVectors\n      word_vectors = KeyedVectors.load_word2vec_format(\n          \"GoogleNews-vectors-negative300.bin.gz\", binary = True\n      )\n      word_index = tokenizer.word_index\n      embedding_dim = 300\n      embedding_matrix = np.zeros((input_dim + 1, embedding_dim))\n      for word, i in word_index.items():\n        try:\n          embedding_vector = word_vectors[word] \n        except KeyError:\n          embedding_vector = np.zeros((embedding_dim,))\n        if i < input_dim + 1:\n          if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector \n      model.add(Embedding(input_dim + 1, embedding_dim))\n      model.layers[0].set_weights([embedding_matrix])\n      model.layers[0].trainable = False\n    model.add(Bidirectional(LSTM(embedding_dim, recurrent_dropout = 0.4)))\n    model.add(BatchNormalization())\n    model.add(Dense(64))\n    model.add(Dropout(0.5))\n    model.add(Dense(32))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n    return model\n  \n  def fit(self, model, x_train, y_train, epochs = 100, batch_size = 128, validation_split = 0.2):\n    callbacks_list = [tensorflow.keras.callbacks.EarlyStopping(monitor = 'val_acc', patience = 1),\n                      tensorflow.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10)]\n    history = model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size,\n              validation_split = validation_split, callbacks = callbacks_list)\n    return history\n  \n  def get_plots(self):\n    pass","618092be":"mm = MyModel()\nmodel = mm.Model(embedding_type = 'glove', tokenizer = tokenizer, input_dim = len(tokenizer.word_index.values()))\nhistory_glove = mm.fit(model, x_train, y_train, batch_size = 128)\nmodel = mm.Model(embedding_type = 'custom', tokenizer = tokenizer, input_dim = len(tokenizer.word_index.values()))\nhistory_custom = mm.fit(model, x_train, y_train, batch_size = 128)\nmodel = mm.Model(embedding_type = 'word2vec', tokenizer = tokenizer, input_dim = len(tokenizer.word_index.values()))\nhistory_w2v = mm.fit(model, x_train, y_train, batch_size = 128)","fac4f4f4":"glove_acc = history_glove.history['val_acc'][-1]\ncustom_acc = history_custom.history['val_acc'][-1]\nw2v_acc = history_w2v.history['val_acc'][-1]\nplt.bar(np.array(['glove', 'custom', 'w2v']), np.array([glove_acc, custom_acc, w2v_acc]), width = 0.2)","2cd10408":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model_arch.jpg', show_shapes = True)","7e9a6d70":"def plot_model_details(data, data_type):\n  fig, (ax1, ax2) = plt.subplots(1, 2, sharex = True, sharey = True, figsize = (10, 5))\n  acc = data['acc']\n  val_acc = data['val_acc']\n  loss = data['loss']\n  val_loss = data['val_loss']\n  epochs = range(1, len(acc) + 1)\n  \n  ax1.plot(epochs, acc, 'b', label = 'accuracy')\n  ax1.plot(epochs, val_acc, 'b', label = 'validation accuracy', color = 'red')\n  ax1.legend()\n  ax2.plot(epochs, loss, 'b', label = 'loss')\n  ax2.plot(epochs, val_loss, 'b', label = 'validation loss', color = 'red')\n  ax2.legend()\n  fig.suptitle(data_type)\n  fig.savefig('modelresults' + '_' + data_type +'.jpg')","7d6264f1":"data_glove = history_glove.history\ndata_custom = history_custom.history\ndata_w2v = history_w2v.history\nplot_model_details(data_glove, 'glove')\nplot_model_details(data_custom, 'custom')\nplot_model_details(data_w2v, 'word2vec')","58d451fc":"preds = model.predict(test_sequences)\npreds = np.array(preds).ravel()\npreds_binary = np.where(preds >= 0.5, 1, 0)","5fb27d47":"submission_df = pd.DataFrame({'id':df_test['id'], 'target':preds_binary})\nsubmission_df.to_csv('submission_1.csv', index = False)","9e35bc20":"Looking for the presence of stopwords and their counts.","9f04fa86":"**Looking at the special characters present and their frequency.**","5f0bc49e":"CHECKING AND REMOVING URLS","5f5fa607":"![](http:\/\/)Finding tweets with Markup Language","68103945":"The following Callbacks where used during training:\n\n\n*   **EarlyStopping**: Stops the training process if the specified metric stops improving \n*   **ReduceLROnPlateau**: Reduce the learning rate when the metric has stopped improving.","44e97688":"* I've referred the following notebook for EDA (Visualizing character, word lengths etc.) &\n    Data Cleaning (Removing URL, stopwords, emojis, links etc).  \n    Please Check it out:\n    https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n\n* Basic EDA & Preprocessing\n* Fitting Bidirectional LSTMs on Text data\n* Using Word2Vec, GloVe and Custom Embedding Layer and Comparing the results\n* The following Callbacks where used during training:\n    1. EarlyStopping: Stops the training process if the specified metric stops improving\n    2. ReduceLROnPlateau: Reduce the learning rate when the metric has stopped improving.","e9a38814":"Looking at the variation in the characters lengths of the tweets.\nThe histogram shows most of the tweets have length of 125 characters.","8dc207a3":"The plots:\n\n\n*   Plot of Validation Accuracy & Training Accuracy\n*   Plot of Validation Loss & Training Loss"}}