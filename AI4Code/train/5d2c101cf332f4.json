{"cell_type":{"9ac1b520":"code","1ed3bdea":"code","d8c8ff44":"code","1e1d1a91":"code","757adc98":"code","0a71d28b":"code","c5714082":"code","474e4ee5":"markdown","1a3e6b07":"markdown","f627c17f":"markdown","0617d7c9":"markdown","de9c2325":"markdown","78452607":"markdown","81d8be19":"markdown","ff966e3f":"markdown","7fd706bf":"markdown"},"source":{"9ac1b520":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nsns.set(font_scale = 1.2)\n\ndata = pd.read_pickle('..\/input\/output4\/output (3).pkl')\ndata.head(5)\n\n# Model notebook will contain a code cell that generates the data we want from our best run. Import the output.pkl file via the 'Add Data' button.\n# output_data = pd.DataFrame({'target': y_valid, 'prediction' : LGB_model.predict_proba(X_valid)[:,1]})\n# output_data['binary'] = np.floor(output_data['prediction']+0.5)     # one of many ways to convert probabilistic predictions to 0\/1 binary predictions\n# output_data.to_pickle(\".\/output.pkl\", compression='infer', storage_options=None)","1ed3bdea":"plt.subplots(1, 2, sharey = True, figsize=(16,8))\n\nprobabilities = data.prediction\nfpr, tpr, thresholds = roc_curve(data.target, data.prediction)\nprec, rec, thresholds = precision_recall_curve(data.target, data.prediction)\nauc = roc_auc_score(data.target, data.prediction)\nno_skill = len(data[data.target==1]) \/ len(data)\n\nplt.subplot(1,2,1)\nplt.plot(fpr, tpr, label = f'Light GBM, AUC = {np.round(auc,6)}')      # plot the ROC\nplt.plot([0, 1], [0, 1], label='No Skill')                             # plot the orange 45 degree line (sometimes called the \"no skill\" line)\nplt.title('Receiver operating characteristic curve')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(rec, prec, label='Light GBM Model')                           # plot the PRC\nplt.plot([0, 1], [no_skill, no_skill], label='No Skill')               # plot the no skill line\nplt.title('Precision - Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.tight_layout()\nplt.show()","d8c8ff44":"CR = pd.DataFrame(classification_report(data.target, data.binary, output_dict=True)).T\nCR","1e1d1a91":"labels = ['Paid -0-','Defaulted -1-']\n\nCM = confusion_matrix(data.target, data.binary)              # confusion matrix\nRM = (((CM.T)\/(CM.sum(axis=1))).T)                           # recall matrix\nPM = (CM\/CM.sum(axis=0))                                     # precision matrix\n\nplots = [CM, RM, PM]\nplot_titles = ['Confusion Matrix', 'Recall Matrix', 'Precision Matrix']\nformats = [\",d\", \".4f\", \".4f\"]\nfig, axes = plt.subplots(1, 3, sharey = True, figsize=(15,6))\nfig.supylabel('Actual Class')\nfor i in range(0,3):\n    plt.subplot(1,3,i+1)\n    sns.heatmap(plots[i], annot=True, cmap='BuPu', alpha = 0.5, fmt = formats[i], xticklabels = labels, yticklabels=labels, cbar = False)\n    plt.xlabel('Predicted Class')\n    plt.title(plot_titles[i])\nplt.tight_layout()\nplt.show()","757adc98":"plt.figure(figsize=[15,3])\nsns.boxplot(data.prediction, data.target, orient = \"h\")\nplt.title('Distribution of predicted probabilities - validation data')\nplt.show()","0a71d28b":"fig, axes = plt.subplots(2, 2, sharey = True, figsize=(10,10))\nfig.suptitle('Histogram plots by classification')\n\nfor y in (0,1):\n    for x in (0,1):\n        Class = data[(data[\"target\"] == x) & (data[\"binary\"] == y)]          # subset the data\n        color = 'green' if x==y else 'red'                                   # green means correct prediction\n        sns.histplot(ax = axes[x,y], data = Class, x = 'prediction',         # plot the class\n                     bins = 50, color = color, kde = True, \n                     alpha = 0.2, edgecolor=\"black\")\n        axes[x,y].set_title(f'target = {x}, predicted = {y}')                # add plot subtitles\n        axes[x,y].set_xlim([y\/2, y\/2 + 0.5])                                 # set x-axis\n\nplt.tight_layout()\nplt.show()","c5714082":"leader = pd.read_csv('..\/input\/leaderboard\/public_leaderboard.csv')\n\nOurBest = 0.78737                           # or whatever our best score was on the test data\nCountScores = leader.Score.count()          # should be 7180\nRank = leader.Score[leader.Score > OurBest].count()\n\nplt.figure(figsize=(15,4))\nplt.title(f'Kaggle Leaderboard - our best test score of {OurBest} beat {np.round(100*(1-Rank\/CountScores),2)}% of public scores')\nsns.histplot(leader.Score, binwidth = 0.001, color = \"green\", kde = True, edgecolor=\"black\")\nplt.plot([OurBest, OurBest], [0, 800], linewidth = 2, color = \"blue\", linestyle = '--') \nplt.xlim(0.72, 0.82)\nplt.ylim(0,500)\nplt.show()","474e4ee5":"### Separation between classes\nIn our early models, there was very little separation - the median probability assigned to a true class '0' was about 7%, vs about 8% for a true class '1'. Our final model is able to create a much wider spread of predictions.","1a3e6b07":"### Plotting the ROC-AUC curve and Precision-Recall Curve\nGood information on derivation of roc_auc:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html\n\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\nOur model was scored on area under the curve, not on accuracy, because we have an unbalanced distribution of targets. The model we are considering here, a light gradient boost machine, scored 78.81% on the validation data.\n\nFor an imbalanced target like our data set (~92% '0' to ~8% '1') the precision-recall curve, which shows the tradeoff between precision and recall at different thresholds, can be a useful way to look at things. High precision means that we have a low false positive rate, and high recall means that we have a low false negative rate. It is easy to achieve one or the other (e.g. a model that classified everything to '0' would have very high precision and very low recall, while a model that classified everything to '1' would have very high recall and very low precision.) A model with high AUC is going to do well on both precision and recall.","f627c17f":"# Final Model Evaluation","0617d7c9":"### Import results\nThe model notebook generated a dataframe - output (3).pkl - that contains the targets, predicted probabilities, and binary predictions for the Light GBM model vs the validation data set. We are not going to run the model again in this notebook; this single dataframe is sufficient to plot the ROC and precision-recall curves, confusion matrices, and many other visualizations.","de9c2325":"### Final Model description\nOur best performing model on validation was a Light GBM model - it achieved 0.779771 on 5-fold cross-validation, 0.788100 on the validation data set, and 0.78360 on the test data set.\n\nOur best scoring model on the __test__ data ended up being an ensemble model that combined the Light GBM, logistic regression and random forest models - it scored 0.78737 on the test data, about a third of a percentage point higher than the pure LGBM. Since we do not know the true labels for the test data, this notebook is going focus on the pure LightGBM model and how it scored against the validation data (we held back 25% of the training data set for validation.)","78452607":"Confusion, recall and precision matrices: precision is probability of correct classification GIVEN the classification, while recall is the probability of correct classification GIVEN the true type. Our model predicted more defaults than there were in the true data set; Of those who defaulted, our model successfully predicted that 68.51% would default (i.e. the model assigned them >50% chance of default.) But of those who we predicted would default, only 19.13% actually defaulted. For a real-life risk model, I think that our priority should be on recall over precision: it is easy to achieve high precision by predicting very low probability of default for everyone, but that doesn't really help us with our objective of identifying which customers are high risk.","81d8be19":"### Density plots\nThis is the confusion matrix reinterpreted as four histograms using probabilities.","ff966e3f":"### Our place on the leaderboard\nOur best placing on the public data set was 0.78637 for a 50\/50 ensemble model of the Light GBM that we explored in this notebook, plus a logistic regression model. The graphic below shows how our best compares to the best public scores from the original 2018 Kaggle competition (7180 official submissions.)","7fd706bf":"### Model Statistics: Accuracy, Recall and Precision\nOf accuracy, recall and precision, we really care the most about recall in this application. It is very easy to make a model that achieves >90% accuracy with an unbalanced target. What we really want to know is whether the people who we think will perform on a loan will actually perform."}}