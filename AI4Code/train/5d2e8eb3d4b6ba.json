{"cell_type":{"5e9c46ac":"code","57ae4529":"code","0df25e8b":"code","98b338fa":"code","4d27d4e0":"code","65a66686":"code","d52518b2":"code","e5648c5e":"code","d9b29588":"code","169b34cc":"code","07d8f7bc":"markdown","bd59403d":"markdown","ee0805db":"markdown","3580abf0":"markdown","09840d17":"markdown","24fff2d4":"markdown","da079ed0":"markdown","14ef294d":"markdown","bfae0903":"markdown","69a06f75":"markdown","36da6a33":"markdown","25fd1f47":"markdown"},"source":{"5e9c46ac":"#Always import the proper libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold","57ae4529":"f = open(\"\/kaggle\/input\/uci-poker-hand-dataset\/poker-hand-training-true.data\",\"r\")\nls_hand = []\nfor line in f:\n    ls_hand.append(line[:-1].split(\",\"))\nfor r in range(len(ls_hand)):\n    for val in range(len(ls_hand[r])):\n        ls_hand[r][val] = int(ls_hand[r][val])\nf.close()\ndf_hands_train = pd.DataFrame(data= ls_hand,columns = [\"S1\", \"C1\",\"S2\", \"C2\",\"S3\", \"C3\",\"S4\", \"C4\",\"S5\", \"C5\",\"Hand Rank\"])\n\nf = open(\"\/kaggle\/input\/uci-poker-hand-dataset\/poker-hand-testing.data\",\"r\")\nls_hand = []\nfor line in f:\n    ls_hand.append(line[:-1].split(\",\"))\nfor r in range(len(ls_hand)):\n    for val in range(len(ls_hand[r])):\n        ls_hand[r][val] = int(ls_hand[r][val])\nf.close()\ndf_hands_test = pd.DataFrame(data= ls_hand,columns = [\"S1\", \"C1\",\"S2\", \"C2\",\"S3\", \"C3\",\"S4\", \"C4\",\"S5\", \"C5\",\"Hand Rank\"])","0df25e8b":"def process_data(data):\n    df = data.copy()\n    cards = df[[\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"]]\n    suits = df[[\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]]\n    cards.values.sort()\n    suits.values.sort()\n    df[[\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"]] = cards\n    df[[\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]] = suits\n    df = df[[\"S1\", \"C1\",\"S2\", \"C2\",\"S3\", \"C3\",\"S4\", \"C4\",\"S5\", \"C5\"]]\n    df = add_counts(df)\n    df = add_diffs(df)\n    df = add_unique_count(df)\n    return df","98b338fa":"def add_counts(df):\n    tmp_card = df[[\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"]]\n    tmp_suit = df[[\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]]\n    #counts how many like cards are in the row, exposes the relationship required for pairs, sets, and quads.\n    df[\"cnt_c1\"] = tmp_card.apply(lambda c: sum(c==c[0]), axis=1)\n    df[\"cnt_c2\"] = tmp_card.apply(lambda c: sum(c==c[1]), axis=1)\n    df[\"cnt_c3\"] = tmp_card.apply(lambda c: sum(c==c[2]), axis=1)\n    df[\"cnt_c4\"] = tmp_card.apply(lambda c: sum(c==c[3]), axis=1)\n    df[\"cnt_c5\"] = tmp_card.apply(lambda c: sum(c==c[4]), axis=1)\n    #counts how many like suits are in the row, allows for easy tracking of flushes\n    df[\"cnt_s1\"] = tmp_suit.apply(lambda s: sum(s==s[0]), axis=1)\n    df[\"cnt_s2\"] = tmp_suit.apply(lambda s: sum(s==s[1]), axis=1)\n    df[\"cnt_s3\"] = tmp_suit.apply(lambda s: sum(s==s[2]), axis=1)\n    df[\"cnt_s4\"] = tmp_suit.apply(lambda s: sum(s==s[3]), axis=1)\n    df[\"cnt_s5\"] = tmp_suit.apply(lambda s: sum(s==s[4]), axis=1)\n    return df","4d27d4e0":"def add_diffs(df):\n    tmp = df\n    #Calculates the difference between cards to determine if a straight is possible\n    df[\"diff_1\"] = tmp[\"C2\"] - tmp[\"C1\"]\n    df[\"diff_2\"] = tmp[\"C3\"] - tmp[\"C2\"]\n    df[\"diff_3\"] = tmp[\"C4\"] - tmp[\"C3\"]\n    df[\"diff_4\"] = tmp[\"C5\"] - tmp[\"C4\"]\n    #Counts how many similar differences there are. Should improve straight detection, and pair detection\n    tmp_diff = df[[\"diff_1\",\"diff_2\",\"diff_3\",\"diff_4\"]]\n    df[\"cnt_diff1\"] = tmp_diff.apply(lambda c: sum(c==c[0]), axis=1)\n    df[\"cnt_diff2\"] = tmp_diff.apply(lambda c: sum(c==c[1]), axis=1)\n    df[\"cnt_diff3\"] = tmp_diff.apply(lambda c: sum(c==c[2]), axis=1)\n    df[\"cnt_diff4\"] = tmp_diff.apply(lambda c: sum(c==c[3]), axis=1)\n    return df","65a66686":"def add_unique_count(df):\n    tmp_suit = df[[\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]]\n    df[\"unique_suit\"] = tmp_suit.apply(lambda s: len(np.unique(s)), axis=1)\n    return df","d52518b2":"x_train = df_hands_train.iloc[:,:-1]\nx_train = process_data(x_train)\ny_train = df_hands_train.iloc[:,-1]","e5648c5e":"x_test = df_hands_test.iloc[:,:-1]\nx_test = process_data(x_test)\ny_test = df_hands_test.iloc[:,-1]","d9b29588":"#vizualize the distribution of the possible hands in the test data.\nprint(y_train.groupby(y_train).size())\ny = y_train.groupby(y_train).size()\nfig = plt.figure()\nax = fig.add_axes([0.1,0.1,.8,.8])\nax.set_title('Hand ranking')\nax.set_xlabel('Rank 0-9')\nax.set_ylabel('Occurences')\nax.set_yscale('log')\nax.set_xticks([0,1,2,3,4,5,6,7,8,9])\nfor val in range(len(y)):\n    ax.annotate(str(y[val]), xy = (val, y[val]), xytext = (val-.4, y[val]))\n    ax.bar(val,y[val])\nplt.show()\nhands = y.sum()\ns = pd.DataFrame()\ns[\"prob_hand\"] = y.apply(lambda x: (x*10000\/\/hands)\/100)\nprint(s)\nfig = plt.figure()\nax = fig.add_axes([0.1,0.1,.8,.8])\nax.set_title('Hand ranking')\nax.set_xlabel('Rank 0-9')\nax.set_ylabel('Occurences')\nax.set_xticks([0,1,2,3,4,5,6,7,8,9])\nfor val in range(len(s)):\n    ax.annotate(str(s.iloc[val,0]), xy = (val, s.iloc[val,0]), xytext = (val-.4, s.iloc[val,0]))\n    ax.bar(val,s.iloc[val,0])\nplt.show()\n\n","169b34cc":"\ndef cross_validation(algo, X_train, Y_train, folds = 10):\n    kf = KFold(n_splits = folds, shuffle=True)\n    acc = []\n    matrix = None\n    first = True\n\n    i=1\n    for train_index, test_index in kf.split(X_train, Y_train):\n        fx_train, fx_test = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n        fy_train, fy_test = Y_train[train_index],Y_train[test_index]\n        algo.fit(fx_train, fy_train)\n        fy_pred = algo.predict(fx_test)\n        score = accuracy_score(fy_test, fy_pred, normalize=True)\n        acc.append(score)\n        i+=1\n    acc = pd.Series(acc)\n    return acc.mean()\n\n\n\n#Decision Tree Classifier\nprint(\"Running Calculations for DecisionTreeClassifier\")\nhands_fit = DecisionTreeClassifier(random_state=1).fit(x_train, y_train)\nscore = cross_validation(hands_fit, x_train, y_train)\nprint(\"Completed with an accuracy of: \", score, \"\\n\")\n\n#Random Forest Classifier\nprint(\"Running Calculations for RandomForestClassifier\")\nfit = RandomForestClassifier(random_state=1).fit(x_train, y_train)\nscore = cross_validation(fit, x_train, y_train)\nprint(\"Completed with an accuracy of: \", score)","07d8f7bc":"The next helper function is the add_diffs function.\n\nThis function accepts a DataFrame which then has 4 new columns added onto it. Each column represents the difference of the card next to it. Therefore when the model is fit, if all values in the columns are equal to \"1\" then it must be a type of straight. The second half of the function is similar to the add_counts function in the sense that it adds up the differences. So if there is a straight the cnt_diff columns should all equal \"4\" except in the case of an ace during a royal straight. ","bd59403d":"# Introduction\n   We are looking to create a predictive model that can classify what type of hand you have given 5 cards and their respective suit.","ee0805db":"We must repeat the same process for the testing data in order to make it usable","3580abf0":"# Conclusion\n   As you can see both methods have at least a 99.9% accuracy in it's prediction. The only difference between the two is that the random forest is marginally better than the decision tree classifier.\n    \n   Given the results, we can conclude that we can accurately predict the given poker hand. The next steps would be adding this model to an ensemble. Particularly a card image processing Neural Network. The neural network would determine the state of the cards on the table, and then pass along the parameters for this model to predict the best current hand. Some tweaks would have to be made so that the model takes into account all the 7 possible cards, and returns the 5 best cards and their classification (pair, straight, or flush). Following this determination, the information can be passed along to another model that determines what bet to place based on the availiable information given by the rest of the table.","09840d17":"# Step 3\n   Vizualize the key features of the data.","24fff2d4":"# Step 1\n   Step one with any data is to import it. \n    \n   Below we have the statement that is importing the data which is of file extension \".data\". Please note that the program is reading line by line and therefore must make appropriate type conversions. \n    \n","da079ed0":"The next helper function is the add_unique_count function.\n\n   This function accepts a DataFrame which is then broken into the suits. Then a lambda function is applied to each of the rows to check and see how many unique suits are in the hand. This helps for checking a flush condition or not. A flush will always have a unique count of \"1\" because all cards must be of the same suit. ","14ef294d":"# Step 4\n   Deciding on the proper model to use\n   \n   Given the data, we know we will be using a classification model. I have chosen to compare the results between the Random forest classification modet and the Decision tree classification model. It is always a good idea to test the fit from several models in order to see which works best. \n   \n   Please not that below I am using a custom cross validation function. I do not like how the cross validate function provided by sklearn returns it's data. ","bfae0903":"The next helper function is the add_counts function.\n\nThis function accepts a DataFrame which is then broken down based on cards. Then for each card, a lambda functions is applied to each line that counts each occurence of that card. This is done for each card within the hand. A new column is then added to the original DataFrame for the count of each card. The same is done for the relationships of the suit.\n\nNote the reason we break them up this way is to not accidentally count a \"2\" in the suits column when we are looking at the cards in hand. ","69a06f75":"I found it interesting to explore the relationship between the occurences and the probablities of each possible hands. As you can see by the data, the majority of hand you will play during a session of poker are worth nothing. If you happen to have a pair, you can develop an appropriate betting strategy in order to see if you are able to cover anyone. Be concerned that although you have a relatively common hand, someone will always have a hand better than yours. If you happen to have a two pair and both cards of the pair are not found in the community cards. You have a relatively strong hand compared to others. The odds of being beat by a set exist, but as shown, it is relatively low compared to other hands. Good luck getting anything else, the odds for the rest are very low. \n\nOne thing to take into account, I did not create this data set and does not reflect probabilities during actual games. Just what is being observed based on the generated set. Although, these probabilites are useful for omaha. Considering you are dealt a hand of 5. ","36da6a33":"   Following the data cleaning, we take that DataFrame and we split it into the training data. \n   - X_train is all the lines except the classification of the hand.\n   - Y_train is the given hand classification.    ","25fd1f47":"# Step 2\n   We must now decide how to clean and explain the relationships between the cards.\n   \n   Each classification of a hand has a different relationship attached to the cards. In order for our model to pick up on these patterns we must find a way to articulate it.\n   - For pairs, sets, and quads (2,3, and 4 of a kind) we must show the common occurences in the hand.\n   - For straights we must show the difference between the cards in an ascending order.\n   - For flushes we must show the common occurences in the suits of the hand.\n   \n   In order to do this we must assign some helper functions to help clean up the data.\n   \n   First, we have to clean up the data to prepare to calculate the relationships mentioned above. Here we are taking the data splitting it up based on card and suit. We will then sort the values before combining it back into one DataFrame and passing onto the next helper function that will add relationships to the data frame.\n    "}}