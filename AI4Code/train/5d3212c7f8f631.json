{"cell_type":{"2a7273e9":"code","a350796b":"code","9e1f7b30":"code","286b8a90":"code","3666c9ff":"code","f5fa65df":"code","84261a36":"code","0392ccdd":"code","f229c1e9":"code","b863b4e8":"code","00670af6":"code","00762206":"code","2b97e8fb":"code","7e376754":"code","95fbc3d0":"code","6f252781":"code","27446832":"code","d52cfa05":"code","b8a75566":"code","f613af4c":"code","e2ea2d2e":"code","3706dd59":"code","e7e8d6b3":"code","e694b67e":"code","9ca9e02c":"code","2d44d237":"code","af93009a":"code","b3f57353":"code","30ad24d3":"code","ba90a87f":"code","63b5283e":"code","d3c7f989":"code","a9ea5310":"code","34d1f8cc":"markdown","1cad0912":"markdown","a0e2293a":"markdown","6ddf93a9":"markdown","0c9937c2":"markdown"},"source":{"2a7273e9":"import os\n__print__ = print\ndef print(string):\n    os.system(f'echo \\\"{string}\\\"')\n    __print__(string)","a350796b":"! pip install pytorch-transformers","9e1f7b30":"from fastai.text import *\nfrom fastai.metrics import *\nfrom pytorch_transformers import RobertaTokenizer, DistilBertTokenizer","286b8a90":"# Creating a config object to store task specific information\nclass Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n        \nconfig = Config(\n    testing=False,\n    seed = 2019,\n    roberta_model_name='roberta-base', # can also be exchnaged with roberta-large \n    epochs=2,\n    use_fp16=False,\n    bs=4, \n    max_seq_len=128, \n    num_labels = 2,\n    hidden_dropout_prob=.10,\n    hidden_size=768, # 1024 for roberta-large\n    start_tok = \"<s>\",\n    end_tok = \"<\/s>\",\n)","3666c9ff":"df = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","f5fa65df":"df.shape","84261a36":"if config.testing: df = df[:5000]\nprint(df.shape)","0392ccdd":"df.head()","f229c1e9":"feat_cols = \"review\"\nlabel_cols = \"sentiment\"","b863b4e8":"class FastAiRobertaTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]","00670af6":"# create fastai tokenizer for roberta\nroberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\nfastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n                             pre_rules=[], post_rules=[])","00762206":"# create fastai vocabulary for roberta\npath = Path()\nroberta_tok.save_vocabulary(path)\n\nwith open('vocab.json', 'r') as f:\n    roberta_vocab_dict = json.load(f)\n    \nfastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))","2b97e8fb":"# Setting up pre-processors\nclass RobertaTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass RobertaNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n\n\ndef get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for Roberta\n    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n    \"\"\"\n    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]","7e376754":"from sklearn.model_selection import train_test_split","95fbc3d0":"train, val = train_test_split(df, shuffle=True, test_size=0.2, random_state=42)","6f252781":"train.shape, val.shape","27446832":"databunch_1 = TextDataBunch.from_df(\".\", train, val, \n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_roberta_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=feat_cols,\n                  label_cols=label_cols,\n                  bs=4,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","d52cfa05":"import torch\nimport torch.nn as nn\nfrom pytorch_transformers import RobertaModel\n\n# defining our model architecture \nclass CustomRobertaModel(nn.Module):\n    def __init__(self,num_labels=2):\n        super(CustomRobertaModel,self).__init__()\n        self.num_labels = num_labels\n        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n        \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n        logits = self.classifier(pooled_output)        \n        return logits","b8a75566":"roberta_model = CustomRobertaModel()\n\nlearn = Learner(databunch_1, roberta_model, metrics=[accuracy, error_rate])","f613af4c":"learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\nlearn.fit_one_cycle(config.epochs, 1e-5, wd=1e-5)","e2ea2d2e":"learn.recorder.plot_losses()","3706dd59":"def roberta_clas_split(self) -> List[nn.Module]:\n\n    \n    roberta = roberta_model.roberta\n    embedder = roberta.embeddings\n    pooler = roberta.pooler\n    encoder = roberta.encoder\n    classifier = [roberta_model.dropout, roberta_model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    print(len(encoder.layer))\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","e7e8d6b3":"x = roberta_clas_split(roberta_model)","e694b67e":"learner = Learner(databunch_1, roberta_model, metrics=[accuracy, error_rate])","9ca9e02c":"learner.layer_groups","2d44d237":"learner.split([x[0], x[2], x[4]])","af93009a":"learner.layer_groups","b3f57353":"learner.freeze()\nlearner.lr_find()\nlearner.recorder.plot()","30ad24d3":"learner.model.roberta.train()\nlearner.fit_one_cycle(config.epochs, max_lr=slice(1e-7,1e-6), wd =(1e-7, 1e-5, 1e-4))\nlearner.recorder.plot_losses()","ba90a87f":"learner.unfreeze()\nlearner.fit_one_cycle(2, max_lr=slice(1e-7,1e-6), wd =(1e-7, 1e-5, 1e-4))\nlearner.recorder.plot_losses()","63b5283e":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    learn.model.roberta.eval()\n    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch_1.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    ordered_preds = preds[reverse_sampler, :]\n    pred_values = np.argmax(ordered_preds, axis=1)\n    return ordered_preds, pred_values","d3c7f989":"preds, pred_values = get_preds_as_nparray(DatasetType.Valid)","a9ea5310":"# accuracy on valid\n(pred_values == databunch_1.valid_ds.y.items).mean()","34d1f8cc":"# Building the Model","1cad0912":"# Splitting the model and Discriminative Layering Technique","a0e2293a":"## Setting Up the Tokenizer","6ddf93a9":"# Using RoBERTa with Fastai","0c9937c2":"# Getting Predictions"}}