{"cell_type":{"bec65520":"code","e41bcb4d":"code","3eb156ed":"code","d363b86e":"code","92b62003":"code","f054992b":"code","612cf63e":"code","67e9610d":"code","c8b0bbd3":"code","f9acef7c":"code","5754fa8c":"code","0e6e7f48":"code","b6aa684d":"code","ed45d291":"code","c8d6210e":"code","6f0330fe":"code","cc775bf1":"code","b2882b6a":"markdown"},"source":{"bec65520":"import numpy as np \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e41bcb4d":"df = pd.read_csv(\"\/kaggle\/input\/wineuci\/Wine.csv\", names = ['class','Alcohol','MalicAcid','Ash','AshAlcalinity','Magnesium','Phenol',\n                                      'Flavanoid','NonFlavanoid','Proanthocyanins','ColorIntensity',\n                                      'Hue','DilutedWines','Proline'])\ndf.head()","3eb156ed":"df.info()","d363b86e":"# to check the if the class is balance\nprint(df.groupby('class').size())","92b62003":"from imblearn.over_sampling import SMOTE\n\n# Resample the minority class. You can change the strategy to 'auto' if you are not sure.\nsm = SMOTE(random_state=7)\n\n# Fit the model to generate the data.\noversampled_trainX, oversampled_trainY = sm.fit_sample(df.drop('class', axis=1), df['class'])\noversampled_train = pd.concat([pd.DataFrame(oversampled_trainY), pd.DataFrame(oversampled_trainX)], axis=1)\noversampled_train.columns = df.columns\noversampled_train.info()","f054992b":"# to chech the class\nprint(oversampled_train.groupby('class').size())","612cf63e":"def importdata():\n    df = oversampled_train\n    return df","67e9610d":"def splitdataset(df):\n    \n    # Seperating the target variable\n    X = df.values[:,1:]  \n    y = df.values[:,0] \n\n    #Split data into training and test datasets (training will be based on 70% of data)\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify =y)\n    \n    # transform data so its distribution will have a mean value 0 and standard deviation of 1\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    \n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n    \n    return X, y, X_train, X_test, y_train, y_test","c8b0bbd3":"from sklearn.tree import DecisionTreeClassifier,export_graphviz\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\nfrom sklearn import tree","f9acef7c":"# Function to perform training with giniIndex. \ndef train_using_gini(X_train, X_test, y_train): \n  \n    # Creating the classifier object \n    #clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n    #        random_state = 100,max_depth=8, min_samples_leaf=3)\n    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100)\n  \n    # Performing training \n    clf_gini.fit(X_train, y_train) \n    return clf_gini","5754fa8c":"# Function to perform training with entropy. \ndef tarin_using_entropy(X_train, X_test, y_train): \n  \n    # Decision tree with entropy \n    clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100) \n  \n    # Performing training \n    clf_entropy.fit(X_train, y_train) \n    return clf_entropy","0e6e7f48":"# Function to make predictions \ndef prediction(X_test, clf_object): \n  \n    # Predicton on test with giniIndex \n    y_pred = clf_object.predict(X_test) \n    return y_pred","b6aa684d":"# Function to show prediction values\ndef pred_result (df,y_test,y_pred_clf):\n\n    df_new = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_clf})\n    df_new['result'] = np.where(df_new['Actual'] == df_new['Predicted'], 'correct', 'wrong')\n    print(df_new)","ed45d291":"# Function to calculate accuracy \ndef cal_accuracy(y_test, y_pred): \n      \n    print(\"Confusion Matrix: \\n\", \n    confusion_matrix(y_test, y_pred)) \n      \n    print (\"Accuracy : \\n\", \n    accuracy_score(y_test,y_pred)*100) \n      \n    print(\"Report : \\n\", \n    classification_report(y_test, y_pred))","c8d6210e":"# Function to draw decision tree\ndef draw_dt (df,clf_object):\n    \n    graph = Source(tree.export_graphviz(clf_object, out_file=None\n                                        , feature_names= df.iloc[:, 1:].columns, class_names=['1', '2', '3'] \n                                        , filled = True))\n    display(SVG(graph.pipe(format='svg')))","6f0330fe":"# Driver code \ndef main(): \n      \n    # Building Phase \n    data = importdata() \n    X, y, X_train, X_test, y_train, y_test = splitdataset(data) \n    clf_gini = train_using_gini(X_train, X_test, y_train) \n    clf_entropy = tarin_using_entropy(X_train, X_test, y_train) \n      \n    # Operational Phase \n    print(\"\\n\\033[1m\"+\"Results Using Gini Index:\"+\"\\033[0;0m\") \n      \n    # Prediction using gini \n    y_pred_gini = prediction(X_test, clf_gini) \n    cal_accuracy(y_test, y_pred_gini)\n    \n    #Prediction result\n    pred_result(data,y_test,y_pred_gini)\n    \n     # Draw tree\n    draw_dt (data,clf_gini)\n    \n    print(\"\\n\\n\\033[1m\" + \"Results Using Entropy:\"+\"\\033[0;0m\") \n    # Prediction using entropy \n    y_pred_entropy = prediction(X_test, clf_entropy) \n    cal_accuracy(y_test, y_pred_entropy)\n    \n    #Prediction result\n    pred_result(data,y_test,y_pred_entropy)\n\n    # Draw tree\n    draw_dt (data,clf_entropy)","cc775bf1":"import sys\nnp.set_printoptions(threshold=sys.maxsize)\n# Calling main function \nif __name__==\"__main__\": \n    main()","b2882b6a":"we can see that the class is imbalance with class 2 as highest"}}