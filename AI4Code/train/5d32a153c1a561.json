{"cell_type":{"4e2ed1f9":"code","8395fd8a":"code","d51ea436":"code","07dc9e65":"code","3e5da926":"code","cca9ffca":"code","74ddf6fa":"code","6a6f3b9c":"code","3c336cea":"code","3aeddbb4":"code","7f439524":"code","b5f96220":"code","7e8daec0":"code","12c8f14a":"code","1b126144":"code","4e9a3859":"code","3d5c0089":"code","081b66a3":"code","3d3cef25":"code","065fd56b":"code","da2a3b21":"code","ba0e9682":"code","849a087e":"code","1d90d79f":"code","7a98194e":"code","211e5f39":"code","10f500fd":"code","1e066264":"code","1c5b90a1":"code","d238b4f4":"code","4cc81f1b":"code","16f902ce":"code","342d5b84":"code","807cede1":"code","b2c496ec":"code","c1206a4f":"code","da7065fa":"code","8b42da14":"code","e29ce606":"code","6aae06b0":"code","3d8bebb5":"code","803d691b":"markdown","4f8d72b0":"markdown","90a93f31":"markdown","9b4229c9":"markdown","a940522f":"markdown","9ad591bb":"markdown","04ff4124":"markdown","7c5cfb54":"markdown","931c848c":"markdown","f9d249c3":"markdown","da8ada90":"markdown","81eed5cf":"markdown","c89f2fa9":"markdown","e9f99a73":"markdown","9c444850":"markdown","1add591b":"markdown","b8cce4a8":"markdown","b6b74da9":"markdown","e461004a":"markdown","af64295c":"markdown","d803517c":"markdown","4788e837":"markdown"},"source":{"4e2ed1f9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8395fd8a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom warnings import filterwarnings\nimport pprint\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import skew","d51ea436":"filterwarnings(action='ignore')","07dc9e65":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3e5da926":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","cca9ffca":"train_df.describe()","74ddf6fa":"train_df.info()","6a6f3b9c":"# seperating categorical features from non-categoricals\n\ncategoricals = train_df.dtypes[train_df.dtypes == 'object'].index\nnon_categoricals = train_df.dtypes[train_df.dtypes != 'object'].index\n\nprint('Categoricals: ', categoricals)\nprint('\\n Non-Categoricals: ', non_categoricals)","3c336cea":"nums = train_df.isna().sum().sort_values(ascending=False)\npercent = train_df.isna().sum()\/train_df.isna().count().sort_values(ascending=False)\nmissings = pd.concat([nums, percent], axis=1, keys=['Total', 'Percent'])\n\nmissings[missings['Total'] != 0]","3aeddbb4":"trap_missings = ['Fence', 'PoolQC', 'Alley', 'FireplaceQu',\n                 'GarageFinish', 'GarageType', 'GarageQual',\n                 'GarageCond', 'MiscFeature', 'BsmtFinType2',\n                 'BsmtFinType1', 'BsmtExposure', 'BsmtCond',\n                 'BsmtQual', 'MasVnrType']\n\nfor col in trap_missings:\n    train_df[col].fillna('None', inplace=True)","7f439524":"# filling numerical missing valeus\n\ntrain_df['LotFrontage'].fillna(train_df['LotFrontage'].mean(), inplace=True)\ntrain_df['GarageYrBlt'].fillna(train_df['GarageYrBlt'].mean(), inplace=True)\ntrain_df['MasVnrArea'].fillna(train_df['MasVnrArea'].mean(), inplace=True)","b5f96220":"# filling Categorical missing values -> using mode\n\ntrain_df['Electrical'] = train_df['Electrical'].fillna(train_df['Electrical'].mode([0]))","7e8daec0":"nums = train_df.isna().sum().sort_values(ascending=False)\npercent = train_df.isna().sum()\/train_df.isna().count().sort_values(ascending=False)\nmissings = pd.concat([nums, percent], axis=1, keys=['Total', 'Percent'])\n\nmissings[missings['Total'] != 0]","12c8f14a":"df = train_df","1b126144":"non_categoricals = non_categoricals.drop(['Id', 'SalePrice'])","4e9a3859":"fig, axes = plt.subplots(6, 6, figsize=(30, 30))\n\nfor col, ax in zip(non_categoricals, axes.flatten()):\n    sns.scatterplot(train_df[col], y=train_df['SalePrice'], ax=ax, alpha=0.3)","3d5c0089":"train_df = train_df.drop(train_df[train_df['GrLivArea']>5000].index)\ntrain_df = train_df.drop(train_df[train_df['LotArea']>200000].index)\ntrain_df = train_df.drop(train_df[train_df['TotalBsmtSF']>4000].index)\ntrain_df = train_df.drop(train_df[train_df['LotFrontage']>200].index)\ntrain_df = train_df.drop(train_df[train_df['1stFlrSF']>4000].index)","081b66a3":"# getting the features which are highly correlated to SalePrice\n\ncols = train_df.corr().nlargest(20, 'SalePrice')['SalePrice'].index\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(train_df[cols].corr(), cmap='Greys', annot=True)","3d3cef25":"none = ['None', 'NA']\nfor row in [train_df]:\n    row['HasPool'] = 1\n    row.loc[(row['PoolQC'].isin(none)), 'HasPool'] = 0\n\n    row['HasWoodDeck'] = 1\n    row.loc[(row['WoodDeckSF'].isin(none)), 'HasWoodDeck'] = 0\n\n    row['HasOpenPorch'] = 1\n    row.loc[(row['OpenPorchSF'].isin(none)), 'HasOpenPorch'] = 0\n\n    row['HasScreenPorch'] = 1\n    row.loc[(row['ScreenPorch'].isin(none)), 'HasScreenPorch'] = 0\n\n    row['HasAlleyAccess'] = 1\n    row.loc[(row['Alley'].isin(none)), 'HasAlleyAccess'] = 0\n\n    row['HasFirePlace'] = 1\n    row.loc[(row['Fireplaces'] == 0), 'HasFirePlace'] = 0\n\n    row['HasGarage'] = 1\n    row.loc[(row['GarageType'].isin(none)), 'HasGarage'] = 0\n    \n    row['HasMVArea'] = 1\n    row.loc[(row['MasVnrArea'] == 0), 'HasMVArea'] = 0\n    \n\n    row['Remodeled'] = 1\n    row.loc[(row['YearBuilt'] == row['YearRemodAdd']), 'Remodeled'] = 0\n\n    row['TotalHouseSF'] = row['1stFlrSF'] + \\\n        row['TotalBsmtSF'] + row['2ndFlrSF']\n\n    row['HasBasement'] = 1\n    row.loc[(row['BsmtFinType1'].isin(none) & (\n        row['BsmtFinType2'].isin(none))), 'HasBasement'] = 0\n\n    row['TotalBathroom'] = row['FullBath'] + \\\n        (row['HalfBath']*0.5) + row['BsmtFullBath'] + \\\n        (row['BsmtHalfBath']*0.5)\n\n    row['TotalHouseQuality'] = row['OverallQual'] + row['OverallCond']\n    ","065fd56b":"cols = train_df.corr().nlargest(20, 'SalePrice')['SalePrice'].index\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(train_df[cols].corr(), cmap='Greys', annot=True)","da2a3b21":"train_df = train_df[cols]\ntrain_df.drop(['GarageArea', '1stFlrSF', 'Fireplaces', 'MasVnrArea', 'BsmtFinSF1'], axis=1, inplace=True)","ba0e9682":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14,6))\n\nsns.distplot(train_df['SalePrice'], fit=stats.norm, ax=ax[0])\nax[0].set_title('Before Normalization')\n\ntrain_df['SalePrice'] = np.log(train_df['SalePrice'])\nax[1].set_title('After Normalization')\nsns.distplot(train_df['SalePrice'], fit=stats.norm, ax=ax[1])","849a087e":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14,6))\n\nsns.distplot(train_df['GrLivArea'], fit=stats.norm, ax=ax[0])\nax[0].set_title('Before Normalization')\n\ntrain_df['GrLivArea'] = np.log(train_df['GrLivArea'])\nax[1].set_title('After Normalization')\nsns.distplot(train_df['GrLivArea'], fit=stats.norm, ax=ax[1])","1d90d79f":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14,6))\n\nsns.distplot(train_df['TotalHouseSF'], fit=stats.norm, ax=ax[0])\nax[0].set_title('Before Normalization')\n\ntrain_df['TotalHouseSF'] = np.log(train_df['TotalHouseSF'])\nax[1].set_title('After Normalization')\nsns.distplot(train_df['TotalHouseSF'], fit=stats.norm, ax=ax[1])","7a98194e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR","211e5f39":"X = train_df.drop(['SalePrice'], axis=1)\ny = train_df['SalePrice']","10f500fd":"X_train, X_val, y_train, y_val = train_test_split(\n    X, y, random_state=42, test_size=0.2)","1e066264":"lr = LinearRegression()\nrf = RandomForestRegressor(random_state=42)\nlgb = LGBMRegressor(random_state=42, objective='regression')","1c5b90a1":"ensemble_regressor = VotingRegressor(\n    [('lr', lr), ('rf', rf), ('lgb', lgb)])","d238b4f4":"for reg in (lr, rf, lgb, ensemble_regressor):\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_val)\n    print(reg.__class__.__name__, mean_squared_error(y_val, y_pred))","4cc81f1b":"trap_missings = ['PoolQC', 'FireplaceQu', 'Alley']","16f902ce":"test_df.drop(['SaleType', 'Exterior1st', 'KitchenQual', 'Utilities', 'MSZoning',\n             'BsmtQual', 'BsmtCond', 'LotFrontage', 'MiscFeature',\n              'Fence', 'Electrical', 'Exterior2nd', 'Functional', 'GarageFinish',\n              'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1', 'GarageCond', 'BsmtExposure',\n              'MasVnrType', 'GarageQual'], axis=1, inplace=True)","342d5b84":"nums = test_df.isna().sum().sort_values(ascending=False)\npercent = test_df.isna().sum()\/test_df.isna().count().sort_values(ascending=False)\nmissings = pd.concat([nums, percent], axis=1, keys=['Total', 'Percent'])\n\nmissings[missings['Total'] != 0]","807cede1":"# replacing None for those were included in the data description\nfor col in trap_missings:\n    test_df[col].fillna('None', inplace=True)\n\n# imputing values for numerical features\ntest_df['GarageYrBlt'].fillna(df['GarageYrBlt'].mean(), inplace=True)\ntest_df['MasVnrArea'].fillna(df['MasVnrArea'].mean(), inplace=True)\ntest_df['TotalBsmtSF'].fillna(df['TotalBsmtSF'].mean(), inplace=True)\ntest_df['GarageArea'].fillna(df['GarageArea'].mean(), inplace=True)\ntest_df['BsmtFullBath'].fillna(df['BsmtFullBath'].mode()[0], inplace=True)\ntest_df['BsmtHalfBath'].fillna(df['BsmtHalfBath'].mode()[0], inplace=True)\ntest_df['GarageCars'].fillna(df['GarageCars'].mode()[0], inplace=True)\ntest_df['BsmtFinType1'].fillna(df['BsmtFinType1'].mode()[0], inplace=True)\ntest_df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0], inplace=True)\ntest_df['GarageType'].fillna(df['GarageType'].mode()[0], inplace=True)","b2c496ec":"nums = test_df.isna().sum().sort_values(ascending=False)\npercent = test_df.isna().sum()\/test_df.isna().count().sort_values(ascending=False)\nmissings = pd.concat([nums, percent], axis=1, keys=['Total', 'Percent'])\n\nmissings[missings['Total'] != 0]","c1206a4f":"none = ['None', 'NA']\nfor row in [test_df]:\n    row['HasPool'] = 1\n    row.loc[(row['PoolQC'].isin(none)), 'HasPool'] = 0\n\n    row['HasWoodDeck'] = 1\n    row.loc[(row['WoodDeckSF'].isin(none)), 'HasWoodDeck'] = 0\n\n    row['HasOpenPorch'] = 1\n    row.loc[(row['OpenPorchSF'].isin(none)), 'HasOpenPorch'] = 0\n\n    row['HasScreenPorch'] = 1\n    row.loc[(row['ScreenPorch'].isin(none)), 'HasScreenPorch'] = 0\n\n    row['HasAlleyAccess'] = 1\n    row.loc[(row['Alley'].isin(none)), 'HasAlleyAccess'] = 0\n\n    row['HasFirePlace'] = 1\n    row.loc[(row['Fireplaces'] == 0), 'HasFirePlace'] = 0\n\n    row['HasGarage'] = 1\n    row.loc[(row['GarageType'].isin(none)), 'HasGarage'] = 0\n    \n    row['HasMVArea'] = 1\n    row.loc[(row['MasVnrArea'] == 0), 'HasMVArea'] = 0\n    \n\n    row['Remodeled'] = 1\n    row.loc[(row['YearBuilt'] == row['YearRemodAdd']), 'Remodeled'] = 0\n\n    row['TotalHouseSF'] = row['1stFlrSF'] + \\\n        row['TotalBsmtSF'] + row['2ndFlrSF']\n\n    row['HasBasement'] = 1\n    row.loc[(row['BsmtFinType1'].isin(none) & (\n        row['BsmtFinType2'].isin(none))), 'HasBasement'] = 0\n\n    row['TotalBathroom'] = row['FullBath'] + \\\n        (row['HalfBath']*0.5) + row['BsmtFullBath'] + \\\n        (row['BsmtHalfBath']*0.5)\n\n    row['TotalHouseQuality'] = row['OverallQual'] + row['OverallCond']","da7065fa":"features = X_train.columns.to_list()\ntest_df = test_df[features]","8b42da14":"test_df['GrLivArea'] = np.log(test_df['GrLivArea'])\ntest_df['TotalHouseSF'] = np.log(test_df['TotalHouseSF'])","e29ce606":"y_pred = np.expm1(ensemble_regressor.predict(test_df))","6aae06b0":"y_pred = pd.DataFrame(y_pred, columns=['SalePrice'])","3d8bebb5":"y_pred","803d691b":"## Finding and handling missing values","4f8d72b0":"According to data description, NaN in some features doesn't mean that the data is missing, it means None: Fence -> Nan means no_fence\nSo we're gonna replace them with None.","90a93f31":"## Outliers","9b4229c9":"For recognizing outlier I'd rather recognize them visually, so we're gonna have some scatter plots for all numerical features.","a940522f":"Hey Guys!\nI hope you had a great day so far. \nThis notebook contais data cleaning, data analysis, feature engineering, normalization and modeling. I tried my best to keep it simple and beginner friendly. Feel free to share your thoughts and ideas with me and ask any question about this code in comment section. \nDon't forget to share this kernel with your friends and please upvote if you've learned anything from it.","9ad591bb":"We dropped column which had the same effect on producing HousePrice and were highly correlated to eachother: Garagecars and GarageArea. And also I dropped MasVnrArea because it contains lots of zero values and does not follow normal distribution. Normalizing columns with zero values by log transformation are a problem since log(0) is undefined. We created HasMVArea so that hopefully we don't lose much information.","04ff4124":"Can we do anything with them? no, outliers are lying to us; so as a punishment we drop them.","7c5cfb54":"All clean!","931c848c":"## Feature importance","f9d249c3":"\n## Predicting the test set","da8ada90":"### Missing values","81eed5cf":"So we just saw a heatmap that could show us how correlated features are. But what if row features are useless? what if there are some hidden features behind this row features? \nCan we add some new features which are made from row features? of couse we can.\nHere's some new features which are made from row features:","c89f2fa9":"## Modeling","e9f99a73":"### Creating new features","9c444850":"Let's see if there's anything missed.","1add591b":"## Normalization","b8cce4a8":"## Loading datasets and getting some info","b6b74da9":"This dataset has so many column which can cause complexing out machine learning model. There is some tricks to get the best out of the features. I wanna use feature selection based on correlation. It chooses features which are more likly to produce SalePrice. ","e461004a":"Checking if there is any change in the correlation after creating new features","af64295c":"All clean!","d803517c":"Here you can see the heatmap of row features. By row I mean default features of the dataset.","4788e837":"## Normalization"}}