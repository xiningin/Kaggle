{"cell_type":{"4da405aa":"code","c5e3cc34":"code","9ea93f40":"code","e84c9d71":"code","9770f71a":"code","b545fd86":"code","7e86dac8":"code","f6f24cb8":"code","57edd1bc":"code","ebfaf34b":"code","432fa204":"code","cc410e0e":"code","e0dbf890":"code","8da5e8c9":"code","463152a1":"code","3597817a":"code","78536ab5":"code","63c73842":"code","870569fa":"code","7c724a7f":"code","0836c89b":"code","177698e0":"code","68751724":"code","b9783c76":"code","6f40fa6a":"code","69f00a13":"code","a3398b00":"code","a04a34a6":"code","a3af881a":"code","b7e2bb29":"code","2c2ac302":"code","9f558776":"code","8b1b08af":"code","48307d9c":"code","8e912c63":"code","9acd30ed":"code","a0bf0663":"code","7a00bd16":"code","4d2eeaa3":"code","bba13c55":"code","9f152d0d":"code","c1b85c6d":"code","5a7f137d":"code","dedfd189":"code","6d5a8ad2":"code","f501ce2f":"markdown","4715f85c":"markdown","a4b3132f":"markdown","48e1cbf6":"markdown","c3d6e301":"markdown","9391f7d8":"markdown","1c133b18":"markdown","b77ebbcf":"markdown","393bd7d8":"markdown","750d39ad":"markdown","8f005ed6":"markdown","75a46da8":"markdown","ff3d3bf0":"markdown","f7e873f3":"markdown","f6fba8aa":"markdown","f75332c0":"markdown","1915d14e":"markdown","65dd27c2":"markdown","1a6af9d1":"markdown","69bc415b":"markdown","7647377e":"markdown","b4ec3afd":"markdown","58a78b80":"markdown","ac4b9414":"markdown","f368cccd":"markdown","2fe08038":"markdown","1f9c6af0":"markdown","927c9300":"markdown","0def382a":"markdown","5ee582cb":"markdown","dc198951":"markdown","6a269f02":"markdown","4ef2c671":"markdown","76c9c6e6":"markdown","5321e1b9":"markdown","63faa6e3":"markdown","5e9402d4":"markdown","30de4d39":"markdown","c59c6bb3":"markdown","dc7dd2d7":"markdown","635f6de3":"markdown","b14b95d3":"markdown","06984875":"markdown","e4c1835a":"markdown","be096c5c":"markdown","ceda72a1":"markdown"},"source":{"4da405aa":"import os\nimport re\nimport pprint\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import svm\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport seaborn as sb\nfrom matplotlib import pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', None)\npd.set_option('max_rows', None)\npd.set_option('display.width', 1000)\nsb.set_style('whitegrid')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5e3cc34":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_df.info()","9ea93f40":"train_df.head()","e84c9d71":"train_df.describe()","9770f71a":"train_df.isnull().sum()","b545fd86":"test_df.isnull().sum()","7e86dac8":"print(\"Total Alives: %3d\" % len(train_df[train_df['Survived'] == 1]))\nprint(\"Total Deads : %3d\" % len(train_df[train_df['Survived'] == 0]))","f6f24cb8":"sb.swarmplot(x = \"Survived\", y = \"Age\", data = train_df)\nplt.show()","57edd1bc":"alives1 = len(train_df[(train_df['Sex'] == 'male') & (train_df['Survived'] == 1)])\ndeads1 = len(train_df[(train_df['Sex'] == 'male') & (train_df['Survived'] == 0)])\nalives2 = len(train_df[(train_df['Sex'] == 'female') & (train_df['Survived'] == 1)])\ndeads2 = len(train_df[(train_df['Sex'] == 'female') & (train_df['Survived'] == 0)])\n\nprint(\"Male   Alive: %3d, Dead: %3d ===> %0.4f\" % (alives1, deads1, alives1 \/ (alives1 + deads1)))\nprint(\"Female Alive: %3d, Dead: %3d ===> %0.4f\" % (alives2, deads2, alives2 \/ (alives2 + deads2)))\n","ebfaf34b":"sb.swarmplot(x = \"Sex\", y = \"Age\", hue='Survived', data = train_df)\nplt.show()","432fa204":"for val in train_df['SibSp'].unique():\n    alives = len(train_df[(train_df['SibSp'] == val) & (train_df['Survived'] == 1)])\n    deads = len(train_df[(train_df['SibSp'] == val) & (train_df['Survived'] == 0)])\n    print(\"SibSp: [%d] Alive: %3d, Dead: %3d ===> %0.4f\" % (val, alives, deads, alives \/ (alives + deads)))","cc410e0e":"for val in train_df['Parch'].unique():\n    alives = len(train_df[(train_df['Parch'] == val) & (train_df['Survived'] == 1)])\n    deads = len(train_df[(train_df['Parch'] == val) & (train_df['Survived'] == 0)])\n    print(\"Parch: [%d] Alive: %3d, Dead: %3d ===> %0.4f\" % (val, alives, deads, alives \/ (alives + deads)))","e0dbf890":"sb.swarmplot(x = \"Survived\", y = \"Fare\", data = train_df)\nplt.show()","8da5e8c9":"sb.factorplot(\"Survived\", col = \"Pclass\", col_wrap = 3, data = train_df, kind = \"count\")\nplt.show()","463152a1":"sb.swarmplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = train_df)\nplt.show()","3597817a":"sb.factorplot(\"Survived\", col = \"Embarked\", col_wrap = 3, data = train_df, kind = \"count\")\nplt.show()","78536ab5":"df = train_df[['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Pclass']]\ndf['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\ncorr = df.corr()   \nmask = np.triu(np.ones_like(corr, dtype=np.bool))    \nplt.figure(figsize=(14, 10))   \nsb.heatmap(corr, mask=mask, cmap='RdBu_r', annot=True, linewidths=0.5, fmt='0.2f')\nplt.show()","63c73842":"print(train_df[train_df['Embarked'].isna() == True])","870569fa":"df = train_df.copy()\ndf.loc[df['Embarked'].isna() == True, 'Embarked'] = 'X'  ## <-- missing value indicator\nprint(\"Total number of first class passengers whose pay about 80.00 fare\")\nfor embarked in [ 'C', 'S', 'Q', 'X' ]:\n    print(\"Embarked: [%s] ===> %d\" % (embarked, len(df[\n                             (df['Embarked'] == embarked) &\n                             (df['Pclass'] == 1) &\n                             (df['Fare'] >= 78) &\n                             (df['Fare'] < 82)])))","7c724a7f":"train_df.loc[train_df['Embarked'].isna() == True, 'Embarked'] = 'S'","0836c89b":"print(test_df[test_df['Fare'].isna() == True])","177698e0":"print(test_df.loc[test_df['Pclass'] == 3, 'Fare'].median())","68751724":"test_df.loc[test_df['Fare'].isna() == True, 'Fare'] = 7.89","b9783c76":"def preprocessingTitle(rec):\n    name = rec['Name']\n    sex = rec['Sex']\n\n    name = re.search('[a-zA-Z]+\\\\.', name).group(0)\n    name = name.replace(\".\", \"\")\n    if name == 'Col' or name == 'Capt' or name  == 'Major':\n        name = 0\n    elif name == 'Rev' and sex == 'male':\n        return 1\n    elif name == 'Dr' and sex == 'female':\n        name = 2\n    elif name == 'Dr' and sex == 'male':\n        return 3\n    elif name == 'Sir' or name == 'Don':\n        name = 4\n    elif name == 'Mme' or name == 'Mrs':\n        name = 5\n    elif name == 'Lady' or name == 'Countess' or name == 'Dona':\n        return 6\n    elif name == 'Miss' or name == 'Ms' or name == 'Mlle' or name == 'Jonkheer':\n        return 7\n    elif name == 'Master':\n        return 8\n    elif name == 'Mr':\n        return 9\n        \n    \n    return name\n\ntrain_df['Title'] = train_df.apply(preprocessingTitle, axis = 1)\ntest_df['Title'] = test_df.apply(preprocessingTitle, axis = 1)","6f40fa6a":"def fillValues(name, *args):\n    key, columns, label = ['PassengerId'], ['Title', 'Sex', 'Fare', 'SibSp', 'Parch', 'Pclass', 'Embarked' ], [name]\n    \n    alldf = []\n    for df in args:\n        alldf.append(df)\n        \n    all_df = pd.concat(alldf)\n    all_df = all_df[key + columns + label]\n    \n    all_df = pd.get_dummies(all_df, columns = ['Title', 'Sex', 'Pclass', 'Embarked'])\n    \n    cols = set(all_df.columns)\n    cols.remove(name)\n    \n\n    all_df_in = all_df.loc[all_df[name].isna() == False, cols]\n    all_df_lb = all_df.loc[all_df[name].isna() == False, label]\n\n    model = ExtraTreesRegressor(random_state = 0)\n    model.fit(all_df_in, all_df_lb)\n    \n    \n    all_df_im = all_df.loc[all_df[name].isna() == True, cols]\n       \n    preds = model.predict(all_df_im)\n    all_df_im[name] = preds\n    \n    \n    for df in args:\n        df.loc[df[name].isna() == True, name] = all_df_im.loc[all_df_im['PassengerId'].isin(df['PassengerId']), name]\n        df[name] = df[name].astype('int64')","69f00a13":"def postrocessingTitle(rec):\n    title = rec['Title']\n    sex = rec['Sex']\n    age = rec['Age']\n    \n    if sex == 'male' and age < 16:\n        return 10\n    elif sex == 'female' and age < 16:\n        return 11\n    elif age >= 55 and sex == 'male':\n        return 12\n    elif age >= 55 and sex == 'female':\n        return 13\n    \n    return title\n    \nfillValues('Age', train_df, test_df)\n\n\ntrain_df['Title'] = train_df.apply(postrocessingTitle, axis = 1)\ntest_df['Title'] = test_df.apply(postrocessingTitle, axis = 1)","a3398b00":"def captureCabinPrefix(val):\n    \n    if str(val) != 'nan':\n        x = re.findall(\"[a-zA-Z]{1}\", val)\n        if len(x) == 0:\n            x = ['X']\n            \n        return x[0][0]\n        \n    return val\n\n\ntrain_df['CabinPrefix'] = train_df['Cabin'].apply(captureCabinPrefix)\ntest_df['CabinPrefix'] = test_df['Cabin'].apply(captureCabinPrefix)\nprint(train_df['CabinPrefix'].unique())","a04a34a6":"def captureCabinRoom(val):\n\n    if str(val) != 'nan':\n        x = re.findall(\"[0-9]+\", val)\n        if len(x) == 0:\n            x = [ 0 ]\n\n        return int(x[0])\n        \n    return 0\n\n\ntrain_df['CabinRoom'] = train_df['Cabin'].apply(captureCabinRoom)\ntest_df['CabinRoom'] = test_df['Cabin'].apply(captureCabinRoom)\nprint(train_df['CabinRoom'].unique())","a3af881a":"for cabin in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T']:\n    alives = len(train_df[(train_df['CabinPrefix'] == cabin) & (train_df['Survived'] == 1)])\n    deads = len(train_df[(train_df['CabinPrefix'] == cabin) & (train_df['Survived'] == 0)])\n    ratio = 0 if alives + deads == 0 else alives \/ (alives + deads)\n    print(\"Cabin [%s] Alive: %3d, Dead: %3d ===> %0.4f\" % (cabin, alives, deads, ratio))","b7e2bb29":"train_df.loc[train_df['CabinPrefix'] == 'T', 'CabinPrefix'] = 'A'\ntrain_df['CabinPrefix'] = train_df['CabinPrefix'].map({ 'A': 0, 'B': 800, 'C': 400, \n                                           'D': 1200, 'E': 1000, 'F': 600, 'G': 200 })\ntest_df['CabinPrefix'] = test_df['CabinPrefix'].map({ 'A': 0, 'B': 800, 'C': 400, \n                                           'D': 1200, 'E': 1000, 'F': 600, 'G': 200 })\nprint(train_df['CabinPrefix'].unique())","2c2ac302":"def calCabin(rec):\n    prefix = rec['CabinPrefix']\n    room = rec['CabinRoom']\n       \n    cabin = 0\n    if str(prefix) != 'nan' and str(room) != 'nan':\n        return int(cabin) + int(prefix)\n        \n    return np.nan\n\ntrain_df['Cabin'] = train_df.apply(calCabin, axis = 1)\ntest_df['Cabin'] = test_df.apply(calCabin, axis = 1)\n\ntrain_df.drop(columns = ['CabinPrefix', 'CabinRoom'], inplace = True)\ntest_df.drop(columns = ['CabinPrefix', 'CabinRoom'], inplace = True)\n","9f558776":"fillValues('Cabin', train_df, test_df)","8b1b08af":"train_df['FamilySize'] = train_df['Parch'] + train_df['SibSp'] + 1\ntest_df['FamilySize'] = test_df['Parch'] + test_df['SibSp'] + 1\n\ntrain_df.drop(columns = ['SibSp', 'Parch'], inplace = True)\ntest_df.drop(columns = ['SibSp', 'Parch'], inplace = True)","48307d9c":"def captureTicketId(val):\n    m = re.findall('[0-9]+', val)\n    if len(m) == 0:\n        return 1\n    \n    big = 0\n    for num in m:\n        if int(num) > big:\n            big = int(num)\n    \n    return big\n\ntrain_df['Ticket'] = train_df['Ticket'].apply(captureTicketId)\ntest_df['Ticket'] = test_df['Ticket'].apply(captureTicketId)\n\ntrain_df['Ticket'] = np.log(train_df['Ticket'])\ntest_df['Ticket'] = np.log(test_df['Ticket'])\n\ntrain_df['Ticket'] = train_df['Ticket'].round(4)\ntest_df['Ticket'] = test_df['Ticket'].round(4)","8e912c63":"titleBinned = {}\ndef calTitle(df):\n    \n    for title in [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 ]:\n        alives = len(df[(df['Title'] == title) & (df['Survived'] == 1)])\n        deads = len(df[(df['Title'] == title) & (df['Survived'] == 0)])\n        ratio = 0 if alives + deads == 0 else alives \/ (alives + deads)\n#        print(\"[%2d]: Alives {%3d} Dead: {%3d} ==> [%0.4f]\" % \n#              (title, alives, deads, ratio))\n        titleBinned[str(title)] = ratio\n    \ndef binTitle(*args):\n    \n    for df in args:\n        for title in [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 ]:\n            df.loc[df['Title'] == title, 'TitleP'] = titleBinned[str(title)]\n            \n        df['Title'] = df['TitleP'].round(4)\n        df.drop(columns = ['TitleP'], inplace = True)  \n        \ncalTitle(train_df)\nbinTitle(train_df, test_df)","9acd30ed":"ageBinned = {}\ndef calAge(df):\n       \n    last = 0\n    for age in [ 5, 10, 15, 20, 25, 30, 35, 40, 50, 55, 60, 65, 70, 75, 100 ]:\n        alivesBoy = len(df[(df['Age'] >= last) & \n                             (df['Age'] < age) & \n                             (df['Sex'] == 'male') &\n                             (df['Survived'] == 1)])\n        deadsBoy = len(df[(df['Age'] >= last) & \n                            (df['Age'] < age) &\n                            (df['Sex'] == 'male') & \n                            (df['Survived'] == 0)])\n        alivesGirl = len(df[(df['Age'] >= last) & \n                             (df['Age'] < age) & \n                             (df['Sex'] == 'female') &\n                             (df['Survived'] == 1)])\n        deadsGirl = len(df[(df['Age'] >= last) & \n                            (df['Age'] < age) &\n                            (df['Sex'] == 'female') & \n                            (df['Survived'] == 0)])\n    \n        ratioBoy = 0 if alivesBoy + deadsBoy == 0 else alivesBoy \/ (alivesBoy + deadsBoy)\n        ratioGirl = 0 if alivesGirl + deadsGirl == 0 else alivesGirl \/ (alivesGirl + deadsGirl)\n        print(\"[%3d, %3d]: Male {%2d, %2d} ==> [%0.4f], Female {%2d, %2d} ==> [%0.4f]\" % \n              (last, age, alivesBoy, deadsBoy, ratioBoy, alivesGirl, deadsGirl, ratioGirl))\n        \n        ageBinned[str(last) + \":\" + str(age) + \":male\"] = round(ratioBoy, 4)\n        ageBinned[str(last) + \":\" + str(age) + \":female\"] = round(ratioGirl, 4)\n        last = age\n   \n        \ndef binAge(*args):\n    \n    for df in args:\n        last = 0\n        for age in [ 5, 10, 15, 20, 25, 30, 35, 40, 50, 55, 60, 65, 70, 75, 100 ]:\n            male = str(last) + \":\" + str(age) + \":male\"\n            female = str(last) + \":\" + str(age) + \":female\"\n            df.loc[(df['Age'] >= last) & (df['Age'] < age) & (df['Sex'] == 'male'), 'AgeP'] = ageBinned[male]\n            df.loc[(df['Age'] >= last) & (df['Age'] < age) & (df['Sex'] == 'female'), 'AgeP'] = ageBinned[female]\n            last = age\n    \n        df['Age'] = df['AgeP'].round(4)\n        df.drop(columns = ['AgeP'], inplace = True)   \n\ncalAge(train_df)\nbinAge(train_df, test_df)","a0bf0663":"fareBinned = {}\ndef calFare(df):\n          \n    last = 0\n    for fare in [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 200, 500, 1000 ]:\n        alivesBoy = len(df[(df['Fare'] >= last) & \n                             (df['Fare'] < fare) & \n                             (df['Sex'] == 'male') &\n                             (df['Survived'] == 1)])\n        deadsBoy = len(df[(df['Fare'] >= last) & \n                            (df['Fare'] < fare) &\n                            (df['Sex'] == 'male') & \n                            (df['Survived'] == 0)])\n        alivesGirl = len(df[(df['Fare'] >= last) & \n                             (df['Fare'] < fare) & \n                             (df['Sex'] == 'female') &\n                             (df['Survived'] == 1)])\n        deadsGirl = len(df[(df['Fare'] >= last) & \n                            (df['Fare'] < fare) &\n                            (df['Sex'] == 'female') & \n                            (df['Survived'] == 0)])\n    \n        ratioBoy = 0 if alivesBoy + deadsBoy == 0 else alivesBoy \/ (alivesBoy + deadsBoy)\n        ratioGirl = 0 if alivesGirl + deadsGirl == 0 else alivesGirl \/ (alivesGirl + deadsGirl)\n        print(\"[%4d, %4d]: Male {%3d, %3d} ==> [%0.4f], Female {%3d, %3d} ==> [%0.4f]\" % \n              (last, fare, alivesBoy, deadsBoy, ratioBoy, alivesGirl, deadsGirl, ratioGirl))\n        fareBinned[str(last) + \":\" + str(fare) + \":male\"] = round(ratioBoy, 4)\n        fareBinned[str(last) + \":\" + str(fare) + \":female\"] = round(ratioGirl, 4)        \n        last = fare\n        \ndef binFare(*args):\n    \n    for df in args:\n        last = 0\n        for fare in [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 200, 500, 1000]:\n            male = str(last) + \":\" + str(fare) + \":male\"\n            female = str(last) + \":\" + str(fare) + \":female\"\n            df.loc[(df['Fare'] >= last) & (df['Fare'] < fare) & (df['Sex'] == 'male'), 'FareP'] = fareBinned[male]\n            df.loc[(df['Fare'] >= last) & (df['Fare'] < fare) & (df['Sex'] == 'female'), 'FareP'] = fareBinned[female]\n            last = fare\n    \n        df['Fare'] = df['FareP'].round(4)\n        df.drop(columns = ['FareP'], inplace = True) \n\ncalFare(train_df)\nbinFare(train_df, test_df)","7a00bd16":"def prepare(*args):\n    \n    label_column = [ 'Survived']\n    categorical_columns = [ 'Sex', 'Title', 'Pclass', 'Embarked' ]\n    numeric_columns = [ 'Age', 'Fare', 'Cabin', 'FamilySize' ]\n    \n    alldf = []\n    for df in args:\n        alldf.append(df)\n        \n    all_df = pd.concat(alldf)    \n    all_df = pd.get_dummies(all_df, columns = categorical_columns)\n\n    train_uni = set(all_df.columns).symmetric_difference(numeric_columns + ['Name', 'PassengerId'] + label_column)\n\n    cat_columns = list(train_uni)\n    \n    all_columns = numeric_columns + cat_columns \n      \n    scaler = MinMaxScaler()\n    all_df[numeric_columns] = scaler.fit_transform(all_df[numeric_columns])\n    \n    return all_df, all_columns\n    \ndef computeRegression(model, name, df1, df2):\n    \n    all_df, all_columns = prepare(df1, df2)\n     \n    if 'Survived' in all_columns:\n        all_columns.remove('Survived')\n\n    all_df_in = all_df.loc[all_df['Survived'].isna() == False, ['PassengerId'] + all_columns]\n    all_df_lb = all_df.loc[all_df['Survived'].isna() == False, 'Survived']\n \n    model.fit(all_df_in[all_columns], all_df_lb)\n        \n    for df in [ df1, df2 ]:\n        work_df = df.copy()\n        work_df[all_columns] = all_df.loc[all_df['PassengerId'].isin(df['PassengerId']), all_columns]\n        df[name] = model.predict(work_df[all_columns])\n        df[name] = df[name].round(4)\n        \n\ncomputeRegression(XGBRegressor(), 'XGBoost', train_df, test_df)\ncomputeRegression(HuberRegressor(), 'Huber', train_df, test_df)\ncomputeRegression(RandomForestRegressor(), 'Forest', train_df, test_df)\ncomputeRegression(TheilSenRegressor(), 'Theil', train_df, test_df)\ncomputeRegression(MLPRegressor(), \"MLP\", train_df, test_df)\n        ","4d2eeaa3":"print(train_df.columns)\nprint(test_df.columns)","bba13c55":"train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\n\ntrain_df['Embarked'] = train_df['Embarked'].map({'S': 0, 'Q': 1, 'C': 2})\ntest_df['Embarked'] = test_df['Embarked'].map({'S': 0, 'Q': 1, 'C': 2})","9f152d0d":"label_column = ['Survived']\nall_features_columns = [ 'Age', 'Fare', 'Title', 'FamilySize', 'Ticket', 'Sex',\n                   'Embarked', 'Cabin', 'Pclass', 'XGBoost', 'Huber', 'Forest', 'Theil', 'MLP' ]\n\n\npca = PCA(n_components = 5)\npca_train_df = pd.DataFrame(pca.fit_transform(train_df[all_features_columns]))\npca_test_df = pd.DataFrame(pca.transform(test_df[all_features_columns]))\n\nmodel = svm.SVC(kernel='rbf', gamma ='auto', C=1.0)\nmodel.fit(pca_train_df, train_df[label_column])\ntrain_df['Prediction'] = model.predict(pca_train_df)","c1b85c6d":"kfold = RepeatedStratifiedKFold(n_splits = 9, n_repeats = 5, random_state = 0)\nresults = cross_val_score(svm.SVC(kernel='rbf', gamma ='auto', C=1.0), \n                          pca_train_df, train_df[label_column], cv = kfold)\nprint(\"Cross Validation Accuracy: %0.4f\" % results.mean())","5a7f137d":"print(confusion_matrix(train_df['Survived'], train_df['Prediction']))","dedfd189":"print(classification_report(train_df['Survived'], train_df['Prediction']))","6d5a8ad2":"test_df['Survived'] = model.predict(pca_test_df)\ntest_df[['PassengerId','Survived']].to_csv('results.csv', index = False)","f501ce2f":"#### observations\n\n1. The passengers whose were embarked at (C)herbourg has the best chance of survival.\n2. The passengers whose were embarked at (S)outhampton has the poorest chance of survival.  This group consists of majority of the passengers.\n3. The passengers whose were embarked at (Q)ueenstown has poor chance of survival, but not as bad as those from Southampton","4715f85c":"#### observations\n\n1. Both Non-survived and survived have similiar distrubtions. Surprisingly this is not a good factor to determine the survivability.\n2. Passengers whose paid expensive fares do not guarantee survival. But those whose paid above 100-300 dollars has a comparatively better chance to survive.\n3. Those whose paid above 500 dollars fares are managed to stay alive.\n\n","a4b3132f":"<a id=\"section-features-ticket-number\"><\/a>\n### 9. Engineering Ticket Number\nClose inspection of the Ticket reveals that some passenegers share the same ticket number. For example, A group of known survivors whose share the same ticket number with another passenger from the test data, the survivability of this test passenger would possibily be affected by this correlation. \n\n1. The numerical value of the ticket vary greatly and ticket numbers are going to be log() so every ticket range has a fair representation.\n\n2. The ticket prefix does not provide any meaning information so it is going to be ignored.","48e1cbf6":"<a id=\"section-features-fill-cabin\"><\/a>\n### 7. Filling the missing Cabins\nThere are 687 missing cabins in the training data and 327 missing cabins in the testing data. Let's utilize the general filler for filling the missing fares.","c3d6e301":"<a id=\"section-lets-begin\"><\/a>\n### Let's begin!","9391f7d8":"* [Data Analysis](#section-data-analysis)\n    - [1. Age](#section-data-age)\n    - [2. Sex](#section-data-sex)\n    - [3. SibSp and Parch](#section-data-sibsp)\n    - [4. Fare](#section-data-fare)\n    - [5. Pclass](#section-data-pclass)\n    - [6. Embarked](#section-data-emabarked)\n    - [7. Features Correlation](#section-data-correlation)\n* [Features Engineering](#section-features-engineering)\n    - [1. Filling the missing Embarked](#section-features-fill-embarked)\n    - [2. Filling the missing Fare](#section-features-fill-fare)\n    - [3. Extracting the Title](#section-features-extract-title)\n    - [4. General Regressor Filler](#section-features-filler)\n    - [5. Filling the missing Age](#section-features-fill-age)\n    - [6. Encoding the Cabin](#section-features-encode-cabin)\n    - [7. Filling the missing Cabin](#section-features-fill-cabin)\n    - [8. Engineering Family Size](#section-features-family-size)\n    - [9. Engineering Ticket Number](#section-features-ticket-number)\n    - [10. Reengineering Title](#section-features-reeng-title)\n    - [11. Reengineering Age](#section-features-reeng-age)\n    - [12. Reengineering Fare](#section-features-reeng-fare)\n    - [13. Engineering Insensitive Ensemble](#section-features-insensitive-ensemble)\n    - [14. Encoding Sex and Embarked](#section-features-encode-sex-embarked)\n* [PCA(5)\/SVM(rbf) Classification](#section-classication)","1c133b18":"#### 6.2 Extracting the Cabin Number","b77ebbcf":"<a id=\"section-data-correlation\"><\/a>\n### 7. Features Correlation","393bd7d8":"<a id=\"section-data-analysis\"><\/a>\n## Data Analysis\n\n","750d39ad":"#### 6.3 Combining the cabin prefix and the cabin number\nLet's examine the survival rates of each cabin prefix","8f005ed6":"#### Sex & Age Diagram\nIn term of social context, both sex and age are base attributes related to every indivdiual. Let's plot a diagram with these two base features and see if it reveals any interesting information. ","75a46da8":"<a id=\"section-features-filler\"><\/a>\n### 4. General ExtraTreesRegressor Filler\nWe need a general reusable function for filling both missing Ages and missing Cabins. ","ff3d3bf0":"<a id=\"section-features-fill-embarked\"><\/a>\n### 1. Filling the two missing Embarked ","f7e873f3":"#### observations\n\n1. Pclass and Fare has the highest correlation (-0.55). First class passengers usually pay higher fare. Although some of the other class pasessengers may paid higher fares as well. We can use Pclass as one of the feature to fill in the missing Fare.\n2. SibSp and Parch have high correlation (0.41). Naturally passengers whose have children themselves have spouses.\n \n","f6fba8aa":"#### SibSp","f75332c0":"<a id=\"section-features-family-size\"><\/a>\n### 8. Engineering Family Size\nAs the previous data analysis has shown, both Sibsp and Parch are highly correlated. The combination of these two provides a single piece of quantitative information for later classification.\n\nFamilySize = Parch + SibSp + self","1915d14e":"<a id=\"section-features-reeng-title\"><\/a>\n### 10. Reengineering Title\nOneHotEncoder is easy to use but it encodes numerical values with arbitrary catergorical values. Perhape we can squeeze more information into the categortical values for the purpose of classification. \n\nEncoding the title consists of two steps.\n1. Calculating the survival probabilities of each title\n2. Binning the titles and assiging the corresponding probability to each title\n\n","65dd27c2":"**D(75.76%), E(75%), B(74.47%), F(61.54%), C(59.32%), G(50%), A(46.67%), T(0.00%)**\n<br\/><br\/>\nNow we can encode the cabin prefix. The most riskest prefix will be assigned with the **lowerest** number and the strongest prefix will be assign with the **highest** number. The largest room number is 148 (less than 200). So each encoded value of the prefixes will be multiples of 200.<br\/>\nCabin T(0.00%) has only a single dead passenger. Let's assign this passenger to the second worst group: Cabin A(46.67%)\n\nCabin Prefix<br\/>\nA - 0 x 200<br\/>\nB - 4 x 200<br\/>\nC - 2 x 200<br\/>\nD - 6 x 200<br\/>\nE - 5 x 200<br\/>\nF - 3 x 200<br\/>\nG - 1 x 200<br\/>\n\n\nCabin = CabinPrefix + CabinRoom ","1a6af9d1":"#### observations\n\n1. The first class passengers have the highest chance of survival. But even among the first class passengers, deads show up almost at every price range except above 500 dollars. Those first class whose paid between 60-300 dollars also has a comparative good chance to survive.\n2. The third class passengers have the lowerest chance of survival. Again no surprise.\n3. The second class passenger have roughly 50%\/50% chance of survival.\n4. Those whose paid less than 10 dollars have the lowest chance of survival regardless of class.","69bc415b":"#### observations\n\n1. Passengers below the age of 10 has a similar survival rate between male and female.\n2. Male passengers between the age of 17 and the age of 50 has a much higher mortality rate than female passengers of the same age group.\n3. Male passengers above the age of 50 generally has lower survival rate than female passengers of the same group, but this group is not as worse as the 17-50 male age group.\n4. The 80 years male individual who managed to stay alive. ","7647377e":"<a id=\"section-classication\"><\/a>\n## PCA(5)\/SVM(rbf) Classification\nEvery feature in the dataset contributes 'some' useful information and removing any least variant or least relevant features is out of the question. All available features will be projected into a five dimensions features spaces by PCA in the hope that data noises will be reduced in this process. SVM with rbf kernel will be employed for the classification. A few tests were conducted with SVM and KNN classifers, and it shows SVM performs better than KNN classifer in this dataset configuration.","b4ec3afd":"<a id=\"section-features-fill-fare\"><\/a>\n### 2. Filling the single missing Fare \nFare and Pclass have the highest data correlation (-0.51). Therefore Pclass will be the primary determining factor for calculating the missing fare","58a78b80":"<a id=\"section-data-embarked\"><\/a>\n### 6. Embarked","ac4b9414":"All features appears normal except the Fare column. The maximum possible fare is above 500 dollars but 75% of the passengers paid around 31 dollars and the standard deviation is 49 dollars. Those whose paid above 500 dollars are outliers.","f368cccd":"<a id=\"section-data-sibsp\"><\/a>\n### 3. SibSp and Parch ","2fe08038":"<a id=\"section-data-sex\"><\/a>\n### 2. Sex ","1f9c6af0":"<a id=\"section-data-fare\"><\/a>\n### 4. Fare","927c9300":"<a id=\"section-features-encode-sex-embarked\"><\/a>\n### 14. Encoding Sex and Embarked\nAs a general rule, the riskest category always assigned with the lowest number and the strongest category always assigned with the highest number.\n","0def382a":"<a id=\"section-features-extract-title\"><\/a>\n### 3. Extracting the Title\nAt this stage arbitrary categorical values are assigned to each title. We need the title for filling the missing ages and cabins more accurately. The title are going to be carefully categorized in the later stage.","5ee582cb":"#### Parch","dc198951":"There are definitely missing ages and cabin in both the training and testing dataset. I am going to propose my own method for extrapolating the missing values. This approach will be discussed in the later section. As for the single missing Fare and the two missing Embarked, I am going to fix them manually.","6a269f02":"<a id=\"section-features-fill-age\"><\/a>\n### 5. Filling the missing ages\nThere are 177 missing ages in the training data and 86 missing ages in the test data. \nLet's utilize the general filler for filling the missing ages. After we fill the missing ages, we need to readjust the title for all passengers. For example, after filling the missing ages, those whose are younger than 16 years old or older 55 years old would need to readjust their title to reflect their survivabilities.\n","4ef2c671":"<a id=\"section-features-reeng-fare\"><\/a>\n### 12. Reengineering Fare\n\nBinning and Encoding the fare consists of two steps.\n1. Calculating the male & female survival probabilities of each fare\n2. Binning the fares and assiging the corresponding probability to each fare class. Fare classes with the same probability would auto-consoliated into a single class.","76c9c6e6":"Female passengers has a much higher survival rate than male passeners.","5321e1b9":"<a id=\"section-features-engineering\"><\/a>\n## Features Engineering","63faa6e3":"<a id=\"section-features-reeng-age\"><\/a>\n### 11. Reengineering Age\n\nBinning and Encoding the age consists of two steps.\n1. Calculating the male & female survival probabilities of each age \n2. Binning the ages and assiging the corresponding probability to each age class. Age classes with the same probability would auto-consoliated into a single class.","5e9402d4":"<a id=\"section-features-insensitive-ensemble\"><\/a>\n### 13. Engineering Insensitive Ensemble\nIt appears that some of the conventional ensemble classifiers overfit extremely easily and do not work well with this particlar dataset. So why not use a non-ensemble classifier to learn the regression results of several ensembled\/non-ensemble regressors. Each regressor learns the training dataset and computes the regression results as a new column of the training and testing data. It is important that each regressor should learn from the same set of features but do not learn also from the results of other regressors.\n\nThe following five new regression columns are going to be introduced to both training and testing datasets. \n1. XGBoost Regression\n2. Huber Regression\n3. Random Forest Regression\n4. TheilSen Regression\n5. MLP Regression\n","30de4d39":"#### 6.1 Extracting the Cabin Prefix","c59c6bb3":"#### observations\n\n1. Children below age 6 has a higher rate of survival.\n2. Young adults betweeen the age of 17 and the age of 30 has the lowerest rate of survival.\n3. Adults between the age of 30 and 60 has a lower rate of survival, but not as worse as the 17-30 age group.\n4. Old adults between the age of 60 and the age of 75 has the highest mortality rates.\n5. There is one 80 years old individual who managed to stay alive. \n\n","dc7dd2d7":"#### observations\n1. Pclass and Fare has the highest correration (-0.51)\n2. The median Fare of all third class passengers is 7.89 dollars.This will be the missing Fare value.","635f6de3":"#### observations\n1. There are equal number of first class passengers whose pay around 80.00 dollars embarked at (C)herbourg or (S)outhampton.\n2. Miss Icard and Mrs Stone both are first class passengers and the passengers embarked at Southampton consists of majority of the passengers. Therefore we shall assume they both were embarked at Southampton.","b14b95d3":"# Titanic Passengers Data Analysis and Classification\n\n![](https:\/\/wallpaperstock.net\/gorgeous-titanic-wallpapers_50246_852x480.jpg)\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n","06984875":"<a id=\"section-features-encode-cabin\"><\/a>\n### 6. Encoding the Cabin\nThe Cabin consists of two pieces of information. The cabin prefix (i.e. A,B,C,D,E,F,G) and the room number. The goal is to preserve these two piece of information and to be able to compare the cabin in a meaning way. For example A15 and A16 are too adjustent rooms and therefore both cabins should share a similar chance of survival. As for the cabin prefix, this piece of information represent the section or region of the ship that of course impact the chance of survival as well.","e4c1835a":"<a id=\"section-data-age\"><\/a>\n### 1. Age","be096c5c":"<a id=\"section-data-pclass\"><\/a>\n### 5. Pclass","ceda72a1":"#### observations\n\n1. Passengers whose have a single spouse (i.e. wife or husband) have the best chance to surivive.\n2. Passengers whose have two parents (father and mother) accompany them have the strongest chance to survive.\n3. Passengers whose have two children also has a good chance to survive.\n4. Passengers whose travel alone has 34% chance to survived. This group consists of the biggest population.\n5. Passengers whose has high values of Parch or SibSp has minimal chance to survive."}}