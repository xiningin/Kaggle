{"cell_type":{"33ba13b7":"code","1e9c1450":"code","414a3087":"code","21264aad":"code","a74f338c":"code","62a6af2d":"code","0a5b3b1f":"code","0c71bf7b":"code","f66c2b8b":"code","3c7275be":"code","b977cf4e":"code","6a37e529":"code","72306200":"markdown"},"source":{"33ba13b7":"  import numpy as np\n  import pandas as pd\n  from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n  from sklearn.model_selection import train_test_split # Import train_test_split function\n  from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation","1e9c1450":"ti = pd.read_csv(\"..\/input\/test-file\/tested.csv\")\nti.describe()","414a3087":"#ti.replace(np.nan,0)\nti.fillna(ti.mean(),inplace=True)","21264aad":"ti.isnull().sum()","a74f338c":"#split dataset in features and target variable\nfeature_cols = ['PassengerId', 'Pclass', 'Age','SibSp',\t'Parch',\t'Fare'] \nX = ti[feature_cols].values # Features\ny = ti.Survived # Target variable","62a6af2d":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)","0a5b3b1f":"#Phase3\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","0c71bf7b":"#Using Logistic Regression Algorithm to the Training Set\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state = 0)\nlr.fit(X_train, y_train)\n\n#Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nkNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nkNN.fit(X_train, y_train)\n\n#Using SVC method of svm class to use Support Vector Machine Algorithm\nfrom sklearn.svm import SVC\nlsvc = SVC(kernel = 'linear', random_state = 0)\nlsvc.fit(X_train, y_train)\n\n#Using SVC method of svm class to use Kernel SVM Algorithm\nfrom sklearn.svm import SVC\nrbf = SVC(kernel = 'rbf', random_state = 0)\nrbf.fit(X_train, y_train)\n\n#Using GaussianNB method of na\u00efve_bayes class to use Na\u00efve Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\n#Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndtc.fit(X_train, y_train)\n\n#Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrfc.fit(X_train, y_train)","f66c2b8b":"y_pred = rfc.predict(X_test)","3c7275be":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","b977cf4e":"#test the three models with the test data and print their accuracy scores\n\n#print('classifier: {}'.format(sc.score(X_test, y_test)))\nprint('lr: {}'.format(lr.score(X_test, y_test)))\nprint('kNN: {}'.format(kNN.score(X_test, y_test)))\nprint('lsvc: {}'.format(lsvc.score(X_test, y_test)))\nprint('rdf: {}'.format(rbf.score(X_test, y_test)))\nprint('gnbr: {}'.format(gnb.score(X_test, y_test)))\nprint('dtc: {}'.format(dtc.score(X_test, y_test)))\nprint('rfc: {}'.format(rfc.score(X_test, y_test)))","6a37e529":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","72306200":"# 3. Use titanic dataset. Use the attributes: Class of the room, Sex, Age, number of siblings\/spouses, number of parents\/children, passenger fare and port of embarkation information. \n- Compare the result of Na\u00efve bayes Classifier with the Decision tree classifier and Logistic Regessor implemented in Lab Session 4."}}