{"cell_type":{"7d055c0d":"code","b8b0993a":"code","518c2157":"code","955626d3":"code","3a334b52":"code","e62ff9aa":"code","fe8e9600":"code","abc8ea6a":"code","842e21ec":"code","49a5ba3c":"code","a0ba8ebb":"code","5e2471b3":"code","aa11f7b5":"code","deede573":"code","fabbccb3":"code","148810ed":"code","12c0ec3e":"code","736729d5":"code","4d9ace6a":"code","3b9a3fe8":"code","b007cdc2":"code","01262973":"code","0442c6cc":"code","7e60b28b":"code","692bd8b2":"code","01211183":"markdown","ba97f0c2":"markdown","e45c4667":"markdown"},"source":{"7d055c0d":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nimport torch\nfrom torchvision import models,transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom pickle import dump\nfrom torch.utils.data import Dataset,DataLoader\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"stopwords\")\ndevice = torch.device('cpu')\ndata = pd.read_csv(\"..\/input\/jobposts\/data job posts.csv\")\ndata","b8b0993a":"data.RequiredQual","518c2157":"data.columns","955626d3":"data.JobDescription","3a334b52":"data.JobRequirment","e62ff9aa":"df = data[[\"RequiredQual\",\"JobDescription\",\"JobRequirment\",\"Title\"]].dropna()\ndf","fe8e9600":"classes = df['Title'].value_counts()[:20]\nkeys = classes.keys().to_list()\n\ndf = df[df['Title'].isin(keys)]\ndf['Title'].value_counts()","abc8ea6a":"def chane_titles(x):\n    x = x.strip()\n    if x == 'Senior Java Developer':\n        return 'Java Developer'\n    elif x == 'Senior Software Engineer':\n        return 'Software Engineer'\n    elif x == 'Senior QA Engineer':\n        return 'Software QA Engineer'\n    elif x == 'Senior Software Developer':\n        return 'Senior Web Developer'\n    elif x =='Senior PHP Developer':\n        return 'PHP Developer'\n    elif x == 'Senior .NET Developer':\n        return '.NET Developer'\n    elif x == 'Senior Web Developer':\n        return 'Web Developer'\n    elif x == 'Database Administrator':\n        return 'Database Admin\/Dev'\n    elif x == 'Database Developer':\n        return 'Database Admin\/Dev'\n\n    else:\n        return x\n        \n    \ndf['Title'] = df['Title'].apply(chane_titles)\ndf['Title'].value_counts()","842e21ec":"df[\"Combined\"] = df.RequiredQual + df.JobDescription + df.JobRequirment\ndf.Combined = df.Combined.apply(lambda x: x.replace(\"\\r\\n\",\" \"))\ndf","49a5ba3c":"df.iloc[0,4]","a0ba8ebb":"df.to_csv(\"Modified.csv\",index=False)","5e2471b3":"class Vocabulary(object):\n    \"\"\"Simple vocabulary wrapper.\"\"\"\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n\n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.word2idx)","aa11f7b5":"def build_vocab(df, threshold=3):\n    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n    counter = Counter()\n    \n    for i in range(len(df)):\n        caption = df.iloc[i,4]\n        tokens = nltk.tokenize.word_tokenize(str(caption))\n        counter.update(tokens)\n\n        if (i+1) % 1000 == 0:\n                print(\"[{}\/{}] Tokenized the sentences.\".format(i+1, len(df)))\n\n    # If the word frequency is less than 'threshold', then the word is discarded.\n    words = [word for word, cnt in counter.items() if cnt >= threshold]\n\n    # Create a vocab wrapper and add some special tokens.\n    vocab = Vocabulary()\n    vocab.add_word('<pad>')\n    vocab.add_word('<start>')\n    vocab.add_word('<end>')\n    vocab.add_word('<unk>')\n\n    # Add the words to the vocabulary.\n    for i, word in enumerate(words):\n        vocab.add_word(word)\n    return vocab","deede573":"v = build_vocab(df)\ndump(v, open('vocab.pkl', 'wb'))\nlen(v)","fabbccb3":"le = LabelEncoder()\ndf[\"TitleUse\"] = le.fit_transform(df.Title)\ndf","148810ed":"df.iloc[:,5].nunique()","12c0ec3e":"x = torch.Tensor(np.array(df.iloc[:,5]))\nx","736729d5":"class Data(Dataset):\n    def __init__(self,df,vocab):\n        self.df = df\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self,idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        tokens = nltk.tokenize.word_tokenize(str(df.iloc[idx,4]))\n        caption = []\n        caption.append(self.vocab('<start>'))\n        caption.extend([self.vocab(token) for token in tokens])\n        caption.append(self.vocab('<end>'))\n        return caption,x[idx]\n\ndef collate_fn(data):\n    data.sort(key=lambda x: len(x[0]), reverse=True)\n    captions,labels = zip(*data)\n    lengths = [len(cap) for cap in captions]\n    targets = torch.zeros(len(captions), max(lengths)).long()\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        targets[i, :end] = torch.Tensor(cap[:end])        \n    return targets.to(device), labels","4d9ace6a":"X_train,X_test,y_train,y_test = train_test_split(df.Combined,df.Title,test_size = 0.15,random_state = 0)\ntrain = Data(pd.DataFrame(X_train),v)\ntest = Data(pd.DataFrame(X_test),v)\ndataloaderTrain = DataLoader(train,4,num_workers=0,collate_fn=collate_fn)\ndataloaderTest = DataLoader(test,4,num_workers=0,collate_fn=collate_fn)","3b9a3fe8":"for i,j in dataloaderTrain:\n    print(i)\n    print(j)\n    print(i.shape,torch.stack(j).shape)\n    break","b007cdc2":"class Model(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n        super(Model, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True,bidirectional = True, dropout= 0.3)\n        self.relu = nn.ReLU()\n        self.linear = nn.Linear(hidden_size*2,256)\n        self.linear3 = nn.Linear(256,19)\n        \n    def forward(self,captions):\n        embeddings = self.embed(captions)\n        hiddens, _ = self.lstm(embeddings)\n        x = self.relu(self.linear(hiddens[:,-1,:]))\n        outputs = self.linear3(x)\n        return outputs","01262973":"from torch.nn.utils.rnn import pack_padded_sequence\ndef train(model,data_loader,data_loaderTest,learning_rate,num_epochs):  \n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    total_step = len(data_loader)\n    print(\"Total steps are: \", total_step)\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        for i,(captions, lengths) in enumerate(data_loader):\n            # Set mini-batch dataset\n            captions = captions.to(device)\n            # Forward, backward and optimize\n            outputs = model(captions)\n            loss = criterion(outputs.to(device),torch.stack(lengths).type(torch.LongTensor).to(device))\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            if i % 50 == 0:\n                print('Epoch [{}\/{}], Step [{}\/{}],Training Loss: {:.4f}'\n                    .format(epoch, num_epochs, i, total_step, loss.item()))\n        correct = 0\n        total = 0\n        model.eval()\n        with torch.no_grad():\n            for data in data_loader:\n                captions, labels = data\n                outputs = model(captions)\n                _, predicted = torch.max(outputs.data, 1)\n                total += torch.stack(labels).type(torch.LongTensor).size(0)\n                correct += (predicted == torch.stack(labels).type(torch.LongTensor).to(device)).sum().item()\n        print(\"Loss after epoch {} is {} and Accuracy is {}\".format(epoch,running_loss\/total_step,100 * correct \/ total))\n        torch.save(model.state_dict(),\"epoch\"+str(epoch)+\".pb\")\n        \n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for data in data_loaderTest:\n                captions, labels = data\n                outputs = model(captions)\n                _, predicted = torch.max(outputs.data, 1)\n                total += torch.stack(labels).type(torch.LongTensor).size(0)\n                correct += (predicted == torch.stack(labels).type(torch.LongTensor).to(device)).sum().item()\n        print(\"Loss after epoch {} is {} and Accuracy is {}\".format(epoch,running_loss,100 * correct \/ total))","0442c6cc":"model = Model(1024,512,len(v),3).to(device)","7e60b28b":"# train(model,dataloaderTrain,dataloaderTest,0.001,45)","692bd8b2":"# torch.save(model.state_dict(), \"epoch8.pb\")\n#torch.save(model.state_dict(), \"epoch8.pb\")","01211183":"# Uncomment below code for training","ba97f0c2":"<h1>Best overall accuracy at epoch 43 <\/h1>\n<h1>training = 99.73457199734572% , testing = 99.62406015037594% <\/h1>\n\n\n<h1>Best testing accuracy at epoch 29 <\/h1>\n<h1>training = 98.4737889847379% , testing = 100.0% <\/h1>","e45c4667":"# Credits to [Sidhant](https:\/\/www.kaggle.com\/thedocs) for the pre-processing !!"}}