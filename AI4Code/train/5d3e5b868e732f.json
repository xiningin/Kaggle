{"cell_type":{"fe93a0b4":"code","fa067f7d":"code","b696a6b3":"code","e1848af5":"code","e318a876":"code","20dd3486":"code","f6442288":"code","c11b65b1":"code","a271bc96":"code","171582d5":"code","38e15dca":"code","a5ae0f19":"code","5dfd9b58":"code","bcda32fa":"code","e07bb36d":"code","ab3cb5c5":"code","30f32dd2":"code","0a29fb6f":"code","46ae2633":"code","c6fd7edf":"code","a197f9ba":"code","0940a20a":"code","00413f90":"code","277f6fda":"code","3f8a7511":"code","5b90f835":"code","689afbd3":"code","a68797f0":"code","dd0f9abd":"code","a9963f13":"code","a30d28e1":"code","e404f957":"code","022bfa5b":"code","ffa197ff":"code","000abd07":"code","afa1b602":"code","86b20b04":"code","dff3796d":"code","8140050f":"code","b21412c4":"markdown","3189de10":"markdown","ae8e5084":"markdown","745ee609":"markdown","5a040e98":"markdown","b3f3392c":"markdown","468267b7":"markdown","99730ac2":"markdown","56651fd5":"markdown","fd9f1d77":"markdown","9c3ae3ae":"markdown","508e827c":"markdown","96bfddf1":"markdown","062f949b":"markdown","e231e052":"markdown"},"source":{"fe93a0b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)  \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fa067f7d":"import matplotlib.pyplot as plt\nimport seaborn as sns","b696a6b3":"responses=pd.read_csv(\"..\/input\/responses.csv\")\ncolumns=pd.read_csv(\"..\/input\/columns.csv\")","e1848af5":"responses.head()","e318a876":"#creating a hobbies dataframe\nhobbies=responses.iloc[:,31:63,]\n","20dd3486":"hobbies.head()\nhobbies.dtypes","f6442288":"print(\"hobbies:\",hobbies.shape)\n","c11b65b1":"#finding missing values in dataset\n#hobbies.isnull().any()\n#Checking if any null values in the columns\nhobbies.loc[hobbies.isnull().any(axis=1)]","a271bc96":"#dropping all the Nan values\nhobbies.dropna(inplace=True)","171582d5":"hobbies.loc[hobbies.isnull().any(axis=1)]","38e15dca":"hobbies.shape\n","a5ae0f19":"hobbies.describe()","5dfd9b58":"# Finding outliers in Internet and FunWith Friends\n\n#sns.boxplot(x=hobbies['Internet'])\nplt.boxplot([hobbies['Internet'],hobbies['Fun with friends']])","bcda32fa":"from scipy import stats\nimport numpy as np\noutliers=[]\n\nz_score=np.abs(stats.zscore(hobbies))\nz_score\n#print(np.where(z_score > 3))","e07bb36d":"zscore_df= pd.DataFrame(hobbies.iloc[[3,8,38,97, 105, 132, 169, 180, 220, 260, 275, 291, 327,\n       373, 408, 416, 517, 519, 557, 656, 672, 675, 687, 699, 760, 804,848],:])","ab3cb5c5":"# Finding out people  who are not Interested in Internet at all\nzscore_df[zscore_df['Internet']==1]","30f32dd2":"hobbies.drop(index=747, inplace=True)","0a29fb6f":"hobbies.shape","46ae2633":"#Finding people with no interest in Socialising\nzscore_df[zscore_df['Fun with friends']==2]","c6fd7edf":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nhobbies_scaled=ss.fit_transform(hobbies)\nhobbies_scaled= pd.DataFrame(hobbies_scaled)\nhobbies_scaled.head(20)","a197f9ba":"#  clustermap plot using Seaborn\n\n\nsns.clustermap(hobbies.corr(), center=0, cmap=\"seismic\",\n                             linewidths=.75, figsize=(13, 13))","0940a20a":"#df= hobbies.T\ndf=hobbies","00413f90":"# Using the elbow method to find  the optimal number of clusters\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nwcss = []\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,max_iter=300,n_init=10,random_state=0)\n    km=kmeans.fit(df)\n    wcss.append(kmeans.inertia_)\nplt.figure()\nplt.plot(range(1,11),wcss, marker=\"o\")\n\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within Cluster Sum of Squares')\n\nplt.show()","277f6fda":"# create a Series for dataframe index and values\nhobbies_series=pd.Series(df.index )","3f8a7511":"from sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\n\n\ndef silhouette_analysis(df, cluster_labels,n,clustering_type,heading):\n    \n    plt.figure(figsize=(15,10))\n    ax= plt.subplot()\n    ax.set_ylim([0, len(df) + (n + 1) * 50])\n    dictofhobbies={i:cluster_labels[i] for i in range(0,len(cluster_labels))}\n    \n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    if clustering_type == 'Kmeans':\n        metrics='euclidean'\n    if clustering_type == 'Agglomerative':\n        metrics='euclidean'\n    if clustering_type == 'GaussianMixture':\n        metrics='mahalanobis'\n    silhouette_avg = silhouette_score(df, cluster_labels,metric=metrics)\n    \n    print(\"For n_clusters =\", n,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(df, cluster_labels)\n    \n   \n    yticks=[]\n    ylabels=[]\n    y_lower = 10\n    for i in range(n):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n        silhouette_labels=[hobby for hobby,cluster_label in dictofhobbies.items() if cluster_label==i ]\n        zipped_values=dict(zip(silhouette_labels,ith_cluster_silhouette_values))      \n        #sorted(zipped_values,key=lambda x:x1])\n        new_zipped_values=sorted(zipped_values.items(), key=lambda x: x[1])\n        ith_cluster_silhouette_values.sort()\n        #print(\"ith_cluster_silhouette_values::\",len(ith_cluster_silhouette_values))\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        #print(\"yupper is::\",y_upper)\n        color = cm.nipy_spectral(float(i) \/ n)\n        pos=np.arange(y_lower, y_upper)\n       \n        ax.barh(pos,ith_cluster_silhouette_values,height=1.0,color=color, edgecolor=\"none\")\n               \n        yticks.extend(pos)\n        ylabels.extend(labels[0] for labels in new_zipped_values)\n        \n        # Label the silhouette plots with their cluster numbers at the middle\n        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n       \n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 50  # 10 for the 0 samples\n\n    ax.set_title(\"The silhouette plot for the various clusters.\")\n    ax.set_xlabel(\"The silhouette coefficient values\")\n    ax.set_ylabel(\"Clusters\")\n    \n    \n    # The vertical line for average silhouette score of all the values\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    \n    hobbies_labels=[]\n    for y in ylabels:\n        hobbies_labels.append(hobbies_series[y])\n    \n    #ax.set_yticks(yticks)\n    #ax.set_yticklabels(hobbies_labels, fontSize=8)  \n    ax.set_xticks([-0.1, 0,0.1, 0.2, 0.4, 0.6, 0.8, 1])\n\n    plt.suptitle((\"Silhouette analysis for \" ,heading, \"with n_clusters = %d\" % n),\n                 fontsize=14, fontweight='bold')\n\nplt.show()\n","5b90f835":"from sklearn.cluster import KMeans\n\nrange_of_clusters=[6,7,8,9]\nfor n in range_of_clusters:\n    kmeans_sil=KMeans(n_clusters=n,max_iter=300,n_init=10,random_state=10)\n    cluster_labels=kmeans_sil.fit_predict(df)\n\n    silhouette_analysis(df,cluster_labels,n,'Kmeans','Kmeans')","689afbd3":"#using agglomerative clustering\nfrom sklearn.cluster import AgglomerativeClustering\n\nrange_of_clusters=[6,7,8,9]\nfor n in range_of_clusters:\n    ag_c= AgglomerativeClustering(n_clusters=n)\n    ag_cluster_labels=ag_c.fit_predict(df)\n    silhouette_analysis(df,ag_cluster_labels,n,'Agglomerative','Agglomerative')","a68797f0":"#also trying the gaussian mixture models\n\nfrom sklearn.mixture import GaussianMixture\nrange_of_clusters=[6,7,8,9]\nfor n in range_of_clusters:\n    gmm= GaussianMixture(n_components=n)\n    gmm_cluster_labels=gmm.fit_predict(df)\n    silhouette_analysis(df,gmm_cluster_labels,n, 'GaussianMixture','GaussianMixture')","dd0f9abd":"#Running PCA on the scaled data\n\n\nfrom sklearn.decomposition import PCA\nplt.figure()\ncmr=[]\nfor n_com in range(2,20):\n    \n    cumulative_ratio=0\n   \n    pca= PCA(n_components=n_com)\n    hobbies_reduced=pca.fit_transform(hobbies_scaled)\n\n    print(\"for components = %d\" %n_com)\n    components= pd.DataFrame(np.round(pca.components_, 4),columns=df.keys())\n   \n    ratios=pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n    \n    for i in range((variance_ratios.shape[0])):\n        \n        cumulative_ratio= cumulative_ratio + variance_ratios.values[i]\n    \n    cmr.append(cumulative_ratio)\n    print(cumulative_ratio)\nvr= variance_ratios.values\nplt.plot(range(1,20),vr*100,marker=\"x\")   \nplt.xlabel(\"PCA Components\")\nplt.ylabel(\"Explained Variance ratio\")\nplt.figure()\nplt.plot(range(2,20),cmr,marker=\"o\")\nplt.xlabel(\"No. of Components\")\nplt.ylabel(\"cumulative Variance ratio\")\nplt.show()","a9963f13":"#running PCA with kmeans \nfor n_comp in [15]:\n    print(\"PCA for components=%d\"%n_comp)\n    pca_km=PCA(n_components=n_comp)\n    df_reduced=pca_km.fit_transform(hobbies_scaled)\n    \n    \n    for n_clust in [6]:\n        kmeans_red_pca=KMeans(n_clusters=n_clust)\n        reduced_cluster_labels=kmeans_red_pca.fit_predict(df_reduced)\n        centers_km=kmeans_red_pca.cluster_centers_\n        silhouette_analysis(df_reduced,reduced_cluster_labels,n_clust,'Kmeans','Kmeans After PCA for components %d' %n_comp)\n        \n    \n","a30d28e1":"#visualizing the 6 clusters in 2 dimensions with 2 principal componenets\n#reducing data to 2 prinicipal components\npca_2comp=PCA(n_components=2)\ndf_reduced_2comp=pca_2comp.fit_transform(hobbies_scaled)\n\n# running Kmeans for 6 clusters on 2 PC's\nkmeans_pca_2comp=KMeans(n_clusters=6)\nreduced_cluster_labels_2comp=kmeans_pca_2comp.fit_predict(df_reduced_2comp)\ncenters_2comp=kmeans_pca_2comp.cluster_centers_\n\n#creating dataframes for cluster labels and 2 PC's\npredictions=pd.DataFrame(reduced_cluster_labels_2comp,columns=['Cluster_pred'])\nreduced_df_2comp=pd.DataFrame(np.round(df_reduced_2comp,4),columns=['Dimension 1','Dimension 2'])\n\n# concatanete the above two dfs\nto_plot=pd.concat([predictions,reduced_df_2comp], axis=1)","e404f957":"to_plot.shape","022bfa5b":"\ndef visualize_clusters(to_plot,centers,n_clusters):\n    \n    plt.figure(figsize=(15,8))\n    ax=plt.subplot()\n\n    colors=['red','green','blue','orange','yellow','purple']\n    for n_clusters in range(n_clusters):\n        \n        #colors = cm.nipy_spectral((to_plot['Cluster_pred']== n_clusters).astype(float) \/ n_clusters)\n        #colors=['red','green','blue','orange','yellow','purple']\n        \n        ax.scatter(to_plot[to_plot['Cluster_pred']==n_clusters].iloc[:, 1],to_plot[to_plot['Cluster_pred']==n_clusters].iloc[:, 2],\n                   marker='.', s=80, lw=0, alpha=0.7,\n                    c=colors[n_clusters], edgecolor='black', label='Cluster %d'%n_clusters)\n\n            \n            # Draw white circles at cluster centers\n        \n\n        for i, c in enumerate(centers):\n           \n            ax.scatter(c[0], c[1], marker='o',\n                        c=\"white\", alpha=1, s=200, edgecolor='k')\n            ax.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                            s=80, edgecolor='k')\n\n    ax.set_title(\"The visualization of the clustered data.\")\n    ax.set_xlabel(\"Dimension 1\")\n    ax.set_ylabel(\"Dimension 2\")\n\n    plt.legend()    \n    plt.show()","ffa197ff":"# visualize clusters\nvisualize_clusters(to_plot,centers_2comp,6)","000abd07":"# Lets recover the data reduced by PCA (also Standard scalar for scaled data)\n\n#finding out the real centers of the data\n#main_centers=ss.inverse_transform(pca_2comp.inverse_transform(centers_2comp))\nreal_centers= ss.inverse_transform(pca_km.inverse_transform(centers_km))\n","afa1b602":"real_centers_df=pd.DataFrame(np.round(real_centers,3), columns = df.columns)","86b20b04":"#Deviation from the median(as mean is sensitive to outliers)\n\ndev_from_median=real_centers_df- hobbies.median()","dff3796d":"dev_from_median","8140050f":"#Plotting the distributions in clusters\nfor i in range(len(dev_from_median.index)):\n    plt.figure(figsize=(10,8))\n  \n    dev_from_median.iloc[i,:].plot(kind='bar')\n    plt.title(\"Distribution of Interests in Cluster %d\"%i)\n    plt.grid(axis='x')\n    #plt.axhline(y=3.5,color=\"red\", linestyle=\"--\")\n    \n    plt.show()\n   ","b21412c4":"15 components gives us almost 75% of variance so taking 15 components with 6 clusters.\n\nRunning Kmeans(as it gives the best silhouette score than agglomerative and GMM) on reduced data","3189de10":"Well , for the above can't say whether these are outliers or just some people don't like socialising. So keeping these in the data. \nLet's see how these affect the grouping.","ae8e5084":"Kmeans gives us the better silhouette avaerage score using 6 or 7 clusters. \nLet's do PCA on the scaled data to find the minimum no. of components needed to best describe the data","745ee609":"Some of the data fall  on the boundaries of other clusters.We see some outliers as well.","5a040e98":"6 or 7 clusters","b3f3392c":"Finding out the correlation between dfferent interests","468267b7":"From the above graph , K=6 shows a substantial decrease in wcss. So choosing 6 as no.of clusters.\n\nLets do silhoutte analysis to find the no. of clusters in Kmeans clustering","99730ac2":"From the above clustermap,\nMedicine, Biology, Chemistry have the highest correlations\nMathematics, Physics, Science and Technology\n\n\nwe can assume that data can be grouped into 6, 7, 8 or may be 9 clusters.\nLets see  how many clusters we get using elbow method.","56651fd5":"Fun with Friends and Internet have higher means. There might be some outliers in there\/ People love Internet and socialising more.\n\nSo will scale the data to get better clusters","fd9f1d77":"This is my firts attempt to unsupervised learning.\n\nI am trying to cluster young people on their Hobbies\/Interests.\n\nAny suggestions\/improvements are most welcome!!","9c3ae3ae":"Assuming young people who are interested in PC are also interested in Internet so assuming person with index'747' as an outlier and removing the same from dataset","508e827c":"We see from above that Clusters 1, 2, 4 have better grouping whereas clusters 0, 3, 5 are somewhat mixed up.\n\n\n\n**Cluster 1: Medicine, Biology, Chemistry, Gardening**\n\n**Cluster 2: People who have Interests in Reading, Art Exhibitions, Writing,Theatre,Musical instruments** \n\n**Cluster 4:Mathematics, Physcics, PC ,Cars, Science and technology, Internet**\n\nCluster 0: Politics, Economy management, History , Law,Mathematics, Religion,Psychology,  etc\n\n","96bfddf1":"Finding similar behaviours\/clusters ,given hobbies and interests","062f949b":"Silhouette analysis using Kmeans, Agglomerative, Gaussian Mixture model clustering","e231e052":"6 Clusters have above average silhouette score and the distribution is also uniform."}}