{"cell_type":{"78162e24":"code","9cf01005":"code","d702edc6":"code","f782b7b9":"code","c7958d52":"code","4c84376d":"code","2216fb03":"code","31f30a2b":"code","59524ac0":"code","77a6e999":"code","15ec8ae1":"code","1a215fb1":"code","f19e638d":"code","47bd77c7":"code","8abb24d5":"code","276d0c49":"code","e6f829a0":"code","1062068c":"code","3b2f915c":"code","bb42966b":"markdown","88cf5e53":"markdown","fc20df40":"markdown","bf0b0d5a":"markdown","e8bdd303":"markdown","7daec755":"markdown","2722c0ca":"markdown","d8c1ab66":"markdown","30d60cff":"markdown","037ae08c":"markdown","e31a6f2c":"markdown","880cae62":"markdown"},"source":{"78162e24":"import time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nimport seaborn as sns\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","9cf01005":"df = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","d702edc6":"df = df.drop('RISK_MM',axis=1)","f782b7b9":"print(df.isna().sum())","c7958d52":"print('Total',len(df.columns),'features\\n',df.dtypes)","4c84376d":"df['Month'] = df['Date'].str.slice(start=5,stop=7) # Get Month from Date\ndf['Date'] = pd.to_datetime(df['Date'],format='%Y\/%m\/%d',errors='ignore')\ndf_dateplot = df.iloc[-900:,:]\nplt.figure(figsize=[20,3])\nplt.plot(df_dateplot['Date'],df_dateplot['MinTemp'],color='blue')\nplt.plot(df_dateplot['Date'],df_dateplot['MaxTemp'],color='red')\nplt.fill_between(df_dateplot['Date'],df_dateplot['MinTemp'],df_dateplot['MaxTemp'], facecolor = '#EBF78F')\nplt.legend()\nplt.show()","2216fb03":"df['Season_Q1'] = (df['Month']=='01') | (df['Month']=='02') | (df['Month']=='03')\ndf['Season_Q2'] = (df['Month']=='04') | (df['Month']=='05') | (df['Month']=='06')\ndf['Season_Q3'] = (df['Month']=='07') | (df['Month']=='08') | (df['Month']=='09')\ndf['Year_FirstHalf'] = df['Season_Q1'] | df['Season_Q2']\ndf['NoRain'] = (df['Rainfall'] == 0)\ndf['Temp_MinMax'] = df['MaxTemp'] - df['MinTemp']\ndf['Temp_delta'] = df['Temp3pm'] - df['Temp9am']\ndf['Humidity_delta'] = df['Humidity3pm'] - df['Humidity9am']\ndf['WindSpeed_delta'] = df['WindSpeed3pm'] - df['WindSpeed9am']\ndf['Cloud_delta'] = df['Cloud3pm'] - df['Cloud9am']\ndf['Pressure_delta'] = df['Pressure3pm'] - df['Pressure9am']\ndf['NoSunshine'] = (df['Sunshine'] == 0)\ndf['HighSunshine'] = (df['Sunshine'] >= df['Sunshine'].median())\ndf['LowHumidity3pm'] = (df['Humidity3pm'] <= df['Humidity3pm'].median())\ndf['LowCloud3pm'] = (df['Cloud3pm'] <= df['Cloud3pm'].mean())\nprint(df.dtypes)","31f30a2b":"df_hist = df.select_dtypes(exclude = ['bool','object'])\ndf_hist.hist(figsize = [15,15],bins = 50)\nplt.show()","59524ac0":"for i in df_hist.columns:\n    df[[i]] = preprocessing.StandardScaler().fit_transform(df[[i]])","77a6e999":"df['Rainfall'] = df['Rainfall'].apply(lambda x: np.log(x) if x>0 else x)\ndf['Evaporation'] = df['Evaporation'].apply(lambda x: np.log(x) if x>0 else x)","15ec8ae1":"features_to_transform = ['Evaporation','Humidity9am','Sunshine','Rainfall']\nfor i in features_to_transform:\n    df[[i]] = preprocessing.QuantileTransformer(n_quantiles=100,output_distribution='normal',subsample=len(df)).fit_transform(df[[i]])","1a215fb1":"df_hist = df[features_to_transform]\ndf_hist.hist(figsize = [15,15],bins = 50)\nplt.show()","f19e638d":"df['Humidity9am_transformer'] = (df['Humidity9am']>4)\ndf['Sunshine_transformer'] = (df['Sunshine']>-4)","47bd77c7":"features_to_drop = ['Date']\ndf = df.drop(features_to_drop,axis=1)\nremained_categorial_data = ['Month','WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow','Location']\ndf_onehotted = pd.get_dummies(df,columns=remained_categorial_data,drop_first=True)","8abb24d5":"asc = df_onehotted.corrwith(df_onehotted['RainTomorrow_Yes']).sort_values(ascending=True)[:10]\ndesc = df_onehotted.corrwith(df_onehotted['RainTomorrow_Yes']).sort_values(ascending=False)[1:11]\nprint(desc)\nprint(asc)","276d0c49":"x_train, x_test, y_train, y_test = train_test_split(df_onehotted.drop(['RainTomorrow_Yes'],axis=1),df_onehotted['RainTomorrow_Yes'],test_size = 0.2, random_state = 0)","e6f829a0":"%%time\n\nimport xgboost as xgb\n\nxgb = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n              colsample_bynode=0.9, colsample_bytree=0.5, gamma=0,\n              grow_policy='lossguide', learning_rate=0.4, max_bin=512,\n              max_delta_step=0, max_depth=8, min_child_weight=0.8, missing=None,\n              n_estimators=100, n_jobs=1, nthread=None, num_parallel_tree=9,\n              objective='binary:hinge', random_state=0, reg_alpha=2,\n              reg_lambda=3, sampling_method='uniform', scale_pos_weight=1,\n              seed=None, silent=None, subsample=0.8, tree_method='hist',\n              verbosity=1)\n\nxgb.fit(x_train, y_train)","1062068c":"pred = xgb.predict(x_test)\nprint('acc',metrics.accuracy_score(y_test,pred))\nprint('f1',metrics.f1_score(y_test,pred))\nprint('matrix',metrics.confusion_matrix(y_test,pred))","3b2f915c":"gain = xgb.get_booster().get_score(importance_type='gain')\ngain = pd.DataFrame.from_dict(gain,orient='index',columns=['gain']).sort_values(by=['gain'],ascending=False)[:10]\nprint(gain.to_string())\ncover = xgb.get_booster().get_score(importance_type='cover')\ncover = pd.DataFrame.from_dict(cover,orient='index',columns=['cover']).sort_values(by=['cover'],ascending=False)[:10]\nprint(cover.to_string())","bb42966b":"Drop **'Date'** and one-hot encode the categorial data using ***pandas.get_dummies***.","88cf5e53":"Now we check feature skewness, range and frequency distribution using histogram plots. As we are using decision tree, we don't really need to care about standardization or the range of each of the features. Instead, we should pay attention to the outliers we may have within the dataset. ***Evaporation*** is one of the features that has significant numbers of outliers.","fc20df40":"Top 10 features with highest gain and coverage scores.","bf0b0d5a":"Let's see the top 20 most correlated features with ***RainTomorrow_Yes***.","e8bdd303":" Before dropping the ***Date***, we can try to extract the months as new features to the dataset. Of course, we can also try season and semi-annual period. But first, let's plot a line chart of Min\/Max temperature in the latest years (last 900 rows):","7daec755":"Drop **'RISK_MM'** as said in the dataset description.","2722c0ca":"Adding some new features, like seasons, semi-annual periods, temperature delta etc. Let your imagination flies here. The training process will take care of it if the features are unimportant. ","d8c1ab66":"After transformation:","30d60cff":"**That's it! Feel free to leave your comments!**","037ae08c":"There are two advantages using QuantileTransformer:\n\n* [0,1] scaling would not be affected by outliers\n* Collapse any outlier by setting them to the a priori defined range boundaries of 0 to 1 (check out ***Rainfall*** !)\n\nNoted that this transform is non-linear. It may distort linear correlations between variables measured at the same scale. [***Link to know more.***](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html) Let me just asssume everything in real life is so complicated that there won't be any linear correlations.","e31a6f2c":"### I was writing this during the devastating 2019-2020 Australia wildfire.  [*Link to know more!*](https:\/\/en.wikipedia.org\/wiki\/2019%E2%80%9320_Australian_bushfire_season)\n![fire](https:\/\/www.telegraph.co.uk\/content\/dam\/news\/2019\/11\/10\/TELEMMGLPICT000215612057_trans_NvBQzQNjv4BqO2wBkCq2Mm4bNdLs0EhrYwRbewe5KDHg_-9Be7aWaMw.jpeg?imwidth=1400)\n### Climate Change was unarguably one of the main reasons of the fire. Let's save our planet together!","880cae62":"We don't need to drop N\/A as XGBoost will handle this for us. Dropping these many rows could be wasteful."}}