{"cell_type":{"54724f48":"code","f8c5dc01":"code","9656cb90":"code","f515a084":"code","d44a832b":"code","090dec0e":"code","fcecdaca":"code","4fdcdb2d":"code","b32e63f6":"code","81a12f74":"code","eb26f936":"code","5d267518":"code","0fe16da9":"code","92d0c992":"code","61fe5a3a":"code","7881cb00":"code","9d811729":"code","4c885eaa":"code","6958c63d":"code","749b1242":"code","5e92f16a":"code","10bde231":"code","58f2e9ed":"code","a8609dd3":"code","eb08482f":"code","04ced82a":"code","a77e5a91":"code","3b12d544":"code","af883550":"code","244b57d3":"code","730ea80c":"code","530ffa06":"code","bbe16223":"code","7f77e494":"code","0e150e9e":"code","190e39ce":"code","009f8d83":"code","8fba7230":"code","1a37d675":"code","5c99ac62":"code","9b5e9e91":"code","88c71734":"code","1ede06fc":"code","476d09ef":"code","08904fb3":"code","63747783":"code","38d70604":"code","46b7802c":"code","f04666a8":"code","787d1d77":"code","49f0b1cf":"code","a2bbd65d":"code","9ecf64ef":"code","e581e77f":"code","c9058c49":"code","48d8002f":"code","3bf8bcfb":"code","ddc387ae":"code","c02686a2":"code","64ae61d5":"code","9c43e6e3":"code","47aca5d3":"code","d462a9ca":"code","5af1e9a1":"markdown","9d15f28e":"markdown","796c0bb8":"markdown","2ac96493":"markdown","c1e799fc":"markdown","76aa31f6":"markdown","22562c50":"markdown","6adb1206":"markdown","e9e7286b":"markdown","f280ad4c":"markdown","e244a8e9":"markdown","d91f98ce":"markdown","93dacd89":"markdown","004ae8f8":"markdown","7fc5feac":"markdown","8b4098d6":"markdown","630f83de":"markdown","86606627":"markdown","e74cd4b1":"markdown","e5c97f9e":"markdown","152a42a4":"markdown","fa981dbf":"markdown","abc7a0b9":"markdown","a7e413d4":"markdown","c6c3275f":"markdown"},"source":{"54724f48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8c5dc01":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold","9656cb90":"df_train = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/test.csv\")","f515a084":"# Check top 10 records of train data\ndf_train.head(10)","d44a832b":"# Check top 10 records of test data\ndf_test.head(10)","090dec0e":"# % of passenges who survived\ndf_train.Survived.value_counts()\/ df_train.shape[0]","fcecdaca":"df_test['Survived'] = 1  #temporarily kept as 1 for all test data passengers\n\n# To be able to filter the data later\ndf_train['Set'] = \"Train\" \ndf_test['Set'] = \"Test\"\n\n# Complete data\ndf_comp = pd.concat([df_train, df_test]).reset_index(drop = True)\ndf_comp.index.nunique()","4fdcdb2d":"#check missing values\nprint(\"Missing Values in the data \\n\",df_comp.isnull().sum()\/df_comp.shape[0])","b32e63f6":"# Distribution of Age\nprint(df_comp.Age.describe())\nsns.displot(df_comp.Age)\n\n# We can perfom median treatment for missing values of Age \ndf_comp.Age = df_comp.Age.fillna(df_comp.Age.median())","81a12f74":"# Create Age Bucket\ndf_comp[\"Age_bucket\"] = pd.cut(df_comp[\"Age\"], 9, \n                                  labels=[\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\"])","eb26f936":"# Distribution of Fare\nprint(df_comp.Fare.describe())\nsns.displot(df_comp.Fare)\n\n# Highly skewed distribution for Fare and seems multimodal histogram","5d267518":"# Clearly, the fare if dependent on Passenger Class\nsns.boxplot(x= df_comp.Pclass, y= np.log(1+df_comp.Fare))","0fe16da9":"# Let's get the median values of Fare by Pclass for missing replacement\nFareByClass = pd.crosstab(index = df_comp.Pclass, columns = 'MedianFare', \\\n                          values = np.log(1+df_comp.Fare), aggfunc= 'median').to_dict()['MedianFare']\nFareByClass","92d0c992":"df_comp.Pclass.map(FareByClass)","61fe5a3a":"# Get a column with log of fare\ndf_comp['LnFare'] = np.log(1+df_comp['Fare'])\n\n# Replace missing fare values with log transformed values by PClass\ndf_comp['LnFare'].fillna(df_comp.Pclass.map(FareByClass),inplace=True)\n\n# Validating if missing values are treated\ndf_comp.loc[:,['Fare','LnFare']][df_comp.Fare.isna()]","7881cb00":"# Bivariate\nsns.boxplot(x = df_comp.Survived[df_comp.Set == \"Train\"],y= df_comp.LnFare[df_comp.Set == \"Train\"] )","9d811729":"print(df_comp.Embarked.value_counts())\ndf_comp.Embarked.fillna('S', inplace=True)","4c885eaa":"# Bivariate\npd.crosstab(index= df_comp.Embarked[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)","6958c63d":"# Let's create a new column CabinAlloted (1\/0)\ndf_comp['CabinNotAlloted'] = df_comp.Cabin.isna().astype(int)\ndf_comp['CabinNotAlloted'].value_counts()\/df_comp.shape[0]","749b1242":"# Bivariate\n# Passengers with no cabin alloted had relatively lower chances of survival\npd.crosstab(index = df_train.Cabin.isna(), columns= df_train.Survived, normalize = 'index').plot.bar(stacked = True)","5e92f16a":"# Let's also fetch the first char of the cabin (which could signify Deck)\ndf_comp['Deck'] = df_comp.Cabin.str[0:1]\nprint(\"Unique decks \\n\", df_comp['Deck'].unique())\n\n# replace missing deck by 'X'\ndf_comp['Deck'].fillna('X',inplace= True)\n\nprint(\"Passengers in each deck \\n\", df_comp['Deck'].value_counts())","10bde231":"# Check this out\nset(df_comp.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'XX'))","58f2e9ed":"# Let us create a new Ticket column with the above feature extracted and also do the missing value at the same time\ndf_comp['TicketType'] = df_comp.Ticket.fillna('XX')\ndf_comp['TicketType'] = df_comp.TicketType.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'XX')\nprint(set(df_comp.TicketType))","a8609dd3":"# Let us do some text cleaning on TicketType\n# Convert to lower\ndf_comp['TicketType'] = df_comp['TicketType'].str.lower()\n\n# Get rid of dots and slash\nimport re\ndf_comp['TicketType'] = df_comp.TicketType.map(lambda x: re.sub(\"[^\\w\\s]+\",\"\",x))\nset(df_comp.TicketType.to_list())","eb08482f":"# Let's check the freq of passengers\ndf_comp.TicketType.value_counts()\/ df_comp.shape[0]","04ced82a":"pd.crosstab(index= df_comp.TicketType, columns= df_comp.Pclass, normalize='index' )","a77e5a91":"### df_comp['Pclass'].value_counts()\/ df_comp.shape[0]","3b12d544":"# Let's see if ticket type is related to Pclass\npd.crosstab(index= df_comp.TicketType, columns= df_comp.Pclass, normalize='index' ).sort_values(by = 1).plot.bar(figsize=(15, 7), stacked = True)\n\nplt.axhline(y = 0.2, color = 'r', linestyle = '-')","af883550":"# Bivariate\n# Check the ticket types when more than 60% did not survive \npd.crosstab(index= df_comp.TicketType[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)\n\nplt.axhline(y = 0.8, color = 'r', linestyle = '-')\nplt.axhline(y = 0.65, color = 'g', linestyle = '-')","244b57d3":"# Let's create bucket for ticket type\ndf_comp['TT_bucket'] = df_comp.TicketType.map(lambda x: 0 if x == 'pc' else 3 if x in ['stono', 'stono2', 'sotono2', 'stonoq', 'aq3', 'a', 'a5'] else 2 if \\\n                                             x in ['sotonoq', 'fa', 'ca', 'fcc', 'scow', 'caston', 'wc', 'c'] else 1)\n\ndf_comp['TT_bucket'].value_counts()","730ea80c":"#Bivariate\npd.crosstab(index= df_comp.TT_bucket[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)","530ffa06":"# Change Sex variable to 1 and 0\ndf_comp['Sex'] = df_comp['Sex'].map(lambda i: 1 if i == 'male' else 0)","bbe16223":"# Bivariate\npd.crosstab(index= df_comp.Sex[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(figsize=(15, 7),stacked = True)","7f77e494":"##combine the number of SibSp & Parch +1 to be FamilySize, a new feature synthesized:\ndf_comp['FamilySize'] = df_comp['SibSp'] + df_comp['Parch'] + 1 ","0e150e9e":"def family_size(x):\n    if x == 1:\n        return \"alone\"\n    else:\n        return \"notalone\"","190e39ce":"df_comp[\"Group\"] = df_comp[\"FamilySize\"].apply(family_size)","009f8d83":"# Bivariate\npd.crosstab(index= df_comp.Group[df_comp.Set == \"Train\"] , columns= df_comp.Survived[df_comp.Set == \"Train\"], normalize='index' ). \\\nsort_values(by = 1).plot.bar(stacked = True)","8fba7230":"# Let's create a backup for our analytical data with all features\ndf_copy_comp = df_comp.copy()","1a37d675":"# Check column names\ndf_comp.columns\n","5c99ac62":"# Drop unnecessary columns\ndf_comp.drop(columns = ['Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'TicketType'], inplace= True)","9b5e9e91":"# Check data\ndf_comp.head()","88c71734":"# Change group to 0 and 1 (0 mean Alone and 1 mean Group)\ndf_comp['Group'] = df_comp['Group'].map(lambda x: 0 if x == \"alone\" else 1)","1ede06fc":"# Get dummies for Embarked\ndf_comp = pd.get_dummies(df_comp, columns= ['Embarked'])","476d09ef":"# Get dummies for Age bucket\ndf_comp = pd.get_dummies(df_comp, columns= ['Age_bucket'])","08904fb3":"# Label encode deck\n# Import label encoder\nfrom sklearn import preprocessing\n  \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n  \n# Encode labels in column 'species'.\ndf_comp['Deck']= label_encoder.fit_transform(df_comp['Deck'])\n  \ndf_comp['Deck'].unique()","63747783":"df_comp.head()","38d70604":"# Model\n# Get train and test data from comp data\n\nnew_train = df_comp[df_comp.Set == \"Train\"].drop([\"Set\", \"PassengerId\"], axis = 1)\nnew_test =  df_comp[df_comp.Set == \"Test\"].drop([\"Set\", \"PassengerId\",\"Survived\"], axis = 1)\n\nnew_train.head()\n#df_test.head()","46b7802c":"# Creat X feature set and y target\nX=new_train.drop(\"Survived\",axis=1).values\ny = new_train.Survived.values","f04666a8":"print(X.shape)\nprint(y.shape)","787d1d77":"from sklearn.model_selection import KFold\n\n# Creating 5 folds (samples)\nkf = KFold(n_splits=5,random_state=42,shuffle=True)\n\n# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=400)","49f0b1cf":"from sklearn import ensemble\nfrom sklearn import metrics\ndef cross_val_fn(n_trees):\n    AUC =[]\n    for dev_index, validation_index in kf.split(X_train):\n        print(\"TRAIN:\", dev_index, \"TEST:\", validation_index)\n        X_dev, X_validation = X_train[dev_index], X_train[validation_index]\n        y_dev, y_validation = y_train[dev_index], y_train[validation_index]\n        clf=ensemble.RandomForestClassifier(n_jobs=-1,n_estimators=n_trees,random_state=400)\n        clf = clf.fit(X_dev, y_dev)\n        ### train my model on dev set and obtain some accuracy measure on validation set\n        # preds=clf.predict(X_validation)\n        probs=clf.predict_proba(X_validation)[:,1]\n        auc = metrics.roc_auc_score(y_validation,probs)\n        AUC.append(auc)\n    print(\"Mean AUC = \",np.array(AUC).mean())\n    return np.array(AUC).mean()","a2bbd65d":"# Blank dictionary for AUC for different iterations of n_estimators\nn_estimator_dict={}\n\n# 2 keys- # tress and AUC\n# Value pairs - n_tress and AUC output by the function\nn_estimator_dict['trees']=[]\nn_estimator_dict['AUC']=[]\n\n# Run many iterations of ensemble models starting from 10 trees till 200 tress with 20 steps frequency.. 10, 30, 50, 70,....190\nfor tree in range(10,200,10):\n    AUC=cross_val_fn(tree)\n    n_estimator_dict['trees'].append(tree)\n    n_estimator_dict['AUC'].append(AUC)","9ecf64ef":"# Checking the outputs in dictionary\ndf_auc = pd.DataFrame(n_estimator_dict)","e581e77f":"sns.lineplot(x = df_auc.trees, y = df_auc.AUC)","c9058c49":"trees=[150,170,190,210, 230, 250]\nmin_samples_split=[2,4,6]","48d8002f":"# Import the library for creating cross product of tress and min sample split\nimport itertools","3bf8bcfb":"# Create a function to run multiple ensembles by lopping over cross product of multiple hyperparameters\ndef cross_val_fn(n_trees, min_samples):\n    AUC =[]\n    for dev_index, validation_index in kf.split(X_train):\n        print(\"TRAIN:\", dev_index, \"TEST:\", validation_index)\n        X_dev, X_validation = X_train[dev_index], X_train[validation_index]\n        y_dev, y_validation = y_train[dev_index], y_train[validation_index]\n        clf=ensemble.RandomForestClassifier(n_jobs=-1,n_estimators=n_trees,min_samples_split=min_samples,\n                                       random_state=400)\n        clf = clf.fit(X_dev, y_dev)\n        ### train my model on dev set and obtain some accuracy measure on validation set\n        # preds=clf.predict(X_validation)\n        \n        probs=clf.predict_proba(X_validation)[:,1]\n        auc = metrics.roc_auc_score(y_validation,probs)\n        AUC.append(auc)\n    print(\"Mean AUC = \",np.array(AUC).mean())\n    return np.array(AUC).mean()","ddc387ae":"# Blank dictionary for AUC for different iterations of n_estimators\nn_estimator_dict={}\nn_estimator_dict['trees']=[]\nn_estimator_dict['Min Sample'] =[]\nn_estimator_dict['AUC']=[]\n\nfor tree,min_samples in itertools.product(trees,min_samples_split):\n    AUC=cross_val_fn(tree, min_samples)\n    n_estimator_dict['trees'].append(tree)\n    n_estimator_dict['Min Sample'].append(min_samples)\n    n_estimator_dict['AUC'].append(AUC)","c02686a2":"# Create dataframe of dictionary\npd.DataFrame(n_estimator_dict)","64ae61d5":"# Finalize RF with n_estimators = 250 and min_sample_split = 6\n# Import random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create classifier object\nclf=RandomForestClassifier(n_estimators=250, min_samples_split=6 ,oob_score=True,n_jobs=-1,random_state=400)\n\n# Fit model\nclf.fit(X,y)","9c43e6e3":"#clf.oob_score_","47aca5d3":"feature_names = new_train.drop(\"Survived\", axis = 1).columns\npd.Series(clf.feature_importances_,index=feature_names.tolist()).sort_values(ascending=False).plot(kind='barh', figsize = (15,7))","d462a9ca":"y_hat = clf.predict(new_test)\nresults_df = pd.DataFrame(data={'PassengerId':df_test['PassengerId'], 'Survived':y_hat})\nresults_df.to_csv('submission-random_forest_kfold.csv', index=False)","5af1e9a1":"# Treating Missing values","9d15f28e":"## % of passenges who survived - 42.8%","796c0bb8":"## Ticket\n### Some ticket numbers have some special alpha charachters like A\/5,CA, SC PARIS etc., while many are just numeric. May mean some categorization of Special class tickets\n#### Let's call the ones with only numbers as XX tickets\n### Let us try and do some text cleaning and feature extraction and then also put the missing values as 'XX'","2ac96493":"## The ticket type is related to Pclass \n## we also observe for the ticket types with less than 20% of 1st class passengers\n###  - stono, stono2, sotono2, stonoq, aq3, a and a5 have death rates >= 80%\n###  - sotonoq, fa, ca, fcc, scow, caston, wc and c have death rates >=65% and <80%\n###  - rest are <65% death rate (>35% survival rate)  \n  \n    \n\n### ** also most of the 'pc' ticket type passengers belong to first class\n","c1e799fc":"## Surival rate for women was higher","76aa31f6":"# Importing important packages","22562c50":"### Age  \n#### Treat by Median","6adb1206":"## Cabin\n### Cabin is interesting with approx 70% missing values\n### It may mean that 70% of the passengers did not have dedicated Cabin assigned.. maybe they were using some common\/ shared rooms\n### It will be interesting to see if passengers with no cabin alloted had lower chances of survival\n### We also observe that the 1st alphbet of the cabin number may mean the Deck number (could be an important feature. Not done in this version of code)","e9e7286b":"## Ticket Type Bucket shows survival rate varies by ticket types","f280ad4c":"### Embarked\n#### Can be replaced by mode value of embarked which is 'S'","e244a8e9":"## Drop the unnecessary columns","d91f98ce":"## Passengers who paid more fare had higher chances of survival","93dacd89":"## CROSS VALIDATION","004ae8f8":"![](https:\/\/preview.redd.it\/0izq0428pe661.jpg?width=960&format=pjpg&auto=webp&s=15022053715fc50198a17c401be035445592fee2)","7fc5feac":"# Read the data","8b4098d6":"### Fare\n#### Multimodal skewed distribution. This could be due to the class of the passengers  \n#### It is worth to check if Fare is dependent on Pclass and then do the treatment of missing values as per the passenger class\n#### Also because of heavy tail and outliers, we may have to perform a log transform to control the variation in Fare\n#### The Fare also has some values < 1, which means ln(Fare) can go negaitve. So we could do ln(1+Fare) transformation","630f83de":"## Being Alone or not did not make much difference","86606627":"## For passengers who boarded the Ship in 'Southampton' had least chances of survival\n## while who boarded in Cherbourg had highest chances of survival","e74cd4b1":"## Check the records in the data\nSurvived is the target variable, while other variables are the raw features in the train data.","e5c97f9e":"# Submission","152a42a4":"## Passengers with no cabin alloted had relatively lower chances of survival","fa981dbf":"### A good strategy to understand features as a whole would be to combine trainig and testing data and then perform univariate analysis\n### So let's do it!!","abc7a0b9":"# Check Missing Values of the features in the complete data\n\n## 1. Age ~3.38% missing values  \n## 2. Ticket ~ 4.9% missing values\n## 3. Fare ~ 0.13% missing values\n## 4. Cabin ~ 69.34% missing values\n## 4. Embarked ~ 0.26% missing values","a7e413d4":"## I hope you like the notebook. Feel free to use it. Make sure you upvote and give credits.\n### All the best \n#### -JM","c6c3275f":"\n\n# **What will we cover?**  \n##  *This code will help you get started with the problem and will also focus on getting some key insights into the data*\n##  *It also captures RF model with CV and submission file*\n\n##   *Contents*\n  \n### 1. Data Understanding  \n### 2. Data Cleaning  \n### 3. Data manipulation and preprocessing  \n### 4. Exploratory Data Analysis (Univariate and Bivariate analysis)  + Vizualizations\n### 5. Model\n### 6. Submission"}}