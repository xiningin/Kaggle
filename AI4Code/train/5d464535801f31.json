{"cell_type":{"8a2e3647":"code","e6470533":"code","caed12b9":"code","56df7b03":"code","012c980f":"code","5625dcc7":"code","ac2aecd3":"code","c4885699":"code","616cd86c":"code","e8642bf6":"code","364627aa":"code","ca56fdf1":"markdown"},"source":{"8a2e3647":"from google.colab import drive\ndrive.mount('\/content\/drive')","e6470533":"#!pip install tensorflow_addons\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, Input, Dense, Add, Multiply,Bidirectional,GRU,Dropout\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error, f1_score, confusion_matrix, recall_score,precision_score\n#from tensorflow.compat.v1.keras.layers import CuDNNLSTM,CuDNNGRU\nimport pandas as pd\nimport numpy as np\nimport random\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\nimport gc\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\nfrom scipy import signal\nimport scipy as sp\nimport os\n\n#add by NorwayPing\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nimport math\nfrom keras.constraints import maxnorm\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nimport math\nfrom sklearn.metrics import confusion_matrix,  plot_confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Any results you write to the current directory are saved as output.\n\nfrom typing import Tuple\ndef augment(X: np.array, y:np.array,z:np.array) -> Tuple[np.array, np.array,np.array]:\n    \n    X = np.vstack((X, np.flip(X, axis=1)))\n    y = np.vstack((y, np.flip(y, axis=1)))\n    z = np.vstack((z, np.flip(z, axis=1)))\n    \n    return X, y,z","caed12b9":"# configurations and main hyperparammeters\nEPOCHS = 150\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 321\nLR = 0.001\nSPLITS = 5\n\n#add by NorwayPing\nTRAIN_LEN = 5000000\nSEEDS = [2020,321, 52]\nCOUNTER_RUN =3\nVALID_SCORE_LIST = []\nF1_SCORE_LIST = []","56df7b03":"\n#add by NorwayPing\nclass_weight_dic= {0: 4.03185976,1:5.07089749,2:9.03344407,3:7.47632808,4:12.42043161,\n  5:17.96057653,6:26.54068687, 7:18.86623369, 8:20.37396411,9:36.72318978,10: 140.23770291}\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","012c980f":"\ndef create_class_weight(y, mu=0.5):\n        classes = np.unique(y)\n        #class_weights = class_weight.compute_class_weight(\"balanced\", classes, y)\n        value_counts = pd.value_counts(y)\n\n        total = len(y)\n        keys = classes\n        class_weights = dict()\n        class_weight_log = []\n\n        for key in keys:\n            score = total \/ (value_counts[key])\n            score_log = math.log(mu * total \/(value_counts[key]))\n            class_weights[key] = round(score, 2) if score > 1.0 else round(1.0, 2)\n            class_weight_log.append( round(score_log, 2) if score_log > 1.0 else round(1.0, 2))\n\n        return class_weights, class_weight_log\n\n        \n# read data\ndef read_data():\n    train = pd.read_csv('\/content\/drive\/My Drive\/ion_kaggle\/data\/train_clean_trend.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('\/content\/drive\/My Drive\/ion_kaggle\/data\/test_clean_trend.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv('\/content\/drive\/My Drive\/ion_kaggle\/data\/sample_submission.csv', dtype={'time': np.float32}) \n    del train['type'],test['type']\n    train['group'] = np.arange(train.shape[0])\/\/500_000\n    aug_df = train[train[\"group\"] == 5].copy()\n    aug_df[\"group\"] = 10\n\n    for col in [\"signal\", \"open_channels\"]:\n        aug_df[col] += train[train[\"group\"] == 8][col].values\n\n    train = train.append(aug_df, sort=False).reset_index(drop=True)\n    del aug_df\n    gc.collect()\n\n    train_pred=np.load(\"\/content\/drive\/My Drive\/ion_kaggle\/data\/lgb_reg_aug_opt.npz\")['valid']  \n    test_pred=np.load(\"\/content\/drive\/My Drive\/ion_kaggle\/data\/lgb_reg_aug_opt.npz\")['test']   \n    train_pred1=np.load(\"\/content\/drive\/My Drive\/ion_kaggle\/data\/lgb_pred_trend.npz\")['valid'] \n    test_pred1=np.load(\"\/content\/drive\/My Drive\/ion_kaggle\/data\/lgb_pred_trend.npz\")['test'] \n\n    return train, test, sub,train_pred,test_pred,train_pred1,test_pred1   \n\n\n\n\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    df['group'] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test,features=['signal','oof_fea']):\n    for item in features:\n        train_input_mean = train[item].mean()\n        train_input_sigma = train[item].std()\n        train[item]= (train[item] - train_input_mean) \/ train_input_sigma\n        test[item] = (test[item] - train_input_mean) \/ train_input_sigma\n    return train, test\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n                                                                                                    \n    return df\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df, batch_size):\n    # create batches\n    df = batching(df, batch_size = batch_size)\n\n    df = lag_with_pct_change(df, [1,2,3])\n\n    df['signal_2'] = df['signal'] ** 2\n    \n    \n    \n    return df\n\n# fillna with the mean and select features for training\ndef feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features\n\n# model function (very important, you can try different arquitectures to get a better score. I believe that top public leaderboard is a 1D Conv + RNN style)\ndef Classifier(shape_):\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n\n    x = wave_block(inp, 16, 3, 12)\n    x = wave_block(x, 32, 3, 8)\n    #x = wave_block(x, 64, 3, 4)\n    x = wave_block(x, 128, 3, 1)\n    #x = Bidirectional(CuDNNGRU(128,return_sequences=True))(x)\n\n    x = Dropout(0.4)(x)\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt) \n    \n\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n\n    return model\n\n# function that decrease the learning as epochs increase (i also change this part of the code)\n\ndef lr_schedule(epoch):\n    if epoch < 40:\n        lr = LR\n    elif epoch < 50:\n        lr = LR \/ 3\n    elif epoch < 60:\n        lr = LR \/ 6\n    elif epoch < 70:\n        lr = LR \/ 9\n    elif epoch < 80:\n        lr = LR \/ 12\n    elif epoch < 100:\n        lr = LR \/ 15\n    else:\n        lr = LR \/ 100\n    return lr\n\ndef lr_schedule(epoch):\n    if epoch < 80:\n        lr = LR\n    elif epoch < 90:\n        lr = LR \/ 4\n    elif epoch < 100:\n        lr = LR \/ 7\n    elif epoch < 110:\n        lr = LR \/ 12\n    else:\n        lr = LR \/ 100\n    return lr\n\ndef lr_schedule(epoch):\n    if epoch < 70:\n      lr = LR\n    elif epoch < 80:\n      lr = LR \/ 3\n    elif epoch < 90:\n      lr = LR \/ 6\n    elif epoch < 100:\n      lr = LR \/ 9\n    elif epoch < 110:\n      lr = LR \/ 12\n    elif epoch < 120:\n      lr = LR \/ 15\n    else:\n      lr = LR \/ 100\n    return lr\n\n# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\nclass MacroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n        \n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n        score = f1_score(self.targets, pred, average = 'macro')\n        print(f'F1 Macro Score: {score:.5f}')\n'''\nclass MacroF1(Callback):\n    def __init__(self, model, inputs, targets,filepath):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n        self.best_f1=0\n        self.filepath=filepath\n        \n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n        score = f1_score(self.targets, pred, average = 'macro')\n        print(f'F1 Macro Score: {score:.5f}')\n        if score>=self.best_f1:\n            self.best_f1=score\n            self.model.save_weights(self.filepath)\n'''\nclass Metric(Callback):\n    def __init__(self, model, callbacks, data):\n        super().__init__()\n        self.model = model\n        self.callbacks = callbacks\n        self.data = data\n\n    def on_train_begin(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def on_epoch_end(self, batch, logs=None):\n        # X_train, y_train = self.data[0][0], self.data[0][1]\n        # y_train = np.argmax(y_train, axis=2).reshape(-1)\n        # train_pred = np.argmax(self.model.predict(X_train), axis=2).reshape(-1)\n        # train_score = recall_score(y_train, train_pred, average=\"macro\")        \n        # train_score = f1_score(y_train, train_pred, average=\"macro\")        \n        # logs['Train_F1Macro'] = train_score\n\n        X_valid, y_valid = self.data[1][0], self.data[1][1]\n        y_valid = np.argmax(y_valid, axis=2).reshape(-1)\n        valid_pred = np.argmax(self.model.predict(X_valid), axis=2).reshape(-1)\n        valid_recall = recall_score(y_valid, valid_pred, average=\"macro\")        \n        valid_precision = precision_score(y_valid, valid_pred, average=\"macro\")        \n        valid_score = f1_score(y_valid, valid_pred, average=\"macro\")        \n        logs['Valid_F1Macro'] = valid_score\n        logs['Valid_Recall'] = valid_recall\n        logs['Valid_Precision'] = valid_precision\n\n        print(f\"Validation F1 Macro {valid_score:1.6f} Validation Recall {valid_recall:1.6f} Validation Precision {valid_precision:1.6f}\")\n        # print(' Train F1 Macro', train_score, 'Validation F1 Macro', valid_score)\n\n        for callback in self.callbacks:\n            callback.on_epoch_end(batch, logs)\n        \n        gc.collect()  \n\n    \n\n        \n","5625dcc7":"def run_cv_model_by_batch(run_number, train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size,is_10_channel=False):\n    \n    seed = SEEDS[run_number]\n    seed_everything(seed)\n    K.clear_session()\n    print(f'seed:{SEED}')\n\n\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    #change by NorwayPing\n    oof_ = np.zeros((len(train)\/\/GROUP_BATCH_SIZE, GROUP_BATCH_SIZE, 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n    org_oof_ = np.zeros((len(train)\/\/GROUP_BATCH_SIZE, GROUP_BATCH_SIZE, 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n     \n    #add by NorwayPing        \n    train_open_channels = train[target]\n    len_train = len(train)\n    group_counts = train['group'].nunique()   \n    group_unique =np.arange(group_counts)    \n    org_group = np.arange(group_counts)    \n    np.random.shuffle(group_unique)\n    dic_group = dict(dict(zip(group_unique, org_group)))    \n    group_id =np.repeat(group_unique,GROUP_BATCH_SIZE)   \n    train['group'] = group_id\n \n    \n    group = train['group']\n    kf = GroupKFold(n_splits=5)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n     \n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n\n    #add by NorwayPing    \n    train_target = np.array(list(train.groupby('group').apply(lambda x: x[target].values)))   \n  \n\n\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n\n    #add by NorwayPing\n    #test_flip = augment(test)\n\n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        \n      \n        train_target_y = train_target[tr_idx]\n        train_x, train_y,train_target_y = augment(train_x, train_y,train_target_y)\n\n        #add by NorwayPing                         \n        org_group_x =   pd.Series(dic_group)[val_idx].values\n        #  org_group_x =   pd.Series(dic_group)[val_idx].values\n        class_weights, class_weight_log = create_class_weight(train_target[tr_idx].reshape(-1))\n\n        #change by NorwayPing to improve class 6,7,8\n        #class_weight_log =create_class_weight(train_y)\n        \n        sample_weight = np.ones(shape=(train_y.shape[0],train_y.shape[1],1))\n        #train_target_y = train_target[tr_idx]        \n        for i in range(11):\n          sample_weight[train_target_y == i] = class_weight_log[i]\n\n                      \n        gc.collect()\n        shape_ = (None, train_x.shape[2]) # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n        model = Classifier(shape_)\n\n        # using our lr_schedule function\n        cp = ModelCheckpoint(f\"model_run{run_number}_fold{n_fold+1}.h5\", monitor='Valid_F1Macro', mode='max',save_best_only=True, verbose=1, period=1)\n        cp.set_model(model)\n        cp_loss = ModelCheckpoint(f\"model_run{run_number}_fold{n_fold+1}.h5\", monitor='val_loss', mode='min',save_best_only=True, verbose=1, period=1)\n        cp_loss.set_model(model)\n        es = EarlyStopping(monitor='val_loss',mode='min',restore_best_weights=True,verbose=1,patience=20)\n        es.set_model(model)\n\n        cb_lr_schedule = LearningRateScheduler(lr_schedule)               \n        metric = Metric(model, [cp,cp_loss,es], [(train_x, train_y), (valid_x, valid_y)]) # ,es\n\n        history = model.fit(train_x, train_y,epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, metric], # MacroF1(model, [valid_x_sig, valid_x_oof], valid_y)\n                  batch_size = nn_batch_size, verbose = 0,\n                  validation_data = (valid_x, valid_y),\n                  sample_weight=sample_weight)\n        '''history = model.fit(train_x, train_y,epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, metric], # MacroF1(model, [valid_x_sig, valid_x_oof], valid_y)\n                  batch_size = nn_batch_size, verbose = 0,\n                  validation_data = (valid_x, valid_y))'''\n        # plot history\n        # summarize history for accuracy\n        plt.figure(figsize=(20,15))\n        plt.plot(history.history['accuracy'], label='train')\n        plt.plot(history.history['val_accuracy'], label='test')\n        plt.legend()\n        plt.show()\n        \n        plt.figure(figsize=(20,15))\n        plt.plot(history.history['loss'], label='train')\n        plt.plot(history.history['val_loss'], label='test')\n        plt.legend()\n        plt.show()\n\n        #model.load_weights(f'run{run_number}-fold{n_fold}.h5')\n\n\n        preds_f = model.predict(valid_x)\n         \n         \n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro') # need to get the class with the biggest probability\n        print(f'Training run {run_number}  fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n\n       \n        #change by NorwayPing\n        #preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_idx,:] += preds_f      \n        org_oof_[org_group_x,:] += preds_f \n\n        te_preds = model.predict(test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds \/ SPLITS\n\n        \n         #add by NorwayPing\n        if is_10_channel:\n          VALID_SCORE_10_CHANNEL_LIST.append(f1_score_)\n        else:\n          VALID_SCORE_LIST.append(f1_score_)\n\n    # calculate the oof macro f1_score\n  \n    f1_score_ = f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis = 2).reshape(-1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training run {run_number} completed. oof macro f1 score : {f1_score_:1.5f}')\n \n    f1_score_ = f1_score(train_open_channels,  np.argmax(org_oof_, axis = 2).reshape(-1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training run {run_number} completed. oof macro f1 org score : {f1_score_:1.5f}')\n\n    #add by NorwayPing \n    if is_10_channel:\n          F1_SCORE_10_CHANNEL_LIST.append(f1_score_)\n    else:\n          F1_SCORE_LIST.append(f1_score_)\n    \n    return org_oof_.reshape(-1, 11),preds_","ac2aecd3":"# this function run our entire program\ndef run_everything():\n    \n    print('Reading Data Started...')\n    train, test, sample_submission,train_pred,test_pred,train_pred1,test_pred1= read_data()\n     \n\n    train['oof_fea']=train_pred\n    test['oof_fea']=test_pred\n    train['oof_fea1']=train_pred1\n    test['oof_fea1']=test_pred1\n\n    '''train=pd.concat([train[0:3600000],train[4000000:]])\n    train=train.reset_index(drop=True)\n    test=test.reset_index(drop=True)'''\n\n   \n    #test=test.reset_index(drop=True)\n    \n    train, test = normalize(train, test,features=['signal','oof_fea','oof_fea1'])\n    print('Reading and Normalizing Data Completed')\n        \n    print('Creating Features')\n    print('Feature Engineering Started...')\n    train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\n    test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n    print(train.shape)\n    train, test, features = feature_selection(train, test)\n    print(train.shape)\n    print('Feature Engineering Completed...')\n        \n   \n    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n    #add by NorwayPing\n    preds_ = np.zeros((len(test), 11))\n    oof_ = np.zeros((len(train), 11))\n    for run in range(COUNTER_RUN):\n        \n        #preds_ +=run_cv_model_by_batch(run, train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\/COUNTER_RUN\n        oof_run,preds_run = run_cv_model_by_batch(run, train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\n        oof_ += oof_run\/COUNTER_RUN\n        preds_ += preds_run\/COUNTER_RUN\n\n    f1_score_ = f1_score(train['open_channels'],  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training completed. overall oof macro f1 score : {f1_score_:1.5f}')\n    \n    \n    print(f'Training valid macro f1 score list: {VALID_SCORE_LIST}')\n    print(f'Training oof macro f1 score list: {F1_SCORE_LIST}')\n\n    print(f'Training valid macro f1 score mean: {np.mean(VALID_SCORE_LIST)}')\n    print(f'Training oof macro f1 score mean: {np.mean(F1_SCORE_LIST)}')\n    \n    print(f'Training valid macro f1 score std: {np.std(VALID_SCORE_LIST)}')\n    print(f'Training oof macro f1 score std: {np.std(F1_SCORE_LIST)}')\n\n    sample_submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n    print(sample_submission['open_channels'].value_counts())\n    sample_submission.to_csv('rank 6-modify-highest -3runs.csv', index=False, float_format='%.4f')\n    np.savez_compressed('rank 6-modify-highest -3runs.npz',valid=oof_, test=preds_)\n    plot_cm(train['open_channels'],  np.argmax(oof_, axis = 1), 'all data wavenet \\n f1=' + str('%.4f' %f1_score_))","c4885699":"run_everything()","616cd86c":"# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(16,16)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='viridis', annot=annot, fmt='', ax=ax)","e8642bf6":"train = pd.read_csv('\/content\/drive\/My Drive\/ion_kaggle\/data\/train_clean_trend.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\ntest  = pd.read_csv('\/content\/drive\/My Drive\/ion_kaggle\/data\/test_clean_trend.csv', dtype={'time': np.float32, 'signal': np.float32})\nsub  = pd.read_csv('\/content\/drive\/My Drive\/ion_kaggle\/data\/sample_submission.csv', dtype={'time': np.float32}) \ndel train['type'],test['type']\ntrain['group'] = np.arange(train.shape[0])\/\/500_000\naug_df = train[train[\"group\"] == 5].copy()\naug_df[\"group\"] = 10\n\nfor col in [\"signal\", \"open_channels\"]:\n    aug_df[col] += train[train[\"group\"] == 8][col].values\n\ntrain = train.append(aug_df, sort=False).reset_index(drop=True)\ndel aug_df\ngc.collect()\n\noof=np.load(\"rank 6-modify-highest -3runs.npz\")['valid'] \n\noof_df = train[['signal','open_channels','group']].copy()\noof_df[\"oof\"] = np.argmax(oof, axis=1)\noof_df = oof_df[oof_df[\"group\"]<10]\ngc.collect()\n\noof_f1 = f1_score(oof_df['open_channels'],oof_df['oof'],average = 'macro')\noof_recall = recall_score(oof_df['open_channels'],oof_df['oof'],average = 'macro')\noof_precision = precision_score(oof_df['open_channels'],oof_df['oof'],average = 'macro')\n\nprint(f\"OOF F1 Macro Score: {oof_f1:.6f} - OOF Recall Score: {oof_recall:.6f} - OOF Precision Score: {oof_precision:.6f}\")\n\n","364627aa":"plot_cm(oof_df['open_channels'],  (oof_df['oof']), 'all data wavenet \\n f1=' + str('%.4f' %oof_f1))","ca56fdf1":"# Thanks to https:\/\/www.kaggle.com\/siavrez\/wavenet-keras, learned a lot."}}