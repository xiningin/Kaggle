{"cell_type":{"16ed40f4":"code","36191ca3":"code","6dbf71a4":"code","cf09c669":"code","61e15e6f":"code","19a1ded1":"code","fff629bd":"code","953f040b":"code","ef1b9931":"code","0d4ea0e6":"code","99550b08":"code","223fcc94":"code","54ab03c8":"code","c5b7e126":"code","659e437d":"code","bb7e8997":"code","a27563b8":"code","6607874d":"code","4ed31abc":"code","200d562f":"code","932bc61d":"code","6e1108b3":"code","2621b5f6":"code","ccfcc32a":"code","bef1ddf8":"code","3f81177b":"code","8c8143bf":"code","5376301e":"code","bdb32041":"code","78d7fe8e":"code","6fd82f3e":"code","c6c41aaf":"code","b3e63f33":"code","163e2ab2":"code","9a605a54":"code","e71b1bc9":"code","3a65b74c":"code","9072c13c":"code","428e6f62":"code","044305c2":"code","c91df1b0":"code","5a51e825":"code","dd8bc818":"code","e25f33fa":"code","ee1ece4c":"code","814b8fe7":"code","e9a12ae5":"code","76360bcc":"code","b68d534d":"code","88d18619":"code","9af70f1d":"code","dad5fca7":"code","d3e45999":"code","79f357f4":"code","0d1006db":"code","28f2d4d3":"code","0e8ed75e":"code","3017fe74":"code","3325a83c":"code","c324070b":"code","86f7cbdc":"code","5ee73bec":"markdown","a93f82a4":"markdown","76137dc4":"markdown","abfd0324":"markdown","3e0d8076":"markdown","d3758882":"markdown","0dbdef1d":"markdown","cc9cbc1b":"markdown","2229c0a8":"markdown","441bffcb":"markdown","b1ac687b":"markdown","531cfefe":"markdown","2b769706":"markdown","596ec840":"markdown","4073176c":"markdown","9c26cc80":"markdown","f7309230":"markdown","27a8c5f2":"markdown","edd60cc8":"markdown","1d8fc64d":"markdown","26a80888":"markdown","27e95509":"markdown"},"source":{"16ed40f4":"import numpy as np \nimport pandas as pd\nimport os","36191ca3":"import tensorflow as tf\ntf.__version__","6dbf71a4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom textblob import TextBlob\nimport re\nfrom PIL import Image\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()","cf09c669":"from wordcloud import WordCloud\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nimport wordcloud ","61e15e6f":"df_raw = pd.read_csv(\"..\/input\/nlp-dataset-sentiments\/dataset\/dataset.csv\", encoding = \"ISO-8859-1\") ","19a1ded1":"df_raw.info() ","fff629bd":"df_raw.isnull().sum() #No null values","953f040b":"df_raw.head()","ef1b9931":"regx_pattern = \"(@[\\S]+)|([^A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\"\n#(@[\\S]+) --> any alpha-numeric word begining with '@' Eg: @viraj\n#([^0-9A-Za-z \\t]) --> any non-numeric,non-alphabetical token Eg: ^,.\n#(\\w+:\\\/\\\/\\S+) --> Hyperlinks\n\nregx_pattern_2 = \"(@[\\S]+)|([#$%&()*+-\/:<=>@[\\\\]^_{|}~\\t\\n])|(\\w+:\\\/\\\/\\S+)\"\n#Do not filter out !,;?","0d4ea0e6":"nltk.download('punkt')","99550b08":"def process_text(text):\n    text=text.strip().lower()\n    text = re.sub(regx_pattern, \" \", text) #text=re.sub(regx_pattern_2, \" \", text)\n    tokenized_text = word_tokenize(text) #nltk.tokenize.TreebankWordTokenizer().tokenize(text)\n    #tokenized_text =[t for t in tokenized_text if len(t)>1] #Include words with length greater than 1\n    return ' '.join(tokenized_text)","223fcc94":"df_raw['Cleaned_Text'] = [process_text(text) for text in df_raw['SentimentText']]","54ab03c8":"num_samples = df_raw.shape[0]\nprint(f\"Number of Samples: {num_samples}\")","c5b7e126":"X_train,X_test,y_train,y_test = train_test_split(df_raw.loc[:,['ItemID', 'SentimentText', 'Cleaned_Text']],\n                                                 df_raw.loc[:,['Sentiment']],\n                                                 test_size = 0.33)","659e437d":"train_samples = len(X_train)\ntest_samples = len(X_test)\nprint(f\"Number of Training Samples: {train_samples}\")\nprint(f\"Number of Test Samples    : {test_samples}\")","bb7e8997":"#Number of tweets with positive and negative sentiments in all data\ndf_raw.groupby(['Sentiment']).count()['ItemID']","a27563b8":"train_data = pd.concat([X_train,y_train], axis = 1)\ntrain_data.head(10)","6607874d":"#Number of tweets with positive and negative sentiments in training_data\ntrain_data.groupby(['Sentiment']).count()['ItemID']","4ed31abc":"test_data = pd.concat([X_test,y_test], axis = 1)\ntest_data.head()","200d562f":"#Number of tweets with positive and negatice sentiments in test_data\ntest_data.groupby(['Sentiment']).count()['ItemID']","932bc61d":"df_join = []\ndf_neg_join = \" \".join(df_raw[df_raw['Sentiment'] == 0]['Cleaned_Text'])\ndf_pos_join = \" \".join(df_raw[df_raw['Sentiment'] == 1]['Cleaned_Text'])\ndf_join.append(df_neg_join)\ndf_join.append(df_pos_join)","6e1108b3":"df_concat = pd.DataFrame(df_join, columns=['Cleaned_Text'])\ndf_concat","2621b5f6":"sentiment_list = ['negative', 'positive']","ccfcc32a":"nltk.download('stopwords')","bef1ddf8":"stop_words=set(stopwords.words(\"english\"))","3f81177b":"stop_words = stop_words.union(['quot', 'lol', 'like', 'know', 'amp', 'think', 'get', 'got', 'one', 'time'])","8c8143bf":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words=stop_words)\ndata_cv = cv.fit_transform(df_concat.Cleaned_Text)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = df_concat.index\ndata_dtm","5376301e":"data = data_dtm.transpose()\ndata.head()","bdb32041":"# Top 30 words per sentiment\ntop_dict = {}\nfor c in data.columns:\n    top = data[c].sort_values(ascending=False).head(30)\n    top_dict[c]= list(zip(top.index, top.values))\ndf_word_count = pd.DataFrame(top_dict)\ndf_word_count = df_word_count.rename(columns={0: 'negative', 1: 'positive'})\ndf_word_count.head(30)","78d7fe8e":"plt.figure(figsize=(35,25))\ni=0\nfor i in df_concat.index:\n    plt.subplot(2, 4, i+1)\n\n    word_cloud = WordCloud(background_color=\"white\",\n                    max_words=100, \n                    mask=None,\n                    stopwords=stop_words)\n    word_cloud.generate(df_concat.loc[i,'Cleaned_Text'])\n    plt.imshow(word_cloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(sentiment_list[i])\n    i=i+1\nplt.show()","6fd82f3e":"unique_neg_words = set(np.squeeze(np.array([sent.split() for sent in df_concat.loc[0,:]])))\nunique_pos_words = set(np.squeeze(np.array([sent.split() for sent in df_concat.loc[1,:]])))\nunique_words_count = [len(unique_neg_words), len(unique_pos_words)]\nunique_words_dict = {\n                        'Sentiment': sentiment_list,\n                        'Unique Words': unique_words_count\n                    }\nunique_words = pd.DataFrame.from_dict(unique_words_dict)\nunique_words","c6c41aaf":"plt.figure(figsize=(15,5))\nplt.barh(unique_words['Sentiment'],\n        unique_words['Unique Words'])\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Number of Unique words\")\nplt.title(\"Unique words in each sentiment class\")\nplt.show()","b3e63f33":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import LSTM, Embedding, Bidirectional\nfrom tensorflow.keras.models import Model","163e2ab2":"# Convert sentences to sequences\nMAX_VOCAB_SIZE = 30000\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE) #filters = ' '\ntokenizer.fit_on_texts(X_train['Cleaned_Text'])\nsequences_train = tokenizer.texts_to_sequences(X_train['Cleaned_Text'])\nsequences_test = tokenizer.texts_to_sequences(X_test['Cleaned_Text'])","9a605a54":"word2idx = tokenizer.word_index\nV = len(word2idx)\nprint(f'Unique Tokens: {V}')","e71b1bc9":"fitted_dict_pairs = {k:word2idx[k] for k in word2idx.keys() if word2idx[k]<MAX_VOCAB_SIZE}","3a65b74c":"import pickle\n  \ntry:\n    write_file = open('Bi-LSTM_Word2Idx', 'wb')\n    pickle.dump(fitted_dict_pairs, write_file)\n    write_file.close()\n  \nexcept:\n    print(\"Something went wrong\")","9072c13c":"#Maximum Length of words chosen\nmax_len = 100 #50","428e6f62":"# pad sequences so that we get a N x T matrix\ndata_train = pad_sequences(sequences_train, maxlen = max_len)\nprint('Shape of data train tensor:', data_train.shape)\n\n# get sequence length\nT = data_train.shape[1]","044305c2":"data_test = pad_sequences(sequences_test, maxlen=T)\nprint('Shape of data test tensor:', data_test.shape)","c91df1b0":"vocab_size = min(MAX_VOCAB_SIZE,V)","5a51e825":"EMBED_DIM = 100\nHIDDEN_DIM = 15","dd8bc818":"ytrain = tf.constant(y_train)\nytest = tf.constant(y_test)","e25f33fa":"BATCH_SIZE = 32","ee1ece4c":"input = Input(shape=(T,))\nx = Embedding(vocab_size, EMBED_DIM, mask_zero = False)(input) #mask_zero = True is rendering a model with similar accuracy but training time exponentially increases so keep this argument as False\nx = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True))(x)\nx = GlobalMaxPooling1D()(x)\noutput = Dense(1, activation='sigmoid')(x)\n\nmodel_1 = Model(input, output)\nmodel_1.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy',tf.keras.metrics.AUC(name='auc')]\n                 )","814b8fe7":"print('Training model...')\nhistory_1 = model_1.fit(\n  data_train,\n  ytrain,\n  epochs = 2,\n  shuffle = True,\n  batch_size = BATCH_SIZE,\n  validation_split = 0.2\n)","e9a12ae5":"# Plot loss per iteration\nplt.plot(history_1.history['loss'], label='loss')\nplt.plot(history_1.history['val_loss'], label='val_loss')\nplt.legend()","76360bcc":"# Plot accuracy per iteration\nplt.plot(history_1.history['accuracy'], label='acc')\nplt.plot(history_1.history['val_accuracy'], label='val_acc')\nplt.legend()","b68d534d":"# Plot auc per iteration\nplt.plot(history_1.history['auc'], label='auc')\nplt.plot(history_1.history['val_auc'], label='val_auc')\nplt.legend()","88d18619":"model_1.evaluate(data_test, ytest)","9af70f1d":"y_pred = model_1.predict(data_test)","dad5fca7":"yPred = np.zeros(shape=(y_pred.shape[0],1))\nfor i,pred in enumerate(y_pred):\n  if pred>=0.5:\n    yPred[i,0] = 1\n  else:\n    yPred[i,0] = 0\ncf_matrix = confusion_matrix(ytest.numpy(), yPred)","d3e45999":"sns.heatmap(cf_matrix, \n            annot=True, \n            fmt='d',\n            xticklabels = ['Negative', 'Positive'],\n            yticklabels = ['Negative', 'Positive'] )","79f357f4":"model_1.save('Sentiment_Analysis_Bi-LSTM')","0d1006db":"model_reloaded = tf.saved_model.load(\".\/Sentiment_Analysis_Bi-LSTM\")","28f2d4d3":"dbfile = open('Bi-LSTM_Word2Idx', 'rb')     \ndb = pickle.load(dbfile)\ndbfile.close()","0e8ed75e":"inference_text_list = ['I want to kill you', \n                       'Omg, he won, yessss', \n                       'Go to hell you idiot',\n                       'You\u2019re scared to tell people how much it hurts, so you keep it all to yourself.',\n                       'As we express our gratitude, we must never forget that the highest appreciation is not to utter words, but to live by them',\n                       'Press on \u2013 nothing can take the place of persistence. Talent will not; nothing is more common than unsuccessful men with talent. Genius will not; unrewarded genius is almost a proverb. Education will not; the world is full of educated derelicts. Perseverance and determination alone are omnipotent.',\n                       'Everyone needs encouragement sometimes, whether it\u2019s to bolster their spirits in a difficult time or simply to remind them that they\u2019re valued. That goes double for your spouse! Choosing just the right words of encouragement can work wonders in making your partner feel loved and appreciated. Post it notes on the mirror, in the car, on the desk, or nightstand, emails to each other throughout a workday \u2013 those just a few ways to express these positive quotes in your marriage.',\n                       'It\u2019s very easy to hurt someone and then say \u201csorry.\u201d But it\u2019s really difficult to get hurt and then say \u201cI m fine\u201d!',\n                       \"Why don't you go fuck yourself\",\n                       'Oh really? Anything else? No No, please keep wasting my time',\n                       'I wish crows feed on your dead body',\n                       'If you die, I will too die mate.',\n                       'There is more to life than finding love and settling down. Live a little',\n                      \"\"\" As you grow older, you realize that a small, cozy cabin in wilderness bears more charm than a huge bungalow with sweeping gardens. You learn that breathing in fresh air, tinged with a lovely earthy scent, makes you feel more at home than holding fancy parties.\n\n                          A book shelf that hugs different lives you have lived. A fireplace where you melt away into a tender mess of memories. An over-expensive bottle of wine that you relish in small sips, lasting it for over a decade. And people you care about, who light up your world just by being there.\n\n                          Tell me, do you not dream of this? To disappear and become one with yourself.\"\"\",\n                        \"In essence, I came out to them, and the notion of having a gay child was so scandalous that they thought I would be 'cured' or get out of the 'phase' if I engaged in a mix of right activities that were appropriately masculine. Like sport, for instance. Or a strict exercise regimen. And I caved in. Not because I wanted to impress them but because I needed a distraction from other things going on in my life (believe me or not, coming out in a heteronormative house was neither the biggest highlight nor the worst stressor of my sad teenage life). \"]\ninference_text_list = [process_text(text) for text in inference_text_list]","3017fe74":"inference_sequence_list = []\nfor text in inference_text_list:\n  tempList = []\n  text = text.split()\n  for token in text:\n    token = token.lower() \n    if token in db.keys():\n      token = db.get(token)\n      tempList.append(token)\n  inference_sequence_list.append(tempList)","3325a83c":"inference_data = pad_sequences(inference_sequence_list, maxlen = max_len)","c324070b":"yPred_reloaded = tf.cast(model_reloaded(list(inference_data)), dtype=tf.float32)\nyPred_reloaded","86f7cbdc":"for prob, text in zip(yPred_reloaded,inference_text_list):\n  if prob>=0.5:\n    print(\"Positive: \", text)\n  else:\n    print(\"Negative: \", text)","5ee73bec":"### **Top 100 words per sentiment reprsented as word cloud**","a93f82a4":"### **Confusion Matrix**","76137dc4":"### **Top 30 words per sentiment**","abfd0324":"### **Save Word-To-Index Mappings for inference**","3e0d8076":"### **Number of unique words per sentiment**","d3758882":"### Let's prepare data to create word clouds of positive and negative tweets","0dbdef1d":"### **Evaluate on test data**\n* **Accuracy**: ~75% \n* **AUC**: ~83%\n* **Note**: Value may change after each run since random seed is not set\n","cc9cbc1b":"### **Convert Text documents into a matrix of tokens**\n*    Add stop words\n*    Fit the data on 'Cleaned_Text'","2229c0a8":"### **Inference Pipeline for Bi-LSTM Model**\n1. Pre-process text to remove unwanted special characters\n3. Loop through the list of texts and map each word\/token of each text to their integer token, append them to the list of integer sequences\n4. Pad the mapped integer sequences using the same pre\/post method used while training the model (here, pre-padding is used) keeping the length of padded sequences consistent with the original model as well\n5. Feed the padded sequences to loaded model for inference\n","441bffcb":"### **Model Architecture: Bi-LSTM**\n\n*  Here, a simple Bidirectional RNN with LSTM with RNN cell is chosen\n*  LSTM is able to capture long term dependencies and capture inform from text as sequences\n*  Before the data is fed to Bidirectional LSTM, it is processed through an embedding layer to get a richer represent of each integer representation of the word\n\n","b1ac687b":"### **Save and download the model**","531cfefe":"## **Initial Imports**","2b769706":"### **Text Preprocessing**","596ec840":"## **NLP Sentiment Analysis**: \n### **Note**: \nIt is recommended to load the pre-trained model and run inference with it to prevent training time overhead. For inference\/testing, skip TRAINING sections and go to the following section:\n**Bi-LSTM: Inference with the reloaded Bi-LSTM model**\n","4073176c":"### **Inference with the reloaded Bi-LSTM model**\n**Note**: Load Word-To-Index Mappings","9c26cc80":"### **Regular Expression to process tweets**","f7309230":"### **Stop words (irrelavent words) that affect visualizations are removed**\n\n\n*   We first load the stop words from nltk package\n*   Upon initial inspection of word clouds, it was revealed that there were common words apart from the ones included in the original stop words list that must also be removed. Therefore we take union and add those words as well\n\n","27a8c5f2":"### **Model Training**\n**Note**: \n1. Only 2 epochs are taken since the model is over-fitting\n2. Average time per epoch = 1 minute + time taken for validation with mask_zero = False","edd60cc8":"## **In this section, we will experiment and create deep learning model architectures for our dataset**","1d8fc64d":"### **Following are the steps taken in the workflow to create our first Model**:\n\n\n1.   Preprocess textual data and convert into tokens, then into 'integer sequences' for the deep learning model. It is worth noting that the data is fitted on training data to prevent data leakage\n2.   Since our model takes a sequence or tensor of constant size, we convert the 'integer sequences' into a constant size of length 50 (value taken experimentally) \n3.   Long sequences are truncated; short sequences are padded with '0' values\n","26a80888":"### **Function to process individual tweet**","27e95509":"## **Load and Format Data**\n**Note**: \n1. The dataset (in the given csv format) must be uploaded to the google colab directory from your local file directory to run the notebook else it will raise an exception. \n\n2. Please ensure to copy the path to the file in colab directory and paste within pd.read_csv() as file address\n\n"}}