{"cell_type":{"620811e3":"code","db0ec94b":"code","12180774":"code","7c0c4699":"code","967bd5a9":"code","92d01167":"code","553c2fa5":"code","14bffb81":"code","0a07d2b4":"code","157259c8":"code","7545db4a":"code","5e7aa7ac":"code","ffd63bc6":"code","91af71a8":"code","2cea8c09":"code","33ddcef9":"code","c7686aea":"code","9607cfba":"code","6a5e904a":"code","2e9ce51c":"code","6f787c41":"code","e583b2d2":"code","3c2c7e4e":"markdown","68fa8972":"markdown","8d95122b":"markdown","92db6a35":"markdown","e296f843":"markdown","9ad8659e":"markdown"},"source":{"620811e3":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","db0ec94b":"# Import train and test data:\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","12180774":"# Split imported train dataset into train and dev (cross_validation) data\ntrain_data, dev_data = train_test_split(train_data, test_size=0.25)","7c0c4699":"# review examples of data \ntrain_data.head()","967bd5a9":"# review information on data, pay attention to missing values and dtypes\ntrain_data.info()\nprint(\"_\" * 40)\ndev_data.info()\nprint(\"_\" * 40)\ntest_data.info()","92d01167":"# look up the passenger with missing Fare value in the test_data: \ntest_data[test_data.Fare.isnull()]","553c2fa5":"# make an estimate of the missing Fare value for the above passenger in 3rd class embarked at S: \"13.9\"\ntest_data[[\"Pclass\", \"Fare\", \"Embarked\"]].groupby([\"Pclass\", \"Embarked\"]).mean()","14bffb81":"# exclude data of weak correlation with survival and with many missing values (don't spend time on new features for this base model):\nX_train = train_data.drop([\"PassengerId\", \"Survived\", \"Name\", \"Age\", \"Ticket\", \"Cabin\"], axis=1)\nY_train = train_data[\"Survived\"]\nX_dev = dev_data.drop([\"PassengerId\", \"Survived\", \"Name\", \"Age\", \"Ticket\", \"Cabin\"], axis=1)\nY_dev = dev_data[\"Survived\"]\nX_test = test_data.drop([\"PassengerId\", \"Name\", \"Age\", \"Ticket\", \"Cabin\"], axis=1)\nX_train.shape, Y_train.shape, X_dev.shape, Y_dev.shape, X_test.shape","0a07d2b4":"# convert alpha-numerical data to numbers in Sex and Embarked, and fill in null data in Embarked and Fare\nX_full = [X_train, X_dev, X_test]\nfor dataset in X_full:\n    dataset[\"Sex\"] = dataset[\"Sex\"].map({\"female\": \"1\", \"male\": \"0\"}).astype(\"int\")\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\").map({\"S\": \"0\", \"C\": \"1\", \"Q\": \"2\"}).astype(\"int\")\nX_test[\"Fare\"] = X_test[\"Fare\"].fillna(13.9)","157259c8":"# Train a Logistic regresson model and predict survival on dev data\nlogit = LogisticRegression()\nlogit.fit(X_train, Y_train)\nacc_logit_train = round(logit.score(X_train, Y_train)*100, 2)\nacc_logit_dev = round(logit.score(X_dev, Y_dev)*100, 2)\nprint(f\"logit: train accuracy = {acc_logit_train}, dev accuracy = {acc_logit_dev}\")","7545db4a":"# Train a Support Vextor Machine n model and predict survival on dev data\nsvc = SVC(C=1.0, kernel='rbf', gamma='auto')\nsvc.fit(X_train, Y_train)\nacc_svc_train = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc_dev = round(svc.score(X_dev, Y_dev) * 100, 2)\nprint(f\"svc: train accuracy = {acc_svc_train}, dev accuracy = {acc_svc_dev}\")","5e7aa7ac":"# Let's now create new features and compare their effect on the model performance. \n# Let's start with extracting titles from Names and adding them as a new feature:\ncombine_data = [train_data, dev_data, test_data]\nfor dataset in combine_data:\n    dataset[\"Title\"] = dataset.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\npd.crosstab(train_data['Title'], train_data['Sex'])","ffd63bc6":"pd.crosstab(test_data['Title'], test_data['Sex'])","91af71a8":"# Replace rare titles with more common ones\nfor dataset in combine_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","2cea8c09":"# Map titles to categories (numbers) for fitting the model\nfor dataset in combine_data:\n    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5})\n    dataset['Title'] = dataset['Title'].fillna(0)","33ddcef9":"# Add the Title feature to our model data\nX_train[\"Title\"] = train_data[\"Title\"]\nX_dev[\"Title\"] = dev_data[\"Title\"]\nX_test[\"Title\"] = test_data[\"Title\"]\nX_train.shape, Y_train.shape, X_dev.shape, Y_dev.shape, X_test.shape\n","c7686aea":"# Now we can train logit on expanded data and the effect on accuracy:\nlogit.fit(X_train, Y_train)\nacc_logit_train = round(logit.score(X_train, Y_train)*100, 2)\nacc_logit_dev = round(logit.score(X_dev, Y_dev)*100, 2)\nprint(f\"logit: train accuracy = {acc_logit_train}, dev accuracy = {acc_logit_dev}\")","9607cfba":"svc.fit(X_train, Y_train)\nacc_svc_train = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc_dev = round(svc.score(X_dev, Y_dev) * 100, 2)\nprint(f\"svc: train accuracy = {acc_svc_train}, dev accuracy = {acc_svc_dev}\")","6a5e904a":"# Let's normalize Fares data with mean and standard deviation of X_train:\nmu = X_train[\"Fare\"].mean()\nsigma = (((X_train[\"Fare\"]-mu)**2).mean())**0.5\nfor subset in X_full:\n    subset[\"Fare\"] = (subset[\"Fare\"] - mu)\/sigma\nprint(mu, sigma)","2e9ce51c":"# Now we can train logit with normalized \"Fare\" data and see the effect on accuracy:\nlogit.fit(X_train, Y_train)\nacc_logit_train = round(logit.score(X_train, Y_train)*100, 2)\nacc_logit_dev = round(logit.score(X_dev, Y_dev)*100, 2)\nprint(f\"logit: train accuracy = {acc_logit_train}, dev accuracy = {acc_logit_dev}\")","6f787c41":"# Support Vector Machines\nsvc.fit(X_train, Y_train)\nacc_svc_train = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc_dev = round(svc.score(X_dev, Y_dev) * 100, 2)\nprint(f\"svc: train accuracy = {acc_svc_train}, dev accuracy = {acc_svc_dev}\")","e583b2d2":"### Choose predictions on test dataset:\n#Y_pred = logit.predict(X_test)\nY_pred = svc.predict(X_test)\n\n### Form the submission file:\nsubmission = pd.DataFrame({\"PassengerId\": test_data[\"PassengerId\"], \"Survived\": Y_pred}).sort_values(by=\"PassengerId\")\nsubmission.to_csv(\"submission.csv\", index=False)","3c2c7e4e":"\n**DESCRIPTION**\n----------\n1. A common sense approach to a new ML problem would be to build a 'quick' model with just a few lines of code to get a baseline accuracy even before engineering any new features from the available data. Otherwise, you can spend many long hours on inventing new features without any measure of effectivenes of those features.\n2. In this quick model based on the Titanic dataset, I just dropped all the data of weak correlation with survival and trained simple built-in logistic regression and svm from sklearn. \n3. Then we can further wrangle the data, create new features and compare their effect against the baseline. See below some examples.","68fa8972":"** DATA: SECOND STEP - ADD 'TITLE' FEATURE AND SEE EFFECT **\n--------------","8d95122b":"** DATA: FIRST STEP - QUICKLY DROP WEAK FEATURES **\n----------------","92db6a35":"** SUBMISSION **\n---------","e296f843":"** DATA: THIRD STEP - NORMALIZE \"FARE\" FEATURE AND SEE EFFECT **\n--------------","9ad8659e":"** ML MODELS - EVALUATE ON SIX FEATURES ONLY **\n-----"}}