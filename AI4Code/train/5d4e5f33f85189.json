{"cell_type":{"2020f679":"code","7eee9c02":"code","95069e22":"code","2f747ae5":"code","a4bde4b5":"code","7ebcf4af":"code","638b2eb9":"code","e7cba503":"code","0c91786c":"code","ed1d6b07":"code","342c1099":"code","fbdfa634":"code","11c65022":"code","bf17b3fd":"code","c98ad228":"code","197cc7d0":"code","8021c358":"code","a0e06254":"code","be8f82a3":"code","242e6dd6":"code","47879ffe":"markdown","1dcc5d35":"markdown","8958dc34":"markdown","bfcf887e":"markdown","3bb0bd5b":"markdown","8f336240":"markdown","97f26e9b":"markdown","e16c0203":"markdown","f909742f":"markdown","b5cb8e31":"markdown","28e025bc":"markdown","7d95c2a7":"markdown"},"source":{"2020f679":"!pip install torchvision","7eee9c02":"!pip uninstall -y transformers\n!pip install transformers","95069e22":"import os\nimport re\nimport json\nimport string\nimport numpy as np\nimport torch\nimport pickle\n\nimport tensorflow as tf\nfrom tensorflow import keras as K\n\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel","2f747ae5":"# Detect hardware, return appropriate distribution strategy.\n# You can see that it is pretty easy to set up.\ntry:\n    # TPU detection: no parameters necessary if TPU_NAME environment\n    # variable is set (always set in Kaggle)\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)","a4bde4b5":"DATASET_URL = 'https:\/\/raw.githubusercontent.com\/deepmind\/xquad\/99910ec0f10151652f6726282ca922dd8eb0207a\/xquad.en.json'\nMODEL_NAME = 'bert-base-multilingual-cased'\nMAX_LEN = 384\n\n# Set language name to save model\nLANGUAGE = 'english'\n\n# Depends on whether we are using TPUs or not, increase BATCH_SIZE\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\n\n# Detect environment\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE',''):\n    print('Detected Kaggle environment')\n    ARTIFACTS_PATH = 'artifacts\/'\nelse:\n    ARTIFACTS_PATH = '..\/artifacts\/'\n    \nif not os.path.exists(ARTIFACTS_PATH):\n    os.makedirs(ARTIFACTS_PATH)","7ebcf4af":"# Import tokenizer from HuggingFace\nslow_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n\nsave_path = '%s%s-%s\/' % (ARTIFACTS_PATH, LANGUAGE, MODEL_NAME)\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\n\nslow_tokenizer.save_pretrained(save_path)\n\n# You can already use the Slow Tokenizer, but its implementation in Rust is much faster.\ntokenizer = BertWordPieceTokenizer('%s\/vocab.txt' % save_path, lowercase=True)","638b2eb9":"# This code is a modified version from https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/\nclass SquadExample:\n    def __init__(\n        self,\n        question,\n        context,\n        start_char_idx,\n        answer_text,\n        all_answers,\n        tokenizer\n    ):\n        self.question = question\n        self.context = context\n        self.start_char_idx = start_char_idx\n        self.answer_text = answer_text\n        self.all_answers = all_answers\n        self.tokenizer = tokenizer\n        self.skip = False\n\n    def preprocess(self):\n        context = self.context\n        question = self.question\n        answer_text = self.answer_text\n        start_char_idx = self.start_char_idx\n\n        # Fix white spaces\n        context = re.sub(r\"\\s+\", ' ', context).strip()\n        question = re.sub(r\"\\s+\", ' ', question).strip()\n        answer = re.sub(r\"\\s+\", ' ', answer_text).strip()\n\n        # Find end token index of answer in context\n        end_char_idx = start_char_idx + len(answer)\n        if end_char_idx >= len(context):\n            self.skip = True\n            return\n\n        # Mark the character indexes in context that are in answer\n        is_char_in_ans = [0] * len(context)\n        for idx in range(start_char_idx, end_char_idx):\n            is_char_in_ans[idx] = 1\n\n        # Encode context (token IDs, mask and token types)\n        tokenized_context = tokenizer.encode(context)\n\n        # Find tokens that were created from answer characters\n        ans_token_idx = []\n        for idx, (start, end) in enumerate(tokenized_context.offsets):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            self.skip = True\n            return\n\n        # Find start and end token index for tokens from answer\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n\n        # Encode question (token IDs, mask and token types)\n        tokenized_question = tokenizer.encode(question)\n\n        # Create inputs\n        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n            tokenized_question.ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = MAX_LEN - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.start_token_idx = start_token_idx\n        self.end_token_idx = end_token_idx\n        self.context_token_to_char = tokenized_context.offsets","e7cba503":"def create_squad_examples(raw_data, tokenizer):\n    squad_examples = []\n    for item in raw_data[\"data\"]:\n        for para in item[\"paragraphs\"]:\n            context = para[\"context\"]\n            for qa in para[\"qas\"]:\n                question = qa[\"question\"]\n                answer_text = qa[\"answers\"][0][\"text\"]\n                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n                squad_eg = SquadExample(\n                    question,\n                    context,\n                    start_char_idx,\n                    answer_text,\n                    all_answers,\n                    tokenizer\n                )\n                squad_eg.preprocess()\n                squad_examples.append(squad_eg)\n    return squad_examples\n\n\ndef create_inputs_targets(squad_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for item in squad_examples:\n        if item.skip == False:\n            for key in dataset_dict:\n                dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y","0c91786c":"dataset_path = K.utils.get_file('dataset.json', DATASET_URL)","ed1d6b07":"with open(dataset_path) as fp:\n    raw_data = json.load(fp)","342c1099":"# Split into train and test sets\nraw_train_data = {}\nraw_eval_data = {}\nraw_train_data['data'], raw_eval_data['data'] = np.split(np.asarray(raw_data['data']), [int(.8*len(raw_data['data']))])","fbdfa634":"train_squad_examples = create_squad_examples(raw_train_data, tokenizer)\nx_train, y_train = create_inputs_targets(train_squad_examples)\nprint(f\"{len(train_squad_examples)} training points created.\")\n\neval_squad_examples = create_squad_examples(raw_eval_data, tokenizer)\nx_eval, y_eval = create_inputs_targets(eval_squad_examples)\nprint(f\"{len(eval_squad_examples)} evaluation points created.\")","11c65022":"def create_model():\n    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype=tf.int32)\n    token_type_ids = tf.keras.layers.Input(shape=(MAX_LEN,), name='token_type_ids', dtype=tf.int32)\n    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype=tf.int32)\n    \n    encoder = TFBertModel.from_pretrained(MODEL_NAME)\n    x = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n    \n    # Huggingface transformers have multiple outputs, embeddings are the first one,\n    # so let's slice out the first position.\n    x = x[0]\n\n    # Define two outputs\n    start_logits = tf.keras.layers.Dense(1, name='start_logit', use_bias=False)(x)\n    start_logits = tf.keras.layers.Flatten()(start_logits)\n\n    end_logits = tf.keras.layers.Dense(1, name='end_logit', use_bias=False)(x)\n    end_logits = tf.keras.layers.Flatten()(end_logits)\n\n    # Normalize outputs with softmax\n    start_probs = tf.keras.layers.Activation(tf.keras.activations.softmax, name='start_probs')(start_logits)\n    end_probs = tf.keras.layers.Activation(tf.keras.activations.softmax, name='end_probs')(end_logits)\n\n    model = tf.keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","bf17b3fd":"with strategy.scope():\n    model = create_model()\n    model.summary()","c98ad228":"# Source: https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/\nclass ExactMatch(tf.keras.callbacks.Callback):\n    def __init__(self, x_eval, y_eval):\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            squad_eg = eval_examples_no_skip[idx]\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            \n            # Get answer from context text\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.context[pred_char_start:]\n\n            # Normalize answers before comparing prediction and true answers\n            normalized_pred_ans = self._normalize_text(pred_ans)\n            normalized_true_ans = [self._normalize_text(_) for _ in squad_eg.all_answers]\n            \n            # If the prediction is contained in the true answer, it counts as a hit\n            if normalized_pred_ans in normalized_true_ans:\n                count += 1\n\n        acc = count \/ len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n    \n    def _normalize_text(self, text):\n        text = text.lower()\n\n        # Remove punctuations\n        exclude = set(string.punctuation)\n        text = ''.join(ch for ch in text if ch not in exclude)\n\n        # Remove articles\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        text = re.sub(regex, ' ', text)\n\n        # Remove extra white spaces\n        text = re.sub(r\"\\s+\", ' ', text).strip()\n\n        return text","197cc7d0":"EPOCHS = 8\n\nwith strategy.scope():\n    exact_match_callback = ExactMatch(x_eval, y_eval)\n    model.fit(\n        x_train,\n        y_train,\n        epochs=EPOCHS,\n        verbose=1,\n        batch_size=BATCH_SIZE,\n        callbacks=[exact_match_callback],\n    )","8021c358":"import pickle\n\nweigh = model.get_weights()\npklfile = '%s%s-%s.pickle' % (ARTIFACTS_PATH, LANGUAGE, MODEL_NAME)\n\nwith open(pklfile, 'wb') as fp:\n    pickle.dump(weigh, fp, protocol= pickle.HIGHEST_PROTOCOL)","a0e06254":"pklfile = '%s%s-%s.pickle' % (ARTIFACTS_PATH, LANGUAGE, MODEL_NAME)\nwith open(pklfile, 'rb') as fp:\n    data = pickle.load(fp)\n    model.set_weights(data)","be8f82a3":"def get_answer_question(question, context, model, tokenizer):\n    # Fix white spaces\n    context = re.sub(r\"\\s+\", ' ', context).strip()\n    question = re.sub(r\"\\s+\", ' ', question).strip()\n\n    # Encode context (token IDs, mask and token types)\n    tokenized_context = tokenizer.encode(context)\n    tokenized_question = tokenizer.encode(question)\n\n    # Create inputs\n    input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n    token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n        tokenized_question.ids[1:]\n    )\n    attention_mask = [1] * len(input_ids)\n\n    # Pad and create attention masks.\n    padding_length = MAX_LEN - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        attention_mask = attention_mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n    elif padding_length < 0:\n        raise Exception('Too long!')\n\n    input_ids = np.asarray(input_ids, dtype='int32')\n    token_type_ids = np.asarray(token_type_ids, dtype='int32')\n    attention_mask = np.asarray(attention_mask, dtype='int32')\n        \n    encoded_input = [\n        np.asarray([input_ids]),\n        np.asarray([token_type_ids]),\n        np.asarray([attention_mask])\n    ]\n    \n    # Get prediction of answer for the given question and context.\n    pred_start, pred_end = model.predict(encoded_input)\n    \n    start = np.argmax(pred_start[0])\n    end = np.argmax(pred_end[0])\n    \n    offsets = tokenized_context.offsets\n    if start >= len(offsets):\n        print('Cannot capture answer.')\n\n    pred_char_start = offsets[start][0]\n    if end < len(offsets):\n        pred_char_end = offsets[end][1]\n        pred_ans = context[pred_char_start:pred_char_end]\n    else:\n        pred_ans = context[pred_char_start:]\n\n    # Remove extra white spaces\n    normalized_pred_ans = re.sub(r\"\\s+\", ' ', pred_ans).strip()\n    \n    return normalized_pred_ans","242e6dd6":"# Write a context and a question about the context\ncontext = \"The Bas\u00edlica de la Sagrada Familia is a large unfinished Roman Catholic minor basilica in the Eixample district of Barcelona, Spain. Designed by Spanish architect Antoni Gaud\u00ed (1852-1926), his work on the building is part of a UNESCO World Heritage Site. On 7 November 2010, Pope Benedict XVI consecrated the church and proclaimed it a minor basilica.\"\nquestion = 'Who designed the Sagrada Familia?'\n\n# Import textwrap library to display context\nimport textwrap\nwrapper = textwrap.TextWrapper(width=80) \n\n# Display\nprint('='*6, ' TEXT ', '='*6)\nprint(wrapper.fill(context))\nprint('='*21)\n\nprint('='*5, 'QUESTION ', '='*5)\nprint(question)\nprint('='*21)\n\n# Infer answer\nanswer = get_answer_question(question, context, model, tokenizer)\nprint('='*5, ' ANSWER ', '='*6)\nprint(answer)","47879ffe":"### Load model weights\n\nOnce you have your model trained, you can load its weights and start using it. Note that you must initialize the tokenizer too!","1dcc5d35":"### Save model weights","8958dc34":"## Tokenize & encode\n\nBERT expects 3 different inputs:\n- Token IDs: the tokens transformed into numbers.\n- Mask: sequence of `0` (if there are PAD tokens in that position) and `1` (otherwise).\n- Segments or Type IDs: sequence of `0` and `1` to distinguish between the first and the second sentence in NSP tasks. In this notebook, we do not need this input, so it will be always `0`.\n\nFor example:\n```\nText:       Is this jacksonville? Yes, it is.\n---------------------------------------------------------------------------------\nTokens:     [CLS] Is    this  ja    ##cks ##on  ##ville ?   [SEP] Yes   ,    it   is   .    [SEP] [PAD] [PAD] ...\nToken IDs:  101   12034 10531 10201 18676 10263 12043   136 102   2160  117  1122 1110 119  102   100   100   ...\nMask:       1     1     1     1     1     1     1       1   1     1     1    1    1    1    1     0     0     ...\nType IDs:   0     0     0     0     0     0     0       0   0     1     1    1    1    1    1     0     0     ...\n```\n\nNotes:\n- Token IDs may be different depending on the tokenizer.\n- I have manually introduced `[SEP]` between both sentences.\n\nFor further details, see BERT paper: https:\/\/arxiv.org\/pdf\/1810.04805.pdf","bfcf887e":"# Build model\n\nThe model should be capable to inform where is the answer in the context. In other words, it has to return the start and end token positions of the answer in the context.","3bb0bd5b":"### Constants\n\n**Dataset.** We will use XQuAD (Cross-lingual Question Answering Dataset), which consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 together with their professional translations into 10 languages. See: https:\/\/github.com\/deepmind\/xquad\n\n**Pretrained BERT Model.** Since we are going to fine-tune a model to answer questions in a specific language, we need a pretrained BERT Model in such language. We will import BERT Multilingual Base model which has been pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. See: https:\/\/huggingface.co\/bert-base-multilingual-cased","8f336240":"## Answering","97f26e9b":"### Evaluation callback\n\nEach `SquadExample` object contains the character level offsets for each token in its input paragraph. We use them to get back the span of text corresponding to the tokens between our predicted start and end tokens. All the ground-truth answers are also present in each `SquadExample` object. We calculate the percentage of data points where the span of text obtained from model predictions matches one of the ground-truth answers.","e16c0203":"# Question-answering with BERT\n\nIn Question-Answering tasks, the model receives a text and a question regarding to the text, and it should mark the beginning and end of the answer in the text.","f909742f":"## Prepare and import modules\n\nWe need to install a few modules:","b5cb8e31":"### Infer answer","28e025bc":"And, then, we can import all the necessary modules.","7d95c2a7":"### Training"}}