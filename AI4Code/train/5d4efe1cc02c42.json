{"cell_type":{"2c64b254":"code","b22957a0":"code","b59efc9d":"code","c1bde94a":"code","0e12ab55":"code","dd0239b8":"code","de0ecd28":"code","cfb7c8f3":"code","188b29ae":"code","a66c6715":"code","d4c44c71":"code","1083ca04":"code","e373693b":"code","784ec122":"code","105b7be9":"code","faefd84d":"code","dd1a6e24":"code","52010df9":"code","83236284":"markdown","0216af24":"markdown","8fa489ee":"markdown","d0eddd00":"markdown","c420a203":"markdown","a690f050":"markdown","4a58a778":"markdown","b5da1660":"markdown","a15c28c2":"markdown","3540f847":"markdown"},"source":{"2c64b254":"import numpy as np\nimport pandas as pd\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport itertools\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input, BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nSEED = 2021\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","b22957a0":"TRAIN_PATH = \"..\/input\/fashionmnist\/fashion-mnist_train.csv\"\nTEST_PATH = \"..\/input\/fashionmnist\/fashion-mnist_test.csv\"\n\nTARGET = \"label\"\nSUBMIT_TARGET = \"Label\"\nTEST_SIZE = 0.2\nLABEL_NUM = 10\n\nIMAGE_SIZE_X = 28\nIMAGE_SIZE_Y =28\nSCALE_SIZE = 255","b59efc9d":"train = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","c1bde94a":"len(train),len(test)","0e12ab55":"train.isnull().sum()","dd0239b8":"test.isnull().sum()","de0ecd28":"y = train[TARGET]\nX = train.drop([TARGET],axis=1)\n\ny_test = test[TARGET]\nX_test = test.drop([TARGET],axis=1)","cfb7c8f3":"X = X\/SCALE_SIZE\nX_test = X_test\/SCALE_SIZE\n\nX.shape,X_test.shape","188b29ae":"X = X.values.reshape(-1,IMAGE_SIZE_X,IMAGE_SIZE_Y,1)\nX.shape","a66c6715":"X_test = X_test.values.reshape(-1,IMAGE_SIZE_X,IMAGE_SIZE_Y,1)\nX_test.shape","d4c44c71":"y = to_categorical(y, num_classes=LABEL_NUM)","1083ca04":"X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=TEST_SIZE,random_state=SEED )","e373693b":"BATCH_SIZE = 64 \nEPOCH = 5\nLEARNING_RATE = 0.001\n\nKERNEL_INITIALIIZERS = tf.keras.initializers.GlorotNormal(seed=SEED)\nPADDING = \"Same\"\nMID_ACTIVATION = \"relu\"\nLAST_ACTIVATION = \"softmax\"\n\nCOMPILE_LOSS = \"categorical_crossentropy\"\nCOMPILE_METRICS = ['accuracy']\nRHO = 0.9\nEPSILON = 1e-08\nDROPOUT = 0.25\nVERBOSE = 1","784ec122":"def define_model():\n    model = Sequential()\n\n    model.add(Input(shape=(IMAGE_SIZE_X,IMAGE_SIZE_Y,1)))\n    model.add(Conv2D(32, kernel_size=(5,5), kernel_initializer=KERNEL_INITIALIIZERS, \n                     padding=PADDING, activation=MID_ACTIVATION))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(DROPOUT, seed=SEED))\n\n    model.add(Conv2D(64, kernel_size=(5,5), kernel_initializer=KERNEL_INITIALIIZERS, \n                     padding=PADDING, activation=MID_ACTIVATION))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(DROPOUT, seed=SEED))\n\n    model.add(Flatten())\n    model.add(Dense(256, kernel_initializer=KERNEL_INITIALIIZERS, activation=MID_ACTIVATION))\n    model.add(Dropout(DROPOUT, seed=SEED))\n    model.add(BatchNormalization())\n    model.add(Dense(LABEL_NUM, kernel_initializer=KERNEL_INITIALIIZERS, activation=LAST_ACTIVATION))\n    \n    decay= 5 * LEARNING_RATE \/ EPOCH\n    optimizer = RMSprop(learning_rate=LEARNING_RATE, rho=RHO, epsilon=EPSILON, decay=decay)\n    \n    model.compile(optimizer=optimizer,loss=COMPILE_LOSS,metrics=COMPILE_METRICS)\n    return model\n\nmodel = define_model()\nmodel.summary()","105b7be9":"train_datagen = ImageDataGenerator(featurewise_center=False,\n                             samplewise_center=False,\n                             featurewise_std_normalization=False,\n                             samplewise_std_normalization=False,\n                             zca_whitening=False,\n                             rotation_range=10,\n                             zoom_range=0.1,\n                             width_shift_range=0.1,\n                             height_shift_range=0.1,\n                             horizontal_flip=False,\n                             vertical_flip=False\n                            )\ntrain_generator = train_datagen.flow(X_train, y_train,\n                                     batch_size=BATCH_SIZE,\n                                     shuffle=True)\n\nval_datagen = ImageDataGenerator()\nval_generator = val_datagen.flow(X_val, y_val,\n                                 batch_size=BATCH_SIZE,\n                                 shuffle=True)\n\nreduceLROnPlateau = ReduceLROnPlateau(monitor='val_acc', \n                                patience=3,\n                                verbose=VERBOSE, \n                                factor=0.5,\n                                min_lr=0.00001)\n\nhistory = model.fit(train_generator,\n                    epochs= EPOCH,\n                    validation_data=val_generator,\n                    verbose=VERBOSE,\n                    callbacks=[reduceLROnPlateau]\n                   )","faefd84d":"results = model.predict(X_test)\nresults = np.argmax(results,axis = 1)\nresults[:5]","dd1a6e24":"y_test[:5]","52010df9":"np.sum(results == y_test)\/len(y_test)","83236284":"# global variables","0216af24":"# define model","8fa489ee":"# split data (input data and target data)","d0eddd00":"# evaluate Model","c420a203":"# train model","a690f050":"# model variables","4a58a778":"# scaling and transformation","b5da1660":"# load data","a15c28c2":"# predict test data target","3540f847":"# split data (train set and validation set)"}}