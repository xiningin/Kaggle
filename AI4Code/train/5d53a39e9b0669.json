{"cell_type":{"6d85c0ab":"code","1916eab2":"code","cee71a3e":"code","1d503dad":"code","9fdc896f":"code","dedbba2e":"code","0824a103":"code","ca422001":"code","10044752":"code","da79a868":"code","145f54ce":"code","d94a89c8":"code","29495221":"code","0fd2d2c0":"code","5f48cd1a":"code","bd404122":"code","dbee8cc3":"code","a5b07c72":"code","ed2c0ec5":"code","2ef333aa":"code","341f1bd2":"code","3e6916cd":"code","21b19a26":"code","649b5a7c":"code","23318e9a":"code","220e4387":"code","2d739b72":"code","05029bb6":"code","f5d499c3":"code","668dfa4b":"code","890ed458":"code","032b14c0":"code","b6a67ef0":"code","7c0c7413":"code","43e663ce":"code","d4943bbf":"code","cb293ea8":"code","94140041":"code","dc4c55f1":"code","57ce8b5d":"code","396bb60d":"code","0e16d2b5":"code","0ff21701":"code","25d11a44":"code","7240601d":"code","7723ec42":"code","ffa75a95":"code","38815060":"code","a4ad0bb8":"code","126fe7f2":"code","b64bac6e":"code","4ffd9879":"code","62ec6394":"code","a1317317":"code","854c28da":"code","22c2fb3b":"code","08551f46":"code","d508ebb1":"code","bd8bb4d3":"code","ac0f6a60":"code","259c62c3":"markdown","0af7167a":"markdown","fc9bb83a":"markdown"},"source":{"6d85c0ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1916eab2":"import tensorflow as tf","cee71a3e":"!nvidia-smi","1d503dad":"df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")","9fdc896f":"df.head()","dedbba2e":"df = df.drop([\"standard_error\"],axis=1)","0824a103":"df","ca422001":"import random","10044752":"import tensorflow as tf","da79a868":"\nind = random.randint(1,2834-5)\n\ndf.iloc[ind].target,df.iloc[ind].excerpt\nexcerpt = df.iloc[ind].excerpt","145f54ce":"list(excerpt),excerpt.split()","d94a89c8":"df[\"excerpt\"] = df[\"excerpt\"].apply(lambda text: text.replace(\"\\n\",\" \"))\ndf[\"length\"] = df[\"excerpt\"].apply(lambda text: len(text.split()))","29495221":"df[\"length\"] = df[\"excerpt\"].apply(lambda excerpt: len(excerpt.split()))\ndf[\"char_lengths\"] = df[\"excerpt\"].apply(lambda excerpt: len(list(excerpt)))\ndf[\"chars\"] = df[\"excerpt\"].apply(lambda excerpt: \" \".join(list(excerpt)))","0fd2d2c0":"char_lengths = df[\"char_lengths\"].to_numpy()","5f48cd1a":"lengths = df[\"length\"].to_numpy()","bd404122":"print(np.percentile(lengths,95))\nprint(np.percentile(char_lengths,95))","dbee8cc3":"df.describe()","a5b07c72":"from tensorflow.keras.layers.experimental.preprocessing import TextVectorization","ed2c0ec5":"import re","2ef333aa":"df.describe()","341f1bd2":"max_length = 198\nmax_char_length = 1161","3e6916cd":"len(df)-567","21b19a26":"test_df = df[-567:]\ntrain_df = df[:2267]","649b5a7c":"train_excerpts  = train_df[\"excerpt\"].to_numpy()\ntrain_chars = train_df[\"chars\"].to_numpy()\ntrain_targets = train_df[\"target\"].to_numpy()\n\ntest_excerpts = test_df[\"excerpt\"].to_numpy()\ntest_chars = test_df[\"chars\"].to_numpy()\ntest_targets = test_df[\"target\"].to_numpy()\n","23318e9a":"train_dataset_values = tf.data.Dataset.from_tensor_slices((train_excerpts,train_chars))\ntrain_dataset_labels = tf.data.Dataset.from_tensor_slices(train_targets)\ntrain_dataset = tf.data.Dataset.zip((train_dataset_values,train_dataset_labels))\ntrain_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntest_dataset_values = tf.data.Dataset.from_tensor_slices((test_excerpts,test_chars))\ntest_dataset_labels = tf.data.Dataset.from_tensor_slices(test_targets)\ntest_dataset = tf.data.Dataset.zip((test_dataset_values,test_dataset_labels))\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","220e4387":"train_dataset","2d739b72":"vocab_size = 26924","05029bb6":"import string\nalphabet = string.ascii_lowercase + string.digits","f5d499c3":"len(alphabet)","668dfa4b":"num_alphabet_tokens = len(alphabet) + 2","890ed458":"vectorizer = TextVectorization(max_tokens=vocab_size,output_sequence_length=max_length)\ncharacter_vectorizer = TextVectorization(max_tokens=num_alphabet_tokens,output_sequence_length=1161)\nvectorizer.adapt(train_excerpts)\ncharacter_vectorizer.adapt(train_chars)\nlen(vectorizer.get_vocabulary())\n","032b14c0":"len(character_vectorizer.get_vocabulary())","b6a67ef0":"#token level embeddings\nembedding = tf.keras.layers.Embedding(input_dim=26748,output_dim=128,input_length=max_length)\ncharacter_embedding = tf.keras.layers.Embedding(input_dim=38,output_dim=128,input_length=max_char_length)","7c0c7413":"from tensorflow.keras import layers\nfrom tensorflow import keras","43e663ce":"token_inputs = layers.Input(shape=(1,),dtype=tf.string,name=\"Token Input Layer\")\nchar_inputs = layers.Input(shape=(1,),dtype=tf.string,name=\"Character Input layer\")\ntoken_vectorizer = vectorizer(token_inputs)\ntoken_embeddings = embedding(token_vectorizer)\ndropout = layers.Dropout(0.5)(token_embeddings)\nx = layers.Bidirectional(layers.LSTM(64,return_sequences=True))(dropout)\nx = layers.Bidirectional(layers.LSTM(64))(x)\ntoken_outputs = layers.Dense(128,activation=\"relu\")(x)\ntoken_model = keras.Model(token_inputs,token_outputs)\n\nchar_vectorizer = character_vectorizer(char_inputs)\nchar_embeddings = character_embedding(char_vectorizer)\ndropout2 = layers.Dropout(0.5)(char_embeddings)\nx = layers.Bidirectional(layers.LSTM(64,return_sequences=True))(dropout2)\nx = layers.Bidirectional(layers.LSTM(64))(x)\nchar_outputs = layers.Dense(128,activation=\"relu\")(x)\nchar_model = keras.Model(char_inputs,char_outputs)\n\nconcat = layers.Concatenate()([token_model.output,char_model.output])\nx = layers.Dropout(0.5)(concat)\nx = layers.Dense(64,activation=\"relu\")(x)\noutputs = layers.Dense(1)(x)\n","d4943bbf":"model_6 = tf.keras.Model([token_model.input,char_model.input],outputs)","cb293ea8":"model_6.summary()","94140041":"from tensorflow.keras.utils import plot_model","dc4c55f1":"plot_model(model_6)","57ce8b5d":"model_6.compile(loss=tf.keras.losses.mse,optimizer=tf.keras.optimizers.Adam(),metrics=tf.keras.metrics.RootMeanSquaredError())","396bb60d":"import tensorflow_hub as hub","0e16d2b5":"import datetime","0ff21701":"def create_tensorboard_callback(dir_name,experiment_name):\n  \"\"\"\n  Creates a TensorBoard callback instand to store log files.\n  Stores log files with the filepath:\n    \"dir_name\/experiment_name\/current_datetime\/\"\n  Args:\n    dir_name: target directory to store TensorBoard log files\n    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n  \"\"\"\n  log_dir = dir_name + \"\/\" + experiment_name + \"\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=log_dir\n  )\n  print(f\"Saving TensorBoard log files to: {log_dir}\")\n  return tensorboard_callback","25d11a44":"model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"checkpoint\/check.ckpt\",monitor=\"val_root_mean_squared_error\",save_best_only=True,save_weights_only=True)","7240601d":"tf.random.set_seed(42)\nmodel_6.fit(train_dataset,epochs=4,validation_data=(test_dataset),callbacks=[create_tensorboard_callback(\"CommonLit\",\"CNN-BiLSTM_V2\"),model_checkpoint])","7723ec42":"ind = random.randint(1,2834-5)\ndf.iloc[ind:ind+5]","ffa75a95":"df.iloc[0].excerpt","38815060":"df_test = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")","a4ad0bb8":"df_test","126fe7f2":"df_test[\"chars\"] = df_test[\"excerpt\"].apply(lambda excerpt: \" \".join(list(excerpt)))","b64bac6e":"excerpts = df_test[\"excerpt\"].to_numpy()\nchars = df_test[\"chars\"].to_numpy()\ndatasets = tf.data.Dataset.from_tensor_slices((excerpts,chars)).batch(32).prefetch(tf.data.AUTOTUNE)","4ffd9879":"chars","62ec6394":"datasets","a1317317":"preds = model_6.predict([excerpts,chars])","854c28da":"preds","22c2fb3b":"submissions = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","08551f46":"submissions","d508ebb1":"submissions[\"target\"] = preds","bd8bb4d3":"submissions","ac0f6a60":"submissions.to_csv(\"\/kaggle\/working\/submission.csv\",index=False)","259c62c3":"**Create Text Vectorization and Embeddings(character level and word level) and Data Cleaning**","0af7167a":"95% of the excerpts are 198 words or less and comprised of 1161 characters or less\n","fc9bb83a":"### inputs = layers.Input(shape=(1,),dtype=tf.string)\nx = vectorizer(inputs)\nx = embedding(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Conv1D(filters=64,kernel_size=1,padding=\"valid\")(x)\nx = layers.MaxPool1D()(x)\nx = layers.Conv1D(filters=32,kernel_size=2,padding=\"valid\")(x)\nx = layers.MaxPool1D()(x)\nx = layers.Conv1D(filters=32,kernel_size=3,padding=\"valid\")(x)\nx = layers.GlobalAveragePooling1D()(x)\noutputs = layers.Dense(1)(x)"}}