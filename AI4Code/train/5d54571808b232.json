{"cell_type":{"beb8ac7e":"code","113f917b":"code","62bcc062":"code","4b132f4e":"code","b30b7dae":"code","874206fc":"code","026ee839":"code","fcb9f9b5":"code","3b283e13":"code","7ae33134":"code","af6dd655":"code","8f207204":"code","96994172":"code","5f1b959b":"code","48bc9ff3":"code","d121a834":"markdown","56f51110":"markdown","566b4300":"markdown","04f5eb2c":"markdown"},"source":{"beb8ac7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import ShuffleSplit\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom catboost import CatBoostRegressor\nimport catboost\n\nfrom xgboost import XGBRegressor\n\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import mean_absolute_error\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","113f917b":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")\nsamp_sub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","62bcc062":"train.shape","4b132f4e":"X = train.drop(['id', 'loss'], axis = 1)\ny = train['loss']","b30b7dae":"X_scaled = StandardScaler().fit_transform(X)\nTest_scaled = StandardScaler().fit_transform(test.drop(['id'], axis = 1))","874206fc":"number = [10, 30, 60, 80, 100]\nfor i in number:\n    pca = PCA(n_components = i)\n    pca.fit(X_scaled)\n    print('Number of components {}, Percentage of variance explained {}'.format(i, sum(pca.explained_variance_ratio_)))","026ee839":"X_train, X_test, Y_train, Y_test = train_test_split(X_scaled[29000:30000], y[29000:30000], shuffle = True)","fcb9f9b5":"%%time\n#CatBoost + GridSearchCV  model\n\n# cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\nCat = CatBoostRegressor(logging_level = 'Silent',\n                          loss_function='RMSE',\n                          learning_rate = 0.01,\n                          depth = 4,\n                          n_estimators = 350)\n\n# grid = {'learning_rate': [0.1],\n#         'depth': [4, 6, 10],\n#         'n_estimators': [500, 1500, 2000]}\n\n# grid_search_result = model.grid_search(grid, \n#                                        X=X_train, \n#                                        y=Y_train,\n#                                        cv = cv_split, \n#                                        plot=True)\n\n# model.fit(X_train, Y_train, \n#           verbose=False)","3b283e13":"%%time\n#LGBMRegressor + GridSearchCV  model\n\n# cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n# grid_n_estimator = [310]\n# grid_learn = [.001]\n\n# LGBM = LGBMRegressor(boosting_type='gbdt',\n#                      objective='regression',\n#                      metric='auc',\n#                      n_jobs = -1)\nLGBM = LGBMRegressor(boosting_type='gbdt',\n                      objective='regression',\n                      metric='auc',\n                      n_estimators = 310,\n                      max_depth = 8,\n                      learning_rate = 0.001, \n                      n_jobs = -1)\n# G_LGBM = GridSearchCV(LGBM, param_grid= {'learning_rate': grid_learn,\n#                                          'n_estimators': grid_n_estimator,\n#                                          'max_depth': [8]},\n#                    cv=cv_split)\n\n# G_LGBM.fit(X_train, Y_train)\n# print('Best Parameters: ', G_LGBM.best_params_)\n# pred = G_LGBM.predict(X_test)","7ae33134":"%%time\n#XGBoost + GridSearchCV  model\n\n# cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n# grid_n_estimator = [400, 500, 600]\n# grid_learn = [.001]\n\n# XGB = XGBRegressor()\nXGB = XGBRegressor(n_estimators = 600,\n                    max_depth = 4,\n                    learning_rate = 0.013, \n                   n_jobs = -1)\n# G_XGB = GridSearchCV(XGB, param_grid= {'learning_rate': grid_learn, \n#                                        'max_depth': [4], \n#                                        'n_estimators': grid_n_estimator},\n#                      cv = cv_split)\n\n\n# G_XGB.fit(X_train, Y_train)\n# print('Best Parameters: ', G_XGB.best_params_)\n# pred = G_XGB.predict(X_test)","af6dd655":"# cv_split = ShuffleSplit(n_splits = 10, \n#                         test_size = .3, \n#                         train_size = .6, \n#                         random_state = 0 )\n\nestimators = [('XGB', XGB),\n              ('Cat', Cat), \n              ('LGBM', LGBM)] \n\nstacked = StackingRegressor(estimators = estimators, \n                            final_estimator = LinearRegression(),\n                            verbose = 4\n#                             cv = cv_split\n                           )\n\nstacked.fit(X_train, Y_train)\n\npred = stacked.predict(X_test)","8f207204":"print('r2 score: ',r2_score(Y_test, pred))\nprint('mean_absolute_error: ',mean_absolute_error(Y_test, pred))\nprint('median_absolute_error: ',median_absolute_error(Y_test, pred))","96994172":"pred = stacked.predict(Test_scaled)\ntest['id'].values.shape, pred.shape, samp_sub.columns.to_list()","5f1b959b":"result = pd.DataFrame(pred)\nresult= pd.concat([test['id'], result], axis = 1)\nresult.columns = samp_sub.columns.to_list()","48bc9ff3":"result.to_csv(\"Simple_Caboost.csv\", index=False)","d121a834":"#### Checkout some metrics","56f51110":"#### Check percentage of variance explained data depending from number of components\n##### Spoiler: As linear as posible:)","566b4300":"#### Run grid search and try to messument time ","04f5eb2c":"#### Split data to train\/test datasets (uncomment if you needed)"}}