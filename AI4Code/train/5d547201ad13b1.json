{"cell_type":{"d92b4b07":"code","2ae1209b":"code","66bb8262":"code","5fbca47f":"code","36be26cb":"code","a2abc651":"code","5364c28c":"code","1440e508":"code","d72f0ff1":"code","f7f6e3d6":"code","ea354d29":"code","1d066211":"code","8823255d":"code","c5681d4b":"code","db5b77a2":"markdown","186eb8fd":"markdown","de9a89a7":"markdown","f6443ee6":"markdown","eca19c72":"markdown","d6cc3952":"markdown","ad54909b":"markdown"},"source":{"d92b4b07":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport keras.layers as layers\nfrom sklearn.model_selection import train_test_split\nimport random\nimport tensorflow_probability as tfp\nimport tensorflow_addons as tfa","2ae1209b":"from tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom keras import backend as K\n\nbatch_size= 32\nimage_size = [256, 256]\n\n\nds = image_dataset_from_directory(\n    '..\/input\/screwanomalies-detection\/screw\/train',\n    labels=None,\n    image_size=image_size,\n    interpolation='nearest',\n    batch_size=batch_size,\n    shuffle=True,\n)\n\ndef convert_to_float(image):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n\ndef trans1(img):\n    return tfa.image.rotate(tf.image.flip_left_right(tf.image.flip_up_down(img)),-.2,fill_mode=\"reflect\",interpolation=\"bilinear\")\n\ndef trans2(img):\n    return tfa.image.rotate(img,-.2,fill_mode=\"reflect\",interpolation=\"bilinear\")\n\ndef trans3(img):\n    return tfa.image.rotate(img,.2,fill_mode=\"reflect\",interpolation=\"bilinear\")\n    \nds1,ds2,ds3,ds4 = ds,ds.map(trans1),ds.map(trans2),ds.map(trans3)\n\nds = ds1.concatenate(ds2).concatenate(ds3).concatenate(ds4)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds = (\n    ds\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","66bb8262":"ds_a = image_dataset_from_directory(\n    '..\/input\/screwanomalies-detection\/screw\/test\/scratch_neck',\n    labels=None,\n    image_size=image_size,\n    interpolation='nearest',\n    batch_size=batch_size,\n    shuffle=True,\n)\nprint(type(ds))\n\ndef convert_to_float(image):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_a = (\n    ds_a\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","5fbca47f":"lrelu = tf.nn.selu","36be26cb":"a,b = image_size\nshape=(a, b,3)\n\n\nencoder_inputs = keras.Input(shape=shape)\nx = layers.BatchNormalization()(encoder_inputs)\nx = layers.Conv2D(8, 4, activation=lrelu, strides=4, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(16, 4, activation=lrelu, strides=4, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(32, 4, activation=lrelu, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(64, 2, activation=lrelu, strides=2, padding=\"same\")(x)\nencoder = keras.Model(encoder_inputs, x, name=\"encoder\")\nencoder.summary()","a2abc651":"a,b,c,d = x.shape\nprint(a,b,c,d)","5364c28c":"a,b,c,d = x.shape\nlatent_inputs = keras.Input(shape=(b,c,d))\nx = layers.BatchNormalization()(latent_inputs)\nx = layers.Conv2DTranspose(128, 4, activation=lrelu, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2DTranspose(64, 4, activation=lrelu, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2DTranspose(32, 8, activation=lrelu, strides=4, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2DTranspose(16, 8, activation=lrelu, strides=4, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n\ndecoder = keras.Model(latent_inputs, x, name=\"decoder\")\ndecoder.summary()","1440e508":"class AE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(AE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\"\n        )\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n    \n    def call(self,x):\n        z = self.encoder(x)\n        reconstruction = self.decoder(z)\n        return z,reconstruction\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            z = self.encoder(data)\n            reconstruction = self.decoder(z)\n            reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n                )\n            )\n            \n            total_loss = reconstruction_loss\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n        }","d72f0ff1":"rop = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2,patience=5, min_lr=0.00001,cooldown=10,verbose=1)","f7f6e3d6":"ae = AE(encoder, decoder)\nae.compile(optimizer=keras.optimizers.Adam())","ea354d29":"history = ae.fit(ds, epochs=500,verbose=1,callbacks=[])","1d066211":"import pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss']].plot()","8823255d":"digit_size, _ = image_size\nn = 4\nfigure = np.zeros((digit_size*3, digit_size * n,3))\nimg = list(ds)[0]\n\nfor i in range(n):\n    _,b_img = ae(img)\n    a = list(b_img)[i]\n    figure[\n                 0*digit_size :  digit_size,\n                i * digit_size :  (i+1)* digit_size,\n            ] = a\n    figure[\n                 1*digit_size :  2*digit_size,\n                i * digit_size :  (i+1)* digit_size,\n            ] = list(img)[i]\n    \n    figure[\n                 2*digit_size :  3*digit_size,\n                i * digit_size :  (i+1)* digit_size,\n            ] = (a-list(img)[i])*10\n\n\nfigsize = 5   \nplt.figure(figsize=(figsize*n, figsize*3))\nplt.imshow(figure)\nplt.show()","c5681d4b":"n = 3\nfigure = np.zeros((digit_size*3, digit_size * n,3))\nimg = list(ds_a)[0]\nfor i in range(n):\n    _,b_img = ae(img)\n    a = list(b_img)[i]\n    figure[\n                 0*digit_size :  digit_size,\n                i * digit_size :  (i+1)* digit_size,\n            ] = a\n    figure[\n                 1*digit_size :  2*digit_size,\n                i * digit_size :  (i+1)* digit_size,\n            ] = list(img)[i]\n    \n    figure[\n                 2*digit_size :  3*digit_size,\n                i * digit_size :  (i+1)* digit_size,\n            ] = (a-list(img)[i])*10\n\nfigsize = 10  \nplt.figure(figsize=(figsize*n, figsize*3))\nplt.imshow(figure)\nplt.show()","db5b77a2":"# **Training**","186eb8fd":"# On test set","de9a89a7":"# On training set","f6443ee6":"# **Data Augmentation**","eca19c72":"# **Making my model**","d6cc3952":"# Well, according to the results the CNN is cable of recreating scratches : we need to reduce the bottleneck size ","ad54909b":"#  **Results Analyse**"}}