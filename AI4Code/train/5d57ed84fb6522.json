{"cell_type":{"dee1783f":"code","0f86d4a8":"code","e3666346":"code","684e0977":"code","75968d71":"code","c57d8ff6":"code","856c6a6f":"code","2ecb3286":"code","ced5126b":"code","8f80e002":"code","74758522":"code","234f8cfa":"code","ee221dac":"code","37d95180":"markdown","3677696f":"markdown"},"source":{"dee1783f":"# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Data visualisation\nimport matplotlib.pyplot as plt\n\n# Fastai\nfrom fastai.vision import *\nfrom fastai.vision.models import *\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.utils\nimport torchvision.datasets as dset\n\nfrom torch import optim\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision.models import *\nfrom torchvision.datasets import ImageFolder\nfrom torch.autograd import Variable\n#import pretrainedmodels\n\nfrom pathlib import Path\nimport sys\n\nfrom glob import glob\nfrom PIL import Image","0f86d4a8":"np.random.seed(42)#To make sure that each time you run this kernal, you will get the same beginning parameters.\n\nBATCH_SIZE=64\nNUMBER_EPOCHS=100\nIMG_SIZE=100","e3666346":"def imshow(img,text=None,should_save=False):#for showing the data you loaded to dataloader\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):# for showing loss value changed with iter\n    plt.plot(iteration,loss)\n    plt.show()","684e0977":"#F09xx are used for validation.\nval_famillies = \"F09\"\n\n#An example of data:\"..\/input\/train\/F00002\/MID1\/P0001_face1.jpg\"\nall_images = glob(\"..\/input\/train\/*\/*\/*.jpg\")\n\ntrain_images = [x for x in all_images if val_famillies not in x]\nval_images = [x for x in all_images if val_famillies in x]\n\ntrain_person_to_images_map = defaultdict(list)#Put the link of each picture under the key word of a person such as \"F0002\/MID1\"\nfor x in train_images:\n    train_person_to_images_map[x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2]].append(x)\n\nval_person_to_images_map = defaultdict(list)\nfor x in val_images:\n    val_person_to_images_map[x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2]].append(x)\n\nppl = [x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2] for x in all_images]\nrelationships = pd.read_csv(\"..\/input\/train_relationships.csv\")\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))#For a List like[p1 p2], zip can return a result like [(p1[0],p2[0]),(p1[1],p2[1]),...]\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]#filter unused relationships\n","75968d71":"train = [x for x in relationships if val_famillies not in x[0]]\nval = [x for x in relationships if val_famillies in x[0]]\n\nprint(\"Total train pairs:\", len(train))    \nprint(\"Total val pairs:\", len(val))    ","c57d8ff6":"class trainingDataset(Dataset):#Get two images and whether they are related.\n    \n    def __init__(self,imageFolderDataset, relationships, transform=None):\n        self.imageFolderDataset = imageFolderDataset    \n        self.relationships = relationships #choose either train or val dataset to use\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        img0_info = self.relationships[index][0]#for each relationship in train_relationships.csv, the first img comes from first row, and the second is either specially choosed related person or randomly choosed non-related person\n        img0_path = glob(\"..\/input\/train\/\"+img0_info+\"\/*.jpg\")\n        img0_path = random.choice(img0_path)\n        \n        cand_relationships = [x for x in self.relationships if x[0]==img0_info or x[1]==img0_info]#found all candidates related to person in img0\n        if cand_relationships==[]:#in case no relationship is mensioned. But it is useless here because I choose the first person line by line.\n            should_get_same_class = 0\n        else:\n            should_get_same_class = random.randint(0,1) \n\n        if should_get_same_class==1:#1 means related, and 0 means non-related.\n            img1_info = random.choice(cand_relationships)#choose the second person from related relationships\n            \n            if img1_info[0]!=img0_info:\n                img1_info=img1_info[0]\n            else:\n                img1_info=img1_info[1]\n            img1_path = glob(\"..\/input\/train\/\"+img1_info+\"\/*.jpg\")#randomly choose a img of this person\n            img1_path = random.choice(img1_path)\n        else:#0 means non-related\n            randChoose = True#in case the chosen person is related to first person\n            while randChoose:\n                img1_path = random.choice(self.imageFolderDataset.imgs)[0]\n                img1_info = img1_path.split(\"\/\")[-3] + \"\/\" + img1_path.split(\"\/\")[-2]\n                randChoose = False\n                for x in cand_relationships:#if so, randomly choose another person\n                    if x[0]==img1_info or x[1]==img1_info:\n                        randChoose = True\n                        break\n                    \n        img0 = Image.open(img0_path)\n        img1 = Image.open(img1_path)\n        \n        if self.transform is not None:#I think the transform is essential if you want to use GPU, because you have to trans data to tensor first.\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n            \n            return img0, img1 , should_get_same_class #the returned data from dataloader is img=[batch_size,channels,width,length], should_get_same_class=[batch_size,label]\n    \n    def __len__(self):\n        return len(self.relationships)#essential for choose the num of data in one epoch","856c6a6f":"folder_dataset = dset.ImageFolder(root='..\/input\/train')\n\ntrainset = trainingDataset(imageFolderDataset=folder_dataset,\n                                        relationships=train,\n                                        transform=transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),\n                                                                      transforms.ToTensor()\n                                                                      ]))\ntrainloader = DataLoader(trainset,\n                        shuffle=True,#whether randomly shuffle data in each epoch, but cannot let data in one batch in order.\n                        num_workers=8,\n                        batch_size=BATCH_SIZE)\nvalset = trainingDataset(imageFolderDataset=folder_dataset,\n                                        relationships=val,\n                                        transform=transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),\n                                                                      transforms.ToTensor()\n                                                                      ]))\nvalloader = DataLoader(valset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=BATCH_SIZE)","2ecb3286":"#only for visualize data in dataloader, it won't matters if you delete this block.\nvis_dataloader = DataLoader(trainset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=8)\ndataiter = iter(vis_dataloader)\n\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[0],example_batch[1]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[2].numpy())","ced5126b":"class SiameseNetwork(nn.Module):# A simple implementation of siamese network, ResNet50 is used, and then connected by three fc layer.\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        #self.cnn1 = models.resnet50(pretrained=True)#resnet50 doesn't work, might because pretrained model recognize all faces as the same.\n        self.cnn1 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(3, 64, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(p=.2),\n            \n            nn.ReflectionPad2d(1),\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(p=.2),\n\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(64, 32, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            nn.Dropout2d(p=.2),\n        )\n        self.fc1 = nn.Linear(2*32*100*100, 500)\n        #self.fc1 = nn.Linear(2*1000, 500)\n        self.fc2 = nn.Linear(500, 500)\n        self.fc3 = nn.Linear(500, 2)\n        \n    def forward(self, input1, input2):#did not know how to let two resnet share the same param.\n        output1 = self.cnn1(input1)\n        output1 = output1.view(output1.size()[0], -1)#make it suitable for fc layer.\n        output2 = self.cnn1(input2)\n        output2 = output2.view(output2.size()[0], -1)\n        \n        output = torch.cat((output1, output2),1)\n        output = F.relu(self.fc1(output))\n        output = F.relu(self.fc2(output))\n        output = self.fc3(output)\n        return output","8f80e002":"net = SiameseNetwork().cuda()\ncriterion = nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\ncounter = []\nloss_history = [] \niteration_number= 0\n\nfor epoch in range(0,NUMBER_EPOCHS):\n    print(\"Epoch\uff1a\", epoch, \" start.\")\n    for i, data in enumerate(trainloader,0):\n        img0, img1 , labels = data #img=tensor[batch_size,channels,width,length], label=tensor[batch_size,label]\n        img0, img1 , labels = img0.cuda(), img1.cuda() , labels.cuda()#move to GPU\n        #print(\"epoch\uff1a\", epoch, \"No.\" , i, \"th inputs\", img0.data.size(), \"labels\", labels.data.size())\n        optimizer.zero_grad()#clear the calculated grad in previous batch\n        outputs = net(img0,img1)\n        loss = criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        if i %10 == 0 :#show changes of loss value after each 10 batches\n            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss.item()))\n            iteration_number +=10\n            counter.append(iteration_number)\n            loss_history.append(loss.item())\n    \n    #test the network after finish each epoch, to have a brief training result.\n        correct_val = 0\n    total_val = 0\n    with torch.no_grad():#essential for testing!!!!\n        for data in valloader:\n            img0, img1 , labels = data\n            img0, img1 , labels = img0.cuda(), img1.cuda() , labels.cuda()\n            outputs = net(img0,img1)\n            _, predicted = torch.max(outputs.data, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n            \n    print('Accuracy of the network on the', total_val, 'val pairs in',val_famillies, ': %d %%' % (100 * correct_val \/ total_val))\n    show_plot(counter,loss_history)","74758522":"class testDataset(Dataset): #different from train dataset, because the data organized in submission.csv is different from train.csv\n    \n    def __init__(self,transform=None):\n        self.test_df = pd.read_csv('..\/input\/sample_submission.csv')#pandas\u7528\u6765\u8bfb\u53d6csv\u6587\u4ef6\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        #data in submission.csv:\n        #       img_pair               is_related\n        #face05508.jpg-face01210.jpg       0\n        #face05820.jpg-face03938.jpg       0\n        \n        img0_path = self.test_df.iloc[index].img_pair.split(\"-\")[0]\n        img1_path = self.test_df.iloc[index].img_pair.split(\"-\")[1]\n        #print(img0_path,'-',img1_path) #reserved to check whether test data is in order.\n        \n        img0 = Image.open('..\/input\/test\/'+img0_path)\n        img1 = Image.open('..\/input\/test\/'+img1_path)\n\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n        \n        return img0, img1\n    \n    def __len__(self):\n        return len(self.test_df)","234f8cfa":"testset = testDataset(transform=transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),\n                                                                      transforms.ToTensor()\n                                                                      ]))\ntestloader = DataLoader(testset,\n                        shuffle=False,\n                        num_workers=0,\n                        batch_size=1)#Both extra workers and batch size lead to data out of order, the submission.csv will be out of order\n#if you have better method, please tell me! thanks a lot!","ee221dac":"test_df = pd.read_csv('..\/input\/sample_submission.csv')#pandas\u7528\u6765\u8bfb\u53d6csv\u6587\u4ef6\npredictions=[]\nwith torch.no_grad():\n    for data in testloader:\n        img0, img1 = data\n        img0, img1 = img0.cuda(), img1.cuda()\n        outputs = net(img0,img1)\n        _, predicted = torch.max(outputs, 1)\n        predictions = np.concatenate((predictions,predicted.cpu().numpy()),0)#taking care of here, the output data format is important for transfer\n        \ntest_df['is_related'] = predictions\ntest_df.to_csv(\"submission.csv\", index=False)#submission.csv should be placed directly in current fold.\ntest_df.head(50)#show the result to be committed","37d95180":"Skills\/Technology to be used:: We will use deep neural networks and transfer learning techniques to train a neural network that will predict if two people are blood related or not given a picture of their face.","3677696f":"**Goal:**:The goal of kinship verification is to determine whether a pair of faces are blood relatives or not. This task has seen lots of attention, which mainly focus on parent-child pair-wise types-- father-daughter (F-D), father-son (F-S), mother-daughter (M-D), mother-son (M-S)-- though some have also focused on siblings pairs-- brother-brother (B-B), sister-sister (S-S). This data supports all prior pair-wise types in sets much larger than previously offered to the research community. In addition, we introduce grandparent-grandchild pairs for the first time-- grandfather-granddaughter (GF-GD), grandfather-grandson (GF-GS),  grandmother-granddaughter (GM-GD), grandmother-grandson (GM-GS). As stated on the results page, our top performing benchmark scored an average accuracy of about 71%, so not only is there room for improvement, but also a need for more reliable systems before such technology is employed for practical purposes.\n\n**Scope:**:Kinship verification has the potential to improve social media tagging, ancestry\/genealogy site suggestions, along with many other applications in technology and media.\n"}}