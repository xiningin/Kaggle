{"cell_type":{"bcce92f5":"code","d545c028":"code","f0da0373":"code","d6f7129e":"code","c4decda5":"code","dc833e9a":"code","49c8d0c0":"code","569e788f":"code","30036c6e":"code","6dec87ee":"code","c3d1deee":"code","8bd6e22c":"code","be7fbde9":"code","6722c7df":"code","3ffe6408":"code","ef77170e":"code","de3fc1ff":"code","17852872":"code","93f5e51e":"code","33f9f3cd":"code","43ca90cf":"code","78688c2e":"code","99ceecb7":"code","061a264c":"code","1e986ead":"code","8598a7f6":"code","57d97d42":"code","f1906c28":"markdown","3e8b3a11":"markdown","dba70202":"markdown","69537285":"markdown","875db6d6":"markdown","4e3dfd8f":"markdown","212fa7f5":"markdown","6358bdbe":"markdown","bcc9b485":"markdown","f1f160b1":"markdown","491dfde5":"markdown"},"source":{"bcce92f5":"import torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import transforms, models\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.optimizer import Optimizer\n\nimport matplotlib.pyplot as plt\nfrom bisect import bisect_right,bisect_left\n\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport time\nimport random\nimport cv2\nfrom PIL import Image\n\n\nimport os\nprint(os.listdir(\"..\/input\"))","d545c028":"# Checking GPU is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('Training on CPU...')\nelse:\n    print('Training on GPU...')","f0da0373":"class CyclicCosAnnealingLR(_LRScheduler):\n    r\"\"\"\n    Implements reset on milestones inspired from CosineAnnealingLR pytorch\n    \n    Set the learning rate of each parameter group using a cosine annealing\n    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n\n    .. math::\n\n        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n\n    When last_epoch > last set milestone, lr is automatically set to \\eta_{min}\n\n    It has been proposed in\n    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n    implements the cosine annealing part of SGDR, and not the restarts.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        milestones (list of ints): List of epoch indices. Must be increasing.\n        eta_min (float): Minimum learning rate. Default: 0.\n        last_epoch (int): The index of last epoch. Default: -1.\n\n    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n        https:\/\/arxiv.org\/abs\/1608.03983\n    \"\"\"\n\n    def __init__(self, optimizer,milestones, eta_min=0, last_epoch=-1):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError('Milestones should be a list of'\n                             ' increasing integers. Got {}', milestones)\n        self.eta_min = eta_min\n        self.milestones=milestones\n        super(CyclicCosAnnealingLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \n        if self.last_epoch >= self.milestones[-1]:\n            return [self.eta_min for base_lr in self.base_lrs]\n\n        idx = bisect_right(self.milestones,self.last_epoch)\n        \n        left_barrier = 0 if idx==0 else self.milestones[idx-1]\n        right_barrier = self.milestones[idx]\n\n        width = right_barrier - left_barrier\n        curr_pos = self.last_epoch- left_barrier \n    \n        return [self.eta_min + (base_lr - self.eta_min) *\n               (1 + math.cos(math.pi * curr_pos\/ width)) \/ 2\n                for base_lr in self.base_lrs]\n\n ##warm up\nclass LearningRateWarmUP(object):\n    def __init__(self, optimizer, target_iteration, target_lr, after_scheduler=None):\n        self.optimizer = optimizer\n        self.target_iteration = target_iteration\n        self.target_lr = target_lr\n        self.num_iterations = 0\n        self.after_scheduler = after_scheduler\n\n    def warmup_learning_rate(self, cur_iteration):\n        warmup_lr = self.target_lr*float(cur_iteration)\/float(self.target_iteration)\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = warmup_lr\n\n    def step(self, cur_iteration):\n        if cur_iteration <= self.target_iteration:\n            self.warmup_learning_rate(cur_iteration)\n        else:\n            self.after_scheduler.step(cur_iteration)","d6f7129e":"class DualCompose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, x):\n        for t in self.transforms:\n            x= t(x)\n        return x\n\n\nclass VerticalFlip:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            img = cv2.flip(img, 0)\n            return img.copy()\n\n\nclass HorizontalFlip:\n    def __init__(self, prob=0.6):\n        self.prob = prob\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            img = cv2.flip(img, 1)\n            return img.copy()\n\n\nclass RandomFlip:\n    def __init__(self, prob=0.6):\n        self.prob = prob\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            d = random.randint(-1, 1)\n            img = cv2.flip(img, d)\n            return img.copy()\n\n\nclass RandomRotate90:\n    def __init__(self, prob=0.6):\n        self.prob = prob\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            factor = random.randint(0, 4)\n            img = np.rot90(img, factor)\n            return img.copy()\n\n\nclass Rotate:\n    def __init__(self, limit=90, prob=0.5):\n        self.prob = prob\n        self.limit = limit\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            angle = random.uniform(-self.limit, self.limit)\n            height, width = img.shape[0:2]\n            mat = cv2.getRotationMatrix2D((width\/2, height\/2), angle, 1.0)\n            img = cv2.warpAffine(img, mat, (height, width),\n                                 flags=cv2.INTER_LINEAR,\n                                 borderMode=cv2.BORDER_REFLECT_101)\n            return img.copy()\n\n\nclass Shift:\n    def __init__(self, limit=50, prob=0.5):\n        self.limit = limit\n        self.prob = prob\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            limit = self.limit\n            dx = round(random.uniform(-limit, limit))\n            dy = round(random.uniform(-limit, limit))\n\n            height, width, channel = img.shape\n            y1 = limit + 1 + dy\n            y2 = y1 + height\n            x1 = limit + 1 + dx\n            x2 = x1 + width\n\n            img1 = cv2.copyMakeBorder(img, limit+1, limit + 1, limit + 1, limit +1,\n                                      borderType=cv2.BORDER_REFLECT_101)\n            img = img1[y1:y2, x1:x2, :]\n            return img.copy()","c4decda5":"# Dataset responsible for manipulating data for training as well as training tests.\nclass DatasetMNIST(torch.utils.data.Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n#         self.JointTransform = DualCompose([\n#                                 RandomFlip(),\n#                                 RandomRotate90(),\n#                                 Rotate(),\n#                                    ])\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        item = self.data.iloc[index]\n                \n        image = item[1:].values.astype(np.uint8).reshape((28, 28))\n        label = item[0]\n#         print(image.shape)\n#         image = np.array(self.JointTransform(image))\n\n#         image = Image.fromarray(image)\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return image, label","dc833e9a":"BATCH_SIZE = 100\nVALID_SIZE = 0.15 # percentage of data for validation\n\ntransform_train = transforms.Compose([\n    transforms.ToPILImage(),\n#     transforms.RandomRotation(0, 0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntransform_valid = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\n# Importing data that will be used for training and validation\ndataset = pd.read_csv('..\/input\/train.csv')\ntest_dataset = pd.read_csv('..\/input\/test.csv')\n\n# Creating datasets for training and validation\ntrain_data = DatasetMNIST(dataset, transform=transform_train)\nvalid_data = DatasetMNIST(dataset, transform=transform_valid)\ntest_data = DatasetMNIST(test_dataset, transform=transform_valid)\n\n# Shuffling data and choosing data that will be used for training and validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(VALID_SIZE * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, sampler=train_sampler)\nvalid_loader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE, sampler=valid_sampler)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n\nprint(f\"Length train: {len(train_idx)}\")\nprint(f\"Length valid: {len(valid_idx)}\")\nprint(f\"Length test: {len(test_loader.dataset)}\")","49c8d0c0":"# Viewing data examples used for training\nfig, axis = plt.subplots(3, 10, figsize=(15, 10))\nimages, labels = next(iter(train_loader))\n\nfor i, ax in enumerate(axis.flat):\n    with torch.no_grad():\n        image, label = images[i], labels[i]\n\n        ax.imshow(image.view(28, 28), cmap='binary') # add image\n        ax.set(title = f\"{label}\") # add label","569e788f":"# Viewing data examples used for validation\nfig, axis = plt.subplots(3, 10, figsize=(15, 10))\nimages, labels = next(iter(valid_loader))\n\nfor i, ax in enumerate(axis.flat):\n    with torch.no_grad():\n        image, label = images[i], labels[i]\n\n        ax.imshow(image.view(28, 28), cmap='binary') # add image\n        ax.set(title = f\"{label}\") # add label","30036c6e":"from torch.optim.optimizer import Optimizer\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] \/ (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:            \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss","6dec87ee":"from torchvision.models import ResNet\n","c3d1deee":"\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel \/\/ reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel \/\/ reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n    \n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None,\n                 *, reduction=16):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n    \n\ndef se_resnet18(num_classes=1000):\n    \"\"\"Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n# num_classes = 10\n# model = models.resnet18(num_classes=num_classes)\n# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n#                                bias=False)\n# print(model)\nmodel = se_resnet18()\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                       bias=False)\nmodel.fc = nn.Linear(512, 10)\nprint(model)\nif train_on_gpu:\n    model.cuda()","8bd6e22c":"class LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        \n    def forward(self, x, target, smoothing=0.1):\n        confidence = 1. - smoothing\n        logprobs = F.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = confidence * nll_loss + smoothing * smooth_loss\n        return loss.mean()","be7fbde9":"LEARNING_RATE = 0.001680\n\ncriterion = LabelSmoothingCrossEntropy()\noptimizer = RAdam(params=model.parameters(), lr=0.003, weight_decay=0.0001)\nmilestones = [10 + x * 40 for x in range(20)]\nprint(milestones)\nscheduler_c = CyclicCosAnnealingLR(optimizer,milestones=milestones,eta_min=5e-5)\nscheduler = LearningRateWarmUP(optimizer=optimizer, target_iteration=10, target_lr=0.003,\n                                   after_scheduler=scheduler_c)","6722c7df":"epochs = 50\nvalid_accuracy_max = -100\ntrain_losses, valid_losses = [], []\nhistory_accuracy = []\n\nfor e in range(1, epochs+1):\n    running_loss = 0\n    scheduler.step(e)\n\n    for images, labels in train_loader:\n#         print(f\"labels:{labels}\")\n        if train_on_gpu:\n            images, labels = images.cuda(), labels.cuda()\n        # Clear the gradients, do this because gradients are accumulated.\n        optimizer.zero_grad()\n        \n        # Forward pass, get our log-probabilities.\n        ps = model(images)\n\n        # Calculate the loss with the logps and the labels.\n        loss = criterion(ps, labels)\n        \n        # Turning loss back.\n        loss.backward()\n        \n        # Take an update step and few the new weights.\n        optimizer.step()\n        \n        running_loss += loss.item()\n    else:\n        valid_loss = 0\n        accuracy = 0\n        \n        # Turn off gradients for validation, saves memory and computations.\n        with torch.no_grad():\n            model.eval() # change the network to evaluation mode\n            for images, labels in valid_loader:\n                if train_on_gpu:\n                    images, labels = images.cuda(), labels.cuda()\n                # Forward pass, get our log-probabilities.\n                #log_ps = model(images)\n                ps = model(images)\n                \n                # Calculating probabilities for each class.\n                #ps = torch.exp(log_ps)\n                \n                # Capturing the class more likely.\n                _, top_class = ps.topk(1, dim=1)\n                \n                # Verifying the prediction with the labels provided.\n                equals = top_class == labels.view(*top_class.shape)\n                \n                valid_loss += criterion(ps, labels)\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n                \n        model.train() # change the network to training mode\n        \n        train_losses.append(running_loss\/len(train_loader))\n        valid_losses.append(valid_loss\/len(valid_loader))\n        history_accuracy.append(accuracy\/len(valid_loader))\n        \n        network_learned = accuracy\/len(valid_loader) > valid_accuracy_max\n        # update learning rate according to accuracy\n#         scheduler.step(accuracy)  \n\n        if e == 1 or e % 5 == 0 or network_learned:\n            start = time.strftime(\"%H:%M:%S\")\n            print(f\"Epoch: {e}\/{epochs}..  | \u23f0: {start}\",\n                  f\"Training Loss: {running_loss\/len(train_loader):.6f}.. \",\n                  f\"Validation Loss: {valid_loss\/len(valid_loader):.6f}.. \",\n                  f\"Test Accuracy: {accuracy\/len(valid_loader):.6f}\")\n        \n        if network_learned:\n            valid_accuracy_max = accuracy\/len(valid_loader)\n            torch.save(model.state_dict(), 'model_mtl_mnist.pt')\n            print('Detected network improvement, saving current model')","3ffe6408":"# Dataset responsible for manipulating data for training as well as training tests.\nclass TestDatasetMNIST(torch.utils.data.Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n#         self.JointTransform = DualCompose([\n#                                 RandomFlip(),\n#                                 RandomRotate90(),\n#                                 Rotate(),\n#                                    ])\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        item = self.data.iloc[index]\n                \n        image = item[:].values.astype(np.uint8).reshape((28, 28))\n#         print(image.shape)\n#         image = np.array(self.JointTransform(image))\n\n#         image = Image.fromarray(image)\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return image\n\ntest_data = TestDatasetMNIST(test_dataset, transform=transform_valid)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n\nprint(f\"Length test: {len(test_loader.dataset)}\")","ef77170e":"for img in test_loader:\n    print(img.size())","de3fc1ff":"T1 = 10\nT2 = 70\naf = 3\n\ndef alpha_weight(step):\n    if step < T1:\n        return 0.0\n    elif step > T2:\n        return af\n    else:\n         return ((step-T1) \/ (T2-T1))*af","17852872":"# Concept from : https:\/\/github.com\/peimengsui\/semi_supervised_mnist\n\nfrom tqdm import tqdm_notebook\n\n\ndef semisup_train(model, train_loader, unlabeled_loader, test_loader):\n    valid_accuracy_max = -100\n    train_losses, valid_losses = [], []\n    history_accuracy = []\n\n    acc_scores = []\n    unlabel = []\n    pseudo_label = []\n\n    alpha_log = []\n    test_acc_log = []\n    test_loss_log = []\n    optimizer = torch.optim.SGD(params=model.parameters(), lr=0.003, weight_decay=0.0001)\n    milestones = [x * 50 for x in range(5)]\n    print(milestones)\n    scheduler = CyclicCosAnnealingLR(optimizer,milestones=milestones,eta_min=5e-5)\n    EPOCHS = 150\n    \n    # Instead of using current epoch we use a \"step\" variable to calculate alpha_weight\n    # This helps the model converge faster\n    step = 10 \n    \n    model.train()\n    for epoch in tqdm_notebook(range(EPOCHS)):\n        scheduler.step(epoch)\n        for batch_idx, x_unlabeled in enumerate(unlabeled_loader):\n            \n            \n            # Forward Pass to get the pseudo labels\n            x_unlabeled = x_unlabeled.cuda()\n            model.eval()\n            output_unlabeled = model(x_unlabeled)\n            _, pseudo_labeled = torch.max(output_unlabeled, 1)\n            model.train()\n            \n            \n            # Now calculate the unlabeled loss using the pseudo label\n            output = model(x_unlabeled)\n            unlabeled_loss = alpha_weight(step) * criterion(output, pseudo_labeled)   \n            \n            # Backpropogate\n            optimizer.zero_grad()\n            unlabeled_loss.backward()\n            optimizer.step()\n            \n                \n        # Normal training procedure\n        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n            X_batch = X_batch.cuda()\n            y_batch = y_batch.cuda()\n            output = model(X_batch)\n            labeled_loss = criterion(output, y_batch)\n\n            optimizer.zero_grad()\n            labeled_loss.backward()\n            optimizer.step()\n\n        # Now we increment step by 1\n        step += 1\n\n\n        # Turn off gradients for validation, saves memory and computations.\n        valid_loss = 0\n        accuracy = 0\n        with torch.no_grad():\n            model.eval() # change the network to evaluation mode\n            for images, labels in test_loader:\n                if train_on_gpu:\n                    images, labels = images.cuda(), labels.cuda()\n                # Forward pass, get our log-probabilities.\n                #log_ps = model(images)\n                ps = model(images)\n                \n                # Calculating probabilities for each class.\n                #ps = torch.exp(log_ps)\n                \n                # Capturing the class more likely.\n                _, top_class = ps.topk(1, dim=1)\n                \n                # Verifying the prediction with the labels provided.\n                equals = top_class == labels.view(*top_class.shape)\n                \n                valid_loss += criterion(ps, labels)\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n                \n        model.train() # change the network to training mode\n        \n#         train_losses.append(running_loss\/len(train_loader))\n#         valid_losses.append(valid_loss\/len(valid_loader))\n#         history_accuracy.append(accuracy\/len(valid_loader))\n        \n        network_learned = accuracy\/len(valid_loader) > valid_accuracy_max\n        # update learning rate according to accuracy\n#         scheduler.step(accuracy)  \n\n        if e == 1 or e % 5 == 0 or network_learned:\n            start = time.strftime(\"%H:%M:%S\")\n            print(f\"Epoch: {epoch}\/{EPOCHS}..  | \u23f0: {start}\",\n                  f\"Test Accuracy: {accuracy\/len(valid_loader):.8f}\")\n        \n        if network_learned:\n            valid_accuracy_max = accuracy\/len(valid_loader)\n            torch.save(model.state_dict(), 'model_mtl_mnist.pt')\n            print('Detected network improvement, saving current model')\n ","93f5e51e":"model.load_state_dict(torch.load('model_mtl_mnist.pt'))\nsemisup_train(model, train_loader, test_loader, valid_loader)","33f9f3cd":"# Viewing training information\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label='Training Loss')\nplt.plot(valid_losses, label='Validation Loss')\nplt.legend(frameon=False)","43ca90cf":"plt.plot(history_accuracy, label='Validation Accuracy')\nplt.legend(frameon=False)","78688c2e":"# Importing trained Network with better loss of validation\nmodel.load_state_dict(torch.load('model_mtl_mnist.pt'))\n\nprint(model)","99ceecb7":"# specify the image classes\nclasses = ['0', '1', '2', '3', '4',\n           '5', '6', '7', '8', '9']\n\n# track test loss\ntest_loss = 0.0\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\n\nmodel.eval()\n# iterate over test data\nfor data, target in valid_loader:\n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        data, target = data.cuda(), target.cuda()\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = model(data)\n    # calculate the batch loss\n    loss = criterion(output, target)\n    # update test loss \n    test_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(output, 1)    \n    # compare predictions to true label\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    # calculate test accuracy for each object class\n    for i in range(BATCH_SIZE):\n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n\n# average test loss\ntest_loss = test_loss\/len(valid_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\n\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %0.4f%% (%2d\/%2d)' % (\n            classes[i], class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2.2f%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","061a264c":"class DatasetSubmissionMNIST(torch.utils.data.Dataset):\n    def __init__(self, file_path, transform=None):\n        self.data = pd.read_csv(file_path)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        image = self.data.iloc[index].values.astype(np.uint8).reshape((28, 28, 1))\n\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return image","1e986ead":"transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\nsubmissionset = DatasetSubmissionMNIST('..\/input\/test.csv', transform=transform)\nsubmissionloader = torch.utils.data.DataLoader(submissionset, batch_size=BATCH_SIZE, shuffle=False)","8598a7f6":"submission = [['ImageId', 'Label']]\n\nwith torch.no_grad():\n    model.eval()\n    image_id = 1\n\n    for images in submissionloader:\n        if train_on_gpu:\n            images = images.cuda()\n        log_ps = model(images)\n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim=1)\n        \n        for prediction in top_class:\n            submission.append([image_id, prediction.item()])\n            image_id += 1\n            \nprint(len(submission) - 1)","57d97d42":"import csv\n\nwith open('submission.csv', 'w') as submissionFile:\n    writer = csv.writer(submissionFile)\n    writer.writerows(submission)\n    \nprint('Submission Complete!')","f1906c28":"### Modeling and Creating Network (CNN)","3e8b3a11":"# data augmentation function","dba70202":"### Import Libraries","69537285":"### Creating Data For Submission","875db6d6":"### Modeling and Creating Data for Training and Validation","4e3dfd8f":"# RAdam","212fa7f5":"# supervised learning","6358bdbe":"# label smoothing","bcc9b485":"# semi supervised learning","f1f160b1":"# cyclicLR","491dfde5":"### Configuring and Training Model"}}