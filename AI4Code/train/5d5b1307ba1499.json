{"cell_type":{"cefc2059":"code","80396700":"code","8fb2f405":"code","f7030b5f":"code","8ac4e0f9":"code","ca2ce3ec":"code","964ca29e":"code","dcc71fe8":"code","3a2f370e":"code","5c1083b2":"code","de9f2cf9":"code","ff3f7b31":"code","a9eb9368":"code","c5fe8d0c":"code","a6b9cfd5":"code","547812be":"code","7f3795c2":"code","f26d419d":"code","ef69c2e9":"code","f109a96d":"code","c090a312":"code","c4545d43":"code","34f6fbbf":"code","13cc6ab1":"code","a22e8d13":"code","069a1873":"code","92748298":"code","69cbbd26":"code","b9b40c60":"code","bd19e4df":"code","c5811416":"code","0e484847":"code","8b16f558":"code","7771c067":"code","67a6b10e":"code","d738e50c":"code","89b74b87":"code","470901dd":"code","5d90ee96":"code","b07d7217":"code","01337de0":"code","c2191548":"code","3a5d53f8":"code","1fbc0c90":"code","e1218913":"code","266b4818":"code","93c1176a":"code","3330c50b":"code","c6e6f018":"code","e5e8ce2f":"code","c68d3bd1":"code","5a1d50f9":"code","16e2ea4e":"code","08d8c920":"code","64e24fe5":"code","2b1678e2":"markdown","c61e92de":"markdown","90e3722d":"markdown","0bbf476c":"markdown","e33e652c":"markdown","7c5185c7":"markdown","fa696bdf":"markdown","b1bc3157":"markdown","40bef8fb":"markdown","3fe93afe":"markdown"},"source":{"cefc2059":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\n\n\n\nfrom sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n\nfrom joblib import dump, load\nimport pickle\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss, brier_score_loss, precision_score, recall_score, f1_score\nfrom datetime import date\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80396700":"train_features_dtypes = {\"cp_type\": \"category\",\"cp_dose\": \"category\"}","8fb2f405":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv',dtype = train_features_dtypes)","f7030b5f":"train_features['train'] = 1","8ac4e0f9":"test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv',dtype = train_features_dtypes)","ca2ce3ec":"test_features['train'] = 0","964ca29e":"temp = pd.concat([train_features,test_features],axis = 0)","dcc71fe8":"del train_features,test_features","3a2f370e":"temp.shape","5c1083b2":"for col, col_dtype in train_features_dtypes.items():\n    if col_dtype == \"category\":\n        temp[col] = temp[col].cat.codes.astype(\"int16\")\n        temp[col] -= temp[col].min()","de9f2cf9":"train_features = temp[temp['train'] == 1].copy()\ntest_features = temp[temp['train'] == 0].copy()\ndel temp","ff3f7b31":"train_features = train_features.drop(columns='train')\ntest_features = test_features.drop(columns='train')","a9eb9368":"test_features.shape","c5fe8d0c":"train_features.shape","a6b9cfd5":"train_features.head()","547812be":"train_features[['cp_type']].info()","7f3795c2":"train_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","f26d419d":"list(train_targets_scored.columns)","ef69c2e9":"train_targets_scored.shape","f109a96d":"train_targets_scored.head()","c090a312":"df = pd.merge(train_features,train_targets_scored,how='inner',on='sig_id')","c4545d43":"df.shape","34f6fbbf":"sample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","13cc6ab1":"sample_submission.shape","a22e8d13":"sample_submission.head()","069a1873":"submission = sample_submission.copy()\nfor col in submission.columns[1:]:\n    submission[col].values[:] = 0\n","92748298":"X_cols = list(train_features.drop('sig_id',axis=1).columns)","69cbbd26":"X_cols#[:5]","b9b40c60":"y_cols = list(train_targets_scored.drop('sig_id',axis=1).columns)","bd19e4df":"nfolds=5","c5811416":"# prepare split\nkf = KFold(n_splits = nfolds)","0e484847":"# base model definition throught sklearn Pipeline\npca = PCA(n_components = 300)\nsvm0 = SVC(C = 0.1,probability =True)\n\nbase_model = Pipeline(steps=[('pca', pca), ('svm', svm0)])\n\nmo_base = MultiOutputClassifier(base_model, n_jobs=-1)","8b16f558":"xtrain = df[X_cols]#.head(1000)  set to small value for testing code","7771c067":"xtrain.shape","67a6b10e":"ytrain = df[y_cols]#.head(1000) set to small value for testing code","d738e50c":"xtest = test_features[X_cols]","89b74b87":"# storage matrices for OOF \/ test predictions\nprval = np.zeros(ytrain.shape)\n","470901dd":"prval.shape","5d90ee96":"#kfold cv \nfor (ff, (id0, id1)) in enumerate(kf.split(xtrain)):\n     \n    x0, x1 = xtrain.loc[id0], xtrain.loc[id1]\n    y0, y1 = np.array(ytrain.loc[id0]), np.array(ytrain.loc[id1])\n    \n    # fix for empty columns\n    check_for_empty_cols = np.where(y0.sum(axis = 0) == 0)[0]\n    if len(check_for_empty_cols):\n        y0[0,check_for_empty_cols] = 1\n    \n    # fit model\n    mo_base.fit(x0,y0)\n    \n    # predicitons\n    prv = mo_base.predict_proba(x1)#[:, 1] see note below, this does not appear to work on a multioutput scenario\n    prf = mo_base.predict_proba(xtest)#[:, 1]\n    \n    # some tactical workarounds to get SVC and MultiOutputClassifier outputs into a workable format, \n    # as predict_proba generates probability of both pos and neg class, we need to cycle through each\n    # target prediction and take the one we want.\n    prv_n = []\n    for i in range(0,206):\n    #     print(i)\n        prv_n.append(prv[i][:, 1])\n    prf_n = []\n    for i in range(0,206):\n    #     print(i)\n        prf_n.append(prf[i][:, 1])\n    # generate the prediction\n    prval[id1,:] = pd.DataFrame(prv_n).T #formatting into dataframe and transpose to line up data \n    prf_n_df = pd.DataFrame(prf_n).T #formatting into dataframe and transpose to line up data \n    prf_n_df.columns = y_cols\n    for i in y_cols:\n        submission[i] += prf_n_df[i] \/ nfolds\n#     print(ff)\n#     print(pd.DataFrame(prf_n).T.shape)\n#     print(pd.DataFrame(prf_n).T.head(2))\n#     print(submission.head(2))\n    ","b07d7217":"submission.shape","01337de0":"submission.head()","c2191548":"def log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip), axis = 1))\n    return loss","3a5d53f8":"#overall model performance\n\nprint(f'Model OOF Metric: {log_loss_metric(ytrain, prval)}')\n\n","1fbc0c90":"prval_df = pd.DataFrame(prval)","e1218913":"prval_df.columns = y_cols","266b4818":"prval_df.head()","93c1176a":"def log_loss_metric_ind(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip)))\n    return loss","3330c50b":"#highlight the worst performing models\nperf_check = []\nfor i in y_cols:\n    perf_check.append((i,log_loss_metric_ind(ytrain[i], prval_df[i])))","c6e6f018":"results = pd.DataFrame(perf_check)","e5e8ce2f":"results.columns = ['target','log_loss']","c68d3bd1":"#worst performing models\nresults.sort_values('log_loss',ascending=False).head(20)","5a1d50f9":"#best performing models\nresults.sort_values('log_loss',ascending=True).head(20)","16e2ea4e":"submission.shape","08d8c920":"submission.head()","64e24fe5":"\n#test_export = test.loc[:, ['id', 'target']]\nsubmission.to_csv('submission.csv', index=False)","2b1678e2":"### 4. Evaluation","c61e92de":"#### 4.1 Overall performance","90e3722d":"#### 4.2 Model Weaknesses","0bbf476c":"Where could we focus efforts to improve models for the next iteration","e33e652c":"### SVC Model + PCA + Identifying on which targets the models perform poorly?","7c5185c7":"### 3. Model","fa696bdf":"\n\nSVM is a margin-based classifier that seeks to maximize the margin, or hyperplane, between the two classes. The model learns by support vectors, which are data points that are relatively close to or at the margin, and these support vectors influence the position and orientation of the hyperplane.\n\nWe will grid search the kernel to be used for SVM as well as the regularization parameter \"C\", which is inversely proportional to regularization. So the bigger C is, the less regularization there is, and the smaller the margin, which will in turn have less support vectors. Conversely, the smaller C, the more regularization there is, and the model seeks to fit a larger margin even if there are misclassified points in your training data.\n","b1bc3157":"### 1. Environment + Setup","40bef8fb":"### 5. Submission","3fe93afe":"### 2. Data Preparation"}}