{"cell_type":{"e4d9c304":"code","82796b6b":"code","754a3c31":"code","6acf38d6":"code","9e0a7004":"code","52512fc0":"code","4d4f84aa":"code","5d68dc25":"code","90d36ca1":"code","6fc6b030":"code","5af04f08":"code","2a805c41":"code","e4893290":"code","1f75a5ef":"code","588449da":"code","9d6b605a":"code","be33ec9c":"code","1a734e31":"code","8ce09dcb":"code","ac90ab48":"code","41b404d0":"code","7c1eadc2":"code","51a48f02":"code","c70ada7b":"code","7ce9dc0d":"code","2f8beb85":"code","69d5220e":"code","94e6b09b":"code","d5d949d9":"code","47b0b1bc":"code","2eafe7c5":"code","fce0c43c":"code","7f17ad5e":"code","d350e69e":"code","bbf3bac6":"code","fb454895":"code","ffa28dac":"code","120e7854":"code","5811ea52":"code","4fa18ea9":"code","099ab916":"code","61fa7033":"code","214e30f3":"code","12f2fd0b":"code","1c5491cb":"code","d322d6bf":"code","f6cbf089":"code","ea72c921":"code","c8671099":"code","e2557b11":"code","dd23f5ba":"code","4d4db575":"markdown","c846f88d":"markdown","291d4b9c":"markdown","89dda4db":"markdown","eaba0882":"markdown","fe5febd7":"markdown","df278e55":"markdown","52625622":"markdown","919a79ed":"markdown","83a5947b":"markdown","0b1c7b70":"markdown","72a1b1a6":"markdown","0a0dc333":"markdown","e82d55b9":"markdown","ce8c600e":"markdown","729b2ba1":"markdown","15556b63":"markdown","ed67490c":"markdown","11b175a9":"markdown","375d4c8a":"markdown","92681aff":"markdown","3ccc5a01":"markdown","ffc4fc19":"markdown","7bcb759f":"markdown","11b5f7c9":"markdown","31f0a9cd":"markdown","0a7e088c":"markdown","ae52de4e":"markdown","5150c647":"markdown","5ead70dc":"markdown","a8cc2b49":"markdown","d6c806c9":"markdown","9051d9ea":"markdown"},"source":{"e4d9c304":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport os, re, string","82796b6b":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","754a3c31":"path = Path('..\/input\/nlp-getting-started')\nos.listdir(path)","6acf38d6":"train_df = pd.read_csv(path\/'train.csv')\ntest_df = pd.read_csv(path\/'test.csv')","9e0a7004":"print(f'Training set size: {len(train_df)}')\nprint(f'Test set size: {len(test_df)}')","52512fc0":"train_df.head()","4d4f84aa":"print(f'Missing values in training set: {train_df.text.isna().sum()}')\nprint(f'Missing values in test set: {test_df.text.isna().sum()}')","5d68dc25":"plt.bar(['No disaster', 'Disaster'], [len(train_df[train_df.target==0]), len(train_df[train_df.target==1])], color=['darkblue', 'darkorange'])\nplt.xlabel('Dependent variable', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=12)\nplt.title('Class distribution', fontsize=16)\nplt.show()","90d36ca1":"print(f'Average target in training set: {np.round(train_df.target.mean(),2)}')","6fc6b030":"diff = len(train_df[train_df.target==0])-len(train_df[train_df.target==1]) \npct_diff = np.round(diff\/len(train_df),2)\ndiff, pct_diff","5af04f08":"lengths_trn = train_df.text.str.len()\nlengths_tst = train_df.text.str.len()\nlengths_trn0 = train_df[train_df.target==0].text.str.len()\nlengths_trn1 = train_df[train_df.target==1].text.str.len()\nprint('Avg length, min length, max length')\nprint('**********************************')\nprint(f'For training set: {int(lengths_trn.mean())}, {lengths_trn.min()}, {lengths_trn.max()}')\nprint(f' - no disaster tweets: {int(lengths_trn0.mean())}, {lengths_trn0.min()}, {lengths_trn0.max()}')\nprint(f' - disaster tweets: {int(lengths_trn1.mean())}, {lengths_trn1.min()}, {lengths_trn1.max()}')\nprint(f'For test set: {int(lengths_tst.mean())}, {lengths_tst.min()}, {lengths_tst.max()}')","2a805c41":"fig, axs = plt.subplots(2, 2, sharex='row', figsize=(10,10))\n\naxs[0, 0].hist(lengths_trn, color='darkgrey')\naxs[0, 0].set_title('Training set', fontsize=16)\naxs[0, 0].set_ylabel('Number of tweets', fontsize=12)\naxs[0, 1].hist(lengths_tst, color='lightgrey')\naxs[0, 1].set_title('Test set', fontsize=16)\naxs[1, 0].hist(lengths_trn0, color='darkblue')\naxs[1, 0].set_title('Training set (no disaster)', fontsize=16)\naxs[1, 0].set_ylabel('Number of tweets', fontsize=12)\naxs[1, 0].set_xlabel('Character lenghts', fontsize=12)\naxs[1, 1].hist(lengths_trn1, color='darkorange')\naxs[1, 1].set_title('Training set (disaster)', fontsize=16)\naxs[1, 1].set_xlabel('Character lenghts', fontsize=12)\n\nplt.show()","e4893290":"re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u20ac\u2018\u2019])')\ndef re_tokenizer(s): return re_tok.sub(r' \\1 ', s).lower().split()","1f75a5ef":"train_df.text[6]","588449da":"print(re_tokenizer(train_df.text[6]))","9d6b605a":"X_train, X_valid, y_train, y_valid = train_test_split(train_df.text, train_df.target, test_size=0.1, random_state=42)","be33ec9c":"print(f'Training set size: {len(X_train)}')\nprint(f'Validation set size: {len(X_valid)}')","1a734e31":"vec = CountVectorizer(ngram_range=(1,2), tokenizer=re_tokenizer, min_df=4, max_df=0.8, strip_accents='unicode', lowercase=False)","8ce09dcb":"train_term_doc = vec.fit_transform(X_train)\nvalid_term_doc = vec.transform(X_valid)\ntrain_term_doc.shape, valid_term_doc.shape","ac90ab48":"vocab = vec.get_feature_names()\nprint(f'Vocabulary size: {len(vocab)}')","41b404d0":"# Rename term-document matrices for convenience and convert labels from pandas series into numpy arrays\nx_train = train_term_doc\ny_train = y_train.values\nx_valid = valid_term_doc\ny_valid = y_valid.values","7c1eadc2":"p1 = np.squeeze(np.asarray(x_train[y_train==1].sum(0)))\np0 = np.squeeze(np.asarray(x_train[y_train==0].sum(0)))","51a48f02":"p1.shape, p1[:10]","c70ada7b":"pr1 = (p1+1) \/ ((y_train==1).sum()+1)\npr0 = (p0+1) \/ ((y_train==0).sum()+1)","7ce9dc0d":"pr1.shape, pr1[:10]","2f8beb85":"vocab[2160:2170]","69d5220e":"pr1[2164], pr0[2164]","94e6b09b":"p1[2164]\/(y_train==1).sum(), (p1[2164]+1)\/((y_train==1).sum()+1)","d5d949d9":"pr1[2164] \/ pr0[2164]","47b0b1bc":"r = np.log(pr1\/pr0)\nr.shape, r[:10]","2eafe7c5":"b = np.log((y_train==1).mean() \/ (y_train==0).mean()); b","fce0c43c":"preds = (x_valid @ r + b) > 0","7f17ad5e":"print(f'Validation accuracy: {(preds == y_valid).mean()}')\nprint(f'Validation F1 score: {f1_score(y_valid, preds)}')","d350e69e":"vec_tfidf = TfidfVectorizer(ngram_range=(1,2), tokenizer=re_tokenizer, lowercase=False,\n               min_df=4, max_df=0.8, strip_accents='unicode', sublinear_tf=True)","bbf3bac6":"train_term_doc_tfidf = vec_tfidf.fit_transform(X_train)\nvalid_term_doc_tfidf = vec_tfidf.transform(X_valid)","fb454895":"def pr(y_i, y, x):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","ffa28dac":"r = np.squeeze(np.asarray(np.log(pr(1, y=y_train, x=train_term_doc_tfidf)\/pr(0, y=y_train, x=train_term_doc_tfidf))))","120e7854":"preds = (valid_term_doc_tfidf @ r + b) > 0","5811ea52":"print(f'Validation accuracy: {(preds == y_valid).mean()}')\nprint(f'Validation F1 score: {f1_score(y_valid, preds)}')","4fa18ea9":"nb_train = train_term_doc_tfidf.multiply(r)\nnb_valid = valid_term_doc_tfidf.multiply(r)\nnb_train.shape, nb_valid.shape","099ab916":"# Setting up the model\nmodel = LogisticRegression(C=4, solver='liblinear')\n# Fitting the model on the training data\nmodel.fit(nb_train, y_train)\n# Getting predictions for the validation set\npreds = model.predict(nb_valid)","61fa7033":"print(f'Validation accuracy: {(preds == y_valid).mean()}')\nprint(f'Validation F1 score: {f1_score(y_valid, preds)}')","214e30f3":"train_term_doc_tfidf = vec_tfidf.fit_transform(train_df.text)\ntest_term_doc_tfidf = vec_tfidf.transform(test_df.text)\ny_train = train_df.target.values","12f2fd0b":"r = np.squeeze(np.asarray(np.log(pr(1, y=y_train, x=train_term_doc_tfidf)\/pr(0, y=y_train, x=train_term_doc_tfidf))))","1c5491cb":"model = LogisticRegression(C=4, solver='liblinear')\nmodel.fit(train_term_doc_tfidf.multiply(r), y_train)\npreds = model.predict(test_term_doc_tfidf.multiply(r))","d322d6bf":"submit = pd.read_csv(path\/'sample_submission.csv')","f6cbf089":"submit.columns","ea72c921":"assert all(submit.id == test_df.id)\nassert len(submit) == len(test_df) == len(preds)","c8671099":"submit.target = preds","e2557b11":"submit.head()","dd23f5ba":"# Save submissions\nsubmit.to_csv('submission_060320.csv', index=False)","4d4db575":"The results from above are not great, so I will try to get some improvement by using a tf-idf model instead of a simple bag-of-words. Tf-idf stands for *term frequency - inverse document frequency* and calculates the ratio of how often a token appears in a document to how important the token is in the entire corpus (assuming that rare tokens are more important than frequent ones).\n\nThe only change in the code is that I now use sklearn's TfidfVectorizer and include sublinear term frequency scaling (which replaces tf with 1 + log(tf) and gives slightly better results). Everything else stays the same.","c846f88d":"# Logistic Regression with NB features","291d4b9c":"Printing out the ratios for the word \"crisis\" in disaster and no disaster tweets, we can see that the token appears in *around* 0.6 percent of all disaster tweets, but only in 0.005 percent of all no disaster tweets.","89dda4db":"Let's now put it all together. In our setting, **Bayes' theorem** is defined in the following way:\n\n\n$ p(\\textrm{target} \\mid \\textrm{tokens}) \\displaystyle \\propto {p(\\textrm{tokens} \\mid \\textrm{target}) \\cdot p(\\textrm{target})} $\n\n* $ p(\\textrm{target} \\mid \\textrm{tokens}) $ is the *posterior*, i.e. the conditional probability of a target given a set of tokens \n* $ p(\\textrm{tokens} \\mid \\textrm{target}) $ is the *likelihood*, i.e. the conditional probability of a set of tokens given the target \n* $ p(\\textrm{target}) $ is the *prior*, i.e. the overall probability of each target in the dataset\n\nIn other words, we want to get the posterior - which is proportional to the likelihood times the prior - to predict the target from tokens.\n\nWithout further diving into the mathematical details, we can compute the **Naive Bayes predictions** like this:\n\n$ \\textrm{predictions} = \\textrm{term-document} \\ \\textrm{matrix} \\cdot \\textrm{r} + \\textrm{b}$\n\nIf the result is larger than zero, the model classifies a tweet as disaster tweet. The training process was actually already done before by calculating the log-count ratios and bias b on the training set. So we can now move on to evaluate the model on the validation set.\n","eaba0882":"Before I start with pre-processing the texts, I will do a short exploratory data analysis. When dealing with NLP datasets, two things I *always* look at first are the distribution of target classes and the distribution of lenghts within each class. ","fe5febd7":"Since the dataset is rather small, I will retrain the model on the entire training set (including the previous validation set) and then make predictions on the unlabeled test set for submitting to kaggle.","df278e55":"Note that I wrote \"*around* 0.6 percent\" before because the actual ratio is slightly different, which is due to the fact that we are adding +1 in numerator and denominator for numerical stability when calculating the ratios. However, the difference is so small we can ignore it.","52625622":"# Naive Bayes with bag-of-words","919a79ed":"# Inference and submission","83a5947b":"Even though the individual ratios can be slightly different, dividing both ratios by each other gives a good measure for how often a given token appears in disaster vs. no disaster tweets. The word \"crisis\" undoubtedly appears way more often in disaster tweets.","0b1c7b70":"b is the same as above, so there is no need to calculate it again","72a1b1a6":"Using tf-idf improved the results, but it turns out we can achieve an even better performance by using the Naive Bayes log-count ratios as input features to a Logistic Regression model. The features are obtained by multiplying the tf-idf term-document matrices for training and validation sets by the log-count ratios r from above.","0a0dc333":"# Exploratory data analysis","e82d55b9":"The first step is to calculate how often each token appears in disaster tweets and no disaster tweets. This is done by only selecting the tweets that correspond to target==1\/target==0 and summing up along the columns (since each column represents one token).","ce8c600e":"The log-count ratios are obtained by doing exactly that for all tokens and taking the log","729b2ba1":"After having experimented with different tokenizers (e.g. nltk and spaCy), I arrived at the conclusion that a simple regular expression tokenizer that can be defined in two lines of code (from Jeremy Howard's [kaggle kernel](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline)) works best in this setting. Also, none of the many pre-processing techniques that I tried (cleaning urls and html tags, removing stopwords and punctuation, stemming and lemmatization) actually improved the performance of the NB-LR algorithm on this specific dataset.\n\nThe tokenizer splits a string of text into separate tokens based on a set of common punctuations, converts all tokens into lowercase and then returns a list of the resulting tokens. Let's see how it works...","15556b63":"# Importing libraries and data","ed67490c":"The next step calculates the ratio of each feature in disaster and no disaster tweets by dividing the token counts from above by the number of disaster\/no disaster tweets. (Note: the ones are added for numerical stability)","11b175a9":"I will fit a simple Logistic Regression model on these features. Hyperparameter C controls the inverse regularization strength (default is L2 regularization with C=1), where a higher value indicates that the model is *less* regulated. Even though we have a small dataset, the model doesn't seem to overfit much, so I can decrease regularization to get better results.","375d4c8a":"Because we cannot feed the tokens directly into a machine learning model, the inputs need to be numericalized first. CountVectorizer creates a *bag-of-words* model of the tokenized texts. It creates a vocabulary of all tokens that appear at least 4 times (min_df) in the corpus but are in less than 80 percent of all documents (max_df). Such frequent words are treated as corpus-specific stopwords and therefore removed. We overwrite the default tokenizer with our regular expression tokenizer from above, replace non-standard characters and deactivate lowercasing (which is already done by the tokenizer).\n\nIn addition to including unigrams (i.e. single tokens), I also include bigrams which are two adjacent tokens (e.g. heavy and rain --> heavy_rain) that appear at least four times in the entire corpus (specified by min_df).","92681aff":"# Naive Bayes with Tf-idf","3ccc5a01":"You can use exactly the same cells as from the section above to calculate the log-count ratios. However, I decided to demonstrate another way to calculate r faster and with less lines of code. The formula to do this is also from the above-cited [kaggle kernel](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline). ","ffc4fc19":"The notebook will have the following structure:\n1. Importing libraries and data\n2. Exploratory data analysis\n3. Tokenization and training\/validation split\n4. Naive Bayes with bag-of-words\n5. Naive Bayes with tf-idf\n5. Logistic Regression with NB features\n6. Inference and submission","7bcb759f":"CountVectorizer returns separate term-document matrices for training and validation sets. The rows in the matrix correspond to documents and the columns to vocabulary items. The values in the matrix represent how often each token appears in each document. Since most tokens don't appear in a given document, the term-document matrices are sparse, i.e. they contain mostly zeros.\n\nIt is important to only call transform (not fit_transform) on the validation set, because the same vocabulary that was created from the training set needs to be used to create the term-document matrix for the validation set.","11b5f7c9":"# Tokenization and train\/valid split","31f0a9cd":"# Introduction and credits","0a7e088c":"Before moving on let's interpret one of these ratios.","ae52de4e":"Before moving on to Naive Bayes, we need to create a separate validation set for our model in order to evaluate how well it performs on data it has not seen during training. The validation set will be composed of 10% randomly selected tweets from the training set.","5150c647":"The length distributions of training and test sets look very similar. Because twitter limits the number of characters, the overall variation in lengths is not too large for this dataset. Interestingly, there is a spike in all four charts shortly before the maximum. When looking into the separate target classes, the average length for no disaster tweets is a bit lower than for disaster tweets. It's not surprising that there are fewer disaster tweets which are very short, as you probably would use more than a only few words for describing a disaster.","5ead70dc":"With 1071, or 14% more no disaster tweets than disaster tweets, we have a slightly imbalanced dataset. However, such a modest imbalance should not be a problem for our model and thus there is no need for rebalancing.\n\nNext I will calculate the lengths by character for all tweets in training set and test set as well as for all no disaster and disaster tweets in the training set separately.","a8cc2b49":"To finalize our Naive Bayes model we also need to calculate bias b, which measures the relative frequency of disaster tweets to no disaster tweets. Since there are only about 43 percent disaster tweets in the training set, the ratio is slightly negative.","d6c806c9":"This notebook is an application of techniques from the fantastic fast.ai course *[A Code-First Introduction to Natural Language Processing](https:\/\/www.fast.ai\/2019\/07\/08\/fastai-nlp\/)* by Rachel Thomas and Jeremy Howard to the kaggle competition *[Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)*. I also want to point out Jeremy Howard's [kaggle kernel](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline) from the earlier *[Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge)* which served as an inspiration for this notebook.\n\nWhile many other notebooks in this competition focus on the application of complex deep learning architectures, such as recurrent neural networks and transformers with pre-trained weights, I want to present another approach by training a fairly simple classification model from scratch that nevertheless delivers surprisingly good results.\n\nThe model I will use was introduced in this [paper](https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf) by Sida Wang and Christopher D. Manning. The paper shows that while Naive Bayes (NB) usually performs better on text classification problems with short input sequences and Support Vector Machines (SVM) are better on longer sequences, a combined approach that uses NB log-count ratios as input features to SVM performs consistently well on a variety of classification tasks and datasets with different sequence lenghts. In line with the above-mentioned kernel I will use a Logistic Regression model instead of SVM, which according to the paper should give similar results.","9051d9ea":"The **log-count ratio** $r$ for feature $f$ is the following:\n\n$r = \\log \\frac{\\text{ratio of feature $f$ in disaster tweets}}{\\text{ratio of feature $f$ in no disaster tweets}}$\n\nwhere features $f$ in the case of NLP are the tokens in our vocabulary (unigrams and bigrams). We will slowly build up to that equation in the next lines of code in order to understand better what is going on."}}