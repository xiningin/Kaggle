{"cell_type":{"e3135f14":"code","cc76620b":"code","f2879853":"code","ae7e1ff7":"code","1b845bb9":"code","71621372":"code","926c7650":"code","04e61c2f":"code","8f0c1ed2":"code","2902dd18":"code","7bc050b1":"code","25ec33c8":"code","6f0cb83b":"code","90f6ed6b":"code","50ade97e":"code","71159e11":"code","e68a3980":"code","944e7065":"code","dce61020":"code","260e1da2":"code","69b4e331":"code","da04845b":"code","50d4f749":"code","9b3b8672":"code","3ebfa8fa":"code","722f96a3":"code","b426c849":"code","938b94ed":"code","3fb668ef":"code","804ef73c":"code","43c0cf5e":"code","015cddd6":"code","f671564b":"code","1907808e":"code","bb5e2a15":"code","c1b4a7ac":"code","aabfde1d":"code","978ff843":"code","c7d8dbbd":"code","e3686d4a":"code","0d8735b1":"code","df4b8307":"code","a89127d4":"code","dd4f6a49":"code","2c211fa5":"code","182946a0":"code","6b55aba3":"code","a04ace56":"code","9c2bd15b":"code","6dd9564e":"code","bbbd7580":"code","9b88ea15":"code","ed644a86":"code","fa1fb5f2":"code","bf76051f":"code","a9d32e1f":"code","f6e891ab":"code","a2f387d7":"code","676d5558":"code","2c2fd5e3":"code","8d6ab1ba":"code","3cd4809c":"code","82d5bab7":"code","d596a4db":"code","9d83d2c1":"markdown","8b8de751":"markdown","2191f994":"markdown","75161812":"markdown","1a87be64":"markdown","0e595c26":"markdown","20df392b":"markdown","a97ae797":"markdown","2e52b20c":"markdown","37b84392":"markdown","38f62c60":"markdown","c2e25b9e":"markdown","761e26c8":"markdown","dd2c520a":"markdown","08d054c7":"markdown"},"source":{"e3135f14":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom scipy.stats import skew\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\n\ndef checknan(ds):\n    dsm = pd.DataFrame(ds.isnull().sum()\/ds.shape[0], columns=['nan%'])\n    return (dsm[dsm['nan%']>0]['nan%'].sort_values(ascending = False))\n\ndef countnan(ds):\n    dsm = pd.DataFrame(ds.isnull().sum(), columns=['nan'])\n    return (dsm[dsm['nan']>0]['nan'].sort_values(ascending = False))","cc76620b":"#dftest = pd.read_csv('test.csv')\n#dftrain = pd.read_csv('train.csv')\n#dftest.shape, dftrain.shape\ndftrain = pd.read_csv(\"..\/input\/train.csv\")\ndftest = pd.read_csv(\"..\/input\/test.csv\")","f2879853":"corr = dftrain.corr()\ncorr2 = dftrain[corr.SalePrice.sort_values(ascending=False).head(11).index]\nsns.pairplot(corr2)","ae7e1ff7":"corr2.columns","1b845bb9":"plt.scatter(dftrain['GrLivArea'], dftrain['SalePrice'])","71621372":"dftrain[(dftrain.SalePrice<300000) & (dftrain.GrLivArea>4000)]","926c7650":"dftrain.drop([523, 1298], axis=0, inplace=True)","04e61c2f":"plt.scatter(dftrain['GrLivArea'], dftrain['SalePrice'])","8f0c1ed2":"corr = dftrain.corr()\ncorr2 = dftrain[corr.SalePrice.sort_values(ascending=False).head(11).index]\nsns.pairplot(corr2)","2902dd18":"corr2.columns","7bc050b1":"def bp(feature):\n    fig = plt.figure(figsize=(10,3))\n    ax1 = fig.add_subplot(121)\n    ax1 = plt.boxplot((dftrain[feature]))\n    plt.xlabel(feature)\n    ax2 = fig.add_subplot(122)\n    ax2 = plt.boxplot(np.log1p(dftrain[feature]))\n    plt.xlabel(feature+' log')\n    plt.show","25ec33c8":"for featr in corr2.columns:\n    bp(featr)","6f0cb83b":"'''took a look but decided apply log to all numeric features.. later on this notebook'''","90f6ed6b":"'''reset dataframe to start point'''\ndf = dftrain\ndf = df.append(dftest, sort=True)\ndf.shape","50ade97e":"checknan(df)","71159e11":"'''check missing in a df'''\nnan_ck = pd.DataFrame(df.isnull().sum(), columns = ['Nan_sum'])\nnan_ck = nan_ck.drop('SalePrice')\nnan_ck['Nan_cnt'] = pd.DataFrame(df.isnull().count())\nnan_ck['Nan%'] = nan_ck['Nan_sum'] \/ nan_ck['Nan_cnt']\nnan_ck = nan_ck[nan_ck['Nan%'] != 0].sort_values(['Nan%'], ascending = False)\nnan_ck.head(80)","e68a3980":"'''low missing data'''\nlowmd = nan_ck[(nan_ck['Nan_sum'] <= 4) & (nan_ck['Nan_sum'] > 0)]\n'''mid missing data'''\nmidmd = nan_ck[(nan_ck['Nan_sum'] <= 486) & (nan_ck['Nan_sum'] > 4)]\n'''hi missing data'''\nhimd = nan_ck[(nan_ck['Nan_sum'] > 486)]","944e7065":"print('himd:\\n',list(himd.index),'\\n\\nlowmd:\\n', list(lowmd.index), '\\n\\nmidmd:\\n', list(midmd.index))","dce61020":"df[himd.index].info()","260e1da2":"df['PoolQC'].value_counts(), \\\ndf['MiscFeature'].value_counts(), \\\ndf['Alley'].value_counts(), \\\ndf['Fence'].value_counts(), \\\ndf['FireplaceQu'].value_counts()","69b4e331":"df[himd.index] = df[himd.index].fillna('NA')","da04845b":"df[lowmd.index].info()","50d4f749":"df[lowmd.index].describe()","9b3b8672":"df[lowmd.index]=df[lowmd.index].fillna(df[lowmd.index].median())","3ebfa8fa":"df[lowmd.index].describe()","722f96a3":"lowmd1 = df[checknan(df[lowmd.index]).index]\nlowmd1.head()","b426c849":"print (lowmd1['MSZoning'].value_counts().head(2),'\\n', '-' * 25)\nprint (lowmd1['Functional'].value_counts().head(2),'\\n', '-' * 25)\nprint (lowmd1['Utilities'].value_counts().head(2),'\\n', '-' * 25)\nprint (lowmd1['SaleType'].value_counts().head(2),'\\n', '-' * 25)\nprint (lowmd1['Electrical'].value_counts().head(2),'\\n', '-' * 25)\nprint (lowmd1['KitchenQual'].value_counts().head(2),'\\n', '-' * 25)\nprint (lowmd1['Exterior1st'].value_counts().head(2),'\\n', '-' * 25)\nprint (lowmd1['Exterior2nd'].value_counts().head(2))","938b94ed":"df['MSZoning'] = df['MSZoning'].fillna('RL')\ndf['Functional'] = df['Functional'].fillna('Typ')\ndf['Utilities'] = df['Utilities'].fillna('AllPub')\ndf['SaleType'] = df['SaleType'].fillna('WD')\ndf['Electrical'] = df['Electrical'].fillna('SBrkr')\ndf['KitchenQual'] = df['KitchenQual'].fillna('TA')\ndf['Exterior1st'] = df['Exterior1st'].fillna('VinylSd')\ndf['Exterior2nd'] = df['Exterior2nd'].fillna('VinylSd')","3fb668ef":"df[midmd.index].info()","804ef73c":"checknan(df[midmd.index].select_dtypes(include=np.number))","43c0cf5e":"print( 'normal >', df['LotFrontage'].skew(), ' -- log >', np.log1p(df['LotFrontage']).skew())\nprint( 'normal >', df['GarageYrBlt'].skew(), ' -- log >', np.log1p(df['GarageYrBlt']).skew())\nprint( 'normal >', df['MasVnrArea'].skew(), ' -- log >', np.log1p(df['MasVnrArea']).skew())","015cddd6":"fig = plt.figure(figsize=(20,15))\n\nax1 = fig.add_subplot(321)\nax1.hist(df[df['LotFrontage']>=0]['LotFrontage'], bins=18)\nplt.xlabel('LotFrontage')\nax2 = fig.add_subplot(322)\nax2.hist(np.log1p(df[df['LotFrontage']>=0]['LotFrontage']), bins=18)\nplt.xlabel('LotFrontage Log')\n\nax3 = fig.add_subplot(323)\nax3.hist(df[df['GarageYrBlt']>=0]['GarageYrBlt'], bins=18)\nplt.xlabel('GarageYrBlt')\nax4 = fig.add_subplot(324)\nax4.hist(np.log1p(df[df['GarageYrBlt']>=0]['GarageYrBlt']), bins=18)\nplt.xlabel('GarageYrBlt Log')\n\nax5 = fig.add_subplot(325)\nax5.hist(df[df['MasVnrArea']>=0]['MasVnrArea'], bins=18)\nplt.xlabel('MasVnrArea')\nax6 = fig.add_subplot(326)\nax6.hist(np.log1p(df[df['MasVnrArea']>=0]['MasVnrArea']), bins=18)\nplt.xlabel('MasVnrArea Log')\n\nplt.show()","f671564b":"df['MasVnrArea'].value_counts().head(10) # set to 0","1907808e":"df['GarageYrBlt'].value_counts().head(10) # set to 2005","bb5e2a15":"'''apply log on to LotFrontage and fillna = median\n    fill MasVnrArea NA = 0.0\n    fill GarageYrBlt NA = 2005.0'''","c1b4a7ac":"df['LotFrontage'] = df['LotFrontage'].fillna(df[df['LotFrontage']>0]['LotFrontage'].median())","aabfde1d":"#df['LotFrontage'] = np.log1p(df['LotFrontage'])","978ff843":"df['MasVnrArea'] = df['MasVnrArea'].fillna(0.0)","c7d8dbbd":"df['GarageYrBlt'] = df['GarageYrBlt'].fillna(2005.0)","e3686d4a":"checknan(df[midmd.index].select_dtypes(include=np.number))","0d8735b1":"checknan(df[midmd.index].select_dtypes(include=['object']))","df4b8307":"lista = list(checknan(df[midmd.index].select_dtypes(include=['object'])).index)","a89127d4":"print('-' * 60)\nfor n in lista:\n    print('',n,'\\n','='*len(n)) \n    print(df[n].value_counts()\/df[n].count(),'\\n','-'*80)","dd4f6a49":"'''\n 'GarageFinish' = Unf\n 'GarageQual' = TA,\n 'GarageCond' = TA,\n 'GarageType' = Attchd,\n 'BsmtCond' = TA,\n 'BsmtExposure' = No,\n 'BsmtQual' = TA,\n 'BsmtFinType2' = Unf,\n 'BsmtFinType1' = Unf,\n 'MasVnrType' = None\n '''","2c211fa5":"lista2 = ['Unf','TA','TA','Attchd','TA','No','TA','Unf','Unf','None']\nlistadict = dict(zip(lista, lista2))\nlistadict","182946a0":"for n in listadict:\n    df[n] = df[n].fillna(listadict[n])","6b55aba3":"''' apply log to full df:\n    SalePrice,\n    GrLivArea, \n    1stFlrSF, \n    GarageArea, \n    TotRmsAbvGrd\n'''","a04ace56":"'''apply log to all numeric features'''\n#for featr in ['GrLivArea', \n#              '1stFlrSF', \n#              'GarageArea', \n#              'TotRmsAbvGrd']:\n#    df[featr] = np.log1p(df[featr])","9c2bd15b":"numerics = list(df.drop(['SalePrice', 'Id'], axis=1).select_dtypes(include=np.number).columns)","6dd9564e":"'''implement squares and cubic features to numerics and and apply log...'''\nfor featr in numerics:\n    df[featr+'2'] =  np.log1p(df[featr] ** 2)\n    df[featr+'3'] =  np.log1p(df[featr] ** 3)\n    df[featr] = np.log1p(df[featr])","bbbd7580":"all = df\nall = pd.get_dummies(all,drop_first=True)","9b88ea15":"X_train = all[:dftrain.shape[0]]\nY_train = X_train[['Id','SalePrice']]\nX_test = all[dftrain.shape[0]:]\nX_train.drop(['Id','SalePrice'], axis=1, inplace=True)\nX_test.drop(['Id','SalePrice'], axis=1, inplace=True)","ed644a86":"X_train.shape, Y_train.shape, X_test.shape","fa1fb5f2":"from sklearn.model_selection import cross_val_score, KFold, learning_curve\nimport xgboost as xgb\nfrom xgboost import XGBClassifier,XGBRegressor\n#from sklearn.grid_search import GridSearchCV \nfrom sklearn import metrics\nseed = 45\nn_folds = 5\nkfold = KFold(n_folds, shuffle=True, random_state=seed).get_n_splits(X_train)","bf76051f":"def error(actual, predicted):\n    actual = np.log(actual)\n    predicted = np.log(predicted)\n    return np.sqrt(np.sum(np.square(actual-predicted))\/len(actual))\n\ndef get_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, \n                                    X_train, \n                                    y=np.log(Y_train.SalePrice), \n                                    scoring=\"neg_mean_squared_error\", \n                                    cv=kfold))\n    return rmse.mean()","a9d32e1f":"xgbmod = XGBRegressor() \n'''w\/o fine tuning'''\nxgbmod.fit(X_train, np.log(Y_train.SalePrice))\nyhat = np.exp(xgbmod.predict(X_train))\nprint('rmse:', get_rmse(xgbmod))","f6e891ab":"import shap\nshap_values = shap.TreeExplainer(xgbmod).shap_values(X_train)\n\nglobal_shap_vals = np.abs(shap_values).mean(0)[:-1]\nvariables_values = pd.DataFrame(list(zip(X_train.columns,global_shap_vals)))\nvariables_values.rename(columns={0:'variable',1:'shap_value'},inplace=True)\nvariables_values.sort_values(by=['shap_value'],ascending=False,inplace=True)\ntop_n = variables_values.head(12)\n\npos=range(0,-top_n.shape[0],-1)\nplt.barh(pos, top_n['shap_value'], color=\"#007fff\")\nplt.yticks(pos, top_n['variable'])\nplt.xlabel(\"mean SHAP value magnitude (do not change in log odds)\")\nplt.gcf().set_size_inches(8, 4)\nplt.gca()\nplt.show()","a2f387d7":"shap.summary_plot(shap_values, X_train)","676d5558":"'''remove shap_0 features to new prediction'''\ntop_n = variables_values\nremove_featr = list(top_n[top_n.shap_value == 0]['variable'])","2c2fd5e3":"X_train.drop(remove_featr, axis=1, inplace = True)\nX_test.drop(remove_featr, axis=1, inplace = True)","8d6ab1ba":"xgbmod = XGBRegressor(colsample_bylevel=1, colsample_bytree=1, learning_rate=0.03,max_delta_step=0, \n                      max_depth=6,min_child_weight=6,n_estimators=450,subsample= 0.5)\n'''w\/ fine tuning'''                      \n\nxgbmod.fit(X_train, np.log(Y_train.SalePrice))\nyhat = np.exp(xgbmod.predict(X_train))\nprint('rmse:', get_rmse(xgbmod))","3cd4809c":"#yhat = np.exp(xgbmod.predict(X_test))\n#        #yhat = np.exp(yhat)\n#Y_test = all[dftrain.shape[0]:]['Id']\n#yhat = pd.DataFrame(yhat, columns = ['SalePrice'])\n#Y_test = pd.DataFrame (Y_test)\n#Y_test['SalePrice'] = yhat.SalePrice\n#Y_test.to_csv('subm6.csv', index= False)","82d5bab7":"#'''gridsearch better params'''\n#\n#xgb_reg = XGBRegressor()\n#parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n#              'colsample_bylevel' : [1],\n#              'learning_rate': [0.03,0.04], #so called `eta` value\n#              'max_depth': [6],\n#              'min_child_weight': [6],\n#              'max_delta_step' : [0],\n#              'reg_lambda': [1],\n#              'subsample': [0.5, 0.6],\n#              'colsample_bytree': [ 1],\n#              'n_estimators': [450, 550]}\n#\n#xgb_grid_reg = GridSearchCV(xgb_reg,\n#                        parameters,\n#                        cv = 2,\n#                        n_jobs = 5,\n#                        verbose=True)\n#\n#xgb_grid_reg.fit(X_train,np.log1p(Y_train.SalePrice))\n#\n#print(xgb_grid_reg.best_score_)\n#print(xgb_grid_reg.best_params_)","d596a4db":"#0.9071702277585866\n#{'colsample_bylevel': 1, 'colsample_bytree': 1, 'learning_rate': 0.03, \n# 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 400, \n# 'nthread': 4, 'reg_lambda': 1, 'subsample': 0.5}\n#\n#0.9093014037566548\n#{'colsample_bylevel': 1, 'colsample_bytree': 1, 'learning_rate': 0.04, \n# 'max_delta_step': 0, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 400, \n# 'nthread': 4, 'reg_lambda': 1, 'subsample': 0.6}\n#\n#0.9075638827570375\n#{'colsample_bylevel': 1, 'colsample_bytree': 1, 'learning_rate': 0.03, \n# 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 500, \n# 'nthread': 4, 'reg_lambda': 1, 'subsample': 0.5}\n#\n#0.9077751136520033\n#{'colsample_bylevel': 1, 'colsample_bytree': 1, 'learning_rate': 0.03, \n# 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 450, \n# 'nthread': 4, 'reg_lambda': 1, 'subsample': 0.5}","9d83d2c1":"### Prepare datasets for modeling","8b8de751":"### classify nan features analysis...","2191f994":"### basic idea: \n1. Brush data (fill missing, add features, log...);\n2. Run a simple XGBoost regression;\n3. Use Shap to find and remove irrelevant features from brushed-reduced dataset and\n4. Tune a XGBoost regression to finish: top 33%\n","75161812":"#### Hi missing data  analysis","1a87be64":"#### Low missing data  analysis","0e595c26":"### review of features to reduce dataset keeping only important ones:","20df392b":"### sumission below gave me top33% position... there is definetely some more data work to be considered..","a97ae797":"### check missing data","2e52b20c":"### hi, low, and mid missing data levels...","37b84392":"### Let the tests begin (xgboost):","38f62c60":"## remove train dataset outliers","c2e25b9e":"## XGBoost fine tuning space....","761e26c8":"### kaggle submit:","dd2c520a":"# House Pricing by Joao Megale","08d054c7":"#### Mid missing data  analysis"}}