{"cell_type":{"cae52389":"code","7771dce4":"code","a993b036":"code","240b1b76":"code","b7c037e6":"code","34f29474":"code","619230da":"code","a4e9bf42":"code","3af47bef":"code","3fb27ee8":"code","1513d9c4":"code","a55d41d5":"code","8aad5868":"code","9cbf806e":"code","16c596e0":"code","aa17b33b":"code","41002287":"code","3b3dbc38":"code","4ba94d9e":"markdown","0972167c":"markdown","09e88018":"markdown","b8bb396a":"markdown","cb4ab373":"markdown","3af739b1":"markdown","4e261718":"markdown","478468a6":"markdown","d3be05a2":"markdown","9ef2666a":"markdown"},"source":{"cae52389":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.models import Model\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler,scale\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\nimport numpy as np \nimport pandas as pd\nimport math\nimport random\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(tf.__version__)\nprint(keras.__version__)","7771dce4":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")","a993b036":"train.drop(['id'],axis = 1, inplace = True)\ntest.drop(['id'],axis = 1, inplace = True)\ny = np.array(train['loss'])\nX = train.drop(['loss'],axis = 1)\nxall = pd.concat([X,test],axis=0,copy=False).reset_index(drop=True)\ny.shape,X.shape,test.shape,xall.shape","240b1b76":"xmedian = pd.DataFrame.median(xall,0)\nx25quan = xall.quantile(0.25,0)\nx75quan = xall.quantile(0.75,0)\nxall = (xall-xmedian)\/(x75quan-x25quan)","b7c037e6":"def quantile_norm(df_input):\n    sorted_df = pd.DataFrame(np.sort(df_input.values,axis=0), index=df_input.index, columns=df_input.columns)\n    mean_df = sorted_df.mean(axis=1)\n    mean_df.index = np.arange(1, len(mean_df) + 1)\n    quantile_df =df_input.rank(axis = 0, method=\"min\").stack().astype(int).map(mean_df).unstack()\n    return(quantile_df)\n\nqall = np.array(quantile_norm(xall))\nqlabeled = qall[:len(train),:]\nqunlabeled = qall[len(train):,:]","34f29474":"# 100 bins for the bins head of the NN :\nball = np.zeros((qall.shape[0],X.shape[1]))\nfor i in range(X.shape[1]):\n    ball[:,i] = pd.qcut(qall[:,i],X.shape[1],labels=False,duplicates = 'drop')\nblabeled = ball[:X.shape[0],:]\nbunlabeled = ball[X.shape[0]:,:]","619230da":"noise = np.random.normal(0, .1, (qall.shape[0],qall.shape[1]))\nqall = np.array(qall)\nxnoisy = qall + noise\nlimit = np.int(0.8 * qall.shape[0])\n# Split 80% training \/ 20% validation sets:\nxtrain = xnoisy[0:limit,:]\nytrain = qall[0:limit,]\nxvalid = xnoisy[limit:qall.shape[0],:]\nyvalid = qall[limit:qall.shape[0],:]\nxtrain.shape, ytrain.shape, xvalid.shape, yvalid.shape","a4e9bf42":"es = tf.keras.callbacks.EarlyStopping(\n    monitor= 'val_loss',\n    min_delta=1e-9, \n    patience=20, \n    verbose=0,\n    mode='min', \n    baseline=None, \n    restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.8, \n    patience=4, \n    verbose=0,\n    mode='min')","3af47bef":"def custom_loss(y_true, y_pred):\n    loss = K.mean(K.square(y_pred - y_true))\n    return loss","3fb27ee8":"def autoencoder():\n    ae_input = layers.Input(shape = (qall.shape[1]))\n    ae_encoded = layers.Dense(\n        units = qall.shape[1],\n        activation ='elu')(ae_input)\n    ae_encoded = layers.Dense(\n        units = qall.shape[1]*3,\n        activation='elu')(ae_encoded)\n    ae_decoded = layers.Dense(\n        units = qall.shape[1],\n        activation='elu')(ae_encoded)\n    \n    return Model(ae_input, ae_decoded),Model(ae_input,ae_encoded)","1513d9c4":"autoencoder,encoder = autoencoder()\nautoencoder.compile(loss= custom_loss,\n                    optimizer = keras.optimizers.Adam(lr=5e-3))\nhistory = autoencoder.fit(xtrain,\n                    ytrain, \n                    epochs = 200,\n                    batch_size = 512,\n                    verbose = 0,\n                    validation_data=(xvalid,yvalid),\n                    callbacks=[es,plateau])","a55d41d5":"eall = encoder.predict(qall) # data encoding by denoiser encoder\nprint(\"max encoded value =\",np.max(eall)) # check if the denoising was successful for variance","8aad5868":"evar = np.var(eall, axis=0,ddof=1)\nevar1 = evar > 0.8  # Threshold variance\na = np.where(evar1 == False,evar1,1)\nnb_col = a.sum()\nprint(\"number of selected columns\",nb_col)\nif ((nb_col < 95) | (nb_col > 110)) == True:\n    sys.exit()  # Stop in case of a too small or too large number of features selected\neall_1 = pd.DataFrame()\nfor i in range(qall.shape[1]*3):\n    if evar1[i] == True:\n        colname = f'col_{i}'\n        eall_1[colname] = eall[:,i]\neall_1 = np.array(eall_1)\nelabeled = eall_1[:len(train),:]\neunlabeled = eall_1[len(train):,:]\nelabeled.shape,eunlabeled.shape","9cbf806e":"pca = PCA(n_components=10)\npall = pca.fit_transform(eall)\nsc_pca = StandardScaler()\npall = sc_pca.fit_transform(pall)","16c596e0":"plabeled = pall[:len(train),:]\npunlabeled = pall[len(train):,:]\nelabeled = np.hstack((elabeled,plabeled))\neunlabeled = np.hstack((eunlabeled,punlabeled))","aa17b33b":"def get_res_model():\n    \n    # Quantile-normalized input:\n    inputQ = layers.Input(shape = (qall.shape[1]))\n\n    # Encoded + principal components input:\n    inputE = layers.Input(shape= (elabeled.shape[1]))\n\n    # Quantile-binned input:\n    inputB = layers.Input(shape = (blabeled.shape [1]))\n    \n    # Feed-forward block:\n    denseQE = layers.Dropout(0.3)(layers.Concatenate()([inputQ,inputE]))\n    denseQE = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units = 300,\n                activation='elu',\n                kernel_initializer = \"lecun_normal\"))(denseQE) \n    \n   # Embedding + convolutional block:\n    embedB = layers.Embedding (input_dim = blabeled.shape [1]+1, \n                              output_dim = 6,\n                              embeddings_regularizer='l2',\n                              embeddings_initializer='lecun_uniform')(inputB)\n    embedB = layers.Dropout(0.3)(embedB)\n    embedB = layers.Conv1D(6,1,activation = 'relu')(embedB)\n    embedB = layers.Flatten()(embedB)\n    \n    # Residual block:\n    hidden = layers.Dropout(0.3)(layers.Concatenate()([denseQE,embedB]))\n    hidden = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units = 64,\n                activation='elu',\n                kernel_initializer = \"lecun_normal\"))(hidden) \n    \n    output = layers.Dropout(0.3)(layers.Concatenate()([embedB,hidden]))\n    output = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units = 32,\n                activation='relu',\n                kernel_initializer = \"lecun_normal\"))(output) \n    \n    output = layers.Dropout(0.4)(layers.Concatenate()([embedB,hidden,output]))\n    output = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units = 32,\n                activation='selu',\n                kernel_initializer = \"lecun_normal\"))(output) \n    output = layers.Dense(\n                units = 1, \n                activation ='selu',\n                kernel_initializer =\"lecun_normal\")(output)\n    # Output:\n    model = Model([inputQ,inputE,inputB],output)\n    model.compile(loss='mse',\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()],\n                  optimizer = keras.optimizers.Adam(lr=0.005))\n    \n    return model","41002287":"N_FOLDS = 10\nSEED = 1\nEPOCH = 100\nN_round = 5\n\noof = np.zeros((y.shape[0],1))\npred = np.zeros((test.shape[0],1))\n\nfor i in range (N_round):\n    \n    oof_round = np.zeros((y.shape[0],1))\n    skf = StratifiedKFold(n_splits=N_FOLDS, \n                          shuffle=True, \n                          random_state=SEED *i\n                         )\n\n    for fold, (tr_idx, ts_idx) in enumerate(skf.split(X,y)):\n        print(f\"\\n------ TRAINING ROUND {i} FOLD {fold} ------\\n\")\n        \n        # Normalized:\n        qtrain = qlabeled[tr_idx]                       \n        qvalid = qlabeled[ts_idx]\n        \n        # Binned:\n        btrain = blabeled[tr_idx]\n        bvalid = blabeled[ts_idx]\n        \n        # Encoded:\n        etrain = elabeled[tr_idx]\n        evalid = elabeled[ts_idx]\n        \n        # target:\n        ytrain = y[tr_idx]\n        yvalid = y[ts_idx]\n \n        K.clear_session()\n\n        #================= MODEL training =========\n\n        model= get_res_model()\n        model.fit([qtrain, etrain, btrain],\n                  ytrain,\n                  batch_size = 2048, \n                  epochs = EPOCH,\n                  validation_data=([qvalid, evalid, bvalid],\n                  yvalid),\n                  callbacks=[es, plateau],\n                  verbose = 0)\n\n        #============== Model prediction ==========\n        \n        pred_round = model.predict([qvalid, evalid, bvalid]) \n        oof[ts_idx] += pred_round \/ N_round\n        oof_round[ts_idx] += pred_round\n        pred += model.predict([qunlabeled, eunlabeled, bunlabeled]) \/ (N_FOLDS * N_round)\n        \n    score_round = math.sqrt(mean_squared_error(y, oof_round))\n    print(f\"==== SCORE round {i}: {score_round} ====\\n\")\n    \nscore_round = math.sqrt(mean_squared_error(y, oof))\nprint(f\"\\n***** FINAL SCORE MODEL : {score_round} *****\\n\") ","3b3dbc38":"sample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\nsample_submission['loss'] = pred\npd.DataFrame(oof).to_csv('oof_2.csv',index = False)\nsample_submission.to_csv('submission_v2.csv',index = False)\ndisplay(pd.read_csv(\"submission_v2.csv\"))","4ba94d9e":"<h2> Training & prediction","0972167c":"![image.png](attachment:dc4a4891-c506-4428-9a0f-4236090c0cb1.png)","09e88018":"<h2> Denoiser AutoEncoder","b8bb396a":"This is a python version of the R notebook : https:\/\/www.kaggle.com\/oxzplvifi\/tabular-denoising-residual-network\nPlease don't forget to upvote :-)\nAnd congrats Oscar Villarreal Escamilla.\n*As I don't know R language, I hope it will be accurate enough.*\nI made some small changes from original, such as the batch_size, rounds, ...\n\nThe NN has 3 inputs and a residual block :\n* 1. Quantile inputs\n* 1. Bins inputs\n* 1. Denoised + PCA inputs\n\n*For comparison purpose, I try to keep the same files names as much as possible.\nSome R functions have no exact equivalent functions in Python (as far as I know) so there are some functions definition and workaround.","cb4ab373":"<h2> Quantile Normalization","3af739b1":"<h2> PCA on encoded features","4e261718":"<h2> Simple bins creation","478468a6":"<h2> Features encoded selection according to variance threshold","d3be05a2":"<h2> NN Model definition","9ef2666a":"<h2>Merge of PCA + encoded features (after threshold)"}}