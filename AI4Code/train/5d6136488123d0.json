{"cell_type":{"9464ceae":"code","00e34177":"code","7d798652":"code","2be776da":"code","f67f1780":"code","815f1a75":"code","602ac61a":"code","498cf57e":"code","a22f46a6":"code","e5c62940":"code","ce561045":"code","4f739cb1":"code","102572e6":"code","5daa71e4":"code","0dce7e6b":"code","8d16fc10":"code","10d4d2ae":"code","29137a3c":"code","bfbb2a59":"code","43452f76":"code","75b2be41":"code","a5fbd483":"code","b99f92b1":"code","c01fe5f2":"code","e5c8ee7d":"code","06446022":"code","b14b42e1":"code","ef3435f9":"code","9ad5bb68":"code","24e7a0d5":"code","42c890f9":"code","1d7d8fb8":"code","a2818472":"code","ba3b6407":"code","40be5df0":"code","1ad4510c":"code","e406a8d0":"code","a76cef02":"code","91b676aa":"code","b0fa1c0e":"code","320a029c":"code","49f189ca":"code","deb3f98d":"code","43290e89":"code","ce0c6202":"code","11e58717":"code","21851863":"code","d0f2d7c3":"code","5be79db0":"code","546893b4":"code","38c85484":"code","a42ed228":"code","d140d7f4":"code","abb1d372":"code","1155dad5":"code","42e66d95":"code","463e2932":"code","cc23b97f":"code","91b0eaf3":"code","28d69d9b":"code","60a07f9d":"code","7b75d740":"code","07d65ab3":"code","30782f8b":"code","a9cc9ebf":"code","52d209c5":"code","f1495b31":"code","8fbc7ec9":"code","690c7c9f":"code","58e35b36":"code","e7b5aec8":"markdown","04fb5211":"markdown","049fc925":"markdown","aa336238":"markdown","01f7d604":"markdown","05ddb7ea":"markdown","ea46404a":"markdown","7f245dee":"markdown","f6bc3a62":"markdown","867b63e3":"markdown","2e5bd3b1":"markdown","0c926b83":"markdown","93418249":"markdown","7ac6a6fb":"markdown","e842c9a6":"markdown","5e12d550":"markdown","212c3872":"markdown","981a8f59":"markdown","35bdb121":"markdown","88d26e30":"markdown","2e909f5b":"markdown","d54c414e":"markdown","05bcacaf":"markdown","b9ccef4e":"markdown","f06d9132":"markdown","48bdaf5a":"markdown","2fb8bdc8":"markdown","70f12b83":"markdown","1f051920":"markdown","0489ece2":"markdown"},"source":{"9464ceae":"import time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","00e34177":"import pandas as pd\n\ndf = pd.read_csv('..\/input\/wikipedia-toxicity\/train.csv', encoding='utf8')\ndf.head()","7d798652":"comment_text = df['comment_text'].to_list()","2be776da":"# 1. Using regular expressions, remove IP addresses\n\nimport re\n\nrule_ip = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n\ncomment_text = [re.sub(rule_ip, '', text) for text in comment_text]","f67f1780":"# 2. Using regular expressions, remove URLs\n\nrule_url = (\"((http|https|http:\/\/www.|https:\/\/www.|www.))\" +\n             \"[a-zA-Z0-9@:%._\\\\+~#?&\/\/=]\" +\n             \"{2,256}\\\\.[a-z]\" +\n             \"{2,6}\\\\b([-a-zA-Z0-9@:%\" +\n             \"._\\\\+~#?&\/\/=]*)\")\n\ncomment_text = [re.sub(rule_url, '', text) for text in comment_text]","815f1a75":"# 3. Normalize the casing\n\ncomment_text = [text.lower() for text in comment_text]","602ac61a":"# 4. Tokenize using word_tokenize from NLTK\n\nfrom nltk.tokenize import word_tokenize\n\ncomment_text_tokens = [word_tokenize(text) for text in comment_text]","498cf57e":"# 5. & 6. Remove stop words and punctuation\n\nfrom nltk.corpus import stopwords\nimport string\n\nsw = stopwords.words('english')\npunc = list(string.punctuation)\nsw_custom = [\"must\", \"would\", 'could', \"'s\", \"n't\", \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\", \"''\", '``','...','\u2022','\u2014',]\n\nprint(sw, \"\\n\\n\", punc, \"\\n\\n\", sw_custom)","a22f46a6":"sw_punc = sw + punc + sw_custom\n\ncomment_text_cleaned = []\nfor item in comment_text_tokens:\n    cleaned_text = [word for word in item if ((word not in sw_punc and word.isascii()) and not word.isnumeric())]\n    comment_text_cleaned.append(cleaned_text)","e5c62940":"# Let us check if any non-alphabatic characters are present in cleaned text data\n\ncomment_text_check = []\nfor item in comment_text_cleaned:\n    for word in item:\n        if not word.isalpha():\n            comment_text_check.append(word)\nprint(comment_text_check)","ce561045":"comment_text_cleaned1 = []\nfor item in comment_text_cleaned:\n    cleaned_text = re.sub('[-+_\/]', ' ', \" \".join(item))\n    cleaned_text = re.sub(\"[.,|:='~^0-9\\\\\\]\", \"\", cleaned_text)\n    comment_text_cleaned1.append(cleaned_text)","4f739cb1":"comment_text_check = []\n\nfor item in comment_text_cleaned1:\n    for word in word_tokenize(item):\n        if not word.isalpha():\n            comment_text_check.append(word)\nprint(comment_text_check)","102572e6":"# 7. Define a function to perform all these steps, you\u2019ll use this later on the actual test set\n\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\n\nsw_custom = [\"must\", \"would\", 'could', \"'s\", \"n't\", \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\", \"''\", '``','...','\u2022','\u2014',]\nsw_punc = stopwords.words('english') + list(string.punctuation) + sw_custom\n   \ndef clean_text(text):\n    # Using regular expressions, remove IP addresses\n    rule_ip = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n    text_data_ip_rem = re.sub(rule_ip, '', text)\n\n    # Using regular expressions, remove URLs\n    rule_url = (\"((http|https|http:\/\/www.|https:\/\/www.|www.))\" +\n                 \"[a-zA-Z0-9@:%._\\\\+~#?&\/\/=]\" +\n                 \"{2,256}\\\\.[a-z]\" +\n                 \"{2,6}\\\\b([-a-zA-Z0-9@:%\" +\n                 \"._\\\\+~#?&\/\/=]*)\")\n    text_data_ip_url_rem = re.sub(rule_url, '', text_data_ip_rem)\n\n    # Normalize the casing\n    text_data_normalized = text_data_ip_url_rem.lower()\n\n    # Tokenize using word_tokenize from NLTK\n    text_data_tokens = word_tokenize(text_data_normalized)\n\n    # Remove stopwords and punctuations\n    text_data_sw_removed = [word for word in text_data_tokens \n                         if ((word not in sw_punc and word.isascii()) and not word.isnumeric())]\n\n    # Further cleaning text data\n    text_data_string = \" \".join(text_data_sw_removed)\n    text_data_string1 = re.sub('[-+_\/]', ' ', text_data_string)\n    text_data_cleaned = re.sub(\"[.,|:='~^0-9\\\\\\]\", \"\", text_data_string1)\n    return text_data_cleaned","5daa71e4":"import nltk\n\ncomment_text_cleaned_merged = []\nfor item in comment_text_cleaned1:\n    comment_text_cleaned_merged = comment_text_cleaned_merged + item.split()\n\ncomment_freq = nltk.FreqDist(comment_text_cleaned_merged)","0dce7e6b":"comment_freq.most_common(200)","8d16fc10":"contextual_stop_words = ['article', 'page', 'pages', 'wikipedia', 'wiki', 'talk', 'please', 'also', 'may', 'edit', 'edits', \n                         'articles', 'user', 'information', 'sources', 'source', 'content', 'wp', 'discussion', 'subject', \n                         'editor', 'editors', 'copyright', 'contributions'] ","10d4d2ae":"# Remove contextual stopwords\n   \ndef remove_contextual_sw(text):\n    cleaned_text = [word for word in text.split() if word not in contextual_stop_words]\n    return \" \".join(cleaned_text)","29137a3c":"comment_text_cleaned2 = [remove_contextual_sw(item) for item in comment_text_cleaned1]\ndf['comment_text_cleaned'] = comment_text_cleaned2\ndf.head()","bfbb2a59":"from nltk.corpus import wordnet\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","43452f76":"from nltk.stem import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\n\ndef my_lemma(text):\n    tokens = word_tokenize(text)    \n    pos_tags = nltk.pos_tag(tokens)    \n    tokens_lemmed = [wnl.lemmatize(item[0],get_wordnet_pos(item[1])) for item in pos_tags]        \n    return \" \".join(tokens_lemmed)","75b2be41":"df['comment_text_lemmed'] = df['comment_text_cleaned'].apply(lambda x: my_lemma(x))","a5fbd483":"df.head()","b99f92b1":"from sklearn.model_selection import train_test_split\n\nX = df['comment_text_lemmed']\ny  = df['toxic']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","c01fe5f2":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_model = TfidfVectorizer(ngram_range=(1,1),stop_words= None)\n\nX_train_vec = tfidf_model.fit_transform(X_train)","e5c8ee7d":"print(tfidf_model.get_feature_names())","06446022":"print(X_train_vec.todense())","b14b42e1":"X_test_vec = tfidf_model.transform(X_test)","ef3435f9":"X_vec = tfidf_model.transform(X)","9ad5bb68":"print(len(tfidf_model.get_feature_names()))","24e7a0d5":"from sklearn.svm import SVC\nclf = SVC(kernel='linear', random_state=1)","42c890f9":"clf.fit(X_train_vec, y_train)","1d7d8fb8":"y_train_pred = clf.predict(X_train_vec)\ny_train_pred","a2818472":"y_test_pred = clf.predict(X_test_vec)\ny_test_pred","ba3b6407":"clf.score(X_train_vec, y_train)*100","40be5df0":"clf.score(X_test_vec, y_test)*100","1ad4510c":"# import libraries for metrics and reporting\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","e406a8d0":"accuracy_score(y_train, y_train_pred)*100","a76cef02":"print(classification_report(y_train, clf.predict(X_train_vec)))","91b676aa":"print(classification_report(y_test, clf.predict(X_test_vec)))","b0fa1c0e":"clf1 = SVC(kernel='linear', class_weight='balanced', random_state=1)","320a029c":"clf1.fit(X_train_vec, y_train)\nprint(classification_report(y_train, clf1.predict(X_train_vec)))","49f189ca":"print(classification_report(y_test, clf1.predict(X_test_vec)))","deb3f98d":"accuracy_score(y_train, clf1.predict(X_train_vec))*100","43290e89":"clf1.score(X_test_vec, y_test)*100","ce0c6202":"from sklearn.model_selection import GridSearchCV","11e58717":"clf2 = SVC(kernel='linear', class_weight='balanced', random_state=1)","21851863":"param_grid = {\n    'C':[1, 5, 10,15,20],\n    'gamma':[1, 0.1, 0.01, 0.001, 0.0001]\n}","d0f2d7c3":"gs = GridSearchCV(estimator=clf2, param_grid=param_grid, cv=3)","5be79db0":"gs.fit(X_vec,y)","546893b4":"gs.best_params_","38c85484":"gs.best_score_*100","a42ed228":"# gs.cv_results_","d140d7f4":"clf3 = SVC(kernel='linear', C=1, gamma=1)","abb1d372":"clf3.fit(X_train_vec, y_train)","1155dad5":"clf3.score(X_train_vec, y_train)*100","42e66d95":"clf3.score(X_test_vec, y_test)*100","463e2932":"import numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\nt1 = time.time()\n\n# Bundling together all preprocessing steps for text data\ndef preprocess_text(text):\n    return my_lemma(remove_contextual_sw(clean_text(text)))\n\n# Creating pipeline for vectorization and classification model\nmodel = Pipeline(steps=[('vectorizer', TfidfVectorizer(ngram_range=(1,1),stop_words= None)),\n                              ('svc', SVC(kernel='linear', class_weight='balanced', random_state=1))\n                             ])\n\nX = df['comment_text'].apply(lambda x: preprocess_text(x))\ny = df['toxic']\n\nparam_grid = {\n    'svc__C':[0.1, 1, 10, 100],\n    'svc__gamma':[10, 1, 0.1, 0.01, 0.001]\n}\n\ni=1\nskf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\naccu_stratified = []\n\nfor train_index,test_index in skf.split(X,y):      \n    print('\\n{} of kfold {}'.format(i, skf.n_splits))    \n    Xtrain, Xtest = X[train_index], X[test_index]\n    ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n    my_model = GridSearchCV(model, param_grid=param_grid, cv=3, n_jobs=-1, scoring= 'accuracy')\n    my_model.fit(Xtrain, ytrain)\n    print(\"Best parameters: \", my_model.best_params_)\n    pred=my_model.predict(Xtest)\n    print('accuracy_score',accuracy_score(ytest,pred)*100)\n    accu_stratified.append(accuracy_score(ytest,pred)*100)\n    i+=1\n    \n# Print the output.\nprint('\\nList of possible accuracy:', accu_stratified)\nprint('\\nMaximum Accuracy That can be obtained from this model is:', max(accu_stratified), '%')\nprint('\\nMinimum Accuracy:', min(accu_stratified), '%')\nprint('\\nOverall Accuracy:', round(np.mean(accu_stratified),2), '%')\nprint('\\nStandard Deviation is:', round(np.std(accu_stratified),4))\n\nt2 = time.time()\nprint(\"\\nTime taken: {} seconds\".format(round(t2-t1,2)))","cc23b97f":"t1 = time.time()\n\n# Bundling together all preprocessing steps for text data\ndef preprocess_text(text):\n    return my_lemma(remove_contextual_sw(clean_text(text)))\n\n# Creating pipeline for vectorization and classification model\nmodel = Pipeline(steps=[('vectorizer', TfidfVectorizer(ngram_range=(1,1),stop_words= None)),\n                              ('svc', SVC(kernel='linear', class_weight='balanced', random_state=1))\n                             ])\n\nX = df['comment_text'].apply(lambda x: preprocess_text(x))\ny = df['toxic']\n\nparam_grid = {\n    'svc__C':[0.1, 1, 10, 100],\n    'svc__gamma':[10, 1, 0.1, 0.01, 0.001]\n}\n\ni=1\nskf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\nrecall_stratified = []\n\nfor train_index,test_index in skf.split(X,y):      \n    print('\\n{} of kfold {}'.format(i, skf.n_splits))    \n    Xtrain, Xtest = X[train_index], X[test_index]\n    ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n    my_model = GridSearchCV(model, param_grid=param_grid, cv=3, n_jobs=-1, scoring= 'recall')\n    my_model.fit(Xtrain, ytrain)\n    print(\"Best parameters: \", my_model.best_params_)\n    pred=my_model.predict(Xtest)\n    recall_value = round(classification_report(ytest, pred, output_dict=True)['weighted avg']['recall']*100,2)\n    print('Recall_value: ', recall_value)\n    recall_stratified.append(recall_value)\n    i+=1\n    \n# Print the output.\nprint('\\nList of possible Recall_value:', recall_stratified)\nprint('\\nMaximum Recall_value That can be obtained from this model is:', max(recall_stratified), '%')\nprint('\\nMinimum Recall_value:', min(recall_stratified), '%')\nprint('\\nOverall Recall_value:', round(np.mean(recall_stratified),2), '%')\nprint('\\nStandard Deviation is:', round(np.std(recall_stratified),4))\n\nt2 = time.time()\nprint(\"\\nTime taken: {} seconds\".format(round(t2-t1,2)))","91b0eaf3":"model_final = SVC(kernel='linear', C=1, gamma=10, class_weight='balanced', random_state=1)\nmodel_final.fit(X_train_vec, y_train)\ny_test_pred = model_final.predict(X_test_vec)\nprint(\"\\nModel accuracy with train data: \", model_final.score(X_train_vec, y_train)*100)\nprint(\"\\nModel accuracy with test data: \", model_final.score(X_test_vec, y_test)*100)\nprint(\"\\nClassification report with train data:\\n\", classification_report(y_train, model_final.predict(X_train_vec)))\nprint(\"\\nClassification report with test data:\\n\", classification_report(y_test, model_final.predict(X_test_vec)))","28d69d9b":"recall_value_test = classification_report(y_test, y_test_pred, output_dict=True)['weighted avg']['recall']*100","60a07f9d":"print(\"\\nModel Recall value with test data: \", recall_value_test)","7b75d740":"f1_score_test = classification_report(y_test, y_test_pred, output_dict=True)['weighted avg']['f1-score']*100","07d65ab3":"print(\"\\nModel f1-score with test data: \", f1_score_test)","30782f8b":"# Separate the comments from the test set that the model identified as toxic\n\nX_test[y_test_pred==1]","a9cc9ebf":"text_merged = []\nfor item in X_test[y_test_pred==1]:\n    text_merged = text_merged + item.split()","52d209c5":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt","f1495b31":"title = 'Word Cloud Analysis for Text Data'\nwordcloud = WordCloud(\n    background_color='white',\n    stopwords=None,\n    max_words=50,\n    max_font_size=40, \n    scale=3,\n    random_state=1 \n).generate(\" \".join(text_merged))\n\nfig = plt.figure(1, figsize=(10, 10))\nplt.axis('off')\nif title: \n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(top=2.3)\n    \nplt.imshow(wordcloud);","8fbc7ec9":"# Make one large list of the terms\n\nfreq_words = nltk.FreqDist(text_merged)\nprint(freq_words.most_common(100))","690c7c9f":"# Get the top 15 terms\n\nfreq_words.most_common(15)","58e35b36":"model = Pipeline(steps=[('vectorizer', TfidfVectorizer(ngram_range=(1,1),stop_words= None)),\n                              ('svc', SVC(kernel='linear', C=1, gamma=10, class_weight='balanced', random_state=1))\n                             ])\nfinal_model = model.fit(X,y)","e7b5aec8":"### Solution:\n**1. Load the data using read_csv function from pandas package**","04fb5211":"**2. Get the comments into a list, for easy text cleanup and manipulation**","049fc925":"**We can see that after setting *class_weight='balanced'* our model performance has improved with better recall and f1-score**","aa336238":"**11. Hyperparameter tuning**\n1.\tImport GridSearch and StratifiedKFold (because of class imbalance)\n2.\tProvide the parameter grid to choose for \u2018C\u2019\n3.\tUse a balanced class weight while instantiating the Support Vector Classifier","01f7d604":"#### Packing all functions in a pipeline for better organizing code:","05ddb7ea":"#### We have cleaned our text data completely. Lets move to next step.","ea46404a":"**Tasks:**\n\n1. Load the data using read_csv function from pandas package\n2. Get the comments into a list, for easy text cleanup and manipulation\n3. Cleanup: \n    1. Using regular expressions, remove IP addresses\n    2. Using regular expressions, remove URLs\n    3. Normalize the casing\n    4. Tokenize using word_tokenize from NLTK\n    5. Remove stop words\n    6. Remove punctuation\n    7. Define a function to perform all these steps, you\u2019ll use this later on the actual test set\n4. Using a counter, find the top terms in the data. \n    1. Can any of these be considered contextual stop words? \n    2. Words like \u201cWikipedia\u201d, \u201cpage\u201d, \u201cedit\u201d are examples of contextual stop words\n    3. If yes, drop these from the data\n5. Separate into train and test sets\n    1. Use train-test method to divide your data into 2 sets: train and test\n    2. Use a 70-30 split\n6. Use TF-IDF values for the terms as feature to get into a vector space model\n    1. Import TF-IDF vectorizer from sklearn\n    2. Instantiate with a maximum of 4000 terms in your vocabulary\n    3. Fit and apply on the train set\n    4. Apply on the test set\n7. Model building: Support Vector Machine\n    1. Instantiate SVC from sklearn with a linear kernel\n    2. Fit on the train data\n    3. Make predictions for the train and the test set\n8. Model evaluation: Accuracy, recall, and f1_score\n    1. Report the accuracy on the train set\n    2. Report the recall on the train set:decent, high, low?\n    3. Get the f1_score on the train set\n9. Looks like you need to adjust  the class imbalance, as the model seems to focus on the 0s\n    1. Adjust the appropriate parameter in the SVC module\n10. Train again with the adjustment and evaluate\n    1. Train the model on the train set\n    2. Evaluate the predictions on the validation set: accuracy, recall, f1_score\n11. Hyperparameter tuning\n    1. Import GridSearch and StratifiedKFold (because of class imbalance)\n    2. Provide the parameter grid to choose for \u2018C\u2019\n    3 Use a balanced class weight while instantiating the Support Vector Classifier\n12. Find the parameters with the best recall in cross validation\n    1. Choose \u2018recall\u2019 as the metric for scoring\n    2. Choose stratified 5 fold cross validation scheme\n    3. Fit on the train set\n13. What are the best parameters?\n14. Predict and evaluate using the best estimator\n    1. Use best estimator from the grid search to make predictions on the test set\n    2. What is the recall on the test set for the toxic comments?\n    3. What is the f1_score?\n15. What are the most prominent terms in the toxic comments?\n    1. Separate the comments from the test set that the model identified as toxic\n    2. Make one large list of the terms\n    3. Get the top 15 terms","7f245dee":"**Solution:** \nREF: https:\/\/chrisalbon.com\/code\/machine_learning\/support_vector_machines\/imbalanced_classes_in_svm\/\n\nIn support vector machines, **C** is a hyperparameter determining the penalty for misclassifying an observation. One method for handling imbalanced classes in support vector machines is to weight **C** by classes, so that\n![image.png](attachment:image.png)\nwhere **C** is the penalty for misclassification, wj is a weight inversely proportional to class j\u2019s frequency, and Cj is the **C** value for class j. \n\nThe general idea is to increase the penalty for misclassifying minority classes to prevent them from being \u201coverwhelmed\u201d by the majority class. In scikit-learn, when using **SVC** we can set the values for **Cj** automatically by setting **class_weight='balanced'**. The **balanced** argument automatically weighs classes such that:\n![image-2.png](attachment:image-2.png)\nwhere wj is the weight to class j, n is the number of observations, nj is the number of observations in class j, and k is the total number of classes.","f6bc3a62":"#### Let us create final model with complete data:","867b63e3":"**We can see from above  output that we need to perform following additional task for more text cleaning:**\n- Replace following characters \"-\", \"_\", \"\/\", \"+\" with \" \"\n- Replace \".\" \",\" \"|\" \":\" \"=\", \"'\", \"~\", \"^\", \"\\\", \"0-9\" and \"\\\" with \"\"","2e5bd3b1":"**3. Cleanup:**","0c926b83":"**recall** value for class **'0'** is 1 (very high) but for class **'1'** is 0.80 (very low compared to recall value for class 0).\n\nSimilarly **f1-score** value for class **'0'** is 0.99 (very high) but for class **'1'** is 0.89 (low compared to recall value for class 0).","93418249":"### Project - II (Wikipedia Toxicity)\n**DESCRIPTION** Using NLP and machine learning, make a model to identify toxic comments from the Talk edit pages on Wikipedia. Help identify the words that make a comment toxic.\n\n**Problem Statement:** Wikipedia is the world\u2019s largest and most popular reference work on the internet with about 500 million unique visitors per month. It also has millions of contributors who can make edits to pages. The Talk edit pages, the key community interaction forum where the contributing community interacts or discusses or debates about the changes pertaining to a particular topic. \n\nWikipedia continuously strives to help online discussion become more productive and respectful. You are a data scientist at Wikipedia who will help Wikipedia to build a predictive model that identifies toxic comments in the discussion and marks them for cleanup by using NLP and machine learning. Post that, help identify the top terms from the toxic comments.","7ac6a6fb":"**8. Model evaluation: Accuracy, recall, and f1_score**\n1.\tReport the accuracy on the train set\n2.\tReport the recall on the train set:decent, high, low?\n3.\tGet the f1_score on the train set","e842c9a6":"### Optional Step:\n**Performing lemmatization on cleaned text:** Since lemmatization will reduce text size by converting each word to its root word which will increase code efficiency without impacting its performance much.","5e12d550":"**13. What are the best parameters?**","212c3872":"**12. Find the parameters with the best recall in cross validation**\n1.\tChoose \u2018recall\u2019 as the metric for scoring\n2.\tChoose stratified 5 fold cross validation scheme\n3.\tFit on the train set","981a8f59":"**4.\tUsing a counter, find the top terms in the data.**\n1.\tCan any of these be considered contextual stop words? \n2.\tWords like \u201cWikipedia\u201d, \u201cpage\u201d, \u201cedit\u201d are examples of contextual stop words\n3.\tIf yes, drop these from the data\n","35bdb121":"**9. Looks like you need to adjust  the class imbalance, as the model seems to focus on the 0s**\n1.\tAdjust the appropriate parameter in the SVC module","88d26e30":"**Domain:** Internet\n\n**Analysis to be done:** Build a text classification model using NLP and machine learning that detects toxic comments.\n\n**Content:**\n- id: identifier number of the comment\n- comment_text: the text in the comment\n- toxic: 0 (non-toxic) \/1 (toxic)\n\n**Steps to perform:** Cleanup the text data, using TF-IDF convert to vector space representation, use Support Vector Machines to detect toxic comments. Finally, get the list of top 15 toxic terms from the comments identified by the model.","2e909f5b":"**15. What are the most prominent terms in the toxic comments?**\n1.\tSeparate the comments from the test set that the model identified as toxic\n2.\tMake one large list of the terms\n3.\tGet the top 15 terms","d54c414e":"#### Hyper parameter tuning using *GridSearchCV* and *StratifiedKFold*","05bcacaf":"**As we observe above SVM classifier model accuracy for train data set is 98.22%**","b9ccef4e":"**Solution:** StratifiedKFolds is a cross-validator and provides train\/test indices to split data in train\/test sets. This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.","f06d9132":"**Solution:** We can see from above output that best parameters to get optimum Recall value (95.3%) are: \n- **Best parameters:  {'C': 1, 'gamma': 10}**","48bdaf5a":"**5. Separate into train and test sets**\n1.\tUse train-test method to divide your data into 2 sets: train and test\n2.\tUse a 70-30 split","2fb8bdc8":"**7. Model building: Support Vector Machine**\n1.\tInstantiate SVC from sklearn with a linear kernel\n2.\tFit on the train data\n3.\tMake predictions for the train and the test set","70f12b83":"**6. Use TF-IDF values for the terms as feature to get into a vector space model**\n1.\tImport TF-IDF vectorizer from sklearn\n2.\tInstantiate with a maximum of 4000 terms in your vocabulary\n3.\tFit and apply on the train set\n4.\tApply on the test set\n","1f051920":"**10. Train again with the adjustment and evaluate**\n1.\tTrain the model on the train set\n2.\tEvaluate the predictions on the validation set: accuracy, recall, f1_score","0489ece2":"**14. Predict and evaluate using the best estimator**\n1.\tUse best estimator from the grid search to make predictions on the test set\n2.\tWhat is the recall on the test set for the toxic comments?\n3.\tWhat is the f1_score?"}}