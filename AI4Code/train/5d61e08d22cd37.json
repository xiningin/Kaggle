{"cell_type":{"d3237daa":"code","d56f0eba":"code","42a42f3f":"code","f18f3b25":"code","47d3cf1e":"code","1f6b2f4d":"code","d237c933":"code","c97296f8":"code","34cf4cf4":"code","2e21044b":"code","a02ad6e6":"code","e655e19c":"code","e983f374":"code","fe534024":"code","f846e27b":"code","4b529263":"code","4aef7330":"code","8159b9f2":"code","96a01002":"code","c7462f12":"code","cdc62ac6":"code","32ea4e51":"code","dede5bc3":"code","51cde91c":"code","858b228c":"code","78311bfc":"code","7be5184d":"code","8292d22d":"code","21a8cf46":"code","5d2e43dc":"code","45d641f5":"code","6e3e568a":"code","59f99a34":"code","9d42005b":"code","c90709a9":"code","787eb41a":"code","c17c9455":"code","0805cd2f":"code","dfcb0c18":"code","82507b1f":"code","17d5a5eb":"code","f5d4253c":"code","1ce7b0b2":"code","774c61ff":"code","791b984a":"code","bc0a6b6a":"code","93fafa88":"markdown","01add4a8":"markdown","e30cb722":"markdown","ac8944f1":"markdown","42abb9c8":"markdown","693a7046":"markdown","f6a02552":"markdown","6fb50d58":"markdown","2a81e133":"markdown","e1631b53":"markdown","0d1d941f":"markdown","20ed3eb6":"markdown","e5e8cfe0":"markdown","47590e36":"markdown","0642f2c8":"markdown","192c3b52":"markdown","36cd9338":"markdown","536dea44":"markdown","b1180af7":"markdown","16d30b29":"markdown","6b064ac4":"markdown","4c5b6505":"markdown"},"source":{"d3237daa":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\" ","d56f0eba":"import numpy as np  \nimport pandas as pd \nfrom datetime import datetime\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, Ridge \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import make_scorer\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.linear_model import LinearRegression\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","42a42f3f":"print(os.listdir(\"..\/input\"))\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)\nprint('start data processing', datetime.now(), )","f18f3b25":"# know your target\ntrain['SalePrice'].describe()","47d3cf1e":"sns.distplot(train['SalePrice']);","1f6b2f4d":"#skewness and kurtosis: \u53ef\u4ee5\u770b\u5230SalePrice\u7684\u504f\u5ea6\u8f83\u5927\uff0clog\u53d8\u6362\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u800c\u4e14\u6bd4\u8d5b\u7684\u635f\u5931\u51fd\u6570\u4e5f\u6b63\u597d\u662flog-rmse\uff0c\u6240\u4ee5\u968f\u540e\u4f1a\u5bf9SalePrice\u4f5clog-transformation\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","d237c933":"# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#much better\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","c97296f8":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","34cf4cf4":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","2e21044b":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show()","a02ad6e6":"def detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx","e655e19c":"outs = detect_outliers(train['GrLivArea'], train['SalePrice'],top=5) #got 1298,523\nouts","e983f374":"outs = detect_outliers(train['LowQualFinSF'], train['SalePrice'],top=5)#got 88\nouts","fe534024":"#\u5f88\u591apublic kernel\u4e2d\u90fd\u7528\u8fd9\u4e9b\u70b9\uff0c88,523,1298\u5f88\u5bb9\u6613\u627e\u5230\uff0c\u5bf9\u4e8e\u5176\u4ed6\u7684outliers\u540e\u9762\u4f1a\u8865\u5145\u8bf4\u660e\n#\u6539\u8fdb\u70b9\uff18\uff1amore or less outliers\noutliers = [30, 88, 462, 523, 632, 1298, 1324]","f846e27b":"#all_outliers\u53ea\u5305\u542b30,88,523,1298\uff0c\u5176\u4ed6\u7684outliers\u662f\u600e\u4e48\u5f97\u5230\u7684\uff1f\n#\u53ef\u80fd\u7684\u539f\u56e0\uff1a\n#1.detect_outliers\u51fd\u6570\u4e2d\u7684\u53c2\u6570\u8bbe\u7f6e\u95ee\u9898\n#2.\u8fd9\u91cc\u4ec5\u4ece\u7279\u5f81\u4e0etrain['SalePrice']\u7684\u5173\u7cfb\u6765\u5bfb\u627eoutliers,\u6216\u8bb8\u4e5f\u53ef\u4ee5\u4ece\u7279\u5f81\u4e0e\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u5bfb\u627eoutliers\nfrom collections import Counter\nall_outliers=[]\nnumeric_features = train.dtypes[train.dtypes != 'object'].index\nfor feature in numeric_features:\n    try:\n        outs = detect_outliers(train[feature], train['SalePrice'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\nfor i in outliers:\n    if i in all_outliers:\n        print(i)","4b529263":"#delete outliers\ntrain = train.drop(train.index[outliers])\ntrain.shape","4aef7330":"#\u5408\u5e76train,test\u7684\u7279\u5f81\uff0c\u4fbf\u4e8e\u7edf\u4e00\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\nfeatures.drop(['Id'], axis=1, inplace=True)\nprint(features.shape)","8159b9f2":"#2.1 \u4e00\u4e9b\u7279\u5f81\u5176\u88ab\u8868\u793a\u6210\u6570\u503c\u7279\u5f81\u7f3a\u4e4f\u610f\u4e49\uff0c\u4f8b\u5982\u5e74\u4efd\u8fd8\u6709\u7c7b\u522b(\u6709\u4e9b\u7c7b\u522b\u4f7f\u7528\u6570\u5b57\u8868\u793a\uff0c\u4f1a\u88ab\u8bef\u8ba4\u4e3a\u662f\u6570\u503c\u53d8\u91cf)\uff0c\u8fd9\u91cc\u5c06\u5176\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\uff0c\u5373\u7c7b\u522b\u578b\u53d8\u91cf\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n# \u6539\u8fdb\u70b91\uff1aOverallQual\uff0cOverallCond\u4e5f\u662f\u7531\u6570\u5b57\u8868\u793a\u7684\u7c7b\u522b\u53d8\u91cf\uff0c\u4f46\u5185\u542b\u987a\u5e8f\u4fe1\u606f\n# features['OverallQual'] = features['OverallQual'].astype(str)\n# features['OverallCond'] = features['OverallCond'].astype(str)","96a01002":"#2.2 numeric_features and \nnumeric_features = features.dtypes[features.dtypes != 'object'].index\nnumeric_features\nlen(numeric_features) #33\ncategory_features = features.dtypes[features.dtypes == 'object'].index\ncategory_features\nlen(category_features) #46","c7462f12":"#2.3 special features with NA---> NO such feature\uff08NA\u4e0d\u662f\u771f\u6b63\u7684\u7f3a\u5931\u503c\uff0c\u800c\u662f\u8be5\u6837\u672c\u6ca1\u6709\u8fd9\u4e2a\u7279\u5f81)\nspecial_features = [\n    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n    'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n    'PoolQC', 'Fence'\n]\nlen(special_features)","cdc62ac6":"#1.\u7c7b\u522b\u578b\u7279\u5f81\uff0c\u4f46\u660e\u8bf4\u4e86\u5177\u6709\u5178\u578b\u503c\u7684\uff1afillna with Typical values\nfeatures['Functional'] = features['Functional'].fillna('Typ') #Typ\tTypical Functionality\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\") #SBrkr\tStandard Circuit Breakers & Romex\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\") #TA\tTypical\/Average\n\n#2.\u5206\u7ec4\u586b\u5145\n#groupby\uff1aGroup DataFrame or Series using a mapper or by a Series of columns.\n#transform\u662f\u4e0egroupby\uff08pandas\u4e2d\u6700\u6709\u7528\u7684\u64cd\u4f5c\u4e4b\u4e00\uff09\u7ec4\u5408\u4f7f\u7528\u7684,\u6062\u590d\u7ef4\u5ea6\n#\u5bf9MSZoning\u6309MSSubClass\u5206\u7ec4\u586b\u5145\u4f17\u6570\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n#\u5bf9LotFrontage\u6309Neighborhood\u5206\u7ec4\u586b\u5145\u4e2d\u4f4d\u6570(\u623f\u5b50\u5230\u8857\u9053\u7684\u8ddd\u79bb\u5148\u6309\u7167\u5730\u7406\u4f4d\u7f6e\u5206\u7ec4\u518d\u586b\u5145\u5404\u81ea\u7684\u4e2d\u4f4d\u6570)\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n#3. fillna with new type: \u2018None\u2019(\u6216\u8005\u5176\u4ed6\u4e0d\u4f1a\u548c\u5df2\u6709\u7c7b\u540d\u91cd\u590d\u7684str\uff09\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\") #note \"None\" is a str, (NA\tNo Pool)\n#\u8f66\u5e93\u76f8\u5173\u7684\u7c7b\u522b\u53d8\u91cf\uff0c\u4f7f\u7528\u65b0\u7c7b\u522b\u5b57\u7b26\u4e32'None'\u586b\u5145\u7a7a\u503c\u3002\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n#\u5730\u4e0b\u5ba4\u76f8\u5173\u7684\u7c7b\u522b\u53d8\u91cf\uff0c\u4f7f\u7528\u5b57\u7b26\u4e32'None'\u586b\u5145\u7a7a\u503c\u3002\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\n#4. fillna with 0: \u6570\u503c\u578b\u7684\u7279\u6b8a\u53d8\u91cf\n#\u8f66\u5e93\u76f8\u5173\u7684\u6570\u503c\u578b\u53d8\u91cf\uff0c\u4f7f\u75280\u586b\u5145\u7a7a\u503c\u3002\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\n\n#5.\u586b\u5145\u4f17\u6570\n#\u5bf9\u4e8e\u5217\u540d\u4e3a'Exterior1st'\u3001'Exterior2nd'\u3001'SaleType'\u7684\u7279\u5f81\u5217\uff0c\u4f7f\u7528\u5217\u4e2d\u7684\u4f17\u6570\u586b\u5145\u7a7a\u503c\u3002\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\n#6. \u7edf\u4e00\u586b\u5145\u5269\u4f59\u7684\u6570\u503c\u7279\u5f81\u548c\u7c7b\u522b\u7279\u5f81\nfeatures[numeric_features] = features[numeric_features].apply(\n            lambda x: x.fillna(0)) #\u6539\u8fdb\u70b92\uff1a\u6ca1\u505a\u6807\u51c6\u5316\uff0c\u8fd9\u91cc\u628a0\u6362\u6210\u5747\u503c\u66f4\u597d\u5427\uff1f\nfeatures[category_features] = features[category_features].apply(\n            lambda x: x.fillna('None')) #\u6539\u8fdb\u70b93\uff1a\u53ef\u4ee5\u8003\u8651\u5c06\u65b0\u7c7b\u522b'None'\u6362\u6210\u4f17\u6570","32ea4e51":"#2.5 data transformation\n#\u6570\u5b57\u578b\u6570\u636e\u5217\u504f\u5ea6\u6821\u6b63\n#\u4f7f\u7528skew()\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6240\u6709\u6574\u578b\u548c\u6d6e\u70b9\u578b\u6570\u636e\u5217\u4e2d\uff0c\u6570\u636e\u5206\u5e03\u7684\u504f\u5ea6\uff08skewness\uff09\u3002\n#\u504f\u5ea6\u662f\u7edf\u8ba1\u6570\u636e\u5206\u5e03\u504f\u659c\u65b9\u5411\u548c\u7a0b\u5ea6\u7684\u5ea6\u91cf\uff0c\u662f\u7edf\u8ba1\u6570\u636e\u5206\u5e03\u975e\u5bf9\u79f0\u7a0b\u5ea6\u7684\u6570\u5b57\u7279\u5f81\u3002\u4ea6\u79f0\u504f\u6001\u3001\u504f\u6001\u7cfb\u6570\u3002 \nskew_features = features[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\n#\u6539\u8fdb\u70b95\uff1a\u8c03\u6574\u9608\u503c\uff0c\u539f\u6587\u4ee50.5\u4f5c\u4e3a\u57fa\u51c6\uff0c\u7edf\u8ba1\u504f\u5ea6\u8d85\u8fc7\u6b64\u6570\u503c\u7684\u9ad8\u504f\u5ea6\u5206\u5e03\u6570\u636e\u5217\uff0c\u83b7\u53d6\u8fd9\u4e9b\u6570\u636e\u5217\u7684index\nhigh_skew = skew_features[skew_features > 0.15]\nskew_index = high_skew.index\n\n#\u5bf9\u9ad8\u504f\u5ea6\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u6b63\u6001\u5206\u5e03\n#Box\u548cCox\u63d0\u51fa\u7684\u53d8\u6362\u53ef\u4ee5\u4f7f\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u6ee1\u8db3\u7ebf\u6027\u6027\u3001\u72ec\u7acb\u6027\u3001\u65b9\u5dee\u9f50\u6b21\u4ee5\u53ca\u6b63\u6001\u6027\u7684\u540c\u65f6\uff0c\u53c8\u4e0d\u4e22\u5931\u4fe1\u606f\n#\u4e5f\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684log\u53d8\u6362\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","dede5bc3":"#2.6 \u7279\u5f81\u5220\u9664\u548c\u878d\u5408\u521b\u5efa\u65b0\u7279\u5f81\n#features['Utilities'].describe()\n#Utilities: all values are the same(AllPub 2914\/2915)\n#Street: Pave 2905\/2917\n#PoolQC: too many missing values, del_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu'] missing>50%\n#\u6539\u8fdb\u70b94\uff1a\u5220\u9664\u66f4\u591a\u7279\u5f81del_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu']\nfeatures = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1) \n#features = features.drop(['Utilities', 'Street', 'PoolQC','MiscFeature', 'Alley', 'Fence'], axis=1) #FireplaceQu\u5efa\u8bae\u4fdd\u7559\n\n#\u878d\u5408\u591a\u4e2a\u7279\u5f81\uff0c\u751f\u6210\u65b0\u7279\u5f81\n#\u6539\u8fdb\u70b96\uff1a\u53ef\u4ee5\u5c1d\u8bd5\u7ec4\u5408\u51fa\u66f4\u591a\u7684\u7279\u5f81\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n#\u7b80\u5316\u7279\u5f81\u3002\u5bf9\u4e8e\u67d0\u4e9b\u5206\u5e03\u5355\u8c03\uff08\u6bd4\u5982100\u4e2a\u6570\u636e\u4e2d\u670999\u4e2a\u7684\u6570\u503c\u662f0.9\uff0c\u53e61\u4e2a\u662f0.1\uff09\u7684\u6570\u5b57\u578b\u6570\u636e\u5217\uff0c\u8fdb\u884c01\u53d6\u503c\u5904\u7406\u3002\n#PoolArea: unique      13, top          0, freq      2905\/2917\n#2ndFlrSF: unique      633, top          0, freq      1668\/2917\n#2ndFlrSF: unique      5, top          0, freq      1420\/2917\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n","51cde91c":"#2.7 get_dummies\nprint(\"before get_dummies:\",features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(\"after get_dummies:\",final_features.shape)\n\nX = final_features.iloc[:len(y), :]\t\nX_sub = final_features.iloc[len(y):, :]\nprint(\"after get_dummies, the dataset size:\",'X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","858b228c":"#2.8 #\u5220\u9664\u53d6\u503c\u8fc7\u4e8e\u5355\u4e00\uff08\u6bd4\u5982\u67d0\u4e2a\u503c\u51fa\u73b0\u4e8699%\u4ee5\u4e0a\uff09\u7684\u7279\u5f81\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94: #\u6539\u8fdb\u70b97\uff1a99.94\u662f\u53ef\u4ee5\u8c03\u6574\u7684\uff0c80,90,95\uff0c99...\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = np.array(X.drop(overfit, axis=1).copy())\ny = np.array(y)\nX_sub = np.array(X_sub.drop(overfit, axis=1).copy())\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n\nprint('feature engineering finished!', datetime.now())","78311bfc":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n#\u5b9a\u4e49\u5747\u65b9\u6839\u5bf9\u6570\u8bef\u5dee\uff08Root Mean Squared Logarithmic Error \uff0cRMSLE\uff09\ndef rmsle(y, y_pred):\n    return np.sqrt(mse(y, y_pred))\n\n#\u521b\u5efa\u6a21\u578b\u8bc4\u5206\u51fd\u6570\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","7be5184d":"#3.1 parameters(for grid search)\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","8292d22d":"#3.2 single model\n#\u6539\u8fdb\u70b99:more models\n#\u6539\u8fdb\u70b910: \u5bf9svr\uff0cGradientBoostingRegressor\uff0cLGBMRegressor\uff0cXGBRegressor\u7b49\u505aGridSearchCV\n#ridge\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n#lasso\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n\n#elastic net\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n#svm\nsvr = make_pipeline(RobustScaler(), SVR(\n    C=20,\n    epsilon=0.008,\n    gamma=0.0003,\n))\n\n#GradientBoosting\uff08\u5c55\u5f00\u5230\u4e00\u9636\u5bfc\u6570\uff09\ngbr = GradientBoostingRegressor(n_estimators=3000,\n                                learning_rate=0.05,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)\n\n#lightgbm\nlightgbm = LGBMRegressor(\n    objective='regression',\n    num_leaves=4,\n    learning_rate=0.01,\n    n_estimators=5000,\n    max_bin=200,\n    bagging_fraction=0.75,\n    bagging_freq=5,\n    bagging_seed=7,\n    feature_fraction=0.2,\n    feature_fraction_seed=7,\n    verbose=-1,\n    #min_data_in_leaf=2,\n    #min_sum_hessian_in_leaf=11\n)\n\n#xgboost\uff08\u5c55\u5f00\u5230\u4e8c\u9636\u5bfc\u6570\uff09\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=3460,\n                       max_depth=3,\n                       min_child_weight=0,\n                       gamma=0,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006)","21a8cf46":"#3.3 stacking\n#StackingCVRegressor\uff1aA 'Stacking Cross-Validation' regressor for scikit-learn estimators.\n#regressors=(...)\u4e2d\u5e76\u6ca1\u6709\u7eb3\u5165\u524d\u9762\u7684svr\u6a21\u578b,\u4f3c\u4e4e\u7eb3\u5165svr\u4e4b\u540e\u6027\u80fd\u53cd\u800c\u53d8\u5dee(why?)\uff1astacking\u6a21\u578b\u7684\u6027\u80fd0.11748--->0.11873\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","5d2e43dc":"#3.4 \u89c2\u5bdf\u5355\u6a21\u578b\u7684\u6548\u679c\nprint('TEST score on CV')\n\nscore = cv_rmse(ridge) #cross_val_score(RidgeCV(alphas),X, y) \u5916\u5c42k-fold\u4ea4\u53c9\u9a8c\u8bc1, \u6bcf\u6b21\u8c03\u7528modelCV.fit\u65f6\u5185\u90e8\u4e5f\u4f1a\u8fdb\u884ck-fold\u4ea4\u53c9\u9a8c\u8bc1\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1024\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1031\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )#0.1031 \n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1023\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )#0.1061\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )#0.1072\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1064","45d641f5":"#3.5 train the stacking model\n#stacking 3\u6b65\u8d70(\u53ef\u4e0d\u7528\u7ba1\u7ec6\u8282\uff0cfit\u4f1a\u5b8c\u6210stacking\u7684\u6574\u4e2a\u6d41\u7a0b)\uff1a\n#1.1 learn first-level model\n#1.2 construct a training set for second-level model\n#2. train the second-level model:\u5b66\u4e60\u7b2c2\u5c42\u7684\u6a21\u578b\uff0c\u4e5f\u5c31\u662f\u5b66\u4e60\u5982\u4f55\u878d\u5408\u7b2c1\u5c42\u7684\u6a21\u578b\n#3. re-learn first-level model on the entire train set\nprint('START Fit')\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X, y) #Fit ensemble regressors and the meta-regressor.","6e3e568a":"#4.1 submit stacking result\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(stack_gen_model.predict(X_sub)))\nsubmission.head()\nsubmission.to_csv(\"submission_stacking.csv\", index=False) #0.11674","59f99a34":"#4.1 \u5728\u6574\u4e2a\u8bad\u7ec3\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u7b2c1\u5c42\u7684\u5355\u6a21\u578b\u548csvr\uff0c\u540e\u9762blending\u7528(\u5982\u679c\u76f4\u63a5\u62ffstacking\u7b2c1\u5c42\u7684\u6a21\u578b\uff0c\u4f1a\u62a5not fit\u7684\u9519\u8bef)\nprint(datetime.now(), 'ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'svr')\nsvr_model_full_data = svr.fit(X, y)","9d42005b":"#\u5f85\u6df7\u5408\u7684models\nmodels = [\n    ridge_model_full_data, lasso_model_full_data, elastic_model_full_data,\n    gbr_model_full_data, xgb_model_full_data, lgb_model_full_data,\n    svr_model_full_data, stack_gen_model\n]\nlen(models)","c90709a9":"#linear blending coefficients: public coefs\n#order: ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, stack\npublic_coefs = [0.1, 0.1, 0.1, 0.1, 0.15, 0.1, 0.1, 0.25]\nbias = 0","787eb41a":"def linear_blend_models_predict(data_x,models,coefs, bias):\n    tmp=[model.predict(data_x) for model in models]\n    tmp = [c*d for c,d in zip(coefs,tmp)]\n    pres=np.array(tmp).swapaxes(0,1) #numpy\u4e2d\u7684reshape\u4e0d\u80fd\u7528\u4e8e\u4ea4\u6362\u7ef4\u5ea6\uff0c\u4e00\u5f00\u59cb\u7684\u79cd\u79cd\u95ee\u9898\uff0c\u7686\u7531\u6b64\u6765\n    pres=np.sum(pres,axis=1)\n    return pres","c17c9455":"#4.2 submit blend_models_with_public_coefs \n\nprint('blending models RMSLE score on train data:')\nprint(rmsle(y, linear_blend_models_predict(X,models,public_coefs, bias)))\n\n#before Blend with Top Kernals submissions\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(linear_blend_models_predict(X_sub,models,public_coefs, bias))) #expm1: exp(x) - 1; \u6ce8\u610f\u8fd8\u8981\u53d6\u6574\n# submission.iloc[:,1] = np.expm1(blend_models_predict(X_sub)) \nsubmission.head()\nsubmission.to_csv(\"submission_blend_models_with_public_coefs.csv\", index=False) #0.11413","0805cd2f":"#linear blending coefficients: coefs got by linear regression \n#\u6ce8\u610f\uff1a\u8fd9\u91cc\u5f88\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u6240\u4ee5alphas3\u7684\u4e0d\u5b9c\u8fc7\u5c0f\n#alphas3 = np.linspace(0,1e3,1001) #\u5982\u679c\u4ece\uff10\u5f00\u59cb\uff0cRidgeCV\u4f1a\u9009\u62e9\uff10\uff0ctrain_rmse:0.027818\uff0c\u4f46test_rmse\u4f1a\u5f88\u5927\uff0c\u5373\u8fc7\u62df\u5408\uff01\n#\u6539\u8fdb\u70b911\uff1a\u5982\u4f55\u5f97\u5230\u66f4\u5408\u9002\u7684\u7cfb\u6570\nalphas3 = [70] #\u53ef\u4ee5\u7ee7\u7eed\u4f18\u5316\uff0c\u4f7ftrain_rmse\u4e0epublic_coefs\u7684\u7ed3\u679c\u63a5\u8fd1\n\ndef blend_models(train_x, train_y, models):\n    tmp = [model.predict(train_x) for model in models]\n    pres = np.array(tmp).swapaxes(0,1) #\u4e00\u5f00\u59cb\u7528\u7684reshape\uff0c\u6ce8\u610f\u8fd9\u4e0epytorch\u4e2d\u4e0d\u540c\uff0c\u4e0d\u80fd\u7528\u4e8e\u591a\u7ef4\u7684\u7ef4\u5ea6\u95f4\u7684\u4ea4\u6362\uff01\uff01\uff01\n    print(pres.shape)  #(1457,8)\n    #\u6ce8\u610f\u8981\u8bbe\u7f6efit_intercept=False\uff0c\u5426\u5219bias\u4f1a\u5f88\u5927\uff0c\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u800c\u7cfb\u6570coef_\u90fd\u5f88\u5c0f\n    #fit_intercept=False\u65f6\u4e0d\u6c42\u622a\u8ddd\uff0c\u4f46\u8981\u6c42\u6570\u636e\u63d0\u524d\u4e2d\u5fc3\u5316\uff0c\u5e76\u4e14\u6b64\u65f6\u4f1a\u5ffd\u7565normalize\u53c2\u6570\n    #linear = LinearRegression(fit_intercept=False)\n    linear = RidgeCV(alphas=alphas3,\n                     cv=kfolds,\n                     fit_intercept=False,\n                     scoring=make_scorer(rmsle, greater_is_better=False)\n                    )\n    linear = linear.fit(pres, train_y)\n    print('linear coefficient:')\n    print(linear.coef_)\n    print('linear bias:')\n    print(linear.intercept_)\n    print('best alpha: %f'%(linear.alpha_))\n    print('best score: %f'%(rmsle(linear.predict(pres), train_y)))\n    return linear.coef_, linear.intercept_","dfcb0c18":"#\u53ef\u4ee5\u5bf9coefs\u5f52\u4e00\u5316\ncoefs, bias = blend_models(X, y, models)\nsum(coefs)\n# coefs=[i\/sum(coefs) for i in coefs.tolist()]\n# coefs\n# from scipy.special import softmax\n# coefs=softmax(coefs)\n# coefs","82507b1f":"#4.3 submit blend_models_with_regression_coefs \nprint('blending models RMSLE score on train data:')\nprint(rmsle(y, linear_blend_models_predict(X,models,coefs,bias))) #0.059305\n\n#before Blend with Top Kernals submissions\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(linear_blend_models_predict(X_sub,models,coefs,bias))) #expm1: exp(x) - 1; \u6ce8\u610f\u8fd8\u8981\u53d6\u6574\n# submission.iloc[:,1] = np.expm1(blend_models_predict(X_sub)) #expm1: exp(x) - 1; \u6ce8\u610f\u8fd8\u8981\u53d6\u6574\nsubmission.head()\nsubmission.to_csv(\"submission_blend_models_with_regression_coefs.csv\", index=False) #0.11492 \u53ef\u4ee5\u5f97\u5230\u76f8\u5f53\u7684\u6548\u679c","17d5a5eb":"#4.4 mixing with the top kernels\n\nprint('Blend with Top Kernals submissions', datetime.now(),)\nsub_1 = pd.read_csv('..\/input\/top-10-0-10943-stacking-mice-and-brutal-force\/House_Prices_submit.csv')\nsub_2 = pd.read_csv('..\/input\/hybrid-svm-benchmark-approach-0-11180-lb-top-2\/hybrid_solution.csv')\nsub_3 = pd.read_csv('..\/input\/lasso-model-for-regression-problem\/lasso_sol.csv')\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(linear_blend_models_predict(X_sub,models,public_coefs, bias)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))  \nsubmission.to_csv(\"submission_blend_top.csv\", index=False) #0.11115\nprint('Save submission', datetime.now())","f5d4253c":"#4.5 Brutal approach to deal with predictions close to outer range \n#\u7b2c\u8d85\u4f4e\u7684\u623f\u4ef7\u66f4\u4f4e\uff0c\u8ba9\u8d85\u9ad8\u7684\u623f\u4ef7\u66f4\u9ad8(\u901a\u5e38\u6765\u8bf4\uff0c\u4f1a\u5c06\u5c0f\u8005\u653e\u5927\uff0c\u5927\u8005\u7f29\u5c0f\uff0c\u4f46\u623f\u4ef7\u6709\u5176\u7279\u6b8a\u6027\uff1a\u6709\u4e9b\u504f\u8fdc\u5730\u533a\u7684\u623f\u5b50\u6bd4\u9884\u6d4b\u66f4\u4f4e\uff0c\n#\u6709\u4e9b\u623f\u5b50\u6bd4\u9884\u6d4b\u9ad8\u5f97\u591a)\uff0c\u8fd9\u91cc\u8ba9\u8fd9\u4e24\u79cd\u6781\u7aef\u60c5\u51b5\u66f4\u6781\u7aef\u4e00\u4e9b\uff0c\u8fd9\u6837\u66f4\u7b26\u5408\u623f\u4ef7\u7684\u7279\u6027\n#\u6ce8\u610f\u7f29\u653e\u7684\u5206\u4f4d\u65700.005,0.995\u4ee5\u53ca\u7f29\u653e\u7cfb\u65700.77,1.1\u53ef\u4ee5\u9002\u5f53\u8c03\u6574\uff0c\u76f8\u5173public kernel\u4e2d\u5e76\u672a\u63d0\u5230\u8fd9\u5757\u513f\u7684\u53c2\u6570\u5982\u4f55\u9009\u62e9\uff0c\u731c\u6d4b\uff1a\u552f\u7ed3\u679c\u8bba\nq1 = submission['SalePrice'].quantile(0.0045) \nq2 = submission['SalePrice'].quantile(0.998)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(\"submission_blend_top_Scale extremes.csv\", index=False) #0.10647(best result)\nprint('Save submission', datetime.now())","1ce7b0b2":"submission = pd.read_csv('..\/input\/house-price-best\/submission_best.csv')\nsubmission.to_csv(\"submission_best.csv\", index=False) #0.10647(best result)\nprint('Save submission best', datetime.now())","774c61ff":"#\u5982\u679c\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6bcf\u4e2a\u5355\u6a21\u578b(\u8df3\u8fc74.1)\uff0c\u76f4\u63a5\u62ffstack_gen_model.regressors\u6765\u7528\uff0c\u4f1a\u53d1\u73b0\uff1a\n#\u53ea\u6709\u7b2c\u4e00\u4e2a\u6a21\u578b\u80fd\u76f4\u63a5predict\uff0c\u5176\u4ed6\u6a21\u578b\u4f1a\u62a5not fit\u9519\u8bef\nstack_gen_model.regressors[0].predict(X)\n#stack_gen_model.regressors[1].predict(X) #error\n#stack_gen_model.regressors[2].predict(X) #error","791b984a":"#how to optimize hyperparameters?\n# from pactools.grid_search import GridSearchCVProgressBar #if u need progress bar\n# def grid_search(model, parameters, train_x, train_y, progress_bar=False, cv=5):\n#     #sklearn\u76840.22\u7248\u672c\u9ed8\u8ba4\u91c7\u75285-fold cv\uff0c\u5f53\u524d\u7248\u672c\u9ed8\u8ba43\u6298\n#     models = GridSearchCVProgressBar(\n#         model, parameters, cv=cv, verbose=1,\n#         n_jobs=6) if progress_bar else GridSearchCV(\n#             model, parameters, cv=cv, n_jobs=6)\n#     models.fit(train_x, train_y)\n#     print(models.best_params_)\n#     print(models.best_score_)\n#     #print(models.best_estimator_)\n\n# params1 = {\n#     'alpha':\n#     [0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n# }\n# grid_search(Ridge(), params1, X, y, progress_bar=True, cv=5)","bc0a6b6a":"#\u4e00\u822c\u60c5\u51b5\u4e0b\uff1a\u5c0f\u8005\u653e\u5927\uff0c\u5927\u8005\u7f29\u5c0f\uff0c\u8ba9\u6781\u7aef\u503c\u5c3d\u53ef\u80fd\u53d8\u5f97\u6b63\u5e38,\u4f46\u4e0d\u9002\u7528\u4e8e\u672c\u95ee\u9898\nq1 = submission['SalePrice'].quantile(0.005)\nq2 = submission['SalePrice'].quantile(0.995)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*1.1)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*0.77)\n\nsubmission.to_csv(\"submission_base_blend_top_Scale extremes2.csv\", index=False) #0.11602 \u53cd\u6548\u679c\nprint('Save submission', datetime.now())","93fafa88":"### linear blending","01add4a8":"### 1.1 know your target","e30cb722":"## 2.feature engineering","ac8944f1":"### 6.TODO(\u672a\u7adf\u4e4b\u4e8b)\n\u65f6\u95f4\u5173\u7cfb\uff0c\u70b9\u5230\u4e3a\u6b62\uff01\u6211\u5728\u524d\u9762\u5927\u6982\u7559\u4e8610\u4e2a\u6539\u8fdb\u70b9\uff0c\u6709\u5174\u8da3\u7684\u670b\u53cb\u53ef\u4ee5\u8bd5\u8bd5\u770b\u3002\n\nI list about 10 possible improvement points in front, you can try them. (I have tried a few, some may be useful and some may be harmful.)\nIf anyone have some progress, please feel free to contact me jjgxw@outlook.com\nUP! UP! UP!","42abb9c8":"## 3. Advanced Regression Techniques","693a7046":"### ensemble:\n* \u5148\u5bf9\u5355\u6a21\u578b\u505astacking\u6a21\u578b\n* \u518d\u5bf9\u5355\u6a21\u578b+stacking\u505a\u4e00\u6b21linear blending","f6a02552":"### 5.4 Why scale extreme values?\n* \u4e00\u822c\u60c5\u51b5\u4e0b\uff1a\u5c0f\u8005\u653e\u5927\uff0c\u5927\u8005\u7f29\u5c0f\uff0c\u8ba9\u6781\u7aef\u503c\u5c3d\u53ef\u80fd\u53d8\u5f97\u6b63\u5e38\n* \u4f46\u623f\u4ef7\u521a\u597d\u4e0e\u4e4b\u76f8\u53cd\uff1a**\u8ba9\u6781\u7aef\u503c\u66f4\u6781\u7aef\uff01**\u56e0\u4e3a\u8fd9\u624d\u662f\u623f\u4ef7\uff01\u6bd4\u5982\u6709\u4e9b\u504f\u8fdc\u5730\u533a\u7684\u623f\u5b50\u4f1a\u6bd4\u9884\u6d4b\u66f4\u4f4e\uff0c\u6709\u4e9b\u623f\u5b50\u6bd4\u9884\u6d4b\u9ad8\u5f97\u591a","6fb50d58":"# House Prices: Advanced Regression Techniques\n### Predict sales prices with detailed feature engineering, automatic outlier detection, Advanced Regression Techniques(GradientBoosting,Xgboost...) and Stacking\n![main](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier_files\/stacking_cv_algorithm.png)\n\n<br>\n\n**Competition Description**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n**Summary**\n- 1.Exploratory Data Analysis (EDA):distribution,outliers...\n- 2.Personalized Feature Engineering\n- 3.Advanced Regression Techniques\n- 4.Ensemble Learning\n\n","2a81e133":"### 1.4 concat train and test","e1631b53":"### 5.1 \u6709\u5fc5\u8981\u91cd\u65b0\u8bad\u7ec3\u4e00\u904d\u7b2c\u4e00\u5c42\u7684\u6a21\u578b\u5417\uff1f\u5728\u8bad\u7ec3stacking\u65f6\u76843\u6b65\u4e2d\u5e94\u8be5\u662f\u5305\u542b\u4e86\u8fd9\u4e00\u6b65\u7684\n\u7531\u4e0b\u9762\u7684\u4ee3\u7801\u53ef\u89c1\uff0c**\u8bad\u7ec3\u5b8cstacking\u4e4b\u540e\uff0c\u5982\u679c\u8981\u4e0e\u5404\u4e2a\u5355\u6a21\u578b\u505ablending,\u786e\u5b9e\u6709\u5fc5\u8981\u518d\u91cd\u65b0\u8bad\u7ec3\u7b2c1\u5c42\u7684\u6a21\u578b**","0d1d941f":"### single model","20ed3eb6":"### \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff1a\n\n* OverallQual\uff0cGrLivArea \u4ee5\u53ca TotalBsmtSF  \u4e0e SalePrice \u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\n* GarageCars \u548c GarageArea \u4e5f\u662f\u76f8\u5173\u6027\u6bd4\u8f83\u5f3a\u7684\u53d8\u91cf. \u8f66\u5e93\u4e2d\u5b58\u50a8\u7684\u8f66\u7684\u6570\u91cf\u662f\u7531\u8f66\u5e93\u7684\u9762\u79ef\u51b3\u5b9a\u7684\uff0c\u5b83\u4eec\u5c31\u50cf\u53cc\u80de\u80ce\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u4e13\u95e8\u533a\u5206 GarageCars \u548c GarageAre\uff0c\u6240\u4ee5\u6211\u4eec\u53ea\u9700\u8981\u5176\u4e2d\u7684\u4e00\u4e2a\u53d8\u91cf\u3002\u8fd9\u91cc\u6211\u4eec\u9009\u62e9\u4e86 GarageCars\uff0c\u56e0\u4e3a\u5b83\u4e0e SalePrice \u7684\u76f8\u5173\u6027\u66f4\u9ad8\u4e00\u4e9b\u3002\n* TotalBsmtSF  \u548c 1stFloor \u4e0e\u4e0a\u8ff0\u60c5\u51b5\u76f8\u540c\uff0c\u6211\u4eec\u9009\u62e9 TotalBsmtS \u3002\n* FullBath \u51e0\u4e4e\u4e0d\u9700\u8981\u8003\u8651\u3002\n* TotRmsAbvGrd \u548c GrLivArea \u4e5f\u662f\u53d8\u91cf\u4e2d\u7684\u53cc\u80de\u80ce\u3002\n* YearBuilt \u548c SalePrice \u76f8\u5173\u6027\u4f3c\u4e4e\u4e0d\u5f3a\u3002","e5e8cfe0":"### 5.3 GridSearchCV","47590e36":"## 4. submit","0642f2c8":"### 7.Refs\n* https:\/\/www.kaggle.com\/itslek\/blend-stack-lr-gb-0-10649-house-prices-v57\n* https:\/\/www.kaggle.com\/zugariy\/regression-blending-and-stacking-v-02#3---Removing-outliers\n* stacking:https:\/\/blog.csdn.net\/wstcjf\/article\/details\/77989963 and http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/#api\n* xgboost:https:\/\/xgboost.readthedocs.io\/en\/latest\/ and https:\/\/blog.csdn.net\/v_july_v\/article\/details\/81410574\n* lightgbm:https:\/\/lightgbm.readthedocs.io\/en\/latest\/","192c3b52":"## 5.Q & A","36cd9338":"### 1.2 Relevance of features-target","536dea44":"### 2.4 fill missing values: \u5148\u7279\u6b8a\u518d\u4e00\u822c\n### \u4e00\u822c\u60c5\u51b5\uff1a\n* numeric_features: \u4e00\u822c\u586b\u5145\u5747\u503c\uff0c\u5bf9\u4e8e\u5176\u4e2d\u7684special_features\uff0c\u586b0\n* category_features: \u4e00\u822c\u586b\u5145\u4f17\u6570\uff0c\u5c11\u6570\u53ef\u4ee5\u586b\u5145\u4e2d\u4f4d\u6570\u7b49\n\n### \u7279\u6b8a\u60c5\u51b5\uff1a\n* Functional,Electrical,KitchenQual\u5177\u6709\u5178\u578b\u503c\uff0c\u5e94\u586b\u5145\u5178\u578b\u503c\n* MSZoning\u8981\u6309MSSubClass\u5206\u7ec4\u586b\u5145\u4f17\u6570\n* LotFrontage\u6309Neighborhood\u5206\u7ec4\u586b\u5145\u4e2d\u4f4d\u6570(\u623f\u5b50\u5230\u8857\u9053\u7684\u8ddd\u79bb\u5148\u6309\u7167\u5730\u7406\u4f4d\u7f6e\u5206\u7ec4\u518d\u586b\u5145\u5404\u81ea\u7684\u4e2d\u4f4d\u6570)","b1180af7":"## 1.Exploratory Data Analysis (EDA)\n### \u4e86\u89e3\u6570\u636e\u7684\u5206\u5e03\uff1a\u7279\u5f81\u5de5\u7a0b\u7684\u57fa\u7840\n* \u5efa\u8bae\u591a\u7528describe\u51fd\u6570\u89c2\u5bdf\u7279\u5f81\u3001target\u7684\u5206\u5e03\u60c5\u51b5\n* \u753b\u51faCorrelation matrix\u3001\u6563\u70b9\u56fe\u3001\u76f4\u65b9\u56fe\u7b49","16d30b29":"### 1.3 automatic outlier detecting","6b064ac4":"### 5.2 detect outliers\n* outliers\u4e00\u822c\u53ef\u4ee5\u901a\u8fc7\u6563\u70b9\u56fe\u89c2\u5bdf\u800c\u5f97\n* \u53ef\u4ee5\u901a\u8fc7LocalOutlierFactor\u81ea\u52a8\u68c0\u6d4b\uff0c\u8be6\u89c1\u524d\u9762\u7684detect_outliers\u51fd\u6570\n\n\u4f46\u5f88\u591apublic kernels\u4e2d\u7684outliers=outliers = [30, 88, 462, 523, 632, 1298, 1324],\u6709\u4e9b\u5f88\u5bb9\u6613\u627e\u5230\uff0c\u5269\u4e0b\u7684\u5b8c\u5168\u4e0d\u77e5\u9053\u54ea\u513f\u6765\u7684\uff01\uff01","4c5b6505":"<br>\n### Load packages"}}