{"cell_type":{"70ee560c":"code","f81bf132":"code","12faa125":"code","6cf5dcb9":"code","104a968a":"code","df277346":"code","9256892a":"code","d3477a67":"code","4fac3ec8":"code","83ab23d0":"code","5b2a350f":"code","23aa7aca":"code","bceb97ed":"code","d09ca03a":"code","52fd2f8c":"code","783f6ea6":"code","61220134":"code","08541298":"code","f1e5899e":"code","e8f06386":"code","384e92c5":"code","fa8f3318":"code","0223f8ca":"code","38873b4a":"code","385f73c1":"code","99048ec4":"code","9c794171":"code","75609a56":"code","dff1f936":"markdown","516ed557":"markdown","a81822eb":"markdown","e87d9617":"markdown","913152c0":"markdown","7aebeb69":"markdown","aa3fdfbc":"markdown","e1d6b17f":"markdown","a6e0d411":"markdown","0325b115":"markdown"},"source":{"70ee560c":"# Depedencies\nimport numpy as np \nimport pandas as pd \nfrom collections import Counter\nimport os\nprint(os.listdir(\"..\/input\"))","f81bf132":"train_df = pd.read_csv('..\/input\/train_relationships.csv')\ntrain_df.head()","12faa125":"train_df.shape","6cf5dcb9":"# Number of families in the train folder\ntrain = !ls ..\/input\/train\/\nlen(train)","104a968a":"# Number of unlabelled images\ntest = !ls ..\/input\/test\/\nlen(test)","df277346":"# fastai and torch imports\nimport torch\nfrom fastai.vision import *\nfrom fastai.metrics import *\n\nnp.random.seed(7)\ntorch.cuda.manual_seed_all(7)","9256892a":"# Looking at the naming conventions\ntrain_folder = Path('..\/input\/train')\ntrain_folder.ls()","d3477a67":"# Looking at specific folder\nspecific_folder = train_folder\/'F0768'\nspecific_folder.ls()","4fac3ec8":"# Looking at individual images belonging to a particular folder\nmore_specific = train_folder\/'F0768\/MID4'\nmore_specific.ls()","83ab23d0":"sample1 = open_image('..\/input\/train\/F0768\/MID4\/P08113_face1.jpg')\nshow_image(sample1)","5b2a350f":"sample2 = open_image('..\/input\/train\/F0768\/MID4\/P08114_face1.jpg')\nshow_image(sample2)","23aa7aca":"sample3 = open_image('..\/input\/train\/F0768\/MID4\/P12113_face2.jpg')\nshow_image(sample3)","bceb97ed":"a = []\nfor i in train_df.p1:\n    try:\n        i2=i\n        i = Path('..\/input\/train\/'+i)\n        a.append(i.ls())\n    except:\n        index_to_drop = train_df.p1[train_df.p1==i2].index.tolist()\n        # print(index_to_drop)\n        train_df.drop(train_df.index[index_to_drop], inplace=True)\n\nlen(train_df), len(a)","d09ca03a":"first_person = pd.DataFrame(train_df.p1)\nsecond_person = pd.DataFrame(train_df.p2)\nlen(first_person)==len(second_person)","52fd2f8c":"print(first_person.head(3))\nprint('\\n')\nprint(second_person.head(3))","783f6ea6":"# Features DataFrame\na = []\nfor i in first_person.p1:\n    # Suspicious code block since there should not be any FileNotFoundError now\n    try:\n        i = Path('..\/input\/train\/'+i)\n        a.append(i.ls())\n    except:\n        pass\n\nb = []\nfor i in a:\n    for ii in i:\n        b.append(ii)\n        \nfeatures = pd.DataFrame()\nfeatures['Path'] = b\nfeatures.head()","61220134":"# Labels DataFrame\na = []\nfor i in second_person.p2:\n    try:\n        i = Path('..\/input\/train\/'+i)\n        a.append(i.ls())\n    except:\n        pass\n\nb = []\nfor i in a:\n    for ii in i:\n        b.append(ii)\\\n        \nlabels = pd.DataFrame()\nlabels['Labels'] = b\nlabels.head()","08541298":"len(features), len(labels)","f1e5899e":"features_new = features[:16307]\nfeatures_new['Labels'] = labels['Labels']\nfeatures_new.head()","e8f06386":"features_databunch = ImageImageList.from_df(features_new, path='.')\nlen(features_databunch)","384e92c5":"img = open_image(features_databunch.items[0])\nimg.shape","fa8f3318":"open_image(features_databunch.items[0])","0223f8ca":"databunch = features_databunch.split_by_rand_pct(0.1, seed=7)\\\n        .label_from_df(cols='Labels')\\\n        .transform(get_transforms(), size=224, tfm_y=True)\\\n        .databunch(bs=64).normalize(imagenet_stats, do_y=True)\n","38873b4a":"databunch.show_batch(rows=4, figsize=(8,8))","385f73c1":"learner = unet_learner(databunch, models.resnet34, wd=1e-3, blur=True, norm_type=NormType.Weight,\n                            y_range=(-3.,3.), loss_func=MSELossFlat()).to_fp16()\nlearner.lr_find()\nlearner.recorder.plot()","99048ec4":"learner.fit_one_cycle(2, pct_start=0.8, max_lr=slice(1e-05, 1e-03))","9c794171":"learner.unfreeze()\nlearner.fit_one_cycle(2, slice(1e-5,1e-3))","75609a56":"learner.validate()","dff1f936":"Instead of `label_from_func` I am using `label_from_df` since I already have that in a nice format. ","516ed557":"Makes sense, since the list `a` contains lists of image paths. Sorry about the naming conventions, though. ","a81822eb":"This seems a bit odd :\/\n\nFrom the data description: \n> the training set is divided in Families (F0123), then individuals (MIDx). Images in the same MIDx folder belong to the same person. Images in the same F0123 folder belong to the same family.\n\nThe above three sample images are of the same MIDx folder from `F0768`. But the last sample is different from the other two samples :\/","e87d9617":"In order to accomplish this, we first discard the entries that are there in the training csv file but in reality they do exist. Ideally we can take either of columns (p1 and p2) for this and do this. ","913152c0":"The idea now is to construct a dataset which will represent image to image mappings. We will take each of the images from the folders listed in the `p1` column of the training labels that are provided and will annotate them using the images which will extracted from the `p2` column. Our dataset should be similar to the following:\n\n![](https:\/\/i.ibb.co\/JcbWrTB\/Screenshot-from-2019-05-18-19-07-35.png)\n\nWhere the left image is serving as the feature vector and the right image is its label. ","7aebeb69":"Referring to the following code block as shown by Jeremy during Lesson 7 (v3, part I):\n```python\ndata = (src.label_from_func(lambda x: path_hr\/x.name)\n           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)\n           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))\n```","aa3fdfbc":"As per the data description provided:\n> train.csv - training labels. Remember, not every individual in a family shares a kinship relationship. For example, a mother and father are kin to their children, but not to each other.\n\n[Shrey Dabhi](https:\/\/www.kaggle.com\/sdabhi23) further clarified it in [this](https:\/\/www.kaggle.com\/c\/recognizing-faces-in-the-wild\/discussion\/92238#latest-531309) discussion thread:\n> The persons `p1` and `p2` in a given row are blood relatives of each other. I guess this is provided because not everyone in a family is a blood relative of one another, and a very good example is given in the description itself!","e1d6b17f":"This mismatch is bound to happen.","a6e0d411":"We now construct separate DataFrames for the features and labels since `fastai`'s `ImageDataBunch`es can be created from DataFrames. ","0325b115":"In this notebook, we will explore the data itself with the help of the `fastai` library. Apart from the state-of-the-art models, it provides a number of handy utilities that are nothing less than golds for a practitioner. "}}