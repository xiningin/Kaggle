{"cell_type":{"6e8bc893":"code","5315c066":"code","5b1ef51e":"code","751c1fce":"code","2a4cc830":"code","ff74beae":"code","8e00a08d":"code","488d0f14":"code","2c22ea9b":"code","db256116":"code","3a35f4cd":"code","0b1a51a5":"code","06e6d033":"code","a51abe50":"code","bd5df7e8":"code","dd98d4a9":"code","c8b720b0":"code","a97bca41":"code","e98483f3":"code","4814005c":"code","b952e625":"code","580309f1":"code","7dcb2aef":"code","76f87b0b":"code","72eec3a1":"code","14ff3bc4":"code","c0654ff1":"code","ab04beb1":"code","1dcfa204":"code","6ec35c6f":"code","d5e96009":"code","af215f6b":"code","f1f173eb":"code","51e7c131":"code","b3c5c2a6":"code","24ac4d1b":"code","b8135856":"code","92fca08a":"code","1f2e16a7":"code","035c938e":"code","114707cc":"code","5ee18ab2":"code","3ad93fff":"code","960cabc7":"code","b2e99dbd":"code","cf5df675":"code","f52e5b8b":"code","c0e6bd05":"code","5716de7a":"code","9185902f":"code","bdfaca8e":"code","56a3fd13":"code","f24488f7":"code","f87ef47d":"code","f5df8be4":"code","64235207":"code","4029d706":"code","0cae79db":"markdown","c69fb927":"markdown","c7ea3ed7":"markdown","92d66cfd":"markdown","7a34fbac":"markdown","1dcfec75":"markdown","c7efebae":"markdown","7e9748ee":"markdown","6969fe94":"markdown","85d3c496":"markdown","da0c2ef7":"markdown","24089d1a":"markdown","93459dd7":"markdown","dd8fac79":"markdown","fd461ae6":"markdown","0132fe21":"markdown","d07ce44e":"markdown","516521c0":"markdown","976fe4c1":"markdown","5c102ea4":"markdown","6528f987":"markdown","6a6499c3":"markdown","eec5a38f":"markdown","6d350f1f":"markdown","a9a0ae86":"markdown","67f24c9b":"markdown","568b860f":"markdown","c36fb83a":"markdown","67ed4e69":"markdown","24d5d9ad":"markdown","0a6c1062":"markdown","6bfaad23":"markdown"},"source":{"6e8bc893":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom pprint import pprint\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","5315c066":"train_df.info()","5b1ef51e":"test_df.info()","751c1fce":"#train_df.describe(include=['O'])\ntrain_df.describe(include='all')","2a4cc830":"train_df['Sex'].unique()","ff74beae":"train_df['Embarked'].unique()","8e00a08d":"fig, ax = plt.subplots(figsize=(7,5))\nfeatures =  [train_df.columns.values]\nprint(features[0])\nsns.heatmap(np.abs(train_df[features[0]].corr()), annot=True, linewidths=.5, cmap='Blues', ax=ax)","488d0f14":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2c22ea9b":"g = sns.FacetGrid(train_df, col='Sex', hue=\"Survived\")\ng.map(sns.histplot, \"Age\", alpha=.5, bins=20)\ng.add_legend()","db256116":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3a35f4cd":"grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid.map(sns.histplot, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","0b1a51a5":"train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","06e6d033":"grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived')\ngrid.map(sns.histplot, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","a51abe50":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","bd5df7e8":"grid = sns.FacetGrid(train_df, col='Parch', hue='Survived')\ngrid.map(sns.histplot, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","dd98d4a9":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c8b720b0":"grid = sns.FacetGrid(train_df, col='SibSp', hue='Survived')\ngrid.map(sns.histplot, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","a97bca41":"train_df['FamilySize'] = train_df['Parch'] + train_df['SibSp']\ntest_df['FamilySize'] = test_df['Parch'] + test_df['SibSp']\ngrid = sns.FacetGrid(train_df, col='FamilySize', hue='Survived')\ngrid.map(sns.histplot, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","e98483f3":"train_df[[\"FamilySize\", \"Survived\"]].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4814005c":"train_df['Title'] = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_df['Title'] = test_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","b952e625":"train_df[['PassengerId','Name']].head(2)","580309f1":"combine = [train_df, test_df]\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","7dcb2aef":"train_df[['PassengerId','Name', 'Title']].head(2)","76f87b0b":"grid = sns.FacetGrid(train_df, col='Title', hue='Sex')\ngrid.map(sns.histplot, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","72eec3a1":"grid = sns.FacetGrid(train_df, col='Title', hue='Survived')\ngrid.map(sns.histplot, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","14ff3bc4":"df = pd.concat([train_df, test_df])\ntitle_age_mean = df[['Title', 'Age']].groupby(['Title'], as_index=True).mean()\ntitle_age_mean","c0654ff1":"mask = train_df['Age'].isna()\ntrain_df[['PassengerId','Age','Title']].loc[mask].head(5)","ab04beb1":"mapping_dict = title_age_mean.to_dict()['Age']\ntrain_df.loc[mask, 'Age'] = train_df.loc[mask, 'Title'].map(mapping_dict)\ntrain_df[['PassengerId','Age','Title']].loc[mask].head(5)","1dcfa204":"mask = test_df['Age'].isna()\ntest_df.loc[mask, 'Age'] = test_df.loc[mask, 'Title'].map(mapping_dict)\ntest_df[['PassengerId','Age','Title']].loc[mask].head(5)","6ec35c6f":"mask = train_df['Embarked'].isna()\ntrain_df[['PassengerId','Embarked']].loc[mask]","d5e96009":"train_df['Embarked'].fillna(train_df['Embarked'].mode().iloc[0], inplace=True)\ntrain_df[['PassengerId','Embarked']].loc[mask]","af215f6b":"train_df['Title'].unique()","f1f173eb":"train_df['Embarked'].unique()","51e7c131":"# To verify dummy encording without `drop_first=True`\ntrain_df = pd.get_dummies(train_df, columns=['Title', 'Embarked'])\ntest_df = pd.get_dummies(test_df, columns=['Title', 'Embarked'])\nprint(f\"train_df's shape:{train_df.shape}, test_df's shape:{test_df.shape}\")","b3c5c2a6":"train_df.iloc[:,[0,11,12,13,14,15,16,17,18]].head(2)","24ac4d1b":"test_df.iloc[:,[0,10,11,12,13,14,15,16,17]].head(2)","b8135856":"train_df[['PassengerId','Sex']].head(2)","92fca08a":"train_df['Sex'] = train_df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntest_df['Sex'] = test_df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df[['PassengerId','Sex']].head(2)","1f2e16a7":"train_df_non_scaled = train_df.copy()\ntest_df_non_scaled = test_df.copy()\n\nfeatures = ['Age', 'SibSp', 'Parch']\nscaler = MinMaxScaler()\n\nscaler.fit(train_df[features])\ntrain_df[features] = scaler.transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])\ntrain_df[features].head()","035c938e":"fig, axes = plt.subplots(1,2, figsize=(12,4))\naxes[0].set_title('Original')\naxes[0].scatter(data=train_df_non_scaled, x=\"Age\", y=\"SibSp\", marker=\"o\", label=\"train_df_non_scaled\")\naxes[0].scatter(data=test_df_non_scaled, x=\"Age\", y=\"SibSp\", marker=\"^\", label=\"test_df_non_scaled\")\naxes[0].legend(loc='upper right')\naxes[1].set_title('Scaled')\naxes[1].scatter(data=train_df, x=\"Age\", y=\"SibSp\", marker=\"o\", label=\"train_df_scaled\")\naxes[1].scatter(data=test_df, x=\"Age\", y=\"SibSp\", marker=\"^\", label=\"test_df_scaled\")\naxes[1].legend(loc='upper right')","114707cc":"# Correcting by dropping features\ndrop_features = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Sex', 'Fare', 'SibSp', 'Parch']\n\nX_train = train_df.drop(drop_features, axis=1)\ny_train = train_df['Survived']\nX_test  = test_df.drop(drop_features, axis=1)\n\nprint(f\"X_train.shape:{X_train.shape}, y_train.shape:{y_train.shape}, X_test.shape:{X_test.shape}\")\n\nX_train.head()","5ee18ab2":"%matplotlib inline\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Convert categorical data to dummy encording\ntrain_df['Title'] = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_df['Title'] = test_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Create a new feature (FamilySize)\ntrain_df['FamilySize'] = train_df['Parch'] + train_df['SibSp']\ntest_df['FamilySize'] = test_df['Parch'] + test_df['SibSp']\n\n# Create a new feature (Title)\ncombine = [train_df, test_df]\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir',\n                                                 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n# Replace missing data\n# Age (train_df, test_df)\ndf_concat = pd.concat([train_df, test_df])\ntitle_age_mean = df_concat[['Title', 'Age']].groupby(['Title'], as_index=True).mean()\nmask = train_df['Age'].isna()\ntrain_df.loc[mask, 'Age'] = train_df.loc[mask, 'Title'].map(mapping_dict)\nmask = test_df['Age'].isna()\ntest_df.loc[mask, 'Age'] = test_df.loc[mask, 'Title'].map(mapping_dict)\n# Embarked\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode().iloc[0], inplace=True)\n\n# Converting categorical features\n# add \"drop_first=True\" option to avoid multicollinearity.  \ntrain_df = pd.get_dummies(train_df, columns=['Title', 'Embarked'], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=['Title', 'Embarked'], drop_first=True)\n\n# Feature scaling\n#features = ['Age', 'SibSp', 'Parch']\nfeatures = ['Age', 'FamilySize']\nscaler = MinMaxScaler()\n##scaler = StandardScaler()\nscaler.fit(train_df[features])\ntrain_df[features] = scaler.transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])\n\n# Drop features\ndrop_features = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Sex', 'Fare', 'SibSp', 'Parch']\nX_train = train_df.drop(drop_features + ['Survived'], axis=1)\ny_train = train_df['Survived']\nX_test  = test_df.drop(drop_features, axis=1)\n\nX_train.head()","3ad93fff":"X_test.head()","960cabc7":"X_train.info()","b2e99dbd":"X_test.info()","cf5df675":"from pprint import pprint\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom numpy.random import seed\nseed(0)\ntf.random.set_seed(0)\n\n#@tf.function(experimental_relax_shapes=True)\ndef deepnnModel(input_shape=None, layer_dims=None, activation='relu', dropout_rate=0.2):\n   \n    X_input = Input(input_shape)\n    X = X_input\n   \n    # hidden layers\n    for layer_dim in layer_dims:\n        X = Dense(layer_dim, activation=activation)(X)\n        X = Dropout(rate=dropout_rate)(X)\n        X = BatchNormalization()(X)\n    \n    # out put layer (binary classification)\n    X = Dense(1, activation='sigmoid')(X)\n\n    model = Model(inputs=X_input, outputs=X, name='deepnnModel')\n    \n    model.compile(\n        optimizer = 'adam',\n        loss = \"binary_crossentropy\",\n        metrics = [\"accuracy\"],\n        )\n\n    return model","f52e5b8b":"model = deepnnModel(input_shape=X_train.shape[1:], layer_dims=((X_train.shape[1],)))\nmodel.summary()","c0e6bd05":"n_features = X_train.shape[1]\n\nparam_grid = {\n    'activation'  : [\"tanh\", \"relu\"],\n    'input_shape'  : [(n_features,),],\n    'layer_dims'  : [(n_features,), (n_features,)*2, (n_features*2,), (n_features*2,) *2, (n_features*10,), (n_features*10,) *2],\n    'batch_size'  : [16, 32,],\n    'dropout_rate': [0.2, 0.4,],\n}\n\nmodel = KerasClassifier(deepnnModel, verbose=0)\ngrid_search= GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n\npprint(param_grid)","5716de7a":"%%time\ngrid_result = grid_search.fit(X_train, y_train, epochs=30)\nprint(f\"Best parameters: {grid_result.best_params_}\")\nprint(f\"Best Crossvalidation score: {grid_result.best_score_:.3f}\")","9185902f":"results = pd.DataFrame(grid_search.cv_results_)\npd.set_option('display.max_colwidth',200)\nresults[['params', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False).head(10)","bdfaca8e":"X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\nprint(f\"X_train2, X_val, y_train2, y_val, {X_train2.shape}, {X_val.shape}, {y_train2.shape}, {y_val.shape}\")\n\nn_features = X_train.shape[1]\n\n# apply best parameters from grid search results\ninput_shape = grid_result.best_params_['input_shape']\nlayer_dims = grid_result.best_params_['layer_dims']\nactivation = grid_result.best_params_['activation']\ndropout_rate = grid_result.best_params_['dropout_rate']\nbatch_size = grid_result.best_params_['batch_size']\n\nprint(f\"input_shape:{input_shape}, layer_dims:{layer_dims}, activation:{activation}, \\\n      dropout_rate:{dropout_rate}. batch_size:{batch_size}\\n\")\n\nmodel = deepnnModel(input_shape=input_shape, layer_dims=layer_dims, activation=activation, dropout_rate=dropout_rate)\nmodel.summary()","56a3fd13":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(x=X_train2,\n                      y=y_train2,\n                      verbose=0,\n                      epochs=500,\n                      batch_size=batch_size,\n                      callbacks=[early_stopping],\n                      validation_data=(X_val, y_val))\nfor k,i in history.history.items():\n    print(f\"{k}: {i[-1]}\")","f24488f7":"history.history\ndef plot_history(history):\n    \n    plt.figure(figsize=(12, 4))\n    plt.subplot(1,2,1)\n    plt.plot(history['accuracy'])\n    plt.plot(history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='lower right')\n\n    plt.subplot(1,2,2)\n    plt.plot(history['loss'])\n    plt.plot(history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper right')\n\n    plt.tight_layout()\n    plt.show()\n\nplot_history(history.history)","f87ef47d":"preds = model.evaluate(x = X_val, y = y_val)\nprint (\"\\nLoss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","f5df8be4":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score, classification_report\n\ny_prob = model.predict(x = X_val)\ny_pred = (y_prob > 0.5) * 1\n\ncm = confusion_matrix(y_true=y_val, y_pred=y_pred)\nConfusionMatrixDisplay(cm, display_labels=('unsurvived','survived')).plot(\n        values_format='.5g', cmap='Blues_r')\n\nprint(classification_report(y_val, y_pred))","64235207":"y_prob = model.predict(x = X_test)\ny_pred = (y_prob > 0.5) * 1\n\nsubmission = pd.DataFrame({\n        'PassengerId': test_df[\"PassengerId\"],\n        'Survived': y_pred.flatten()\n    })\nprint(submission)\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)\n","4029d706":"from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nX_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\nprint(f\"X_train2, X_val, y_train2, y_val, {X_train2.shape}, {X_val.shape}, {y_train2.shape}, {y_val.shape}\")\n\nn_features = X_train.shape[1]\n\nprint(f\"input_shape:{input_shape}, layer_dims:{layer_dims}, activation:{activation}, dropout_rate:{dropout_rate}. batch_size:{batch_size}\\n\")\n\nmodel = KerasClassifier(lambda:deepnnModel(\n    input_shape=input_shape, \n    layer_dims=layer_dims, \n    activation=activation,\n    dropout_rate=dropout_rate),\n                        verbose=0)\n\nhistory = model.fit(x=X_train2,\n          y=y_train2,\n          verbose=0,\n          epochs=50,\n          batch_size=batch_size,)\n\nperm = PermutationImportance(model, random_state=42).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = X_val.columns.tolist())","0cae79db":"## Finding best paremeters (Grid Search)\n\n- Grid searh tells that how much hidden layers and number of nodes are appropriate","c69fb927":"## Age\nFilling in null value with `Title`'s each mean value. (**train_df** and **test_df**)","c7ea3ed7":"## Converting numerical feature (feature scaling)\n\nRescaling `Age`,`SibSp` and `Parch` by min-max normalization.","92d66cfd":"# Correlation\n\n`Pclass` and `Fare` have relatively high correlation. `Fare` will be dropped.","7a34fbac":"# Titanic ML competition with Tensorflow Keras\n\n**Almost all the observation and analitics are based on [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions). (Thanks for the great work!**)\n\n__References__\n\n- [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)\n- [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n- [Permutation Importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n\n__Goal__\n\n- Solve this problem using deep neural network\n- Preprocess data\n  - which features to be dropped or not\n  - how to replace null data\n- Ajust parameters and exlain model  \n  - how to find best parameters (Grid search)\n  - which features does the model learned (Permitation importance)","1dcfec75":"## Permutation feature importance\n\nConfirming the feature importance using permutation importance.\n\n- Permutation importance expectedly shows that the model well learned from `Title_XX`,(whick actually reflects `Sex`) ,and `Pclass`. ","c7efebae":"## Finding categorical features\n\n- **Categorical features:** `Sex` , `Embarked` ","7e9748ee":"## Survival rate by **Parch**\n\n`Parch` seems to affect survival rate.\n- Survival rate of the people who have '1' to '3' **parents \/ children** are high,but '4' to '5' are not. ","6969fe94":"# Data overview\n\n- **train_df** has `Survived` columns.\n- **Data entries:** \n    - Train dataset: `891`\n    - Test dataset: `418`\n","85d3c496":"## Embarked\n\nFilling in them with  **mode value** (only **train_df**)","da0c2ef7":"# Confusion matrix","24089d1a":"#  Replacing missing value\n\n- `Age` and `Embarked` have missing value.","93459dd7":"## Survival rate by **Sex**","dd8fac79":"# Creating a new feature (FamilySize)\n\nCreate a new feature `FamilySize` that is sum of `Parch` and `SibSp`\n","fd461ae6":"# Converting categorical features","0132fe21":"# Creating a new feature (**Title**)\n\nCreate a new feature `Title` from `Name`.","d07ce44e":"## Title and Embarked (dummy encording)","516521c0":"## Preprocessing data\n\nNow you are ready for preprocessing data\n\n- I added `drop_first=True` at the **pd.get_dummies** to avoid multicollinearity. ","976fe4c1":"# Training model with Keras and findinf best parameters ","5c102ea4":"## Survival rate by **Embarked**","6528f987":"## Survival rate by **Title**\n\n- `Master`, `Miss` and `Mrs`(women) has high `Survived` rate","6a6499c3":"## Survival rate by **Pclass**","eec5a38f":"# Visualizing data\n\n-  `Sex`, `Pclass`, and `Embarked` looks important features, \n- but it is not clear about `SibSp` and `Parch` because these features have a bias. (***See also #Permitation Importance***).","6d350f1f":"## Defining Model","a9a0ae86":"## Verifing and visualizing score\nApply the best paramters to the model","67f24c9b":"## Sex (ordinal encoding)\n\n- It would be better being dropped, because it has strong correlation with `Title`. (See ##Confirming `Title` reflects `Sex` )","568b860f":"## Survival rate by **SibSp**\n\n`SibSp` seems to affect with survival rate. \n- Survival rate of the people who have '1' and '2' **siblings \/ spouses** are high, but '5', '8' are not. \n","c36fb83a":"# Dropping features\n\n- From the above observation, these features will be drroped.\n\n`PassengerId`, `Name`, `Ticket`, `Cabin`, `Sex`, `Fare`, `SibSp` and `Parch` \n","67ed4e69":"Check if the relationship between **train_df** and **test_df** has **NOT** been changed by feature scaling.\n\n- For example, compare between original data and scaled data,  the relationship of positions are same, while scale has been changed.","24d5d9ad":"## Confirming **Title** reflects **Sex** \n\n- `Title` reflects `Sex`, or gender feature.\n- `Sex` should be dropped when training the model.","0a6c1062":"## Output submission.csv","6bfaad23":"## Finding columns including **null**\n\n- **Columns including `null` data**:  `Age`, `Cabin`, `Embarked` and `Fare`\n- Make sure to check it in **train_df** and **test_df**"}}