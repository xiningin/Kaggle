{"cell_type":{"71746faf":"code","5edeb5ae":"code","d141773c":"code","fb438adc":"code","b0e9beb2":"code","7ea52b89":"code","20233217":"code","b2f76666":"code","d54c88a7":"code","bf936066":"code","a2805f56":"code","4855c1f8":"code","02cdf100":"markdown","3697ddc7":"markdown"},"source":{"71746faf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5edeb5ae":"!pip install pyspark","d141773c":"from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as f\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nfrom IPython.display import display, Image\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import  cosine_similarity\nimport pandas as pd \nfrom pyspark.sql.types import StructType,StructField, StringType\n\nspark = SparkSession.builder.getOrCreate()","fb438adc":"books = spark.read.csv('..\/input\/databooks\/dataset\/books.csv', header = True, inferSchema=True)\nratings = spark.read.csv('..\/input\/databooks\/dataset\/ratings.csv', header = True, inferSchema=True)\nbook_tags = spark.read.csv('..\/input\/databooks\/dataset\/book_tags.csv', header = True, inferSchema=True)\ntags = spark.read.csv('..\/input\/databooks\/dataset\/tags.csv', header = True, inferSchema=True)","b0e9beb2":"dtbooks=books.withColumn('authors',f.lower(f.regexp_replace('authors',\" \",\"\"))).toPandas()","7ea52b89":"count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ncount_matrix = count.fit_transform(dtbooks['authors'])\ncosine_sim = cosine_similarity(count_matrix, count_matrix)","20233217":"indices = pd.Series(dtbooks.index, index=dtbooks['title'])\ntitles=dtbooks['title']","b2f76666":"#x\u00e2y d\u1ef1ng b\u1ed9 l\u1ecdc Hybrid\nidx = indices[\"The Hunger Games (The Hunger Games, #1)\"]\nsim_scores = list(enumerate(cosine_sim[idx]))\nsim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\nsim_scores = sim_scores[1:31] # top 30 book d\u1ef1a v\u00e0o similarity scores \nbook_indices = [i[0] for i in sim_scores]\nbook_id=spark.createDataFrame(books.toPandas().iloc[book_indices]).select('id')\nratings_df=book_id.join(ratings,book_id.id == ratings.book_id).select('book_id','user_id','rating')\nals  = ALS(maxIter=10,regParam=0.1,rank=10,userCol=\"user_id\",itemCol=\"book_id\",ratingCol=\"rating\")\n#t\u00e1ch d\u1eef li\u1ec7u \u0111\u1ec3 train \ntraining_df,validation_df = ratings.randomSplit([8.0,2.0])\n#fit d\u1eef li\u1ec7u \u0111\u1ec3 c\u00f3 model v\u00e0 transform t\u1eadp test\nmodel = als.fit(training_df)\npredictions= model.transform(ratings_df)","d54c88a7":"predictions.show(10)","bf936066":"RecommentsHybrid=model.recommendForAllUsers(5).cache()\nRecommentsHybrid.show(5,False)\nRecommentsHybrid.printSchema()","a2805f56":"one_userHybrid =predictions.filter(f.col(\"user_id\")==10462).join(books,books.id == predictions.book_id).select(\"id\",\"title\",'image_url','prediction')\none_userHybrid.show(5,False)","4855c1f8":"user_ID = 10462 \n#loc book \u0111\u00e3 \u0111\u01b0\u1ee3c d\u00e1nh gi\u00e1\nratedBooks = ratings_df.filter(f.col('user_id')==10462).select('book_id').rdd.flatMap(lambda x: x).collect() \n#loc book ch\u01b0a \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1\nbook_to_be_rated = (ratings_df \n                      .filter(f.col('book_id').isin(ratedBooks)) \n                      .select('book_id').distinct() \n                      .withColumn('user_id',f.lit(user_ID)) \n                     )\nbook_to_be_rated.sort('book_id').show(5) \n# d\u00f9ng model transform book_to_be_rated\nuser_book_predictions = model.transform(book_to_be_rated)\nuser_book_predictions.orderBy('prediction',ascendIng=False).show(5) ","02cdf100":"# Top 5 books for all user, for each user","3697ddc7":"# Top unrated books for a user"}}