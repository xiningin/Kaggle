{"cell_type":{"332f3818":"code","e8ee71cd":"code","ae324e5e":"code","1e67caa1":"code","0990903a":"code","bd95bc63":"code","42050c3a":"code","23c46cfb":"code","d0cae2a8":"code","14c23ef7":"code","dd4aee84":"code","aaed1d6d":"code","81b454aa":"code","68fa85dc":"code","303f03e9":"code","4c33c397":"code","63f2d077":"code","e304c4b7":"code","07e8b53f":"code","aeafc523":"code","53907730":"code","db8c3047":"code","12150264":"code","8ddfcde8":"markdown","87fbc7e0":"markdown","f6ab0816":"markdown","6d4086e6":"markdown","6c5ee8bc":"markdown","41560372":"markdown","f89b91dd":"markdown","4b8c2c17":"markdown","c583fb98":"markdown","d1c05b7d":"markdown"},"source":{"332f3818":"import os\nCODE_DIR = 'pixel2style2pixel'\n\n!git clone https:\/\/github.com\/eladrich\/pixel2style2pixel.git $CODE_DIR\n    \n!wget https:\/\/github.com\/ninja-build\/ninja\/releases\/download\/v1.8.2\/ninja-linux.zip\n!unzip ninja-linux.zip -d \/usr\/local\/bin\/\n!update-alternatives --install \/usr\/bin\/ninja ninja \/usr\/local\/bin\/ninja 1 --force \n\n\nos.makedirs('toonify_results')\nos.chdir(f'.\/{CODE_DIR}')","e8ee71cd":"from argparse import Namespace\nimport time\nimport os\nimport cv2\nimport sys\nimport glob\nimport pprint\nimport random\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\nsys.path.append(\".\")\nsys.path.append(\"..\")\n\nfrom datasets import augmentations\nfrom utils.common import tensor2im, log_input_image\nfrom models.psp import pSp\n\n%load_ext autoreload\n%autoreload 2","ae324e5e":"# experiment_type = 'ffhq_encode'\n# experiment_type = 'ffhq_frontalize'\n# experiment_type = 'celebs_sketch_to_face'\n# experiment_type = 'celebs_seg_to_face'\n# experiment_type = 'celebs_super_resolution'\nexperiment_type = 'toonify'","1e67caa1":"EXPERIMENT_DATA_ARGS = {\n    \"toonify\": {\n        \"model_path\": \"..\/..\/input\/pixel2style2pixel-pretrained-checkpoints-pytorch\/psp_ffhq_toonify.pt\",\n        \"image_path\": \"..\/..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/000020.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    }\n}","0990903a":"EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]","bd95bc63":"model_path = EXPERIMENT_ARGS['model_path']\nckpt = torch.load(model_path, map_location='cpu')","42050c3a":"opts = ckpt['opts']\npprint.pprint(opts)","23c46cfb":"# update the training options\nopts['checkpoint_path'] = model_path\nif 'learn_in_w' not in opts:\n    opts['learn_in_w'] = False","d0cae2a8":"opts= Namespace(**opts)\nnet = pSp(opts)\nnet.eval()\nnet.cuda()\nprint('Model successfully loaded!')","14c23ef7":"image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\noriginal_image = Image.open(image_path)","dd4aee84":"input_image = original_image.resize((256, 256))\ninput_image","aaed1d6d":"img_transforms = EXPERIMENT_ARGS['transform']\ntransformed_image = img_transforms(input_image)","81b454aa":"latent_mask = None\n\ndef run_on_batch(inputs, net, latent_mask=None):\n    result_batch = []\n    for image_idx, input_image in enumerate(inputs):\n        # get latent vector to inject into our input image\n        vec_to_inject = np.random.randn(1, 512).astype('float32')\n        _, latent_to_inject = net(torch.from_numpy(vec_to_inject).to(\"cuda\"),\n                                  input_code=True,\n                                  return_latents=True)\n        # get output image with injected style vector\n        res = net(input_image.unsqueeze(0).to(\"cuda\").float(),\n                  latent_mask=latent_mask,\n                  inject_latent=latent_to_inject)\n        result_batch.append(res)\n    result_batch = torch.cat(result_batch, dim=0)\n    return result_batch","68fa85dc":"with torch.no_grad():\n    tic = time.time()\n    result_image = run_on_batch(transformed_image.unsqueeze(0), net, latent_mask)[0]\n    toc = time.time()\n    print('Inference took {:.4f} seconds.'.format(toc - tic))","303f03e9":"input_vis_image = log_input_image(transformed_image, opts)\noutput_image = tensor2im(result_image)","4c33c397":"res = np.concatenate([np.array(input_vis_image.resize((256, 256))),\n                      np.array(output_image.resize((256, 256)))], axis=1)","63f2d077":"res_image = Image.fromarray(res)\n\nres_image.save('..\/toonify_results\/sample.jpg')\nres_image","e304c4b7":"res_image = Image.fromarray(res)\nres_image","07e8b53f":"image_paths = random.sample(glob.glob(os.path.join('..\/..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/', '*.jpg')), 100)","aeafc523":"image_paths[0].split('\/')[-1]","53907730":"for image_path in image_paths:\n    input_image = Image.open(image_path)\n    input_image = input_image.resize((256, 256))\n    img_transforms = EXPERIMENT_ARGS['transform']\n    transformed_image = img_transforms(input_image)\n    \n    with torch.no_grad():\n        result_image = run_on_batch(transformed_image.unsqueeze(0), net, latent_mask)[0]\n        \n    input_vis_image = log_input_image(transformed_image, opts)\n    output_image = tensor2im(result_image)\n    res = np.concatenate([np.array(input_vis_image.resize((256, 256))), np.array(output_image.resize((256, 256)))], axis=1)\n    res_image = Image.fromarray(res)\n    res_image.save(f\"..\/toonify_results\/{image_path.split('\/')[-1]}\")","db8c3047":"os.chdir(f'..\/')","12150264":"!rm -rf pixel2stylepixel\n!rm ninja-linux.zip","8ddfcde8":"## Load Pretrained Model\n> We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary.","87fbc7e0":"### Visualize Result","f6ab0816":"## Select Experiment Type\n> Select which experiment you wish to perform inference on:\n> 1. ffhq_encode\n> 2. ffhq_frontalize\n> 3. celebs_sketch_to_face\n> 4. celebs_seg_to_face\n> 5. celebs_super_resolution\n> 6. toonify","6d4086e6":"> Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on. While we provide default values to run this script, feel free to change as needed.","6c5ee8bc":"## Perform Inference","41560372":"## Define Inference Parameters","f89b91dd":"## Visualize Input","4b8c2c17":"<h3><center>Cartoonify results using Pixel2Style2Pixel Model<\/center><\/h3>\n<img src=\"https:\/\/github.com\/eladrich\/pixel2style2pixel\/raw\/master\/docs\/toonify_input.jpg\" width=\"900\" height=\"750\"\/>\n<img src=\"https:\/\/github.com\/eladrich\/pixel2style2pixel\/raw\/master\/docs\/toonify_output.jpg\" width=\"900\" height=\"750\"\/>\n<h4><\/h4>\n<h4><center><a href=\"https:\/\/github.com\/eladrich\/pixel2style2pixel\">Source: Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation [Elad Richardson et. al.]<\/a><\/center><\/h4>","c583fb98":"## Introduction\n\n### The notebook is a demo of [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https:\/\/arxiv.org\/pdf\/2008.00951.pdf) obtained from the authors' original [pixel2style2pixel implementation](https:\/\/github.com\/eladrich\/pixel2style2pixel).","d1c05b7d":"## Acknowledgements\n\n### This work was inspired by and derives codes from the official [Pixel2Style2Pixel implementation](https:\/\/github.com\/eladrich\/pixel2style2pixel). If you use this work, you should cite the research work [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https:\/\/arxiv.org\/abs\/2008.00951) and cite \/ star \ud83c\udf1f the [official implementation](https:\/\/github.com\/eladrich\/pixel2style2pixel)."}}