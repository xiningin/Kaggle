{"cell_type":{"2cabb928":"code","25cb8c12":"code","e2918223":"code","ef842d77":"code","e3b8e86f":"code","1cee6794":"code","da7ab3ab":"code","85032de2":"code","6926b168":"code","03115e0c":"code","0ee55adc":"code","99d14426":"code","c061b6c4":"code","478fd4a8":"code","6e54419e":"code","8d32b4e4":"code","c681c9be":"code","d9664dbf":"code","fc54cb05":"code","8bea2c28":"code","110cfd3c":"markdown","9aef9c8e":"markdown","dd272e14":"markdown","436018dd":"markdown","b4e8c487":"markdown","a44c0d2f":"markdown","52be44ef":"markdown","d7b76491":"markdown","8d4ea962":"markdown","deac7da6":"markdown","05daf1b0":"markdown","3162cbd2":"markdown"},"source":{"2cabb928":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n","25cb8c12":"import seaborn as sns\nimport matplotlib.pyplot as plt","e2918223":"\ntrain_data=pd.read_csv(\"..\/input\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/test.csv\")\ntrain_data.head()","ef842d77":"test_data.head()","e3b8e86f":"train_data.info()","1cee6794":"test_data.info()","da7ab3ab":"train_data.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1, inplace=True)\ntest_data.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1, inplace=True)","85032de2":"plt.figure(figsize=(20,5))\nplt.subplot(121)\nsns.countplot('Embarked', data=train_data)\nplt.subplot(122)\nsns.countplot('Embarked', data=test_data)","6926b168":"Values = {'Age':train_data['Age'].mean(), 'Embarked':\"S\"}\ntrain_data.fillna(value=Values, inplace=True)\nValues_test = {'Age':test_data['Age'].mean(), 'Fare':test_data['Fare'].mean()}\ntest_data.fillna(value=Values_test, inplace=True)","03115e0c":"NewData_train=pd.get_dummies(train_data, prefix=['New1', 'New2'])\nNewData_test=pd.get_dummies(test_data, prefix=['New1', 'New2'])","0ee55adc":"y_train = NewData_train[\"Survived\"]\nX_train = NewData_train.drop('Survived', axis=1)","99d14426":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled_train = scaler.fit_transform(X_train)\nX_scaled_train =pd.DataFrame(X_scaled_train)\nX_scaled_test = scaler.fit_transform(NewData_test)\nX_scaled_test =pd.DataFrame(X_scaled_test)","c061b6c4":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_scaled_train, y_train)\ny_pred_train = classifier.predict(X_scaled_train) ","478fd4a8":"from sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(y_pred_train,y_train)\nsns.heatmap(cm, annot=True)\nprint(classification_report(y_pred_train,y_train))","6e54419e":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators=20, random_state=0)  \nregressor.fit(X_scaled_train, y_train)  ","8d32b4e4":"y_pred_train = regressor.predict(X_scaled_train)  \ny_pred_train = pd.DataFrame(y_pred_train)\ny_pred_train.rename(columns={0:'col1'}, inplace=True)\ny_pred_train.loc[y_pred_train['col1'] < 0.5, 'col1'] = 0\ny_pred_train.loc[y_pred_train['col1'] >= 0.5, 'col1'] = 1","c681c9be":"from sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(y_pred_train,y_train)\nsns.heatmap(cm, annot=True)\nprint(classification_report(y_pred_train,y_train))","d9664dbf":"Y_predict_test = regressor.predict(X_scaled_test)\nY_predict_test = pd.DataFrame(Y_predict_test)\nY_predict_test.rename(columns={0:'Suvival'}, inplace=True)\nY_predict_test.loc[Y_predict_test['Suvival'] < 0.5, 'Suvival'] = 0\nY_predict_test.loc[Y_predict_test['Suvival'] >= 0.5, 'Suvival'] = 1","fc54cb05":"Y_predict_test","8bea2c28":"sns.countplot(\"Suvival\", data=Y_predict_test)","110cfd3c":"checking the labels of test data set","9aef9c8e":"Thank you!","dd272e14":"In Sex parameter are two category both are strings , so need to convert into number","436018dd":"Supervised Learning Machine Learning Model \n\nThis is about Binary Classification (Supervised Learning) model. We need to predict whether passenger survived or not based on various features given in the data set. Some of the features may not contribute to the outcome, therefore I have deleted those feature while training the model. Model can generated using various ML model like, Logistic Regression, Random Forest Regression, Na\u00efve Base classification, K-Near classification.\n\nAmong many ML model available, Which one to choose? The most important factors need to consider while choosing model  is number of features and number of training set. Since here few features are there, therefore it is better to develop model using Random Forest regression. I have achieved 99% accuracy on test set with Random Forest. \n\nIf the sample size high, then simple Logistic Regression will works fine. But here there are only 900 training set, Logistic regression is not suitable. In my Logistic regression i have got 80% Accuracy\n","b4e8c487":"All the feature column are scaled using StandardScaler","a44c0d2f":"NaN values in Age parameter are replaced with mean age  and Nan value in Emarked are replaced with most common S ","52be44ef":"Here \"survival\" is taken as output y X_train will be all other features","d7b76491":"Some of the parameters not contribute to the outcome , those parameter are dropped from tarin as well as test set","8d4ea962":"**Model using Random Forest Regressor,\u00b6**\nCHeck out the results of the random forest and compare it with that of Logistic regression model. You can find out that, Random forest model is more accuarte","deac7da6":"*Loading the data set using pandas and checking features","05daf1b0":"**Model using Logistic Regression**","3162cbd2":"From above it is evident that. parameters like Age, Cabin, Embarked has NaN values, We need to replace with appropriate values\n\nSimillarly , Below test info shows Nan values in Age, fare and Cabin"}}