{"cell_type":{"09833012":"code","a1d9bfe0":"code","cad91318":"code","b385e7d9":"code","857a5303":"markdown","22801421":"markdown","373a6c27":"markdown","6afbd4a9":"markdown","a1bb23ae":"markdown","f75121df":"markdown"},"source":{"09833012":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Load data\ndf_train = pd.read_csv('\/kaggle\/input\/inf131-2019\/train.csv')\nX = df_train.drop(['casual', 'registered', 'cnt', 'atemp', 'windspeed'], axis=1)\ny = df_train['cnt']\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# score function\ndef rmsle_score(y_true, y_pred):\n    for i, y in enumerate(y_pred):\n        if y_pred[i] < 0:\n            y_pred[i] = 0\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n","a1d9bfe0":"# Random Forest\nrf = RandomForestRegressor(n_jobs=-1, random_state=0)\nparameters = {'n_estimators': [200, 400], 'max_depth': [15, 25]}\nrf_cv = GridSearchCV(rf, parameters, cv=5, n_jobs=-1)\nrf_cv.fit(X_train, y_train)\ny_pred = rf_cv.predict(X_test)\n    \nprint('Random Forest Best Parameters:', rf_cv.best_params_)\nprint('Random Forest RMSLE score:', rmsle_score(y_test, y_pred))\n    ","cad91318":"# KNN\nknn = KNeighborsRegressor(n_jobs=-1)\nparameters = {'n_neighbors': [4, 6, 8], 'weights': ['uniform', 'distance'], 'p': [1, 2]}\nknn_cv = GridSearchCV(knn, parameters, cv=5, n_jobs=-1)\nknn_cv.fit(X_train, y_train)\ny_pred_knn = knn_cv.predict(X_test)\n\nprint('KNN Best Parameters:', knn_cv.best_params_)\nprint('KNN RMSLE score:', rmsle_score(y_test, y_pred_knn))\n\n# Decision Tree\ndt = DecisionTreeRegressor(random_state=0)\nparameters = {'max_depth': [14, 16, 18], 'min_samples_leaf': [2, 4, 6]}\ndt_cv = GridSearchCV(dt, parameters, cv=5, n_jobs=-1)\ndt_cv.fit(X_train, y_train)\ny_pred_dt = dt_cv.predict(X_test)\n\nprint('Decision Tree Best Parameters:', dt_cv.best_params_)\nprint('Decision Tree RMSLE score:', rmsle_score(y_test, y_pred_dt))","b385e7d9":"# Use best parameters found before\nknn = KNeighborsRegressor(n_jobs=-1, n_neighbors=8, weights='distance', p=1)\ndt = DecisionTreeRegressor(random_state=0, max_depth=16, min_samples_leaf=4)\n\n# Voting\nvoting = VotingRegressor(estimators=[('knn', knn), ('dt', dt)], weights=None, n_jobs=-1)\nvoting.fit(X_train, y_train)\ny_pred_voting = voting.predict(X_test)\n    \n\nprint('Voting RMSLE score:', rmsle_score(y_test, y_pred_voting))","857a5303":"\u0392\u03bb\u03ad\u03c0\u03bf\u03c5\u03bc\u03b5 \u03c0\u03c9\u03c2 \u03c5\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9 \u03b2\u03b5\u03bb\u03c4\u03af\u03c9\u03c3\u03b7. \u039f KNN \u03b5\u03af\u03c7\u03b5 score 0.403 \u03ba\u03b1\u03b9 \u03c4\u03bf Decision Tree 0.415, \u03b1\u03bb\u03bb\u03ac \u03cc\u03c4\u03b1\u03bd \u03c3\u03c5\u03bd\u03b4\u03c5\u03ac\u03c3\u03c4\u03b7\u03ba\u03b1\u03bd \u03ad\u03b4\u03c9\u03c3\u03b1\u03bd score 0.375. \u03a4\u03b9 \u03b8\u03b1 \u03b3\u03b9\u03bd\u03cc\u03c4\u03b1\u03bd \u03b1\u03bd \u03c3\u03c5\u03bd\u03b4\u03c5\u03ac\u03b6\u03b1\u03bc\u03b5 \u03c4\u03bf Random Forest \u03bc\u03b5 score 0.353 \u03bc\u03b5 \u03ac\u03bb\u03bb\u03bf\u03c5\u03c2 \u03b1\u03ba\u03cc\u03bc\u03b1 \u03ba\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03bf\u03c5\u03c2 \u03b1\u03bb\u03b3\u03bf\u03c1\u03af\u03b8\u03bc\u03bf\u03c5\u03c2, \u03ac\u03c1\u03b1\u03b3\u03b5? \u03a4\u03af\u03c0\u03bf\u03c4\u03b1 \u03c4\u03c1\u03bf\u03bc\u03b5\u03c1\u03cc. \u0391\u03bb\u03bb\u03ac \u03c0\u03b9\u03b8\u03b1\u03bd\u03cc\u03c4\u03b1\u03c4\u03b1 \u03b8\u03b1 \u03c5\u03c0\u03ae\u03c1\u03c7\u03b5 \u03bc\u03b9\u03b1 \u03bc\u03b9\u03ba\u03c1\u03ae \u03b2\u03b5\u03bb\u03c4\u03af\u03c9\u03c3\u03b7.\n","22801421":"\u0391\u03c2 \u03c3\u03c5\u03bd\u03b4\u03c5\u03ac\u03c3\u03bf\u03c5\u03bc\u03b5 \u03c4\u03ce\u03c1\u03b1 \u03c4\u03bf \u039a\u039d\u039d \u03ba\u03b1\u03b9 \u03c4\u03bf Decision Tree \u03bc\u03b5 \u03c4\u03bf Voting Ensemble \u03ae \u03b1\u03bb\u03bb\u03b9\u03ce\u03c2 Averaging (\u03b1\u03c6\u03bf\u03cd \u03bf\u03c5\u03c3\u03b9\u03b1\u03c3\u03c4\u03b9\u03ba\u03ac \u03b5\u03af\u03bd\u03b1\u03b9 \u03bf \u03bc\u03ad\u03c3\u03bf\u03c2 \u03cc\u03c1\u03bf\u03c2) \u03ae Weighted Average \u03b1\u03bd \u03b3\u03af\u03bd\u03b5\u03b9 \u03c7\u03c1\u03ae\u03c3\u03b7 \u03b2\u03b1\u03c1\u03ce\u03bd.","373a6c27":"\u0388\u03c6\u03c4\u03b9\u03b1\u03be\u03b1 \u03b1\u03c5\u03c4\u03cc \u03c4\u03bf notebook \u03b5\u03bb\u03c0\u03af\u03b6\u03bf\u03bd\u03c4\u03b1\u03c2 \u03cc\u03c4\u03b9 \u03b8\u03b1 \u03b2\u03bf\u03b7\u03b8\u03ae\u03c3\u03c9 \u03cc\u03c3\u03bf\u03c5\u03c2 \u03ad\u03c7\u03bf\u03c5\u03bd \u03ba\u03bf\u03bb\u03bb\u03ae\u03c3\u03b5\u03b9 (\u03ba\u03b1\u03b9 \u03b5\u03c0\u03b5\u03b9\u03b4\u03ae \u03b1\u03c0\u03bb\u03ac \u03ae\u03b8\u03b5\u03bb\u03b1 \u03bd\u03b1 \u03b4\u03bf\u03ba\u03b9\u03bc\u03ac\u03c3\u03c9 \u03bd\u03b1 \u03c6\u03c4\u03b9\u03ac\u03be\u03c9 \u03ad\u03bd\u03b1 notebook \u03c3\u03c4\u03bf kaggle).\n\u0394\u03b5\u03af\u03c7\u03bd\u03c9 \u03c0\u03c9\u03c2 \u03bd\u03b1 \u03c7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03ae\u03c3\u03b5\u03c4\u03b5 grid search cross validation, \u03ad\u03bd\u03b1\u03bd \u03c3\u03c7\u03b5\u03c4\u03b9\u03ba\u03ac \u03ba\u03b1\u03bb\u03cc \u03b1\u03bb\u03b3\u03cc\u03c1\u03b9\u03b8\u03bc\u03bf \u03cc\u03c0\u03c9\u03c2 \u03c4\u03bf Random Forest \u03ba\u03b1\u03b9 \u03c4\u03bf voting ensemble. \u03a3\u03c4\u03bf \u03c4\u03ad\u03bb\u03bf\u03c2 \u03b4\u03af\u03bd\u03c9 \u03ba\u03b1\u03b9 \u03bc\u03b5\u03c1\u03b9\u03ba\u03ad\u03c2 \u03b9\u03b4\u03ad\u03b5\u03c2 \u03b3\u03b9\u03b1 \u03cc\u03c0\u03bf\u03b9\u03bf\u03bd \u03ad\u03c7\u03b5\u03b9 \u03c7\u03c1\u03cc\u03bd\u03bf \u03ba\u03b9 \u03cc\u03c1\u03b5\u03be\u03b7.","6afbd4a9":"\u0391\u03c2 \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 train \u03c4\u03bf Random Forest \u03bc\u03b5 GridSearchCV. \u039f\u03b9 \u03c0\u03b1\u03c1\u03ac\u03bc\u03b5\u03c4\u03c1\u03bf\u03b9 \u03c0\u03bf\u03c5 \u03c0\u03c1\u03bf\u03c3\u03c0\u03b1\u03b8\u03ce \u03bd\u03b1 \u03b2\u03b5\u03bb\u03c4\u03b9\u03c3\u03c4\u03bf\u03c0\u03bf\u03b9\u03ae\u03c3\u03c9 \u03b5\u03af\u03bd\u03b1\u03b9 \u03bf\u03b9 n_estimators (\u03c0\u03cc\u03c3\u03b1 \u03b4\u03ad\u03bd\u03c4\u03c1\u03b1 \u03b8\u03b1 \u03c6\u03c4\u03b9\u03b1\u03c7\u03c4\u03bf\u03cd\u03bd) \u03ba\u03b1\u03b9 \u03c4\u03bf max_depth \u03c4\u03c9\u03bd \u03b4\u03ad\u03bd\u03c4\u03c1\u03c9\u03bd. \u0395\u03af\u03bd\u03b1\u03b9 \u03b1\u03c0\u03cc \u03c4\u03b9\u03c2 \u03c3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03ba\u03cc\u03c4\u03b5\u03c1\u03b5\u03c2 \u03c0\u03b1\u03c1\u03b1\u03bc\u03ad\u03c4\u03c1\u03bf\u03c5\u03c2 \u03c3\u03b5 \u03cc\u03bb\u03b1 \u03c4\u03b1 tree-based \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1, \u03b1\u03bb\u03bb\u03ac \u03c5\u03c0\u03ac\u03c1\u03c7\u03bf\u03c5\u03bd \u03ba\u03b9 \u03ac\u03bb\u03bb\u03b5\u03c2 \u03c7\u03c1\u03ae\u03c3\u03b9\u03bc\u03b5\u03c2. \u0394\u03bf\u03ba\u03b9\u03bc\u03ac\u03b6\u03c9 \u03bc\u03cc\u03bd\u03bf 2 \u03c4\u03b9\u03bc\u03ad\u03c2 \u03b3\u03b9\u03b1 \u03ba\u03ac\u03b8\u03b5 \u03c0\u03b1\u03c1\u03ac\u03bc\u03b5\u03c4\u03c1\u03bf \u03b3\u03b9\u03b1 \u03c4\u03b1\u03c7\u03cd\u03c4\u03b7\u03c4\u03b1, \u03b1\u03bb\u03bb\u03ac \u03c7\u03c1\u03b5\u03b9\u03ac\u03b6\u03b5\u03c4\u03b1\u03b9 \u03bd\u03b1 \u03b4\u03bf\u03ba\u03b9\u03bc\u03b1\u03c3\u03c4\u03bf\u03cd\u03bd \u03b1\u03c1\u03ba\u03b5\u03c4\u03ad\u03c2 \u03c4\u03b9\u03bc\u03ad\u03c2 \u03b3\u03b5\u03bd\u03b9\u03ba\u03ac.","a1bb23ae":"### \u0386\u03bb\u03bb\u03b5\u03c2 \u03b9\u03b4\u03ad\u03b5\u03c2\n* \u039a\u03b1\u03bb\u03bf\u03af \u03b1\u03bb\u03b3\u03cc\u03c1\u03b9\u03b8\u03bc\u03bf\u03b9 \u03cc\u03c0\u03c9\u03c2 \u03c4\u03bf Random Forest \u03c3\u03c4\u03bf scikit-learn, (\u03c0\u03c7. Adaboost, Extra Trees, Gradient Boosting...).\n* \u039a\u03b1\u03bb\u03bf\u03af \u03b1\u03bb\u03b3\u03cc\u03c1\u03b9\u03b8\u03bc\u03bf\u03b9 \u03b1\u03c0\u03cc \u03ac\u03bb\u03bb\u03b5\u03c2 \u03b2\u03b9\u03b2\u03bb\u03b9\u03bf\u03b8\u03ae\u03ba\u03b5\u03c2 (\u03c0\u03c7. XGBoost \u03b3\u03b9\u03b1 Gradient Boosting \u03ae Tensorflow\/Keras \u03b3\u03b9\u03b1 \u03bd\u03b5\u03c5\u03c1\u03c9\u03bd\u03b9\u03ba\u03ac \u03b4\u03af\u03ba\u03c4\u03c5\u03b1).\n* \u0395\u03c0\u03b9\u03bb\u03bf\u03b3\u03ae \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03c9\u03bd \u03c0\u03bf\u03c5 \u03b4\u03bf\u03c5\u03bb\u03b5\u03cd\u03bf\u03c5\u03bd \u03ba\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03b1 \u03bc\u03b5\u03c4\u03b1\u03be\u03cd \u03c4\u03bf\u03c5\u03c2 \u03ba\u03b1\u03b9 \u03c7\u03c1\u03ae\u03c3\u03b7 \u03b2\u03b1\u03c1\u03ce\u03bd \u03c3\u03c4\u03bf Voting Regressor.\n* \u03a7\u03c1\u03ae\u03c3\u03b7 \u03c0\u03b9\u03bf \u03c0\u03b5\u03c1\u03af\u03c0\u03bb\u03bf\u03ba\u03c9\u03bd ensemble \u03c4\u03b5\u03c7\u03bd\u03b9\u03ba\u03ce\u03bd \u03b1\u03c0\u03cc \u03c4\u03bf Voting (\u03c0\u03c7. Blending, Stacking).","f75121df":"\u03a4\u03ce\u03c1\u03b1 \u03b8\u03b1 \u03ba\u03ac\u03bd\u03c9 train \u03b4\u03cd\u03bf \u03af\u03c3\u03c9\u03c2 \u03cc\u03c7\u03b9 \u03ba\u03b1\u03b9 \u03c4\u03cc\u03c3\u03bf \u03ba\u03b1\u03bb\u03ac \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 \u03cc\u03c3\u03bf \u03c4\u03bf Random Forest, \u03ad\u03bd\u03b1 KNN \u03ba\u03b1\u03b9 \u03ad\u03bd\u03b1 Decision Tree \u03b8\u03ad\u03bb\u03bf\u03bd\u03c4\u03b1\u03c2 \u03bd\u03b1 \u03b4\u03b5\u03af\u03be\u03c9 \u03c4\u03b1 \u03b1\u03c0\u03bf\u03c4\u03b5\u03bb\u03ad\u03c3\u03bc\u03b1\u03c4\u03b1 \u03c4\u03bf\u03c5 \u03c3\u03c5\u03bd\u03b4\u03c5\u03b1\u03c3\u03bc\u03bf\u03cd \u03c4\u03bf\u03c5\u03c2 \u03bc\u03b5 Voting Ensemble."}}