{"cell_type":{"d9c8ce8e":"code","2a2bf3ad":"code","107dfc25":"code","6d4b915e":"code","ac2232be":"code","ac6aa897":"code","0448c3a4":"code","ffa050ad":"code","134d5ba5":"code","2da9c822":"code","0e159627":"code","72586502":"code","70e7b0ec":"code","fb8dae7c":"code","d4f48a47":"code","ba3c78a5":"code","acb290b2":"code","07a5d773":"code","c6165387":"code","ec40d856":"code","d623ef8b":"code","ab732668":"code","63282223":"code","5770f192":"code","2023370d":"code","f11fbda1":"code","67ccda36":"code","837fa40f":"code","3e3b7958":"code","defaa822":"code","c3abe072":"code","59e83681":"markdown","f78517bd":"markdown","9d0dfe84":"markdown","a182de16":"markdown"},"source":{"d9c8ce8e":"%%capture\n!python -m spacy download en\n!python -m spacy download de","2a2bf3ad":"import os\nimport re\nimport time\nimport math\nimport random\nimport unicodedata\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport spacy\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","107dfc25":"SEED = 28\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","6d4b915e":"data_df = pd.read_csv('..\/input\/english-to-german\/deu.txt', sep='\\t', usecols=[0, 1])\ndata_df.columns = ['en', 'de']\ndata_df.head()","ac2232be":"data_df.shape","ac6aa897":"plt.figure(figsize=(12, 6))\nplt.style.use('ggplot')\nplt.subplot(1, 2, 1)\nsns.distplot(data_df['en'].str.split().apply(len))\nplt.title('Distribution of English sentences length')\nplt.xlabel('Length')\n\nplt.style.use('ggplot')\nplt.subplot(1, 2, 2)\nsns.distplot(data_df['de'].str.split().apply(len))\nplt.title('Distribution of German sentences length')\nplt.xlabel('Length')\nplt.show()","0448c3a4":"seq_len_en = 20\nseq_len_de = 20","ffa050ad":"train_df, valid_df = train_test_split(data_df, test_size=0.1, shuffle=True, random_state=28)\n\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\n\nprint(train_df.shape)\nprint(valid_df.shape)","134d5ba5":"for i in range(len(train_df)-5, len(train_df)):\n    print(f'ENGLISH:\\n{train_df.iloc[i][\"en\"]},\\nGERMAN:\\n{train_df.iloc[i][\"de\"]}\\n{\"=\"*92}')","2da9c822":"class Vocabulary:\n    def __init__(self, freq_threshold=2, language='en', preprocessor=None, reverse=False):\n        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.tokenizer = spacy.load(language)\n        self.freq_threshold = freq_threshold\n        self.preprocessor = preprocessor\n        self.reverse = reverse\n\n    def __len__(self):\n        return len(self.itos)\n\n    def tokenize(self, text):\n        if self.reverse:\n            return [token.text.lower() for token in self.tokenizer.tokenizer(text)][::-1]\n        else:\n            return [token.text.lower() for token in self.tokenizer.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = len(self.itos)\n\n        for sentence in sentence_list:\n            # Preprocess the sentence using given preprocessor.\n            if self.preprocessor:\n                sentence = self.preprocessor(sentence)\n\n            for word in self.tokenize(sentence):\n                if word in frequencies:\n                    frequencies[word] += 1\n                else:\n                    frequencies[word] = 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenize(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n            for token in tokenized_text\n        ]","0e159627":"# Converts the unicode file to ascii\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n\n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    # Reference:- https:\/\/stackoverflow.com\/questions\/3645931\/python-padding-punctuation-with-white-spaces-keeping-punctuation\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n\n    w = w.strip()\n    return w","72586502":"%%time\n# Build vocab using training data\nfreq_threshold = 2\nen_vocab = Vocabulary(freq_threshold=freq_threshold, language=\"en\", preprocessor=preprocess_sentence, reverse=False)\nde_vocab = Vocabulary(freq_threshold=freq_threshold, language=\"de\", preprocessor=preprocess_sentence, reverse=True)\n\n# build vocab for both english and german\nen_vocab.build_vocabulary(train_df[\"en\"].tolist())\nde_vocab.build_vocabulary(train_df[\"de\"].tolist())","70e7b0ec":"class CustomTranslationDataset(Dataset):    \n    def __init__(self, df, en_vocab, de_vocab):\n        super().__init__()\n        self.df = df\n        self.en_vocab = en_vocab\n        self.de_vocab = de_vocab\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def _get_numericalized(self, sentence, vocab):\n        \"\"\"Numericalize given text using prebuilt vocab.\"\"\"\n        numericalized = [vocab.stoi[\"<sos>\"]]\n        numericalized.extend(vocab.numericalize(sentence))\n        numericalized.append(vocab.stoi[\"<eos>\"])\n        return numericalized\n\n    def __getitem__(self, index):\n        en_numericalized = self._get_numericalized(self.df.iloc[index][\"en\"], self.en_vocab)\n        de_numericalized = self._get_numericalized(self.df.iloc[index][\"de\"], self.de_vocab)\n\n        return torch.tensor(de_numericalized), torch.tensor(en_numericalized)","fb8dae7c":"class CustomCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        src = [item[0] for item in batch]\n        src = pad_sequence(src, batch_first=False, padding_value=self.pad_idx)\n        \n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n\n        return src, targets","d4f48a47":"BATCH_SIZE = 256\n\n# Define dataset and dataloader\ntrain_dataset = CustomTranslationDataset(train_df, en_vocab, de_vocab)\nvalid_dataset = CustomTranslationDataset(valid_df, en_vocab, de_vocab)\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=4,\n    shuffle=False,\n    collate_fn=CustomCollate(pad_idx=en_vocab.stoi[\"<pad>\"])\n)\n\nvalid_loader = DataLoader(\n    dataset=valid_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=4,\n    shuffle=False,\n    collate_fn=CustomCollate(pad_idx=en_vocab.stoi[\"<pad>\"])\n)","ba3c78a5":"fun_de = np.vectorize(lambda x: de_vocab.itos[x])\nfun_en = np.vectorize(lambda x: en_vocab.itos[x])","acb290b2":"print(f\"Unique tokens in source (de) vocabulary: {len(de_vocab)}\")\nprint(f\"Unique tokens in target (en) vocabulary: {len(en_vocab)}\")","07a5d773":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","c6165387":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout=0.2):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=0.0 if n_layers==1 else dropout)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        outputs, hidden_state = self.gru(x)\n        return hidden_state\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout=0.2):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim+hidden_dim, hidden_dim, n_layers, dropout=0.0 if n_layers==1 else dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim*2 + emb_dim, output_dim)\n    \n    def forward(self, x, hidden_state, context):\n        x = x.unsqueeze(0)\n        x = self.embedding(x)\n        x = self.dropout(x)\n        emb_con = torch.cat((x, context), dim=2)\n        outputs, hidden_state = self.gru(emb_con, hidden_state)\n        outputs = torch.cat((x.squeeze(0), hidden_state.squeeze(0), context.squeeze(0)), dim=1)\n        preds = self.fc(outputs)\n        return preds, hidden_state\n\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n        assert self.encoder.hidden_dim == decoder.hidden_dim\n        assert self.encoder.n_layers == decoder.n_layers\n    \n    def forward(self, x, y, teacher_forcing_ratio=0.75):\n        \n        target_len = y.shape[0]\n        batch_size = y.shape[1]\n        target_vocab_size = self.decoder.output_dim  # Output dim\n        \n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        \n        # Encode the source text using encoder. Last hidden state of encoder is context vector.\n        context = self.encoder(x)\n        \n        # context also used as the initial hidden state of the decoder\n        hidden_state = context\n        \n        # First input is <sos>\n        input = y[0,:]\n        \n        # Decode the encoded vector using decoder\n        for t in range(1, target_len):\n            output, hidden_state = self.decoder(input, hidden_state, context)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            pred = output.argmax(1)\n            input = y[t] if teacher_force else pred\n        \n        return outputs","ec40d856":"# Initialize all models\ninput_dim = len(de_vocab)\noutput_dim = len(en_vocab)\nemb_dim = 256\nhidden_dim = 512\nn_layers = 1\ndropout = 0.4\n\nencoder = Encoder(input_dim, emb_dim, hidden_dim, n_layers, dropout)\ndecoder = Decoder(output_dim, emb_dim, hidden_dim, n_layers, dropout)\nmodel = EncoderDecoder(encoder, decoder).to(device)","d623ef8b":"# Initialized weights as defined in paper\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.normal_(param.data, mean=0, std=0.01)\n        \nmodel.apply(init_weights)","ab732668":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","63282223":"optimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=en_vocab.stoi[\"<pad>\"])","5770f192":"def train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in tqdm(enumerate(iterator), total=len(iterator), position=0, leave=True):\n        src = batch[0].to(device)\n        trg = batch[1].to(device)\n\n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        #trg = [trg len, batch size]\n        #output = [trg len, batch size, output dim]\n        \n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg len - 1) * batch size]\n        #output = [(trg len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","2023370d":"def evaluate(model, iterator, criterion):\n    model.eval()    \n    epoch_loss = 0\n\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(iterator), total=len(iterator), position=0, leave=True):\n            src = batch[0].to(device)\n            trg = batch[1].to(device)\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg len - 1) * batch size]\n            #output = [(trg len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","f11fbda1":"def inference(model, sentence):\n    model.eval()\n    result = []\n\n    with torch.no_grad():\n        sentence = sentence.to(device)\n        \n        context = model.encoder(sentence)\n        hidden_state = context\n\n        # First input to decoder is \"<sos>\"\n        inp = torch.tensor([en_vocab.stoi[\"<sos>\"]]).to(device)\n\n        # Decode the encoded vector using decoder until max length is reached or <eos> is generated.\n        for t in range(1, seq_len_en):\n            output, hidden_state = model.decoder(inp, hidden_state, context)\n            pred = output.argmax(1)\n            if pred == en_vocab.stoi[\"<eos>\"]:\n                break\n            result.append(en_vocab.itos[pred.item()])\n            inp = pred\n            \n    return \" \".join(result)","67ccda36":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","837fa40f":"for sample_batch in valid_loader:\n    break","3e3b7958":"N_EPOCHS = 15\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nsample_source = ' '.join([word for word in fun_de(sample_batch[0][:, 101]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\nsample_target = ' '.join([word for word in fun_en(sample_batch[1][:, 101]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_loader, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best_model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n    print(f'\\t Sample Source (German): {sample_source}')\n    print(f'\\t Sample Target (English): {sample_target}')\n    print(f'\\t Generated: {inference(model, sample_batch[0][:, 101].reshape(-1, 1))}\\n')","defaa822":"# Load the best model.\nmodel_path = \".\/best_model.pt\"\nmodel.load_state_dict(torch.load(model_path))","c3abe072":"for idx in range(20):\n    print(f'ACTUAL GERMAN: {\" \".join([word for word in fun_de(sample_batch[0][:, idx]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])}')\n    print(f'ACTUAL: ENGLISH: {\" \".join([word for word in fun_en(sample_batch[1][:, idx]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])}')\n    print(f'GENERATED BY MODEL: {inference(model, sample_batch[0][:, idx].reshape(-1, 1))}')\n    print(\"=\"*92)","59e83681":"## Results","f78517bd":"In this notebook, I've implemented the slight improvement over basic version of Seq2Seq model (without attention mechanism), also know as encoder-decoder model. Most of the modeling and training part are referenced from part-I of this great tutorial series: https:\/\/github.com\/bentrevett\/pytorch-seq2seq.\n\nBut in this series, the preprocessed data is being used for training\/evaluation (because pytorch's Multi30k class provides all the heavy lifting), so it's bit difficult to generalize the structure for custom dataset implementation. So in this notebook, I've implemented data preprocessing like tokenization, padding etc. from scratch using spacy and pure pytorch.\n\nHere are some other references I've used:\n\n* [Original research paper: Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https:\/\/arxiv.org\/abs\/1406.1078)\n* [Creating custom dataset for NLP tasks](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/22635a65d8cf462aa44199357928e61c0ecda000\/ML\/Pytorch\/more_advanced\/image_captioning\/get_loader.py)","9d0dfe84":"## Modeling","a182de16":"As you can see, this works well on short sentences."}}