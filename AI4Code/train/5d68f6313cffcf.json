{"cell_type":{"3986c16a":"code","96d6d5ce":"code","3c57f7c3":"code","aba02100":"code","a8fa77b9":"code","5b8a3d75":"code","91dd7c23":"code","be2dcafa":"code","59ea14f7":"code","9c4c4ddb":"code","fa327c6d":"code","0569a29d":"code","1d37e6c8":"code","bd760822":"code","6cdddd4f":"code","68a775fc":"code","ee02c93f":"code","e5924958":"code","71c42d00":"code","57e033c5":"code","f9f6ca28":"code","4671daa2":"code","5bcc1e99":"code","31ea0cc0":"code","8a8697df":"code","a59eb7fd":"code","d65334f2":"code","f3f9d51e":"code","8a3f8e4b":"code","f6b35333":"code","3632be33":"code","48493272":"code","16a372b5":"code","ee4d3a15":"code","426c83f2":"code","281552c4":"code","78efec78":"code","a8443995":"code","9b68c236":"code","725591b0":"code","111f8e77":"code","6a510c92":"code","83b551ed":"code","65e8bd46":"code","e61930e4":"code","a804f1d0":"code","25b9ac23":"code","f4ccc99e":"code","d1835491":"code","abf01977":"markdown","40f2fe30":"markdown","bb2e96f7":"markdown","c93e6685":"markdown","360df091":"markdown","b9cf8760":"markdown","1f58251a":"markdown","9676bbfd":"markdown","d16b76d4":"markdown","13da55c3":"markdown","e314c6a4":"markdown","50ab517e":"markdown","b51a1deb":"markdown","2db48aeb":"markdown","deb79e40":"markdown","9a118a92":"markdown","da915070":"markdown","3b659134":"markdown","24da6fbc":"markdown","c0988fc3":"markdown","6fffd28b":"markdown","ff7a9c95":"markdown","5bdffc88":"markdown","8e9db6c8":"markdown","6319dedb":"markdown","1d676677":"markdown","4c499825":"markdown","6b722a3b":"markdown","0d7d1368":"markdown","14da5d8a":"markdown","334460c5":"markdown","0117e82a":"markdown","95e4551b":"markdown","c34b6eae":"markdown","8d05401c":"markdown","6ce07ac7":"markdown","04d60ccc":"markdown","0b5bb944":"markdown","6f33e100":"markdown","8b70e5dd":"markdown","b82f38f1":"markdown","ae983d99":"markdown","bc11cfb8":"markdown","3299fcd9":"markdown","84e864a6":"markdown","7c2b0e3b":"markdown","10b2d914":"markdown","cbdaa148":"markdown","7a355048":"markdown","f1196fbe":"markdown","1c2b64b9":"markdown","775e7bed":"markdown"},"source":{"3986c16a":"import numpy as np\nimport pandas as pd\nimport datetime # manipulating date formats\nimport os\n\nfrom matplotlib import pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose,STL\n\n# settings\nplt.style.use('seaborn-deep')","96d6d5ce":"ts = pd.read_csv(\"..\/input\/helsinki-energy-consumption\/daily.csv\", parse_dates=['timestamp'])\nts","3c57f7c3":"print(ts.info())\nts.describe()","aba02100":"ts.groupby(['reportingGroup','unit','locationName'])['locationName'].count()","a8fa77b9":"ts = ts[ts['unit'] != 'm3']\nts","5b8a3d75":"groups = ts.groupby('locationName')\n\n# divide into different dataframes for each location\nlocs = [groups.get_group(df).set_index('timestamp').value for df in groups.groups]\n\n# remove duplicated rows\nlocs = [df[~df.index.duplicated(keep='first')] for df in locs]\n\nts = pd.concat(locs, join='outer', axis=1, ignore_index=True)\nts","91dd7c23":"ts = ts.loc[:,ts.isna().sum() < 100]\nts.head()","be2dcafa":"ts = ts.sort_index().interpolate()\nts.head()","59ea14f7":"ts= ts.sum(axis=1).to_frame(name='kWh')\nts.index.rename('Date',inplace=True)","9c4c4ddb":"ts.plot(figsize = (12,6),style = 'o')","fa327c6d":"ts2017,ts2018,ts2019 = ts['2017-01-01':'2018-01-01'],ts['2018-01-01':'2019-01-01'],ts['2019-01-01':'2020-01-01']\nfigs,ax =  plt.subplots(3,figsize=(10,15))\n\n\nax[0].plot(ts2017,'o')\nax[0].set_title('Energy Consumption 2017')\nax[0].grid(True)\nax[1].plot(ts2018,'o')\nax[1].set_title('Energy Consumption 2018')\nax[1].grid(True)\nax[2].plot(ts2019,'o')\nax[2].set_title('Energy Consumption 2019')\nax[2].grid(True)\n\nplt.show()","0569a29d":"feb_end = '2017-02-28'\n\nts.loc[:feb_end].plot(figsize = (12,6))\nplt.suptitle('Energy Consumption in January 2017')","1d37e6c8":"# The day of the week with Monday=0,..., Sunday=6\nts.loc[:feb_end].index.dayofweek","bd760822":"decompose = seasonal_decompose(ts2017.values, period = 7, model=\"additive\")\nplt.rcParams.update({'figure.figsize':(10,9)})\nf = decompose.plot()","6cdddd4f":"stl = STL(ts2017,seasonal=7,robust=True)\nres = stl.fit()\nfig = res.plot()","68a775fc":"# Basic imports\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\n\n# Machine learning basics\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE, r2_score\nfrom sklearn import datasets, linear_model","ee02c93f":"train_end = '2019-12-01'\ntest_end = '2020-01-01'\ndemo_start = '2019-11-25'\ndemo = ts[demo_start:test_end]\ntrain,test = ts[:train_end], ts[train_end:]\n\ntrain.plot(figsize=(12,6), style = 'o',grid=True)","e5924958":"X = (train.index - train.index[0]).days.values.reshape(-1, 1)\ny = train.kWh.values.reshape(-1, 1)\n\nX_demo = (demo.index - train.index[0]).days.values.reshape(-1,1)\ny_demo = demo.kWh.values.reshape(-1, 1)\n\nX_test = (test.index - train.index[0]).days.values.reshape(-1,1)\ny_test = test.kWh.values.reshape(-1, 1)\n\nregr = linear_model.LinearRegression()\nregr.fit(X,y)\n\nregr_pred_demo = regr.predict(X_demo)\nregr_pred_test = regr.predict(X_test)\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set(title='Energy consumption', xlabel='Date', ylabel='kWh')\n\nax.plot(demo.index, y_demo,'o')\nax.plot(demo.index, regr_pred_demo, linewidth=3)\n\nerr = 'Coefficients on test set: ' + str(round(regr.coef_.take(0).take(0),2)) + '\\n Root mean squared error on test set: ' + \\\n    str(round(sqrt(MSE(y_test, regr_pred_test)),2)) + '\\nCoefficient of determination on test set: ' + \\\n        str(round(r2_score(y_test, regr_pred_test), 2))\n\nplt.figtext(0.12, -0.08, err, ha=\"left\",fontsize=15,va='center')\nplt.xticks(rotation=45)\nlegend = ax.legend([\"Observed\", \"Forecast\"])\nax.grid(True)","71c42d00":"# Basic imports\nimport numpy as np\nimport pandas as pd\nimport datetime # manipulating date formats\nimport itertools\nimport time\nimport holidays\nfrom sklearn.metrics import mean_squared_error as MSE, r2_score\n\n# Stats packages\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.statespace.tools import diff\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","57e033c5":"def test_stationarity(timeseries):\n    # Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries,maxlag=7*4, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (round(dfoutput,3))\n\ntest_stationarity(ts.kWh)","f9f6ca28":"# invert differenced forecast\ndef invert_diff(series, first_element_before):\n    cumsum = series.cumsum()\n    return cumsum.fillna(0) + first_element_original\n\ndiffed = diff(ts,seasonal_periods=7)\ndiffed.plot(figsize=(10,6))","4671daa2":"test_stationarity(diffed.kWh)","5bcc1e99":"acf = plot_acf(diffed)\npacf = plot_pacf(diffed)","31ea0cc0":"begin = '2017-01-01'\nabv = '2019-12-31'\nhol = pd.to_datetime(holidays.CountryHoliday('Finland')[begin:abv])\n\nexo = ts.copy()\nexo[\"holidays\"] = 0\nexo.loc[hol] = 1\n\nexo.drop([\"kWh\"],axis=1,inplace=True)\nexo_train,exo_test = exo[:train_end],exo[train_end:abv]\nexo_test[:10]","8a8697df":"def sarimax(ts,exo,all_param):\n    results = []\n    for param in all_param:\n        try:\n            mod = SARIMAX(ts,\n                          exog = exo,\n                          order=param[0],\n                          seasonal_order=param[1])\n            res = mod.fit()\n            results.append((res,res.aic,param))\n            print('Tried out SARIMAX{}x{} - AIC:{}'.format(param[0], param[1], round(res.aic,2)))\n        except Exception as e:\n            print(e)\n            continue\n            \n    return results","a59eb7fd":"# set parameter range\np,d,q = range(0,3),[1],range(0,3)\nP,D,Q,s = range(0,3),[1],range(0,3),[7]\n# list of all parameter combos\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = list(itertools.product(P, D, Q, s))\nall_param = list(itertools.product(pdq,seasonal_pdq))\n\nall_res = sarimax(train,exo_train,all_param)","d65334f2":"all_res.sort(key=lambda x: x[1])\nall_res[:5]","f3f9d51e":"res = all_res[0][0]\nres.plot_diagnostics(figsize=(15, 12))\n\nplt.show()\nprint(\"Ljung-box p-values:\\n\" + str(res.test_serial_correlation(method='ljungbox')[0][1]))\nres.summary()","8a3f8e4b":"pred_test = res.get_prediction(start=train_end,end=test_end,exog=exo_test)\nerr = '\\nRoot mean squared error: %.2f'% sqrt(MSE(test, pred_test.predicted_mean))\n\npred = res.get_prediction(start=begin,end=test_end,exog=exo_test)\npred_ci = pred.conf_int()\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set(title='Energy consumption', ylabel='kWh')\n\nts.plot(ax=ax, style = 'o')\npred.predicted_mean.plot(ax=ax, style='o')\nci = pred_ci.loc[demo_start:]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nplt.figtext(0.12, -0.06, err, ha=\"left\",fontsize=15,va='center')\nlegend = ax.legend([\"Train Set Observed\",\"Test Set Observed\", \"Forecast\"])","f6b35333":"pred_test = res.get_prediction(start=train_end,end=test_end,exog=exo_test)\n# The root mean squared error\nerr = '\\nRoot mean squared error: %.2f'% sqrt(MSE(test, pred_test.predicted_mean))\n\npred = res.get_prediction(start=demo_start,end=test_end,exog=exo_test)\npred_ci = pred.conf_int()\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set(title='Energy consumption', ylabel='kWh')\n\ntrain[demo_start:].plot(ax=ax)\ntest.plot(ax=ax)\npred.predicted_mean.plot(ax=ax)\nci = pred_ci.loc[demo_start:]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nplt.figtext(0.12, -0.06, err, ha=\"left\",fontsize=15,va='center')\nlegend = ax.legend([\"Train Set Observed\",\"Test Set Observed\", \"Forecast\"])\nax.grid(True)","3632be33":"import numpy as np\nimport pandas as pd\nimport torch\nimport seaborn as sns\nimport datetime # manipulating date formats\nimport holidays\nimport torch\nimport torch\nimport torch.nn as nn\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error as MSE, r2_score\nfrom torch.autograd import Variable\nfrom fastprogress import master_bar, progress_bar\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import mean_squared_error as MSE, r2_score\nfrom torch.autograd import Variable\n\n# settings\nplt.style.use('seaborn-deep')\nimport warnings\nwarnings.filterwarnings(\"ignore\")","48493272":"import holidays\n\nbegin = '2017-01-01'\nend = '2019-12-31' # errors within the holidays package\nhol = pd.to_datetime(holidays.CountryHoliday('Finland')[begin:end])\n\nexo = ts.copy()\nexo[\"holidays\"] = 0\nexo.loc[hol] = 1\nexo.loc[\"2020-01-01\",\"holidays\"] = 1\n\nts[\"holidays\"] = exo[\"holidays\"]\nts","16a372b5":"date = ts.index\ntimestamp_s = date.map(datetime.datetime.timestamp)","ee4d3a15":"week = 7*24*60*60\nts['Week sin'] = np.sin(timestamp_s * (2 * np.pi \/ week))\nts['Week cos'] = np.cos(timestamp_s * (2 * np.pi \/ week))","426c83f2":"plt.plot(np.array(ts['Week sin'])[:22])\nplt.plot(np.array(ts['Week cos'])[:22])\n\nplt.xlabel('Time [d]')\nplt.title('Time of week signal')","281552c4":"ts.head(10)","78efec78":"ts.describe()","a8443995":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_kwh = MinMaxScaler(feature_range=(0, 1)).fit(ts[[\"kWh\"]])\nts_normalized = pd.DataFrame(scaler.fit_transform(ts),\n                             columns = ts.columns.values,\n                            index = ts.index)","9b68c236":"ts_normalized","725591b0":"melt_plot = ts_normalized.melt(var_name='Column', value_name='Normalized value')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized value', data=melt_plot)\n_ = ax.set_xticklabels(ts_normalized.keys(), rotation=90)","111f8e77":"train_end = '2019-12-01'\ntest_end = '2020-01-01'\ndemo_start = '2019-11-25'\n\ntest_df = ts_normalized[train_end:]\ntrain_val = ts_normalized[:train_end]\n\nn = len(train_val)\n\ntrain_df = train_val[:int(n*0.8)]\nval_df = train_val[int(n*0.8):]\n\ncolumn_indices = {name: i for i, name in enumerate(ts.columns)}\nnum_features = ts.shape[1]\ntrain_df","6a510c92":"def split_sequence(df, fea_width=28, label_width=1, shift=1, label_col_indices=[0]):\n    \n    \"\"\" Split sequence with sliding window into\n        sequences of context features and label.\n        Args:\n            df (DataFrame): the target time series\n            fea_width (int): Length of features vector.\n            label_width (int): Length of labels vector.\n            shift (int): Distance between the features vector and the labels one. \n            label_col_indices (list(int)): list of the columns for the labels.\n            ra\n        Return:\n            X (np.array): sequence of features\n            y (np.array): sequence of labels\n    \"\"\"\n    arr = df.to_numpy()\n\n    # Work out the label column indices.\n\n    window_size = fea_width + shift + label_width - 1\n    fea_slice = slice(0, fea_width)\n    fea_indices = np.arange(window_size)[fea_slice]\n\n    label_slice = slice(window_size - label_width, None)\n    label_indices = np.arange(window_size)[label_slice]\n    \n    \n    print(\"\\n window_size: {} \\n fea_indices: {} \\n label_indices: {}\"\n          .format(window_size,fea_indices,label_indices))\n    \n    \n    def split_window(window):\n        features = window[fea_slice]\n        labels = window[label_slice]\n#         print(label_col)\n#         print([label_col_indices[name] for name in label_col])\n        if label_col_indices is not None:\n            labels = np.stack(\n                [labels[:, label_col_indices[idx]] \n                 for idx in label_col_indices],\n                axis=-1)\n        \n        labels = labels.flatten()\n        \n        return features, labels\n            \n    n = len(arr)\n    X,y = list(),list()\n    for i in range(0, n, shift):\n        \n        window = arr[i:i+window_size]\n        if (len(window) != window_size): break\n        \n        # Find the end of this pattern:\n        features,labels = split_window(window)\n        X.append(features)\n        y.append(labels)\n        \n#     print(label_col_indices)\n#     if randomize == True:\n#         random.shuffle(X)\n#         random.shuffle(y)\n    \n    return X,y","83b551ed":"fea_width, label_width, shift, label_col_indices = 28, 1, 1, [0]\n\nX_ts,y_ts = split_sequence(ts_normalized)\n\nX_train,y_train = split_sequence(train_df)\nX_val,y_val = split_sequence(val_df)\nX_test,y_test = split_sequence(test_df)\n\nX_train[0].shape,y_train[0].shape","65e8bd46":"def to_tensor(ls):\n    return Variable(torch.cuda.FloatTensor(np.array(ls)))\n\nX_ts,y_ts = to_tensor(X_ts),to_tensor(y_ts)\n\nX_train,y_train = to_tensor(X_train),to_tensor(y_train)\nX_val,y_val = to_tensor(X_val),to_tensor(y_val)\nX_test = to_tensor(X_test)\n\nX_train","e61930e4":"class LSTM(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.batch_size = 1\n        #self.seq_length = seq_length\n        self.LSTM = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,batch_first=True,dropout = 0.25)\n        \n        \n        \n        self.fc = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(p=0.2)\n    def forward(self, x):\n        h_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).cuda())\n         \n        \n        c_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).cuda())\n        \n       \n        _, (hn, cn) = self.LSTM(x, (h_1, c_1))\n     \n        #print(\"hidden state shpe is:\",hn.size())\n        y = hn.view(-1, self.hidden_size)\n        \n        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n        #print(\"final state shape is:\",final_state.shape)\n        out = self.fc(final_state)\n        #out = self.dropout(out)\n        #print(out.size())\n        return out\n\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nnum_epochs = 300\nlearning_rate = 1e-3\ninput_size = 4\nhidden_size = 512\nnum_layers = 2\n\nnum_classes = 1\n\nmodel = LSTM(num_classes, input_size, hidden_size, num_layers)\nmodel.cuda()\n\n\nmodel.apply(init_weights)\n\ncriterion = torch.nn.MSELoss().cuda()    # Will be transformed to root mean-squared error for regression\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=100, factor =0.5 ,min_lr=1e-7, eps=1e-08)","a804f1d0":"for epoch in progress_bar(range(1,num_epochs+1)):\n    model.train()\n    optimizer.zero_grad()\n    y_pred = model(X_train)\n    \n    loss = torch.sqrt(criterion(y_pred, y_train))\n    loss.backward()\n    \n    optimizer.step()\n    \n    model.eval()\n    valid = model(X_val)\n    val_loss = torch.sqrt(criterion(valid, y_val))\n    scheduler.step(val_loss)\n    \n    if epoch%20 == 1:\n        print(\"Epoch: %d, loss: %1.5f validation loss:  %1.5f \" \n              %(epoch, loss.item(),val_loss.item()))\n\nprint(\"Final: \\nEpoch: %d, loss: %1.5f validation loss:  %1.5f \" \n              %(epoch, loss.item(),val_loss.item()))","25b9ac23":"model.eval()\n\n## Inverse Normalize \ny_test_pred = scaler_kwh.inverse_transform(model(X_test).cpu().detach().numpy())\ny_test_obs = scaler_kwh.inverse_transform(y_test)\n\nprint('The root mean squared error of the model is ' +\n      str(round(np.sqrt(MSE(y_test_obs,y_test_pred)),2)))","f4ccc99e":"y_pred = scaler_kwh.inverse_transform(model(X_ts).cpu().detach().numpy())\ny_obs = scaler_kwh.inverse_transform(y_ts.cpu().numpy())\n\npred_df = pd.DataFrame(y_pred,index = ts.index[-len(y_pred):],columns = [\"kWh\"])\nobs_df = pd.DataFrame(y_obs,index = ts.index[-len(y_pred):],columns = [\"kWh\"])\n\nfig, ax = plt.subplots()\nplot_df = pd.concat([obs_df,pred_df],axis=1)\nplot_df.plot(ax=ax,figsize=(12,7),style = 'o', grid=True)\nax.legend([\"Observed\", \"Forecasts\"])\nax.set_title(\"Energy Consumption 2017-2019\")\nax.set_ylabel(\"kWh\")","d1835491":"fig, ax = plt.subplots()\nplot_df.loc[train_end:].plot(ax = ax, figsize=(12,7), grid=True)\n\nax.legend([\"Observed\", \"Forecasts\"])\nax.set_title(\"Energy Consumption December 2019\")\nax.set_ylabel(\"kWh\")","abf01977":"### Data Visualization <a class=\"anchor\" id=\"1.2.\"><\/a>","40f2fe30":"Helsinki Energy Demand Prediction\n==============\nThis notebook aims to predict the aggregate energy demand from a selected list of locations in the City of Helsinki using both the classical Box-Jenkins method and an advanced Deep Learning model. \n\nThe data is fetched from the [Nuuka open API](https:\/\/helsinki-openapi.nuuka.cloud\/swagger\/index.html#\/), courtesy of [Avoindata.fi](avoindata.fi). Visit our Github repository at [github.com\/quan-possible\/energy-demand-prediction](https:\/\/github.com\/quan-possible\/energy-demand-prediction).\n\nAuthors: [Bruce Nguyen](https:\/\/github.com\/quan-possible) and [Son Le](https:\/\/github.com\/SonAlexLe)","bb2e96f7":"The seasonality within each year is very much predictable. The demand peaks in winter-spring then bottoms out during summer.\n\nLet us proceed to zoom in the plot further and investigate monthly, weekly and daily time periods.","c93e6685":"#### Holidays Data Generation <a class=\"anchor\" id=\"4.1.1.\"><\/a>\n\nTo get started, the exogenous 'holiday' variable can be reused entirely from the SARIMAX model.","360df091":"Unfortunately, we can see that there are many problems within the view we created. The most obvious one is that the unit is not the same for all the data points. In fact, only one place has its electricity measured in m3. We can fix this with a simple mask.","b9cf8760":"The forecasts appear to fit the observed values well, but is worse than that of the SARIMA with consistent under-prediction of the max values. Let us look at the prediction on the test set compare it with that of the SARIMAX.","1f58251a":"Now, we can see the trends and the seasonality clearly. Concerning the residuals, it resembles white noise to some extent, thus indicating that the model is a good fit. However, it also exhibits some abnormalities with clear patterns, especially in the summer period. This suggests that the residuals also erroneously take into account of the trend component - a \"leakage\". The problem mostly stems from this method of decomposition itself, being a very old and outdated technique. \n\nFortunately, we can overcome this by substituting it for a novel, more sophisticated decomposition method called [STL decomposition](https:\/\/otexts.com\/fpp2\/stl.html). The technique is implemented in the class [STL](https:\/\/www.statsmodels.org\/devel\/generated\/statsmodels.tsa.seasonal.STL.html) in the same package.","9676bbfd":"The only step left is to aggregate all the values into a single 'energy consumption' column. We will also rename the columns for convenience.","d16b76d4":"## Table of Contents\n1. [Exploratory Data Analysis](#1.)\n    1. [Data Cleaning](#1.1.)\n    2. [Data Visualization](#1.2.)\n2. [Baseline Model](#2.)\n3. [SARIMAX](#3.)\n    1. [Model Identification](#3.1.)\n    2. [Model Estimation](#3.2.)\n    3. [Model Validation](#3.3.)\n1. [LSTM](#4.)\n    1. [Feature Engineering](#4.1.)\n        1. [Holidays Data Generation](#4.1.1.)\n        2. [Cyclical feature encoding](#4.1.2.)\n        3. [Feature Vector Normalization](#4.1.3.)\n    2. [Modeling](#4.2.)\n3. [Further Readings](#5.)","13da55c3":"## Baseline Model <a class=\"anchor\" id=\"2.\"><\/a>\n\nIn every modeling process, there needs to be a baseline model whose results can be used to assess our more sophisticated models. Such a model should be easy to implement and can provide an adequate result. In this case, a simple linear regression model would suffice.\n\nConceptually, the energy consumption is the forecast variable $y$, while the time stamp is the predictor variable $x$ for this specific model. We can mathematically formulate the equation for this regression as follow\n$$\ny_t = \\beta_0 + \\beta_1 t + \\epsilon_t,\n$$\nwhere $y_t$ is the data, and $t$ is the date time. ","e314c6a4":"First, we can take a look at the consumption during each year.","50ab517e":"### Model Estimation <a class=\"anchor\" id=\"3.2.\"><\/a>","b51a1deb":"### Model Identification <a class=\"anchor\" id=\"3.1.\"><\/a>\nMoving on to predicting the time series using SARIMAX models. Theoretically, they are simply combinations of Seasonal ARIMA with exogenous variables, which improves outliers modeling. One shorthand notation for the SARIMA model is\n$$\n\\text{SARIMA } (p,d,q) \\times (P,D,Q)S,\n$$\nwhere $p$ = non-seasonal autoregressive (AR) order, $d$ = non-seasonal differencing, $q$ = non-seasonal moving average (MA) order, $P$ = seasonal AR order, $D$ = seasonal differencing, $Q$ = seasonal MA order, and $S$ = time span of repeating seasonal pattern. The use of ARIMA models to fit a time series is called the [Box-Jenkins procedure](https:\/\/en.wikipedia.org\/wiki\/Box%E2%80%93Jenkins_method).\n\nIn order to apply the model, we first need to guarantee that the time series is stationary, or can be transformed into a stationary one. To simply put, a stationary time series is one whose properties is not dependent on time, with the mean, variance and autocorrelation structure do not change over time.\n\nWe can see from the initial plot that the time series is clearly not stationary, with visible seasonality and possibly trend. However, to be more precise, we can employ various stationarity tests. Here, the one we used is the [Augmented Dickey-Fuller test](https:\/\/en.wikipedia.org\/wiki\/Augmented_Dickey%E2%80%93Fuller_test), which is one of the commonly-used statistics.","2db48aeb":"## Modeling <a class=\"anchor\" id=\"4.2.\"><\/a>\n\nCertainly, we need to split the dataset into train, validation, and test sets first. The test set here will be of the same time period as the one in the SARIMAX model, so we can compare the performance more easily.","deb79e40":"In addition, the number of recorded values are not the same for all the locations, meaning that there are missing values for each location. This will take a bit more effort, starting with creating a new `DataFrame` consisting only the locations as columns.","9a118a92":"So far, so good. Now we need to check the error of the model on the test set without normalization. ","da915070":"We can see that the quality of the decomposition is much better. That said, much of the patterns we have seen is still very much present. This means we have some detrending and deseasonalizing to do in the next step - modelling. ","3b659134":"### Feature Vector Normalization <a class=\"anchor\" id=\"4.1.3.\"><\/a>\n\nSince there are big discrepancies between the scales of the different features, we need to normalize them into a common scale, without distorting the differences in the ranges of values or losing information.","24da6fbc":"In the following section, we will further investigate the time series by decomposing it in to the 3 components:\n* Trend-cycle - increases or decreases within the data in the long-term or not of a fixed frequency. \n* Seasonal - pattern occurs when a time series is affected by seasonal factors.\n* Residuals - the remainder after removing the 2 aforementioned components.\n\nIf we are looking at an additive decomposition, this can be mathematically formulated as\n$$\ny_t = S_t + T_t + R_t,\n$$\nwhere $y_t$ is the data, $S_t$ is the seasonal component, $T_t$ is the trend-cycle component, and $R_t$ is the remainder component, all at period $t$. Similarly, a multiplicative decomposition would be written as\n$$\ny_t = S_t \\times T_t \\times R_t.\n$$","c0988fc3":"According to the AIC score, the best model to choose is\n$$\n\\text{SARIMA } (1,1,2) \\times (1,1,2)7.\n$$\n\nEven though the [parsimony principle](http:\/\/rpierse.esy.es\/rpierse\/files\/fe2.pdf) is violated for this model, the margin is small enough, and we can gain from some flexibility.","6fffd28b":"We can see that there is not a big variation between the AICs, i.e. most of the models are of similar quality. Let us list the estimated best models.","ff7a9c95":"Despite better fitness during some time points, the model consistently under-predict the values by a wide margin. Concerning the holiday period, the forecasts still have the same defects as that of the SARIMAX model.\n\nIn conclusion, even though the presented models have presented reasonable accuracy, time series forecasting still presents many challenges. That said, this is still a very active research area, offering much improvement in the future.","5bdffc88":"The `NaN` values are now visible, now we only need to decide what to do with them. Our approach is to drop columns (locations) where there are more than 100 missing entries, since such columns are not significant to our analysis anymore.","8e9db6c8":"In order for the LSTM to work, we need to create sliding windows from the data so that for each datapoint, its features is the window of a determined size of the past values . In this project, the width of the features vector is 28 days, or 4 weeks, while there is only one label for each window. \n\n<img src=\"https:\/\/github.com\/quan-possible\/energy-demand-prediction\/blob\/main\/daily\/data_window.png?raw=true\" alt=\"A window of data\">","6319dedb":"Since the p-value is much smaller than $\\alpha = 0.05$, the differenced time series is stationary. It follows that the suitable values for the terms $d$ and $D$ are both 1.\n\nWe now proceed to check the correlation structure within the transformed data using [Autocorrelation Function (ACF)](https:\/\/www.statsmodels.org\/devel\/generated\/statsmodels.tsa.stattools.acf.html) and [Partial Autocorrelation Function (PACF)](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.pacf.html) plots. This step is crucial in order to identify the suitable order of the AR and MA terms, both non-seasonal and seasonal.","1d676677":"We can see that the p-value is bigger than $\\alpha = 0.05$. Therefore, the time series is unsurprisingly not stationary.  \n\nIn order to stationarize the data, we need to remove the trend and seasonal components. We can do that by differencing the data by the desired order of differencing. Intuitively, this is analogous to taking the derivatives of functions.","4c499825":"We start with splitting the data into train set and test set. To make the problem more challenging, we intentionally design our test set to be overlapped with the winter holiday season, which can help us more accurately judge the performance of the models when there is a strong presence of consecutive outliers.\n\nIn order to view both the [in-sample](https:\/\/otexts.com\/fpp2\/accuracy.html) and [out-of-sample](https:\/\/otexts.com\/fpp2\/accuracy.html) prediction, a part of the time series that contain both the data from the test set and train set is set aside, called `demo`. We will plot on the `demo` set in the end of this section.  ","6b722a3b":"From the plots, a complex pattern with unexpected increases and decreases in the values of the functions can be found. Therefore, we cannot preclude any orders for the AR and MA terms. \n\nWith regards to the seasonal terms, the plots show expected behaviours with spikes at lags equals to the number of days in a week. It can be hypothesized that P = 1 and Q = 2, with the tapering autocorrelation function and the sinusoidal shape of the partial autocorrelation function. Let us check our guesses in the next section where we apply the model.","0d7d1368":"We first apply the [classical decomposition method](https:\/\/otexts.com\/fpp2\/classical-decomposition.html) implemented in the class [`seasonal_decompose`](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.seasonal.seasonal_decompose.html) from the [`statsmodels`](https:\/\/www.statsmodels.org) package. For clarity, we only use a year of data.","14da5d8a":"Now, the data looks much more appropriate for SARIMAX modeling. We can check again with the [Dickey-Fuller test](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.adfuller.html).","334460c5":"To determine the goodness of fit of the model, we can examine its residuals using the standard assumption: the error term $\\epsilon_t$ should be white noise drawings from a fixed distribution with a constant mean and variance.\n\nWe can check this by looking at the various plots showing the distribution of the residuals. In addition, the Ljung-box test can also be used to do this more precisely. ","0117e82a":"## SARIMAX <a class=\"anchor\" id=\"3.\"><\/a>","95e4551b":"### Data Cleaning <a class=\"anchor\" id=\"1.1.\"><\/a>","c34b6eae":"We now can finally get to building the neural network! Our model has 2 hidden LSTM layers with 512 neurons each. The input and output layers has 4 and 1 nodes corresponding to the shape of the features and labels vector respectively. The Adam optimizer is used, and the learning rate is dynamically modified using a scheduler.\n\nThe architecture is inspired by [OmerS. Learning Pytorch LSTM Deep Learning with M5 Data](https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data).","8d05401c":"### Model Validation <a class=\"anchor\" id=\"3.3.\"><\/a>","6ce07ac7":"Concerning the rest of the columns, we can just interpolate the missing values linearly. This method is simple but effective, given that the number of missing points is nominal.","04d60ccc":"Upon examining the plot, we can see that there is a very visible pattern that repeats itself every 7 days. Therefore, it can be deduced that there is a weekly seasonality within the time series. We can further explore this pattern by looking at the days of the weeks during the time period.","0b5bb944":"For the exogenous variables, one prime candidate is the date of holidays in Finland. We can procure such data using the [holidays package](https:\/\/pypi.org\/project\/holidays\/).","6f33e100":"From the result, we can see that energy consumption level is at its height during the weekdays, then drops significantly during the weekends. This very much fits our expectation, since factories and workplaces, which follow such a schedule, are places that consume the most energy.","8b70e5dd":"### Feature Engineering <a class=\"anchor\" id=\"4.1.\"><\/a>\n\nBy preparing the data well in advance, we can help simplify models and improve their performance by a great extent with comparatively little effort. This means to come up with better representations of data or adding more useful information, i.e. to engineer features.","b82f38f1":"## Exploratory Data Analysis <a class=\"anchor\" id=\"1.\"><\/a>","ae983d99":"## LSTM <a class=\"anchor\" id=\"4.\"><\/a> ","bc11cfb8":"# Further Readings <a class=\"anchor\" id=\"5.\"><\/a>\n\n* [Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition. *OTexts: Melbourne, Australia*](https:\/\/otexts.com\/fpp2\/)\n* [Shumway, R., & Stoffer, D. (2017). Time Series Analysis and Its Applications. *Springer Texts In Statistics*. https:\/\/doi.org\/10.1007\/978-3-319-52452-8](https:\/\/www.springer.com\/gp\/book\/9783319524511)\n*[SARIMAX: Introduction. *Statsmodels.org. (2021)*. Retrieved 17 January 2021.](https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_sarimax_stata.html.)\n* [NIST\/SEMATECH e-Handbook of Statistical Methods. Retrieved 17 January 2021.](https:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section4\/pmc445.htm)\n* [Time series forecasting: TensorFlow Core.](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/time_series#feature_engineering)\n* [Zhang, G. (2012). Neural Networks for Time-Series Forecasting. *Handbook of Natural Computing*, 461\u2013477.](https:\/\/link.springer.com\/referenceworkentry\/10.1007%2F978-3-540-92910-9_14#citeas)","3299fcd9":"From what we can see, the data contains the electricity consumption from various locations in Helsinki in the time period in question. However, since we are interested only in the aggregate demand by date (without location), the task is to sum up the values from those locations.\n\nIn order to do that, we need to get more information about the eneregy consumption from each location and remove any abnormality, if need be.","84e864a6":"From the plots, the residuals seem to be normally distributed around 0 - which is the condition that we need - with slightly heavy tails. However, looking at the Ljung box statistics, we cannot reject the hypothesis that the data are not independently distributed, since the p-values are smaller than $\\alpha = 0.05$ for some lags from 6 onwards. \n\nNevertheless, let us use this model to predict on the test set and judge it for ourselves.","7c2b0e3b":"The result looks very satisfactory! During the first 2 weeks of the month, the forecasted values fit well to the actual ones, possibly with the exception of the 6th of December - the Independence day of Finland.\n\nWith regards to the winter holiday season, the model unfortunately did not do as well. Contrary to the first 2 weeks, the only day where the values are more accurately predicted is the 27th, despite the addition of the \"holiday\" variable. This shows the challenges of forecasting during exceptional time periods. Nevertheless, the model still show promises in forecasting when the data behaves predictably.","10b2d914":"We start by reading the data and try to get familiar with it.","cbdaa148":"This is only a marginal improvement over the SARIMAX model. In fact, we can see that the LSTM model performs worse than the classical method after accounting for the far greater level of complexity of the Deep Neural Networks. This is in agreement with the literature on time series forecasting, with deep learning methods having not delivered on their promises.\n\nNevertheless, we can further examine the fitness of the model on the data by visualizing the results, starting with the all the data available.","7a355048":"The only secondary step left is to convert the data into Pytorch Tensors.","f1196fbe":"Now we can finally get to apply the model. In order to expedite the process, we can use the Grid Search method, which involves experimenting with many different parameters and selecting the best by a loss function. For ARIMA models, Akaike Information Criterion (AIC) is a widely used estimator, and we will also use it for this task. ","1c2b64b9":"Now, we know that we need to do much better than a root mean squared error of 101,738 kWh!","775e7bed":"#### Cyclical feature encoding <a class=\"anchor\" id=\"4.1.2.\"><\/a>\n\nThe separate datetime data on its own is not very useful to the learning task. Therefore, we need a way to encode the periodicity of the time series from our knowledge into the data. This can be achieved by modelling time as a circular scale, which is simply mapping the date time values using trigonometric functions."}}