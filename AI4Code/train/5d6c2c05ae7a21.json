{"cell_type":{"1262afb6":"code","c616beb6":"code","eb149db7":"code","3f38d7d1":"code","8f5dbf04":"code","8ab44562":"code","39d15d81":"code","ffb4d12e":"code","a989cbdf":"code","0798d9f2":"code","bf676946":"code","838179f5":"code","d9e3945a":"code","3e8f14a7":"code","f7d54d6d":"code","a603bdc6":"code","55497973":"code","5d761423":"code","42306f75":"code","18aa8626":"code","0a14f11f":"code","a05d005e":"code","3737b551":"code","a41a1b5e":"code","a479719c":"code","7675b3b6":"code","6d7661d4":"code","368ae4e9":"code","e3eb5792":"code","bbbdaf88":"code","393575f3":"code","85d85f2b":"markdown","96371ff2":"markdown","0473ed69":"markdown"},"source":{"1262afb6":"import os\nimport math\nimport time\nimport random\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as K\nfrom tensorflow.keras import Sequential, utils, regularizers, Model, Input\nfrom tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout, AvgPool1D\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\n","c616beb6":"train = pd.read_csv('..\/input\/heartpicture\/train.csv')\ntest = pd.read_csv('..\/input\/heartpicture\/testA.csv')\ntrain.describe()","eb149db7":"train['label'].value_counts()","3f38d7d1":"ids = []\nfor id, row in train.groupby('label').apply(lambda x: x.iloc[0]).iterrows():\n    ids.append(int(id))\n    signals = list(map(float, row['heartbeat_signals'].split(',')))\n    sns.lineplot(data=signals)\n\nplt.legend(ids)\nplt.show()","8f5dbf04":"\n# \u6570\u636e\u7cbe\u5ea6\u91cf\u5316\u538b\u7f29\ndef reduce_mem_usage(df):\n    # \u5904\u7406\u524d \u6570\u636e\u96c6\u603b\u5185\u5b58\u8ba1\u7b97\n    start_mem = df.memory_usage().sum() \/ 1024**2 \n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    # \u904d\u5386\u7279\u5f81\u5217\n    for col in df.columns:\n        # \u5f53\u524d\u7279\u5f81\u7c7b\u578b\n        col_type = df[col].dtype\n        # \u5904\u7406 numeric \u578b\u6570\u636e\n        if col_type != object:\n            c_min = df[col].min()  # \u6700\u5c0f\u503c\n            c_max = df[col].max()  # \u6700\u5927\u503c\n            # int \u578b\u6570\u636e \u7cbe\u5ea6\u8f6c\u6362\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            # float \u578b\u6570\u636e \u7cbe\u5ea6\u8f6c\u6362\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        # \u5904\u7406 object \u578b\u6570\u636e\n        else:\n            df[col] = df[col].astype('category')  # object \u8f6c category\n    \n    # \u5904\u7406\u540e \u6570\u636e\u96c6\u603b\u5185\u5b58\u8ba1\u7b97\n    end_mem = df.memory_usage().sum() \/ 1024**2 \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\n\n# \u8bad\u7ec3\u96c6\u7279\u5f81\u5904\u7406\u4e0e\u7cbe\u5ea6\u91cf\u5316\ntrain_list = []\nfor items in train.values:\n    train_list.append([items[0]] + [float(i) for i in (items[1].split(','))] + [items[2]])\ntrain = pd.DataFrame(np.array(train_list))\ntrain.columns = ['id'] + [str(i) for i in range(len(train_list[0])-2)] + ['label']  # \u7279\u5f81\u5206\u79bb\ntrain = reduce_mem_usage(train)  # \u7cbe\u5ea6\u91cf\u5316\ntrain=train.drop([\"id\"],axis=1)\n\n# \u6d4b\u8bd5\u96c6\u7279\u5f81\u5904\u7406\u4e0e\u7cbe\u5ea6\u91cf\u5316\ntest_list=[]\nfor items in test.values:\n    test_list.append([items[0]] + [float(i) for i in items[1].split(',')])\ntest = pd.DataFrame(np.array(test_list))\ntest.columns = ['id'] + [str(i) for i in range(len(test_list[0])-1)]  # \u7279\u5f81\u5206\u79bb\ntest = reduce_mem_usage(test)  # \u7cbe\u5ea6\u91cf\u5316","8ab44562":"train[train.columns[-1]].value_counts()","39d15d81":"\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=2021, n_jobs=-1)\ny_train = train['label']\nx_train = train.drop(['label'], axis=1)\nX, Y = smote.fit_resample(x_train, y_train) ","ffb4d12e":" from sklearn.model_selection import train_test_split","a989cbdf":" X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.2, random_state=11)","0798d9f2":"from keras.utils.np_utils import to_categorical\nY_train=to_categorical(y_train,4)\nY_test=to_categorical(y_test,4)","bf676946":"import numpy as np","838179f5":"X_train =np.array(X_train).reshape(len(X_train), X_train.shape[1],1)\nX_test = np.array(X_test).reshape(len(X_test), X_test.shape[1],1)","d9e3945a":"## define input size\nfeature_num = X_train.shape[1]","3e8f14a7":"X_train","f7d54d6d":"import tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D,BatchNormalization,Flatten,MaxPool1D,Dense,Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import Dropout","a603bdc6":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nwith strategy.scope()\n    model = tf.keras.Sequential()","55497973":"\ninput=(feature_num,1)\n\n\nmodel.add(Conv1D(16,(3),strides=(1),padding='same',activation='relu',input_shape=input))\nmodel.add(Conv1D(32,(3),strides=(1),dilation_rate=2,padding='same',activation='relu'))\nmodel.add(Conv1D(64,(3),strides=(1),dilation_rate=2,padding='same',activation='relu'))\nmodel.add(Conv1D(64,(3),strides=(1),dilation_rate=2,padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(3,2,padding='same'))\n\nmodel.add(Conv1D(128,(5),dilation_rate=2,padding='same',activation='relu'))\nmodel.add(Conv1D(128,(5),dilation_rate=2,padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(3,2,padding='same'))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu', kernel_initializer='he_normal'))\nmodel.add(Dense(4,activation='softmax'))","5d761423":"model.summary()","42306f75":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","18aa8626":"callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\nhistory=model.fit(X_train, Y_train,epochs=30,callbacks=callbacks, batch_size=64,validation_data=(X_test,Y_test))","0a14f11f":"import matplotlib.pyplot as plt\ny_pred=model.predict(X_test)\nfig1, ax_acc = plt.subplots()\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Model - Accuracy')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show()","a05d005e":"target_names=['0','1','2','3']\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix \ny_true=[]\nfor element in Y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)\nprediction.reshape(len(prediction),1)\ncnf_matrix = confusion_matrix(y_true, prediction)","3737b551":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n  \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a41a1b5e":"plt.figure(figsize=(10, 10))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],normalize=True,\n                      title='Confusion matrix, with normalization')\nplt.show()","a479719c":"My_X_test=test.drop([\"id\"],axis=1)","7675b3b6":"My_X_test","6d7661d4":"My_X_test = tf.expand_dims(My_X_test, -1) ","368ae4e9":"prediction_proba=model.predict(My_X_test)\nprediction_proba","e3eb5792":"df=pd.DataFrame( prediction_proba)","bbbdaf88":"df.index.name = \"id\"\ndf.columns=[\"label_0\",\"label_1\",\"label_2\",\"label_3\"]\ndf.index=df.index+100000","393575f3":"df.to_csv(\".\/submit.csv\")","85d85f2b":"TEST PART","96371ff2":"\u6570\u636e\u9884\u5904\u7406****","0473ed69":"\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3"}}