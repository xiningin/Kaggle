{"cell_type":{"c1e72d86":"code","20dbe6af":"code","ff109200":"code","3cb87cc9":"code","99a3188d":"code","818c73a3":"code","8414d362":"code","0e9b474f":"code","41946397":"code","2eca63f1":"code","bc2ca105":"code","10707a72":"code","6d1caf2b":"code","e791c51a":"code","edf0c2a6":"code","effb9fb3":"code","8291e811":"code","4f378a7b":"code","3ef121d0":"code","41f7eef9":"code","2194dd99":"code","1a0ddf73":"code","2bcc27fd":"code","9fe7cbcc":"code","db64dc05":"code","06a579e0":"code","dec03d74":"code","cd949a51":"code","b9ca630e":"code","857c79ed":"code","0b22f431":"code","0992beab":"code","a2d0bf3a":"code","c95d3e0f":"markdown","4e570ec0":"markdown","912917d7":"markdown","226894a7":"markdown","c2891c8b":"markdown","845f3635":"markdown","c729472a":"markdown","34e6afe7":"markdown"},"source":{"c1e72d86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re           # Regular Expression\nimport nltk         # Natural Language Toolkit\nimport string \nfrom nltk.corpus import stopwords  # Stop Words\nfrom nltk.stem import PorterStemmer # Stemming Tool \nfrom nltk.tokenize import TweetTokenizer # Tokenizer\nimport emoji    # Print Emojis\nfrom nltk.stem import WordNetLemmatizer # Lemmatizer Tools\n\n# dictionary for lemmatization\nnltk.download('stopwords')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","20dbe6af":"fake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv', encoding_errors='ignore')\nreal = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv', encoding_errors='ignore')\nrandom = pd.read_csv('..\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv', encoding_errors='ignore')","ff109200":"fake.tail(10)","3cb87cc9":"fake.shape","99a3188d":"real.tail(10)","818c73a3":"real.shape","8414d362":"random.tail(10)","0e9b474f":"fake.drop(['title', 'subject','date'], axis = 1,inplace=True)\nfake.head(10)","41946397":"real.drop(['title', 'subject','date'], axis = 1,inplace=True)\nreal.head(10)","2eca63f1":"random = random['full_text']","bc2ca105":"random = random.dropna()","10707a72":"random.head(100)","6d1caf2b":"random.shape","e791c51a":"# Removing all Urdu\/Arabic characters from the tweets data\nreg = re.compile(r'[\\u0600-\\u06ff]+', re.UNICODE)\nrandom = random.apply(lambda x: re.sub(reg, \"\", x))","edf0c2a6":"random.head(100)","effb9fb3":"# removing extra spaces\nrandom = random.apply(lambda x: re.sub(r'[  ]+', \" \", x))\n\n# converting to lowercase letters \nrandom = random.apply(lambda x: x.strip().lower())\n\n# remove hyperlinks\nrandom = random.apply(lambda x: re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\n\n# removing @Mentions\nrandom = random.apply(lambda x:re.sub(r'@.+?\\s', '', x))\n\n# removing extra symbols\nrandom = random.apply(lambda x: re.sub(r'#', '', x))\nrandom = random.apply(lambda x: re.sub(r'rt : ', '', x))\nrandom = random.apply(lambda x: re.sub(r'\\n', ' ', x))","8291e811":"random = random.drop_duplicates()\nrandom.shape","4f378a7b":"tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                           reduce_len=True)\nrandom = random.apply(tokenizer.tokenize)","3ef121d0":"random","41f7eef9":"stopwords_english = stopwords.words('english')\n\ndef clean(x):\n    return [y for y in x if y not in stopwords_english and y not in string.punctuation\n          and (len(y) > 1 or emoji.is_emoji(y)) ]\n\nrandom_data = random.apply(clean)","2194dd99":"stemmer = PorterStemmer()\ndef stem(x):\n    return [stemmer.stem(y) for y in x]\n\n#stemming\nstemmed_tweets = random_data.apply(stem)","1a0ddf73":"lematizer = WordNetLemmatizer()\ndef lema(x):\n    return [lematizer.lemmatize(y) for y in x]\n\n#lema\nlemmatized_tweets = random_data.apply(lema)","2bcc27fd":"tweet = pd.DataFrame({'text':lemmatized_tweets})\ntweet.head(10)","9fe7cbcc":"tweet_list = tweet['text'].to_list()","db64dc05":"sample_tweet = tweet_list[50:100]\nsample_tweet","06a579e0":"real_list = real['text'].to_list()","dec03d74":"sample_real = real_list[50]\nsample_real","cd949a51":"fake_list = fake['text'].to_list()","b9ca630e":"sample_fake = fake_list[50]\nsample_fake","857c79ed":"def cosine_distance_countvectorizer_method(s1, s2):\n    \n    # sentences to list\n    allsentences = [s1, s2]\n    \n    # packages\n    from sklearn.feature_extraction.text import CountVectorizer\n    from scipy.spatial import distance\n    \n    # text to vector\n    vectorizer = CountVectorizer()\n    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n     # distance of similarity\n    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n    print('Similarity of two dataset are equal to ',round((1-cosine)*100,2),'%')\n    return cosine","0b22f431":"# A tweet taken dataset of Random Tweet of Pakistan\nrandom_tweet = \"As a retd govt servant and someone above 65, banks will not give me a loan because I don't have a son (a man) as a guarantor. Banks will not accept a daughter even if she is a professional as a guarantor. This in the 21st century when we fight for equal rights for women.\"\n","0992beab":"cosine_distance_countvectorizer_method(random_tweet, sample_real)","a2d0bf3a":"cosine_distance_countvectorizer_method(random_tweet, sample_fake)","c95d3e0f":"# Cosine Similarity Calculator","4e570ec0":"# Preprocessing of Tweets Like Tokenization, Stemming, Lemmatization etc.","912917d7":"### This example shows that our sample dataset tweet is more similar to Fake dataset tweet....Astonishing","226894a7":"# Data Analysis","c2891c8b":"# Upload the Datasets ","845f3635":"# Picking Tweets TEXT from all Datasets","c729472a":"# Data To be Checked for Cosine Similarity","34e6afe7":"## Fake Tweets Dataset"}}