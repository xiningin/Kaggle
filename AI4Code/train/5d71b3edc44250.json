{"cell_type":{"f427a1d4":"code","3a21bdc4":"code","cc4aec5a":"code","092b2e0e":"code","e20bd48a":"code","cb7725fb":"code","f59b7fe9":"code","d2bf2cb4":"code","96c018ef":"code","00356cb1":"code","7a4a5bf6":"markdown","15d5633f":"markdown","eb2bc2cb":"markdown","891d014b":"markdown","1c46e815":"markdown","3a050521":"markdown","6f53205f":"markdown","646721d8":"markdown","d491a877":"markdown","03edc3ad":"markdown","82483c75":"markdown","7f77f058":"markdown","8863c555":"markdown","29bf41e1":"markdown","05f4cd0f":"markdown","68a2f448":"markdown","63c1e574":"markdown","d0960806":"markdown","e3bfb713":"markdown","70df7409":"markdown","6f59cadd":"markdown","ea5fb5e7":"markdown","998d498f":"markdown","2af9889a":"markdown","9aa3a234":"markdown","2806b2d4":"markdown","22bc43e3":"markdown","6678721d":"markdown","1b51e782":"markdown","dcd2dcb8":"markdown","c1db0695":"markdown","e15f3368":"markdown","2d81c561":"markdown","8d1b8c80":"markdown","e7396bcb":"markdown"},"source":{"f427a1d4":"import pandas as pd","3a21bdc4":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")","cc4aec5a":"print(train.loc[0, \"excerpt\"])","092b2e0e":"train.loc[0, \"target\"]","e20bd48a":"train.head(2).T","cb7725fb":"train[\"target\"].plot(kind=\"hist\", bins=100)","f59b7fe9":"train[\"standard_error\"].plot(kind=\"hist\", bins=100)","d2bf2cb4":"# TODO: Use some gensim and\/or spacy to explore the dataset?","96c018ef":"class Dataset:\n    def __init__(self, excerpt, tokenizer, max_len):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","00356cb1":"import torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n\n\n\n\ndef generate_predictions(model_path, max_len):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            output = output.logits.detach().cpu().numpy().ravel().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)\n","7a4a5bf6":"A notebook were I will explore some of the best tools offered by [HuggingFace](https:\/\/huggingface.co\/) and apply these to the \ncompetition's data.\n\nLet's first start by exploring the dataset.","15d5633f":"## AutoNLP","eb2bc2cb":"![hugginface logo](https:\/\/huggingface.co\/front\/assets\/huggingface_logo.svg)","891d014b":"Still a **WIP**. Try to finish before end of August.","1c46e815":"Now that we have a better understanding of the dataset, we \ncan explore it content, particulary the textual column.","3a050521":"Next, we will explore a new tool offered by HuggingFace: autonlp.\n\n\nThis works via a CLI and can be used to automatically to train an NLP model on a given task and dataset.\n\nLet's see how it works on the dataset.","6f53205f":"## TPUs with HuggingFace?","646721d8":"These are less known but are valuable if you want to quickly prototype and\/or you want to deploy a model in produciton.\n\nIn what follows, some screenshots: ","d491a877":"First, you need to get an invitation and then an API key. Notice that the product is still in beta as of end of May 2021. \n\nThen, here are the main steps (from [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek) himeself in this [discussion](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/237795)):\n    \n    \n- Step-1: Login: `autonlp login --api-key YOUR_HF_API_KEY`\n\n- Step-2: Create a project. You can choose any name: `autonlp create_project --name readability --language en --task single_column_regression --max_models 50`\n\n- Step-3: Upload training data: `autonlp upload --project readability --split train --col_mapping excerpt:text,target:target --files ~\/datasets\/read\/train.csv`\n\n- Step-4: Upload validation data: `autonlp upload --project readability --split valid --col_mapping excerpt:text,target:target --files ~\/datasets\/read\/valid.csv`\n\n- Step-5: Train models: `autonlp train --project readability`","03edc3ad":"Let's start with basic exploration, i.e. loading the data, checking the \ncolumns, their content, and so on.\nFor that, we will mainly use the beloved Pandas.","82483c75":"## Gensim data exploration","7f77f058":"=> This is a hard example.","8863c555":"- A very good analysis of what huggingface is: https:\/\/marksaroufim.substack.com\/p\/huggingface\n\n\n- Another great EDA and a baseline by Andrada Olteanu: https:\/\/www.kaggle.com\/andradaolteanu\/i-commonlit-explore-xgbrf-repeatedfold-model\n- AutoClasses example by Abhishek Thakur: https:\/\/www.kaggle.com\/abhishek\/yum-yum-yum\n- Explanation of how the target is computed: https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240886#1318829\n- AutoNLP discussion: https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/237795","29bf41e1":"Let's see some code: \n\n\n","05f4cd0f":"# Datasets","68a2f448":"If you have been living inside a cave recently, huggingface is **THE** NLP company.\n\nThey are offering many NLP services and open-source code.\n\nMost of you might know it for their most popular library: [transformers](https:\/\/github.com\/huggingface\/transformers).\n\nThe library has started around ?\n\nLet's start with exploring further this library.","63c1e574":"# Additional resources","d0960806":"First, let's check the training data.","e3bfb713":"Another one of the popular hugginface libraries is the [datasets](https:\/\/github.com\/huggingface\/datasets) one. ","70df7409":"Let's load the training data first.","6f59cadd":"Let's use some of gensim to explore the data.","ea5fb5e7":"## NLP specific exploraiton","998d498f":"## Basic exploration","2af9889a":"# Explore the training data","9aa3a234":"<img src=\"https:\/\/huggingface.co\/course\/static\/chapter1\/transformers_chrono.png\">","2806b2d4":"A new fresh course just came from the HuggingFace team (June 2021).","22bc43e3":"This is one of the best NLP deep learning library available. It has been started when the \ntransformers revolution started but now includes models for almost any NLP task and recent paper.","6678721d":"Let's apply these steps.","1b51e782":"For now, the first 4 chapters are available and the remaining 8 will release up to the end of 2021.\nIf you have some time and want to learn more about modern NLP and transformers particularly, give it a try.","dcd2dcb8":"It is possible to use TPUs with HuggingFace of course. Let's see how it could be done...","c1db0695":"# What is HuggingFace?","e15f3368":"## Transformers","2d81c561":"## TPUs with JAX?","8d1b8c80":"# Online services","e7396bcb":"## NLP COURSE"}}