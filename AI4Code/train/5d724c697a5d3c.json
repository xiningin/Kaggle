{"cell_type":{"00a05661":"code","f6fa17dd":"code","70c11e95":"code","b4aaeda3":"code","41393b76":"code","0efbabc5":"code","44f31d46":"code","5c471be2":"code","c90960c6":"code","3009505c":"code","bb09b553":"markdown"},"source":{"00a05661":"import torch\ndevice = 'gpu' if torch.cuda.is_available() else 'cpu'\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler, PolynomialFeatures, LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nimport warnings\nwarnings.filterwarnings('ignore')\nimport optuna\nimport hyperopt\nimport tqdm\nimport gc\nimport os\nroot_path = '\/kaggle\/input\/tabular-playground-series-may-2021'","f6fa17dd":"train = pd.read_csv(os.path.join(root_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(root_path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(root_path, 'sample_submission.csv'))\n\n#label mapping\nunique_targets = train['target'].unique().tolist()\nlabel_mapping = dict(zip(unique_targets, [int(i[-1]) - 1 for i in unique_targets]))\n\nlabel_mapping\n\n#preprocessing\n\ntrain['target'] = train['target'].map(label_mapping)\ndataset = pd.concat([train, test], axis = 0, ignore_index = True)\ntrain_len = len(train)\n\nfeatures = dataset.drop(['id', 'target'], axis=1).columns.tolist()\ncategorical_feature_columns = (dataset[features].apply(lambda x: x.nunique(), axis = 0)\n                               .rename('n_unique').to_frame()\n                               .query('n_unique < 10').index.tolist())\n\nlabel = LabelEncoder()\n\nfor column in categorical_feature_columns:\n    label.fit(dataset[column])\n    dataset[column] = label.transform(dataset[column])\n        \ncategorical_features = list(range(len(categorical_feature_columns)))\n\ntrain_preprocessed = dataset[:train_len]\ntest_preprocessed = dataset[train_len:]\n\nassert train_preprocessed.shape[1] == test_preprocessed.shape[1]\n\ndel train, test\ngc.collect()\ncat_indices = [features.index(i) for i in categorical_feature_columns]","70c11e95":"!pip install ngboost==0.3.10","b4aaeda3":"from ngboost import NGBClassifier\nfrom ngboost.distns import k_categorical\n\nOPTUNA_OPTIMIZATION = True\nN_SPLITS = 5 #Number of folds for validation\nN_TRIALS = 3 #Number of trials to find best hyperparameters\nTIME = 3600*2 #Time to run optimization (alternative to N_TRIALS)\nFOLD_RANDOM_SEED = 42\nREPEAT = True \n\nFIXED_PARAMS = {\"random_state\": 42,\n                \"Dist\": k_categorical(4), \n                \"verbose\": True,\n                \"verbose_eval\": 100,\n                \"n_estimators\": 500}\n\nEARLY_STOP = 50","41393b76":"from sklearn.tree import DecisionTreeRegressor\n\ndtr_friedman_3 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3)\ndtr_friedman_5 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\ndtr_mse_3 = DecisionTreeRegressor(criterion='mse', max_depth=3)\ndtr_mse_5 = DecisionTreeRegressor(criterion='mse', max_depth=5)","0efbabc5":"skfold = StratifiedKFold(N_SPLITS, shuffle = True, random_state = FOLD_RANDOM_SEED)\nif REPEAT:\n    skfold = RepeatedStratifiedKFold(N_SPLITS, n_repeats=2, random_state=FOLD_RANDOM_SEED)\n\ndef objective(trial, cv=skfold):\n    \n    param_to_search_ngb = {\n        \"Base\": trial.suggest_categorical(\"Base\", [dtr_friedman_3,dtr_friedman_5,\n                                                   dtr_mse_3,dtr_mse_5]),\n        \"natural_gradient\": trial.suggest_categorical(\"natural_gradient\", [True, False]),\n        \"col_sample\": trial.suggest_float('col_sample', 1E-16, 1.0),\n        \"minibatch_frac\": trial.suggest_float('minibatch_frac', 1E-16, 1.0),\n        \"learning_rate\": trial.suggest_categorical('learning_rate', [0.001, 0.005, 0.01, 0.05, 0.1]),\n    }\n    \n    param_ngb = param_to_search_ngb.copy()\n    param_ngb.update(FIXED_PARAMS)\n    \n    val_losses = []\n    losses = []\n    \n    for kfold, (train_idx, val_idx) in tqdm.tqdm(enumerate(cv.split(train_preprocessed[features].values, \n                                                                    train_preprocessed['target'].values))):\n        \n        X_train = train_preprocessed.loc[train_idx, features].values\n        y_train = train_preprocessed.loc[train_idx, 'target'].astype(int)\n        \n        X_valid = train_preprocessed.loc[val_idx, features].values\n        y_valid = train_preprocessed.loc[val_idx, 'target'].astype(int)\n        \n        model = NGBClassifier(**param_ngb)  \n        model.fit(X_train, y_train, X_val = X_valid, Y_val = y_valid,\n                  early_stopping_rounds = EARLY_STOP)\n        scores = model.predict_proba(X_valid)\n        loss = log_loss(y_valid, scores)\n        losses.append(loss)\n    \n    return np.average(losses)","44f31d46":"if OPTUNA_OPTIMIZATION:\n    study = optuna.create_study(study_name = 'ngb_parameter_opt', direction=\"minimize\")\n    \n    #study.optimize(objective, n_trials=1, show_progress_bar = True)\n    study.optimize(objective, timeout=TIME, show_progress_bar = True) \n    \n    trial = study.best_trial\n    \n    print(\"  Value: {}\".format(trial.value))\n    \n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    best_params = FIXED_PARAMS.copy()\n    best_params.update(trial.params)\n","5c471be2":"if OPTUNA_OPTIMIZATION:\n    final_model = NGBClassifier(**best_params)","c90960c6":"test_preds = []\nloglosses = []\nfor kfold, (train_idx, val_idx) in enumerate(skfold.split(train_preprocessed[features].values, \n                                                       train_preprocessed['target'].values)):\n        \n        final_model = NGBClassifier(**best_params)\n        \n        X_train = train_preprocessed.loc[train_idx, features].values\n        y_train = train_preprocessed.loc[train_idx, 'target'].values.astype(int)\n        X_valid = train_preprocessed.loc[val_idx, features].values\n        y_valid = train_preprocessed.loc[val_idx, 'target'].astype(int)\n        \n        final_model.fit(X_train, y_train, X_val = X_valid, Y_val = y_valid, \n                        early_stopping_rounds=EARLY_STOP)\n        \n        probs = final_model.predict_proba(X_valid)\n        \n        logloss = log_loss(y_valid, probs)\n        loglosses.append(logloss)\n        print('Fold: {}\\t Validation logloss: {}\\n'.format(kfold, logloss))\n        \n        test_preds.append(final_model.predict_proba(test_preprocessed[features].values))\n        \nprint(\"Best Parameters mean logloss: {}\".format(np.mean(loglosses)))","3009505c":"test_predictions = np.mean(test_preds, axis = 0)\nassert len(test_predictions) == len(test_preprocessed)\npredictions_df = pd.DataFrame(test_predictions, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sample_submission['id']\npredictions_df.to_csv(\"submission.csv\", index = False)","bb09b553":"## Tabular Playground Series May 2021\n\n<img src=\"https:\/\/i.imgur.com\/uHVJtv0.png\">\n<img src=\"https:\/\/opengraph.githubassets.com\/275715e91fe6af32b0c8907606985ff18606c16c36c6d74935f5e9a10b4608c1\/stanfordmlgroup\/ngboost\">\n\n<br><br>\n\n### Notebook Contents:\n\nGiven the probabilistic nature of the output I've decided to try [NGBoost](https:\/\/stanfordmlgroup.github.io\/projects\/ngboost\/) a wonderful Gradient Boosting library developed by the great [Stanford ML group](https:\/\/stanfordmlgroup.github.io\/). \n\nA first try gave results comparable to those I obtained using LightGBM (+ 5hrs of Optuna hyperparameters search). \n\n**Under construction**\n\n##### Props\n\nProps to [corochann](https:\/\/www.kaggle.com\/corochann\/optuna-tutorial-for-hyperparameter-optimization), I believe this notebook is the best you can find about Optuna.\n\n<h5> Versioning <\/h5>\n\nV3 was the submitted Run with 1.08820 Public Leaderboard score."}}