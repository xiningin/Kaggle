{"cell_type":{"53e7828b":"code","82c5c493":"code","b956e527":"code","dc129727":"code","d2e5a210":"code","857f071a":"code","dc0c1e48":"code","4618eb58":"code","fcaed321":"code","38af3e66":"code","5b3ef47a":"code","f2243bd6":"code","d57e165a":"code","e17f0c91":"code","ad20371c":"code","173bb2ce":"code","1d6fc0e3":"code","03f47da7":"code","a6d8d6bf":"code","eb8f7797":"code","268c09c4":"code","00b55203":"code","ed462f2a":"code","c3a40f21":"code","9c62349c":"code","31f64dde":"code","4eca3bc4":"code","b2d49cfb":"code","f1d34b63":"code","51d3b7cf":"code","3b56b95f":"code","bebd92b2":"code","cc144d00":"code","a07b1b08":"code","8928d696":"code","49e836e4":"code","884c16b9":"code","e2c6eba9":"code","30c6a905":"markdown","253debc5":"markdown","1fda151a":"markdown","50d67608":"markdown","bccb9589":"markdown","b731cfde":"markdown","364889f9":"markdown","55348eb8":"markdown","a4dca09b":"markdown","f4648e3f":"markdown","f314f95f":"markdown","1781611f":"markdown","28dd5349":"markdown","53a64201":"markdown"},"source":{"53e7828b":"import numpy as np\nimport pandas as pd","82c5c493":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')","b956e527":"train.head()","dc129727":"pd.DataFrame(train).nunique(axis=0)","d2e5a210":"train.shape[1]","857f071a":"test_x = test.copy()","dc0c1e48":"from sklearn.preprocessing import LabelEncoder\n\n# we will drop some data \/ columns that seem insignificant\n# drop the PassengerID variables\n# Drop the Name, Ticket & Cabin variables\ntrain = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ntest_x = test_x.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n\n# apply label encoding to categorical variables\nfor c in ['Sex', 'Embarked']:\n    # fit the labels using the training data\n    le = LabelEncoder()\n    le.fit(train[c].fillna('NA'))\n    \n    # return the encoded labels for the training and test data\n    train[c] = le.transform(train[c].fillna('NA'))\n    test_x[c] = le.transform(test_x[c].fillna('NA'))","4618eb58":"for i in range(train.shape[1]):\n    num = len(np.unique(train.iloc[:, i]))\n    percent = float(num) \/ train.shape[0] * 100\n    print('%d, %d, %.1f%%' % (i, num, percent))","fcaed321":"dups = train.duplicated()\nprint(dups.any())\nprint(train[dups])","38af3e66":"#print shape before deleting dups\nprint(train.shape)\ntrain.drop_duplicates(inplace=True)\n#print shape after deleting dups\nprint(train.shape)","5b3ef47a":"# calculate summary statistics\ndata_mean, data_std = np.mean(train['Age']), np.std(train['Age'])\n\n# define outliers\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off\n\n# identify outliers\noutliers = [x for x in train['Age'] if x < lower or x > upper]\nprint('Number of outliers: %d' % len(outliers))","f2243bd6":"# calculate summary statistics\ndata_mean, data_std = np.mean(train['Fare']), np.std(train['Fare'])\n\n# define outliers\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off\n\n# identify outliers\noutliers = [x for x in train['Fare'] if x < lower or x > upper]\nprint('Number of outliers: %d' % len(outliers))","d57e165a":"outliers[:10]","e17f0c91":"train.sort_values(\"Fare\", ascending = False).head(10)","ad20371c":"train.sort_values(\"Fare\", ascending = True).head(10)","173bb2ce":"# familyMembers\nfamilyMembers = pd.DataFrame(train['Parch'] + train['SibSp'])\ntrain = pd.DataFrame(pd.concat([train, familyMembers], axis=1))\ntrain.rename(columns={0: 'familyMembers'}, inplace=True)","1d6fc0e3":"train.head()","03f47da7":"# familyMembers\nfamilyMembers = pd.DataFrame(test_x['Parch'] + test['SibSp'])\ntest_x = pd.DataFrame(pd.concat([test_x, familyMembers], axis=1))\ntest_x.rename(columns={0: 'familyMembers'}, inplace=True)","a6d8d6bf":"test.head()","eb8f7797":"# farePerFamily\nfarePerFamily = pd.DataFrame(train['Fare']\/(train['familyMembers']+1))\ntrain = pd.DataFrame(pd.concat([train, farePerFamily], axis=1))\ntrain.rename(columns={0: 'farePerFamily'}, inplace=True)","268c09c4":"train.head()","00b55203":"# farePerFamily\nfarePerFamily = pd.DataFrame(test_x['Fare']\/(test_x['familyMembers']+1))\ntest_x = pd.DataFrame(pd.concat([test_x, farePerFamily], axis=1))\ntest_x.rename(columns={0: 'farePerFamily'}, inplace=True)","ed462f2a":"test_x.head()","c3a40f21":"train_x = train.drop(['Survived'], axis=1)\ntrain_y = train['Survived']","9c62349c":"train_x.head()","31f64dde":"test_x.head()","4eca3bc4":"from xgboost import XGBClassifier\n\n# create a model and fit it to training data\nmodel = XGBClassifier(n_estimators=20, random_state=71)\nmodel.fit(train_x, train_y)\n\n# output predicted probabilities for the test data\npred = model.predict_proba(test_x)[:,1]\n\n# convert into binary predications\npred_label = np.where(pred >0.5, 1, 0)\n\n# create a submission file\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\nsubmission.to_csv('submission_first.csv', index=False)","b2d49cfb":"from sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import KFold\n\n#create lists to store the scores for each fold\nscores_accuracy = []\nscores_logloss = []\n\n# setup cross validation\n# split the training data into 4\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\nfor tr_idx, va_idx in kf.split(train_x):\n    # split the training data into training and validation\n    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n    \n    # train the model\n    model = XGBClassifier(n_estimators=20, random_state=71)\n    model.fit(tr_x, tr_y)\n    \n    # output predictions probabilities for the validation data\n    va_pred = model.predict_proba(va_x)[:,1]\n    \n    # calculate scores for the validation data\n    logloss = log_loss(va_y, va_pred)\n    accuracy = accuracy_score(va_y, va_pred > 0.5)\n    \n    # store the scores for this fold\n    scores_logloss.append(logloss)\n    scores_accuracy.append(accuracy)\n    \n# calculate the mean scores using all folds\nlogloss = np.mean(scores_logloss)\naccuracy = np.mean(scores_accuracy)\nprint(f'logloss: {logloss:.4f}, accuracy: {accuracy:.4f}')","f1d34b63":"import itertools\n\n# setup parameters\nparam_space = {\n    'max_depth': [3, 5, 7],\n    'min_child_weight': [1.0, 2.0, 4.0]\n    \n}\n\n# try various hyperparameter combinations\nparam_combinations = itertools.product(param_space['max_depth'], \n                                       param_space['min_child_weight'])\n\n# create lists to store scores for the hyperparameter combinations\nparams = []\nscores = []\n\n# perform cross validation for each hyperparameter combination\n\nfor max_depth, min_child_weight in param_combinations:\n    \n    score_folds = []\n    # setup cross validation\n    # split the training data into 4\n    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n    for tr_idx, va_idx in kf.split(train_x):\n        # split the training data into training and validation\n        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n        # train the model\n        model = XGBClassifier(n_estimators=20, random_state=71,\n                             max_depth=max_depth, min_child_weight=min_child_weight)\n        model.fit(tr_x, tr_y)\n\n        # output predictions probabilities for the validation data\n        va_pred = model.predict_proba(va_x)[:,1]\n        \n        # calculate scores for the validation data\n        logloss = log_loss(va_y, va_pred)\n        # store the scores for this fold\n        score_folds.append(logloss)\n    \n    # calculate the mean scores using all folds\n    score_mean = np.mean(score_folds)\n\n    # store the scores for the hyperparameter combinations\n    params.append((max_depth, min_child_weight))\n    scores.append(score_mean)\n    \n# set the parameters to the best values and highest score\nbest_idx = np.argsort(scores)[0] # look at what this does\nbest_param = params[best_idx]\nprint(f'max_depth: {best_param[0]}, min_child_weight: {best_param[1]}')","51d3b7cf":"train_x.head()","3b56b95f":"test_x.head()","bebd92b2":"from sklearn.preprocessing import OneHotEncoder\n\n# Copy the above feature engineering\ntrain_x2 = train_x\ntest_x2 = test_x\n\n# setup one-hot encoding\ncat_cols = ['Sex', 'Embarked', 'Pclass']\nohe = OneHotEncoder(categories='auto', sparse=False)\nohe.fit(train_x2[cat_cols].fillna('NA'))\n\n# create column names for dummy one-hot encoding variables\nohe_columns = []\nfor i, c in enumerate(cat_cols):\n    ohe_columns +=[f'{c}_{v}' for v in ohe.categories_[i]]","cc144d00":"ohe_columns","a07b1b08":"# Create DataFrames for one-hot encoding\nohe_train_x2 = pd.DataFrame(ohe.transform(train_x2[cat_cols].fillna('NA')), columns=ohe_columns)\nohe_test_x2 = pd.DataFrame(ohe.transform(test_x2[cat_cols].fillna('NA')), columns=ohe_columns)","8928d696":"ohe_train_x2.head()","49e836e4":"# drop original columns before one-hot encoding\ntrain_x2 = train_x2.drop(cat_cols, axis=1)\ntest_x2 = test_x2.drop(cat_cols, axis=1)\n\n# append the one-hot encoded columns\ntrain_x2 = pd.concat([train_x2, ohe_train_x2], axis=1)\ntest_x2 = pd.concat([test_x2, ohe_test_x2], axis=1)\n\n# replace missing values in columns with mean of values\nnum_cols = ['Age', 'SibSp', 'Parch', 'Fare']\nfor col in num_cols:\n    train_x2[col].fillna(train_x2[col].mean(), inplace=True)\n    test_x2[col].fillna(train_x2[col].mean(), inplace=True)\n    \n# make a logarithmic transformation of the fare variables\n# When taking the logarithm, it is normal to avoid negative divergence of the value when the true value is 0.\n# Takes the logarithm after adding 1 as shown in the above equation. You can use numpy's log1p function\n\ntrain_x2['Fare'] = np.log1p(train_x2['Fare'])\ntest_x2['Fare'] = np.log1p(test_x2['Fare'])","884c16b9":"from sklearn.linear_model import LogisticRegression\n\n# xgboost model\nmodel_xgb = XGBClassifier(n_estimators=20, random_state=71)\nmodel_xgb.fit(train_x, train_y)\npred_xgb = model_xgb.predict_proba(test_x)[:, 1]\n\n# logistic regression model\nmodel_lr = LogisticRegression(solver='lbfgs', max_iter=300)\nmodel_lr.fit(train_x2, train_y)\npred_lr = model_lr.predict_proba(test_x2)[:,1]\n\n# take the weighted average of the predictions\npred_ens = pred_xgb * 0.8 + pred_lr * 0.2\npred_label_ens = np.where(pred > 0.5, 1, 0)                                 ","e2c6eba9":"# Create a submission file\nsubmission_ens = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label_ens})\nsubmission_ens.to_csv('submission_ens.csv', index=False)","30c6a905":"If you submit the above you will get a 0.52224 score without any feature engineering\n\nHowever, with the feature engineering above my score is: .78404","253debc5":"Create our Model. In this case we will do xgboost","1fda151a":"WOW. that's interesting that there 2217 outliers in the fare. And how high priced they are. It doesn't seem like we'd cut these. However it would be interesting to see how many survived. \n\nPerhaps we need to split the fares. ","50d67608":"Find duplicates","bccb9589":"Tune the Model","b731cfde":"Ensemble XGB and Logistic Regression","364889f9":"Delete dups","55348eb8":"Cross Validation","a4dca09b":"The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)","f4648e3f":"Setup to Ensemble\n\nLet's run logistic regression now so that we can ensemble","f314f95f":"Therefore, we will also output an index called logloss. Log loss is out of prediction probability\nThe higher the penalty, the better the index. Learn more about logloss\nFor example, \"2.3.4 Evaluation index in binary classification-when the probability of being a positive example is used as the predicted value\"\n","1781611f":"We will choose the features and target data","28dd5349":"What percentage are unique values within their rows?","53a64201":"BEST: max_depth: 5, min_child_weight: 1.0"}}