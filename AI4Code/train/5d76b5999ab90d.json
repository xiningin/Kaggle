{"cell_type":{"f95c53bb":"code","d134b301":"code","e46c2f44":"code","7ddbd9be":"code","2a9e60d7":"code","73498d89":"code","e6d17f34":"code","3c0ebdd4":"code","f2f2a2f3":"code","e6f04a3a":"code","8f979acb":"code","8796344f":"markdown","4ba79090":"markdown","60624a91":"markdown","798766a1":"markdown","bc0e40cf":"markdown","f8079aa1":"markdown","d9e13f17":"markdown","6eb56fbd":"markdown","b67fbe7c":"markdown","54b77f0c":"markdown","7fc1242b":"markdown","47e48a7c":"markdown"},"source":{"f95c53bb":"# pytorch libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# for visualizing the results\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# for reading input data\nimport pandas as pd\n\n# for parsing the FEN of chess positions\nimport re\n","d134b301":"def fen_to_bit_vector(fen):\n    # piece placement - lowercase for black pieces, uppercase for white pieces. numbers represent consequtive spaces. \/ represents a new row \n    # active color - whose turn it is, either 'w' or 'b'\n    # castling rights - which castling moves are still legal K or k for kingside and Q or q for queenside, '-' if no legal castling moves for either player\n    # en passant - if the last move was a pawn moving up two squares, this is the space behind the square for the purposes of en passant\n    # halfmove clock - number of moves without a pawn move or piece capture, after 50 of which the game is a draw\n    # fullmove number - number of full turns starting at 1, increments after black's move\n\n    # Example FEN of starting position\n    # rnbqkbnr\/pppppppp\/8\/8\/8\/8\/PPPPPPPP\/RNBQKBNR w KQkq - 0 1\n    \n    parts = re.split(\" \", fen)\n    piece_placement = re.split(\"\/\", parts[0])\n    active_color = parts[1]\n    castling_rights = parts[2]\n    en_passant = parts[3]\n    halfmove_clock = int(parts[4])\n    fullmove_clock = int(parts[5])\n\n    bit_vector = np.zeros((13, 8, 8), dtype=np.uint8)\n    \n    # piece to layer structure taken from reference [1]\n    piece_to_layer = {\n        'R': 1,\n        'N': 2,\n        'B': 3,\n        'Q': 4,\n        'K': 5,\n        'P': 6,\n        'p': 7,\n        'k': 8,\n        'q': 9,\n        'b': 10,\n        'n': 11,\n        'r': 12\n    }\n    \n    castling = {\n        'K': (7,7),\n        'Q': (7,0),\n        'k': (0,7),\n        'q': (0,0),\n    }\n\n    for r, row in enumerate(piece_placement):\n        c = 0\n        for piece in row:\n            if piece in piece_to_layer:\n                bit_vector[piece_to_layer[piece], r, c] = 1\n                c += 1\n            else:\n                c += int(piece)\n    \n    if en_passant != '-':\n        bit_vector[0, ord(en_passant[0]) - ord('a'), int(en_passant[1]) - 1] = 1\n    \n    if castling_rights != '-':\n        for char in castling_rights:\n            bit_vector[0, castling[char][0], castling[char][1]] = 1\n    \n    if active_color == 'w':\n        bit_vector[0, 7, 4] = 1\n    else:\n        bit_vector[0, 0, 4] = 1\n\n    if halfmove_clock > 0:\n        c = 7\n        while halfmove_clock > 0:\n            bit_vector[0, 3, c] = halfmove_clock%2\n            halfmove_clock = halfmove_clock \/\/ 2\n            c -= 1\n            if c < 0:\n                break\n\n    if fullmove_clock > 0:\n        c = 7\n        while fullmove_clock > 0:\n            bit_vector[0, 4, c] = fullmove_clock%2\n            fullmove_clock = fullmove_clock \/\/ 2\n            c -= 1\n            if c < 0:\n                break\n\n    return bit_vector\n\n","e46c2f44":"fen = \"rnbqkbnr\/pppppppp\/8\/8\/8\/8\/PPPPPPPP\/RNBQKBNR w KQkq - 0 1\"\nboard = fen_to_bit_vector(fen)\nprint(board)\n","7ddbd9be":"class Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(832, 832)\n        self.fc2 = nn.Linear(832, 416)\n        self.fc3 = nn.Linear(416, 208)\n        self.fc4 = nn.Linear(208, 104)\n        self.fc5 = nn.Linear(104, 1)\n\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = self.fc5(x)\n        return x\n\n","2a9e60d7":"# ChessDataset code and eval_to_int code taken from reference [1]\nclass ChessDataset(Dataset):\n    def __init__(self, data_frame):\n        self.fens = torch.from_numpy(np.array([*map(fen_to_bit_vector, data_frame[\"FEN\"])], dtype=np.float32))\n        self.evals = torch.Tensor([[x] for x in data_frame[\"Evaluation\"]])\n        self._len = len(self.evals)\n        \n    def __len__(self):\n        return self._len\n    \n    def __getitem__(self, index):\n        return self.fens[index], self.evals[index]\n\n\ndef eval_to_int(evaluation):\n    try:\n        res = int(evaluation)\n    except ValueError:\n        res = 10000 if evaluation[1] == '+' else -10000\n    return res \/ 100\n\n","73498d89":"def AdamW_main():\n    MAX_DATA = 100000\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device {}\".format(device))\n\n    print(\"Preparing Training Data...\")\n    train_data = pd.read_csv(\"\/kaggle\/input\/chess-evaluations\/chessData.csv\")\n    train_data = train_data[:MAX_DATA]\n    train_data[\"Evaluation\"] = train_data[\"Evaluation\"].map(eval_to_int)\n    trainset = ChessDataset(train_data)\n    \n    print(\"Preparing Test Data...\")\n    test_data = pd.read_csv(\"\/kaggle\/input\/chess-evaluations\/tactic_evals.csv\")\n    test_data = test_data[:MAX_DATA]\n    test_data[\"Evaluation\"] = test_data[\"Evaluation\"].map(eval_to_int)\n    testset = ChessDataset(test_data)\n\n    batch_size = 10\n\n    print(\"Converting to pytorch Dataset...\")\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n    net = Net().to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(net.parameters())\n\n\n    for epoch in range(10):  # loop over the dataset multiple times\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            if i % 2000 == 1999:    # print every 2000 mini-batches\n                # denominator for loss should represent the number of positions evaluated \n                # independent of the batch size\n                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss \/ (2000*len(labels))))\n                running_loss = 0.0\n\n    print('Finished Training')\n\n    PATH = '.\/chess.pth'\n    torch.save(net.state_dict(), PATH)\n\n    print('Evaluating model')\n\n    count = 0\n    total_loss = 0\n    # since we're not training, we don't need to calculate the gradients for our outputs\n    with torch.no_grad():\n        for data in testloader:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n            \n            # count should represent the number of positions evaluated \n            # independent of the batch size\n            count += len(labels)\n            total_loss += loss\n            if count % 10000 == 0:\n                print('Average error of the model on the {} tactics positions is {}'.format(count, loss\/count))\n    #print('Average error of the model on the {} tactics positions is {}'.format(count, loss\/count))\n","e6d17f34":"AdamW_main()","3c0ebdd4":"def SGD_main():\n    MAX_DATA = 100000\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device {}\".format(device))\n\n    print(\"Preparing Training Data...\")\n    train_data = pd.read_csv(\"\/kaggle\/input\/chess-evaluations\/chessData.csv\")\n    train_data = train_data[:MAX_DATA]\n    train_data[\"Evaluation\"] = train_data[\"Evaluation\"].map(eval_to_int)\n    trainset = ChessDataset(train_data)\n    \n    print(\"Preparing Test Data...\")\n    test_data = pd.read_csv(\"\/kaggle\/input\/chess-evaluations\/tactic_evals.csv\")\n    test_data = test_data[:MAX_DATA]\n    test_data[\"Evaluation\"] = test_data[\"Evaluation\"].map(eval_to_int)\n    testset = ChessDataset(test_data)\n\n    batch_size = 10\n\n    print(\"Converting to pytorch Dataset...\")\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n    net = Net().to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.SGD(net.parameters(), lr = 0.01)\n\n\n    for epoch in range(10):  # loop over the dataset multiple times\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            if i % 2000 == 1999:    # print every 2000 mini-batches\n                # denominator for loss should represent the number of positions evaluated \n                # independent of the batch size\n                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss \/ (2000*len(labels))))\n                running_loss = 0.0\n\n    print('Finished Training')\n\n    PATH = '.\/chess.pth'\n    torch.save(net.state_dict(), PATH)\n\n    print('Evaluating model')\n\n    count = 0\n    total_loss = 0\n    # since we're not training, we don't need to calculate the gradients for our outputs\n    with torch.no_grad():\n        for data in testloader:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n            \n            # count should represent the number of positions evaluated \n            # independent of the batch size\n            count += len(labels)\n            total_loss += loss\n            if count % 10000 == 0:\n                print('Average error of the model on the {} tactics positions is {}'.format(count, loss\/count))\n    #print('Average error of the model on the {} tactics positions is {}'.format(count, loss\/count))\n","f2f2a2f3":"SGD_main()","e6f04a3a":"def SGD_lr_main(learning_rate):\n    MAX_DATA = 100000\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device {}\".format(device))\n\n    print(\"Preparing Training Data...\")\n    train_data = pd.read_csv(\"\/kaggle\/input\/chess-evaluations\/chessData.csv\")\n    train_data = train_data[:MAX_DATA]\n    train_data[\"Evaluation\"] = train_data[\"Evaluation\"].map(eval_to_int)\n    trainset = ChessDataset(train_data)\n    \n    print(\"Preparing Test Data...\")\n    test_data = pd.read_csv(\"\/kaggle\/input\/chess-evaluations\/tactic_evals.csv\")\n    test_data = test_data[:MAX_DATA]\n    test_data[\"Evaluation\"] = test_data[\"Evaluation\"].map(eval_to_int)\n    testset = ChessDataset(test_data)\n\n    batch_size = 10\n\n    print(\"Converting to pytorch Dataset...\")\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n    net = Net().to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.SGD(net.parameters(), lr = learning_rate)\n\n\n    for epoch in range(10):  # loop over the dataset multiple times\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            if i % 2000 == 1999:    # print every 2000 mini-batches\n                # denominator for loss should represent the number of positions evaluated \n                # independent of the batch size\n                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss \/ (2000*len(labels))))\n                running_loss = 0.0\n\n    print('Finished Training')\n\n    PATH = '.\/chess.pth'\n    torch.save(net.state_dict(), PATH)\n\n    print('Evaluating model')\n\n    count = 0\n    total_loss = 0\n    # since we're not training, we don't need to calculate the gradients for our outputs\n    with torch.no_grad():\n        for data in testloader:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n            \n            # count should represent the number of positions evaluated \n            # independent of the batch size\n            count += len(labels)\n            total_loss += loss\n            if count % 10000 == 0:\n                print('Average error of the model on the {} tactics positions is {}'.format(count, loss\/count))\n    #print('Average error of the model on the {} tactics positions is {}'.format(count, loss\/count))\n","8f979acb":"SGD_lr_main(0.001)","8796344f":"# Contribution\n\n* Representation of castling rights, en passant, active color, halfmoves and fullmoves in an 8x8 grid\n* Neural Network Architecture\n* Testing AdamW vs SVG","4ba79090":"An average error of 0.0207 is a slight improvement, but not so much that we can definitively say that SGD did better than AdamW. Since the training loss did not decrease by much throughout the 10 epochs, it's much more likely that either SGD is a worse optimizer for this classification problem or the learning rate is too large which is preventing the model from converging to the local optimum. Let's try again but with a learning rate of 0.001","60624a91":"To represent a chess position, it is common to use [Forsyth\u2013Edwards Notation (FEN)](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Forsyth%E2%80%93Edwards_Notation) which contains all the necessary information to reconstruct a chess game from the current position. To make this information usable for a neural network, we will use a bit (actually a byte) to represent if a specific piece (white rook, white knight, etc...) is on a specific square on the 8x8 chess board. Since there are 6 different pieces and two different players, that means there are 12 specific pieces that could potentially be on each square. \n\nHowever, we still need to keep track of information like whose turn it is, which castling options are still legal, if en passant is possible, how many half moves since a pawn move or piece capture, and how many turns the game has had. To do this we use an additional 8x8 board where the rook locations represent castling rights, the 3rd and 6th rank (row) keep track of possible en passant moves, the e1 and e8 sqaure represent whose on move, and the 4th and 5th rank represent the number of half moves and full moves as binary numbers (max possible being 255) respectively.\n\nBelow is a function to do this conversion.","798766a1":"# Challenges\n\nThis dataset (or at least the chessData.csv dataset) has over 16 million positions and evaluations which makes it rather difficult to work with. Because of this I've had to limit the amount of data actually used to 200,000 positions, 100,000 in the training dataset and 100,000 in the test dataset. ","bc0e40cf":"# Chess Position Evaluator\n\nChess has been a classic benchmark for Artifical Intelligence, Machine Learning, and Data Mining since their inception. Due to the simple deterministic rules of chess coupled with the exponential possibilities and difficult to evaluate positions, it has gained the focus of many researchers in Computer Science. Today, there are several sophisticated chess AIs that compete on a level above even the strongest chess Grand Masters in the world. While chess engines like stockfish (which this dataset is based off of) and AlphaZero are too complex to try to compete with here, lets see if we can use the evaluations of Stockfish to build a comperable position evaluation function.\n\nFor this project I will be using the pytorch library and building a Neural Network with it.","f8079aa1":"# Results\n\nAn average error of 0.0225 seems very reasonable, however this is not as amazing as it might at first seem since positions where stockfish's evaluation is not forced mate have been normalized to a range from approximately -1.5 to 1.5 and positions where stockfish's evalutaion is forced mate have been set to -100 or 100. By convention a positive score indicates an advantage for white and a negative score indicates an advantage for black.","d9e13f17":"The first 8x8 board (0th index) contains all the extra information and the following 12 boards (1 to 12) represent the locations of the pieces in the order \n\n1. White Rook\n2. White Knight\n3. White Bishop\n4. White Queen\n5. White King\n6. White Pawn\n7. Black Pawn\n8. Black King\n9. Black Queen\n10. Black Bishop\n11. Black Knight\n12. Black Rook\n\nNotice how the pieces line up correctly with the starting position with the first board correctly indicating it is white to move.","6eb56fbd":"The training error seems like it's decreasing even if sporadic, but the classification error got slightly worse. It seems like AdamW is simply more consistant and converges faster to a local optimum.","b67fbe7c":"# Testing Optimization Algorithms\n\nOriginally we used the AdamW algorithm for the optimization step, but what if we try Stochastic Gradient Descent (SGD)?","54b77f0c":"# Neural Network\n\nWe'll begin with a simple Feed-Forward Neural Network that's fully connected. Neural Networks are named for their structure being analogous to neurons in the human brain. The idea is that, in the human brain, when a neuron gets an electrical impulse through its synapses it will sometimes fire an electrical impulse to other neurons, creating a chain reaction. For neural networks, our neurons are nodes our synapses are edges (with corresponding weights) and the firing of the neuron is the activation function and output of the node. \n\n![Perceptron](http:\/\/starship-knowledge.com\/wp-content\/uploads\/2020\/10\/Perceptrons-1024x724.jpeg)\n\nThe goal of the Neural Network is to have weights such that after all the chain reactions of nodes taking in inputs and producing outputs, the information output of the final node represents the evaluation of the chess position that began the process. In order to actually find such weights we will use a method known as backpropagation, which iteratively adjusts the weights in the network to nudge the output closer to the answer we desire.\n\nTechnically speaking, for each training record (a FEN and an evaulation) we input the position into the Neural Network, and after we get a result we compute the error between the result and the correct evaluation which can be represented as a error function. To change the weights in such a way as to minimize this error function we compute the gradient of the error function and adjust the weights in the opposite direction. This means that if we overshoot we want to decrease our evaluation and if we undershoot we want to increase our evaluation.\n\n![Gradient Descent](https:\/\/sebastianraschka.com\/images\/blog\/2015\/singlelayer_neural_networks_files\/perceptron_gradient_descent_1.png)\n","7fc1242b":"# Conclusion\n\nAdamW seems to be a more consistant algorithm and one which converges more quickly to a local optimum based on the comparision of AdamW, SGD(lr=0.01), and SGD(lr=0.001)\n\nThis means the best model had an average error of 0.0225 as opposed to the lowest average error of 0.0207 which is not significant enough of a difference to justify the inconsistancy of the SGD optimizer.","47e48a7c":"# References\n\n1. https:\/\/www.kaggle.com\/ronakbadhe\/chess-evaluation-prediction\n2. https:\/\/en.wikipedia.org\/wiki\/Forsyth%E2%80%93Edwards_Notation\n3. http:\/\/starship-knowledge.com\/wp-content\/uploads\/2020\/10\/Perceptrons-1024x724.jpeg\n4. https:\/\/starship-knowledge.com\/wp-content\/uploads\/2020\/10\/Perceptrons-1024x724.jpeg"}}