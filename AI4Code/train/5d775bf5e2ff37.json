{"cell_type":{"fe486528":"code","c651bd0e":"code","2da60242":"code","731fed4b":"code","e6bf72bc":"code","46260990":"code","24d24086":"code","44839cb4":"code","2c64935d":"code","7c8486b0":"code","c0a491be":"code","87a3f738":"markdown","1f73c954":"markdown","6176e9fa":"markdown","fe43f2f9":"markdown","39dd86c5":"markdown","09c6c0da":"markdown","01e3f726":"markdown","48791c1a":"markdown","0c0a8089":"markdown","7ec38154":"markdown","06ad4090":"markdown","47b9bb71":"markdown","43396552":"markdown","324e4257":"markdown","9a809702":"markdown"},"source":{"fe486528":"import os\nimport time\nimport pickle\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n# from sklearn.metrics import log_loss, roc_auc_score this is way to track down if the model is overfitting","c651bd0e":"#DATA_PATH = '..\/input\/jane-street-market-prediction\/' no need to import the data because we're going to only load pytorch and tensorflow models.\nCACHE_PATH = '..\/input\/mlp012003weights'\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n    # with gzip.open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n    # with gzip.open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\nf_mean = np.load(f'{CACHE_PATH}\/f_mean_online.npy')","2da60242":"# list of the features\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\n# list of all the features\nall_feat_cols = [col for col in feat_cols]\n\n# add two more features to the feature list\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n# resp 1,2,3,4\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']","731fed4b":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F","e6bf72bc":"##### Model&Data fnc\nclass Model(nn.Module):\n    \n    def __init__(self):\n        \n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.9) # 0.2\n\n        dropout_rate = 0.9 # 0.2\n        hidden_size = 256\n        \n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        \n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x","46260990":"# Since we're going to onyl load the model, then use it for inference, then it's better to use cpu\n# Otherwise, you want to retrain the model, then enable the GPU for faster calculations\n\nif torch.cuda.is_available():\n    print('using device: cuda')\n    torch.device(\"cuda:0\")\nelse:\n    print('using device: cpu')\n    device = torch.device('cpu')","24d24086":"NFOLDS = 5\n\nmodel_list = []\ntmp = np.zeros(len(feat_cols))\nfor _fold in range(NFOLDS):\n    torch.cuda.empty_cache()\n    model = Model()\n    model.to(device)\n    model_weights = f\"{CACHE_PATH}\/online_model{_fold}.pth\"\n    #model.load_state_dict(torch.load(model_weights))\n    model.load_state_dict(torch.load(model_weights, map_location=device))\n    model.eval()\n    model_list.append(model)","44839cb4":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa","2c64935d":"SEED = 1111\n\nnp.random.seed(SEED)\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 200\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.8, 0.6, 0.4, 0.2] # all 0.2\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(len(feat_cols), 5, hidden_units, dropout_rates, label_smoothing, learning_rate)","7c8486b0":"# Fit the model and save it with \n#clf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n#clf.save(f'model.h5')\n\n# Load the Fitted model\n# !ls ..\/input\/jane-street-with-keras-nn-overfit\/\nclf.load_weights('..\/input\/jane-street-with-keras-nn-overfit\/model.h5')\n\n# If you have several models, the you can store into a list\n#tf_models = [clf]","c0a491be":"th = 0.503 # 0.5 0.501 0.0502 0.053 did not have an effect\nimport janestreet\njanestreet.competition.make_env.__called__ = False\n\nenv = janestreet.make_env()\nenv_iter = env.iter_test()\n\n#test_df has one single row of data with all the feautures\n# pred_dfd has 1 or 0 which is an action that comes from test_df\n\nfor (test_df, pred_df) in tqdm(env_iter):\n\n    # Data Processing\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, feat_cols].values\n        \n        # if there is only one value eqauls nan, then x_tt.sum will return nan, then np.isnan will be true, and enters the loop\n        if np.isnan(x_tt.sum()):\n            # Replace NaN with zero and infinity with large finite numbers (default behaviour)\n            x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean # the score goew down if we take the if statement\n\n    \n        cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n        cross_1_2 = x_tt[:, 1] \/ (x_tt[:, 2] + 1e-5)\n        feature_inp = np.concatenate((x_tt, np.array(cross_41_42_43).reshape(x_tt.shape[0], 1), np.array(cross_1_2).reshape(x_tt.shape[0], 1),), axis=1)\n\n        \n        # PytTorch prediction \n        torch_pred = np.zeros((1, len(target_cols)))\n        for model in model_list:\n            torch_pred += model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() \/ NFOLDS\n        torch_pred = np.median(torch_pred)\n\n        \n        # TensofFlow prediction\n        # I foyu have several TF models then use\n        #tf_pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in tf_models],axis=0))\n        # If you have only one model then use\n        tf_pred = np.median(clf(x_tt))\n\n        \n        # PyTorch and TensorFlow Average prediction\n        pred = torch_pred * 0.44 + tf_pred * 0.56\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n        \n    else:\n        pred_df.action = 0\n        \n    env.predict(pred_df)","87a3f738":"# TensorFlow\n### Libs","1f73c954":"### TensorFlow Load model weights","6176e9fa":"# Best Score\n\nVersion 9 score = 11170.252","fe43f2f9":"### Feature Engineering","39dd86c5":"# My trials so far\n\n###TF(Tensorflow model) PT(Pytorch) MLP\n\nv1  : no change. <br\/>\nv2  : th =  0.502 to 0.503 ... (trials in anothe rnotebook)\nv3  : with if for x_tt with th=0.5 (inference section) => no score change. <br\/>\nv4  : changed the number of epochs which got me the same initial score.<br\/>\nv5  : epoch 300, same initial score. <br\/>\nv6  : I forgot what I have changed XD. <br\/>\nv7  : changed PT dropout rate, same initial score. <br\/>\nv8  : changed PT 0.6 TF 0.4 deacrease comparing to initial score. <br\/>\n**v9  : changed PT 0.4 TF 0.6 best score (1170.252).** <br\/>\nv10 : 0.5 for both PT and TF but changed the droptout rates for TF.<br\/><br\/>\n\nv16 : PT droptout 0.9 0.5 -_-\nv17 : PT droptout 0.9 0.6 -_-\nv18 : PT droptout 0.9 0.7 -_-\nv19 : PT droptout 0.9 0.8 -_-\nv20 : PT droptout 0.9 0.9 -_-\nv21 : PT 0.41 TF 0.51 inference pred pending \n\n\nFeel free to comment regarding any issues =) <br\/><br\/>\nother trials are pending...","09c6c0da":"# The importance of the DropOut Rate\n\nThe Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.<br\/> \nInputs not set to 0 are scaled up by 1\/(1 - rate) such that the sum over all inputs is unchanged.<br\/>\nhttps:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/\n\nDropout is a regularization technique. You should use it only to reduce variance (validation performance vs training performance).<br\/>\nIt is not intended to reduce the bias, and you should not use it in this way.<br\/>\nDropout works by probabilistically removing, or \u201cdropping out,\u201d inputs to a layer, which may be input variables in the data sample or activations from a previous layer.\nWith dropout (dropout rate less than some small value), the accuracy will gradually increase and loss will gradually decrease.<br\/>\nWhen you increase dropout beyond a certain threshold, it results in the model not being able to fit properly.<br\/>\nhttps:\/\/stackoverflow.com\/questions\/59044351\/can-dropout-increases-training-data-performance\n\nDropout prevents overfitting due to a layer's \"over-reliance\" on a few of its inputs. Because these inputs aren't always present during training (i.e. they are dropped at random), the layer learns to use all of its inputs, improving generalization.<br\/>\nhttps:\/\/stats.stackexchange.com\/questions\/374742\/does-dropout-regularization-prevent-overfitting-due-to-too-many-iterations\n\nThe default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, where 1.0 means no dropout, and 0.0 means no outputs from the layer.<br\/>\nA good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8.\nhttps:\/\/machinelearningmastery.com\/dropout-for-regularizing-deep-neural-networks\/","01e3f726":"### PyTorch Load Model weights","48791c1a":"### TensorFlow model\n\nTo understand the following code, refer to my detailed notebook: <br\/>\n[Explanation of the model](https:\/\/www.kaggle.com\/mouafekmk\/simple-mlp)\n","0c0a8089":"### PyTorch on CPU or GPU ","7ec38154":"\n\n# Inference","06ad4090":"# PyTorch\nI have used this course before, to be able to understand bacis about about Pytorch:<br\/>\nhttps:\/\/www.udacity.com\/course\/deep-learning-pytorch--ud188\n\n### PyTorch Libs","47b9bb71":"# Libs & Config","43396552":"### PyTorch Model\n\nI have used this course before, to be able to understand bacis about about Pytorch:<br\/>\nhttps:\/\/www.udacity.com\/course\/deep-learning-pytorch--ud188","324e4257":"# Data Save\/Load","9a809702":"### Kudos (massive respect for sharing, even when it's not perfect)\nhttps:\/\/www.kaggle.com\/a763337092\/blending-tensorflow-and-pytorch\n\n### Pytorch source code:\nhttps:\/\/www.kaggle.com\/a763337092\/neural-network-starter-pytorch-version<br\/>\nhttps:\/\/www.kaggle.com\/a763337092\/pytorch-resnet-starter-training\n\n### Tensorflow source code:\nhttps:\/\/www.kaggle.com\/code1110\/jane-street-with-keras-nn-overfit <br\/>\n\nP.S.<br\/>\nBe aware that the model maybe **overfitting**.<br\/>\nAs you can see, this notebook it based on the effort of many other competitors, and I was so curious about their result, so I decided to experiment with their work myself ans share it with you in this humble notebook of mine. <br\/><br\/>\nUpvote if it brings some added value =)"}}