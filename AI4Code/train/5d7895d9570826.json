{"cell_type":{"2ebd5d6a":"code","be00197f":"code","887403f0":"code","0aa4aa07":"code","5f89bb97":"code","5fb18784":"code","511a57d5":"code","b2c75c15":"code","3b55ed9b":"code","98258f63":"code","24a84a8d":"code","8fd5b587":"code","cd96e011":"code","a61e5b61":"code","5191c338":"code","8d48a4c3":"code","e712a52e":"code","940eee38":"code","3a258ed6":"code","b688f704":"code","7213435f":"code","3fa68f07":"code","0c9a2848":"code","ee71eb51":"code","215e99fa":"code","f457d03e":"code","8971c9fe":"code","c2653fca":"code","13606cf4":"code","3c9a6523":"code","ece164f1":"code","3749fa73":"code","c7c396c6":"code","935a1fed":"markdown","dc92e6c4":"markdown","7c13c919":"markdown","0018047b":"markdown","91afac97":"markdown","cdad2dba":"markdown","e1b0a1f7":"markdown"},"source":{"2ebd5d6a":"import pandas as pd\nimport numpy as np\nimport os, cv2\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\nfrom sklearn.metrics.pairwise import pairwise_distances\n\nfrom matplotlib import pyplot as plt\ntorch.cuda.empty_cache()","be00197f":"def draw_image(image_array, bbox):\n    image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n    if bbox is not None:\n        cv2.rectangle(image_array,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,255,0),2)\n    plt.imshow(image_array)","887403f0":"IMAGE_SIZE = 112\nGET_VAL_SCORE = False\nIMAGE_DIR = \"..\/input\/deepfashion\/shorts\/shorts\"\nTEST_DIR = \"..\/input\/deepfashion\/woman_shorts\"\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nON_CPU = DEVICE == torch.device('cpu')\nTRAIN_BATCH_SIZE = 8 if ON_CPU else 16\nVALID_BATCH_SIZE = 4 if ON_CPU else 8\nNUM_EPOCHS = 2 if ON_CPU else 4\n","0aa4aa07":"dfannos = pd.read_csv(\"..\/input\/deepfashion\/image_annos.csv\")\ndfannos = dfannos[dfannos['category_name'] == 'shorts'].iloc[:,[3,6]]\ndfannos.drop_duplicates(inplace=True)\ndfannos['image_id'] = dfannos['image'].apply(lambda x:x[6:])\nboundaries = dfannos['bounding_box'].str.split(',',expand=True)\nboundaries[0] = boundaries[0].str.slice(start=1)\nboundaries[3] = boundaries[3].str.slice(stop=-1)\ndfannos[['x_min','y_min','x_max','y_max']] = boundaries\ndfannos[['x_min','y_min','x_max','y_max']] = dfannos[['x_min','y_min','x_max','y_max']].astype(int)\n\nimage_list = os.listdir(IMAGE_DIR)\ndfannos = dfannos[dfannos['image_id'].isin(image_list)]\ndfannos.drop(['image','bounding_box'],axis=1,inplace=True)\nprint(\"data size : \",dfannos.shape)\nprint(\"data columns : \",dfannos.columns)\nprint(dfannos.head())","5f89bb97":"dftest = pd.DataFrame(os.listdir(TEST_DIR), columns = ['image_id'])\ndfvalid = dfannos.sample(int(dfannos.shape[0]*0.2))\ndftrain = dfannos[~dfannos['image_id'].isin(dfvalid['image_id'])]\nprint(dfvalid.shape,dftrain.shape, dftest.shape)","5fb18784":"class FashionDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None, crop_image = False):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.crop_image = crop_image\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image_height = image.shape[0]\n        image_width = image.shape[1]\n\n        if 'x_min' in records.columns:\n            boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n            boxes[:,0] = boxes[:,0] \/ image_width * IMAGE_SIZE\n            boxes[:,1] = boxes[:,1] \/ image_height * IMAGE_SIZE\n            boxes[:,2] = boxes[:,2] \/ image_width * IMAGE_SIZE\n            boxes[:,3] = boxes[:,3] \/ image_height * IMAGE_SIZE\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            area = torch.as_tensor(area, dtype=torch.float32)\n        else:\n            boxes = torch.zeros((records.shape[0],4), dtype=torch.float32)\n            area = torch.zeros((records.shape[0],), dtype=torch.float32)\n        \n        if self.crop_image:\n            image = image[records['y_min'].min():records['y_max'].max(), records['x_min'].min():records['x_max'].max()]\n        image = cv2.resize(image,dsize=(IMAGE_SIZE,IMAGE_SIZE))\n\n        image \/= 255.0\n        image_info = [image_id, image_height, image_width]\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['area'] = area\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n        return image, image_info, target\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","511a57d5":"# Albumentations\ndef get_train_transform():\n    return A.Compose([\n        A.GaussNoise(var_limit=(1e-3,1e-8),p=0.5),\n        A.Rotate(limit=(-10,10),p=0.5),\n        A.HorizontalFlip(p=0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","b2c75c15":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n","3b55ed9b":"def get_model(backbone_model = None):\n    if backbone_model == None:\n        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    else:\n        backbone = resnet_fpn_backbone(backbone_model, pretrained=True)\n        model = FasterRCNN(backbone, num_classes=2)\n    return model","98258f63":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = FashionDataset(dftrain, IMAGE_DIR, get_train_transform())\nvalid_dataset = FashionDataset(dfvalid, IMAGE_DIR, get_valid_transform())\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=TRAIN_BATCH_SIZE,\n    shuffle=False,\n    num_workers=1,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=1,\n    collate_fn=collate_fn\n)\n","24a84a8d":"images, image_ids, targets = next(iter(train_data_loader))\nimages = list(image.to(DEVICE) for image in images)\ntargets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\nimage_len = TRAIN_BATCH_SIZE if TRAIN_BATCH_SIZE < 8 else 8\n\nfig,ax = plt.subplots(1,image_len,figsize=(15,40))\nfor i, (image,target) in enumerate(zip(images,targets)):\n    sample_image = image.permute(1,2,0).cpu().numpy()\n    for bboxes in target['boxes']:\n        bbox = bboxes.cpu().numpy().astype(np.int32)\n        cv2.rectangle(sample_image,(bbox[0], bbox[1]),(bbox[2], bbox[3]),(220, 0, 0), 1)\n    ax[i].imshow(sample_image)\n    ax[i].set_axis_off()\n    if i >= image_len - 1:\n        break\nplt.show()","8fd5b587":"model = get_model()\nmodel.to(DEVICE)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","cd96e011":"train_loss_hist = Averager()\ntotal_train_loss = []\nval_loss_hist = Averager()\ntotal_val_loss = []\nmodel.train()\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss_hist.reset()\n    \n    for batch_index, (images, image_info, targets) in enumerate(train_data_loader):\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        train_loss_hist.send(losses.item())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if batch_index % 50 == 0:\n            print(f\"Iteration #{batch_index} loss: {losses.item()}\")\n    \n    if GET_VAL_SCORE:    \n        for _, (images, image_info, targets) in enumerate(valid_data_loader):\n\n            images = list(image.to(DEVICE) for image in images)\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n            val_loss_dict = model(images, targets)\n            val_loss = sum(loss for loss in val_loss_dict.values())\n            val_loss_hist.send(val_loss.item())   \n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    \n    total_train_loss.append(train_loss_hist.value)\n    total_val_loss.append(val_loss_hist.value)\n    print(f\"Epoch #{epoch} loss: {train_loss_hist.value}\")   ","a61e5b61":"plt.plot(total_train_loss, label = \"Training Loss\", color = \"deepskyblue\")\nplt.plot(total_val_loss, label = \"Validation Loss\", color = \"darkorange\")\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.show()","5191c338":"torch.save(model.state_dict(), 'fasterRCNN.pth')","8d48a4c3":"test_dataset = FashionDataset(dftest, TEST_DIR, get_valid_transform())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=1,\n    collate_fn=collate_fn\n)","e712a52e":"model = get_model()\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nmodel.load_state_dict(torch.load('..\/input\/fashion-fasterrcnn\/fasterRCNN_2.pth',map_location=DEVICE))\nmodel.to(DEVICE)\nmodel.eval()","940eee38":"images, image_info, _ = next(iter(test_data_loader))\nprint(image_info)\nimages = list(img.to(DEVICE) for img in images)\ntargets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\nmodel.eval()\n\noutputs = model(images)\noutputs = [{k: v.to(DEVICE) for k, v in t.items()} for t in outputs]\n\nfig, ax = plt.subplots(1, 4, figsize=(16, 8))\nfor i, (image,target) in enumerate(zip(images,outputs)):\n    sample_image = image.permute(1,2,0).cpu().numpy()\n    for bboxes,score in zip(target['boxes'], target['scores']):\n        if score > 0.9:\n            bbox = bboxes.cpu().detach().numpy().astype(np.int32)\n            cv2.rectangle(sample_image,(bbox[0], bbox[1]),(bbox[2], bbox[3]),(220, 0, 0), 1)\n    ax[i].imshow(sample_image)\nplt.show()","3a258ed6":"image_list = []\nfor images, image_info,_ in test_data_loader:\n\n    images = list(img.to(DEVICE) for img in images)\n    outputs = model(images)\n    outputs = [{k: v.to(DEVICE) for k, v in t.items()} for t in outputs]\n    \n    for i, (image_info,output) in enumerate(zip(image_info,outputs)):\n        for bboxes,score in zip(output['boxes'], output['scores']):\n            if score > 0.8:\n                bbox = bboxes.cpu().detach().numpy().astype(np.int32)\n                score = np.round(score.cpu().detach().numpy().astype(np.float),2)\n                image_list.append(np.hstack([image_info, bbox, score]))","b688f704":"dfCroppedImage = pd.DataFrame(np.array(image_list).reshape(-1,8),columns = ['image_id','height','width','x_min','y_min','x_max','y_max','score'])\nfor col in ['height','width','x_min','y_min','x_max','y_max']:\n    dfCroppedImage[col] = dfCroppedImage[col].astype(int)\n\ndfCroppedImage['x_min'] = ((dfCroppedImage['x_min'] * dfCroppedImage['width']) \/ IMAGE_SIZE).astype(int)\ndfCroppedImage['x_max'] = ((dfCroppedImage['x_max'] * dfCroppedImage['width']) \/ IMAGE_SIZE).astype(int)\ndfCroppedImage['y_min'] = ((dfCroppedImage['y_min'] * dfCroppedImage['height']) \/ IMAGE_SIZE).astype(int)\ndfCroppedImage['y_max'] = ((dfCroppedImage['y_max'] * dfCroppedImage['height']) \/ IMAGE_SIZE).astype(int)","7213435f":"dfCroppedImage.to_pickle(\".\/bboxes.pkl\")","3fa68f07":"dftest = pd.read_pickle(\"..\/input\/fashion-fasterrcnn\/bboxesv2.pkl\")\ndftest.head()","0c9a2848":"dfsample = dftest.sample(5)\nfig,ax = plt.subplots(1,5,figsize=(16,20))\nfor i, (idx, row) in enumerate(dfsample.iterrows()):\n    img = plt.imread(TEST_DIR + '\/' + row['image_id'])\n    cv2.rectangle(img, (row['x_min'], row['y_min']), (row['x_max'], row['y_max']), (0,255,0),3)\n    ax[i].imshow(img)\n    ax[i].set_axis_off()\nplt.show()","ee71eb51":"test_dataset = FashionDataset(dftest, TEST_DIR, get_valid_transform(),crop_image = True)\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=0\n)","215e99fa":"sim_model = torch.hub.load('pytorch\/vision:v0.6.0', 'resnet50', pretrained=True)\nsim_model.fc = torch.nn.Identity()\nsim_model.eval()","f457d03e":"images, image_info,_ = next(iter(test_data_loader))\nimages = list(image.to(DEVICE) for image in images)\nfor i, image in enumerate(images):\n    sample_image = image.permute(1,2,0).cpu().numpy()\n    plt.imshow(sample_image)\nplt.show()","8971c9fe":"dfResult = pd.DataFrame()\nimage_list = []\nfor i, (images, image_info,_) in enumerate(test_data_loader):\n    outputs = sim_model(images)\n    dfa = pd.DataFrame(outputs).astype(\"float\")\n    dfa['image'] = list(image_info[0])\n    dfResult = dfResult.append(dfa)\ndfResult.index = dfResult['image']\ndfResult.drop('image',axis=1,inplace=True)","c2653fca":"cosine_similarity = 1 - pairwise_distances(dfResult,metric='cosine')","13606cf4":"def get_recommender(idx, df, top_n = 5):\n    sim_idx    = indices[idx]\n    sim_scores = list(enumerate(cosine_similarity[sim_idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:top_n+1]\n    idx_rec    = [i[0] for i in sim_scores]\n    idx_sim    = [i[1] for i in sim_scores]\n    \n    return indices.iloc[idx_rec].index, idx_sim","3c9a6523":"indices = pd.Series(range(len(dfResult)), index=dfResult.index)\nindices","ece164f1":"sim_index = '604028.jpg'\nsim_indices,sim = get_recommender(sim_index, dfResult, top_n = 5)\nsim_indices","3749fa73":"img = plt.imread(TEST_DIR + '\/' + sim_index)\nplt.imshow(img)\nplt.show()","c7c396c6":"similar_list = [TEST_DIR + '\/' + x for x in sim_indices]\nfor similar in similar_list:\n    img = plt.imread(similar)\n    plt.imshow(img)\n    plt.show()","935a1fed":"Training is finished. Save model parameters, download and save it to dataset.","dc92e6c4":"# Create DataSet\n- It creates dataset on the fly from dataframe. Dataframe includes image id and bounding box coordinates if exists.\n- We cant get all images to memory, so we get images via batch_size. Higher batch_size would give memory error.\n- After getting image from directory, it transforms image and convert to tensors.\n- After trials best image size is found as 112. (112, 224, 297 and 448 are tested) \n- If its train dataset, find bounding box data with respect to new image_size.\n- If its test dataset, no need to return anythind except resized images and image ids\n- If its similarity dataset, crop images from bounding boxes.\n- Dataset returns image as tensor, image_info and target values.\n    - image is image value as tensor\n    - image_info includes image_id, width and height respectively\n    - target values includes the bounding box, area and image specific data used for Faster RCNN algorithm","7c13c919":"Faster RCNN model is used with original Resnet50\nOther models can be used as backbone","0018047b":"bounding boxes of all images are found. Save bounding boxes of all images.","91afac97":"# Similarity","cdad2dba":"# Test","e1b0a1f7":"# Train"}}