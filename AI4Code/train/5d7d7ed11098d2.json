{"cell_type":{"fa2fe92a":"code","1a43504b":"code","d86b2b91":"code","c62a1dd3":"code","af42b0c5":"code","23be64c2":"code","eb359019":"code","3b29be58":"code","ad1feac6":"code","a2234c0a":"code","f11f4521":"code","b12a3acc":"markdown","c21519cb":"markdown","6d237050":"markdown","40013c7b":"markdown","2cb64f0a":"markdown","5eb86f3d":"markdown","cd68beb6":"markdown"},"source":{"fa2fe92a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('..\/input\/Churn_Modelling.csv')\n\n#include relevant columns within x and y\nx = dataset.iloc[:, 3:13]\ny = dataset.iloc[:, 13]\ndataset.head()","1a43504b":"x.head()","d86b2b91":"#deal with categorical data --> encode them\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_x = LabelEncoder()\nx.iloc[:, 1] = labelencoder_x.fit_transform(x.iloc[:, 1]) #applying on Geography\nx.head()","c62a1dd3":"#apply encoder on Gender as well\nlabelencoder_x_2 = LabelEncoder()\nx.iloc[:, 2] = labelencoder_x_2.fit_transform(x.iloc[:, 2]) #applying on Gender\nx.head()","af42b0c5":"#One hot encoding. \n\nfrom keras.utils import to_categorical\nencoded = pd.DataFrame(to_categorical(x.iloc[:, 1]))\n#no need to encode Gender, as there are only two categories\n\nx = pd.concat([encoded, x], axis = 1)\nx.head()","23be64c2":"#Dropping the existing \"geography\" category, and one of the onehotcoded columns.\n\nx = x.drop(['Geography', 0], axis = 1)\nx.head()","eb359019":"#train and test set split, and feature scaling\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3b29be58":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense #to add layers\n\n#there is no rule on how many nodes each hidden layer should have\nclassifier = Sequential()\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n#init --> initialize weights according to uniform distribution\n#input_dim is required for the first hidden layer, as it is the first starting point. --> number of nodes.\n#output_dim --> number of nodes of the hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n#input_dim --> remove it as it already knows what to expect.\n\n#the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n#output_dim should be 1, as output is binary outcome, and activation should be 'sigmoid'\n#If dependent variables have more than two categories, use activation = 'softmax'\n\n#compile the model --> backpropagation -> gradient descent\nclassifier.compile(optimizer = 'adam', loss = \"binary_crossentropy\", metrics = ['accuracy'])\n#optimizer = algorithm to find the optimal set of weights in ANN\n#loss = functions that should be optimized. if more than two categories, use \"categorical_crossentropy\"\n#metrics = criterion used to calculate the performance of the model.","ad1feac6":"classifier.fit(X_train, Y_train, batch_size = 10, nb_epoch = 20)\n#batch_size = the number of observations after which you want to update the weights\n#           batch size and epochs should be tuned through experiments.\n#epoch = going through the whole dataset","a2234c0a":"#predicting the results\n\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5) #to classify each probability into True or False\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\nprint (cm, '\\n\\n', y_pred[:5, :])","f11f4521":"#accuracy\nprint ((1548 + 139)\/2000)","b12a3acc":"Created by: Sangwook Cheon\n\nDate: Dec 23, 2018\n\nThis is step-by-step guide to Artificial Neural Networks (ANN), which I created for reference. I added some useful notes along the way to clarify things. This notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of ANN.\n\n## Content:\n### IMPLEMENTATION\n### 1. Data preprocessing\n### 2. Build the Keras model\n### 3. Compile and fit the model\n### 4. Make predictions and determine accuracy","c21519cb":"### 1. Data processing\nData processing is crucial for ANNs to work properly. All steps are required, including feature scaling.","6d237050":"### 2. Build the model using Keras\nANN with many hidden layers is one of the branches of Deep Learning. ","40013c7b":"The fact that accuracy on train and test set are similar shows that the model did not overfit on the train set. Hyperparameters can be tuned to obtain better results.","2cb64f0a":"Now let's run the model.","5eb86f3d":"# Some notes on ANNs\n\n## The Neuron\nAxon: Transmitters of signals\nDentrites: Receivers of signals\n\nThe ANNs imitate the behavior of human brain. Each neuron receives certain inputs from the previous neurons, and process that information to send signals to others.","cd68beb6":"# The Activation Function\n\nOptions:\n* Threshold Function\n* Sigmoid Function\n* Rectified Linear Unit (ReLU)\n* Tanh\n\nFor binary classification, Threhold Function or Sigmoid Function should be used.\nIt is common to apply ReLU to hidden layers, and Sigmoid to the final layer to produce results.\n\n# Dataset overview (used in this kernel)\nA bank is trying to see whether or not customers will be leaving the bank, based on various information about each customer. These features include Credit Score, Gender, Balance, etc. (Please see the view of the dataset below). We will apply ANN to find meaningful correlations between these independent variables, and determine if a customer will leave or stay in the bank."}}