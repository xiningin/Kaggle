{"cell_type":{"a51e4582":"code","125800d3":"code","31c0b4e2":"code","14b9d997":"code","0ee0cb83":"code","8d32bc95":"code","5752c763":"code","38125a13":"code","879a97ac":"code","978990a0":"code","3c8a510c":"code","f538f316":"code","826f454f":"code","d9ca4e80":"code","1938d665":"code","d3adc29a":"markdown"},"source":{"a51e4582":"!pip install --no-deps '..\/input\/timm-package\/timm-0.1.26-py3-none-any.whl' > \/dev\/null\n!pip install --no-deps '..\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > \/dev\/null","125800d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport sys\nsys.path.insert(0, \"..\/input\/weightedboxesfusion\")\nsys.path.insert(0, \"..\/input\/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"..\/input\/omegaconf\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom torchvision import transforms\nfrom matplotlib import pyplot as plt\nfrom torchvision import transforms\nimport torchvision \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torch.utils.data import Dataset,DataLoader\nimport glob\nimport albumentations as A\nimport cv2\nfrom albumentations.pytorch.transforms import ToTensorV2\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nimport ensemble_boxes\nfrom ensemble_boxes import *\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31c0b4e2":"import effdet\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\n","14b9d997":"config = get_efficientdet_config('tf_efficientdet_d5')\nmodel = EfficientDet(config, pretrained_backbone=False)\n\nconfig.num_classes = 1\nconfig.image_size=512\nmodel.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\ncheckpoint = torch.load('..\/input\/efficientd5newtry\/d5-efficient-25e.bin')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel = DetBenchEval(model, config)\nmodel.to(device)\nmodel.eval();\n","0ee0cb83":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","8d32bc95":"DATA_ROOT_PATH = \"..\/input\/global-wheat-detection\/test\"\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","5752c763":"dataset = DatasetRetriever(np.array([path.split('\/')[-1][:-4] for path in glob.glob(f'{DATA_ROOT_PATH}\/*.jpg')]),get_valid_transforms())\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=1,\n    drop_last=False,\n    collate_fn=collate_fn\n)","38125a13":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","879a97ac":"class BaseWheatTTA:\n    \"\"\" author: @shonenkov \"\"\"\n    image_size = 512\n\n    def augment(self, image):\n        raise NotImplementedError\n    \n    def batch_augment(self, images):\n        raise NotImplementedError\n    \n    def deaugment_boxes(self, boxes):\n        raise NotImplementedError\n\nclass TTAHorizontalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n\n    def augment(self, image):\n        return image.flip(1)\n    \n    def batch_augment(self, images):\n        return images.flip(2)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n        return boxes\n\nclass TTAVerticalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return image.flip(2)\n    \n    def batch_augment(self, images):\n        return images.flip(3)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n        return boxes\n    \nclass TTARotate90(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 1, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 1, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = self.image_size - boxes[:, [1,3]]\n        res_boxes[:, [1,3]] = boxes[:, [2,0]]\n        return res_boxes\n    \nclass TTARotate180(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 2, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 2, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,1,2,3]] = self.image_size - boxes[:, [2,3,0,1]]\n        return boxes\n    \nclass TTARotate270(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 3, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 3, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = boxes[:, [1,3]]\n        res_boxes[:, [1,3]] = self.image_size - boxes[:, [2,0]]\n        return res_boxes\n    \nclass TTACompose(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def augment(self, image):\n        for transform in self.transforms:\n            image = transform.augment(image)\n        return image\n    \n    def batch_augment(self, images):\n        for transform in self.transforms:\n            images = transform.batch_augment(images)\n        return images\n    \n    def prepare_boxes(self, boxes):\n        result_boxes = boxes.copy()\n        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n        return result_boxes\n    \n    def deaugment_boxes(self, boxes):\n        for transform in self.transforms[::-1]:\n            boxes = transform.deaugment_boxes(boxes)\n        return self.prepare_boxes(boxes)","978990a0":"# from itertools import product\n\n# tta_transforms = []\n# for tta_combination in product([TTAHorizontalFlip(), None], \n#                                [TTAVerticalFlip(), None],\n#                                [TTARotate90(), None]):\n# tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))\n    \n    \nfrom itertools import product\n\ntta_transforms = []\nfor tta_combination in product([TTAHorizontalFlip(), None], \n                               [TTAVerticalFlip(), None],\n                               [TTARotate90(), TTARotate180(), TTARotate270(), None]):\n    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))","3c8a510c":"def make_tta_predictions(images, score_threshold=0.3):\n#   model.eval()\n  model.to(device)\n  with torch.no_grad():\n      images = torch.stack(images).float().cuda()\n      predictions = []\n      for tta_transform in tta_transforms:\n          result = []\n          det = model(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n\n          for i in range(images.shape[0]):\n              boxes = det[i].detach().cpu().numpy()[:,:4]    \n              scores = det[i].detach().cpu().numpy()[:,4]\n              indexes = np.where(scores > score_threshold)[0]\n              boxes = boxes[indexes]\n              boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n              boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n              boxes = tta_transform.deaugment_boxes(boxes.copy())\n              result.append({\n                  'boxes': boxes,\n                  'scores': scores[indexes],\n              })\n          predictions.append(result)\n  return predictions\n\n\ndef run_wbf(predictions, image_index, image_size=512, iou_thr=0.4, skip_box_thr=0.3,score_threshold= 0.3, weights=None):\n    boxes = [(prediction[image_index]['boxes']\/(image_size)).tolist() for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).astype(int).tolist() for prediction in predictions]\n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    indexes = np.where(scores > score_threshold)[0]\n    boxes = boxes[indexes]    \n    scores = scores[indexes]\n    boxes = boxes*(image_size)\n    return boxes, scores, labels","f538f316":"# results = []\n# model.eval()\n# model.to(device)\n# for imgs, image_ids in data_loader:\n#     images = torch.stack(imgs).cuda().float()\n# #     x = torch.tensor([1.0] * 4, dtype=torch.float).to(device)\n# #     y = torch.tensor([images[0].shape[-2:]] * 4, dtype=torch.float).to(device)\n#     predictions = model.forward(images, torch.tensor([1.0] * 1, dtype=torch.float).to(device))\n#     for i, pred in enumerate(predictions):\n#         boxes = pred.cpu().detach().numpy()[:,:4]\n#         boxes[:, 2] = boxes[:, 2] + boxes[:, 0] # xmax\n#         boxes[:, 3] = boxes[:, 3] + boxes[:, 1] # ymin\n#         # x,y,x,y\n#         scores = pred.cpu().detach().numpy()[:,4]\n#         indexes = np.where(scores > 0.4)[0]\n#         boxes = boxes[indexes]\n#         boxes = (boxes*(2)).round().astype(np.int32).clip(min=0, max=1023)\n# #         boxes[:,[1,0,3,2]] = boxes[:,[0,1,2,3]] \n        \n        \n#         image_id = image_ids[i]\n        \n        \n        \n# #         sample = images[i].permute(1,2,0).cpu().numpy().copy()\n# #         sample = cv2.resize(sample,(1024,1024))\n# # #         boxes = boxes.astype(np.int32).clip(min=0, max=1023)\n# # #         boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n# # #         boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n\n        \n\n\n# #         fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n# # #         boxes[:,[1,0,3,2]] = boxes[:,[0,1,2,3]]  \n\n# #         for box in boxes:\n# #             cv2.rectangle(sample,\n# #                           (box[0], box[1]),\n# #                           (box[2], box[3]),\n# #                           (1, 0, 0), 2) #red\n\n\n# #         ax.set_axis_off()\n# #         ax.imshow(sample);\n        \n    \n    \n    \n#         boxes[:, 2] = boxes[:, 2] - boxes[:, 0]# w = xman-xmin\n#         boxes[:, 3] = boxes[:, 3] - boxes[:, 1]# h = yman-ymin\n        \n#         result = {\n#             'image_id': image_id,\n#             'PredictionString': format_prediction_string(boxes, scores)\n#         }\n#         results.append(result)\n#         break;","826f454f":"for j, (images ,image_ids) in enumerate(data_loader):\n    break\n    \n# images = torch.stack(images).cuda().float()\n    \npredictions = make_tta_predictions(images)\n\ni = 0\n\nsample = images[i].permute(1,2,0).cpu().numpy()\n\nboxes, scores, labels = run_wbf(predictions, image_index=i)\nboxes = boxes.round().astype(np.int32).clip(min=0, max=511)\n# boxes[:,[1,0,3,2]] = boxes[:,[0,1,2,3]] \n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor j,box in enumerate(boxes):\n    cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 2)\n    score=str(float(\"{:.3f}\".format(scores[j])))\n\n#     cv2.putText(\n#               sample,\n#               score,\n#               org=(int(box[0]), int(box[1] )), # bottom left\n#               fontFace=cv2.FONT_HERSHEY_PLAIN,\n#               fontScale=1,\n#               color=(255,0, 0),\n#               thickness=2\n#             )\n# out = model.forward(images,torch.tensor([1.0] * 4, dtype=torch.float).to(device))\n    \nax.set_axis_off()\nax.imshow(sample);    \n","d9ca4e80":"results = []\nmodel.eval()\nmodel.to(device)\nfor images, image_ids in data_loader:\n    predictions = make_tta_predictions(images)\n    for i, image in enumerate(images):\n        boxes, scores, labels = run_wbf(predictions, image_index=i)\n        boxes = (boxes*2).round().astype(np.int32).clip(min=0, max=1023)\n        image_id = image_ids[i]\n        \n#         boxes[:, 2] = boxes[:, 2] + boxes[:, 0] # xmax\n#         boxes[:, 3] = boxes[:, 3] + boxes[:, 1] # ymin\n#         boxes[:,[1,0,3,2]] = boxes[:,[0,1,2,3]] \n\n#         fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n#         sample = images[i].permute(1,2,0).cpu().numpy()\n#         sample = cv2.resize(sample, (1024,1024))\n#         for j,box in enumerate(boxes):\n#             cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 3)\n#             score=str(float(\"{:.3f}\".format(scores[j])))\n\n#             cv2.putText(\n#                       sample,\n#                       score,\n#                       org=(int(box[0]), int(box[1] )), # bottom left\n#                       fontFace=cv2.FONT_HERSHEY_PLAIN,\n#                       fontScale=3,\n#                       color=(255,0, 0),\n#                       thickness=2\n#                     )\n#         ax.set_axis_off()\n#         ax.imshow(sample)\n        \n    \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n        results.append(result)","1938d665":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head(10)","d3adc29a":"**Test**"}}