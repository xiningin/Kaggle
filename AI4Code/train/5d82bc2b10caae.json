{"cell_type":{"657e7339":"code","d2f2bc64":"code","4b1ace6c":"code","329d9221":"code","d355a0c9":"code","805b0f16":"code","19cfc060":"code","62044267":"code","ba59bb32":"code","981750fe":"code","b6d6e915":"code","2408cec1":"code","27d209a1":"code","0fb31a07":"code","b2d22782":"code","2f396339":"code","d30aec8d":"code","e87d721a":"code","2724a5ff":"code","a69e5f3b":"code","318259a8":"code","693287f1":"code","c50067a6":"code","f7f7afbb":"code","22c63b72":"code","5ce8cb4c":"code","1f00cdb8":"code","57929412":"code","0d89733b":"markdown","b5308051":"markdown","656716f4":"markdown"},"source":{"657e7339":"import numpy as np\nimport pandas as pd\nimport tensorflow\nimport tensorflow.keras.layers as layers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","d2f2bc64":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')","4b1ace6c":"import math\ndef convert(train):\n    train['cont1_int'] = train['cont1'].astype(int)\n    train['cont2_int'] = train['cont2'].astype(int)\n    train['cont3_int'] = train['cont3'].astype(int)\n    train['cont4_int'] = train['cont4'].astype(int)\n    train['cont5_int'] = train['cont5'].astype(int)\n    train['cont6_int'] = train['cont6'].astype(int)\n    train['cont7_int'] = train['cont7'].astype(int)\n    train['cont8_int'] = train['cont8'].astype(int)\n    train['cont9_int'] = train['cont9'].astype(int)\n    train['cont10_int'] = train['cont10'].astype(int)\n    train['cont11_int'] = train['cont11'].astype(int)\n    train['cont12_int'] = train['cont12'].astype(int)\n    train['cont13_int'] = train['cont13'].astype(int)\n    train['cont14_int'] = train['cont14'].astype(int)\n\n    train['cont1_in'] = train['cont1'].apply(lambda x: math.modf(x)[0])\n    train['cont2_in'] = train['cont2'].apply(lambda x: math.modf(x)[0])\n    train['cont3_in'] = train['cont3'].apply(lambda x: math.modf(x)[0])\n    train['cont4_in'] = train['cont4'].apply(lambda x: math.modf(x)[0])\n    train['cont5_in'] = train['cont5'].apply(lambda x: math.modf(x)[0])\n    train['cont6_in'] = train['cont6'].apply(lambda x: math.modf(x)[0])\n    train['cont7_in'] = train['cont7'].apply(lambda x: math.modf(x)[0])\n    train['cont8_in'] = train['cont8'].apply(lambda x: math.modf(x)[0])\n    train['cont9_in'] = train['cont9'].apply(lambda x: math.modf(x)[0])\n    train['cont10_in'] = train['cont10'].apply(lambda x: math.modf(x)[0])\n    train['cont11_in'] = train['cont11'].apply(lambda x: math.modf(x)[0])\n    train['cont12_in'] = train['cont12'].apply(lambda x: math.modf(x)[0])\n    train['cont13_in'] = train['cont13'].apply(lambda x: math.modf(x)[0])\n    train['cont14_in'] = train['cont14'].apply(lambda x: math.modf(x)[0])\n    return train","329d9221":"train = convert(train)\ntest = convert(test)","d355a0c9":"numerical_cols = [f'cont{i}' for i in range(1, 15)]\ntarget_col = 'target'\n\nfor c in numerical_cols:\n    prep = StandardScaler()\n    train[c] = prep.fit_transform(train[[c]])\n    test[c] = prep.transform(test[[c]])\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop('id', axis=1)","805b0f16":"X_train","19cfc060":"# import seaborn as sns\n# import matplotlib.pyplot as plt\n# plt.figure(figsize = (20,6))\n# sns.countplot(x = 'cont1', hue = 'target', data = train)","62044267":"corr = train.corr(method = 'pearson')\ncorr = corr.abs()\ncorr.style.background_gradient(cmap='inferno')","ba59bb32":"from fastai import *\nfrom fastai.tabular import *","981750fe":"train = train.sort_values(by='target', ascending=False)\ntrain = train.reset_index(drop=True)","b6d6e915":"train","2408cec1":"# train.target = np.log(train.target)","27d209a1":"def RMSE_fn(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.array(y_true, float).reshape(-1, 1) - np.array(y_pred, float).reshape(-1, 1), 2)))","0fb31a07":"# cv = KFold(n_splits=5, shuffle=True, random_state=7)\n\n# y_preds = []\n# models = []\n# oof_train = np.zeros((len(X_train),))\n\n# for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n#     X_tr = X_train.loc[train_index, :]\n#     X_val = X_train.loc[valid_index, :]\n#     y_tr = y_train.loc[train_index]\n#     y_val = y_train.loc[valid_index]\n\n#     model = tensorflow.keras.Sequential([\n#         layers.Dense(64, activation='relu'),\n#         layers.Dense(16, activation='relu'),\n#         layers.Dense(1, activation='linear'),\n#     ])\n\n#     model.compile(\n#         optimizer='adam',\n#         loss='mse',\n#         metrics=[tensorflow.keras.metrics.RootMeanSquaredError()]\n#     )\n\n#     early_stopping = tensorflow.keras.callbacks.EarlyStopping(\n#         patience=10,\n#         min_delta=0.001,\n#         restore_best_weights=True,\n#     )\n\n#     model.fit(\n#         X_tr, y_tr,\n#         validation_data=(X_val, y_val),\n#         batch_size=30000,\n#         epochs=1000,\n#         callbacks=[early_stopping],\n#     )\n\n#     oof_train[valid_index] = model.predict(X_val).reshape(1, -1)[0]\n#     y_pred = model.predict(X_test).reshape(1, -1)[0]\n\n#     y_preds.append(y_pred)\n#     models.append(model)\n","b2d22782":"# print(f'CV: {mean_squared_error(y_train, oof_train, squared=False)}')","2f396339":"# sub = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\n# y_sub = sum(y_preds) \/ len(y_preds)\n# sub['target'] = y_sub\n# sub.to_csv('submission.csv', index=False)\n# sub.head()","d30aec8d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport datetime\n# from kaggle.competitions import nflrush\nimport tqdm\nimport re\nfrom string import punctuation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras.utils import plot_model\nimport keras.backend as K\nimport tensorflow as tf\n\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]\n","e87d721a":"!pip install deeptables","2724a5ff":"import numpy as np\nfrom deeptables.models import deeptable, deepnets\nfrom deeptables.datasets import dsutils\nfrom sklearn.model_selection import train_test_split","a69e5f3b":"# df = dsutils.load_bank()\n# df_train, df_test = train_test_split(train, test_size=0.2, random_state=42)","318259a8":"#training\n# config = deeptable.ModelConfig(nets=deepnets.xDeepFM, earlystopping_patience=15, metrics=[\"RootMeanSquaredError\"])\nconfig = deeptable.ModelConfig(nets =['linear','cin_nets','dnn_nets'],\n    stacking_op = 'add', earlystopping_patience=15, metrics=[\"RootMeanSquaredError\"])\n\ndt = deeptable.DeepTable(config=config)\nmodel, history = dt.fit(X_train, y_train, epochs=25)\n# nets =['linear','cin_nets','dnn_nets'],\n#     stacking_op = 'add',","693287f1":"X_test","c50067a6":"X_test = test.drop('id', axis=1)\n","f7f7afbb":"test","22c63b72":"y_pred = dt.best_model.predict(X_test)","5ce8cb4c":"preds=dt.predict(test.iloc[:,1:])","1f00cdb8":"sub=pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")\nsub.target = preds\nsub.to_csv(\"submission.csv\", index=False)","57929412":"sub","0d89733b":"## Why use DeepTables?\nFree preprocessing and processing.\nEasy to expert data scientist or a business analyst without modeling ability.\nSimpler than the traditional machine learning algorithm which highly depends on manual feature engineering.\nExcellent performance out of the box.\nBuiltin a group of neural network components (NETs) from the most excellent research results in recent years.\nExtremely easy to use.\nOnly 5 lines of code can complete the modeling of any data set.\nVery open architecture design.\nsupports plug-in extension.","b5308051":"DeepTables: Deep-learning Toolkit for Tabular data\nDeepTables(DT) is a easy-to-use toolkit that enables deep learning to unleash great power on tabular data.\n\nOverview\nMLP (also known as Fully-connected neural networks) have been shown inefficient in learning distribution representation. The \u201cadd\u201d operations of the perceptron layer have been proven poor performance to exploring multiplicative feature interactions. In most cases, manual feature engineering is necessary and this work requires extensive domain knowledge and very cumbersome. How learning feature interactions efficiently in neural networks becomes the most important problem.\n\nA lot of models have been proposed to CTR prediction and continue to outperform existing state-of-the-art approaches to the late years. Well-known examples include FM, DeepFM, Wide&Deep, DCN, PNN, etc. These models can also provide good performance on tabular data under reasonable utilization.\n\nDT aims to utilize the latest research findings to provide users with an end-to-end toolkit on tabular data.\n\nDT has been designed with these key goals in mind:\n\nEasy to use, non-experts can also use.\nProvide good performance out of the box.\nFlexible architecture and easy expansion by user.\nDT follow these steps to build a neural network:\n\nCategory features -> Embedding Layer.\nContinuous feature -> Dense Layer or to Embedding Layer after discretization\/categorization.\nEmbedding\/Dense layers -> Feature Interactions\/Extractions nets.\nStacking(add\/concat) outputs of nets as the output of the model.","656716f4":"## Welcome to DeepTables"}}