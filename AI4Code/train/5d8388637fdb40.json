{"cell_type":{"c631e9ed":"code","599e3f43":"code","9892b267":"code","56a85b40":"code","62f4b587":"code","2e4f3dc5":"code","7c71ba6a":"code","66d04783":"code","67910170":"code","8d981df2":"code","e6f5f1ef":"code","499d25c8":"code","b6202075":"code","59627032":"code","89d8230d":"code","358beeb3":"code","3ed6d24a":"markdown"},"source":{"c631e9ed":"!pip install ..\/input\/nfl-lib\/timm-0.1.26-py3-none-any.whl\n!tar xfz ..\/input\/nfl-lib\/pkgs.tgz\n# for pytorch1.6\ncmd = \"sed -i -e 's\/ \\\/ \/ \\\/\\\/ \/' timm-efficientdet-pytorch\/effdet\/bench.py\"\n!$cmd","599e3f43":"import sys\nsys.path.insert(0, \"timm-efficientdet-pytorch\")\nsys.path.insert(0, \"omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nimport gc\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nDATA_ROOT_PATH = 'test_images'\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything(SEED)","9892b267":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}\/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","56a85b40":"out_dir = DATA_ROOT_PATH\nif not os.path.exists(out_dir):\n    !mkdir -p $out_dir\n    video_dir = '\/kaggle\/input\/nfl-impact-detection\/test'\n    uniq_video = [path.split('\/')[-1] for path in glob(f'{video_dir}\/*.mp4')]\n    for video_name in uniq_video:\n        mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)","62f4b587":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","2e4f3dc5":"class DatasetRetriever(Dataset):\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","7c71ba6a":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = 2\n    config.image_size=512 #512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.0001))#0.001,0.01\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\nnet = load_net('..\/input\/nfl-models\/best-checkpoint-018epoch.bin')","66d04783":"dataset = DatasetRetriever(\n    image_ids=np.array([path.split('\/')[-1] for path in glob(f'{DATA_ROOT_PATH}\/*.png')]),\n    transforms=get_valid_transforms()\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=32, #16\n    shuffle=False,\n    num_workers=5, #4\n    drop_last=False,\n    collate_fn=collate_fn\n)","67910170":"def make_predictions(images, score_threshold=0.5):\n    images = torch.stack(images).cuda().float()\n    box_list = []\n    score_list = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]   \n            label = det[i].detach().cpu().numpy()[:,5]\n            # useing only label = 2\n            indexes = np.where((scores > score_threshold) & (label == 2))[0]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])\n    return box_list, score_list\nimport matplotlib.pyplot as plt","8d981df2":"#check prediction\n\ncnt = 0\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=0.4)#0.4\n    for i in range(len(images)):\n        sample = images[i].permute(1,2,0).cpu().numpy()\n        boxes = box_list[i].astype(np.int32).clip(min=0, max=511)\n        scores = score_list[i]\n        if len(scores) >= 1:\n            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n            sample = cv2.resize(sample , (int(1280), int(720)))\n            for box,score in zip(boxes,scores):\n                box[0] = box[0] * 1280 \/ 512\n                box[1] = box[1] * 720 \/ 512\n                box[2] = box[2] * 1280 \/ 512\n                box[3] = box[3] * 720 \/ 512\n                cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 3)\n            ax.set_axis_off()\n            ax.imshow(sample);\n            cnt += 1\n    if cnt >= 10:\n        break","e6f5f1ef":"result_image_ids = []\nresults_boxes = []\nresults_scores = []\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=0.4)#0.4\n    for i, image in enumerate(images):\n        boxes = box_list[i]\n        scores = score_list[i]\n        image_id = image_ids[i]\n        boxes[:, 0] = (boxes[:, 0] * 1280 \/ 512)\n        boxes[:, 1] = (boxes[:, 1] * 720 \/ 512)\n        boxes[:, 2] = (boxes[:, 2] * 1280 \/ 512)\n        boxes[:, 3] = (boxes[:, 3] * 720 \/ 512)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        boxes = boxes.astype(np.int32)\n        boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n        boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n        boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n        boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n        result_image_ids += [image_id]*len(boxes)\n        results_boxes.append(boxes)\n        results_scores.append(scores)","499d25c8":"box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['left', 'top', 'width', 'height'])\ntest_df = pd.DataFrame({'scores':np.concatenate(results_scores), 'image_name':result_image_ids})\ntest_df = pd.concat([test_df, box_df], axis=1)\n\ntest_df = test_df[test_df.scores > 0.3]\ntest_df.shape","b6202075":"test_df.head()","59627032":"#gameKey,playID,view,video,frame,left,width,top,height\n#57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1\ntest_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ntest_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\"]]\ntest_df","89d8230d":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n!mv * \/tmp\/","358beeb3":"import nflimpact\nenv = nflimpact.make_env()\nenv.predict(test_df) # df is a pandas dataframe of your entire submission file","3ed6d24a":"This notebook detects 2 class objects.\n- class1: helmet without impact\n- class2: helmet with impact\n\nObject Detection part is based on [EfficientDet notebook](https:\/\/www.kaggle.com\/shonenkov\/training-efficientdet) for [global wheat detection competition](https:\/\/www.kaggle.com\/c\/global-wheat-detection) by [shonenkov](https:\/\/www.kaggle.com\/shonenkov), which is using [github repos efficientdet-pytorch](https:\/\/github.com\/rwightman\/efficientdet-pytorch) by [@rwightman](https:\/\/www.kaggle.com\/rwightman).\n\nTraining part can be foud [here](https:\/\/www.kaggle.com\/its7171\/2class-object-detection-training\/)."}}