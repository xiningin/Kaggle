{"cell_type":{"af015171":"code","015af6c8":"code","3d7814cf":"code","5365cc6f":"code","18359071":"code","d9bed63b":"code","2ed20004":"code","af3d7362":"code","824727dd":"code","ce9bdb11":"code","a085f29b":"code","76882a70":"markdown","ca0f0272":"markdown"},"source":{"af015171":"import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport time\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport numpy as np \nimport pandas as pd","015af6c8":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","3d7814cf":"def get_classes():\n    return sorted(os.listdir('..\/input\/fruits-360_dataset\/fruits-360\/Training'))\n    \nclasses = get_classes()\nprint(classes)\n","5365cc6f":"img_size = 100\nimg_sc = int(((img_size-2)\/4)-1)","18359071":"class Conv(nn.Module):\n    def __init__(self, layer1_neurons, layer2_neurons):\n        super(Conv, self).__init__()\n        \n        self.layer1_neurons = layer1_neurons\n        self.layer2_neurons = layer2_neurons\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        \n        self.img_sc_param = (img_sc**2)*20\n        \n        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 100, kernel_size = 3)\n        self.conv2 = nn.Conv2d(in_channels = 100, out_channels = 20, kernel_size = 3)\n        \n        self.lin1 = nn.Linear(self.img_sc_param, self.layer1_neurons)\n        self.lin2 = nn.Linear(self.layer1_neurons, self.layer2_neurons)\n        self.lin3 = nn.Linear(self.layer2_neurons, 104)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.pool(x)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        \n        x = x.view(-1, self.img_sc_param)\n        \n        x = self.lin1(x)\n        x = F.relu(x)\n        \n        x = self.lin2(x)\n        x = F.relu(x)\n        \n        x = self.lin3(x)\n        \n        return x\n\ndef train(conv, loader, epochs):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(conv.parameters(), lr = 0.01, momentum = 0.9)\n    \n    for epoch in range(epochs):\n        r_loss = 0.0\n        for i, data in enumerate(loader, 0):\n            inputs, labels = data\n\n            optimizer.zero_grad()\n\n            outputs = conv(inputs.cuda())\n            loss = criterion(outputs, labels.cuda())\n            loss.backward()\n            optimizer.step()\n\n            r_loss += loss.item()\n            \n            every_n = 700\n            if i % every_n == every_n - 1:\n                print('[%d, %5d] loss: %.3f' %\n                      (epoch + 1, i + 1, r_loss \/ every_n))\n                r_loss = 0.0\n\ndef test(conv, test_loader):\n    #print(len(test_loader))\n    dataiter = iter(test_loader)\n    classes = get_classes()\n    \n    #print(len(dataiter))\n    \n    for i in range(10000):\n        images, labels = dataiter.next()\n        #print(labels)\n        if i % 1000==0:\n            outputs = conv(images.cuda())\n            _, predicteds = torch.max(outputs, 1)\n            predicted = classes[predicteds[0]]\n            groundtruth = classes[labels[0]]\n            \n            #print(i,\"-\",predicteds[0])\n            print(i)\n            print(\"Prediction: {}\".format(predicted))\n            print(\"Real: {}\".format(groundtruth))\n        \n\ndef get_predictions(conv, data_loader):\n    y_true = []\n    y_pred = []\n    \n    for data in data_loader:\n        images, labels = data\n        images = images.cuda()\n        labels = labels.cuda()\n        \n        outputs = conv(images)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true += labels.tolist()\n        y_pred += predicted.tolist()\n    \n    return [y_true, y_pred]\n\n\ndef validate(conv, test_loader):\n    y_true, y_pred = get_predictions(conv, test_loader)\n    \n    correct = (np.array(y_true) == np.array(y_pred)).sum()\n    total = len(y_true)\n    validation = correct \/ total\n    \n    return validation\n\ndef get_metrics(conv, test_loader):\n    y_true, y_pred = get_predictions(conv, test_loader)\n    \n    classes = get_classes()\n\n    y_true = [classes[a] for a in y_true]\n    y_pred = [classes[a] for a in y_pred]\n    \n    classification = classification_report(y_true, y_pred, labels = classes, target_names = classes)\n    \n    confusion = confusion_matrix(y_true, y_pred, labels = classes)\n    \n    return [classification, confusion]\n","d9bed63b":"conv = Conv(3000, 300)\n\nconv.cuda()","2ed20004":"transform = transforms.Compose([\n    transforms.Resize(size = (img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0,0,0), std=(1,1,1))\n])\n\ntrain_data = torchvision.datasets.ImageFolder('..\/input\/fruits-360_dataset\/fruits-360\/Training', transform = transform)\ntest_data = torchvision.datasets.ImageFolder('..\/input\/fruits-360_dataset\/fruits-360\/Test', transform = transform)","af3d7362":"train_data_split, validation_data_split = torch.utils.data.random_split(train_data, [int(len(train_data)*0.4), len(train_data)-int(len(train_data)*0.4)])\n\ntrain_loader = torch.utils.data.DataLoader(train_data_split, 10, shuffle=True, num_workers = 2)\nvalidation_loader = torch.utils.data.DataLoader(validation_data_split, 1, shuffle=True, num_workers = 2)\ntest_loader = torch.utils.data.DataLoader(test_data, 1, shuffle=True, num_workers = 2)\n","824727dd":"print(\"Train\")\ntrain(conv, train_loader, 5)\n\nprint('Validation')\nconv_validation = validate(conv, validation_loader)\n\nprint('Validation score')\nprint(conv_validation)","ce9bdb11":"print(\"Test\")\ntest(conv, test_loader)","a085f29b":"classification, confusion = get_metrics(conv, test_loader)\nprint()\nprint('classification report:')\nprint(classification)\n\nprint()\nprint('confusion report:')\nprint(confusion)","76882a70":"\u041d\u0430\u0445\u043e\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441\u044b","ca0f0272":"\u041f\u043e\u0434\u0433\u043e\u0442\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435, \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b"}}