{"cell_type":{"837b6610":"code","f51f0cbb":"code","4d451b73":"code","d6414c93":"code","2859eaf4":"code","ecce633b":"code","60719abf":"code","6bd6a0dc":"code","62ba6b99":"code","cefcbba0":"code","107a8fa5":"code","22cb610f":"code","1956f947":"code","a8c7a539":"code","5152dd4d":"code","5ab69201":"code","64a09183":"code","ed0abd75":"code","3e77b57b":"code","5e63e77d":"code","e702c4c1":"code","b3cf2cf7":"code","51adedbc":"code","e237c0f4":"code","1036e8f7":"markdown","c7e5eb19":"markdown","84abb45f":"markdown","e7ecf2dc":"markdown","976f79bc":"markdown","9b78300d":"markdown","15faddef":"markdown","910609cc":"markdown","dfb387d9":"markdown","e69e7ee2":"markdown","59a4b930":"markdown","7bd2b7dd":"markdown","74092585":"markdown","01edad30":"markdown","f920505a":"markdown","2f275160":"markdown","b5aaac62":"markdown","77dbb68d":"markdown","05727ed3":"markdown","7d709e6e":"markdown","02db12f5":"markdown"},"source":{"837b6610":"# General Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Model Imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.impute import SimpleImputer\nimport scikitplot as skplt","f51f0cbb":"# This class is used for graphing box and wisker plots\nclass visualizer:\n    def __init__(self, df, colname, fields, dimensions, figsize=None):\n        self.df = df\n        self.colname = colname\n        self.fields = fields\n        self.figsz = figsize\n        self.dim = dimensions\n        \n    def data(self, field):\n        vals = list(set(self.df[self.colname]))\n        return [self.df[self.df[self.colname] == i][field].values for i in vals]\n          \n    def plot(self, ax, field):\n        ax.set_title(field)\n        ax.set_xlabel(\"Classification\")\n        sns.boxplot(ax=ax, data = self.data(field), showfliers=False)\n    \n    def graph(self):\n        fig, axes = plt.subplots(nrows=self.dim[0], ncols=self.dim[1], figsize=self.figsz)\n        for ax, field in zip(axes.reshape(-1), self.fields):\n            self.plot(ax, field)\n                       ","4d451b73":"# Reading in the kepler exoplanet data from csv\nkepler_df = pd.read_csv(\"..\/input\/cumulative.csv\")\n\nprint(\"Dimensions: \", kepler_df.shape)\nkepler_df.head()","d6414c93":"agg_fxns = {\n    'koi_disposition': ['count'],\n    'koi_score': ['mean']\n}\n\nkepler_df.groupby(['koi_disposition']).agg(agg_fxns)","2859eaf4":"agg_fxns = {\n    'koi_pdisposition': ['count'],\n    'koi_score': ['mean']\n}\n\nkepler_df.groupby(['koi_pdisposition']).agg(agg_fxns)","ecce633b":"# Replace the enumerated values in the kepler_df\nkepler_df = kepler_df.replace({'CONFIRMED': 0, 'CANDIDATE': 1, 'FALSE POSITIVE': 2})\n\n# Clean the Data Frame by removing all labels, and error values\nkepler_df = kepler_df.drop(['rowid', 'kepid', 'kepoi_name', 'kepler_name', 'koi_tce_delivname', \n                     'koi_period_err1', 'koi_period_err2', \n                     'koi_time0bk_err1', 'koi_time0bk_err2', \n                     'koi_impact_err1', 'koi_impact_err2',\n                     'koi_depth_err1', 'koi_depth_err2', \n                     'koi_prad_err1', 'koi_prad_err2', \n                     'koi_insol_err1', 'koi_insol_err2',\n                     'koi_steff_err1', 'koi_steff_err2', \n                     'koi_slogg_err1', 'koi_slogg_err2', \n                     'koi_srad_err1', 'koi_srad_err2', \n                     'koi_duration_err1', 'koi_duration_err2', \n                     'koi_teq_err1', 'koi_teq_err2'], axis=1)\n\n# Dealing with null data fields\n# kepler_df.fillna(0, inplace=True)\nkepler_df.fillna(kepler_df.mean(), inplace=True)","60719abf":"# Building a correlation matrix\ncorr = kepler_df.corr()\nf, ax = plt.subplots(figsize=(9, 7))\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)","6bd6a0dc":"agg_fxns = {\n    'koi_score': ['mean'],\n    'koi_fpflag_nt': ['mean'],\n    'koi_fpflag_ss': ['mean'],\n    'koi_fpflag_co': ['mean'],\n    'koi_fpflag_ec': ['mean'],\n    'koi_depth': ['mean'],\n    'koi_teq': ['mean'],\n    'koi_model_snr': ['mean']\n}\nkepler_df.groupby('koi_disposition').agg(agg_fxns).transpose()","62ba6b99":"fields = ['koi_score', 'koi_depth', 'koi_teq', 'koi_model_snr']\nid = 'koi_disposition'\n\nv = visualizer(kepler_df, 'koi_disposition', fields, (1,4), (15,5))\nv.graph()","cefcbba0":"kepler_df.groupby('koi_pdisposition').agg(agg_fxns).transpose()","107a8fa5":"fields = ['koi_score', 'koi_depth', 'koi_teq', 'koi_model_snr']\nid = 'koi_pdisposition'\n\nv = visualizer(kepler_df, 'koi_pdisposition', fields, (1,4), (15,5))\nv.graph()","22cb610f":"# kepler_df.dtypes\n\ndata = kepler_df.values\n\nscaler = MinMaxScaler(feature_range=[0, 1])\n\n# Removing first two columns because they are labels\nrescaled_data = scaler.fit_transform(data[:, 2:])\n\npca = PCA().fit(rescaled_data)\n\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.xlim((0,15))\nplt.title('Variance in KOI Data')\nplt.show()","1956f947":"pca = PCA(n_components=8)\ndataset = pca.fit_transform(rescaled_data)\ndataset.shape","a8c7a539":"fig = plt.figure()\nax = Axes3D(fig)\n\nax.scatter(dataset[:, 0], dataset[:, 1], dataset[:, 2], c=data[:,0])","5152dd4d":"# Plot the differnt clusters\ntitles = [\"Front View\", \"Top View\", \"Right View\"]\nmarker = []\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,5))\nfor i, ax in enumerate(axes.reshape(-1)):\n    ax.scatter(dataset[:, i], dataset[:, i+1], c=data[:,0], marker=\"2\")\n    ax.set_title(titles[i])  ","5ab69201":"# classification = data[:, 1:2] # koi_pdisposition\nclassification = data[:, :1]\ntrain, test, trainC, testC = train_test_split(dataset, classification, test_size=0.33, random_state=0)\n\nprint(\"Training set size: \", train.shape)\nprint(\"Test set size: \", test.shape)","64a09183":"# target_names = ['CANDIDATE', 'FALSE POSITIVE'] # koi_pdisposition\ntarget_names = ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']\n\ndef run_model(model):\n    # Train and fit the model\n    model.fit(train, trainC.ravel())\n    # Predict the test data\n    predictions = model.predict(test)\n    # Compute model metrics\n    overall_precision = np.mean(predictions == testC.ravel()) \n    print(\"Percentage of correct predictions: \", overall_precision)\n    report = classification_report(predictions, testC, target_names=target_names, output_dict=True)\n    report = pd.DataFrame(data=report).drop(['micro avg', 'macro avg', 'weighted avg'], axis=1)\n    print(\"\\n\", report)\n    return report.drop(['precision', 'recall', 'support']) ","ed0abd75":"gc = GaussianNB()\ngc_report = run_model(gc)","3e77b57b":"knn = KNeighborsClassifier(n_neighbors=7)\nknn_report = run_model(knn)","5e63e77d":"svm = SVC(gamma='auto', probability=True)\nsvm_report = run_model(svm)","e702c4c1":"dt = tree.DecisionTreeClassifier()\ndt_report = run_model(dt)","b3cf2cf7":"nn = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(5, 2), random_state=1)\nnn_report = run_model(nn)","51adedbc":"def plot_roc(ax, model, title):\n    probas = model.predict_proba(test)\n    skplt.metrics.plot_roc(y_true=testC, y_probas=probas, ax=ax, title=title)\n\nmodels = [(gc, \"Niave Bayes\"), (knn, \"knn\"), (svm, \"SVM\"), (dt, \"Decision Tree\"), (nn, \"Neural network\")]\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(30,6))\nfor ax, model in zip(axes, models):\n    plot_roc(ax, model[0], model[1])","e237c0f4":"reports = [gc_report, knn_report, svm_report, dt_report, nn_report]\ncumulative = pd.DataFrame()\nfor report in reports:\n    cumulative = pd.concat([cumulative, report])\n    \ncumulative.set_index(pd.Index([\"Naive Bayes f1-score\", \"KNN f1-score\",\n                               \"SVM f1-score\", \"Decision Tree f1-score\", \"Neural Network f1-score\"]))","1036e8f7":"#### Naive Bayes\nNaive Bayes is a supervised learning technique that utilizes Bayes\u2019 Theorem along with some other assumptions about the conditional independence of the features to classify points. When classifying points, the label with the highest conditional probability is the label given to the point.\n\n[Naive Bayes Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n\n[Naive Bayes Description](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html)","c7e5eb19":"The precision metrics above for the Naive Bayes model indicates that when a data point is classified as CONFIRMED or FALSE POSITIVE it is right over 90% of the time, and only 35% of the time is it right when classified as a CANDIDATE. Additionally, the recall metric shows that the model only predicts 59% of the CONFIRMED labels, 76% of the CANDIDATE labels and 97% of the FALSE POSITIVE labels.\n\n#### KNN\nKNN is a supervised learning algorithm that uses the proximity of data points to classify new points. Rather than learning a function used to classify new data points, KNN keeps the entire training set and uses it to classify the new points. KNN classifies new points by looking at their k nearest neighbors, which is determine by some proximity measurement, and the most prominent label in the k neighbors is the label given to the new point.\n\n[KNN Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)\n\n[KNN Description](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)","84abb45f":"#### Cleaning the data","e7ecf2dc":"The precision metrics above for the neural network indicates that when a data point is classified as CONFIRMED it is right 79% of the time, if it is classified as a CANDIDATE it is right 58% of the time, and finally if it is classified as FALSE POSITIVE it is right 98% of the time. Additionally, the recall metric shows that the model only predicts 66% of the CONFIRMED labels, 72% of the CANDIDATE labels and 98% of the FALSE POSITIVE labels.\n\n#### Model Analysis\n[Scikitplot Documentation](https:\/\/scikit-plot.readthedocs.io\/en\/stable\/)","976f79bc":"## Data Eploration and Analysis\n\n#### Kepler Exoplanet Data\nThe Kepler space observatory, a satellite built and monitored by NASA, was launched in 2009 with the hopes of discovering other planets that could inhabit life (i.e. Exoplanets). In order to find these planets the observatory must discover other solar systems. The observatory does this by identifying stars that have signatures that indicate transiting planets. These stars are called Kepler's objects of interest (KOI). Although their stellar fluctuations follow patterns consistent with that of a star that has transiting planets, some of the patterns identified are due to other influences. Therefore, there are KOIs that are hosts to planet(s) and some that mimic the signature of a host star. The following data is a record of all the Kepler objects of interest that the Kepler space observatory has been monitoring. \n\n[Data Field Descriptions](https:\/\/exoplanetarchive.ipac.caltech.edu\/docs\/API_kepcandidate_columns.html)\n","9b78300d":"The precision metrics above for the kth nearest neighbor model indicates that when a data point is classified as CONFIRMED it is right 62% of the time, if it is classified as a CANDIDATE it is right 35% of the time, and finally if it is classified as FALSE POSITIVE it is right 98% of the time. Additionally, the recall metric shows that the model only predicts 62% of the CONFIRMED labels, 66% of the CANDIDATE labels and 98% of the FALSE POSITIVE labels.\n\n#### SVM\nSupport vector machines are a supervised learning model that tries to find the largest gap that separates the different labels within the data. It then uses this gap to determine the label of new points being classified.\n\n[SVM Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)\n\n[SVM Description](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine)","15faddef":"#### Correlations between columns\nA lot of the fields within the dataset are of attributes that all KOIs have in common. Therefore most of the field will have little influence when trying to determine the classification of a KOI. Therefore, a correlation matrix can be used in order to identify which columns have an influence on the classification.\n\n[Correlation Matricies](https:\/\/www.datascience.com\/blog\/introduction-to-correlation-learn-data-science-tutorials)\n\n**NOTE: When analyzing the correlation between columns, columns that represented a label or error were disregarded.**","910609cc":"The Kepler objects of interest have three columns that describe their current classification.\n\n| Column name      |  data type |  Description                                                            |\n|------------------|------------|-------------------------------------------------------------------------| \n| koi_disposition  | object     |  Current classification of the KOI                                      | \n| koi_pdisposition | object     |  Identifies the most probable classification                            | \n| koi_score        | float64    |  Confidence value of the KOI's disposition (lower values are good for false positives)                              |\n\nEach KOI can be classified in one of the following ways. \n\n| Classification | Meaning                                                                                 |\n|----------------|-----------------------------------------------------------------------------------------|\n| CONFIRMED (0)      | KOI is a host star                                                        |\n| CANDIDATE (1)     | KOI has passed all prior tests used in identifying false positives |\n| FALSE POSITIVE (2) | KOI is NOT a host star                |\n\n\n#### Composition of Classifications","dfb387d9":"From the correlation matrix, columns with a value higher than 0.2 and lower that -0.2 when compared with the koi_disposition and koi_pdisposition columns, were extracted and identified as columns of interest.\n  \n| Column name      |  data type |  Description                                                            | \n|------------------|------------|-------------------------------------------------------------------------|\n| koi_score        | float64    |  Confidence value of the KOI's disposition                              | \n| koi_fpflag_nt          | int64    |  A KOI whose light curve is not consistent with that of a transiting planet. This includes, but is not limited to, instrumental artifacts, non-eclipsing variable stars, and spurious (very low SNR) detections.      | \n| koi_fpflag_ss        | int64    |  A KOI that is observed to have a significant secondary event, transit shape, or out-of-eclipse variability, which indicates that the transit-like event is most likely caused by an eclipsing binary. However, self-luminous, hot Jupiter\u2019s with a visible secondary eclipse will also have this flag set, but with a disposition of PC.                                              | \n| koi_fpflag_co        | int64    |  The source of the signal is from a nearby star, as inferred by measuring the centroid location of the image both in and out of transit, or by the strength of the transit signal in the target's outer (halo) pixels as compared to the transit signal from the pixels in the optimal (or core) aperture.                | \n| koi_fpflag_ec         | int64    |  The KOI shares the same period and epoch as another object and is judged to be the result of flux contamination in the aperture or electronic crosstalk.                                                   | \n| koi_depth       | float64    | The fraction of stellar flux lost at the minimum of the planetary transit.                                                           | \n| koi_teq       | float64    |  Approximation for the temperature of the planet.                                                           | \n| koi_model_snr       | float64    |  Transit depth normalized by the mean uncertainty in the flux during the transits.                                                            |  \n\n#### Analysis of the columns of interest\n","e69e7ee2":"The graphs and table above illustrate the distributions of the values for each classification type in koi_pdispostion column. As we can see from the four graphs the distributions for CANDIDATE (0) KOIs has less variability, variation and tends to have a lower median on average when compared to the FALSE POSITIVE (1) distribution.\n\n#### Dimensionality Reduction\nAfter exploring the data it is obvious that there are only a handful of data fields that actually correlate to the classification of the star. Therefore, there is no need for all of the fields within the dataset, so it can be reduced using PCA. By using PCA a majority of the variation within the data will be captured even though the dataset has been shrunk. Thus, classification on the smaller dataset will still have the same effect as on the larger set.  \n\n[PCA Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit)\n\n[PCA Description](https:\/\/towardsdatascience.com\/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe)\n","59a4b930":"## Models\nMeasurement Definitions\n* **f1-score**:  F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. \n* **precision**: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n* **recall**:  Recall is the ratio of correctly predicted positive observations to the all observations in actual class\n\n[Sklearn Metrics Documentation](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.metrics)\n\n\n[Model Performance Interpretation](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)","7bd2b7dd":"The precision metrics above for the decision tree model indicates that when a data point is classified as CONFIRMED it is right 66% of the time, if it is classified as a CANDIDATE it is right 58% of the time, and finally if it is classified as FALSE POSITIVE it is right 96% of the time. Additionally, the recall metric shows that the model only predicts 62% of the CONFIRMED labels, 62% of the CANDIDATE labels and 97% of the FALSE POSITIVE labels.\n\n\n#### Neural Network\nNeural networks are a supervised learning technique that uses the input data and their known labels to learn a non-linear function that takes an input and outputs a classification.\n\n[Neural Network Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n\n[Neural Network Description](https:\/\/scikit-learn.org\/stable\/modules\/neural_networks_supervised.html)","74092585":"As shown in the graph above there is only a need for eight principle components to capture the majority of the variance within the data as expected.  ","01edad30":"The graphs and table above illustrate the distribution of the values for each classification type in koi_dispostion column. As we can see from the four graphs the distributions for CONFIRMED(0) and CANDIDATE (1) KOIs are pretty similar, whereas the FALSE POSITIVE (2) distributions seem to have more variation and higher medians.","f920505a":"The precision metrics above for the support vector machine indicates that when a data point is classified as CONFIRMED or FALSE POSITIVE it is right over 94% of the time, and only 30% of the time is it right when classified as a CANDIDATE. Additionally, the recall metric shows that the model only predicts 57% of the CONFIRMED labels, 79% of the CANDIDATE labels and 98% of the FALSE POSITIVE labels.\n\n#### Decision Tree\nThe decision tree is a supervised learning model that predicts the value of a target variable by learning simple decision rules that are inferred from the data features.\n\n[Decision Tree Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n\n[Decision Tree Description](https:\/\/scikit-learn.org\/stable\/modules\/tree.html)","2f275160":"#### Individual Model Results\n- **Naive Bayes**: The ROC curve shows that the naive Bayes model has high accuracy for each label and can achieve a high true positive rate with a relatively low false positive rate. Furthermore, the f1-score for the CONFIRMED and FALSE POSITIVE indicates high precision and recall for those labels. The lower f1-score reiterates that the precision value for the CANDIDATE label is low, thus the model doesn\u2019t predict CANDIDATE labels very accurately.\n\n- **KNN**: The ROC curve shows that the KNN model has high accuracy for each label and can achieve a high true positive rate with a relatively low false positive rate. Furthermore, the f1-score for the FALSE POSITIVE labels indicates high precision and recall for that label. The lower f1-score reiterates that the precision and recall value for the CANDIDATE label is low, thus the model doesn\u2019t predict CANDIDATE labels very accurately, and it doesn\u2019t recognize them very well.\n\n- **SVM**: The ROC curve shows that the SVM has high accuracy for each label and can achieve a high true positive rate with a relatively low false positive rate. Furthermore, the f1-score for the CONFIRMED and FALSE POSITIVE indicates high precision and recall for those labels. The lower f1-score reiterates that the precision value for the CANDIDATE label is low, thus the model doesn\u2019t predict CANDIDATE labels very accurately.\n\n- **Decision Tree**: The ROC curve shows that the decision tree has a alright accuracy for each label, and can achieve a high true positive rate with a higher false positive rate. Furthermore, the f1-score for the CONFIRMED and FALSE POSITIVE indicates high precision and recall for those labels.\n\n- **Neural Network**: The ROC curve shows that the neural network has high accuracy for each label and can achieve a high true positive rate with a relatively low false positive rate. Furthermore, the f1-score for the CONFIRMED and FALSE POSITIVE indicates high precision and recall for those labels. \n\n#### Model Error \nThe models recall and precision values are lower for the CANDIDATE and CONFIRMED label due to the overlapping and coupled definitions for the two labels. \n\n## Conclusion\nEach model is a viable candidate based off its accuracy, precision, and recall measurement. There are two type of classification approaches appropriate for this data set.\n\n1. CANDIDATE vs. FALSE POSITIVE: Since each star labeled as CONFIRMED technically meets the requirement of a CANDIDATE star you could use the classifier to separate stars into a CONFIRMED\/CANDIDATE group and a FALSE POSITIVE group. Providing a model that essentially flags stars as not important, thus thinning the number of stars to be confirmed. Based on the results of all the models each one could be used for this type of classifier, but the SVM would be the best due to it high precision rate in both CONFIRMED and FALSE POSITIVE labels.\n\n2. A classifying model: This model would classify each star into its perspective label. Based on the results only the neural network has high enough scores all around to do this type of classification, and even with its scores it wouldn't perform exceedingly well.\n\nOverall every model could be used but the two the standout the most are the neural network and the SVM. Additional work that could be done in order to better the performance of the classifiers would be parameter tuning , readjusting some of the model choices to better fit the distribution of the data, and exploring the use of the koi_pdisposition as a label. ","b5aaac62":"The graphs above illustrate the data set after it has been mapped to the lower space using only the first eight principle components. Visually it only shows the mapping of the first three principle components, and it is already apparent that there is separation between the FALSE POSITIVE labels and the other labels. This obvious separation signifies that the data will work well when classifying FALSE POSITIVE labels with different classification models. But also indicates that there will be difficulty when trying to distinguish between CONFIRMED and CANDIDATE labels due to their points overlapping.\n\n#### Splitting data into training and test set","77dbb68d":"<div align=\"center\">\n<font size=4><b>\nEvaluating classification models for the KOI dataset\n<\/b>\n<font size=3><b>\n<br>Will Udstrand\n<br>April 15, 2019\n<br><\/font><\/b><\/div>\n\n---\n\n## Overview\nThere are over 100 billion galaxies in the observable universe each containing billions of stars and of those stars roughly 10 percent of them are hosts stars. Kepler objects of Interest are the stars that have been identified as potential hosts. These host stars are important when searching for planets outside our solar system. The goal of this notebook is to explore different classification techniques that could be employed in order to help classify KOIs. The KOI dataset provided by NASA contains the attributes of each KOI and their current classification as a host, candidate or a non-host star. With this dataset five different classification models will be trained and tested, then there individual performances will be compared in order to determine which model is best fit for classifying KOIs.\n\nClassification models used\n- Naive Bayes\n- Kth Nearest Neighbor\n- Support Vector Machine\n- Decision Tree\n- Multilevel Perceptron (Neural Network) \n\n\n\n## References\n* [KOI Dataset](https:\/\/www.kaggle.com\/nasa\/kepler-exoplanet-search-results)\n* [Exoplanet Info](https:\/\/en.wikipedia.org\/wiki\/Exoplanet)\n* [Kepler Objects of Interest Info](https:\/\/en.wikipedia.org\/wiki\/Kepler_object_of_interest)\n* [Number of stars in the universe](https:\/\/www.ast.cam.ac.uk\/public\/ask\/2360)\n    \n## Imports\n","05727ed3":"#### Visualization of the Reduced Space","7d709e6e":"[ROC Curve Explanation](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic)\n\n[Area Under the ROC Curve Explanation](http:\/\/gim.unmc.edu\/dxtests\/roc3.htm)\n\nThe ROC curve shows the relationship between the true positive rate (i.e. correct classifications) and the false positive rate (i.e. incorrect classifications) with the area under the curve representing the accuracy of the model predications. Therefore, the closer the area is to one the better and it's ideal to have a curve that approaches a true positive rate of one quickly.","02db12f5":"## Helper Class"}}