{"cell_type":{"6e0c377d":"code","d088cde0":"code","af3c3cd3":"code","42de437f":"code","7075e38e":"code","99f9f232":"code","18a99a91":"code","2451b9ed":"code","988ba2fd":"code","4cc4db00":"code","705340c0":"code","d889f784":"code","1702dea9":"code","2a186e2c":"code","95347fd9":"code","6b96c2a6":"code","2ec47e25":"markdown","c4a7f07f":"markdown","0b816df4":"markdown","c61685d9":"markdown","de87a000":"markdown","1f054341":"markdown","53aa8ae8":"markdown","70e10d8b":"markdown","3f2d4831":"markdown","d1ab5be9":"markdown","56b1b832":"markdown","9c80899d":"markdown","6752b6b5":"markdown","2ba41d0a":"markdown","f9921003":"markdown","e254d94a":"markdown","dc5f92f1":"markdown","f00274b4":"markdown","06266b0f":"markdown","e6959475":"markdown"},"source":{"6e0c377d":"# import libraries\nimport math\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport numpy.random as rng\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import expon\nfrom collections import Counter","d088cde0":"# Exponential RV generation function\n\ndef inverse_transform_exponential(size, exp_lambda = 1.0):\n    \"\"\"\n    Generates a variate of the exponential distribution with parameter lambda using the Inverse Transform Method\n    \n    \"\"\"\n    # generate random uniforms using rng.random()\n    random_uniform_numbers = rng.random(size)\n    exponential_inverse_transform_list = []\n    \n    # loop through each uniform and apply the exponential transformation and result to the list\n    for index, value in enumerate(random_uniform_numbers):\n        exponential_transformation = (-1\/exp_lambda)*math.log(value)\n        exponential_inverse_transform_list.append(exponential_transformation)\n    \n    # return our list\n    return exponential_inverse_transform_list","af3c3cd3":"# Create Charts\nfig, ax = plt.subplots(2, 2, figsize=(15,10))\n# Plot our results in four separate figures\nax[0, 0].hist(inverse_transform_exponential(10000,0.5), bins = 50, density = True, stacked = True)\nax[0, 1].hist(inverse_transform_exponential(10000,1.0), bins = 50, density = True, stacked = True)\nax[1, 0].hist(inverse_transform_exponential(10000,2.0), bins = 50, density=True, stacked = True)\nax[1, 1].hist(inverse_transform_exponential(10000,3.0), bins = 50, density = True, stacked = True)\n# Titles\nax[0, 0].set_title(\"$ \\lambda = 0.5 $\")\nax[0, 1].set_title(\"$ \\lambda = 1 $\")\nax[1, 0].set_title(\"$ \\lambda = 2 $\")\nax[1, 1].set_title(\"$ \\lambda = 3 $\")\n# Graph Limits\nax[0,0].set_xlim([0, 15])\nax[0,1].set_xlim([0, 7])\nax[1,0].set_xlim([0, 3])\nax[1,1].set_xlim([0, 3])\n# Let's check to see how close our results are by creating exponentials in Scipy and by calculating the inverse cdf using the ppf() function and setting scale = 1\/lambda\n# We'll then plot the pdf of that curve and overlay it over our previous histograms\n# Source: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.expon.html\nx = np.linspace(expon.ppf(0.00001), expon.ppf(0.99999), 10000)\nax[0,0].plot(x, expon.pdf(x,scale = 2),'r-', lw = 2,alpha = 1,label ='Calculated PDF')\nax[0,1].plot(x, expon.pdf(x,scale = 1),'r-', lw = 2,alpha = 1, label ='Calculated PDF')\nax[1,0].plot(x, expon.pdf(x,scale = 0.5),'r-', lw = 2,alpha = 1, label ='Calculated PDF')\nax[1,1].plot(x, expon.pdf(x,scale = .33333),'r-', lw = 2,alpha = 1, label ='Calculated PDF')\nax[0,0].legend(loc = 'best', frameon = False)\nax[0,1].legend(loc = 'best', frameon = False)\nax[1,0].legend(loc = 'best', frameon = False)\nax[1,1].legend(loc = 'best', frameon = False)\nfig.tight_layout()\nplt.title(\"Exponential Variate Generation\")\nplt.show()","42de437f":"# Weibull RV generation function\ndef inverse_transform_weibull(size,alpha,beta):\n    \"\"\"\n    Generates a variate of the Weibull distribution with scale = alpha and shape = beta using Inverse Transform Method\n    \"\"\"\n    random_uniform_numbers = rng.random(size)\n    weibull_inverse_transform_list = []\n    \n    for index, value in enumerate(random_uniform_numbers):\n        weibull_transformation = (-alpha*(math.log(1-value)))**(1 \/ beta)\n        weibull_inverse_transform_list.append(weibull_transformation)\n    return weibull_inverse_transform_list","7075e38e":"# Create a function to create weibull histograms\n# alpha = scale\n# beta = shape\ndef plot_weibull_hist(ax, alpha, beta):\n    ax.hist(inverse_transform_weibull(10000, alpha, beta), histtype=\"stepfilled\", bins=100, range=(0,2.5),alpha=0.4, density = True, stacked = True, label = \"\\u03B1 =\" + str(alpha) + \" , \" \"\\u03B2 = \" + str(beta))\n# https:\/\/matplotlib.org\/2.0.2\/examples\/style_sheets\/plot_bmh.html\nplt.style.use('bmh')\n# Let's plot some different distributions holding beta constant\nfig, ax = plt.subplots(figsize=(15,10))\nplot_weibull_hist(ax, 1, 1.5)\nplot_weibull_hist(ax, 1, 2)\nplot_weibull_hist(ax, 1, 5)\nplt.legend(loc = 'best', frameon = False)\nplt.title(\"Weibull Variate Generation\")\nplt.show()","99f9f232":"from scipy.stats import weibull_min\nfig, ax = plt.subplots(figsize=(15,10))\ny = np.linspace(weibull_min.ppf(0.01, 3),weibull_min.ppf(0.99, 3), 100)\nplt.plot(y, weibull_min.pdf(y,1.5),'blue', lw = 3, alpha = 0.8, label='alpha = 1, beta = 1.5')\nplt.plot(y, weibull_min.pdf(y,2),'green', lw = 3, alpha = 0.8, label='alpha = 1, beta = 2')\nplt.plot(y, weibull_min.pdf(y,5),'red', lw = 3, alpha = 0.8, label='alpha = 1, beta = 5')\nplt.legend(loc = 'best', frameon = False)\nplt.title(\"Weibull Variate Generation using SciPy\")\nplt.show()","18a99a91":"# Geometric RV generation function\ndef inverse_transform_geometric(size, probability = 0.5):\n    \"\"\"\n    Generates a variate of the geometric distribution with probability p using the Inverse Transform Method\n    \"\"\"\n    random_uniform_numbers = rng.random(size)\n    geometric_inverse_transform_list = []\n    \n    for index, value in enumerate(random_uniform_numbers):\n        geometric_transformation = math.ceil((math.log(value)) \/ (math.log(1-probability)))\n        geometric_inverse_transform_list.append(geometric_transformation)\n    return geometric_inverse_transform_list","2451b9ed":"# Create a plot\nfig, ax = plt.subplots(figsize=(10,8))\nplt.title('Geometric Distribution',fontsize=30)\nplt.xlabel('x')\nplt.ylabel('P(X = x)')\n# Call our function to create geometric RV's and then plot our results on a histogram\nax.hist(inverse_transform_geometric(100000,0.25),bins=20, density = True, stacked = True, alpha = 0.9, ec='black', color = 'c', label='Our Results')\n# Let's overlay Scipy's generation of a geometric distribution to see if our results are accurate\nfrom scipy.stats import geom\nr = geom.rvs(0.25, size = 100000)\nax.hist(r, bins = 20,density = True, stacked = True, alpha = 0.5, ec='black', label='Scipy Results')\nax.legend(loc='best', frameon=False)\nplt.show()","988ba2fd":"# Bernoulli RV generation function\ndef inverse_transform_bernoulli(p, size):\n    \"\"\"\n    Generates a variate of the Bernoulli distribution with probability p using the Inverse Transform Method\n    \"\"\"\n    random_uniform_numbers = rng.random(size)\n    bernoulli_inverse_transform_list = []\n    \n    for index, value in enumerate(random_uniform_numbers):\n        if value > 1-p:\n            bernoulli_inverse_transform_list.append(1)\n        if value <= 1-p:\n            bernoulli_inverse_transform_list.append(0)\n    return bernoulli_inverse_transform_list","4cc4db00":"# Plot our results in four separate figures\nfig, ax = plt.subplots(2,2, figsize=(10,8))\nax[0,0].hist(inverse_transform_bernoulli(0.5,10000), density = True, stacked = True, color=\"coral\")\nax[0,1].hist(inverse_transform_bernoulli(0.3,10000), density = True, stacked = True, color=\"darkorange\")\nax[1,0].hist(inverse_transform_bernoulli(0.7,10000), density = True, stacked = True, color=\"goldenrod\")\nax[1,1].hist(inverse_transform_bernoulli(0.90,10000), density = True, stacked = True, color=\"indianred\")\nax[0, 0].set_title(\"$ p = 0.5 $\")\nax[0, 1].set_title(\"$ p = 0.3 $\")\nax[1, 0].set_title(\"$ p = 0.7 $\")\nax[1, 1].set_title(\"$ p = 0.9 $\")\nfig.tight_layout()\nplt.title(\"Bernoulli Variate Generation\")\nplt.show()","705340c0":"# import our libraries\nfrom scipy.stats import gamma, norm, uniform, t\n# Define x axis\nx = np.arange(start = -10, stop = 15, step = 0.01)\nplt.figure(num = 1, figsize = (10,6))\n# Plot Target Distribution - Normal Distribution\nplt.plot(x, norm.pdf(x, loc = 3, scale = 2), lw = 2, label = 'Target Distribution - f(s) = Normal Distribution')\n# Plot our Candidate Distribution - T Distribution\nplt.plot(x, t.pdf(x, df = 2, loc = 2.5), lw = 2, label = 'Candidate Distribution - g(s) = T Distribution')\nplt.title(\"Acceptance-Rejection Method without Majorizing Function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nplt.legend(loc=\"upper right\")\nplt.show()","d889f784":"# Define x axis\nx = np.arange(start = -10, stop = 15, step = 0.01)\nplt.figure(num = 1, figsize = (10,6))\n# Plot Target Distribution - Normal Distribution\nplt.plot(x, norm.pdf(x, loc = 3, scale = 2.5), lw = 2, label = 'Target Distribution - f(s) = Normal Distribution')\n# Choose value for M\nM = 3\n# degrees of freedom (df) for T Distribution\ndf = 2\n# Plot our Candidate Distribution - T Distribution\nplt.plot(x, t.pdf(x, df, loc = 2.5, scale = 2), lw = 2, label = 'Candidate Distribution - f(s) = T Distribution')\nplt.plot(x, M*t.pdf(x, df, loc = 2.5, scale = 2), lw = 2, label = 'Majorizing Function - M* g(s)')\nplt.title(\"Acceptance-Rejection Method with Majorizing Function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nplt.legend(loc=\"upper right\")\nplt.show()","1702dea9":"# Now that we have our maximizing function ready, let's implement our AR algorithm\n# Size of samples we want returned\nsize = 10000\naccepted = []\nrejected = []\naccept_count = 0\nreject_count = 0\n\nwhile accept_count < size:\n    Y = t.rvs(df, loc = 2.5, scale = 2, random_state = None)\n    U = np.random.rand(1)\n    \n    if U * (M * t.pdf(Y, df, loc = 2.5, scale = 2)) <= norm.pdf(Y, loc = 3, scale = 2.5):\n        accepted.append(Y)\n        accept_count += 1\n    else: \n        rejected.append(Y)\n        reject_count += 1","2a186e2c":"# Let's plot our results\nplt.figure(num = 2, figsize = (10,6))\nplt.plot(x, norm.pdf(x, loc = 3, scale = 2.5), lw = 2, label = 'Target Distribution - f(s) = Normal Distribution')\nplt.hist(accepted, bins = 50, density = True, ec = 'black')\nplt.title(\"Results of AR Sampling Method\")\nplt.show()","95347fd9":"# Implementation of Knuth Algorithm\n# function to generate poisson random variables with arrival rate(lambda)\ndef acceptance_rejection_poisson(rate = 3):\n    \"\"\"\n    Generates a variate of the Poisson distribution with rate parameter lambda using Knuth's Algorithm\n    \"\"\"\n    p = 1\n    n = -1\n    a = math.exp(-rate)\n \n    while True: \n        n = n + 1\n        U = rng.random()\n        p = p*U\n        if a > p:\n            break\n    return n","6b96c2a6":"# let's take some samples and plot them\nsample_size = 10000\nrate = 2\nY = []\nfor i in range(sample_size):\n    Y.append(acceptance_rejection_poisson(rate = rate))\n\nplt.hist(Y,bins = 50, ec='black', density = True)\nplt.title(\"Generated Poisson Random Variates\")\nplt.show()","2ec47e25":"# Pseudo-Random Number Generators (PRNs)\n#### \\*Spoiler Alert\nComputers don't actually generate truly random uniform numbers! Instead, they generate \"pseudo\" random numbers which appear to be uniform and random, but are actually generated by a completely deterministic and repeatable process.\n\n![https:\/\/www.wordsonimages.com\/photo?id=54869-John+von+neumann+famous+quotes](attachment:1e7699e0-472d-49fc-b4e4-dde1c2f6d1eb.PNG)\n\n\n## Some Good Random Number Generators\n* Linear Congruential Generator\n* Marsenne Twister (Python, R, MATLAB)\n\n## One Bad Random Number Generator\nThe goal of a generator is to give an algorithm that produces a sequence of pseudo-random numbers that appear to be independent and identically distributed Unif(0,1). Thus, it is of absolute importance that our generator's output doesn't appear to break apart over time or to exhibit any noticeably deterministic qualities. \n\nIn what is widely considered to be one of the most ill-conceived random number generators ever designed, we present you with IBM's RANDU, which was used primarily in the 1960s and 1970s.\n\n### RANDU\n![1200px-Randu.png](attachment:a2891073-82ee-41ad-b713-1c7a4c5a4f4c.png)\n*Source: Wikipedia*\n\nWithout getting into too much detail, we can see when we look at RANDU output in 3D, we see that the numbers fall into 15 2D hyperplanes, which suggests that they are absolutely not independent and identically distriuted Uniform(0,1) numbers. \n\n##### A further exploration of PRN generators is well-deserved, but will not be the content of the rest of this notebook","c4a7f07f":"![@reddit](https:\/\/media.giphy.com\/media\/V0IdVIIW1y5d6\/giphy.gif)\n\n*Source: Reddit*","0b816df4":"### Let's Compare to SciPy's Implementation of Weibull","c61685d9":"# Exponential Distribution Random Variate Generation #\n\n## $ X \\sim Expo(\\lambda) $\nWith rate parameter $ \\lambda $\n\n### Examples\nThe exponential distribution is defined as the continuous probability distribution of the time between events in a Poisson process.\n\n##### Some common uses: \n* Measuring the expected time for an event to occur such as in Physics in measuring radioactive decay\n* Product Reliability (the amount of time a product lasts)\n\n### Assumptions\n* Continuous\n* $ \\lambda > 0 $ \n* Events occur independently at a constant average rate\n* Memoryless\n\n### Cumulative Distribution Function\n#### $ F(x) = P(X \\le x) = 1 - e^{-\\lambda x}, x\\ge 0 $\n\n### Inverse Cumulative Distribution Function\n#### $ F^{-1}(x) = -\\frac{1}{\\lambda} \\ln(1 - x)  $\n\n### Inverse Transform \n#### Letting $ X = F_X^{-1}(U)$\n\n#### $ F^{-1}(U) = -\\frac{1}{\\lambda} \\ln(1 - U)  $\nor\n#### $ F^{-1}(U) = -\\frac{1}{\\lambda} \\ln(U)  $\n\n","de87a000":"### Plot Both Distributions","1f054341":"# Weibull Distribution Random Variate Generation #\n\n## $ X \\sim Weibull(\\alpha, \\beta) $\nWith scale parameter $ \\alpha $ and shape parameter $ \\beta $\n\n### Examples\nThe Weibull distribution is continuous probability distribution primarily used in reliability and survival analysis to model the lifetime of an object, organism, or service time.\n\n### Assumptions\n* Continuous\n* $ \\alpha > 0 $\n* $ \\beta > 0 $\n* A value of $ \\beta < 1 $ indicates that the failure rate decreases over time\n* A value of $ \\beta = 1 $ indicates that the failure rate is constant over time\n* A value of $ \\beta > 1 $ indicates that the failure rate increases over time \n\n### Cumulative Distribution Function\n#### $ F(x) = P(X \\le x) = 1 - e^{-(1\/\\alpha)x^{\\beta}},  x > 0 $\n\n### Inverse Transform\n#### $ F^{-1}(U) = [-\\alpha  ln(U)]^{1\/\\beta}, 0 < U < 1 $\n","53aa8ae8":"# Random Variate Generation Techniques #\nBryan Ciotola\n\n## Purpose \n\nMost, if not all programming languages offer simple ways of generating random variates of widely used probability distributions, but how exactly are these random variates being generated on a computational level? Our goal is to strip away the abstraction and explore the beauty that is hiding behind the surface.\n\n### Steps to Generate Random Variates\n   1. Generate uniform pseudo-random numbers $ U \\sim Unif(0,1) $\n   2. Manipulate these numbers using various techniques\n\n**Note**: \n*The emphasis and body of this document will focus on the methods and routines of manipulating pseudo-random numbers, but we will provide a brief introduction into pseudo-random number generators as well as showcase one bad generator.*","70e10d8b":"# Poisson Distribution Random Variate Generation\n\n## $ X \\sim Poisson(\\lambda) $\nwhere $ \\lambda = rate = E(X) = Var(X) $\n### Probability Mass Function\n#### $ P(X=x) = e^{-\\lambda}\\frac{\\lambda^{x}}{x!}, x = 0,1,2,... $\n\n### Cumulative Distribution Function\n#### $ F(x) = P(X \\le x) = \\frac{\\Gamma(x+1, \\mu)}{\\Gamma(x+1)}, x = 0,1,2,... $\n\n### Examples\nThe Poisson distribution is a discrete probability distribution that can be used to model the number of events in a fixed interval of time or space.\n* The number of customer arrivals in an hour \n* The number of stop signs on a street\n* The number of earthquakes that occur in a decade\n\n### Assumptions\n* Events must occur with a known constant mean rate\n* Events occur independently of one another\n\n### Algorithm\nThere are several ways of generating random samples from a Poisson distribution, and the method we will use is a variation of the Acceptance-Rejection method.\nThe idea behind this algorithm is that the inter-arrival times of a homogeneous Poisson process ($ A_i $) form independent exponentially distributed random variables. We can then generate exponential variables by the inverse transform method and then through the use of logarithmic properties take the product of them:\n\n\n#### $ \\sum_{i=1}^{n} A{i} \\le \\sum_{i=1}^{n+1} A{i} $\n\n#### $ =\\frac{-1}{\\lambda} ln(\\prod_{i=1}^{n}{U_i}) \\le 1 \\le \\frac{-1}{\\lambda} ln(\\prod_{i=1}^{n+1}{U_i}) $\n\n### $ =\\prod_{i=1}^{n}{U_i} \\ge e^{-\\lambda} > \\prod_{i=1}^{n+1}{U_i} $\n\n\n### Pseudocode:\n\n    a = e^-lambda\n    p = 1\n    n = -1\n    \n    do:\n        n = n + 1\n        Generate U from U(0,1)\n        p = p*U\n    \n    while p < a:\n          \n    Return n","3f2d4831":"### Implement Maximizing Function","d1ab5be9":"# Random Variate Generation Techniques\n> ### We can generate random observations (variates) of **ANY** probability distribution simply by manipulating independent and identically uniformly distributed random numbers between 0 and 1 ","56b1b832":"### Sampling Using Acceptance-Rejection Method ","9c80899d":"# Geometric Distribution Random Variate Generation #\n\n## $ X \\sim Geometric(p) $\nWith probability of success $ p $ and probability of failure $ q $ or $ 1 - p $\n\n### Examples\nThe geometric distribution can be used to model the number of failures before the first success in repeated mutually independent Bernoulli trials, each with probability of success $ p $. \n\nFor example, the geometric distribution with $ p = 1\/36 $ would be an appropriate model for the number of rolls of a pair of fair dice prior to rolling the first double 1 (\"snake eyes\").\n\n\n### Assumptions\n* Discrete\n* There are two possible outcomes for each trial (success or failure)\n* The trials are independent\n* The probability of success is the same for each tria\n\n\n### Probability Mass Function\n#### $ P(X = x) = p q^{x-1} $\n\nWhere $ x \\ge 1 $ and $ q = (1-p) $\n\n\n### Cumulative Distribution Function\n#### $ F(x) = P(X \\le x) = 1 - q^x $\n\nUsing the discrete case of the inverse transform method, we want to solve the inequality where:\n\n#### $ 1 - q^{x-1} \\le U < 1-q^j $\n\nAfter some algebra and mathematical intuition, we can generalize that:\n\n### $ X = \\left \\lceil \\frac{\\ln(U)}{\\ln(1-p)} \\right \\rceil $\n","6752b6b5":"### Bibliography\n\nDevroye, Luc. Non-Uniform Random Variate Generation. Springer-Verlag, 2013.\n\nFoppa, Ivo M. A Historical Introduction to Mathematical Modeling of Infectious Diseases: Seminal Papers in Epidemiology. Academic Press Is an Imprint of Elsevier, 2017.\n\nGoldsman, David. \u201cRandom Number Generation.\u201d ISYE 6644 - Simulation, Georgia Institute of Technology\n\nGoldsman, David. \u201cRandom Variate Generation.\u201d ISYE 6644 - Simulation, Georgia Institute of Technology\n\nGoldsman, David. \u201cMore Random Variate Generation.\u201d ISYE 6644 - Simulation, Georgia Institute of Technology\n\nInverse Transform Sampling, Five Minute Stats, 2 Feb. 2016, https:\/\/stephens999.github.io\/fiveMinuteStats\/inverse_transform_sampling.html. \n\nJianhua, Zhao. \u201cChapter 3: Methods for Generating Random Variables.\u201d https:\/\/www.ynufe.edu.cn\/pub\/tsxy\/jhzhao\/teach\/CS\/notes\/lec3.pdf. \n\nKeeler, Paul. Simulating Poisson Random Variables \u2013 Direct Method, 5 Dec. 2019, https:\/\/hpaulkeeler.com\/simulating-poisson-random-variables-direct-method\/. \n\nKristiadi, Agustinus. Rejection Sampling, 2021, https:\/\/agustinus.kristia.de\/techblog\/2015\/10\/21\/rejection-sampling\/. \n\nLeemis, Larry. \u201cUnivariate Distribution Relationships.\u201d Univariate Distribution Relationship Chart, http:\/\/www.math.wm.edu\/~leemis\/chart\/UDR\/UDR.html.\n\nWikipedia, Wikimedia Foundation, 3 Feb. 2021, https:\/\/www.wikipedia.org\/. \n\nRoss, Sheldon M. Simulation. Academic Press, 2013. \n\nSarkar, Tirthajyoti. How to Generate Random Variables from Scratch (No Library Used), Towards Data Science, 23 May 2020, https:\/\/towardsdatascience.com\/how-to-generate-random-variables-from-scratch-no-library-used-4b71eb3c8dc7. \n\nSigman, Karl. \u201cAcceptance-Rejection Method.\u201d 4404-Notes-ARM.pdf. http:\/\/www.columbia.edu\/~ks20\/4404-Sigman\/4404-Notes-ARM.pdf. \n\nSigman, Karl. \u201cInverse Transform Method.\u201d 4404-Notes-ITM.pdf. http:\/\/www.columbia.edu\/~ks20\/4404-Sigman\/4404-Notes-ITM.pdf. \n\nViadinugroho, Raden Aurelius Andhika. Generate Random Variable Using Inverse Transform Method in Python, Towards Data Science, 17 Mar. 2021, https:\/\/towardsdatascience.com\/generate-random-variable-using-inverse-transform-method-in-python-8e5392f170a3. \n\n","2ba41d0a":"# Summary\nIn this study, we explored the Inverse Transform and Acceptance-Rejection methods for generating random variates from both continuous and discrete probability distributions. We were able to generate these variates simply by having access to Uniform(0,1) random numbers and without the explicit use of programming language libraries.\n\n![@jbum](https:\/\/media.giphy.com\/media\/3ohzdDGQNIbDsTMS1W\/giphy.gif)\n\n*Source: Giphy*\n\n### Further Exploration\n* The Convolution Method\n* The Composition Method\n* Special Case Methods such as Box-Muller and Polar methods to generate variates of Normal distribution","f9921003":"# Normal Distribution Random Variate Generation\n*(note we are using Scipy.stats library to generate our distributions here to easily illustrate the the Acceptance-Rejection method)*\n\n## $ X \\sim Nor(\\mu, \\sigma^{2}) $\n\nwhere $ \\mu = mean $ and $ \\sigma^{2} = variance $ \n\n### Examples\nThe normal distribution is a continuous probability distribution that can be used for modeling adult shoe size, income distribution in an economy, new-born baby weights, etc. It is perhaps the most ubiquitous probability distribution. \n\nNormal distributions are important in statistics and are often used to represent random variables whose distributions are not known. Their importance is partly due to the central limit theorem that states that under some conditions, the average of many samples of a random variable is itself a random variable whose distribution converges to a normal distribution as the number of samples increases. \n\n### Probability Density Function\n![@Wikipedia](attachment:1ad1454d-2c56-44e6-bb23-c4f61099360d.PNG)\n\n### Cumulative Distribution Function\n![@Wikipedia](attachment:15031c2b-2a17-4d6f-ad3a-42875456c87e.PNG)\n\n\n### Algorithm\nSuppose we want to sample from $ Nor(3,2.5) $ distribution using the T-distribution\n\nLet's choose\n* $ f(s) \\sim Nor(3,2.5) $\n* $ g(s) \\sim T(2) $\n* Find our scaling number $ M $\n    1. Let $ U \\sim Unif(0,1) $\n    2. Let $ Y \\sim g(Y) $\n    3. Accept sample if $ U \\le f(s)\/Mg(s) $\n\n","e254d94a":"# Bernoulli Distribution Random Variate Generation #\n\n## $ X \\sim Bernoulli(p) $\n\n### Probability Mass Function\n#### $ P(X = x) = p^{x}(1-p)^{1-x} $\n\nWhere $ x = 0, 1 $ and \nprobability of success $ p $\n\n\n### Cumulative Distribution Function\n<h4> $\n  X=\\begin{cases}\n    0, & \\text{if $ x < 0 $}.\\\\\n    1-p, & \\text{if $ 0 \\le x < 1 $}.\\\\\n    1, & \\text{if $ x \\ge 1 $}.\n  \\end{cases}\n$ <\/h4>\n\n### Examples\nThe Bernoulli distribution is a discrete probability distribution associated with the notion of a Bernoulli trial, which is an experiment with exactly two outcomes: *success and failure*.\n\nThe performance of a fixed number of trials with fixed probability of success on each trial is a Bernoulli trial. The distribution of heads and tails in coin tossing, for example, is a Bernoulli distribution with p = 1\/2.\n\n### Assumptions\n* Discrete\n* Only two outcomes: 0 or 1 (failure or success)\n\n\n### Algorithm\n* Generate $ U \\sim Unif(0,1) $\n* Set X = 0 if $ U \\le 1 - p $\n* X = 1 if  $ U > 1 - p $","dc5f92f1":"### Import Libraries\n*Before we get to it let's import all the libraries we will need for our code to run below. We will also call some more specific libraries in their respective code blocks below*","f00274b4":"### Approach \n\nIn this notebook we will use Uniform[0,1] random numbers to generate random variates from both continuous and discrete probability distributions using:\n* The Inverse Transform Method\n* Acceptance-Rejection Method\n\n\n\n","06266b0f":"# The Inverse Transform Method\n\n### Overview\nSuppose we want to generate a random variable X with cumulative distribution function \n\n<h4> $ F_X(x) = P(X \\le x) $ <\/h4>\n\nAll we need to do is generate a random variable U that is uniformly distributed in (0,1) and set\n\n<h4> $ X = F_X^{-1}{U} $ <\/h4>\n\n\n\n### Algorithm \n\n#### Continuous Case\n\nAssume we want to generate a random variable X with cumulative distribution function $ F_X(x) = P(X \\le x) $. We then want to:\n1. Generate <h4> $ U \\sim Unif(0,1) $ <\/h4>\n2. Let <h4> $ X = F_X^{-1}(U) $ <\/h4>\n\n#### Discrete Case\n\nAssume we want to generate a random variable X with probability mass function $ P(X = x_i) = p_i $. We then want to:\n1. Generate $ U \\sim Unif(0,1) $\n2. Determine the index $ i $ such that <h3> $ \\sum_{i=1}^{k-1}{p_i} < U \\le \\sum_{i=1}^{k}{p_i}$ <\/h3>\n\n   and return <h3> $ X = x_k $ <\/h3> \n\nPerhaps an easier way to understand this is to look at it a little differently:\n\nGenerate a random variable U where U is uniformly distributed in (0,1) and set\n\n<h3> $ X = \\begin{cases}\nx_1, & \\text{if $ U \\le p_i $}.\\\\\n    x_k, & \\text{if $ \\sum_{i=1}^{k-1}{p_i} < U \\le \\sum_{i=1}^{k}{p_i}$}.\n\\end{cases} $ <\/h3> \n\n### Benefits\/Drawbacks\n* Can be used in practice as long as we are able to get a formula for $ F^{-1}{(x)} $\n* For continuous distribution, calculating a closed form cumulative distribution function is impossible to do analytically for most distributions, and thus this method can be computationally inefficient compared to other existing methods","e6959475":"# Acceptance-Rejection Method for Generating Random Variates\n\n## Overview\n\nIn many cases, the target distribution's CDF is either not easily invertible or inverting it in the first place is computationally expensive. The Acceptance-Rejection method is a simple algorithm to use in these cases. We start by assuming that our probability distribution $ F $ with pdf $ f(s) $ is not easy to sample from. \n\n### How Do We Sample From f(s)?\n* The idea then is that we want to find an alternative, easy to sample from, probability distribution $ G $ with pdf $ g(s) $ that is **\"close\"** to $ f(s) $\n* We then scale $ g(s) $ by $ M $ so that it is \"above\" $ f(s) $\n\n### Algorithm\n\n1. Let $ U \\sim Unif(0,1) $\n2. Let $ Y \\sim g(y) $ \n3. If $ U \\le \\frac{f(s)}{M*g(s)} $ then $ X=Y $; else reject and go to step 1\n\n### Benefits\/Drawbacks\n* Can lead to a lot of rejected samples (inefficient)\n* Can be difficult to select to most optimum alternative probability distribution and scaling function M"}}