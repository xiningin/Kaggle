{"cell_type":{"f8354fb0":"code","0f2fd03e":"code","1f893e20":"code","4b32bf2c":"code","a40ea1e3":"code","dbd0c74c":"code","10b59bf6":"code","49f48138":"code","42837456":"code","62414c93":"code","f826ff75":"code","e4e79bee":"code","72f32e26":"code","68d39989":"code","f610257f":"code","d342ca03":"code","9ccdeab3":"code","921579ff":"code","37aa5f51":"code","5eff1a33":"code","fb130144":"code","a4919c03":"code","2ee2cbec":"code","3de1fdf2":"code","0639dea8":"code","2c6160cb":"code","4064d300":"code","44f5b810":"code","a8eef064":"code","a5743d4e":"code","8c4e2263":"code","26ad3349":"code","f7a54a26":"code","0007e4a3":"code","994d633c":"markdown","570084e8":"markdown","17d4ddc2":"markdown","d99d2ce9":"markdown","1edba86a":"markdown","3f734a12":"markdown","736478ba":"markdown","fd8b22f4":"markdown","6b6846a0":"markdown","272de544":"markdown","f06e85db":"markdown","5479f0d4":"markdown","500630f0":"markdown","b9e18b0b":"markdown","77c7f6ca":"markdown","75ceb418":"markdown","e1eaebc0":"markdown","efb05d77":"markdown","42bd1bcb":"markdown","cd18d393":"markdown","da9b2e1b":"markdown","75a96aee":"markdown","d08e75d3":"markdown","b8a53612":"markdown"},"source":{"f8354fb0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0f2fd03e":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","1f893e20":"df = pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')\ndf.head(5)","4b32bf2c":"df.isna().sum()","a40ea1e3":"df.describe()","dbd0c74c":"df.info()","10b59bf6":"plt.figure(figsize=(25, 15))\n\nfor i, column in enumerate(df.columns):\n    plt.subplot(4, 6, i + 1)\n    sns.histplot(data=df[column])\n    \nplt.tight_layout()\nplt.show()","49f48138":"(df > 0).all(1)","42837456":"plt.figure(figsize=(20,10))\nsns.boxplot(data = df,palette = \"Set1\")\nplt.xticks(rotation=90)\nplt.show()","62414c93":"# Function to set upper and lower bound to 3rd standard deviation and remove outliers\n\ndef removeOutlier(att, df):\n\n    lowerbound = att.mean() - 3 * att.std()\n    upperbound = att.mean() + 3 * att.std()\n    print('lowerbound: ',lowerbound,' -------- upperbound: ', upperbound )\n\n    df1 = df[(att > lowerbound) & (att < upperbound)]\n\n    print((df.shape[0] - df1.shape[0]), ' number of outliers from ', df.shape[0] )\n    print(' ******************************************************\\n')\n    \n    df = df1.copy()\n\n    return df","f826ff75":"df = removeOutlier(df.histogram_variance, df)\ndf = removeOutlier(df.histogram_median, df)\ndf = removeOutlier(df.histogram_mean, df)\ndf = removeOutlier(df.histogram_mode, df)\ndf = removeOutlier(df.percentage_of_time_with_abnormal_long_term_variability, df)\ndf = removeOutlier(df.mean_value_of_short_term_variability, df)","e4e79bee":"df.shape","72f32e26":"plt.figure(figsize=(20,10))\nsns.boxplot(data = df,palette = \"Set1\")\nplt.xticks(rotation=90)\nplt.show()","68d39989":"plt.figure(figsize=(20,10))\ng = sns.heatmap(df.corr(),annot = True)","f610257f":"sns.lmplot(data =df,x=\"baseline value\",y=\"fetal_movement\", hue=\"fetal_health\", legend_out=False)\nplt.show()","d342ca03":"sns.lmplot(data =df,x=\"abnormal_short_term_variability\",y=\"fetal_movement\", hue=\"fetal_health\",legend_out=False)\nplt.show()","9ccdeab3":"sns.lmplot(data =df,x=\"prolongued_decelerations\",y=\"fetal_movement\", hue=\"fetal_health\",legend_out=False)\nplt.show()","921579ff":"sns.lmplot(data =df,x=\"percentage_of_time_with_abnormal_long_term_variability\",y=\"fetal_movement\", hue=\"fetal_health\",legend_out=False)\nplt.show()","37aa5f51":"sns.lmplot(data =df,x=\"histogram_min\",y=\"fetal_movement\", hue=\"fetal_health\",legend_out=False)\nplt.show()","5eff1a33":"sns.countplot(x=\"fetal_health\",data = df)\nplt.show()","fb130144":"df.fetal_health.value_counts()","a4919c03":"from sklearn.utils import resample\n\n# Separate Target Classes\ndf_1 = df[df.fetal_health==1]\ndf_2 = df[df.fetal_health==2]\ndf_3 = df[df.fetal_health==3]\n \n# Upsample minority class\ndf_2_upsampled = resample(df_2, \n                                 replace=True,     # sample with replacement\n                                 n_samples=1601,    # to match majority class\n                                 random_state=123) # reproducible results\n\ndf_3_upsampled = resample(df_3, \n                                 replace=True,     # sample with replacement\n                                 n_samples=1601,    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_1, df_2_upsampled, df_3_upsampled])\n \n# Display new class counts\ndf_upsampled.fetal_health.value_counts()","2ee2cbec":"x = df_upsampled.drop('fetal_health', axis = 1)\ny = df_upsampled['fetal_health'] \nx.head()","3de1fdf2":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.25, random_state = 0)","0639dea8":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","2c6160cb":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\nfrom mlxtend.plotting import plot_confusion_matrix\n\ndef evaluator(y_test, y_pred):    \n    \n    # Accuracy:\n    print('Accuracy is: ', accuracy_score(y_test,y_pred))\n    print('')\n    # Classification Report:\n    print('Classification Report: \\n',classification_report(y_test,y_pred))\n\n    print('Confusion Matrix: \\n\\n')\n    plt.style.use(\"ggplot\")\n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(conf_mat = cm,figsize=(8,6),show_normed=True)","4064d300":"model_accuracy = pd.DataFrame(columns=['Model','Accuracy'])\nmodels = {\n          \"KNN\" : KNeighborsClassifier(),\n          \"DT\" : DecisionTreeClassifier(),\n          'RFC' : RandomForestClassifier(),\n          'GBC' : GradientBoostingClassifier(),\n          'XGB' : XGBClassifier()\n          }\n\nfor test, clf in models.items():\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    acc = accuracy_score(y_test,y_pred)\n    train_pred = clf.predict(x_train)\n    train_acc = accuracy_score(y_train, train_pred)\n    print(\"\\n\", test + ' scores')\n    print(acc)\n    print(classification_report(y_test,y_pred))\n    print(confusion_matrix(y_test,y_pred))\n    print('*' * 100,\"\\n\")\n    model_accuracy = model_accuracy.append({'Model': test, 'Accuracy': acc, 'Train_acc': train_acc}, ignore_index=True)","44f5b810":"model_accuracy.sort_values(ascending=False, by = 'Accuracy')","a8eef064":"from xgboost import XGBClassifier\n\ndt_classifier = XGBClassifier()\n\ndt_classifier.fit(x_train,y_train)","a5743d4e":"pred_dt = dt_classifier.predict(x_test)\n\nevaluator(y_test, pred_dt)","8c4e2263":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier()\n\nrf_classifier.fit(x_train,y_train)","26ad3349":"pred_rf = rf_classifier.predict(x_test)\n\nevaluator(y_test, pred_rf)","f7a54a26":"important_features = pd.DataFrame({'Features': x.columns, \n                                   'Importance': rf_classifier.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()\nprint(important_features)","0007e4a3":"input = np.array([[134.0,\t0.003,\t0.0,\t0.008,\t0.003,\t0.0,\t0.0,\t16.0,\t2.4,\t0.0,\t23.0,\t117.0,\t53.0,\t170.0,\t11.0,\t0.0,\t137.0,\t134.0,\t137.0,\t13.0,\t1.0]])\nz=rf_classifier.predict(input)\nprint(z)\n\nfor i in range(len(z)):\n  if z[i]== 1:\n    print(\"Normal\")\n  elif z[i]==2:\n    print(\"Suspect\")\n  else:\n    print(\"Pathological\")","994d633c":"# Feature Scaling: Standardization","570084e8":"Since the mean and median of all the columns are similar we have good quality data","17d4ddc2":"# Splitting the Dataset","d99d2ce9":"A lot of Outliers are still present in the dataset.","1edba86a":"# Looking for outliers in the data","3f734a12":"Balancing Dataset:","736478ba":"Accuracies of the following models for the dataset is comapared. We will select the one with the highest accuracy score.\nSince our classification was highly imbalanced, we will also have to look at the precision, recall, F1 scores.","fd8b22f4":"# Loading Dataset","6b6846a0":"# Create function to compare accuracy measures for various algorithms","272de544":"# Importing Libraries","f06e85db":"As seen in the heatmap and the above chart, abnormal_short_term_variability, prolongued decelerations and percentage_of_time_abnormal_long_term_variability do affect fetal health significantly. But RFT has also helped in highlighting other features which are important which we didn't observe during the EDA phase.\nAlso, keeping in mind that this dataset is from the medical field it will be ignorant to drop certain features without seeking advice from a doctor. ","5479f0d4":"we have 21 features and One Label column","500630f0":"# Description of the Data","b9e18b0b":"# Let us see how variables with positive correlations affect each class","77c7f6ca":"# Random Forest Classifier:","75ceb418":"Data\nThis dataset contains 2126 records of features extracted from Cardiotocogram exams, which were then classified by three expert obstetritians into 3 classes\n\n\n1.   Normal\n2.   Suspect\n3. Pathalogical","e1eaebc0":"# Checking for number of missing values in each column\n\n","efb05d77":"# Separating Fetures and Target Variable","42bd1bcb":"Since we have sufficient data, Oversampling is done to give better accuracy.\nHere we duplicate examples from the minority class","cd18d393":"# Distribution of Target class: Highly imbalanced","da9b2e1b":"Important Features\nRandom forest helps in getting the feature importance","75a96aee":"# XGB Classifier","d08e75d3":"We can see that below features show positive correlation:\nbaseline_value,\nabnormal_short_term_variability,\nprolongued_decelerations,\npercentage_of_time_with_abnormal_long_term_variability,\nhistogram_min\n\nWe will use randon forest algorithm to create a feature importance table\n\nDue to lack of domain knowledge, it is better not to drop an features.","b8a53612":"# Correlation HeatMap"}}