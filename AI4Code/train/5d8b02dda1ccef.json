{"cell_type":{"fa039c6e":"code","7c628742":"code","1f044bfe":"code","c92d05f9":"code","9da52b9c":"code","12eac991":"code","d77f1c64":"code","f32bcc22":"code","19b72dfe":"code","e5f3f828":"code","835fbfd9":"code","ce41898f":"code","5b89e67b":"code","e924b995":"code","14db4e1b":"code","6a5fb253":"code","b047de36":"code","df76cd5f":"code","5aa18acb":"code","326a4788":"code","86d06100":"code","0c859495":"code","d46c80f9":"code","3d9b6bf9":"code","ed6a0213":"code","dfc98d9d":"code","6b40f2b0":"code","e96fcf2a":"code","f2549863":"code","908ada12":"code","d003645c":"code","5518f138":"code","4f1fe01e":"code","95e58076":"code","36417bba":"code","0c94369f":"code","cf0a42db":"code","d5abdc89":"code","483f8673":"code","4f443487":"code","6e02aad6":"code","b4267976":"code","ab8d8314":"code","926dff4f":"code","a09c51e3":"code","bafd9b1a":"markdown","0df07178":"markdown","a726070a":"markdown","3703000e":"markdown","1607c4ee":"markdown","4304cefd":"markdown","3fc557d8":"markdown","57251b9e":"markdown","1a3a5a54":"markdown","d9b8fe37":"markdown","21babd29":"markdown","0b9f8664":"markdown","ecd8c6a9":"markdown","24ff95e6":"markdown","7e29f586":"markdown","ef1d177c":"markdown","54795fe9":"markdown","fca36c7e":"markdown","77331d11":"markdown","7b71d9c5":"markdown"},"source":{"fa039c6e":"# Installing mandatory libraries\n! pip install profanity_check \n! pip install tld","7c628742":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard\n\n# Clear any logs from previous runs\n!rm -rf .\/logs\/ \n\n# Common imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport sklearn\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"C3_Deep Learning Classification\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"imgs\", CHAPTER_ID)\n\ndef image_path(fig_id):\n    return os.path.join(PROJECT_ROOT_DIR, \"imgs\", CHAPTER_ID, fig_id)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"svg\", resolution=300):\n    if not os.path.exists(IMAGES_PATH):\n        os.makedirs(IMAGES_PATH)\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension)","1f044bfe":"# Verifying pathname of dataset before loading\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename));\n        print(os.listdir(\"..\/input\"))","c92d05f9":"# Load Datasets\ndef loadDataset(file_name):\n    df = pd.read_csv(file_name,engine = 'python')\n    return df\n\ndf_train = loadDataset(\"\/kaggle\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_train_data.csv\/Webpages_Classification_train_data.csv\")\ndf_test = loadDataset(\"\/kaggle\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_test_data.csv\/Webpages_Classification_test_data.csv\")\n#Ensuring correct sequence of columns & dropping IP Address\ndf_train = df_train[['url','url_len','geo_loc','tld','who_is','https','js_len','js_obf_len','label']]\ndf_test = df_test[['url','url_len','geo_loc','tld','who_is','https','js_len','js_obf_len','label']]","9da52b9c":"#vectorising the URL Text\nfrom urllib.parse import urlparse\nfrom tld import get_tld\n\nstart_time= time.time()\n#Function for cleaning the URL text before vectorization\ndef clean_url(url):\n    url_text=\"\"\n    try:\n        domain = get_tld(url, as_object=True)\n        domain = get_tld(url, as_object=True)\n        url_parsed = urlparse(url)\n        url_text= url_parsed.netloc.replace(domain.tld,\" \").replace('www',' ') +\" \"+ url_parsed.path+\" \"+url_parsed.params+\" \"+url_parsed.query+\" \"+url_parsed.fragment\n        url_text = url_text.translate(str.maketrans({'?':' ','\\\\':' ','.':' ',';':' ','\/':' ','\\'':' '}))\n        url_text.strip(' ')\n        url_text.lower()\n    except:\n        url_text = url_text.translate(str.maketrans({'?':' ','\\\\':' ','.':' ',';':' ','\/':' ','\\'':' '}))\n        url_text.strip(' ')\n    return url_text\n\ndf_test['url'] = df_test['url'].map(clean_url)\ndf_train['url'] = df_train['url'].map(clean_url)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","12eac991":"# give profanity score to each URL using the Profanity_Check Library\nfrom profanity_check import predict_prob\n\nstart_time= time.time()\n#Function for calculating profanity in a dataset column\ndef predict_profanity(df):\n    arr=predict_prob(df['url'].astype(str).to_numpy())\n    arr= arr.round(decimals=3)\n    df['url'] = pd.DataFrame(data=arr,columns=['url'])\n    #df['url']= df_test['url'].astype(float).round(decimals=3) #rounding probability to 3 decimal places\n    return df['url']\n\ndf_train['url']= predict_profanity(df_train)\ndf_test['url']= predict_profanity(df_test)\n\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","d77f1c64":"#Looking for NaN, if any\nprint(df_train['url'].isnull().sum())\nprint(df_test['url'].isnull().sum())","f32bcc22":"#Rename 'url' to 'url_vect'\ndf_train.rename(columns = {\"url\": \"url_vect\"}, inplace = True)\ndf_test.rename(columns = {\"url\": \"url_vect\"}, inplace = True)\ndf_train","19b72dfe":"#converting 'label' values to numerical value for classification\nimport time\n\nstart_time= time.time()\n\ndf_test['label'].replace(to_replace =\"good\", value =1, inplace=True)\ndf_train['label'].replace(to_replace =\"good\", value =1, inplace=True)\ndf_test['label'].replace(to_replace =\"bad\", value =0, inplace=True)\ndf_train['label'].replace(to_replace =\"bad\", value =0, inplace=True)\n\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","e5f3f828":"df_test","835fbfd9":"df_train","ce41898f":"# No of Classes in Label\ndf_train['label'].unique()","5b89e67b":"# Class Distribution of Labels\ndf_train.groupby('label').size()","e924b995":"# Analysis of Postives and Negatives in the Dataset\nneg, pos = np.bincount(df_train['label'])\ntotal = neg + pos\nprint ('Total of Samples: %s'% total)\nprint('Positive: {} ({:.2f}% of total)'.format(pos, 100 * pos \/ total))\nprint('Negative: {} ({:.2f}% of total)'.format(neg, 100 * neg \/ total))","14db4e1b":"#Class Labels shown in Histogram\ndf_train[\"label\"].hist()\n#save_fig(\"Fig2\")","6a5fb253":"# Representation of Labels in the Stack form\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# create dummy variable then group by that\n# set the legend to false because we'll fix it later\ndf_train.assign(dummy = 1).groupby(['dummy','label']).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).to_frame().unstack().plot(kind='bar',stacked=True,legend=False, color={'grey','white'}, linewidth=0.50, ec='k')\n# or it'll show up as 'dummy' \nplt.xlabel('Benign\/Malicious Webpages')\n# disable ticks in the x axis\nplt.xticks([])\n# fix the legend or it'll include the dummy variable\ncurrent_handles, _ = plt.gca().get_legend_handles_labels()\nreversed_handles = reversed(current_handles)\ncorrect_labels = reversed(['Malicious','Benign'])\nplt.legend(reversed_handles,correct_labels)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n#save_fig(\"Fig3\")\nplt.show()","b047de36":"#Selection lower numbers as of now for fast testing\n\ntrain= df_train.iloc[:1000000,]\nval= df_train.iloc[1000001:,]\ntest= df_test.iloc[0:,]\n\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","df76cd5f":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers","5aa18acb":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  dataframe = dataframe.copy()\n  labels = dataframe.pop('label')\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  return ds","326a4788":"#Using a Batch Size of 2048 and copying data to tf.data dataset\nbatch_size = 2048\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","86d06100":"# For checking the batch\n#example_batch = next(iter(train_ds))[0]\n#example_batch","0c859495":"feature_columns = []\n\n# numeric cols\nfor header in ['url_vect', 'url_len', 'js_len', 'js_obf_len']:\n  feature_columns.append(feature_column.numeric_column(header))\n\n#Categorical Columns\nwho_is = feature_column.categorical_column_with_vocabulary_list('who_is', ['complete', 'incomplete'])\nwho_is_one_hot = feature_column.indicator_column(who_is)\nhttps = feature_column.categorical_column_with_vocabulary_list('https', ['yes', 'no'])\nhttps_one_hot = feature_column.indicator_column(https)\nfeature_columns.append(https_one_hot)\nfeature_columns.append(who_is_one_hot)\n\n# Hashed Categorical Columns\ngeo_loc_hashed = feature_column.categorical_column_with_hash_bucket('geo_loc', hash_bucket_size=230)\ntld_hashed = feature_column.categorical_column_with_hash_bucket('tld', hash_bucket_size=1200)\ngeo_loc_indicator = feature_column.indicator_column(geo_loc_hashed)\ntld_indicator = feature_column.indicator_column(tld_hashed)\nfeature_columns.append(tld_indicator)\nfeature_columns.append(geo_loc_indicator)\n\n#Creating Feature Layer\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)","d46c80f9":"#Using Initial Bias to overcome Class Imbalance\ninitial_bias = np.log([pos\/neg])\ninitial_bias","3d9b6bf9":"#Making a Tensorflow Model\nfrom tensorflow import keras\n\nMETRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n]\n\ndef make_model(metrics = METRICS, output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    model = keras.Sequential([\n        feature_layer,\n        layers.Dense(128, activation='relu'),\n        layers.Dense(32, activation='relu'),\n        keras.layers.Dropout(0.1),\n        keras.layers.Dense(1, activation='sigmoid',\n        bias_initializer=output_bias),\n    ])\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(lr=1e-3),\n        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n        metrics=metrics)\n    return model","ed6a0213":"#Initialize the Model\nmodel = make_model()\nmodel_initial_bias = make_model(output_bias = initial_bias)","dfc98d9d":"# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1 \/ neg)*(total)\/2.0 \nweight_for_1 = (1 \/ pos)*(total)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","6b40f2b0":"#Fitting the Model with Class Weights\nfrom datetime import datetime\n\n#Defining Early Stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy', \n    verbose=1,\n    patience=20,\n    mode='max',\n    restore_best_weights=True)\n\n# Define the Keras TensorBoard callback.\nlogdir=\"logs\/fit\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n\nstart_time= time.time()\nzero_bias_history = model.fit(train_ds, epochs=40,validation_data=val_ds,\n          callbacks=[early_stopping,tensorboard_callback],\n          class_weight=class_weight\n         )\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","e96fcf2a":"#Fitting the Model with Class Weights & Inital Bias\nfrom datetime import datetime\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy', \n    verbose=1,\n    patience=20,\n    mode='max',\n    restore_best_weights=True)\n\n# Define the Keras TensorBoard callback.\nlogdir=\"logs\/fit\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n\nstart_time= time.time()\ninitial_bias_history = model_initial_bias.fit(train_ds, epochs=40,validation_data=val_ds,\n          callbacks=[early_stopping,tensorboard_callback],\n          class_weight=class_weight\n         )\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","f2549863":"# The final model Summary\nmodel_initial_bias.summary()","908ada12":"#Accuracy & Loss - Class Weights w\/o Initial Bias\nresults = model.evaluate(test_ds)\n#print(\"Accuracy\", accuracy)\nprint(\"Loss: {0}, Accuracy: {1}\".format(results[0],results[5]))","d003645c":"#Accuracy & Loss - Class Weights with Initial Bias\nstart_time= time.time()\nresults = model_initial_bias.evaluate(test_ds)\n#print(\"Accuracy\", accuracy)\nprint(\"Loss: {0}, Accuracy: {1}\".format(results[0],results[5]))\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","5518f138":"start_time= time.time()\nX_test, y_test = iter(test_ds).next()\nX_train, y_train = iter(train_ds).next()\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","4f1fe01e":"#Confusion Matrix\n\nstart_time= time.time()\ny_pred=model_initial_bias.predict_classes(X_test)\ncon_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()\ncon_mat_norm = np.around(con_mat.astype('float') \/ con_mat.sum(axis=1)[:, np.newaxis], decimals=4)\ncon_mat_df = pd.DataFrame(con_mat_norm,index = ['Benign','Malicious'], columns = ['Benign','Malicious'])\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","95e58076":"# Confusion Matrix\ncon_mat_df","36417bba":"# Plotting the Confusion Matrix Using Matploit & Seaborn\nimport seaborn as sns\n\nstart_time= time.time()\nfigure = plt.figure(figsize=(6,6))\nsns.heatmap(con_mat_df,annot=True,cmap=plt.cm.binary,fmt='g',linewidths=0.50,linecolor='black')\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n#save_fig(\"Fig7\")\nplt.show()\n\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","0c94369f":"mpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\ndef plot_loss(history, label, n):\n    # Use a log scale to show the wide range of values.\n    plt.semilogy(history.epoch,  history.history['loss'],\n               color=colors[n], label='Train '+label)\n    plt.semilogy(history.epoch,  history.history['val_loss'],\n          color=colors[n], label='Val '+label,\n          linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()","cf0a42db":"plot_loss(zero_bias_history, \"Zero Bias\", 0)\nplot_loss(initial_bias_history, \"Initial Bias\", 1)\n#save_fig(\"Fig8\")","d5abdc89":"def plot_metrics(history):\n    metrics =  ['loss', 'auc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n\n        plt.legend()\n","483f8673":"plot_metrics(initial_bias_history)","4f443487":"def plot_roc(name, labels, predictions, **kwargs):\n    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n\n    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n    plt.xlabel('False Positive Rate [%]')\n    plt.ylabel('True Positive Rate [%]')\n    plt.xlim([-0.5,20])\n    plt.ylim([80,100.5])\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_aspect('equal')","6e02aad6":"train_predictions_baseline = model_initial_bias.predict(X_train, batch_size=2048)\ntest_predictions_baseline = model_initial_bias.predict(X_test, batch_size=2048)","b4267976":"plot_roc(\"Train\", y_train, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\nplt.legend(loc='lower right')\n#save_fig(\"Fig9\")","ab8d8314":"def plot_cm(labels, predictions, p=0.5):\n    cm = tf.math.confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\n    print('True Negatives: ', cm[0][0])\n    print('False Positives: ', cm[0][1])\n    print('False Negatives: ', cm[1][0])\n    print('True Positives: ', cm[1][1])\n    print('Total : ', np.sum(cm[1]))","926dff4f":"\n#for name, value in zip(model.metrics_names, results):\n#    print(name, ': ', value)\n# print()\n# plot_cm(y_test, y_pred)","a09c51e3":"tensorboard --logdir logs  --port=8021","bafd9b1a":"### Evaluation of Model & Results","0df07178":"\nThe correct bias to set can be derived from:\n\n$$ p_0 = pos\/(pos + neg) = 1\/(1+e^{-b_0}) $$\n$$ b_0 = -log_e(1\/p_0 - 1) $$\n$$ b_0 = log_e(pos\/neg)$$","a726070a":"#### Confusion Matrix","3703000e":"With this initialization the initial loss should be approximately:\n\n$$-p_0log(p_0)-(1-p_0)log(1-p_0) = 0.01317$$","1607c4ee":"### Setting the Initial Bias","4304cefd":"#### ROC Plot","3fc557d8":"This loss is about 50 times less than a naive initialisation","57251b9e":"### Plotting of Metrics: Loss, AUC, Precision & Recall","1a3a5a54":"#### Making Class Specific Weights","d9b8fe37":"### Earmarking Validation, Train & Test Sets","21babd29":"#### Fitting the Model","0b9f8664":"### Analysis of Class Imbalance","ecd8c6a9":"## Basic Initialisation","24ff95e6":"#### Tensorflow TF Dataset","7e29f586":"### Showing the Influence of Initial Bias","ef1d177c":"#### Making the Feature Layer with Feature Columns","54795fe9":"# <font style=\"color:red;\">Deep Learning Webpage Classification Model with Structured Data <br\/> (Balanced Using Class Weights & Faster Convergence Using Initial Bias)<\/font>","fca36c7e":"### Loading Dataset","77331d11":"## Tensor Flow Model Building","7b71d9c5":" ### Vectorizing URL Text Using Profanity Score"}}