{"cell_type":{"d1fff708":"code","b47940c6":"code","3ed53324":"code","d396b52d":"code","4d5d1a4c":"code","aae59fd5":"code","48acd081":"code","ebe7d70d":"code","8487b410":"code","c1227c5c":"code","dca28ce6":"code","142bd716":"code","e4a37059":"code","3113c64e":"code","c6f8cf46":"code","6e69d14b":"code","e96b1d21":"code","23567254":"code","aa9c315c":"code","5a17b010":"code","e4af5ec5":"code","06e58465":"code","5a951306":"code","35046f11":"code","e62ff9d7":"code","ff42f5fc":"code","1c3ec4f3":"code","1ef3c249":"code","dfcdb8ba":"markdown","07bf3268":"markdown","62f73308":"markdown"},"source":{"d1fff708":"import json\nimport matplotlib.pyplot as plt\nimport mind_utility as mu\nimport numpy as np\nimport os\nimport pandas as pd\nimport scipy.sparse as sparse\n\n%matplotlib inline","b47940c6":"#load the dataset \ntrain_path_dir = \"\/kaggle\/input\/microsoft-news-recommendation-dataset-train\/\"\ntest_path_dir = \"\/kaggle\/input\/microsoft-news-recommendation-dataset-dev\/\"","3ed53324":"train_news_df = pd.read_csv(\n    os.path.join(train_path_dir, \"news.tsv\"), sep=\"\\t\", header=None, names=mu.NEWS_COLUMN_NAMES)\ntest_news_df = pd.read_csv(\n    os.path.join(test_path_dir, \"news.tsv\"), sep=\"\\t\", header=None, names=mu.NEWS_COLUMN_NAMES)\n\nprint(train_news_df.shape, test_news_df.shape)\ntrain_news_df.head(2)","d396b52d":"#number of unique news in train and test set\nprint(\"Num of unique news in train set: \", train_news_df[\"news_id\"].nunique())\nprint(\"Num of unique news in test set: \", test_news_df[\"news_id\"].nunique())\nprint(\"Num of unique news for both: \", pd.concat([train_news_df[\"news_id\"], \n                                                  test_news_df[\"news_id\"]]).nunique())\ncommon_news = np.intersect1d(train_news_df[\"news_id\"], test_news_df[\"news_id\"])\nprint(\"Num of unique news common for both: \", len(common_news))","4d5d1a4c":"#since these are all news data, we can just concatenate both \nnews_df = pd.concat([train_news_df, test_news_df]).drop_duplicates()\ndel train_news_df\ndel test_news_df\nnews_df.shape","aae59fd5":"#preprocessing\n\n#fill in the the Nan values\nnews_df.loc[news_df[\"abstract\"].isnull(), \"abstract\"]  = \"\"\nnews_df.loc[news_df[\"title_entities\"].isnull(), \"title_entities\"] = \"[]\"\nnews_df.loc[news_df[\"abstract_entities\"].isnull(), \"abstract_entities\"] = \"[]\"\n\n#transform the entities to proper data type\nnews_df[\"title_entities\"] = news_df[\"title_entities\"].apply(lambda x: json.loads(x))\nnews_df[\"abstract_entities\"] = news_df[\"abstract_entities\"].apply(lambda x: json.loads(x))\n\n#make the news_id as index\nnews_df = news_df.set_index(\"news_id\")","48acd081":"#distribution of news categories\nnews_df[\"category\"].value_counts()","ebe7d70d":"#distribution of news subcategories\nnews_df[\"subcategory\"].value_counts().head(10)","8487b410":"#examine the number of title and abstract entities\npd.concat([\n    news_df[\"title_entities\"].apply(lambda x: len(x)).value_counts(),\n    news_df[\"abstract_entities\"].apply(lambda x: len(x)).value_counts()], axis=1)","c1227c5c":"#check how many news have entities at least one detected title\/abstract entities\n(news_df[\"title_entities\"].apply(lambda x: len(x)>0) | \\\nnews_df[\"abstract_entities\"].apply(lambda x: len(x)>0)).value_counts()\n","dca28ce6":"train_behaviors_df = pd.read_csv(\n    os.path.join(train_path_dir, \"behaviors.tsv\"), sep=\"\\t\", header=None, names=mu.BEHAVIORS_COLUMN_NAMES)\ntest_behaviors_df = pd.read_csv(\n    os.path.join(test_path_dir, \"behaviors.tsv\"), sep=\"\\t\", header=None, names=mu.BEHAVIORS_COLUMN_NAMES)\nprint(train_behaviors_df.shape, test_behaviors_df.shape)\ntrain_behaviors_df.head(2)","142bd716":"#preprocessing the data types of the columns\nfor behaviors_df in [train_behaviors_df, test_behaviors_df]:\n    behaviors_df[\"time\"] = pd.to_datetime(behaviors_df[\"time\"])\n    behaviors_df[\"history\"] = behaviors_df[\"history\"].fillna(\"\")","e4a37059":"print(\"Train : \", train_behaviors_df[\"time\"].min(), train_behaviors_df[\"time\"].max())\nprint(\"Test : \", test_behaviors_df[\"time\"].min(), test_behaviors_df[\"time\"].max())","3113c64e":"%%time\ntrain_user_id_2_history = train_behaviors_df.set_index(\"user_id\")[\"history\"].to_dict()\ntrain_user_id_2_history = {user_id: history.split() for user_id, history in train_user_id_2_history.items()}\nprint(len(train_user_id_2_history))\n#show sample data\nlist(train_user_id_2_history.items())[:1]","c6f8cf46":"%%time\ntest_user_id_2_history = test_behaviors_df.set_index(\"user_id\")[\"history\"].to_dict()\ntest_user_id_2_history = {user_id: history.split() for user_id, history in test_user_id_2_history.items()}\nprint(len(test_user_id_2_history))\n#show sample data\nlist(test_user_id_2_history.items())[:1]","6e69d14b":"%%time\ndef merge_dicts(list_dicts):\n    final_dict = {}\n    for dict_ in list_dicts:\n        final_dict.update(dict_)\n    return final_dict\n\ntrain_user_id_2_impressions = train_behaviors_df.set_index(\"user_id\")[\"impressions\"].str.split()\\\n    .apply(lambda list_: {val.split(\"-\")[0]: int(val.split(\"-\")[1]) for val in list_})\\\n    .groupby(level=\"user_id\").apply(lambda list_dicts: list_dicts.values)\ntrain_user_id_2_impressions = train_user_id_2_impressions.apply(lambda x: merge_dicts(x)).to_dict()\nprint(len(train_user_id_2_impressions))\nlist(train_user_id_2_impressions.items())[:1]","e96b1d21":"%%time\ntest_user_id_2_impressions = test_behaviors_df.set_index(\"user_id\")[\"impressions\"].str.split()\\\n    .apply(lambda list_: {val.split(\"-\")[0]: int(val.split(\"-\")[1]) for val in list_})\\\n    .groupby(level=\"user_id\").apply(lambda list_dicts: list_dicts.values)\ntest_user_id_2_impressions = test_user_id_2_impressions.apply(lambda x: merge_dicts(x)).to_dict()\nprint(len(test_user_id_2_impressions))\nlist(test_user_id_2_impressions.items())[:1]","23567254":"news_id_2_news_index = dict(zip(news_df.index, \n                                np.arange(len(news_df.index))))\ntrain_user_id_2_user_index = dict(zip(train_behaviors_df[\"user_id\"].unique(), \n                                      np.arange(len(train_behaviors_df[\"user_id\"].unique()))))\ntest_user_id_2_user_index = dict(zip(test_behaviors_df[\"user_id\"].unique(), \n                                      np.arange(len(test_behaviors_df[\"user_id\"].unique()))))","aa9c315c":"#create the train_matrix_impressions\nrows_impressions = []\ncols_impressions = []\nvals_impressions = []\nfor user_id, impressions in list(train_user_id_2_impressions.items()):\n    for news_id in impressions:\n        rows_impressions.append(train_user_id_2_user_index[user_id])\n        cols_impressions.append(news_id_2_news_index[news_id])\n        vals_impressions.append(impressions[news_id])\n\n# for news that were not opened by the user will be scored as -1 instead of 0\nvals_impressions = [-1 if i==0 else i for i in vals_impressions]\n\n# create the sparse matrix for the training data impressions\ntrain_matrix_impressions = sparse.lil_matrix((len(train_user_id_2_user_index), len(news_id_2_news_index)))\ntrain_matrix_impressions[rows_impressions, cols_impressions] = vals_impressions\ntrain_matrix_impressions = train_matrix_impressions.tocsr()\ntrain_matrix_impressions","5a17b010":"#create the test_matrix_impressions\nrows_impressions = []\ncols_impressions = []\nvals_impressions = []\nfor user_id, impressions in list(test_user_id_2_impressions.items()):\n    for news_id in impressions:\n        rows_impressions.append(test_user_id_2_user_index[user_id])\n        cols_impressions.append(news_id_2_news_index[news_id])\n        vals_impressions.append(impressions[news_id])\n\n# for news that were not opened by the user will be scored as -1 instead of 0\nvals_impressions = [-1 if i==0 else i for i in vals_impressions]\n\n# create the sparse matrix for the test data impressions\ntest_matrix_impressions = sparse.lil_matrix((len(test_user_id_2_user_index), len(news_id_2_news_index)))\ntest_matrix_impressions[rows_impressions, cols_impressions] = vals_impressions\ntest_matrix_impressions = test_matrix_impressions.tocsr()\ntest_matrix_impressions","e4af5ec5":"#create the train_matrix_history\nrows_history = []\ncols_history = []\nvals_history = []\nfor user_id, history in list(train_user_id_2_history.items()):\n    for news_id in history:\n        rows_history.append(train_user_id_2_user_index[user_id])\n        cols_history.append(news_id_2_news_index[news_id])\n        vals_history.append(1)\n\n# create the sparse matrix for the training data history\ntrain_matrix_history = sparse.lil_matrix((len(train_user_id_2_user_index), len(news_id_2_news_index)))\ntrain_matrix_history[rows_history, cols_history] = vals_history\ntrain_matrix_history = train_matrix_history.tocsr()\ntrain_matrix_history","06e58465":"#create the test_matrix_history\nrows_history = []\ncols_history = []\nvals_history = []\nfor user_id, history in list(test_user_id_2_history.items()):\n    for news_id in history:\n        rows_history.append(test_user_id_2_user_index[user_id])\n        cols_history.append(news_id_2_news_index[news_id])\n        vals_history.append(1)\n\n# create the sparse matrix for the test data history\ntest_matrix_history = sparse.lil_matrix((len(test_user_id_2_user_index), len(news_id_2_news_index)))\ntest_matrix_history[rows_history, cols_history] = vals_history\ntest_matrix_history = test_matrix_history.tocsr()\ntest_matrix_history","5a951306":"print(train_matrix_impressions.shape, train_matrix_history.shape)\nprint(test_matrix_impressions.shape, test_matrix_history.shape)","35046f11":"#visualize train_matrix_history\nfig, ax = plt.subplots(figsize=(30, 40))\nax.spy(train_matrix_history, markersize=1)\nax.set_xlabel(\"News\", size=25)\nax.set_ylabel(\"Users\", size=25)","e62ff9d7":"#visualize train_matrix_impressions\nfig, ax = plt.subplots(figsize=(30, 40))\nax.spy(test_matrix_impressions, markersize=1)\nax.set_xlabel(\"News\", size=25)\nax.set_ylabel(\"Users\", size=25)","ff42f5fc":"#visualize test_matrix_history\nfig, ax = plt.subplots(figsize=(30, 40))\nax.spy(test_matrix_history, markersize=1)\nax.set_xlabel(\"News\", size=25)\nax.set_ylabel(\"Users\", size=25)","1c3ec4f3":"#visualize test_matrix_impressions\nfig, ax = plt.subplots(figsize=(30, 40))\nax.spy(test_matrix_impressions, markersize=1)\nax.set_xlabel(\"News\", size=25)\nax.set_ylabel(\"Users\", size=25)","1ef3c249":"pd.to_pickle(train_matrix_history, \"train_matrix_history.pkl\")\npd.to_pickle(train_matrix_impressions, \"train_matrix_impressions.pkl\")\npd.to_pickle(train_user_id_2_user_index, \"train_user_id_2_user_index.pkl\")\n\npd.to_pickle(test_matrix_history, \"test_matrix_history.pkl\")\npd.to_pickle(test_matrix_impressions, \"test_matrix_impressions.pkl\")\npd.to_pickle(test_user_id_2_user_index, \"test_user_id_2_user_index.pkl\")\n\npd.to_pickle(news_id_2_news_index, \"news_id_2_news_index.pkl\")","dfcdb8ba":"----\n## `behaviors.tsv`","07bf3268":"----\n## Create the sparse matrix for the user-item interaction matrix","62f73308":"----\n## `news.tsv`"}}