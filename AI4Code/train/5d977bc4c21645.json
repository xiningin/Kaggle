{"cell_type":{"b35fd064":"code","8772b450":"code","19b14ef5":"code","ce8cf9ff":"code","c7458ed8":"code","aebac694":"code","bc62ee7d":"code","d2c31b6b":"code","98365a97":"code","1cd5f1eb":"code","e95ecf9a":"code","3434c793":"code","2284cf46":"code","37f9f13e":"code","51001f9e":"code","804bbe69":"code","6a759026":"code","308ae2ad":"code","a09962c6":"code","f29ff046":"code","0bbf9a82":"code","f9c5f139":"code","9afe30eb":"code","12fbe234":"code","288f9f5b":"code","9673101e":"code","e6a087db":"code","d73861d8":"code","3ca86baf":"code","256b617c":"markdown","f1572fb6":"markdown","f7fe13a6":"markdown","287a5898":"markdown","5cd8eee0":"markdown","4b9548a5":"markdown","ee38b8ae":"markdown","a43b38cf":"markdown","40baa254":"markdown","7069699e":"markdown","94e28108":"markdown","12535687":"markdown","d137e5ed":"markdown","176b8208":"markdown","0f0ee6fd":"markdown","f116d554":"markdown","c80d24dc":"markdown","dccd6124":"markdown","e8a857eb":"markdown","9884c09f":"markdown","4e4cea82":"markdown"},"source":{"b35fd064":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical, Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8772b450":"path = '\/kaggle\/input\/landmark-recognition-2021\/'\nos.listdir(path)","19b14ef5":"train_data = pd.read_csv(path+'train.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","ce8cf9ff":"def plot_examples(landmark_id=1):\n    \"\"\" Plot 5 examples of images with the same landmark_id \"\"\"\n    \n    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n    fig.subplots_adjust(hspace = .2, wspace=.2)\n    axs = axs.ravel()\n    for i in range(5):\n        idx = train_data[train_data['landmark_id']==landmark_id].index[i]\n        image_id = train_data.loc[idx, 'id']\n        file = image_id+'.jpg'\n        subpath = '\/'.join([char for char in image_id[0:3]])\n        img = cv2.imread(path+'train\/'+subpath+'\/'+file)\n        axs[i].imshow(img)\n        axs[i].set_title('landmark_id: '+str(landmark_id))\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])","c7458ed8":"print('Samples train:', len(train_data))\nprint('Samples test:', len(samp_subm))","aebac694":"train_data.head()","bc62ee7d":"len(train_data['landmark_id'].unique())","d2c31b6b":"samp_subm.head()","98365a97":"train_data.head()","1cd5f1eb":"image_id = train_data.loc[0, 'id']\nfile = image_id+'.jpg'\nsubpath = '\/'.join([char for char in image_id[0:3]]) ","e95ecf9a":"file in os.listdir(path+'train\/'+subpath)","3434c793":"img = cv2.imread(path+'train\/'+subpath+'\/'+file)\nplt.imshow(img)\nplt.show()","2284cf46":"img.shape","37f9f13e":"plot_examples(landmark_id = 138982)","51001f9e":"plot_examples(landmark_id = 126637)","804bbe69":"plot_examples(landmark_id = 83144)","6a759026":"list_IDs_train, list_IDs_val = train_test_split(list(train_data.index)[:500000], test_size=0.33, random_state=2021)\nlist_IDs_test = list(samp_subm.index)","308ae2ad":"print('Number train samples:', len(list_IDs_train))\nprint('Number val samples:', len(list_IDs_val))\nprint('Number test samples:', len(list_IDs_test))","a09962c6":"img_size = 32\nimg_channel = 3\nbatch_size = 64\n\nnum_classes = len(train_data['landmark_id'].value_counts())","f29ff046":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, data, img_size, img_channel, batch_size):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.data = data\n        self.img_size = img_size\n        self.img_channel = img_channel\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.list_IDs))\n        \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_*self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n            \n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, self.img_size, self.img_size, self.img_channel))\n        y = np.zeros((self.batch_size, 1), dtype=int)\n        for i, ID in enumerate(list_IDs_temp):\n            \n            image_id = self.data.loc[ID, 'id']\n            file = image_id+'.jpg'\n            subpath = '\/'.join([char for char in image_id[0:3]]) \n            \n            img = cv2.imread(self.path+subpath+'\/'+file)\n            \n            img = cv2.resize(img, (self.img_size, self.img_size))\n            X[i, ] = img\/255\n            if self.path.find('train')>=0:\n                y[i, ] = self.data.loc[ID, 'landmark_id']\n            else:\n                y[i, ] = 0\n        return X, y","0bbf9a82":"train_generator = DataGenerator(path+'train\/', list_IDs_train, train_data, img_size, img_channel, batch_size)\nval_generator = DataGenerator(path+'train\/', list_IDs_val, train_data, img_size, img_channel, batch_size)\ntest_generator = DataGenerator(path+'test\/', list_IDs_test, samp_subm, img_size, img_channel, batch_size)","f9c5f139":"weights='..\/input\/models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nconv_base = ResNet50(weights=weights,\n                     include_top=False,\n                     input_shape=(img_size, img_size, img_channel))\nconv_base.trainable = True","9afe30eb":"model = Sequential()\nmodel.add(conv_base)\nmodel.add(Flatten())\n#model.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(optimizer = Adam(lr=1e-4),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=['sparse_categorical_accuracy'])\n\nmodel.summary()","12fbe234":"epochs = 1","288f9f5b":"history = model.fit_generator(generator=train_generator,\n                              validation_data=val_generator,\n                              epochs = epochs, workers=4)","9673101e":"y_pred = model.predict_generator(test_generator, verbose=1)","e6a087db":"for i in range(len(samp_subm.index)):\n    category = np.argmax(y_pred[i])\n    score = y_pred[i][np.argmax(y_pred[i])].round(2)\n    samp_subm.loc[i, 'landmarks'] = str(category)+' '+str(score)","d73861d8":"samp_subm.head()","3ca86baf":"samp_subm.to_csv('submission.csv', index=False)","256b617c":"# Path","f1572fb6":"# Libraries\nWe use some standard python packages and the libraries of scikit learn and keras. ","f7fe13a6":"# Plot Some Examples\nWe plot some examples of images with the same **landmark_id** in a row.","287a5898":"# Data Generator\n\nWe use a data generator to load the data on demand.","5cd8eee0":"# Split Data\nWe define train, validation and test data.","4b9548a5":"# Overview\nFirst we look on the size of the dataset:","ee38b8ae":"Plot the image:","a43b38cf":"Use the DataGenerator class to define the data generators for train, validation and test data:","40baa254":"Look on the image shape:","7069699e":"# Predict Test Data","94e28108":"Load pretrained model:","12535687":"For each test image, we have to predict one landmark label and a corresponding confidence score. ","d137e5ed":"# Functions","176b8208":"Is the file located in the subpath?","0f0ee6fd":"Define Model","f116d554":"# Load Data","c80d24dc":"# Intro\nWelcome to the [Google Landmark Recognition 2021](https:\/\/www.kaggle.com\/c\/landmark-recognition-2021) compedition\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/29762\/logos\/header.png)\n\nThis notebook will give you a guideline to start step by step with this compedition. We focus on:\n* the underlying structure of the data,\n* a data generator to load the image data on demand during the prediction process.\n\nWe use a simple model with a pretrained model on a subset of the train data to clarify the workflow. Additionally we recommend to use the power of GPU.\n\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span>","dccd6124":"# Find Image\nWe consider the first image of the train data set and plot it. The first 3 characters ares used for the subpath which is the location of the image. ","e8a857eb":"There are 81313 unique classes:","9884c09f":"# Model","4e4cea82":"# Export"}}