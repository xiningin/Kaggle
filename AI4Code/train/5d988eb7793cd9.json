{"cell_type":{"48efe5c0":"code","525a50af":"code","8edd8d1d":"code","1aa968ab":"code","6ab59a25":"code","038778ba":"code","f7a0292b":"code","94a4e50b":"code","5eeaeb1b":"code","97508c78":"code","3046fc64":"code","d9c2929e":"code","12a952b7":"code","418ea8fa":"code","7fa69b24":"code","80082d95":"code","eb969eec":"code","eccec294":"code","ab1a806a":"code","f944e03d":"code","fad64ba5":"code","187c6858":"code","c37f18b3":"code","e8989dd1":"code","99e54799":"code","699f843e":"code","d728da52":"code","0b0be7b8":"code","2e3a9522":"code","2b178275":"code","fe7b093a":"code","f4d477b5":"code","43ffb3e8":"code","b619a5db":"code","ef9cf3d8":"code","af30c7a3":"code","88ad5aaa":"code","a71b12fe":"code","9cc84bf9":"code","8a02b556":"code","0ad1eece":"code","7843b0b3":"code","6d4df46f":"code","dfc93260":"code","0d414663":"code","a97b01a9":"code","1b0bfd0a":"code","b28bdb13":"code","5e929019":"code","34654767":"code","f2f885b5":"code","cf35a4ac":"code","22b6f905":"code","81318309":"code","14288e0a":"code","25ef9b11":"code","15edb4d2":"code","912b1e0b":"code","293c71a6":"code","92818df2":"code","a2717abc":"code","5cc6eb72":"code","a3d1b158":"code","5a50d165":"code","b4e13ee8":"code","5b23d695":"code","1bf05b37":"code","94555ee0":"code","a196c6f2":"code","0acf42e8":"code","316ec59f":"code","46bc3857":"code","82a5f277":"markdown","31421430":"markdown","16784a3f":"markdown","24c5ddec":"markdown","bfa5aa37":"markdown","f7568644":"markdown","c48b9610":"markdown","c0adf5cc":"markdown","5bde17d8":"markdown","2854c093":"markdown","1f1600a9":"markdown","159cbeec":"markdown","c67c312f":"markdown","cca6def6":"markdown","30490c33":"markdown","87447c8e":"markdown","cb3c5e50":"markdown","b74334e1":"markdown","f5d7168c":"markdown","87f31dc2":"markdown","023aa8b8":"markdown","6a1eee94":"markdown","8d77f6d4":"markdown","67b7d35d":"markdown"},"source":{"48efe5c0":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, warnings, datetime, math\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings('ignore')","525a50af":"########################### Helpers\n#################################################################################\n## -------------------\n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n## -------------------","8edd8d1d":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_csv('..\/input\/train_transaction.csv')\ntest_df = pd.read_csv('..\/input\/test_transaction.csv')\ntest_df['isFraud'] = 0\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv')","1aa968ab":"########################### Base check\n#################################################################################\n\nfor df in [train_df, test_df, train_identity, test_identity]:\n    original = df.copy()\n    df = reduce_mem_usage(df)\n\n    for col in list(df):\n        if df[col].dtype!='O':\n            if (df[col]-original[col]).sum()!=0:\n                df[col] = original[col]\n                #print('Bad transformation', col)","6ab59a25":"train_df.columns","038778ba":"train_identity.head()","f7a0292b":"temp_df = pd.concat([train_df, test_df])\npercent_missing = (temp_df.isnull().mean() * 100) \nprint(percent_missing.sort_values(ascending=False))","94a4e50b":"### Removing columns greater than a certain threshold\n#thresh = 90\n#reduced_col = (percent_missing[percent_missing<thresh])\n#reduced_col.sort_values(ascending=False)","5eeaeb1b":"# numnerical columns\nnum_cols = train_df._get_numeric_data().columns\nprint('Total number of numerical columns are ', len(num_cols))\nprint(num_cols)\nprint('-------------------------------------------------------------------------------')\n# categorical columns\ncat_cols = set(train_df.columns) - set(num_cols)\nprint('Total number of categorical columns are', len(cat_cols))\nprint(cat_cols)","97508c78":"# Plotting categorical features\ndef make_categorical_plots(Vs):\n    col = 4\n    row = len(Vs)\/\/col+1\n    fig = plt.figure(figsize=(20,row*5))\n    for i,v in enumerate(Vs):\n        ax = plt.subplot(row,col,i+1)\n        g1 = sns.barplot(x=v, y=\"isFraud\", data=train_df, ax = ax)\n        g1.legend()\n        plt.title(v+\" barplot w.r.t target\", fontsize=16)\n        g1.set_xlabel(v+ \" values\", fontsize=16)\n        g1.set_ylabel(\"Probability\", fontsize=16)\n        #plt.title('Column: '+ str(v), fontsize=16)\n        plt.subplots_adjust(hspace = 0.5)\n    plt.show()","3046fc64":"# Plotting desity plot for numerical features\ndef make_density_plots(Vs):\n    col = 2\n    row = len(Vs)\/\/col+1\n    fig = plt.figure(figsize=(20,row*5))\n    for i,v in enumerate(Vs):\n        ax = plt.subplot(row,col,i+1)\n        g1 = sns.distplot(train_df[train_df['isFraud'] == 1][v].dropna(), label='Fraud',\n                          ax=ax )\n        g1 = sns.distplot(train_df[train_df['isFraud'] == 0][v].dropna(), label='NoFraud',\n                              ax=ax)\n        g1.legend()\n        plt.title(v+\" values distribution w.r.t target\", fontsize=16)\n        g1.set_xlabel(v+ \" values\", fontsize=16)\n        g1.set_ylabel(\"Probability\", fontsize=16)\n        #plt.title('Column: '+ str(v), fontsize=16)\n        plt.subplots_adjust(hspace = 0.5)\n    plt.show()","d9c2929e":"# Function to plot histogram\ndef make_histogram_plots(Vs):\n    col = 4\n    row = len(Vs)\/\/4+1\n    plt.figure(figsize=(20,row*5))\n    for i,v in enumerate(Vs):\n        #print(v)\n        plt.subplot(row,col,i+1)\n        plt.title('Column: '+ str(v))\n        h = plt.hist(train_df[v],bins=100)\n        if len(h[0])>1: plt.ylim((0,np.sort(h[0])[-2]))\n    plt.show()","12a952b7":"import re","418ea8fa":"total_col = train_df.columns \n# Finding all columns starting with 'card'\nr = re.compile(\"^card\")\ncard_list = list(filter(r.match, total_col)) # Read Note\nprint(card_list)","7fa69b24":"make_density_plots(['card1', 'card2', 'card3','card5'])","80082d95":"# Finding all columns starting with 'addr'\nr = re.compile(\"^addr\")\naddr_list = list(filter(r.match, total_col)) # Read Note\nprint(addr_list)","eb969eec":"make_density_plots(addr_list)","eccec294":"# Finding all columns starting with 'V'\nr = re.compile(\"^V\")\nV_list = list(filter(r.match, total_col)) # Read Note\nprint(V_list)","ab1a806a":"# Plotting first 10 values\nmake_density_plots(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10'])","f944e03d":"# col_corr = set() # Set of all the names of deleted columns\ncorr_matrix = train_df[V_list].corr()","fad64ba5":"####### Removing multi-correlated columns and retaining only that column which has high unique value, \n#### This conveys that it has high variance, and may be has high impact on the model\n#### Here, first we are making correlation matrix and then finding the correlations of columns with other columns\n#### which are highly correlated. Then removing all other columns except for the columns which has highest unique value. \ncol_corr = set()\nthreshold = 0.80\nfor i in range(len(corr_matrix.columns)):\n    high_corr = []\n    high_corr.append(corr_matrix.columns[i])\n    for j in range(i):\n        mx = 0\n        vx = corr_matrix.columns[i]\n        #print('vx', vx)\n        if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n            high_corr.append(corr_matrix.columns[j])\n            \n    if len(high_corr)>1:\n        #print('high_corr',high_corr )\n        for col in high_corr:\n            n = train_df[col].nunique()\n            if n>mx:\n                mx = n\n                vx = col\n        #print('vx', vx)        \n        high_corr.remove(str(vx))\n        col_corr.update(set(high_corr))\n        #print('col_corr', col_corr)","187c6858":"print(list(col_corr))","c37f18b3":"# V_list_ = [x for x in V_list if x not in col_corr]\n# print(\"Length of new V_list is\", len(V_list_))\n# print(V_list_)","e8989dd1":"# with threshold value of .80\n# This is the output after finding the features which are highly correlated. \n# You can find how to get these columns 3 cells above\nV_list_ = ['V1', 'V3', 'V5', 'V7', 'V9', 'V13', 'V20', 'V24', 'V26', 'V28', 'V30', 'V36', 'V38', 'V45',\n           'V47', 'V54', 'V55', 'V56', 'V62', 'V67', 'V76', 'V78', 'V83', 'V87', 'V88', 'V107', 'V109',\n           'V110', 'V113', 'V115', 'V116', 'V119', 'V121', 'V122', 'V125', 'V138', 'V140', 'V142', \n           'V147', 'V158', 'V160', 'V162', 'V166', 'V169', 'V174', 'V185', 'V198', 'V201', 'V209', \n           'V210', 'V220', 'V221', 'V223', 'V239', 'V240', 'V241', 'V251', 'V252', 'V260', 'V262',\n           'V267', 'V271', 'V281', 'V282', 'V283', 'V286', 'V289', 'V291', 'V301', 'V303', 'V305', \n           'V307', 'V310', 'V325', 'V339']","99e54799":"#make_density_plots(['V_list_'])","699f843e":"# Plotting values\nmake_density_plots(['TransactionAmt'])","d728da52":"# Finding all columns starting with 'D'\nr = re.compile(\"^D\")\nD_list = list(filter(r.match, total_col)) # Read Note\nprint(D_list)","0b0be7b8":"make_density_plots(D_list)","2e3a9522":"# Finding all columns starting with 'M'\nr = re.compile(\"^M\")\nM_list = list(filter(r.match, total_col)) # Read Note\nprint(M_list)","2b178275":"make_categorical_plots(['M1','M2','M3','M4','M5','M6','M7','M8','M9'])","fe7b093a":"make_categorical_plots(['addr1', 'addr2'])","f4d477b5":"train_identity.head()","43ffb3e8":"train_identity.columns","b619a5db":"temp_identity_df = pd.concat([train_identity, test_identity])\npercent_missing = (temp_identity_df.isnull().mean() * 100) \nprint(percent_missing.sort_values(ascending=False))","ef9cf3d8":"# numnerical columns\nnum_identity_cols = train_identity._get_numeric_data().columns\nprint('Total number of numerical columns are ', len(num_identity_cols))\nprint(num_identity_cols)\nprint('-------------------------------------------------------------------------------')\n# categorical columns\ncat_identity_cols = set(train_identity.columns) - set(num_identity_cols)\nprint('Total number of categorical columns are', len(cat_identity_cols))\nprint(cat_identity_cols)","af30c7a3":"train_df['TransactionAmt'] \/ train_df.groupby(['card1'])['TransactionAmt'].transform('mean')","88ad5aaa":"train_df.head()","a71b12fe":"# Find the groups\n#train_df.groupby(['card1']).groups","9cc84bf9":"sns.countplot(x=\"ProductCD\", data=train_df)","8a02b556":"sns.countplot(x=\"M4\", data=train_df)","0ad1eece":"train_df['card1'].nunique()","7843b0b3":"# Freq encoding\ni_cols = ['card1','card2','M4','D9',\n          'addr1','addr2','dist1','dist2',\n          'P_emaildomain', 'R_emaildomain'\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    valid_card = temp_df[col].value_counts().to_dict()   \n    train_df[col+'_fq_enc'] = train_df[col].map(valid_card)\n    test_df[col+'_fq_enc']  = test_df[col].map(valid_card)","6d4df46f":"(train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)).nunique()","dfc93260":"len(train_df)","0d414663":"TARGET = 'isFraud'\nfor col in ['card1','card2','addr1','addr2','M4']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","a97b01a9":"i_cols = V_list_\n\nfor df in [train_df, test_df]:\n    df['V_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n    df['V_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)","1b0bfd0a":"# Binary encoding for M columns\ni_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\nfor df in [train_df, test_df]:\n    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)","b28bdb13":"# Let's add some kind of client uID based on cardID ad addr columns\n# The value will be very specific for each client so we need to remove it\n# from final feature. But we can use it for aggregations.\ntrain_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\ntest_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n\ntrain_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\ntest_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n\ntrain_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)+'_'+train_df['R_emaildomain'].astype(str)\ntest_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)+'_'+test_df['R_emaildomain'].astype(str)\n\n# Check if the Transaction Amount is common or not (we can use freq encoding here)\n# In our dialog with a model we are telling to trust or not to these values   \ntrain_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\ntest_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n","5e929019":"i_cols = ['card1','card2','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n        \n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n    \n        train_df[new_col_name] = train_df[col].map(temp_df)\n        test_df[new_col_name]  = test_df[col].map(temp_df)","34654767":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nfor df in [train_df, test_df]:\n    \n    # Temporary variables for aggregation\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n    \n    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)","f2f885b5":"df[['DT','DT_M','DT_W', 'DT_D','DT_hour', 'DT_day_week', 'DT_day_month' ]].head()","cf35a4ac":"# Total transactions per timeblock\nfor col in ['DT_M','DT_W','DT_D', 'DT_hour', 'DT_day_week', 'DT_day_month']:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train_df[col+'_total'] = train_df[col].map(fq_encode)\n    test_df[col+'_total']  = test_df[col].map(fq_encode)\n    ","22b6f905":"train_df[['DT_M_total','DT_W_total','DT_D_total', 'DT_hour_total', 'DT_day_week_total', 'DT_day_month_total']].head()","81318309":"V_col_remove = [col for col in V_list if col not in V_list_]\n#print(V_col_remove)","14288e0a":"for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n    train_df[col] = train_df[col].map({'T':1, 'F':0})\n    test_df[col]  = test_df[col].map({'T':1, 'F':0})","25ef9b11":"train_df.dtypes","15edb4d2":"train_df['ProductCD'].head()\n#.fillna('unseen_before_label')","912b1e0b":"train_df['ProductCD'].isnull().sum()","293c71a6":"train_df['ProductCD'].count()","92818df2":"train_df['ProductCD'].fillna('unseen_before_label').head()","a2717abc":"train_df.select_dtypes(include=['object'])","5cc6eb72":"for col in list(train_df):\n    if train_df[col].dtype=='object':\n        print(col)","a3d1b158":"# Frequency encoding for categorical column and then doing categorical encoding\nfor col in list(train_df):\n    if train_df[col].dtype=='object':\n        print(col)\n        train_df[col] = train_df[col].fillna('NaN')\n        test_df[col]  = test_df[col].fillna('NaN')\n        \n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])\n        \n        train_df[col] = train_df[col].astype('category')\n        test_df[col] = test_df[col].astype('category')","5a50d165":"train_df['R_emaildomain']","b4e13ee8":"rm_cols = [\n\n    'uid','uid2','uid3',            \n    'DT','DT_M', 'DT_W', 'DT_D','DT_hour', 'DT_day_week', 'DT_day_month' # Already we have considered these\n    \n]\n\nrm_cols.extend(V_col_remove)\n#rm_cols","5b23d695":"features_columns = [col for col in list(train_df) if col not in rm_cols]","1bf05b37":"len(train_df)","94555ee0":"len(test_df)","a196c6f2":"features_columns.remove('isFraud')\nfeatures_columns","0acf42e8":"########################### Model\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nSEED = 10\n\ndef make_predictions(train_df, test_df, features_columns, target, lgb_params, NFOLDS=3):\n    N_SPLITS = 10    \n    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n    # Main Data\n    X,y = train_df[features_columns], train_df[TARGET]\n\n    # Test Data and expport DF\n    P,P_y  = test_df[features_columns], test_df[TARGET]\n    \n    test_df = test_df[['TransactionID',target]]    \n    predictions = np.zeros(len(test_df))\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=y)):\n        print('Fold:',fold_+1)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]    \n        train_data = lgb.Dataset(tr_x, label=tr_y)\n        #valid_data = lgb.Dataset(vl_x, label=v_y)  \n        \n        if LOCAL_TEST:\n            valid_data = lgb.Dataset(P, label=P_y) \n        else:\n            valid_data = lgb.Dataset(vl_x, label=vl_y)  \n\n\n        estimator = lgb.train(\n                lgb_params,\n                train_data,\n                valid_sets = [train_data, valid_data],\n                verbose_eval = 1000,\n            )\n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p\/NFOLDS\n        \n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n       # del tr_x, tr_y, vl_x, vl_y, train_data, valid_data\n       # gc.collect()\n        \n    test_df['prediction']  = predictions\n    \n    return test_df       ","316ec59f":"LOCAL_TEST = False\n# Model params\nfrom sklearn import metrics\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':800,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } \n# Model Train\nif LOCAL_TEST:\n    lgb_params['learning_rate'] = 0.01\n    lgb_params['n_estimators'] = 20000\n    lgb_params['early_stopping_rounds'] = 100\n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\nelse:\n    lgb_params['learning_rate'] = 0.01\n    lgb_params['n_estimators'] = 800\n    lgb_params['early_stopping_rounds'] = 100    \n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, NFOLDS=2)","46bc3857":"# submission\ntest_predictions['isFraud'] = test_predictions['prediction']\ntest_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","82a5f277":"# Pre-processing","31421430":"So many features have missing values close to 96%.","16784a3f":"Let's plot this new V_list_ ","24c5ddec":"### Plotting Functions","bfa5aa37":"As you can see, most of the graphs tends to have same distribution. So, first let's find correlation and plot the graph for the variables which are not highly correlated. ","f7568644":"## Plotting different graphs","c48b9610":"## Understanding data types and missing values present in the data","c0adf5cc":"Now, let's join the train_identity data with the train_df and then we will plot the numerical and the categorical columns","5bde17d8":"### Plotting addr graph","2854c093":"### Removing the columns which are highly correlated","1f1600a9":"### Plotting V data","159cbeec":"### Plotting addr data","c67c312f":"Since we have indentified some of the columns through plotting which were important, such as: ['card1','card2','M4','D9','addr1','addr2','dist1','dist2', 'P_emaildomain', 'R_emaildomain']. Most of these are categorical featues. Also, we will consider Transaction Data to make various featues. Let's create some new features","cca6def6":"Here, addr1 has high change to contribute to the model because high variance","30490c33":"### Plotting D data","87447c8e":"Since there are too many columns, let's plot few columns to understand if we can get any significance interpretation","cb3c5e50":"Not much to say about the result from the graph above ","b74334e1":"In the above card_list, card4 and card6 are categorical columns, so we will plot only for ['card1', 'card2', 'card3','card5']\n### Plotting density plots\nPlotting density plot in based on the fraud vs non-fraud","f5d7168c":"Column D9 seems to be important","87f31dc2":"As you can see, the variance for card 1 and card 2 is high and it is possible that they contribute high in the model building","023aa8b8":"### Finding correlation for columns starting with V","6a1eee94":"## Looking into train_identity.csv","8d77f6d4":"## Plotting M data","67b7d35d":"### Plotting 'card' graph"}}