{"cell_type":{"b581f6f2":"code","8166b702":"code","23d1d8da":"code","4067dfc7":"code","c9078e28":"code","78490174":"code","589526f3":"code","466c1e10":"code","0ad06b09":"code","58aeb9eb":"code","d75c1d1e":"code","305ff3a7":"code","da237beb":"code","6cfde6e2":"markdown"},"source":{"b581f6f2":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation,\\\nConfiguration, Action, row_col, adjacent_positions, translate, min_distance,random_agent, GreedyAgent\n\nfrom kaggle_environments import make\nimport numpy as np\nfrom random import choice\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport random\nfrom collections import deque\nimport time","8166b702":"# ACTIONS = [e.name for e in Action]\nACTIONS = ['NORTH', 'SOUTH', 'WEST', 'EAST']","23d1d8da":"env = make(\"hungry_geese\")\ndisplay(env.reset())\n\ntrainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\ntrainer.reset()","4067dfc7":"\ndef DQNet():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.Input(shape=(7,11,17)))\n    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,5), activation='relu'))\n    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))\n    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n#     model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    model.add(tf.keras.layers.Dense(32, activation='relu'))\n    model.add(tf.keras.layers.Dense(4, activation='linear'))\n    \n    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n    \n    return model\n\nmodel = DQNet()\n# model.build(input_shape=(None, 7, 11, 17))\nmodel.summary()","c9078e28":"def centerize(b):\n    dy, dx = np.where(b[0])\n    centerize_y = (np.arange(0,7)-3+dy[0])%7\n    centerize_x = (np.arange(0,11)-5+dx[0])%11\n    \n    b = b[:, centerize_y,:]\n    b = b[:, :,centerize_x]\n    \n    return b\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n        \n    b = b.reshape(-1, 7, 11)\n    b = centerize(b)\n    b = np.transpose(b, (1,2,0))\n\n    return b","78490174":"def train(env, replay_memory, model, target_model, done):\n    \n    learning_rate = 0.7 # Learning rate\n    discount_factor = 0.618\n    batch_size = 512 * 2\n\n    MIN_REPLAY_SIZE = 3000\n    if len(replay_memory) < MIN_REPLAY_SIZE:\n        return\n\n    \n    mini_batch = random.sample(replay_memory, batch_size)\n    \n    rewards = np.array([transition[2] for transition in mini_batch])\n    \n    current_states = np.array([transition[0] for transition in mini_batch])\n    current_qs_list = model.predict(current_states)\n    \n    \n    new_current_states = np.array([transition[3] for transition in mini_batch])\n    \n#     non_final_states = np.array([transition[3] for transition in mini_batch if not transition[4]])\n#     non_final_index_mask = np.array([False if transition[4] else True  for transition in mini_batch])\n    \n    \n#     future_qs_list = tf.zeros(batch_size)\n    \n    future_qs_list = target_model.predict(new_current_states)\n    \n#     next_state_reward = future_qs_list*discount_factor + rewards\n\n    X = []\n    Y = []\n    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n        if not done:\n            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n        else:\n            max_future_q = reward\n\n#         current_qs = current_qs_list[index]\n        current_qs = [0]*4\n        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n\n        X.append(observation)\n        Y.append(current_qs)\n#     print(Y)\n    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)","589526f3":"def main():\n    trainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\n    trainer.reset()\n    is_render = False\n    \n    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n    max_epsilon = 1 # You can't explore more than 100% of the time\n    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n    decay = 0.01\n    \n    # An episode a full game\n    train_episodes = 20000\n    test_episodes = 100\n\n    # 1. Initialize the Target and Main models\n    # Main Model (updated every 4 steps)\n    model = DQNet()\n    # Target Model (updated every 100 steps)\n    target_model = DQNet()\n    target_model.set_weights(model.get_weights())\n\n    replay_memory = deque(maxlen=50000)\n\n    target_update_counter = 0\n\n    # X = states, y = actions\n    X = []\n    y = []\n    \n    observation_list = []\n    steps_to_update_target_model = 0\n\n    for episode in range(train_episodes):\n        total_training_rewards = 0\n        observation = trainer.reset()\n        observation_list = []\n        \n        done = False\n        while not done:\n            observation_list.append(observation)\n            encoded_observation = make_input(observation_list)\n            \n            steps_to_update_target_model += 1\n            if is_render:\n                env.render(mode=\"ipython\", width=500, height=450)\n\n            random_number = np.random.rand()\n            # 2. Explore using the Epsilon Greedy Exploration Strategy\n            if random_number <= epsilon or len(replay_memory) < 5000:\n                # Explore\n                g_agent = GreedyAgent(Configuration({'rows': 7, 'columns': 11}))\n                action = g_agent(Observation(observation))\n                \n                action = ACTIONS.index(action)\n            else:\n                # Exploit best known action\n                # model dims are (batch, env.observation_space.n)\n                encoded_reshaped = encoded_observation.reshape(-1,7,11,17)\n                \n                predicted = model.predict(encoded_reshaped)\n                action = np.argmax(predicted)\n#             print(action)\n            new_observation, reward, done, info = trainer.step(ACTIONS[action])\n            if done and reward == 0:\n                reward = -1000\n            new_encoded_observation = make_input(observation_list)\n            replay_memory.append([encoded_observation, action, reward, new_encoded_observation, done])\n\n            # 3. Update the Main Network using the Bellman Equation\n            if steps_to_update_target_model % 4 == 0 or done:\n#                 print(\"***********Train************\")\n                train(env, replay_memory, model, target_model, done)\n\n            observation = new_observation\n            total_training_rewards += reward\n\n            if done:\n                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n                total_training_rewards += 1\n\n                if steps_to_update_target_model >= 100:\n                    print('Copying main network weights to the target network weights')\n                    target_model.set_weights(model.get_weights())\n                    steps_to_update_target_model = 0\n                break\n\n        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n#     env.close()\n    model.save('my_model.h5')\n\nif __name__ == '__main__':\n    main()","466c1e10":"obs_list = []\nobs = trainer.reset()\nobs_list.append(obs)\nmodel = tf.keras.models.load_model('my_model.h5')\n\ndone = False\ntotal_reward = 0\nwhile not done:\n    obs_list.append(obs)\n    encoded_observation = make_input(obs_list)\n    encoded_reshaped = encoded_observation.reshape(-1,7,11,17)\n                \n    predicted = model.predict(encoded_reshaped)\n#     print(predicted)\n    action = np.argmax(predicted)\n#     print(action)\n    new_obs, reward, done, info = trainer.step(ACTIONS[action])\n#     env.render(mode=\"ipython\", width=500, height=450)\n    \n    obs = new_obs\n    total_reward += reward\n    \n    if done:\n        print(f'Total reward: {total_reward}')\n        \n   \n","0ad06b09":"%%writefile main.py\n\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\n\np = Path('\/kaggle_simulations\/agent\/')\nif p.exists():\n    sys.path.append(str(p))\nelse:\n    p = Path('__file__').resolve().parent","58aeb9eb":"%%writefile -a main.py\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\ndef centerize(b):\n    dy, dx = np.where(b[0])\n    centerize_y = (np.arange(0,7)-3+dy[0])%7\n    centerize_x = (np.arange(0,11)-5+dx[0])%11\n    \n    b = b[:, centerize_y,:]\n    b = b[:, :,centerize_x]\n    \n    return b\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n        \n    b = b.reshape(-1, 7, 11)\n    b = centerize(b)\n    b = np.transpose(b, (1,2,0))\n\n    return b\n\nmodel = tf.keras.models.load_model(str(p\/'my_model.h5'))\n\nobses = []\n\ndef agent(obs_dict, config_dict):\n    obses.append(obs_dict)\n\n    X_test = make_input(obses)\n#     X_test = np.transpose(X_test, (1,2,0))\n    X_test = X_test.reshape(-1,7,11,17) # channel last.\n    \n    # avoid suicide\n#     obstacles = X_test[:,:,:,[8,9,10,11,12]].max(axis=3) - X_test[:,:,:,[4,5,6,7]].max(axis=3) # body + opposite_side - my tail\n#     obstacles = np.array([obstacles[0,2,5], obstacles[0,4,5], obstacles[0,3,4], obstacles[0,3,6]])\n    \n    y_pred = model.predict(X_test) \n\n    \n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(y_pred)]","d75c1d1e":"from kaggle_environments import make\nenv = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run(['main.py', 'main.py','main.py','main.py'])\nenv.render(mode=\"ipython\", width=500, height=450)","305ff3a7":"!tar -czf submission.tar.gz main.py my_model.h5","da237beb":"# str(p\/'model')","6cfde6e2":"https:\/\/towardsdatascience.com\/deep-q-learning-tutorial-mindqn-2a4c855abffc"}}