{"cell_type":{"0283ad28":"code","29c073c6":"code","5a4b68a9":"code","5d60694e":"code","5cd2c02d":"code","4f7c473a":"code","4d9cd2f1":"code","0480f4de":"code","2d469979":"code","e9c799b4":"code","318dd1c3":"code","2b2dad1a":"code","b23a849e":"code","68360ad1":"code","7fb27519":"code","c1539694":"code","4bf86ef1":"code","b3733e9a":"code","8f72c211":"code","f9f875cd":"code","1c90f51c":"code","04595fab":"code","f46db211":"code","3293fd5d":"code","be1ff39b":"code","e407fc91":"code","e203955c":"code","23c4f8af":"code","0d0c0025":"code","ba08334e":"code","9d87520f":"code","c2bd10a0":"code","c4f51fe8":"code","3f53be79":"code","1882fa82":"code","f57da17c":"code","7ba11ecf":"code","35276601":"markdown","b71e0989":"markdown","a1e02325":"markdown","5422e927":"markdown","fe743b96":"markdown","87be3fa4":"markdown","8a5952c1":"markdown","dcd9ad58":"markdown","e24fae77":"markdown","96e0d818":"markdown","6eb743f0":"markdown","5760a8c8":"markdown","6b6304fe":"markdown","9b90473f":"markdown","9378827c":"markdown","8dac54e9":"markdown","4fe39f67":"markdown","5823acba":"markdown","8cab7bd2":"markdown","e3d69670":"markdown","7c7fd709":"markdown","a8cf47c6":"markdown","52e40d50":"markdown","cbbe974a":"markdown","e3ea799e":"markdown","2b5e18bd":"markdown","1006ceff":"markdown","3c2c5c34":"markdown","cfaa66e9":"markdown"},"source":{"0283ad28":"import warnings \nwarnings.filterwarnings('ignore')\n\n# ************** DATA MANIPULATION *****************\n\nimport pandas as pd\nimport numpy as np\n\n# ************** DATA VISUALIZATION ****************\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\n\n# ************* METRICS ****************************\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nsns.set(style='white', context='notebook')\n\n","29c073c6":"df = pd.read_csv('..\/input\/adult-census-income\/adult.csv')","5a4b68a9":"df.info()","5d60694e":"df.describe().T","5cd2c02d":"df[df == '?'] = np.nan\ndf.info()","4f7c473a":"for col in ['workclass', 'occupation', 'native.country']:\n    df[col].fillna(df[col].mode()[0], inplace=True)","4d9cd2f1":"sns.set_style(\"whitegrid\")\nplt.figure(figsize = (8,5))\nplt.title('Income Distribution of Adults', fontsize=18, fontweight='bold')\neda_percentage = df['income'].value_counts(normalize = True).rename_axis('income').reset_index(name = 'Percentage')\n\nax = sns.barplot(x = 'income', y = 'Percentage', data = eda_percentage.head(10), palette='Greens_r')\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.0%}', (x + width\/2, y + height*1.02), ha='center', fontweight='bold')","0480f4de":"def age_group(x):\n    x = int(x)\n    x = abs(x)\n    if( 18 < x < 31 ):\n        return \"19-30\"\n    if( 30 < x < 41 ):\n        return \"31-40\"\n    if( 40 < x < 51 ):\n        return \"41-50\"\n    if( 50 < x < 61 ):\n        return \"51-60\"\n    if( 60 < x < 71 ):\n        return \"61-70\"\n    else:\n        return \"Greater than 70\"\n\ndf['age_group'] = df['age'].apply(age_group)","2d469979":"plt.figure(figsize=(12,6))\norder_list = ['19-30', '31-40', '41-50', '51-60', '61-70', 'Greater than 70']\nsns.countplot(df['age_group'], hue = df['income'], palette='Greens_r', order = order_list)\nplt.title('Income of Individuals of Different Age Groups', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","e9c799b4":"plt.figure(figsize=(12,6))\n#order_list = ['19-30', '31-40', '41-50', '51-60', '61-70', 'Greater than 70']\nsns.countplot(df['workclass'], hue = df['income'], palette='Greens_r')\nplt.title('Income of Individuals of Different Working CLasses', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=16,rotation = 90)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","318dd1c3":"plt.figure(figsize=(15,6))\norder_list = ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th', \n                'HS-grad ', 'Some-college', 'Bachelors', 'Masters', 'Doctorate', 'Prof-school', \n              'Assoc-acdm', 'Assoc-voc']\nsns.countplot(df['education'], hue = df['income'], palette='Greens_r', order= order_list)\nplt.title('Income of Individuals of Different Education Levels', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=16,rotation = 90)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","2b2dad1a":"plt.figure(figsize=(12,6))\n#order_list = ['19-30', '31-40', '41-50', '51-60', '61-70', 'Greater than 70']\nsns.countplot(df['marital.status'], hue = df['income'], palette='Greens_r')\nplt.title('Income of Individuals of Different Marital Status', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=16,rotation = 90)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","b23a849e":"plt.figure(figsize=(18,6))\n#order_list = ['19-30', '31-40', '41-50', '51-60', '61-70', 'Greater than 70']\nsns.countplot(df['occupation'], hue = df['income'], palette='Greens_r')\nplt.title('Income of Individuals of Different Occupations', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=16,rotation = 90)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","68360ad1":"plt.figure(figsize=(12,6))\n#order_list = ['19-30', '31-40', '41-50', '51-60', '61-70', 'Greater than 70']\nsns.countplot(df['relationship'], hue = df['income'], palette='Greens_r')\nplt.title('Income of Individuals of Different Relationship', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","7fb27519":"plt.figure(figsize=(12,6))\n#order_list = ['19-30', '31-40', '41-50', '51-60', '61-70', 'Greater than 70']\nsns.countplot(df['race'], hue = df['income'], palette='Greens_r')\nplt.title('Income of Individuals of Different Races', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","c1539694":"plt.figure(figsize=(12,6))\n#order_list = ['19-30', '31-40', '41-50', '51-60', '61-70', 'Greater than 70']\nsns.countplot(df['sex'], hue = df['income'], palette='Greens_r')\nplt.title('Income of Individuals of Different Genders', fontsize=18, fontweight='bold')\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)","4bf86ef1":"df['income']=df['income'].map({'<=50K': 0, '>50K': 1})\n","b3733e9a":"plt.figure(figsize = (12,10))\nplt.title(\"Correlation between different features of the dataset\", fontsize = 18, fontweight = 'bold')\nsns.heatmap(df.corr(), cmap = 'Greens_r', annot = True)\nplt.xticks(fontsize=12, rotation = 90)\nplt.yticks(fontsize=12, rotation = 90)\nplt.legend(fontsize=12)","8f72c211":"#Distributing Age column in 3 significant parts and plotting it corresponding to the output feature(income)\n\n#Combining the lower grades of education together\n\ndf.drop(['education.num'], axis = 1, inplace = True)\ndf['education'].replace(['11th', '9th', '7th-8th', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'],\n                             ' School', inplace = True)\n\ndf['race'].replace(['Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'],' Other', inplace = True)\n\n","f9f875cd":"df.drop('age_group', inplace = True, axis = 1)","1c90f51c":"from sklearn.preprocessing import LabelEncoder\ncategorical = ['workclass','education', 'marital.status', 'occupation', 'relationship',\n               'race', 'sex','native.country']\nlabel_encoder = LabelEncoder()\nfor col in categorical:\n    label_encoder.fit(df[col])\n    df[col] = label_encoder.transform(df[col])\n    \nfrom sklearn.model_selection import train_test_split\nx = df[['workclass','education', 'marital.status', 'occupation', 'relationship',\n               'race', 'sex','native.country', 'age', 'fnlwgt', 'capital.gain', 'capital.loss', 'hours.per.week']]\ny = df['income']\n    \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n","04595fab":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x.columns)\nx_test = pd.DataFrame(scaler.transform(x_test), columns = x.columns)","f46db211":"def sigmoid(x, weight):\n    z = np.dot(x, weight)\n    return 1 \/ (1 + np.exp(-z))","3293fd5d":"def loss(h, y):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()","be1ff39b":"def gradient_descent(x, h, y):\n    return np.dot(x.T, (h - y)) \/ y.shape[0]\ndef update_weight_loss(weight, learning_rate, gradient):\n    return weight - learning_rate * gradient","e407fc91":"def log_likelihood(x, y, weights):\n    z = np.dot(x, weights)\n    ll = np.sum( y*z - np.log(1 + np.exp(z)) )\n    return ll","e203955c":"def gradient_ascent(x, h, y):\n    return np.dot(x.T, y - h)\ndef update_weight_mle(weight, learning_rate, gradient):\n    return weight + learning_rate * gradient","23c4f8af":"import time\nstart_time = time.time()\n\nnum_iter = 100000\n\nintercept = np.ones((x.shape[0], 1)) \nx = np.concatenate((intercept, x), axis=1)\ntheta = np.zeros(x.shape[1])\n\nfor i in range(num_iter):\n    h = sigmoid(x, theta)\n    gradient = gradient_descent(x, h, y)\n    theta = update_weight_loss(theta, 0.1, gradient)\n    \nprint(\"Training time (Log Reg using Gradient descent):\" + str(time.time() - start_time) + \" seconds\")\nprint(\"Learning rate: {}\\nIteration: {}\".format(0.1, num_iter))","0d0c0025":"result = sigmoid(x, theta)","ba08334e":"f = pd.DataFrame(np.around(result, decimals=6)).join(y)\nf['pred'] = f[0].apply(lambda x : 0 if x < 0.5 else 1)\nprint(\"Accuracy (Loss minimization):\")\nf.loc[f['pred']==f['income']].shape[0] \/ f.shape[0] * 100","9d87520f":"start_time = time.time()\nnum_iter = 100000\nx2 = df[['workclass','education', 'marital.status', 'occupation', 'relationship',\n               'race', 'sex','native.country', 'age', 'fnlwgt', 'capital.gain', 'capital.loss', 'hours.per.week']]\n\nintercept2 = np.ones((x2.shape[0], 1))\n\nx2 = np.concatenate((intercept2, x2), axis=1)\ntheta2 = np.zeros(x2.shape[1])\n\nfor i in range(num_iter):\n    h2 = sigmoid(x2, theta2)\n    gradient2 = gradient_ascent(x2, h2, y) #np.dot(X.T, (h - y)) \/ y.size\n    theta2 = update_weight_mle(theta2, 0.1, gradient2)\n    \nprint(\"Training time (Log Reg using MLE):\" + str(time.time() - start_time) + \"seconds\")\nprint(\"Learning rate: {}\\nIteration: {}\".format(0.1, num_iter))\n","c2bd10a0":"result2 = sigmoid(x2, theta2)\nprint(\"Accuracy (Maximum Likelihood Estimation):\")\nf2 = pd.DataFrame(result2).join(y)\nf2.loc[f2[0]==f2['income']].shape[0] \/ f2.shape[0] * 100","c4f51fe8":"from sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nx = df[['workclass','education', 'marital.status', 'occupation', 'relationship',\n               'race', 'sex','native.country', 'age', 'fnlwgt', 'capital.gain', 'capital.loss', 'hours.per.week']]\ny = df['income']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n    \nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x.columns)\nx_test = pd.DataFrame(scaler.transform(x_test), columns = x.columns)","3f53be79":"from sklearn.metrics import accuracy_score\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\n\nY_pred = logreg.predict(x_test)\n\nacc = accuracy_score(y_test, Y_pred)\nprint(\"Logistic Regression\",acc)\n\n","1882fa82":"X = df[['workclass','education', 'marital.status', 'occupation', 'relationship',\n               'race', 'sex','native.country', 'age', 'fnlwgt', 'capital.gain', 'capital.loss', 'hours.per.week']]\ny = df['income']\n\n \nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nlr = LogisticRegression(solver='lbfgs', max_iter=10000)\nrs = []\nacc = []\nfor i in range(1,25,1):\n    X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = i)\n    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n    model_lr_rs = lr.fit(X_train, y_train)\n    predict_values_lr_rs = model_lr_rs.predict(X_test)\n    acc.append(accuracy_score(y_test, predict_values_lr_rs))\n    rs.append(i)","f57da17c":"plt.figure(figsize=(10,10))\nplt.plot(rs, acc, color ='red')\n\nfor i in range(len(rs)):\n    print(rs[i],acc[i])","7ba11ecf":"for i in range(0,24):\n    if acc[i] > 0.808:\n        print(acc[i])","35276601":"\n<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Logistic Regression Conclusion: <\/h3>\n\nWe see that the **Logistic Regression using Sklearn** gives us a maximum accuracy of **80.9** %.\nThinking about this, the model that we prepared from scratch was not so bad as compared to the inbuilt one!\n\n\n\n","b71e0989":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Logistic Regression using MLE: <\/h3>\n\nHere, we see that on implementing **Logistic Regression** from Scratch using **Maximum Likelihood Estimation**, we are able to achieve an accuracy of **78.9** %. \nLet us try importing Scikit Learn and compare our model with the inbuilt Logistic Regression of the package.\n","a1e02325":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nA few points to note over here:-\n\n* Adults in **Exec-managerial** role are equally likely to earn more than **50K** dollars an year. \n* There's close to **33%** probablity for an adult in **Prof-specialty** to earn more than **50K** dollars an year.\n* Adults working in **Farming-fishing**, **Machine-op-inspect**, **Other-service**, **Adm-clerical**, **Transport-moving** are very less likely to earn more than **50K** dollars an year.\n* Around **25%** of the people working in **Sales** earn more than **50K** dollars an year.","5422e927":"![Alt Text](https:\/\/www.owen.org\/wp-content\/uploads\/inequaltyscreencap.png)\n\n<h1 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> About this Kernel <\/h1>\n\nThis dataset has been taken from the famous UCI Machine Learning Repository. The goal of this notebook is to accurately predict whether or not an adult makes more than 50000 US Dollars in an year on the basis of the feautures given. \n\n\n<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Introduction <\/h3>\n\nInt his notebook, I will be using only one Machine Learning Algorithm to implement the predictions. This notebook is going  to be different from my previous works because I'll try to implement Logistic Regression from scratch rather than using scikit-learn. \n\n> So, without any further delay, lets get into it!\n","fe743b96":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> About the Dataset <\/h3>\n\n* **Age**: Describes the age of individuals. Continuous.\n* **Workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n* **fnlwgt**: Continuous.\n* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n* **education-num**: Number of years spent in education. Continuous.\n* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n* **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n* **sex**: Female, Male.\n* **capital-gain**: Continuous.\n* **capital-loss**: Continuous.\n* **hours-per-week**: Continuous.\n* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n* **salary**: >50K,<=50K","87be3fa4":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Gradient Ascent: <\/h3>\n\n* So after seeing the **Cost Function** and **Sigmoid Function**, now let\u2019s think of an algorithm which can combine these two function and gives us the desired result. \n* The algorithm is the Gradient Ascent algorithm.\n* So **Gradient Ascent** is an iterative optimization algorithm for finding local maxima of a differentiable function. \n* The algorithm moves in the direction of gradient calculated at each and every point of the cost function curve till the stopping criteria meets.\n\n![Alt Text](https:\/\/slidetodoc.com\/presentation_image_h\/23f0886a9c3cfd7f583d5c571e50f5dc\/image-30.jpg)\n\n\n> The idea behind **Gradient Ascent** is that gradient points **\u2018uphill\u2019**. \n\nSo if you slowly slowly moves towards the direction of gradient then you eventually make it to the **global maxima**. \n\nGradient ascent has an analogy in which we have to imagine ourselves at the bottom of a mountain valley and left stranded and blindfolded, our objective is to reach the top of the hill. Now to maximize our log likelihood we need to run the gradient ascent function on each parameter i.e.\n\n**weights = weights + learning_rate * gradient**\n\n","8a5952c1":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nIn the above graph, we have segregated the incomes of adults on the basis of their different working classes.\n\n* It is quite interesting to see that **Self-Employed** is the only category where the number of people who earn more than 50K dollars exceed those earning less than it! It might be conducive to suggest that when you handle your own business, start-up, etc. you are more likely to earn better if it gets successful!\n* People working in the **Private Sector** face a significant difference between their pays where more than **75%** of them earn less than 50K dollars an year!\n* There is a very minute difference between the number of people whose income is more or less than 50K dollars an year who work for the **Federal Govt.**\n* People belonging to the category **Without-Pay** and **Never-Worked** hardly contain any data in both the categories.","dcd9ad58":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Maximum Likelihood Estimation: <\/h3>\n\n\n* It its simplest, **MLE** is a method for estimating parameters. \n* Every time we fit a statistical or machine learning model, we are estimating parameters. A single variable linear regression has the equation:\n\n<h4> Y = B0 + B1*X <\/h4>\n\nOur goal when we fit this model is to estimate the parameters **B0** and **B1** given our observed values of **Y** and **X**. **Maximum Likelihood Estimation** is a way to estimate the parameters of a model, given what we observe.\n\nMLE asks the question, **\u201cGiven the data that we observe (our sample), what are the model parameters that maximize the likelihood of the observed data occurring?\u201d**\n","e24fae77":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\n* One thing to note here is that **Married-civ-spouse** is the only category which has comparable number of people belonging to both categories. \n* For others, there are less than **25%** of the adults earning more than **50K** dollars an year.","96e0d818":"##### ","6eb743f0":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nHere, we have tried to plot a correlation map in order to see whether or not the independent features are related to the dependent features.\n\n* We see that most of the features are positively correlated with the **Income Variable**.\n* An important thing to note over here is that, we have not covered the object datatypes in this correlation graph. ","5760a8c8":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nIn the EDA Section, I wanted to start by analysing our target variable first. We have come across a very obvious observation!\n\n* The number of people earning more than **50K dollars** an year is one third of the people earning less than it.\n* We should also keep in mind that this data was collected in **1996**, so 50K dollars of that tme, might not be the same in today's time!","6b6304fe":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> About Logistic Regression: <\/h3>\n\n\nBefore we start coding let us first understand or atleast try to understand the things happening at the back-end of **Scikit Learn's Logistic Regression**. The aim of this section is to explain the math behind Logistic Regression and to accomplish the first objective of this kernel. \n\nTo be able to do this we must answer the question, how does a Logistic Regression work? In theory, a Logistic regression takes input and returns an output of probability, a value between 0 and 1. How does a Logistic Regression do that? With the help of a function called a logistic function or most commonly known as a **sigmoid**. This sigmoid function is reponsible for predicting or classifying a given input.\n\n<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> About the Sigmoid function: <\/h3>\n\n* It is a mathematical function having a characteristic that can take any real value and map it to between 0 to 1 shaped like the letter \u201cS\u201d. \n* The sigmoid function also called a **logistic function**.\n* The sigmoid function g(z) takes features and weights z as an input and returns a result between 0 and 1. \n* The output of the sigmoid function is an actual prediction \u0177.\n* So, if the value of z goes to **positive infinity** then the predicted value of y will become 1 and if it goes to **negative infinity** then the predicted value of y will become 0. \n* And if the outcome of the sigmoid function is more than 0.5 then we classify that label as **class 1** or positive class and if it is less than 0.5 then we can classify it to negative class or label as **class 0**.\n\n![Alt Text](https:\/\/miro.medium.com\/max\/932\/1*bCCcQhMjHGaI89i-7i3xFw.png)\n","9b90473f":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nWe make an interesting observation over here. We see that people earning more than 50K dollars increase as the age increases upto a certain extent.\n* As one might imagine, the number of people earning more than **50K** is quite negligible amongst people of **age group 19-30**.\n* For the people of age group **41-50** and **51-60**, the number of people earning **more than 50K** is quite comparable to those earning less than it!","9378827c":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> About the Gradient Descent: <\/h3>\n\n**Gradient descent** is an iterative optimization algorithm, which finds the minimum of a differentiable function. In this process, we try different values and update them to reach the optimal ones, minimizing the cost.\n\n* Lets think first step, every thing starts with initializing weights and bias. Therefore cost is dependent with them.\n* In order to decrease cost, we need to update **weights** and **bias**.\n* In other words, our model needs to learn the **parameters weights** and **bias** that minimize cost function. This technique is called gradient descent.\n\n![Alt Text](https:\/\/i.stack.imgur.com\/zgdnk.png)\n\n\n1. The idea is you first select any random point from the function. \n2. Then you need to compute the derivative of J(). \n3. This will point to the direction of the **local minimum**. \n4. Now multiply that resultant gradient with the **Learning Rate**. \n5. The Learning Rate has no fixed value, and is to be decided based on problems.\n6. Now, you need to subtract the result from to get the new .\n7. This update of should be simultaneously done for every (i).\n\n**Do these steps repeatedly until you reach the local or global minimum. By reaching the global minimum, you have achieved the lowest possible loss in your prediction.**\n\n\n![Alt Text](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcS8pJOKA1xjb4sG01ekpv-nwdMHa-OSTTyqHRYDSBEO4W6PE2f3dkHJoDltb4lN2d-dAOs&usqp=CAU)","8dac54e9":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nAn important point to notice over here is that, except **Whites** there are very few people of different races. Due to this one may fail to notice the exact percentage and relationship of people earning more than 50K dollars an year. ","4fe39f67":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nWe can draw a few insightful conclusions from this graph.\n\n* For the people who have completed their **education up till 12th Standard**, there are just a handful of them who earn more than 50K dollars an year. Most of the people end up earning below 50K!\n* For the people belonging to **Bachelors**, **Masters**, **Doctorate**, **Prof-school** category in the education level, there are more number of people who are earning greater than **50K** dollars an year than the number of people earning less than it.\n* In case of **Assoc-acad** or **Assoc-voc**, there are a few people who earn more than **50K** dollars an year!","5823acba":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\nWe can see a significant gap in the earnings between males and females over here. \n\n* For **Females**, there are less than **10%** of the adults earning more than **50K** dollars an year.\n* In case of **Males** there are close to **33%** of them earning more than **50K** dollars an year.","8cab7bd2":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Final Implementation: <\/h3>\n","e3d69670":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\n* **Wives** are equally likely to earn more than **50K** dollars an year.\n* For **Husbands**, although significant, there is less possibility of them to earn more than **50K** dollars an year.\n* There are just a handful of **Unmarried** people earning more than **50K** dollars an year.\n","7c7fd709":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\n* The dataset contains absolutely **no null values**! \n* Age, Final Weight, Education Number, Capital Gain, Capital Loss and Hours Per Week are integer columns.\n* There are no Float Datatypes in the dataset.\n* Workclass, Education, Marital Status, Occupation, Relationship, Race, Sec, Native Country and Income are of object datatypes.\n* Although the dataset does not contain any null values, a closer look (see cell 3) tells us that there are a lot of **'?'** values in our dataset. We will have to **replace** those values!","a8cf47c6":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\n* The minimum and maximum age of people in the dataset is 19 and 90 years respectively, while the average age is 37.\n* The minimum and maximum years spent on education is 1 and 16 respectively, whereas the mean education level is 10 years.\n* While the minimum and average capital gain is 0, maximum is 99999. This seems odd, maybe some error within the data collection.\n* The number of hours spent per week varies between 1 to 99 and the average being 40 hours.","52e40d50":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Loading the data <\/h3>\n","cbbe974a":"df.head()","e3ea799e":"<h1 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Exploratory Data Analysis <\/h1>\n\n![Alt Text](https:\/\/www.statistika.co\/images\/services\/Exploratory%20Data%20Analysis%20-%20EDA%201000x468.jpg)\n\n**Before beginning, let us understand a bit about our data.**\n> Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n\nPrediction task is to determine whether a person makes over 50K a year.","2b5e18bd":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Observation: <\/h3>\n\n* We can see that the columns **workclass**, **occupation**, and **native.country** contains null values. We would have to find out a way to deal with them!\n* There can be many ways to impute missing values, but right now, for the sake of simplicity we would impute them using **mode**!","1006ceff":"<h3 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> About the Loss Function: <\/h3>\n\nWeights is a vital part of **Logistic Regression** and other Machine Learning algorithms and we want to find the best values for them. To start we pick random values and we need a way to measure how well the algorithm performs using those random weights. That measure is computed using the loss function. \n\nThe **loss function** is defined as:\n\n![Alt Text](https:\/\/miro.medium.com\/max\/1838\/1*dEZxrHeNGlhfNt-JyRLpig.png)\n\n\n","3c2c5c34":"<h1 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Summary and Conclusion: <\/h1>\n\n\nIn this kernel, we've created a **logistic regression from scratch**. We've learned the computations happening at the back-end of a Logistic Regression. We've transormed these equations and mathematical functions into python codes. We've trained our logistic regression function in two ways: through loss minimizing using gradient descent and maximizing the likelihood using gradient ascent. The Adult Income Dataset was used for training and also evaluation.\n\nI'm still trying to find out how I could improve the accuracy of the **Loss Minimizing Logistic Regression**. If you can help me with that, I'll be glad to improve this notebook!\n\nDespite all of these, our function performed quite well I would say, (LOL) it's not that far out from the accuracy of sklearn, however there are other metrics to consider in comparing these models as well.\n\nTo wrap things up let us review our objectives and wether we've accomplished them. The first objective was to understand the dataset itself and our EDA helped us gain better insights of the same. Secondly, we wanted to understand the theory behind Logistic Regression. The third objective was to implement the Logistic Regression without using built-in Logistic Regression libraries, yes we've done that as well, and it was trained, and evaluated. \n\n\n> This logistic regression implementation would probably be never used in production and it is unlikely that it will defeat sklearn's own LogisticRegression module, however the goal of this kernel was to understand intrecately the structure of different algorithms, in this case, Logistic Regression. \n\nThank you so much for checking out this notebook!","cfaa66e9":"<h1 style = \"font-family: Comic Sans MS;background-color:#7DF0A5\t\"> Model Implementation <\/h1>\n\n* So, coming to the part I'm finally excited about! \n* This notebook is special to me as it is the first time I'm trying to implement any model from scratch. \n\n* Here, I'll be using **Logistic Regression Model** and try to build it from scratch. \n* I will then do the same by using Scikit-learn package and see if there's a major difference in the accuracy!"}}