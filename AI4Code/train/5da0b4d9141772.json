{"cell_type":{"d3b5e5d6":"code","fd3e5ddc":"code","374d158e":"code","f80bfd0a":"code","03aeaea6":"code","2d9cc8ac":"code","b1438726":"code","fd2110b5":"code","1d2f0253":"code","427b2a62":"code","e60895f8":"code","3e2c5ae9":"code","4d87a2f0":"code","5ec43756":"code","b3bcdaef":"code","35646f69":"code","b5727380":"code","63307fba":"code","ccb79920":"code","346dd09f":"code","2b371eee":"code","4130b62e":"code","a92a19ca":"code","67817888":"code","454f17ba":"code","1b9bab1c":"code","8a089b37":"code","b6286a26":"code","9b0f4946":"code","f6173895":"code","694f0ae1":"code","b8eb9e56":"code","fc5ba3c6":"code","918b96f1":"code","9da6345c":"code","82afb792":"code","2d381066":"code","08b20dc8":"code","3c931da4":"code","0537d936":"code","9d1e7cbf":"code","31342541":"code","4102965b":"code","31202f36":"code","18f900c8":"code","d30f8e0f":"code","13dc46d2":"code","024d3955":"markdown","8f34207b":"markdown","2b1a05b1":"markdown","95eb4030":"markdown","6add7c26":"markdown","02923573":"markdown","9232577f":"markdown","dbe556d4":"markdown","30cc53d0":"markdown","0f14c6a2":"markdown","528b23ce":"markdown","18c3e413":"markdown","087ba326":"markdown","7ad71691":"markdown","e2e992e9":"markdown","a5e819f5":"markdown","a46f34a6":"markdown","10f444e9":"markdown","9c0b0557":"markdown","6c5fd1a0":"markdown","ab6ff2a4":"markdown","8b023454":"markdown","78675148":"markdown","a4ff2085":"markdown","9fc37f3d":"markdown","a1c2ada6":"markdown","ac3f501a":"markdown","5d49d554":"markdown","c74740ea":"markdown","8fae3b28":"markdown","d3167b47":"markdown","661a1b6b":"markdown","bbf73675":"markdown","510fa001":"markdown","b4457746":"markdown","95849925":"markdown","ac60e333":"markdown","12af7c74":"markdown","cd81e04d":"markdown","bf541834":"markdown","0afbca0f":"markdown","a290b036":"markdown","3460d7e9":"markdown","14f79b02":"markdown","999a7dc3":"markdown","c6a838f6":"markdown","d5013519":"markdown","381fec3f":"markdown"},"source":{"d3b5e5d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport seaborn as sns\n\nfrom collections import Counter\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fd3e5ddc":"data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\nprint(plt.style.available)\nplt.style.use('ggplot')","374d158e":"data.head()","f80bfd0a":"data.info()","03aeaea6":"data.describe()","2d9cc8ac":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Sex\"\n        output: bar plot & value count    \n    \"\"\"\n    # get feature\n    var = data[variable]\n    # caount number of categorical variable (value\/sample)\n    varValue = var.value_counts()\n    \n    #visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","b1438726":"category = [\"sex\", \"cp\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]\nfor c in category:\n    bar_plot(c)\n","fd2110b5":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(data[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","1d2f0253":"numericVar = [\"age\", \"trestbps\", \"chol\", \"fbs\", \"thalach\", \"oldpeak\"]\nfor n in numericVar:\n    plot_hist(n)","427b2a62":"# sex - target\ndata[[\"sex\", \"target\"]].groupby([\"sex\"], as_index = False).mean().sort_values(by = \"target\", ascending =False)","e60895f8":"# cp - target\ndata[[\"cp\", \"target\"]].groupby([\"cp\"], as_index = False).mean().sort_values(by = \"target\", ascending =False)","3e2c5ae9":"# restecg - target\ndata[[\"restecg\", \"target\"]].groupby([\"restecg\"], as_index = False).mean().sort_values(by = \"target\", ascending =False)","4d87a2f0":"# exang - target\ndata[[\"exang\", \"target\"]].groupby([\"exang\"], as_index = False).mean().sort_values(by = \"target\", ascending =False)","5ec43756":"# slope - target\ndata[[\"slope\", \"target\"]].groupby([\"slope\"], as_index = False).mean().sort_values(by = \"target\", ascending =False)","b3bcdaef":"# ca - target\ndata[[\"ca\", \"target\"]].groupby([\"ca\"], as_index = False).mean().sort_values(by = \"target\", ascending =False)","35646f69":"# thal - target\ndata[[\"thal\", \"target\"]].groupby([\"thal\"], as_index = False).mean().sort_values(by = \"target\", ascending =False)","b5727380":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        #Detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        #store indices\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","63307fba":"data.loc[detect_outliers(data,[\"age\", \"trestbps\", \"chol\", \"fbs\", \"thalach\", \"oldpeak\"])]","ccb79920":"data.columns[data.isnull().any()]","346dd09f":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(data[[\"age\", \"trestbps\", \"chol\", \"fbs\", \"thalach\", \"oldpeak\",\n                      \"sex\", \"cp\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]].corr(), annot = True)\nplt.show()","2b371eee":"g = sns.factorplot(x = \"thal\", y = \"target\", data = data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Disease Probability\")\nplt.show()","4130b62e":"g = sns.factorplot(x = \"ca\", y = \"target\", data = data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Disease Probability\")\nplt.show()","a92a19ca":"g = sns.factorplot(x = \"slope\", y = \"target\", data = data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Disease Probability\")\nplt.show()","67817888":"g = sns.factorplot(x = \"exang\", y = \"target\", data = data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Disease Probability\")\nplt.show()","454f17ba":"g = sns.factorplot(x = \"cp\", y = \"target\", data = data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Disease Probability\")\nplt.show()","1b9bab1c":"g = sns.FacetGrid(data, col = \"target\", size = 6)\ng.map(sns.distplot, \"oldpeak\", bins = 25)\nplt.show()","8a089b37":"g = sns.FacetGrid(data, col = \"target\")\ng.map(sns.distplot, \"thalach\", bins = 25)\nplt.show()","b6286a26":"g = sns.FacetGrid(data, col = \"target\", row = \"slope\", size = 3)\ng.map(plt.hist, \"oldpeak\", bins = 25)\ng.add_legend()\nplt.show()","9b0f4946":"g = sns.FacetGrid(data, col = \"target\", row = \"slope\", size = 3)\ng.map(plt.hist, \"thalach\", bins = 25)\ng.add_legend()\nplt.show()","f6173895":"g = sns.FacetGrid(data, col = \"target\", row = \"exang\", size = 4)\ng.map(plt.hist, \"cp\", bins = 25)\ng.add_legend()\nplt.show()","694f0ae1":"g = sns.FacetGrid(data, col = \"target\", row = \"exang\", size = 4)\ng.map(plt.hist, \"thalach\", bins = 25)\ng.add_legend()\nplt.show()","b8eb9e56":"g = sns.FacetGrid(data, col = \"target\", row = \"cp\", size = 2)\ng.map(plt.hist, \"thalach\", bins = 25)\ng.add_legend()\nplt.show()","fc5ba3c6":"g = sns.FacetGrid(data, col=\"target\", size = 8)\ng.map(plt.scatter, \"oldpeak\", \"thalach\", edgecolor=\"w\")\ng.add_legend()\nplt.show()","918b96f1":"g = sns.FacetGrid(data, col=\"target\", size = 8)\ng.map(sns.kdeplot, \"age\", \"thalach\", edgecolor=\"w\")\ng.add_legend()\nplt.show()","9da6345c":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nx,y = data.loc[:,data.columns != 'target'], data.loc[:,'target']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'. format(prediction))","82afb792":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\nknn = KNeighborsClassifier(n_neighbors=3)\nx,y = data.loc[:,data.columns != 'target'], data.loc[:,'target']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x)\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test))","2d381066":"neig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","08b20dc8":"data1 = data[data['target'] == 1]\nx = np.array(data1.loc[:,'oldpeak']).reshape(-1,1)\ny = np.array(data1.loc[:,'thalach']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('oldpeak')\nplt.ylabel('thalach')\nplt.show()","3c931da4":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('oldpeak')\nplt.ylabel('thalach')\nplt.show()","0537d936":"data.head()","9d1e7cbf":"# Ridge\nfrom sklearn.linear_model import Ridge\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2, test_size = 0.3)\nridge = Ridge(alpha = 0.1, normalize = True)\nridge.fit(x_train,y_train)\nridge_predict = ridge.predict(x_test)\nprint('Ridge score: ',ridge.score(x_test,y_test))","31342541":"# Lasso\nfrom sklearn.linear_model import Lasso\nx = np.array(data1.loc[:,['thalach','oldpeak','trestbps','chol']])\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 3, test_size = 0.3)\nlasso = Lasso(alpha = 0.1, normalize = True)\nlasso.fit(x_train,y_train)\nridge_predict = lasso.predict(x_test)\nprint('Lasso score: ',lasso.score(x_test,y_test))\nprint('Lasso coefficients: ',lasso.coef_)","4102965b":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = data.loc[:,data.columns != 'target'], data.loc[:,'target']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))","31202f36":"# visualize with seaborn library\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","18f900c8":"# ROC Curve with logistic regression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n# have disease = 1 and no disease = 0\nx,y = data.loc[:,(data.columns != 'target')], data.loc[:,'target']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","d30f8e0f":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x,y)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","13dc46d2":"# grid search cross validation with 2 hyperparameter\n# 1. hyperparameter is C:logistic regression regularization parameter\n# 2. penalty l1 or l2\n# Hyperparameter grid\nparam_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 12)\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=3)\nlogreg_cv.fit(x_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\nprint(\"Best Accuracy: {}\".format(logreg_cv.best_score_))","024d3955":"<a id = \"27\"><\/a><br>\n## ROC Curve with Logistic Regression\n* logistic regression output is probabilities\n* If probability is higher than 0.5 data is labeled 1(abnormal) else 0(normal)\n* By default logistic regression threshold is 0.5\n* ROC is receiver operationg characteristic. In this curve x axis is false positive rate and y axis is true positive rate\n* If the curve in plot is closer to left-top corner, test is more accurate.\n* Roc curve score is auc that is computation area under the curve from prediction scores\n* We want auc to closer 1\n* fpr = False Positive Rate\n* tpr = True Positive Rate","8f34207b":" <a id = \"10\"><\/a><br>\n * ca -- target\n","2b1a05b1":" <a id = \"16\"><\/a><br>  \n * slope -- oldpeak -- target","95eb4030":"* The disease risk inceases at age between 40-60 and thalach between 150-185","6add7c26":"> Patiens who has chest pain, has a very high probability of a hearth disease","02923573":"* Heart disease risk increases especially when oldpeak < 2 and thalach > 150","9232577f":" <a id = \"17\"><\/a><br>  \n * slope -- thalach -- target","dbe556d4":" <a id = \"22\"><\/a><br>  \n * thalach -- age -- target","30cc53d0":"It seems that probability of hearth disease (target in this instance) has correlation with:\n* thal (-)\n* ca (-)\n* slope (+)\n* exang (-)\n* cp (+)\n* oldpeak (-)\n* thalac (+)\n\nIt is also seen that:\n* slope has correlation with:\n    * oldpeak (-)\n    * thalac (+)\n* exang has correlation with:\n    * cp (-)\n    * thalac (-)\n* cp has correlation with:\n    * thalac (+)\n* oldpeak has correlation with:\n    * thalac (+)\n* thalac has correlation with:\n    * age (-)\n\nNow we will visualize these relations","0f14c6a2":"<a id = \"8\"><\/a><br>\n# VISUALIZATION\n* Correlation Between Features vs Hearth Disease","528b23ce":"<a id = \"13\"><\/a><br>\n* cp -- target","18c3e413":"* Linear regression\n\ny = ax + b where y = target, x = feature and a = parameter of model","087ba326":"<a id = \"18\"><\/a><br>  \n* exang -- cp -- target","7ad71691":" <a id = \"9\"><\/a>\n * thal -- target\n","e2e992e9":"<a id = \"29\"><\/a><br>\n# CONCLUTION\nIn this tutorial I tried to show you:\n* How to visualize and understand the data\n* How to implement ML models","a5e819f5":"# Which features are related with a hearth disease?\n<font color = 'blue'>\nContent:\n\n1. [LOAD AND CHECK DATA](#1)\n1. [VARIABLE DESCRIPTION](#2)\n    * [Categorical Variable](#3)\n    * [Numerical Variable](#4)\n1. [BASIC DATA ANALYSIS](#5)\n1. [OUTLIER DETECTION](#6)\n1. [MISSING VALUE](#7)\n    * [Find Missing Value](#7)\n    * [Fill Missing Value](#7)\n1. [VISUALIZATION](#8)\n    * [Correlation Between Features vs Hearth Disease](#8)\n    * [thal -- target](#9)\n    * [ca -- target](#10)\n    * [slope -- target](#11)\n    * [exang -- target](#12)\n    * [cp -- target](#13)\n    * [oldpeak -- target](#14)\n    * [thalach -- target](#15)\n    * [slope -- oldpeak -- target](#16)\n    * [slope -- thalach -- target](#17)\n    * [exang -- cp -- target](#18)    \n    * [exang -- thalach -- target](#19)\n    * [cp -- thalach -- target](#20)\n    * [oldpeak -- thalach -- target](#21)\n    * [thalach -- age -- target](#22)\n1. [IMPLEMENTING ML ALGORITHMS](#23)\n    * [K-Nearest Neighbors (KNN)](#23)\n    * [Regression](#24)\n    * [Regularized Regression](#25)\n    * [Accuracy](#26)\n    * [ROC Curve with Logistic Regression](#27)\n    * [Hyperparameter Tuning](#28)\n1. [CONCLUTION](#29)\n\n        ","a46f34a6":" <a id = \"23\"><\/a><br>  \n # IMPLEMENTING ML ALGORITHMS\n <a id = \"23\"><\/a><br>  \n ## K-Nearest Neighbors (KNN)\n * KNN is a clasification method.\n * It looks for the K number of closest data points.","10f444e9":"<a id = \"25\"><\/a><br>\n## Regularized Regression\n* Linear regression may result in overfitting because it can give high coefficient to a feature. \n* In order to solve this problem we use regularized regression. Lets check out \"Ringe\" and \"Lasso\" regressions. ","9c0b0557":"* The risk is higher for slope=2 and thalach>150 patients\n* The risk is lower for slope=1 and thalach>150 patients","6c5fd1a0":"What I have done is training my model with the data and make prediction of a possible hearth disease. Bu I do not know whether my prediction is true or not. In other means I do not know the accuracy of my model.\n\nTo overcome this, a general rule is dividing the data into train and test parts. Than we first train or fit the model with \"train\" part of the data and test the results with the \"test\" part of the data. Lets do it!","ab6ff2a4":" <a id = \"20\"><\/a><br>  \n * cp -- thalach -- target","8b023454":"* As thalach rises over 150, the risk increases","78675148":"As you can see there is a correlation between chest pain and hearth disease.","a4ff2085":"<a id = \"4\"><\/a><br>\n## Numerical Variables\n* age\n* trestbps\n* chol\n* fbs\n* thalach\n* oldpeak","9fc37f3d":"Other grid search example with 2 hyperparameter\n\n* First hyperparameter is C:logistic regression regularization parameter\n* If C is high: overfit\n* If C is low: underfit\n* Second hyperparameter is penalty(lost function): l1 (Lasso) or l2(Ridge) as we learnt at linear regression part.","a1c2ada6":"<a id = \"14\"><\/a><br>    \n* oldpeak -- target","ac3f501a":"<a id = \"6\"><\/a><br>\n# OUTLIER DETECTION","5d49d554":"* For o<oldpeak<2, there is a higher risk of disease","c74740ea":"<a id = \"26\"><\/a><br>\n## Accuracy\nLets discuss about the accuracy. The accuracy of our model shows the percentage of the correct predictions. But does it really make sence to know this percentage? Think about the %70 KNN acuracy above. Lets say that %70 of the patients have heart disease. If our model predicts that all the patients have hearth disease, than it means that the model has %70 accuracy.   \n\nTo get rid of this confusion, we calculate the confusion matrix. We calculate:\n* tp = Prediction is positive(normal) and actual is positive(normal).\n* fp = Prediction is positive(normal) and actual is negative(abnormal).\n* fn = Prediction is negative(abnormal) and actual is positive(normal).\n* tn = Prediction is negative(abnormal) and actual is negative(abnormal)","8fae3b28":"<a id = \"15\"><\/a><br>  \n* thalach -- target","d3167b47":" <a id = \"21\"><\/a><br>  \n * oldpeak -- thalach -- target","661a1b6b":"<a id = \"7\"><\/a><br>\n# MISSING VALUE\n* Find Missing Value\n* Fill Missing Value","bbf73675":"No outliers detected in the data.","510fa001":"* exang = 0 patients have a higher risk then exang = 1","b4457746":"<a id = \"2\"><\/a><br>\n# VARIABLE DESCRIPTION\n\n1. age: Age of patient\n1. sex: Gender of patient (1:Male, 0:Female)\n1. cp: chest pain type (4 values)\n1. trestbps: resting blood pressure\n1. chol: serum cholestoral in mg\/dl\n1. fbs: fasting blood sugar > 120 mg\/dl\n1. restecg: resting electrocardiographic results (values 0,1,2)\n1. thalach: maximum heart rate achieved\n1. exang: exercise induced angina (1: yes, 0: no)\n1. oldpeak: ST depression induced by exercise relative to rest\n1. slope: the slope of the peak exercise ST segment (values 0,1,2)\n1. ca: number of major vessels (0-3) colored by flourosopy\n1. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n1. target: Presence of heart disease (1: yes, 0: No)","95849925":" <a id = \"24\"><\/a><br>  \n ## Regression\n * It is a supervised learning model\n * I will show linear and logistic regression","ac60e333":"<a id = \"5\"><\/a><br>\n# BASIC DATA ANALYSIS\n* sex - target\n* cp - target\n* restecg - target\n* exang - target\n* slope - target\n* ca - target\n* thal - target","12af7c74":"There isn't any missing value so we don't need to fill either.","cd81e04d":"<a id = \"28\"><\/a><br>\n## Hyperparameter Tuning\nHyperparameter tuning:\n* try all of combinations of different parameters\n* fit all of them\n* measure prediction performance\n* see how well each performs\n* finally choose best hyperparameters","bf541834":" <a id = \"19\"><\/a><br>  \n * exang -- thalach -- target","0afbca0f":"<a id = \"12\"><\/a><br> \n* exang -- target","a290b036":"So the accuracy of my model is %63. Is this a good score? Is it possible to hit higher scores by changing the K value? Lets find it out!","3460d7e9":"<a id = \"11\"><\/a><br>\n* slope -- target\n\n","14f79b02":"* ca = 0 or 4 patients have a higher risk then ca = 1, 2 or 3","999a7dc3":"<a id = \"1\"><\/a><br>\n# LOAD AND CHECK DATA","c6a838f6":"* Patiens whose thal = 2 have a very high heart disease probability. \n* Also thal = 0 patients have a higher risk then thal = 1 or 3","d5013519":"* slope = 2 patients have a higher risk then slope = 0 or 1","381fec3f":"<a id = \"3\"><\/a><br>\n## Categorical Variables\n* sex\n* cp\n* restecg\n* exang\n* slope\n* ca\n* thal\n* target"}}