{"cell_type":{"f14259d5":"code","c42c29b2":"code","19ecd9c5":"code","76764490":"code","90d60196":"code","f28ce506":"code","f9ea9d98":"code","41b84347":"code","8f3f1ed2":"code","768951cc":"code","f8f5830f":"code","cc3fd7fa":"code","f0f2e61e":"code","e0facc94":"code","a87436ff":"code","515e0ab9":"code","2885ee11":"code","36e57922":"code","be6a8759":"code","000e6d8c":"code","2ba85c3b":"code","8c4c1270":"code","f4d9cbc4":"code","c9d91309":"code","f8e1672d":"code","6bb2c818":"code","6bc062a8":"code","e33e6eb1":"code","2cc062cb":"code","bdc06772":"code","f0692e35":"code","0916d365":"code","cb41a4bc":"code","ee1d8968":"code","0d1c19f5":"code","2610dc73":"code","79ce358c":"code","72be3ae0":"code","c902a42b":"code","3ffde67b":"code","7ccc87f4":"code","efc3af87":"code","9678a3c2":"code","c2cebdd4":"code","a8770173":"code","ae5ed847":"code","409bfbad":"code","a937bc9a":"code","df2de01f":"code","2f4b4f8c":"code","6b1e1151":"code","6c248599":"code","1309d877":"code","719db118":"code","623c1233":"code","3f6ad63b":"code","841fb90e":"code","bf6e7725":"code","c0ecc069":"code","f872bd53":"code","0f8d7ac1":"code","ee9b3ad7":"code","1f95d9af":"code","026cb37a":"code","b7adb099":"code","8743a518":"code","4289ec1f":"code","efc0ef34":"code","0118e12b":"code","e462ccac":"code","bd974a67":"code","a857e679":"code","97c97d9a":"code","729790fc":"code","511b6925":"code","e388a466":"code","e35da482":"code","a936a618":"code","20348f5d":"code","9a89b6b2":"code","57797bf2":"code","0234693c":"code","f956d31b":"code","502573fd":"code","b6a44940":"code","d2c1dc57":"code","8baa7973":"code","cc6d6e60":"code","e20b08e6":"markdown","cfa5f287":"markdown","e7ef1ba5":"markdown","e36c218c":"markdown","8c8f0995":"markdown","d825f79f":"markdown","7f4f9a76":"markdown","94508c10":"markdown","9c10f6ca":"markdown","08faf324":"markdown","9caead31":"markdown","95c1df57":"markdown","7b287911":"markdown","adc54ef2":"markdown","96f018af":"markdown","815d8582":"markdown","6501d9c6":"markdown","ab555a4f":"markdown","4be72ee6":"markdown","b48f4b70":"markdown","d8681ec4":"markdown","370de1c7":"markdown","ce60104a":"markdown","15171c43":"markdown","406f06e1":"markdown","d99c2850":"markdown","a347da26":"markdown","795dbc7b":"markdown"},"source":{"f14259d5":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os  \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c42c29b2":"trainData = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntestData = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","19ecd9c5":"trainData.describe(include='all')\n# PassengerID is jsut Identity number like row ID (passenger ID)","76764490":"testData.describe(include='all')","90d60196":"trainData.info()\n# A total of 891 rows and 12 features, with only age,embarked and cabin having missing values or nulls\n# 5 Categorical features:Name,Sex,Ticket,Cabin and Embarked\n# 4 Discrete Features: SibSp, Parch, Pclass, Survived \n# 3 Continous Numerical Features: Fare,Age,PassengerID ","f28ce506":"testData.info()\n#Age, Fare and Cabin Features have missing values","f9ea9d98":"trainData.isnull().sum()\n# for Age 177 and Cabin 687 out of  891 total rows\n# One way to solve for age is  we can fill these missing values with mean of total\n# another is too remove the data rows with missing values - but if we do so the dataset will shrink in good amount\n# so let's try first with mean values for age or dig deep in data to find out better way to fill it up\n# for Cabin, missing values represent general class or non-luxury passengers\n# for Embarked, two missing value - one way to fix ->data row can be dropped","41b84347":"# let's Visualize the data with all the features using heatmap to see null or blank values in features\nplt.figure(figsize=(5,5))\nsns.heatmap(trainData.isnull(),yticklabels=False,cbar=False,cmap='magma')\n# this is classic way to see missing data - if figure size is not given single value miss are sometime not displayed","8f3f1ed2":"testData.isnull().sum()","768951cc":"plt.figure(figsize=(5,5))\nsns.heatmap(testData.isnull(),yticklabels=False,cbar=False,cmap='magma')\n#Age, Fare and Cabin Features have missing values","f8f5830f":"trainDataCopy = trainData.copy()\ntestDataCopy = testData.copy()","cc3fd7fa":"# Let's check the count how many survived and how many died\npassengerSurvived = trainDataCopy[trainDataCopy['Survived']==1]\npassengerDied = trainDataCopy[trainDataCopy['Survived']==0]\n\n# survived vs died proportion in the dataset\n\nprint('Total =',len(trainDataCopy))\nprint('Number of survivals =',len(passengerSurvived))\nprint('% of survivals by total=',1.*(len(passengerSurvived)\/len(trainDataCopy)*100),\"%\")\nprint('Number of deaths =',len(passengerDied))\nprint('% of deaths by total =',1.*(len(passengerDied)\/len(trainDataCopy))*100,\"%\")\n\n#The dataset is imblanaced -> having ~40% survival and ~60% deaths -> Ideally we should have 50% for each\n#for training another thing that can be done is -> get the data set balanced\n# and also divid the train data further into test and train","f0f2e61e":"# Let's look into the data sets for survial and death\npassengerSurvived.describe(include='all')\n# Sex is influencing the survial and death - female passengers are more survived compared to male\n# Parch , Fare and Pclass are influencive faetures ","e0facc94":"passengerDied.describe(include='all')","a87436ff":"trainDataCopy.hist(bins=30,figsize=(10,10),color='r')\n# As per histogram  we find that most of the feature data varies and is not having common traits in it\n# Fare, Parch, Age and SibSp are tail heavy -> outliers exists\n# We can drop Passengerid","515e0ab9":"trainDataCopy['CabinBool'] = trainDataCopy['Cabin']\ntestDataCopy['CabinBool'] = testDataCopy['Cabin']\npassengerSurvived['CabinBool'] = passengerSurvived['Cabin']\npassengerDied['CabinBool'] = passengerDied['Cabin']\ntrainDataCopy['Cabin'].unique()\n# Let's look into the unique variable we have in Cabin\n# We can't do encoding for Cabin variable as its cardinality is really high, \n# Instead let's try something else by giving missing Cabin # a value 0 and all other as 1","2885ee11":"trainDataCopy['CabinBool'].fillna(0,inplace=True)\ntrainDataCopy['CabinBool'] = trainDataCopy['CabinBool'].apply(lambda x: 0 if x == 0 else 1)\n\npassengerSurvived['CabinBool'].fillna(0,inplace=True)\npassengerSurvived['CabinBool'] = passengerSurvived['CabinBool'].apply(lambda x: 0 if x == 0 else 1)\n\npassengerDied['CabinBool'].fillna(0,inplace=True)\npassengerDied['CabinBool'] = passengerDied['CabinBool'].apply(lambda x: 0 if x == 0 else 1)\n\ntestDataCopy['CabinBool'].fillna(0,inplace=True)\ntestDataCopy['CabinBool'] = testDataCopy['CabinBool'].apply(lambda x: 0 if x == 0 else 1)\n\ntrainDataCopy['CabinBool'].unique()","36e57922":"# Let's have some assuptions and check them by analysis!\n# Cabin # is for Upper Class Passenger - socio-economic status (SES)?\n# Upper Class Passenger - socio-economic status (SES)-> means More survival?\n# Cabin # means High Fare?\n# Cabin #, High Fare means -> More Survival?\n# Or there is no relation of Cabin and Fare with Survival?","be6a8759":"plt.figure(figsize=[20,5])\nsns.barplot(y='CabinBool',x='Pclass',data=trainDataCopy) \n# Class 1 Passenger data contains maximum # of Cabin (value 1) \n# Clas 2 and 3 also contains some data for Cabin, which says Cabin # is not Pclass based feature\n# (low correlation- to be checked in further steps)","000e6d8c":"sns.barplot(y='Survived',x='Pclass',data=trainDataCopy)\n# Class 1 Survives the most (value 1) then class 2 and least is class 3","2ba85c3b":"# Does cabin # is fare based then?\nplt.figure(figsize=[20,5])\nplt.scatter(y='CabinBool',x='Fare',data=trainDataCopy)\n# Cabin# is not Fare depended too, as the scatter plot explains for low fare value we have Cabin values as well as missing values ","8c4c1270":"# Does high fare means -> more survival?\nplt.figure(figsize=[20,5])\nplt.scatter(y='Survived',x='Fare',data=trainDataCopy)\n# Survival is not Fare depended too, as the scatter plot explains for low fare value we have survival values as well as non-survival passengers\n# this will be reconfirmed with low correlation too in further steps","f4d9cbc4":"plt.figure(figsize=[20,5])\nsns.countplot(x='CabinBool',hue='Survived',data=passengerSurvived)","c9d91309":"plt.figure(figsize=[20,5])\nsns.countplot(x='CabinBool',hue='Survived',data=passengerDied)","f8e1672d":"#Another way to look into same\nsns.countplot(x='Survived',hue='CabinBool',data=trainDataCopy)\n# We have Cabin # missing for survived passengers.\n# Should we reconfirm what we found? Does Cabin play any role in survival? \n# As there is no strong sign in the data to give solid proof\n# of importance of Cabin #, seeing its cardinality too, \n# We can assume it to be not a Important feature for our modelling, and give up on using it ","6bb2c818":"# There are many ways to deal with missing values\n# 1. to fill them up using a method \n# 2. To neglect the feature if it is not that important and has low influence on target\n# 3. Drop the missing value rows\ntrainDataCopy.isnull().sum()","6bc062a8":"trainDataTestCopy=trainDataCopy.copy()\ntrainDataTestCopy['Age'].fillna(trainDataTestCopy['Age'].mean(),inplace=True)","e33e6eb1":"plt.figure(figsize=[20,5])\nsns.scatterplot(x='Age',y='PassengerId',hue='Survived',data=trainDataCopy)\n# No clear cluster to say which Age group surives the most\n# Let's check correlation now and see if Age really correlates with survival","2cc062cb":"# How to fill up the missing Age values?\n\nprint('% Missing Age value in Train Dataset=',1.*((trainDataCopy['Age'].isnull().sum()\/trainDataCopy.shape[0] )* 100 ),\"%\")\nprint('% Missing Age value in Test(Output) Dataset=',1.*((testDataCopy['Age'].isnull().sum()\/trainDataCopy.shape[0] )* 100 ),\"%\")\n\n# If missing values would have been less than 5% of total dataset, I would have choose to drop all missing value rows\n# But it is ~20% and 9%, we need to look for a way to fill up missing values \n# Easiest way could be to just put in the general mean ? Let's see what it does:","bdc06772":"plt.figure(figsize=(12,7))\n\nsns.kdeplot(trainDataTestCopy['Age'], label = 'AgeMean', shade = True, color = 'b')\nsns.kdeplot(trainDataCopy['Age'], label = 'Age', shade = True, color = 'r')\n\nplt.xlabel('Age')","f0692e35":"# Directly using the Mean for filling up missing value is wrong method - this skews the dataset for a age group\n# Instead let's look for a pattern in the data for age vs other relevant features - will revisit this latter","0916d365":"testDataCopy['Fare'].fillna(testDataCopy['Fare'].mean(),inplace=True)","cb41a4bc":"testDataCopy.isnull().sum()","ee1d8968":"trainDataCopy = trainDataCopy.dropna(how='any',subset=['Embarked'])\ntrainDataCopy.isnull().sum()","0d1c19f5":"# Let's check the co-relation between the features\ncorrelation = trainDataCopy.corr()\n# heatmap  \nf, ax = plt.subplots(figsize = (8, 8))\nsns.heatmap(correlation,annot=True)\n\n# As per the correlation heatmap -> SibSp and Parch are low correlated \n# this basically means if someone has child onboard, may have spouce onboard too, or if sibling then may have parent too\n# Fare is also low correlated to Cabin \n# Another intereseting thing is Survial is also low correlated to Fare and Cabin\n# We can do feature engineering and modify features\n# In overall low correlation (less than 0.5) is seen in some features but we can negelect it","2610dc73":"#let's dig deep into viualization\n\n# Plot between features and target\n\nplt.figure(figsize=[20,20])\nplt.subplot(411)\nsns.countplot(x='Age',hue='Survived',data=trainDataCopy)\nplt.subplot(412)\nsns.countplot(x='Sex',hue='Survived',data=trainDataCopy)\nplt.subplot(413)\nsns.countplot(x='Pclass',hue='Survived',data=trainDataCopy)\nplt.subplot(414)\nsns.countplot(x='SibSp',hue='Survived',data=trainDataCopy)\n\n# Survival is quit distributed over the age but slightly higher for underage -> outlier aged passengers\n# Female are priortized and survived more than males\n# Pclass 1 and 2 has maximum survival rate\n# Low SibSp 0,1 is more favourbale for survival a passenger without dependents ","79ce358c":"plt.figure(figsize=[20,7])\nsns.countplot(x='Parch',hue='Survived',data=trainDataCopy)\nplt.figure(figsize=[20,7])\nsns.countplot(x='Fare',hue='Survived',data=trainDataCopy)\nplt.figure(figsize=[20,7])\nsns.countplot(x='Embarked',hue='Survived',data=trainDataCopy)\nplt.figure(figsize=[20,7])\nsns.countplot(x='CabinBool',hue='Survived',data=trainDataCopy)\n# Low Parch 0,1 is more favourbale for survival a passenger without dependents  \n","72be3ae0":"# KDE (Kernel Density Estimate) is used for visualizing the Probability Density of a continuous variable. \n# KDE describes the probability density at different values in a continuous variable.\n\nplt.figure(figsize=(12,7))\n\nsns.kdeplot(passengerSurvived['Age'], label = 'Survived', shade = True, color = 'b')\nsns.kdeplot(passengerDied['Age'], label = 'Died', shade = True, color = 'r')\n\nplt.xlabel('Age')","c902a42b":"plt.figure(figsize=(12,7))\n\nsns.kdeplot(passengerSurvived['Fare'], label = 'Survived', shade = True, color = 'b')\nsns.kdeplot(passengerDied['Fare'], label = 'Died', shade = True, color = 'r')\n\nplt.xlabel('Fare')","3ffde67b":"# let's see BoxPlot for checking outliers \n\nplt.figure(figsize=(5, 5))\ntrainDataCopy.boxplot(column='Fare')\nplt.figure(figsize=(5, 5))\ntrainDataCopy.boxplot(column='SibSp')\nplt.figure(figsize=(5, 5))\ntrainDataCopy.boxplot(column='Parch')\nplt.figure(figsize=(5, 5))\ntrainDataCopy.boxplot(column='Pclass')","7ccc87f4":"trainDataCopy['Fare'].max()\n# A clear outlier","efc3af87":"\ndef iWantOnlyTitle(myName,title_list):\n    \n    index1=myName.find(', ')\n    index2=myName.find('.')\n    titleGuess = myName[index1+2:index2]\n    if titleGuess not in title_list:\n        title_list.append(titleGuess)\n    return titleGuess","9678a3c2":"title_list=[]\ntrainDataCopy['Title']=trainDataCopy['Name'].map(lambda x: iWantOnlyTitle(x,title_list))\ntestDataCopy['Title']=testDataCopy['Name'].map(lambda x: iWantOnlyTitle(x,title_list))\ntrainDataCopy['Title'].describe()","c2cebdd4":"testDataCopy['Title'].unique()","a8770173":"# We need to combine some of the Titles together so that we have common bucket of unique titles for both sets of data \n# Replacing all titles with mr, mrs, miss, master -> Copied directly from the site and modified a bit\ndef replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col','Sir','Mr']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme','Mrs']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms','Miss']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    elif title in ['Dona','Lady']:\n        return 'Lady'\n    elif title in ['Master']:\n        return 'Master'\n    else:\n        return 'OtherTitle'","ae5ed847":"trainDataCopy['Title']=trainDataCopy.apply(replace_titles, axis=1)\ntestDataCopy['Title']=testDataCopy.apply(replace_titles, axis=1)","409bfbad":"trainDataCopy['Title'].unique()","a937bc9a":"testDataCopy['Title'].unique()","df2de01f":"# Found some kernels doing Pclass*Age -> really not sure why? both are reallu not correlated but still?\n# I do think by Plcass based age means has some logic \n# but still not sure how to modify the age to imporve the model\n# trainDataCopy['Age'].fillna(trainDataCopy['Age'].mean(),inplace=True)","2f4b4f8c":"#uniqueTicketcount\ntrainDataCopy['Ticket'].unique().shape[0]\n# We see a pattern in ticket feature -> some are only numerical,left are having some text and then numerical values\n# Let's separate them out","6b1e1151":"def filterTicket(fullTicket):\n    numerical = ['0','1','2','3','4','5','6','7','8','9']\n    #print(fullTicket[0])\n    if type(fullTicket)==str or fullTicket!='':        \n        if fullTicket[0] in numerical:\n            return '#'\n        else:\n            return fullTicket[0]\n    else:\n        return 'Blank'","6c248599":"trainDataCopy['filterTickets'] = trainDataCopy['Ticket'].apply(filterTicket) \ntrainDataCopy['filterTickets'].unique()","1309d877":"testDataCopy['filterTickets'] = testDataCopy['Ticket'].apply(filterTicket) \ntestDataCopy['filterTickets'].unique()","719db118":"def filterCabin(fullCabin):\n    numerical = ['0','1','2','3','4','5','6','7','8','9']\n    #print(fullTicket[0])\n    \n    if type(fullCabin)!=str:\n        return 'Blank'\n    else:\n        if fullCabin[0] in numerical:\n            return '#'\n        else:\n            return fullCabin[0]","623c1233":"trainDataCopy['filterCabin'] = trainDataCopy['Cabin'].apply(filterCabin) \ntrainDataCopy['filterCabin'].unique()","3f6ad63b":"testDataCopy['filterCabin'] = testDataCopy['Cabin'].apply(filterCabin) \ntestDataCopy['filterCabin'].unique()","841fb90e":"# First drop \ntrainDataCopy['Sex'] = trainDataCopy['Sex'].apply(lambda x: 1 if x == 'male' else 0)\ntrainData_cleaned0= trainDataCopy.drop(['Ticket','PassengerId','filterCabin','Cabin','CabinBool'],axis=1)\ntrainData_cleaned1= trainDataCopy.drop(['Ticket','PassengerId','Age','Name','filterCabin','Cabin','CabinBool'],axis=1)\n\ntestDataCopy['Sex'] = testDataCopy['Sex'].apply(lambda x: 1 if x == 'male' else 0)\ntestData_cleaned0= testDataCopy.drop(['Ticket','PassengerId','filterCabin','Cabin','CabinBool'],axis=1)\ntestData_cleaned1= testDataCopy.drop(['Ticket','PassengerId','Age','Name','filterCabin','Cabin','CabinBool'],axis=1)\n\n# Encoding - another way is to use onehotencoder from sklearn library preporcessing sub library\ntrainData_with_dummies1 = pd.get_dummies(trainData_cleaned1,drop_first=True)\n\ntestData_with_dummies1 = pd.get_dummies(testData_cleaned1,drop_first=True)\n\ntrainData_with_dummies1.head()","bf6e7725":"testData_with_dummies1","c0ecc069":"# As we found Cabin and Fare correlated - we can merge them later on or check and compare results without too\nsurvivedTarget = trainData_with_dummies1['Survived']\nx=trainData_with_dummies1.drop(['Survived'],axis=1)\nx","f872bd53":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(x)\nX_test_file = scaler.fit_transform(testData_with_dummies1)\nY_train = survivedTarget\n","0f8d7ac1":"from sklearn.model_selection import train_test_split\nX_train_model, X_test_model, y_train_model, y_test_model = train_test_split( X_train, Y_train, test_size=0.33, random_state=42)","ee9b3ad7":"X_train_model.shape[0]","1f95d9af":"X_test_model.shape[0]","026cb37a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression()\nmodel.fit(X_train_model,y_train_model)\ny_pred_model = model.predict(X_test_model)","b7adb099":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint(\"Accuracy {} %\".format( 100 * accuracy_score(y_pred_model, y_test_model)))\n","8743a518":"# Testing Set Performance\ncm = confusion_matrix(y_pred_model, y_test_model)\nsns.heatmap(cm, annot=True)","4289ec1f":"print(classification_report(y_pred_model, y_test_model))","efc0ef34":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train_model, y_train_model)","0118e12b":"y_pred_RFM = model.predict(X_test_model)","e462ccac":"print(\"Accuracy {} %\".format( 100 * accuracy_score(y_pred_RFM, y_test_model)))","bd974a67":"# Testing Set Performance\ncm = confusion_matrix(y_pred_RFM, y_test_model)\nsns.heatmap(cm, annot=True)","a857e679":"print(classification_report(y_test_model, y_pred_RFM))","97c97d9a":"import tensorflow as tf","729790fc":"X_train_model.shape","511b6925":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(units=500, activation='relu', input_shape=(18, )))\nmodel.add(tf.keras.layers.Dense(units=500, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=500, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nmodel.summary()","e388a466":"model.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])","e35da482":"# oversampler = SMOTE(random_state=0)\n# smote_train, smote_target = oversampler.fit_sample(X_train, y_train)\n# epochs_hist = model.fit(smote_train, smote_target, epochs = 100, batch_size = 50)\nepochs_hist = model.fit(X_train_model, y_train_model, epochs = 150, batch_size = 50)","a936a618":"y_pred_DLM = model.predict(X_test_model)\n#y_pred_DLM\n#y_pred_DLM = (y_pred > 0.5)","20348f5d":"y_pred_DLM = (y_pred > 0.5)","9a89b6b2":"epochs_hist.history.keys()","57797bf2":"plt.plot(epochs_hist.history['loss'])\nplt.title('Model Loss Progress During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss')\nplt.legend(['Training Loss'])","0234693c":"plt.plot(epochs_hist.history['accuracy'])\nplt.title('Model Accuracy Progress During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training Accuracy')\nplt.legend(['Training Accuracy'])","f956d31b":"# Testing Set Performance\ncm = confusion_matrix(y_test_model, y_pred_DLM)\nsns.heatmap(cm, annot=True)","502573fd":"print(classification_report(y_test_model, y_pred_DLM))","b6a44940":"print(\"Accuracy {} %\".format( 100 * accuracy_score(y_pred_DLM, y_test_model)))","d2c1dc57":"scaler2 = MinMaxScaler()\nX_test = scaler2.fit_transform(testData_with_dummies1)","8baa7973":"y_pred = model.predict(X_test)\nresultFrame1 = pd.DataFrame(testData['PassengerId'],columns = ['PassengerId'])\nresultFrame1['Survived'] = pd.DataFrame( y_pred,columns = ['Survived'])\nresultFrame1\ny_pred","cc6d6e60":"resultFrame1.to_csv('ResultsTitanic.csv', index=False)","e20b08e6":"#### The dataset is not exactly balanced one","cfa5f287":"### b) Random Forest Model","e7ef1ba5":"#### Dealing with Cabin Values","e36c218c":"### Feature's histogram Analysis","8c8f0995":"## Correlation Checking","d825f79f":"#### Dealing with Missing Cabin#s","7f4f9a76":"#### Dealing with Name feature:Feature engineering\n","94508c10":"#### Scalling","9c10f6ca":"### Final Output File Based on","08faf324":"### c) Deep Learning Model","9caead31":"### a) Logistic Regression","95c1df57":"#### Dealing with missing Age values","7b287911":"#### Dealing with Embarked and Fare","adc54ef2":"## Deep Dive into Viualization (Survived VS Features)","96f018af":"###### we already seen Cabin is not that important feature-But I wanted to have a try using it too, but seems like encoding and using it is also a issue","815d8582":"## #2 Preprocessing Steps: Exploring the descriptive statistics of the variables","6501d9c6":"## Survived VS Died","ab555a4f":"Variable\tDefinition\t(Key)\n\nsurvival\tSurvival (0 = No, 1 = Yes)\n\npclass\tTicket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n\nsex\tSex\t(male\/femal)\n\nAge\tAge in years #\n\nsibsp\t# of siblings \/ spouses aboard the Titanic\t(#)\n\nparch\t# of parents \/ children aboard the Titanic\t(#)\n\nticket\tTicket number\t(abcd#)\n\nfare\tPassenger fare\t(#)\n\ncabin\tCabin number\t(a#)\n\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","4be72ee6":"### Dealing with missing values:  Age,Cabin,Fare and Embarked for Test and Train Datasets","b48f4b70":"##### Conclusion 3: Cabin #s doesn't relate with Survival -> Instead we see more survivals missing values for Cabin\n##### Conclusion 4: With high cardinality and no solid releation with Survival we assume Cabin can be dropped from modelling","d8681ec4":"### #3 Data Cleaning: Encoding for Feature 'Embarked', 'Sex', 'Name' And Scaling","370de1c7":"#### Final Cleaning And Encoding","ce60104a":"#### Dealing with missing values Age","15171c43":"## #1 Loading Data Sets ","406f06e1":"##### Conclusion 1: Plcass does relate partially with survival -> as we see all three class surviving \n##### Conclusion 2: Plcass doesn't relate with Cabin #s -> Instead we see all three class missing values for Cabin","d99c2850":"#### Performance","a347da26":"#### Splitting the trainData into test and train","795dbc7b":"#### Dealing with Ticket feature"}}