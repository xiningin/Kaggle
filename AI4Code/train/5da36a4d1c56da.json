{"cell_type":{"d8c2184a":"code","6dcc7468":"code","c311c3a5":"code","6c9e1984":"code","e9bdd948":"code","f54e0ffb":"code","1bad90f5":"code","0cb0e0e1":"code","662a8d77":"code","dcf2e9f7":"code","9fe22a13":"code","d1516b4a":"code","f48bb9cf":"code","f0a80baa":"code","d64bfcce":"code","365e7476":"code","cf89716e":"code","2d1dde6d":"code","9cca029e":"code","54236d5e":"code","7def2272":"code","49ee5bff":"code","a5785b23":"code","4650b768":"code","d5a1dd7a":"code","dbff6f99":"code","d19a7ba9":"code","f580e3dc":"markdown","e3d3883b":"markdown","8b9b80b2":"markdown","53bc11f2":"markdown","da802a74":"markdown","28a4ff4c":"markdown","1c6d1b1f":"markdown","363dd186":"markdown","f95f2153":"markdown","3a3eb688":"markdown","e59995d0":"markdown","c15db464":"markdown","0cdfd0a1":"markdown","b9d8995c":"markdown","f4f2c5a7":"markdown","f1e2059a":"markdown","c53933b4":"markdown","f93211a4":"markdown","b1c583aa":"markdown","72644af6":"markdown","4ca5a057":"markdown","1e504b2f":"markdown","6cad5a20":"markdown","c03a08fd":"markdown","75cf6a7d":"markdown","56188789":"markdown","f529a289":"markdown","b2d33826":"markdown","c9bb897c":"markdown"},"source":{"d8c2184a":"#For data handling\nimport numpy as np\nimport pandas as pd\n\n#For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Import raw data\nraw_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nraw_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nraw_train.head()","6dcc7468":"raw_test.head()","c311c3a5":"print('Information about the train set:\\n')\nprint(raw_train.info(), '\\n'*3,'*'*50, '\\n'*3)\nprint('Information about the test set:\\n')\nprint(raw_test.info())","6c9e1984":"#Histogram of the 'Age' feature\nplt.figure(figsize=(25,10))\nsns.distplot(raw_train['Age'])","e9bdd948":"#Barplot of the 'Embarked' feature\nplt.figure(figsize=(25,10))\nsns.barplot(x=raw_train.groupby(by='Embarked').count().index, y=raw_train.groupby(by='Embarked').count().iloc[:,0])","f54e0ffb":"print('Pclass:', raw_train['Pclass'].unique())\nprint('Name:', raw_train['Name'].unique().shape)\nprint('Sex:', raw_train['Sex'].unique())\nprint('Age:', raw_train['Age'].unique().shape)\nprint('SibSp:', raw_train['SibSp'].unique())\nprint('Parch:', raw_train['Parch'].unique())\nprint('Ticket:', raw_train['Ticket'].unique().shape)\nprint('Fare:', raw_train['Fare'].unique().shape)\nprint('Cabin:', raw_train['Cabin'].unique().shape)\nprint('Embarked:', raw_train['Embarked'].unique())","1bad90f5":"print('Pclass:', raw_test['Pclass'].unique())\nprint('Name:', raw_test['Name'].unique().shape)\nprint('Sex:', raw_test['Sex'].unique())\nprint('Age:', raw_test['Age'].unique().shape)\nprint('SibSp:', raw_test['SibSp'].unique())\nprint('Parch:', raw_test['Parch'].unique())\nprint('Ticket:', raw_test['Ticket'].unique().shape)\nprint('Fare:', raw_test['Fare'].unique().shape)\nprint('Cabin:', raw_test['Cabin'].unique().shape)\nprint('Embarked:', raw_test['Embarked'].unique())","0cb0e0e1":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.model_selection import train_test_split\n\n#Features Selection\ncolumns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Name', 'PassengerId']\ntrain = raw_train[np.append(columns, ['Survived'])].copy().set_index('PassengerId')\ntest = raw_test[columns].copy().set_index('PassengerId')\n\n#Override OneHotEncoder to have the column names created automatically (Title_Mr, Title_Mrs...) \nclass OneHotEncoder(SklearnOneHotEncoder):\n    def __init__(self, **kwargs):\n        super(OneHotEncoder, self).__init__(**kwargs)\n        self.fit_flag = False\n\n    def fit(self, X, **kwargs):\n        out = super().fit(X)\n        self.fit_flag = True\n        return out\n\n    def transform(self, X, categories, index='', name='', **kwargs):\n        sparse_matrix = super(OneHotEncoder, self).transform(X)\n        new_columns = self.get_new_columns(X=X, name=name, categories=categories)\n        d_out = pd.DataFrame(sparse_matrix.toarray(), columns=new_columns, index=index)\n        return d_out\n\n    def fit_transform(self, X, categories, index, name, **kwargs):\n        self.fit(X)\n        return self.transform(X, categories=categories, index=index, name=name)\n\n    def get_new_columns(self, X, name, categories):\n        new_columns = []\n        for j in range(len(categories)):\n            new_columns.append('{}_{}'.format(name, categories[j]))\n        return new_columns\n\n#Create a class for the transformation which will help use again the fitted encoder\/scaler for the test set\nclass data_preparation():\n    def __init__(self):\n        self.enc_sex = LabelEncoder()\n        self.scaler_age = StandardScaler()\n        self.enc_title = LabelEncoder()\n        self.onehotenc_title = OneHotEncoder(handle_unknown='ignore')\n        self.onehotenc_pclass = OneHotEncoder(handle_unknown='ignore')\n        self.enc_embarked = LabelEncoder()\n        self.onehotenc_embarked = OneHotEncoder(handle_unknown='ignore')\n        \n        \n    def transform(self, data_untouched):\n        data = data_untouched.copy(deep=True)\n        data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\n        data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n        \n        #For the test set\/Already fitted\n        try:\n            data['Sex'] = self.enc_sex.transform(data['Sex'].values)\n            data['Age'] = self.scaler_age.transform(data['Age'].values.reshape(-1, 1))\n            data['Title'] = data['Title'].map(lambda x: self.enc_title.transform([x])[0] if x in self.enc_title.classes_ else -1)\n            data['Embarked'] = data['Embarked'].map(lambda x: self.enc_embarked.transform([str(x)])[0] if x in self.enc_embarked.classes_ else -1)\n            data = pd.concat([self.onehotenc_title.transform(data['Title'].values.reshape(-1,1), categories=self.enc_title.classes_,name='Title', index=data.index),\n                              self.onehotenc_pclass.transform(data['Pclass'].values.reshape(-1,1), categories=[3,2,1], name='Pclass', index=data.index),\n                              self.onehotenc_embarked.transform(data['Embarked'].values.reshape(-1,1), categories=self.enc_embarked.classes_, name='Embarked', index=data.index),\n                              data],axis=1)\n        \n        #For the train set\/Not yet fitted    \n        except NotFittedError:\n            self.mean_age = data['Age'].median()\n            self.median_fare = data['Fare'].median()\n            data['Sex'] = self.enc_sex.fit_transform(data['Sex'].values)\n            data['Age'] = self.scaler_age.fit_transform(data['Age'].values.reshape(-1, 1))\n            data['Title'] = self.enc_title.fit_transform(data['Title'].values)\n            data['Embarked'] = self.enc_embarked.fit_transform(data['Embarked'].astype(str).values)\n            data = pd.concat([self.onehotenc_title.fit_transform(data['Title'].values.reshape(-1,1), categories=self.enc_title.classes_, name='Title', index=data.index), \n                              self.onehotenc_pclass.fit_transform(data['Pclass'].values.reshape(-1,1), categories=[3,2,1], name='Pclass', index=data.index),\n                              self.onehotenc_embarked.fit_transform(data['Embarked'].values.reshape(-1,1), categories=self.enc_embarked.classes_, name='Embarked', index=data.index),\n                              data], axis=1)\n\n        \n        data['Family_size'] = data['SibSp'] + data['Parch']\n        data['IsAlone'] = (data['Family_size'] == 0).astype(int)\n        data['Age'] = data['Age'].fillna(self.mean_age)\n        data['Fare'] = data['Fare'].fillna(self.median_fare)\n        data = data.drop(columns=['Pclass', 'Embarked', 'Title', 'Name'])\n            \n        return data\n\n#Apply the transformation\ndata_prep = data_preparation()\nX_train_full = data_prep.transform(train.drop(columns='Survived'))\ny_train_full = train['Survived']\nX_test = data_prep.transform(test)\n\n#Fix Randomness and split the train and validation set\nnp.random.seed(0)\nX_train, X_val, y_train, y_val =  train_test_split(X_train_full, y_train_full, test_size=0.10, random_state=50)","662a8d77":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.1, size=15)\nsns.heatmap(X_train_full[['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].astype(float).corr(method='pearson'),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","dcf2e9f7":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n#Model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import VotingClassifier","9fe22a13":"#Dictionnary of parameters to try\nlist_params = {\n    'Tree':{'max_depth': np.logspace(2,8,6)},\n    'SVC':{'C' : np.linspace(0.01,1,3), 'kernel':['poly'], 'degree':np.linspace(2,8,4).astype(int)},\n    'KNN':{'n_neighbors' : np.linspace(2,30,29).astype(int)},\n    'gauss':{'var_smoothing' : np.logspace(-8,-10,3)},\n    'bern':{'alpha' : np.linspace(0.1,1,10)}\n}\n\n#Base models\nlist_base_model = {\n    'Tree' : DecisionTreeClassifier(random_state=0),\n    'SVC' : SVC(random_state=0),\n    'KNN' : KNeighborsClassifier(),\n    'gauss' : GaussianNB(),\n    'bern' : BernoulliNB()\n}\n\n#Apply GridSearch, fit and \nlist_models = pd.DataFrame(list_params.items(), columns = ['name', 'params']).set_index('name')\nlist_models['model'] = list_base_model.values()\nlist_models['grid_model'] = list_models.apply(lambda x: GridSearchCV(x['model'], x['params']), axis=1)\nlist_models['grid_model'] = list_models.apply(lambda x: x['grid_model'].fit(X_train, y_train), axis=1)\nlist_models['score'] = list_models.apply(lambda x: x['grid_model'].predict(X_val), axis=1, result_type='reduce').apply(lambda x: accuracy_score(x, y_val))\nprint('Score for each base model using GridSearch:')\nprint(list_models['score'])","d1516b4a":"models = (('Tree', DecisionTreeClassifier(**list_models.loc['Tree', 'grid_model'].best_params_, random_state=40)),\n          #We have to enable the probability estimates for the soft voting.\n          ('SVC', SVC(**list_models.loc['SVC', 'grid_model'].best_params_, random_state=40, probability=True)),\n          ('KNN', KNeighborsClassifier(**list_models.loc['KNN', 'grid_model'].best_params_)),\n          ('gauss', GaussianNB(**list_models.loc['gauss', 'grid_model'].best_params_)),\n          ('bern', BernoulliNB(**list_models.loc['bern', 'grid_model'].best_params_)))\n\nvot = VotingClassifier(models, voting='hard')\nvot.fit(X_train,y_train)\nprint('Score of the VotingClassifer with hard voting:', accuracy_score(vot.predict(X_val), y_val))","f48bb9cf":"vot = VotingClassifier(models, voting='soft')\nvot.fit(X_train,y_train)\nprint('Score of the VotingClassifer with soft voting:', accuracy_score(vot.predict(X_val), y_val))","f0a80baa":"vot = VotingClassifier(models, voting='soft', weights=list_models['score']\/list_models['score'].sum())\nvot.fit(X_train,y_train)\nprint('Score of the VotingClassifer with soft weighted voting:', accuracy_score(vot.predict(X_val), y_val))","d64bfcce":"vot = VotingClassifier(models, voting='soft')\nvot.fit(X_train_full,y_train_full)\nsubmission = pd.DataFrame({'PassengerId': test.index, 'Survived': vot.predict(X_test)})\nsubmission.to_csv('submission_stack.csv', index=False)","365e7476":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nmodels = (('Tree', DecisionTreeClassifier(**list_models.loc['Tree', 'grid_model'].best_params_, random_state=40)),\n                   ('SVC', SVC(**list_models.loc['SVC', 'grid_model'].best_params_, random_state=40, probability=True)),\n                   ('KNN', KNeighborsClassifier(**list_models.loc['KNN', 'grid_model'].best_params_)),\n                   ('gauss', GaussianNB(**list_models.loc['gauss', 'grid_model'].best_params_)),\n                   ('bern', BernoulliNB(**list_models.loc['bern', 'grid_model'].best_params_)))\n\nfinal_estimator = MLPClassifier(hidden_layer_sizes = (100,50,), random_state=1)\nstack = StackingClassifier(models, final_estimator=final_estimator)\nstack.fit(X_train,y_train)\nprint('Score of the StackingClassifer with hard voting:', accuracy_score(stack.predict(X_val), y_val))","cf89716e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n#Base models\nmodels = (('Tree', DecisionTreeClassifier(**list_models.loc['Tree', 'grid_model'].best_params_, random_state=40)),\n                   ('SVC', SVC(**list_models.loc['SVC', 'grid_model'].best_params_, random_state=40, probability=True)),\n                   ('KNN', KNeighborsClassifier(**list_models.loc['KNN', 'grid_model'].best_params_)),\n                   ('gauss', GaussianNB(**list_models.loc['gauss', 'grid_model'].best_params_)),\n                   ('bern', BernoulliNB(**list_models.loc['bern', 'grid_model'].best_params_)))\n\n#First layer of stacking (MLP + Logistic Regression)\nMLP = MLPClassifier(hidden_layer_sizes = (50,5,), random_state=1)\nstack_MLP = StackingClassifier(models, final_estimator=MLP)\n\nlog = LogisticRegression()\nstack_log = StackingClassifier(models, final_estimator=log)\n\nQDA = QuadraticDiscriminantAnalysis()\nstack_QDA = StackingClassifier(models, final_estimator=QDA)\n\n#Second layer of stack (small MLP)\nsmall_MLP = MLPClassifier(hidden_layer_sizes = (5,), random_state=10)\nstack_final = StackingClassifier([('MLP',stack_MLP), ('log', stack_log), ('QDA', stack_QDA)], final_estimator=small_MLP)\nstack_final.fit(X_train,y_train)\nprint('Score of the StackingClassifer:', accuracy_score(stack_final.predict(X_val), y_val))","2d1dde6d":"from sklearn.ensemble import BaggingClassifier\n\n#Bagging models\nlist_base_model = {\n    'Tree' : BaggingClassifier(DecisionTreeClassifier(max_depth=10), n_estimators=100, max_samples=0.5, random_state=10),\n    'SVC' : BaggingClassifier(SVC(C=0.2), n_estimators=20, max_samples=0.5, random_state=10),\n    'KNN' : BaggingClassifier(KNeighborsClassifier(n_neighbors=8, weights='distance'), n_estimators=50, max_samples=0.3, random_state=10),\n    'gauss' : BaggingClassifier(GaussianNB(), n_estimators=100, max_samples=0.5, random_state=10),\n    'bern' : BaggingClassifier(BernoulliNB(alpha=0.01, binarize=0.5), n_estimators=200, max_samples=0.4, random_state=10)\n}\n\nlist_models_bag = pd.DataFrame(list_base_model.items(), columns = ['name', 'model']).set_index('name')\nlist_models_bag['model'] = list_models_bag.apply(lambda x: x['model'].fit(X_train, y_train), axis=1)\nlist_models_bag['score'] = list_models_bag.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='reduce').apply(lambda x: accuracy_score(x, y_val))\nprint('Score for Bagging model on samples:')\nprint(list_models_bag['score'])","9cca029e":"print('Accuracy using all model:',accuracy_score(list_models_bag.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='expand').mode().values.reshape(-1,), y_val))","54236d5e":"list_base_model = {\n    'Tree' : BaggingClassifier(DecisionTreeClassifier(max_depth=10), n_estimators=10, max_features=0.7, random_state=10),\n    'SVC' : BaggingClassifier(SVC(C=0.2), n_estimators=50, max_features=0.3, random_state=10),\n    'KNN' : BaggingClassifier(KNeighborsClassifier(n_neighbors=8, weights='distance'), n_estimators=10, max_features=0.7, random_state=10),\n    'gauss' : BaggingClassifier(GaussianNB(), n_estimators=10, max_features=0.7, random_state=10),\n    'bern' : BaggingClassifier(BernoulliNB(alpha=0.01, binarize=0.5), n_estimators=10, max_features=0.7, random_state=10)\n}\n\nlist_models_bag = pd.DataFrame(list_base_model.items(), columns = ['name', 'model']).set_index('name')\nlist_models_bag['model'] = list_models_bag.apply(lambda x: x['model'].fit(X_train, y_train), axis=1)\nlist_models_bag['score'] = list_models_bag.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='reduce').apply(lambda x: accuracy_score(x, y_val))\nprint('Score for Bagging model on features:')\nprint(list_models_bag['score'])","7def2272":"print('Accuracy using all model:',accuracy_score(list_models_bag.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='expand').mode().values.reshape(-1,), y_val))","49ee5bff":"list_base_model = {\n    'Tree' : BaggingClassifier(DecisionTreeClassifier(max_depth=10), n_estimators=10, max_features=0.7, max_samples=0.5, random_state=10),\n    'SVC' : BaggingClassifier(SVC(C=0.2), n_estimators=50, max_features=0.4, max_samples=0.4, random_state=10),\n    'KNN' : BaggingClassifier(KNeighborsClassifier(n_neighbors=8, weights='distance'), n_estimators=50, max_features=0.5, max_samples=0.5, random_state=10),\n    'gauss' : BaggingClassifier(GaussianNB(), n_estimators=50, max_features=0.5, max_samples=0.5, random_state=10),\n    'bern' : BaggingClassifier(BernoulliNB(alpha=0.01, binarize=0.5), n_estimators=50, max_features=0.5, max_samples=0.5, random_state=10)\n}\n\nlist_models_bag = pd.DataFrame(list_base_model.items(), columns = ['name', 'model']).set_index('name')\nlist_models_bag['model'] = list_models_bag.apply(lambda x: x['model'].fit(X_train, y_train), axis=1)\nlist_models_bag['score'] = list_models_bag.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='reduce').apply(lambda x: accuracy_score(x, y_val))\nprint('Score for Bagging model on features:')\nprint(list_models_bag['score'])","a5785b23":"print('Accuracy using all model:',accuracy_score(list_models_bag.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='expand').mode().values.reshape(-1,), y_val))","4650b768":"#Train again on the whole train set and predict on the test set\nlist_models_bag = pd.DataFrame(list_base_model.items(), columns = ['name', 'model']).set_index('name')\nlist_models_bag['model'] = list_models_bag.apply(lambda x: x['model'].fit(X_train_full, y_train_full), axis=1)\nmy_submission = pd.DataFrame({'PassengerId': test.index, 'Survived': list_models_bag.apply(lambda x: x['model'].predict(X_test), axis=1, result_type='expand').mode().values.reshape(-1,)})\nmy_submission.to_csv('submission_bag.csv', index=False)","d5a1dd7a":"from sklearn.ensemble import AdaBoostClassifier\n\nlist_base_model = {\n    'Tree' : AdaBoostClassifier(DecisionTreeClassifier(max_depth=10), n_estimators=20, random_state=0, learning_rate=1.05),\n    'SVC' : AdaBoostClassifier(SVC(C=0.3), algorithm='SAMME',n_estimators=5, random_state=0, learning_rate=1.1),\n    'gauss' : AdaBoostClassifier(GaussianNB(), n_estimators=5, random_state=0, learning_rate=1.3),\n    'bern' : AdaBoostClassifier(BernoulliNB(alpha=0.01, binarize=0.5), n_estimators=5, random_state=0, learning_rate=1.3)\n}\n\nlist_models_boost = pd.DataFrame(list_base_model.items(), columns = ['name', 'model']).set_index('name')\nlist_models_boost['model'] = list_models_boost.apply(lambda x: x['model'].fit(X_train, y_train), axis=1)\nlist_models_boost['score'] = list_models_boost.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='reduce').apply(lambda x: accuracy_score(x, y_val))\nprint('Score for Boosting model:')\nprint(list_models_boost['score'])","dbff6f99":"print('Accuracy using all model:',accuracy_score(list_models_boost.apply(lambda x: x['model'].predict(X_val), axis=1, result_type='expand').mode().iloc[0,:].values.reshape(-1,), y_val))","d19a7ba9":"#Train again on the whole train set and predict on the test set\nlist_models_boost = pd.DataFrame(list_base_model.items(), columns = ['name', 'model']).set_index('name')\nlist_models_boost['model'] = list_models_boost.apply(lambda x: x['model'].fit(X_train_full, y_train_full), axis=1)\nmy_submission = pd.DataFrame({'PassengerId': test.index, 'Survived': list_models_boost.apply(lambda x: x['model'].predict(X_test), axis=1, result_type='expand').mode().iloc[0,:].astype(int).values.reshape(-1,)})\nmy_submission.to_csv('submission_boost.csv', index=False)","f580e3dc":"We need to check the missing values in the datasets.","e3d3883b":"# Stacking\n\nWhat is stacking ? We will try to use again the advantages of different models but this time we won't decide how the predictions should be aggregated. We will train another model (what could be called a meta-model) to learn what is the best way to aggregate them. The features will be the predictons of each model and the output the 'Survived' variable. We will use a MLP from Scikit-Learn with 2 hidden layers of size 100 and 50. For the base models, we will use the same models than before.","8b9b80b2":"The interesting part of the last plot is the presence of a peak in the beginning. This shows there were many babies aboard. It could have influence the survival probabilities of the parents. Sadly we don't have the relative links between those babies and their parents. We have information about relation but without knowing if they are babies\/parents relation ('Parch' features).","53bc11f2":"In all cases, the VotingClassifier model has a better accuracy on the validation set than each independent estimators.\nLet's train it again on the whole train dataset and predict on the test set.","da802a74":"After submitting, we have a 77,5% accuracy.","28a4ff4c":"So obviously, the Pclass (1,2 and 3) features are correlated since they are the same variables. We have the same reasoning with Embarked (C, Q and S) features. Another strong correlations is the fare\/ticket variables which makes sense. If you have a first class ticket, you will pay more. SibSp\/Parch variables are also a bit correlated. It might be due to family travelling together. Otherwise the features are not strongly correlated which is good. It will be easier to obtain the interesting information from the data.","1c6d1b1f":"# Assess the data\n\nWe are going first to import the main librairies, import the train and test set and have a first look at the data.","363dd186":"We don't have better results and the MLP is not converging. In this case, we are using a model too complex. The model is taking more time than the previous models without having a better performance. It is one example that you should always aims for simple model and use complex model if the simple one failed.","f95f2153":"We can again add majority vote over our boosting models.","3a3eb688":"Finally, we can train it again on the whole train dataset and predict on the test set.","e59995d0":"Since we don't have the 'cabin' features, it would be nice to have a feature showing where the person could be on the boat. Some people might have drown with the boat without having the possibility to leave the boat because they had a bad location. It doesn't seem irrational that the embarkation point is correlated with the person's cabin. So we will use this feature.\n\nWe will now go deeper into the data and look at the different values that each feature can take. ","c15db464":"# Boosting\nFinally Boosting ! This time instead of training several same models in parallel, we will train them sequentially. Each model is built with focus on the missclassified observations by the previous one. The most simple model is too add a weight to each sample. Putting more weight to observations in the dataset that were missclassified by the previous models will force the model to focus on the mistakes of the previous models. This allows a better fitting by reducing the bias (it can also reduces variance).  \n\nWe will use the [AdaBoostClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html) and apply again a GridSearch. We can't use KNN with AdaBoostClassifier. So we won't use this estimator. AdaBoostClassifier doesn't support the 'SAMME.R' algorithm (converge faster than 'SAMME') for SVC. We have to use 'SAMME'.","0cdfd0a1":"![481634_titanic-romantic-scene-jack-and-rose-hd-wallpapers_1920x1080_h%20%281%29.jpg](attachment:481634_titanic-romantic-scene-jack-and-rose-hd-wallpapers_1920x1080_h%20%281%29.jpg)\n\n# A simple guide towards ensembling\n\nAND YOU'RE HERE IN MY HEART  \nAND MY HEART WILL GO ON AND ON  \nTUT TUT TUT TUT\n\nHum, hum... Sorry I got distracted. Let's focus on the data !\n\nEnsembling is about combining severals models to combine their advantages. There are many ways of doing it, like majority vote which is pretty simple to understand or boosting which is more complex.\n\nSo how can we implement ensembling ? We will go through different steps to show how it can be implemented with scikit-learn. We will use the well-known Titanic Dataset for that purpose. The objective will be to predict which passengers survived the Titanic shipwreck using the following variables.\n\n\n### Description of the features\n\n* **PassengerId**: Discret, Id for the passenger, will be used for the index\n* **Survived**: Binary, The variable we want to predict\n* **Pclass**: Categorical, Ticket class\n* **Name**: String, Name of the passenger\n* **Sex**: Binary, Sex of the passenger\n* **Age**: Discrete, Age of the passenger\n* **SibSp**: Discrete, Number of siblings\/spouse aboard the boat\n* **Parch**: Discrete, Number of children\/parents aboard the boat\n* **Ticket**: String, Ticket number\n* **Fare**: Continuous, Passenger Fare\n* **Cabin**: String, Cabin Number, many missings values\n* **Embarked**: Categorical, Port of Embarkation\n\n\nThis notebook will just remind shortly how ensembling works but it assumes that you have knowledge about it. If you don't have any knowledge in this field, I recommend you this [article](https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205) co-written by Joseph Rocca and Baptiste Rocca.","b9d8995c":"There are too many missing values in the 'Cabin' feature. Trying to fill it will bring more noises than useful information. So, we will remove it.\nThere is a small number of missing values in the 'Age' feature. We could discard them but since the dataset is pretty small, we want to keep as many data as we can. We can apply the same reasoning for the 'Embarked' feature and 'Fare' feature (in the test set).  \nWe could look for complex methods to fill the missing values but in this case we will use a simple median (for 'Age' and 'Fare') and mode (for 'Embarked') filling.\n\nLet's have a few plots of the data.","f4f2c5a7":"We have a better accuracy with the hard mode and it's even better with the soft mode.   \nIt is also possible to add weights on the classifiers, allowing the more precise estimators to have a stronger influence on the decision. So let's use the accuracy we calculated for each model.","f1e2059a":"After submitting, we have a 72,2% accuracy.  \nMy best submission score is around 77,9% with a RandomForestClassifier. This shows that ensembling can be helpful to combine weak learners but it might not be the best choice for simple cases like this one !\n\nHope the notebook was helpful and you enjoyed it ! If you want to give an advice for corrections or for other paths to explore, I would accept it with joy :)","c53933b4":"### Random Features Subset\nWe can try the Random Subspaces method. It make random subset of features instead of random subets of samples.","f93211a4":"### Random Features and Samples Subset","b1c583aa":"# Data Engineering\n\nTime for data preparation ! We will apply the following transformations:\n\n1. Fill the NaN values \n2. Encode the 'Pclass', 'Embarked', 'Sex'\n3. Normalize 'Age'\n4. Retrieve the title of each person in the 'Name' features and encode it\n5. Create a feature 'Family_size' and 'IsAlone' using 'SibSp' and 'Parch' features. They will represent the size of the family aboard and if the person is alone aboard\n\nFinally we will prepare the train and validation set to train and evaluate our models.","72644af6":"After submitting, we have a 76,5% accuracy.","4ca5a057":"We can train it again on the whole train dataset and predict on the test set.","1e504b2f":"Now we can do the majority vote with our models. [VotingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html) from Scikit-learn allows us to train the models we chose and apply a soft or hard vote. The hard voting is taking the mode over the prediction of the different models. The soft voting average the predicted probabilities. Let's try the hard voting !","6cad5a20":"We can compare it with the results of our first GridSearch without Stacking and Bagging.\n>name  \nTree     0.766667  \nSVC      0.644444  \ngauss    0.744444  \nbern     0.800000  \n>Name: score, dtype: float64  \n\nWe are able to increase the accuracy on the DecisionTreeClassifier and GaussianNB but not on the others. We could apply a GridSearchCV or RandomizedSearchCV to look maybe for better parameters but it would take a bit of time.","c03a08fd":"We can compare it with the results of our first GridSearch without Stacking and Bagging.\n>name  \nTree     0.766667  \nSVC      0.644444  \nKNN      0.733333  \ngauss    0.744444  \nbern     0.800000  \n>Name: score, dtype: float64  \n\nSo we have better results with bagging. That's great !  \nWe can even add majority vote over our bagging models.","75cf6a7d":"# Introduction to Ensembling\n\nMaybe the easiest way of doing ensembling is to train several models independently, make predictions and then to use a metrics on those predictions. We can do a simple majority vote on them. If one or two models will do a mistake on a prediction, it should be counterbalanced by the others models through the vote. Let's see how hard and soft voting works.  \n\nWe will choose 5 models, search the best hyperparameters with GridSearch and then apply the majority vote.","56188789":"And the soft voting !","f529a289":"# Bagging\n\nLet's get to bagging !\nThis time instead of using different estimators, we are going to use the same estimator several times. One of the way to use bagging is to train each estimor on different random samples subsets of the whole set and then aggregate like we did before (hard or soft voting). It can be done with [BaggingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html). But others type of bagging can be done through this function. Another one could be to train on different random features subsets (Random Subspaces).\n\n### Random samples subset","b2d33826":"# Visualisation\n\nNow that we have the dataset ready, plotting the correlation between the features might be usefull.","c9bb897c":"Again, we have better results with this ensembling method than the single models.  \n\nWe could make another layer of stacking. Another layer means training several metal-models and then use the prediction of those meta-models to train a last one which will predict our output."}}