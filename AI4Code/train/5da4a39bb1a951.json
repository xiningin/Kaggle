{"cell_type":{"2d0941b8":"code","720d660a":"code","f6f38f03":"code","379e6fd3":"code","8a1d3697":"code","06110f48":"code","29fb3d66":"code","3043fbd5":"code","c4b6111e":"code","713f0c3a":"code","f75fef39":"code","7cf060ea":"code","a590d740":"code","602d3256":"code","d4e89e54":"code","61b2027f":"code","f0df826a":"code","a9d3595f":"code","1d3fe3b4":"code","807611e7":"code","b28f5265":"code","5e68263b":"code","83ef5560":"code","b36dd8b2":"markdown","99cd6c59":"markdown","dd814064":"markdown","c768a937":"markdown","59603656":"markdown","2dfe6e30":"markdown"},"source":{"2d0941b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","720d660a":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")","f6f38f03":"#lib\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing, metrics\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm\nimport copy\nimport math","379e6fd3":"train.head(5)","8a1d3697":"test.head()","06110f48":"sub.head()","29fb3d66":"#Checking Null values in training data\ntrain.isnull().sum()","3043fbd5":"# Checking Null values in test data\ntest.isnull().sum()","c4b6111e":"print(\"There are {} products in {} stores in {} countries\".format(len(train['product'].unique()),len(train['store'].unique()), len(train['country'].unique())))\nprint(\"Product Names: {}\".format(train['product'].unique()))\nprint(\"Store Names: {}\".format(train['store'].unique()))\nprint(\"Country Names: {}\".format(train['country'].unique()))","713f0c3a":"more_selling_product = train.groupby(\"product\")[\"num_sold\"].agg(\"max\")\nmore_selling_product","f75fef39":"how_much_sold = pd.DataFrame(train.groupby(\"product\")[\"num_sold\"].sum()).reset_index()\n# print(how_much_sold)\nsale_percentage = round(how_much_sold['num_sold']\/how_much_sold['num_sold'].sum()*100,2)\n# print(sale_percentage)\nhow_much_sold['sale_per'] = sale_percentage\nhow_much_sold","7cf060ea":"all_data = pd.concat([train, test], axis=0, ignore_index=True)\nall_data","a590d740":"all_data['date'] = pd.to_datetime(all_data['date'])\nall_data['year'] = all_data['date'].dt.year\nall_data['month'] = all_data['date'].dt.month\nall_data['day'] = all_data['date'].dt.day\nall_data['dayofweek'] = all_data['date'].dt.dayofweek\nall_data['dayofmonth'] = all_data['date'].dt.days_in_month\nall_data['dayofyear'] = all_data['date'].dt.dayofyear\nall_data['weekday'] = all_data['date'].dt.weekday\nall_data['weekofyear'] = all_data['date'].dt.weekofyear","602d3256":"all_data.head(3)","d4e89e54":"all_data.drop(columns=[\"row_id\", \"date\", \"num_sold\"], inplace=True)\nall_data.head(3)","61b2027f":"le = LabelEncoder()\nfor col in ['country', 'store', 'product']:\n    all_data[col] = le.fit_transform(all_data[col])\n    \nall_data.head(3)","f0df826a":"train2 = all_data[:len(train)]\ntest2 = all_data[len(train):]\ny = train['num_sold']","a9d3595f":"from sklearn.model_selection import TimeSeriesSplit\nfolds = TimeSeriesSplit(n_splits=4)","1d3fe3b4":"def smape(actual, predicted):\n    numerator = np.abs(predicted - actual)\n    denominator = (np.abs(actual) + np.abs(predicted)) \/ 2\n    \n    return np.mean(numerator \/ denominator)*100","807611e7":"from catboost import CatBoostRegressor\n\ny_pred = np.zeros(len(test))\nscores = []\n\nfor fold, (train_id, test_id) in enumerate(folds.split(train)):\n    print(\"Fold: \", fold)\n    \n    # Splitting\n    X_train, y_train = train2.iloc[train_id], y.iloc[train_id]\n    X_valid, y_valid = train2.iloc[test_id], y.iloc[test_id]\n    \n\n    cat = CatBoostRegressor(\n        iterations=10000,\n        bootstrap_type='Bayesian',\n        boosting_type='Plain',\n        loss_function='MAE',\n        eval_metric='SMAPE'\n    )\n\n    cat.fit(\n        X_train, y_train, \n        eval_set=(X_valid, y_valid),\n        early_stopping_rounds=1000,\n        verbose=1000\n    )\n    \n    print('\\n')\n    \n    # Evaluation\n    valid_pred = cat.predict(X_valid)\n    valid_score = smape(y_valid, np.ceil(valid_pred))\n    scores.append(valid_score)\n    \n    # Prediction for submission\n    y_pred += cat.predict(test2) \/ folds.n_splits","b28f5265":"score = np.array(scores).mean()\nprint('Mean SMAPE score: ', score)","5e68263b":"sub.num_sold = np.ceil(y_pred)\nsub.head(10)","83ef5560":"sub.to_csv('submission.csv', index=False)","b36dd8b2":"# Explore the Data","99cd6c59":"### Sale & Percentange of Sale","dd814064":"### Which Product is sell more","c768a937":"# Feature Engineering","59603656":"We have time series data that's why we insight on each country, I mean Holiday!! of each coutnry ","2dfe6e30":"> Our data is already clean no null value present.... Wow!!"}}