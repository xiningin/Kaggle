{"cell_type":{"80cfa4c9":"code","356a3717":"code","ca24baec":"code","e384c85e":"code","5f7e9361":"code","9a8d0dc8":"code","41674442":"code","e5744875":"code","c6fee34c":"code","1f3ef51b":"code","28af05ff":"code","4e3f77e1":"code","48b87f18":"code","d17fa090":"code","30f9d2c8":"code","428a1517":"code","e9defcb4":"code","7a403c41":"code","ad4b1cd3":"code","4376b17e":"code","49cbbe67":"code","6055fa9c":"code","f8f0879a":"code","a54acec3":"code","1e8713c4":"code","b787dc51":"code","b90a5ecd":"code","9e346e0c":"code","d4c67290":"code","244ec5c0":"code","3ded5f91":"code","bb32e77e":"code","2c8c29bc":"code","0fde3e89":"code","aa3decbd":"code","ef8ed16c":"code","58f72b07":"code","69ff7056":"code","cc1f0b6b":"code","79847702":"code","371f16c9":"code","0d544677":"code","6c3c795e":"code","9db55384":"code","836178bc":"code","ce2a6a09":"code","bced2607":"code","5fea891e":"code","84a337cf":"code","421e7ff8":"code","4515ebb8":"code","f39e5859":"code","8799a4cc":"code","066d93bf":"code","5c7d400b":"code","03bc9c0f":"code","96f4e61a":"code","811e81cf":"code","f570d1bf":"markdown","2b0fdd2c":"markdown","b2f3aaac":"markdown","600ba24c":"markdown","d169608a":"markdown","5e1408cc":"markdown"},"source":{"80cfa4c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","356a3717":"#import library\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas import DataFrame\nfrom sklearn.model_selection import GridSearchCV \n#modeling parametes\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n","ca24baec":"#read data\ndataset = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 8].values\nprint(dataset)","e384c85e":"dataset.head()","5f7e9361":"dataset.info()","9a8d0dc8":"#itertools dataset feature\nimport itertools\n\ncolumns=dataset.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    dataset[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","41674442":"#itertools data label\n\ndata1 = dataset[dataset[\"Outcome\"]==1]\ncolumns = dataset.columns[:8]\nplt.subplots(figsize=(18,15))\nlength =len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    plt.ylabel(\"Count\")\n    data1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","e5744875":"#print dataset outcome\nprint(dataset.groupby('Outcome').size())","c6fee34c":"#ploting data outcome label count\nimport seaborn as sns\nsns.countplot(dataset['Outcome'],label=\"Count\")","1f3ef51b":"#brief analisis dataset\nsns.pairplot(data=dataset,hue='Outcome',diag_kind='kde')\nplt.show()","28af05ff":"# Splitting the dataset into the Training set and Test set\n# create data test 0.25 between data training and seed paarameter 42\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, \n                                                    random_state = 42)","4e3f77e1":"X_train.shape","48b87f18":"X_test.shape","d17fa090":"y_train.shape","30f9d2c8":"y_test.shape","428a1517":"#scalling feature\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n#x\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nprint('ini data x train',X_train)\nprint('ini data x train',X_test)","e9defcb4":"#gradient boasting method \n\n# Parameter evaluation with GSC validation\ngbe = GradientBoostingClassifier(random_state=42)\nparameters={'learning_rate': [0.05, 0.1, 0.5],\n            'max_features': [0.5, 1],\n            'max_depth': [3, 4, 5]\n}\ngridsearch=GridSearchCV(gbe, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X, y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","7a403c41":"#gradient boasting method \n\n# Adjusting development threshold\ngbi = GradientBoostingClassifier(learning_rate=0.05, max_depth=3,\n                                 max_features=0.5,\n                                 random_state=42)\nX_train,X_test,y_train, y_test = train_test_split(X, y, random_state=42)\ngbi.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbi.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbi.score(X_test, y_test)))","ad4b1cd3":"#gradient boasting method\n\n# Storing the prediction\ny_pred = gbi.predict_proba(X_test)[:,1]\nprint('y prediksi',y_pred)\n","4376b17e":"#gradient boasting method\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred.round())\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","49cbbe67":"from sklearn.metrics import f1_score\n# Plotting the predictions\n\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)\nprint('akurasi model gradient bosting :',round(roc_auc_score(y_test,y_pred),5))","6055fa9c":"#clasification report method\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n# accuracy_score(y_test, y_pred.round(), normalize=True)\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred.round() ))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test,y_pred.round()))\nprint('akurasi model gradient bosting :',round(roc_auc_score(y_test,y_pred),5))","f8f0879a":"GB = GradientBoostingClassifier()\nGBscore = round(roc_auc_score(y_test,y_pred),5)\nprint (GBscore)","a54acec3":"#polting roc auc curve method GB\nfrom sklearn import metrics\ngbi.fit(X_train, y_train)\ny_pred = gbi.predict(X_test)\n\ny_pred_proba = gbi.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","1e8713c4":"#decision Tree\n\n# Parameter evaluation\ntreeclf = DecisionTreeClassifier(random_state=42)\nparameters = {'max_depth': [6, 7, 8, 9],\n              'min_samples_split': [2, 3, 4, 5,6],\n              'max_features': [1, 2, 3, 4,5,6]\n}\ngridsearch=GridSearchCV(treeclf, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X,y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","b787dc51":"#decision Tree\n\n# Adjusting development threshold2\ntree = DecisionTreeClassifier(max_depth = 6, \n                              max_features = 4, \n                              min_samples_split = 4, \n                              random_state=42)\nX_train,X_test,y_train,y_test = train_test_split(X, y, random_state=42)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","b90a5ecd":"#decision Tree\n\n# Predicting the Test set results\ny_pred = tree.predict(X_test)\n# y_pred.shape\nprint('y predict', y_pred)","9e346e0c":"#decision Tree\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","d4c67290":"#decision Tree\n\n# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","244ec5c0":"#decision tree clasification report\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model decision tree :',round(roc_auc_score(y_test,y_pred),5))","3ded5f91":"DS = DecisionTreeClassifier()\nDSscore = round(roc_auc_score(y_test,y_pred),5)\nprint (DSscore)","bb32e77e":"#polting roc auc curve method DC\nfrom sklearn import metrics\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\n\ny_pred = tree.predict(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","2c8c29bc":"#K-NN model\n# Parameter evaluation\nknnclf = KNeighborsClassifier()\nparameters={'n_neighbors': range(1, 20)}\ngridsearch=GridSearchCV(knnclf, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X, y)\nprint('grid',gridsearch)","0fde3e89":"print(gridsearch.best_params_)\nprint(gridsearch.best_score_)","aa3decbd":"#K-NN model\n# Fitting K-NN to the Training set\nknnClassifier = KNeighborsClassifier(n_neighbors = 18)\nknnClassifier.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'.format(knnClassifier.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'.format(knnClassifier.score(X_test, y_test)))","ef8ed16c":"#K-NN\n# Predicting the Test set results\ny_pred = knnClassifier.predict(X_test)\nprint('y predict',y_pred)","58f72b07":"#K-NN\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","69ff7056":"# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","cc1f0b6b":"# KNN clasification report\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model K-NN:',round(roc_auc_score(y_test,y_pred),5))","79847702":"KNN = KNeighborsClassifier()\nKNNscore = round(roc_auc_score(y_test,y_pred),5)\nprint (KNNscore)","371f16c9":"#polting roc auc curve method K-NN\nfrom sklearn import metrics\nknnclf.fit(X_train, y_train)\ny_pred = knnClassifier.predict(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","0d544677":"#Logistic Regresion\n# Parameter evaluation\nlogclf = LogisticRegression(random_state=42)\nparameters={'C': [1, 4, 10], 'penalty': ['l1', 'l2']}\ngridsearch=GridSearchCV(logclf, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X, y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","6c3c795e":"#Logistic Regresion\n\n# Adjusting development threshold\nlogreg_classifier = LogisticRegression(C = 1, penalty = 'l1')\nX_train,X_test,y_train, y_test = train_test_split(X, y, random_state=42)\nlogreg_classifier.fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg_classifier.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg_classifier.score(X_test, y_test)))","9db55384":"#Logistic Regresion\n\n# Predicting the Test set results\ny_pred = logreg_classifier.predict(X_test)\nprint('y predict',y_pred)","836178bc":"#Logistic Regresion\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","ce2a6a09":"#Logistic Regresion\n\n# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","bced2607":"#Logistic Regresion clasification report\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model Logistic Regresion : ',round(roc_auc_score(y_test,y_pred),5))","5fea891e":"LS = LogisticRegression()\nLSscore = round(roc_auc_score(y_test,y_pred),5)\nprint (LSscore)","84a337cf":"#polting roc auc curve method LG\nfrom sklearn import metrics\nlogreg_classifier.fit(X_train, y_train)\ny_pred = logreg_classifier.predict(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","421e7ff8":"#Random Forest \n\n# Parameter evaluation\n#evaluasi parameter\nrfclf = RandomForestClassifier(random_state=42)\nparameters={'n_estimators': [50, 100],\n            'max_features': ['auto', 'sqrt', 'log2'],\n            'max_depth' : [4,5,6,7],\n            'criterion' :['gini', 'entropy']\n}\ngridsearch=GridSearchCV(rfclf, parameters, cv=50, scoring='roc_auc', n_jobs = -1)\ngridsearch.fit(X, y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","4515ebb8":"#Random Forest \n\n#acuaration random forest\nrf = RandomForestClassifier(n_estimators=100, criterion = 'gini', max_depth = 6, \n                            max_features = 'auto', random_state=0)\nrf.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test, y_test)))","f39e5859":"#Random Forest \n# Predicting the Test set results\ny_pred = rf.predict(X_test)\nprint('y predict', y_pred)","8799a4cc":"#random Forest\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('T - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","066d93bf":"#Random Forest\n# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","5c7d400b":"#RF clasification Report\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model Random Forest :',round(roc_auc_score(y_test,y_pred),5))","03bc9c0f":"RF = RandomForestClassifier()\nRFscore = round(roc_auc_score(y_test,y_pred),5)\nprint (RFscore)","96f4e61a":"#polting roc auc curve method LG\nfrom sklearn import metrics\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","811e81cf":"# plotly comparasion\nimport plotly\nfrom plotly.offline import init_notebook_mode, iplot\nplotly.offline.init_notebook_mode(connected=True)\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport itertools\nplt.style.use('fivethirtyeight')\n\nscores=[GBscore,KNNscore,DSscore,LSscore,RFscore]\nAlgorthmsName=[\"Gradient Boasting\",\"K-NN\",\"Decision Tree\",\"Logistic Regresion\",\"Random Forest\"]\n#create traces\ntrace1 = go.Scatter(\n    x = AlgorthmsName,\n    y= scores,\n    name='Algortms Name',\n    marker =dict(color='rgba(0,255,0,0.5)',\n               line =dict(color='rgb(0,0,0)',width=2)),\n                text=AlgorthmsName\n)\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",\n                  xaxis= dict(title= 'ML Algorithms',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Accuracy Scores',ticklen= 5,zeroline= False))\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","f570d1bf":"In the gradient boasting algorithm this time we use the adjusting development threshold Gradient Boosting Classifier (learning_rate= 0.05, max_depth= 3, max_features= 0.5, random_state= 42), result this accuracy on training set: 0.882, accuracy on test set: 0.750","2b0fdd2c":"Random Forest algorithm we use the\ncustomize development threshold Random\nForest Classifier (n_estimators= 100, criterion=\n&#39;gini&#39;, max_depth= 6, max_features= &#39;auto&#39;,\nrandom_state= 0), produce accuracy on this\ntraining set: 0.917, accuracy on the test set:\n0.745","b2f3aaac":"The Decision Tree algorithm we use the\ncustomize development threshold Decision Tree\nClassifier (max_depth= 6, max_features= 4,\nmin_samples_split= 4, random_state= 42),\nproduce accuracy on this training set: 0.852,\naccuracy on the test set: 0.729","600ba24c":"Logistic Regression algorithm we use\nthe Customize development threshold\nlogreg_classifier= Logistic Regression (C = 1,\npenalty = &#39;l1&#39;), produce accuracy on this training\nset: 0.783, accuracy on the test set: 0.724","d169608a":"The final results of the trial have been\ndone as the table above, there you can see\nthat the best level of accuracy is the\nGradient Boasting method with accuracy\n(0.81) and for the method with the worst\naccuracy is K-NN (0.64), followed by the\nresults of the Logistic Regression method\nwith accuracy (0.705),","5e1408cc":"Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning (ML) techniques allows us to obtain predictive, the dataset we are testing is pima-indian-diabetes with a dataset of 765 raw data with 8 data features and 1 data label we developed a method to achieve the best accuracy from the 5 methods we use with the stages of separation traning and testing the dataset, scaling features, parameters evaluation, confusion matrix and we get the accuracy of each method, and the results of the accuracy we get with these 5 methods Gradient-boasting is best with an accuracy score of 0.8, Decision Tree 0.72, Random Forest 0.72, next is Logistic Regression 0.7, and then followed by K-NN method with a score of 0.65"}}