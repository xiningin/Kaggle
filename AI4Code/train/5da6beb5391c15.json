{"cell_type":{"2bfde418":"code","99de54e4":"code","e7f8a6a3":"code","12f15c73":"code","8f10c481":"code","a11a3f97":"code","d830c74a":"code","2af6132d":"code","579be500":"code","632296b6":"code","631ab675":"code","2efb084f":"code","f2b4ace3":"code","7bdd773c":"code","c3f91f55":"code","9aac07b0":"code","29127cf2":"code","fd15d0c8":"code","5df8a6a6":"code","4d2c98e9":"markdown","2bb34f1c":"markdown","47b50f82":"markdown","2d02f4c2":"markdown"},"source":{"2bfde418":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","99de54e4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom keras.layers import Dense, Convolution2D, MaxPooling2D, ZeroPadding2D, Flatten, Dropout\nfrom keras.models import Sequential\nfrom nltk import word_tokenize\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import SGD , Adam\nfrom keras.layers import Conv2D , BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\n","e7f8a6a3":"df = pd.read_csv('..\/input\/fer2013.csv')\nprint(df.shape)","12f15c73":"print(df.columns)","8f10c481":"df.head()","a11a3f97":"pixels = df.loc[:,'pixels'].values\n\nprint(pixels.shape)\nprint(type(pixels))","d830c74a":"px = []\nfor x in pixels : \n    x = word_tokenize(x)\n    x = [float(t) for t in x]\n    px.append(x)","2af6132d":"x = np.array(px)\nprint(x.shape)","579be500":"y = df.loc[:, 'emotion'].values\nprint(y.shape)\nprint(type(y))","632296b6":"for ix in range(5,10):\n    plt.figure(ix)\n    plt.imshow(x[ix].reshape((48, 48)), interpolation='none', cmap='gray')\nplt.show()\n","631ab675":"x = x \/ 255\nx_train = x[0:28710,:]\ny_train = y[0:28710]\nprint (x_train.shape, y_train.shape)\nx_val = x[28710:32300,:]\ny_val = y[28710:32300]\nprint (x_val.shape, y_val.shape)\n","2efb084f":"from keras.utils import np_utils\n\nx_train = x_train.reshape((x_train.shape[0],48, 48,1 ))\nx_crossval = x_val.reshape((x_val.shape[0],48, 48,1))\nprint (y.shape)\nyy = np_utils.to_categorical(y, 7)\nprint (yy.shape)\ny_train = yy[:28710]\ny_crossval = yy[28710:32300]\nprint (x_crossval.shape, y_crossval.shape)\n","f2b4ace3":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,  \n        rotation_range=10,  \n        zoom_range = 0.0,  \n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        horizontal_flip=False, \n        vertical_flip=False)  \n\ndatagen.fit(x_train)\n","7bdd773c":"##https:\/\/github.com\/bckenstler\/CLR\nimport keras\n\nclass CyclicLR(keras.callbacks.Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","c3f91f55":"from keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D , BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model\nfrom keras.initializers import glorot_uniform\n\n","9aac07b0":"from keras.layers import SpatialDropout2D\nfrom keras import regularizers\n\n\nmodel=Sequential()\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(48,48,1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(SpatialDropout2D(0.5))\n\n\nmodel.add(Conv2D(128, (3, 3),activation='relu',padding='same'))\nmodel.add(Conv2D(128, (3, 3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(SpatialDropout2D(0.5))\n\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same',kernel_regularizer=regularizers.l2(0.09)))\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same',kernel_regularizer=regularizers.l2(0.09)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(SpatialDropout2D(0.5))\n\n\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256,activation = 'relu',kernel_regularizer=regularizers.l2(0.09)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.7))\n\nmodel.add(Dense(256,activation = 'relu',kernel_regularizer=regularizers.l2(0.09)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(7,activation = 'softmax'))\n\nprint(model.summary())","29127cf2":"clr_triangular = CyclicLR(mode='triangular2',step_size = 200)\n","fd15d0c8":"opt = Adam(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n\nhistory = model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n                    validation_data=(x_crossval, y_crossval),\n                    steps_per_epoch=x.shape[0] \/\/ 64,\n                    callbacks=[clr_triangular],\n                    epochs = 1800)\n","5df8a6a6":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","4d2c98e9":"## Data Augmentation ","2bb34f1c":"### Importing The Data ","47b50f82":"### Transfroming pixels to the right format","2d02f4c2":"## The Model "}}