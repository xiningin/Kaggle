{"cell_type":{"bedbab56":"code","991b62ae":"code","4c6bb199":"code","6377745e":"code","5da71cba":"code","263775e2":"code","218185da":"code","9dd293ff":"code","0d63c6cd":"code","46912f9e":"code","95c0623e":"code","1824b4fa":"code","b827cebe":"code","11ef366f":"code","4a706bc5":"code","a06e5bfe":"code","6272478a":"code","6eb40d88":"markdown","e2700cff":"markdown","bd1ecf20":"markdown","d45419d5":"markdown","6bce1d62":"markdown","bc9c0f40":"markdown","5fd1cba9":"markdown","87ff0a35":"markdown","dffad581":"markdown","95ea04af":"markdown","11424ddd":"markdown","3eafa9dd":"markdown","8868d287":"markdown","2b21e5bb":"markdown","a4ff940f":"markdown","76cf7377":"markdown","861afabf":"markdown","44826d1e":"markdown","d8084602":"markdown"},"source":{"bedbab56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","991b62ae":"import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cufflinks as cf\nimport seaborn.apionly as sns\nimport scipy as sp\nimport matplotlib.patches as mpatches\n%matplotlib inline","4c6bb199":"import pandas as pd\nmulti = pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\")\nmulti_2= pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\")\nmulti_3= pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\")","6377745e":"#multiple ans question\nmulans= {'Q9' : range(11,19),'Q12' : range(22,34),'Q13' :range(35,47),'Q16':range(56,68),'Q17':range(69,81),'Q18':range(82,94),'Q20':range(97,109),'Q21':range(110,115),'Q24':range(118,130),'Q25':range(131,139),'Q26':range(140,147),'Q27':range(148,154),'Q28':range(155,167),'Q29':range(168,180),'Q30':range(181,193),'Q31':range(194,206),'Q32':range(207,219),'Q33':range(220,232),'Q34':range(233,245)}\n#one answer question\noneans= list(multi.columns[1:11])+list(multi.columns[20:22])+list(multi.columns[48:49])+list(multi.columns[55:56])+list(multi.columns[95:96])+list(multi.columns[116:117])+list(multi.columns[117:118])\n\nmulti.rename(columns ={'Time from Start to Finish (seconds)':'number'},inplace = True)\nmulti_2.rename(columns ={'Time from Start to Finish (seconds)':'number'},inplace = True)\nmulti_3.rename(columns ={'Time from Start to Finish (seconds)':'number'},inplace = True)\nmulti.columns","5da71cba":"def convert(s): \n    str1 = \"\" \n    return(str1.join(s)) \n\ndef item_name(x):\n    languages = []\n    for col in multi.iloc[0,mulans[x]]:\n        lal = col.split(\"-\")\n        doom = lal[2]\n        doom_2 = list(doom)\n        del doom_2[0]\n        doom_3 = convert(doom_2)\n        languages.append(doom_3)\n    return languages\n\ndef perfect(j):\n    nice = []\n    for col in list(multi.columns[mulans[j]]):\n        for col2 in item_name(j):\n            q_1 =multi.loc[multi[col] == col2]\n            q_1.rename(columns={col: j},inplace = True)\n            nice.append(q_1)\n    nice = pd.concat(nice)\n    return nice\nduit = perfect('Q9')","263775e2":"q9_2_d = multi.loc[multi['Q9_Part_2'] != 'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data']\nq9_3_d = q9_2_d.loc[q9_2_d['Q9_Part_3'] != 'Build prototypes to explore applying machine learning to new areas']\nq9_4_d = q9_3_d.loc[q9_3_d['Q9_Part_4'] != 'Build and\/or run a machine learning service that operationally improves my product or workflows']\nq9_5_d = q9_4_d.loc[q9_4_d['Q9_Part_5'] != 'Experimentation and iteration to improve existing ML models']\nq9_6_d = q9_5_d.loc[q9_5_d['Q9_Part_6'] != 'Do research that advances the state of the art of machine learning']\nq9_7_d = q9_6_d.loc[q9_6_d['Q9_Part_7'] != 'None of these activities are an important part of my role at work']\nq9_8_d = q9_7_d.loc[q9_7_d['Q9_Part_8'] != 'Other']\nq9_9_d = q9_8_d.loc[q9_7_d['Q9_Part_1'] == 'Analyze and understand data to influence product or business decisions']\n\nwe = pd.pivot_table(q9_9_d,values=['number'], index=['Q8'],aggfunc= lambda x :len(x))\nwe_t = pd.pivot_table(duit,values=['number'], index=['Q8','Q9'],aggfunc= lambda x :len(x))\n\n\nwe_r = we_t.reset_index()\nwe_a = we_r.loc[we_r['Q9'] != 'Other']\nwe_b = we_a.set_index('Q8')\nwe_group = we_b.groupby('Q8').sum()\n\ndata = we['number']\/we_group['number']\ndata_1 = data.reset_index()\ndata_2 = data_1.sort_values('number', ascending = False)\nplt.figure(figsize =(10,7))\naxf = sns.barplot(x='Q8',y = 'number', data = data_2, palette = 'Spectral')\nx_axis=range(4)\nlabels = data_2['Q8']\nimport textwrap\nfrom  textwrap import fill\nplt.xticks(x_axis, [textwrap.fill(label, 20) for label in labels], \n           rotation = 0, fontsize=12, horizontalalignment=\"center\")\nplt.title('Percentage of workers who analyse data only, for different companies with different levels of machine learning involvement', size = 10)\nplt.ylabel(' % Number of Respondents',size = 10)\nplt.xlabel('Figure 1',size = 10)","218185da":"multi['Q4'].replace({'Bachelor\u2019s degree':'University degree'\n                              ,'Doctoral degree':'University degree'\n                              ,'Master\u2019s degree':'University degree'\n                              ,'Professional degree':'University degree'\n                              ,'I prefer not to answer':'Without University degree'\n                              ,'No formal education past high school':'Without University degree'\n                             , 'Some college\/university study without earning a bachelor\u2019s degree':'Without University degree'},inplace = True)\nplp = pd.pivot_table(multi[1:],values=['number'], index=['Q4','Q10'],aggfunc= lambda x :len(x))\nplp_1 = plp.unstack()\nplp_1.columns = plp_1.columns.droplevel()\nplp_2 = plp_1.transpose()\nplp_2 = plp_2.div(plp_2.sum())*100\nxaxis1 = [\"0 -1K\", \"1 - 2K\", \"2 - 3K\", \"3 - 4K\", \"4 - 5K\", \"5 - 7.5K\", \"7.5 - 10K\", \"10 - 15K\", \"15 - 20K\", \"20 - 25K\", \"25 - 30K\", \"30 - 40K\", \"40 - 50K\", \"50 - 60K\", \"60 - 70K\", \"70 - 80K\", \"80 - 90K\", \"90 - 100K\", \"100 - 125K\", \"125 - 150K\", \"150 - 200K\", \"200 - 250K\", \"250 - 300K\", \"300 - 500K\", \"> $500K\"]\nplp_3 = plp_2.rename(index = {'$0-999':'0-999','> $500,000':'greater than 500,000'})\nplp_4 = plp_3.reset_index()\nplp_4['length'] = plp_4['Q10'].str.len()\nplp_5 = plp_4.sort_values(['length','Q10'])\nplp_5['money'] = xaxis1\nplp_a = plp_5.set_index('money')\nplp_b = plp_a.drop(['Q10'],axis = 1)\nplp_6= plp_b.drop(columns=['length'])\naxa = plp_6.plot(kind ='bar',stacked = False,figsize = (13,7),cmap = \"Set3\",alpha = 0.85, width = 0.85)\naxa.legend(loc = 'center left',bbox_to_anchor =(1, 0.5))\naxa.set_xticklabels(axa.get_xticklabels(), rotation=45)\nplt.title('Compensation of respondents who graduated with \/ without a university degree ', size = 15)\nplt.ylabel('% Number of Respondents',size = 13)\nplt.xlabel('Figure 2',size = 10)","9dd293ff":"indx = 'Q5' #Approximately how much money have you spent on machine learning and\/or cloud computing products at your work in the past 5 years?\ndef convert(s): \n    str1 = \"\" \n    return(str1.join(s)) \n\nif indx in oneans:\n    bob = pd.pivot_table(multi[1:],values=['number'], index=indx,aggfunc= lambda x :len(x))\nelse:\n    win=[]\n    for col in multi.iloc[0,mulans[indx]]:\n        lal = col.split(\"-\")\n        doom = lal[2]\n        doom_2 = list(doom)\n        del doom_2[0]\n        doom_3 = convert(doom_2)\n        win.append(doom_3)\n    \n    result = []\n    for col in mulans[indx]:\n        we =[]\n        for row in range(1,19718):\n            lal = multi.iloc[row,col]\n            we.append(lal)\n        withoutna = [x for x in we if str(x) != 'nan']\n        result.append(len(withoutna))\n    bob=pd.DataFrame(data=result,index=win) \nbob_1 = bob.reset_index()\nbob_2 = bob_1.sort_values('number')\nplt.figure(figsize = (13,8))\naxs = sns.barplot(x='Q5',y = 'number', data = bob_2, palette = 'Spectral')\nfor p in axs.patches:\n    axs.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'top', xytext = (0, 10), textcoords = 'offset points')\nplt.title('Jobs of respondents of the survey',size = 15)\nplt.ylabel('Number of respondents',size = 13)\nx_axis = range(12)\nlabels = bob_2['Q5']\nimport textwrap\nfrom  textwrap import fill\nplt.xticks(x_axis, [textwrap.fill(label, 13) for label in labels], \n           rotation = 0, fontsize=12, horizontalalignment=\"center\")\naxs.set_xticklabels(axs.get_xticklabels(), rotation = 50)\nplt.xlabel('Figure 3',size = 10)","0d63c6cd":"student_compare = pd.pivot_table(multi_2[1:],values=['number'], index=['Q5','Q15'],aggfunc= lambda x :len(x))\nsc = student_compare.loc['Student']\nsc_1 = sc.loc['I have never written code']\nsc_2 = sc.iloc[0:6]\nsc_3 = sc_2.sum()\nsc_4 = pd.concat([sc_1,sc_3],axis =1)\nsc_4.rename(columns={0: \"Students who have written code\",'I have never written code':'Students who never written code'},inplace = True)\nsc_4.rename(index={'number': \"student\"},inplace = True)\nsc_5 = sc_4.transpose()\npie_label = sc_5['student'].sort_values().index\npie_counts = sc_5['student'].sort_values()\ncmap = plt.get_cmap('Set3')\ncolors = [cmap(i) for i in np.linspace(0, 1, 8)]\nstudent_pit = plt.pie(pie_counts, labels = pie_label, autopct='%1.1f%%', shadow=True, colors=colors)\nplt.title('Percentage of students who have written code \/ never written code before',size = 13)\nplt.xlabel('Figure 4',size = 10)","46912f9e":"duit = perfect('Q9')\nbusiness_duit =  pd.pivot_table(duit,values=['number'], index=['Q5','Q9'],aggfunc= lambda x :len(x))\n\nst = business_duit.loc['Business Analyst']\nst_1 = st.reset_index()\nplt.figure(figsize = (13,8))\naxy = sns.barplot(x='Q9',y = 'number', data = st_1, palette = 'Spectral')\nfor p in axy.patches:\n    axy.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'top', xytext = (0, 10), textcoords = 'offset points')\nplt.title('Job scope of respondents who are business analysts' , size = 15)\nplt.xlabel('Figure 5',size = 13)\nplt.ylabel('Number of respondents',size = 13)\nx_axis = range(8)\nlabels = st_1['Q9']\nimport textwrap\nfrom  textwrap import fill\nplt.xticks(x_axis, [textwrap.fill(label, 13) for label in labels], \n           rotation = 0, fontsize=12, horizontalalignment=\"center\")\n\nplt.xlabel('Figure 5',size = 10)\n","95c0623e":"research = pd.pivot_table(multi_3[1:],values=['number'], index=['Q9_Part_6','Q4'],aggfunc= lambda x :len(x))\nresearch_1 = research.reset_index()\nresearch_2 = research_1.sort_values('number')\nplt.figure(figsize = (12,8))\nre = sns.barplot(x='Q4',y = 'number', data = research_2, palette = 'Spectral')\nfor p in re.patches:\n    re.annotate(format(p.get_height(), '.1f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nlabels = research_2['Q4']\nx_axis=range(8)\nimport textwrap\nfrom  textwrap import fill\nplt.xticks(x_axis, [textwrap.fill(label, 13) for label in labels], \n           rotation = 0, fontsize=12, horizontalalignment=\"center\")\nplt.title('Education level of respondents who does research in state of art machine learning' , size = 15)\nplt.ylabel('Number of respondents',size = 13)\nplt.xlabel('Figure 6',size = 10)","1824b4fa":"toom = pd.pivot_table(multi[1:],values=['number'], index=['Q23','Q10'],aggfunc= lambda x :len(x))\ntoom_1 = toom.unstack()\ntoom_1.columns = toom_1.columns.droplevel()\ntoom_1 = toom_1.reindex(index = ['< 1 years',\n '1-2 years',\n'2-3 years',\n '3-4 years',\n'4-5 years',\n '5-10 years',\n '10-15 years',\n '20+ years'])\n\ntoom_1 = toom_1[['$0-999',\n '1,000-1,999',\n '2,000-2,999',\n '3,000-3,999',\n '4,000-4,999',\n '5,000-7,499',\n '7,500-9,999',\n '10,000-14,999',\n '15,000-19,999',\n '20,000-24,999',\n '25,000-29,999',\n '30,000-39,999',\n '40,000-49,999',\n '50,000-59,999',\n '60,000-69,999',\n '70,000-79,999',\n '80,000-89,999',\n '90,000-99,999',\n '100,000-124,999',\n '125,000-149,999',\n '150,000-199,999',\n '200,000-249,999',\n '250,000-299,999',\n '300,000-500,000','> $500,000']]\n\ntoom_1 = toom_1.div(toom_1.sum())\ntoom_2 = toom_1.transpose()\ntoom_3 = toom_2.reset_index()\nxaxis1 = [\"0 -1K\", \"1 - 2K\", \"2 - 3K\", \"3 - 4K\", \"4 - 5K\", \"5 - 7.5K\", \"7.5 - 10K\", \"10 - 15K\", \"15 - 20K\", \"20 - 25K\", \"25 - 30K\", \"30 - 40K\", \"40 - 50K\", \"50 - 60K\", \"60 - 70K\", \"70 - 80K\", \"80 - 90K\", \"90 - 100K\", \"100 - 125K\", \"125 - 150K\", \"150 - 200K\", \"200 - 250K\", \"250 - 300K\", \"300 - 500K\", \"> $500K\"]\ntoom_3['money'] = xaxis1\ntoom_4 = toom_3.set_index('money')\ntoom_4 = toom_4.drop(['Q10'],axis = 1)\n\naxm = toom_4.plot(kind ='bar',stacked =True,figsize = (13,7),cmap = \"Set3\",alpha = 0.85, width = 0.85)\naxm.legend(loc = 'center left',bbox_to_anchor =(1, 0.5))\naxm.set_xticklabels(axm.get_xticklabels(), rotation=45)\nplt.title('Correlation between machine learning experience, number of respondents and yearly income', size = 15)\nplt.ylabel(' % Number of Respondents',size = 13)\nplt.xlabel('Figure 7',size = 10)","b827cebe":"boom = pd.pivot_table(multi[1:],values=['number'], index=['Q10','Q15'],aggfunc= lambda x :len(x))\nboom_1 = boom.unstack()\nboom_1.columns = boom_1.columns.droplevel()\nboom_2 = boom_1.rename(index = {'$0-999':'0-999','> $500,000':'greater than 500,000'})\nboom_3 = boom_2.reset_index()\nxaxis1 = [\"0 -1K\", \"1 - 2K\", \"2 - 3K\", \"3 - 4K\", \"4 - 5K\", \"5 - 7.5K\", \"7.5 - 10K\", \"10 - 15K\", \"15 - 20K\", \"20 - 25K\", \"25 - 30K\", \"30 - 40K\", \"40 - 50K\", \"50 - 60K\", \"60 - 70K\", \"70 - 80K\", \"80 - 90K\", \"90 - 100K\", \"100 - 125K\", \"125 - 150K\", \"150 - 200K\", \"200 - 250K\", \"250 - 300K\", \"300 - 500K\", \"> $500K\"]\nboom_3['length'] = boom_3['Q10'].str.len()\nboom_4 = boom_3.sort_values(['length','Q10'])\nboom_5= boom_4.drop(columns=['length'])\nboom_5['money'] = xaxis1\nboom_6 = boom_5.set_index('money')\nboom_6 = boom_6[['I have never written code', '< 1 years','1-2 years','3-5 years','5-10 years','10-20 years','20+ years']]\nboom_7 = boom_6.transpose()\nboom_7 = boom_7.div(boom_7.sum())\ntoom_3['money'] = xaxis1\nboom_8=boom_7.transpose()\naxn = boom_8.plot(kind ='bar',stacked =True,figsize = (13,7),cmap = \"Set3\",alpha = 0.85, width = 0.85)\naxn.legend(loc = 'center left',bbox_to_anchor =(1, 0.5))\naxn.set_xticklabels(axn.get_xticklabels(), rotation=45)\nplt.title('Correlation between coding experience, number of respondents and yearly income', size = 15)\nplt.ylabel(' % Number of Respondents',size = 13)\nplt.xlabel('Figure 8',size = 10)","11ef366f":"q9_2_d = multi.loc[multi['Q9_Part_2'] != 'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data']\nq9_3_d = q9_2_d.loc[q9_2_d['Q9_Part_3'] != 'Build prototypes to explore applying machine learning to new areas']\nq9_4_d = q9_3_d.loc[q9_3_d['Q9_Part_4'] != 'Build and\/or run a machine learning service that operationally improves my product or workflows']\nq9_5_d = q9_4_d.loc[q9_4_d['Q9_Part_5'] != 'Experimentation and iteration to improve existing ML models']\nq9_6_d = q9_5_d.loc[q9_5_d['Q9_Part_6'] != 'Do research that advances the state of the art of machine learning']\nq9_7_d = q9_6_d.loc[q9_6_d['Q9_Part_7'] != 'None of these activities are an important part of my role at work']\nq9_8_d = q9_7_d.loc[q9_7_d['Q9_Part_8'] != 'Other']\nq9_9_d = q9_8_d.loc[q9_7_d['Q9_Part_1'] == 'Analyze and understand data to influence product or business decisions']\n\nyear = pd.pivot_table(q9_9_d[1:],values=['number'], index=['Q10'],aggfunc= lambda x :len(x))\nyear.rename(columns={\"number\": \"analyze data only\"},inplace = True)\nyear_1 = year.rename(index = {'$0-999':'0-999','> $500,000':'greater than 500,000'})\nyear_1.reset_index(inplace= True)\nyear_1['length'] = year_1['Q10'].str.len()\nyear_2 = year_1.sort_values(['length','Q10'])\nyear_3= year_2.drop(columns=['length'])\nyear_3.set_index('Q10',inplace=True)\n\nyear_tot = pd.pivot_table(multi[1:],values=['number'], index=['Q10'],aggfunc= lambda x :len(x))\nyear_tot.rename(columns={\"number\": \"all\"},inplace = True)\n\nyear_tot_1 = year_tot.rename(index = {'$0-999':'0-999','> $500,000':'greater than 500,000'})\nyear_tot_1.reset_index(inplace= True)\nyear_tot_1['length'] = year_tot_1['Q10'].str.len()\nyear_tot_2 = year_tot_1.sort_values(['length','Q10'])\nyear_tot_3= year_tot_2.drop(columns=['length'])\nyear_tot_3.set_index('Q10',inplace=True)\n\ntgt = pd.concat([year_tot_3,year_3],axis = 1)\ntgt_1 = tgt.reset_index()\ntgt_1['total'] = tgt_1['all'] + tgt_1['analyze data only']\ntgt_1['% all'] = tgt_1['all']\/tgt_1['total']\ntgt_1['% analyze data only'] = tgt_1['analyze data only']\/tgt_1['total']\ntgt_2 = tgt_1.drop(columns=['all', 'analyze data only','total'])\ntgt_3 = tgt_2.set_index('Q10')\nax4 = tgt_3.plot(kind ='bar',stacked = True,figsize = (13,7),rot = 1,cmap = \"Set3\",alpha = 0.85, width = 0.85,label = xaxis1)\nax4.legend(loc = 'center left',bbox_to_anchor =(1, 0.5)) \nlabels = research_2['Q4']\nx_axis=range(8)\nax4.set_xticklabels(axn.get_xticklabels(), rotation=45)\nplt.xlabel('Figure 9',size = 10)\n","4a706bc5":"indx = 'Q15' #Approximately how much money have you spent on machine learning and\/or cloud computing products at your work in the past 5 years?\ndef convert(s): \n    str1 = \"\" \n    return(str1.join(s)) \n\nif indx in oneans:\n    tob = pd.pivot_table(multi_3[1:],values=['number'], index=indx,aggfunc= lambda x :len(x))\nelse:\n    win=[]\n    for col in multi_3.iloc[0,mulans[indx]]:\n        lal = col.split(\"-\")\n        doom = lal[2]\n        doom_2 = list(doom)\n        del doom_2[0]\n        doom_3 = convert(doom_2)\n        win.append(doom_3)\n    \n    result = []\n    for col in mulans[indx]:\n        we =[]\n        for row in range(1,19718):\n            lal = multi_3.iloc[row,col]\n            we.append(lal)\n        withoutna = [x for x in we if str(x) != 'nan']\n        result.append(len(withoutna))\n    tob=pd.DataFrame(data=result,index=win) \n\ntob_1= tob.reindex (index = ['I have never written code', '< 1 years','1-2 years','3-5 years','5-10 years','10-20 years','20+ years'])\ntob_2 = tob_1.reset_index()\nplt.figure(figsize = (12,8))\naxc = sns.barplot(x='Q15',y = 'number', data = tob_2, palette = 'Spectral')\nfor p in axc.patches:\n    axc.annotate(format(p.get_height(), '.1f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n    \nplt.title('Coding experience', size = 15)\nplt.ylabel('Number of respondents',size = 13)\nplt.xlabel('Figure 10',size = 10)\n","a06e5bfe":"w = perfect('Q9')\ncount_q9 = pd.pivot_table(w[1:],values=['number'], index=['Q3','Q9'],aggfunc= lambda x :len(x))\nq9_india = count_q9.iloc[150:158]\nq9_india_1 = q9_india.unstack(level = -1)\nq9_usa = count_q9.iloc[435:443]\nq9_usa_1 = q9_usa.unstack(level = -1)\nq9_tpt = pd.concat([q9_india_1,q9_usa_1])\nq9_tpt.columns = q9_tpt.columns.droplevel()\nq9_tot = q9_tpt.transpose()\nq9_tot = q9_tot.div(q9_tot.sum())*100\nq9_tot_1 = q9_tot.reset_index()\nplt.figure(figsize = (13,8))\naxr = q9_tot_1.plot(kind ='bar',stacked = False,figsize = (13,7),cmap = \"Set3\",alpha = 0.85, width = 0.70)\naxr.legend(loc = 'center left',bbox_to_anchor =(1, 0.5))\nlabels = q9_tot.index\nx_axis=range(8)\nimport textwrap\nfrom  textwrap import fill\nplt.xticks(x_axis, [textwrap.fill(label, 13) for label in labels], \n           rotation = 0, fontsize=12, horizontalalignment=\"center\")\nfor p in axr.patches:\n    axr.annotate(format(p.get_height(), '.1f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title('India and USA', size = 15)\nplt.ylabel(' % Number of Respondents',size = 13)\nplt.xlabel('Figure 11',size = 10)","6272478a":"count_q5 = pd.pivot_table(multi[1:],values=['number'], index=['Q3','Q5'],aggfunc= lambda x :len(x))\n\nq5_india = count_q5.iloc[222:234]\nq5_usa = count_q5.iloc[635:647]\nq5_india_1 = q5_india.unstack(level = -1)\nq5_usa_1 = q5_usa.unstack(level = -1)\nq5_tpt = pd.concat([q5_india_1,q5_usa_1])\nq5_tpt.columns = q5_tpt.columns.droplevel()\nq5_tot = q5_tpt.transpose()\nq5_tot = q5_tot.div(q5_tot.sum())*100\nq5_tot_1 = q5_tot.reset_index()\nplt.figure(figsize = (13,8))\naxr = q5_tot_1.plot(kind ='bar',stacked = False,figsize = (13,7),cmap = \"Set3\",alpha = 0.85, width = 0.70)\naxr.legend(loc = 'center left',bbox_to_anchor =(1, 0.5))\nlabels = q5_tot.index\nx_axis=range(12)\nimport textwrap\nfrom  textwrap import fill\nplt.xticks(x_axis, [textwrap.fill(label, 13) for label in labels], \n           rotation = 0, fontsize=12, horizontalalignment=\"center\")\nfor p in axr.patches:\n    axr.annotate(format(p.get_height(), '.1f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title('India and USA', size = 15)\nplt.ylabel(' % Number of Respondents',size = 13)\naxr.set_xticklabels(axr.get_xticklabels(), rotation=45)\nplt.xlabel('Figure 12',size = 10)\n","6eb40d88":"In figure 5, the majority of business analyst requires analysing data as part of their role, when machine learning starts to take over this aspect of the job to generate graphs and insights, there is no longer such a great need to analyse data manually. Employees could focus more time on other tasks that provide the business with real value, such as coming up with innovative strategies and the means to improve processes; and the same is expected for other jobs such as banking, consulting, etc.\n\nSecondly, as explained above jobs would become more competitive with machine learning. Students would find it harder to find roles in machine learning and data analysing related jobs as they would be competition from the existing working professionals as well. This would force them to seek roles that might not be data related, again making their coding skills almost redundant.\n\nThe long term impact of machine learning, on the other hand, is still rather unclear. It is suggested that in the long run it might have a positive impact on job creation, especially in the field of cybersecurity. Increasing use of machine learning, would allow already-pervasive criminal enterprises \u2013 hackers, malware, and other threats \u2013 to grow exponentially, requiring developers, testers, and security experts to mitigate threats to vital public infrastructure and meet increasing individual identity concerns. (Appen, 2019)","e2700cff":"**Working-class Data Analysers**\n\nThe group most directly affected by the introduction of ML is the working-class data analysers. ML aims to replace their jobs completely, making their role redundant and thus the highly likely possibility of them being retrenched.\n\nFrom the figure above, a majority of those currently performing data analysis tasks at the workplace possess an academic (Bachelor\u2019s, Master\u2019s, Doctor\u2019s) or professional qualification (Professional degree). Due to the relatively field-specific nature of these qualifications, this group of people is likely to have invested significant resources into becoming an expert in the area of data analysis and switching to another field would likely affect their career path and financial situation negatively.\n\nThe alternative to switching to a vastly different field would be to expand their skill set to include the necessary skills to remain relevant in the data science field. Traditionally, people would complete a university course and get a certificate. However, we now live in the digital age and as a result have more options, namely Massive Open Online Courses (MOOCs). MOOCs are not online university degrees but in recent years MOOC companies like Coursera, Udacity, and edX have partnered with top universities, lending credibility to their courses and certification. The advantages of MOOCs include being able to offer courses that a local university may not be offering, more flexibility depending on course, and generally a lower cost than the equivalent university course. \n\n","bd1ecf20":"**REFERENCES **\n\n* Aravind Sekar. What is the difference between Data Science and Machine Learning?. Analytics Training. Available at: https:\/\/analyticstraining.com\/what-is-the-difference-between-data-science-and-machine-learning\/ [Accessed 30th November 2019]\n* Sterling Osborne. April 2019. Was it worth studying a data science masters?, Medium. Available at: https:\/\/towardsdatascience.com\/was-it-worth-studying-a-data-science-masters-c469e5b3e020 [Accessed 30th November 2019]\n* Appen. (2019). What New Jobs Will AI Create? | Appen. [online] Available at: https:\/\/appen.com\/blog\/what-new-jobs-will-ai-create\/ [Accessed 1 Dec. 2019].\n* TIOBE Index for November 2019. TIOBE. Available at: https:\/\/www.tiobe.com\/tiobe-index\/ [Accessed 30th November 2019]","d45419d5":"India and US has roughly equal number of people who analyses data .The graph also indicates that India has a larger majority of students,which means that this group would be the largest hit. This is because the replacement of jobs with machine learning is predicted to be a faster changing process, than that of people changing their skillset\/degree. ","6bce1d62":"India would be greater affected by this change because in the short term because the majority of the population are either students\/ working class data scientist. US would undergo a change as well, however, because the 4 different categories ( researchers, students, top earners, working class analyst) are more spread out the effect would be less than that of India.","bc9c0f40":"**Researchers**","5fd1cba9":"The trend is globally the same as for both topics, although the answer \"I have never written code\" provides some extra information, regarding people who do not do data analysis. Even though it seems like an important skill to master, it is not mandatory depending on the job you do. The same applies for Machine Learning but the data does not tell people who do not do ML and people who did a few apart.\n\n\nBefore we dive into analysing the effect of machine learning on top earners(>100,00), I will like to explain my reasoning on why these people earn so much. The main reason is because they are doing tasks that cannot be done by others, showing that their skills are rare and in high demand. Figure 7 shows that these \u2018top skills\u2019 can be gained by experience as is evident with the increase median salary with age. \n","87ff0a35":"Secondly, the majority of the population have coding experience of below 5 years. This means that they would not have the experience required to earn top dollars. However, there is a possibility that a domino effect might occur. Where young people replace the age group above them, and the effect goes on all the way to the people with the most experience. In the long run( after 5 years), however, I would predict the compensation of top earners to start decreasing because of the number of young coders; when they gain experience there would no longer be a skill deficiency at the top level, which we lead to a lower skills shortage at the top level and thus lower salaries for the top earners.","dffad581":"Firstly, because the task they are doing is not replaceable by machine learning at the moment. Figure 9 shows the percentage of tasks that are just analysing compared to other tasks. The decreasing trend indicates that the higher the compensation of the person, the more varied their task are; there is an exception for the >500,000 catogary because of the small number of respondents in that catogary,however, we would predict the similar trend if there were more data.","95ea04af":">**Introduction**\n\n\u201cThe development of full artificial intelligence could spell the end of the human race\u2026\u201d. A truly grim warning by Stephen Hawking in an interview with the BBC back in December 2014. Five years have passed and now more than ever we rely on some form of artificial intelligence in our lives, either directly or indirectly. Use Google Maps to travel? AI. Use Amazon\u2019s Alexa? AI. Browse the internet? AI. However, most people don\u2019t care about those examples as much as one singular issue, \u2018Will I lose my job to AI?\u2019. This article aims to explore how AI will affect jobs in both the short and long term.\n\nTo begin with, AI is a term many would say is too broad and overused. Therefore, let us focus on a subset of AI, namely Data Science (DS) and Machine Learning (ML). Data science is a broad term, referring to the processing and analysis of data to generate insights. The role of analysing data and making predictions and decisions that would affect a business has traditionally fallen to humans. After all, since humans can do it, why change something that works? That all changed in the past decade as the world became more connected and big data became a thing. Businesses are now able to obtain huge amounts of data (whether useful or not) for a relatively low cost. Processing this data could give new insights. The only issue? Humans were just too slow to process all that data. \n\nEnter machine learning, a subset of data science and refers to the ability of a system to process data autonomously without human intervention using algorithms. Machine learning processes the data significantly faster than a human and seems like the ideal solution to our problem. However, there are side effects, notably, the lack of human involvement raises the issue of job security for those who are currently analysing data. Is that a legitimate concern and are they the only people who should be concerned about the introduction of ML?\n\nA general assumption is made here in which the data collected in this survey can represent the population as a whole in this particular field of study.\n","11424ddd":"The direct impact of machine learning to those who are researching in the state of art of machine learning is likely to be negligible. Mainly because machine learning would not be able to replace their role at the moment; the majority of people who do research also have a Master or PhD level of education making it hard for other workers with a lower educational background to replace their role as people would need to spend years furthering their studies in order to do research. \n\nThe implications of the increased usage in machine learning would instead create more jobs in research, as people are needed to continually improve and upgrade the existing AI intelligence to make it even better. But overall, the effect on this group of people would not be as significant as the other groups.\n","3eafa9dd":"Many jobs such as banking, consulting, and business analysts rely heavily on analysing data to predict a future outcome and most companies value coding knowledge amongst their employees. As seen in figure 4, this has driven a huge portion of students to gain some experience in writing code in hopes of appearing more appealing to potential employers. While no doubt having coding experience remains preferable to employers, machine learning has reduced the importance of this particular skill. \n","8868d287":"**US vs India**\n\nA comparison between countries was made to see the significance of the effect of machine learning on these four groups of people in both of these countries. A comparison was made between  only two countries the USA and India, because most of the respondents of the Kaggle survey are from these two countries, a meaningful comparison could not be drawn from the rest of the countries because of the limited amount of data available. \n\n\nAs an overview, these are the effects of an advancement in machine learning for these 4 categories of people. Researchers and top earners are least affected, whereas students and working-class data analysts are greater affected. A comparison was made between the two countries the USA and India, because most of the respondents of the Kaggle survey are from these two countries, a meaningful comparisons could not be drawn from the rest of the countries because of the limited amount of data available. \n\n","2b21e5bb":"**Conclusion**\n\nThe analysis presented in this notebook highlights many interesting points.\nStudents who study basic coding as part of their degree would not find such a skill to be as useful as it used to be because of the impact of machine learning. There is an exception however for students who studied degrees such as computer science because the increased usage of machine learning would lead to an increase in jobs in more technical roles such as cybersecurity.\nThe impact of machine learning researchers is likely to be minimal because the majority of researchers requires a high education level, which makes their job rather secure. Predicted increase in the number of jobs in the near future because of the continued advancement of machine learning which would lead to more research\nFor top earners, there would be hardly any impact in the short term is minimal because most people do not have the required experience to replace them. However, in the long term, the top earner\u2019s salary is likely to decrease because more people would be able to have sufficient experience\nIndia would be more affected by the increased usage of machine learning compared to the US, this is mainly because the majority of the population are relatively new to coding and they also have a huge majority of students who are seeking to go into the data field making it even more competitive.\n","a4ff940f":"In figure 3, we can see students is one of the major respondents of this survey. This translates to a significant interest in data analysis amongst the student population. Thus, the increased usage of machine learning in the industry over the next few years will impact their potential jobs in the future. So how exactly will machine learning affect current students in the short and long-term?\n\n \nIn the short term, the use of machine learning in business will decrease the importance of possessing coding knowledge in the data science field. Before the introduction of machine learning, raw data was processed and analysed by people. Machine learning effectively means that, for the most part, data is processed using software and insights generated without human intervention. Therefore, the skill of coding the algorithms used to analyse data are less relevant than they used to be.\n","76cf7377":"**Top Earners**","861afabf":"Figure 1 shows the trend where the more established the ML methods are the lower the percentage of employees that does analysing and understanding data to improve products only. This trend suggests that ML has the ability to replace the task of understanding and analysing data, and thus would allow employees to focus on more important tasks. Although the percentage of employees who just analyse data as their job is low yet we have to take into account the number of people who work in the data industry; Kaggle alone has already millions of users which equates to around at least 50,000 people doing the jobs that are replaceable by ML. Let us explore how different groups of people are affected by ML.","44826d1e":"However, comparing the cost differences between a MOOCs and university degrees is thinking short term and in the long term the real difference is in the salary one earns. As seen in the figure above comparing the compensation of those with a degree and those without, we can establish that there is no significant difference in the amount they earn. In some compensation amount categories those there tend to be more of university degree holders and in other the opposite. Therefore, there is no advantage in taking a university degree compared to MOOCs when it comes to the compensation received at work.","d8084602":"**Students**"}}