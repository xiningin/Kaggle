{"cell_type":{"1fae9fe1":"code","9ed6cb54":"code","6365de25":"code","ee32f1e4":"code","f2b08d0f":"code","a1936587":"code","864f0ef3":"code","2073d1a5":"code","c127692e":"code","e26525fd":"code","f9c276a4":"code","64bdb41d":"code","7ecfa865":"code","c349ff48":"code","4203f02f":"code","0dd8b87e":"code","7ddc9109":"code","323ee4a3":"code","68a9711d":"code","158dce09":"code","061da2b0":"code","48326871":"code","eb29dcd7":"code","4b986ebc":"code","783f5339":"code","d091c5e8":"code","268a3e87":"code","29e35e4b":"code","fbff47d6":"code","08efff36":"code","ab65dcfc":"code","16e37b6f":"code","e237c874":"code","f3b4c354":"code","22378875":"code","b9a59bb0":"code","5a67b599":"code","392a77dd":"code","11d5f46c":"code","1191902a":"code","f218511d":"code","46d2df42":"code","599df847":"code","8145f898":"code","961de12a":"code","99b56ba2":"code","55d53743":"code","00bf325c":"code","edb55920":"code","93713c67":"code","4d754d47":"code","c650a283":"code","26c5dd4a":"code","71365e3d":"code","2e3bdc5f":"code","4b03c50b":"code","f584a985":"code","c21101ce":"code","ec55060f":"code","1d368988":"code","79763316":"code","4ecc232d":"code","0fd8aadc":"code","0e942aa9":"code","3c0a8495":"code","ee450683":"code","19c6738f":"code","c3575428":"code","20a758d1":"code","a8725db7":"code","569af119":"code","b0386e37":"code","a52208f9":"code","c3bdf472":"code","fbc22e03":"code","880650d0":"code","5fcbe6fa":"code","800ec53f":"code","fbe52c75":"code","b9c44553":"code","7e2d6eeb":"code","61f8b6d2":"code","db144292":"code","13f6f82b":"code","be5bfd8b":"code","30cf293c":"code","4080b3cb":"code","2b9d98ea":"code","92c6e918":"code","227feeb6":"code","76a370df":"code","fb700c1b":"code","a3610505":"code","21516db4":"code","b3de1baf":"code","3f669b5f":"code","796ba0f0":"code","7f96df29":"code","7324793b":"code","37e132ff":"code","6b79feda":"code","ecf57a06":"code","16c002f2":"code","265f56eb":"code","7a5c9fda":"code","55f8723a":"markdown","b255d5e6":"markdown","dac6e35b":"markdown","f26d755b":"markdown","559b0a9e":"markdown","7155c9e0":"markdown","9e8e36d0":"markdown","dfdd9cf5":"markdown","fbb87f5e":"markdown","b022eb41":"markdown","79e5fb6c":"markdown","34f20feb":"markdown","46fe0031":"markdown","d214c74e":"markdown","2d9080b4":"markdown","db8e436f":"markdown","ccc18dfa":"markdown","45e65c81":"markdown","4c58ad6f":"markdown","b9c48e09":"markdown","e04ade21":"markdown","8defd1a8":"markdown","0f0bef72":"markdown","0844f80d":"markdown","0bab37d3":"markdown","417a59db":"markdown","0150b438":"markdown","6fbd5e16":"markdown","0c989ff1":"markdown","338962f3":"markdown","0cbbdc54":"markdown","1cf6cf96":"markdown","9a3e311f":"markdown","0146dbe0":"markdown","f3e4f49b":"markdown","f4583c68":"markdown","beb56580":"markdown","c6cf6ce1":"markdown","843b943c":"markdown","1c9120da":"markdown","00405ff9":"markdown","53e7a395":"markdown"},"source":{"1fae9fe1":"import warnings\nwarnings.filterwarnings(\"ignore\")\n#Data Manipulation and Treatment\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n#Plotting and Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nfrom scipy import stats\nimport itertools\n#Scikit-Learn for Modeling\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\n# statistics\n#from statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\nfrom statsmodels.distributions.empirical_distribution import ECDF\n# time series analysis\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","9ed6cb54":"# additional store data\n\nstore = pd.read_csv(\"..\/input\/store.csv\")\nstore.describe()\nstore.head()","6365de25":"# importing train data\n\ntrain = pd.read_csv(\"..\/input\/train.csv\", parse_dates = True, low_memory = False, index_col = 'Date')\ntrain.describe()\ntrain.head()","ee32f1e4":"# data extraction\n\ntrain['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Day'] = train.index.day\ntrain['WeekOfYear'] = train.index.weekofyear\n","f2b08d0f":"train.info()","a1936587":"train.head()","864f0ef3":"# adding new variable\n\ntrain['SalePerCustomer'] = train['Sales']\/train['Customers']\ntrain['SalePerCustomer'].describe()","2073d1a5":"train.isnull().sum()","c127692e":"train.fillna(0, inplace = True)","e26525fd":"sns.set(style = \"ticks\")# to format into seaborn \nc = '#386B7F' # basic color for plots\nplt.figure(figsize = (12, 6))\n\nplt.subplot(311)\ncdf = ECDF(train['Sales'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('Sales'); plt.ylabel('ECDF');\n\n# plot second ECDF  \nplt.subplot(312)\ncdf = ECDF(train['Customers'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('Customers');\n\n# plot second ECDF  \nplt.subplot(313)\ncdf = ECDF(train['SalePerCustomer'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('Sale per Customer');","f9c276a4":"# closed stores\ntrain[(train.Open == 0) & (train.Sales == 0)]","64bdb41d":"# opened stores with zero sales\nzero_sales = train[(train.Open != 0) & (train.Sales == 0)]\nprint(\"In total: \", zero_sales.shape)\nzero_sales.head(5)","7ecfa865":"#print(\"Closed stores and days which didn't have any sales won't be counted into the forecasts.\")\n\ntrain = train[(train[\"Open\"] != 0) & (train['Sales'] != 0)]\n\nprint(\"In total: \", train.shape)","c349ff48":"train=train.drop(columns=train[(train.Open == 1) & (train.Sales == 0)].index)","4203f02f":"{\"Mean\":np.mean(train.Sales),\"Median\":np.median(train.Sales)}","0dd8b87e":"train.Customers.describe()","7ddc9109":"{\"Mean\":np.mean(train.Customers),\"Median\":np.median(train.Customers)}","323ee4a3":"store.head()","68a9711d":"# missing values?\nstore.isnull().sum()","158dce09":"# missing values in CompetitionDistance\nstore[pd.isnull(store.CompetitionDistance)]\n","061da2b0":"store['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True)","48326871":"# no promo = no information about the promo?\n_ = store[pd.isnull(store.Promo2SinceWeek)]\n_[_.Promo2 != 0].shape","eb29dcd7":"# replace NA's by 0\nstore.fillna(0, inplace = True)","4b986ebc":"print(\"Joining train set with an additional store information.\")\n\n# by specifying inner join we make sure that only those observations \n# that are present in both train and store sets are merged together\ntrain_store = pd.merge(train, store, how = 'inner', on = 'Store')\n\nprint(\"In total: \", train_store.shape)\ntrain_store.head(1000)\n","783f5339":"train_store.head(100)","d091c5e8":"train_store.groupby('StoreType')['Sales'].describe()","268a3e87":"train_store.groupby('StoreType')['Customers', 'Sales'].sum()","29e35e4b":"# sales trends\nsns.factorplot(data = train_store, x = 'Month', y = \"Sales\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               color = c)","fbff47d6":"# sales trends\nsns.factorplot(data = train_store, x = 'Month', y = \"Customers\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', \n               color = c)","08efff36":"# sale per customer trends\nsns.factorplot(data = train_store, x = 'Month', y = \"SalePerCustomer\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               color = c)","ab65dcfc":"# customers\nsns.factorplot(data = train_store, x = 'Month', y = \"Sales\", \n               col = 'DayOfWeek', # per store type in cols\n               palette = 'plasma', \n               hue = 'StoreType',\n               row = 'StoreType', # per store type in rows\n               color = c)","16e37b6f":"# stores which are opened on Sundays\ntrain_store[(train_store.Open == 1) & (train_store.DayOfWeek == 7)]['Store'].unique()","e237c874":"# competition open time (in months)\ntrain_store['CompetitionOpen'] = 12 * (train_store.Year - train_store.CompetitionOpenSinceYear) + \\\n        (train_store.Month - train_store.CompetitionOpenSinceMonth)\n    \n# Promo open time\ntrain_store['PromoOpen'] = 12 * (train_store.Year - train_store.Promo2SinceYear) + \\\n        (train_store.WeekOfYear - train_store.Promo2SinceWeek) \/ 4.0\n\n# replace NA's by 0\ntrain_store.fillna(0, inplace = True)\n\n# average PromoOpen time and CompetitionOpen time per store type\ntrain_store.loc[:, ['StoreType', 'Sales', 'Customers', 'PromoOpen', 'CompetitionOpen']].groupby('StoreType').mean()","f3b4c354":"# HeatMap\n# Compute the correlation matrix \n# exclude 'Open' variable\ncorr_all = train_store.drop('Open', axis = 1).corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_all, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_all, mask = mask, annot = True, square = True, linewidths = 0.5, ax = ax, cmap = \"BrBG\", fmt='.2f')      \nplt.show()","22378875":"# sale per customer trends\nsns.factorplot(data = train_store, x = 'DayOfWeek', y = \"Sales\", \n               col = 'Promo', \n               row = 'Promo2',\n               hue = 'Promo2',\n               palette = 'RdPu')","b9a59bb0":"train_store.head()","5a67b599":"# preparation: input should be float type\ntrain['Sales'] = train['Sales'] * 1.0\n\n# store types\nsales_a = train[train.Store == 2]['Sales']\nsales_b = train[train.Store == 85]['Sales']\nsales_c = train[train.Store == 1]['Sales']\nsales_d = train[train.Store == 13]['Sales']\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\n\n# store types\nsales_a.resample('W').sum().plot(color = c, ax = ax1)\nsales_b.resample('W').sum().plot(color = c, ax = ax2)\nsales_c.resample('W').sum().plot(color = c, ax = ax3)\nsales_d.resample('W').sum().plot(color = c, ax = ax4)","392a77dd":"f, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\n\n# monthly\ndecomposition_a = seasonal_decompose(sales_a, model = 'additive', freq = 365)\ndecomposition_a.trend.plot(color = c, ax = ax1)\n\ndecomposition_b = seasonal_decompose(sales_b, model = 'additive', freq = 365)\ndecomposition_b.trend.plot(color = c, ax = ax2)\n\ndecomposition_c = seasonal_decompose(sales_c, model = 'additive', freq = 365)\ndecomposition_c.trend.plot(color = c, ax = ax3)\n\ndecomposition_d = seasonal_decompose(sales_d, model = 'additive', freq = 365)\ndecomposition_d.trend.plot(color = c, ax = ax4)","11d5f46c":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries, window = 12, cutoff = 0.01):\n\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window).mean()\n    rolstd = timeseries.rolling(window).std()\n\n    #Plot rolling statistics:\n    fig = plt.figure(figsize=(12, 8))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    pvalue = dftest[1]\n    if pvalue < cutoff:\n        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n    else:\n        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n    \n    print(dfoutput)","1191902a":"from scipy import stats\nfrom scipy.stats import normaltest\ndef residual_plot(model):\n\n    resid = model.resid\n    print(normaltest(resid))\n    # returns a 2-tuple of the chi-squared statistic, and the associated p-value. the p-value is very small, meaning\n    # the residual is not a normal distribution\n\n    fig = plt.figure(figsize=(12,8))\n    ax0 = fig.add_subplot(111)\n\n    sns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n\n    # Get the fitted parameters used by the function\n    (mu, sigma) = stats.norm.fit(resid)\n\n    #Now plot the distribution using \n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n    plt.ylabel('Frequency')\n    plt.title('Residual distribution')\n\n\n    # ACF and PACF\n    fig = plt.figure(figsize=(12,8))\n    ax1 = fig.add_subplot(211)\n    fig = sm.graphics.tsa.plot_acf(model.resid, lags=40, ax=ax1)\n    ax2 = fig.add_subplot(212)\n    fig = sm.graphics.tsa.plot_pacf(model.resid, lags=40, ax=ax2)","f218511d":"test_stationarity(sales_a)","46d2df42":"first_diff_a = sales_a - sales_a.shift(1)\nfirst_diff_a = first_diff_a.dropna(inplace = False)\ntest_stationarity(first_diff_a, window = 12)","599df847":"test_stationarity(sales_b)","8145f898":"first_diff_b = sales_b - sales_b.shift(1)\nfirst_diff_b = first_diff_b.dropna(inplace = False)\ntest_stationarity(first_diff_b, window = 12)","961de12a":"test_stationarity(sales_c)","99b56ba2":"first_diff_c = sales_c - sales_c.shift(1)\nfirst_diff_c = first_diff_c.dropna(inplace = False)\ntest_stationarity(first_diff_c, window = 12)","55d53743":"test_stationarity(sales_d)","00bf325c":"first_diff_d = sales_d - sales_d.shift(1)\nfirst_diff_d = first_diff_d.dropna(inplace = False)\ntest_stationarity(first_diff_d, window = 12)","edb55920":"# figure for subplots\nplt.figure(figsize = (12, 8))\n\n# acf and pacf for A\nplt.subplot(421); plot_acf(sales_a, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(422); plot_pacf(sales_a, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for B\nplt.subplot(423); plot_acf(sales_b, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(424); plot_pacf(sales_b, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for C\nplt.subplot(425); plot_acf(sales_c, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(426); plot_pacf(sales_c, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for D\nplt.subplot(427); plot_acf(sales_d, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(428); plot_pacf(sales_d, lags = 50, ax = plt.gca(), color = c)\n\nplt.show()","93713c67":"# figure for subplots\nplt.figure(figsize = (12, 8))\n\n# acf and pacf for A\nplt.subplot(421); plot_acf(first_diff_a, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(422); plot_pacf(first_diff_a, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for B\nplt.subplot(423); plot_acf(first_diff_b, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(424); plot_pacf(first_diff_b, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for C\nplt.subplot(425); plot_acf(first_diff_c, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(426); plot_pacf(first_diff_c, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for D\nplt.subplot(427); plot_acf(first_diff_d, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(428); plot_pacf(first_diff_d, lags = 50, ax = plt.gca(), color = c)\n\nplt.show()","4d754d47":"arima_mod_a = sm.tsa.ARIMA(sales_a, (11,1,0)).fit(disp=False)\nprint(arima_mod_a.summary())","c650a283":"residual_plot(arima_mod_a)","26c5dd4a":"sarima_mod_a = sm.tsa.statespace.SARIMAX(sales_a, trend='n', order=(11,1,0), seasonal_order=(2,1,0,12)).fit()\nprint(sarima_mod_a.summary())","71365e3d":"residual_plot(sarima_mod_a)","2e3bdc5f":"print(sales_a.shape)\nsales_a.head()","4b03c50b":"sales_a_reindex = sales_a.reindex(index=sales_a.index[::-1])","f584a985":"sales_a_reindex","c21101ce":"mydata_a = sales_a_reindex\n#mydata_a = sales_a_reindex.loc['2013-01-02':'2015-01-21']\n#mydata_test = data.loc['2017-01-01':]","ec55060f":"print(mydata_a)","1d368988":"temp_df =pd.DataFrame(mydata_a)","79763316":"mydata_a = temp_df","4ecc232d":"sarima_mod_a_train = sm.tsa.statespace.SARIMAX(mydata_a, trend='n', order=(11,1,0), seasonal_order=(2,1,0,12)).fit()\nprint(sarima_mod_a_train.summary())","0fd8aadc":"residual_plot(sarima_mod_a_train)","0e942aa9":"plt.figure(figsize=(50,10))\nplt.plot(mydata_a, c='red')\nplt.plot(sarima_mod_a_train.fittedvalues, c='blue')\nplt.ylabel(\"Sales\")\nplt.xlabel(\"Time\")","3c0a8495":"#forecast = sarima_mod_a_train.predict(start =mydata_a.loc['2015-01-21':], dynamic= True)  \n#plt.plot(mydata_a.loc['2013-01-02':'2015-01-21'])\nplt.figure(figsize=(30,10))\nforecast = sarima_mod_a_train.predict(start = 625, end = 783, dynamic= False)  \nplt.plot(mydata_a.iloc[1:625])\nplt.plot(forecast, c = \"red\")\nforecast\n#pred_ci = forecast.conf_int()\n#pred_ci.head()\n\n#start_index = 624\n#end_index = 784\n#mydata_a['forecast'] = sarima_mod_a_train.predict(start = start_index, end= end_index, dynamic= True)  \n#mydata_a[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))","ee450683":"arima_mod_b = sm.tsa.ARIMA(sales_b, (1,1,0)).fit(disp=False)\nprint(arima_mod_b.summary())","19c6738f":"residual_plot(arima_mod_b)","c3575428":"sarima_mod_b = sm.tsa.statespace.SARIMAX(sales_b, trend='n', order=(11,1,0), seasonal_order=(2,1,0,12)).fit()\nprint(sarima_mod_b.summary())","20a758d1":"residual_plot(sarima_mod_b)","a8725db7":"print(sales_b.shape)\nsales_b.head()","569af119":"sales_b_reindex = sales_b.reindex(index=sales_b.index[::-1])","b0386e37":"#sales_b_reindex.head(100)","a52208f9":"mydata_b = sales_b_reindex","c3bdf472":"temp_df =pd.DataFrame(mydata_b)","fbc22e03":"mydata_b = temp_df","880650d0":"sarima_mod_b_train = sm.tsa.statespace.SARIMAX(mydata_b, trend='n', order=(11,1,0), seasonal_order=(2,1,0,12)).fit()\nprint(sarima_mod_b_train.summary())","5fcbe6fa":"residual_plot(sarima_mod_b_train)","800ec53f":"plt.figure(figsize=(50,10))\nplt.plot(mydata_b, c='red')\nplt.plot(sarima_mod_b_train.fittedvalues, c='blue')\nplt.ylabel(\"Sales\")\nplt.xlabel(\"Time\")","fbe52c75":"plt.figure(figsize=(30,10))\nforecast = sarima_mod_b_train.predict(start = 755, end = 941, dynamic= False)  \nplt.plot(mydata_b.iloc[1:755])\nplt.plot(forecast, c = \"red\")\nforecast","b9c44553":"arima_mod_c = sm.tsa.ARIMA(sales_c, (11,1,0)).fit(disp=False)\nprint(arima_mod_c.summary())","7e2d6eeb":"residual_plot(arima_mod_c)","61f8b6d2":"sarima_mod_c = sm.tsa.statespace.SARIMAX(sales_c, trend='n', order=(11,1,0)).fit()\nprint(sarima_mod_c.summary())","db144292":"residual_plot(sarima_mod_c)","13f6f82b":"sales_c_reindex = sales_c.reindex(index=sales_c.index[::-1])","be5bfd8b":"mydata_c = sales_c_reindex","30cf293c":"temp_df =pd.DataFrame(mydata_c)","4080b3cb":"mydata_c = temp_df","2b9d98ea":"sarima_mod_c_train = sm.tsa.statespace.SARIMAX(mydata_c, trend='n', order=(11,1,0), seasonal_order=(2,1,0,12)).fit()\nprint(sarima_mod_c_train.summary())","92c6e918":"residual_plot(sarima_mod_c_train)","227feeb6":"print(sales_c.shape)\nsales_c.head()","76a370df":"plt.figure(figsize=(50,10))\nplt.plot(mydata_c, c='red')\nplt.plot(sarima_mod_c_train.fittedvalues, c='blue')\nplt.ylabel(\"Sales\")\nplt.xlabel(\"Time\")","fb700c1b":"plt.figure(figsize=(30,10))\nforecast = sarima_mod_c_train.predict(start = 625, end = 780, dynamic= False)  \nplt.plot(mydata_c.iloc[1:625])\nplt.plot(forecast, c = \"red\")\nforecast","a3610505":"arima_mod_d = sm.tsa.ARIMA(sales_d, (11,1,0)).fit(disp=False)\nprint(arima_mod_d.summary())","21516db4":"residual_plot(arima_mod_d)","b3de1baf":"sarima_mod_d = sm.tsa.statespace.SARIMAX(sales_d, trend='n', order=(11,1,0),seasonal_order=(2,1,0,12)).fit()\nprint(sarima_mod_d.summary())","3f669b5f":"residual_plot(sarima_mod_d)","796ba0f0":"print(sales_d.shape)\nsales_d.head()","7f96df29":"sales_d_reindex = sales_d.reindex(index=sales_d.index[::-1])","7324793b":"mydata_d = sales_d_reindex","37e132ff":"temp_df =pd.DataFrame(mydata_d)","6b79feda":"mydata_d = temp_df","ecf57a06":"sarima_mod_d_train = sm.tsa.statespace.SARIMAX(mydata_d, trend='n', order=(11,1,0),seasonal_order=(2,1,0,12)).fit()\nprint(sarima_mod_d_train.summary())","16c002f2":"residual_plot(sarima_mod_d_train)","265f56eb":"# Stote type D\nplt.figure(figsize=(50,10))\nplt.plot(mydata_d, c='red')\nplt.plot(sarima_mod_d_train.fittedvalues, c='blue')\nplt.ylabel(\"Sales\")\nplt.xlabel(\"Time\")","7a5c9fda":"plt.figure(figsize=(30,10))\nforecast = sarima_mod_d_train.predict(start = 495, end = 620, dynamic= False)  \nplt.plot(mydata_d.iloc[1:495])\nplt.plot(forecast, c = \"red\")\nforecast","55f8723a":"All store types follow the same trend but at different scales depending on the presence of the (first) promotion Promo and StoreType itself (case for B).\n\n###### Already at this point, we can see that Sales escalate towards Christmas holidays. But we'll talk about seasonalities and trends later in the Time Series Analysis section.","b255d5e6":"#### Approach:\n    \nI first do the habitual data treatment and cleansing.\nIn order to understand better the patterns of the data, I will make use of libraries like matplotlib and seaborn to deep dive cases in the dataset and give better visibility on what is happening with the different types of Rossman drug stores.\nThis Exploratory analysis will help me move forward with the correlation analysis and feature engineering part of the project.","dac6e35b":"# Rossmann Store Sales Forecast\n\nRossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\nThis notebook mainly focuses on the Time Series Analysis (seasonal decomposition, trends, autocorrelation).\nWe then disscuss advantages and drawbacks of modeling with Seasonal ARIMA and Prophet.\n","f26d755b":"## Stationarize the data:\n\nWhen running a linear regression the assumption is that all of the observations are all independent of each other. In a time series, however, we know that observations are time dependent. It turns out that a lot of nice results that hold for independent random variables (law of large numbers and central limit theorem to name a couple) hold for stationary random variables. So by making the data stationary, we can actually apply regression techniques to this time dependent variable.\n\nThere are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment there is the Dickey-Fuller test.\n\nif the \u2018Test Statistic\u2019 is greater than the \u2018Critical Value\u2019 than the time series is stationary. Below is code that will help you visualize the time series and test for stationarity.","559b0a9e":"Overall sales seems to increase, however not for the StoreType C (a third from the top). Eventhough the StoreType A is the most selling store type in the dataset, it seems that it cab follow the same decresing trajectory as StoreType C did.","7155c9e0":"As mentioned before, we have a strong positive correlation between the amount of Sales and Customers of a store. We can also observe a positive correlation between the fact that the store had a running promotion (Promo equal to 1) and amount of Customers.\n\nHowever, as soon as the store continues a consecutive promotion (Promo2 equal to 1) the number of Customers and Sales seems to stay the same or even decrease, which is described by the pale negative correlation on the heatmap. The same negative correlation is observed between the presence of the promotion in the store and the day of a week.","9e8e36d0":"StoreType B has the highest average of Sales among all others, however we have much less data for it. So let's print an overall sum of Sales and Customers to see which StoreType is the most selling and crowded one:","dfdd9cf5":"So now we need to transform the data to make it more stationary. There are various transformations you can do to stationarize the data.\nThe first thing we want to do is take a first difference of the data. This should help to eliminate the overall trend from the data.","fbb87f5e":"#### ECDF: empirical cumulative distribution function\nTo get the first impression about continious variables in the data we can plot ECDF.","b022eb41":"# Loading Dataset","79e5fb6c":"We can read these plots horizontally. Each horizontal pair is for one 'StoreType', from A to D. In general, those plots are showing the correlation of the series with itself, lagged by x time units correlation of the series with itself, lagged by x time units.\n\nThere is at two things common for each pair of plots: non randomnes of the time series and high lag-1 (which will probably need a higher order of differencing d\/D).\n\n- Type A and type B: Both types show seasonalities at certain lags. For type A, it is each 12th observation with positives spikes at the 12 (s) and 24(2s) lags and so on. For type B it's a weekly trend with positives spikes at the 7(s), 14(2s), 21(3s) and 28(4s) lags.\n- Type C and type D: Plots of these two types are more complex. It seems like each observation is coorrelated to its adjacent observations.","34f20feb":"#### Dataset:\n\n<li>https:\/\/www.kaggle.com\/c\/rossmann-store-sales<\/li>","46fe0031":"1. Store: a unique Id for each store\n2. StoreType: differentiates between 4 different store models: a, b, c, d\n3. Assortment: describes an assortment level: a = basic, b = extra, c = extended\n4. CompetitionDistance: distance in meters to the nearest competitor store\n5. CompetitionOpenSince[Month\/Year]: gives the approximate year and month of the time the nearest competitor was opened\n6. Promo2: Promo2 is a continuing a promotion for some stores: 0 = store is not participating, 1 = store is participating\n7. Promo2Since[Year\/Week]: describes the year and calendar week when the store started participating in Promo2\n8. PromoInterval: describes the consecutive intervals Promo2 is started, naming the months the promotion is started.\n    E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store","d214c74e":"No, if there's no Promo2 then there's no information about it. We can replace these values by zeros. The same goes for tha variables deducted from the competition, CompetitionOpenSinceMonth and CompetitionOpenSinceYear.","2d9080b4":"Interestingly enough, there are opened store with no sales on working days. There're only 54 days in the data, so we can assume that there were external factors involved, for example manifestations.","db8e436f":"There're 172817 closed stores in the data. It is about 10% of the total amount of observations. To avoid any biased forecasts we will drop these values.\n\nWhat about opened stores with zero sales?","ccc18dfa":"#### Store types\nIn this section we will closely look at different levels of StoreType and how the main metric Sales is distributed among them.","45e65c81":"#### Conclusion\n\n#### Pros\n- Arima can catch interactions between external features, which could improve the forecasting power of a model, But in case of Facebook prophet we cant use interactions term.\n\n- Eventhough Prophet offers an automated solution for ARIMA, this methodology is under development and not completely stable.\n\n#### Cons\n- Fitting seasonal ARIMA model needs 4 to 5 whole seasons in the dataset, which can be the the biggest drawback for new companies.\n\n- Seasonal ARIMA in Python has 7 hyperparameters which can be tuned only manually affecting significantly the speed of the forecasting process.","4c58ad6f":"Want to see more of Kernels like this one? Leave an upvote then.","b9c48e09":"To complete our preliminary data analysis, we can add variables describing the period of time during which competition and promotion were opened:","e04ade21":"#### Plot the ACF and PACF charts and find the optimal parameters\n\nThe next step is to determine the tuning parameters of the model by looking at the autocorrelation and partial autocorrelation graphs. There are many rules and best practices about how to select the appropriate AR, MA, SAR, and MAR terms for the model. The chart below provides a brief guide on how to read the autocorrelation and partial autocorrelation graphs to select the proper terms. The big issue as with all models is that you don\u2019t want to overfit your model to the data by using too many terms.\n\nThe next step in ourtime series analysis is to review Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots.\n\nACF is a measure of the correlation between the timeseries with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant \u2018t1\u2019\u2026\u2019tn\u2019 with series at instant \u2018t1-5\u2019\u2026\u2019tn-5\u2019 (t1-5 and tn being end points).\n\nPACF, on the other hand, measures the correlation between the timeseries with a lagged version of itself but after eliminating the variations explained by the intervening comparisons. Eg. at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.","8defd1a8":"The most selling and crowded StoreType A doesn't appear to be the one the most exposed to competitors. Instead it's a StoreType B, which also has the longest running period of promotion.","0f0bef72":"There are several things here:\n\nIn case of no promotion, both Promo and Promo2 are equal to 0, Sales tend to peak on Sunday (!). Though we should note that StoreType C doesn't work on Sundays. So it is mainly data from StoreType A, B and D.\nOn the contrary, stores that run the promotion tend to make most of the Sales on Monday. This fact could be a good indicator for Rossmann marketing campaigns. The same trend follow the stores which have both promotion at the same time (Promo and Promo2 are equal to 1).\nPromo2 alone doesn't seem to be correlated to any significant change in the Sales amount. This can be also prooved by the blue pale area on the heatmap above.","0844f80d":"About 20% of data has zero amount of sales\/customers that we need to deal with and almost 80% of time daily amount of sales was less than 1000. So what about zero sales, is it only due to the fact that the store is closed?","0bab37d3":"# Time-Series Analysis per Store Type\n\nARIMA is one of the most classic time series forecasting models. During the modeling process, we mainly want to find 3 parameters. Auto-regression(AR) term, namly the lags of previous value; Integral(I) term for non-stationary differencing and Moving Average(MA) for error term.\nIn this section, we will analyse time series data: its trends, sesonalities and autocorrelation. Usually at the end of the analysis, we are able to develop a seasonal ARIMA (Autoregression Integrated Moving Average) model","417a59db":"What is interesting is that when the AR model is appropriately specified, the the residuals from this model can be used to directly observe the uncorrelated error. This residual can be used to further investigate alternative MA and ARMA model specifications directly by regression.","0150b438":"Retail sales for StoreType A and C tend to peak for the Christmas season and then decline after the holidays. We might have seen the same trend for StoreType D (at the bottom) but there is no information from July 2014 to January 2015 about these stores as they were closed.","6fbd5e16":"### Yearly trend\nThe next thing to check the presence of a trend in series.\nAnother tool to visualize the data is the seasonal_decompose function in statsmodel. With this, the trend and seasonality become even more obvious.","0c989ff1":"### store.csv file EDA","338962f3":"We see that stores of StoreType C are all closed on Sundays, whereas others are most of the time opened. Interestingly enough, stores of StoreType D are closed on Sundays only from October to December.\n\nBt the way what are the stores which are opened on Sundays?","0cbbdc54":"##### In this section we go through the train and store data, handle missing values and create new features for further analysis.","1cf6cf96":"On average customers spend about 9.50$ per day. Though there are days with Sales equal to zero.","9a3e311f":"# Importing Required Libraries","0146dbe0":"Apperently this information is simply missing from the data. No particular pattern observed. In this case, it makes a complete sense to replace NaN with the median values (which is twice less that the average).","f3e4f49b":"### Conclusion of EDA\n- The most selling and crowded StoreType is A.\n- The best \"Sale per Customer\" StoreType D indicates to the higher Buyer Cart. We could also assume that the stores of this types are situated in the rural areas, so that customers prefer buying more but less often.\n- Low SalePerCustomer amount for StoreType B indicates to the possible fact that people shop there essentially for small things. - Which can also indicate to the label of this store type - \"urban\" - as it's more accessible for public, and customers don't mind shopping there from time to time during a week.\n- Customers tends to buy more on Mondays when there's one promotion running (Promo) and on Sundays when there is no promotion at all (both Promo and Promo1 are equal to 0).\n- Promo2 alone doesn't seem to be correlated to any significant change in the Sales amount.","f4583c68":"### Build Model:\n\nHow to determin p, d, q\nIt's easy to determin I. In our case, we see the first order differencing make the ts stationary. I = 1.\n\nAR model might be investigated first with lag length selected from the PACF or via empirical investigation. In our case, it's clearly that within 11 lags the AR is significant. Which means, we can use AR = 11\n\nTo avoid the potential for incorrectly specifying the MA order (in the case where the MA is first tried then the MA order is being set to 0), it may often make sense to extend the lag observed from the last significant term in the PACF.\n","beb56580":"##### Missing values\n##### Closed stores and zero sales stores","c6cf6ce1":"### Seasonality\n\nWe take four stores from store types to represent their group:\n\n- Store number 2 for StoreType A\n- Store number 85 for StoreType B,\n- Store number 1 for StoreType C\n- Store number 13 for StoreType D.\n\nIt also makes sense to downsample the data from days to weeks using the resample method to see the present trends more clearly.","843b943c":"Eventhough the plots above showed StoreType B as the most selling and performant one, in reality it is not true. The highest SalePerCustomer amount is observed at the StoreType D. \n\nLow SalePerCustomer amount for StoreType B describes its Buyer Cart: there are a lot of people who shop essentially for \"small\" things (or in a little quantity). Plus we saw that overall this StoreType generated the least amount of sales and customers over the period.","1c9120da":"Clearly stores of type A. StoreType D goes on the second place in both Sales and Customers. What about date periods? Seaborn's facet grid is the best tool for this task:","00405ff9":"Continuing further with missing data. What about Promo2SinceWeek?","53e7a395":"#### A quick glimpse at the data:\n\n- Sales: the turnover for any given day (target variable).\n- Customers: the number of customers on a given day.\n- Open: an indicator for whether the store was open: 0 = closed, 1 = open.\n- Promo: indicates whether a store is running a promo on that day.\n- StateHoliday: indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays.\n- SchoolHoliday: indicates if the (Store, Date) was affected by the closure of public schools.\n    \nWe are dealing with time series data so it will probably serve us to extract dates for further analysis. We also have two likely correlated vaiables in the dataset, which can be combined into a new feature."}}