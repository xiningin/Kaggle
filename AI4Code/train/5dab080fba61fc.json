{"cell_type":{"1075e064":"code","3e7e71b1":"code","eb7345e7":"code","560fa632":"code","978122ec":"code","52de58cf":"code","5e56d14f":"code","eb46dbd7":"code","8aed5c4f":"code","62043506":"code","382179ed":"code","865269ca":"code","29098371":"code","e8797446":"code","70a60da7":"code","633752fe":"code","00f78c2c":"code","df8c1eca":"code","b6b69b1c":"code","61ab779f":"code","a080f601":"code","a9561989":"code","dbc22b76":"code","9d03c5b5":"code","06dba618":"code","e6437ae1":"code","f31d5d5f":"code","13e55600":"code","c5031830":"code","12c6d5c9":"code","765f90d9":"code","8ec9c2ad":"code","6b42fd2f":"code","b19db2e1":"code","3c5cdc53":"code","972d77cf":"code","8de5e9ea":"code","e52c2f3c":"code","fcdbe739":"markdown","914f77c3":"markdown","eae34b65":"markdown","6d0b217f":"markdown","a41def9d":"markdown","36ca7b74":"markdown","dc03df2e":"markdown","b84b753f":"markdown","97df5685":"markdown","294575fd":"markdown","77ae040a":"markdown","abf1f931":"markdown","b0c8eb30":"markdown","406272bc":"markdown","7db8cd2f":"markdown","1ac01870":"markdown","e891b5b1":"markdown","24c6a767":"markdown","71cade79":"markdown","531a9871":"markdown","b93d1f99":"markdown","3dab601f":"markdown","4838c006":"markdown","8a21142c":"markdown","6be2cc87":"markdown","bfe7c5c9":"markdown","b039738e":"markdown","3d76feca":"markdown","062a0d81":"markdown","09a17714":"markdown","8dc28489":"markdown","00f56e77":"markdown","2f6f8ff6":"markdown","d58ba9af":"markdown","c13bbed3":"markdown","7bfb043f":"markdown","6c406c6f":"markdown","92190f1f":"markdown","96edf777":"markdown","73ed0d6f":"markdown"},"source":{"1075e064":"from google.cloud import bigquery\nimport pandas as pd\n\ndef stack_user_answers(x):\n    '''Returns a data frame of Stack Overflow answers for requested user IDs\n    Args:\n    * x - (list) List of user ids as strings to be queried\n     \n    Return\n    * (pd.DataFrame) - DataFrame of Stack Overflow answers\n    '''\n\n    # create our bigquery client\n    client = bigquery.Client()\n\n    # create our SQL string query\n    query = \"\"\"\n    SELECT\n      id, owner_display_name, body, creation_date, owner_user_id, score, tags\n    FROM\n      `bigquery-public-data.stackoverflow.posts_answers`\n    WHERE \n      owner_user_id IN (\"\"\" + \",\".join(x) + \"\"\")\n    \"\"\"\n\n    # make our query using the client\n    query_job = client.query(query)\n\n    # iterate through the result\n    iterator = query_job.result()\n    rows = list(iterator)\n\n    # Transform the rows into a nice pandas dataframe\n    data = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n    \n    return(data)","3e7e71b1":"import requests\nfrom bs4 import BeautifulSoup\nurl = 'https:\/\/stackoverflow.com\/tags\/python\/topusers'\narticle_response = requests.get(url)\nsouped_page = BeautifulSoup(article_response.text, 'html.parser')\ninfo = souped_page.find_all('div',{'user-details'})\ninfo[0]\n","eb7345e7":"css_selector = \".grid--cell+ .grid--cell .user-details a\"","560fa632":"import requests\nimport lxml.html\nimport numpy as np\nfrom lxml import html\n!pip install cssselect\nfrom lxml.cssselect import CSSSelector\nimport re\n\ndef get_top_x_answerers(url, x, quiet = False):\n    '''Returns a list of the user ids \n     Args:\n     * url - (str) URL that we are querying for user details\n     * x - (int) The number of users we want to return\n     * quiet -(bool) Do we want to print the info of the top x users. Default = False\n     \n     Return\n     * (list) - List of integers for the user\n     '''\n    \n    # Get out page of html\n    article_response = requests.get(url)\n    \n    # Get the html tree from the page\n    tree = html.fromstring(article_response.content)\n    \n    # Pass in the selected CSS\n    answerers = tree.cssselect(css_selector)\n    \n    # Build a dictionary of the user names and ids\n    top_users = {}\n    for i in range(x):\n        \n        s = answerers[i].get(\"href\")\n        if not quiet:\n            print(answerers[i].get(\"href\"))\n        \n        # use a regex string to grab the name of the user\n        user = re.findall(\"\/[\\D]+\/[\\d]+\/([\\D|\\d]+)\",s)[0]\n        \n        # another regex for their id\n        user_id = (re.findall(\"[\\d]+\",s)[0])\n        \n        # fill in our dictionary with the key being their name and the value as the user_id\n        top_users[user] = user_id\n    \n    # let's just return the user ids, i.e. the values in our dict that we built above\n    return(top_users.values())","978122ec":"# Let's grab the top 10 python users\npython_url = 'https:\/\/stackoverflow.com\/tags\/python\/topusers'\npython = get_top_x_answerers(url = python_url, x = 10, quiet = False)\n\nr_url = 'https:\/\/stackoverflow.com\/tags\/r\/topusers'\nr = get_top_x_answerers(url = r_url, x = 10, quiet = True)","52de58cf":"# get the answers from our top users\ntry:\n    r_df = stack_user_answers(r)\nexcept:\n    r_df = pd.read_csv(\"..\/input\/stack-answers-r-python\/r_10.csv\")\n    \ntry:    \n    python_df = stack_user_answers(python)\nexcept:\n    python_df = pd.read_csv(\"..\/input\/stack-answers-r-python\/python_10.csv\")","5e56d14f":"# mark the answers as either r or python and bind the data frames together\nr_df['language'] = \"r\"\npython_df['language'] = \"python\"\n\ndata = r_df.append(python_df)\ndata.head(5)","eb46dbd7":"def accepted_answer(id):\n    '''Returns a boolean value (0 or 1) for if the answer ID was accepted\n    Args:\n    * id - (Numeric) Answer ID\n     \n    Return\n    * 1 or 0\n    '''\n    # convert our answer ID to a string\n    id = str(id)\n    \n    # create the url of the answer\n    url = \"https:\/\/stackoverflow.com\/a\/\" + id\n    \n    # grab the url html page\n    article_response = requests.get(url)\n    \n    # turn the content of the html page into a string\n    tree = html.fromstring(article_response.content)\n    \n    # search for the answer in the page\n    div = tree.get_element_by_id(\"answer-\" + id)\n    \n    # for the answer check the itemprop class for whether it was accepted\n    if div.get(\"itemprop\") == 'acceptedAnswer':\n        return 1\n    else:\n        return 0\n        ","8aed5c4f":"import json\n\ndef accepted_api(ids):\n    '''Returns a list of Booleans for whether the answer was accepted\n    Args:\n    * ids - (Pandas series) Series of numeric answer ids\n     \n    Return\n    * List of Booleans\n    '''\n    \n    # Build our API request URL\n    base_url = \"http:\/\/api.stackexchange.com\/answers\/\"\n    end_url = \"?order=desc&sort=activity&site=stackoverflow\"\n    \n    # Concatentate our numeric answer IDs as a string \n    ids = \";\".join(ids.map(str))\n    url_req = '{}{}{}'.format(base_url,ids,end_url)\n    resp = requests.get(url_req).json()\n    return([c['is_accepted'] for c in resp['items']])\n","62043506":"import itertools\n\ndef chunks(seq, size):\n    '''Returns a generator object with each element representing the next size x elements from seq\n    Args:\n    * seq - (List) List of elements to break up into chunks of size = size\n    * size - (Numeric) Numeric for length of each chunk\n     \n    Return\n    * (Pandas series) Series of numeric answer ids\n    '''\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","382179ed":"try:\n    accepteds = [accepted_api(group) for group in chunks(data['id'], 100)]\n    data['accepted'] = pd.Series(list(itertools.chain.from_iterable(accepteds)))\nexcept:\n    accepteds = pd.read_csv(\"..\/input\/stack-answers-r-python\/accepted.csv\")\n    data = pd.merge(data, accepteds, on='id')\n    \ndata.head(5)","865269ca":"data['body'][0]","29098371":"bs = BeautifulSoup(data['body'][0], 'html.parser')\nprint(bs.text)","e8797446":"from html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef just_text(x):\n    return(re.sub(\"\\n<pre><code>.*?<\/code><\/pre>|<\/p>|<p>|<code>.*?<\/code>|\\n\",\" \", x,flags=re.DOTALL))\n    \ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()\n\nprint(data['body'][0:1].apply(just_text).apply(strip_tags)[0])","70a60da7":"def clean_soup(x):\n    '''Clean a block of text using BeautifulSoup\n    Args:\n    * x - (str) Text block to be cleaned\n     \n    Return\n    * Cleaned str\n    '''\n    bs = BeautifulSoup(x, 'html.parser')\n    return(bs.text)\n\ndata['text'] = data['body'].apply(clean_soup)","633752fe":"from textblob import TextBlob\nimport nltk.sentiment.vader\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nanalyzer = SentimentIntensityAnalyzer()\n\ndemo = \"This is a terrific meetup :)\"\nblob = TextBlob(demo)\nprint(blob.sentiment)\n\nscores = analyzer.polarity_scores(demo)\nprint(scores)","00f78c2c":"from textblob import TextBlob\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nanalyzer = SentimentIntensityAnalyzer()\n\ndef nlp_pol_subj(text):\n    '''Carries out sentiment analysis on text\n    Args:\n    * x - (str) Text block to be analysed\n     \n    Return\n    * Tuple of length 3 containing the polarity, subjectivity and compound score\n    '''\n    \n    blob = TextBlob(text)\n    scores = analyzer.polarity_scores(text)\n    return blob.sentiment.polarity, blob.sentiment.subjectivity, scores['compound']","df8c1eca":"## This is the code we would use to run this but it will be too slow for a meetup, so we will have this commented out for the moment\nnlp_df = data['text'].map(nlp_pol_subj).apply(pd.Series, index = [\"Polarity\", \"Subjectivity\",\"Compound\"])\ndata = pd.concat([data, nlp_df], axis = 1)","b6b69b1c":"data.to_pickle(\"data.pkl\")","61ab779f":"import pandas as pd\ndata = pd.read_pickle(\"..\/input\/stack-overflow-output\/data.pkl\")\ndata.creation_date = [pd.to_datetime(p) for p in data.creation_date.values]","a080f601":"data.head(5)","a9561989":"from matplotlib import dates\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import preprocessing\n\n# convert the user id into a string for grouping\ndata['person'] = data['owner_user_id'].apply(str)\n\n# conver the date into a numeric\ndata['datenum'] = dates.date2num(data['creation_date'])\ndata['year'] = [y.year for y in data['creation_date']]\ndata['hour'] = [y.hour for y in data['creation_date']]\ndata['minute'] = [y.minute for y in data['creation_date']]\n\n# also let's scale the datenum and the score\ndata['datenum_scaled'] = preprocessing.scale(data['datenum'])\ndata['score_scaled'] = preprocessing.scale(data['score'])\n\n# lastly let's give each person an id from 0 - 9 for the 10 python and 10 r answerers\nclassnames, indices = np.unique(data['person'], return_inverse=True)\ndata['uid_by_language'] = indices\nr_values = data.loc[data['language']==\"r\",\"uid_by_language\"].unique()\npython_values = data.loc[data['language']==\"python\",\"uid_by_language\"].unique()\n\nnew_r = [r_values.tolist().index(x) if x in r_values else None for x in data.loc[data['language'] == \"r\", 'uid_by_language'].values ] \ndata.loc[data['language']==\"r\",\"uid_by_language\"] = new_r\n\nnew_python = [python_values.tolist().index(x) if x in python_values else None for x in data.loc[data['language'] == \"python\" , 'uid_by_language'].values ] \ndata.loc[data['language']==\"python\",\"uid_by_language\"] = new_python","dbc22b76":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib import dates\nimport seaborn as sns\nfrom sklearn import preprocessing\n\npl = sns.lmplot(x=\"datenum_scaled\", y=\"Compound\", data=data, col = \"language\", height=4, sharex=False)","9d03c5b5":"sns.lmplot(x=\"datenum_scaled\", y=\"Compound\", data=data, col = \"language\", fit_reg=True, x_estimator=np.mean, x_bins=20, sharex=False)","06dba618":"#!conda install -c districtdatalabs yellowbrick\nimport yellowbrick\nfrom sklearn.linear_model import Ridge\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn.linear_model import LinearRegression\n\n# construct our linear regression model\nmodel = LinearRegression(fit_intercept=True)\nx = data.datenum_scaled\ny = data.Compound\n\n# fit our model to the data\nmodel.fit(x[:, np.newaxis], y)\n\n\n# Instantiate the linear model and visualizer\nvisualizer = ResidualsPlot(model = model)\n\nvisualizer.fit(x[:, np.newaxis], y)  # Fit the training data to the model\nvisualizer.poof()                    # Draw\/show\/poof the data","e6437ae1":"violins = sns.catplot(x=\"person\", y=\"Compound\", data=data, col='language', kind = \"violin\", width=6)\nviolins.set_xticklabels(rotation=90)","f31d5d5f":"sns.lmplot(x=\"datenum_scaled\", y=\"Compound\", data=data, \n           row = \"uid_by_language\", col = \"language\", height=4, \n           fit_reg=True, x_estimator=np.mean, x_bins=20, sharex=False)","13e55600":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# construct our mixed effect model, with a random slope and intercept for each person\nmd = smf.mixedlm(\"Compound ~ 0 + datenum_scaled + language\", data, groups=data[\"person\"], re_formula=\"~datenum_scaled\")\nmdf = md.fit()\nprint(mdf.summary())","c5031830":"# construct our mixed effect model, with a random slope and intercept for each person\nmd = smf.mixedlm(\"Compound ~ 0 + datenum_scaled*language\", data, groups=data[\"person\"], re_formula=\"~datenum_scaled\")\nmdf = md.fit()\nprint(mdf.summary())","12c6d5c9":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\nlanguages = data.language.unique()\ndata = pd.concat([data,pd.get_dummies(data.language)],axis=1)\n\ntraining_colummns = [\"Subjectivity\", \"Compound\", \"r\", \"python\", \"hour\"]\nX = data[training_colummns]\ny = data['accepted']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","765f90d9":"clf = LogisticRegression(random_state=0, solver='lbfgs',\n                         multi_class='multinomial')\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)","8ec9c2ad":"print('f1 score {}'.format(f1_score(y_test, y_pred_log_reg, average='weighted')))\nprint('recall score {}'.format(recall_score(y_test, y_pred_log_reg, average='weighted')))\nprint('precision score {}'.format(precision_score(y_test, y_pred_log_reg, average='weighted')))","6b42fd2f":"{key:value for key, value in zip(sorted(data['language'].unique()), f1_score(y_test, y_pred_log_reg, average=None))}","b19db2e1":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)","3c5cdc53":"y_pred_rf = rf.predict(X_test)","972d77cf":"print('f1 score {}'.format(f1_score(y_test, y_pred_rf, average='weighted')))\nprint('recall score {}'.format(recall_score(y_test, y_pred_rf, average='weighted')))\nprint('precision score {}'.format(precision_score(y_test, y_pred_rf, average='weighted')))\n","8de5e9ea":"{key:value for key, value in zip(sorted(data['language'].unique()), f1_score(y_test, y_pred_rf, average=None))}","e52c2f3c":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X.columns, rf.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)","fcdbe739":"Now we have a function to return information about answers from Stack Overflow for given user IDs. But which user's should we get data for?\n\n---\n\nYou can have a look for the user id's of anyone you would like to, but for the purposes of this meetup let's take the top 10 user who answer questions tagged with either the `r` or `python` tag. These individuals are likely to have the most amount of data on (i.e. answers) over the longest period of time so these seem like a good starting point. \n\nTo collect these individuals we will need to write a little function to scrape these user's IDs from the Stack Overflow website. \n\nWe have looked at scraping information from web pages in a previous meetup - [Web Scraping and NLP](https:\/\/github.com\/central-ldn-data-sci\/web-scraping-and-nlp-CLDSPN). In that meetup we were selecting elements from a page of html based on their **tags** and **class** elements using the package `BeautifulSoup`. For a refresher of that approach have a look back at that meetup. \n\nToday we will be scraping information that cannot be easily scraped using attributes because the same attributes are used for other information on the page. For example if we head to https:\/\/stackoverflow.com\/tags\/python\/topusers:\n\n![](https:\/\/raw.githubusercontent.com\/central-ldn-data-sci\/stack_overflow\/master\/images\/Screenshot%20from%202019-05-27%2016-55-29.png)","914f77c3":"Just like last time we will start with a logistic regression model to train to our data for predicting whether an answer was accepted.","eae34b65":"# 4. Tidy Our Dataset\n\nWe are very close to finishing creating our dataset. At the moment we have lots of great information and we will soon be wanting to have a go at some sentiment analysis on the answers given. However, if we have a look at the answers given we can see there is a little of code included in user's answers:","6d0b217f":"# 7. Accepted Answers or Not\n\nOne last piece of analysis left to do! \n\nSo far we have had a look at whether people were getting more angry or not over time. And they are not - hurray. But can we still suggest to people that it is in their best interest to be nicer, i.e. do the nicer answers become accepted more? Or is it just down to how subjective they are. For this we will have a go at building a few models to test how well we can predict if an answer is accepted or not based on their sentiment (and other factors if you have time).\n\n","a41def9d":"TextBlob returns a value pair representing the sentiment scores for the text.\n\nHere is a section of the TextBlob documentation that explains what the values in the value pair mean:\n\nFrom https:\/\/textblob.readthedocs.io\/en\/dev\/api_reference.html#textblob.blob.TextBlob.sentiment\n\n> `TextBlob.sentiment`\n\n> Return a tuple(value pair) of form (polarity, subjectivity ) where polarity is a float(number) within the range [-1.0, 1.0] and subjectivity is a float(number) within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n\nSo we can see that it thinks the sentence is slightly positive but completely subjective.\n\n---\n\nHowever, if we have a look at the polarity when using the VADER sentiment analysis, we can see that it believes the sentence is more strongly positive (likely because of the smiley face!). The [documentation](https:\/\/github.com\/cjhutto\/vaderSentiment#about-the-scoring) can explain further what the values returned mean:\n\n* The pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence.\n\n* The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n\n---\n\nSo let's return the polarity, subjectivity and compound score and add it to our dataset.\n","36ca7b74":"As we can see this will get us all the user's. It may be easier in this case to scrape using CSS selectors. \n\n---\n\nTo scrape using CSS selectors there is a fantastic tool called [SelectorGadget](https:\/\/selectorgadget.com\/) that helps us create a CSS selector that can get us specific information from web page. It can easily be installed as a Chrome extension as well or added as a Bookmark Bar javascript function. \n\nBefore going forward I recommend having a look at the [SelectorGadget](https:\/\/selectorgadget.com\/) web page and seeing how to use it to create a selector that will grab all the Top Users from the python webpage. \n\n---\n\nOnce you have done that hopefully you will be able to identify the following CSS selector:\n\n","dc03df2e":"# 6. Linear Modelling\n\nNow that we have our dataset, let's do some plotting. Let's first scale our data so that it is on similar scales and plot the trends over time for each user:","b84b753f":"Okay so that's the least useful plot! There is so much data that it's very difficult to view but it does look like the residuals are mostly equal across the output range. \n\nNext we should check for differences in the data due to structuring and repeat observations, i.e. due to multiple points coming from the same individuals.","97df5685":"Now let's use this function to work out whether the answers were accepted\n\n(N.B. Again due to throttling I have saved the output as a csv to be read in when we inevitably have throttling issues - If you want to run this yourselves you will need to register with Stack Overflow API to get a secret key to increase your request limit - more details [here](https:\/\/api.stackexchange.com\/docs))","294575fd":"Hmmm... that's quite noisy! Let's bin the answers within time periods and plot it again:","77ae040a":"Okay - maybe there are some people who are becoming more\/less positive over time :( \n\nBUT - are people in general becoming more or less positive over time. To do this we want to try and use all the data while acknowledging that the data in not independent. This is where the mixed effect model comes in, and we can use it to see the impact of time on the Compound score by treating the individual people as random effects, with each individual having a random slope and intercept with respect to time:","abf1f931":"# 3. Build our data set\n\nNow that we have functions to start creating our dataset, let's use them! After we have finished we should end up with a dataset that looks like the following:\n\n![](https:\/\/raw.githubusercontent.com\/central-ldn-data-sci\/stack_overflow\/master\/images\/Screenshot%20from%202019-05-27%2017-43-06.png)\n\nFirstly, let's get the IDs of the top 10 users for both R and Python questions:","b0c8eb30":"# Welcome to this month's Central London Data Science Meetup! \n\nLet's be honest, we have all seen a comment on Stack Overflow that has seemed rude. If you're not convinced just search \"stack overflow rude\".\n\nBut why are they? Stack overflow is an amazingly useful tool and the fact that random members of the internet will take large amounts of time to answer 1000s of questions with in depth answers is still amazing. However, as always with the internet, a small minority of people can ruin it for everyone else!\n\nBut why is this the case? Are there any patterns as to why? For example, are people more rude at certain times of the day? Could these patterns be then used to help advise people about how to post better questions and answers? Does this have any impact on how well received the answer is? Are nicer answers more often accepted?\n\nThese may seem like ambitious aims but in this meetup we will look at a few tools that can help you begin to tackle this question and bring together a range of useful generalist skills that should help any data scientist. \n","406272bc":"Okay we can now start to see the data a bit more clearly. Looking at that we might start to think that there are some patterns between the positivity (as determined by the Compound score) and time. For example, we might think that r users are getting less positive over time :( \n\nHowever, it's very hard to test this just by looking at these plots. For example, we know this data came from multiple users and as a result the data are likely not suitable for modelling with a simple linear relationship. We looked at this idea in a previous meetup on [**Mixed Effects Models**](https:\/\/www.kaggle.com\/ojwatson\/mixed-models). Make sure to have a read through that meetup for a refresher first.\n\n---\n\nNow you are all familiar with the assumptions necessary for using a linear model. One of these is that the residuals are homoscedastic, which means that the residuals are normally distributed in relation to the predicted value, i.e. are our predictions equally bad (or good) across our predicted values. We can look at this by plotting the residuals, using the package yellowbrick:","7db8cd2f":"\n## N.B. If you are less interested in how this data was generated (i.e the above) then you can start here by loading in the dataset","1ac01870":"How about that - We can see that actually the trends are not significantly changing over time. This can be seen in the parameter table above for the `datenum_scaled` parameter, which although the estimated parameter is negative (`-0.004`), the 95% confidence interval includes 0 (`-0.022  0.013`), so we can not say that this is changing over time as the real value could in fact be 0. \n\nWe can however say that the intercept for both languages is statistically positive. We could also change our setup to look at how each language is changing over time.","e891b5b1":"The actual written code and the html tags (`<pre>`, `<p>` etc) are unlikely to change the results of the sentiment analysis very much (unless they have given their variable names etc very aggressive or friendly names). However, they will slow down how quickly we can calculate the sentiment etc of each answer. So it would be a good idea to tidy these responses up to just store the actual English response given. There are a couple of tools that we can use for this. We can either use `BeautifulSoup` to parse the response and then grab the text:","24c6a767":"We can see that the BeautifulSoup text clean is a bit neater so let's turn that into a function to apply to our `body` data. ","71cade79":"# 9. Take Home Challenges\n\nWe explored a lot of things here, but we really only touched the surface of a lot of topics. So have a go at answering these open ended questions\/suggestions of what analysis could be done next. \n\n### 1. Stack Overflow User Reputation and Badges\n\nWay back at the top of the meetup, we scraped the user IDs of the top users for the `r` and `python` tags from Stack Overflow. However, we could have grabbed more information! Go back and write a new scraping function to grab their reputation and the number of badges they have! Maybe these will be useful features in further analysis. \n\n### 2. Don't scrape - use BigQeury\n\nOR... you could use knowledge of the user IDs to query BigQuery for this information. Or use the Stack Overflow API. Have a go at getting this data this way.\n\n### 3. Top 10 revisited\n\nSo far we mostly used the sentiment analysis as features for looking at trends overall for the top 10 users. But, we could have changed the approach and modelled the user as a fixed effect to statistically test for whether each user has become more\/less positive over time. Look back at some of the earlier material in the [Mixed Effects Meetup](https:\/\/www.kaggle.com\/ojwatson\/mixed-models) and build a linear model with the user treated as a fixed effect. Are there any user's who statistically get more or less positive over time?\n\n### 4. More features plz.\n\nSentiment analysis was great at reviewing the answers given by the user's as a whole. However, we could have taken it one step further and assessed the text for specific words associated with accepted answers. Does it just take a few nice words to make your answer more likely to be accepted? Have a look back at Zack's [NLP Meetup Notebook](https:\/\/github.com\/central-ldn-data-sci\/web-scraping-and-nlp-CLDSPN\/blob\/master\/%5BCOMPLETED%5D%20Web%20Scraping%20and%20NLP.ipynb) to see how we can get the sentiment for each word and then use that to predict which words are more often associated with accepted answers.\n\n### 5. Scores on the doors\n\nLastly, our original extracte dataset from BigQuery also included a score column. This is the actual score of the answer as voted by user's for its usefulness etc. This surely must be linked to whether an answer is accepted. Have a go at bringing this into the random forest and see how this helps. Is there a threshold though for answers to usually be accepted?\n\n### 6. Does being positve actually make an accepted answer\n\nAt the end of section 7 above, we wrote:\n\n> Above we have demonstrated that the Compound score is an important feature in the trained random forest for predicting whether an aswer is accepted or not. We don't know what the branches in the fitted random forest are doing. For example, the fitted random forest may allocate increased probability of an answer being excepted if the compound score is very negative. All we show with the feature importance scores is whether they are useful features for the prediction. If you wanna go a bit further then have a look in the extension material.  \n\nHow could we acknowledge this fact and work out if positve answers are accepted more often. We could, for example, use the compound score to annotate our data set more and assign each answer as a positive, neutral or negative answer with one host encoding. We could then maybe compare the accepted proportions between positive and negative answers. And to acknowledge differences between users we could still use a mixed-effect framework for this. For some inspiration of this have a look at this great guide: https:\/\/lindeloev.github.io\/tests-as-linear\/","531a9871":"Or we could use provided html parsers provided within `html.parser` and wrap this with some regex to get just the text. The following goes one step further than `BeautifulSoup` and will actually remove the coded contents as well as the html tags:","b93d1f99":"Now let's apply this function in chunks of 100 at a time to respect the API return sizes and then append the result to our data frame.\n\nFirst, we need to write a function that turns our `data['id']` column into chunks of 100.","3dab601f":"Now using those ids, let's create a dataframe of the answers from BigQuery:\n\n(N.B. This code can error at times due to 403 Server Errors - so I have wrapped it in try statements that will read the data in if they fail from the same data saved to file","4838c006":"# 8. Wrap Up\n\nWell done for getting this far! This was a really long meetup with a lot of material brought in (especially at the end). But if you have managed to work your way to here you will have picked up a huge array of skills, in particular how you can go from a research question to an actual dataset and some preliminary analysis. Reward yourselves with pizza!!!\n\n![](https:\/\/i.imgflip.com\/321u7a.jpg)\n\nThis was my last meetup topic for a while as I need to write up my PhD. :( Hopefully, this last topic covered lots of areas and refreshed some of our greatest hits. Don't worry if you did not have time to go through everything...\n\n... or if you did and fancy there is some more material for those interested.","8a21142c":"# 1. Getting data from BigQuery\n\nFirst we need to get our data for our investigation. [BigQuery](https:\/\/cloud.google.com\/bigquery\/) by Google is a data warehouse that can be very useful for larger projects. They also maintain a number of public database collections that make for excellent data sets to test your data science skills on. \n\nIn today's problem we are going to be looking at the Stack Overflow database. BigQuery has very usefully maintained a database of questions and answers and other metadata from Stack Overflow. A description of this database can be found [here](https:\/\/cloud.google.com\/blog\/products\/gcp\/google-bigquery-public-datasets-now-include-stack-overflow-q-a) and is very worth a read through as it describes the data fully. \n\n---\n\nThere are a few ways that we can do about downloading data from BigQuery. \n\n#### 1. Go to BigQuery directly and create our SQL request and download the dataset manually\n\nIf you head to [BigQuery](https:\/\/cloud.google.com\/bigquery\/) and register for a Google Cloud account you can query the databases directly from their Google Cloud Platform. This approach can be very useful if you don't know what data it is you want and you want to have an interactive dashboard from which you can view the structure of each database hosted within BigQuery. This is where I went first to work out which tables within the Stack Overflow BigQuery database we needed for this meetup. \n\nFor example, below you can see an overview of what this interface looks like:\n\n![](https:\/\/raw.githubusercontent.com\/central-ldn-data-sci\/stack_overflow\/master\/images\/Screenshot%20from%202019-05-26%2016-20-44.png)\n\n#### 2. Query BigQuery directly using python\n\nThe second option is to use the `BigQuery` namespace provided by the `google.cloud` package. This is what we will demonstrate in this meetup. \n\n# 1. Getting data from BigQuery\n\nFirst we need to get our data for our investigation. [BigQuery](https:\/\/cloud.google.com\/bigquery\/) by Google is a data warehouse that can be very useful for larger projects. They also maintain a number of public database collections that make for excellent data sets to test your data science skills on. \n\nIn today's problem we are going to be looking at the Stack Overflow database. BigQuery has very usefully maintained a database of questions and answers and other metadata from Stack Overflow. A description of this database can be found [here](https:\/\/cloud.google.com\/blog\/products\/gcp\/google-bigquery-public-datasets-now-include-stack-overflow-q-a) and is very worth a read through as it describes the data fully. \n\n---\n\nThere are a few ways that we can do about downloading data from BigQuery. \n\n#### 1. Go to BigQuery directly and create our SQL request and download the dataset manually\n\nIf you head to [BigQuery](https:\/\/cloud.google.com\/bigquery\/) and register for a Google Cloud account you can query the databases directly from their Google Cloud Platform. This approach can be very useful if you don't know what data it is you want and you want to have an interactive dashboard from which you can view the structure of each database hosted within BigQuery. This is where I went first to work out which tables within the Stack Overflow BigQuery database we needed for this meetup. \n\nFor example, below you can see an overview of what this interface looks like:\n\n![](https:\/\/raw.githubusercontent.com\/central-ldn-data-sci\/stack_overflow\/master\/images\/Screenshot%20from%202019-05-26%2016-20-44.png)\n\n#### 2. Query BigQuery directly using python\n\nThe second option is to use the `BigQuery` namespace provided by the `google.cloud` package. This is what we will demonstrate in this meetup. \n\nFor some guidance on the next section of code have a look at [this link](https:\/\/www.kaggle.com\/product-feedback\/48573).\n\n---\n","6be2cc87":"Okay, so that is a bit better across both coding languages. But what was important in determining whether the answer was accepted? I.e. what was the model importance associated with each predictor. \n\nTo do that we need to look back in our fitted model for the `feature_importances` and then we can plot these to see if the positivity had any impact. ","bfe7c5c9":"For example, we are trying to get the user id for the users on the right, i.e. the All Time top answerers. If we were to inspect the page for their attributes we could try and scrape the information this way:","b039738e":"Ew - seems like this was not a very good model. In particular it performed terribly for the python answers.\n\nNext we'll try the random forest classifier. We had a look at what random forests are and more generally about what classification and regression trees are in one of [our meetups](https:\/\/github.com\/central-ldn-data-sci\/pushingTrees\/blob\/master\/theory.ipynb). In that meetup we actually visualised what the tree actually is and then had a go at building one from scratch. But for now, let's stick with sklearn :)","3d76feca":"# 2. Finding users from Stack Overflow","062a0d81":"---\n\n\nMaking our dataset is taking a long time and has had to run a lot of intensive calculations so it would be a good idea to save this data so that we can quickly load it again in the future.","09a17714":"However, we now have our wonderful dataset ready to go. Have a break and some pizza as that was a lot of work. But here is our dataset with sentiment analyses added in:","8dc28489":"Again no significant changes over time for either language!","00f56e77":"This is a great start for our dataset, but we could add more information. For example, it would be excellent to know if these answers were the accpeted answer or not.\n\nUnfortunately this information (I don't think) is stored in the BigQuery database. So what do we do:\n\n#### 1. Assume that Stack Overflow does not have an API and scrape this information\n\nIt's never a good idea to scrape a website for information before checking whether there is an API or not. However, let's pretend we have done this and think about how we can scrape the required information from the Stack Overflow website. ","2f6f8ff6":"# 5. Sentiment Analysis\n\nNow that we have our dataset we can use the cleaned text of each answer given, and calculate what the sentiment of the answer given is. \n\nFor a reminder of sentiment analysis you can look back at one of our [earlier meetups](https:\/\/github.com\/central-ldn-data-sci\/web-scraping-and-nlp-CLDSPN\/blob\/master\/%5BCOMPLETED%5D%20Web%20Scraping%20and%20NLP.ipynb) on NLP looking at assessing the subjectivity and polarity of articles on Tech Crunch. In that meetup we used `TextBlob` for our sentiment analysis. We will be doing the same here but also bringing in an alternative NLP tool from `nltk` that is specifically tailored for sentiments expressed in social media - VADER. For more info see their [github](https:\/\/github.com\/cjhutto\/vaderSentiment). \n\nFor example, to estimate the sentiment for the following sentence: \"This is a terrific meetup :)\"\n\n---","d58ba9af":"Now that our model has been fit we can look at the predictions and see how well our model has performed. We had a [previous meetup](https:\/\/www.kaggle.com\/blairyoung\/diagnosing-with-evaluation-metics) that looked at how you can diagnose your machine learning model. If you need a refresher then have a head over to https:\/\/www.kaggle.com\/blairyoung\/diagnosing-with-evaluation-metics and have a go at that notebook.","c13bbed3":"Lastly, let's tag these answers as either `r` or `python` before joining the two data frames together.","7bfb043f":"Let's now construct a function to grab the user idss with this CSS. ","6c406c6f":"However, you will quickly become throttled by Stack Overflow. \n\n![](https:\/\/raw.githubusercontent.com\/central-ldn-data-sci\/stack_overflow\/master\/images\/Screenshot%20from%202019-05-27%2013-37-03.png)\n\n#### 2. Look for an API\n\nThere is another solution, though, which is to use their [API](https:\/\/api.stackexchange.com\/). If we look at their API we can see that if we query the **answers** endpoint (http:\/\/api.stackexchange.com\/answers\/) we can grab information about whether the answer was accepted. We also can see that we can only make API requests (without registering) at 100 at a time, so let's make a function that returns a list of whether an answer (as supplied using its id) is accepted. ","92190f1f":"Phew! We have now got a really great dataset to start analyzing. If it seemed like this took ages then don't worry as the actual analysis is often actually quicker! If we remember Blair's meetup last time he came up with this gem that is very true:\n\n> '90% of a data scientist time is spent sourcing and then cleaning the data'","96edf777":"The following code would be used to add the sentiment analysis. \n\nHowever, it takes a long time to execute so comment it out if you like and we can load in this data afterwards as it is saved as a pickled object. \n\nIt is the same dataset though so don't worry :)","73ed0d6f":"So the positivity is an important feature! :) And just about as important as the subjectivity of the answer. Interestingly, the hour of the answer being given was important. Maybe there is more information there (are people nicer in the morning?).\n\n---\n\nImportantly though - have we actually shown that if you are nicer your answer is more likely to be accepted? \n\n...\n\nNo - Above we have demonstrated that the Compound score is an important feature in the trained random forest for predicting whether an aswer is accepted or not. We don't know what the branches in the fitted random forest are doing. For example, the fitted random forest may allocate increased probability of an answer being excepted if the compound score is very negative. All we show with the feature importance scores is whether they are useful features for the prediction. If you wanna go a bit further then have a look in the extension material.  "}}