{"cell_type":{"2bc2125d":"code","2e15ea43":"code","2107fe77":"code","ca249e61":"code","24c7be67":"code","adbbbb1d":"code","30654d06":"code","8ee5aad4":"code","6ca89581":"code","8c35c848":"code","f2034a54":"code","1d1619e7":"code","0dd03ad0":"code","59dd0362":"code","dfcd3618":"code","01d3ac2a":"code","c8b15b02":"code","56398177":"code","5aab3a18":"code","088c321c":"code","adf6e0b8":"code","49ab6429":"code","6c5afe80":"code","46958311":"code","fcdbdf75":"code","04cb7e44":"code","dd9ccfbf":"code","57f0bd96":"code","69470ffc":"code","8ad8bbe1":"code","1f4d051b":"code","a51d3d7c":"code","c48f2a47":"code","94037872":"code","0c835169":"code","d1f1a073":"code","0b1ea14e":"code","8eb37f6b":"code","ab3c0e0a":"code","04a92a2e":"code","f9e16368":"markdown","4f09ad30":"markdown","34db10ee":"markdown","c33ddef0":"markdown","cb5d406d":"markdown","8e21999f":"markdown","904086e0":"markdown"},"source":{"2bc2125d":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","2e15ea43":"!pip install fastai==0.7.0 --no-deps\n!pip install torch==0.4.1 torchvision==0.2.1\n\nfrom fastai.conv_learner import *","2107fe77":"print(os.listdir(\"..\/input\/\"))\nPATH = '..\/input\/planet-understanding-the-amazon-from-space\/'","ca249e61":"TMP_PATH = \"\/tmp\/tmp\"\nMODEL_PATH = \"\/tmp\/model\"","24c7be67":"ls {PATH}","adbbbb1d":"df = pd.read_csv(PATH+'sample_submission_v2.csv')\ndf.head(5)","30654d06":"from fastai.plots import *","8ee5aad4":"def get_1st(path): return glob(f'{path}\/*.*')[0]","6ca89581":"dc_path = '..\/input\/newdogscats\/dogscats\/dogscats\/train\/'\nlist_paths = [get_1st(f\"{dc_path}cats\"), get_1st(f\"{dc_path}dogs\")]\nplots_from_files(list_paths, titles=[\"cat\", \"dog\"], maintitle=\"Single-label classification\")","8c35c848":"list_paths = [f\"{PATH}train-jpg\/train_0.jpg\", f\"{PATH}train-jpg\/train_1.jpg\"]\ntitles=[\"haze primary\", \"agriculture clear primary water\"]\nplots_from_files(list_paths, titles=titles, maintitle=\"Multi-label classification\")","f2034a54":"# planet.py\n\nfrom fastai.imports import *\nfrom fastai.transforms import *\nfrom fastai.dataset import *\nfrom sklearn.metrics import fbeta_score\nimport warnings\n\ndef f2(preds, targs, start=0.17, end=0.24, step=0.01):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return max([fbeta_score(targs, (preds>th), 2, average='samples')\n                    for th in np.arange(start,end,step)])\n\nmetrics=[f2]\nf_model = resnet34","1d1619e7":"label_csv = f'{PATH}train_v2.csv'\nn = len(list(open(label_csv)))-1\nval_idxs = get_cv_idxs(n)","0dd03ad0":"def get_data(sz):\n    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down, max_zoom=1.05)\n    return ImageClassifierData.from_csv(PATH, 'train-jpg', label_csv, tfms=tfms,\n                    suffix='.jpg', val_idxs=val_idxs, test_name='test-jpg-v2')","59dd0362":"data = get_data(256)","dfcd3618":"x,y = next(iter(data.val_dl))","01d3ac2a":"y","c8b15b02":"list(zip(data.classes, y[0]))","56398177":"plt.imshow(data.val_ds.denorm(to_np(x))[0]*1.4);","5aab3a18":"sz=64","088c321c":"data = get_data(sz)","adf6e0b8":"data = data.resize(int(sz*1.3), TMP_PATH)","49ab6429":"learn = ConvLearner.pretrained(f_model, data, metrics=metrics)","6c5afe80":"lrf=learn.lr_find()\nlearn.sched.plot()","46958311":"lr = 0.2","fcdbdf75":"learn.fit(lr, 3, cycle_len=1, cycle_mult=2)","04cb7e44":"lrs = np.array([lr\/9,lr\/3,lr])","dd9ccfbf":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)","57f0bd96":"learn.save(f'{sz}')","69470ffc":"learn.sched.plot_loss()","8ad8bbe1":"sz=128","1f4d051b":"learn.set_data(get_data(sz))\nlearn.freeze()\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)","a51d3d7c":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\nlearn.save(f'{sz}')","c48f2a47":"sz=256","94037872":"learn.set_data(get_data(sz))\nlearn.freeze()\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)","0c835169":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\nlearn.save(f'{sz}')","d1f1a073":"multi_preds, y = learn.TTA()\npreds = np.mean(multi_preds, 0)","0b1ea14e":"f2(preds,y)","8eb37f6b":"multi_preds, y = learn.TTA(is_test=True)","ab3c0e0a":"preds = np.mean(multi_preds, 0)\npreds.shape","04a92a2e":"test_fnames = [os.path.basename(f).split(\".\")[0] for f in data.test_ds.fnames]\nclasses = np.array(data.classes, dtype=str)\nres = [\" \".join(classes[np.where(pp > 0.2)]) for pp in preds] \ntest_df = pd.DataFrame(res, index=test_fnames, columns=['tags'])\ntest_df.head(5)\ntest_df.to_csv('submission.csv', index_label='image_name')\ndf = pd.read_csv('submission.csv')\ndf.head(20)","f9e16368":"## Multi-label classification","4f09ad30":"## Multi-label versus single-label classification","34db10ee":"### End","c33ddef0":"In multi-label classification each sample can belong to one or more classes. In the previous example, the first images belongs to two classes: *haze* and *primary*. The second image belongs to four classes: *agriculture*, *clear*, *primary* and  *water*.","cb5d406d":"In single-label classification each sample belongs to one class. In the previous example, each image is either a *dog* or a *cat*.","8e21999f":"## Multi-label models for Planet dataset","904086e0":"We use a different set of data augmentations for this dataset - we also allow vertical flips, since we don't expect vertical orientation of satellite images to change our classifications."}}