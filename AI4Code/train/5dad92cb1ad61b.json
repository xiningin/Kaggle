{"cell_type":{"57c20fac":"code","abd794e2":"code","fa28b627":"code","2ed44848":"code","38f88466":"code","729706b1":"code","078e1641":"code","830c8dd6":"code","9d5ae283":"code","8d2237a5":"markdown","81679bd7":"markdown","298c8ccf":"markdown","c1ff8a25":"markdown","16384555":"markdown","e8b2cb20":"markdown","8aedb568":"markdown","0a647843":"markdown","ed84fcff":"markdown","2623e500":"markdown","80b9df02":"markdown"},"source":{"57c20fac":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","abd794e2":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","fa28b627":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","2ed44848":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","38f88466":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","729706b1":"# Define the model \n\nmodel_1 = RandomForestRegressor(random_state=1)\n\n# Train the model (will take about 10 minutes to run)\nmodel_1.fit(X_train, y_train)\npreds_valid = model_1.predict(X_valid)\nprint(\"MSE from Random forest: \",mean_squared_error(y_valid, preds_valid, squared=False))","078e1641":"from xgboost import XGBRegressor\nmodel_2 = XGBRegressor(n_estimators=1000,learning_rate=0.05) # Your code here\n\n# Fit the model\nmodel_2.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False) # Your code here\n\n# Get predictions\npredictions_2 = model_2.predict(X_valid) # Your code here\n\n# Calculate MAE\nmae_2 = mean_squared_error(y_valid,predictions_2,squared=False) #, Your code here\n\n# Uncomment to print MAE\nprint(\"MSE from XGBoost:\" , mae_2)\n","830c8dd6":"from lightgbm import LGBMRegressor\n\nlgbm_parameters = {\n    'metric': 'rmse', \n    'n_jobs': -1,\n    'n_estimators': 50000,\n    'reg_alpha': 10.924491968127692,\n    'reg_lambda': 17.396730654687218,\n    'colsample_bytree': 0.21497646795452627,\n    'subsample': 0.7582562557431147,\n    'learning_rate': 0.009985133666265425,\n    'max_depth': 20,\n    'num_leaves': 65,\n    'min_child_samples': 27,\n    'max_bin': 523,\n    'cat_l2': 0.025083670064082797\n}\n\nlgbm_model = LGBMRegressor(**lgbm_parameters)\n# Fit the model\nlgbm_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False) # Your code here\n\n# Get predictions\npredictions_3 = lgbm_model.predict(X_valid) # Your code here\n\n# Calculate MAE\nmae_3 = mean_squared_error(y_valid,predictions_3,squared=False) #, Your code here\n\n# Uncomment to print MAE\nprint(\"MSE from Light GBM:\" , mae_3)\n","9d5ae283":"# Use the model to generate predictions\npredictions = lgbm_model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","8d2237a5":"Hi! In this notebook, you'll learn how to make your first submission in **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**.\n\nThis notebook shows how to implement three models - a. Random Forest b. XG Boost c. Light GBM to get a whooping score for your first submission!\n# Step 1: Import helpful libraries\n\n","81679bd7":"### As Light GBM outperforms other models, hence we will use it for prediction and submission.","298c8ccf":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","c1ff8a25":"## And there you made your first submission!!\n\n## Please upvote if you find this useful :)","16384555":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","e8b2cb20":"## c. Light GBM","8aedb568":"Next, we break off a validation set from the training data.","0a647843":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","ed84fcff":"# Step 4: Train a model\n\n## a. Random Forest","2623e500":"## b. XG Boost","80b9df02":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`."}}