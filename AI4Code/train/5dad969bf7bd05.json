{"cell_type":{"f1b590ff":"code","051f491b":"code","fd86927a":"code","275f9516":"code","cd0fb934":"code","12d47fa4":"code","7d52daf8":"code","9afe32d4":"code","8abeb9fe":"code","4e036084":"code","c0f9ba41":"code","8eefa071":"code","018c597c":"code","58080da4":"markdown","a8ad2dd3":"markdown","52adc75e":"markdown","e3de531b":"markdown","c53b0257":"markdown","0faa0080":"markdown","c735911b":"markdown","9476df9a":"markdown","37999580":"markdown","84216d37":"markdown"},"source":{"f1b590ff":"# Imports\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torchvision.io import read_image","051f491b":"train_path = '..\/input\/intel-image-classification\/seg_train' + '\/seg_train'\ntest_path = '..\/input\/intel-image-classification\/seg_test' + '\/seg_test'\npred_path = '..\/input\/intel-image-classification\/seg_pred' + '\/seg_pred'","fd86927a":"ds = datasets.ImageFolder(train_path, transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n]))\n\nloader = DataLoader(ds, batch_size=1000)\nmean_list = []\nstd_list = []\n\nfor data in loader:\n    # shape (batch_size, 3, height, width)\n    numpy_image = data[0].numpy()\n\n    # shape (3,)\n    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n    batch_std = np.std(numpy_image, axis=(0,2,3))\n\n    mean_list.append(batch_mean)\n    std_list.append(batch_std)\n\n    # shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n    mean_list = np.array(mean_list).mean(axis=0)\n    std_list = np.array(std_list).mean(axis=0)\n\ndel ds, loader","275f9516":"# Mean and Standard Deviation of dataset\nprint(\"Mean:\", mean_list, \"Std:\", std_list)","cd0fb934":"transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean_list, std=std_list)\n])\n\ntrainset = datasets.ImageFolder(train_path, transform = transform)\ntestset = datasets.ImageFolder(test_path, transform = transform)","12d47fa4":"batch_size = 128\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True) \ntest_loader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = True)\n\nclasses = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']","7d52daf8":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(repr(device))\n\n# Define AlexNet model\nclass AlexNet(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(AlexNet, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=11, stride=4, padding=2)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(4096, num_classes)\n    \n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv3(x))\n        x = self.relu(self.conv4(x))\n        x = self.relu(self.conv5(x))\n        x = self.maxpool(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), 256 * 6 * 6)\n        x = self.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout2(x)\n        return self.fc3(x)","9afe32d4":"from tqdm import tqdm\n\n# Define function for checking accuracy\ndef evaluate_model(loader, criterion, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval()  # Switch model to evaluation mode\n\n    with torch.no_grad():  # We don't need to compute gradients here\n        for x, y in tqdm(loader, ascii=\".>=\", desc=\"=> Evaluating\"):\n            x = x.to(device=device)\n            y = y.to(device=device)\n\n            scores = model(x)\n            loss = criterion(scores, y)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)  # predictions.shape[0]\n        \n        accuracy = (num_correct \/ num_samples)\n        print(f\"=> Loss: {loss.item(): .3f} - Accuracy: {accuracy.item(): .3f}\")\n\n    # Switch back to training mode\n    model.train()\n    return loss.item(), accuracy.item()\n\n# Load and save checkpoint\ndef save_checkpoint(state, filename=\"alexnet_chkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint...\")\n    torch.save(state, filename)\n\ndef load_checkpoint(model, optimizer, checkpoint):\n    print(\"=> Loading checkpoint...\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","8abeb9fe":"num_classes = len(classes)\n\n# Initialize network\nmodel = AlexNet(input_channels=3, num_classes=num_classes).to(device)\n\nlearning_rate = 0.0001\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Load weights and parameters from checkpoint if available\nload_model = False\nif load_model:\n    load_checkpoint(model, optimizer, torch.load(\"alexnet_chkpoint.pth.tar\"))","4e036084":"# Train network\nprint(\"Training started...\")\nnum_epochs = 20\ntrain_losses = []\ntrain_accs = []\ntest_losses = []\ntest_accs = []\n\nfor epoch in range(num_epochs):\n    if epoch != 0 and epoch % 5 == 0:\n        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n        save_checkpoint(checkpoint)\n    \n    num_correct = 0\n    num_samples = 0\n    \n    loop = tqdm(enumerate(train_loader), total=len(train_loader), ascii=\".>=\")\n    for batch_idx, (data, targets) in loop:\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # Forward step\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # Backward step\n        optimizer.zero_grad()  # To clear out previous step's gradients\n        loss.backward()\n\n        # Gradient descent\n        optimizer.step()\n        \n        # Calculate ratio of correct predictions\n        _, predictions = scores.max(1)\n        num_correct += (predictions == targets).sum()\n        num_samples += predictions.size(0)  # predictions.shape[0]\n        \n        # Update loss and accuracy on progress bar\n        accuracy = (num_correct \/ num_samples)\n        loop.set_description(f\"=> Epoch {epoch + 1}\/{num_epochs}\")\n        loop.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n    \n#     # Compute accuracy at the end of epoch\n#     accuracy = (num_correct \/ num_samples) * 100\n#     print(f\"=> Epoch {epoch} \/ {num_epochs}: Loss - {loss.data: .5f}   \"\n#             f\"Accuracy - {num_correct} \/ {num_samples} = {accuracy:.2f}%\")\n    \n    # Compute test loss and accuracy\n    test_loss, test_acc = evaluate_model(test_loader, criterion, model)\n    \n    train_losses.append(loss.item())\n    train_accs.append(accuracy.item())\n    \n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n","c0f9ba41":"print(\"Test Accuracy:\")\ntest_loss, test_acc = evaluate_model(test_loader, criterion, model)","8eefa071":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(range(1, num_epochs + 1), train_losses, label='train_loss')\nplt.plot(range(1, num_epochs + 1), test_losses, label='test_loss')\nplt.title('Loss Graph')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks(range(2, num_epochs + 1, 2))\nplt.legend()\nplt.show()\n\nplt.plot(range(1, num_epochs + 1), train_accs, label='train_accuracy')\nplt.plot(range(1, num_epochs + 1), test_accs, label='test_accuracy')\nplt.title('Accuracy Graph')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.xticks(range(2, num_epochs + 1, 2))\nplt.legend()\nplt.show()","018c597c":"with torch.no_grad():\n    for idx, batch in enumerate(test_loader):\n        if idx < 5:\n            # Retrieve item\n            item = batch[0]\n            image = item[0]\n            true_target = classes[batch[1][0]]\n\n            # Generate prediction\n            prediction = model(image.reshape(1, 3, 224, 224).to(device))\n\n            # Predicted class value using argmax\n            predicted_class = classes[np.argmax(prediction.cpu())]\n\n            # Reshape image\n            image = (image.permute(1, 2, 0) * std_list) + mean_list\n\n            # Show result\n            plt.imshow(image)\n            plt.title(f'Predicted: {predicted_class} - Actual: {true_target}')\n            plt.axis('off')\n            plt.show()","58080da4":"### Initializing model, loss, and optimizer","a8ad2dd3":"## Preprocessing train and test datasets","52adc75e":"### Setting up train and test dataloaders","e3de531b":"## Training the AlexNet model and plotting loss\/accuracy graphs","c53b0257":"## Plotting loss and accuracy graphs and generating some predictions","0faa0080":"### Utility functions for accuracy and checkpoints","c735911b":"### Normalizing, and resizing train and test datasets","9476df9a":"## Defining and initializing AlexNet model","37999580":"### Calculating mean and standard deviation of dataset","84216d37":"## Importing PyTorch and NumPy libraries and setting paths"}}