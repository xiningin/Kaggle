{"cell_type":{"95d27f2f":"code","69be5f7f":"code","3951f374":"code","efc1995a":"code","a062b1dd":"code","93cc489b":"code","d024c568":"code","b9e8f8d1":"code","b0e80f6d":"code","a6301bdd":"code","9be655e7":"code","9cf82fee":"code","586640e6":"code","f1f000b7":"code","81eafd37":"code","39bb4482":"code","69aa700a":"code","85d9bfc7":"code","c68518ce":"code","8a576205":"code","3bc67ad6":"code","ed70a746":"code","3c5f5559":"code","db2c66a5":"code","e17d0434":"code","be3cf973":"code","45a05a5a":"code","4ceec133":"code","c4f004fa":"code","b0e6c693":"code","07d5e39c":"code","24ee30eb":"code","b4b4d679":"code","c47c68ff":"code","5bd93c35":"code","bd008c56":"code","cda9d2ef":"code","2ae9acc2":"code","d09c0d2b":"code","5501f11d":"code","92275c5d":"code","c90afd13":"code","a3e4a5a0":"code","938e1649":"code","f88ed097":"code","36363412":"code","0b9f45aa":"code","7eb750ce":"code","209497f9":"code","40401f07":"code","b6df435a":"markdown","28fd7004":"markdown","06c8e851":"markdown","24b900cd":"markdown","37c9bcbb":"markdown","9d667e76":"markdown","231d5af2":"markdown","3f960fe1":"markdown"},"source":{"95d27f2f":"import re\nimport nltk\nimport string\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npd.set_option(\"display.max_colwidth\",200)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","69be5f7f":"train_tweets = pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntest_tweets = pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')","3951f374":"train_tweets.head()","efc1995a":"train_tweets.info()","a062b1dd":"sns.countplot(data=train_tweets, x='label', hue='label')\nplt.title('Types of comments : 0 - > Non Rasict\/Sexist , 1 - > Rasict\/Sexist')\nplt.xlabel('Tweets')\nplt.show()","93cc489b":"train_tweets[train_tweets['label']==1].head()","d024c568":"train_tweets[train_tweets['label']==0].head()","b9e8f8d1":"train_tweets['label'].value_counts()","b0e80f6d":"test_tweets.head()","a6301bdd":"train_len = train_tweets['tweet'].str.len()\ntest_len = test_tweets['tweet'].str.len()","9be655e7":"print(\"train data length :\" , train_len)\nprint(\"test data length :\" , test_len)","9cf82fee":"plt.hist(train_len, bins=20,label='train_tweets')\nplt.hist(test_len , bins=20, label='test_tweets')\nplt.legend()\nplt.show()","586640e6":"dataset = train_tweets.append(test_tweets,ignore_index=True)","f1f000b7":"dataset.head()","81eafd37":"dataset.shape","39bb4482":"def remove_pattern(input_text,pattern):\n    r = re.findall(pattern, input_text)\n    for i in r:\n        input_text = re.sub(i,\"\",input_text)\n    return input_text","69aa700a":"dataset['tidy_tweet'] = np.vectorize(remove_pattern)(dataset['tweet'],\"@[\\w]*\")","85d9bfc7":"dataset.head()","c68518ce":"dataset['tidy_tweet'] = dataset['tidy_tweet'].str.replace('[^a-zA-Z#]',\" \")","8a576205":"dataset.head()","3bc67ad6":"stop_words = nltk.corpus.stopwords.words('english')","ed70a746":"stop_words[:10]","3c5f5559":"def remove_stopword(input_text):\n    txt_clean = \" \".join([word for word in input_text.split() if len(word)>3])\n    return txt_clean","db2c66a5":"dataset['tidy_tweet'] = dataset['tidy_tweet'].apply(lambda x:remove_stopword(x))","e17d0434":"dataset.head()","be3cf973":"tokenized_tweet = dataset['tidy_tweet'].apply(lambda x: x.split())\ntokenized_tweet.head()","45a05a5a":"from nltk.stem import PorterStemmer","4ceec133":"pstem = PorterStemmer()","c4f004fa":"tokenized_tweet = tokenized_tweet.apply(lambda x:[pstem.stem(i) for i in x])","b0e6c693":"tokenized_tweet","07d5e39c":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = \" \".join(tokenized_tweet[i])\ndataset['tidy_tweet'] = tokenized_tweet","24ee30eb":"dataset.head()","b4b4d679":"from wordcloud import WordCloud\nall_words = ' '.join([text for text in dataset['tidy_tweet']])  \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","c47c68ff":"all_words = ' '.join([text for text in dataset['tidy_tweet'][dataset['label']==0]])  \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","5bd93c35":"all_words = ' '.join([text for text in dataset['tidy_tweet'][dataset['label']==1]])  \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","bd008c56":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer","cda9d2ef":"bow_vector = CountVectorizer(max_df=0.90,min_df=2,max_features=1000,stop_words='english')\nbow = bow_vector.fit_transform(dataset['tidy_tweet'])\nbow.shape","2ae9acc2":"bow.data","d09c0d2b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import f1_score","5501f11d":"X = bow[:31962,:]","92275c5d":"y = bow[31962:,:]","c90afd13":"x_train,x_test,y_train,y_test = train_test_split(X,train_tweets['label'],test_size=0.3)","a3e4a5a0":"lg = LogisticRegression()","938e1649":"lg.fit(x_train,y_train)","f88ed097":"pred = lg.predict_proba(x_test)","36363412":"pred","0b9f45aa":"pred_int = pred[:,1]>=0.3","7eb750ce":"pred_int = pred_int.astype(np.int)","209497f9":"pred_int","40401f07":"f1_score(y_test,pred_int)","b6df435a":"**Words in racist\/sexist tweets**","28fd7004":"# Visualization","06c8e851":"**Words in non racist\/sexist tweets**","24b900cd":"Not Rasict & Sexist Tweets","37c9bcbb":"Distribution of length of the tweets, in terms of words, in both train and test data.","9d667e76":"# Stemming","231d5af2":"Rasict & Sexist Tweets","3f960fe1":"# Text Normalization"}}