{"cell_type":{"4e09ed58":"code","1ca42451":"code","01c787eb":"code","e07dc98a":"code","b76e6690":"code","acfd2689":"code","7ce37bea":"code","3481ef51":"code","dba2e437":"code","0ae2c63a":"code","9289a75e":"code","fb1197c1":"code","2c40b2b1":"code","227b1409":"code","0c8e1d63":"code","ffbb0e53":"code","a94cdafb":"code","5b1d8773":"code","d2de3067":"code","f3d68544":"code","117c997a":"code","d893ee32":"code","47a762f9":"code","343fb502":"code","4c10792f":"code","756f3bdf":"markdown","f1fa7631":"markdown","b1b03321":"markdown","07db40a1":"markdown","9aae5236":"markdown","c340b3de":"markdown","86a8a6cf":"markdown","d079959a":"markdown","c0b55df1":"markdown","bfc3f0a7":"markdown"},"source":{"4e09ed58":"## Most Important\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pathlib import Path\nfrom PIL import Image\n\n## less Important\nfrom functools import partial\nimport os\nfrom scipy import stats\nimport missingno as msno\nimport joblib\nimport tarfile\nimport shutil\nimport urllib\nfrom skimage import io\n## Sklearn\nfrom sklearn import datasets\n## Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n## Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n## tensorflow & Keras\nimport tensorflow as tf    ## i will use tf for every thing and for keras using tf.keras\nimport keras\nfrom keras.models import Model,Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Input,LeakyReLU,AveragePooling2D\nfrom keras.layers.normalization import batch_normalization\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\n","1ca42451":"#this loads multiple images \nic_train = io.ImageCollection('\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/train\/*.png')\n\nprint('Type:', type(ic_train))\nprint(\"number of training data\",len(ic_train))","01c787eb":"ic_test = io.ImageCollection('\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/test\/*.png')\n\nprint(\"number of test data\",len(ic_test))","e07dc98a":"train_dir='\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/train\/'\ntrain_imgs=os.listdir(train_dir)","b76e6690":"##first way:opencv\n ## second way :pil\n# # creating a object (not returned as numpy array ,opens in new window)\nim = Image.open(os.path.join(train_dir,train_imgs[0])) \n# im.show()\n# convert image to numpy array\ndata = np.asarray(im)\nplt.imshow(data);","acfd2689":"## third method skimage io\nimage = io.imread(os.path.join(train_dir,train_imgs[0]))\n\nprint(type(image))\nprint(image.dtype)\nprint(image.shape)\nprint(image.min(), image.max())\n\nplt.imshow(image);","7ce37bea":"## fourth way using plt\nimage = plt.imread(os.path.join(train_dir,train_imgs[0]))\nprint(type(image))\nprint(image.dtype)\nprint(image.shape)\nprint(image.min(), image.max())\nplt.imshow(image);","3481ef51":"%matplotlib inline\n\n# Parameters for our graph; we'll output images in a 4x4 configuration\nnrows = 4\nncols = 4\n\npic_index = 0 # Index for iterating over images","dba2e437":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols*4, nrows*4)\n\npic_index+=8\n\nnext_img = [os.path.join(train_dir, fname) \n                for fname in train_imgs[ pic_index-8:pic_index] \n               ]\n\nfor i, img_path in enumerate(next_img):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = plt.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n","0ae2c63a":"train_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/train.csv')\ntrain_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/train')\n\n## read these all training images paths as Series\ntrain_images_paths = pd.Series(sorted(list(train_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntrain_images_paths.head()","9289a75e":"train_full_labels = train_labels['label'].values\ntrain_full_set = np.empty((13440, 32, 32, 3), dtype=np.float32)  #take only the first 3 channels\n\nfor idx, path in enumerate(train_images_paths):\n    img = plt.imread(path)\n    img = img[:,:,:3]\n    train_full_set[idx] = img\n    \nprint('train_full_set.shape =>', train_full_set.shape)\nprint('train_full_labels.shape =>', train_full_labels.shape)","fb1197c1":"test_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/test.csv')\ntest_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/test')\n\n## read these all training images paths as Series\ntest_images_paths = pd.Series(sorted(list(test_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntest_images_paths.head()\n\n\nprint('Number of Instances in test_set is', len(test_images_paths))\n\n","2c40b2b1":"test_full_set = np.empty((3360, 32, 32, 3), dtype=np.float32)  #take only the first 3 channels\n\nfor idx, path in enumerate(test_images_paths):\n    img = plt.imread(path)\n    img = img[:,:,:3]\n    test_full_set[idx] = img\n    \nprint('test_full_set.shape =>', test_full_set.shape)","227b1409":"def create_model():\n\n    \n    model = Sequential()\n\n    model.add(Conv2D(filters=64, kernel_size=3, input_shape=(32, 32, 3), activation='relu'))\n    model.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation='relu'))\n    model.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation='relu'))\n    model.add(MaxPooling2D(pool_size=2,strides=(1, 1)))\n    \n    \n    model.add(Conv2D(filters=128, kernel_size=3, activation='relu',padding=\"same\"))\n    model.add(Conv2D(filters=128, kernel_size=3, padding=\"valid\", activation='relu'))\n    \n    model.add(MaxPooling2D(pool_size=2,strides=(1, 1)))\n    \n    \n    model.add(Conv2D(filters=256, kernel_size=3, padding=\"same\", activation='relu'))\n    model.add(Conv2D(filters=256, kernel_size=3, padding=\"valid\", activation='relu'))\n    model.add(MaxPooling2D(pool_size=2,strides=(1, 1)))\n    model.add(AveragePooling2D())\n    \n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    model.add(Dense(29, activation='softmax'))\n    \n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model\n","0c8e1d63":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False,\n        validation_split = 0.1)  # randomly flip images","ffbb0e53":"callback_list = [\n    ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=15),\n    EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=25),\n    ModelCheckpoint(\n    filepath=\"best.hdf5\",\n    monitor='val_accuracy', \n    verbose=1, \n    save_best_only=True, \n    mode='max')\n    ]","a94cdafb":"EPOCHS = 150\nBATCH_SIZE = 128\nENSEMBLES = 10 # number of models to ensemble\nresult_list = [] # store results for correlation matrix\nhistories = [] # store histories for training and validation curves\nresults = np.zeros((test_full_set.shape[0],29))\n\n\n\nfor i in range(ENSEMBLES):\n    # split training and validation sets\n    X_train_tmp, X_val, y_train_tmp, y_val = train_test_split(train_full_set, train_full_labels, test_size=0.1, random_state=i)\n    # create model\n    model = create_model()\n    # fit the model\n    print('training No.', i)\n    history = model.fit(datagen.flow(X_train_tmp, y_train_tmp, batch_size=BATCH_SIZE),\n                   epochs=EPOCHS,\n                   callbacks=callback_list,\n                   validation_data=(X_val, y_val),\n                   )\n    # save results\n    histories.append(history)\n    result = model.predict(test_full_set)\n    results += result\n    result_list.append(result)","5b1d8773":"# check correlation of each predictions\ncorr_preds = pd.DataFrame([np.argmax(result, axis=1) for result in result_list]).T.corr()\nfig = sns.heatmap(corr_preds, annot=True, fmt='.3f', cmap='rainbow')\nfig.set_title('Predictions correlation matrix', fontsize=16, y=1.05)\nplt.show()","d2de3067":"loss_all_data, acc_all_data = model.evaluate(train_full_set, train_full_labels, verbose=0)\nprint('loss_all_data =>', loss_all_data)\nprint('acc_all_data =>', acc_all_data)","f3d68544":"fig, ax = plt.subplots(2, 1, figsize=(12,6))\n\nfor e in range(ENSEMBLES):\n    loss = histories[e].history['loss']\n    val_loss = histories[e].history['val_loss']\n    acc = histories[e].history['accuracy']\n    val_acc = histories[e].history['val_accuracy']\n    ax[0].set_title('loss')\n    ax[0].plot(loss, 'b', linewidth=1)\n    ax[0].plot(val_loss, 'r', linewidth=1)\n    ax[0].grid(color='black', linestyle='-', linewidth=0.2)\n    ax[1].set_title('accuracy')\n    ax[1].plot(acc, 'b', linewidth=1)\n    ax[1].plot(val_acc, 'r', linewidth=1)\n    ax[1].grid(color='black', linestyle='-', linewidth=0.2)\n    \nax[0].legend(['Training loss', 'Validation loss'], shadow=True)     \nax[1].legend(['Training accuracy', 'Validation accuracy'], shadow=True)\n\nplt.tight_layout()\nplt.show()","117c997a":"results = np.argmax(results, axis=1)\nresults = pd.Series(results, name='Label')","d893ee32":"test_labels['label'] = results","47a762f9":"test_labels['label'].value_counts()","343fb502":"\n test_labels   \n","4c10792f":"test_labels[['id', 'label']].to_csv('\/kaggle\/working\/submission.csv', index=False)","756f3bdf":"## First Model","f1fa7631":"# Preparing Train Data for training ","b1b03321":"# Preparing Test Data :","07db40a1":"# Results","9aae5236":"# Importing the Libraries","c340b3de":"# Visualizing images ","86a8a6cf":"# Data Augmentation :","d079959a":"## Model Building","c0b55df1":"# Exploratory Analysis","bfc3f0a7":"# Saving Results"}}