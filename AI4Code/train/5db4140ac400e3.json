{"cell_type":{"0096d11f":"code","29b6539d":"code","3b2f77da":"code","8dcd64e6":"code","c9237f3c":"code","4021a200":"code","3f8ba860":"code","760ad4b7":"code","0f3664af":"code","73077287":"code","413d7cd1":"code","0eeee44c":"markdown","1ed36961":"markdown","6c557dc6":"markdown","76829a46":"markdown","a4f83a83":"markdown","7f0f8e76":"markdown","aa03ed04":"markdown","02a9745c":"markdown","cea2e43f":"markdown","bfd4992c":"markdown"},"source":{"0096d11f":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))","29b6539d":"import glob\nimage = glob.glob('..\/input\/all-dogs\/all-dogs\/*')\nbreed = glob.glob('..\/input\/annotation\/Annotation\/*')\nannot = glob.glob('..\/input\/annotation\/Annotation\/*\/*')\nprint(len(image), len(breed), len(annot))","3b2f77da":"# Let's take a look at the content of an annotation file. I choose one with two dogs in the image, and\n# there are two bounding boxes specified. \n!cat ..\/input\/annotation\/Annotation\/n02097658-silky_terrier\/n02097658_98","8dcd64e6":"import xml.etree.ElementTree as ET","c9237f3c":"def get_bbox(annot):\n    \"\"\"\n    This extracts and returns values of bounding boxes\n    \"\"\"\n    xml = annot\n    tree = ET.parse(xml)\n    root = tree.getroot()\n    objects = root.findall('object')\n    bbox = []\n    for o in objects:\n        bndbox = o.find('bndbox')\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        bbox.append((xmin,ymin,xmax,ymax))\n    return bbox","4021a200":"# test\nbbox = get_bbox('..\/input\/annotation\/Annotation\/n02097658-silky_terrier\/n02097658_98')\nprint(bbox[0], bbox[1], len(bbox))","3f8ba860":"def get_image(annot):\n    \"\"\"\n    Retrieve the corresponding image given annotation file\n    \"\"\"\n    img_path = '..\/input\/all-dogs\/all-dogs\/'\n    file = annot.split('\/')\n    img_filename = img_path+file[-1]+'.jpg'\n    return img_filename","760ad4b7":"plt.figure(figsize=(10,10))\nfor i in range(12):\n    plt.subplot(3,4,i+1)\n    plt.axis(\"off\")\n    dog = get_image(annot[i])\n    im = Image.open(dog)\n    im = im.resize((64,64), Image.ANTIALIAS)\n    plt.imshow(im)","0f3664af":"plt.figure(figsize=(10,10))\nfor i in range(12):\n    bbox = get_bbox(annot[i])\n    dog = get_image(annot[i])\n    im = Image.open(dog)\n    for j in range(len(bbox)):\n        im = im.crop(bbox[j])\n        im = im.resize((64,64), Image.ANTIALIAS)\n    plt.subplot(3,4,i+1)\n    plt.axis(\"off\")\n    plt.imshow(im)","73077287":"test = '..\/input\/annotation\/Annotation\/n02097658-silky_terrier\/n02097658_98'\nbox = get_bbox(test)\ndog = get_image(test)\nim = Image.open(dog)\nplt.imshow(im)","413d7cd1":"plt.figure(figsize=(6,6))\nfor j in range(len(box)):\n    im = Image.open(dog)\n    im = im.crop(box[j])\n    im = im.resize((64,64), Image.ANTIALIAS)\n    plt.subplot(1,2, j+1)\n    plt.axis(\"off\")\n    plt.imshow(im)","0eeee44c":"# Load & crop images to bounding boxes","1ed36961":"#### Let's see some images","6c557dc6":"__Note:__ The number of images in the 'all-dogs' folder is 20579, which is one less than that in the 'Annotation' folder. Hmm..","76829a46":"#### Let's compare the above when cropped to bounding boxes","a4f83a83":"# Introduction","7f0f8e76":"Started on 2 July 2019\n\n**References:**\n1. https:\/\/www.kaggle.com\/whizzkid\/crop-images-using-bounding-box\n2. https:\/\/www.kaggle.com\/guillaumedesforges\/loading-the-cropped-dogs-seamlessly-with-pytorch\n3. https:\/\/www.kaggle.com\/guillaumedesforges\/usable-complete-data-loading-utility\n4. https:\/\/towardsdatascience.com\/processing-xml-in-python-elementtree-c8992941efd2","aa03ed04":"#### Function to extract bounding box values from annotation files.\nPython has a built-in library called ElementTree that has functions to read and manipulate XML files.","02a9745c":"#### The files are:\n* all-dogs.zip - All dog images contained in the [Stanford Dogs Dataset][1]\n* Annotations.zip - Class labels, bounding boxes\n\n#### The dataset is from the [Stanford Dogs Dataset][1]. It is useful to refer to the [webpage][1]. \n[1]: http:\/\/vision.stanford.edu\/aditya86\/ImageNetDogs\/","cea2e43f":"#### Let's check the image with two dogs","bfd4992c":"#### What I learned from scanning the [Stanford Dogs Dataset][1]:\n* There are 20,580 images. These are in the 'all-dogs' directory. The filename of the .jpg is the identifier.\n* There are 120 sub-folders in the 'Annotation' directory. Each sub-folder represents a dog breed, and there are ~150 to 200 annotation files in each folder and they correspond to the images in the 'all-dogs' directory via the same identifier filename.\n* The annotation files contain the dog breed labels and bounding boxes.\n* There are images with people in them. The bounding boxes in the respective annotation files define where the dogs are in the images.\n* There are also images with several dogs. For these, there will be more than one bounding box in the corresponding annotation file.\n[1]: http:\/\/vision.stanford.edu\/aditya86\/ImageNetDogs\/"}}