{"cell_type":{"850f055d":"code","ba0a68d4":"code","7ceca96a":"code","b723eb28":"code","96d1f506":"code","6e2573fd":"code","7cec2c7b":"code","74a41932":"code","0492a0c2":"code","b4168b70":"code","c90769da":"code","ebeb525a":"code","8fd5139b":"code","65839450":"code","17ffb647":"code","838471aa":"code","46692ead":"code","70417f08":"code","da42e8ef":"code","78c481f5":"code","4b815021":"code","4e7f7784":"code","1462ea89":"code","35c690e0":"code","74dbb91f":"code","9cd03e02":"markdown"},"source":{"850f055d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba0a68d4":"#importing all required libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import r2_score,mean_squared_error","7ceca96a":"#loading csv file for analysis\n\ndf = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')","b723eb28":"#checking for null values\n\ndf.isnull().sum()","96d1f506":"#since not all of our values are numeric, I used this library to encode values to perform visualizations and prediction\n\nfrom sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\n\nencode.fit(df.sex.drop_duplicates()) \ndf.sex = encode.transform(df.sex)\n\nencode.fit(df.smoker.drop_duplicates())\ndf.smoker = encode.transform(df.smoker)\n\nencode.fit(df.region.drop_duplicates())\ndf.region = encode.transform(df.region)","6e2573fd":"#updated dataset with encoded values with no null values\n\ndf","7cec2c7b":"#I used this to see how trends on each graph looks like  \n\nsns.pairplot(df)","74a41932":"#we see here that there is some correlation between insurance charges and smoking habits\n#I expected BMI and age to have more correlation with insurance charges \\_(:|)_\/ \n\ndf.corr()['charges'].sort_values()","0492a0c2":"#this heatmap shows the correlation that we found in the last step. Darker squares indicated higher correlation\n\nplt.figure(figsize = (8,6))\ncorr = df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap = 'Blues', square = True)","b4168b70":"#lets get started with some visualizations\n\nplt.figure(figsize = (8,6))\nsns.distplot(df['age'], bins = 10, color = 'red')","c90769da":"plt.figure(figsize = (8,6))\nsns.countplot(x = 'sex', palette = 'husl', data = df)","ebeb525a":"plt.figure(figsize = (8,6))\nsns.distplot(df['bmi'], bins = 10)","8fd5139b":"plt.figure(figsize = (8,6))\nsns.countplot(x = 'children', palette = 'husl', data = df)","65839450":"plt.figure(figsize=(8,6))\nsns.countplot(x = 'smoker', hue = 'sex', data = df, palette = 'husl')","17ffb647":"plt.figure(figsize=(8,6))\nsns.countplot(x = 'region', data = df, palette = 'PuBu')","838471aa":"plt.figure(figsize = (8,6))\nsns.distplot(df['charges'], bins = 10, color = 'purple')","46692ead":"x = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values","70417f08":"#to perform predictions, I divided the x and y values to training and testing values to an 80\/20 split\n\nfrom sklearn.model_selection import train_test_split \n\nxTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 0)","da42e8ef":"#here we import the linear regression model from scikit learning \n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\nreg.fit(xTrain, yTrain)","78c481f5":"yPred = reg.predict(xTest)","4b815021":"#almost 80% accuracy, pretty good\n\nprint(reg.score(xTest,yTest))","4e7f7784":"#importing decision tree regression model \n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(random_state = 0)\ntree.fit(xTrain, yTrain)\ntreeYpred = tree.predict(xTest)","1462ea89":"#65% accuracy, we're going to pass on this one \n\nprint(r2_score(yTest,treeYpred))","35c690e0":"#Random forest regression for the win\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrr = RandomForestRegressor(n_estimators = 10, random_state = 0)\nfrr.fit(xTrain, yTrain)\nfrrYpred = frr.predict(xTest)","74dbb91f":"#87% accuracy, this is it!\n\nprint(r2_score(yTest,frrYpred))","9cd03e02":"Hope you liked it!"}}