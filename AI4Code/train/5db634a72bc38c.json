{"cell_type":{"f6136332":"code","b5ef3c2e":"code","56fcb9ad":"code","7e529195":"code","b880db6d":"code","856c5894":"code","fee770e5":"code","750930c5":"code","ddd57cc9":"code","9ab65c4b":"code","1606d2bc":"code","9d577f03":"code","95e1864d":"code","ff2230e7":"code","a2ffba45":"code","b658c2d3":"code","2cb8a337":"code","88accafe":"code","c2cc210d":"code","ae410adc":"code","71cc0490":"code","134cadf4":"code","a64325e8":"code","090b403e":"code","29bb504b":"code","ba35887a":"code","30aa94fc":"code","286f99ea":"code","f7d8676f":"code","d0b7adcf":"code","665f1111":"code","c6a03657":"code","154923d1":"code","8b9bbfcc":"code","251ac43a":"code","8a7258e4":"code","a78a0c6c":"code","966645be":"code","8bc33744":"code","8f784a27":"code","3080c486":"code","ef9e48fc":"code","a5c5cbd7":"code","4e8aedb8":"code","0f09ef58":"code","ce38d283":"code","6a045ff5":"code","9964e05d":"code","e4bf05f6":"code","34fe4dd7":"code","ce1e7b6d":"code","3075d870":"code","94e89da1":"code","6602eadc":"markdown","e73cd918":"markdown","30a96bb4":"markdown","50bdae13":"markdown","4dfd8094":"markdown","fe3e9853":"markdown","259ce1b7":"markdown","a00e6f47":"markdown","a3c55def":"markdown","4a7827e6":"markdown","b1a91d27":"markdown","2a0a73c7":"markdown","ed4b7dd2":"markdown","dfec69a9":"markdown","d8b8b40b":"markdown","6c338a20":"markdown","af839502":"markdown","0f5d94f9":"markdown","4c8a2f7c":"markdown","64d3de43":"markdown","f020f76d":"markdown","4aa82ce4":"markdown","17966985":"markdown","2073aae0":"markdown","d43ef828":"markdown","a4c92143":"markdown","a4e16846":"markdown","b626dc4c":"markdown","42c459d2":"markdown","23bb369d":"markdown","7461f105":"markdown","dbc36947":"markdown"},"source":{"f6136332":"# Import necessary modules\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Import the necessary modules\nfrom sklearn.preprocessing import StandardScaler\n# Import necessary modules\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression, Lasso, LogisticRegression, ElasticNet, Ridge\nfrom sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, roc_curve, roc_auc_score\n# set style setting\nplt.style.use('ggplot')","b5ef3c2e":"iris = datasets.load_iris()\ntype(iris) # it's a dictionary contains features and target","56fcb9ad":"print(iris.keys())","7e529195":"type(iris.data), type(iris.target)","b880db6d":"iris.data.shape","856c5894":"iris.target_names","fee770e5":"X = iris.data\ny = iris.target\ndf = pd.DataFrame(X, columns=iris.feature_names)\nprint(df.head())","750930c5":"_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8, 8], s=150, marker = 'D')\n# The diagonal create histogram and non-diagonal create scatter plot based on species","ddd57cc9":"columns = ['infants', 'water', 'budget', 'physician', 'salvador',\n       'religious', 'satellite', 'aid', 'missile', 'immigration', 'synfuels',\n       'education', 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa'] \ndf = pd.read_csv('..\/input\/supervised\/house-votes-84.csv', names=['party'] + columns)\n\nfor column in df:\n    if column != 'party':\n        df[column].replace({'n': '0', 'y': '1', '?': np.nan}, inplace=True)\ndf.dropna(inplace=True)\ndf[columns] = df[columns].apply(np.int64)  \n","9ab65c4b":"df.info()","1606d2bc":"df.describe()","9d577f03":"df.head()","95e1864d":"# Example for countplot\nplt.figure()\nsns.countplot(x='education', hue='party', data=df, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()","ff2230e7":"# for satellite\nplt.figure()\nsns.countplot(x='satellite', hue='party', data=df, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()","a2ffba45":"# for missile\nplt.figure()\nsns.countplot(x='missile', hue='party', data=df, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()","b658c2d3":"# the use of .drop() to drop the target variable 'party' from the feature array X as well as the use of the \n# .values attribute to ensure X and y are NumPy arrays.\n# Create arrays for the features and the response variable\ny = df['party'].values # target mean predict variable\nX = df.drop('party', axis=1).values # features\n\n# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X, y)\n","2cb8a337":"# Predict the labels for the training data X\n# y_pred = knn.predict(X)\nX_new = np.array([[0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897,\n        0.42310646, 0.9807642 , 0.68482974, 0.4809319 , 0.39211752,\n        0.34317802, 0.72904971, 0.43857224, 0.0596779 , 0.39804426,\n        0.73799541]])\n# Predict and print the label for the new data point X_new\nnew_prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(new_prediction))","88accafe":"# Load the digits dataset: digits\ndigits = datasets.load_digits()\n\n# Print the keys and DESCR of the dataset\nprint(digits.keys())\nprint(digits.DESCR)\n\n# Print the shape of the images and data keys\nprint(digits.images.shape)\nprint(digits.data.shape)\n\n# Display digit 1010\n# print(digits.images[0])\nfor img in digits.images[:10]:    \n    plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.show()","c2cc210d":"# Create feature and target arrays\nX = digits.data\ny = digits.target\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))","ae410adc":"# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors = k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","71cc0490":"# Read the CSV file into a DataFrame: df\ndf = pd.read_csv('..\/input\/supervised\/gm_2008_region.csv')\n\n# Create arrays for features and target variable\ny = np.array(df['life'])\nX = np.array(df['fertility'])\n\n# Print the dimensions of y and X before reshaping\nprint(\"Dimensions of y before reshaping: \", y.shape)\nprint(\"Dimensions of X before reshaping: \", X.shape)\n\n# Reshape X and y\ny_reshaped = y.reshape(-1, 1)\nX_reshaped = X.reshape(-1, 1)\n\n# Print the dimensions of y_reshaped and X_reshaped\nprint(\"Dimensions of y after reshaping: \", y_reshaped.shape)\nprint(\"Dimensions of X after reshaping: \", X_reshaped.shape)","134cadf4":"sns.heatmap(df.corr(), square=True, cmap='RdYlGn')","a64325e8":"df.info()","090b403e":"df.describe()","29bb504b":"df.head()","ba35887a":"df.corr()","30aa94fc":"# Create arrays for features and target variable\ny = np.array(df['life'])\nX_fertility = np.array(df['fertility'])\n\n# Reshape X and y\ny = y.reshape(-1, 1)\nX_fertility = X.reshape(-1, 1)\n\nprint(y.shape)\nprint(X_fertility.shape)","286f99ea":"# Create the regressor: reg\nreg = LinearRegression()\n\n# Create the prediction space\nprediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n\n# Fit the model to the data\nreg.fit(X_fertility, y)\n\n# Compute predictions over the prediction space: y_pred\ny_pred = reg.predict(prediction_space)\n# Print R^2 \nprint('Score R^2: {}'.format(reg.score(X_fertility, y)))\n\n# This plot show a negative correlation b\/w fertility and life\nsns.relplot(x='fertility', y='life', data=df, kind='scatter')\n# Plot regression line\nplt.plot(prediction_space, y_pred, color='black', linewidth=3)\nplt.show()\n","f7d8676f":"df.head()","d0b7adcf":"X = df.drop(['life', 'Region'], axis=1).values\ny = df['life'].values","665f1111":"# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n\n# Create the regressor: reg_all\nreg_all = LinearRegression()\n\n# Fit the regressor to the training data\nreg_all.fit(X_train, y_train)\n\n# Predict on the test data: y_pred\ny_pred = reg_all.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse)) # variance b\/w riduals","c6a03657":"# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))","154923d1":"# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Perform 3-fold CV\ncvscores_3 = cross_val_score(reg, X, y,cv=3)\nprint(np.mean(cvscores_3))\n\n# Perform 10-fold CV\ncvscores_10 = cross_val_score(reg, X, y,cv=10)\nprint(np.mean(cvscores_10))\n\n# you can check how much time consumed for cv=10\n# %time itcross_val_score(reg, X, y,cv=10)","8b9bbfcc":"# Instantiate a lasso regressor: lasso\nlasso = Lasso(alpha=0.4, normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X, y)\n\n# Compute and print the coefficients\nlasso_coef = lasso.fit(X, y).coef_\nprint(lasso_coef)\n\ndf_columns = df.drop(['life', 'Region'], axis=1).columns\n# Plot the coefficients\nplt.plot(range(len(df_columns)), lasso_coef)\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\nplt.margins(0.02)\nplt.show()\n\n# According to the lasso algorithm, it seems like 'child_mortality' is the most important \n# feature when predicting life expectancy.","251ac43a":"df = pd.read_csv('..\/input\/supervised\/diabetes.csv')\nX = df.drop('diabetes', axis=1)\ny = df['diabetes']","8a7258e4":"# Create training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n# Instantiate a k-NN classifier: knn\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","a78a0c6c":"X = df.drop('diabetes', axis=1)\ny = df['diabetes']\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n# Create the classifier: logreg\nlogreg = LogisticRegression(solver='liblinear')\n\n# Fit the classifier to the training data\nlogreg.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","966645be":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","8bc33744":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))","8f784a27":"X = df.drop('diabetes', axis=1)\ny = df['diabetes']\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression(solver='liblinear')\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint(\"Best score is {}\".format(logreg_cv.best_score_))","3080c486":"# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression(solver='liblinear')\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train, y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n","ef9e48fc":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the hyperparameter grid\nl1_space = np.linspace(0, 1, 30)\nparam_grid = {'l1_ratio': l1_space}\n\n# Instantiate the ElasticNet regressor: elastic_net\nelastic_net = ElasticNet(normalize=True, tol=1e-2)\n\n# Setup the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n\n# Fit it to the training data\ngm_cv.fit(X_train, y_train)\n\n# Predict on the test set and compute metrics\ny_pred = gm_cv.predict(X_test)\nr2 = gm_cv.score(X_test, y_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))\nprint(\"Tuned ElasticNet MSE: {}\".format(mse))","a5c5cbd7":"# Read 'gapminder.csv' into a DataFrame: df\ndf = pd.read_csv('..\/input\/supervised\/gm_2008_region.csv')\n\n# Create a boxplot of life expectancy per region\ndf.boxplot('life', 'Region', rot=60)\n\n# Show the plot\nplt.show()","4e8aedb8":"# Create dummy variables: df_region\ndf_region = pd.get_dummies(df)\n\n# Print the columns of df_region\nprint(df_region.columns)\n\n# Create dummy variables with drop_first=True: df_region\ndf_region = pd.get_dummies(df, drop_first=True)\n\n# Print the new columns of df_region\nprint(df_region.columns)","0f09ef58":"# Instantiate a ridge regressor: ridge\nridge = Ridge(alpha=0.5, normalize=True)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge, X, y, cv=5)\n\n# Print the cross-validated scores\nprint(ridge_cv)","ce38d283":"columns = ['infants', 'water', 'budget', 'physician', 'salvador',\n       'religious', 'satellite', 'aid', 'missile', 'immigration', 'synfuels',\n       'education', 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa'] \ndf = pd.read_csv('..\/input\/supervised\/house-votes-84.csv', names=['party'] + columns)\nfor column in df:\n    if column != 'party':\n        df[column].replace({'n': 0, 'y': 1, '?': np.nan}, inplace=True)\n# df[columns] = df[columns].apply(np.int64)\n# Convert '?' to NaN\n# df[df == '?'] = np.nan\n\n# Print the number of NaNs\nprint(df.isnull().sum())\n\n# Print shape of original DataFrame\nprint(\"Shape of Original DataFrame: {}\".format(df.shape))\n\n# Drop missing values and print shape of new DataFrame\n# df = df.dropna()\n\n# # Print shape of new DataFrame\n# print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))\n# df.describe()\n\ny = df['party'] # target mean predict variable\nX = df.drop('party', axis=1) # features","6a045ff5":"# Setup the pipeline steps: steps\n# we convert all NA values to their mean \nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\n# imp.fit(X)\n# X = imp.transform(X)\nsteps = [('imputation', imp),\n        ('logistic_regression', LogisticRegression(solver='liblinear'))]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(classification_report(y_test, y_pred))","9964e05d":"df = pd.read_csv('..\/input\/supervised\/white-wine.csv')\nX = df.drop('quality', axis=1).values\ny = df['quality'].values\nprint(X.shape, y.shape)","e4bf05f6":"# Scale the features: X_scaled\nX_scaled = StandardScaler().fit_transform(X)\n\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n\n# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))","34fe4dd7":"# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n        \n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train, y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))","ce1e7b6d":"# Read the CSV file into a DataFrame: df\ndf = pd.read_csv('..\/input\/supervised\/gm_2008_region.csv')\ndf.head()","3075d870":"X = df.drop(['life', 'Region'], axis=1).values\ny = df['life'].values","94e89da1":"# Setup the pipeline steps: steps\nsteps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='mean')),\n         ('scaler', StandardScaler()),\n         ('elasticnet', ElasticNet())]\n\n# Create the pipeline: pipeline \npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ngm_cv.fit(X_train, y_train)\n\n# Compute and print the metrics\nr2 = gm_cv.score(X_test, y_test)\nprint(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))","6602eadc":"## Exploring the Gapminder data\nAs always, it is important to explore your data before building models.","e73cd918":"## Regression with categorical features\nHaving created the dummy variables from the 'Region' feature, you can build regression models as you did before. Here, you'll use ridge regression to perform 5-fold cross-validation.","30a96bb4":"## Regularization II: Lasso\nIts ability to perform feature selection in this way becomes even more useful when you are dealing with data involving thousands of features.","50bdae13":"## Hyperparameter tuning with GridSearchCV\nLinear regression: Choosing parameters\nRidge\/lasso regression: Choosing alpha\nk-Nearest Neighbors: Choosing n_neighbors\nParameters like alpha and k: Hyperparameters\nHyperparameters cannot be learned by ing the mode","4dfd8094":"## Exploring categorical features\nThe Gapminder dataset contained a categorical 'Region' feature, to explore this feature. Boxplots are particularly useful for visualizing categorical features","fe3e9853":"## Train\/Test Split + Fit\/Predict\/Accuracy\nNow that you have learned about the importance of splitting your data into training and test sets, it's time to practice doing this on the digits dataset! After creating arrays for the features and target variable, you will split them into training and test sets, fit a k-NN classifier to the training data, and then compute its accuracy using the .score() method.","259ce1b7":"## Centering and scaling your data\nFeatures on larger scales can unduly inuence the model.We want features to be on a similar scale Scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features.","a00e6f47":"## Dropping missing data\nConvert the '?'s to NaNs, and then drop the rows that contain them from the DataFrame.","a3c55def":"## k-Nearest Neighbors: Fit\nHaving explored the Congressional voting records dataset, it is time now to build your first classifier. In this exercise, you will fit a k-Nearest Neighbors classifier to the voting dataset, which has once again been pre-loaded for you into a DataFrame df.\n\nThe features need to be in an array where each column is a feature and each row a different observation or data point - in this case, a Congressman's voting record. The target needs to be a single column with the same number of observations as the feature data. We have done this for you in this exercise. Notice we named the feature array X and response variable y: This is in accordance with the common scikit-learn practice.","4a7827e6":"## Imputing missing data in a ML Pipeline I\nmputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow.","b1a91d27":"# Preprocessing and pipelines\nscikit-learn allows transformers and estimators to be chained together and used as a single unit, and pre-processing techniques enhance our modeling performance.","2a0a73c7":"## Overfitting and underfitting\nYou will now construct such a curve for the digits dataset! In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting.","ed4b7dd2":"## The digits recognition dataset\nUp until now, you have been performing binary classification, since the target variable had two possible outcomes. Hugo, however, got to perform multi-class classification in the videos, where the target variable could take on three possible outcomes. Why does he get to have all the fun?! In the following exercises, you'll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn's included datasets, and that is the one we will use in this exercise.","dfec69a9":"# Fine-tuning your model\nHaving trained your model, your next task is to evaluate its performance. In this chapter, you will learn about some of the other metrics available in scikit-learn that will allow you to assess your model's performance in a more nuanced manner. Next, learn to optimize your classification and regression models using hyperparameter tuning.","d8b8b40b":"## Hold-out set in practice II: Regression\nRemember lasso and ridge regression from the previous chapter? Lasso used the  penalty to regularize, while ridge used the  penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the  and  penalties:\n\n\nIn scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an  penalty, and anything lower is a combination of  and .\n\nIn this exercise, you will GridSearchCV to tune the 'l1_ratio' of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model's performance.","6c338a20":"## K-Fold CV comparison\nCross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset.","af839502":"## Hold-out set in practice I: Classification\nYou will now practice evaluating a model with tuned hyperparameters on a hold-out set. The feature array and target variable array from the diabetes dataset have been pre-loaded as X and y.\n\nIn addition to , logistic regression has a 'penalty' hyperparameter which specifies whether to use 'l1' or 'l2' regularization. Your job in this exercise is to create a hold-out set, tune the 'C' and 'penalty' hyperparameters of a logistic regression classifier using GridSearchCV on the training set.","0f5d94f9":"## Numerical EDA\nIn this chapter, you'll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. Your goal will be to predict their party affiliation ('Democrat' or 'Republican') based on how they voted on certain key issues. Here, it's worth noting that we have preprocessed this dataset to deal with missing values. This is so that your focus can be directed towards understanding how to train and evaluate supervised learning models. Once you have mastered these fundamentals, you will be introduced to preprocessing techniques in Chapter 4 and have the chance to apply them there yourself - including on this very same dataset!","4c8a2f7c":"## AUC computation\nSay you have a binary classifier that in fact is just randomly making guesses. It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. The Area under this ROC curve would be 0.5. AUC is an informative metric to evaluate a model. If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!","64d3de43":"## 5-fold cross-validation\nCross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data.\n\nIn this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn's cross_val_score() function uses  as the metric of choice for regression. Since you are performing 5-fold cross-validation, the function will return 5 scores. Your job is to compute these 5 scores and then take their average.","f020f76d":"# Classification\nYou\u2019ll apply what you learn to a political dataset, where you classify the party affiliation of United States congressmen based on their voting records.","4aa82ce4":"## Plotting an ROC curve\nClassification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models. The predict_proba() method which returns the probability of a given sample being in a particular class. Having built a logistic regression model, you'll now evaluate its performance by plotting an ROC curve. ","17966985":"## Creating dummy variables\nscikit-learn does not accept non-numerical features. 'Region' feature contains very useful information that can predict life expectancy. For example, Sub-Saharan Africa has a lower life expectancy compared to Europe and Central Asia. Therefore, if you are trying to predict life expectancy, it would be preferable to retain the 'Region' feature. To do this, you need to binarize it by creating dummy variables","2073aae0":"## Importing data for supervised learning\nIn this chapter, you will work with Gapminder data that we have consolidated into one CSV file available in the workspace as 'gapminder.csv'. Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country's GDP, fertility rate, and population.\n\nSince the target variable here is quantitative, this is a regression problem. To begin, you will fit a linear regression with just one feature: 'fertility', which is the average number of children a woman in a given country gives birth to. In later exercises, you will use all the features to build regression models.","d43ef828":"## Metrics for classification\nEvaluate the performance of binary classifiers by computing a confusion matrix and generating a classification report.\n\nClassification report consisted of three rows, and an additional support column. The support gives the number of samples of the true response that lie in that class.The support was the number of Republicans or Democrats in the test set on which the classification report was computed. The precision, recall, and f1-score columns, then, gave the respective metrics for that particular class.\n\nWe wll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. ","a4c92143":"## Exploratory data analysis (EDA)","a4e16846":"## Building a logistic regression model\nit very easy to try different models, since the Train-Test-Split\/Instantiate\/Fit\/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as 'estimators'.","b626dc4c":"# Regression\nproblem requires a continuous outcome? Regression is best suited to solving such problems. You will learn about fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data.","42c459d2":"## Visual EDA\nexplore the voting behavior further by generating countplots for the 'satellite' and 'missile' bills, and answer the following question: Of these two bills, for which ones do Democrats vote resoundingly in favor of, compared to Republicans?","23bb369d":"## Pipeline for regression\npipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV.","7461f105":"## k-Nearest Neighbors: Predict\nHaving fit a k-NN classifier, you can now use it to predict the label of a new data point. However, there is no unlabeled data available since all of it was used to fit the model! You can still use the .predict() method on the X that was used to fit the model, but it is not a good indicator of the model's ability to generalize to new, unseen data.\n\nIn the next video, Hugo will discuss a solution to this problem. For now, a random unlabeled data point has been generated and is available to you as X_new. You will use your classifier to predict the label for this new data point, as well as on the training data X that the model has already seen. Using .predict() on X_new will generate 1 prediction, while using it on X will generate 435 predictions: 1 for each sample.","dbc36947":"## Fit & predict for regression\nNow, you will fit a linear regression and predict life expectancy using just one feature. You saw Andy do this earlier using the 'RM' feature of the Boston housing dataset. In this exercise, you will use the 'fertility' feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is 'life'. "}}