{"cell_type":{"197aa1d0":"code","03e54529":"code","7ce4e21d":"code","dbac9649":"code","9e5af538":"code","b7f7dc85":"code","9daf1bcd":"code","717f96a6":"code","a1384e6f":"code","c498628d":"code","5624b06d":"code","1f3556aa":"code","93b9eb51":"code","50992217":"code","1bc0aebe":"code","4485baed":"code","7fb7f87b":"code","67562f11":"code","98d8719b":"code","b902cda8":"code","92623fc2":"code","6b91eacb":"code","23bc68bc":"code","17f670c8":"code","ab3ccb5e":"code","1704a806":"code","6aec544b":"code","70468bc8":"code","c40ace3a":"code","718a5833":"code","42ba0a75":"code","aa4f7591":"code","4cf2d0bf":"code","6e9dd027":"code","565853df":"code","67cd7451":"code","42be76c6":"code","e05726fc":"code","7451177c":"code","00e802c5":"markdown","2b992502":"markdown","9d29cea8":"markdown","2a974790":"markdown","b4e99897":"markdown","c3d20ca6":"markdown","d3db79ed":"markdown","37008eb4":"markdown","0f331758":"markdown","25835894":"markdown","d40607da":"markdown","81c92031":"markdown","9653507b":"markdown","b88ea1ce":"markdown","7cc6fae3":"markdown","95e404c4":"markdown","5916ed10":"markdown","35b45ca9":"markdown","0f2b8368":"markdown","a74b2886":"markdown","54fe66d7":"markdown","97ea68f0":"markdown"},"source":{"197aa1d0":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport pylab\nfrom string import punctuation, digits\nimport os\n\n","03e54529":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npylab.rc('figure', figsize=(10,7))","7ce4e21d":"def perceptron_single_step_update(feature_vector,label,current_theta,current_theta_0):\n    \n    if(np.dot(label,(np.dot(current_theta,feature_vector.transpose())+current_theta_0))<=0):\n        \n        current_theta=current_theta+np.dot(label,feature_vector)\n        current_theta_0=current_theta_0+label\n        \n    return (current_theta,current_theta_0)","dbac9649":"def perceptron(feature_matrix, labels, T):\n\n    update=(np.zeros(feature_matrix.shape[1]),0)\n    for t in range(T):\n        for i in range(feature_matrix.shape[0]):\n\n            update=perceptron_single_step_update(feature_matrix[i],labels[i],update[0],update[1])\n\n    return(update)","9e5af538":"def hinge_loss_single(feature_vector, label, theta, theta_0):\n\n    y = np.dot(theta, feature_vector) + theta_0\n    loss = max(0.0, 1 - y * label)\n    return loss\n    raise NotImplementedError\n    \ndef hinge_loss_full(feature_matrix, labels, theta, theta_0):\n\n    loss=0\n    for vec, y in zip(feature_matrix, labels):\n        loss=loss+hinge_loss_single(vec,y,theta,theta_0)\n    \n    return (loss\/len(labels))\n ","b7f7dc85":"def plot_classifier(theta,theta_0): # to visualise our model\n    good=toy_data[0]==1\n    plt.scatter(toy_data[1][good],toy_data[2][good],c='blue',alpha=0.8,label='Classified True')\n    plt.scatter(toy_data[1][~good],toy_data[2][~good],c='r',alpha=0.8,label='Classified False')\n\n    xmin,xmax=plt.axis()[:2]\n    \n    x=np.linspace(xmin,xmax)\n    y = -(theta[0]*x + theta_0) \/ (theta[1] + 1e-16)\n    plt.plot(x,y,label='Classifier',lw=2)\n    plt.xlabel(\"Values of x1\")\n    plt.ylabel(\"Values of x2\")\n    plt.title(\"Linear classifier for a 2-d feature vector\")\n    plt.legend()","9daf1bcd":"toy_data=np.loadtxt(\"\/kaggle\/input\/toy_data.tsv\",delimiter='\\t',unpack=True)\ntoy_label,toy_feature= toy_data[0],toy_data[[1,2]]","717f96a6":"theta,theta_0=perceptron(toy_feature.transpose(),toy_label,15)\nplot_classifier(theta,theta_0)\nprint(f\"Average hinge loss for Perceptron algorithm is {hinge_loss_full(toy_feature.transpose(),toy_label,theta,theta_0):.4f}\")","a1384e6f":"def get_order(n_samples):\n    try:\n        with open(str(n_samples) + '.txt') as fp:\n            line = fp.readline()\n            return list(map(int, line.split(',')))\n    except FileNotFoundError:\n        random.seed(1)\n        indices = list(range(n_samples))\n        random.shuffle(indices)\n        return indices","c498628d":"def perceptron_stochaistic(feature_matrix, labels, T):\n\n    update=(np.zeros(feature_matrix.shape[1]),0)\n    for t in range(T):\n        for i in get_order(feature_matrix.shape[0]):\n            # \n            update=perceptron_single_step_update(feature_matrix[i],labels[i],update[0],update[1])\n\n    return(update)","5624b06d":"theta,theta_0=perceptron_stochaistic(toy_feature.transpose(),toy_label,15)\nplot_classifier(theta,theta_0)\nprint(f\"Average hinge loss for Perceptron algorithm is {hinge_loss_full(toy_feature.transpose(),toy_label,theta,theta_0):.4f}\")","1f3556aa":"def average_perceptron(feature_matrix, labels, T):\n\n    \n    update=(np.zeros(feature_matrix.shape[1]),0) \n    theta_sum=np.array(update[0])\n    theta_0_sum=update[1]\n    count=0\n    for t in range(T):\n        for i in get_order(feature_matrix.shape[0]):\n            update=perceptron_single_step_update(feature_matrix[i],labels[i],update[0],update[1])\n            theta_sum=theta_sum+np.array(update[0])\n            theta_0_sum=theta_0_sum+update[1]\n            count=count+1\n            \n    avg_theta=theta_sum\/count\n    avg_theta_0=theta_0_sum\/count\n    \n    return (avg_theta,avg_theta_0)","93b9eb51":"theta,theta_0=average_perceptron(toy_feature.transpose(),toy_label,15)\nplot_classifier(theta,theta_0)\nprint(f\"Average hinge loss for Average Perceptron algorithm is {hinge_loss_full(toy_feature.transpose(),toy_label,theta,theta_0):.4f}\")","50992217":"def pegasos_single_step_update(feature_vector,label,L,eta,current_theta,current_theta_0):\n    \n    if(np.dot(label,(np.dot(current_theta,feature_vector.transpose())+current_theta_0))<=1):\n        current_theta=current_theta*(1-eta*L)+eta*np.dot(label,feature_vector)\n        current_theta_0=current_theta_0+eta*label\n    else:\n        current_theta=current_theta*(1-eta*L)\n    \n    return (current_theta,current_theta_0)\n\n\ndef pegasos(feature_matrix, labels, T, L):\n\n    update=(np.zeros(feature_matrix.shape[1]),0) #why not just theta and theta zero\n    count=0\n    for t in range(T):\n        \n        for i in get_order(feature_matrix.shape[0]):\n            count=count+1\n            eta=1\/np.sqrt(count)\n            \n            update=pegasos_single_step_update(feature_matrix[i],labels[i],L,eta,update[0],update[1])        \n    return update","1bc0aebe":"theta,theta_0=pegasos(toy_feature.transpose(),toy_label,T=15,L=0.1)\nplot_classifier(theta,theta_0)\nprint(f\"Average hinge loss for Pegasos algorithm is {hinge_loss_full(toy_feature.transpose(),toy_label,theta,theta_0):.4f}\")","4485baed":"train_data=pd.read_csv(\"\/kaggle\/input\/reviews_train.tsv\",delimiter='\\t',engine ='python')\nvalidation_data=pd.read_csv(\"\/kaggle\/input\/reviews_val.tsv\",delimiter='\\t',engine ='python')\ntest_data=pd.read_csv(\"\/kaggle\/input\/reviews_test.tsv\",delimiter='\\t',engine ='python')","7fb7f87b":"train_data.head()","67562f11":"train_text=train_data['text'].values\ntrain_label=train_data['sentiment'].values\n\nval_text=validation_data['text'].values\nval_label=validation_data['sentiment'].values\n\ntest_text=test_data['text'].values\ntest_label=test_data['sentiment'].values","98d8719b":"def words_corpus(text_array):\n    count=0\n    dictionary={}\n    for text in text_array:\n        for c in punctuation + digits:\n            text = text.replace(c, ' ' + c + ' ')\n            \n        \n        for word in text.lower().split():\n            if word not in dictionary.keys():\n                dictionary[word]=len(dictionary)\n    return dictionary","b902cda8":"def text_to_feature(text_feature,dic):\n    feature_matrix=np.zeros([len(text_feature),len(dic)])\n   \n    for i,text in enumerate(text_feature):\n        for c in punctuation + digits:\n            text = text.replace(c, ' ' + c + ' ')\n        for word in text.lower().split():\n            \n            if word in dic:   ##For loop\n                feature_matrix[i,dic[word]]=1\n        \n    return feature_matrix","92623fc2":"dictionary=words_corpus(train_text)","6b91eacb":"train_feature_matrix=text_to_feature(train_text,dictionary)\nval_feature_matrix=text_to_feature(val_text,dictionary)\ntest_feature_matrix=text_to_feature(test_text,dictionary)\n\nprint(f\"Size of Training data: {train_feature_matrix.shape}\")\nprint(f\"Size of Validation data: {val_feature_matrix.shape}\")\nprint(f\"Size of Test data: {test_feature_matrix.shape}\")","23bc68bc":"def classify(feature_matrix, theta, theta_0):\n    \n    predictions=[]\n    for i in range(len(feature_matrix)):\n        \n       if(np.dot(theta,feature_matrix[i].transpose())+theta_0)>0:\n           \n           predictions.append(1)\n       else:\n           predictions.append(-1)\n           \n    return np.array(predictions)\n","17f670c8":"def accuracy(preds, targets):\n\n    return (preds == targets).mean()","ab3ccb5e":"def classifier_accuracy(classifier,train_feature_matrix,val_feature_matrix,train_labels,val_labels,**kwargs):\n\n    thetas=classifier(train_feature_matrix,train_labels,**kwargs)\n        \n    predictions_train=classify(train_feature_matrix,thetas[0],thetas[1])\n    predictions_val=classify(val_feature_matrix,thetas[0],thetas[1])\n    \n    train_acc=accuracy(predictions_train,train_labels)\n    val_acc=accuracy(predictions_val,val_labels)\n    \n    return (train_acc,val_acc)","1704a806":"train_accuracy_perc, val_accuracy_perc = classifier_accuracy(perceptron_stochaistic,train_feature_matrix,val_feature_matrix,train_label,val_label,T=15)\nprint(\"{:35} {:.4f}\".format(\"Training accuracy for perceptron:\", train_accuracy_perc))\nprint(\"{:35} {:.4f}\".format(\"Validation accuracy for perceptron:\", val_accuracy_perc))","6aec544b":"train_accuracy_avper, val_accuracy_avper = classifier_accuracy(average_perceptron,train_feature_matrix,val_feature_matrix,train_label,val_label,T=15)\nprint(\"{:35} {:.4f}\".format(\"Training accuracy for Average perceptron:\", train_accuracy_avper))\nprint(\"{:35} {:.4f}\".format(\"Validation accuracy for Average perceptron:\", val_accuracy_avper))","70468bc8":"train_accuracy_peag, val_accuracy_peag = classifier_accuracy(pegasos,train_feature_matrix,val_feature_matrix,train_label,val_label,T=15,L=0.01)\nprint(\"{:35}{:.4f}\".format(\"Training accuracy for Pegasos:\", train_accuracy_peag))\nprint(\"{:35} {:.4f}\".format(\"Validation accuracy for Pegasos:\", val_accuracy_peag))","c40ace3a":"def tune_param (train_fn,param_vals,train_feature,train_label,val_feature,val_label,**kwargs):\n    train_accuracy=[]\n    val_accuracy=[]\n    \n    for t in param_vals:\n        theta,theta_0=train_fn(train_feature,train_label,t,**kwargs)\n        train_predict=classify(train_feature,theta,theta_0)\n        t_acc=accuracy(train_predict,train_label)\n        train_accuracy.append(t_acc)\n        \n        val_predict=classify(val_feature,theta,theta_0)\n        v_acc=accuracy(val_predict,val_label)\n        val_accuracy.append(v_acc)\n        print(f\"For T = {t} ========== Training Accuracy = {t_acc:.4f} ========== Validation Accuracy = {v_acc:.4f}\")\n        \n    return train_accuracy,val_accuracy","718a5833":"def plot_accuracy(algo_name, param_name, param_vals, acc_train, acc_val):\n   \n    \n    plt.subplots()\n    plt.plot(param_vals, acc_train, '-o')\n    plt.plot(param_vals, acc_val, '-o')\n\n  \n    algo_name = ' '.join((word.capitalize() for word in algo_name.split(' ')))\n    param_name = param_name.capitalize()\n    plt.suptitle('Classification Accuracy vs {} ({})'.format(param_name, algo_name))\n    plt.legend(['train','val'], loc='upper right', title='Partition')\n    plt.xlabel(param_name)\n    #plt.ylim(0.5,1)\n    plt.ylabel('Accuracy (%)')\n    plt.show()","42ba0a75":"data = (train_feature_matrix, train_label, val_feature_matrix, val_label)\nTs = [5,10,15,20,25,30,40,50]","aa4f7591":"train_accuracy_per, val_accuracy_per=tune_param(perceptron_stochaistic,Ts,*data)\nprint(f\"The best Parameter for Perceptron algorithm is T = {Ts[val_accuracy_per.index(max(val_accuracy_per))]} and maximum accuracy reached is {max(val_accuracy_per):.4f}\")","4cf2d0bf":"plot_accuracy(\"Perceptron\",\"T\",Ts,train_accuracy_per,val_accuracy_per)","6e9dd027":"train_accuracy_avgp, val_accuracy_avgp=tune_param(average_perceptron,Ts,*data)\nprint(f\"The best Parameter for Average Perceptron algorithm is T = {Ts[val_accuracy_avgp.index(max(val_accuracy_avgp))]} and maximum accuracy reached is {max(val_accuracy_avgp):.4f}\")","565853df":"plot_accuracy(\"Average Perceptron\",\"T\",Ts,train_accuracy_avgp,val_accuracy_avgp)","67cd7451":"train_accuracy_peg, val_accuracy_peg=tune_param(pegasos,Ts,*data,L=0.01)\nprint(f\"\\nThe best Parameter for Pegasos algorithm is T = {Ts[val_accuracy_peg.index(max(val_accuracy_peg))]} and maximum accuracy reached is {max(val_accuracy_peg):.4f}\")","42be76c6":"plot_accuracy(\"Pegasos\",\"T\",Ts,train_accuracy_peg,val_accuracy_peg)","e05726fc":"parameters=pegasos(train_feature_matrix,train_label,T=25,L=0.01)\npred=classify(test_feature_matrix,parameters[0],parameters[1])\nacc=accuracy(pred,test_label)\n\nprint(f\"The accuracy of the best fitted algorithm for Test data is : {acc:.4f}\")","7451177c":"results=pd.DataFrame(pred,columns=['Predicted_Labels'])\nresults.to_csv(\"submission.csv\", index=False)","00e802c5":"### Thank you!","2b992502":"Great. An acurracy of `0.8100` for complete new data set is pretty good. We can cleary see our model is robust.","9d29cea8":"#### Much better indeed!\n\nThe loss is down to 0.3388 as there are a few inevitable points that are still misclassified.\n\n### 2.2 Average Perceptron:\n\nLet's now move to average perceptron algorithm.\nThe average perceptron will add a modification to the original perceptron algorithm: since the basic algorithm continues updating as the algorithm runs, nudging parameters in possibly conflicting directions, it is better to take an average of those parameters as the final answer. Every update of the algorithm is the same as before. The returned parameters  \u03b8 , however, are an average of the  \u03b8 s across the  n  steps:\n\n$$\\theta_{final}=\\frac{1}{n}(\\theta^{(1)}+\\theta^{(2)}+\\theta^{(3)}+...+\\theta^{(n)}) $$","2a974790":"Looks like gradient decent is performing the best. But wait till we tune our models to get the maximum out of it.\nNow that our models are ready, let's do the fun part. <b>Let's do some sentiment predictions on Amazon customer reviews on food products.<\/b>","b4e99897":"### 2.3 Pegasos Algorithm:\n\nPegasos algorithm is simple stochastic gradient decent. The goal here is to minimize the cost function.\nThe cost function is combination of two things:\n- Loss function: Defines the loss incurred due to points slipping into the marginal boundaries.\n- Regularisation: It tries to keep the marginal boundaries as far as possible from the decision boundary in order to have a robust model.\n\n\n$$ J(\\theta,\\theta_0)\\: =\\: \\frac{1}{n} \\sum{\\underbrace{loss_h (y^{(i)}(\\theta.x^{(i)}+\\theta_0))}_{Hinge\\: loss}\\: +\\:\\underbrace{\\frac{\\lambda}{2}\\parallel{\\theta}\\:\\parallel}_{Regularisation}}  $$\n\n- In order to update the parameters, the old parameters are tweaked in the negative direction of the derivative of the cost function. This in every pass takes us towards the minimum value of $\\nabla J(\\theta,\\theta_0)$\n- Unlike perceptron, the SGD will make an update in every step. Even if there is no loss incurred there will be changes due to the involvement of regularisation term.\n- We are also going to include a step size (or learning rate) to make sure that we are not taking too large steps which might take us to somewhere the value of objective function is same or even higher.","c3d20ca6":"### 4. Transforming the data\n\n- We will convert review texts into feature vectors using a bag of words approach. We start by compiling all the words that appear in a training set of reviews into a dictionary , thereby producing a list of  d  unique words.\n- We can then transform each of the reviews into a feature vector of length  d  by setting the  ith  coordinate of the feature vector to  1  if the  ith  word in the dictionary appears in the review, or  0  otherwise.\n- For instance, consider two simple documents \u201cI love you 3000\" and \u201clove endgame\". In this case, the dictionary is the set  {I;love;3000;you;endgame} , and the documents are represented as  (1;1;1;1;0)  and  (0;1;0;0;1).","d3db79ed":"We are just interested in the `sentiments` which are the `labels` and the `text` which will be the `feature vector` for our model. However we need to convert the text into some sort of feature matrix, an array of `1s` and `0s`","37008eb4":"###### Now that we have all the tools we need, let's see how our model works by importing some data. ","0f331758":"Okay, so we are able to reduce the loss even further by simply averaging out the paramameters. But things are not always that way simple.\n\n $$ Everything\\:  should  \\:be\\: made\\: as\\: simple\\: as\\: possible,\\: but\\: no\\: simpler.\\: -\\: Albert\\: Einstein$$","25835894":"### 3. Loading the datasets","d40607da":"\n## 2. Building Models\n### 2.1 Perceptron algorithm:\nThe Perceptron algorithm is the simplest type of artificial neural network.\nIt is a model of a single neuron that can be used for two-class classification problems and provides the foundation for later developing much larger networks. \n- The model takes the training data and some random initial weights (often 0) as input.\n- It then goes through each example\/row of data and make an update to the parameters if there is a misclassification.\n- If there exist a linear classifier that correctly classifies the examples, then perceptron algorithm will definitely find that. \n\nFurther readings: http:\/\/ciml.info\/dl\/v0_8\/ciml-v0_8-ch03.pdf\n\nAn example is considered misclassified when:\n\n$$y^{(i)}(\\theta.x^{(i)}+\\theta_0)\\leq 0 $$\nwhere,\n\n $ x^{(i)} $ is an example vector\n \n $y^{(i)}$ is the example label\n \n $\\theta$ and $\\theta_0$ are the parameters\n \n \nOnce there is a misclassification the algorithm will update the parameters as per the following rule:\n\nif $$y^{(i)}(\\theta.x^{(i)}+\\theta_0)\\leq 0 $$ then\n        \n$$\\theta=\\theta + y^{(i)} x^{(i)}$$\n$$\\theta_0=\\theta_0 + y^{(i)}$$\n\n##### Yes, as simple as that! \n\nNow, we need to run this algorithm over T epochs to get our final result. So lets get started.","81c92031":"After analysing a number of `T` values we can choose Pegasos algorithm with `T=25` as the best performing model. Hence, we are going to use it to predict the `test` data.\n\nOne thing to consider is Pegasos takes two hyperparameter, the other one being learning rate. We can follow the same approch to find the best learning rate. I found L=0.01 gives the best result.\n","9653507b":"### 6. Model Tuning\n\nOptimizing algoritinm is deciding what hyperparameters to choose. For perceptron we only have one hyperparameter that is number of epochs\/passes, T. \n\nIf we make many many passes over the training data, then the algorithm is likely to overfit. (This would be like studying too long for an exam and just confusing yourself.) On the other hand, going over the data only one time might lead to underfitting. \n\nHence we have to choose a right value for T. There is no specific formula to abide by as such; we just have to see how the training and test loss changes with T and make an optimum choice.","b88ea1ce":"### 7. Classifying the Test data\n\nFinally, let's implement the best algoritm, coupled with the best parameters to predict the labels for the test data.","7cc6fae3":"Oops!! That's a lot of points misclassified there and hence a high loss. How could we try to reduce that?\nOne important thing here to consider is the order in which the updates are performed. We are simply looping over the algorithm in a constant order, which is not a good idea.\n\nConsider what the perceptron algorithm would do on a data set that consisted of 500 positive examples followed by 500 negatives\nexamples. After seeing the first few positive examples (maybe five),it would likely decide that every example is positive, and would stop learning anything. It would do well for a while (next 495 examples), until it hit the batch of negative examples. Then it would take a while (maybe ten examples) before it would start predicting everything as negative. By the end of one pass through the data, it would really only have learned from a handful of examples (fifteen in this case).\n\nHence, we need to shuffle the orders in which the examples are learned and updated before each epoch. ","95e404c4":"Great! Now lets see if we have done any better!","5916ed10":"### Label Classification and Accuracy","35b45ca9":"###### Let's define loss function to check our models performance.\n\nA loss is incurred whenever the points cross the decision boundary (marginal boundary in fact) and is given by:\n\n$$loss = 1-y^{(i)}(\\theta.x^{(i)}+\\theta_0) $$\n\nAlso a function for a quick visualisation of our linear classifier.","0f2b8368":"The goal of this kernel is to create some of the basic supervised learning algorithms from scratch, understand them by using a lower dimensional dataset and then implement them to Predict the sentiments of a set of Amazon customer reviews on Food products.\n\nWe will be defining a few of the most basic and fundamental type linear classifier, i.e. perceptron, average perceptron and Stochastic gradient method and comparing them based on their performance on validation data. \n\nThese are binary classifiers and we will use them to classify between $y \\in \\{1,-1\\}$.\n\nIn the end, we will do some parameter tuning to see which hyperparameters are best suited for our chosen algorithm.\n\nSteps followed\n\n1. Importing the libraries.\n2. Building Models\n\n    - 2.1 Perceptron algorithm\n    - 2.2 Average Perceptron algorithm\n    - 2.3 Pegasos algorithm\n    \n    \n3. Loading the data\n4. Data transformation\n5. Label Classification and Accuracy\n6. Model Tuning\n7. Classifying the test data\n","a74b2886":"The feature vectors are now ready to be used. We see that there are `4000` examples to train our models on. We will validate our models and further tune them using `validation data` and finally when we have made decision on what model to use, we will feed in the `test data`, which our model hasn't seen before, and see how it performs.\n\nLet's classify our data.","54fe66d7":"### 1. Importing Libraries","97ea68f0":"We can clearly see `Pegasos` and `Average Perceptron` are doing better.\n\nBut the issue is `Average Perceptron` is overfitting on the training data. It is following the examples too closely and picking up the noise. As a result it may underperform on different dataset which it hasn't seen before. `Pegasos` seem to be more robust and can handle different sets of training\/test set better than others."}}