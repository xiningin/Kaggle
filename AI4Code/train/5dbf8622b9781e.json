{"cell_type":{"0f88cbf8":"code","598a49fc":"code","6da9499c":"code","9d2c01a5":"code","aaa81306":"code","b2234490":"code","d943daf7":"code","be9d0890":"code","52d7581e":"code","cbfbd3f6":"code","334255f1":"code","5f5f6051":"code","557a14e9":"code","9afe120e":"code","ab151496":"code","839bffed":"code","43638de6":"code","55f05724":"code","174e6022":"markdown","fd215aac":"markdown","fb53ecdf":"markdown","47937f5e":"markdown","6bbd1cdc":"markdown","f72c38b4":"markdown","7eece144":"markdown","b64364ab":"markdown","625868cc":"markdown","66823b98":"markdown"},"source":{"0f88cbf8":"import matplotlib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nprint(f'matplotlib: {matplotlib.__version__}')\nprint(f'tensorflow: {tf.__version__}')\nprint(f'pandas    : {pd.__version__}')\nprint(f'numpy     : {np.__version__}')","598a49fc":"# Load the data\ndf = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf.describe()","6da9499c":"df.head(10)","9d2c01a5":"from sklearn import preprocessing\n\ndef format_feats(in_feats):\n    x = in_feats.values #returns a numpy array\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    return pd.DataFrame(x_scaled, columns=in_feats.columns)\n\n# Apply some data formatting\ndef format_data(data):\n    # One-hot encode 'Embarked' column\n    data = pd.get_dummies(data, columns=['Sex','Embarked'])\n    # Drop columns that require additional processing\n    data = data.drop(['Name','Ticket','Cabin'], axis=1)\n    # Fill null values with the mean of the column\n    data.fillna(data.mean(), inplace=True)\n    # Return the results\n    return data\n\n# This should split the data into our features and our labels\ndata = format_data(df)\ndata.describe()","aaa81306":"# Plot histogram of labels\ndata.hist(column=['Survived'],bins=2);","b2234490":"# Get the number of labels that are in the second class 'Survived==1'\ndef tally_survivors(dat):\n    num_survived = len(dat[dat['Survived']==1])\n    num_died     = len(dat[dat['Survived']==0])\n    print(f'   survivors    : {num_survived}')\n    print(f'   non-survivors: {num_died}')\n    return num_died-num_survived\n\nprint('BEFORE TRIM:')\ndiff = tally_survivors(data)\n\n# Reduce the number of non-survivors in the training set\n# to match the number of survivors\n# =============\n\n# Get the list of indices for non-survivors\nindices = data[data['Survived']==0].index\n# Get a list of indices to remove and remove them\nremoved = np.random.choice(np.array(indices), size=diff, replace=False)\ntrain   = data.drop(index=removed)\n\n# Plot the remaining distribution\nprint('AFTER TRIM:')\nnew_diff = tally_survivors(train)\ntrain['Survived'].hist(bins=2);","d943daf7":"# Features\nfeats  = train.drop(['Survived'], axis=1)\n# Classification labels\nlabels = train['Survived']","be9d0890":"# Generate the model\nfrom tensorflow import nn\nfrom tensorflow.keras import layers, Sequential\n\n# Set Dropout rate\ndrpout = 0.2\n\n# Create a function for model construction\n# This will help for testing different model architectures.\ndef model_construct(inputs, n=[16], outputs=2,\n                    activ=nn.relu):\n    # Add the outputs to the list of nodes\n    n.append(outputs)\n    \n    # Input layer\n    layer_list = []\n    layer_list.append(layers.Dense(units=n[0],\n                                   activation=activ,\n                                   input_shape=[inputs,]))\n    layer_list.append(layers.Dropout(rate=drpout))\n    \n    # Loop over the hidden layers\n    for i in range(len(n)-1):\n        layer_list.append(layers.Dense(units=n[i+1], activation=activ))\n        layer_list.append(layers.Dropout(rate=drpout))\n        \n    # Remove the last dropout layer\n    layer_list.pop()\n    # Change final activation function\n    layer_list[-1] = layers.Dense(units=2, activation='softmax')\n    \n    # Put it all together\n    return Sequential(layers=layer_list)","52d7581e":"from tensorflow.keras.callbacks import EarlyStopping\n\ndef train_model(model, epochs=5, verbose=False, valsplit=0.2):\n    # Define a callback for early stopping\n    callbacks = []\n    if (valsplit > 0.0):\n        early_stop = EarlyStopping(monitor='val_loss', patience=50)\n        callbacks.append(early_stop)\n        \n    # Compile the model with the appropriate loss function and optimizer\n    # Make sure to track accuracy\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(0.001),\n                  metrics=['accuracy'])\n    history = model.fit(feats, labels, \n                        validation_split=valsplit,\n                        epochs=epochs, verbose=verbose, callbacks=callbacks)\n    \n    # ================================================\n    # Everything below here is just for visualization\n    # ================================================\n    \n    hist = history.history\n    \n    # Print the final information\n    print(f\"   Train Acc.: {hist['accuracy'][-1]:0.2f}%\")\n    print(f\"   Train loss: {hist['loss'][-1]}\")\n    if (valsplit > 0.0):\n        print(f\"   Test Acc. : {hist['val_accuracy'][-1]:0.2f}%\")\n        print(f\"   Test loss : {hist['val_loss'][-1]}\")\n    \n    # Now plot the accuracy and loss over time\n    plt.subplot(211)\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.plot(hist['accuracy'], label='train acc.')\n    if (valsplit > 0.0):\n        plt.plot(hist['val_accuracy'], label='train acc.')\n        plt.legend();\n    \n    # Plot the loss\n    plt.subplot(212)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.yscale('log')\n    plt.plot(history.history['loss'], label='train loss')\n    plt.title('Loss')\n    if (valsplit > 0.0):\n        plt.plot(history.history['val_loss'], label='test loss')\n        plt.legend();\n    \n    return hist['val_accuracy'][-1]","cbfbd3f6":"# Some tracking stats for the best model\nmodel    = None\naccuracy = 0.0","334255f1":"def update_best(test_model, best_model, best_accuracy):\n    # Summary of model\n    print(test_model.summary())\n    \n    # Train and get model accuracy\n    acc = train_model(test_model, epochs=1000)\n    \n    # Update best model\n    if (acc > best_accuracy):\n        print(\"Model is better!\")\n        best_model = test_model\n        best_accuracy = acc\n    else:\n        print(\"Model not better :(\")\n        \n    return","5f5f6051":"inputs = len(feats.columns)\nprint(f'Num inputs: {inputs}')","557a14e9":"# Give it a try\nprint(\"Test 1:\")\nmodel1 = model_construct(inputs, n=[256])\nupdate_best(model1, model, accuracy)","9afe120e":"print(\"Test 2:\")\nupdate_best(model_construct(inputs, n=[256, 64]), model, accuracy)","ab151496":"print(\"Test 3:\")\nmodel3 = model_construct(inputs, n=[8], activ='relu')\nupdate_best(model3, model, accuracy)","839bffed":"model = model1\nprint(model.summary())\naccuracy = train_model(model, epochs=1000, valsplit=0.0)","43638de6":"# Load and process the testing data\ntest_df    = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_feats = format_data(test_df)\n\n# Compute the results\nresults = model.predict(test_feats)\nresults = [np.argmax(res) for res in results]\nprint(results[:10])\nplt.hist(results, bins=2, range=(-0.5,1.5), density=True)\n\n# Load it all into a dataframe\nsubmission_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], \n                              'Survived'   : results})\nsubmission_df.describe()","55f05724":"submission_df.to_csv('submission.csv', index=False)","174e6022":"## Format the data\nNext step is to format the data so that we can use it to actually train and test our data.","fd215aac":"## Building the model\nNow we need to build a model that is capable of being trained and generating predictions. For this attempt I will be using TensorFlow.\n\nNote that we will create a function for generating our model from a list of nodes per layer. This will help us to more easily tune these parameters as we search for the best model.","fb53ecdf":"Now we keep the model with the highest accuracy, but since they all have pretty much the same accuracy we will just take the simplest model.","47937f5e":"Let's see what the breakdown of our labels is between our two categories. Ideally it will be 50\/50, or really close to it.","6bbd1cdc":"The formatting that we will apply includes the following:\n* **One-hot encode**: 'Sex', 'Embarked'\n* **Remove**: 'Name', 'Ticket', 'Cabin'\n* **Fill *null* values** with the mean of the associated column.","f72c38b4":"Now get the features and labels for training.","7eece144":"And for training\/testing the model...","b64364ab":"## Submit the best result\nNow that we have the best result, we will get the predictions and submit them.","625868cc":"# Titanic: Machine Learning From Disaster\nIn this notebook I'm going to expand on my previous attempt that used scikit-learn random forests and try to use TensorFlow as the learning framework this time.\n\nStep 1: Load the modules and see what versions we have installed.","66823b98":"Ok, so that's not great, there's about 36% more labels in the first classification. If we leave our data this way, it has the potential to skew the classification algorithm towards the '0' label, since that seems to be the safest bet when the class is uncertain. so we need to remove entries from our training until we get to a more 50\/50 split."}}