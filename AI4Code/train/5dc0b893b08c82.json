{"cell_type":{"8e097e27":"code","7364b945":"code","17bf69cc":"code","5c5dcf12":"code","ee881b1f":"code","03e71395":"code","58a24eab":"code","47d86a15":"code","3a73d180":"code","a22c057f":"code","9fd427b8":"code","7c9e9d32":"code","68cf5dff":"code","5a83b858":"code","601b0726":"code","ed818c2d":"code","b5b6c615":"code","d7299090":"code","2323e1c9":"code","6de667a2":"code","f968a449":"code","b57f2531":"code","91ca3ac0":"code","a73100ca":"code","6651c454":"code","9a30fa72":"code","8bdbebde":"code","57bd2682":"code","94182060":"code","d825d675":"code","23da3522":"code","9113d126":"code","ceb904ce":"code","c773a512":"markdown","46e3c12e":"markdown","91d3d042":"markdown","69ddd785":"markdown","8bbaa703":"markdown","4247cbc4":"markdown","c63de43b":"markdown","6e567337":"markdown","c386ac44":"markdown","ac3645b6":"markdown","81abd7af":"markdown","71974bd0":"markdown","66ef7af4":"markdown","5b940071":"markdown","23d4cbb3":"markdown","e3b5e14e":"markdown","253865de":"markdown","d77b12da":"markdown","6ddf79c5":"markdown"},"source":{"8e097e27":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7364b945":"import pandas as pd \ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint('Data Loading is done!')","17bf69cc":"train.head()","5c5dcf12":"train.shape, test.shape","ee881b1f":"print(train.info())","03e71395":"print(test.info())","58a24eab":"print(test.head())","47d86a15":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(train['target'].value_counts())\nnews_class = train['target'].value_counts()\nlabels = ['Non-Disaster', 'Disaster']\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.bar(labels, news_class, color=['green', 'orange'])\n\nplt.show()","3a73d180":"disaster_tweet_len = train[train['target']==1]['text'].str.len()\nnon_disaster_tweet_len = train[train['target']==0]['text'].str.len()\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\nax[0].hist(disaster_tweet_len, color='green')\nax[0].set_title(\"Disaster Tweet Length\")\n\nax[1].hist(non_disaster_tweet_len, color='orange')\nax[1].set_title(\"Non Disaster Tweet Length\")\n\nfig.suptitle('All words in Tweets')\nplt.show()","a22c057f":"fig, ax = plt.subplots(1, 2, figsize=(12, 6))\nax[0].boxplot(disaster_tweet_len, labels=['counts'], showmeans=True)\nax[0].set_title(\"Disaster Tweet Length\")\n\nax[1].boxplot(non_disaster_tweet_len, labels=['counts'], showmeans=True)\nax[1].set_title(\"Non Disaster Tweet Length\")\n\nfig.suptitle('All words in Tweets')\nplt.show()","9fd427b8":"import numpy as np\ndisaster_tweet_len = train[train['target']==1]['text'].str.len()\nnon_disaster_tweet_len = train[train['target']==0]['text'].str.len()\n\nprint(\"Max Length of Disaster Tweet: {}\".format(np.max(disaster_tweet_len)))\nprint(\"Min Length of Disaster Tweet: {}\".format(np.min(disaster_tweet_len)))\nprint(\"Mean Length of Disaster Tweet: {:.2f}\".format(np.mean(disaster_tweet_len)))\nprint(\"Median Length of Disaster Tweet: {}\".format(np.median(disaster_tweet_len)))\n\nprint(\"Max Length of Non Disaster Tweet: {}\".format(np.max(non_disaster_tweet_len)))\nprint(\"Min Length of Non Disaster Tweet: {}\".format(np.min(non_disaster_tweet_len)))\nprint(\"Mean Length of Non Disaster Tweet: {:.2f}\".format(np.mean(non_disaster_tweet_len)))\nprint(\"Median Length of Non Disaster Tweet: {}\".format(np.median(non_disaster_tweet_len)))","7c9e9d32":"from wordcloud import WordCloud, STOPWORDS\n\ndisaster_tweet_keywords = dict(train[train['target']==1]['keyword'].value_counts())\nnon_disaster_tweet_keywords = dict(train[train['target']==0]['keyword'].value_counts())\n\nstopwords = set(STOPWORDS)\ndisaster_wordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").\\\ngenerate_from_frequencies(disaster_tweet_keywords)\nnon_disaster_wordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").\\\ngenerate_from_frequencies(non_disaster_tweet_keywords)\n\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 10))\nax[0].imshow(disaster_wordcloud, interpolation='bilinear')\nax[0].axis('off')\nax[0].set_title(\"Disaster Tweet\")\nax[1].imshow(non_disaster_wordcloud, interpolation='bilinear')\nax[1].axis('off')\nax[1].set_title(\"Non Disaster Tweet\")\nfig.show()","68cf5dff":"import pandas as pd\ndef check_na(data):\n  isnull_na = (data.isnull().sum() \/ len(data)) * 100\n  data_na = isnull_na.drop(isnull_na[isnull_na == 0].index).sort_values(ascending=False)\n  missing_data = pd.DataFrame({'Missing Ratio': data_na, \n                               'Data Type': data.dtypes[data_na.index]})\n  print(\"\uacb0\uce21\uce58 \ub370\uc774\ud130 \uceec\ub7fc\uacfc \uac74\uc218:\\n\", missing_data)\n\ncheck_na(train)\ncheck_na(test)","5a83b858":"test_id = test['id']\n\nfor datas in [train, test]:\n  datas = datas.drop(['id', 'keyword', 'location'], axis=1, inplace=True)\n\ntrain.shape, test.shape","601b0726":"import re\n\ndef remove_url(text):\n  url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n  return url.sub(r'', text)\n\nsample_text = \"\uc0c8\ub85c\uc6b4 \uce90\uae00 \ub300\ud68c\uac00 \uc5f4\ub838\uc2b5\ub2c8\ub2e4. \uc8fc\uc18c: https:\/\/www.kaggle.com\/c\/nlp-getting-started\"\nremove_url(sample_text)","ed818c2d":"def remove_html(text):\n  html = re.compile(r'<.*?>')\n  return html.sub(r'', text)\n\nsample_text =\"\"\"<div>\n<h1> Real News or Fake News <\/h1>\n<p> Kaggle Machine Learning <\/p>\n<\/div>\"\"\"\n\nprint(remove_html(sample_text))","b5b6c615":"!pip install emoji --upgrade","d7299090":"import emoji\nprint(emoji.emojize('Phd is very easy!!! :thumbs_up:'))","2323e1c9":"def remove_emoji(text):\n  emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n  return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Hello, \ud83d\udc4d\")","6de667a2":"def remove_punct(text):\n  return re.sub(\"[^a-zA-Z]\", \" \", text)\n\nsample_text = \"Hello!, Can I have one question?.., Is it #Outbreak?\"\nremove_punct(sample_text)","f968a449":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nprint(\"Total Length of stopwords:\", len(stopwords.words('english')))\nprint(stopwords.words('english')[:10])","b57f2531":"import string\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\ndef data_cleansing(text, remove_stopwords = False):\n  # remove url \n  url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n  cleaned_text = url.sub(r'', text)\n\n  # remove html\n  html = re.compile(r'<.*?>')\n  cleaned_text = html.sub(r'', cleaned_text)\n\n  # remove emoji\n  emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n  cleaned_text = emoji_pattern.sub(r'', cleaned_text)\n\n  # Special Letters to empty space\n  cleaned_text = re.sub(\"[^a-zA-Z]\", \" \", cleaned_text)\n\n  # Lowercase\n  cleaned_text = cleaned_text.lower().split()\n\n  if remove_stopwords:\n    stops = set(stopwords.words(\"english\"))\n    cleaned_text = [word for word in cleaned_text if not word in stops]\n    clean_review = ' '.join(cleaned_text)\n  else:\n    clean_review = ' '.join(cleaned_text)\n\n  return clean_review","91ca3ac0":"clean_train_reviews = []\nfor datas in [train, test]:\n    datas['cleaned_text'] = datas['text'].apply(lambda x : data_cleansing(x, remove_stopwords=True))\n\ntrain.head(5)","a73100ca":"test.head(5)","6651c454":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['As you know, I want to be with you']\nvector = CountVectorizer()\nprint(vector.fit_transform(corpus).toarray()) \nprint(vector.vocabulary_)","9a30fa72":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['As you know, I want to be with you', \n          'Thank you, but I cannot be with you']\nvector = CountVectorizer()\nprint(vector.fit_transform(corpus).toarray()) \nprint(vector.vocabulary_)","8bdbebde":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = ['Can I have lunch with you?', \n          'No, I cannot have it with you.', \n          'Because, I need to study later']\ntfidfv = TfidfVectorizer().fit(corpus)\nprint(np.round(tfidfv.transform(corpus).toarray(), 2))\nprint(tfidfv.vocabulary_)","57bd2682":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc","94182060":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(min_df = 0.0, analyzer='char', sublinear_tf=True, ngram_range=(1, 3), max_features = 10000)\nX = vectorizer.fit_transform(train['cleaned_text']).todense()\ny = train['target'].values","d825d675":"print(X.shape)\nprint(y.shape)","23da3522":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","9113d126":"from sklearn.linear_model import LogisticRegression\nlgs = LogisticRegression(class_weight = 'balanced')\nlgs.fit(X_train, y_train)","ceb904ce":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nX_testset = vectorizer.transform(test['cleaned_text']).todense()\nprint(\"The Shape of Test Dataset:\", X_testset.shape)\n\ny_test_pred = lgs.predict(X_testset)\nprint(\"The Predict Value:\", y_test_pred)\ny_test_pred = np.where(y_test_pred >= 0.5, 1, 0)\nprint(\"The Predict Class:\", y_test_pred)\n\nsubmission_file = pd.DataFrame({'id': test_id, 'target': y_test_pred})\nprint(submission_file.head())\n\nsubmission_file.to_csv('submission.csv', index = False)","c773a512":"- \uc2e4\uc81c \uc8fc\uc694 \ud0a4\uc6cc\ub4dc\ub97c \ubcf4\uba74 \ucc28\uc774\uc810\uc774 \uc788\uc5b4 \ubcf4\uc778\ub2e4. \n    + Disaster Tweet: Outbreak, wreckage\n    + Non Disaster Tweet:body%20bags, ","46e3c12e":"## Remove Special Letters\n- \uae30\uc874 \ud2b9\uc218 \ubb38\uc790\ub294 \uc81c\uac70\ud558\ub294 \ud568\uc218\ub97c \uc791\uc131\ud55c\ub2e4. ","91d3d042":"#### \ubd88\uc6a9\uc5b4 \uc81c\uac70\n- \ubd88\uc6a9\uc5b4\ub97c \uc81c\uac70\ud558\uae30 \uc704\ud574\uc11c\ub294 \ubcc4\ub3c4\uc758 \ub77c\uc774\ube0c\ub7ec\ub9ac\uac00 \ud544\uc694\ud558\ub2e4. ","69ddd785":"- \ubaa8\ub378\ub9c1\uc5d0 \ud544\uc694 \uc5c6\ub294 \ubcc0\uc218\ub4e4\uc740 \uc81c\uac70\ud558\uc600\ub2e4. ","8bbaa703":"### data cleansing \n- \ub370\uc774\ud130 \uc804\ucc98\ub9ac\ub97c \uc804\uccb4 \ud568\uc218\ub97c \uc791\uc131\ud55c\ub2e4. ","4247cbc4":"## EDA (Exploratory Data Analysis)\n- Target \ub370\uc774\ud130\uc5d0 \ub300\ud574 \uc2dc\uac01\ud654\ub97c \ud574\ubcf4\ub3c4\ub85d \ud55c\ub2e4. ","c63de43b":"- Non Disaster \ub370\uc774\ud130\uc640 Disaster \ub370\uc774\ud130\ub97c \ube44\uad50\ud574\ubcf8\ub2e4.\n- \uc804\uccb4\uc801\uc73c\ub85c \uc9c4\uc9dc \uc7ac\ub09c \ub274\uc2a4\uc758 \ub370\uc774\ud130\uc758 \ud3c9\uade0\uc801\uc778 \uae38\uc774\uac00 \uae38\uc5b4\ubcf4\uc774\ub294 \uacbd\ud5a5\uc774 \uc788\ub2e4. ","6e567337":"### TfidfVectorizer","c386ac44":"## \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\n- \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc624\ub3c4\ub85d \ud55c\ub2e4. ","ac3645b6":"#### remove_emoji\n- \uc774\ubc88\uc5d0\ub294 \ud14d\uc2a4\ud2b8\uc5d0 \uc788\ub294 emoji\ub97c \uc81c\uac70\ud558\ub294 \ucf54\ub4dc\ub97c \uc791\uc131\ud55c\ub2e4.  \n","81abd7af":"### Modeling","71974bd0":"#### remove_url\n- url\ub9cc \uc81c\uac70\ud558\ub294 \ud568\uc218\ub97c \ub9cc\ub4e0\ub2e4. ","66ef7af4":"### Text Cleansing\n- HTML \ud0dc\uadf8 \uc81c\uac70\n- \ud2b9\uc218\ubb38\uc790 \uacf5\ubc31\uc73c\ub85c \ubc14\uafb8\uae30\n- \ub300\ubb38\uc790 \uc18c\ubb38\uc790\ub85c \ubc14\uafbc \ud6c4, \ub9ac\uc2a4\ud2b8\ub85c \ub9cc\ub4e4\uae30\n- \ubd88\uc6a9\uc5b4 \uc81c\uac70\ud558\uae30","5b940071":"- \uc774\uc81c \uac01 \ub370\uc774\ud130\uc5d0 \ubc18\ubcf5\ubb38\uc744 \ud65c\uc6a9\ud558\uc5ec \ud14d\uc2a4\ud2b8 \uc804\ucc98\ub9ac\ub97c \uc2dc\ud589\ud55c\ub2e4. \n- \uae30\uc874 text\uc640 cleaned_text\ub97c \ube44\uad50\ud574\ubcf8\ub2e4. ","23d4cbb3":"## Text Transformation Vectorizer\n- \ud14d\uc2a4\ud2b8\ub294 \ubb38\uc790\uc774\uae30 \ub54c\ubb38\uc5d0, \uc774\uc81c \uc774\ub97c \uc22b\uc790\ub85c \ubcc0\uacbd\ud574\uc57c \ud55c\ub2e4. ","e3b5e14e":"## Feature Engineering","253865de":"### Count Vectorizer\n- CountVectorizer\ub294 \uac01 \ud14d\uc2a4\ud2b8\ub97c \uc54c\ud30c\ubcb3\uc73c\ub85c \uc815\ub82c\ud55c \ud6c4, \ube48\ub3c4\uc218\ub85c \uacc4\uc0b0\ud55c\ub2e4. ","d77b12da":"### \uacb0\uce21\uce58 \ud655\uc778\n- \ud14d\uc2a4\ud2b8\uc5d0\ub294 \uacb0\uce21\uce58\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc74c","6ddf79c5":"#### remove_html\n- \uc774\ubc88\uc5d0\ub294 html \ucf54\ub4dc\ub97c \uc81c\uac70\ud558\ub294 \uc815\uaddc\ud45c\ud604\uc2dd\uc744 \ud655\uc778\ud558\uace0, \uc2e4\uc81c html \ud0dc\uadf8\uac00 \uc5c6\uc5b4\uc9c0\ub294\uc9c0 \ud655\uc778\ud55c\ub2e4. "}}