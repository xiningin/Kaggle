{"cell_type":{"6826fcc4":"code","aa59e8c4":"code","978a3070":"code","24337fa6":"code","faadfd35":"code","3f265587":"code","61b17653":"code","3417585e":"code","218f77db":"code","8a6b4893":"code","17c5ab4b":"code","1a27a924":"code","cbd3f15a":"code","5f5b3a1f":"code","8eb93e35":"code","daa6d035":"code","d3fcd0d4":"code","2581da9d":"code","e9a46ef3":"code","5cec237a":"code","29a15d77":"code","d90fe061":"code","68dce7a2":"code","4d97530a":"code","812c2d38":"code","df041b05":"code","f5317038":"code","36294726":"code","9973d183":"markdown","410bbcde":"markdown","49627d01":"markdown","2f9beb71":"markdown","4b1fa0a0":"markdown","374de2f8":"markdown","33dfb61b":"markdown","780b4e31":"markdown","f04e7b9b":"markdown","6b43a45f":"markdown","3159bb4e":"markdown","3320e22e":"markdown","415f2732":"markdown","fdc9ec07":"markdown","db156eff":"markdown","9ff05b6a":"markdown","d35214fa":"markdown","04624caf":"markdown","55ba12c1":"markdown","d326ef2c":"markdown","4cbeaa00":"markdown","44f20779":"markdown","dcc94123":"markdown","6c698371":"markdown","4a33d1d5":"markdown","7af6f237":"markdown","a5f4b184":"markdown","0c9b60ff":"markdown","2a3eb54a":"markdown","bb43e034":"markdown","dcceaf6c":"markdown","8fa36830":"markdown","20d2bf98":"markdown","7f8d2e4b":"markdown","5b24773a":"markdown","2ef9fe44":"markdown","c186692a":"markdown","8d29d0cf":"markdown","b7370e90":"markdown","83292b29":"markdown","f812e1aa":"markdown","6be15f52":"markdown","da7695a3":"markdown"},"source":{"6826fcc4":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n%matplotlib inline","aa59e8c4":"x = torch.linspace(0,799,800)","978a3070":"y = torch.sin(x*2*3.1416\/40)","24337fa6":"plt.figure(figsize=(12,4))\nplt.xlim(-10,801)\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"sin\")\nplt.title(\"Sin plot\")\nplt.plot(y.numpy(),color='#8000ff')\nplt.show()","faadfd35":"test_size = 40\ntrain_set = y[:-test_size]\ntest_set = y[-test_size:]","3f265587":"plt.figure(figsize=(12,4))\nplt.xlim(-10,801)\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"sin\")\nplt.title(\"Sin plot\")\nplt.plot(train_set.numpy(),color='#8000ff')\nplt.plot(range(760,800),test_set.numpy(),color=\"#ff8000\")\nplt.show()","61b17653":"def input_data(seq,ws):\n    out = []\n    L = len(seq)\n    \n    for i in range(L-ws):\n        window = seq[i:i+ws]\n        label = seq[i+ws:i+ws+1]\n        out.append((window,label))\n    \n    return out","3417585e":"window_size = 40\ntrain_data = input_data(train_set, window_size)\nlen(train_data)","218f77db":"train_data[0]","8a6b4893":"class LSTM(nn.Module):\n    \n    def __init__(self,input_size = 1, hidden_size = 50, out_size = 1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(input_size, hidden_size)\n        self.linear = nn.Linear(hidden_size,out_size)\n        self.hidden = (torch.zeros(1,1,hidden_size),torch.zeros(1,1,hidden_size))\n    \n    def forward(self,seq):\n        lstm_out, self.hidden = self.lstm(seq.view(len(seq),1,-1), self.hidden)\n        pred = self.linear(lstm_out.view(len(seq),-1))\n        return pred[-1]","17c5ab4b":"torch.manual_seed(42)\nmodel = LSTM()\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","1a27a924":"model","cbd3f15a":"epochs = 10\nfuture = 40\n\nfor i in range(epochs):\n    \n    for seq, y_train in train_data:\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size),\n                       torch.zeros(1,1,model.hidden_size))\n        \n        y_pred = model(seq)\n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        \n    print(f\"Epoch {i} Loss: {loss.item()}\")\n    \n    preds = train_set[-window_size:].tolist()\n    for f in range(future):\n        seq = torch.FloatTensor(preds[-window_size:])\n        with torch.no_grad():\n            model.hidden = (torch.zeros(1,1,model.hidden_size),\n                           torch.zeros(1,1,model.hidden_size))\n            preds.append(model(seq).item())\n        \n    loss = criterion(torch.tensor(preds[-window_size:]), y[760:])\n    print(f\"Performance on test range: {loss}\")\n    \n    plt.figure(figsize=(12,4))\n    plt.xlim(700,801)\n    plt.grid(True)\n    plt.plot(y.numpy(),color='#8000ff')\n    plt.plot(range(760,800),preds[window_size:],color='#ff8000')\n    plt.show()","5f5b3a1f":"df = pd.read_csv(\"\/kaggle\/input\/for-simple-exercises-time-series-forecasting\/Alcohol_Sales.csv\", index_col = 0, parse_dates = True)\ndf.head()","8eb93e35":"df.dropna(inplace=True)\nlen(df)","daa6d035":"plt.figure(figsize = (12,4))\nplt.title('Alcohol Sales')\nplt.ylabel('Sales in million dollars')\nplt.grid(True)\nplt.autoscale(axis='x',tight=True)\nplt.plot(df['S4248SM144NCEN'],color='#8000ff')\nplt.show()","d3fcd0d4":"#extracting the time series values\ny = df['S4248SM144NCEN'].values.astype(float) \n\n#defining a test size\ntest_size = 12\n\n#create train and test splits\ntrain_set = y[:-test_size]\ntest_set = y[-test_size:]\ntest_set","2581da9d":"from sklearn.preprocessing import MinMaxScaler\n\n# instantiate a scaler\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\n# normalize the training set\ntrain_norm = scaler.fit_transform(train_set.reshape(-1, 1))","e9a46ef3":"# convert train_norm to a tensor\ntrain_norm = torch.FloatTensor(train_norm).view(-1)\n\n# define a window size\nwindow_size = 12\n# define a function to create sequence\/label tuples\ndef input_data(seq,ws):\n    out = []\n    L = len(seq)\n    for i in range(L-ws):\n        window = seq[i:i+ws]\n        label = seq[i+ws:i+ws+1]\n        out.append((window,label))\n    return out\n\n# apply input_data to train_norm\ntrain_data = input_data(train_norm, window_size)\nlen(train_data)","5cec237a":"train_data[0]","29a15d77":"class LSTMnetwork(nn.Module):\n    def __init__(self,input_size=1,hidden_size=100,output_size=1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # add an LSTM layer:\n        self.lstm = nn.LSTM(input_size,hidden_size)\n        \n        # add a fully-connected layer:\n        self.linear = nn.Linear(hidden_size,output_size)\n        \n        # initializing h0 and c0:\n        self.hidden = (torch.zeros(1,1,self.hidden_size),\n                       torch.zeros(1,1,self.hidden_size))\n\n    def forward(self,seq):\n        lstm_out, self.hidden = self.lstm(\n            seq.view(len(seq),1,-1), self.hidden)\n        pred = self.linear(lstm_out.view(len(seq),-1))\n        return pred[-1]","d90fe061":"torch.manual_seed(42)\n\n# instantiate\nmodel = LSTMnetwork()\n\n# loss\ncriterion = nn.MSELoss()\n\n#optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nmodel","68dce7a2":"epochs = 100\n\nimport time\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    for seq, y_train in train_data:\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size),\n                        torch.zeros(1,1,model.hidden_size))\n        \n        y_pred = model(seq)\n        \n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        \n    print(f'Epoch: {epoch+1:2} Loss: {loss.item():10.8f}')\n    \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","4d97530a":"future = 12\n\npreds = train_norm[-window_size:].tolist()\n\nmodel.eval()\n\nfor i in range(future):\n    seq = torch.FloatTensor(preds[-window_size:])\n    with torch.no_grad():\n        model.hidden = (torch.zeros(1,1,model.hidden_size),\n                        torch.zeros(1,1,model.hidden_size))\n        preds.append(model(seq).item())\npreds[window_size:]","812c2d38":"df['S4248SM144NCEN'][-12:]","df041b05":"true_predictions = scaler.inverse_transform(np.array(preds[window_size:]).reshape(-1, 1))\ntrue_predictions","f5317038":"x = np.arange('2018-02-01', '2019-02-01', dtype='datetime64[M]').astype('datetime64[D]')\nplt.figure(figsize=(12,4))\nplt.title('Alcohol Sales')\nplt.ylabel('Sales in million dollars')\nplt.grid(True)\nplt.autoscale(axis='x',tight=True)\nplt.plot(df['S4248SM144NCEN'], color='#8000ff')\nplt.plot(x,true_predictions, color='#ff8000')\nplt.show()","36294726":"fig = plt.figure(figsize=(12,4))\nplt.title('Alcohol Sales')\nplt.ylabel('Sales in million dollars')\nplt.grid(True)\nplt.autoscale(axis='x',tight=True)\nfig.autofmt_xdate()\n\nplt.plot(df['S4248SM144NCEN']['2017-01-01':], color='#8000ff')\nplt.plot(x,true_predictions, color='#ff8000')\nplt.show()","9973d183":"##### 5.2.3 Prepare data for LSTM model","410bbcde":"While working with LSTM models, we divide the training sequence into series of overlapping windows. The label used for comparison is the next value in the sequence.\n\nFor example if we have series of of 12 records and a window size of 3, we feed [x1, x2, x3] into the model, and compare the prediction to `x4`. Then we backdrop, update parameters, and feed [x2, x3, x4] into the model and compare the prediction to `x5`. To ease this process, I'm defining a function `input_data(seq,ws)` that created a list of (seq,labels) tuples. If `ws` is the window size, then the total number of (seq,labels) tuples will be `len(series)-ws`.","49627d01":"#### 3.1 Splitting the data in train\/test set <a id=7><\/a>","2f9beb71":"##### 5.1.1 Importing the data","4b1fa0a0":"Table of Contents: <a id=100><\/a>\n\n1. [Packages](#1)\n2. [Data definition](#2)\n    - 2.1 [Declaring a tensor `x`](#3)\n    - 2.2 [Creating a tensor `y` as a sin function of `x`](#4)\n    - 2.3 [Plotting `y`](#5)\n3. [Batching the data](#6)\n    - 3.1 [Splitting the data in train\/test set](#7)\n    - 3.2 [Creating the batches of data](#8)\n4. [Defining the model](#9)\n    - 4.1 [Model class](#10)\n    - 4.2 [Model instantiation](#11)\n    - 4.3 [Training](#12)\n5. [Alcohol Sales dataset](#13)\n    - 5.1 [Loading and plotting](#14)\n    - 5.2 [Prepare and normalize](#15)\n    - 5.3 [Modelling](#16)\n    - 5.4 [Predictions](#17)","374de2f8":"#### 5.2 Prepare and normalize <a id=15><\/a>","33dfb61b":"#### 2.2 Creating a tensor `y` as a sin function of `x` <a id=4><\/a>","780b4e31":"##### 5.5.5 Zooming the test predictions","f04e7b9b":"In this notebook, I'm going to train a very simple LSTM model, which is a type of RNN architecture to do time series prediction. Given some input data, it should be able to generate a prediction for the next step. I'll be using a **Sin** wave as an example as it's very easy to visualiase the behaviour of a sin wave.\n","6b43a45f":"##### 5.1.2 Dropping the empty rows","3159bb4e":"#### 4.1 Model Class <a id=10><\/a>","3320e22e":"##### 5.1.3 Plotting the Time Series Data","415f2732":"#### 5.1 Loading and plotting <a id=14><\/a>","fdc9ec07":"### 3. Batching the data <a id=6><\/a>\n[back to top](#100)","db156eff":"##### 3.1.1 Plotting the training\/testing set","9ff05b6a":"##### 5.4.3 Inverting the normalised values","d35214fa":"##### 5.2.2 Normalize the data","04624caf":"##### 5.2.4 Printing the first tuple","55ba12c1":"##### 5.3.1 Model definition","d326ef2c":"##### 5.3.4 Training","4cbeaa00":"##### 5.2.1 Preparing the data","44f20779":"### 1. Packages <a id=1><\/a>\n[back to top](#100)","dcc94123":"#### 5.3 Modelling <a id=16><\/a>","6c698371":"##### 3.2.1 Calling the `input_data` function\nThe length of `x` = 800\n\nThe length of `train_set` = 800 - 40 = 760\n\nThe length of `train_data` = 760 - 40 - 720","4a33d1d5":"#### 3.2 Creating the batches of data <a id=8><\/a>","7af6f237":"<h1><center>Recurrent Neural Network in PyTorch<\/center><\/h1>","a5f4b184":"During training, I'm visualising the prediction process for the test data on the go. It will give a better understanding of how the training is being carried out in each epoch. The training sequence is represented in <span style=\"color:#8000ff\">purple<\/span> while the predicted sequence in represented in <span style=\"color:#ff8000\">orange<\/span>.","0c9b60ff":"### 5. Alcohol Sales dataset <a id=13><\/a>\n[back to top](#100)","2a3eb54a":"### If you liked the notebook, consider giving an upvote.\n[back to top](#100)","bb43e034":"Recurrent Neural Networks are a type of neural networks that are designed to work on sequence prediction models. RNNs can be used for text data, speech data, classification problems and generative models. Unlike ANNs, RNNs' prediction are based on the past prediction as well as the current input. RNNs are networks with loops in them allowing information to persist.\n\nEach node of an **RNN** consists of 2 inputs:\n1. Memory unit\n2. Event unit\n\n`M(t-1)` is the memory unit or the output of the previous prediction. `E(t)` is the current event or the information being provided at the present time. `M(t)` is the output of the current node or the output at the present time in the sequence.","dcceaf6c":"### 2. Data definition <a id=2><\/a>\n[back to top](#100)","8fa36830":"##### 4.2.1 Printing the model","20d2bf98":"#### 4.3 Training <a id = 12><\/a>","7f8d2e4b":"#### 2.3 Plotting `y` <a id=5><\/a>","5b24773a":"##### 5.4.2 Original test set","2ef9fe44":"#### 5.4 Predictions <a id=17><\/a>","c186692a":"##### 5.3.3 Instantiation, loss and optimizer","8d29d0cf":"### 4. Defining the model <a id=9><\/a>\n[back to top](#100)","b7370e90":"#### 4.2 Model Instantiation <a id = 11><\/a>","83292b29":"##### 5.4.1 Test set predictions","f812e1aa":"#### 2.1 Declaring a tensor `x` <a id=3><\/a>","6be15f52":"##### 5.4.4 Plotting","da7695a3":"##### 3.2.2 Checking the 1st value from train_data"}}