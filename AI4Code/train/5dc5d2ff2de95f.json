{"cell_type":{"80ff1494":"code","8df313ca":"code","1d3d41a8":"code","b17f5d90":"code","eec22bc7":"code","a0beebe4":"code","240e00b4":"code","c05d61b0":"code","9388d468":"code","b348958c":"code","eb872061":"code","d933704e":"code","a845e199":"code","0bfad95d":"code","34503743":"code","27cfa26c":"code","eed6c74f":"code","0df14c42":"code","342e1ace":"code","9b5965cb":"code","a1865f5c":"code","00fd03b9":"code","af36ef92":"code","7f3f6255":"markdown","be93d30e":"markdown","15327245":"markdown","dd3faa63":"markdown","d6a676ba":"markdown","79fd485c":"markdown","9f592216":"markdown","bea61451":"markdown","71699484":"markdown","765311d9":"markdown","fa79fe31":"markdown"},"source":{"80ff1494":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input \\data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8df313ca":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1d3d41a8":"df = pd.read_csv('\/kaggle\/input\/medical-insurance-premium-prediction\/Medicalpremium.csv')\ndf.head()","b17f5d90":"df.info()\n","eec22bc7":"df.columns","a0beebe4":"## Target Variable \nsns.kdeplot(df.PremiumPrice)","240e00b4":"for i in df.columns:\n    print(i, len(df[i].unique()))","c05d61b0":"binary_feature = [i for i in df.columns if (len(df[i].unique()) == 2)]\ntarget_feature = 'PremiumPrice'\ndisceate_feature = [i for i in df.columns if (i not in binary_feature) and (i not in target_feature)]","9388d468":"fig, ax = plt.subplots(5,2, figsize = (20,30))\nsns.countplot(ax =ax[0,0], data = df , x = binary_feature[0])\nsns.barplot(ax =ax[0,1], data = df , x = binary_feature[0], y = target_feature)\nsns.countplot(ax =ax[1,0], data = df , x = binary_feature[1])\nsns.barplot(ax =ax[1,1], data = df , x = binary_feature[1], y = target_feature)\nsns.countplot(ax =ax[2,0], data = df , x = binary_feature[2])\nsns.barplot(ax =ax[2,1], data = df , x = binary_feature[2], y = target_feature)\nsns.countplot(ax =ax[3,0], data = df , x = binary_feature[3])\nsns.barplot(ax =ax[3,1], data = df , x = binary_feature[3], y = target_feature)\nsns.countplot(ax =ax[4,0], data = df , x = binary_feature[4])\nsns.barplot(ax =ax[4,1], data = df , x = binary_feature[4], y = target_feature)\n# sns.countplot(ax =ax[5,0], data = df , x = binary_feature[5])\/\/\n# sns.barplot(ax =ax[5,1], data = df , x = binary_feature[5], y = target_feature)","b348958c":"disceate_feature","eb872061":"fig, ax = plt.subplots(4,2, figsize = (20,15))\nsns.countplot(ax =ax[0,0], data = df , x = disceate_feature[0])\nsns.barplot(ax =ax[0,1], data = df , x = disceate_feature[0], y = target_feature)\nsns.countplot(ax =ax[1,0], data = df , x = disceate_feature[1])\nsns.barplot(ax =ax[1,1], data = df , x = disceate_feature[1], y = target_feature)\nsns.countplot(ax =ax[2,0], data = df , x = disceate_feature[2])\nsns.barplot(ax =ax[2,1], data = df , x = disceate_feature[2], y = target_feature)\nsns.countplot(ax =ax[2,0], data = df , x = disceate_feature[2])\nsns.barplot(ax =ax[2,1], data = df , x = disceate_feature[2], y = target_feature)\nsns.countplot(ax =ax[3,0], data = df , x = disceate_feature[3])\nsns.barplot(ax =ax[3,1], data = df , x = disceate_feature[3], y = target_feature)","d933704e":"plt.figure(figsize =(20,8))\nsns.heatmap(df.corr(), annot =True)","a845e199":"df['bmi'] = (df.Weight)\/(((df.Height)\/100)**2)","0bfad95d":"len(df.columns)","34503743":"fig, ax = plt.subplots(3,2, figsize = (20,15))\nsns.boxplot(ax =ax[0,0], data = df , x = 'Age')\nsns.boxplot(ax =ax[0,1], data = df , x = \"Height\")\nsns.boxplot(ax =ax[1,0], data = df , x = \"Weight\")\nsns.boxplot(ax =ax[1,1], data = df , x = \"NumberOfMajorSurgeries\")\nsns.boxplot(ax =ax[2,0], data = df , x = \"PremiumPrice\")\nsns.boxplot(ax =ax[2,1], data = df , x = 'bmi')\ncols = [i for i  in df.columns if i not in binary_feature]\ncols","27cfa26c":"from sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nX = df.drop('PremiumPrice', axis =1)\ny =df.PremiumPrice","eed6c74f":"model = SelectFromModel(Lasso(alpha = 0.05)).fit(X,y)","0df14c42":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","342e1ace":"scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_scaled = scaler.transform(X)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","9b5965cb":"scores = []\n\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n\nprint(np.mean(scores))","a1865f5c":"scores = []\n# on scaled data\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n\nprint(np.mean(scores))","00fd03b9":"scores = []\nfrom sklearn.ensemble import RandomForestRegressor\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2)\n    \n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n\nprint(np.mean(scores))","af36ef92":"import numpy as np\n\ntarget= np.array(df['PremiumPrice'])\nfeatures = df.drop('PremiumPrice', axis = 1)\nfeature_list = list(features.columns)\nfeatures = np.array(features)\n\n## RANDOM FOREST - KFOLD AND MODEL \n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\n    \nkf = KFold(n_splits=10,random_state=42,shuffle=True)\naccuracies = []\nfor train_index, test_index in kf.split(features):\n\n    data_train   = features[train_index]\n    target_train = target[train_index]\n\n    data_test    = features[test_index]\n    target_test  = target[test_index]\n\n    rf = RandomForestRegressor(n_estimators = 10, \n                               random_state = 42, \n                               criterion = 'mse',\n                               bootstrap=True)\n    \n    rf.fit(data_train, target_train)\n\n    predictions = rf.predict(data_test)\n\n    errors = abs(predictions - target_test)\n\n    print('Mean Absolute Error:', round(np.mean(errors), 2))\n    \n    mape = 100 * (errors \/ target_test)\n    accuracy = 100 - np.mean(mape)\n    print('Accuracy:', round(accuracy, 2), '%.')\n    print('train  accuracy  ', np.sqrt(mean_squared_error(target_train, rf.predict(data_train))))\n    print('train  accuracy  ', np.sqrt(mean_squared_error(target_test, rf.predict(data_test))))\n    print('r2score train ', r2_score(target_train, rf.predict(data_train)))\n    print('r2score test ' , r2_score(target_test, rf.predict(data_test)))\n    \n    \n\n    accuracies.append(accuracy)\n\naverage_accuracy = np.mean(accuracies)\nprint('Average accuracy:', average_accuracy)","7f3f6255":"## Model building","be93d30e":"`Above data has no value misssing & all values are in integer. \n","15327245":"We have sitution related to outlier . because of having less amount of data we can not drop samples.","dd3faa63":"## checking for correlations","d6a676ba":"Above graph shows following basic conclusions:\n1. As age & number of surgeries increases the premiunprice also increase.\n2. we don't conclude relationship of Height or weight to the target variable. ","79fd485c":"### Working on Binary features ","9f592216":"Based on visualzation following point may be colcluded.\n1. If you have a disease the premium price is higher than others\n2. count of Blood pressure & diabities patient is higher than other list of disease","bea61451":"1. All features \n2. Target Variable \n3. Primary correlations using heatmap(sns)\n","71699484":"## Knowing about data & target Varible. ","765311d9":"## Deriving new features ","fa79fe31":"## Target Feature"}}