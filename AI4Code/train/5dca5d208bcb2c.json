{"cell_type":{"92c4c607":"code","de489512":"code","e91570b2":"code","56dbbea3":"code","e74394fe":"code","1896e8f6":"code","df7a6d82":"code","9c90519c":"code","c95c5bcd":"code","554128e9":"code","3788f75d":"code","122f5945":"code","7e5a3140":"code","4c31b404":"code","9f6ac07b":"code","7e463975":"code","d51a51c9":"code","829c1230":"code","6b0837bf":"code","6075fc67":"code","e463cf4e":"code","0b4e5964":"code","f38981fe":"code","462498fc":"code","d53fa4f4":"code","f3a98716":"code","f661c1c3":"code","7ceadba1":"code","6a59a13f":"code","cbbd4105":"code","fb9c15b7":"code","f0e45042":"code","96ba7588":"code","6d8e14c8":"code","0d65d2ea":"code","514387b6":"code","099549d1":"code","c5b711cf":"code","85d1d622":"code","a204361a":"code","7ec6f66b":"code","b613bc5d":"code","c157f679":"code","cc049505":"code","df88cf79":"code","a1433965":"code","eff5e4b1":"code","326eaa24":"code","762a3c1d":"code","b9a9c682":"code","170f03c6":"code","dc8f2f09":"code","00641097":"code","dc85729a":"code","7ad32988":"code","9f7b5388":"code","d794a0db":"code","3709ff5d":"code","adc0ea62":"code","6dcdb957":"code","424876ea":"code","8b97a04d":"code","44674042":"code","6efb9d67":"code","86468132":"code","0d454a34":"code","6f1c13d1":"code","e40f63ee":"code","2069527d":"code","2e22935c":"code","3cc28ec7":"code","65134096":"code","7858fd43":"code","a72baa27":"markdown","8196e720":"markdown","f0558f7c":"markdown","460bb722":"markdown","8c10327e":"markdown","a66e4749":"markdown","c87ade53":"markdown","2bd49e21":"markdown","5ea2432f":"markdown","674d6cc6":"markdown","85c76942":"markdown","a3ed27b9":"markdown","db487cab":"markdown","040fc150":"markdown","ba049f3b":"markdown","3fc004c1":"markdown","8fc169d6":"markdown","af7cd1e4":"markdown","1303f9e4":"markdown","c4b3ec7d":"markdown","7f60e97f":"markdown","2fc51105":"markdown","18580e20":"markdown","b410f645":"markdown","0d044d6f":"markdown","3e703d9e":"markdown","331795ba":"markdown","128048c9":"markdown"},"source":{"92c4c607":"#files in the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","de489512":"#importation of data manipulation, plotting and grid formating Modules\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\n# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n\n#metrics evaluation Modules\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,classification_report","e91570b2":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","56dbbea3":"train.shape, test.shape","e74394fe":"train.head() #Taking a snapshot on the train table","1896e8f6":"test.head()","df7a6d82":"train.info() # code gives us some more detailed information about the dataset","9c90519c":"test.info()","c95c5bcd":"#brief summary on the numerical columns\ntrain.describe()","554128e9":"#brief summary on categorical variables\ntrain.describe(include=['O'])","3788f75d":"train.isnull().sum()","122f5945":"test.isnull().sum()","7e5a3140":"All_rows = train.shape[0] ","4c31b404":"Survived = train[train['Survived']==1]\nlen(Survived)","9f6ac07b":"Did_Not_Survive= train[train['Survived']==0]\nlen(Did_Not_Survive)","7e463975":"percent_of_survived = (len(Survived)\/All_rows)*100\npercent_of_Not_survive= (len(Did_Not_Survive)\/All_rows)*100","d51a51c9":"print(\"The percentage of passengers who survived: %.2f%% \"%percent_of_survived)\nprint(\"The percentage of passengers who did not survive: %.2f%%\"%percent_of_Not_survive)","829c1230":"train.groupby('Sex').Sex.count()","6b0837bf":"train.groupby('Pclass').Pclass.count()","6075fc67":"train.groupby('Embarked').Embarked.count()","e463cf4e":"train.groupby(['Sex','Pclass']).PassengerId.count()","0b4e5964":"train.groupby(['Sex','Survived']).PassengerId.count()","f38981fe":"fig, ax = plt.subplots()\n\nsns.catplot(\"Survived\", hue=\"Sex\", data=train, kind=\"count\", \n            palette={'male':\"Blue\", 'female':\"Green\"}, ax=ax)\n\nplt.close(1) # delete the extra figure we don't need\n\nax.legend(title=\"Gender\")\nplt.show()","462498fc":"train.groupby(['Pclass','Survived']).PassengerId.count()","d53fa4f4":"fig, ax= plt.subplots()\n\nsns.catplot(\"Survived\", hue=\"Pclass\", data=train, kind=\"count\", \n            palette={1:\"yellow\", 2:\"orange\", 3:\"red\"},ax=ax)\n\nax.legend(title=\"Passenger Class\")\nax.set_title(\"Pclass vs. Survival for Titanic Passengers\");\n\nplt.close(1)# we delete the extra figure created that we don't need","f3a98716":"table =pd.crosstab(train['Parch'],train['Survived'])\ntable","f661c1c3":"# Let's plot the survival class against the Frequency\nlabels = ['Did_Not_Survive','Survived']\nclasses = pd.value_counts(train['Survived'], sort = True)\nclasses.plot(kind = 'bar',rot=0)\nplt.title(\"Survival class distribution\")\nplt.xticks(range(2), labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","7ceadba1":"sns.factorplot('Sex', 'Survived', hue='Pclass', size=8, aspect=2, data=train)","6a59a13f":"fig, ax = plt.subplots(figsize=(8,8))\n\nax.hist(train[train[\"Survived\"]==1][\"Age\"], bins=15, alpha=0.8, color=\"blue\", label=\"survived\")\nax.hist(train[train[\"Survived\"]==0][\"Age\"], bins=15, alpha=0.8, color=\"green\", label=\"did not survive\")\n\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"Count of passengers\")\n\nfig.suptitle(\"Age vs. Survival for Titanic Passengers\")\n\nax.legend();","cbbd4105":"fig, ax = plt.subplots(figsize=(8,8))\n\nax.hist(train[train[\"Survived\"]==1][\"Fare\"], bins=15, alpha=0.5, color=\"blue\", label=\"survived\")\nax.hist(train[train[\"Survived\"]==0][\"Fare\"], bins=15, alpha=0.5, color=\"red\", label=\"did not survive\")\n\nax.set_xlabel(\"Fare\")\nax.set_ylabel(\"Count of passengers\")\n\nfig.suptitle(\"Fare vs. Survival for Titanic Passengers\")\n\nax.legend();","fb9c15b7":"train.groupby(['SibSp','Survived']).PassengerId.count()","f0e45042":"fig, ax = plt.subplots(figsize=(10,10))\n\nsns.catplot(\"Survived\", hue=\"SibSp\", data=train, kind=\"count\", \n            palette={1:\"yellow\", 2:\"orange\", 3:\"red\",4:'green',5:'brown',0:'cyan',8:'violet'}, ax=ax)\n\nplt.close(1) # catplot creates an extra figure we don't need\n\nax.legend(title=\"spouses\/sib aboard \")","96ba7588":"train.groupby(['Parch','Survived']).PassengerId.count()","6d8e14c8":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.catplot(\"Survived\", hue=\"Parch\", data=train, kind=\"count\", \n            palette={1:\"yellow\", 2:\"orange\", 3:\"red\",4:'green',5:'brown',0:'cyan',6:'purple'}, ax=ax)\n\nplt.close(1) # catplot creates an extra figure we don't need\n\nax.legend(title=\"parents\/children aboard \")","0d65d2ea":"# heat map of correlation of features\ncorrelation_matrix = train.corr()\nfig = plt.figure(figsize=(8,4))\nsns.heatmap(correlation_matrix,vmax=0.8,annot=True) \nplt.show()","514387b6":"cols_to_drop= ['Name','Ticket','Cabin']","099549d1":"train = train.drop(columns=cols_to_drop,axis=1)\ntest = test.drop(columns=cols_to_drop,axis=1)\ny_train = train['Survived']\ntrain = train.drop(columns='Survived',axis=1)","c5b711cf":"# combining train and test dataset\ndata_combined = [train, test]","85d1d622":"for dataset in data_combined:\n    dataset['Age']= dataset['Age'].fillna(dataset['Age'].median())","a204361a":"train.isnull().sum()['Age']# shows no missing values in age","7ec6f66b":"train.isnull().sum()['Age']# shows no missing values in age","b613bc5d":"#lets do some preprocessing on the fare column to fill in the nan value with median value\nfor dataset in data_combined:\n    dataset['Fare']= dataset['Fare'].fillna(dataset['Fare'].median())","c157f679":"test.isnull().sum()['Fare'] # double check to see that there are no nan values in Fare","cc049505":"for dataset in data_combined:\n    dataset['Embarked']= dataset['Embarked'].fillna('S')","df88cf79":"train.isnull().sum()['Embarked'] # shows that there are no more missing values in this column.","a1433965":"#converting Pclass to a categorical variable\nfor dataset in data_combined:\n    dataset['Pclass']= dataset['Pclass'].astype('str')","eff5e4b1":"train.head(2)","326eaa24":"test.head(2)","762a3c1d":"#selection of categorical variables\ncat_cols = [cname for cname in dataset.columns \n                    if  dataset[cname].dtype == \"object\"]","b9a9c682":"Train_cat_colsOH= pd.get_dummies(train[cat_cols])\nTest_cat_colsOH= pd.get_dummies(test[cat_cols])","170f03c6":"Train_cat_colsOH.head()","dc8f2f09":"Test_cat_colsOH.head()","00641097":"#Select numerical columns\nnum_cols = [cname for cname in dataset.columns \n            if dataset[cname].dtype in ['int64', 'float64']]","dc85729a":"train_num_data = pd.DataFrame(train[num_cols])\ntest_num_data = pd.DataFrame(test[num_cols])","7ad32988":"train =pd.concat([Train_cat_colsOH, train_num_data],axis=1) \ntest =pd.concat([Test_cat_colsOH, test_num_data],axis=1) ","9f7b5388":"train.head()","d794a0db":"test.head()","3709ff5d":"#lets drop the PassengerId column from the test and train features.\nX_train = train.drop(columns='PassengerId',axis=1)\nX_test = test.drop(columns='PassengerId',axis=1).copy()","adc0ea62":"X_train.shape , y_train.shape, X_test.shape","6dcdb957":"LOG_R_clf = LogisticRegression()\nLOG_R_clf.fit(X_train, y_train)\ny_pred_LR = LOG_R_clf.predict(X_test)\nLOG_R_score=LOG_R_clf.score(X_train,y_train)*100\nprint(\"The Logistic Regression train Accuracy = {:.2f}\".format(LOG_R_score))","424876ea":"SVC_clf = SVC()\nSVC_clf.fit(X_train, y_train)\ny_pred_SVC = SVC_clf.predict(X_test)\nSVC_score =SVC_clf.score(X_train,y_train)*100\nprint(\"The SVC train Accuracy = {:.2f}\".format(SVC_score))","8b97a04d":"KNN_clf = KNeighborsClassifier(n_neighbors = 3)\nKNN_clf.fit(X_train, y_train)\ny_pred_KNN = KNN_clf.predict(X_test)\nKNN_score =KNN_clf.score(X_train,y_train)*100\nprint(\"The KNeighbors Classifier train Accuracy = {:.2f}\".format(KNN_score))","44674042":"RF_clf = RandomForestClassifier(n_estimators=100)\nRF_clf.fit(X_train, y_train)\nprediction_train= RF_clf.predict(X_train)\nprediction_test = RF_clf.predict(X_test)\nRF_score=RF_clf.score(X_train,y_train)*100\nprint(\"The Random Forest train Accuracy = {:.2f}\".format(RF_score))","6efb9d67":"GB_clf = GradientBoostingClassifier(n_estimators=100)\nGB_clf.fit(X_train,y_train)\ny_pred_GB = GB_clf.predict(X_test)\nGB_score=GB_clf.score(X_train,y_train)*100\nprint(\"Gradient Boosting Classifier train Accuracy = {:.2f}\".format(GB_score))","86468132":"fig = plt.figure(figsize=(10,8))\n\n# Dataframe to hold the results\nmodel_comparison = pd.DataFrame({'model': ['LR_clf', 'SVC_clf',\n                                           'KNN_clf', 'RF_clf',\n                                            'GB_clf'],\n                                 'Accuracy': [LOG_R_score,SVC_score,KNN_score,RF_score,GB_score ]})\n\n# Horizontal bar chart of train accuracy\nmodel_comparison.sort_values('Accuracy').plot(x = 'model', y = 'Accuracy', \n                                kind = 'barh',color = 'blue', edgecolor = 'black')\n# Plot formatting\nplt.ylabel('Model'); plt.yticks(size = 10); plt.xlabel('Accuracy Score'); plt.xticks(size = 10)\nplt.title('Model Comparison on Train Accuracy Score', size = 14);","0d454a34":"GBC_precison_score  = precision_score(y_train,prediction_train)\nGBC_recall_score    = recall_score(y_train,prediction_train)\nGBC_f1_score        = f1_score(y_train,prediction_train)","6f1c13d1":"print(classification_report(y_train,prediction_train))","e40f63ee":"print(\"The gradient boosting precision_score = {:.2f}\".format(GBC_precison_score))","2069527d":"print(\"The gradient boosting recall_score = {:.2f}\".format(GBC_recall_score))","2e22935c":"print(\"The gradient boosting f1_score = {:.2f}\".format(GBC_f1_score))","3cc28ec7":"#comfusion matrix plot\nLabel_1 = ['Predicted_Positive', 'Predicted_Negative']\nLabel_2 = [ 'True Did_Not_Survive','True Survived', ]\nconf_matrix = confusion_matrix(y_train, prediction_train)\nplt.figure(figsize=(10, 6))\nsns.heatmap(conf_matrix, xticklabels=Label_1, yticklabels=Label_2, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('')\nplt.xlabel('Predicted class')\nplt.show()","65134096":"#Create a submission file on Kaggle\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction_test\n    })\n\nsubmission.to_csv('submission.csv', index=False)","7858fd43":"submission.head(10).set_index('PassengerId')","a72baa27":"**We can see from the code above that most people boarded the ship from Southampton(S)**","8196e720":"**This notebook is still under improvements. All comments are welcome**","f0558f7c":"**From the above code we realise that we have both categorical variables and numerical ones in both the train and test dataframes.\nwe also realise that there are 4 variables with missing values which include: Age,Cabin,Fare and Embarked in the train and test dataframes.**","460bb722":"**Lets do some preprocessing on the emabarkation variable and fill in missing values with the common embarkation port. 'S' = Southampton**","8c10327e":"**This note book comprises of the following in brief**","a66e4749":"**Visualise the Train accuracy of the models to as to select one with a better accuracy performance.**","c87ade53":"**From our plot above, we can see that women from class 1 have 99% chances of survival whereas men have close to 40% survival chances in the same class.\nWomen from the 3rd class have about 50% chances of survival whereas men have less than 20% survival chances in the same class.**","2bd49e21":"**We have 177 records with missing age, 2 records with missing embarked and the most records with missing values are from the Cabin variable.**","5ea2432f":"1. Data formatting\n2. Exploratory Data Analysis\n3. Model Selection\n4. Metrics Evaluations\n5. Model prediction and submission","674d6cc6":"**We are going to consider the classifier with the highest accuracy, which is the Random Forest Classifier and take it's prediction on the test set as the prediction for the model.**","85c76942":"**From the code above we can see that the average age of the people who boarded the ship was about 29 years although we also have some older people as old as 80 years.\nThe average fare was 32 in as much as we had tickets that were as expensive as 512.**","a3ed27b9":"**From the code above, we realise that the highest number of survivers did not have any family relation**","db487cab":"From the code above, we see that most passengers used the 3rd class\nprobably because it was less costly","040fc150":"FEATURE PREPROCESSING AND SELECTION.\nWe are going to do the following:\n1. Preprocessing on some variables of high significance as seen in the matrix above.\n2. Feature selection by droping those features that  won't be of great significance in our modelling process\n3. We shall combine the train and test data before doing the preprocessing.","ba049f3b":"Generally we had much more count of non-survivers than those who survived from the plot above. ","3fc004c1":"**From the above code, we realise that most survivers were from the upper class,non-survivers were from the third class.**","8fc169d6":"**From the above code, we can see that there were more women who survived than men.\nA nearly 3\/4 of men died on the ship and 3\/4 of women survived.**","af7cd1e4":"**According to our classification report, the positive class was considered as class 0 and the negative class as class 1. \nFrom the heatmap above, we can see that the model mis-classified 7 passengers as False Negatives(FN), implying that it classified them among passengers who survived and yet they did not survive in the true sense.\nThe model also mis-classified 11 passengers as False Positives (FP), implying that it classified them among those who did not survive and yet in real sense they survived**","1303f9e4":"**Using the code above, we can note that the common port where most passengers embarked from was Southampton(S), there were also more male than female passengers on this ship.\nWe have 147 unique cabin string values.**","c4b3ec7d":"**In the test dataframe we have 1 record with a missing Fare, 86 records with missing Age and 327 records\nwith missing cabin entries.**","7f60e97f":"**The code above shows that we had more males passengers in the 1st class than females. The highest number of females were in the 3rd class whereas the highest number of males were in the 3rd class. Generally, there were more males passengers across all the classes than females.**","2fc51105":"Code above shows that we had more males than females.","18580e20":"**From the plot above, we can see that there we more non-survivers who did not travel with a parent or children compared to the survivers who did not travel with a parent or children.\nHowever most survivers had one parent or a single child.**","b410f645":"**Model evaluations on various models.**","0d044d6f":"**From the correlation matrix, we can see some features that are positively correlated to the target column and some features are negatively correlated. we will use most of the columns here.**","3e703d9e":"**From the catplot we can see that we had more non-survivers who had no siblings or spouses.\nIt can be noted also that the highest number of survivers had no siblings or spouses.**","331795ba":"**The histogram above shows that most passengers who did not survive had cheaper fares.\nWe see that passengers who survived had a little more costly fares than those who did not survive.\nThe distribution is skewed to the right with few people being able to aford costly fares as well as surviving.**","128048c9":"**The highest number of passengers who did not survive were between ages of 15 and 30 years.\nThe highest number of passengers who survived were between ages of 15 and 35 years old.\nThe histogram for graphs is generally skewed to the right.\nWe can see that we also had more elderly people who did not survive compared to the elderly who survived.**"}}