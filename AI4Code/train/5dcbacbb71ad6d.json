{"cell_type":{"7b3d6824":"code","63190cfd":"code","f0c17bf1":"code","39e60533":"code","88a5d2b1":"code","e3d1789b":"code","39673bc5":"code","b381cdbf":"code","aba78bc0":"code","4488026d":"code","ccc56faa":"code","a6cb8223":"code","dc96be72":"code","4d03b6a9":"code","34971107":"code","49a304c1":"code","a68ed7e2":"code","04775e5b":"code","d456d050":"code","d83d4fbc":"code","e71387aa":"code","b1063f7a":"code","9d35d748":"code","da0bd11d":"code","893b2241":"code","cedb42af":"code","db95d76c":"code","86df67f2":"code","e4a0f254":"code","cd9117f7":"code","4e22571b":"code","fdf7ab0d":"code","5bf1b3d4":"code","5cecbf16":"code","6bfb3b91":"code","d17c86cc":"code","a723d38c":"code","42a31f2a":"code","803c289e":"code","48aef0a9":"code","26915120":"code","f690121a":"code","8a385ea2":"code","3af804dd":"code","e8bdac27":"code","4a727ac4":"code","3b3495f0":"code","77905248":"code","20dccce7":"code","fe469c7c":"code","db604469":"code","bfe01f01":"code","903dd1e9":"code","2ae8515b":"code","8c7cd2a2":"code","6cad226a":"code","7d37ca69":"code","0b24295f":"markdown","c1425ae6":"markdown","73d2a46b":"markdown","e29ba177":"markdown","eca2d99c":"markdown","a3af5509":"markdown","55de2e7d":"markdown","59ed3d10":"markdown"},"source":{"7b3d6824":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","63190cfd":"import featuretools as ft","f0c17bf1":"data = ft.demo.load_mock_customer()","39e60533":"customers_df = data[\"customers\"]","88a5d2b1":"customers_df.head()","e3d1789b":"sessions_df = data['sessions']","39673bc5":"sessions_df.head(5)","b381cdbf":"transactions_df = data[\"transactions\"]","aba78bc0":"transactions_df.head(5)","4488026d":"# Create new entityset\nes = ft.EntitySet(id = 'customers')","ccc56faa":"# Create an entity from the customers dataframe\n\nes = es.entity_from_dataframe(entity_id = 'customers', dataframe = customers_df, \n                              index = 'customer_id', time_index = 'join_date' ,variable_types =  {\"zip_code\": ft.variable_types.ZIPCode})","a6cb8223":"es","dc96be72":"es = es.entity_from_dataframe(entity_id=\"transactions\",\n                                 dataframe=transactions_df,\n                                 index=\"transaction_id\",\n                               time_index=\"transaction_time\",\n                               variable_types={\"product_id\": ft.variable_types.Categorical})","4d03b6a9":"ft.variable_types.ALL_VARIABLE_TYPES","34971107":"es","49a304c1":"es = es.entity_from_dataframe(entity_id=\"sessions\",\n            dataframe=sessions_df,\n            index=\"session_id\", time_index = 'session_start')","a68ed7e2":"es","04775e5b":"\n\ncust_relationship = ft.Relationship(es[\"customers\"][\"customer_id\"],\n                       es[\"sessions\"][\"customer_id\"])\n\n# Add the relationship to the entity set\nes = es.add_relationship(cust_relationship)\n","d456d050":"\nsess_relationship = ft.Relationship(es[\"sessions\"][\"session_id\"],\n                       es[\"transactions\"][\"session_id\"])\n\n# Add the relationship to the entity set\nes = es.add_relationship(sess_relationship)\n\n","d83d4fbc":"es","e71387aa":"feature_matrix, feature_defs = ft.dfs(entityset=es,\n                                        target_entity=\"customers\",max_depth = 3)","b1063f7a":"feature_matrix","9d35d748":"len(feature_defs)","da0bd11d":"feature_defs","893b2241":"# Lets talk about categorical features \nsessions_df.head()\n","cedb42af":"pd.get_dummies(sessions_df['device'],drop_first=True).head()","db95d76c":"df = pd.DataFrame(\n       [[ 'low', 'London'], [ 'medium', 'New York'], [ 'high', 'Dubai']],\n       columns=['Temperature', 'City'])\n","86df67f2":"df","e4a0f254":"map_dict = {'low':0,'medium':1,'high':2}\ndef map_values(x):\n    return map_dict[x]\ndf['Temperature_oe'] = df['Temperature'].apply(lambda x: map_values(x))","cd9117f7":"df","4e22571b":"from sklearn.preprocessing import LabelEncoder\n# create a labelencoder object\nle = LabelEncoder()\n# fit and transform on the data\nsessions_df['device_le'] = le.fit_transform(sessions_df['device'])\nsessions_df.head()","fdf7ab0d":"sessions_df.head()","5bf1b3d4":"\nplayers = pd.read_csv(\"..\/input\/fifa19\/data.csv\")","5cecbf16":"len(players.Club.unique())","6bfb3b91":"\nfrom category_encoders.binary import BinaryEncoder\n# create a Binaryencoder object\nbe = BinaryEncoder(cols = ['Club'],)\n# fit and transform on the data\nplayers = be.fit_transform(players)","d17c86cc":"players.head()","a723d38c":"\nplayers = pd.read_csv(\"..\/input\/fifa19\/data.csv\")\n\nfrom category_encoders.hashing import HashingEncoder\n# create a HashingEncoder object\nbe = HashingEncoder(cols = ['Club'])\n# fit and transform on the data\nplayers = be.fit_transform(players)","42a31f2a":"players.head()","803c289e":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")","48aef0a9":"train.head()","26915120":"# taken from https:\/\/medium.com\/@pouryaayria\/k-fold-target-encoding-dfe9a594874b\nfrom sklearn import base\nfrom sklearn.model_selection import KFold\n\nclass KFoldTargetEncoderTrain(base.BaseEstimator,\n                               base.TransformerMixin):\n    def __init__(self,colnames,targetName,\n                  n_fold=5, verbosity=True,\n                  discardOriginal_col=False):\n        self.colnames = colnames\n        self.targetName = targetName\n        self.n_fold = n_fold\n        self.verbosity = verbosity\n        self.discardOriginal_col = discardOriginal_col\n    def fit(self, X, y=None):\n        return self\n    def transform(self,X):\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in X.columns)\n        assert(self.targetName in X.columns)\n        mean_of_target = X[self.targetName].mean()\n        kf = KFold(n_splits = self.n_fold,\n                   shuffle = True, random_state=2019)\n        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n        X[col_mean_name] = np.nan\n        for tr_ind, val_ind in kf.split(X):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)\n                                     [self.targetName].mean())\n            X[col_mean_name].fillna(mean_of_target, inplace = True)\n        if self.verbosity:\n            encoded_feature = X[col_mean_name].values\n            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,self.targetName,                    \n                   np.corrcoef(X[self.targetName].values,\n                               encoded_feature)[0][1]))\n        if self.discardOriginal_col:\n            X = X.drop(self.targetName, axis=1)\n        return X","f690121a":"targetc = KFoldTargetEncoderTrain('Pclass','Survived',n_fold=5)\nnew_train = targetc.fit_transform(train)","8a385ea2":"new_train[['Pclass_Kfold_Target_Enc','Pclass']].head()","3af804dd":"train = pd.read_csv(\"..\/input\/nyc-taxi-trip-duration\/train.csv\")","e8bdac27":"train = train.sample(500)","4a727ac4":"def haversine_array(lat1, lng1, lat2, lng2): \n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n    AVG_EARTH_RADIUS = 6371 # in km \n    lat = lat2 - lat1 \n    lng = lng2 - lng1 \n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) *      np.sin(lng * 0.5) ** 2 \n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) \n    return h","3b3495f0":"train['haversine_distance'] = train.apply(lambda x: haversine_array(x['pickup_latitude'], x['pickup_longitude'], x['dropoff_latitude'], x['dropoff_longitude']),axis=1)","77905248":"def dummy_manhattan_distance(lat1, lng1, lat2, lng2): \n    a = haversine_array(lat1, lng1, lat1, lng2) \n    b = haversine_array(lat1, lng1, lat2, lng1) \n    return a + b","20dccce7":"train['manhattan_distance'] = train.apply(lambda x: dummy_manhattan_distance(x['pickup_latitude'], x['pickup_longitude'], x['dropoff_latitude'], x['dropoff_longitude']),axis=1)","fe469c7c":"def bearing_array(lat1, lng1, lat2, lng2): \n    AVG_EARTH_RADIUS = 6371 # in km \n    lng_delta_rad = np.radians(lng2 - lng1) \n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n    y = np.sin(lng_delta_rad) * np.cos(lat2) \n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) \n    return np.degrees(np.arctan2(y, x))","db604469":"train['bearing'] = train.apply(lambda x: bearing_array(x['pickup_latitude'], x['pickup_longitude'], x['dropoff_latitude'], x['dropoff_longitude']),axis=1)","bfe01f01":"train.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) \/ 2 \ntrain.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) \/ 2","903dd1e9":"train.head()","2ae8515b":"import plotly_express as px\n","8c7cd2a2":"px.histogram(train,x='trip_duration')","6cad226a":"train['log_trip_duration'] = train['trip_duration'].apply(lambda x: np.log(1+x))","7d37ca69":"px.histogram(train,x='log_trip_duration')","0b24295f":"## Target\/Mean Encoding","c1425ae6":"## Label Encoder","73d2a46b":"# 3. How best to use Latitude and Longitude features\u200a-\u200aPart\u00a01:","e29ba177":"# 2. Handling Categorical Features: Label\/Binary\/Hashing and Target\/Mean Encoding\n\n## Ordinal Encoding","eca2d99c":"# log feature transformation","a3af5509":"## Binary Encoder","55de2e7d":"## Hashing Encoder","59ed3d10":"# 1. Automatic Feature Creation using featuretools:"}}