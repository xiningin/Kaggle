{"cell_type":{"6ea5bf26":"code","c9c90a0a":"code","5001a05d":"code","a4855b0c":"code","d1a5c6e4":"code","dae4a6a5":"code","d2941fc1":"code","849f6a27":"code","994eb3da":"code","ebdd74a4":"code","c3c33de1":"code","738423bb":"code","5879cd47":"code","91463f85":"code","0a17a770":"code","4d420a3e":"code","48133ccc":"code","a2835017":"code","ab50ca46":"code","5f73aa81":"code","07b26568":"code","d97813c2":"code","e31a5420":"code","7de73ac6":"code","8cfed3ca":"code","0e072277":"code","4cb448f4":"code","5028ec65":"markdown","85b8fdf2":"markdown","af181dc4":"markdown","5a5daf29":"markdown","a788b40e":"markdown","fe1b76cb":"markdown","4adf7805":"markdown","45cf5c5e":"markdown","162c287c":"markdown","11e5a42d":"markdown","88e35bf0":"markdown","36a383ee":"markdown","7429e9a9":"markdown","48104153":"markdown"},"source":{"6ea5bf26":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c9c90a0a":"%%time\ntrain = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","5001a05d":"target = train['target']\ntrain_id = train['id']\ntest_id = test['id']\n\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","a4855b0c":"df = pd.concat([train, test], axis=0, sort=False )","d1a5c6e4":"bin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\ndf['bin_3'] = df['bin_3'].map(bin_dict)\ndf['bin_4'] = df['bin_4'].map(bin_dict)","dae4a6a5":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\n                    prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], \n                    drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","d2941fc1":"from pandas.api.types import CategoricalDtype \n\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)\n\ndf.ord_1 = df.ord_1.astype(ord_1)\ndf.ord_2 = df.ord_2.astype(ord_2)\ndf.ord_3 = df.ord_3.astype(ord_3)\ndf.ord_4 = df.ord_4.astype(ord_4)\n\ndf.ord_1 = df.ord_1.cat.codes\ndf.ord_2 = df.ord_2.cat.codes\ndf.ord_3 = df.ord_3.cat.codes\ndf.ord_4 = df.ord_4.cat.codes","849f6a27":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n    return df\n\ndf = date_cyc_enc(df, 'day', 7)\ndf = date_cyc_enc(df, 'month', 12)","994eb3da":"%%time\nfrom sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nfor f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_5']:\n    lbl = LabelEncoder()\n    lbl.fit(df[f])\n    df[f'le_{f}'] = lbl.transform(df[f])","ebdd74a4":"df.drop(['nom_5','nom_6','nom_7','nom_8','nom_9', 'ord_5'] , axis=1, inplace=True)","c3c33de1":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","738423bb":"df = reduce_mem_usage(df)","5879cd47":"#https:\/\/slundberg.github.io\/shap\/notebooks\/plots\/decision_plot.html (Good !!)","91463f85":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap","0a17a770":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:]\n\ntrain.shape","4d420a3e":"X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.2, random_state = 0)","48133ccc":"#  RandomForest\nModel=LGBMClassifier(max_depth=10, n_estimators=1000, n_jobs=-1, num_leaves=45,  learning_rate=0.01)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\n#print(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('Roc_Auc is ',roc_auc_score(y_pred,y_val))","a2835017":"perm = PermutationImportance(Model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = X_val.columns.tolist(),top=42)","ab50ca46":"explainer = shap.TreeExplainer(Model)\nexpected_value = explainer.expected_value\nif isinstance(expected_value, list):\n    expected_value = expected_value[1]\nprint(f\"Explainer expected value: {expected_value}\")\n\nselect = range(20)\nfeatures = X_val.iloc[select]\n#features_display = features.columns\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    shap_values = explainer.shap_values(features)[1]\n    shap_interaction_values = explainer.shap_interaction_values(features)\nif isinstance(shap_interaction_values, list):\n    shap_interaction_values = shap_interaction_values[1]","5f73aa81":"shap.decision_plot(expected_value, shap_values, features)","07b26568":"shap.decision_plot(expected_value, shap_values, features,link='logit')","d97813c2":"# Our naive cutoff point is zero log odds (probability 0.5).\ny_pred = (shap_values.sum(1) + expected_value) > 0\nmisclassified = y_pred != y_val.iloc[select]\nshap.decision_plot(expected_value, shap_values, features, highlight=misclassified, link='logit')","e31a5420":"shap.decision_plot(expected_value, shap_values[misclassified], features[misclassified],\n                    link='logit', highlight=1)","7de73ac6":"shap.initjs()\nshap.force_plot(expected_value, shap_values[misclassified], features[misclassified])","8cfed3ca":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=Model, dataset=X_val, model_features=features.columns, feature='le_ord_5')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'le_ord_5')\nplt.show()","0e072277":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=Model, dataset=X_val, model_features=features.columns, feature='ord_4')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'ord_4')\nplt.show()","4cb448f4":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['le_ord_5', 'ord_4']\ninter1  =  pdp.pdp_interact(model=Model, dataset=X_val, model_features=features.columns, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', plot_pdp=True)\nplt.show()","5028ec65":"How do we interpret it in this model of category variables? <br>\nI'd appreciate it if you could share your knowledge.","85b8fdf2":"# PDP","af181dc4":"Observations can be highlighted using a dotted line style. Here, we highlight a misclassified observation.","5a5daf29":"## 2D PDP","a788b40e":"https:\/\/eli5.readthedocs.io\/en\/latest\/autodocs\/eli5.html","fe1b76cb":"In a nutshell, the larger the value of le_ord_5, the more likely it is to hit the target, and that data does not seem to require interpretation.","4adf7805":"A few items are worth pointing out as you interpret this plot\n\n* The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n* A blue shaded area indicates level of confidence","45cf5c5e":"* boosting_type (string, optional (default='gbdt')) \u2013 \u2018gbdt\u2019, traditional Gradient Boosting Decision Tree. \u2018dart\u2019, Dropouts meet Multiple Additive Regression Trees. \u2018goss\u2019, Gradient-based One-Side Sampling. \u2018rf\u2019, Random Forest.\n* num_leaves (int, optional (default=31)) \u2013 Maximum tree leaves for base learners.\n* max_depth (int, optional (default=-1)) \u2013 Maximum tree depth for base learners, <=0 means no limit.\n* learning_rate (float, optional (default=0.1)) \u2013 Boosting learning rate. You can use callbacks parameter of fit method to shrink\/adapt learning rate in training using reset_parameter callback. Note, that this will ignore the learning_rate argument in training.\n* n_estimators (int, optional (default=100)) \u2013 Number of boosted trees to fit.\n* subsample_for_bin (int, optional (default=200000)) \u2013 Number of samples for constructing bins.\n* objective (string, callable or None, optional (default=None)) \u2013 Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below). Default: \u2018regression\u2019 for LGBMRegressor, \u2018binary\u2019 or \u2018multiclass\u2019 for LGBMClassifier, \u2018lambdarank\u2019 for LGBMRanker.\n* class_weight (dict, 'balanced' or None, optional (default=None)) \u2013 Weights associated with classes in the form {class_label: weight}. Use this parameter only for multi-class classification task; for binary classification task you may use is_unbalance or scale_pos_weight parameters. Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities. You may want to consider performing probability calibration (https:\/\/scikit-learn.org\/stable\/modules\/calibration.html) of your model. The \u2018balanced\u2019 mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y)). If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n* min_split_gain (float, optional (default=0.)) \u2013 Minimum loss reduction required to make a further partition on a leaf node of the tree.\n* min_child_weight (float, optional (default=1e-3)) \u2013 Minimum sum of instance weight (hessian) needed in a child (leaf).\n* min_child_samples (int, optional (default=20)) \u2013 Minimum number of data needed in a child (leaf).\n* subsample (float, optional (default=1.)) \u2013 Subsample ratio of the training instance.\n* subsample_freq (int, optional (default=0)) \u2013 Frequence of subsample, <=0 means no enable.\n* colsample_bytree (float, optional (default=1.)) \u2013 Subsample ratio of columns when constructing each tree.\n* reg_alpha (float, optional (default=0.)) \u2013 L1 regularization term on weights.\n* reg_lambda (float, optional (default=0.)) \u2013 L2 regularization term on weights.\n* random_state (int or None, optional (default=None)) \u2013 Random number seed. If None, default seeds in C++ code will be used.\n* n_jobs (int, optional (default=-1)) \u2013 Number of parallel threads.\n* silent (bool, optional (default=True)) \u2013 Whether to print messages while running boosting.\n* importance_type (string, optional (default='split')) \u2013 The type of feature importance to be filled into feature_importances_. If \u2018split\u2019, result contains numbers of times the feature is used in a model. If \u2018gain\u2019, result contains total gains of splits which use the feature.\n* **kwargs \u2013\n* Other parameters for the model. Check http:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html for more parameters.","162c287c":"Here is the code to create the Partial Dependence Plot using the PDPBox library.","11e5a42d":"Refer to the decision plot of the 20 test observations below. _Note: This plot isn't informative by itself; we use it only to illustrate the primary concepts._\n\n* The x-axis represents the model's output. In this case, the units are log odds.\n* The plot is centered on the x-axis at explainer.expected_value. All SHAP values are relative to the model's expected value like a linear model's effects are relative to the intercept.\n* The y-axis lists the model's features. By default, the features are ordered by descending importance. The importance is calculated over the observations plotted. _This is usually different than the importance ordering for the entire dataset._ In addition to feature importance ordering, the decision plot also supports hierarchical cluster feature ordering and user-defined feature ordering.\n* Each observation's prediction is represented by a colored line. At the top of the plot, each line strikes the x-axis at its corresponding observation's predicted value. This value determines the color of the line on a spectrum.\n* Moving from the bottom of the plot to the top, SHAP values for each feature are added to the model's base value. This shows how each feature contributes to the overall prediction.\n* At the bottom of the plot, the observations converge at explainer.expected_value.","88e35bf0":"**TO BE Continue**","36a383ee":"# SHAP","7429e9a9":"Like the force plot, the decision plot supports link='logit' to transform log odds to probabilities.","48104153":"* As you move down the top of the graph, the importance of the feature decreases.\n* The features that are shown in green indicate that they have a positive impact on our prediction\n* The features that are shown in white indicate that they have no effect on our prediction\n* The features shown in red indicate that they have a negative impact on our prediction"}}