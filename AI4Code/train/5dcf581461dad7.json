{"cell_type":{"9670d954":"code","6951d644":"code","7f676499":"code","2200bceb":"code","5626f6db":"code","050590e4":"code","f9fabd36":"code","ba8912f8":"code","5bddfc43":"code","e99a0653":"code","5f13e28c":"code","7c15c969":"code","a07bcc2c":"code","89cf4c2e":"code","45de9c62":"code","6bf2ca2b":"code","9c87ff69":"code","f6a7a29f":"code","62f6ee43":"code","3b75fc69":"code","0d57b7da":"code","018fa0b1":"code","a53d0852":"code","4d8439e1":"code","2807684a":"code","08c25fd3":"code","c3918b02":"code","ed39a33b":"code","089bff8a":"code","aaee40c3":"code","8bdea36d":"code","6c64ab5c":"code","9b69bd66":"code","f732c957":"code","d4b53d5d":"code","8921aacc":"code","cd606877":"code","633f9149":"code","aa9e4658":"code","00d54b63":"code","102912ed":"code","5d59c8b8":"code","da2269b3":"code","4b4aa16d":"code","c6068cbe":"code","53aeede6":"code","99f1475f":"code","ef98c71c":"code","d5a68678":"markdown"},"source":{"9670d954":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# import other needed packages and functions\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\nimport lightgbm\nfrom sklearn.model_selection import cross_val_score\nimport itertools\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6951d644":"# read in csv files and make dfs\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# make copy of test df for submission\nsubmission = test_df.copy()\n\n# combine train and test dfs into 1 df of all data\nall_df = pd.concat([train_df, test_df], sort=False)","7f676499":"# Define function to inspect data frames. Prints first few lines, determines size\/shape of data frame,\n# shows descriptive statistics, shows data types, shows missing or incomplete data, check for duplicate data.\n\ndef inspect_df(df):\n    print('Header:')\n    print('{}'.format(df.head()))\n    print()\n    print('Shape: {}'.format(df.shape))\n    print()\n    print('Statistics:')\n    print('{}'.format(df.describe()))\n    print()\n    print('Info:')\n    print('{}'.format(df.info()))\n    \n# use inspect_df on all_df\n\ninspect_df(all_df)","2200bceb":"# look at proportions of passengers by Pclass\n\nall_df.Pclass.value_counts(normalize=True, sort=False)","5626f6db":"# look at proportions of passengers by Sex\n\nall_df.Sex.value_counts(normalize=True)","050590e4":"# inspect null values for Embarked\n\nall_df[all_df.Embarked.isnull()]","f9fabd36":"# fill missing values for Embarked with information found online\n\nall_df.loc[[61, 829], ['Embarked']] = 'S'","ba8912f8":"# plot histogram of Ages\n\nplt.hist(data = all_df, x = 'Age', bins = 40);","5bddfc43":"# plot histogram of Fare\n\nplt.hist(data = all_df, x = 'Fare', bins = 40);","e99a0653":"# plot histogram of siblings and spouses\n\nplt.hist(data = all_df, x = 'SibSp', bins = 8);","5f13e28c":"# plot histogram of parents and children\n\nplt.hist(data = all_df, x = 'Parch', bins = 9);","7c15c969":"# inspect columns and missing values again\n\nall_df.info()","a07bcc2c":"# find passenger with missing fare data\n\nall_df[all_df.Fare.isnull()]","89cf4c2e":"# get average fare of each Pclass\n\nall_df.Fare.groupby(all_df.Pclass).mean()","45de9c62":"#fill nan fare value with rounded mean for class 3\n\nall_df.loc[152, ['Fare']] = 13","6bf2ca2b":"# fill missing values for ages with the mean age value for each passengers Pclass and Sex\n\nall_df.Age = all_df.Age.groupby([all_df.Pclass, all_df.Sex]).transform(lambda x: x.fillna(x.mean()))","9c87ff69":"# Extract titles from names and make new Title column, then get survival rate of each title\n\nall_df['Title'] = all_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nall_df.Survived.groupby(all_df.Title).mean()","f6a7a29f":"# get value counts for title occurences\n\nall_df.Title.value_counts()","62f6ee43":"# Replace uncommon titles with more common values and view new occurences\n\nall_df.Title = all_df.Title.replace(['Capt', 'Col', 'Dr', 'Major', 'Rev', 'Don', 'Sir', 'Jonkheer'], 'Mr')\nall_df.Title = all_df.Title.replace(['Ms', 'Mlle'], 'Miss')\nall_df.Title = all_df.Title.replace(['Mme', 'Lady', 'Countess', 'Dona'], 'Mrs')\nall_df.Title.value_counts()","3b75fc69":"# combine sibsp and parch to one family column\nall_df['Fam'] = all_df.SibSp + all_df.Parch\n\n# make ticket frequency column for number of occurences of ticket number\nall_df['Ticket_Frequency'] = all_df.groupby('Ticket')['Ticket'].transform('count')\n    \n# make column for solo vs travel with family\nall_df.loc[all_df['Fam'] == 0, 'Solo'] = 1\nall_df.loc[all_df['Ticket_Frequency'] == 1, 'Solo'] = 1\nall_df.Solo = all_df.Solo.fillna(0)\n       \n# bin fare column to 9 quantiles and encode as ordinal\nall_df['Fare'] = pd.qcut(all_df.Fare, q=9, labels=np.arange(1,10))\n    \n# bin age column to 10 quantiles and encode as ordinal\nall_df['Age'] = pd.qcut(all_df.Age, q=10, labels=np.arange(1,11))\n    \n# one-hot encode sex column and capitalize sex columns for consistency\nall_df = pd.concat([all_df, pd.get_dummies(all_df.Sex)], axis=1)\nall_df.rename(columns={'male':'Male', 'female':'Female'}, inplace=True)\n    \n# one-hot encode embarked column\nall_df = pd.concat([all_df, pd.get_dummies(all_df.Embarked, prefix='Embarked')], axis=1)\n    \n# one-hot encode title column\nall_df = pd.concat([all_df, pd.get_dummies(all_df.Title)], axis=1)\n    \n# drop unwanted columns (name, sex, cabin, embarked and title have been replaced with one hot encoding, ticket replaced with ticket frequency,\n# cabin has too many missing values, sibsp and parch replaced with fam and solo columns)\nall_df = all_df.drop(columns=['Name', 'Sex', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'Embarked', 'Title'])\n    \n# inspect columns and number of values for resulting df\nall_df.info()","0d57b7da":"# inspect survival rates for number of family members onboard\n\nall_df.Survived.groupby(all_df.Fam).mean()","018fa0b1":"# bin fam column values and get value counts\n\nall_df.Fam = pd.cut(all_df.Fam, bins=[0, 1, 4, 7, 11], include_lowest=True, right=False, labels=[1, 2, 3, 4])\nall_df.Fam.value_counts()","a53d0852":"# inspect survival rate for number of people traveling in group\n\nall_df.Survived.groupby(all_df.Ticket_Frequency).mean()","4d8439e1":"# bin ticket frequency column values and get value counts\n\nall_df.Ticket_Frequency = pd.cut(all_df.Ticket_Frequency, bins=[0, 2, 5, 9, 12], right=False, labels=[1, 2, 3, 4])\nall_df.Ticket_Frequency.value_counts()","2807684a":"# make list of all columns and view it\n\ncols = list(all_df)\ncols","08c25fd3":"# use min max scaler to scale all feature columns to range 0-1\n\nscaler = MinMaxScaler()\nall_df[cols] = scaler.fit_transform(all_df[cols])","c3918b02":"# make array of survived labels (training), drop survived column from all_df, split all_df into features (training) and test_df, and make array of feature column names\n\nlabels = all_df.loc[:890, 'Survived']\nall_df = all_df.drop(columns = 'Survived')\nfeatures = all_df.iloc[:891]\ntest_df = all_df.iloc[891:]\nfeat_names = features.columns.values","ed39a33b":"# use SelectKBest to narrow down to top features and use result to transform train and test features dfs\n\nk = SelectKBest(k=11)\nk.fit(features, labels)\nk_scores = (k.scores_)\nfeatures = k.transform(features)\ntest_df = k.transform(test_df)","089bff8a":"# make df to show scores for all features and print\n\nfeat_scores = pd.DataFrame()\nfeat_scores['Feature'] = feat_names\nfeat_scores['Score'] = k_scores\nfeat_scores","aaee40c3":"# split training data into train and test subsets for validation\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=3)","8bdea36d":"# setup base classifiers\n\ngb = GradientBoostingClassifier()\nrf = RandomForestClassifier()\net = ExtraTreesClassifier()\nab = AdaBoostClassifier()\ndt = DecisionTreeClassifier()\nlr = LogisticRegression(solver='liblinear')\nkn = KNeighborsClassifier()\nsvc = SVC(gamma='auto', probability=True)\ngnb = GaussianNB()","6c64ab5c":"# get train base classifiers and get initial validation scores\n\ngb.fit(features_train, labels_train)\nprint('GB Score:', gb.score(features_test, labels_test))\nrf.fit(features_train, labels_train)\nprint('RF Score:', rf.score(features_test, labels_test))\net.fit(features_train, labels_train)\nprint('ET Score:', et.score(features_test, labels_test))\nab.fit(features_train, labels_train)\nprint('AB Score:', ab.score(features_test, labels_test))\ndt.fit(features_train, labels_train)\nprint('DT Score:', dt.score(features_test, labels_test))\nlr.fit(features_train, labels_train)\nprint('LR Score:', lr.score(features_test, labels_test))\nkn.fit(features_train, labels_train)\nprint('KN Score:', kn.score(features_test, labels_test))\nsvc.fit(features_train, labels_train)\nprint('SVC Score:', svc.score(features_test, labels_test))\ngnb.fit(features_train, labels_train)\nprint('GNB Score:', gnb.score(features_test, labels_test))","9b69bd66":"# use gridsearch to tune base classifier hyperparameters\n\nalg = gb\nparams = {'n_estimators': (10, 25, 50, 100), 'learning_rate': (0.01, 0.1, 0.5, 1, 5, 10)}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\ngb = clf.best_estimator_","f732c957":"# use gridsearch to tune base classifier hyperparameters\n\nalg = rf\nparams = {'n_estimators': (10, 25, 50, 100), 'min_samples_split': (2, 3, 4, 5, 10), 'min_samples_leaf': (1, 2, 3, 4, 5)}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\nrf = clf.best_estimator_","d4b53d5d":"# use gridsearch to tune base classifier hyperparameters\n\nalg = et\nparams = {'n_estimators': (10, 25, 50, 100), 'min_samples_split': (2, 3, 4, 5, 10), 'min_samples_leaf': (1, 2, 3, 4, 5)}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\net = clf.best_estimator_","8921aacc":"# use gridsearch to tune base classifier hyperparameters\n\nalg = ab\nparams = {'n_estimators': (10, 25, 50, 100), 'learning_rate': (0.01, 0.1, 0.5, 1, 5, 10)}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\nab = clf.best_estimator_","cd606877":"# use gridsearch to tune base classifier hyperparameters\n\nalg = dt\nparams = {'min_samples_split': (2, 3, 4, 5, 10), 'min_samples_leaf': (1, 2, 3, 4, 5)}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\ndt = clf.best_estimator_","633f9149":"# use gridsearch to tune base classifier hyperparameters\n\nalg = lr\nparams = {'penalty': ('l1', 'l2'), 'C': (0.01, 0.1, 0.5, 1, 5, 10), 'max_iter': (100, 500)}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\nlr = clf.best_estimator_","aa9e4658":"# use gridsearch to tune base classifier hyperparameters\n\nalg = kn\nparams = {'n_neighbors': (2, 3, 4, 5, 10, 20)}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\nkn = clf.best_estimator_","00d54b63":"# use gridsearch to tune base classifier hyperparameters\n\nalg = svc\nparams = {'C': (0.01, 0.1, 0.5, 1, 5, 10), 'kernel': ('linear', 'poly', 'rbf', 'sigmoid')}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features_train, labels_train)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\nsvc = clf.best_estimator_","102912ed":"# retrain base classifiers and get validation scores\n\ngb.fit(features_train, labels_train)\nprint('GB Score:', gb.score(features_test, labels_test))\nrf.fit(features_train, labels_train)\nprint('RF Score:', rf.score(features_test, labels_test))\net.fit(features_train, labels_train)\nprint('ET Score:', et.score(features_test, labels_test))\nab.fit(features_train, labels_train)\nprint('AB Score:', ab.score(features_test, labels_test))\ndt.fit(features_train, labels_train)\nprint('DT Score:', dt.score(features_test, labels_test))\nlr.fit(features_train, labels_train)\nprint('LR Score:', lr.score(features_test, labels_test))\nkn.fit(features_train, labels_train)\nprint('KN Score:', kn.score(features_test, labels_test))\nsvc.fit(features_train, labels_train)\nprint('SVC Score:', svc.score(features_test, labels_test))\ngnb.fit(features_train, labels_train)\nprint('GNB Score:', gnb.score(features_test, labels_test))","5d59c8b8":"# setup voting classifier and get initial cross val score\n\nvote = VotingClassifier(estimators=[('gb',gb), ('rf',rf), ('et',et), ('ab',ab), ('dt',dt), ('lr',lr), ('kn',kn), ('svc',svc), ('gnb',gnb)], voting='soft')\nvote.fit(features, labels)\ncross_val_score(vote, features, labels, cv=5, scoring='accuracy').mean()","da2269b3":"# retrain voting classifier and get validation score\n\nvote.fit(features_train, labels_train)\nprint('Voting Score:', vote.score(features_test, labels_test))","4b4aa16d":"# use itertools combinations to make list of tuples of all possible different combinations of classifiers\n\nclfs = [('gb',gb), ('rf',rf), ('et',et), ('ab',ab), ('dt',dt), ('lr',lr), ('kn',kn), ('svc',svc), ('gnb',gnb)]\ncombs = []\n\nfor i in range(2, len(clfs)+1):\n    comb = [list(x) for x in itertools.combinations(clfs, i)]\n    combs.extend(comb)","c6068cbe":"# Tune voting classifier to use best combination of base classifiers\n\nalg = vote\nparams = {'estimators': combs}\nclf = GridSearchCV(alg, params, cv = 5, scoring = 'accuracy', n_jobs = -1)\nclf.fit(features, labels)\nprint(\"Best Parameters:\", clf.best_params_)\nprint(\"Best Score:\", clf.best_score_)\nvote = clf.best_estimator_","53aeede6":"# retrain voting classifier and get validation score\n\nvote.fit(features_train, labels_train)\nprint('Voting Score:', vote.score(features_test, labels_test))","99f1475f":"# retrain voting classifier with full train set, use to make probability predictions and make df of probs, set threshold for probablities and use to convert probs to predictions\n\nvote.fit(features, labels)\npred_prob = pd.DataFrame(vote.predict_proba(test_df))\nthreshold = 0.55\ny_pred = pred_prob.applymap(lambda x: 1 if x>threshold else 0)","ef98c71c":"# add predictions submission df as survived column, drop all columns but passenger ID and survived, write submission to csv without index to generate submission file\n\nsubmission['Survived'] = y_pred[1].astype(int)\nsubmission = submission[['PassengerId', 'Survived']]\nsubmission.to_csv('submission.csv', index=False)","d5a68678":"Looked up Mrs. Stone and Miss Icard online, they boarded in Southampton."}}