{"cell_type":{"b75ce089":"code","579addbb":"code","0466ae69":"code","61a34c1b":"code","b0236312":"code","6390f9d9":"code","36833fd0":"code","0cff38c0":"code","e127f58e":"code","557ef97f":"code","b29ee013":"code","2b9a39e0":"code","beec3d1a":"code","b5c51fec":"code","faf2fb4f":"code","460a051d":"code","60190df7":"code","79994da3":"code","1a93c7a2":"code","88f03223":"code","b7756ee5":"code","19a89491":"code","ac7b801d":"code","b05c815e":"code","c9f6b7d3":"code","098c8373":"code","293784f0":"code","d95dd1d4":"markdown","689ae370":"markdown","f3dcbeaa":"markdown","66141bcd":"markdown","db68bc7d":"markdown","70eacc0b":"markdown","fd926c44":"markdown","9f9d4679":"markdown","d6fa8ab1":"markdown","1ca817bd":"markdown","2d0376f0":"markdown","2417550e":"markdown","ceeac3db":"markdown","8d0f6239":"markdown","55acaff1":"markdown","d63afd98":"markdown","8787bf2d":"markdown","8c5624b0":"markdown","058ea8fd":"markdown","26cc69f2":"markdown","003eab90":"markdown","0c59a931":"markdown"},"source":{"b75ce089":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","579addbb":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","0466ae69":"df.info()","61a34c1b":"for col in df.columns:\n    print(\"The minimum value fore the columns {} is {}\".format(col, df[col].min()))\n","b0236312":" df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)\ndf.info()","6390f9d9":"import seaborn as sns\nfrom itertools import cycle\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\nsns.countplot(df['Outcome'])\nplt.show()","36833fd0":"df['Outcome'].value_counts()","0cff38c0":"import plotly.graph_objects as go\n\nlabels = ['Diabetic', 'Non-Diabetic']\npercentages = [34.89, 65.10]\nfig = go.Figure(data=[go.Pie(labels=labels, values=percentages, pull=[0.05, 0])])\nfig.show()","e127f58e":"def msv_1(data, thresh = 20, color = 'black', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n    plt.axhline(y = thresh, color = 'r', linestyle = '-')\n    \n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    \n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+2.5, f'Columns with more than {thresh}% missing values', fontsize=10, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 0.5, f'Columns with less than {thresh}% missing values', fontsize=10, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    \n    return plt.show()\nmsv_1(df, 15, color=sns.color_palette('Oranges',15))","557ef97f":"df['Insulin'] = df['Insulin'].fillna(df['Insulin'].median()) # Filling null values with the median.\n\nfor col in ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI']:\n    df[col] = df[col].fillna(df[col].mean())\ndf.isnull().sum()","b29ee013":"columns=df.columns\ncolumns=list(columns)\ncolumns.pop()\nprint(\"Column names except for the target column are :\",columns)\n\n#Graphs to be plotted with these colors\ncolours=['r','c','k','m','r','y','b','g']\nsns.set(rc={'figure.figsize':(15,17)})\nsns.set_style(style='white')\nfor i in range(len(columns)):\n    \n    plt.subplot(4,2,i+1)\n    sns.distplot(df[columns[i]], hist=True, rug=True, color=colours[i])","2b9a39e0":"sns.lmplot(data=df, x=\"Age\", y=\"BloodPressure\",hue = \"Outcome\",palette=\"Set2\",col = \"Outcome\")\nsns.lmplot(data=df, x=\"Age\", y=\"Glucose\",hue = \"Outcome\",palette=\"Set1\")\nsns.lmplot(data=df, x=\"Age\", y=\"SkinThickness\",hue = \"Outcome\",palette=\"Set3\")","beec3d1a":"import plotly.express as px\nfor i in df.columns:\n    if i!='Outcome':\n        fig = px.box(df, df[\"Outcome\"],y=df[i], color = 'Outcome')\n        fig.show()\n","b5c51fec":"fig = px.histogram(df, x = df['Pregnancies'], color = 'Outcome')\nfig.update_layout(\n    bargap=0.2)\nfig.show()","faf2fb4f":"from scipy.stats import skew\nfor col in df.drop('Outcome', axis = 1).columns:\n    print(\"Skewness for the column {} is {}\".format(col, df[col].skew()))","460a051d":"corr = df.corr()\nf, ax = plt.subplots(figsize=(20, 10))\nsns.heatmap(corr,vmax=1.0, center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": .5}, annot = True)","60190df7":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = df.drop('Outcome', axis = 1)\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)\n\nsc = StandardScaler()\nX_train =  pd.DataFrame(sc.fit_transform(X_train),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'])\nX_test = pd.DataFrame(sc.fit_transform(X_test),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'])","79994da3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV","1a93c7a2":"log_params = {'penalty':['l1', 'l2'], \n              'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 100], \n              'solver':['liblinear', 'saga']} \nlog_model = GridSearchCV(LogisticRegression(), log_params, cv=5) #Tuning the hyper-parameters\nlog_model.fit(X_train, y_train)\nlog_predict = log_model.predict(X_test)\nlog_score = log_model.best_score_","88f03223":"# knn\nknn_params = {'n_neighbors': list(range(3, 20, 2)),\n          'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n          'metric':['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\nknn_model = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5) #Tuning the hyper-parameters\nknn_model.fit(X_train, y_train)\nknn_predict = knn_model.predict(X_test)\nknn_score = knn_model.best_score_","b7756ee5":"# Support Vector Classification(svc)\nsvc_params = {'C': [0.001, 0.01, 0.1, 1],\n              'kernel': [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]}\nsvc_model = GridSearchCV(SVC(), svc_params, cv=5) #Tuning the hyper-parameters\nsvc_model.fit(X_train, y_train)\nsvc_predict = svc_model.predict(X_test)\nsvc_score = svc_model.best_score_","19a89491":"# Decision Tree\ndt_params = {'criterion' : ['gini', 'entropy'],\n              'splitter': ['random', 'best'], \n              'max_depth': [3, 5, 7, 9, 11, 13]}\ndt_model = GridSearchCV(DecisionTreeClassifier(), dt_params, cv=5) #Tuning the hyper-parameters\ndt_model.fit(X_train, y_train)\ndt_predict = dt_model.predict(X_test)\ndt_score = dt_model.best_score_","ac7b801d":"# Random Forest\nrf_params = {'criterion' : ['gini', 'entropy'],\n             'n_estimators': list(range(5, 26, 5)),\n             'max_depth': list(range(3, 20, 2))}\nrf_model = GridSearchCV(RandomForestClassifier(), rf_params, cv=5) #Tuning the hyper-parameters\nrf_model.fit(X_train, y_train)\nrf_predict = rf_model.predict(X_test)\nrf_score = rf_model.best_score_","b05c815e":"# lgb\nlgb_params = {'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n                   'learning_rate': [0.01, 0.05, 0.1],\n                   'num_leaves': [7, 15, 31],\n                  }\nlgb_model = GridSearchCV(LGBMClassifier(), lgb_params, cv=5) #Tuning the hyper-parameters\nlgb_model.fit(X_train, y_train)\nlgb_predict = lgb_model.predict(X_test)\nlgb_score = lgb_model.best_score_\n","c9f6b7d3":"# xgb\nxgb_params = {'max_depth': [3, 5, 7, 9],\n              'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n              'learning_rate': [0.01, 0.05, 0.1]}\nxgb_model = GridSearchCV(xgb.XGBClassifier(eval_metric='logloss'), xgb_params, cv=5) #Tuning the hyper-parameters\nxgb_model.fit(X_train, y_train)\nxgb_predict = xgb_model.predict(X_test)\nxgb_score = xgb_model.best_score_","098c8373":"models = ['LogisticRegression', 'KNeighborsClassifier', 'SVC', 'DecisionTreeClassifier', \n          'RandomForestClassifier', 'LGBMClassifier', 'XGBClassifier']\nscores = [log_score, knn_score, svc_score,dt_score,rf_score, lgb_score, xgb_score]\nscore_table = pd.DataFrame({'Model':models, 'Score':scores})\nscore_table.sort_values(by='Score', axis=0, ascending=False)\nprint(score_table.sort_values(by='Score', ascending=False))\nsns.barplot(x = score_table['Score'], y = score_table['Model'], palette='inferno');","293784f0":"from sklearn import metrics\nprint('Classification Report_test','\\n',metrics.classification_report(y_test, log_predict))","d95dd1d4":"From the above heatmap, we can observe that all the features are weakly correlated, so that removes multicollinearity out of equation.","689ae370":"We see columns **Insulin** and **DiabetesPedigreeFunction** are quite skewed. On the other hand, columns like **Pregnancies, Glucose, BloodPressure, SkinThickness** and **BMI** are not that much skewed. We can fill null values with the mean for these columns, but for columns like Insulin and DiabetesPedigreeFunction, we will have to replace them will median due to the effect of skewness.","f3dcbeaa":"Thus, **Logistic Regression** is the best performing model.","66141bcd":"We can see that **higher the number of pregnancies**, **more is the risk of having diabetes**.","db68bc7d":"## Dataset Splitting and Features Scaling","70eacc0b":"**All null values have been taken care of.**","fd926c44":"The different columns present in the dataset are:\n\n- **Pregnancies** -> Number of times Pregnant\n\n- **Glucose** -> Plasma glucose concentration\n\n- **BloodPressure** -> Diastolic blood pressure (mm Hg)\n\n- **SkinThickness** -> Triceps skin fold thickness (mm)\n\n- **Insulin** -> 2-Hour serum insulin (mu U\/ml)\n\n- **BMI** -> Body Mass Index\n\n- **DiabetesPedigreeFunction** -> Diabetes pedigree function (a function which scores likelihood of diabetes based on family history).\n\n- **Age** -> Age in years\n\n- **Outcome** -> Whether the lady is diabetic or not, 0 represents the person is not diabetic and 1 represents that the person is diabetic.","9f9d4679":"Let's have a look at the distribution of the data.","d6fa8ab1":"## Setting up Environment","1ca817bd":"## Evaluation\nTime to evaluate the models.","2d0376f0":"Let's now check **skewness of the data**.","2417550e":"#### Thus, we reach a few important conclusions.\n- Higher Glucose level leads to more chances of Diabetes!\n- Probabilty of diabetes is higher when Blood pressure is high.\n- Higher the Insulin level more the chances of diabetes.\n- Higher the BMI more the chances of diabetes.\n- Diabetic people have higher DiabetesPedigreeFunction value i.e. genetic influence plays some role in the Diabetes among patients.\n- There is less chance of diabetes among young people.","ceeac3db":"## Correlation Matrix","8d0f6239":"## EDA and Visualization","55acaff1":"## Baseline Models","d63afd98":"### **Let's now check how Diabetes affects the chances of being pregnant.**","8787bf2d":"Let's model these one by one.","8c5624b0":"## Cleaning dataset","058ea8fd":"Hope you liked the notebook, any suggestions would be highly appreciated.\n\n**Please upvote if you liked it.**","26cc69f2":"## Modelling","003eab90":"**Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high.**\nThe objective of the dataset is to **diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset**. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n### Context\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n### Content\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","0c59a931":"Let's replace all 0 values with null."}}