{"cell_type":{"653a00ca":"code","5b2dde90":"code","8492d7c5":"code","06160ee1":"code","80931bf1":"code","2088565f":"code","60aa3f65":"code","362cd5e7":"code","3bd44fd8":"code","030abe27":"code","cc25d55b":"code","ee3bea84":"code","4371ef61":"code","2dab79f0":"code","c2686169":"code","a5c9b714":"code","e0583441":"code","eacd3b77":"code","a916c291":"code","9d661a47":"code","54fb9e3a":"code","e3eba706":"code","cbe904c7":"code","15c9c318":"code","f5561bfa":"code","d0a7269e":"code","ac9c5b0f":"code","6bab0e70":"code","9fa8e342":"code","c3f23da0":"code","17fd178f":"code","024c585f":"code","8cd0324a":"code","eed0c261":"markdown","2426dae4":"markdown","03c1fe0f":"markdown","e34a3f48":"markdown","479ea481":"markdown","205e17e4":"markdown","c99b5f85":"markdown","3b1a5662":"markdown","42ef2979":"markdown","a3be3af6":"markdown","27a1e06d":"markdown","3e4aa14d":"markdown","af608165":"markdown","e36469c6":"markdown","355c2540":"markdown","ef3e7f24":"markdown","18a5e2bb":"markdown","588fb152":"markdown","389352e5":"markdown","9e4929dd":"markdown","e3d59a58":"markdown","d2ef2656":"markdown","9bb48f64":"markdown","b121523c":"markdown","9eac21ea":"markdown","3122be94":"markdown","525774e6":"markdown","7a28d5b1":"markdown","99de7114":"markdown","81538997":"markdown","3e39505a":"markdown","9c00e391":"markdown","906b2a45":"markdown"},"source":{"653a00ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b2dde90":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# np.set_printoptions(suppress=True)","8492d7c5":"df = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')\nprint(f\"sample size = {df.shape[0]}\\nnumber of columns = {df.shape[1]}\")\ndf.head()","06160ee1":"df.info()","80931bf1":"df.describe()","2088565f":"fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,5))\nax1.hist(df['price']\/1000000, bins=50)\nax2.hist(df['price']\/1000000, bins=50)\nax2.set_xlim(0,2)\nax2.set_xlabel('price in millions')\nax3.boxplot(df['price']\/1000000);","60aa3f65":"# the % house price over 2 millions\n(df['price'] > 2000000).mean() * 100","362cd5e7":"df = df[df['price'] <= 2000000]","3bd44fd8":"df.corr().style.background_gradient(cmap='coolwarm')","030abe27":"fig, ax = plt.subplots()\nfig.set_size_inches(15, 8)\nsns.heatmap(df.corr().round(4), annot=True, ax=ax);","cc25d55b":"pd.DataFrame(df.corr()['price'].sort_values(ascending=False)).style.background_gradient(cmap='coolwarm')#.set_precision(2)","ee3bea84":"sns.jointplot(x='sqft_living', y='sqft_above', data=df);","4371ef61":"sns.jointplot(x='sqft_living', y='price', data=df);","2dab79f0":"sns.jointplot(x='yr_built', y='price', data=df);","c2686169":"fig, ax =plt.subplots(1,2, sharey=True)\nfig.set_size_inches(12,6)\nax[0].scatter(x='lat', y='price', data=df, alpha=0.2)\nax[1].scatter(x='long', y='price', data=df, alpha=0.2)","a5c9b714":"df['above_47.5_lat'] = (df['lat'] > 47.5).astype(int)\ndf['above_47.5_lat'].value_counts().plot.bar()","e0583441":"df[['price', 'above_47.5_lat']].corr()","eacd3b77":"# features = ['sqft_living']\nfeatures  = ['sqft_living', 'grade', 'sqft_living15', 'bathrooms','lat', 'view', 'bedrooms', 'sqft_basement', 'floors']\nfeatures  = ['sqft_living', 'grade', 'sqft_living15', 'bathrooms','above_47.5_lat', 'view', 'bedrooms', 'sqft_basement', 'floors']","a916c291":"X = df[features]#.to_numpy()\nprint(X.shape)\nX.iloc[0]","9d661a47":"y = df['price']#.to_numpy()\n# y = y[:, None]\ny.shape","54fb9e3a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","e3eba706":"# Build a model with all default parameters\nmodel = LinearRegression()","cbe904c7":"model.fit(x_train, y_train)","15c9c318":"y_pred = model.predict(x_test)\ny_pred","f5561bfa":"# coefficient of determination - how well observed outcomes are replicated by the model, with 1 be the perfect score, it can have negative score because the model can be arbitrarily worse\nprint('R squared = ', model.score(x_test, y_test))","d0a7269e":"mean_absolute_error(y_test, y_pred)","ac9c5b0f":"print('Ratio of mean absolute error to the mean true outcomes: ', mean_absolute_error(y_test, y_pred) \/ y_test.mean())","6bab0e70":"sns.displot((y_pred - y_test).values, kde=True)\nplt.title('prediction error\/residual distribution')\nplt.xlabel('prediction error')","9fa8e342":"theta = model.coef_\nprint(\"Model coefficents\/theta(1-n):\\n\", theta)","c3f23da0":"theta0 = model.intercept_\nprint('The model intercept\/theta(0): ', theta0)","17fd178f":"test_input = x_test.head(1).values[0]\nprint('first test input is:\\n', test_input)\n\npredicted_price = theta @ test_input + theta0\nprint('\\nCalculated prediction is:\\t\\t\\t', predicted_price)\n\nprint('The first predicted model from y_pred is:\\t', y_pred[0])","024c585f":"df_test = x_test.copy()\ndf_test['price'] = y_test\ndf_test.reset_index(inplace=True)\ndf_test['pred'] = y_pred.round()\ncolumns = features + ['price','pred']\ndf_test[columns]","8cd0324a":"def plot_scatter(ax, by):\n    ax.scatter(df_test[by], df_test['price'], alpha=0.3, label='real_price')\n    ax.scatter(df_test[by], df_test['pred'], alpha=0.3, label='prediction')\n    ax.set_title('house price by ' + by)\n    ax.legend()\n\n# plot top n features\nn = 3 if len(features) > 3 else len(features)\nfig, ax = plt.subplots(1, n, sharey=True)\nfor i in range(n):\n    fig.set_size_inches(6*n,6)\n    if isinstance(ax, np.ndarray):\n        plot_scatter(ax[i], features[i])\n    else:\n        plot_scatter(ax, features[i])","eed0c261":"Now lets compare its correlation with price:","2426dae4":"How model predict the price? \n\nThe price is calculated as: $$h_{\\theta}(X)=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$ \n\nWhere $\\theta_{0}$ is the intercept, and $\\theta_{1}...\\theta_{n}$ are the coefficients. This can be calculated very efficiently using matrix multiplication of coefficients and X features, then plus the intercept scalar value as:\n\n```coefficients @ X + intercept``` \n\nNow let's calculate the prediction using the coefficients and intercept for the first test sample, it should match the first value from y_pred","03c1fe0f":"## Fit\/train the model\nWe can see how simple the scikit learn provided API is for train a model, simply call the fit method.","e34a3f48":"## Predict price using the test set","479ea481":"## Look inside the model:  model parameters and how model predict price\n\nExamine the model coefficients\/theta(1-n) and intercept\/theta0","205e17e4":"## Results:\n\nHere we see for single feature of sqrt_living, our model got R squared score of 0.465, and mean_absolute_error of 153402 which counts about 30% of the mean price\n\nWhen we increase to 9 features we achieved score of 0.657, which is a big improvment, and mean_absolute_error reduced to 115007 which now counts only about 22% of the mean price\n\nwe can further improve it by re-engineer the feature of lat into a new feature of above_47.5_lat, doing so we further improved score to 0.685, and reduced mean_absolute_error to 110659 which now counts only about 21% of the mean price","c99b5f85":"From above it show that most expensive houses are located at above latitude of 47.5, so we can add a feature to denote if it is above 47.5 latitude, we then convert the value to 1 if it is True and to 0 if it is False, so it can be feed into model. ","3b1a5662":"Get the training features as X","42ef2979":"From above plot we see the house price has a long tail with a few very expensive houses, but most house prices are below 2 million dollrs, we could remove the house sample with price over 2 millions which are outliers based on the statistic boxplot","a3be3af6":"We also see that yr_built has a low correlation with price, which is a little counter intuitive, so let's plot the relationship between yr_built and price","27a1e06d":"Plot the correlation between loaction and price","3e4aa14d":"## Select features\n\nwhat feature(s) do we want to pick?\n\nLet's first do a simply model that only consider one feature of sqft_living. Then we will choose multiple features to to train model to compare model performance from the two approaches.\n\n1. Model with only one feature: sqft_living\n2. Model with mutiple features: we already see that sqrt_above is highly correlated with sqft_living so we exclude it from the feature, we will pick the features that has corr value above 0.2:\nsqft_living, grade, sqft_living15, bathrooms, lat, view, bedrooms, sqft_basement, floors\n3. Replace \"lat\" feature with re-engineered new feature of \"above_47.5_lat\"\n\nUncomment the below code to try both feature selection approaches","af608165":"Plot the target price\/prediction against top (3) features","e36469c6":"sqrt_living has the highest correlation price, so we can keep sqrt_living and remove the correlated sqrt_above from our feature when train model. Now let's visualize relationship between sqft_living and price","355c2540":"From above we see sqrt_living and sqrt_above has very high corrlation of 0.87. Let's visualize it below:","ef3e7f24":"So the houses which have price above 2 millions only count for less than 1% of all house data, we can simply remove them","18a5e2bb":"Check the data type and statistics","588fb152":"## Split training and test data\n\nHere we choose 80\/20 split where we use 80% of sample data to train model, and set aside 20% of sample to test the model prediction. We also set random_state so we can the same random sample for train and test for model evaluation later","389352e5":"The above correlation data may not display the color gird properly, as in github for example. Without color grid, it is hard to visualize the different correlations. So let's also plot the heatmap of correlations.","9e4929dd":"## Defind a linear regression model\n\nusing default hyper paramters https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html\n\nLinearRegression fits a linear model with coefficients w = (w1, \u2026, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.","e3d59a58":"## Want to better understand the how the model gets trained and the mathematics behind it?\n\nIn this exercise we built a LinearRegression model to predice new outcomes based on the historical outcomes or events, for additional information on the linear regression model please refer to https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html\n\nTo better understand the mathematics behind the model, I highly recommend the popular machine learning course from coursera: https:\/\/www.coursera.org\/learn\/machine-learning","d2ef2656":"## Get the data","9bb48f64":"## Exploratory data analysis\n\nCheck the house price distributions","b121523c":"Check the corralation related to house price ","9eac21ea":"We can see the prediction error is normal distributed with center around 0","3122be94":"## Plot the predicted price and price against the input features\n\nLet's first build a dataframe with test features, target label, and prediction","525774e6":"The result show that year built does not really have meaningful impact to price","7a28d5b1":"Get the target labels as y. Here we have lower case y to denote it as vector, not matrix","99de7114":"Check the correlations","81538997":"## Evaluate the model","3e39505a":"Check the error distribution","9c00e391":"# Introduction to Machine Learning with Scikit Learn \n\n#### What is machine learning?\n\nThere are many ways to describe what is machine learning, you can find one at https:\/\/www.ibm.com\/cloud\/learn\/machine-learning\n\n\nI like what Addreas Mueller described in the youtube video https:\/\/www.youtube.com\/watch?v=4PXAztQtoTg:\n> Predictive Modeling 101: Make predictions of outcome of repeated events.\n> Machine learning is useful when the frequency of the repetitive envent is high, or the historical observations or data is large, and an individual mistake is not too costly \n\n> All models are wrong, but some are useful - George Box\n\nScikit Learn - Library of Machine Learning algorithms, built on top of Python, NumPy, SciPy, Cython https:\/\/scikit-learn.org\/\n\nNow let's build a model using the KC house data set from https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction to predict house price\n","906b2a45":"We can see the correlation increased from 0.362183 of \"lat\" to 0.4367 of out new feature \"above_47.5_lat\""}}