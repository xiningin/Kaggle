{"cell_type":{"ee094705":"code","75b5eafa":"code","c5a0c8f0":"code","b7bee82d":"code","d9056311":"code","efb657ff":"code","e9b474dd":"code","6e1252e6":"code","de909e90":"code","93373928":"code","36242f2e":"code","db7e407e":"code","b4683c52":"code","7fa72fdf":"code","5fb7b6ff":"code","eb739c43":"code","6e6df588":"code","167332ad":"code","7be33ac6":"code","a42f507b":"code","1c9d4f70":"code","b3009ed2":"code","ca8f37c2":"code","671f8f71":"code","777f8c15":"code","f68b62dd":"code","3a5f9138":"code","5ac4b3a3":"code","d79a1b54":"code","09652221":"code","95670149":"code","328d2db0":"code","cb65a6de":"code","9927588a":"code","b5e42867":"code","bf16a8e5":"code","17944f98":"code","e8eeb898":"code","99668c3b":"code","9bd55e0e":"code","f1014314":"code","0d4c4f0d":"code","8cce5239":"code","8ee117a3":"markdown","d24ef2e2":"markdown","7b91f9a8":"markdown","09dc817b":"markdown","a993cbfc":"markdown","8af26fc8":"markdown","f8f0b351":"markdown","7a5a4d49":"markdown","abf3d240":"markdown","b7e48422":"markdown","b5acc48c":"markdown","0ce7fba3":"markdown","425170dd":"markdown","2061f535":"markdown","e6e49113":"markdown","8d09ea7f":"markdown","13292fa0":"markdown","ebd9e541":"markdown","44d41d88":"markdown","9522b73c":"markdown","d93e50ea":"markdown","cebcb89d":"markdown","4447423a":"markdown","31a6e9f6":"markdown","b4b2dfb2":"markdown","921bc2bd":"markdown","0f5b5de2":"markdown","87fe7d08":"markdown","a846e774":"markdown","91aea3f4":"markdown","15c149bf":"markdown","99b079ad":"markdown","cc942195":"markdown","9cb2bfea":"markdown","523803c8":"markdown","74b32ca5":"markdown"},"source":{"ee094705":"import warnings\nwarnings.filterwarnings('ignore')","75b5eafa":"!pip install xgboost\n!pip install imbalanced-learn","c5a0c8f0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","b7bee82d":"df = pd.read_csv('..\/input\/paysim1\/PS_20174392719_1491204439457_log.csv')\ndf.head()","d9056311":"df.info()","efb657ff":"frauds = df[df.isFraud == 1]\nnon_frauds = df[df.isFraud == 0]\n\nfrauds['balanceChange'] = frauds['newbalanceOrig'] - frauds['oldbalanceOrg']\nnon_frauds['balanceChange'] = non_frauds['newbalanceOrig'] - non_frauds['oldbalanceOrg']\n\nfrauds_mean_balancechange = frauds['balanceChange'].mean()\nnfrauds_mean_balancechange = non_frauds['balanceChange'].mean()\n\nwidth = 0.8\nfig, ax = plt.subplots(1,1, figsize = (10, 6))\nax.bar(0.5, nfrauds_mean_balancechange, width, label='Avg. Non Fraud Account Balance Change', align='center')\nax.bar(1.5, frauds_mean_balancechange, width, label='Avg. Fraud Account Balance Change')\nfig.legend(loc='best')\nplt.axis([0, 2, -1500000, 200000])","e9b474dd":"num_positive_frauds = len(frauds[frauds['balanceChange'] > 0])\nnum_positive_frauds","6e1252e6":"paysim_negative = pd.concat([frauds[frauds['balanceChange'] <= 0], non_frauds[non_frauds['balanceChange'] <= 0]])","de909e90":"num_frauds = len(paysim_negative[paysim_negative['isFraud'] == 1])\nnum_frauds \/ len(paysim_negative)","93373928":"paysim_negative.corr()","36242f2e":"paysim_byClient = paysim_negative[['nameDest', 'oldbalanceOrg', 'oldbalanceDest', 'balanceChange']].groupby(['nameDest']).mean()\nfrauds_byClient = paysim_negative[['nameDest', 'isFraud']].groupby(['nameDest']).sum()\nclientData = pd.concat([paysim_byClient, frauds_byClient], axis=1)\nclientData['numTrans'] = paysim_negative[['nameDest', 'isFraud']].groupby(['nameDest']).count()['isFraud']\nclientData = clientData.sort_values(by='isFraud', ascending=False)\nclientData.head(20)","db7e407e":"from scipy import stats\n\noldBalance = clientData[['oldbalanceOrg', 'isFraud']].groupby(['oldbalanceOrg']).sum()\nkde = stats.gaussian_kde(oldBalance.index)\nxx = np.linspace(0, 9, 1000)\nplt.plot(xx, kde(xx))\noldBalance[1:].hist(bins=10)","b4683c52":"newBalanceOrig = paysim_negative[['newbalanceOrig', 'isFraud']].groupby(['newbalanceOrig']).sum()\nnewBalanceOrig[1:].hist(bins=10)","7fa72fdf":"oldBalanceDest = paysim_negative[['oldbalanceDest', 'isFraud']].groupby(['oldbalanceDest']).sum()\noldBalanceDest[1:].hist(bins=10)","5fb7b6ff":"newBalanceDest = paysim_negative[['newbalanceDest', 'isFraud']].groupby(['newbalanceDest']).sum()\nnewBalanceDest[1:].hist(bins=10)","eb739c43":"numTrans = clientData[['numTrans', 'isFraud']].groupby(['numTrans']).sum()\nkde = stats.gaussian_kde(numTrans.index)\nxx = np.linspace(0, max(numTrans.isFraud), 1000)\nplt.plot(xx, kde(xx))\nnumTrans.hist(bins=10)","6e6df588":"ax = df.type.value_counts().plot.bar(figsize = (12, 6))\nax.set_title(\"Distribution of transactions for different payment types\", fontsize = 20)\nplt.xticks(rotation = 45)","167332ad":"df.drop(columns = ['step', 'isFlaggedFraud', 'nameOrig', 'nameDest'], inplace = True)\ndf.isFraud.value_counts()","7be33ac6":"X = df[['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']].copy()\ny = df['isFraud'].copy()\nX.type = X.type.astype('category')","a42f507b":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncol_transform = make_column_transformer((['type'], OneHotEncoder(sparse = False, drop = 'first')),\n                                        remainder = 'passthrough')\nX = pd.DataFrame(col_transform.fit_transform(X))","1c9d4f70":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom datetime import datetime\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n\ncv_pr_aucs = []\nrecalls = []\nprecisions = []\nthresholds_list = []\nskf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    print(\"Started training the model for the current fold at\", datetime.now())\n    rf_model = RandomForestClassifier(random_state = 42)\n    rf_model.fit(X_train, y_train)\n    y_pred = list(rf_model.predict_proba(X_test)[:, 1])\n    precision, recall, thresholds = precision_recall_curve(y_test.tolist(), y_pred)\n    recalls.append(recall)\n    precisions.append(precision)\n    thresholds_list.append(thresholds)\n    cv_pr_aucs.append(auc(recall, precision))\n    print(\"Completed training the model for the current fold at\", datetime.now(), \"\\n\")","b3009ed2":"rf_model = RandomForestClassifier(random_state = 42)\nrf_model.fit(X, y)\n\nfig, axis = plt.subplots(figsize = (10, 6))\nfeature_list = ['CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\naxis.bar(feature_list, (rf_model.feature_importances_ * 100))\nplt.xticks(rotation = 45)\naxis.set_title('Feature Importance from Random Forests', fontsize = 17)\naxis.set_xlabel('Feature', fontsize = 17)\naxis.set_ylabel('Importance (%)', fontsize = 17)","ca8f37c2":"print(\"Area under PR curve:\", np.mean(cv_pr_aucs))\n\nprecision = np.mean(precisions, axis = 0)\nrecall = np.mean(recalls, axis = 0)\nthreshold = np.mean(thresholds_list, axis = 0)\n\nfig = plt.figure(figsize = (10, 6))\nplt.plot(recall, precision, marker = '.')\nplt.plot([0, 1], [sum(y == 1)\/len(y), sum(y == 1)\/len(y)], linestyle = '--')\nax = plt.gca()\nax.axvline(x = recall[int(np.where(threshold == 0.5)[0])], color = 'r', linestyle = '--', ymin = 0.05, ymax = 1)\nax.annotate('50% Threshold', (recall[int(np.where(threshold == 0.5)[0])] - 0.085, recall[int(np.where(threshold == 0.5)[0])] - 0.3), fontsize = 15)\nax.set_title(\"Precision-Recall Curve\", fontsize = 17)\nax.set_xlabel('Recall', fontsize = 17)\nax.set_ylabel('Precision', fontsize = 17)","671f8f71":"from xgboost import XGBClassifier\n\ncv_pr_aucs = []\nrecalls = []\nprecisions = []\nskf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    print(\"Started training the model for the current fold at\", datetime.now())\n    xgb_model = XGBClassifier(random_state = 42)\n    xgb_model.fit(X_train, y_train)\n    y_pred = list(xgb_model.predict_proba(X_test)[:, 1])\n    precision, recall, thresholds = precision_recall_curve(y_test.tolist(), y_pred)\n    recalls.append(recall)\n    precisions.append(precision)\n    cv_pr_aucs.append(auc(recall, precision))\n    print(\"Completed training the model for the current fold at\", datetime.now(), \"\\n\")\n    \nprint(\"Area under PR curve:\", np.mean(cv_pr_aucs))","777f8c15":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators': [10, 50, 100],\n              'criterion': ['gini', 'entropy'],\n              'max_depth': [None, 3, 5]\n             }","f68b62dd":"from sklearn.utils import resample\n\ncv_pr_aucs = []\nrecalls = []\nprecisions = []\nthresholds_list = []\nskf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]    \n    \n    X_frauds_df = X_train[y_train == 1]\n    y_frauds_df = y_train[y_train == 1]\n    \n    X_not_frauds_df = X_train[y_train == 0]\n    y_not_frauds_df = y_train[y_train == 0]\n    \n    X_not_frauds_df, y_not_frauds_df = resample(X_not_frauds_df, y_not_frauds_df, replace = False, n_samples = len(X_frauds_df), random_state = 42)\n    \n    X_train = pd.concat([X_frauds_df, X_not_frauds_df])\n    y_train = pd.concat([y_frauds_df, y_not_frauds_df])\n    \n    X_train = X_train.reset_index().drop(columns = ['index'])\n    y_train = y_train.reset_index().drop(columns = ['index'])\n    \n    print(\"Started training the model for the current fold at\", datetime.now())\n    rf_model = RandomForestClassifier(random_state = 42)\n    rf_model = GridSearchCV(rf_model, parameters, cv = 5, scoring = 'average_precision')\n    rf_model.fit(X_train, y_train)\n    y_pred = list(rf_model.predict_proba(X_test)[:, 1])\n    precision, recall, thresholds = precision_recall_curve(y_test.tolist(), y_pred)\n    recalls.append(recall)\n    precisions.append(precision)\n    thresholds_list.append(thresholds)\n    cv_pr_aucs.append(auc(recall, precision))\n    print(\"Completed training the model for the current fold at\", datetime.now(), \"\\n\")","3a5f9138":"print(\"Area under PR curve:\", np.mean(cv_pr_aucs))","5ac4b3a3":"parameters = {'n_estimators': [10, 50, 100],\n              'criterion': ['gini', 'entropy'],\n              'max_depth': [2, 3, 6]\n             }","d79a1b54":"cv_pr_aucs = []\nrecalls = []\nprecisions = []\nskf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]    \n    \n    X_frauds_df = X_train[y_train == 1]\n    y_frauds_df = y_train[y_train == 1]\n    \n    X_not_frauds_df = X_train[y_train == 0]\n    y_not_frauds_df = y_train[y_train == 0]\n    \n    X_not_frauds_df, y_not_frauds_df = resample(X_not_frauds_df, y_not_frauds_df, replace = False, n_samples = len(X_frauds_df), random_state = 42)\n    \n    X_train = pd.concat([X_frauds_df, X_not_frauds_df])\n    y_train = pd.concat([y_frauds_df, y_not_frauds_df])\n    \n    X_train = X_train.reset_index().drop(columns = ['index'])\n    y_train = y_train.reset_index().drop(columns = ['index'])\n    \n    print(\"Started training the model for the current fold at\", datetime.now())\n    xgb_model = XGBClassifier(random_state = 42)\n    xgb_model = GridSearchCV(xgb_model, parameters, cv = 5, scoring = 'average_precision')\n    xgb_model.fit(X_train, y_train)\n    y_pred = list(xgb_model.predict_proba(X_test)[:, 1])\n    precision, recall, thresholds = precision_recall_curve(y_test.tolist(), y_pred)\n    recalls.append(recall)\n    precisions.append(precision)\n    cv_pr_aucs.append(auc(recall, precision))\n    print(\"Completed training the model for the current fold at\", datetime.now(), \"\\n\")","09652221":"print(\"Area under PR curve:\", np.mean(cv_pr_aucs))","95670149":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout","328d2db0":"model = Sequential()\nmodel.add(Dense(12, input_dim = 9, activation = 'relu'))\nmodel.add(Dropout(0.3, seed = 42))\nmodel.add(Dense(8, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","cb65a6de":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, stratify = y, random_state = 42)\n\nX_frauds_df = X_train[y_train == 1]\ny_frauds_df = y_train[y_train == 1]\n\nX_not_frauds_df = X_train[y_train == 0]\ny_not_frauds_df = y_train[y_train == 0]\n\nX_not_frauds_df, y_not_frauds_df = resample(X_not_frauds_df, y_not_frauds_df, replace = False, n_samples = len(X_frauds_df), random_state = 42)\n\nX_train = pd.concat([X_frauds_df, X_not_frauds_df])\ny_train = pd.concat([y_frauds_df, y_not_frauds_df])\n\nX_train = X_train.reset_index().drop(columns = ['index'])\ny_train = y_train.reset_index().drop(columns = ['index'])\n\nstd_scaler = StandardScaler()\nX_train = pd.DataFrame(std_scaler.fit_transform(X_train))","9927588a":"model.fit(X_train, y_train, epochs = 150, batch_size = 10)","b5e42867":"losses = model.history.history['loss']\naccs = model.history.history['acc']","bf16a8e5":"fig = plt.figure(figsize = (10, 6))\nplt.plot(losses)\nax = plt.gca()\nax.set_title(\"Loss (Cross-Entropy) vs Epochs\", fontsize = 17)\nax.set_xlabel('Epoch', fontsize = 17)\nax.set_ylabel('Loss', fontsize = 17)","17944f98":"fig = plt.figure(figsize = (10, 6))\nplt.plot(np.array(accs)*100)\nax = plt.gca()\nax.set_title(\"Accuracy vs Epochs\", fontsize = 17)\nax.set_xlabel('Epoch', fontsize = 17)\nax.set_ylabel('Accuracy (%)', fontsize = 17)","e8eeb898":"y_pred = model.predict_proba(std_scaler.transform(X_test))[:, 0]\nprecision, recall, thresholds = precision_recall_curve(y_test.tolist(), y_pred)\nprint(\"Area under PR curve: \", auc(recall, precision))","99668c3b":"fig = plt.figure(figsize = (10, 6))\nplt.plot(recall, precision, marker = '.')\nplt.plot([0, 1], [sum(y == 1)\/len(y), sum(y == 1)\/len(y)], linestyle = '--')\nax = plt.gca()\nax.axvline(x = recall[int(np.where((thresholds > 0.49) & (thresholds < 0.51))[0][0])], color = 'r', linestyle = '--', ymin = 0.05, ymax = 0.96)\nax.annotate('50% Threshold', (recall[int(np.where((thresholds > 0.49) & (thresholds < 0.51))[0][0])] - 0.15, recall[int(np.where((thresholds > 0.49) & (thresholds < 0.51))[0][0])] - 0.3), fontsize = 15)\nax.set_title(\"Precision-Recall Curve\", fontsize = 17)\nax.set_xlabel('Recall', fontsize = 17)\nax.set_ylabel('Precision', fontsize = 17)","9bd55e0e":"from imblearn.over_sampling import SMOTE\nfrom datetime import datetime\n\ncv_pr_aucs = []\nrecalls = []\nprecisions = []\nthresholds_list = []\nskf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]   \n    smote = SMOTE(random_state = 42)\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n    X_train = pd.DataFrame(X_train)\n    y_train = pd.Series(y_train)\n    \n    print(\"Started training the model for the current fold at\", datetime.now())\n    rf_model = RandomForestClassifier(random_state = 42)\n    rf_model.fit(X_train, y_train)\n    y_pred = list(rf_model.predict_proba(X_test)[:, 1])\n    precision, recall, thresholds = precision_recall_curve(y_test.tolist(), y_pred)\n    recalls.append(recall)\n    precisions.append(precision)\n    thresholds_list.append(thresholds)\n    cv_pr_aucs.append(auc(recall, precision))\n    print(\"Completed training the model for the current fold at\", datetime.now(), \"\\n\")","f1014314":"print(\"Area under PR curve:\", np.mean(cv_pr_aucs))\n\nprecision = np.mean(precisions, axis = 0)\nrecall = np.mean(recalls, axis = 0)\nthreshold = np.mean(thresholds_list, axis = 0)\n\nfig = plt.figure(figsize = (10, 6))\nplt.plot(recall, precision, marker = '.')\nplt.plot([0, 1], [sum(y == 1)\/len(y), sum(y == 1)\/len(y)], linestyle = '--')\nax = plt.gca()\nax.axvline(x = recall[int(np.where(threshold == 0.5)[0])], color = 'r', linestyle = '--', ymin = 0.05, ymax = 1)\nax.annotate('50% Threshold', (recall[int(np.where(threshold == 0.5)[0])] - 0.085, recall[int(np.where(threshold == 0.5)[0])] - 0.3), fontsize = 15)\nax.set_title(\"Precision-Recall Curve\", fontsize = 17)\nax.set_xlabel('Recall', fontsize = 17)\nax.set_ylabel('Precision', fontsize = 17)","0d4c4f0d":"cv_pr_aucs = []\nrecalls = []\nprecisions = []\nthresholds_list = []\nskf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]   \n    smote = SMOTE(random_state = 42)\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n    X_train = pd.DataFrame(X_train)\n    y_train = pd.Series(y_train)\n    \n    print(\"Started training the model for the current fold at\", datetime.now())\n    xgb_model = XGBClassifier(random_state = 42)\n    xgb_model.fit(X_train, y_train)\n    y_pred = list(xgb_model.predict_proba(X_test)[:, 1])\n    precision, recall, thresholds = precision_recall_curve(y_test.tolist(), y_pred)\n    recalls.append(recall)\n    precisions.append(precision)\n    thresholds_list.append(thresholds)\n    cv_pr_aucs.append(auc(recall, precision))\n    print(\"Completed training the model for the current fold at\", datetime.now(), \"\\n\")","8cce5239":"print(\"Area under PR curve:\", np.mean(cv_pr_aucs))","8ee117a3":"### XGBoost","d24ef2e2":"Thus, we have analyzed the dataset and obtained a fairly accurate predictive model using Random Forests without any sampling, closely followed by Random Forests with oversampling. In order to reduce our chances of performing a Type II error (predicting a transaction as NOT FRAUD when it actually is one), we can move our threshold to less that 50% to increase Recall at the expense of Precision. This needs to be studied on a case-by-case basis.","7b91f9a8":"We are running a neural network only for the undersampling case as the training time for the other two cases (oversampling and no sampling) are extremely high (more than 8 hours) and the results are far worse than some of the other less complex models.\n\nAlso, we are not using cross-validation to evaluate the model as it's extremely time-consuming. Instead, we are using the train-test set approach.","09dc817b":"Let's plot the feature importances since this happens to be the best model we've obtained.","a993cbfc":"Let's first load the necessary libraries","8af26fc8":"### XGBoost","f8f0b351":"#### We can group the most relevant features by the name of the client who receives the funds","7a5a4d49":"Let's split-up the features and the label.","abf3d240":"#### We can see that most people who commit fraud have less than a dollar in their account","b7e48422":"Now that we have a better understanding of our dataset, let's move onto preprocessing the data and creating predictive models.","b5acc48c":"# Financial Fraud Detection\n\nWith the rise of digital financing fraud, there is a need to be able to be able to better identify fraudulent transactions. The goal of this analysis is to better understand the dataset we have, preprocess it and create models for predictions.\n\n**General Steps:**\n- **Step 1**: Load prerequisites and required modules\n- **Step 2**: Import dataset\n- **Step 3**: Perform exploratory data analysis\n- **Step 4**: Preprocess data for predictive modelling\n- **Step 5**: Analyse the models obtained and decide on the best one","0ce7fba3":"### Importing the libraries","425170dd":"## Feature Descriptions:\n\n**step**: Maps a unit of time in the real world. In this case 1 step is 1 hour of time.\n\n**type**: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER\n\n**amount**: amount of the transaction in local currency\n\n**nameOrig**: customer who started the transaction\n\n**oldbalanceOrg**: initial balance before the transaction\n\n**newbalanceOrig**: customer's balance after the transaction.\n\n**nameDest**: recipient ID of the transaction.\n\n**oldbalanceDest**: initial recipient balance before the transaction.\n\n**newbalanceDest**: recipient's balance after the transaction.\n\n**isFraud**: identifies a fraudulent transaction (1) and non fraudulent (0)\n\n**isFlaggedFraud**: flags illegal attempts to transfer more than 200.000 in a single transaction.","2061f535":"#### We can already see that fraudulent transactions have behavior that deviates greatly from the norm! Specifically, the average bank account balance change when fraud occurs is very large and negative\n\n## This invites the question: How often does a fraudulent transaction lead to a positive account balance change?","e6e49113":"In this case, since it's practically feasible given the relatively smaller dataset owing to undersampling, we would be using GridSearchCV to try out different hyperparameter combinations.","8d09ea7f":"### Data Preprocessing","13292fa0":"### Neural Network","ebd9e541":"### We can figure out what percentage of these non-positive balance change transactions turn out to be fradulent","44d41d88":"### XGBoost","9522b73c":"## Type III - Oversampling\n\n### Random Forests","d93e50ea":"## Type II - Undersampling\n\n### Random Forests","cebcb89d":"Now, let's encode the categorical variable 'type' using one-hot encoding. In order to prevent data leakage, we will be doing this transformation based on only the training dataset.","4447423a":"## Conclusion","31a6e9f6":"#### After fraud, account balances creep up by a few dollars\n\n\n### So most fraudulent transactions leave victim's with only dollars in their accounts and go to recipients with low account balances","b4b2dfb2":"### Frauds account for only 0.17% of suspected transactions! This reveals that our dataset is highly imbalanced between frauds and non-frauds\n\n#### In fact, the data has so few frauds that a stupid classifier (i.e. always predicting not fraud) would get ~99.8% accuracy on our selected data! Because of this, accuracy will not be an important metric to consider while building our model\n\n\n### Let's consider now what features are correlated with the fraudulent transactions we do have","921bc2bd":"#### Also, fraud is more spread out versus the number of transactions feature, but with a majority of fraud committed by clients who perform a relatively small number of transactions.\n\nWe can also see how transactions are distributed across various payment types.","0f5b5de2":"### None of the fradulent transactions result in a positive account balance change! \n\nThis makes sense if we consider that criminals would not be in the business of giving money away.\n\n### Now, let's drop all the transactions that resulted in a positive account balance change, so that we are left with only transactions where we would expect fraud to happen ","87fe7d08":"### Prerequisites","a846e774":"It's quite intuitive that the old and new balances seem to be the most important features looking at the EDA above.\n\nNow, let's plot the precision-recall curve and calculate the area under it.","91aea3f4":"#### A plot of the account balance of the fraudsters BEFORE the fradulent transactions","15c149bf":"### We can look further into each feature\n\n\n#### First, we can plot number of frauds versus the original account balance of victims","99b079ad":"Let's start off by dropping unnecessary features. 'step' refers to a timestep but there is no way of tying multiple transactions together to form a sequence of related payments. So it doesn't serve any purpose in our models. 'isFlaggedFraud' predominantly features zeroes with around 15 rows with a one. 'nameOrig' and 'nameDest' are identifiers which shouldn't be a part of models.","cc942195":"# Predictive Modelling\n\nFor a problem with highly imbalanced classes, there are three approaches one can take.\n- **No sampling**: Train models as it is without disturbing the balance of the dataset\n- **Undersampling**: Randomly select observations from the bigger class such that it ends up having the same number of observations as the smaller class.\n- **Oversampling**: Systematically create new observations for the smaller class such that it ends up having the same number of observations as the bigger class.\n\nFor all these approaches, we are using Random Forests and XGBoost as they are known to perform well on imbalanced classes. Just for the undersampling case, we are also training an artifical neural network (as it is feasible and practical in this case). \n\nIn order to evaluate the models, we are using 5-fold cross-validation with the evaluation metric being the area under the precision-recall curve.\n\n\n## Type I - No sampling technique\n\n### Random Forests","9cb2bfea":"#### We can do a similar thing with the resulting acccount balance of victims AFTER the fraud occurs","523803c8":"The network we are creating consists of an input layer with 9 nodes, the first hidden layer with 12 nodes and RELU activation functions, the second hidden layer with 8 nodes and RELU activation functions and finally, the output node with a sigmoid activation function to get the prediction probability.\n\nWe are also standardizing the features as it improves the performance and accuracy of neural networks.","74b32ca5":"Given that XGBoost leads to a different set of thresholds for every fold of the cross-validation, we are unable to plot the curve."}}