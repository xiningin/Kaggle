{"cell_type":{"f13324ef":"code","df892fc2":"code","b26e5cc5":"code","95ebd0e3":"code","0682c666":"code","948aa27a":"code","c9d86496":"code","3fa406a8":"code","fabc75cf":"code","d73c02e8":"code","749e4ad5":"code","36791369":"code","c075550d":"code","dc174c0c":"code","80e4b1c5":"code","e9d62131":"code","d33d59a3":"code","6e3cfdd0":"code","72972d11":"code","03eb7785":"code","8380b280":"code","cee9c3bd":"code","d515117e":"code","6546f04b":"code","7f6a2c97":"markdown","c8d26ab4":"markdown","64aee7a5":"markdown","61467436":"markdown","3f415bb2":"markdown","2494741d":"markdown","382f338a":"markdown"},"source":{"f13324ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df892fc2":"import tensorflow as tf\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras import models, layers\nfrom keras.utils import np_utils\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","b26e5cc5":"sample_submission = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\")\nsample_submission.head()","95ebd0e3":"train_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntrain_df.head()","0682c666":"missing_value_counts = train_df.isnull().sum()\ntotal_missing_values = missing_value_counts.sum()\ntotal_missing_values","948aa27a":"print(train_df.info())","c9d86496":"test_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\ntest_df.head()","3fa406a8":"print(test_df.info())","fabc75cf":"X = train_df.drop(\"label\", axis=1)\ny = train_df.loc[:, \"label\"]","d73c02e8":"plt.figure(figsize=(5,5))\nplt.xticks(size=10)\nsns.countplot(y, linewidth = 2, edgecolor = sns.color_palette(\"Set2\"))\nplt.title(\" Label distribution of training dataset\", fontdict={\"color\": \"black\", \"fontsize\": 15})\nplt.show()","749e4ad5":"train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.2)","36791369":"train_images = np.array(train_x)\ntrain_images = train_images.reshape(len(train_images), 28,28,1)\ntrain_images = train_images.astype(\"float32\")\ntrain_images = train_images \/ 255\ntrain_labels = np.array(train_y)","c075550d":"val_images = np.array(val_x)\nval_images = val_images.reshape(len(val_images), 28,28,1)\nval_images = val_images.astype(\"float32\")\nval_images = val_images \/ 255\nval_labels = np.array(val_y)","dc174c0c":"test_images = np.array(test_df)\ntest_images = test_images.reshape(len(test_images), 28,28,1)\ntest_images = test_images.astype(\"float32\")\ntest_images = test_images \/ 255","80e4b1c5":"fig, axes = plt.subplots(5,5, figsize=(15, 15))\naxes = axes.ravel()\n\nfor i in np.arange(0, 25):\n    axes[i].imshow(train_images[i], cmap=\"Greys\")\n    axes[i].set_title(\"Digit = %s\" % train_labels[i])\n    axes[i].axis(\"off\")\n    plt.subplots_adjust(wspace=1)","e9d62131":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(10, activation=\"softmax\"))\nmodel.summary()","d33d59a3":"model.compile(optimizer=\"adam\",\n             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n             metrics=[\"accuracy\"])","6e3cfdd0":"batch_size = 264\nhistory = model.fit(train_images, train_labels, epochs=10, batch_size=batch_size,\n                    verbose=1,validation_data=(val_images, val_labels))","72972d11":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, [\"loss\", \"val_loss\"]].plot()\nhistory_frame.loc[:, [\"accuracy\", \"val_accuracy\"]].plot()","03eb7785":"predictions = model.predict(test_images)\nprediction_classes = np.argmax(predictions, axis=1)","8380b280":"predictions[0]","cee9c3bd":"np.argmax(predictions[0])","d515117e":"fig, axes = plt.subplots(5,5, figsize=(15,15))\naxes= axes.ravel()\n\nfor i in np.arange(0,25):\n    axes[i].imshow(test_images[i], cmap=\"Greys\")\n    axes[i].set_title(\"Predict = %s\" % prediction_classes[i])\n    axes[i].axis(\"off\")\n    plt.subplots_adjust(wspace=0.5)","6546f04b":"output = pd.DataFrame({\"ImageId\": sample_submission.ImageId, \"label\": prediction_classes})\noutput.to_csv(\"My_submission\", index=False)\noutput.head()","7f6a2c97":"# Model building","c8d26ab4":"# Loading, exploring and visualization","64aee7a5":"# Model training and validation","61467436":"# Submission","3f415bb2":"# Predictions","2494741d":"# Import libraries","382f338a":"# Preprocessing"}}