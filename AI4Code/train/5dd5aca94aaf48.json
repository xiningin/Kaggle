{"cell_type":{"5cd707bb":"code","e6feee80":"code","90a11f16":"code","f5d80395":"code","cfe1ba7f":"code","7eabddaa":"code","a7a0d58d":"code","5f79949e":"code","75e84fc0":"code","4bf26abd":"code","07e939ce":"code","4d863598":"code","86ae13e0":"code","4821b08a":"code","79e556b3":"code","1e8a6f9e":"code","6a3eb531":"code","46b4da82":"code","93f3c10c":"code","6b3e7709":"code","57a0bc1a":"code","c6a756f5":"code","7a242789":"markdown","896f4eb0":"markdown"},"source":{"5cd707bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6feee80":"import re\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\n\nnltk.download('punkt') # one time execution\nnltk.download('stopwords')# one time execution","90a11f16":"# Read the CSV file\ndf = pd.read_csv('\/kaggle\/input\/nlp-specialization-data\/tennis_articles_v4.csv')\nprint(df.shape)\ndf.head(3)","f5d80395":"# split the the text in the articles into sentences\nsentences = []\nfor s in df['article_text']:\n    sentences.append(sent_tokenize(s))  ","cfe1ba7f":"# flatten the list\nsentences = [y for x in sentences for y in x]","7eabddaa":"# remove punctuations, numbers and special characters\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","a7a0d58d":"stop_words = stopwords.words('english')","5f79949e":"# function to remove stopwords\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","75e84fc0":"# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","4bf26abd":"print(len(clean_sentences))\nclean_sentences[:15]","07e939ce":"from gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = '\/kaggle\/input\/glove6b\/glove.6B.100d.txt'","4d863598":"# Extract word vectors\nword_embeddings = {}\nf = open(glove_input_file, encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","86ae13e0":"len(word_embeddings)","4821b08a":"word_embeddings['the'].shape","79e556b3":"sentence_vectors = []\nfor i in clean_sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])\/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","1e8a6f9e":"len(sentence_vectors)","6a3eb531":"# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])\nsim_mat.shape","46b4da82":"from sklearn.metrics.pairwise import cosine_similarity","93f3c10c":"for i in range(len(sentences)):\n    for j in range(len(sentences)):\n        if i != j:\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n\nprint(sim_mat.shape)\nsim_mat[:5,:5]  ","6b3e7709":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)","57a0bc1a":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","c6a756f5":"# Specify number of sentences to form the summary\nsn = 10\n\n# Generate summary\nfor i in range(sn):\n    print(ranked_sentences[i][1])","7a242789":"### Further Readings : \n* Text summarization using TextRank in NLP - https:\/\/medium.com\/data-science-in-your-pocket\/text-summarization-using-textrank-in-nlp-4bce52c5b390\n* Find the original article here https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/introduction-text-summarization-textrank-python\/\n* link for Cosine Similarity explanation\nhttps:\/\/medium.com\/datadriveninvestor\/cosine-similarity-cosine-distance-6571387f9bf8","896f4eb0":"The next step is to find similarities among the sentences. We will use cosine similarity to find similarity between a pair of sentences. Let's create an empty similarity matrix for this task and populate it with cosine similarities of the sentences."}}