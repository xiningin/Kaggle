{"cell_type":{"72b10ab1":"code","885b07fc":"code","80dcbc7d":"code","23433aa9":"code","43ee8f61":"code","08dc1608":"code","d3358918":"code","ce17a498":"code","204ff44e":"code","977a3220":"markdown","b530c3cb":"markdown","ac5bf7cb":"markdown","4428a84d":"markdown"},"source":{"72b10ab1":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","885b07fc":"# Loading the training dataset\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\n","80dcbc7d":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns\n","23433aa9":"scaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)","43ee8f61":"# Find best hyperparameters (roc_auc)\nrandom_state = 0\nclf = LogisticRegression(random_state = random_state)\nparam_grid = {'class_weight' : ['balanced'], \n              'penalty' : ['l1'],  \n              'C' : [0.0001, 0.0005, 0.001, \n                     0.005, 0.01, 0.05, 0.1, 0.5, 1, \n                     10, 100, 1000, 1500, 2000, 2500, \n                     2600, 2700, 2800, 2900, 3000, 3100, 3200  \n                     ] , # This hyperparameter is lambda \n              'max_iter' : [100, 1000, 2000, 5000, 10000] }\n\n# Make an roc_auc scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\ngrid = GridSearchCV(estimator = clf, param_grid = param_grid , \n                    scoring = scorer, verbose = 1, cv=20,\n                    n_jobs = -1)\n\n\nX = df_train.values\n\ngrid.fit(X,y)\n\nprint(\"Best Score:\" + str(grid.best_score_))\n\nbest_parameters = grid.best_params_\n","08dc1608":"# We are going to print the hyperparameters of the best model \nbest_clf = grid.best_estimator_\nprint(best_clf)","d3358918":"model = LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l1', random_state=0,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False);\n\nmodel.fit(X, y);\n","ce17a498":"df_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)  \nX = df_test.values\n\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]  ","204ff44e":"# submit prediction\nsmpsb_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"submission.csv\", index=None)\n","977a3220":"We are going to standardize the explanatory variables by removing the mean and scaling to unit variance. The reason for that is to help convergence of the technique used in the optimization. ","b530c3cb":"Finally, we are going to generate the submission file. ","ac5bf7cb":"We are going to perform a grid search in order to find a good value for the hyperparameters $\\lambda$ of Lasso regression. The following web page is a good reference: \n\n[Tuning ML Hyperparameters](https:\/\/alfurka.github.io\/2018-11-18-grid-search\/)","4428a84d":"First of all I have used ideas from these websites: \n\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/lasso-regression\/\n\n\nIn this kernel we are going to use Lasso regression which is a type of linear regression that produces [sparse models](https:\/\/www.quora.com\/Why-need-to-find-sparse-models-in-machine-learning). \n\nLasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of the coefficients. "}}