{"cell_type":{"3d492f37":"code","3417d725":"code","575bc7dc":"code","7e563b19":"code","4fa5de62":"code","7dc884c9":"code","30e65104":"code","283b5a92":"code","39d484f3":"code","fbad1093":"code","acddb0de":"code","fb66df34":"code","b8735431":"code","733e33eb":"code","39f90343":"code","e5995fcd":"code","c3dbf36c":"code","2c96f7a4":"code","e8e3946d":"code","278d77c1":"code","e970451a":"code","a920b73f":"code","b82e97cf":"code","4b787e51":"code","6d3369fa":"code","4ab759b7":"code","70ce4bd1":"code","d1fa611d":"code","7277d94a":"code","f22f2032":"code","a68fb441":"code","0643fe56":"code","368aadd1":"code","e9761c2a":"code","8eeccfa8":"code","d6b70fb5":"code","5a2cf27e":"code","ea7f5693":"code","cb39683e":"code","ed019b1b":"code","b72cc909":"code","efe6b793":"code","132c2cc8":"code","34fa139a":"code","774b47e5":"code","1b8fcb7a":"code","abee69e1":"code","e1432656":"code","f055efc9":"code","c4c8b5bd":"code","9bb6dcd3":"code","72fb6faf":"code","95edb481":"code","ab45a1ec":"code","89a9c9c6":"code","d13d3e8e":"code","1c18a70e":"code","ab1c275f":"code","bf3a69ba":"code","802668b8":"code","5944ac51":"code","b20270a4":"code","44ddc299":"code","0887844b":"markdown","b03ae551":"markdown","26d32907":"markdown","a0a8337d":"markdown","d7b00aa4":"markdown","ac3da4dd":"markdown","4fbc258e":"markdown","0663dbee":"markdown","7f9c74e2":"markdown","c3ace7f4":"markdown","4bf8c8cd":"markdown","1c849e42":"markdown","bc5ecea6":"markdown","c7e77fce":"markdown","838307a7":"markdown","4a1907cf":"markdown","f8ffa15f":"markdown","6ebf6d5b":"markdown","8fd1a2d2":"markdown","ab58ecb2":"markdown","7d1b5aed":"markdown","1dfcfdbb":"markdown","df32d352":"markdown","e546eb88":"markdown","120f2913":"markdown","04a5e05f":"markdown","0b2f499f":"markdown","1e5769e7":"markdown","975b6c84":"markdown","c91a8d72":"markdown","61ac24f1":"markdown","928ec1b2":"markdown","da71cf06":"markdown","700df591":"markdown","ec3b7909":"markdown","327e9672":"markdown","70702f6f":"markdown","6233fe54":"markdown","23176d8b":"markdown","bf82db1a":"markdown","9ce073d2":"markdown","d8b86cc0":"markdown","bf91f552":"markdown","e17b9a62":"markdown","592ff80d":"markdown","7f90600b":"markdown","0c8450ae":"markdown","fc2d8200":"markdown","35c4dfed":"markdown","8832beb5":"markdown","ededa71f":"markdown","584e3421":"markdown","ae56a408":"markdown","f295ad67":"markdown"},"source":{"3d492f37":"import warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nimport sklearn.preprocessing as pp\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering, AffinityPropagation, Birch, DBSCAN, OPTICS\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n\n%matplotlib inline\npd.set_option('precision', 2)\nsns.set_style('whitegrid')","3417d725":"data = pd.read_excel('\/kaggle\/input\/uci-online-retail-ii-data-set\/online_retail_II.xlsx')","575bc7dc":"data.shape","7e563b19":"data.head()","4fa5de62":"data.info()","7dc884c9":"# Identify the number of NAs in each feature and select only those having NAs\ntotal_NA = data.isnull().sum()[data.isnull().sum() != 0]\n\n# Calculate the percentage of NA in each feature\npercent_NA = data.isnull().sum()[data.isnull().sum() != 0]\/data.shape[0]\n\n# Summarize our findings in a dataframe\nmissing = pd.concat([total_NA, percent_NA], axis=1, keys=['Total NAs', 'Percentage']).sort_values('Total NAs', ascending=False)\nmissing","30e65104":"# Drop transactions with missing Customer ID\ndata.dropna(axis=0, subset=['Customer ID'], inplace= True)","283b5a92":"print('Number of duplicated records: ', data.duplicated(keep='first').sum())","39d484f3":"indx = data[data.duplicated(keep='first')].index\ndata.drop(index = indx, inplace= True)","fbad1093":"data[['StockCode']] = data['StockCode'].astype(str)\ndata[['Customer ID']] = data['Customer ID'].astype(int).astype(str)","acddb0de":"data.dtypes.value_counts()","fb66df34":"# Summary statistics of categorical variables\ndata.select_dtypes(include='object').describe().T","b8735431":"# Summary statistics of \"InvocieDate\" variable\ndata[['InvoiceDate']].describe().T","733e33eb":"# Summary statistics of numeric variables\ndata.select_dtypes(include= ['int64', 'float64']).describe().transpose()","39f90343":"x = data.Country.apply(lambda x: x if x == 'United Kingdom' else 'Not UK').value_counts().rename('#Customers')\ny = (x\/data.shape[0]).rename('%Customers')\npd.concat([x, y], axis= 1)","e5995fcd":"# Drop cancelled transactions\nindx = data.Invoice[data.Invoice.str.contains('C') == True].index\ndata.drop(index= indx, inplace= True)","c3dbf36c":"# Drop transactions with price zero\nindx = data.loc[data.Price == 0].index\ndata.drop(index= indx, inplace= True)","2c96f7a4":"# Amount per transaction which is the product of sale price and quantity\ndata['Amount'] = data['Price'] * data['Quantity']","e8e3946d":"# Create new variable for Invoice time in hours\ndata['Transaction_time'] = data.InvoiceDate.apply(lambda x : x.time().hour)","278d77c1":"# Create new variable for Invoice date\ndata['Transaction_date'] = data.InvoiceDate.apply(lambda x : x.date())\ndata['Transaction_date'] = data.Transaction_date.apply(lambda x: x.replace(day = 1))","e970451a":"# calculate the no. of months since transaction date .\nref = datetime.strptime('2010-12', '%Y-%m')\ndata['Mnths_since_purchase'] = data.Transaction_date.apply(lambda x: \\\n                                        relativedelta(ref,x).years*12 + relativedelta(ref,x).months)\nRecency = data.groupby('Customer ID').agg({'Mnths_since_purchase' : 'min'}).copy().rename(columns= {'Mnths_since_purchase':'Recency'})","a920b73f":"# Calculate the number of months since the first purchase for each customer\ndata['First_purchase'] = data['Mnths_since_purchase'].copy()\nFirst_purchase = data.groupby('Customer ID').agg({'First_purchase' : 'max'}).copy().rename(columns= {'Mnths_since_purchase':'First_purchase'})","b82e97cf":"Frequency = data.groupby(['Customer ID',\n                    'Transaction_date']).agg({'Invoice' : 'nunique'}).groupby(['Customer ID']).agg({'Invoice' : 'mean'}).copy().rename(columns= {'Invoice':'Frequency'})","4b787e51":"Monetary_value = data.groupby(['Customer ID',\n                    'Invoice']).agg({'Amount' : 'sum'}).groupby(['Customer ID']).agg({'Amount' : 'mean'}).copy().rename(columns= {'Invoice':'Frequency',\n                                                              'Amount': 'Monetary_value'})","6d3369fa":"# Calculate Average number of unique items in each transaction for each customer\nunique_items = data.groupby(['Customer ID', 'Invoice']).agg({'StockCode': 'nunique'}).groupby(['Customer ID']\\\n            ).agg({'StockCode':'mean'}).rename(columns={'StockCode': 'Unique_items'})","4ab759b7":"# Create transformed data for Clustering\ndata_transformed = pd.concat([Recency, First_purchase, Frequency, Monetary_value,unique_items], axis=1)\ndata_transformed.describe()","70ce4bd1":"# Plot the distribution of all variables that will be used for model training\nfig, ax = plt.subplots(ncols=2, nrows=3, figsize=(9,9))\nsns.distplot(data_transformed.Recency, ax= ax[0][0], kde= False)\nsns.distplot(data_transformed.First_purchase, ax= ax[0][1], kde= False)\nsns.distplot(data_transformed.Frequency, ax= ax[1][0], kde= False)\nsns.distplot(data_transformed.Monetary_value, ax= ax[1][1], kde= False)\nsns.distplot(data_transformed.Unique_items, ax= ax[2][0], kde= False)","d1fa611d":"# Define frequency threshold value and drop customers who exceed the threshold\nfreq_stats = data_transformed['Frequency'].describe()\nfreq_threshold = freq_stats['mean'] + 3 * freq_stats['std']\nindx = data_transformed.loc[data_transformed.Frequency > freq_threshold].index\ndata_transformed.drop(index = indx, inplace= True)","7277d94a":"# Define Monetary value threshold value and drop customers who exceed the threshold\nm_stats = data_transformed['Monetary_value'].describe()\nm_threshold = m_stats['mean'] + 3 * m_stats['std']\nindx = data_transformed.loc[data_transformed.Monetary_value > m_threshold].index\ndata_transformed.drop(index = indx, inplace= True)","f22f2032":"fig, ax = plt.subplots(ncols=2, nrows=3, figsize=(9,9))\nsns.distplot(data_transformed.Recency, ax= ax[0][0], kde= False)\nsns.distplot(data_transformed.First_purchase, ax= ax[0][1], kde= False)\nsns.distplot(data_transformed.Frequency, ax= ax[1][0], kde= False)\nsns.distplot(data_transformed.Monetary_value, ax= ax[1][1], kde= False)\nsns.distplot(data_transformed.Unique_items, ax= ax[2][0], kde= False)","a68fb441":"# Normalize the four variables\nscaler = pp.StandardScaler()\ndata_transformed_scaled = pd.DataFrame(scaler.fit_transform(data_transformed),\n                                       columns= data_transformed.columns)","0643fe56":"fig, ax = plt.subplots(ncols=2, nrows=3, figsize=(9,9))\nsns.distplot(data_transformed_scaled.Recency, ax= ax[0][0], kde= False)\nsns.distplot(data_transformed_scaled.First_purchase, ax= ax[0][1], kde= False)\nsns.distplot(data_transformed_scaled.Frequency, ax= ax[1][0], kde= False)\nsns.distplot(data_transformed_scaled.Monetary_value, ax= ax[1][1], kde= False)\nsns.distplot(data_transformed_scaled.Unique_items, ax= ax[2][0], kde= False)","368aadd1":"# A function to automate the model fiting and prediction\ndef model_train(estimator, data, a,b):\n    db = []\n    ca = []\n    sc = []\n    bic = []\n    aic = []\n    n_clusters = {'n_clusters':[]}\n    if (estimator == AffinityPropagation)|(estimator == DBSCAN)|(estimator == OPTICS)|(estimator==Birch):\n        est = estimator()\n        est.fit(data)  \n        labels = est.labels_\n        if np.unique(est.labels_).shape[0] > 1:\n            db.append(davies_bouldin_score(data, labels))\n            ca.append(calinski_harabasz_score(data, labels))\n            sc.append(silhouette_score(data, labels))\n            n_clusters['n_clusters'].append('N\/A')\n        else:\n            n_clusters['n_clusters'].append(np.unique(est.labels_).shape[0])\n    \n    else:\n        for k in range(a, b):\n            if estimator == GaussianMixture:\n                est = estimator(n_components= k)\n                labels = est.fit_predict(data)\n            else:\n                est = estimator(n_clusters= k)\n                est.fit(data)\n                labels = est.labels_\n\n            db.append(davies_bouldin_score(data, labels))\n            ca.append(calinski_harabasz_score(data, labels))\n            sc.append(silhouette_score(data, labels))\n\n        n_clusters['n_clusters'].append(np.argmin(db) + a)\n        n_clusters['n_clusters'].append(np.argmax(ca) + a)\n        n_clusters['n_clusters'].append(np.argmax(sc) + a)\n    return db, ca, sc, labels, n_clusters['n_clusters']","e9761c2a":"#Plot different measures against No. of clusters for algorithms requiring no. of clusters a priori.\ndef plot_scores(a,b, db, ca, sc):\n    fig, ax = plt.subplots(nrows= 1, ncols=3, figsize=(15,4))\n    ax[0].plot(range(a, b), db, \"bo-\", label= 'Davies_Bouldin_Score')\n    ax[1].plot(range(a, b), ca, \"rx-\", label = 'Calinski_Harabasz_Score')\n    ax[2].plot(range(a, b), sc, \"g.-\", label = 'Silhouette_Score')\n    ax[0].set_xlabel(\"$k$\", fontsize=14)\n    ax[1].set_xlabel(\"$k$\", fontsize=14)\n    ax[2].set_xlabel(\"$k$\", fontsize=14)\n    ax[0].set_ylabel('Davies Bouldin Score', fontsize=14)\n    ax[1].set_ylabel('Calinski Harabasz Score', fontsize=14)\n    ax[2].set_ylabel('Silhouette Score', fontsize=14)\n#     plt.legend(loc=(1,0),fontsize=14)\n    plt.show()","8eeccfa8":"clusterers = [KMeans, AffinityPropagation, AgglomerativeClustering, Birch,\n             DBSCAN, GaussianMixture, OPTICS, SpectralClustering]\n\nScores ={'Davies_Bouldin_Score': [], \n         'Calinski_Harabasz_Score': [],\n         'Silhouette_Score': [],\n        'n_clusters': []}\n\nclusterer_names = ['KMeans', 'Affinity Propagation', 'Agglomerative Clustering', 'Birch',\n             'DBSCAN', 'Gaussian Mixture Model', 'OPTICS', 'Spectral Clustering']\n\nfor i in clusterers:\n    db, ca, sc, labels, n_clusters= model_train(i, data_transformed_scaled, 3, 8)\n\n    Scores['Davies_Bouldin_Score'].append(np.min(db))\n    Scores['Calinski_Harabasz_Score'].append(np.max(ca))\n    Scores['Silhouette_Score'].append(np.max(sc))\n    Scores['n_clusters'].append(n_clusters)","d6b70fb5":"models_scores = pd.DataFrame(Scores, index= clusterer_names)\nmodels_scores","5a2cf27e":"models_scores.loc[models_scores.Davies_Bouldin_Score == models_scores.Davies_Bouldin_Score.min()]","ea7f5693":"models_scores.loc[models_scores.Calinski_Harabasz_Score == models_scores.Calinski_Harabasz_Score.max()]","cb39683e":"models_scores.loc[models_scores.Silhouette_Score == models_scores.Silhouette_Score.max()]","ed019b1b":"def cluster_stats(model, data, data_transformed):\n    df = data_transformed.copy()\n    df['Cluster'] = pd.Series(model.labels_, name= 'Cluster', index= data_transformed.index)\n    df['No._Purchases'] = data.groupby('Customer ID')['Invoice'].count()[df.index]\n    df['Total_Amount'] = data.groupby('Customer ID')['Amount'].sum()[df.index]\n    cluster_stats = df.groupby('Cluster').agg({'Recency': ['min', 'mean','max'],\n                                       'Frequency': ['min', 'mean','max'],\n                                       'Monetary_value': ['min', 'mean','max'],\n                                       'First_purchase': ['min', 'mean','max'],\n                                       'Unique_items': ['min', 'mean','max']}).copy().round(1)\n    return cluster_stats, df","b72cc909":"def clusters_summary(df, data):\n    columns = {'#Customers':[], '#Purchases':[], 'Total_Amount':[]}\n    indx =[]\n    for i in np.sort(df.Cluster.unique()):\n        columns['#Customers'].append(data.iloc[df.loc[df.Cluster == i].index].shape[0])\n        columns['#Purchases'].append(df['No._Purchases'].loc[df.Cluster == i].sum())\n        columns['Total_Amount'].append(df['Total_Amount'].loc[df.Cluster == i].sum())\n        indx.append('Cluster{}'.format(i))\n    \n    # Synthesis a data frame for cluster summanry\n    clusters_summary = pd.DataFrame(data= columns, index = indx)\n\n    clusters_summary['%customers'] = (clusters_summary['#Customers']\/df.shape[0])*100\n    clusters_summary['%transactions'] = (clusters_summary['#Purchases']\/df['No._Purchases'].sum())*100\n    clusters_summary['%sales_amount'] = (clusters_summary['Total_Amount']\/df['Total_Amount'].sum())*100\n    columnsOrder = ['#Customers', '%customers', '#Purchases', '%transactions', 'Total_Amount', '%sales_amount']\n    return clusters_summary.reindex(columns=columnsOrder)    ","efe6b793":"def plot_3d(cluster_stat):\n    fig = plt.figure(figsize=(9,7))\n    ax = fig.add_subplot(111, projection='3d')\n    scatter = ax.scatter(cluster_stat['Recency'], cluster_stat['Frequency'], cluster_stat['Monetary_value'],\n                     c=cluster_stat['Cluster'], s=60, marker=\".\",\n                         cmap= 'prism', edgecolor= 'k', linewidths= 0.6)\n    # produce a legend with the unique colors from the scatter\n#     legend1 = ax.legend(*scatter.legend_elements(),\n#                         loc=\"center right\", title=\"Customer Segments\",\n#               bbox_to_anchor=(0.75, 0, 0.5, 1), fontsize= 12)\n#     ax.add_artist(legend1)\n    ax.set_xlabel('Recency', fontsize= 12)\n    ax.set_ylabel('Frequency', fontsize= 12)\n    ax.set_zlabel('Monetary_value', fontsize= 12)","132c2cc8":"def plot_dist(df, col):\n    n= df['Cluster'].nunique()\n    mpl.rcParams['figure.figsize'] = (12,12)\n    fig, ax = plt.subplots(ncols=2 , nrows= (n\/\/2))\n    k = 0\n    h=0\n    for j in col:\n        for i in range(n):\n            sns.distplot(df[j][df.Cluster ==i], hist= False, label= 'Cluster{}'.format(i),\n                         ax= ax[k][h], kde= True)\n        ax[k][h].set_xlabel('{}'.format(j), fontsize= 14)\n        h+=1\n        if h%2==0:\n            h=0\n            k +=1","34fa139a":"specc_3 = SpectralClustering(n_clusters= 3).fit(data_transformed_scaled)","774b47e5":"cluster_stats(specc_3, data, data_transformed)[0]","1b8fcb7a":"clusters_summary(cluster_stats(specc_3, data, data_transformed)[1], data)","abee69e1":"plot_3d(cluster_stats(specc_3, data, data_transformed)[1])","e1432656":"specc_4 = SpectralClustering(n_clusters= 4).fit(data_transformed_scaled)","f055efc9":"cluster_stats(specc_4, data, data_transformed)[0]","c4c8b5bd":"clusters_summary(cluster_stats(specc_4, data, data_transformed)[1], data)","9bb6dcd3":"plot_3d(cluster_stats(specc_4, data, data_transformed)[1])","72fb6faf":"kmeans = KMeans(n_clusters= 5, max_iter= 1000, random_state= 42).fit(data_transformed_scaled)","95edb481":"cluster_stats(kmeans, data, data_transformed)[0]","ab45a1ec":"kmeans_cs = clusters_summary(cluster_stats(kmeans, data, data_transformed)[1], data)\nkmeans_cs","89a9c9c6":"plot_3d(cluster_stats(kmeans, data, data_transformed)[1])","d13d3e8e":"Scores ={'Davies_Bouldin_Score': [], \n         'Calinski_Harabasz_Score': [],\n         'Silhouette_Score': []}\n\nfor k in range(4,9):\n    kmeans = KMeans(n_clusters= k).fit(data_transformed_scaled)\n    Scores['Davies_Bouldin_Score'].append(davies_bouldin_score(data_transformed_scaled, \n                                                               kmeans.labels_))\n    Scores['Calinski_Harabasz_Score'].append(calinski_harabasz_score(data_transformed_scaled, \n                                                                     kmeans.labels_))\n    Scores['Silhouette_Score'].append(silhouette_score(data_transformed_scaled, \n                                                       kmeans.labels_))","1c18a70e":"plot_scores(4,9,Scores['Davies_Bouldin_Score'], Scores['Calinski_Harabasz_Score'],\n            Scores['Silhouette_Score'])","ab1c275f":"kmeans = KMeans(n_clusters= 6, max_iter= 1000, random_state= 42).fit(data_transformed_scaled)","bf3a69ba":"kmeans_results = cluster_stats(kmeans, data, data_transformed)[1]\ncluster_stats(kmeans, data, data_transformed)[0]","802668b8":"kmeans_cs = clusters_summary(cluster_stats(kmeans, data, data_transformed)[1], data)\nkmeans_cs","5944ac51":"plot_3d(cluster_stats(kmeans, data, data_transformed)[1])","b20270a4":"# Two Pie charts to compare clusters in terms of represented population proportion and total sales\n# amount proportion\ndef func(pct):\n    return \"{:.1f}%\".format(pct)\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15, 6), subplot_kw=dict(aspect=\"equal\"))\n\nwedges, text1, autotexts = ax[1].pie(kmeans_cs['%sales_amount'].values,\n                                  autopct=lambda pct: func(pct),\n                                  textprops=dict(color=\"w\", fontsize= 12))\nwedges, text2, autotexts = ax[0].pie(kmeans_cs['%customers'].values,\n                                  autopct=lambda pct: func(pct),\n                                  textprops=dict(color=\"w\", fontsize= 12))\nax[0].legend(kmeans_cs.index,\n          title=\"Customer Segments\",\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1), fontsize= 12)\n\nax[1].set_title(\"Proportion of total sales amount\", fontsize= 18)\nax[0].set_title(\"Proportion of Population\", fontsize= 18)","44ddc299":"plot_dist(kmeans_results, ['Recency', 'First_purchase', 'Frequency', 'Monetary_value',\n                           'Unique_items', 'Total_Amount'])","0887844b":"### 1. Spectral Clustering","b03ae551":"The plots of different evaluation metrics used at different K values indicates that K=6 can yield a good solution. So, we will examine this solution.","26d32907":"We will start by trying a bunch of clustering algorithms and select most promising ones. Since the ground truth labels are not known, evaluation will be performed using the model itself. We will use three measures to evaluate clustering models:<br\/>\n * Davies Bouldin Score:it signifies the average similarity between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Zero is the lowest possible score. Values closer to zero indicate a better partition.\n * Calinski Harabasz Score (also known as the Variance Ratio Criterion): it is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The score is higher when clusters are dense and well separated.\n * Silhouette Score: The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters","a0a8337d":"* **Best model according to \"Davies_Bouldin_Score\":**","d7b00aa4":"* Generate summary statistics for all numerical variables.","ac3da4dd":"* **Best model according to \"Silhouette_Score\":**","4fbc258e":"* Customers from UK and Outside UK.","0663dbee":"By examining the above summary data frames and figure, we can notice that KMeans actually results in better cluster separation compared to spectral clustering. However, the results doesn't seem very informative and we thick we can get better solution by further model optimization. Hence, we will recheck the best number of clusters for KMeans.","7f9c74e2":"* Some transaction have a sale price of zero, these could be free gifts for some customers and not true sales transaction. So, we will drop them.","c3ace7f4":"We can see that *Cluster1* alone contains 4034 which is 98.1 of the whole population accounting for 97.37 of the total sales amount! Also, this result in not meaning full.<br\/>\nLet's try KMeans with 5 clusters!","4bf8c8cd":"* For each transaction, calculate the number of months since transaction date to the end of the year (Dec 2010).","1c849e42":"We will try training the following algorithms:<br\/>\n* KMeans.\n* Affinity Propagation.\n* Agglomerative hierarchical clustering.\n* Birch.\n* DBSCAN.\n* Gaussian Mixture Model.\n* OPTICS.\n* Spectral Clustering.\n\nFor those algorithms need the number of clusters a priori we will try the range of values between 3 and 7 and select the best model based on evaluation metrics.","bc5ecea6":"* KMeans with 5 clusters:","c7e77fce":"Now, we have 3 promising models, Spectral clustering with 3, and 4 clusters and Kmeans with 5 clusters. Next, we will find the best model with the best number of clusters that results in clearest interpretation of separation between clusters.","838307a7":"* For each customer, calculate the number of months since his\/her first purchase.","4a1907cf":"* Spectral Clustering with 3 Clusters:","f8ffa15f":"Now, we can use the resulted clusters to answer the following questions:<br\/>\n* Who are the most \/ least valuable customers to the business? What are the distinct characteristics of them?<br\/>\n* Who are the most \/ least loyal customers, and how are they characterized?\n* What are customers' purchase behavior patterns? Which products \/ items have customers purchased together often? In which sequence the products have been purchased?\n* What are the sales patterns in terms of various perspectives such as products\/items, regions and time (weekly, monthly, quarterly, yearly and seasonally), and so on?","6ebf6d5b":"* Assert that StockCode and Customer ID are of string data type not integers or floats.","8fd1a2d2":"* For each customer, calculate the average purchase amount per transaction (Monetary value).","ab58ecb2":"* Calculate the sales amount for each transaction which is the product of sale price and quantity.","7d1b5aed":"* Generate descriptive statistics for the \"InvoiceDate\" variable.","1dfcfdbb":"* Transactions with missing Customer IDs are not useful for creating customers segments. So, we drop them.","df32d352":"## 3.3 Optimize selected models","e546eb88":"* For each customer, calculate the average number of transactions per month (Frequency).","120f2913":"* Basic data info, number of records and number of variables.","04a5e05f":"We can see that *Cluster0* alone contains 3424 which is 83.25 of the whole population accounting for 95.14 of the total sales amount! Clearly, this result in not meaning full.<br\/>\nLet's try Spectral Clustering with 4 clusters!","0b2f499f":"* **Best model according to \"Calinski_Harabasz_Score\":**","1e5769e7":"In this notebook we tackle the problem of Customer segmentation which plays a crucial rule in modern customer-centric marketing. This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01\/12\/2009 and 09\/12\/2010.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers. The dataset is available at <a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail+II\">UCI datasets<\/a>.<br\/>\n\n**Contents:**\n* 1. Data exploring and preprocessing.\n* 2. Feature engineering.\n* 3. Modeling.\n* 4. Cluster analysis.","975b6c84":"* Number of different data types in the dataset.","c91a8d72":"# 1. Data exploring and preprocessing","61ac24f1":"* Transactions with a letter 'C' at the beginning of its invoice numbers are canceled transactions, so we drop them.","928ec1b2":"* Assemble all new features into a new data frame to be used for model training later.","da71cf06":"We will use the RFM model in addition to another two measures, First_purchase and No. of unique items, to create customer segments. RFM stands for *Recency, Frequency, and Monetary_value*:\n* Recency: is a measure of how long it has been since a customer last placed an order with the company.<br\/>\n* First_purchase: Time in month since the first purchase.<br\/>\n* Frequency: is a measure of how often a customer orders from the company.<br\/>\n* Monetary Value: The a mount that a customer spends on an average transaction.<br\/>\n* Unique_items: is measure of the average number of unique items per transaction. This measure can be used to distinguish between organizational customers and individual customers.","700df591":"*Cluster2* contains 232 customers, composing 5% of the whole population and accounts for 5% of the total sales. This segment has fairly high profitability with an average monetary_value of \u00a3438 per transaction and moderate frequency with an average 1.1 transaction per month. What is interesting about this cluster is the large average number of unique items in each transaction. This indicates that most of customers in this segment are actually organizational customers not individuals. \n\nFurther, Customers in *Cluster0* are more recent than those in CLuster2. This segment is the largest one, including 32% of the whole population and accounts for 41% of the total sales. This segment includes the loyal customers who started shopping with the online retailer in the first quarter of the year with an average first_purchase of 10.4 and maintained a moderate purchase frequency and high recency with an average of 1.6.<br\/>\n\nFinally, *CLuster4* contains about 9% of the whole population and accounts for 21% of the total sales amount. Customers in this segment shop frequently with an average frequency of 2.2 transaction per month. They also have moderate monetary-value, \u00a3308.7 per transaction. This segment can be considered as the second most profitable segment.","ec3b7909":"# 3. Model training","327e9672":"*References:<br\/>\nChen, D., Sain, S. & Guo, K. J: \"Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining\" (2012) 19: 197. https:\/\/doi.org\/10.1057\/dbm.2012.17*\n\n*Kumar V., & Werner Reinartz: Customer Relationship Management_Concept, Strategy, and Tools. 10.1007\/978-3-662-55381-7*","70702f6f":"* Generate summary statistics for all categorical variables.","6233fe54":"* Spectral Clustering with 4 clusters:","23176d8b":"* Plot the distribution for all variables to detect potential outliers and decide whether normalization is needed.","bf82db1a":"*Cluster5* contains 314 customers, composed of 7% of the whole population. This group seems to be the *most profitable group* as it accounts for 19% of the total sales amount. Most of the customers in this group have started shopping with the online retailer in the second quarter of the year with an average first_purchase of 7.2, and continued to the end of the year with an average recency of 2.7 months since last purchase. Also, customers in this group seems to shop frequently during the month with an average frequency of 1.2 transactions per month. Thus, this group can also be categorized as high recency and high frequency.<br\/>\n\nIn contrast, *Cluster1* includes 834 customers, representing 19.8% of the whole population and accounts for only 5.3% of he total sales amount. This group seems to be the least profitable group as none of the customers in this group purchased anything in the last five months of the year. Even for the first seven months of the year, the consumers didn't shop often, and the average value of frequency was only 1 transaction per month. <br\/>\n\n*Cluster3* contains 1096 customers, composed of 26% of the whole population and accounts for 8% of the total sales amount. This group includes new customers with an average first_purchase of 3 and average recency of 1.8.","9ce073d2":"* Plots show that variables have very different scales and some of them are heavily skewed. So we will normalize all variables.","d8b86cc0":"* Summary of the number and percentage of missing values in each variable.","bf91f552":"# 2. Feature Engineering","e17b9a62":"### 2. KMeans","592ff80d":"The plots show some potential outliers in both frequency and Monetary value, so we will drop those customers from our dataset. Such that customers having more than Mean + 3 Std (i.e. Z-score > 3). will be dropped.","7f90600b":"## 3.2 Select a few promising models","0c8450ae":"# 4. Cluster analysis","fc2d8200":"Drop duplicated records.","35c4dfed":"By examining cluster_summary dataframe, cluster_stats dataframe and the scatter plot above , it is interesting to see that each cluster indeed contains a group of consumers that have certain distinct and intrinsic features as detailed below.","8832beb5":"* KMeans with 6 clusters:","ededa71f":"* Separate transaction time (in hours) from transaction date.","584e3421":"* For each customer, calculate the average number of unique items in each transaction.","ae56a408":"## 3.1 Trying bunch of candidate models","f295ad67":"* Check duplicated records"}}