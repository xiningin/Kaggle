{"cell_type":{"bc3087ee":"code","cdd8fa89":"code","e6c4fb47":"code","c9882de4":"code","e4ded7e1":"code","9bc25a03":"code","c24edc01":"code","fa0b63e4":"code","16933250":"code","b2ebed94":"code","de629a09":"code","5ccd9792":"code","17d81954":"code","58cf2b58":"code","551eb2db":"code","e24423f4":"code","f09ce419":"code","d23b1d8d":"code","b54a6d6c":"code","3fd99ee2":"code","2365b369":"code","aeaba32a":"markdown","cd729eab":"markdown","2c71829c":"markdown","c60fbd98":"markdown"},"source":{"bc3087ee":"import os\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np","cdd8fa89":"import torch.nn.functional as F\nfrom copy import deepcopy","e6c4fb47":"from torchvision import models","c9882de4":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","e4ded7e1":"dim = 512\ntransform = transforms.Compose([\n    transforms.CenterCrop(dim),\n    transforms.ToTensor()\n])","9bc25a03":"def openImageFromPath(path):\n    img = Image.open(path)\n    img = transform(img).unsqueeze(0) ## Add extra dimensions\n    return img.to(device, torch.float)","c24edc01":"style_img_path = '..\/input\/image-style-transfergoogle-images\/Image_style_transfer\/art\/art_7.jpg'\ncontent_img_path = '..\/input\/image-style-transfergoogle-images\/Image_style_transfer\/train\/train_4.jpg'","fa0b63e4":"style_img =  openImageFromPath(style_img_path)\ncontent_img = openImageFromPath(content_img_path)","16933250":"unloader = transforms.ToPILImage()    ","b2ebed94":"def display(tensor_img, title=None):\n    img = tensor_img.cpu().clone()\n    img = img.squeeze(0)\n    img = unloader(img)\n    plt.title(title)\n    plt.imshow(img)","de629a09":"display(style_img)","5ccd9792":"display(content_img)","17d81954":"class ContentLoss(nn.Module):\n    '''Implements content loss as a pytorch layer'''\n    def __init__(self, target):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n    \n    def forward(self, inputTensor):\n        self.loss = F.mse_loss(inputTensor, self.target)\n        return inputTensor","58cf2b58":"def normalisedGramMatrix(inputMatrix):\n    d0, d1, d2, d3 = inputMatrix.size()\n    features = inputMatrix.view(d0*d1, d2*d3)\n    gmMatrix = features @ features.t()\n    return gmMatrix \/(d0*d1*d2*d3)","551eb2db":"class StyleLoss(nn.Module):\n    def __init__(self, targetfeature):\n        super(StyleLoss, self).__init__()\n        self.target = normalisedGramMatrix(targetfeature).detach()\n        \n    def forward(self, inputTensor):\n        gmatrix = normalisedGramMatrix(inputTensor)\n        self.loss = F.mse_loss(gmatrix, self.target)\n        return inputTensor","e24423f4":"cnnModel = models.vgg16(pretrained = True).features.to(device).eval()","f09ce419":"cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n# create a module to normalize input image so we can easily put it in a\n# nn.Sequential\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n\n    def forward(self, img):\n        # normalize img\n        return (img - self.mean) \/ self.std","d23b1d8d":"content_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    cnn = deepcopy(cnnModel)\n\n    # normalization module\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n\n    # just in order to have an iterable access to or list of content\/syle\n    # losses\n    content_losses = []\n    style_losses = []\n\n    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n    # to put in modules that are supposed to be activated sequentially\n    model = nn.Sequential(normalization)\n\n    i = 0  # increment every time we see a conv\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            # The in-place version doesn't play very nicely with the ContentLoss\n            # and StyleLoss we insert below. So we replace with out-of-place\n            # ones here.\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            # add content loss:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            # add style loss:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # now we trim off the layers after the last content and style losses\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses","b54a6d6c":"input_img = content_img.clone()\n# if you want to use white noise instead uncomment the below line:\n# input_img = torch.randn(content_img.data.size(), device=device)\n\n# add the original input image to the figure:\nplt.figure()\ndisplay(input_img, title='Input Image')","3fd99ee2":"def get_input_optimizer(input_img):\n    # this line to show that input is a parameter that requires a gradient\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer","2365b369":"def run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=300,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            # correct the values of updated input image\n            input_img.data.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            style_score *= style_weight\n            content_score *= content_weight\n\n            loss = style_score + content_score\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # a last correction...\n    input_img.data.clamp_(0, 1)\n\n    return input_img\n\noutput = run_style_transfer(cnnModel, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)\n\nplt.figure()\ndisplay(output, title='Output Image')\nplt.show()","aeaba32a":"# Content Loss","cd729eab":"# Neural Style Transfer\nThis is a tutorial heavily taken from pytorch website","2c71829c":"## Set the device","c60fbd98":"# Gram Matrix Calculation for Style Loss"}}