{"cell_type":{"987f4259":"code","775d2aa2":"code","7d743bcd":"code","90401c81":"code","6ad5ab6f":"code","03b3e802":"code","52fe4a4c":"code","7928c6f7":"code","25f3c3da":"code","44aeee32":"code","8ee69e8c":"code","545720eb":"code","2172471b":"code","48ae16a0":"code","a6d8d675":"code","9ad0ec05":"code","42eb33ae":"code","8f5e5fe1":"code","2cfc8821":"code","a3316647":"code","7326ae4b":"code","f681f487":"code","f99547c0":"code","2febb3f1":"code","0fc47bbd":"code","26dadc1b":"code","72edb593":"code","4fd1bd0e":"code","968a36a4":"code","e3dc5ecf":"code","0fdf7d47":"code","81809c86":"code","d130ab5e":"code","12454223":"code","c442da33":"code","b92071ae":"code","8532928f":"code","97cf1024":"code","34744118":"code","db5517e0":"code","9a1dae20":"code","f60d9e56":"code","f0d79644":"code","5c8b2000":"code","02d4842f":"code","1c420551":"code","57690361":"code","cc3c9158":"code","39355fd6":"code","fc22e227":"code","eaa8ad1b":"code","dca358b5":"code","d8fdcb75":"code","d7c9da09":"code","e64e0a9f":"code","9dc8f3b4":"code","f322bc34":"code","879d5e78":"code","04e9ac92":"markdown","c4773af0":"markdown","4140a0dc":"markdown","492bd2ab":"markdown","9647d1a4":"markdown","bd16c573":"markdown","4b1090c9":"markdown","c7958660":"markdown","7f7037b7":"markdown","910aeacd":"markdown","7c77d840":"markdown","1afe089e":"markdown","f1696ed5":"markdown","f0a52aab":"markdown","00474ce8":"markdown","a9422bc2":"markdown","4ed0e3e9":"markdown","f76029f5":"markdown","c7063bc8":"markdown","0ab9d592":"markdown","e996fd39":"markdown","f5246b2e":"markdown","132f4df8":"markdown","24ca073d":"markdown","7cfb3ccd":"markdown"},"source":{"987f4259":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","775d2aa2":"# imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n#3ways to access system files\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/\"]).decode(\"utf8\"))\nprint(check_output([\"ls\", \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"]).decode(\"utf8\"))\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7d743bcd":"# setting the number of cross validations used in the Model part \nnr_cv = 5\n\n# switch for using log values for SalePrice and features     \nuse_logvals = 1    \n# target used for correlation \ntarget = 'SalePrice_Log'\n    \n# only columns with correlation above this threshold value  \n# are used for the ML Regressors in Part 3\nmin_val_corr = 0.4    \n    \n# switch for dropping columns that are similar to others already used and show a high correlation to these     \ndrop_similar = 1","90401c81":"def get_best_score(grid):\n    \"\"\"Function to return best score\n    args : grid\n    output : best_score\"\"\"\n    \n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","6ad5ab6f":"def print_cols_large_corr(df,nr_c,targ):\n    \"\"\"\n    Function to print columns with larger correlations\n    args:\n        df = dataframe\n        nr_c = num of columns\n        targ = target column\n    \"\"\"\n    corr=df.corr()\n    corr_abs=corr.abs()\n    print(corr_abs.nlargest(nr_c, targ)[targ])","03b3e802":"def plot_corr_matrix(df, nr_c, targ) :\n    \"\"\"\n    Function to plot correlation matrix between variables and target\n    args:\n        df = dataframe\n        nr_c = num of columns\n        targ = target column\n    \"\"\"\n    \n    corr = df.corr()\n    corr_abs = corr.abs()\n    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n    cm = np.corrcoef(df[cols].values.T)\n\n    plt.figure(figsize=(nr_c\/1.5, nr_c\/1.5))\n    sns.set(font_scale=1.25)\n    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values\n               )\n    plt.show()","52fe4a4c":"df_train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","7928c6f7":"print(df_train.shape)\ndf_train.info()","25f3c3da":"df_train.head()","44aeee32":"df_test.head()","8ee69e8c":"df_train.describe()","545720eb":"df_test.describe()","2172471b":"print(df_test.shape)\ndf_test.info()","48ae16a0":"print(df_train.shape)\nprint(df_test.shape)","a6d8d675":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","9ad0ec05":"# going from non-normal to logarithmic distribution\ndf_train['SalePrice_Log']=np.log(df_train['SalePrice'])\nsns.distplot(df_train['SalePrice_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())\n# dropping old SalePrice column\ndf_train.drop('SalePrice',axis=1,inplace=True)","42eb33ae":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","8f5e5fe1":"print('Numerical columns in train set :')\nprint(df_train[numerical_feats].columns)\nprint(\"*\"*100)\nprint('Categorical columns in train set: ')\nprint(df_train[categorical_feats].columns)","2cfc8821":"df_train[numerical_feats].head()","a3316647":"df_train[categorical_feats].head()","7326ae4b":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","f681f487":"# unique values in Pool\ndf_train.PoolQC.unique()","f99547c0":"# Columns which have NaN values present in them\nnan_cols = [i for i in df_train.columns if df_train[i].isnull().any()]\nprint(len(nan_cols))\nnan_cols","2febb3f1":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']\n","0fc47bbd":"len(cols_fillna)","26dadc1b":"# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('None',inplace=True)\n    df_test[col].fillna('None',inplace=True)","72edb593":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","4fd1bd0e":"# df_train['LotFrontage'].head()\ncols=['LotFrontage','GarageYrBlt','MasVnrArea','SalePrice_Log','ExterCond']\nfor col in cols:\n    print(df_train[i].dtype)","968a36a4":"# fillna with mean for the remaining columns: LotFrontage, GarageYrBlt, MasVnrArea\ndf_train.fillna(df_train.mean(), inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)","e3dc5ecf":"total=df_train.isnull().sum().sort_values(ascending=False)\npercent=(df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data=pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data.head()\n\n\n# total = df_train.isnull().sum().sort_values(ascending=False)\n# percent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\n# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# missing_data.head(5)","0fdf7d47":"#Now, we should have 0 Nan columns\n# Columns which have NaN values present in them\nnan_cols = [i for i in df_train.columns if df_train[i].isnull().any()]\nprint(len(nan_cols))\nnan_cols","81809c86":"df_train.isnull().sum().sum()","d130ab5e":"df_test.isnull().sum().sum()","12454223":"numerical_feats","c442da33":"categorical_feats","b92071ae":"len(numerical_feats)+len(categorical_feats)","8532928f":"for col in numerical_feats:\n    print('{:15}'.format(col), \n          'Skewness: {:05.2f}'.format(df_train[col].skew()) , \n          '   ' ,\n          'Kurtosis: {:06.2f}'.format(df_train[col].kurt())  \n         )","97cf1024":"# lets check skewness and kurtosis for GrLivArea\nsns.distplot(df_train['GrLivArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea'].kurt())","34744118":"sns.distplot(df_train['LotArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea'].kurt())","db5517e0":"# transforming to make closer to normal dstribution (log) for GrLivArea and LotArea\nfor df in [df_train, df_test]:\n    df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n    df.drop('GrLivArea', inplace= True, axis = 1)\n    df['LotArea_Log'] = np.log(df['LotArea'])\n    df.drop('LotArea', inplace= True, axis = 1)\n    \n    \n    \nnumerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n","9a1dae20":"print(len(numerical_feats))\nnumerical_feats","f60d9e56":"sns.distplot(df_train['GrLivArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea_Log'].kurt())","f0d79644":"sns.distplot(df_train['LotArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea_Log'].kurt())","5c8b2000":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['Id', 'SalePrice', 'SalePrice_Log']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target])\n            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","02d4842f":"sns.distplot(df_train['OverallQual'])","1c420551":"df_train = df_train.drop(\n    df_train[(df_train['OverallQual']==10) & (df_train['SalePrice_Log']<12.3)].index)","57690361":"df_train = df_train.drop(\n    df_train[(df_train['GrLivArea_Log']>8.3) & (df_train['SalePrice_Log']<12.5)].index)","cc3c9158":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']","39355fd6":"df_train.info()","fc22e227":"df_train.isna()","eaa8ad1b":"numerical_columns_train","dca358b5":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","d8fdcb75":"df_test= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_train= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","d7c9da09":"df_test.head()","e64e0a9f":"df=df_train\ndf.head()","9dc8f3b4":"df.shape","f322bc34":"df.info()","879d5e78":"# Analyse data\n# clean data\n# take significant columns\n# make base model\n# apply feature engineering\n# try to beat model , use many model\n# predict","04e9ac92":"**obersvation: **\nIn train set, we have 1460 rows and 79 dependent variables + 1 id column + 1 target variable\nIn test set, we have 1459 rows and 78 dependent variables + 1 id column\n\ndf train has 81 columns (79 features + id and target SalePrice) and 1460 entries (number of rows or house sales)  \ndf test has 80 columns (79 features + id) and 1459 entries  \nThere is lots of info that is probably related to the SalePrice like the area, the neighborhood, the condition and quality.   \nMaybe other features are not so important for predicting the target, also there might be a strong correlation for some of the features (like GarageCars and GarageArea).\nFor some columns many values are missing: only 7 values for Pool QC in df train and 3 in df test","c4773af0":"**skewness and kurtosis** - These two statistics are called \"shape\" statistics, i.e., they describe the shape of the distribution.  \n\n**Skewness(+\/-)** is a measure of the symmetry in a distribution.  A symmetrical dataset will have a skewness equal to 0.  So, a normal distribution will have a skewness of 0.   Skewness essentially measures the relative size of the two tails.  \n\n**Kurtosis** is a measure of the combined sizes of the two tails (heaviness of the tails).  It measures the amount of probability in the tails.  The value is often compared to the kurtosis of the normal distribution, which is equal to 3.  If the kurtosis is greater than 3, then the dataset has heavier tails than a normal distribution (more in the tails).  If the kurtosis is less than 3, then the dataset has lighter tails than a normal distribution (less in the tails).","4140a0dc":"# Part 1: Exploratory Data Analysis","492bd2ab":"1. 1. **Missing values in test data ?**","9647d1a4":"**Settings and switches**\n\n**Here one can choose settings for optimal performance and runtime.**  \n**For example, nr_cv sets the number of cross validations used in GridsearchCV, and**  \n**min_val_corr is the minimum value for the correlation coefficient to the target (only features with larger correlation will be used).** ","bd16c573":"### log transform\nLike the target variable, also some of the feature values are not normally distributed and it is therefore better to use log values in df_train and df_test. Checking for skewness and kurtosis:","4b1090c9":"![](http:\/\/)![](http:\/\/)**Missing values in train data ?**","c7958660":"**Outliers**","7f7037b7":"### Numerical and Categorical features","910aeacd":"### The target variable : Distribution of SalePrice","7c77d840":"![](http:\/\/)**Load data**","1afe089e":"## We'll be going throught following topics :\n* EDA with Pandas and Seaborn  \n<li>Find features with strong correlation to target  \n<li>Data Wrangling, convert categorical to numerical  \n<li>apply the basic Regression models of sklearn  \n<li>use gridsearchCV to find the best parameters for each model  \n<li>compare the performance of the Regressors and choose best one  ","f1696ed5":"### List of features with missing values","f0a52aab":"### Plots of relation to target for all numerical features","00474ce8":"## 1.2 Relation of features to target (SalePrice_log)","a9422bc2":"# House Prices: Advanced Regression Techniques","4ed0e3e9":"**Competition Description**\n\n![](https:\/\/www.reno.gov\/Home\/ShowImage?id=7739&t=635620964226970000)\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith **79** explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nPractice Skills  \nCreative feature engineering   \nAdvanced regression techniques like random forest and gradient boosting  \n\n**Acknowledgments**  \nThe [Ames Housing dataset](http:\/\/www.amstat.org\/publications\/jse\/v19n3\/decock.pdf) was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. \n\nYou can download the txt file here: [**download**](https:\/\/www.kaggle.com\/c\/5407\/download\/data_description.txt)","f76029f5":"**Filling missing values**  \nFor a few columns there is lots of NaN entries.  \nHowever, reading the data description we find this is not missing data:  \nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.  ","c7063bc8":"# common used function","0ab9d592":"**Find columns with strong correlation to target**  \nOnly those with r > min_val_corr are used in the ML Regressors in Part 3  \nThe value for min_val_corr can be chosen in global settings","e996fd39":"**Conclusion from EDA on numerical columns:**\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.  \nFor other features like 'MSSubClass' the correlation is very weak.  \nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.  \nThis threshold value can be choosen in the global settings : min_val_corr  \n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:  \n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF',  'LowQualFinSF',  'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',   \n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.  \nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)","f5246b2e":"**The notebook is organized as follows:**\n\n* **[Part 0: Imports, Settings and switches, Global functions](#Part-0-:-Imports,-Settings,-Functions)**  \nimport libraries  \nsettings for number of cross validations  \n* **[define functions that are used often](#common used function)**  \n\n* **[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)**  \n1.1 Get an overview of the features (numerical and categorical) and first look on the target variable SalePrice  \n[shape, info, head and describe](#shape,-info,-head-and-describe)  \n[Distribution of the target variable SalePrice](#The-target-variable-:-Distribution-of-SalePrice)  \n[Numerical and Categorical features](#Numerical-and-Categorical-features)  \n[List of features with missing values](#List-of-features-with-missing-values) and Filling missing values  \n[log transform](#log-transform)  \n1.2 Relation of all features to target SalePrice  \n[Seaborn regression plots for numerical features](#Plots-of-relation-to-target-for-all-numerical-features)  \n[List of numerical features and their correlation coefficient to target](#List-of-numerical-features-and-their-correlation-coefficient-to-target)  \n[Seaborn boxplots for categorical features](#Relation-to-SalePrice-for-all-categorical-features)  \n[List of categorical features and their unique values](#List-of-categorical-features-and-their-unique-values)  \n1.3 Determine the columns that show strong correlation to target  \n[Correlation matrix 1](#Correlation-matrix-1) : all numerical features  \nDetermine features with largest correlation to SalePrice_Log\n\n\n* **[Part 2: Data wrangling](#Part-2:-Data-wrangling)**  \n[Dropping all columns with weak correlation to SalePrice](#Dropping-all-columns-with-weak-correlation-to-SalePrice)  \n[Convert categorical columns to numerical](#Convert-categorical-columns-to-numerical)  \n[Checking correlation to SalePrice for the new numerical columns](#Checking-correlation-to-SalePrice-for-the-new-numerical-columns)  \nuse only features with strong correlation to target  \n[Correlation Matrix 2 (including converted categorical columns)](#Correlation-Matrix-2-:-All-features-with-strong-correlation-to-SalePrice)  \ncreate datasets for ML algorithms  \nOne Hot Encoder  \n[StandardScaler](#StandardScaler)\n\n* **[Part 3: Scikit-learn basic regression models and comparison of results](#Part-3:-Scikit-learn-basic-regression-models-and-comparison-of-results)**  \nimplement GridsearchCV with RMSE metric for Hyperparameter tuning  \nfor these models from sklearn:  \n[Linear Regression](#Linear-Regression)  \n[Ridge](#Ridge)  \n[Lasso](#Lasso)  \n[Elastic Net](#Elastic-Net)  \n[Stochastic Gradient Descent](#SGDRegressor)  \n[DecisionTreeRegressor](#DecisionTreeRegressor)  \n[Random Forest Regressor](#RandomForestRegressor)  \n[KNN Regressor](#KNN-Regressor)  \nbaed on RMSE metric, compare performance of the regressors with their optimized parameters,  \nthen explore correlation of the predictions and make submission with mean of best models  \nComparison plot: [RMSE of all models](#Comparison-plot:-RMSE-of-all-models)  \n[Correlation of model results](#Correlation-of-model-results)  \nMean of best models\n\n\nNote on scores:  \nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","132f4df8":"observations from the distribution of SalePrice:  \n<li>Deviate from the normal distribution.\n<li>Have appreciable positive skewness.\n<li>Show peakedness.\n\nAs we see, the target variable SalePrice is not normally distributed.  \nThis can reduce the performance of the ML regression models because some assume normal distribution,   \nsee [sklearn info on preprocessing](http:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html)  \nTherfore we make a log transformation, the resulting distribution looks much better.  ","24ca073d":"## 1.1 Overview of features and relation to target\n\nLet's get a first overview of the train and test dataset  \nHow many rows and columns are there?  \nWhat are the names of the features (columns)?  \nWhich features are numerical, which are categorical?  \nHow many values are missing?  \nThe **shape** and **info** methods answer these questions  \n**head** displays some rows of the dataset  \n**describe** gives a summary of the statistics (only for numerical columns)","7cfb3ccd":"# Part-0-:-Imports,-Settings,-Functions"}}