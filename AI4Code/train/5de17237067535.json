{"cell_type":{"329854ed":"code","03339f09":"code","c9b7a691":"code","0aca1d27":"code","edbe7f2c":"code","ae8d6e86":"code","4914f220":"code","ae5b7e8c":"code","ddb17931":"code","2fb3a451":"code","cecb9f00":"code","61245a25":"code","6cb43a78":"code","d4a221e5":"code","6e7fd4c6":"code","5e0acff2":"code","cb32de96":"code","30974536":"code","f04cac90":"code","61d71f93":"code","8e0f2c66":"code","07fa5e00":"code","b8f8f984":"code","caefed9f":"code","a128aed9":"code","49e15bab":"code","f66c601b":"code","62fa4c03":"code","fd055c02":"code","208b3063":"code","4b0c35c8":"code","325b556f":"markdown","6749f2a3":"markdown","f0a3a468":"markdown","88c9f4dd":"markdown","dddce3e2":"markdown","2b7c68c8":"markdown","434c12ea":"markdown","5fee599c":"markdown","0de7ecbf":"markdown"},"source":{"329854ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","03339f09":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/kaancanimg\/kaan.jpg\")","c9b7a691":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp.png\")","0aca1d27":"#reading data from csv file,we have two types of voice -male- or -female- ( 0 or 1 we'll say)\ndata=pd.read_csv(\"..\/input\/voicedata\/voice.csv\")\n\n#First we'll observe our features with info() command\n#drop column that has no effect our classification,but our data is already clean\n#check out that your data which is used to classify features cannot be object it must be\n#categorical or integer !!\nprint(\"Data before arrangement: \",data.info())\n\n#we'll convert the label column which has objects in the type of object into the integer,mean 0 or 1\n#the label column must be in the type of categorical or int\n\ndata.label=[ 1 if each=='female' else 0 for each in data.label ] \nprint(\"Data after arrangement: \",data.info()) # so now we've fixed it\n\n#1 and 0's binary data will be in yaxis ,don't forget to convert them into numpy array\ny=data.label.values\n#dropping label column and assigning other columns to x_data\nx_data=data.drop(['label'],axis=1)\n","edbe7f2c":"y","ae8d6e86":"x_data.tail()","4914f220":"data.corr()","ae5b7e8c":"import seaborn as sns \n\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","ddb17931":"#import the train and test library\nfrom sklearn.model_selection import train_test_split\n\n#we are normalizing this data,means giving every data a new value within 0 and 1\nx=( x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\n\n#initializing the train and test datas\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)\n#we had the feature names as columns previously so to see it better implement them as\n#their tranpose\nprint('x_train: ',x_train.shape )\nprint('Images: ',x_train.shape[0],'Pixels: ',x_train.shape[1])\nprint('y_train labels: ',y_train.shape )\nprint('x_test: ',x_test.shape )\nprint('Images: ',x_test.shape[0],'Pixels: ',x_test.shape[1])\nprint('y_test labels: ',y_test.shape )\n\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\n\n#we've taken transposes of datas because we wanted to make it as seems in picture above\n#mean taking the pixels at the left of that picture\n#let's investigate the sizes of train and test data\n\nprint('\\nAfter Transposes------------\\n\\n','x_train: ',x_train.shape )\nprint('Images: ',x_train.shape[0],'Pixels: ',x_train.shape[1])\nprint('y_train labels: ',y_train.shape )\nprint('x_test: ',x_test.shape )\nprint('Images: ',x_test.shape[0],'Pixels: ',x_test.shape[1])\nprint('y_test labels: ',y_test.shape )","2fb3a451":"x_train.tail()","cecb9f00":"x_test.tail()","61245a25":"y","6cb43a78":"y_test[:10]","d4a221e5":"def sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head","6e7fd4c6":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp7.png\")\n","5e0acff2":"def initialize_weights_and_bias( dimension ):\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b","cb32de96":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp3.png\")","30974536":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp2.png\")","f04cac90":"def forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    #weight has (30,1) dimensions,so initially create weight as (pixel_num,1)-->\n    #x_train has (30,455) dimensions\n    #so get the transpose of the weight matrix make it (30,1) in dimensions\n    z=np.dot(w.T,x_train)+b # and make matrix multiplication (1,30)x(30,455) we get the z as ( 1,455 )\n    print('z is:',z)\n    y_head=sigmoid(z)\n    loss=-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost=(np.sum(loss))\/x_train.shape[1]\n    \n    #back propagation\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias=np.sum( y_head-y_train)\/x_train.shape[1]\n    gradients={'derivative_weight':derivative_weight,'derivative_bias':derivative_bias}\n    \n    return cost,gradients\n    ","61d71f93":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp4.png\")","8e0f2c66":"#updating learning parameters\ndef update( w, b, x_train, y_train, learning_rate, number_of_iteration ):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n    #updating learning parameters is number of iteration times\n    \n    for i in range( number_of_iteration):\n        #make forward and backward propagation and find cost ad gradients\n        cost,gradients=forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        #update process\n        w=w-learning_rate*gradients[\"derivative_weight\"]\n        b=b-learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n           cost_list2.append(cost)\n           index.append(i)\n           #print('Cost after iteration %i: %f' %(i,cost))\n    \n   #we update(learn) parameters weight and bias\n   \n    parameters={'weight':w,'bias':b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel('Number Of Iteration')\n    plt.ylabel('Cost')\n    plt.show()\n    return parameters,gradients,cost_list\n","07fa5e00":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp5.png\")","b8f8f984":" # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b) #probabilistic result\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","caefed9f":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp6.png\")","a128aed9":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","49e15bab":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp8.png\")","f66c601b":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 1500)","62fa4c03":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 3000)","fd055c02":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 5000)","208b3063":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/screen\/pp9.png\")","4b0c35c8":"from sklearn import linear_model\n\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\n\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","325b556f":"**Loss Function and Cost Function**<br>\nLet\u2019s define y as the true label (y = 0 or y = 1) and y_hat as the predicted output (or the probability that y = 1 given inputs w and x). Therefore, the probability that y = 0 given inputs w and x is (1 - y_hat), as shown below.","6749f2a3":"The **shape of the matrix w** is determined by the **number of units in the layer you\u2019re mapping to** and the **number of units in the layer you\u2019re mapping from**\u2014 hence the shape [1 x 3] because the output layer has 1 unit and the input layer has 3 units (not counting the bias term).We initialize it (dimension=pixel,1) and then we are taking the tranpose again while we are making matrix multiplication,\nin forward backward propagation.","f0a3a468":"We multiply the inputs x by variables **w** called **weights** to get the output. You can think of weights as the strengths of the connections between input and output \u2014 for example, if w1 has a higher value than w2, that implies that the input x1 influences the output more than x2 does. The value b is called the bias term (it isn\u2019t multiplied by a weight) and is responsible for shifting the function so that it\u2019s not constrained to the origin. Representing the function in matrix form yields:","88c9f4dd":"**Step by Step Description** <br>\n\n1. We multiply all **x values( features )** with **weights** one by one and then add **bias** value to the result.\n2. We call the result as **z**,and then put **z** into **sigmoid function**.Which gives a value between **0 and 1** to each input.And then we get the **y_head**,which we call as the real prediction results previously.\n3. Now we have both **y** and **y_head** values,so use them to find loss value by using them in the **loss function.**\n4. By gathering **all loss values together** we find the **cost value.**\n5. If the **cost value is so high** then **we have to go back to update the values of weights and bias.**\n6. Up until that point we've made **forward propagation**,now we will make **back propagation**.Find the derivative of cost function according to the both weight and bias and find the slope.\n7. If the slopes that we've found earlier are closing 0 slowly then it means that we are so close the cost value.There will be no update required as we go forward.\n8. If we are far from the cost value then it means that we have update weight and bias values.","dddce3e2":"In these two equations, the partial derivatives dw and db represent the effect that a change in w and b have on the cost function, respectively. By finding the slope and taking the negative of that slope, we ensure that we will always move in the direction of the minimum. To get a better understanding, let\u2019s see this graphically for dw:","2b7c68c8":"One of the reasons we use this cost function for logistic regression is that it\u2019s a convex function with a single global optimum. Imagine rolling a ball down the bowl-shaped function \u2014 it would settle at the bottom; similarly, to find the minimum of the cost function, we need to get to the lowest point. To do that, we can start from anywhere on the function and iteratively move down in the direction of the steepest slope, adjusting the values of w and b that lead us to the minimum. The formulas are:\n","434c12ea":"When the derivative term is positive, we move in the opposite direction towards a decreasing value of w and when the derivative is negative we move in the direction of increasing w, thereby ensuring that we\u2019re always moving toward the minimum.\nThe alpha term in front of the partial derivative is called the learning rate and is a measure of how big a step to take at each iteration. The choice of learning parameters is an important one \u2014 too small and the model will take an undue amount of time to find the minimum, too large and the model might overshoot the minimum and fail to converge.\nGradient descent is the essence of the learning process \u2014 through it the machine learns what values of weights and biases minimize the cost function. It does this by iteratively comparing its predicted output for a set of data to the true output in a process called training.","5fee599c":"The **goal of the loss function** is to **minimize the error between the predicted and desired output** and thus arrive at an optimal solution for one training example. However, to get useful results we need to take the average of the loss function over an entire training set that contains many examples (for a total of m examples). This is defined as the cost function J(w,b) and we\u2019ll find the parameters w and b that minimize the overall cost function:","0de7ecbf":"**Logistic Regression**<br>\nLogistic regression is a binary classification method. For example, after being trained on images of cats and dogs and then being given a picture that it has never seen before of a cat (y=0) or a dog (y=1), can the machine predict the correct type? As we\u2019ll see, even a simple algorithm like logistic regression can do surprisingly well.\nWe want to model a function that can take in any number of inputs and constrain the output to be between 0 and 1. Where have we seen that before? You guessed it \u2014 our humble neuron, armed with the sigmoid activation function, boldly comes in to save the day! Now is there a way to measure how good our predicted output is compared to the true label?<br>\n\n"}}