{"cell_type":{"ad24d713":"code","ab2f09e9":"code","fd0c157f":"code","f16e3d46":"code","c763f813":"code","a1288efa":"code","852f3bc8":"code","ce61e8a8":"code","d2acfc31":"code","29101029":"code","c88402c3":"code","9f07804e":"code","6afdd9e5":"code","7a687924":"code","1b9b6440":"code","789d44c9":"code","b3d16830":"code","d1053d8a":"code","0f088c38":"code","fc6173aa":"code","e8ac5254":"code","37f98120":"code","847d8bdb":"code","9fbb47af":"code","31d8aafb":"code","ad22e9e8":"code","53b8bc13":"code","2d848c36":"code","7222fb6b":"code","0513efa4":"code","dc4a833f":"code","58ba02c1":"code","c6c94588":"code","e8cf33d3":"code","ddc988af":"code","0ac68041":"code","473c48ff":"code","6e84820e":"code","f83bf460":"code","b22b0910":"code","b4749190":"code","64359846":"code","80f4ebaa":"code","5774420b":"code","3fba70e3":"code","e3092f41":"code","f99ffeb2":"code","fa7b91ac":"code","3eaca559":"code","ba9872c5":"code","7547ac1c":"code","a424143d":"code","dcef7c90":"code","8d92bb98":"code","5556cc3a":"code","e62ddd1c":"code","2713e33a":"code","8056cdf5":"code","50c01b18":"markdown","0eecd649":"markdown","3574b050":"markdown","4e79fe07":"markdown","5324ade2":"markdown","89b9d4ea":"markdown","5718d5d6":"markdown","5d54f48a":"markdown","b6fae741":"markdown","547b14f7":"markdown","1e58ae87":"markdown","38c924ce":"markdown","ec7fbd86":"markdown","32a3cd89":"markdown","ffc1674a":"markdown","68e669f1":"markdown","5c34901d":"markdown","796b7955":"markdown","153751be":"markdown","92753760":"markdown","ef123b60":"markdown","8ab1ab67":"markdown","36362788":"markdown","76d5e18e":"markdown","3972a3f3":"markdown","ffd455c8":"markdown","736bcecd":"markdown","110a8c1d":"markdown","ad14a694":"markdown","397d13bb":"markdown","e7298a01":"markdown","84265c90":"markdown","c2c2da46":"markdown","51043862":"markdown","f8ffd502":"markdown","c8ce228b":"markdown","3146e77c":"markdown","12cb1d03":"markdown"},"source":{"ad24d713":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')","ab2f09e9":"data = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\ndata","fd0c157f":"print(f\"Target: 'Income'\\nUnique Values in Income: {data.income.unique()}\\nNumber of unique values: {data.income.nunique()}\")","f16e3d46":"data['income'] = data['income'].str.replace('<=50K', '0')\ndata['income'] = data['income'].str.replace('>50K', '1')\ndata['income'] = data['income'].astype(np.int64)","c763f813":"data.income.dtypes","a1288efa":"ds = data.copy()\nprint(f\"Unique values in 'education': {ds.education.nunique()}\\nUnique values in 'Education_num': {ds['education.num'].nunique()}\")","852f3bc8":"ds.drop(['education'], axis = 1, inplace = True)","ce61e8a8":"plt.title(\"Null values in the data\", fontsize = 12)\nsns.heatmap(ds.isnull(), cmap = 'inferno')\nplt.show()","d2acfc31":"print(\"Datatype of every feature: \")\nds.dtypes","29101029":"print(\"Number of unique values in every feature: \")\nds.nunique()","c88402c3":"ds.workclass.value_counts()","9f07804e":"ds['workclass'] = ds['workclass'].str.replace('Never-worked', 'Without-pay')","6afdd9e5":"ds['workclass'] = ds['workclass'].replace('?', np.NaN)","7a687924":"plt.figure(figsize = (10,6))\nplt.title(\"Income of people according to their workclass\", fontsize = 16)\nsns.countplot(y = ds['workclass'], hue = ds['income'])\nplt.show()","1b9b6440":"from scipy.stats import mode\nworkclass_mode = ds.pivot_table(values='workclass', columns='occupation',aggfunc=(lambda x:mode(x).mode[0]))\nworkclass_mode","789d44c9":"loc1 = ds['workclass'].isnull()\nds.loc[loc1, 'workclass'] = ds.loc[loc1,'occupation'].apply(lambda x: workclass_mode[x])","b3d16830":"workclass_enc = (ds.groupby('workclass').size()) \/ len(ds)\nprint(workclass_enc)\n\nds['workclass_enc'] = ds['workclass'].apply(lambda x : workclass_enc[x])\nds['workclass_enc'].head(3)","d1053d8a":"ds.drop(['workclass'], axis = 1, inplace = True)","0f088c38":"ds.occupation.value_counts()","fc6173aa":"ds['occupation'] = ds['occupation'].replace('?', np.NaN)\nds = ds.loc[ds['occupation'].isnull() == False]\nds","e8ac5254":"plt.style.use('ggplot')\nplt.figure(figsize = (10,6))\nplt.title(\"Income of people according to their occupation\", fontsize = 16)\nsns.countplot(y = ds['occupation'], hue = ds['income'])\nplt.show()","37f98120":"occupation_enc = (ds.groupby('occupation').size()) \/ len(ds)\nprint(occupation_enc)\n\nds['occupation_enc'] = ds['occupation'].apply(lambda x : occupation_enc[x])\nds['occupation_enc'].head(3)","847d8bdb":"ds.drop(['occupation'], axis = 1, inplace = True)","9fbb47af":"ds['native.country'].loc[ds['native.country'] == 'United-States'] = 'usa'\nds['native.country'].loc[ds['native.country'] != 'usa'] = 'non_usa'\nds['native.country'].value_counts()","31d8aafb":"plt.style.use('default')","ad22e9e8":"plt.style.use('seaborn-pastel')","53b8bc13":"plt.figure(figsize = (8,3))\nplt.title(\"Income of people according to their native country\", fontsize = 16)\nsns.countplot(y = ds['native.country'], hue = ds['income'])\nplt.show()","2d848c36":"ds['country_enc'] = ds['native.country'].map({'usa' : 1, 'non_usa' : 0})\nds.drop(['native.country'], axis = 1, inplace = True)","7222fb6b":"plt.title(\"Income of people by their sex\", fontsize = 16)\nsns.countplot(x = ds['sex'], hue = ds['income'])\nplt.show()","0513efa4":"ds['sex_enc'] = ds['sex'].map({'Male' : 1, 'Female' : 0})\nds.drop(['sex'], axis = 1, inplace = True)","dc4a833f":"plt.style.use('default')","58ba02c1":"plt.style.use('seaborn-talk')","c6c94588":"plt.title(\"Income of people by Marital Status\", fontsize = 16)\nsns.countplot(y = ds['marital.status'], hue = ds['income'])\nplt.show()","e8cf33d3":"marital_status_enc = (ds.groupby('marital.status').size()) \/ len(ds)\nprint(marital_status_enc)\n\nds['marital_status_enc'] = ds['marital.status'].apply(lambda x : marital_status_enc[x])\nds['marital_status_enc'].head(3)","ddc988af":"ds.drop(['marital.status'], axis = 1, inplace = True)","0ac68041":"plt.style.use('bmh')","473c48ff":"plt.figure(figsize = (12,4))\n\nplt.subplot(1, 2, 1)\nsns.countplot(y = ds['race'], hue = ds['income'])\nplt.title(\"Income respective to Race\", fontsize = 12)\n\nplt.subplot(1, 2, 2)\nsns.countplot(y = ds['relationship'], hue = ds['income'])\nplt.title(\"Income respective to Relationship\", fontsize = 12)\n\nplt.tight_layout(pad = 4)\nplt.show()","6e84820e":"race_enc = (ds.groupby('race').size()) \/ len(ds)\nprint(race_enc,'\\n')\nds['race_enc'] = ds['race'].apply(lambda x : race_enc[x])\n\nrelationship_enc = (ds.groupby('relationship').size()) \/ len(ds)\nprint(relationship_enc)\nds['relationship_enc'] = ds['relationship'].apply(lambda x : relationship_enc[x])","f83bf460":"ds.drop(['race', 'relationship'], axis = 1, inplace = True)\nnew_ds = ds.drop(['income'], axis = 1)\nnew_ds['income'] = ds['income']\nnew_ds","b22b0910":"plt.style.use('default')","b4749190":"plt.style.use('ggplot')","64359846":"clist = ['fnlwgt','age','capital.gain','capital.loss','hours.per.week']\nplt.figure(figsize = (12,6))\nfor i in range(0, len(clist)):\n    plt.subplot(2,3, i+1)\n    sns.boxplot(ds[clist[i]], color = 'skyblue')\nprint(\"BoxPlots of the features:\")\nplt.show()","80f4ebaa":"from scipy.stats import zscore\nzabs = np.abs(zscore(new_ds.loc[:,'fnlwgt':'hours.per.week']))\nprint(np.shape(np.where(zabs >= 3)))\nnew_ds = new_ds[(zabs < 3).all(axis = 1)]\nnew_ds","5774420b":"plt.figure(figsize = (14, 8))\nplt.title(\"Correlation between target and features:\")\nsns.heatmap(new_ds.corr(), annot = True)\nplt.show()","3fba70e3":"from sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\nnew_ds.loc[:,'age':'hours.per.week'] = scale.fit_transform(new_ds.loc[:,'age':'hours.per.week'])\nnew_ds","e3092f41":"plt.figure(figsize = (8, 4))\nplt.title(\"Values distribution in target class: Income\")\nsns.countplot(data = new_ds, x = 'income')\nplt.show()","f99ffeb2":"from imblearn.combine import SMOTETomek\nx = new_ds.loc[:,\"age\":\"relationship_enc\"]\ny = new_ds.loc[:,\"income\"]\nsmk = SMOTETomek()\nx_new, y_new = smk.fit_resample(x, y)","fa7b91ac":"plt.figure(figsize = (8, 4))\nplt.title(\"Values in target class after using SMOTETomek\")\nsns.countplot(x = y_new)\nplt.show()","3eaca559":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nmax_accuracy = 0\nbest_rs = 0\nfor i in range(1, 150):\n    x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size = 0.30, random_state = i)\n    lg = LogisticRegression()\n    lg.fit(x_train, y_train)\n    pred = lg.predict(x_test)\n    acc = accuracy_score(y_test, pred)\n    if acc > max_accuracy: # after each iteration, acc is replace by the best possible accuracy\n        max_accuracy = acc\n        best_rs = i\nprint(f\"Best Random State is {best_rs}, {max_accuracy*100}\")","ba9872c5":"x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size = 0.30, random_state = 67)","7547ac1c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","a424143d":"# For Logistic Regression\nlg = LogisticRegression()\nlg.fit(x_train, y_train)\npred_lg = lg.predict(x_test)\nprint(\"Accuracy Score of Logistic Regression model is\", accuracy_score(y_test, pred_lg)*100)\n\n# For Decision Tree Classifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train, y_train)\npred_dtc = dtc.predict(x_test)\nprint(\"Accuracy Score of Decision Tree Classifier model is\", accuracy_score(y_test, pred_dtc)*100)\n\n# For K-Nearest Neighbour Classifier\nknc = KNeighborsClassifier(n_neighbors = 5)\nknc.fit(x_train, y_train)\npred_knc = knc.predict(x_test)\nprint(\"Accuracy Score of K-Nearest Neighbour Classifier model is\", accuracy_score(y_test, pred_knc)*100)\n\n# For Support Vector Classifier\nsvc = SVC(kernel = 'rbf')\nsvc.fit(x_train, y_train)\npred_svc = svc.predict(x_test)\nprint(\"Accuracy Score of Support Vector Classifier model is\", accuracy_score(y_test, pred_svc)*100)\n\n# For Random Forest Classifier\nrfc = RandomForestClassifier()\nrfc.fit(x_train, y_train)\npred_rfc = rfc.predict(x_test)\nprint(\"Accuracy Score of Random Forest model is\", accuracy_score(y_test, pred_rfc)*100)\n\n# For MultinomialNB\nnb = MultinomialNB() # making the Multinomial Naive Bayes class\nnb.fit(x_train, y_train) # fitting the model\npred_nb = nb.predict(x_test) # predicting the values\nprint(\"Accuracy Score of MultinomialNB model is\", accuracy_score(y_test, pred_nb)*100)\n\n# For ADA Boost Classifier\nada= AdaBoostClassifier()\nada.fit(x_train, y_train) # fitting the model\npred_ada = ada.predict(x_test) # predicting the values\nprint(\"Accuracy Score of ADA Boost model is\", accuracy_score(y_test, pred_ada)*100)","dcef7c90":"from sklearn.model_selection import cross_val_score\n\nlg_scores = cross_val_score(lg, x_new, y_new, cv = 10) # cross validating the model\nprint(lg_scores) # accuracy scores of each cross validation cycle\nprint(f\"Mean of accuracy scores is for Logistic Regression is {lg_scores.mean()*100}\\n\")\n\ndtc_scores = cross_val_score(dtc, x_new, y_new, cv = 10)\nprint(dtc_scores)\nprint(f\"Mean of accuracy scores is for Decision Tree Classifier is {dtc_scores.mean()*100}\\n\")\n\nknc_scores = cross_val_score(knc, x_new, y_new, cv = 10)\nprint(knc_scores)\nprint(f\"Mean of accuracy scores is for KNN Classifier is {knc_scores.mean()*100}\\n\")\n\nsvc_scores = cross_val_score(svc, x_new, y_new, cv = 10)\nprint(svc_scores)\nprint(f\"Mean of accuracy scores is for SVC Classifier is {svc_scores.mean()*100}\\n\")\n\nrfc_scores = cross_val_score(rfc, x_new, y_new, cv = 10)\nprint(rfc_scores)\nprint(f\"Mean of accuracy scores is for Random Forest Classifier is {rfc_scores.mean()*100}\\n\")\n\nnb_scores = cross_val_score(nb, x_new, y_new, cv = 10)\nprint(nb_scores)\nprint(f\"Mean of accuracy scores is for MultinomialNB is {nb_scores.mean()*100}\\n\")\n\nada_scores = cross_val_score(ada, x_new, y_new, cv = 10)\nprint(ada_scores)\nprint(f\"Mean of accuracy scores is for ADA Boost Classifier is {ada_scores.mean()*100}\\n\")","8d92bb98":"# Checking for difference between accuracy and mean accuracies.\nlis3 = ['Logistic Regression','Decision Tree Classifier','KNeighbors Classifier','SVC', 'Random Forest Classifier', \n        'MultinomialNB', 'ADA Boost Classifier']\n\nlis1 = [accuracy_score(y_test, pred_lg)*100, accuracy_score(y_test, pred_dtc)*100, accuracy_score(y_test, pred_knc)*100, \n        accuracy_score(y_test, pred_svc)*100, accuracy_score(y_test, pred_rfc)*100, accuracy_score(y_test, pred_nb)*100,\n        accuracy_score(y_test, pred_ada)*100]\n\nlis2 = [lg_scores.mean()*100, dtc_scores.mean()*100, knc_scores.mean()*100, svc_scores.mean()*100, rfc_scores.mean()*100, \n        nb_scores.mean()*100, ada_scores.mean()*100]\n\nfor i in range(0, 7):\n    dif = (lis1[i]) - (lis2[i])\n    print(lis3[i], dif)","5556cc3a":"from sklearn.model_selection import GridSearchCV\nrfc = RandomForestClassifier()\nparam = dict()\nparam['criterion'] = ['gini', 'entropy']\nparam['n_estimators'] = [1, 2, 4, 8, 10, 16, 32, 64, 100, 200]\nparam['min_samples_split'] = [1,2,5,8,10,15,20,25,50,55,60,80,100]\n\n\ngs = GridSearchCV(estimator = rfc, param_grid = param, scoring='f1', cv = 5, n_jobs = 3)\ngs.fit(x_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)","e62ddd1c":"rfc = RandomForestClassifier(criterion = 'entropy', min_samples_split = 2, n_estimators = 100)\nrfc.fit(x_train, y_train)\nprint(rfc.score(x_train, y_train))\npred_rfc = rfc.predict(x_test)","2713e33a":"from sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nprint(\"Accuracy Score of RFC model is\", accuracy_score(y_test, pred_rfc)*100)\nprint(\"Confusion matrix for RFC Model is\")\nprint(confusion_matrix(y_test, pred_rfc))\nprint(\"Classification Report of the RFC Model is\")\nprint(classification_report(y_test, pred_rfc))\n\nplot_roc_curve(rfc, x_test, y_test) # arg. are model name, feature testing data, label testing data.\nplt.title(\"Recevier's Operating Characteristic\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.show()","8056cdf5":"import joblib\njoblib.dump(rfc, 'Census Income Prediction.obj') # saving the model as an object","50c01b18":"AS we can see from the above table that the data is now more normalised and can be used by the models for learning.\n![](https:\/\/kiarofoods.com\/wp-content\/uploads\/2019\/10\/line_break.png)\n# Data Imbalance:\nIf the data is imbalanced, it can cause the overfitting and bias in the odel prediction. So it is important to check and cure the data imbalance if present. We check the target variable to see if it is balanced or not.","0eecd649":"We can see that male have more salary than female. Also in the dataset, the number of men are more than women. Encoding this feature with one hot encoding.","3574b050":"**'Sex':** Similarly, encoding the sex using one hot encoding.","4e79fe07":"In work class, *majority of the people are private employees*. The *minority of people are either working without-pay or they have never-worked*. We can combine the values of these two values as one. first we remove the blank space from the column is present in any values.","5324ade2":"Best possible random state is 67, so using it to split the data","89b9d4ea":"**Random forest classifier is the best model with highest cross validation mean score and accuracy score**. We will use it for the model building.\n## Hyperparameter Tuning:\nTuning the parameters of the Random Forest in order to obtain the best possible parameters for model building.","5718d5d6":"**White people have a higher salary as compared to other races**. Similarly, **husband in the family have a higher salary as compared to other relationship in the family.** Encoding both these columns","5d54f48a":"Majority of people whose income is greater than 50K are either executive managers or they belong to any professional speciality. Now, encoding the occupation by frequency of the values in the column.","b6fae741":"## Outliers:\nWe check if any outliers are present in the continous attributes of the dataset. We check it both by visualisations and the zscore for the continous columns.","547b14f7":"In the problem, we have 'Income' as the Target variable. we see that we have only two values which are to be predicted, either the income is greater than 50K, which is Yes, or the income is less than or equal to 50K, which is No. We will label encode the target variable.","1e58ae87":"As we can see that we now have a balanced dataset, so we can model ahead with the model building part.\n![](https:\/\/kiarofoods.com\/wp-content\/uploads\/2019\/10\/line_break.png)\n# Model Building:\nStarting with the spliting of the training and testing data. For that, we check to see what is the best random state.","38c924ce":"We can see that, we have encoded the values of the target variable, and converted it into int data-type. This problem is a classification problem with 'Income' as the target variable. Making a copy of the dataset to work ahead","ec7fbd86":"![](https:\/\/kiarofoods.com\/wp-content\/uploads\/2019\/10\/line_break.png)\n# Exploratory Data Analysis:\n**Problem Type Identification:** We have the target variable available with us. So, it a supervised machine learning problem. First we try to find out the type of supervised machine learning that we have in this case study by lookin at the target variable","32a3cd89":"We see that for the feature 'Education', we already have the encoded values in feature 'Education_num'. 'Education' will be removed from the dataset.","ffc1674a":"After the hyperparameter tuning, **the best parameters for Random Forest Classifier are 'crietrion' = 'entropy', 'min_samples_split' = 2, 'n_estimators' = 100**. We build the model using these parameters.","68e669f1":"We see that the **majority of people who have income more than 50K a year are from private sector**. Same goes for the people with income less than 50K. But *for the Self Employed sector, the number of people whose income > 50K are more than the number of people whose income < 50K.* Now, moving ahead with replacing the null values and encoding the feature. **We will replace the NaN values in the 'Workclass' feature by the mode of the column, grouping it by the 'Occupation' feature.** We now have 7 unique values in Workclass feature. We can encode these values using the frequency encoding technique.","5c34901d":"As we can see that data is imbalanced. In order **to remove the data imbalance, we use the SMOTETomek class to create synthetic values using KNN algorithm.**","796b7955":"**Outliers are present in the continous columns of the feature**. We will check the z-score of the features and and clip them from the data.","153751be":"WE have a total of 2566 outliers in the data. After removing the outliers, we have 28213 observations left.\n## Correlation:\nChecking the correlation between the features and target variable to see which of them columns are more related to target.","92753760":"**We will drop the rows where the occupation is NaN.**","ef123b60":"**Married people have a higher income as compared to others.** Encoding the feature","8ab1ab67":"## Importing Important libraries","36362788":"As we cannn see that after removing the null values from 'occupation', we are left with 30718 observations.","76d5e18e":"## Model Fitting:\nFitting 7 different models to check which model gives the best accuracy.","3972a3f3":"Best accuracy score is given by Random Forest Classifier model. In order to avoid the bias and overfitting or underfitting, we cross validate the models and check the mean accuracy score of them.\n## Cross Validation:\nCross validating the m,odels to see if they are underfitting or overfitting and to prevent bias. We will compare the mean accuracy scores of the model.","ffd455c8":"*'Workclass', 'Marital_status', 'Occupation', 'Relationship', 'Race', 'Sex', 'Native_country' are the categorical variables in the data*. Proper encoding or conversion of these variables is necessary for the feature engineering. We will look at these attributes and convert them one by one.\n\n**'Workclass':** Starting off with the work class, we look the number of unique values and value counts for those values","736bcecd":"**'Occupation':** Similar to 'Workclass', we will look at the unique values and value counts in the 'Occupation' feature.","110a8c1d":"Now, we have 8 unique values in this feature. But, we see that **there are some values where we have '?' in the column. This values can be replaced with NaN values.**","ad14a694":"After the model evaluation , we get the **precision and recall for both the target variable as 0.92 and 0.91**. The **f1- score of the model is 0.92**. The ROC curve gave us **the AUC score which is 0.98**. Model evaluation gives the results that ***the prediction is very accurate.***\n![](https:\/\/miro.medium.com\/max\/2400\/1*IH10jlQEJ7GW1_oq8s7WPw.png)\n# Serialisation:\nNow we save the Random Forest Classifier Model as an object using joblib.","397d13bb":"**'Marital_status':** Looking at the iincome of people according to their marital status.","e7298a01":"![](https:\/\/kiarofoods.com\/wp-content\/uploads\/2019\/10\/line_break.png)\n# Model Evaluation:\nWe have build the model after the cross validation and hyper parameter tuning. It is now time to evaluate the model using the classification report, confusion matrix and ROC curve.","84265c90":"'Capital_gain', 'Education_num', 'Marital_status_enc', 'Relationship_enc' are most correlated to the Income of the observations.\n## Scaling:\nAs we see that the values of attributes in the dataset vary largely, so it is important to scale the data. Using the Min-Max scaler in order to bring normalisation in the data.","c2c2da46":"**Majority of people with higher income belong to the USA**. We also have more number of people from USA then any other country combined in this dataset. Encoding this feature using one hot encoding.","51043862":"From the heatmap, we see that the dataset consists of no null values. But for some features, we have '?' as the values present. **'?' will be considered as null values.** We move ahead with the feature engineering part. Checking the datatypes of the columns","f8ffd502":"**'Native_country':** We are checking for the salary on people in USA and outside USA, so , **we will convert all the values where country is not USA to 'non-usa'.** This way, we can encode the values by one-hot encoding without increasing the curse of dimensionality.","c8ce228b":"#### Loading the data onto notebook.","3146e77c":"Similarly, **for 'Race' and 'Relationship'**","12cb1d03":"Checking to see that is there any Null values present in the data that we have. Handling the null values will be the first thing that we need to do. Then, we have a look at the data-types of the other features and the value counts and unique values in those features."}}