{"cell_type":{"be88cdcf":"code","fabd9727":"code","957dd7d9":"code","4a6af539":"code","ae2c4e17":"code","86e805e3":"code","fc48adbf":"code","060faa07":"code","bbb72759":"code","b028374d":"code","de09bf3a":"code","ffb7529d":"code","54a74da3":"code","7b2b8aff":"code","94027de5":"code","dc56799d":"code","d3810952":"code","7911818b":"code","415b7a83":"code","31d77bfe":"code","0dcc7101":"code","f355f7eb":"code","754e6947":"code","915864b0":"code","14c502b2":"code","d3050508":"code","11dcf16c":"code","4b69cdb2":"code","1a9eff54":"code","52572221":"code","86c57a2f":"code","348c210b":"code","41c11f58":"code","a9063c5b":"code","84576e24":"code","3c3fe6e9":"code","cc258ca8":"code","bcc0f23e":"code","7208cebf":"code","01dbdd35":"code","47bef6ea":"code","19a39725":"code","d63dad0e":"code","cb6de83c":"markdown","382142b3":"markdown","ac4f0718":"markdown","53f6c857":"markdown","0d6ebe9e":"markdown","3fe9f8bb":"markdown","7f5c7e66":"markdown","7e878ee9":"markdown","2a0c9e3b":"markdown","49d95990":"markdown","59180297":"markdown","bbf55b75":"markdown","2c7abd6b":"markdown","b68e3e87":"markdown"},"source":{"be88cdcf":"#load fundamental libraary\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #visualization\nimport seaborn as sns #visualization\n\nimport warnings\nwarnings.filterwarnings('ignore')","fabd9727":"#load data\ndataset = pd.read_csv(\"..\/input\/listings.csv\")\ndataset2 = pd.read_csv('..\/input\/listings_summary.csv')","957dd7d9":"dataset.head(1)","4a6af539":"#get significant features for according to me from this dataset and show 1 example\nimportant_features = [\"room_type\",\"reviews_per_month\",\"number_of_reviews\",\"minimum_nights\",\"calculated_host_listings_count\",\"availability_365\",\"longitude\",\"latitude\",\"price\"]\ndataset = dataset[important_features]\ndataset.head(1)","ae2c4e17":"dataset.info()","86e805e3":"#look at nulls\nfig, ax = plt.subplots(figsize = (9, 5))\nsns.heatmap(dataset.isnull(), cmap = \"cubehelix_r\", yticklabels='')\nplt.show()","fc48adbf":"del dataset[\"reviews_per_month\"]","060faa07":"dataset2.head(1)","bbb72759":"dataset2.info()","b028374d":"#look at nulls\nfig, ax = plt.subplots(figsize = (20, 5))\nsns.heatmap(dataset2.isna(), cmap = \"cubehelix_r\", yticklabels='')\nplt.show()","de09bf3a":"#ignore features that contain more than %30 \ncols = dataset2.columns.values\nlenght = dataset2.shape[0]\ncol_list = [] \nfor i in cols:\n    x = dataset2[i]\n    rate = round(x.isna().sum()\/lenght,3)\n    if rate < 0.30:\n        col_list.append(i)\n        print(i,\"contain :%\",100*rate ,\" ok :)\")\n    else:\n        print(i,\"contain :%\",rate*100 ,\" fail !!\")","ffb7529d":"#selectable features\nprint(\"Selectable feature count : \",len(col_list))","54a74da3":"#get important feature\nget_features = [\"cancellation_policy\",\"amenities\",\"availability_365\",\"requires_license\",\"instant_bookable\",\n                \"guests_included\",\"extra_people\",\"review_scores_rating\",\"require_guest_phone_verification\",\n               \"bathrooms\",\"bedrooms\",\"beds\",\"bed_type\",\"accommodates\",\"host_total_listings_count\",\"host_has_profile_pic\",\n               \"host_identity_verified\",\"is_location_exact\",\"property_type\",\"host_is_superhost\",\"room_type\",\"maximum_nights\",\"minimum_nights\"]\ndataset2 = dataset2[get_features]\ndataset2.head()","7b2b8aff":"dataset2.info()","94027de5":"_dataset = dataset.copy()","dc56799d":"#create distance feature from latitude and longtitude \nfrom geopy.distance import great_circle\ndef distance_from_berlin(lat, lon):\n    berlin_centre = (52.5027778, 13.404166666666667)\n    record = (lat, lon)\n    return great_circle(berlin_centre, record).km\n\n#add distanse dataset\n_dataset['distance'] = _dataset.apply(lambda x: distance_from_berlin(x.latitude, x.longitude), axis=1)\n\ndel _dataset['latitude']\ndel _dataset['longitude']\n_dataset.head(1)","d3810952":"#nominal_categorical\nprint(_dataset.room_type.unique())\nx = _dataset[[\"room_type\"]]\nx.room_type = pd.Categorical(x['room_type'])\ndel _dataset['room_type']\ndummies_room_type = pd.get_dummies(x, prefix = 'room_type')\n_dataset = pd.concat([_dataset,dummies_room_type], axis=1)\n_dataset.head(3)","7911818b":"_dataset2 = dataset2.copy()","415b7a83":"_dataset2.head()","31d77bfe":"#del same features\ncol_list2 = list(_dataset.columns.values)\ncol_list2.append(\"room_type\")\nfor i in col_list2:\n    if i in _dataset2.columns.values:\n        print(\"deleted \",i,\" feature!\")\n        del _dataset2[i]","0dcc7101":"#convert string true and false to numeric\ndef object2bool(x):\n    if x==\"t\" or x==\"T\":\n        return 1.0\n    elif x==\"f\" or x==\"F\":\n        return 0.0\n    else:\n        return None\n\nboolean_features = [\"require_guest_phone_verification\",\"host_is_superhost\",\"host_has_profile_pic\",\n                    \"host_identity_verified\",\"is_location_exact\",\"requires_license\",\"instant_bookable\"]\n\nfor i in boolean_features:\n    _dataset2[i] = _dataset2[i].map(object2bool)\n\n_dataset2.head()","f355f7eb":"#nominal_categorical bed_type and property_type\nfor i in [\"bed_type\",\"property_type\",\"cancellation_policy\"]:\n    x = _dataset2[[i]]\n    x.room_type = pd.Categorical(x[i])\n    del _dataset2[i]\n    dummies = pd.get_dummies(x, prefix = i)\n    _dataset2 = pd.concat([_dataset2,dummies], axis=1)\n\n_dataset2.head(3)","754e6947":"#amenities count\ndef amenities_counter(x):\n    return len(x.split(\",\"))\n\n_dataset2.amenities = _dataset2.amenities.map(amenities_counter)\n_dataset2.head(3)","915864b0":"#clean $ sign\ndef clean_price(x):\n    return float(x.replace(\"$\",\"\"))\n\n_dataset2.extra_people = _dataset2.extra_people.map(clean_price)\n_dataset2.head(3)","14c502b2":"_dataset2 = _dataset2.astype(float)\n_dataset = _dataset.astype(float)","d3050508":"#get features that contain nulls\nnull_cols = []\nfor i in _dataset2.columns.values:\n    if _dataset2[i].isna().sum()>0:\n        null_cols.append(i)\n\nprint(\"Features that contain nulls;  \\n\\n\",null_cols)","11dcf16c":"#fill nulls with median\nfor i in null_cols:\n    median = _dataset2[i].median()\n    _dataset2[i] = _dataset2[i].fillna(median)","4b69cdb2":"print(\"Dataset2 null count :\",_dataset2.isna().any().sum())\nprint(\"Dataset null count :\",_dataset.isna().any().sum())","1a9eff54":"#concat\ndata = pd.concat([_dataset,_dataset2],axis=1)\ndata.shape","52572221":"data.describe()","86c57a2f":"#outlier detection with z score\ndef detect_outlier(data_1):\n    feature_outliers=[]\n    threshold=7\n    mean_1 = np.mean(data_1)\n    std_1 =np.std(data_1)\n    \n    counter=0\n    for y in data_1:\n        z_score= (y - mean_1)\/std_1 \n        if np.abs(z_score) > threshold:\n            feature_outliers.append(counter)\n        counter += 1\n    return feature_outliers","348c210b":"outliers = np.array([])\nprint(\"--Feature and Outlier Counts--\\n\")\nfor i in data.columns:\n    f_out = detect_outlier(data[i])\n    outliers = np.concatenate((outliers,np.asarray(f_out)))\n    print(i ,\" outlier count :\",len(f_out))\n                              \noutliers = np.unique(outliers,0)\nprint(\"Total Unique Outlier Index Count:\",len(outliers))","41c11f58":"clean_data = data.drop(outliers,axis=0)\nclean_data.describe()","a9063c5b":"f, ax = plt.subplots( figsize=(35,35) )\nsns.heatmap(data.corr(),annot=True,linewidths=1)\nplt.show()","84576e24":"#split data test and train\nfrom sklearn.model_selection import train_test_split\n\nX = clean_data.drop([\"price\"],axis=1)\ny = clean_data[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state=42)","3c3fe6e9":"# normalize data\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range=(0, 1))\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)","cc258ca8":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.metrics import mean_squared_error\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks.callbacks import ModelCheckpoint\n\n#set parameters\nbatch_size = 32\nepochs = 25\n\nmodel = Sequential()\nmodel.add(Dense(512,input_shape=(data.shape[1]-1,), activation= 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation= 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation= 'relu'))\nmodel.add(Dense(1, activation= 'relu'))\n\nfilepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n\nmodel.compile(loss = mean_squared_error,\n              optimizer = Adam(),\n              metrics=['mean_squared_error'])\n\nhist = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test),callbacks=callbacks_list)","bcc0f23e":"#plot training history\nfigsize=(10,5)\nax,_ = plt.subplots(figsize=figsize)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('modelloss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'],loc='upperleft')\nplt.show()","7208cebf":"ls","01dbdd35":"import os\nimport copy\n\nsaved_loss_file = None\nloss_min = 100000\n# r=root, d=directories, f = files\nfor r, d, f in os.walk('.\/'):\n    for file in f:\n        if '.hdf5' in file:\n            filename = os.path.join(r, file)\n            loss = int(filename.split('-')[3].split('.')[0])\n            if loss < loss_min:\n                loss_min = loss\n                saved_loss_file = filename\n                \nprint(\"Saved min loss : \",loss_min,\"\\nFile :\",saved_loss_file)","47bef6ea":"#load model\nfrom keras.models import load_model\nbest_model = load_model(saved_loss_file)","19a39725":"#prediction\npred_mlp = best_model.predict(X_test)","d63dad0e":"#evaluate\nprint('\\n# Model Evaluate')\nresults = best_model.evaluate(X_test, y_test)\nprint('Test Mse Loss:', results[0])\n\nRMSE = np.sqrt(results[0])\nprint(\"RMSE:\",RMSE)\n","cb6de83c":"Thanks for read this kernal :) \n\nIf you have suggestion for me, please write comment. I want to improve myself.\n\nSee You :)","382142b3":"<h2>Working on listings_summary dataset<\/h2> ","ac4f0718":"* **We should fill nan values. I will fill nan values with median.**","53f6c857":"<h3>Working on listings.csv dataset<\/h3>","0d6ebe9e":"* **Nice correalation matrix. There is no too much coraleted feature. **","3fe9f8bb":"\nNow, There is no null data in listining.csv. It is good news for us :)\n\nLet's take a look dataset2.","7f5c7e66":"* **There are too many null values in some feature. We can ignore feature that contains too many nulls**","7e878ee9":"<h4>What We learn from info ?<\/h4>\n\n* We have 22552 sample. \n\nFeature contains null;\n\n <p>**#####** => Contains too much Nan values<\/p>\n <p>**#**     => Contains too few Nan values<\/p>\n\n* review_scores_rating #####\n* bathrooms #\n* bedrooms #\n* beds #\n* host_has_profile_pic #\n* host_identity_verified #\n* host_total_listings_count #\n\n\n\n\n* **We need handle this feature. If feature contains too low nan values, I fill it with majority values of feature. Else I  will using diffirent teqniques.**\n\n* **We can create new features from exist features. Example; We can create distance from latitude and longtitude. This feature response this question. \"How far is Home\/Flat to Berlin ?\"** \n\nLet's Start!!","2a0c9e3b":"<h2> Welcome <\/h2>\n\nI try to make prediction price, data cleaning and feature engineering in this kenel. if you catch my error, please warn me :)\n\n**Prediction Result;**\n\n**Root Mean Squered Error : 28 \u00b1 1** \n\n\nIf you like it and want to support, please vote. This will increase my motivation :)\n","49d95990":"<h3>OK :) There is no Null data and all feature transformed to numeric.<\/h3>\n\nNow, We can concat dataset and dataset2.","59180297":"<h2>Clean Outlier with Z-score<\/h2>","bbf55b75":"Now, We oveview datasets and get significant features according to us. ","2c7abd6b":"<h2>Modeling<\/h2>","b68e3e87":"* **There are a lot of nan values in reviews_per_month. It is more meaningful than number_of_reviews a bit but There aren't a lot of diffirence. I will delete reviews_per_month.**"}}