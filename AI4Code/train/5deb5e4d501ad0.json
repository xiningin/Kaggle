{"cell_type":{"d86d9153":"code","fc2113f3":"code","40faf5ba":"code","a5ae0cc4":"code","c7215e0e":"code","18ce0827":"code","ca4653f6":"markdown","73a23931":"markdown","01f30133":"markdown","0634c92f":"markdown","774b1662":"markdown"},"source":{"d86d9153":"import matplotlib.pyplot as plt\nimg = plt.imread('..\/input\/kaggle-images\/matching.png')\nplt.figure(figsize=(15, 5))\nplt.imshow(img)\nplt.axis('off')\nplt.show()","fc2113f3":"import re\nimport gc\nimport os\nimport json\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\ntqdm.pandas()","40faf5ba":"test_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\n\ndef num_words(x):\n    try:\n        return len(x.split())\n    except:\n        return 0\n    \ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n\ndef read_json_pub(filename, train_data_path=test_files_path, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    \n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","a5ae0cc4":"df1 = pd.read_csv('..\/input\/dataset-search-metadata-for-datasets\/dataset_metadata_2020_10_16.csv')[['name']]\ndf1['length'] = df1.name.progress_apply(num_words)\n\ngc.collect()\n\ndf2 = pd.read_csv('..\/input\/dataset-search-metadata-for-datasets\/dataset_metadata_2020_08_17.csv\/dataset_metadata_2020_08_17.csv')[['name']]\ndf2['length'] = df2.name.progress_apply(num_words)\n\ngc.collect()\n\ndf = pd.concat([df1, df2]).reset_index(drop=True)\n\ndel df1, df2\ngc.collect()","c7215e0e":"dict_of_set_of_datasets = {}\n\nfor i in range(1, 25):\n    dict_of_set_of_datasets[i] = set(df[df.length==i].name.values)","18ce0827":"match_predictions = pd.DataFrame(index=[i.replace('.json', '') for i in os.listdir('..\/input\/coleridgeinitiative-show-us-the-data\/train')],\n                                 columns = ['PredictionString'])\n\nfor id_, row in tqdm(match_predictions.iterrows(), total=len(match_predictions)):\n    \n    predictions = []\n    \n    large_string = str(read_json_pub(id_, test_files_path))\n    text_list = clean_text(large_string).split()\n    \n    for indx, words in enumerate(zip(*[text_list[s:] + ['random_text']*s for s in range(26)])):\n        for j in range(1, 25):\n            if ' '.join(words[:j]) in dict_of_set_of_datasets[j]:\n                predictions.append(' '.join(words[:j]))\n                break\n                \n    predictions = '|'.join(predictions)\n    if predictions!='':\n        match_predictions.loc[id_,'PredictionString'] = predictions\n\nmatch_predictions.to_csv('submission.csv')","ca4653f6":"## Sets based on number of words in the dataset name","73a23931":"## Efficient Keyword Matching\n'in' method can help in seaching from a few thounsand dataset names, but if you are having a dataset name list consisting of Millions of names(like [this](https:\/\/www.kaggle.com\/googleai\/dataset-search-metadata-for-datasets)), then it is not going to help.<br><br>\nThis kernel does that by - \n+ Splits paragraph into words\n+ Creates sets of dataset names with different word count\n+ Then picks \u2018n\u2019 consecutive words, joins them, and searches in the set<br>","01f30133":"## Get all the dataset names and their word count in a dataframe","0634c92f":"## Utility functions","774b1662":"# Keyword matching"}}