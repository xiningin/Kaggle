{"cell_type":{"3b2a99a3":"code","5c40c5ce":"code","920fcd51":"code","30389862":"code","98b4fa10":"code","3c3a37b2":"code","8137b58d":"code","e35f4166":"code","346119f9":"code","3c949643":"code","ad9eff5a":"code","e6772218":"code","4cc6de9b":"code","6bf9bc07":"code","c763d7ae":"code","44ce9967":"code","b951d179":"code","4b720b3f":"code","ccb34f3f":"code","5a01ce53":"code","21ac5bec":"code","012c3720":"code","41231d02":"code","f5adc390":"code","3f9d8976":"code","fc772f0f":"code","3bb56afb":"code","7991b499":"code","ee726ee4":"code","9e81ea85":"code","d7295421":"code","a60b44ae":"code","be540558":"code","bbf8f9fe":"code","506eb634":"code","859bf4cf":"code","278583a8":"code","e6641a25":"code","1db12668":"code","5a970f6a":"code","b6c1ff51":"code","e8c3a399":"code","3aae304b":"code","d6f38863":"code","04204147":"code","11960312":"code","8874dcc5":"code","76736482":"code","6dbc42b1":"code","0891a116":"code","088565fb":"code","c3d2f183":"code","8b17eab8":"code","8444fa7f":"code","a5dcee7e":"code","9a455b7d":"code","165ea0f3":"code","cfc23b30":"code","ee8eecf1":"code","7ba66bde":"code","f66d87de":"code","2667d8f3":"code","affee37e":"markdown","513870b2":"markdown"},"source":{"3b2a99a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c40c5ce":"df = pd.read_csv('\/kaggle\/input\/us-accidents\/US_Accidents_June20.csv')\ndf.head()","920fcd51":"import matplotlib.pyplot as plt\nimport seaborn as sns","30389862":"plt.figure(figsize = (23,9))\nsns.heatmap(df.corr(), annot = True )","98b4fa10":"df.drop(['Wind_Chill(F)', 'End_Lat', 'End_Lng'], axis = 1, inplace = True)","3c3a37b2":"df.info()","8137b58d":"df.count()\/3513617  #Lets see the percentage of non-null values for each column","e35f4166":"#It seems like Number is just missing too many values, TMC is also missing a lot but we may be able to feature_engineer it along with other\ndf.drop(['Number', 'ID'], axis = 1, inplace = True) #ID is also useless to us","346119f9":"df['TMC'].value_counts() #TMC doesn't really correlate with anything and is also a classification meaning we can't really replace any values for it","3c949643":"df.dropna(subset = ['TMC'], inplace = True)","ad9eff5a":"df.isnull().sum()","e6772218":"#For Temperature, Humidity, Pressure, Visibility, Wind_speed, and Precipitation we can just get their means\nvalues = {'Temperature(F)': df['Temperature(F)'].mean(), 'Humidity(%)': df['Humidity(%)'].mean(), 'Pressure(in)': df['Pressure(in)'].mean(), 'Visibility(mi)': df['Visibility(mi)'].mean(), 'Wind_Speed(mph)' : df['Wind_Speed(mph)'].mean(), 'Precipitation(in)': df['Precipitation(in)'].mean() }\ndf.fillna(value = values, inplace = True)\ndf.isnull().sum()","4cc6de9b":"#Okay I think we can just drop everything else now\ndf.dropna(inplace = True)\ndf.isnull().sum()","6bf9bc07":"df.info()","c763d7ae":"#All the twilights seem to be pretty much the same thing so I'll drop them\ndf.drop(['Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight'], axis = 1, inplace = True)","44ce9967":"df.info() #Okay so lets start making dummy variables","b951d179":"df['Source'].value_counts()","4b720b3f":"source = pd.get_dummies(df['Source'])\ndf = pd.concat([df.drop('Source', axis = 1), source], axis = 1)","ccb34f3f":"df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors = 'coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'], errors = 'coerce')\n\ndf['Year'] = df['Start_Time'].dt.year\ndf['Month'] = df['Start_Time'].dt.month\ndf['Day'] = df['Start_Time'].dt.day\ndf['Hour'] = df['Start_Time'].dt.hour\n\ndf['Duration'] = round((df['End_Time']- df['Start_Time'])\/np.timedelta64(1,'m'))","5a01ce53":"neg_outliers=df['Duration']<=0\n\ndf[neg_outliers] = np.nan\n\ndf.dropna(subset=['Duration'],axis=0,inplace=True)","21ac5bec":"df.drop(['Start_Time', 'End_Time'], axis = 1, inplace = True)","012c3720":"df.info()","41231d02":"df['Country'].value_counts()","f5adc390":"#Country is useless because this is only happening in the U.S. Also County and City are just too specific for me to use and have way too many categories\ndf.drop(['Country', 'County', 'City'], axis = 1, inplace = True)","3f9d8976":"#Zipcode, Timezone, Airport_Code, Weather_Timestamp are also pretty useless to me\ndf.drop(['Zipcode', 'Timezone', 'Airport_Code', 'Weather_Timestamp'], axis = 1, inplace = True)","fc772f0f":"df.info()","3bb56afb":"#I'm going to use street just so I can see if they were on a highway or not\n\ndef location(street):\n    if 'I-' in street:\n        return 1\n    else:\n        return 0\n\ndf['highway'] = df['Street'].apply(location)\ndf.drop('Street', axis = 1, inplace = True)","7991b499":"df['highway'].head()","ee726ee4":"df.info()","9e81ea85":"#State is just too broad to affect the severity of the accident and the description just has the information in the other variables\ndf.drop(['State', 'Description'], axis = 1, inplace = True)","d7295421":"df['Side'].value_counts()","a60b44ae":"#There seems to be one random value in side so lets get rid of it and then create dummy variables for it\nvalue = df[(df['Side'] != 'R') & (df['Side'] != 'L')].index\ndf.drop(value, inplace = True)\ndf['Side'].value_counts()","be540558":"sides = pd.get_dummies(df['Side'], drop_first = True)\nsides = sides.rename({'R' : 'Side'}, axis = 1)\ndf = pd.concat([df.drop('Side', axis = 1), sides], axis = 1)","bbf8f9fe":"df.info()","506eb634":"df['Wind_Direction'].value_counts() #Way too many directions, lets just split it up into Calm, North, South, East, West, and Variable","859bf4cf":"df['Wind_Direction'] = df['Wind_Direction'].apply(lambda dire: dire[0])\ndf['Wind_Direction'].value_counts()","278583a8":"wind = pd.get_dummies(df['Wind_Direction'], drop_first = True)\ndf = pd.concat([df.drop('Wind_Direction', axis = 1), wind], axis = 1)\ndf.info()","e6641a25":"df['Weather_Condition'].value_counts().head(30) #Rain (and drizzle), Snow, Thunder (and storm), Cloud (and Overcast), Clear (and Fair), haze (and Smoke and fog) ","1db12668":"def weather(kind):\n    if 'Rain' in kind or 'Snow' in kind or 'Storm' in kind or 'Thunder' in kind or 'Drizzle' in kind:\n        return 'Slippery'\n    elif 'Fog' in kind or 'Smoke' in kind or 'Haze' in kind or 'Mist'in kind:\n        return 'Vis_obstruct'\n    else:\n        return 'Fair'\n    \nweather = df['Weather_Condition'].apply(weather)\nweather.value_counts()","5a970f6a":"weather_type = pd.get_dummies(weather, drop_first = True)\nweather_type.head()","b6c1ff51":"df = pd.concat([df.drop('Weather_Condition', axis = 1) , weather_type], axis = 1)\ndf.info()","e8c3a399":"df['Sunrise_Sunset'].value_counts()","3aae304b":"sky = pd.get_dummies(df['Sunrise_Sunset'], drop_first = True)\nsky.head()","d6f38863":"df = pd.concat([df.drop('Sunrise_Sunset', axis = 1), sky], axis = 1)\ndf.info()","04204147":"#Now lets see if we can reduce any of the columns by seeing how correlated they are with each other\nplt.figure(figsize = (26,16))\nsns.heatmap(df.corr(), annot = True)","11960312":"df['Turning_Loop'].value_counts() #Turning_Loop is just all zeroes so it's useless","8874dcc5":"#It also seems like Visibility and Slippery are correlated also MapQuest and MapQuest_Bing\ndf.drop(['Turning_Loop', 'Slippery', 'MapQuest-Bing'], axis = 1, inplace = True)","76736482":"df.info()","6dbc42b1":"from sklearn.model_selection import train_test_split\nX = df.drop('Severity', axis = 1)\ny_rfc = df['Severity']\ny_nn = df['Severity']","0891a116":"X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X,y_rfc, test_size = 0.3, random_state = 101)","088565fb":"#Because this is Multi-Classification, lets start with Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators= 100)\nrfc.fit(X_train_r, y_train_r)","c3d2f183":"from sklearn.metrics import confusion_matrix, classification_report\ny_rfc_pred = rfc.predict(X_test_r)","8b17eab8":"print(confusion_matrix(y_test_r, y_rfc_pred))\nprint('\\n')\nprint(classification_report(y_test_r, y_rfc_pred))","8444fa7f":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder","a5dcee7e":"encoder = LabelEncoder()\nencoder.fit(y_nn)\ny_nn = encoder.transform(y_nn)\ny_nn = to_categorical(y_nn)","9a455b7d":"X_train, X_test, y_train, y_test = train_test_split(X,y_nn, test_size = 0.3, random_state = 101)","165ea0f3":"#Lets try Neural Network\nfrom sklearn.preprocessing import MinMaxScaler\nscaler  = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","cfc23b30":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)","ee8eecf1":"model = Sequential()\n\nmodel.add(Dense(39, activation = 'relu', input_dim = len(df.columns) - 1))\nmodel.add(Dropout(rate = 0.4))\n\nmodel.add(Dense(20 , activation = 'relu'))\nmodel.add(Dropout(rate = 0.4))\n\nmodel.add(Dense(10 , activation = 'relu'))\nmodel.add(Dropout(rate = 0.4))\n\nmodel.add(Dense(4, activation = 'softmax'))\n\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","7ba66bde":"model.fit(X_train, y_train, epochs = 30, callbacks = [early_stop],batch_size = 256, validation_data = (X_test, y_test))","f66d87de":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","2667d8f3":"scores = model.evaluate(X_test, y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","affee37e":"Temperature and Wind_Chill(F) have correlation of 0.99 (should probably just get rid of one cause their basically the same thing). Bump and Traffic_calming have correlation of 0.66 so fairly high. Crossing and Traffic_signal have a correlation of 0.45, no surprise there. Start_Lat has a 100% correlation with End_Lat so I'll get rid of one, same thing with Start_Lng and End_Lng","513870b2":"Now we make changes with the Time. Thank you to Deepak Deepu for helping me out with this one."}}