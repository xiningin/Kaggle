{"cell_type":{"4ab3dc72":"code","c3affb77":"code","194af216":"code","97481131":"code","f9b1f95e":"code","67ba0358":"code","c4d8b69b":"code","e6e0599d":"code","f1319653":"code","24944471":"code","1a9ce710":"code","9581cd98":"code","b52cf8da":"code","1511ec91":"code","9739ea81":"code","d340e9ce":"code","6a3387c5":"code","c50c3b49":"code","9e458325":"code","35636da4":"code","87b10557":"code","1e382f51":"code","027c9102":"code","e350e7d7":"code","4bef0504":"code","8f3ef07d":"code","65eb19cf":"code","11157c0c":"code","c290f3d6":"code","163cdbc5":"code","0c135e3a":"code","14f5fd9c":"code","63176fda":"code","5cc50e64":"code","061de8b6":"code","b1d200d0":"code","aed6da89":"code","88a78710":"code","7b3c628c":"code","a4a943ce":"code","0698170d":"code","99f606a0":"code","8d0ef1bd":"code","ecbe384f":"code","71a9240c":"code","7d89c3a1":"code","1e442ab5":"code","5c2f021d":"code","16cfc18f":"code","b9b4f56b":"code","635d0f98":"code","b980e439":"code","2028b9af":"markdown"},"source":{"4ab3dc72":"import pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn import tree\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n\n\ndata = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndata1 = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n","c3affb77":"data1.head()","194af216":"data.head()","97481131":"data.tail()","f9b1f95e":"data.dtypes","67ba0358":"data.columns","c4d8b69b":"data.iloc[45]","e6e0599d":"data.iloc[60]","f1319653":"data.iloc[400]","24944471":"data.shape","1a9ce710":"data.describe","9581cd98":"data.iloc[0:8].corr()","b52cf8da":"data['Age'].fillna(data['Age'].mean())","1511ec91":"data.iloc[0:80].corr()","9739ea81":"data.iloc[0:405].corr()","d340e9ce":"data.iloc[1:5].corr()","6a3387c5":"data.iloc[2:5].corr()","c50c3b49":"data.iloc[1:110].corr()","9e458325":"data.describe()","35636da4":"data1.count()","87b10557":"data1.describe()","1e382f51":"data.columns","027c9102":"data1.isnull().sum()","e350e7d7":"data.isnull().sum()","4bef0504":"data.columns","8f3ef07d":"data.drop(['Name','Sex','Ticket','Cabin','Embarked','Pclass'],axis='columns',inplace=True)","65eb19cf":"data.shape","11157c0c":"data.columns","c290f3d6":"data.dtypes","163cdbc5":"data.describe","0c135e3a":"data1.columns","14f5fd9c":"# Passengers that survived vs passengers that passed away\nprint(data.Survived.value_counts())\n","63176fda":"# As proportions\nprint(data[\"Survived\"].value_counts(normalize = True))\n","5cc50e64":"# Create the column Child and assign to 'NaN'\ndata[\"Child\"] = float('NaN')\n","061de8b6":"# Assign 1 to passengers under 18, 0 to those 18 or older. Print the new column.\ndata[\"Child\"][data[\"Age\"] < 18] = 1\ndata[\"Child\"][data[\"Age\"] >= 18] = 0\nprint(data[\"Child\"])\n","b1d200d0":"# Print normalized Survival Rates for passengers under 18\nprint(data[\"Survived\"][data[\"Child\"] == 1].value_counts(normalize = True))\n","aed6da89":"# Print normalized Survival Rates for passengers 18 or older\nprint(data[\"Survived\"][data[\"Child\"] == 0].value_counts(normalize = True))\n","88a78710":"# Create a copy of test: data1\ntest = data1.copy()\n","7b3c628c":"# Initialize a Survived column to 0\ntest[\"Survived\"] = 0","a4a943ce":"# Set Survived to 1 if Sex equals \"female\"\ntest[\"Survived\"][test[\"Sex\"] == \"female\"] = 1\nprint(test.Survived)","0698170d":"# Impute the Embarked variable\ntest[\"Embarked\"] = test[\"Embarked\"].fillna(\"S\")\n","99f606a0":"# Convert the Embarked classes to integer form\ntest[\"Embarked\"][test[\"Embarked\"] == \"S\"] = 0\ntest[\"Embarked\"][test[\"Embarked\"] == \"C\"] = 1\ntest[\"Embarked\"][test[\"Embarked\"] == \"Q\"] = 2\n","8d0ef1bd":"# Print the Sex and Embarked columns\nprint(test[\"Sex\"])\nprint(test[\"Embarked\"])","ecbe384f":"# Print the  data to see the available features\nprint(data)","71a9240c":"data.columns","7d89c3a1":"# Create the target and features numpy arrays: target, features_one\ntarget = data[\"Survived\"].values\nfeatures_one = data[[\"PassengerId\",\"SibSp\" , \"Fare\"]].values\n","1e442ab5":"# Print the train data to see the available features\nprint(data)\n\n# Create the target and features numpy arrays: target, features_one\ntarget = data[\"Survived\"].values\nfeatures_one = data[[\"PassengerId\", \"Fare\"]].values\n\n# Fit your first decision tree: my_tree_one\nmy_tree_one = tree.DecisionTreeClassifier()\nmy_tree_one = my_tree_one.fit(features_one, target)\n","5c2f021d":"# Impute the missing value with the median\ntest.Fare[152] = test.Fare.median()\n\n# Extract the features from the test set: Pclass, Sex, Age, and Fare.\ntest_features = test[[\"Pclass\",\"Fare\"]].values\n\n# Make your prediction using the test set and print them.\nmy_prediction = my_tree_one.predict(test_features)\nprint(my_prediction)\n","16cfc18f":"# Create a new array with the added features: features_two\nfeatures_two = data[[\"PassengerId\", \"Fare\", \"SibSp\", \"Parch\"]].values\n\n#Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5 : my_tree_two\nmax_depth = 10\nmin_samples_split = 5\nmy_tree_two = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)\nmy_tree_two = my_tree_two.fit(features_two, target)\n\n#Print the score of the new decison tree\nprint(my_tree_two.score(features_two, target))","b9b4f56b":"# Create train_two with the newly defined feature\ntrain_two = data.copy()\ntrain_two[\"family_size\"] = data[\"SibSp\"] + data[\"Parch\"] + 1\n\n# Create a new feature set and add the new feature\nfeatures_three = train_two[[\"PassengerId\", \"Fare\", \"SibSp\", \"Parch\", \"family_size\"]].values\n\n# Define the tree classifier, then fit the model\nmy_tree_three = tree.DecisionTreeClassifier()\nmy_tree_three = my_tree_three.fit(features_three, target)\n\n# Print the score of this decision tree\nprint(my_tree_three.score(features_three, target))\n","635d0f98":"# Import the `RandomForestClassifier`\nfrom sklearn.ensemble import RandomForestClassifier\n\n# We want the Pclass, Age, Sex, Fare,SibSp, Parch, and Embarked variables\nfeatures_forest = data[[\"PassengerId\", \"Fare\", \"SibSp\", \"Parch\"]].values\n\n# Building and fitting my_forest\nforest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\nmy_forest = forest.fit(features_forest, target)\n\n# Print the score of the fitted random forest\nprint(my_forest.score(features_forest, target))\n\n# Compute predictions on our test set features then print the length of the prediction vector\ntest_features = test[[\"PassengerId\", \"Fare\", \"SibSp\", \"Parch\"]].values\npred_forest = my_forest.predict(test_features)\nprint(len(pred_forest))\n","b980e439":"#Request and print the `.feature_importances_` attribute\nprint(my_tree_two.feature_importances_)\nprint(my_forest.feature_importances_)\n\n#Compute and print the mean accuracy score for both models\nprint(my_tree_two.score(features_two, target))\nprint(my_forest.score(features_forest, target))","2028b9af":"# Drop the 'name','sex','Cabin' and 'Embarked' columns\n\n# Examine the shape of the DataFrame (again)"}}