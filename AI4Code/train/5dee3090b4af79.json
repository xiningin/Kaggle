{"cell_type":{"2b7b9aaa":"code","060dcba3":"code","e74234df":"code","8b465a45":"code","266e3cc5":"code","dfc606cc":"code","bbf708c4":"code","cee8f4a0":"code","1687a552":"code","5be408bb":"code","28c96885":"code","a72d2606":"code","62bfa731":"code","bfa71f97":"code","113b5891":"code","14b3b1c4":"code","d4df6706":"code","30ffd877":"code","808d9081":"code","69b5c21a":"code","09337556":"code","e6075528":"code","b9bc6895":"code","94fbde13":"code","4a31e022":"code","15b30f8a":"code","4e746873":"code","120ad889":"code","136dad41":"code","58f7a167":"code","781133f1":"code","d93cd4a7":"code","cbbcdfd5":"code","cf4ea427":"code","09ee44b9":"code","b9eeaf24":"code","94152764":"code","158e60fd":"code","b6255251":"code","9cd0500f":"code","4d086ad9":"code","ae40cf41":"code","08b9045f":"code","c642679b":"code","0b3ca512":"code","e385f198":"code","40b258b1":"code","63ffde07":"code","8cc6a03d":"code","7020ef05":"code","34004816":"code","4abbf4c8":"code","b1d47172":"code","b54d4d6f":"code","f3d4ef96":"code","1bdd644f":"code","78d3fded":"code","14a26bce":"code","2dd57f90":"code","2589db3c":"code","c509ef94":"markdown","e7260285":"markdown","216d02b6":"markdown","06b01434":"markdown","8cf652f6":"markdown","595dd0da":"markdown","6c282e79":"markdown","acc280f9":"markdown","ff61f854":"markdown","0e576a1b":"markdown","ed54a46c":"markdown","91ef9aa6":"markdown","28a54614":"markdown","4e139f82":"markdown","c3ea89ab":"markdown","7c49fad9":"markdown","a75a7681":"markdown","e191a81d":"markdown","6589789a":"markdown","534370c9":"markdown","6392edf1":"markdown","2a33ad92":"markdown","fb8b2b25":"markdown","117628f8":"markdown","629175dc":"markdown","dd5b3b88":"markdown","cd75b5d3":"markdown","f360e76b":"markdown","a9651448":"markdown","b71463a0":"markdown","d1fff285":"markdown","94e704c7":"markdown","f507f0d1":"markdown","bd42575c":"markdown","a510a958":"markdown","94c0abcc":"markdown","c6f9bcfe":"markdown","57fcd24d":"markdown","84380123":"markdown","c51ef238":"markdown","d5ef20dd":"markdown"},"source":{"2b7b9aaa":"!jupyter nbextension enable --py widgetsnbextension","060dcba3":"import os\nfrom os import listdir\nfrom os.path import isfile, join\n\nfrom tqdm.auto import tqdm\nimport joblib\nimport gc\nimport time\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\nimport librosa\nimport librosa.display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.utils import Progbar\n\nfrom IPython.core.display import display, HTML\nimport IPython.display as ipd\nfrom functools import partial\n\nfrom imblearn.over_sampling import RandomOverSampler\n\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\nnp.set_printoptions(threshold=100000)","e74234df":"librosa.__version__","8b465a45":"tf.__version__","266e3cc5":"os.getcwd()","dfc606cc":"ON_KAGGLE = True\nMODEL_FOLDER_NAME = 'resnet50\/'","bbf708c4":"# train folder\nTRAIN_FOLDER = \"..\/input\/birdsong-recognition\/train_audio\/\"\n\nif (ON_KAGGLE): # if on kaggle\n    MODEL_PATH = '..\/input\/birdcall-models\/'  # load model from memory\n    metric_log = joblib.load(open(MODEL_PATH + \"metric_log.pkl\", \"rb\")) # load metric_log from memory\n    NOCALL_TRAIN_PATH = \"..\/input\/bird-backgrounds\/\" # nocall audio\nelse: # if not on kaggle, pick up from last epoch\n    MODEL_PATH = '\/Users\/longy\/Documents\/checkpoints\/birdcall\/' + MODEL_FOLDER_NAME\n    if os.path.isfile(MODEL_PATH + \"metric_log.pkl\"):\n        metric_log = joblib.load(open(MODEL_PATH + \"metric_log.pkl\", \"rb\"))\n        start_epoch = len(metric_log[0])\n    NOCALL_TRAIN_PATH = \"..\/input\/nocall\/\"","cee8f4a0":"# set default sampling rate to 32kHz\nDEFAULT_SR = 32000\n\n# frequency range\nF_MIN, F_MAX = 20, DEFAULT_SR\/2\n\n# number of threads\nNUM_THREADS = 8\n\n# number of samples in window\nN_FFT=2048\n\n# step in samples\nHOP_LENGTH = 512\n\n# number of mel bins\nN_MELS=128\n\n# number of decibels below reference value to preserve in log-melspectrogram\nTOP_DB=80\n\n# clip length to train and predict on\nNUM_SECONDS=5\n\n# percentage of training data vs. validation data\nTRAIN_SIZE=0.8","1687a552":"train_info = pd.read_csv(\"..\/input\/birdsong-recognition\/train.csv\").drop_duplicates()\nprint(train_info.shape)\nprint(len(train_info.ebird_code.unique()))\nprint(train_info.columns)\ntrain_info.head()","5be408bb":"# number of birds to make prediction for, adding 'nocall'\nnum_birds = len(train_info.ebird_code.unique())+1\n\n# categorize ebird_code\ntrain_info['ebird_code_cat'] = train_info.ebird_code.astype('category').cat.codes","28c96885":"# create lookup for abbreviated name\nnocall_label='nocall'\nname_lookup = dict(zip(train_info.ebird_code.astype('category').cat.codes, \n                       train_info.ebird_code.astype('category')))\n\nnocall_code = np.max(train_info.ebird_code.astype('category').cat.codes.unique())+1\nname_lookup[nocall_code]=nocall_label\n\n# create reverse lookup for code (from abbreviated name)\ncode_lookup={v:k for k,v in name_lookup.items()}\n\n# create lookup for sampling rate\nsr_lookup = dict(zip(train_info.filename, train_info.sampling_rate))","a72d2606":"# example\nexample = train_info.iloc[0,:]\n\n# filename\nfilename = example.filename\n\n# ebird\nbird = example.ebird_code\n\n# sampling rate\nsr = example.sampling_rate\n\n# duration of clip\nduration = example.duration\n\nprint(\"#ebird code: {}\\n\".format(bird))\nprint(\"#label: {}\\n\".format(example.primary_label))\nprint(\"#secondary labels: {}\\n\".format(example.secondary_labels))\nprint(\"#description:\\n {}\\n\".format(example.description))\nprint(\"#type: {}\\n\".format(example.type))\nprint(\"#saw bird: {}\\n\".format(example.bird_seen))\nprint(\"#sampling rate: {} Hz\\n\".format(sr))\nprint(\"#recording length: {} seconds\\n\".format(duration))","62bfa731":"# shared in discussion: https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/179592\ndef load_clip(path):\n    clip, sr_native = librosa.core.audio.__audioread_load(path, offset=0.0, duration=None, dtype=np.float32)\n    clip = librosa.to_mono(clip)\n    sr = DEFAULT_SR\n    if sr_native > 0:\n        clip = librosa.resample(clip, sr_native, sr, res_type='kaiser_fast')\n    return clip, sr","bfa71f97":"sound_clip, sr = load_clip(TRAIN_FOLDER + bird + '\/' + filename)","113b5891":"ipd.Audio(TRAIN_FOLDER + bird + '\/' + filename)","14b3b1c4":"melspectrogram = librosa.feature.melspectrogram(sound_clip, sr=sr, fmin=F_MIN, fmax=F_MAX)\nprint(\"In this case, melspectrogram computed {} mel-frequency spectrogram coefficients over {} frames.\".format(melspectrogram.shape[0], melspectrogram.shape[1]))\nmelspectrogram = librosa.power_to_db(melspectrogram).astype(np.float32)\nplt.figure(figsize=(20,3))\nlibrosa.display.specshow(melspectrogram, sr=sr, x_axis='time', y_axis='mel', fmin=F_MIN, fmax=F_MAX)\nplt.colorbar(format='%+2.0f dB')\n\nprint(melspectrogram.shape)\nprint(\"no white noise:\")\nipd.Audio(sound_clip, rate=sr)","d4df6706":"def plot_db_freq(clip):\n    S = librosa.stft(clip)\n    D = librosa.amplitude_to_db(np.abs(S))\n    D_AVG = np.mean(D, axis=1)\n\n    x_ticks_positions = [n for n in range(0, N_FFT \/\/ 2, N_FFT \/\/ 16)]\n    x_ticks_labels = [str(sr \/ N_FFT * n) + 'Hz' for n in x_ticks_positions]\n\n    plt.figure(figsize=(10,5))\n    plt.plot(D_AVG)\n    plt.xticks(x_ticks_positions, x_ticks_labels)\n    plt.xlabel('Frequency')\n    plt.ylabel('dB')\n    plt.show()","30ffd877":"plot_db_freq(sound_clip)\nipd.Audio(sound_clip, rate=sr)","808d9081":"def add_white_noise(clip, wn_rate):\n    return clip + wn_rate * np.random.randn(len(clip)) # randn: standard normal distribution","69b5c21a":"wn_rate = 0.01\nsound_clip_with_wn = add_white_noise(sound_clip, wn_rate)\nplot_db_freq(sound_clip_with_wn)","09337556":"def add_pink_noise(clip, freq_floor,freq_ceil, n_freq):\n    freq = np.linspace(freq_floor,freq_ceil,n_freq)\n    noise = np.zeros(len(clip))\n    for f in freq:\n        amp = 1\/f**1\n        noise = noise + amp*np.sin(2*np.pi*f*clip+np.random.rand(1)*2*np.pi) # rand: uniform distribution\n    return noise","e6075528":"sound_clip_with_pn = add_pink_noise(sound_clip,1,100,30)\nplot_db_freq(sound_clip_with_pn)","b9bc6895":"# get melspectrogram\ndef librosa_get_melspec(sound_clip, sr):\n    melspectrogram = librosa.feature.melspectrogram(sound_clip, \n                                                    n_fft=N_FFT, \n                                                    win_length=N_FFT, \n                                                    center=False, \n                                                    sr=sr, \n                                                    fmin=F_MIN, \n                                                    fmax=F_MAX)\n    melspectrogram = librosa.power_to_db(melspectrogram).astype(np.float32)\n    return melspectrogram\n\n# standardize 2D image, convert to grayscale: https:\/\/www.kaggle.com\/daisukelab\/cnn-2d-basic-solution-powered-by-fast-ai\ndef np_to_grayscale(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    X = standardize(X)\n    X = stack(X)\n    return X\n\ndef standardize(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    mean = mean or X.mean()\n    std = std or X.std()\n    Xstd = (X - mean) \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef stack(X):\n    return np.stack([X,X,X],axis=-1)\n\n# crop 5 seconds\ndef crop_image(img, sr, random=True, num_seconds=NUM_SECONDS, hop_length=HOP_LENGTH):\n    height, width = img.shape\n    duration = width*hop_length\/sr\n    if duration>num_seconds: \n        if random: #randomly crop 5 seconds\n            end_second = np.random.uniform(low=num_seconds,high=duration, size=1)[0]\n            start_second = end_second-num_seconds\n        else: #crop first 5 seconds\n            end_second = num_seconds\n            start_second = end_second-num_seconds\n    else:\n        end_second = duration\n        start_second = 0\n    \n    start_frame = int(np.floor(start_second*sr\/hop_length))\n    end_frame = int(np.round(end_second*sr\/hop_length))\n    \n    return img[:, start_frame:end_frame].astype(np.float32)","94fbde13":"n_col = 3\nn_img = 9\n\nfig, axs = plt.subplots(n_img\/\/n_col,n_col,figsize=(5*n_col,5*(n_img\/\/n_col-1)))\nfor i in range(0,n_img):\n    x = i\/\/n_col\n    y = i-x*n_col\n    ax = axs[x][y]\n    \n    sc = add_white_noise(sound_clip, i*0.005)\n    melspec = librosa_get_melspec(sc, sr)\n    cropped_melspec = crop_image(melspec,sr,random=False)\n    cropped_img = np_to_grayscale(cropped_melspec)\n\n    ax.imshow(cropped_img)\n    ax.set_title(\"White Noise Rate: \" + str(i*0.005))","4a31e022":"n_col = 3\nn_img = 9\n\nfig, axs = plt.subplots(n_img\/\/n_col,n_col,figsize=(5*n_col,5*(n_img\/\/n_col-1)))\nfor i in range(0,n_img):\n    x = i\/\/n_col\n    y = i-x*n_col\n    ax = axs[x][y]\n    \n    sc = add_pink_noise(sound_clip,1,100*(x+1),30*(y+1))\n    melspec = librosa_get_melspec(sc, sr)\n    cropped_melspec = crop_image(melspec,sr,random=False)\n    cropped_img = np_to_grayscale(cropped_melspec)\n\n    ax.imshow(cropped_img)\n    ax.set_title(\"Pink Noise: {}-{} Hz in a total of {} steps\".format(1,100*(x+1),30*(y+1)))","15b30f8a":"nocall_info = [(nocall_code, join(NOCALL_TRAIN_PATH, f)) for f in listdir(NOCALL_TRAIN_PATH) if isfile(join(NOCALL_TRAIN_PATH, f))]\nnocall_info = pd.DataFrame(nocall_info, columns=['ebird_code_cat', 'filepath'])","4e746873":"nocall_audio,sr = load_clip(nocall_info.iloc[0].filepath)\nipd.Audio(nocall_audio,rate=sr)","120ad889":"nocall_melspec = librosa_get_melspec(nocall_audio, sr)\nprint(nocall_melspec.shape)\nplt.figure(figsize=(20,3))\nplt.imshow(np_to_grayscale(nocall_melspec))","136dad41":"train_info['filepath'] = TRAIN_FOLDER+train_info[\"ebird_code\"]+\"\/\"+train_info[\"filename\"]\ntrain_info.ebird_code_cat = train_info.ebird_code_cat.astype('int32')\ntrain_info.filepath = train_info.filepath.astype('string')\ntrain_info[['ebird_code_cat', 'filepath']].head(1)","58f7a167":"nocall_info.ebird_code_cat = nocall_info.ebird_code_cat.astype('int32')\nnocall_info.head(1)","781133f1":"def get_birdcall_audio(filepath, label):\n    audio, sr = load_clip(filepath.numpy())\n    return audio, tf.cast(sr,tf.float32), label\n\nextract = lambda x,y: tf.py_function(get_birdcall_audio,[x,y], [tf.float32,tf.float32,tf.int32])","d93cd4a7":"def crop_and_pad(sound_clip,sr,label):\n    length = tf.size(sound_clip,out_type=tf.int32)\n    length = tf.cast(length,tf.float32)\n    duration = length\/sr\n    \n    if (duration>=NUM_SECONDS):\n        # randomly select end second\n        end_second = tf.random.uniform(shape=[],\n                                       minval=NUM_SECONDS,\n                                       maxval=duration,\n                                       dtype=tf.float32)\n        # transform second to sample\n        cut_max = end_second*sr\n        cut_min = (end_second-NUM_SECONDS)*sr\n\n        # cast to integer\n        cut_min = tf.cast(cut_min, tf.int32)\n        cut_max = tf.cast(cut_max, tf.int32)\n\n        # cut clip\n        sound_clip = sound_clip[cut_min:cut_max]\n    else:\n        zero_padding_len = tf.cast((NUM_SECONDS-duration)*sr, tf.int32)\n        sound_clip = tf.concat([sound_clip,tf.zeros(zero_padding_len)],axis=0)\n    \n    return sound_clip,sr,label","cbbcdfd5":"augment_choices = tf.range(0,3,dtype=tf.float32)\nwr_ceil = 0.05\nfreq_ceil_choices = 100. * tf.range(2,4,dtype=tf.float32) # 200, 300\nn_freq = 30","cf4ea427":"def augment(sound_clip,sr,label):\n    choice = tf.random.shuffle(augment_choices)[0]\n    length = tf.size(sound_clip, out_type=tf.int32)\n    \n    if tf.math.equal(choice,1): # white noise\n        wr_rate = tf.random.uniform([],0,wr_ceil)\n        sound_clip = sound_clip + wr_rate * tf.random.normal([length])\n    elif tf.math.equal(choice,2): # pink noise\n        freq_ceil = tf.random.shuffle(freq_ceil_choices)[0]\n        freqs = tf.linspace(1.,freq_ceil,n_freq)\n\n        i0 = tf.constant(0)\n        s0 = tf.zeros(shape=length)\n        \n        c = lambda i,s: i < n_freq\n        b = lambda i,s: [i+1, \n                         s + 1\/freqs[i] * tf.math.sin(2*np.pi*(1\/freqs[i])*sound_clip+tf.random.uniform(shape=[],minval=0,maxval=1,dtype=tf.float32)*2*np.pi)]\n        \n        _,sound_clip = tf.while_loop(c,b,loop_vars=[i0,s0],shape_invariants=[i0.get_shape(),sound_clip.get_shape()])\n    \n    return sound_clip,sr,label","09ee44b9":"def log10(x):\n    numerator = tf.math.log(x)\n    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n    return tf.divide(numerator, denominator)\n\ndef get_melspec(sound_clip,sr,label):\n    spec = tf.signal.stft(sound_clip,frame_length=N_FFT,frame_step=HOP_LENGTH)\n    spec = tf.abs(spec)\n    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins=N_MELS, \n                                                                        num_spectrogram_bins=N_FFT\/\/2+1, \n                                                                        sample_rate=sr, \n                                                                        lower_edge_hertz=F_MIN, \n                                                                        upper_edge_hertz=F_MAX,\n                                                                        dtype=tf.dtypes.float32)\n    melspec = tf.matmul(spec, linear_to_mel_weight_matrix)\n    log_melspec = 10*log10(tf.transpose(melspec)**2)\n    ref = tf.math.reduce_max(log_melspec)-TOP_DB\n    log_melspec = tf.where(log_melspec<ref,ref,log_melspec)\n    \n    return log_melspec,label","b9eeaf24":"eps=1e-6\ndef to_grayscale(melspec, label):\n    mean = tf.math.reduce_mean(melspec)\n    std = tf.math.reduce_std(melspec)\n    xstd = (melspec-mean)\/(std + eps)\n    norm_min = tf.math.reduce_min(xstd)\n    norm_max = tf.math.reduce_max(xstd)\n    \n    if (norm_max-norm_min>eps):\n        v = 255*(xstd-norm_min)\/(norm_max-norm_min)\n        v = tf.cast(v, tf.uint8)\n    else:\n        v = tf.zeros_like(xstd, dtype=tf.uint8)\n    return tf.stack([v,v,v],axis=2), label","94152764":"birdcall_train, birdcall_val = train_test_split(train_info,\n                                                stratify=train_info.ebird_code_cat, \n                                                train_size=TRAIN_SIZE)\nnocall_train, nocall_val = train_test_split(nocall_info,train_size=TRAIN_SIZE)\n\ntrain, val = (pd.concat([birdcall_train[[\"filepath\", \"ebird_code_cat\"]],nocall_train[[\"filepath\", \"ebird_code_cat\"]]],axis=0), \n              pd.concat([birdcall_val[[\"filepath\", \"ebird_code_cat\"]],nocall_val[[\"filepath\", \"ebird_code_cat\"]]],axis=0))","158e60fd":"def overresample(df):\n    ros = RandomOverSampler()\n    resampled_df, _ = ros.fit_resample(df, df.ebird_code_cat)\n    resampled_df, _ = train_test_split(resampled_df, \n                                       stratify = resampled_df.ebird_code_cat, \n                                       train_size=int(np.mean(df.ebird_code_cat.value_counts().values))\/int(np.max(df.ebird_code_cat.value_counts().values)))\n    return resampled_df ","b6255251":"resampled_train = overresample(train)\nresampled_train.ebird_code_cat.value_counts()","9cd0500f":"def stream_files(df, train=True):\n    if train:\n        ds = (tf.data.Dataset.from_tensor_slices((df.filepath.values,df.ebird_code_cat.values)).\n              map(extract,num_parallel_calls=NUM_THREADS).\n              map(crop_and_pad, num_parallel_calls=NUM_THREADS).\n              map(augment, num_parallel_calls=NUM_THREADS).\n              map(get_melspec, num_parallel_calls=NUM_THREADS))\n    else:\n        ds = (tf.data.Dataset.from_tensor_slices((df.filepath.values,df.ebird_code_cat.values)).\n              map(extract,num_parallel_calls=NUM_THREADS).\n              map(crop_and_pad, num_parallel_calls=NUM_THREADS).\n              map(get_melspec, num_parallel_calls=NUM_THREADS))\n\n    return ds","4d086ad9":"train_files = (stream_files(resampled_train))","ae40cf41":"n_col = 4\nn_img = 16\n\nfig, axs = plt.subplots(n_img\/\/n_col,n_col,figsize=(5*n_col,5*(n_img\/\/n_col-1)))\nfor i, (melspec,label) in enumerate(train_files.take(n_img)):\n    x = i\/\/n_col\n    y = i-x*n_col\n    ax = axs[x][y]\n    ax.imshow(melspec)\n    ax.set_title(name_lookup[label.numpy()])","08b9045f":"for i, (melspec,label) in enumerate(train_files.take(1)):\n    img_height = melspec.numpy().shape[0]\n    img_width = melspec.numpy().shape[1]\n\nprint(\"image width: {}, height: {}\".format(img_width, img_height))","c642679b":"epochs = 100\nbatch_size = 64\nlr=.001\nearly_stopping=20\nthreshold=0.5","0b3ca512":"img_resize = [img_height, img_width]\n\nmodel = tf.keras.Sequential()\nresnet50 = tf.keras.applications.ResNet50(include_top=False, \n                                          pooling='avg', \n                                          weights=None,\n                                          input_shape=(img_height, img_width, 3))\n\nmodel.add(resnet50)\nmodel.add(tf.keras.layers.Dense(num_birds, activation='sigmoid'))","e385f198":"# Adam\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n# cross entropy loss function\nloss_fn = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n\n# metrics\nf1_train = tfa.metrics.F1Score(num_birds, 'micro')\nf1_val = tfa.metrics.F1Score(num_birds, 'micro')\n\n# for logging results\nmetric_name = [\"loss\", \"f1\", \"val-f1\"]\n\nbest_f1_val = None\nbest_epoch = None\n\nstart_epoch = 0\nepoch_num = []\nmetric_log_val_f1 = []\nmetric_log_f1 = []\n\n# compile\nmodel.compile(optimizer=optimizer,loss=loss_fn)\n\n# summary\nmodel.summary()","40b258b1":"@tf.function\ndef train_step(x,y): \n    with tf.GradientTape() as tape:\n        x,y = tf.vectorized_map(lambda a: to_grayscale(a[0],a[1]),(x,y))\n        \n        # predict\n        y_pred = model(x)\n        y_truth = tf.one_hot(y, num_birds)\n\n        # calculate loss\n        loss = loss_fn(y_truth, y_pred)\n\n    # calculate gradients\n    grads = tape.gradient(loss,model.trainable_weights)\n    optimizer.apply_gradients(zip(grads,model.trainable_weights))\n\n    # compute metrics\n    f1_train.update_state(y_truth,y_pred)\n    \n    return loss","63ffde07":"@tf.function\ndef val_step(x,y):\n    x,y = tf.vectorized_map(lambda a: to_grayscale(a[0],a[1]),(x,y))\n    \n    y_pred = model(x)\n    y_truth = tf.one_hot(y, num_birds)\n    \n    f1_val.update_state(y_truth,y_pred)","8cc6a03d":"resampled_train = overresample(train)\nprint(resampled_train.shape)","7020ef05":"val_ds = stream_files(val, train=False).batch(batch_size)","34004816":"steps_per_epoch = int(np.ceil(len(resampled_train)\/batch_size))\nsteps_per_epoch","4abbf4c8":"if (ON_KAGGLE==False):\n    with np.errstate(all='raise'):\n        # for each epoch\n        for epoch in range(start_epoch, epochs):\n            print(\"epoch %d\" % (epoch,))\n\n            # reset states\n            f1_train.reset_states()\n            f1_val.reset_states()\n            \n            # set up progress bar\n            pb_i = Progbar(target=steps_per_epoch,stateful_metrics=metric_name)\n\n            # get balanced data from train, shuffle, batch\n            resampled_train = overresample(train)\n            train_files = stream_files(resampled_train).batch(batch_size).prefetch(2)\n\n            for (x_batch, y_batch) in train_files:\n                # get loss\n                loss = train_step(x_batch, y_batch)\n                # update progress\n                pb_i.add(1, values=[('loss', loss), ('f1', f1_train.result()), ('val-f1', f1_val.result())])\n\n            # validate\n            for (x_batch_val, y_batch_val) in val_ds:\n                val_step(x_batch_val, y_batch_val)\n\n            # update f1 for validation set\n            pb_i.update(steps_per_epoch, \n                        values=[('loss', loss), ('f1', f1_train.result()),('val-f1', f1_val.result())],\n                        finalize=True)\n\n            # log results\n            epoch_num.append(epoch)\n            metric_log_f1.append(f1_train.result().numpy())\n            metric_log_val_f1.append(f1_val.result().numpy())\n\n            # save checkpoint after each epoch\n            model.save_weights(MODEL_PATH + 'model')\n            joblib.dump([epoch_num,metric_log_val_f1,metric_log_f1], open(MODEL_PATH + \"metric_log.pkl\", \"wb\"))\n\n            # early stopping if f1 score hasn't improved on validation set\n            if best_f1_val is None or f1_val.result()>best_f1_val:\n                best_f1_val,best_epoch=f1_val.result(),epoch\n\n            if best_epoch<epoch-early_stopping:\n                model.stop_training=True\n                print(\"training stopped early at epoch %d\" % epoch)\n                break\nelse:\n    model.load_weights(MODEL_PATH + 'model')\n    metric_log_val_f1 = metric_log[1]\n    metric_log_f1 = metric_log[2]","b1d47172":"plt.plot(metric_log_val_f1, label='validation f1')\nplt.plot(metric_log_f1, label='training f1')\nplt.xlabel('Epoch')\nplt.ylabel('F1 Score')\nplt.legend(loc='lower right')","b54d4d6f":"BASE_TEST_DIR = '..\/input\/birdsong-recognition' if os.path.exists('..\/input\/birdsong-recognition\/test_audio') else '..\/input\/birdcall-check'\nTEST_FOLDER = f'{BASE_TEST_DIR}\/test_audio\/'\nTEST_FOLDER","f3d4ef96":"df_test = pd.read_csv(f'{BASE_TEST_DIR}\/test.csv')\ndf_test[\"filepath\"] = TEST_FOLDER + df_test.audio_id + '.mp3'\ndf_test.head()","1bdd644f":"def postprocess(y_prob, top = 3):\n    y_candidate = tf.where(tf.where(y_prob > threshold, 1.0, 0.0) * y_prob > 0)\n    y_top = tf.argsort(y_candidate, direction='DESCENDING')\n    if len(y_top)>0: return [name_lookup[code.numpy()] for code in y_top[0][:top]]\n    return []","78d3fded":"def make_prediction(audio, site):\n    sound_clip,sr=audio\n    \n    if (site=='site_1' or site=='site_2'):\n        melspec = librosa_get_melspec(sound_clip, sr=sr)\n        x = tf.expand_dims(np_to_grayscale(melspec).astype(np.uint8), axis=0)\n        y_prob = model(x)\n        y = postprocess(y_prob)\n    else:\n        duration = librosa.get_duration(sound_clip,sr)\n        if duration<NUM_SECONDS:\n            padded_clip = np.concatenate([sound_clip, np.zeros(int(sr*(NUM_SECONDS-duration)))])\n            melspec = librosa_get_melspec(padded_clip, sr=sr)\n            x = tf.expand_dims(np_to_grayscale(melspec).astype(np.uint8), axis=0)\n            y_prob = model(x)\n            y = postprocess(y_prob)\n        else:\n            start_second, end_second = 0, NUM_SECONDS\n            y = []\n            \n            # predict for each 5 seconds\n            while end_second<=duration:\n                clip = sound_clip[start_second*sr:end_second*sr]\n                melspec = librosa_get_melspec(clip, sr=sr)\n                x = tf.expand_dims(np_to_grayscale(melspec).astype(np.uint8), axis=0)\n                clip_prob = model(x)\n                clip_y = postprocess(clip_prob)\n                y.extend(clip_y)\n                start_second += NUM_SECONDS\n                end_second += NUM_SECONDS\n                \n            # predict for remaining time: at least 1 second\n            if end_second-duration>=1:\n                last_clip = np.concatenate([sound_clip[start_second*sr:], np.zeros(int(sr*(end_second-duration)))])\n                melspec = librosa_get_melspec(last_clip, sr=sr)\n                x = tf.expand_dims(np_to_grayscale(melspec).astype(np.uint8), axis=0)\n                clip_prob = model(x)\n                clip_y = postprocess(clip_prob)\n                y.extend(clip_y)\n    return y","14a26bce":"# remove redundancy from memory\ndel train_info, birdcall_train, birdcall_val, nocall_info, nocall_train, nocall_val, resampled_train\ngc.collect()","2dd57f90":"cached_audio = dict()\nstart = time.time()\n\npreds = []\nfor i in tqdm(df_test.index):\n    row = df_test.iloc[i]\n    start_time = row.seconds-NUM_SECONDS\n    row_id, site, filepath = row.row_id, row.site, row.filepath\n    \n    if (filepath in cached_audio):\n        loaded_clip, sr = cached_audio[filepath]\n    else:\n        loaded_clip, sr = load_clip(filepath)\n        cached_audio[filepath] = (loaded_clip, sr)\n        \n    if site=='site_1' or site=='site_2':\n        audio = loaded_clip[int(start_time*sr):int((start_time+NUM_SECONDS)*sr)], sr\n    else:\n        audio = loaded_clip, sr\n        \n    pred = make_prediction(audio, site)\n    pred = ' '.join([bird for bird in np.unique(pred) if bird!=nocall_label])\n    if (len(pred)==0): pred=nocall_label\n    preds.append([row_id, pred])\n    \nprint(\"prediction finished in %d seconds\" % ((time.time() - start)))\npreds = pd.DataFrame(preds, columns=['row_id', 'birds'])\npreds.head()","2589db3c":"preds.to_csv('submission.csv', index=False)","c509ef94":"### Show Timbre\nTimbre is the quality of sound that distinguishes the tone of different instruments and voices even if the sounds have the same pitch and loudness.","e7260285":"#### white noise","216d02b6":"# Explore Training Data","06b01434":"# Submission to evaluate on test data","8cf652f6":"## Load","595dd0da":"### add pink noise","6c282e79":"### validation step","acc280f9":"5. create training and validation","ff61f854":"Definition: Plot x=Frequency in Hz and y=Decibel for the entire clip. Useful to show the effect of adding noise to clip.","0e576a1b":"## Transform","ed54a46c":"### train step","91ef9aa6":"### compile model","28a54614":"We can take a peek at the mel-spectrograms from training data:","4e139f82":"1. randomly crop 5 seconds from clip","c3ea89ab":"### set up params","7c49fad9":"Definition: short-time-fourier-transform (stft) in each window to represent frequencies in mel-scale (such that equal distances in pitch sounded equally distant to the listener)","a75a7681":"4. standardize and stack to rgb(#,#,#)","e191a81d":"- choice 1: no augmentation\n- choice 2: white noise up to 0.5\n- choice 3: pink noise (200 or 300),30","6589789a":"#### get image width and height","534370c9":"**mel-spectrogram**","6392edf1":"### add white noise","2a33ad92":"# Get nocall data","fb8b2b25":"### experiment with different augmentation parameters","117628f8":"# Install and Load Packages","629175dc":"#### Pink Noise","dd5b3b88":"## Extract","cd75b5d3":"### Pull things together","f360e76b":"### Load sound clip","a9651448":"## create code for each bird, add code for nocall","b71463a0":"### Plot F1 Score","d1fff285":"## Examine one audio file","94e704c7":"#### Decibel-Frequency chart","f507f0d1":"2. augment data by adding white noise or pink noise","bd42575c":"resnet50","a510a958":"3. get mel-spectrogram","94c0abcc":"# Define global variables","c6f9bcfe":"## create lookup between bird name and bird code","57fcd24d":"# Build Pipeline","84380123":"#### useful functions","c51ef238":"balance dataset by oversampling on the minority classes","d5ef20dd":"### start training"}}