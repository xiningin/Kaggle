{"cell_type":{"ef381246":"code","636077bb":"code","23e77840":"code","064462ba":"code","853915fd":"code","59f1bf9f":"code","8f46acde":"code","baa92ec7":"code","918b855f":"code","9c0758c3":"code","dd4cdb4c":"code","fb1a263d":"code","05279a87":"code","cdf848b2":"code","f03e3fc8":"code","0def8df6":"code","afce637d":"code","f811d995":"code","6c6c3ac6":"code","b82f3788":"code","5fe6bfad":"code","31aedde9":"code","7aa90131":"code","2de3b14b":"code","840c0a3e":"code","e441bd2d":"code","5d7059f5":"code","6efdf579":"code","6c03381d":"code","d84ce2e8":"code","a97651f9":"code","43c5bcf7":"code","dcde9c6a":"code","df40c7a2":"code","5a4fc8db":"code","c6078bc8":"code","426e3839":"code","768369c8":"code","8553186b":"code","c70be709":"code","6ba7ebe1":"code","90e3787e":"code","67cd6316":"code","04f6e874":"code","53ffaa6b":"code","926adfab":"code","bbab94be":"code","2bc3c7c7":"code","99408eac":"code","21ae7552":"code","270b7cc4":"code","16935819":"code","6393d3e9":"code","953f893c":"code","1ce096e0":"code","4e93cf98":"code","bef3bd26":"code","820b1ca0":"code","4afa6cad":"code","2d9881d1":"markdown","dfb4b6c2":"markdown","1eb14c58":"markdown","51635c37":"markdown","6ffb0d19":"markdown","f7d77ee3":"markdown","678a2082":"markdown","d0ae1199":"markdown","aef8f3e4":"markdown","d1607f04":"markdown","10c62fef":"markdown","8715a9f7":"markdown","88f9c933":"markdown","7850c849":"markdown","5af3c117":"markdown","9e84129a":"markdown","a3f4e4b8":"markdown","8624c4cd":"markdown","edbe758e":"markdown"},"source":{"ef381246":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","636077bb":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve,f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics  import accuracy_score,classification_report,roc_auc_score,plot_roc_curve,plot_precision_recall_curve\nfrom sklearn.decomposition import PCA\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings('ignore')","23e77840":"data=pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-marketing.csv')","064462ba":"data","853915fd":"data.info()","59f1bf9f":"data.describe(include=np.number)","8f46acde":"data.describe(include=object)","baa92ec7":"data.isnull().sum() # no missing value is found","918b855f":"data.columns","9c0758c3":"num_var=data.select_dtypes(include=np.number).columns.to_list()\ncat_var=data.select_dtypes(include=object).columns.to_list()","dd4cdb4c":"data1=data.copy()# just to retain the original data","fb1a263d":"i=1\nplt.figure(figsize=(15,20))\nfor j in num_var:\n    plt.subplot(5,3,i)\n    sns.boxplot(data=data1,x=j)\n    i+=1","05279a87":"def outlier_tret(x):\n    upper=x.quantile(0.98)\n    lower=x.quantile(0.2)\n    x=np.where(x>upper,upper,x)\n    x=np.where(x<lower,lower,x)\n    return x","cdf848b2":"data1[num_var]=data1[num_var].apply(lambda x: outlier_tret(x))","f03e3fc8":"i=1\nplt.figure(figsize=(15,20))\nfor j in num_var:\n    plt.subplot(5,3,i)\n    sns.boxplot(data=data1,x=j)\n    i+=1","0def8df6":"data.hist(figsize=(15,15),color='red')\nplt.show()","afce637d":"sns.pairplot(data=data1)","f811d995":"def barPerc(df,xVar,ax):\n    '''\n    barPerc(): Add percentage for hues to bar plots\n    args:\n        df: pandas dataframe\n        xVar: (string) X variable \n        ax: Axes object (for Seaborn Countplot\/Bar plot or\n                         pandas bar plot)\n    '''\n    # 1. how many X categories\n    ##   check for NaN and remove\n    numX=len([x for x in df[xVar].unique() if x==x])\n\n    # 2. The bars are created in hue order, organize them\n    bars = ax.patches\n    ## 2a. For each X variable\n    for ind in range(numX):\n        ## 2b. Get every hue bar\n        ##     ex. 8 X categories, 4 hues =>\n        ##    [0, 8, 16, 24] are hue bars for 1st X category\n        hueBars=bars[ind:][::numX]\n        ## 2c. Get the total height (for percentages)\n        total = sum([x.get_height() for x in hueBars])\n\n        # 3. Print the percentage on the bars\n        for bar in hueBars:\n            ax.text(bar.get_x() + bar.get_width()\/2.,\n                    bar.get_height(),\n                    f'{bar.get_height()\/total:.0%}',\n                    ha=\"center\",va=\"bottom\")","6c6c3ac6":"i=1\nplt.figure(figsize=(20,80))\nfor col in cat_var:\n    plt.subplot(9,2,i)\n    ax=sns.countplot(data=data1,x=col,hue='y')\n    barPerc(df=data1,xVar=col,ax=ax)\n    plt.xticks(rotation=45)\n    i+=1","b82f3788":"num_var","5fe6bfad":"i=1\nplt.figure(figsize=(15,20))\nfor j in num_var:\n    plt.subplot(5,2,i)\n    sns.boxplot(data=data1,x='y',y=j)\n    i+=1\n    ","31aedde9":"age_yes=data1.groupby(by='y').get_group('yes')[['age']]\nage_no=data1.groupby(by='y').get_group('no')[['age']]\np=stats.ttest_ind(age_no,age_yes)\nprint(age_yes.mean())\nprint(age_no.mean())\nprint(p)","7aa90131":"age_group_yes=data1.groupby(by='y').get_group('yes')[['age group']]\nage_group_no=data1.groupby(by='y').get_group('no')[['age group']]\np=stats.ttest_ind(age_group_no,age_group_yes)\nprint(age_group_yes.mean())\nprint(age_group_no.mean())\nprint(p)","2de3b14b":"ax=sns.countplot(data=data1,x='age group',hue='y')\nbarPerc(df=data1,xVar='age group',ax=ax)","840c0a3e":"salary_yes=data1.groupby(by='y').get_group('yes')[['salary']]\nsalary_no=data1.groupby(by='y').get_group('no')[['salary']]\np=stats.ttest_ind(salary_no,salary_yes)\nprint(salary_yes.mean())\nprint(salary_no.mean())\nprint(p)","e441bd2d":"balance_yes=data1.groupby(by='y').get_group('yes')[['balance']]\nbalance_no=data1.groupby(by='y').get_group('no')[['balance']]\np=stats.ttest_ind(balance_no,balance_yes)\nprint(balance_yes.mean())\nprint(balance_no.mean())\nprint(p)","5d7059f5":"duration_yes=data1.groupby(by='y').get_group('yes')[['duration']]\nduration_no=data1.groupby(by='y').get_group('no')[['duration']]\np=stats.ttest_ind(duration_no,duration_yes)\nprint(duration_yes.mean())\nprint(duration_no.mean())\nprint(p)","6efdf579":"cam_yes=data1.groupby(by='y').get_group('yes')[['campaign']]\ncam_no=data1.groupby(by='y').get_group('no')[['campaign']]\np=stats.ttest_ind(cam_no,cam_yes)\nprint(cam_yes.mean())\nprint(cam_no.mean())\nprint(p)","6c03381d":"data2=data1.copy() # to retain the data1","d84ce2e8":"data2.drop(columns='age group',inplace=True)","a97651f9":"data2=pd.get_dummies(data=data2,columns=cat_var,drop_first=True)","43c5bcf7":"data2","dcde9c6a":"corr=data2.corr()[['y_yes']].sort_values(by='y_yes',ascending=False)","df40c7a2":"\ncorr.plot(kind='bar',figsize=(15,8))\nplt.show()","5a4fc8db":"sc=StandardScaler()","c6078bc8":"num_var.remove('age group') # removing age group","426e3839":"scaled=sc.fit_transform(data2[num_var])\nscaled=pd.DataFrame(scaled,columns=num_var)\nscaled_data=pd.concat([scaled,data2.drop(columns=num_var)],axis=1)","768369c8":"scaled_data1=scaled_data.drop(columns='y_yes')","8553186b":"scaled_data1","c70be709":"pca=PCA()","6ba7ebe1":"model=pca.fit_transform(scaled_data1)\nmodel","90e3787e":"np.cumsum(pca.explained_variance_ratio_)","67cd6316":"feature=range(pca.n_components_)","04f6e874":"plt.figure(figsize=(20,5))\nplt.bar(feature,pca.explained_variance_ratio_)\nplt.step(feature,np.cumsum(pca.explained_variance_ratio_),color='red')\nplt.axhline(y=.95,color='green')\nplt.grid(axis='x')\nplt.text(0.5,.85,'95% cutt-off thresold',color='green',fontsize=15)\nplt.show()","53ffaa6b":"pca1=PCA(n_components=25,whiten=True)","926adfab":"model=pca1.fit_transform(scaled_data1)","bbab94be":"np.cumsum(pca1.explained_variance_ratio_)","2bc3c7c7":"Y=scaled_data[['y_yes']]","99408eac":"Y.value_counts()# ratio is 1:8 so it is not imbalance classification problem","21ae7552":"x_train,x_test,y_train,y_test=train_test_split(model,Y,test_size=.3,random_state=0)","270b7cc4":"lg=LogisticRegression()\ndt=DecisionTreeClassifier()\nrf=RandomForestClassifier()\nnb=GaussianNB()\nknn=KNeighborsClassifier()\ngbc=GradientBoostingClassifier()\nadb=AdaBoostClassifier()\nsgd=SGDClassifier()\nsvc=SVC()\nmlpc=MLPClassifier()","16935819":"algo=[lg,dt,rf,nb,knn,adb,sgd,svc,mlpc]","6393d3e9":"for i in algo:\n    i.fit(x_train,y_train)\n    acc=i.score(x_test,y_test)\n    print(acc,i)","953f893c":"dt1=DecisionTreeClassifier()","1ce096e0":"dt1.fit(x_train,y_train)","4e93cf98":"y_pred=dt1.predict(x_test)","bef3bd26":"print(classification_report(y_pred,y_test))","820b1ca0":"plot_roc_curve(dt1,x_test,y_test)","4afa6cad":"plot_precision_recall_curve(dt1,x_test,y_test)","2d9881d1":"#### we choose decision tree classifier model","dfb4b6c2":"## FEATURE ENGINEERING","1eb14c58":"## EDA","51635c37":"#### lower the marketing campaign,higher the chance of selling product\n#### greater the previous campaign response greater the chance of selling the product","6ffb0d19":"#### p-value is less than 0.05, which means that both group of age group is different, the age group 6 -(60-69) is more likely to buy the product","f7d77ee3":"#### age and age group are the same so we will keep only age","678a2082":"### MODEL BUILDING","d0ae1199":"#### here we can see the correlation of different variables with y_yes","aef8f3e4":"### UNIVARIATE ANALYSIS","d1607f04":"#### we get p-value<.05,which means that the mean age of both group is significantly different ,so the higher age person are more likely to buy the product. ","10c62fef":"#### the mean age of the group of customer who is buyer of product yes or no ,is seem to be the equal ,so lets apply t-test to check whether mean age  for both category is same  or not","8715a9f7":"#### ouliers has been removed ","88f9c933":"#### here p-value is less than 0.05, so the mean salary for the both group (yes ,no) is significantly different,so we coclude that higher salary earning customer are more willing to buy the product","7850c849":"Banking dataset that helps running a campaign.\nhow do banks select target customers so that chances of selling their product maximize, this will help you understand these things.\n\nTry to build a classification model that predicts if the customer will buy the product or not!\nAge group:\n10 - 19 = 1\n\n20 - 29 = 2\n\n30 - 39 = 3\n\n40 - 49 = 4\n\n50 - 59 = 5\n\n60 - 69 = 6\n\n70 - 79 = 7\n\n80 - 89 = 8\n\n90 - 99 = 9\n\n\nage:\nage of the customer.\n\nage group:\nwhat age group does the customer lie\n\neligible:\nif the customer is eligible for the talk or not.\n\njob:\nwhat does the customer do?\n\nsalary:\nsalary of the customer\n\nmarital:\nmarried or not?\n\neducation:\nlevel of education complitrd\n\nmarital-education:\nmarried or not- education.\n\ntargeted:\nif the customer or being targeted or not\n\ndefault:\nif the customer in default list or not\n\nbalance:\nremaining balance in their accounts\n\nhousing:\nhousing\n\nloan:\nhas prior loan or not\n\ncontact:\nsource of this information\n\nday:\ntoday's date\n\nmonth:\nmonth\n\nduration:\nnumber of days\n\ncampaign:\nmarketing camoaign\n\npdays:\nnumber of days that passed by after the client was last contacted\n\nprevious:\nprevious response\n\npoutcome:\noutcome of the previous marketing campaign\n\ny:\nyes or no\n\nresponse:\nresponse of the actor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","5af3c117":"#### from above plot we can make an inference about the  specific type of customer who are more willing to buy the product.\n#### So we will choose only those category where the yes% is high, means more % of customer in that category are willing to buy the product \n \n#### 1- eligible         : No \n#### 2- job              : student\n#### 3- marital          : single\n#### 4- education        : tertiary\n#### 5-marital education :married tertiary,single secondary\n#### 6-targeted          : NO\n#### 7-default           : NO\n#### 8-housing           :NO\n#### 9-loan               :NO\n#### 10-contact           :cellular\n#### 11-month             : april\n#### 12-poutcome          : success\n#### So these are the group of customer where  they is more chance of getting sell of product ,so we should first target them.","9e84129a":"#### we can see that the data is containing some outlier ,lets remove these outliers","a3f4e4b8":"### BIVARIATE ANALYSIS","8624c4cd":"#### customer having higher balance is more willing to by the product","edbe758e":"#### higher the duration ,higher the chance of product selling"}}