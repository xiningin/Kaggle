{"cell_type":{"cae5a35b":"code","435054f0":"code","e0886679":"code","0f6ad823":"code","23ec6e7d":"code","8902f0f2":"code","e8f33a58":"code","b39b00d7":"markdown","08d4902a":"markdown","4ed32484":"markdown","e2ae4099":"markdown","77b77c26":"markdown","a62aea74":"markdown","ae790203":"markdown","0343ba3b":"markdown","6ac03c24":"markdown","37760e34":"markdown","52274e4d":"markdown","a0b74a74":"markdown","2c1fcd1c":"markdown","a0f8acb6":"markdown","9d0d9bdd":"markdown","14059fd3":"markdown","bd49f37d":"markdown","9996677b":"markdown","0fdf0cb9":"markdown","26b62e31":"markdown","cba1f257":"markdown"},"source":{"cae5a35b":"#Importing the libraries\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n#to load MNIST - popular dataset of written digits\nfrom sklearn.datasets import load_digits as LoadData ","435054f0":"#Let's load the data - it is pretty high-dimensional(64 features, each image of digit is 8x8)\nX, y = LoadData(n_class=10, return_X_y = True, as_frame = True)\nprint(X.shape) #printing its shape\nX.head(5) #and the first 5 elements","e0886679":"#Let's print a digit\nprint(np.array(X.iloc[9]).reshape(8,8))\nplt.imshow(np.array(X.iloc[9]).reshape(8,8), cmap = 'binary', interpolation = 'bilinear')","0f6ad823":"from sklearn.decomposition import PCA\npca = PCA(n_components = 0.95)\nX_reduced = pca.fit_transform(X)\nprint(X_reduced.shape)\n#Now, instead of 64 dimensions there is only 29.\n#So, the features are reduced almost in half, but the varience dropped only 5% lower\n#This happens because we have a lot of 0-valued cells in the image.","23ec6e7d":"#Now let's recover out original data from compressed and recognise the same digit we had!\n#We can recognise it because the amount of varience dropped was pretty small, so the recovered version is almost the same. \nX_recovered = pca.inverse_transform(X_reduced)\nplt.imshow(X_recovered[9].reshape(8,8), cmap = 'binary', interpolation = 'bilinear')","8902f0f2":"X_centered = X - X.mean(axis = 0) #Centering the data\nU, S, Vt = np.linalg.svd(X_centered) #Decomposing the centered matrix\n#The first principal component looks like this:\nVt.T[:, 0]\n#Vt is a transposed matrix, and if we transpose it one more time, it would give us the original V.\n#It is clear, that the PCs lie along vertical axis, so Vt.T[:, 0] gives us the first vector.","e8f33a58":"#In order to make projection of the data, we need to introduce Weights, which are our principal components\nn_of_components = 29\nW_d =  Vt.T[:, :n_of_components]\nX_dproj = X_centered.dot(W_d)\nprint(X_dproj.shape)\n#Now X_dproj is the projection of our samples onto the hyperplane of the first 29 PCs\nprint(X_dproj.iloc[0])","b39b00d7":"# PCA\nWhen it comes to popularity among all the techiques, ***Principal Component Analysis*** is the most used dimensionality reduction algorithm. ***PCA***, as well as the others helps reduce the dimensionality of a dataset by projecting the data onto a lower-dimensional hyperplane.\n\n\nPCA's algorithm uses ***Principal Components*** in order to calculate new features and keep most of the information simultaneously. The reason is, that ***Principal Components*** keep the most varience when the information is projected onto them.\n> Lines, which preserve most of the information when data is projected onto them, are called ***Principal Components*** ","08d4902a":"#### But **why do we need Covarience Matrix?** \nThe point is, covarience matrix is just a tool that helps us find the line that maximizes the varience - ***Principal Component***.\nThe relation is pretty easy to spot, just because the word **\"*varience*\"** is in **\"Co*varience* Matrix\"**.","4ed32484":"# The algorithm of PCA\nThe algorithm of PCA consists of several steps:\n1. ***Centering the data***\n2. ***Computing Covarience Matrix***\n3. ***Finding Eigenvalues, Eigenvectors and Principal Components***\n4. ***Deciding what PCs to choose***\n5. ***Projecting and creating new features***","e2ae4099":"![PCA.JPG](attachment:75496cad-917a-460a-a3f5-ccd25d1e7e04.JPG)","77b77c26":"# DR Techniques\nThe method can also be applied in different ways, and this means there are a lot of Dimensionality Reduction Techniques. Some of the mostly used are:\n1. ***Factor Analysis (FA)***\n2. ***Linear Discriminant Analysis (LDA)***\n3. ***Truncated Singular Value Decomposition (tSVD)***\n4. ***Principal Component Analysis (PCA)***, which I am going to talk about in this notebook.","a62aea74":"### 2. Computing Covarience Matrix\n***Covarience Matrix*** can be thought of as a matrix, which shows how data features relate to each other.\n\n\nEach covarience in this matrix shows how one feature is related to the other one. When one feature goes up in values - does the other one go up, go down or stay the same in values? **Covarience** answers this question. \n\n\nSimpler graph is represented below.","ae790203":"<img src=https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/Covar.png>\n\n(Picture by geeks-for-geeks, topic: 'Mathematics | Covariance and Correlation')","0343ba3b":"# Principal Component Analysis\n___","6ac03c24":"#### To calculate ***Covarience***, we simply use this formula: \n\n\n$\\displaystyle Cov(x, y) = \\frac{\\sum^{N}_{i=1}(x_i -\\mu)(y_i-\\nu)}{N}$\n\nBut, since the data is centered in our case, we can use this formula instead:\n\n$\\displaystyle Cov(x, y) = \\frac{\\sum^{N}_{i=1}x_i y_i}{N}$\n\nWhere: \n1. $x$ is the feature 1, $x_i$ is the sample's value along feature 1\n2. $y$ is the feature 2, $y_i$ is the sample's value along feature 2 \n3. $\\mu$ - is the center of feature 1 (mean of feature 1). In our case it is equal to $0$\n4. $\\nu$ - is the center of feature 2 (mean of feature 2). In our case it is equal to $0$","37760e34":"### 1. Centering the data\n***Centering*** is important in the PCA algorithm, because ***Principal Components*** always penetrate the origin of the data - its center. So, to make the process of computations possible, we consider ***centering*** the data","52274e4d":"![DimensReduct.JPG](attachment:9897f8e1-ac04-4f52-bf89-368fb5bfde16.JPG)","a0b74a74":"**Dimensionality Reduction** is simply a method which helps reduce the number of features in a dataset, remaining most of the data's information and importance. It is not perfect (since we lose some of the varience) but surely is helpful.\n\n\nThere are a lot of cases when the method is applied:\n- ***Data compression.*** (After performing the method, data takes less storage space.) \n- ***Removing redundant features***\n- ***Reducing time of computation*** and so on.","2c1fcd1c":"### 5. Projecting points onto chosen PCA's\nWhen we have chosen our principal components, projection is really simple: \n$\\displaystyle X_d = X \\cdot W_d \\\\\nW_d = V$ \n\n\nWhere $X_d$ is a matrix of new data with new features, $V$ is a matrix of ***Principal Components*** ","a0f8acb6":"# 2 Method (SVD)\nWe can use standart technique for matrices, known as ***Singular Value Decomposition***.\n***Singular Value Decomposition*** is a method, by which our X matrix is decomposed into three, and one of those matrices is the ***Principal Components' matrix***.\n\n$\\displaystyle X = U \\cdot \\Sigma \\cdot V^T$\n\nWhere $V$ is a matrix of **PCs**:\n\n$V = \\begin{pmatrix}\n|  & | & \\cdots & | \\\\\n\\overrightarrow{v_1} & \\overrightarrow{v_2} & \\cdots & \\overrightarrow{v_n} \\\\\n| &  | & \\cdots & | \\end{pmatrix}$\n\n(***SVD*** is applied in Scikit-Learn's PCA class too, but this method gives more understanding, insight and control of what's going on)","9d0d9bdd":"# Dimensionality Reduction\nThe Data used in such fields as Machine Learning and Data Science can vary in its size and shape.\nSometimes Datasets contain so many features that there is not enough computational power to use them for training models (or it is not really convenient).\nBut how can we possibly change the situation and still get the predictions?\n\n\n**Dimensionality Reduction** - that's the answer!","14059fd3":"### Let's gain more understanding about PCA by examining maths along with the algorithm!","bd49f37d":"\n#### The ***Covarience Matrix*** looks like this:\n\n\n$\\displaystyle \\begin{pmatrix}\nCov(f_1, f_1) & Cov(f_1,f_2) & \\cdots & Cov(f_1, f_n) \\\\\nCov(f_2, f_1) & Cov(f_2, f_2) & \\cdots & Cov(f_2, f_n) \\\\\n\\vdots & \\vdots & \\ddots & \\cdots \\\\\nCov(f_n, f_1) & Cov(f_n, f_2) & \\cdots & Cov(f_n, f_n) \\end{pmatrix}$ \n\n\nWhich is equal to the following:\n\n\n$\\begin{pmatrix}\nVar(f_1) & Cov(f_1,f_2) & \\cdots & Cov(f_1, f_n) \\\\\nCov(f_2, f_1) & Var(f_2) & \\cdots & Cov(f_2, f_n) \\\\\n\\vdots & \\vdots & \\ddots & \\cdots \\\\\nCov(f_n, f_1) & Cov(f_n, f_2) & \\cdots & Var(f_n) \\end{pmatrix}$ \n\n\nWhere $f_i$ is one of the features","9996677b":"### 4. Choosing PCA's with the most varience\n$\\displaystyle \\text{PCA}_i \\text{ percentage} = \\frac{Var(\\text{X} \\cdot \\overrightarrow{v}_i)}{\\sum^{n}_{i=1}Var(\\text{X} \\cdot \\overrightarrow{v}_i)}$\n\n($\\text{X} \\cdot \\overrightarrow{v}_i$ is the projection of the dataset X onto a ***Principal Component*** $\\overrightarrow{v}$)\n\nSo, now we are going to consider the **percentage** of the varience each **PCA saves** and going to choose the most valueable.","0fdf0cb9":"# PCA on MNIST","26b62e31":"### 3. Finding Eigenvalues, Eigenvectors and Principal Components\n***Eigenvalues and Eigenvectors*** are very important when finding ***Principal Components***. \n\n**Definition.** Let's think of a square matrix $A$ as if it is a linear transformation. So, the matrix tranforms our linear space!\n\n\nLet's take some vector $\\overrightarrow{v}$ that does not change direction when we transorm our linear space, just changes the size (in new space, it is $\\lambda\\cdot\\overrightarrow{v}$ instead of just $\\overrightarrow{v}$). \nThen we would call $\\lambda$ an ***Eigenvalue*** and $\\overrightarrow{v}$ an ***Eigenvector***!\n\n\n*(3Blue1Brown has a great video on the topic of E&E: https:\/\/www.youtube.com\/watch?v=PFDu9oVAE-g&t=534s)* \n\n\nSo, using **Covarience Matrix** as a linear transormation we find a bunch of **Eigenvectors**, which match the ***Principal Components*** lines.\n> When we use ***Covarience Matrix*** as a linear transformation, ***Eigenvectors*** represent lines(PCs), along which the varience is preserved the most.","cba1f257":"# 1 Method (sklearn)\nWe can reduce dimensionality using sklean library, which is very convenient. There is a possibility to write either the exact number of components, or a varience percentage we need to save."}}