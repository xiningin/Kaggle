{"cell_type":{"85d6dce1":"code","47c1539f":"code","6a916609":"code","460927e3":"code","a928afcc":"code","a41a49b1":"code","30f3f4ca":"code","59d531bb":"code","d68b2238":"code","1e2b9b6f":"code","85f5ae97":"code","00a77331":"code","851a0149":"code","757d1643":"code","b63dc9e4":"code","b078c522":"code","165900d9":"code","1bbfe0a0":"code","bc82ae75":"code","df213647":"markdown","8dcb007b":"markdown","11ae6b7d":"markdown","9ee9e6b3":"markdown","640ba702":"markdown","a64aac45":"markdown","7aa5226d":"markdown","bbe7006a":"markdown"},"source":{"85d6dce1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47c1539f":"import gc\nimport random\n\nfrom IPython import display as ipd\nfrom tqdm import tqdm\nimport lightgbm as lgb\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\n\nimport optuna \nfrom optuna.visualization.matplotlib import plot_optimization_history\nfrom optuna.visualization.matplotlib import plot_param_importances","6a916609":"def seeding(SEED, use_tf=False):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    if use_tf:\n        tf.random.set_seed(SEED)\n    print('seeding done!!!')\n    \n## https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298201\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)    ","460927e3":"RANDOM_SEED = 42\nDEBUG = True\nTUNING = False\n\nseeding(RANDOM_SEED)\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')","a928afcc":"train.head()","a41a49b1":"## targets distribution by country\n\nf, (ax1,ax2,ax3) = plt.subplots(3, 1, figsize=(16, 16))\nsns.despine(f)\ng1 = sns.histplot( data=train[train['country'] == 'Finland'], x = 'num_sold', hue='product', ax=ax1,  palette=\"rainbow\")\ng1.set_title(\"Finland\")\ng2 = sns.histplot( data=train[train['country'] == 'Norway'], x = 'num_sold', hue='product', ax=ax2,  palette=\"rainbow\")\ng2.set_title(\"Norway\")\ng3 = sns.histplot( data=train[train['country'] == 'Sweden'], x = 'num_sold', hue='product', ax=ax3,  palette=\"rainbow\")\ng3.set_title(\"Sweden\")","30f3f4ca":"f, ax1 = plt.subplots(1, 1, figsize=(16, 6))\n\nsns.boxplot( data=train, x=\"country\", y=\"num_sold\", hue=\"product\", ax=ax1, palette=\"Spectral\")\nsns.despine(left=True)","59d531bb":"## Another nice way of showing distribution\n\nf, ax1 = plt.subplots(1, 1, figsize=(16, 6))\nproduct_order = [\"Kaggle Mug\", \"Kaggle Hat\", \"Kaggle Sticker\"]\nsns.boxenplot(x=\"product\", y=\"num_sold\", palette=\"rainbow\", hue='country', order=product_order, scale=\"linear\", data=train)","d68b2238":"def process_dates(df):\n    df.date = pd.to_datetime(df.date)\n    df['month'] = df.date.dt.month\n    df['week'] = df.date.dt.week\n    df['weekday'] = df.date.dt.weekday\n    df['dayofweek'] = df.date.dt.dayofweek\n    df['dayofyear'] = df.date.dt.dayofyear\n    df['day'] = df.date.dt.day\n    return df\n\ntrain = process_dates(train)\ntest = process_dates(test)","1e2b9b6f":"print(f'Train unique days: {train.day.unique().size}, test: {test.day.unique().size}')\nprint(f'Train unique weeks: {train.week.unique().size}, test: {test.week.unique().size}')\nprint(f'Train unique dayofweeks: {train.dayofweek.unique().size}, test: {train.dayofweek.unique().size}')\nprint(f'Train unique months: {train.month.unique().size}, test: {train.month.unique().size}')\nprint(f'Train unique dayofyear: {train.dayofyear.unique().size}, test: {train.dayofyear.unique().size}')","85f5ae97":"target = train.num_sold\ntrain.drop(['row_id','num_sold','date'], axis=1, inplace=True)\ntest.drop(['row_id', 'date'], axis=1, inplace=True)","00a77331":"country_encoder = LabelEncoder()\ntrain['country_enc'] = country_encoder.fit_transform(train['country'])\ntest['country_enc'] = country_encoder.transform(test['country'])\n\nstore_encoder = LabelEncoder()\ntrain['store_enc'] = store_encoder.fit_transform(train['store'])\ntest['store_enc'] = store_encoder.transform(test['store'])\n\nproduct_encoder = LabelEncoder()\ntrain['product_enc'] = product_encoder.fit_transform(train['product'])\ntest['product_enc'] = product_encoder.transform(test['product'])\n\ntrain.drop(['country','store','product'], axis=1, inplace=True)\ntest.drop(['country','store','product'], axis=1, inplace=True)","851a0149":"for col in train.columns:\n    train[col] = pd.Categorical(train[col])\nfor col in test.columns:\n    test[col] = pd.Categorical(test[col])","757d1643":"NUM_BOOST_ROUND = 2000\nEARLY_STOPPING_ROUNDS = 50\nVERBOSE_EVAL = 100\n\ndef objective(trial, X, y):\n    \n    param_grid = {\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'n_estimators': trial.suggest_categorical('n_estimators', [2000]),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n        'num_leaves': trial.suggest_int('num_leaves', 50, 2000, step=50),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 200, 2000, step=100),\n        'max_bin': trial.suggest_int('max_bin', 200, 300),\n        'lambda_l1': trial.suggest_int('lambda_l1', 0, 100, step=5),\n        'lambda_l2': trial.suggest_int('lambda_l2', 0, 100, step=5),        \n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }    \n        \n    X_train, X_valid, y_train, y_valid = train_test_split( X, y, test_size=0.25, random_state=RANDOM_SEED, shuffle=True)\n    eval_results = {}  # to record eval results for plotting\n    \n    model = lgb.train(\n        param_grid, valid_names=[\"train\", \"valid\"], \n        train_set=lgb.Dataset(X_train, y_train ), \n        num_boost_round = NUM_BOOST_ROUND,\n        valid_sets = [lgb.Dataset(X_valid, y_valid)],\n        callbacks=[lgb.log_evaluation(VERBOSE_EVAL), \n           lgb.early_stopping(EARLY_STOPPING_ROUNDS, False, True),\n           lgb.record_evaluation(eval_result=eval_results)],        \n    )    \n    \n    oof_pred = model.predict(X_valid)\n    return SMAPE(y_valid, oof_pred)        ","b63dc9e4":"N_TRIALS = 100\n\nif TUNING:\n    study = optuna.create_study(direction='minimize')\n    objective_func = lambda trial: objective(trial, train, target)\n    study.optimize(objective_func, n_trials=N_TRIALS)  # number of iterations\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","b078c522":"def run_train(X, y, run_params, splits, num_boost_round, verbose_eval, early_stopping_rounds ):\n    scores = []\n    models = []\n    eval_results = {}  # to record eval results for plotting\n    folds = StratifiedKFold(n_splits=splits)\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n+1} started')\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        model = lgb.train(\n            run_params, valid_names=[\"train\", \"valid\"], \n            train_set=lgb.Dataset(X_train, y_train ), \n            num_boost_round = num_boost_round,\n            valid_sets = [lgb.Dataset(X_valid, y_valid)],\n            callbacks=[lgb.log_evaluation(verbose_eval), \n               lgb.early_stopping(early_stopping_rounds, False, True),\n               lgb.record_evaluation(eval_result=eval_results)],\n        )\n\n        y_predicted = model.predict(X_valid)\n        score = SMAPE(y_valid, y_predicted)   \n        print(f'SMAPE: {score}')\n\n        models.append(model)\n        scores.append(score)\n    return scores, models, eval_results\n\n\nTOTAL_SPLITS = 5\nNUM_BOOST_ROUND = 8000\nEARLY_STOPPING_ROUNDS = 100\nVERBOSE_EVAL = 200\n    \nrun_params = {\n    'verbose': -1, \n    'boosting_type': 'gbdt', \n    'objective': 'regression', \n    'metric': ['rmse'],\n    'learning_rate': 0.03600124778051181,\n    'num_leaves': 1400,\n    'max_depth': 9,\n    'min_data_in_leaf': 200,\n    'max_bin': 240,\n    'lambda_l1': 45,\n    'lambda_l2': 20,\n    'feature_fraction': 0.9033256488572796,\n    'bagging_fraction': 0.9728721582350929,\n    'bagging_freq': 1,\n    'min_child_samples': 69,\n}\n\nscores, models, eval_results = run_train(train, target, run_params, TOTAL_SPLITS, NUM_BOOST_ROUND, \n                                          VERBOSE_EVAL, EARLY_STOPPING_ROUNDS)\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))","165900d9":"ax = lgb.plot_metric(eval_results, metric='rmse')\nplt.show()","1bbfe0a0":"y_pred = np.zeros(len(test))\nfor model in models:\n    y_pred += model.predict(test).reshape(-1)\n    \ny_pred = y_pred \/ len(models)","bc82ae75":"submission['num_sold'] = np.round(y_pred).astype(int)\nsubmission.to_csv('submission.csv', index=False, float_format='%.6f')\nsubmission.head(20)","df213647":"## Targets distribution display","8dcb007b":"### Model and train","11ae6b7d":"### Data Load","9ee9e6b3":"### Very simple date-based FE","640ba702":"### Utils","a64aac45":"### Encode category columns ","7aa5226d":"### Plot metrics","bbe7006a":"### Tuning"}}