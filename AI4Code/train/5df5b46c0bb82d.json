{"cell_type":{"76a543d1":"code","d5b048a8":"code","85375acd":"code","e18f9573":"code","ce2b08be":"code","3861f08c":"code","e2e61dab":"code","a760f5fe":"code","7343d660":"code","91615a37":"code","3189937c":"code","4ffd6c36":"code","1a75dcec":"code","e00522d9":"code","f4b37fd1":"code","1fb082eb":"code","644d05ff":"code","c07ca448":"code","d088b82a":"code","27d179ad":"code","b5aee82b":"code","9d8110ce":"code","864953dd":"code","9caade2e":"code","8cb590be":"code","2291b205":"code","19754874":"code","7a9e22c0":"code","8ee2b906":"code","e285a1b0":"code","8596aa18":"code","41f6a1cc":"code","da0b9484":"code","458d9deb":"code","5429067d":"code","f6a34302":"code","28a16c45":"code","1dc86b28":"code","e896a2b3":"code","48c5f240":"code","9d05053e":"code","733eb318":"code","bc00a695":"code","7ec2bae8":"code","c7cef098":"code","7d41ed7b":"code","1a3ae494":"code","adbcfd14":"code","b99f4e5e":"code","1064eae3":"code","822a09aa":"code","172f0649":"code","d4a6cda5":"code","bd2c2bf4":"code","baa5d0c1":"code","1ea55ffe":"code","968a36cc":"code","49933c08":"code","8d8fd518":"code","54e23e84":"markdown","7855f41f":"markdown","c9459918":"markdown","b404f762":"markdown","c9fb6e0f":"markdown","399dcd41":"markdown","456cbe2b":"markdown","fddaa689":"markdown","04d97b7e":"markdown","df2baffd":"markdown","41e08928":"markdown","c266bff5":"markdown","74a17159":"markdown","d05a2e33":"markdown","fc8f286b":"markdown","2eefcede":"markdown","6b495a7d":"markdown","f16e5b29":"markdown","6aa7326f":"markdown","32e97909":"markdown","92c996f4":"markdown","8f453a7c":"markdown","28bf93c9":"markdown","51effa67":"markdown","f277d309":"markdown","725c6d2b":"markdown","1a4a5466":"markdown","96e193c3":"markdown","fa9a930c":"markdown","14031c10":"markdown","29683626":"markdown","807e2e7c":"markdown","b8c9fdcd":"markdown","7c50eff3":"markdown","b5bdc856":"markdown","db3916be":"markdown","0ba7b4a5":"markdown","b5769a69":"markdown","2a521654":"markdown","afd9d62e":"markdown","ed40d580":"markdown","6fb98765":"markdown","f850a83a":"markdown","b4c51afa":"markdown","0b7fa288":"markdown","0181d4fe":"markdown","9eec04d3":"markdown","ea27a06d":"markdown","127e62b9":"markdown","1f631759":"markdown","934f2798":"markdown","a2885224":"markdown","aae307ca":"markdown","5180ae09":"markdown","5c6bd8c1":"markdown","a0563ff0":"markdown","6d03b643":"markdown","448644ab":"markdown","c137e4f9":"markdown","3dc3f899":"markdown"},"source":{"76a543d1":"#Imports \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","d5b048a8":"#start_df = pd.read_csv('..\/input\/loan.csv', low_memory=False)","85375acd":"df = pd.read_csv('..\/input\/loan.csv', low_memory=False)","e18f9573":"df = start_df.copy(deep=True)\ndf.head()","ce2b08be":"df.shape","3861f08c":"df.columns","e2e61dab":"df_description = pd.read_excel('..\/input\/LCDataDictionary.xlsx').dropna()\ndf_description.style.set_properties(subset=['Description'], **{'width': '1000px'})","a760f5fe":"def null_values(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","7343d660":"# Missing values statistics\nmiss_values = null_values(df)\n","91615a37":"miss_values","3189937c":"#added Target(loans that are considered bad) for better visulization\ntarget_list = ['Default' , 'Charged Off', '(31-120 days)' , 'Late (16-30 days)' , 'Does not meet the credit policy. Status:Charged Off', 'In Grace Period']\ndf['TARGET'] = [1 if i in(target_list) else 0 for i in df['loan_status']]\n\ndf['TARGET'].value_counts()\n","4ffd6c36":"df.drop('loan_status',axis=1,inplace=True)","1a75dcec":"# Number of each type of column\ndf.dtypes.value_counts().sort_values().plot(kind='barh')\nplt.title('Number of columns distributed by Data Types',fontsize=20)\nplt.xlabel('Number of columns',fontsize=15)\nplt.ylabel('Data type',fontsize=15)","e00522d9":"#how many categorical data do the columns having 'object' data types contain:\ndf.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","f4b37fd1":"df['emp_length'].fillna(value=0,inplace=True)\n\ndf['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n\ndf['emp_length'].value_counts().sort_values().plot(kind='barh',figsize=(18,8))\nplt.title('Number of loans distributed by Employment Years',fontsize=20)\nplt.xlabel('Number of loans',fontsize=15)\nplt.ylabel('Years worked',fontsize=15);","1fb082eb":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"pymnt_plan\", split=True)\nplt.title(\"Payment plan - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","644d05ff":"temp = [i for i in df.count()<887379 *0.30]\ndf.drop(df.columns[temp],axis=1,inplace=True)","c07ca448":"corr = df.corr()[\"TARGET\"].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', corr.tail(10))\nprint('\\nMost Negative Correlations:\\n', corr.head(10))","d088b82a":" dti_corr = df.corr()['dti'].sort_values()","27d179ad":"dti_corr","b5aee82b":"fig = plt.figure(figsize=(22,6))\nsns.kdeplot(df.loc[df['TARGET'] == 1, 'int_rate'], label = 'target = 1')\nsns.kdeplot(df.loc[df['TARGET'] == 0, 'int_rate'], label = 'target = 0');\nplt.xlabel('Interest Rate (%)',fontsize=15)\nplt.ylabel('Density',fontsize=15)\nplt.title('Distribution of Interest Rate',fontsize=20);","9d8110ce":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"term\", split=True,color='pink')\nplt.title(\"Term - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","864953dd":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"application_type\", split=True,color='green')\nplt.title(\"Application Type - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","9caade2e":"df['application_type'].value_counts()","8cb590be":"fig = plt.figure(figsize=(18,8))\nsns.violinplot(x=\"TARGET\",y=\"int_rate\",data=df, hue=\"grade\")\nplt.title(\"Grade - Interest Rate\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Interest Rate\", fontsize=15);","2291b205":"df.corr()['annual_inc'].sort_values().tail(10)","19754874":"fig = plt.figure(figsize=(18,10))\ndf[df['TARGET']==1].groupby('addr_state')['TARGET'].count().sort_values().plot(kind='barh')\nplt.ylabel('State',fontsize=15)\nplt.xlabel('Number of loans',fontsize=15)\nplt.title('Number of defaulted loans per state',fontsize=20);","7a9e22c0":"fig = plt.figure(figsize=(18,10))\ndf[df['TARGET']==0].groupby('addr_state')['TARGET'].count().sort_values().plot(kind='barh')\nplt.ylabel('State')\nplt.xlabel('Number of loans')\nplt.title('Number of not-defaulted loans per state');","8ee2b906":"df['member_id'].value_counts().head(2)","e285a1b0":"df['emp_title'].value_counts().head()","8596aa18":"df.drop(['id','member_id','emp_title','title','zip_code','url'],axis=1,inplace=True)","41f6a1cc":"df.shape","da0b9484":"df.info()","458d9deb":"df['issue_d']= pd.to_datetime(df['issue_d']).apply(lambda x: int(x.strftime('%Y')))\ndf['last_pymnt_d']= pd.to_datetime(df['last_pymnt_d'].fillna('2016-01-01')).apply(lambda x: int(x.strftime('%m')))\ndf['last_credit_pull_d']= pd.to_datetime(df['last_credit_pull_d'].fillna(\"2016-01-01\")).apply(lambda x: int(x.strftime('%m')))\ndf['earliest_cr_line']= pd.to_datetime(df['earliest_cr_line'].fillna('2001-08-01')).apply(lambda x: int(x.strftime('%m')))\ndf['next_pymnt_d'] = pd.to_datetime(df['next_pymnt_d'].fillna(value = '2016-02-01')).apply(lambda x:int(x.strftime(\"%Y\")))","5429067d":"from sklearn import preprocessing","f6a34302":"count = 0\n\nfor col in df:\n    if df[col].dtype == 'object':\n        if len(list(df[col].unique())) <= 2:     \n            le = preprocessing.LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n            count += 1\n            print (col)\n            \nprint('%d columns were label encoded.' % count)","28a16c45":"df = pd.get_dummies(df)\nprint(df.shape)","1dc86b28":"df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(df['mths_since_last_delinq'].median())","e896a2b3":"df.dropna(inplace=True)","48c5f240":"df.count().sort_values()","9d05053e":"df['TARGET'].value_counts()","733eb318":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","bc00a695":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))    \n        ","7ec2bae8":"from sklearn.model_selection import train_test_split","c7cef098":"X_train, X_test, y_train, y_test = train_test_split(df.drop('TARGET',axis=1),df['TARGET'],test_size=0.15,random_state=101)","7d41ed7b":"del start_df\ngc.collect()","1a3ae494":"from sklearn.preprocessing import StandardScaler","adbcfd14":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","b99f4e5e":"from imblearn.over_sampling import SMOTE","1064eae3":"sm = SMOTE(random_state=12, ratio = 1.0)\nx_train_r, y_train_r = sm.fit_sample(X_train, y_train)","822a09aa":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(C = 0.0001,random_state=21)\n\nlog_reg.fit(x_train_r, y_train_r)","172f0649":"print_score(log_reg, x_train_r, y_train_r, X_test, y_test, train=False)","d4a6cda5":"from sklearn.ensemble import RandomForestClassifier","bd2c2bf4":"clf_rf = RandomForestClassifier(n_estimators=40, random_state=21)\nclf_rf.fit(x_train_r, y_train_r)","baa5d0c1":"print_score(clf_rf, x_train_r, y_train_r, X_test, y_test, train=False)","1ea55ffe":"from sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom lightgbm import LGBMClassifier","968a36cc":"def kfold_lightgbm(train_df, num_folds, stratified = False):\n    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n    \n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=47)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n\n    oof_preds = np.zeros(train_df.shape[0])\n\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET']]\n    \n    # Splitting the training set into folds for Cross Validation\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=32,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.04,\n            reg_lambda=0.073,\n            min_split_gain=0.0222415,\n            min_child_weight=40,\n            silent=-1,\n            verbose=-1,\n            )\n\n        # Fitting the model and evaluating by AUC\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n        print_score(clf, train_x, train_y, valid_x, valid_y, train=False)\n        # Dataframe holding the different features and their importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        # Freeing up memory\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    display_importances(feature_importance_df)\n    return feature_importance_df","49933c08":"def display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(15, 12))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')","8d8fd518":"feat_importance = kfold_lightgbm(df, num_folds= 3, stratified= False)","54e23e84":"So, now we have 48 columns remaining. Let's print them out to get a quick look of what we are dealing with,","7855f41f":"> Naturally, the defaulted loans had no payment plan","c9459918":"> The column looks fine. Also, it can be seen that people who have worked for 10 or more years are more likely to take a loan.","b404f762":"First, importing necessary libraries,","c9fb6e0f":"Now, for modeling I will be using two ensemble methods and comparing them.\n\ni) Bootstrap Aggregrating or Bagging\n\nii) Boosting","399dcd41":"The accuracy came out to be satisfactory for the baseline along with the recall score. However, precision seems to be very off.","456cbe2b":"> Both target classes have similar kind of interest rates by grades.","fddaa689":"Conducting train test split.","04d97b7e":"## Exploratory Data Analysis","df2baffd":"Let me remove all the columns with more than 70% missing data as they won't be helping for modelling and exploration.","41e08928":"Function to use LightGBM with Kfold cross validation,","c266bff5":"* Oversampling only the training set using Synthetic Minority Oversampling Technique ([SMOTE](https:\/\/jair.org\/index.php\/jair\/article\/view\/10302))","74a17159":"As we can see, LightGBM did a great job for getting high precision as well as a high recall. Hence, this model is the best in terms of the 3 models that we evaluated.","d05a2e33":"**Creating a baseline for accuracy and recall using Logistic regression, **","fc8f286b":"First, let's check the description of the various column fields in the dataset.","2eefcede":"We are now left with a reasonable amount of data for modelling.\n\n---","6b495a7d":"## 2) Boosting:\n\n* Train weak classifiers \n* Add them to a final strong classifier by weighting. Weighting by accuracy (typically)\n* Once added, the data are reweighted\n  * Misclassified samples gain weight \n  * Algo is forced to learn more from misclassified samples    ","f16e5b29":"Now, I'll be trying out different models to get the best prediction score.","6aa7326f":"# Modeling","32e97909":"> It can be seen that the interest rate is also highly positively correlated with the debt to income ratio.","92c996f4":"Creating a classification report function,","8f453a7c":"I'll be filling the null values with 0 assuming that the borrower hasn't worked many years for his data to be recorded. Also, I'll be using regex to extract the number of years from all of the data.","28bf93c9":">  We would want to label encode the columns having only 2 categorical data and one-hot encode columns with more than 2 categorical data. Also, columns like emp_title, url, desc, etc. should be dropped because there aren't any large number of unique data for any of the categories they contain. Also, Principal Component Analysis can be carried out for the one-hot encoded columns to bring the feature dimensions down.","51effa67":"For our case, overfitting will be a huge concern. So, I'm using Random Forest as it is known to decrease overfitting by selecting features at random. ","f277d309":"# 1) Bagging - Random Forest\n\n* Ensemble of Decision Trees\n\n* Training via the bagging method (Repeated sampling with replacement)\n  * Bagging: Sample from samples\n  * RF: Sample from predictors. $m=sqrt(p)$ for classification and $m=p\/3$ for regression problems.\n\n* Utilise uncorrelated trees","725c6d2b":"** Violin-plot of TARGET classes with distribution of interest rate differentiated by the loan grades. **","1a4a5466":"> Most of the Loans of higher terms have high amount and vice versa for the TARGET classes.","96e193c3":"Printing out the column names,","fa9a930c":"> So we have quite a number of columns having objects data type which are going to pose a problem while modelling. ","14031c10":"Let's see if we have any members taking multiple loans.","29683626":"Standardizing features by removing the mean and scaling to unit variance","807e2e7c":"\nLet's see how we can handle our categorical data. Two methods we can use are Label Encoding and One Hot Encoding.\n\nThe problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. So, If we only have two unique values for a categorical variable (such as Yes\/No), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the better option.\n\nHowever, due to the large number of columns originated after One-Hot Encoding, we may have to conduct Principle Component Analysis (PCA) for dimensionality reduction.","b8c9fdcd":"Function for displaying the importance of the features,","7c50eff3":"> It can be seen that there are more number of loans taken amount from the same states where there are more number of defaulted risk. This is why the state cannot be taken as a major feature for knowing if a loan will be defaulted or not.\n","b5bdc856":"Then, seeing the distribution of data types we are working with,","db3916be":"> The density of interest rates follow kind of a Gaussian distribution with more density on interest rates between 12%-18%.","0ba7b4a5":"Examining further on debt to income ratio and interest rate,","b5769a69":"We have high precision but a low recall for our validation set. Using this model is not a good idea as most of our default loans will be falsely classified.","2a521654":"> Besides from the perfect correlation of TARGET column with itself, columns like int_rate which is interest rate, out_prncp_inv which is remaining outstanding principal, etc. have high positive correlation with the TARGET column and these are quite true as higher the interest rate, higher it is harder for a borrower to pay back a loan. However, columns like out_prncp_inv, out_prncp, total_rec_int, total_rec_late_fee, inq_last_6mths and revol_util are bound to be higher when a borrower doesn't pay back a loan and thus doesn't carry much significance. So, the column of interest after int_rate could be the dti which is the Debt to Income ratio which understandably will affect if a borrower can pay back a loan or not.\n\n> Also, columns like recoveries, total_rev_hi_lim, etc. have negative correlation with the TARGET column as a borrower who has paid back money is more likely to repay the loan.","afd9d62e":"> The annual income of the applicant has high positive correlation with the amount of loan they have taken.","ed40d580":"And one-hot encoding the rest categorical columns,","6fb98765":"# \nAs we had observe, some columns like annual_inc, int_rate, etc. may be much useful for building our model but on the other hand, some columns like id, member_id, etc. will not be helping. \n\nAlso, columns like 'title' and 'emp_title' are text which cannot be one-hot encoded \/ label encoded as they have arbitrary categorical text and very less unique data for each of their categories.","f850a83a":"Let us also check the correlation of annual income with loan amount taken. ","b4c51afa":"The memory usage is 325+ MB. Some of these columns still look like they could need some work i.e. more cleaning!\u00a0\n\nI will be fixing the data types and then handling the missing data.","0b7fa288":"For the 'mths_since_last_delinq' column, I'll be filling in the missing value with the median of the columns as the data in the column is continuous.","0181d4fe":"## Anomaly Detection","9eec04d3":"And checking the count,","ea27a06d":"> Suprisingly there is not a single member taking loan more than once. So, member id column can also be dropped along with the id column.","127e62b9":"** Violin-plot of TARGET classes with distribution of loan amount differentiated by the terms. **","1f631759":"First, I'll be converting the date object columns into integer number of years or months just because I do not want to blow up the number of feature columns by performing one-hot encoding on them. For filling the null values I have taken the dates with the highest number of counts.","934f2798":"# Cleaning the data","a2885224":"# Understanding the data ","aae307ca":"So all the loans that have been defaulted are from individuals rather than from two or more people. ","5180ae09":"For boosting I will be using the [LightGBM](https:\/\/www.youtube.com\/watch?v=5CWwwtEM2TA) classifier (evalulation metric as AUC) along with [Kfold cross validation](https:\/\/www.youtube.com\/watch?v=TIgfjmp-4BA).","5c6bd8c1":"> Seeing the number of joint applicants in comparison to the the total applicants, it **isn't** significant enough to conclude that the loan taken by all Joint applicants are paid back. ","a0563ff0":"** From where do most of the loans tend to be defaulted? **","6d03b643":"Let us do make some Kernel Density Estimation Plots to see how the interest rate and debt to income ratio are distributed for the two classes in the TARGET column.","448644ab":"Freeing up the memory.","c137e4f9":"** Violin-plot of TARGET classes with distribution of loan amount differentiated by the application type. **","3dc3f899":"However for columns like 'total_rev_hi_lim','tot_col_ammnt',etc.\u00a0, I won't be filling in the missing data because they will certainly be of high feature importance due to their description. If they do not seem to be of high importance we can always re-iterate and fill the missing values later.\u00a0\n\nSo, dropping all remaining null values,"}}