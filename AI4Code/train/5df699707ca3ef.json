{"cell_type":{"fe5a904e":"code","43e479b7":"code","91833d69":"code","8ca8ec38":"code","5c51dc79":"code","279a398f":"code","3c64649b":"code","562535fd":"code","e75ae8c4":"code","f63face9":"code","b4ba5753":"code","db4f2de6":"code","ee48090e":"code","0450f6e6":"code","406e2fef":"code","f9e51dbb":"code","6469bc95":"code","020be0a6":"code","de4a659c":"code","51743716":"code","d1a82cb6":"code","a5630416":"code","5451adc7":"code","257612f3":"code","5ca95abb":"code","383836fc":"code","903810ca":"code","d2a5d47b":"code","8ac4bc5f":"code","4da354ee":"code","20a891c7":"code","57863780":"code","59ed57d0":"code","87690d78":"code","e46b991d":"code","a2a004fc":"code","b1b2b463":"code","634bdb5a":"code","42eab719":"code","04f0478f":"code","1c08d88d":"code","28fae8c3":"code","103d6701":"code","45d9d7d6":"code","40593ed0":"code","fc90e0ac":"code","5be187cd":"code","27821dc7":"code","b7dc8364":"code","746ce3b7":"code","3f4dd770":"code","47ed372a":"code","0d1b537e":"code","e8c9ff57":"code","4832bc05":"code","cb6120cb":"code","c8b0e433":"code","34fa78bf":"code","05acd9c7":"code","74300f3d":"code","f274bbaa":"code","29a30ebd":"code","cf5bf9e8":"code","54b18e4f":"code","9cb216ff":"code","cd992dec":"code","cc1c6ce1":"code","b172d258":"code","34e4b233":"code","0085ac36":"code","fa7a3d3f":"code","611205e8":"code","d84e34da":"code","a0371066":"code","1c317b70":"code","fed16ba1":"code","df7d359e":"code","8ca09d6a":"code","034661a6":"code","532c6f25":"code","3656c6cb":"code","679eef05":"code","6b25bd4f":"code","061bcd6a":"code","6ec96795":"code","f484fe68":"code","1a07de2d":"code","d65cbac1":"code","126739e1":"code","40f0293c":"code","0e29d615":"code","b9904158":"code","6b368d74":"code","e6d7b168":"code","0fafa989":"code","7c8d6e71":"markdown","146ad71d":"markdown","7a78f4c7":"markdown","ffb7d72d":"markdown","3da2b330":"markdown","b1b26d64":"markdown","5d183c3b":"markdown","5957cf63":"markdown","4e2fc400":"markdown"},"source":{"fe5a904e":"%pylab inline","43e479b7":"import pandas as pd\n#import numpy as np\n\nimport seaborn as sns\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict, train_test_split, GridSearchCV\n\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n\nimport os","91833d69":"print(os.listdir(\"..\/input\/ml-regression-2020\"))","8ca8ec38":"train = pd.read_csv('..\/input\/ml-regression-2020\/train.csv')\ntrain.head()","5c51dc79":"train.info()","279a398f":"# There are some differences in column names from data_dictionary\ntrain.columns.tolist()","3c64649b":"train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","562535fd":"test = pd.read_csv('..\/input\/ml-regression-2020\/test.csv')\ntest.head()","e75ae8c4":"test.info()","f63face9":"macro = pd.read_csv('..\/input\/ml-regression-2020\/macro.csv')\nmacro.columns.tolist()","b4ba5753":"#features with max amount of missing values\ntrain.isnull().sum().sort_values(ascending=False).head(20)","db4f2de6":"summ_null = train.isnull().sum()\/len(train)\nsumm_null[summ_null > 0.25]","ee48090e":"#flat features\nsumm_null=train.isnull().sum()\/len(train)\nsumm_null.head(13)","0450f6e6":"#subarea or raion:common features\ntrain.isnull().sum()['sub_area':'detention_facility_raion']","406e2fef":"#raion:population features\ntrain.isnull().sum()['full_all':'0_13_female']","f9e51dbb":"# raion:building features\ntrain.isnull().sum()['raion_build_count_with_material_info':'build_count_after_1995']","6469bc95":"# metro features\ntrain.isnull().sum()[84:89]","020be0a6":"train.isnull().sum()['green_part_500':'market_count_5000'].sort_values(ascending=False).head(30)","de4a659c":"train.loc[:,'green_part_500':'market_count_5000'].drop_duplicates()\n","51743716":"uninf_features = ['cafe_sum_500_min_price_avg','cafe_sum_500_max_price_avg', 'cafe_avg_price_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'mosque_count_1000', 'cafe_count_1500_price_high', 'mosque_count_1500', 'cafe_count_2000_price_high', 'mosque_count_2000']\ntrain.drop(uninf_features, axis=1).columns.tolist()\n#train.drop(train.loc[:,'green_part_500':'market_count_3000'].columns, axis=1).columns.tolist()","d1a82cb6":"num_rows = len(train.index)\nlow_information_cols = [] #\n\nfor col in train.columns:\n    cnts = train[col].value_counts(dropna=False)\n    top_pct = (cnts\/num_rows).iloc[0]\n    \n    if top_pct > 0.90:\n        low_information_cols.append(col)\n        print('{0}: {1:.5f}%'.format(col, top_pct*100))\n        print(cnts)\n        print()\n","a5630416":"uninf_features = ['cafe_count_500_price_4000','cafe_count_500_price_high', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'mosque_count_1000', 'cafe_count_1500_price_high', 'mosque_count_1500', 'cafe_count_2000_price_high', 'mosque_count_2000']\ntrain.drop(uninf_features, axis=1).columns.tolist()\n#train.drop(train.loc[:,'green_part_500':'market_count_3000'].columns, axis=1).columns.tolist()","5451adc7":"macro.isnull().sum().sort_values(ascending=False).head(20)","257612f3":"macro[['timestamp','rent_price_4+room_bus',\n 'rent_price_3room_bus',\n 'rent_price_2room_bus',\n 'rent_price_1room_bus',\n 'rent_price_3room_eco',\n 'rent_price_2room_eco',\n 'rent_price_1room_eco']].dropna().drop_duplicates()","5ca95abb":"plt.plot(macro['timestamp'],macro['rent_price_2room_eco'])","383836fc":"macro['rent_price_2room_eco'].drop_duplicates()","903810ca":"train.dtypes.value_counts()","d2a5d47b":"train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","8ac4bc5f":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in train:\n    if train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(train[col])\n            # Transform both training and testing data\n            train[col] = le.transform(train[col])\n            test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)","4da354ee":"unique(train['ecology'].tolist())","20a891c7":"train['ecology'].replace({'no data': np.nan}, inplace = True)\ntrain['ecology'].replace({'poor': 2.}, inplace = True)\ntrain['ecology'].replace({'satisfactory': 3.}, inplace = True)\ntrain['ecology'].replace({'good': 4.}, inplace = True)\ntrain['ecology'].replace({'excellent': 5.}, inplace = True)\nunique(train['ecology'].tolist())\n\ntest['ecology'].replace({'no data': np.nan}, inplace = True)\ntest['ecology'].replace({'poor': 2.}, inplace = True)\ntest['ecology'].replace({'satisfactory': 3.}, inplace = True)\ntest['ecology'].replace({'good': 4.}, inplace = True)\ntest['ecology'].replace({'excellent': 5.}, inplace = True)\nunique(test['ecology'].tolist())","57863780":"train['timestamp'] = pd.to_datetime(train['timestamp'])\ntrain['year'] = train['timestamp'].dt.year\ntrain['month'] = train['timestamp'].dt.month\n#train['weekday'] = train['timestamp'].dt.weekday\ntrain=train.drop('timestamp', axis=1)\n\ntest['timestamp'] = pd.to_datetime(test['timestamp'])\ntest['year'] = test['timestamp'].dt.year\ntest['month'] = test['timestamp'].dt.month\n#test['weekday'] = train['timestamp'].dt.weekday\ntest=test.drop('timestamp', axis=1)","59ed57d0":"train_raion = train.loc[:, 'sub_area':'build_count_after_1995']\ntrain_raion = train_raion.drop_duplicates()\ntrain_raion","87690d78":"# features with max amount of missing values\nmis = train_raion.isnull().sum()\nmis.sort_values(ascending=False).head(20)","e46b991d":"#feature 'hospital_beds_raion' is need to be dropped\ntrain=train.drop('hospital_beds_raion', axis=1)\ntest=test.drop('hospital_beds_raion', axis=1)","a2a004fc":"train.loc[:, 'area_m':'build_count_after_1995'].drop_duplicates()","b1b2b463":"train_raion_built = pd.merge(train['sub_area'], train.loc[:, 'raion_build_count_with_material_info':'build_count_after_1995'], left_index = True, right_index = True)\ntrain_raion_built = train_raion_built.drop_duplicates()\ntrain_raion_built","634bdb5a":"# idx = train_raion_built.index[train_raion_built.isnull().any(1)]\n# nans = train_raion_built.iloc[idx]\n#sub_area_with_out_inf_build = unique(nans['sub_area'].tolist())\n#unique(sub_area_with_out_inf_build)\n#sub_area_with_out_inf_build","42eab719":"pylab.rcParams['figure.figsize'] = (8, 4)  # default size of all figures\n\ndef pairplot(df, target):\n    ncol, nrow = 4, df.shape[1] \/\/ 4 + (df.shape[1] % 4 > 0)\n    plt.figure(figsize=(ncol * 4, nrow * 4))\n\n    for i, feature in enumerate(df.columns):\n        plt.subplot(nrow, ncol, i + 1)\n        plt.scatter(df[feature], target, s=10, marker='o', alpha=.6)\n        plt.xlabel(feature)\n        if i % ncol == 0:\n            plt.ylabel('target')","04f0478f":"pairplot(train, train['price'])","1c08d88d":"train[train['full_sq']>5000]","28fae8c3":"train['full_sq'].replace({5326: np.nan}, inplace = True)","103d6701":"train['life_sq']=train['life_sq'].apply(lambda x: x if x < 300 else np.nan)","45d9d7d6":"plt.scatter(train['life_sq'], train['price'])","40593ed0":"pairplot(train.loc[:, 'full_sq':'build_count_after_1995'], train['price'])","fc90e0ac":"train[train['price']>80000000].loc[:, 'full_sq':'sub_area']","5be187cd":"train[train['max_floor']>60].loc[:, 'full_sq':'sub_area']","27821dc7":"train['max_floor'].replace({99.0: np.nan}, inplace = True)","b7dc8364":"plt.scatter(train['max_floor'], train['price'])","746ce3b7":"train[train['build_year']>2200].loc[:, 'full_sq':'sub_area']","3f4dd770":"train['build_year']=train['build_year'].apply(lambda x: x if x > 1800 else np.nan)\ntrain['build_year'].replace({20052009.0: 2005.0}, inplace = True)","47ed372a":"plt.scatter(train['build_year'], train['price'])","0d1b537e":"train[train['num_room']>10].loc[:, 'full_sq':'sub_area']","e8c9ff57":"train[train['num_room']==0].loc[:, 'full_sq':'sub_area']","4832bc05":"train['num_room'].replace({19.0: np.nan}, inplace = True)\n#train['num_room'].replace({0.0: np.nan}, inplace = True)","cb6120cb":"plt.scatter(train['num_room'], train['price'])","c8b0e433":"train[train['kitch_sq']>500].loc[:, 'full_sq':'sub_area']","34fa78bf":"train['kitch_sq']=train['kitch_sq'].apply(lambda x: x if x < 500 else np.nan)","05acd9c7":"plt.scatter(train['kitch_sq'], train['price'])","74300f3d":"train[train['kitch_sq']>=train['full_sq']].loc[:, 'full_sq':'sub_area']","f274bbaa":"train[train['state']>30].loc[:, 'full_sq':'sub_area']","29a30ebd":"train['state'].replace({33.0: np.nan}, inplace = True)","cf5bf9e8":"plt.scatter(train['state'], train['price'])","54b18e4f":"pairplot(train.loc[:, 'full_sq':'build_count_after_1995'], train['price'])","9cb216ff":"key = ['full_sq', 'life_sq', 'floor', 'build_year', 'num_room', 'sub_area', 'price']\ntrain = train.drop_duplicates(subset=key)\ntrain","cd992dec":"# Find correlations with the target and sort\ntrain.loc[:, 'full_sq':'build_count_after_1995']\ncorrelations = train.corr()['price'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","cc1c6ce1":"train[train['full_sq']>300]","b172d258":"train['full_sq']=train['full_sq'].apply(lambda x: x if x < 300 else np.nan)","34e4b233":"plt.scatter(train['full_sq'], train['price'])","0085ac36":"train[train['full_sq']<9]","fa7a3d3f":"train['full_sq']=train['full_sq'].apply(lambda x: x if x >=9 else np.nan)\nplt.scatter(train['full_sq'], train['price'])","611205e8":"# Find correlations with the target and sort\n# tr=pd.merge(train.loc[:, 'timestamp':'build_count_after_1995'],train['price'], left_index = True, right_index = True)\ncorrelations = train.corr()['price'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","d84e34da":"model1 = train[['full_sq', 'sport_count_5000', 'trc_sqm_5000', 'office_sqm_5000','zd_vokzaly_avto_km', 'price']]\nmed1 = model1.median()\nmodel1 = model1.fillna(med1)\nmodel1","a0371066":"from sklearn.model_selection import train_test_split, GridSearchCV\nx_train, x_test, y_train, y_test = train_test_split(model1.drop('price',axis=1), model1['price'], train_size=0.8, random_state=42)","1c317b70":"from sklearn.metrics import mean_absolute_error\n\nlin = LinearRegression().fit(x_train,y_train)\nerror = mean_absolute_error(y_test, lin.predict(x_test))\nprint('ERROR = %.4f' %error)","fed16ba1":"las = Lasso().fit(x_train,y_train)\nprint('ERROR = %.4f' %mean_absolute_error(y_test, lin.predict(x_test)))","df7d359e":"from sklearn.preprocessing import StandardScaler\n#from sklearn.preprocessing import MinMaxScaler\nstandart = StandardScaler()\nstandart.fit(x_train)\nx_train_std = standart.transform(x_train)\nlin_std = Ridge().fit(x_train_std,y_train)\nx_test_std = standart.transform(x_test)\nerror = mean_absolute_error(y_test, lin_std.predict(x_test_std))\nprint('ERROR = %.4f' %error)","8ca09d6a":"from sklearn.ensemble import GradientBoostingRegressor\n\nmodel2 = pd.merge(train.loc[:, 'full_sq':'0_13_female'],train.loc[:, 'green_part_5000':'price'], left_index = True, right_index = True)\nmodel2 = model2.drop('sub_area',axis=1)\nmodel2 = model2.dropna()\nx_train, x_test, y_train, y_test = train_test_split(model2.drop('price',axis=1), model2['price'], train_size=0.8, random_state=42)\nstd = StandardScaler()\nstd.fit(x_train)\nx_train_std = std.transform(x_train)\nlin_std = GradientBoostingRegressor(criterion = 'mae').fit(x_train_std,y_train)\nx_test_std = std.transform(x_test)\nerror = mean_absolute_error(y_test, lin_std.predict(x_test_std))\nprint('ERROR = %.4f' %error)","034661a6":"kf = KFold(n_splits = 4, shuffle=True, random_state = 0)\nlin = LinearRegression()\nmodel2 = pd.merge(train.loc[:, 'full_sq':'0_13_female'],train.loc[:, 'green_part_5000':'price'], left_index = True, right_index = True)\nmodel2 = model2.drop('sub_area',axis=1)\nmodel2 = model2.dropna()\nX = model2.drop('price',axis=1)\nY = model2['price']\nmedian(cross_val_score(lin, X, Y, scoring='neg_mean_absolute_error',cv=kf))","532c6f25":"model3 = pd.merge(train.loc[:, 'full_sq':'0_13_female'],train.loc[:, 'green_part_5000':'price'], left_index = True, right_index = True)\nmodel3.isnull().sum().sort_values(ascending=False).head(20)","3656c6cb":"col_to_drop = ['build_year', 'state', 'kitch_sq', 'max_floor', 'num_room', 'material', 'preschool_quota', 'school_quota', 'life_sq', 'cafe_sum_5000_max_price_avg', 'cafe_sum_5000_min_price_avg']\nmodel3=model3.drop(col_to_drop,axis=1)\nmodel3","679eef05":"model3.isnull().sum().sort_values(ascending=False).head(20)","6b25bd4f":"med = model3.median()\nmodel3 = model3.fillna(med)\nmodel3","061bcd6a":"x_train, x_test, y_train, y_test = train_test_split(model3.drop(['sub_area', 'price'],axis=1), model3['price'], train_size=0.8, random_state=42)","6ec96795":"X = model2.drop('price',axis=1)\nY = model2['price']\nlas = Lasso()\nmedian(cross_val_score(las, X, Y, scoring='neg_mean_absolute_error',cv=kf))","f484fe68":"from sklearn.linear_model import RidgeCV\n\nl = []\nfor p in arange(-6,7,1):\n    l.append(10.**p)\n\nregRidge = RidgeCV(alphas = l)\nmedian(cross_val_score(las, X, Y, scoring='neg_mean_absolute_error',cv=kf))","1a07de2d":"std = StandardScaler()\nstd.fit(x_train)\nx_train_std = std.transform(x_train)\n#lin_std = GradientBoostingRegressor(criterion = 'mae').fit(x_train_std,y_train)","d65cbac1":"forest = RandomForestRegressor()\nmedian(cross_val_score(forest, X, Y, scoring='neg_mean_absolute_error',cv=kf))","126739e1":"x_test_std = std.transform(x_test)\nerror = mean_absolute_error(y_test, lin_std.predict(x_test_std))\nprint('ERROR = %.4f' %error)","40f0293c":"x_test = pd.merge(test.loc[:, 'full_sq':'0_13_female'],test.loc[:, 'green_part_5000':'market_count_5000'], left_index = True, right_index = True)\nx_test.isnull().sum().sort_values(ascending=False).head(20)","0e29d615":"x_test=x_test.drop(col_to_drop,axis=1)\nx_test","b9904158":"x_test.isnull().sum().sort_values(ascending=False).head(20)","6b368d74":"med2 = x_test.median()\nx_test = x_test.fillna(med2)\nx_test","e6d7b168":"x_test=x_test.drop('sub_area', axis=1)\nx_test_std = std.transform(x_test)\nx_test_std","0fafa989":"lin_std.predict(x_test_std)","7c8d6e71":"### macro","146ad71d":"## Anomalies","7a78f4c7":"### Train","ffb7d72d":"# Examine Missing Values","3da2b330":"# Column Types","b1b26d64":"## I have divided features in a few groups: flat, raion and location","5d183c3b":"# Encoding Categorical Variables","5957cf63":"### Submission","4e2fc400":"## column 'sub_area' have match with the combination of columns ['area_m':'build_count_after_1995'] -> can be dropped"}}