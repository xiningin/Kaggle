{"cell_type":{"f130c225":"code","caae5f82":"code","5dc7ba20":"code","131db056":"code","386ec03c":"code","d4a499b0":"code","18ec44fb":"code","add5f7f4":"code","2ebb8337":"code","d3d7e229":"code","5e7a853d":"code","52f5c032":"code","1f113a1f":"code","55f0415f":"code","253301b5":"code","e1a6eb0f":"code","f5c0d52e":"code","9613b8e9":"code","bd1a333d":"code","072b50fc":"code","1479b398":"code","b36d90c9":"code","b7841904":"code","7b1281e4":"code","138332fe":"code","ae0d9824":"code","f43b7446":"code","f950f776":"code","45a16948":"code","8dcfa7a1":"code","31ee6622":"code","43594064":"code","655c9a19":"code","e84a178f":"code","170ccefd":"code","34fd74fc":"code","0a23e4c0":"code","a2dd9611":"code","5c01d90a":"code","4a52c6e1":"code","aa434dd4":"code","fc426d8a":"code","af5fd811":"code","8c6731b5":"code","8046445b":"code","f26388fe":"code","3d395ea1":"code","af1b2505":"code","b25aeca9":"code","c7a5895a":"code","efea3005":"code","9615b040":"code","1274087b":"code","9ea76e14":"code","0f0d4924":"code","6566942d":"code","8c8930e2":"code","5d37f9e9":"code","40190612":"code","fe136167":"code","941a5ed1":"code","7ddc13db":"code","7822b731":"code","2e090c42":"code","043844c6":"code","a994fb14":"code","f3b9516e":"code","f2a6c0bc":"code","279a40a5":"code","b5723aff":"code","59d574e7":"code","66051aaf":"code","5ff0d22b":"code","2ec8989f":"code","90ae34a1":"code","8ff10e83":"code","e8ada9e7":"code","f309cda3":"code","e59dbbe3":"code","f38573dd":"code","4042c80f":"code","8a00b503":"code","ff1d3f3c":"code","43284269":"code","b77c05b4":"code","5350bf91":"code","193c6d0a":"code","041c9c2e":"code","65ea6157":"code","9ce7c9c0":"code","f964cfdc":"code","814feb8b":"code","e92d9a97":"code","20254fce":"code","affbf966":"markdown","936e9876":"markdown","c0bbac0a":"markdown","b5a0ea7c":"markdown","c4228cf9":"markdown","113bfd84":"markdown","d35a847a":"markdown","a0a433ae":"markdown","97784932":"markdown"},"source":{"f130c225":"!pip install chart_studio\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n# Set seeds to make the experiment more reproducible.\nimport tensorflow as tf\nfrom numpy.random import seed\ntf.random.set_seed(1)\nseed(1)","caae5f82":"import numpy as np \ndata = pd.read_csv('..\/input\/climate-hour\/climate_hour.csv', parse_dates=['Date Time'],index_col = 0, header=0)\ndata = data.sort_values(['Date Time'])\ndata.head() ","5dc7ba20":"new_data = data['T (degC)']\n#new_data = new_data.array.reshape(-1, 1 )                                 \nnew_data = pd.DataFrame({'Date Time': data.index, 'T (degC)':new_data.values})\nnew_data = new_data.set_index(['Date Time'])\nnew_data.head()","131db056":"from sklearn.preprocessing import MinMaxScaler\ntemp_scaler = MinMaxScaler()\ntemp_scaler.fit(new_data) \nnormalized_temp = temp_scaler.transform(new_data) ","386ec03c":"normalized_temp = pd.DataFrame(normalized_temp, columns=['Normalized Temperature'])\nnormalized_temp.index = new_data.index\nnormalized_temp.head()","d4a499b0":"data_scaler = MinMaxScaler()\ndata_scaler.fit(data) \nnormalized_data = data_scaler.transform(data) \n\nnormalized_df = pd.DataFrame(normalized_data, columns=['p (mbar)','T (degC)','Tpot (K)','Tdew (degC)','rh (%)','VPmax (mbar)','VPact (mbar)','VPdef (mbar)','sh (g\/kg)','H2OC (mmol\/mol)','rho (g\/m**3)','wv (m\/s)','max. wv (m\/s)','wd (deg)'])\nnormalized_df = normalized_df.set_index(data.index)\nnormalized_df.head()","18ec44fb":"def series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Target timestep (t=lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","add5f7f4":"window = 24\nseries = series_to_supervised(normalized_df, window=window)\nseries.head()","2ebb8337":"print(series.values.shape)\nprint(np.isnan(series.values).any())","d3d7e229":"# Label #52517\n#labels_col = 'T (degC)(t+%d)' % lag_size\nlabels_col = 'T (degC)(t)'\nlabels = series[labels_col]\nseries = series.drop(labels_col, axis=1)\nX_train = series['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nX_valid = series['01.01.2015 00:00:00':'2017-01-01 00:00:00'] \nY_train = labels['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nY_valid = labels['01.01.2015 00:00:00':'2017-01-01 00:00:00']\nprint('Train set shape', X_train.shape)\nprint('Validation set shape', X_valid.shape)","5e7a853d":"epochs = 10\nbatch = 254\nlr = 0.0003\nadam = optimizers.Adam(lr)","52f5c032":"import time\nname = \"model-mlp{}\".format(int(time.time()))","1f113a1f":"model_mlp = Sequential()\nmodel_mlp.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\nmodel_mlp.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\nmodel_mlp.add(Dense(1))","55f0415f":"model_mlp.compile(loss='mae', optimizer=adam, metrics=['mse','accuracy']) \nmodel_mlp.summary()\n#Saving the model :\nmodel_mlp.save(name)   ","253301b5":"mlp_history = model_mlp.fit(X_train.values, Y_train, validation_data=(X_valid.values, Y_valid), epochs=epochs, verbose=2)\n#Saving history in a csv file :\nhist_df = pd.DataFrame(mlp_history.history) \nhist_csv_file = 'mlp-history-{}.csv'.format(int(time.time()))\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)             ","e1a6eb0f":"import matplotlib.pyplot as plt \nmlp_train_loss = mlp_history.history['loss']\nmlp_test_loss = mlp_history.history['val_loss']\n\nepoch_count = range(1, len(mlp_train_loss)+1)\n\nplt.plot(epoch_count, mlp_train_loss)\nplt.plot(epoch_count, mlp_test_loss)\nplt.title('loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.xlabel('Loss')\nplt.show()","f5c0d52e":"#ajout\u00e9 :\nmlp_train_pred = model_mlp.predict(X_train.values)\nmlp_valid_pred = model_mlp.predict(X_valid.values)\n\nprint('Train rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_train, mlp_train_pred)))\nprint('Validation rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_valid, mlp_valid_pred)))","9613b8e9":"from sklearn.metrics import mean_absolute_error\nprint('Train mae (avec normalisation):', mean_absolute_error(Y_train, mlp_train_pred))\nprint('Validation mae (avec normalisation):', mean_absolute_error(Y_valid, mlp_valid_pred))","bd1a333d":"normalized_mlp_predictions = pd.DataFrame(Y_valid.values, columns=['Temperature'])\nnormalized_mlp_predictions.index = X_valid.index \nnormalized_mlp_predictions['Predicted Temperature'] = mlp_valid_pred\nnormalized_mlp_predictions.head()","072b50fc":"normalized_mlp_predictions.tail()","1479b398":"normalized_mlp_predictions.plot()","b36d90c9":"#y_valid = Y_valid.values\n#Y_valid_inverse = scaler.inverse_transform(y_valid.reshape(-1,1))\ny_val_dataset = np.zeros(shape=(len(mlp_valid_pred), 14) )\ny_val_dataset[:,1] = normalized_mlp_predictions['Temperature'] \ny_val_inv = data_scaler.inverse_transform(y_val_dataset)[:,1]\nprint(y_val_inv)","b7841904":"y_val = new_data['01.01.2015 00:00:00':'2016-12-31 23:00:00']\nprint(y_val)","7b1281e4":"print(mlp_valid_pred)\nprint(normalized_mlp_predictions['Predicted Temperature'])","138332fe":"pred_dataset = np.zeros(shape=(len(mlp_valid_pred), 14) )\npred_dataset[:,1] = mlp_valid_pred.reshape(17470) #normalized_mlp_predictions['Predicted Temperature'] \ny_pred_inv = data_scaler.inverse_transform(pred_dataset)[:,1]","ae0d9824":"mlp_predictions = pd.DataFrame(y_val.values, columns=['True Temperature'])\n#mlp_predictions['Date Time'] = dates[52565:] \nmlp_predictions.index = y_val.index #new_data['31.12.2014 23:00:00':'2017-01-01 00:00:00'].index #X_valid.index \nmlp_predictions['Predicted Temperature'] = y_pred_inv\nprint(mlp_predictions)","f43b7446":"mlp_predictions.to_csv('new-mlp-predictions.csv')","f950f776":"from sklearn.metrics import mean_absolute_error\nprint('Validation mae (sans normalisation):', mean_absolute_error(mlp_predictions['True Temperature'], mlp_predictions['Predicted Temperature']))","45a16948":"mlp_predictions.plot()","8dcfa7a1":"import matplotlib.pyplot as plt \nmlp_true_temp = mlp_predictions['True Temperature']\ntime_stamp = mlp_predictions.index\nplt.plot(time_stamp, mlp_true_temp)\nplt.xlabel('Time')\nplt.ylabel('True Temperature C\u00b0')\nplt.show() ","31ee6622":"import matplotlib.pyplot as plt \nmlp_pred_temp = mlp_predictions['Predicted Temperature']\ntime_stamp = mlp_predictions.index\nplt.plot(time_stamp, mlp_pred_temp)\nplt.xlabel('Time')\nplt.xlabel('Predicted Temperature C\u00b0')\nplt.show() ","43594064":"X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\nprint('Train set shape', X_train_series.shape)\nprint('Validation set shape', X_valid_series.shape)","655c9a19":"model_cnn = Sequential()\nmodel_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(50, activation='relu'))\nmodel_cnn.add(Dense(1))\nmodel_cnn.compile(loss='mae', optimizer=adam, metrics=['mse','accuracy']) #'mse'\nmodel_cnn.summary()","e84a178f":"cnn_history = model_cnn.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=epochs, verbose=2)","170ccefd":"#Saving history in a csv file :\ncnn_hist_df = pd.DataFrame(cnn_history.history) \ncnn_hist_csv_file = 'cnn-history-{}.csv'.format(int(time.time()))\nwith open(cnn_hist_csv_file, mode='w') as f:\n    cnn_hist_df.to_csv(f) ","34fd74fc":"import matplotlib.pyplot as plt \ncnn_train_loss = cnn_history.history['loss']\ncnn_test_loss = cnn_history.history['val_loss']\n\nepoch_count = range(1, len(cnn_train_loss)+1)\n\nplt.plot(epoch_count, cnn_train_loss)\nplt.plot(epoch_count, cnn_test_loss)\nplt.title('loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.xlabel('Accuracy')\nplt.show()","0a23e4c0":"#Normalized predictions:\ncnn_train_pred = model_cnn.predict(X_train_series)\ncnn_valid_pred = model_cnn.predict(X_valid_series)\n\nprint('Train rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\nprint('Validation rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))","a2dd9611":"from sklearn.metrics import mean_absolute_error\nprint('Train mae (avec normalisation):', mean_absolute_error(Y_train, cnn_train_pred))\nprint('Validation mae (avec normalisation):', mean_absolute_error(Y_valid, cnn_valid_pred))","5c01d90a":"normalized_cnn_predictions = pd.DataFrame(Y_valid.values, columns=['Temperature'])\nnormalized_cnn_predictions.index = X_valid.index \nnormalized_cnn_predictions['Predicted Temperature'] = cnn_valid_pred\nnormalized_cnn_predictions.head()","4a52c6e1":"normalized_cnn_predictions.tail()","aa434dd4":"normalized_cnn_predictions.plot()","fc426d8a":"print(cnn_valid_pred)\nprint(normalized_cnn_predictions['Temperature'] )","af5fd811":"y_val_cnn_dataset = np.zeros(shape=(len(cnn_valid_pred), 14) )\ny_val_cnn_dataset[:,1] = normalized_cnn_predictions['Temperature']\ny_val_inv_cnn = data_scaler.inverse_transform(y_val_cnn_dataset)[:,1]\nprint(y_val_inv_cnn)","8c6731b5":"pred_cnn_dataset = np.zeros(shape=(len(cnn_valid_pred), 14) )\npred_cnn_dataset[:,1] = cnn_valid_pred.reshape(17470)\ny_pred_inv_cnn = data_scaler.inverse_transform(pred_cnn_dataset)[:,1]","8046445b":"cnn_predictions = pd.DataFrame(y_val.values, columns=['True Temperature'])\ncnn_predictions.index = y_val.index \ncnn_predictions['Predicted Temperature'] = y_pred_inv_cnn\nprint(cnn_predictions)","f26388fe":"cnn_predictions.to_csv('new-cnn-predictions.csv')","3d395ea1":"from sklearn.metrics import mean_absolute_error\nprint('Validation mae (sans normalisation):', mean_absolute_error(cnn_predictions['True Temperature'], cnn_predictions['Predicted Temperature']))","af1b2505":"cnn_predictions.plot()","b25aeca9":"import matplotlib.pyplot as plt \ncnn_true_temp = cnn_predictions['True Temperature']\ntime_stamp = cnn_predictions.index\nplt.plot(time_stamp, cnn_true_temp)\nplt.xlabel('Time')\nplt.ylabel('True Temperature C\u00b0')\nplt.show() ","c7a5895a":"import matplotlib.pyplot as plt \ncnn_pred_temp = cnn_predictions['Predicted Temperature']\ntime_stamp = cnn_predictions.index\nplt.plot(time_stamp, cnn_pred_temp)\nplt.xlabel('Time')\nplt.xlabel('Predicted Temperature C\u00b0')\nplt.show() ","efea3005":"X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\nprint('Train set shape', X_train_series.shape)\nprint('Validation set shape', X_valid_series.shape)","9615b040":"from keras.regularizers import l1\n# instantiate regularizer\nreg = l1(0.001)","1274087b":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2]),activity_regularizer=l1(0.001)))\nmodel_lstm.add(Dense(1)) #, activation='softmax')  #(Dense(5, activation='softmax')) #, activation='relu'\nmodel_lstm.compile(loss='mae', optimizer=adam, metrics=['mse','accuracy']) #'mse'\nmodel_lstm.summary()","9ea76e14":"print(X_train_series.shape)\nprint(X_train_series.shape[1], X_train_series.shape[2])\nprint(X_train_series.shape[1], X_train_series.shape[2])","0f0d4924":"lstm_history = model_lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=10, verbose=2)","6566942d":"print(np.isnan(Y_train).any())","8c8930e2":"#Saving history in a csv file :\nlstm_hist_df = pd.DataFrame(lstm_history.history) \nlstm_hist_csv_file = 'lstm-history-{}.csv'.format(int(time.time()))\nwith open(lstm_hist_csv_file, mode='w') as f:\n    lstm_hist_df.to_csv(f) ","5d37f9e9":"import matplotlib.pyplot as plt \nlstm_train_loss = lstm_history.history['loss']\nlstm_test_loss = lstm_history.history['val_loss']\n\nepoch_count = range(1, len(lstm_train_loss)+1)\n\nplt.plot(epoch_count, lstm_train_loss)\nplt.plot(epoch_count, lstm_test_loss)\nplt.title('loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.xlabel('Accuracy')\nplt.show()","40190612":"#Normalized predictions:\nlstm_train_pred = model_lstm.predict(X_train_series)\nlstm_valid_pred = model_lstm.predict(X_valid_series)\n\nprint('Train rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\nprint('Validation rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_valid, lstm_valid_pred)))","fe136167":"from sklearn.metrics import mean_absolute_error\nprint('Train mae (avec normalisation):', mean_absolute_error(Y_train, lstm_train_pred))\nprint('Validation mae (avec normalisation):', mean_absolute_error(Y_valid, lstm_valid_pred))","941a5ed1":"normalized_lstm_predictions = pd.DataFrame(Y_valid.values, columns=['Temperature'])\nnormalized_lstm_predictions.index = X_valid.index \nnormalized_lstm_predictions['Predicted Temperature'] = lstm_valid_pred\nnormalized_lstm_predictions.head()","7ddc13db":"normalized_lstm_predictions.tail()","7822b731":"normalized_lstm_predictions.plot()","2e090c42":"print(lstm_valid_pred)\nprint(normalized_lstm_predictions['Temperature'] )","043844c6":"y_val_lstm_dataset = np.zeros(shape=(len(lstm_valid_pred), 14) )\ny_val_lstm_dataset[:,1] = normalized_lstm_predictions['Temperature']\ny_val_inv_lstm = data_scaler.inverse_transform(y_val_lstm_dataset)[:,1]\nprint(y_val_inv_lstm)","a994fb14":"pred_lstm_dataset = np.zeros(shape=(len(lstm_valid_pred), 14) )\npred_lstm_dataset[:,1] = lstm_valid_pred.reshape(17470)\ny_pred_inv_lstm = data_scaler.inverse_transform(pred_lstm_dataset)[:,1]","f3b9516e":"lstm_predictions = pd.DataFrame(y_val.values, columns=['True Temperature'])\nlstm_predictions.index = y_val.index \nlstm_predictions['Predicted Temperature'] = y_pred_inv_lstm\nprint(lstm_predictions)","f2a6c0bc":"lstm_predictions.to_csv('new-lstm-predictions.csv') ","279a40a5":"from sklearn.metrics import mean_absolute_error\nprint('Validation mae (sans normalisation):', mean_absolute_error(lstm_predictions['True Temperature'], lstm_predictions['Predicted Temperature']))","b5723aff":"lstm_predictions.plot()","59d574e7":"import matplotlib.pyplot as plt \nlstm_true_temp = lstm_predictions['True Temperature']\ntime_stamp = lstm_predictions.index\nplt.plot(time_stamp, lstm_true_temp)\nplt.xlabel('Time')\nplt.ylabel('True Temperature C\u00b0')\nplt.show() ","66051aaf":"import matplotlib.pyplot as plt \nlstm_pred_temp = lstm_predictions['Predicted Temperature']\ntime_stamp = lstm_predictions.index\nplt.plot(time_stamp, lstm_pred_temp)\nplt.xlabel('Time')\nplt.xlabel('Predicted Temperature C\u00b0')\nplt.show() ","5ff0d22b":"X_train","2ec8989f":"#original\n'''\nsubsequences = 5\ntimesteps = X_train_series.shape[1]\/\/subsequences\nX_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))\nX_valid_series_sub = X_valid_series.reshape((X_valid_series.shape[0], subsequences, timesteps, 1))\nprint('Train set shape', X_train_series_sub.shape)\nprint('Validation set shape', X_valid_series_sub.shape)\n'''","90ae34a1":"subsequences = 1\ntimesteps = X_train_series.shape[1]\/\/subsequences\nX_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))\nX_valid_series_sub = X_valid_series.reshape((X_valid_series.shape[0], subsequences, timesteps, 1))\nprint('Train set shape', X_train_series_sub.shape)\nprint('Validation set shape', X_valid_series_sub.shape)","8ff10e83":"model_cnn_lstm = Sequential()\nmodel_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\nmodel_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\nmodel_cnn_lstm.add(TimeDistributed(Flatten()))\nmodel_cnn_lstm.add(LSTM(50, activation='relu'))\nmodel_cnn_lstm.add(Dense(1))\nmodel_cnn_lstm.compile(loss='mae', optimizer=adam, metrics=['mse','accuracy']) #'mse'","e8ada9e7":"cnn_lstm_history = model_cnn_lstm.fit(X_train_series_sub, Y_train, validation_data=(X_valid_series_sub, Y_valid), epochs=epochs, verbose=2)","f309cda3":"#Saving history in a csv file :\ncnn_lstm_hist_df = pd.DataFrame(cnn_lstm_history.history) \ncnn_lstm_hist_csv_file = 'cnn-lstm-history-{}.csv'.format(int(time.time()))\nwith open(cnn_lstm_hist_csv_file, mode='w') as f:\n    cnn_lstm_hist_df.to_csv(f) ","e59dbbe3":"import matplotlib.pyplot as plt \ncnn_lstm_train_loss = cnn_lstm_history.history['loss']\ncnn_lstm_test_loss = cnn_lstm_history.history['val_loss']\n\nepoch_count = range(1, len(cnn_lstm_train_loss)+1)\n\nplt.plot(epoch_count, cnn_lstm_train_loss)\nplt.plot(epoch_count, cnn_lstm_test_loss)\nplt.title('loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.xlabel('History')\nplt.show()","f38573dd":"#Normalized predictions:\ncnn_lstm_train_pred = model_cnn_lstm.predict(X_train_series_sub)\ncnn_lstm_valid_pred = model_cnn_lstm.predict(X_valid_series_sub)\n\nprint('Train rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_train, cnn_lstm_train_pred)))\nprint('Validation rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_valid, cnn_lstm_valid_pred)))","4042c80f":"from sklearn.metrics import mean_absolute_error\nprint('Train mae (avec normalisation):', mean_absolute_error(Y_train, cnn_lstm_train_pred))\nprint('Validation mae (avec normalisation):', mean_absolute_error(Y_valid, cnn_lstm_valid_pred))","8a00b503":"normalized_cnn_lstm_predictions = pd.DataFrame(Y_valid.values, columns=['Temperature'])\nnormalized_cnn_lstm_predictions.index = X_valid.index \nnormalized_cnn_lstm_predictions['Predicted Temperature'] = cnn_lstm_valid_pred\nnormalized_cnn_lstm_predictions.head()","ff1d3f3c":"normalized_cnn_lstm_predictions.tail()","43284269":"normalized_cnn_lstm_predictions.plot()","b77c05b4":"print(cnn_lstm_valid_pred)\nprint(normalized_cnn_lstm_predictions['Temperature'] )","5350bf91":"y_val_cnn_lstm_dataset = np.zeros(shape=(len(cnn_lstm_valid_pred), 14) )\ny_val_cnn_lstm_dataset[:,1] = normalized_cnn_lstm_predictions['Temperature']\ny_val_inv_cnn_lstm = data_scaler.inverse_transform(y_val_cnn_lstm_dataset)[:,1]\nprint(y_val_inv_cnn_lstm)","193c6d0a":"pred_cnn_lstm_dataset = np.zeros(shape=(len(cnn_lstm_valid_pred), 14) )\npred_cnn_lstm_dataset[:,1] = cnn_lstm_valid_pred.reshape(17470)\ny_pred_inv_cnn_lstm = data_scaler.inverse_transform(pred_cnn_lstm_dataset)[:,1]","041c9c2e":"cnn_lstm_predictions = pd.DataFrame(y_val.values, columns=['True Temperature'])\ncnn_lstm_predictions.index = y_val.index \ncnn_lstm_predictions['Predicted Temperature'] = y_pred_inv_cnn_lstm\nprint(cnn_lstm_predictions)","65ea6157":"cnn_lstm_predictions.to_csv('new-cnn-lstm-predictions.csv') ","9ce7c9c0":"from sklearn.metrics import mean_absolute_error\nprint('Validation mae (sans normalisation):', mean_absolute_error(cnn_lstm_predictions['True Temperature'], cnn_lstm_predictions['Predicted Temperature']))","f964cfdc":"cnn_lstm_predictions.plot()","814feb8b":"import matplotlib.pyplot as plt \ncnn_lstm_true_temp = cnn_lstm_predictions['True Temperature']\ntime_stamp = cnn_lstm_predictions.index\nplt.plot(time_stamp, cnn_lstm_true_temp)\nplt.xlabel('Time')\nplt.ylabel('True Temperature C\u00b0')\nplt.show() ","e92d9a97":"import matplotlib.pyplot as plt \ncnn_lstm_pred_temp = cnn_lstm_predictions['Predicted Temperature']\ntime_stamp = cnn_lstm_predictions.index\nplt.plot(time_stamp, cnn_lstm_pred_temp)\nplt.xlabel('Time')\nplt.xlabel('Predicted Temperature C\u00b0')\nplt.show() ","20254fce":"fig, axes = plt.subplots(2, 2, sharex=True, sharey=True,figsize=(22,12))\nax1, ax2 = axes[0]\nax3, ax4 = axes[1]\n\nax1.plot(mlp_history.history['loss'], label='Train loss')\nax1.plot(mlp_history.history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('MLP')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('MSE')\n\nax2.plot(cnn_history.history['loss'], label='Train loss')\nax2.plot(cnn_history.history['val_loss'], label='Validation loss')\nax2.legend(loc='best')\nax2.set_title('CNN')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('MSE')\n\nax3.plot(lstm_history.history['loss'], label='Train loss')\nax3.plot(lstm_history.history['val_loss'], label='Validation loss')\nax3.legend(loc='best')\nax3.set_title('LSTM')\nax3.set_xlabel('Epochs')\nax3.set_ylabel('MSE')\n\nax4.plot(cnn_lstm_history.history['loss'], label='Train loss')\nax4.plot(cnn_lstm_history.history['val_loss'], label='Validation loss')\nax4.legend(loc='best')\nax4.set_title('CNN-LSTM')\nax4.set_xlabel('Epochs')\nax4.set_ylabel('MSE')\n\nplt.show()","affbf966":"**Comparing models**","936e9876":"**MLP for Time Series Forecasting**\n* First we will use a Multilayer Perceptron model or MLP model, here our model will have input features equal to the window size.\n* The thing with MLP models is that the model don't take the input as sequenced data, so for the model, it is just receiving inputs and don't treat them as sequenced data, that may be a problem since the model won't see the data with the sequence patter that it has.\n* Input shape **[samples, timesteps]**.","c0bbac0a":"**LSTM for Time Series Forecasting**\n* Now the LSTM model actually sees the input data as a sequence, so it's able to learn patterns from sequenced data (assuming it exists) better than the other ones, especially patterns from long sequences.\n* Input shape **[samples, timesteps, features]**.","b5a0ea7c":"***Train\/validation split***","c4228cf9":"**CNN-LSTM for Time Series Forecasting**\n* Input shape **[samples, subsequences, timesteps, features]**.","113bfd84":"**CNN for Time Series Forecasting**\n* For the CNN model we will use one convolutional hidden layer followed by a max pooling layer. The filter maps are then flattened before being interpreted by a Dense layer and outputting a prediction.\n* The convolutional layer should be able to identify patterns between the timesteps.\n* Input shape **[samples, timesteps, features]**.","d35a847a":"***Transform the data into a time series problem***","a0a433ae":"**Data preprocess**\n* Reshape from **[samples, timesteps]** into **[samples, timesteps, features]**.\n* This same reshaped data will be used on the CNN and the LSTM model.","97784932":"**Data preprocess**\n* Reshape from **[samples, timesteps, features]** into **[samples, subsequences, timesteps, features]**."}}