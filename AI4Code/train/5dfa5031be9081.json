{"cell_type":{"385fc2f3":"code","a8bfd145":"code","bd87f04b":"code","dcddb748":"code","008cfa29":"code","c7212499":"code","a851b7e5":"code","0c282c7d":"code","fde56dc7":"code","cb956a91":"code","5e82a1e6":"code","9af667df":"code","5a7b07eb":"code","0dc6d80b":"code","cf745fd7":"code","df0630ef":"code","e4f0365b":"code","49a54333":"code","0edc2945":"code","400b4e3e":"code","7f56d993":"code","9a772385":"code","e8604687":"code","4612e3aa":"code","003f1aa1":"code","67346446":"code","30da77a6":"code","50f34081":"code","48c28f77":"code","52c1e761":"code","e4b3e536":"code","d6f51d9f":"code","a5350788":"code","01ab4083":"code","191b58a7":"code","fae97b92":"code","fd6a03d7":"code","549307e0":"code","29866738":"code","ce49df36":"code","02e10ff2":"code","8c2df2ed":"code","6ca09a56":"code","14b594cd":"code","42c823c0":"code","24425b81":"code","5806b46d":"code","af8d5d85":"code","7b0eb040":"code","cc626956":"code","e36d285f":"code","3a8658b1":"code","27cd0e98":"code","e8ea2de0":"code","44945007":"code","9839686a":"code","9dc5c854":"code","224c5ccd":"code","6cdeab64":"code","d4c860a8":"code","fa84cab0":"code","31723973":"markdown","72572391":"markdown","25249e45":"markdown","a8a5b2c5":"markdown","89fe1708":"markdown","082b3096":"markdown","c0a781c3":"markdown","52aac8c2":"markdown","d3381a7a":"markdown","27e0da7c":"markdown","9f580501":"markdown","8e011faa":"markdown","1793bc36":"markdown","fc2945ff":"markdown","f213d193":"markdown","b8262831":"markdown","f747c659":"markdown","7a0c8a8c":"markdown","9cc33657":"markdown","50e4bc7a":"markdown","a87cec7d":"markdown","27eaca2f":"markdown","d434be3c":"markdown","38cecd17":"markdown","45478710":"markdown"},"source":{"385fc2f3":"## Load Dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(color_codes = True)\nplt.style.use('seaborn-muted')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a8bfd145":"## Load Dataset\n\ndf = pd.read_csv(\"..\/input\/xAPI-Edu-Data\/xAPI-Edu-Data.csv\")","bd87f04b":"df.head().T","dcddb748":"df['Class'].value_counts()","008cfa29":"sns.countplot(x = \"Class\", data = df, palette = \"Greens_d\");","c7212499":"## Missing Values\ndf.isnull().values.sum()","a851b7e5":"df.dtypes.value_counts()","0c282c7d":"print('Data Types')\nfor col in df.columns:\n    if df[col].dtypes == object:\n        print(\"\\n\",f\"{df[col].value_counts()}\")\n        print('**'*7)","fde56dc7":"df.describe()","cb956a91":"sns.set(style=\"ticks\")\nsns.pairplot(df, hue=\"Class\"); ","5e82a1e6":"plt.rcParams['figure.figsize'] = (18, 8)\n\nplt.subplot(1, 2, 1)\nsns.set(style = 'whitegrid')\nsns.distplot(df['raisedhands'])\nplt.title('Distribution of Student Participating', fontsize = 20)\nplt.xlabel('Range of Raised Hands')\nplt.ylabel('Count')\n\n\nplt.subplot(1, 2, 2)\nsns.set(style = 'whitegrid')\nsns.distplot(df['VisITedResources'])\nplt.title('Distribution of Student Visiting Resources', fontsize = 20)\nplt.xlabel('Range of Resources')\nplt.ylabel('Count')\nplt.show()","9af667df":"## Slice\ndata = df[['PlaceofBirth', 'GradeID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey', 'gender']]","5a7b07eb":"stage_map = {'lowerlevel': 0,'MiddleSchool': 1, 'HighSchool':2}\ndf['StageID'] = df['StageID'].map(stage_map)\n\nsection_map = {'A':0, 'B':1, 'C':2}\ndf['SectionID'] = df['SectionID'].map(section_map)\n\nsat_map = {'Bad':0, 'Good':1}\ndf['ParentschoolSatisfaction'] = df['ParentschoolSatisfaction'].map(sat_map)\n\nabsence_map = {'Under-7': 0, 'Above-7': 1}\ndf['StudentAbsenceDays'] = df['StudentAbsenceDays'].map(absence_map)\n\nclass_map = {'L':0, 'M':1, 'H':2}\ndf['Class'] = df['Class'].map(class_map)","0dc6d80b":"## Drop\ndrp = ['PlaceofBirth', 'GradeID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey', 'NationalITy', 'gender']\n\ndf.drop(columns = drp, inplace = True)","cf745fd7":"from pandas import plotting\n\nplt.rcParams['figure.figsize'] = (15, 10)\nlabels = ['Bad', 'Good']\n\nplotting.andrews_curves(df.drop(\"Class\", axis=1), \"ParentschoolSatisfaction\")\nplt.title('Andrew Curves for Parents\\' Satisfaction', fontsize = 20)\nplt.legend(labels)\nplt.show()","df0630ef":"## Parent Satisfaction on Grades\n\nplt.rcParams['figure.figsize'] = (18, 7)\nsns.boxenplot(df['ParentschoolSatisfaction'], df['Class'], palette = 'Blues')\nplt.title('Parent Satisfaction on Grades', fontsize = 20)\nplt.xticks([0,1], labels)\nplt.show()","e4f0365b":"## One-Hot Encode\ndata = pd.get_dummies(data, prefix=['POBirth', 'Grade', 'Topic', 'Semester', 'Relation', 'PASurvey', 'Gender'])\ndata = data.astype('int')\n\n## Join Back\ndf_w_dummy = data.join(df)","49a54333":"df_w_dummy.head().T","0edc2945":"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler","400b4e3e":"array = df_w_dummy.values\n\nX_knn = array[:, 0:52]\ny_knn = array[:, 52]\nX_knn = StandardScaler().fit(X_knn).transform(X_knn.astype(float))","7f56d993":"X_train, X_val, y_train, y_val = train_test_split( X_knn, y_knn, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_val.shape,  y_val.shape)","9a772385":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))","e8604687":"for n in range(1, Ks):\n\n    # Train Model and Predict\n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train, y_train)\n    yhat = neigh.predict(X_val)\n    mean_acc[n-1] = accuracy_score(y_val, yhat)\n    std_acc[n-1] = np.std(yhat == y_val)\/np.sqrt(yhat.shape[0])\n\nprint(f\"{mean_acc}\")","4612e3aa":"plt.plot(range(1, Ks), mean_acc, 'g')\nplt.fill_between(range(1, Ks), mean_acc - 1 * std_acc, mean_acc + 1 * std_acc, alpha = 0.10)\nplt.fill_between(range(1, Ks), mean_acc - 3 * std_acc, mean_acc + 3 * std_acc, alpha = 0.10, color = \"green\")\n\nplt.legend(('Accuracy ', '+\/- 1xstd', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","003f1aa1":"## Best k\nprint(\"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)","67346446":"from sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    km.fit(X_train)\n    wcss.append(km.inertia_)\n    \nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('wcss')\nplt.show()","30da77a6":"import scipy.cluster.hierarchy as shc\n\nplt.figure(figsize = (18, 50))\nplt.title(\"Patient Dendograms\")\ndend = shc.dendrogram(shc.linkage(X_knn, method = 'ward'), leaf_rotation = 0, leaf_font_size = 12, orientation = 'right')","50f34081":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X_knn)","48c28f77":"print(y_hc)","52c1e761":"## Filter Values\nX_vc = df.iloc[:, [3, -1]].values","e4b3e536":"from sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    km.fit(X_vc)\n    wcss.append(km.inertia_)\n    \nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('wcss')\nplt.show()","d6f51d9f":"km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_means = km.fit_predict(X_vc)\n\nplt.scatter(X_vc[y_means == 0, 0], X_vc[y_means == 0, 1], s = 100, c = 'pink', label = 'miser')\nplt.scatter(X_vc[y_means == 1, 0], X_vc[y_means == 1, 1], s = 100, c = 'yellow', label = 'general')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Visited Resources')\nplt.ylabel('Score Class')\nplt.legend()\nplt.grid()\nplt.show()","a5350788":"km.cluster_centers_.shape","01ab4083":"## Splitting into Validation Dataset\n\narray = df_w_dummy.values\nX = array[:, 0:52]\ny = array[:, 52]\n\n## Scaling\nX = StandardScaler().fit(X).transform(X.astype(float))\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.25, random_state = 1)","191b58a7":"print(\"There are \" + str(X_train.shape[0]) + \" rows and \" + str(X_train.shape[1]) + \" columns for Training and \" + str(X_validation.shape[0]) + \" rows for Testing.\")","fae97b92":"## Model Selection\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma = 'auto')))\n\n## evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits = 10, random_state=1, shuffle = True)\n    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","fd6a03d7":"## plotting the results\nplt.boxplot(results, labels = names)\nplt.title('Algorithm Comparison')\nplt.show()","549307e0":"from numpy.random import randn\n\ndata = {i : randn() for i in range(7)}","29866738":"from sklearn.model_selection import GridSearchCV\n\nLDA = LinearDiscriminantAnalysis()\n\nparameter_grid = {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': [None, 'auto'], 'store_covariance':[True, False]}\n\ngrid_search = GridSearchCV(LDA, param_grid = parameter_grid,\n                          cv = kfold, scoring = 'roc_auc')\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best Score: {:.5f}\".format(grid_search.best_score_))\nprint(\"Best Parameters: {}\".format(grid_search.best_params_))","ce49df36":"## Fitting LR Model with Spec.\n\nLDA = LinearDiscriminantAnalysis\n\nmodel = LDA(solver = \"lsqr\") #least square solutions\nmodel.fit(X_train, y_train)\npredict = model.predict(X_validation)","02e10ff2":"## Evaluate Predictions\nprint(\"Acc_Score\",\"\\n\\n\",accuracy_score(y_validation, predict),\"\\n\")\nprint(\"Confusion_Score\",\"\\n\\n\",confusion_matrix(y_validation, predict),\"\\n\")\nprint(\"Classification_Report\",\"\\n\\n\",classification_report(y_validation, predict),\"\\n\")","8c2df2ed":"sns.heatmap(confusion_matrix(y_validation, predict), annot=True, center = 0);","6ca09a56":"## SMOTE\n\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE()\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)","14b594cd":"## ADASYN\n\nfrom imblearn.over_sampling import ADASYN\nada = ADASYN(random_state = 130)\nX_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)","42c823c0":"## SMOTE + Tomek Links\n\nfrom imblearn.combine import SMOTETomek\nsmtom = SMOTETomek(random_state = 139)\nX_train_smtom, y_train_smtom = smtom.fit_resample(X_train, y_train)","24425b81":"## SMOTE + ENN\n\nfrom imblearn.combine import SMOTEENN\nsmenn = SMOTEENN()\nX_train_smenn, y_train_smenn = smenn.fit_resample(X_train, y_train)","5806b46d":"def evaluate_model(clf, X_validate, y_validate, model_name, oversample_type):\n    print('-----------------------------------------------')\n    print('Model ', model_name)\n    print('Data Type ', oversample_type)\n    \n    y_pred = clf.predict(X_validate)\n    f1 = f1_score(y_validate, y_pred, average = \"weighted\")\n    recall = recall_score(y_validate, y_pred, average = \"weighted\")\n    precision = precision_score(y_validate, y_pred, average = \"weighted\")\n\n    print(classification_report(y_validate, y_pred))\n    print(\"F1 Score \", f1)\n    print(\"Recall \", recall)\n    print(\"Precision \", precision)\n    return [model_name, oversample_type, f1, recall, precision]","af8d5d85":"models = {\n    'LDA': LinearDiscriminantAnalysis(solver= \"lsqr\"),\n    'DecisionTrees': DecisionTreeClassifier(random_state = 42),\n    'RandomForest':RandomForestClassifier(random_state = 42),\n    'LinearSVC':LinearSVC(random_state = 42),\n    'AdaBoostClassifier':AdaBoostClassifier(random_state = 42),\n    'SGD':SGDClassifier(random_state = 42),\n    \"CART\": DecisionTreeClassifier(random_state = 42)\n}","7b0eb040":"oversampled_data = {\n    'ACTUAL':[X_train, y_train],\n    'SMOTE':[X_train_sm, y_train_sm],\n    'ADASYN':[X_train_ada, y_train_ada],\n    'SMOTE_TOMEK':[X_train_smtom, y_train_smtom],\n    'SMOTE_ENN':[X_train_smenn, y_train_smenn]\n}","cc626956":"final_output = []\n\nfor model_k, model_clf in models.items():\n    for data_type, data in oversampled_data.items():\n        model_clf.fit(data[0], data[1])\n        final_output.append(evaluate_model(model_clf, X_validation, y_validation, model_k, data_type))","e36d285f":"final_df = pd.DataFrame(final_output, columns=['Model', 'DataType', 'F1', 'Recall', 'Precision'])","3a8658b1":"adjusted_model_df = final_df.sort_values(by = 'F1', ascending = False)\nadjusted_model_df = adjusted_model_df.reset_index()\nadjusted_model_df.drop(columns = 'index', inplace = True)","27cd0e98":"adjusted_model_df.head(10)","e8ea2de0":"param_grid = {\n  'criterion':['gini', 'entropy'],\n  'max_depth': [10, 20, 40, 80, 100],\n  'max_features': ['auto', 'sqrt'],\n  'n_estimators': [200, 400, 600, 800, 1000, 2000]\n}","44945007":"rfc = RandomForestClassifier(random_state=42)\n\nrfc_cv = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, verbose=2)\n\nrfc_cv.fit(X_train_ada, y_train_ada)","9839686a":"best_params = rfc_cv.best_params_\n\nprint(\"Best Parameters: {}\".format(best_params))","9dc5c854":"rf = RandomForestClassifier(n_estimators = best_params['n_estimators'], criterion = best_params['criterion'], max_depth = best_params['max_depth'], max_features = best_params['max_features'])\n\nrf.fit(X_train_ada, y_train_ada)\n\nevaluate_model(rf, X_validation, y_validation, \"RandomForest\", \"Ada\")","224c5ccd":"adj_predict = rf.predict(X_validation)","6cdeab64":"sns.heatmap(confusion_matrix(y_validation, adj_predict), annot=True, center = 0);","d4c860a8":"# view the feature scores\nclf = RandomForestClassifier(n_estimators = best_params['n_estimators'], criterion = best_params['criterion'], max_depth = best_params['max_depth'], max_features = best_params['max_features'])\n\nclf.fit(X_train, y_train)\n\nfeature_scores = pd.Series(clf.feature_importances_, index=df_w_dummy.drop(\"Class\", axis=1).columns).sort_values(ascending=False)\n\nfeature_scores","fa84cab0":"# Creating a seaborn bar plot\n\nf, ax = plt.subplots(figsize=(30, 24))\nax = sns.barplot(x=feature_scores, y=feature_scores.index, data=df)\nax.set_title(\"Visualize feature scores of the features\")\nax.set_yticklabels(feature_scores.index)\nax.set_xlabel(\"Feature importance score\")\nax.set_ylabel(\"Features\")\nplt.show()","31723973":"## Filter Values\nX_vc = df.iloc[:, [3, -1]].values","72572391":"It is important to note that because of the scaling and application of the above techniques to manage over-fitting the data has been reshaped and a confusion matrix would not be an evaluation metric. Considering this we use the F1 Score as a tangible evaluation metrics.","25249e45":"Models to Test.","a8a5b2c5":"SUMMARY STATISTICS","89fe1708":"Function to Search through adjusted and original data sets","082b3096":"## Hierachical Clustering: k = 5","c0a781c3":"Final Output.","52aac8c2":"Feature Importance","d3381a7a":"Input the Best Parameters","27e0da7c":"## Modeling\n\n1. Separate out a validation dataset.\n1. Set-up the test harness to use 10-folds cross validation.\n1. Build multiple different models to predict species from flower measurements.\n1. Select the best model.","9f580501":"Sets.","8e011faa":"## K-Nearest Neighbour [KNN]\n\n1. Normalize Data: Data standardization   give the data a zero mean and unit variance, it is good practice, especially for algorithms such KNN which is based on distance of cases.\n\n1. Selecting K: Run a for-loop on a number of k-values, and pick the smallest most accuratte k.","1793bc36":"Two.\n\n2. Visualization","fc2945ff":"### Hierarchical Cluster: Visited Resources Vs Class","f213d193":"GridSearch","b8262831":"Train - Test - Split","f747c659":"## Managing Model for Overfitting\n\n1. SMOTE Technique\n1. ADASYN\n1. SMOTE + Tomek Links\n1. SMOTE + ENN","7a0c8a8c":"\nTop10 Models and Dataset.","9cc33657":"Selecting K","50e4bc7a":"K-Visualization","a87cec7d":"GridSearch\n\n1. Parameter Grid\n1. Search","27eaca2f":"Normalize Data","d434be3c":"But we know better given the graph, 5 is the best k to work with.","38cecd17":"VIP Features","45478710":"## MODELLING"}}