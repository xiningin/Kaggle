{"cell_type":{"e81bfed3":"code","f164311d":"code","1a3c3ffa":"code","49de66f2":"code","4962f245":"code","b00de059":"code","db656cdd":"code","471a2bb3":"code","a9e30209":"code","2155d419":"code","89b075d8":"code","02b8430c":"code","1fae714b":"code","9126b067":"code","c71ce7f7":"code","485c337d":"code","5c9cfbd2":"code","796c50fb":"code","0de11dc7":"code","78a6c317":"code","6a639fd1":"code","56125282":"code","1f2c6afb":"code","60f0afe9":"code","44867b8f":"code","0d7266fc":"code","654c7204":"code","db938a4d":"code","251b7284":"markdown","45143249":"markdown","b05f5dc6":"markdown","dc686f4d":"markdown","a977ab7a":"markdown","e0c3a77c":"markdown","009e4a4f":"markdown","e60cb2c3":"markdown","412cfb2a":"markdown","184240d1":"markdown","b354f0de":"markdown","d0e5df21":"markdown"},"source":{"e81bfed3":"import pandas as pd\npd.set_option('display.max_colwidth', 500)\nfrom sklearn.model_selection import train_test_split\nfrom ktext.preprocess import processor","f164311d":"traindf, testdf = train_test_split(pd.read_csv('..\/input\/github_issues.csv').sample(n=40000), \n                                   test_size=.10)\ntrain_body_raw = traindf.body.tolist()\ntrain_title_raw = traindf.issue_title.tolist()\ntraindf.head()","1a3c3ffa":"# Preview what is in this list\ntrain_title_raw[0]","49de66f2":"num_encoder_tokens = 10000\nbody_pp = processor(keep_n=num_encoder_tokens, padding_maxlen=50)\ntrain_body_vecs = body_pp.fit_transform(train_body_raw)","4962f245":"print('\\noriginal string:\\n', train_body_raw[0], '\\n')\nprint('after pre-processing:\\n', train_body_vecs[0], '\\n')","b00de059":"# Instantiate a text processor for the titles, with some different parameters\n# append_indicators = True appends the tokens '_start_' and '_end_' to each document\n# padding = 'post' means that zero padding is appended to the end of the of the document (default is 'pre')\n\nnum_decoder_tokens=9000\ntitle_pp = processor(append_indicators=True, keep_n=num_decoder_tokens, \n                     padding_maxlen=12, padding ='post')\n\n# process the title data\ntrain_title_vecs = title_pp.fit_transform(train_title_raw)","db656cdd":"max(title_pp.id2token.keys())","471a2bb3":"def load_encoder_inputs(vectorized_body):\n    encoder_input_data = vectorized_body\n    doc_length = encoder_input_data.shape[1]\n    print(f'Shape of encoder input: {encoder_input_data.shape}')\n    return encoder_input_data, doc_length\n\n\ndef load_decoder_inputs(vectorized_title):\n    # For Decoder Input, you don't need the last word as that is only for predictionwhen we are training using Teacher Forcing.\n    decoder_input_data = vectorized_title[:, :-1]\n\n    # Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n    decoder_target_data = vectorized_title[:, 1:]\n\n    print(f'Shape of decoder input: {decoder_input_data.shape}')\n    print(f'Shape of decoder target: {decoder_target_data.shape}')\n    return decoder_input_data, decoder_target_data","a9e30209":"import numpy as np\nencoder_input_data, doc_length = load_encoder_inputs(train_body_vecs)\ndecoder_input_data, decoder_target_data = load_decoder_inputs(train_title_vecs)\nnum_encoder_tokens = max(body_pp.id2token.keys()) + 1\nnum_decoder_tokens = max(title_pp.id2token.keys()) + 1","2155d419":"from keras.models import Model\nfrom keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\nfrom keras import optimizers","89b075d8":"#setting latent dimensions arbitarily for embedding and hidden units\nlatent_dim = 80\n\n##### Define Model Architecture ######\n\n########################\n#### Encoder Model ####\nencoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n\n# Word embeding for encoder (ex: Issue Body)\nx = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\nx = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n\n# We do not need the `encoder_output` just the hidden state.\n_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n\n# Encapsulate the encoder as a separate entity so we can just encode without decoding if we want to.\nencoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n\nseq2seq_encoder_out = encoder_model(encoder_inputs)\n\n########################\n#### Decoder Model ####\ndecoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n\n# Word Embedding For Decoder (ex: Issue Titles)\ndec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\ndec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n\n# Set up the decoder, using `decoder_state_input` as initial state.\ndecoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\ndecoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\nx = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n\n# Dense layer for prediction\ndecoder_dense = Dense(num_decoder_tokens+2, activation='softmax', name='Final-Output-Dense')\n#softmax is a mathematical exponential function to calculate the probability distribution\ndecoder_outputs = decoder_dense(x)\n\n########################\n#### Seq2Seq Model ####\n\nseq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nseq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy') \n#sparse_categorical_crossentropy is used to calculate probabilistic loss between label and predictions given word embeddimgs","02b8430c":"from keras.callbacks import CSVLogger, ModelCheckpoint\n\nscript_name_base = 'tutorial_seq2seq'\nmodel_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n                                   save_best_only=True)\n\nbatch_size = 100\nepochs = 4\nhistory = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.10, callbacks=[model_checkpoint])","1fae714b":"!pip install annoy\nfrom annoy import AnnoyIndex\nfrom tqdm import tqdm\nimport logging\nimport nltk\nfrom nltk.translate.bleu_score import corpus_bleu","9126b067":"def extract_decoder_model(model):\n    \"\"\"\n    Here we extract the decoder from the original model.\n    Inputs: keras model object\n    Outputs: A Keras model object with the following inputs and outputs:\n    Inputs of Keras Model That Is Returned:\n    1: the embedding index for the last predicted word or the <Start> indicator\n    2: the last hidden state\n    Outputs of Keras Model That Is Returned:\n    1.  Prediction (class probabilities) for the next word\n    2.  The hidden state of the decoder, to be fed back into the decoder at the next time step\n    \n    \"\"\"\n    # the latent dimension is the same so we copy it from the decoder output\n    latent_dim = model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n\n    # Reconstruct the input into the decoder\n    decoder_inputs = model.get_layer('Decoder-Input').input\n    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n\n    # Creating a layer for the feedback loop from predictions back into the GRU\n    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n\n    # Crete a layer to reuse the weights\n    # There are two outputs, 1- is the embedding layer output for the teacher forcing\n    #                        2- is the hidden state\n    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n\n    # Reconstruct dense layers\n    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n                          [dense_out, gru_state_out])\n    return decoder_model\n\ndef extract_encoder_model(model):\n    \"\"\"\n    Here we extract the encoder from the original Sequence to Sequence Model.\n    Input:keras model object with body of issue as input\n    Returns: keras model object which is encoding of the issue with the last hidden state\n    \"\"\"\n    encoder_model = model.get_layer('Encoder-Model')\n    return encoder_model","c71ce7f7":"class Seq2Seq_Inference(object):\n    def __init__(self,\n                 encoder_preprocessor,\n                 decoder_preprocessor,\n                 seq2seq_model):\n\n        self.pp_body = encoder_preprocessor\n        self.pp_title = decoder_preprocessor\n        self.seq2seq_model = seq2seq_model\n        self.encoder_model = extract_encoder_model(seq2seq_model)\n        self.decoder_model = extract_decoder_model(seq2seq_model)\n        self.default_max_len_title = self.pp_title.padding_maxlen\n        self.nn = None\n        self.rec_df = None\n\n    def generate_issue_title(self,\n                             raw_input_text,\n                             max_len_title=None):\n        \"\"\"\n        To generate a title given the body of an issue usin the seq2seq model .\n        Inputs: The body of the issue text as an input string\n        max_len_title: The maximum length of the title the model will generate\n        \"\"\"\n        if max_len_title is None:\n            max_len_title = self.default_max_len_title\n        # get the encoder's features for the decoder\n        raw_tokenized = self.pp_body.transform([raw_input_text])\n        body_encoding = self.encoder_model.predict(raw_tokenized)\n        # we want to save the encoder's embedding before its updated by decoder to use as an embedding for other tasks.\n        original_body_encoding = body_encoding\n        state_value = np.array(self.pp_title.token2id['_start_']).reshape(1, 1)\n\n        decoded_sentence = []\n        stop_condition = False\n        while not stop_condition:\n            preds, st = self.decoder_model.predict([state_value, body_encoding])\n\n            # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n            # Argmax will return the integer index corresponding to the prediction + 2 since we chopped off first two\n            pred_idx = np.argmax(preds[:, :, 2:]) + 2\n\n            # retrieve word from index prediction\n            pred_word_str = self.pp_title.id2token[pred_idx]\n\n            if pred_word_str == '_end_' or len(decoded_sentence) >= max_len_title:\n                stop_condition = True\n                break\n            decoded_sentence.append(pred_word_str)\n\n            # update the decoder for the next word\n            body_encoding = st\n            state_value = np.array(pred_idx).reshape(1, 1)\n\n        return original_body_encoding, ' '.join(decoded_sentence)\n\n\n    def print_example(self,\n                      i,\n                      body_text,\n                      title_text,\n                      url,\n                      threshold):\n        \"\"\"\n        Prints examples\n        \"\"\"\n        if i:\n            print('\\n\\n==============================================')\n            print(f'============== Example # {i} =================\\n')\n\n        if url:\n            print(url)\n\n        print(f\"Issue Body:\\n {body_text} \\n\")\n\n        if title_text:\n            print(f\"Original Title:\\n {title_text}\")\n\n        emb, gen_title = self.generate_issue_title(body_text)\n        print(f\"\\n****** Machine Generated Title (Prediction) ******:\\n {gen_title}\")\n        \n        if self.nn:\n            # return neighbors and distances\n            n, d = self.nn.get_nns_by_vector(emb.flatten(), n=4,\n                                             include_distances=True)\n            neighbors = n[1:]\n            dist = d[1:]\n\n            if min(dist) <= threshold:\n                cols = ['issue_url', 'issue_title', 'body']\n                dfcopy = self.rec_df.iloc[neighbors][cols].copy(deep=True)\n                dfcopy['dist'] = dist\n                similar_issues_df = dfcopy.query(f'dist <= {threshold}')\n\n                print(\"\\n** Similar Issues (using encoder embedding) **:\\n\")\n                display(similar_issues_df)\n\n\n    def demo_model_predictions(self,\n                               n,\n                               issue_df,\n                               threshold=1):\n        \"\"\"\n        Pick n random Issues and display predictions.\n        Input: n- Number of issues to display from issue_df\n               issue_df- pandas DataFrame that contains two columns: `body` and `issue_title`.\n               threshold- float distance threshold for recommendation of similar issues.\n        Output: Prints the original issue body and the model's prediction.\n        \"\"\"\n        # Extract body and title from DF\n        body_text = issue_df.body.tolist()\n        title_text = issue_df.issue_title.tolist()\n        url = issue_df.issue_url.tolist()\n\n        if (len(body_text)==1):\n            demo_list=[0]\n        else:\n            demo_list = np.random.randint(low=1, high=len(body_text), size=n)\n        for i in demo_list:\n            self.print_example(i,\n                               body_text=body_text[i],\n                               title_text=title_text[i],\n                               url=url[i],\n                               threshold=threshold)\n            \n    def prepare_recommender(self, vectorized_array, original_df):\n        \"\"\"\n        Use the annoy library to build recommender\n        Parameters\n        ----------\n        vectorized_array : List[List[int]]\n            This is the list of list of integers that represents your corpus\n            that is fed into the seq2seq model for training.\n        original_df : pandas.DataFrame\n            This is the original dataframe that has the columns\n            ['issue_url', 'issue_title', 'body']\n        Returns\n        -------\n        annoy.AnnoyIndex  object (see https:\/\/github.com\/spotify\/annoy)\n        \"\"\"\n        self.rec_df = original_df\n        emb = self.encoder_model.predict(x=vectorized_array,\n                                         batch_size=vectorized_array.shape[0]\/\/200)\n\n        f = emb.shape[1]\n        self.nn = AnnoyIndex(f)\n        logging.warning('Adding embeddings')\n        for i in tqdm(range(len(emb))):\n            self.nn.add_item(i, emb[i])\n        logging.warning('Building trees for similarity lookup.')\n        self.nn.build(50)\n        return self.nn\n    \n    def evaluate_model(self, holdout_bodies, holdout_titles):\n        \"\"\"\n        Method for calculating BLEU Score.\n        Parameters\n        ----------\n        holdout_bodies : List[str]\n            These are the issue bodies that we want to summarize\n        holdout_titles : List[str]\n            This is the ground truth we are trying to predict --> issue titles\n        Returns\n        -------\n        bleu : float\n            The BLEU Score\n        \"\"\"\n        actual, predicted = list(), list()\n        assert len(holdout_bodies) == len(holdout_titles)\n        num_examples = len(holdout_bodies)\n\n        logging.warning('Generating predictions.')\n        # step over the whole set TODO: parallelize this\n        for i in tqdm(range(num_examples)):\n            _, yhat = self.generate_issue_title(holdout_bodies[i])\n\n            actual.append(self.pp_title.process_text([holdout_titles[i]])[0])\n            predicted.append(self.pp_title.process_text([yhat])[0])\n        # calculate BLEU score\n        logging.warning('Calculating BLEU.')\n        \n        #must be careful with nltk api for corpus_bleu!, \n        # expects List[List[List[str]]] for ground truth, using List[List[str]] will give you\n        # erroneous results.\n        bleu = corpus_bleu([[a] for a in actual], predicted)\n        return bleu*4","485c337d":"seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n                                 decoder_preprocessor=title_pp,\n                                 seq2seq_model=seq2seq_Model)","5c9cfbd2":"seq2seq_inf.demo_model_predictions(n=5, issue_df=testdf)","796c50fb":"testdf.head()","0de11dc7":"url='https:\/\/github.com\/github\/hub\/issues\/2634'\ntitle='React-native cant run on AVD'\nbody='Hi there I recently started working with react native when I started I completely follow the setup docs at https:\/\/reactnative.dev\/docs\/getting-started so after that I tried running the app on an android emulator everything is good I also installed the SDK and have android revision 29 also installed intel 86_64 system image now I got this error also have the environment variables set up even though getting this and im using vs code for devloping'\ndata = [[url, title, body]]\ncustomdf=pd.DataFrame(data, columns = ['issue_url','issue_title', 'body'])\ncustomdf.head()","78a6c317":"seq2seq_inf.demo_model_predictions(n=1, issue_df=customdf)","6a639fd1":"url='https:\/\/github.com\/github\/hub\/issues\/2627'\ntitle='How to display the data from the api response in flutter if its not in array format'\nbody='The problem Im trying to solve:I have problem in displaying the response data in flutter and i am not able to display it without list format How I imagine hub could expose this functionality:'\ndata = [[url, title, body]]\ncustomdf=pd.DataFrame(data, columns = ['issue_url','issue_title', 'body'])\ncustomdf.head()","56125282":"seq2seq_inf.demo_model_predictions(n=1, issue_df=customdf)","1f2c6afb":"# Read All 5M data points\nall_data_df = pd.read_csv('..\/input\/github_issues.csv').sample(n=200)\n# Extract the bodies from this dataframe\nall_data_bodies = all_data_df['body'].tolist()\n\n# transform all of the data using the ktext processor\nall_data_vectorized = body_pp.transform_parallel(all_data_bodies)","60f0afe9":"import dill as dpickle\n# save transformed data\nwith open('all_data_vectorized.dpkl', 'wb') as f:\n    dpickle.dump(all_data_vectorized, f)","44867b8f":"seq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n                                    decoder_preprocessor=title_pp,\n                                    seq2seq_model=seq2seq_Model)\nrecsys_annoyobj = seq2seq_inf_rec.prepare_recommender(all_data_vectorized, all_data_df)","0d7266fc":"seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=customdf, threshold=1)","654c7204":"title='Have an option to Choose from one or more github accounts or a login feature and a logout feature '\nbody='I use two GitHub accounts in my system when I do hub create the hub is creating in the repo in my work account. to switch between them I need to remove the hub file and then re-auth. is there a fix for it already?'\ntitle_list=[title]\nbody_list=[body]","db938a4d":"seq2seq_inf.evaluate_model(holdout_bodies=body_list, holdout_titles = title_list)","251b7284":"# Testing results","45143249":"# Training the model","b05f5dc6":"## Isssue Body and Title are stored in seperate lists using tolist.  The following code shows us the first issue title entry in the list:","dc686f4d":"# Custom Input","a977ab7a":"# BLEU Score Calculation","e0c3a77c":"## Use `ktext` to pre-process data","009e4a4f":"# Create the encoder decoder model","e60cb2c3":"# Inference model","412cfb2a":"# Read Data And Preview\n## To spilt into train and test sets","184240d1":"## An example of processed issue bodies","b354f0de":"# Similar titles prediction","d0e5df21":"# NLP Project Team 4"}}