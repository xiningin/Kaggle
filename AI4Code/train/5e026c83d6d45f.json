{"cell_type":{"89fc2dbe":"code","5470c008":"code","5ebda753":"code","170b0fb2":"code","2009b495":"code","16d2a601":"code","c7e726e8":"code","e85eb628":"code","022a0254":"code","3bbb452e":"code","520b14e2":"markdown"},"source":{"89fc2dbe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5470c008":"!pip install pyspark","5ebda753":"import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()","170b0fb2":"import numpy as np\nimport scipy.sparse as sps\nfrom pyspark.ml.linalg import Vectors","2009b495":"from pyspark.sql import SparkSession\nimport pyspark.sql.functions as f\nimport pyspark.sql.types as t\nimport os\nfrom os.path import isfile, join\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nspark = SparkSession.builder.getOrCreate()","16d2a601":"df = spark.read.csv(\"\/kaggle\/input\/it2034ch1502-car-acceptability-prediction\/train.csv\", header = True)\nX_test = spark.read.csv(\"\/kaggle\/input\/it2034ch1502-car-acceptability-prediction\/test.csv\", header = True)","c7e726e8":"df = df.replace('5more','5')\ndf = df.replace('more','5')\nX_test = X_test.replace('5more','5')\nX_test = X_test.replace('more','5')","e85eb628":"df = StringIndexer(inputCol=\"acceptability\",outputCol=\"acceptability1\").fit(df).transform(df)\ndf.show(5)","022a0254":"ColumnTrain= [\"buying_price\",\"maintenance_price\",\"number_of_doors\",\"carry_capacity\",\"trunk_size\",\"safety\"]\nstring_indexer = [\n    StringIndexer(inputCol=x, outputCol=x + \"1\", handleInvalid = \"skip\")\n    for x in ColumnTrain\n]\nOHTinputs = [\"buying_price1\",\"maintenance_price1\",\"trunk_size1\",\"safety1\"]\nOnehotColumns = [\n    OneHotEncoder(\n        inputCols = [f\"{x}\" for x in OHTinputs],\n        outputCols = [f\"{x}2\" for x in OHTinputs]\n    )\n]\nassemblerColumns = ['carry_capacity1','buying_price12',\n 'maintenance_price12',\n 'number_of_doors1',\n 'trunk_size12',\n 'safety12']\nVector_assembler = VectorAssembler(\n    inputCols = assemblerColumns, outputCol = \"features\"\n)\nstages = []\nstages += string_indexer\nstages += OnehotColumns\nstages += [Vector_assembler]\npipeline = Pipeline().setStages(stages)\nmodel = pipeline.fit(df)\ndf_transform = model.transform(df)\ndf_transform.select(\"features\").show(5)","3bbb452e":"df_test = model.transform(X_test)\nfrom pyspark.ml.classification import RandomForestClassifier\nrdf = RandomForestClassifier(labelCol =\"acceptability1\", featuresCol =\"features\", maxDepth=30, maxBins=128, numTrees=500 ).fit(df_transform)\nY_prediction = rdf.transform(df_test)\nresult = Y_prediction.select(\"car_id\",\"prediction\")\nresult = result.withColumn(\"acceptability\",result.prediction.cast('string'))\nresult = result.select('car_id','acceptability')\nresult = result.replace('0.0','unacc')\nresult = result.replace('1.0','acc')\nresult = result.replace('2.0','good')\nresult = result.replace('3.0','vgood')\nresult.toPandas().to_csv('sampleSubmission.csv', index = False)","520b14e2":"5 Brothers Team : \n1. Nguy\u1ec5n Th\u00e0nh T\u00e0i - CH2002015\n2. Nguy\u1ec5n Gia Trung - CH2002022\n3. Nguy\u1ec5n Cao Th\u1eafng - CH2002017\n4. T\u1ea1 Nguy\u1ec5n Thanh Nh\u00e2n - CH2002020\n5. Phan \u0110\u00ecnh Duy Th\u00f4ng - CH2002019"}}