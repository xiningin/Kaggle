{"cell_type":{"8fe239df":"code","af8a815a":"code","063b9cbd":"code","6efe5671":"code","8c7ea41f":"code","0ffd0cb5":"code","f922ec3a":"code","c1241224":"code","0d8c1028":"code","85c47f8b":"code","004629a7":"code","09177e60":"code","962569fa":"code","5896d954":"code","0cf34c9e":"code","b6248a3f":"code","76f00965":"code","1daea8ea":"code","0b37c081":"code","d02bf89e":"code","87ede85d":"code","7823c4ad":"code","12c03246":"code","4fba9e97":"code","b187af6b":"code","22c1daa5":"code","4cca42b3":"code","dcc93019":"code","19a6ca09":"code","7b426bec":"code","fe696140":"code","658b83da":"code","13535ed8":"code","92dfc958":"code","f2e4bd13":"code","3c5d7e4f":"code","2d068ee5":"code","3de9333a":"code","2cc43cbd":"code","c1973fc5":"code","1e2b8fec":"code","f9d25215":"code","018d340f":"code","4d2b2132":"code","3fb6c0cc":"code","17c992ec":"code","9661c5bf":"code","bde32941":"code","12b390d7":"code","08072fde":"code","5f802321":"code","bfcbe99f":"code","2c382334":"code","706f0f3c":"code","5bd7a760":"code","903acb73":"code","c67acda0":"code","fa34f1a6":"markdown","1752efdc":"markdown","a5d7ee94":"markdown","cf3162a4":"markdown","6c7fe08e":"markdown","f63dd2f9":"markdown","42964742":"markdown","d72fd00e":"markdown","6acb43b4":"markdown","56940870":"markdown","22d5a5fd":"markdown","00a3032d":"markdown","c82f4ea1":"markdown","5cb9284b":"markdown","934a71c9":"markdown"},"source":{"8fe239df":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.preprocessing import LabelEncoder","af8a815a":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_df.head()","063b9cbd":"test_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_df.head()","6efe5671":"gender_df = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ngender_df.head()","8c7ea41f":"print(list(train_df.columns))\nprint(len(train_df.columns))","0ffd0cb5":"train_df.describe()","f922ec3a":"# describe the categorical features\ntrain_df.describe(include=['O'])","c1241224":"train_df.info()","0d8c1028":"# Adding an empty column \"succeeded\" in \"test_df\"\ntest_df['Survived'] = np.nan","85c47f8b":"def showNullsTabel():\n    train_null = train_df.isnull().sum()\n    test_null = test_df.isnull().sum()\n    \n    null_df = pd.DataFrame({'train_Null' : train_null,\n                        'test_Null'  : test_null ,\n                        'train_Null(%)' :round((train_null\/len(train_df))*100, 2),\n                        'test_Null(%)' : round((test_null\/len(test_df))*100, 2)})\n    return null_df","004629a7":"showNullsTabel()","09177e60":"# Show the relationship between ('SibSp', 'Parch', 'Pclass', 'Sex') and \"Survived\"\nfeature = ['SibSp', 'Parch', 'Pclass', 'Sex']\nfor feat in feature:\n    f, x = plt.subplots(figsize=(8, 6))\n    sns.histplot(data=train_df, x=feat, hue=\"Survived\", multiple=\"dodge\")\n    plt.show()","962569fa":"# Show the percentage of 'Survived' in each 'Embarked'\ntrain_df[['Embarked', 'Survived']].groupby('Embarked', as_index=False).mean().sort_values(by='Survived', ascending=False)","5896d954":"showNullsTabel()","0cf34c9e":"# Show the rows that has null 'Fare'\ntest_df.loc[test_df.Fare.isnull()]","b6248a3f":"print('Before : Fare null = ', test_df.Fare.isnull().sum())\n\n# We choose the data that has  :  'Pclass' : 3, 'Embarked' : S, 'SibSp' : 0 and 'Parch' : 0\nsimilar_Data = test_df.loc[((test_df.Pclass == 3) &\n                              (test_df.Embarked == \"S\") & \n                              (test_df.SibSp == 0) & \n                              (test_df.Parch == 0))]\n\n# fill null with mean of `Fare` of 'similar_Data'\ntest_df.Fare.fillna(round(similar_Data.Fare.mean()), inplace=True)\n\nprint('After : Fare null = ', test_df.Fare.isnull().sum())","76f00965":"# Show the rows that has null 'Fare'\ntrain_df.loc[train_df.Embarked.isnull()]","1daea8ea":"print('Before : Embarked null = ', train_df.Embarked.isnull().sum())\n\n# We choose the data that has  :  'Survived' : 1, 'Pclass' : 1, 'SibSp' : 0 , 'Parch' : 0 and 'Sex' : 'female'\nsimilar_Data = train_df.loc[((train_df.Survived == 1) &\n                             (train_df.Pclass == 1) & \n                             (train_df.SibSp == 0) & \n                             (train_df.Parch == 0) & \n                             (train_df.Sex == 'female'))]\n\nfreq_port = similar_Data.Embarked.dropna().mode()[0]\ntrain_df.Embarked = train_df.Embarked.fillna(freq_port)\n\n\nprint('After : Embarked null = ', train_df.Embarked.isnull().sum())","0b37c081":"total_data = [train_df, test_df]\n\n# Maping Embarked\nfor dataset in total_data:\n    dataset['Embarked'] = dataset.Embarked.map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","d02bf89e":"# Add new feature `Have_cabin`\nfor data in total_data:\n    data['Have_cabin'] = data.Cabin.apply(lambda Cabin : 0 if type(Cabin)==float else 1)\ntrain_df.head()","87ede85d":"train_df[['Have_cabin', 'Survived']].groupby('Have_cabin', as_index=False).mean().sort_values(by='Survived', ascending=False)","7823c4ad":"print(\"Before  :  train shape  :  {},   test shape  :  {}\".format(train_df.shape,test_df.shape))\n\n# Drop 'Cabin' feature from training and testing data\ntrain_df = train_df.drop('Cabin', axis=1)\ntest_df = test_df.drop('Cabin', axis=1)\ntotal_data = [train_df, test_df]\n\nprint(\"After   :  train shape  :  {},   test shape  :  {}\".format(train_df.shape, test_df.shape))","12c03246":"showNullsTabel()","4fba9e97":"# Show the rows that has null 'Age'\ntrain_df.loc[(train_df.Age.isnull())]","b187af6b":"# bring the features that have negative or positive correlation with 'Age'\ncorr = train_df.corr()\nf, x = plt.subplots(figsize=(12, 10))\nsns.heatmap(corr, square=True)\nplt.show()","22c1daa5":"for data in total_data:\n    for Pclass in sorted(data.Pclass.unique()):\n        for SibSp in sorted(data.loc[(data.Pclass == Pclass)].SibSp.unique()):\n            similar_Age_Data = data.loc[(data.Pclass == Pclass) & (data.SibSp == SibSp)].Age\n\n            if similar_Age_Data.isnull().sum() == len(similar_Age_Data):\n                print('the problem in state : total_data[{}] , Pclass = {} , SibSp = {}'.format(total_data.index(data), Pclass, SibSp))","4cca42b3":"train_df.loc[(train_df.Pclass == 3) & (train_df.SibSp == 8)]","dcc93019":"for data in total_data:\n    for Pclass in sorted(data.Pclass.unique()):\n        for SibSp in sorted(data.loc[(data.Pclass == Pclass)].SibSp.unique()):\n            similar_Age_Data = data.loc[(data.Pclass == Pclass) & (data.SibSp == SibSp)].Age\n            \n            # for solving the problem we use this condition\n            if similar_Age_Data.isnull().sum() == len(similar_Age_Data):\n                similar_Age_Data = data.loc[(train_df.Pclass == Pclass)].Age\n                \n            data.loc[ (data.Age.isnull()) & (data.Pclass == Pclass) & (data.SibSp == SibSp),'Age'] = round(similar_Age_Data.mean())","19a6ca09":"showNullsTabel()","7b426bec":"# maping 'Sex'\nprint('Before : Sex in train = {}  ,  Sex in test = {}'.format(train_df.Sex.unique(), test_df.Sex.unique())) \n\nfor data in total_data:\n    data['Sex'] = data.Sex.map({'female' : 0, 'male' : 1})\n\nprint('After : Sex in train = {}  ,  Sex in test = {}'.format(train_df.Sex.unique(), test_df.Sex.unique())) ","fe696140":"# Create a new feature Title, containing the titles of passenger names\nfor dataset in total_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","658b83da":"# Group all titles\nfor dataset in total_data:\n    dataset['Title'] = dataset.Title.replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset.Title.replace('Mlle', 'Miss')\n    dataset['Title'] = dataset.Title.replace('Ms', 'Miss')\n    dataset['Title'] = dataset.Title.replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby('Title', as_index=False).mean()","13535ed8":"# Maping 'Title'\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in total_data:\n    dataset['Title'] = dataset.Title.map(title_mapping)\n    dataset['Title'] = dataset.Title.fillna(0)\n\n# Drop 'Name' feature\ntrain_df = train_df.drop('Name', axis=1)\ntest_df = test_df.drop('Name', axis=1)\ntotal_data = [train_df, test_df]\ntrain_df.shape, test_df.shape\n\ntrain_df.head()","92dfc958":"# Banding 'Age' and create new feature 'AgeBand'\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","f2e4bd13":"# Maping 'Age'\ndef AgeType(Age):\n    if Age <= 16 :\n        return 0\n    if 16 < Age <= 32 :\n        return 1\n    if 32 < Age <= 48 :\n        return 2\n    if 48 < Age <= 64 :\n        return 3\n    if 64 < Age:\n        return 4\n    \nfor dataset in total_data:    \n    dataset['Age'] = dataset.Age.apply(AgeType)","3c5d7e4f":"# drop 'AgeBand'\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ntotal_data = [train_df, test_df]\ntrain_df.head()","2d068ee5":"# Banding 'Fare' and create new feature 'FareBand'\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","3de9333a":"# maping 'Fare'\ndef FareType(Fare):\n    if Fare <= 7.91 :\n        return 0\n    if 7.91 < Fare <= 14.454 :\n        return 1\n    if 14.454 < Fare <= 31 :\n        return 2\n    if 31 < Fare :\n        return 3\n    \n\nfor data in total_data:\n    data['Fare'] = data['Fare'].apply(FareType)\n                \ntotal_data = [train_df, test_df]\n    \ntrain_df.head(10)","2cc43cbd":"# drop 'FareBand'\ntrain_df = train_df.drop(['FareBand'], axis=1)\ntotal_data = [train_df, test_df]\ntrain_df.head(10)","c1973fc5":"# for training\ndef train_Tick_Count(val):\n    return train_df[\"Ticket\"].value_counts()[val]\n\ntrain_df['Share_ticket'] = train_df[\"Ticket\"].apply(train_Tick_Count)\n\n\n# for testing\ndef test_Tick_Count(val):\n    return test_df[\"Ticket\"].value_counts()[val]\n\ntest_df['Share_ticket'] = test_df[\"Ticket\"].apply(test_Tick_Count)","1e2b8fec":"# ploting 'Share_ticket'  with respect to the feature of 'Survived'\nf, x = plt.subplots(figsize=(8, 6))\nsns.histplot(data=train_df, x='Share_ticket', hue=\"Survived\", multiple=\"dodge\")\nplt.show()\n\n# Show the percentage of 'Survived' with respect to the feature of 'Share_ticket'\ntrain_df[['Share_ticket', 'Survived']].groupby('Share_ticket', as_index=False).mean().sort_values(by='Survived', ascending=False)","f9d25215":"train_df = train_df.drop(\"Ticket\", axis=1)\ntest_df = test_df.drop(\"Ticket\", axis=1)","018d340f":"X_train = train_df.drop([\"Survived\", \"PassengerId\"], axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop([\"PassengerId\", \"Survived\"], axis=1)\nY_test = gender_df['Survived']\n\nprint('X_train shape = ', X_train.shape)\nprint('Y_train shape = ', Y_train.shape)\nprint('X_test shape = ', X_test.shape)\nprint('Y_test shape = ', Y_test.shape)","4d2b2132":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\ntrain_acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\ntest_acc_log = round(logreg.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_log)\nprint('test acc   :  ', test_acc_log)","3fb6c0cc":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\ntrain_acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\ntest_acc_svc = round(svc.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_svc)\nprint('test acc   :  ', test_acc_svc)","17c992ec":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\ntrain_acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\ntest_acc_knn = round(knn.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_knn)\nprint('test acc   :  ', test_acc_knn)","9661c5bf":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\ntrain_acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\ntest_acc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_gaussian)\nprint('test acc   :  ', test_acc_gaussian)","bde32941":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\ntrain_acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\ntest_acc_perceptron = round(perceptron.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_perceptron)\nprint('test acc   :  ', test_acc_perceptron)","12b390d7":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\ntrain_acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\ntest_acc_linear_svc = round(linear_svc.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_linear_svc)\nprint('test acc   :  ', test_acc_linear_svc)","08072fde":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\ntrain_acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\ntest_acc_sgd = round(sgd.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_sgd)\nprint('test acc   :  ', test_acc_sgd)","5f802321":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\ntrain_acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\ntest_acc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_decision_tree)\nprint('test acc   :  ', test_acc_decision_tree)","bfcbe99f":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\ntrain_acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\ntest_acc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2)\nprint('train acc  :  ', train_acc_random_forest)\nprint('test acc   :  ', test_acc_random_forest)","2c382334":"# Linear Discriminant Analysis and Logistic Regression\n\nlda = LDA(n_components = 1)\nX_train = lda.fit_transform(X_train, Y_train)\nX_test = lda.transform(X_test)\n\n\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, Y_train)\n\nY_pred = classifier.predict(X_test)\n\ntrain_acc_LDA_log = round(classifier.score(X_train, Y_train) * 100, 2)\ntest_acc_LDA_log = round(classifier.score(X_test, Y_test) * 100, 2)\n\nprint('train score  = ', train_acc_LDA_log )\nprint('test score  = ', test_acc_LDA_log )","706f0f3c":"# Create score table\nscore_table = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC',\n              'Decision Tree', 'LDA and Logistic Regression'],\n    'Train score': [train_acc_svc, train_acc_knn, train_acc_log, \n              train_acc_random_forest, train_acc_gaussian, train_acc_perceptron, \n              train_acc_sgd, train_acc_linear_svc, train_acc_decision_tree, train_acc_LDA_log],\n    'Test score': [test_acc_svc, test_acc_knn, test_acc_log, \n              test_acc_random_forest, test_acc_gaussian, test_acc_perceptron, \n              test_acc_sgd, test_acc_linear_svc, test_acc_decision_tree, test_acc_LDA_log]})\nscore_table = score_table.sort_values(by='Test score', ascending=False)","5bd7a760":"score_table['difference'] = np.abs(score_table['Train score'] - score_table['Test score'])\nscore_table","903acb73":"# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(Y_pred, Y_test)\nprint(cm)","c67acda0":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","fa34f1a6":"**To detect this problem we use  :**","1752efdc":"**The idea would be:**\n> * Split data by `Pclass`\n> * Split the resulting data by `SibSp`\n> * Fill nulls `Age` by the mean ages in all small sets\n   \n**But there is a problem, the ages in small groups may be all missing**","a5d7ee94":"**Nice, we filled in all the missing values**","cf3162a4":"**This is my first notebook in kaggle, can you share your opinion. I look forward to getting better :)**","6c7fe08e":"**Let's check this out**","f63dd2f9":"**Adding a new feature containing the number of people who participated in the ticket**","42964742":"**After analyzing the age class, we see that `Pclass` and `SibSp` have the best relationship with age**","d72fd00e":"**Now the same thing with `Embarked`**","6acb43b4":"**Good, Now let's implement the idea and use a condition to avoid the problem**","56940870":"**Let's start with `Fare`**","22d5a5fd":"# **Acquire data and Analyze by describing**","00a3032d":"# **Importing Libraries**","c82f4ea1":"# **Modeling**","5cb9284b":"**We will try to fill in the missing value in `Fare`  with average `Fare` of the data similar to our sample**","934a71c9":"# **Data cleaning**"}}