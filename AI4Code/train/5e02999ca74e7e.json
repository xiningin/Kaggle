{"cell_type":{"b2d4e42a":"code","ce00a42d":"code","3faafb5c":"code","b173f919":"code","8dcdbc08":"code","dcc57d3c":"code","f2a497f7":"code","abccc1d2":"code","f61fc084":"code","5363fb5a":"code","7ef608f4":"code","cf7395cf":"code","f0a8c189":"code","619de253":"code","d387a1ae":"code","cd9fb0c0":"code","74cd0a70":"code","139f0bdb":"code","ee6ce1e5":"code","bd8254ba":"markdown","9728ab9e":"markdown","f4bae4dc":"markdown","5b572527":"markdown","75eff972":"markdown","55b5e485":"markdown","61bf4ca5":"markdown","83769cb1":"markdown","ea9cfa99":"markdown","73dc656f":"markdown","b19fc38d":"markdown","39d9b0f5":"markdown","4b79c6b7":"markdown"},"source":{"b2d4e42a":"import pandas as pd \nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ce00a42d":"warnings.simplefilter('ignore')\n\npd.set_option('display.width', 100)\npd.set_option('display.max_rows', 25)\npd.set_option('display.max_columns', 25)","3faafb5c":"train = pd.read_csv('..\/input\/supplement-sales-prediction\/TRAIN.csv')\ntest = pd.read_csv('..\/input\/supplement-sales-prediction\/TEST_FINAL.csv')\nsub = pd.read_csv('..\/input\/supplement-sales-prediction\/SAMPLE.csv')","b173f919":"train # check data train","8dcdbc08":"test # check data test","dcc57d3c":"train.isna().mean().to_frame() # let's hunt the null value","f2a497f7":"test.isna().mean().to_frame() # let's hunt the null value","abccc1d2":"train.dtypes.to_frame() # check the type","f61fc084":"test.dtypes.to_frame() # check the type","5363fb5a":"train[['Year', 'Month', 'Day']] = train['Date'].str.split('-', expand=True).astype('float64')\ntest[['Year', 'Month', 'Day']] = train['Date'].str.split('-', expand=True).astype('float64')\ndel train['Date']\ndel train['#Order']\ndel test['Date']","7ef608f4":"from xgboost import XGBRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder() # Let's gooo!\n\nfor i in train.columns: # Make a for loops\n    if train[i].dtype == 'object': \n        encoder.fit_transform(list(train[i].values)) # Fit transform\n        train[i] = encoder.transform(train[i].values) # Transform\n         \n        for j in train.columns: # Make a for loops again\n            if train[j].dtype == 'int':\n                train[j] = train[j].astype('float64') # Change the type\n\nfor k in test.columns: # Make a for loops\n    if test[k].dtype == 'object': \n        encoder.fit_transform(list(test[k].values)) # Fit transform\n        test[k] = encoder.transform(test[k].values) # Transform\n         \n        for m in test.columns: # Make a for loops again\n            if test[m].dtype == 'int':\n                test[m] = test[m].astype('float64') # Change the type\n\n                \nX = train.drop(columns=['ID', 'Sales']) # Data X\ny = train['Sales'] # Data y     \nX_test = test.drop(columns=['ID'])\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y,test_size=0.3, \n    random_state=42\n) \n\npipe = Pipeline([ # Our Pipeline\n    ('scaler', MinMaxScaler()),\n    ('transformer', QuantileTransformer()),\n    ('model', XGBRegressor(\n        learning_rate=0.09,\n        n_estimators=12000,\n        random_state=42,\n        objective='reg:squarederror',\n        booster='gbtree',\n        tree_method='gpu_hist'\n    ))\n])\n\npipe.fit(X_train, y_train) # Train Data","cf7395cf":"from sklearn import metrics\nimport math","f0a8c189":"pred_train = pipe.predict(X_train) # Predict Train Data\npred_valid = pipe.predict(X_valid) # Predict Valid Data","619de253":"train_r2_score = metrics.r2_score(y_train, pred_train) # R2_score\nprint(f'Train R2_score: {train_r2_score}')\n\ntrain_mse = metrics.mean_squared_error(y_train, pred_train) # MSE Score\nprint(f'Train MSE : {train_mse}')\n\ntrain_RMSE = math.sqrt(metrics.mean_squared_error(y_train, pred_train)) # SQRT MSE Score\nprint(f'Train RMSE : {train_RMSE}')","d387a1ae":"train = pd.DataFrame(\n    {'Predicted Sales':pred_train, 'Actual Sales':y_train}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntrain = train.reset_index()\ntrain = train.drop(\n    ['index'],axis=1\n)\n\nplt.plot(train[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","cd9fb0c0":"test_r2_score = metrics.r2_score(y_valid, pred_valid) # R2_score\nprint(f'Test R2_score: {test_r2_score}')\n\ntest_mse = metrics.mean_squared_error(y_valid, pred_valid) # MSE Score\nprint(f'Test MSE : {test_mse}')\n\ntest_RMSE = math.sqrt(metrics.mean_squared_error(y_valid, pred_valid)) # SQRT MSE Score\nprint(f'Test RMSE : {test_RMSE}')","74cd0a70":"test = pd.DataFrame(\n    {'Predicted Sales':pred_valid, 'Actual Sales':y_valid}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntest = test.reset_index()\ntest = test.drop(\n    ['index'],axis=1\n)\n\nplt.plot(test[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","139f0bdb":"pred_test = pipe.predict(X_test)","ee6ce1e5":"sub.Sales = pred_test\nsub.head()","bd8254ba":"# **That's it! thanks for watching! hope you guys like it! don't forget to give me feedback and upvote if you like it!**","9728ab9e":"# **Import Libraries**\n\nImport the module that we want to use for this research.","f4bae4dc":"### **Evaluate Valid Data**","5b572527":"# **Model Evaluation**","75eff972":"# **Data Preprocessing**\n\nWe must check the data every time we want to make a model, because this is the important thing, if you suddenly meet a bad dataset, wether you want it or not, you must clean the dataset.","55b5e485":"# **Encoding, Splitting, Modeling**","61bf4ca5":"### **Evaluate Train Data**","83769cb1":"### **Visualization for Actual and Predicted Sales in Training Data**","ea9cfa99":"### **Setup Libraries**","73dc656f":"> Great job!, the dataset not contain null value!","b19fc38d":"### **Visualization for Actual and Predicted Sales in Validation Data**","39d9b0f5":"# **Load Dataset**","4b79c6b7":"## **Here's my another notebook that i made:**\n\n**Data Analysis and Visualization:**\n\n- [Amazon Top Selling Book](https:\/\/www.kaggle.com\/knightbearr\/amazon-book-eda-top-5-knightbearr)\n- [Apple Stock Price Analysis](https:\/\/www.kaggle.com\/knightbearr\/apple-stock-price-analysis-knightbearr)\n- [World Covid Vaccination](https:\/\/www.kaggle.com\/knightbearr\/data-visualization-world-vaccination-knightbearr)\n- [Netflix Time Series Visualization](https:\/\/www.kaggle.com\/knightbearr\/netflix-visualization-time-series-knightbearr)\n- [Taiwan Weight Stock Analysist](https:\/\/www.kaggle.com\/knightbearr\/taiwan-weight-stock-index-analysis-knightbearr)\n\n**Regression and Classification:**\n\n- [Pizza Price Prediction](https:\/\/www.kaggle.com\/knightbearr\/pizza-price-prediction-xgb-knightbearr)\n- [S&P 500 Companies](https:\/\/www.kaggle.com\/knightbearr\/pricesales-eda-rfr-knightbearr)\n- [Credit Card Fraud Detection](https:\/\/www.kaggle.com\/knightbearr\/credit-card-fraud-detection-knightbearr)\n- [Car Price V3](https:\/\/www.kaggle.com\/knightbearr\/car-price-v3-xgbregressor-knightbearr)\n- [House Price Iran](https:\/\/www.kaggle.com\/knightbearr\/house-price-iran-knightbearr)\n- [Loan Prediction](https:\/\/www.kaggle.com\/knightbearr\/loan-prediction-eda-knightbearr)\n\n**Deep Learning:**\n\n- [Rock Paper Scissors](https:\/\/www.kaggle.com\/knightbearr\/rock-paper-scissors-knightbearr)\n\n**Some Python Code:**\n\n- [Python Cheat Sheet](https:\/\/www.kaggle.com\/knightbearr\/python-cheat-sheet-knightbearr)\n- [22 Python Progam](https:\/\/www.kaggle.com\/knightbearr\/22-simple-python-program-knightbearr)"}}