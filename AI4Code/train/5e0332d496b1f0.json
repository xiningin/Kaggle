{"cell_type":{"39314dc7":"code","f6d89a73":"code","2db08089":"code","ddb2b038":"code","4bc5926f":"code","780d73f9":"code","ad983897":"code","6ca035c0":"code","515ae2bb":"code","dfd10a7b":"code","6ede2c3a":"code","df079c2d":"code","b1f0f25e":"code","ccb7d659":"code","2168a36f":"code","72afda99":"code","1d2d416a":"code","fb8bbb50":"code","55328c16":"code","6e226305":"code","9c2a6ffc":"code","ec02263b":"code","c369e44b":"code","56937cdc":"code","f9bac38f":"markdown","94fec044":"markdown","95ba8e69":"markdown"},"source":{"39314dc7":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as plt\nimport optuna\nimport pickle\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor\nfrom pathlib import Path","f6d89a73":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-feb-2021\/')","2db08089":"df_train = pd.read_csv(input_path \/ \"train.csv\", index_col=\"id\")\ndf_test = pd.read_csv(input_path \/ \"test.csv\", index_col=\"id\")\ndf_preds_example = pd.read_csv(input_path \/ \"sample_submission.csv\")\ndf_train","ddb2b038":"for c in df_train.columns:\n    if df_train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df_train[c].values) + list(df_test[c].values))\n        df_train[c] = lbl.transform(df_train[c].values).astype(\"int32\")\n        df_test[c] = lbl.transform(df_test[c].values).astype(\"int32\")","4bc5926f":"df_train.head()","780d73f9":"df_target = df_train.pop('target')\nX_train, X_val, y_train, y_val = train_test_split(df_train, df_target, test_size=0.5, random_state=43)\nX_test = df_test\nX_train","ad983897":"y_train","6ca035c0":"def get_model_xgb(X_train, X_val, y_train, y_val):\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dval = xgb.DMatrix(X_val, label=y_val)\n    param = {'tree_method':'gpu_hist'}\n    num_round = 100\n    return xgb.train(param, dtrain, num_round, early_stopping_rounds=10, evals=[(dval, \"eval\")])","515ae2bb":"xgb_model = get_model_xgb(X_train, X_val, y_train, y_val)","dfd10a7b":"# Comparison\n\n#Median: 0.889\n#Linear regression: 0.870\n#Boosting trees: 0.851","6ede2c3a":"dtest = xgb.DMatrix(X_test)","df079c2d":"preds_xgb = xgb_model.predict(dtest)\ndf_preds_example[\"target\"] = preds_xgb\ndf_preds_example.to_csv(\"preds_xgb_solo.csv\", index=False)","b1f0f25e":"dummy_df = pd.DataFrame([\n    [\"Yes\", 0, 0.1, 0, 0, 0.9],\n    [\"Yes\", 0, 0.15, 0, 5, 0.96],\n    [\"Yes\", 50, 0.5, 3, 0, 0.5],\n    [\"No\", 10, 0.4, 5, 5, 0.2],    \n    [\"No\", 11, 0.6, 25, 0, 0.3],\n], columns=[\"Is ferrari\", \"Years on license\", \"Crime rate\", \"Car age\", \"Penalty points\", \"Chance of insurance claim\"])\ndummy_df","ccb7d659":"# Decision Tree\ndef GetChance(row):\n    if row[\"Is ferrari\"] == \"Yes\":\n        if row[\"Years on license\"] < 25:\n            return 0.93\n        else:\n            return 0.5\n    else:\n        return 0.25","2168a36f":"# Boosting tree\ndef GetFerrariPenalty(row):\n    if row[\"Is ferrari\"] == \"Yes\":\n        if row[\"Years on license\"] < 25:\n            return 0.93\n        else:\n            return 0.5\n    else:\n        return 0.25\n\ndummy_df_residualised_1 = pd.DataFrame([\n    [\"Yes\", 0, 0.1, 0, 0, -0.03],\n    [\"Yes\", 0, 0.15, 0, 5, 0.03],\n    [\"Yes\", 50, 0.5, 3, 0, 0.0],\n    [\"No\", 10, 0.4, 5, 5, -0.05],    \n    [\"No\", 11, 0.6, 25, 0, 0.05],\n], columns=[\"Is ferrari\", \"Years on license\", \"Crime rate\", \"Car age\", \"Penalty points\", \"Chance of insurance claim\"])\ndummy_df_residualised_1","72afda99":"def GetGoodDrinvingBoost(row):\n    if row[\"Penalty points\"] > 3:\n        return 0.04\n    else:\n        return -0.04\n    \ndummy_df_residualised_2 = pd.DataFrame([\n    [\"Yes\", 0, 0.1, 0, 0, 0.01],\n    [\"Yes\", 0, 0.15, 0, 5, -0.01],\n    [\"Yes\", 50, 0.5, 3, 0, -0.04],\n    [\"No\", 10, 0.4, 5, 5, -0.01],    \n    [\"No\", 11, 0.6, 25, 0, 0.01],\n], columns=[\"Is ferrari\", \"Years on license\", \"Crime rate\", \"Car age\", \"Penalty points\", \"Chance of insurance claim\"])\ndummy_df_residualised_2","1d2d416a":"def get_model_lgb(X_train, X_val, y_train, y_val):\n    cat_columns = [f'cat{cat_index}' for cat_index in range(10)]\n    \n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_columns)\n    validation_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_columns)\n    param = {'objective': 'regression', 'metric':'rmse'}\n    \n    return lgb.train(param, train_data, 1000,  valid_sets=validation_data, early_stopping_rounds=10, categorical_feature=cat_columns)","fb8bbb50":"lgb_model = get_model_lgb(X_train, X_val, y_train, y_val)","55328c16":"def get_model_cat(X_train, X_val, y_train, y_val):\n    cat_columns = [f'cat{cat_index}' for cat_index in range(10)]\n    model = CatBoostRegressor(\n        eval_metric='RMSE',\n        task_type='GPU',\n        iterations=1000,\n        od_type=\"Iter\",\n        od_wait=10,\n        learning_rate=0.3,\n        metric_period=25\n    )\n    \n    model.fit(X_train, y=y_train, cat_features=cat_columns, eval_set=(X_val, y_val))\n    return model","6e226305":"cat_model = get_model_cat(X_train, X_val, y_train, y_val)","9c2a6ffc":"preds_xgb = xgb_model.predict(dtest)\npreds_lgb = lgb_model.predict(X_test)\npreds_cat = cat_model.predict(X_test)\n\ndf_preds_example[\"target\"] = np.mean(np.vstack([preds_xgb, preds_lgb, preds_cat]), axis=0)\ndf_preds_example.to_csv(\"preds_combined_mean.csv\", index=False)\n\ndf_preds_example[\"target\"] = np.median(np.vstack([preds_xgb, preds_lgb, preds_cat]), axis=0)\ndf_preds_example.to_csv(\"preds_combined_median.csv\", index=False)","ec02263b":"def objective(trial, X_train, X_val, y_train, y_val):\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dval = xgb.DMatrix(X_val, label=y_val)\n    \n    max_depth = trial.suggest_int('max_depth', 3, 6)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.1, 0.5)\n    subsample = trial.suggest_uniform('subsample', 0.1, 1)\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.1, 1)\n    num_parallel_tree = trial.suggest_int('num_parallel_tree', 1, 2)\n    min_child_weight = trial.suggest_uniform('min_child_weight', 1, 250)\n    lambd = trial.suggest_uniform('lambd', 1, 1.1)\n    alpha = trial.suggest_uniform('alpha', 0, 0.2)\n    num_round = 1000\n    \n    param = {'max_depth':max_depth,\n             'learning_rate':learning_rate,\n             'objective':'reg:squarederror',\n             'subsample':subsample,\n             'colsample_bytree':colsample_bytree,\n             'num_parallel_tree':num_parallel_tree,\n             'lambda':lambd,\n             'alpha':alpha,\n            'tree_method':'gpu_hist'}\n    \n    bst = xgb.train(param, dtrain, num_round, early_stopping_rounds=2, evals=[(dval, \"eval\")])\n    return float(bst.eval(dval).split(\":\")[1])","c369e44b":"study = optuna.create_study()\n\nstudy.optimize(lambda trial: objective(trial, X_train, X_val, y_train, y_val), n_trials=5)","56937cdc":"study.best_params","f9bac38f":"Possible future steps\n# Feature engineering\n# Cross validation\n# Adding a Neural Network to the ensemble(Pytorch\/Tensorflow)\n# Model stacking","94fec044":"# What are some simple commands for notebooks?\nCTRL+ENTER - run the current cell  \nLEFT CLICK ON CELL\/ENTER - start text editing cell  \nLEFT CLICK OFF CELL\/ESC - stop text editing cell  \nb - create new code cell","95ba8e69":"# If you are already familiar with boosting trees:\nHave a look at the hyper parameter list for XGBoost here: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html  \nWe will be doing a hyper parameter search later, so make a shortlist of what you think would be good to experiment with!  \n# If you are not familiar with boosting trees:\nI will run through a high level overview"}}