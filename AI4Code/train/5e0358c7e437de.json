{"cell_type":{"8a7f6b60":"code","95b710b5":"code","13fab38e":"code","57fbeb04":"code","49157e05":"code","f2b1fe8d":"code","82efd7e0":"code","3bf1302c":"code","f18b65fa":"code","b2f69076":"code","075ccf91":"markdown","9810e65f":"markdown","5297c511":"markdown","ea410f75":"markdown","cf8f318a":"markdown","77375883":"markdown","00526571":"markdown","325cb42c":"markdown","92ba7c6c":"markdown"},"source":{"8a7f6b60":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax","95b710b5":"train = pd.read_csv(\"..\/input\/support-data\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/support-data\/test.csv\", index_col=0)\n#print(train.head())\n#print(train.shape)\n#print(test.shape)","13fab38e":"plt.scatter(train[\"GrLivArea\"], train[\"SalePrice\"], alpha=0.9)\nplt.xlabel(\"Ground living area\")\nplt.ylabel(\"Sale price\")\nplt.show()\n\ntrain = train[train[\"GrLivArea\"] < 4200]","57fbeb04":"X = pd.concat([train.drop(\"SalePrice\", axis=1), test])\n\nplt.hist(train[\"SalePrice\"], bins = 40)\nplt.show()\ny = np.log(train[\"SalePrice\"])\nplt.hist(y, bins = 40)\nplt.show()","49157e05":"nans = X.isna().sum().sort_values(ascending=False)\nnans = nans[nans > 0]\nfig, ax = plt.subplots(figsize=(10, 6))\nax.bar(nans.index, nans.values)\nax.set_ylabel(\"No. of missing values\")\nax.xaxis.set_tick_params(rotation=90)\nplt.show()\n\ncols = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageCond\", \"GarageQual\", \"GarageFinish\", \"GarageType\", \"BsmtCond\", \"BsmtExposure\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\"]\nX[cols] = X[cols].fillna(\"None\")\n\ncols = [\"GarageYrBlt\", \"MasVnrArea\", \"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\"]\nX[cols] = X[cols].fillna(X[cols].mean())\ncols = [\"MasVnrType\", \"MSZoning\", \"Utilities\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"KitchenQual\", \"Functional\"]\nX[cols] = X.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))\ncols = [\"GarageArea\", \"LotFrontage\"]\nX[cols] = X.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mean()))\n\nnans = X.isna().sum().sort_values(ascending=False)\nnans = nans[nans > 0]\nfig, ax = plt.subplots(figsize=(10, 6))\nax.bar(nans.index, nans.values)\nax.set_ylabel(\"No. of missing values\")\nax.xaxis.set_tick_params(rotation=90)\nplt.show()\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in X.columns:\n    if X[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\nskew_features = X[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    X[i] = boxcox1p(X[i], boxcox_normmax(X[i] + 1))\n\n    X[\"TotalSF\"] = X[\"GrLivArea\"] + X[\"TotalBsmtSF\"]\nX[\"TotalPorchSF\"] = X[\"OpenPorchSF\"] + X[\"EnclosedPorch\"] + X[\"3SsnPorch\"] + X[\"ScreenPorch\"]\nX[\"TotalBath\"] = X[\"FullBath\"] + X[\"BsmtFullBath\"] + 0.5 * (X[\"BsmtHalfBath\"] + X[\"HalfBath\"])\nX['Total_sqr_footage'] = X['BsmtFinSF1'] + X['BsmtFinSF2'] + X['1stFlrSF'] + X['2ndFlrSF']\n\ncols = [\"MSSubClass\", \"YrSold\"]\nX[cols] = X[cols].astype(\"category\")\n\nX = X.drop(['Utilities', 'Street', 'PoolQC', ], axis=1)\n\ncols = X.select_dtypes(np.number).columns\nX[cols] = RobustScaler().fit_transform(X[cols])\n\n#X['haspool'] = X['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n#X['has2ndfloor'] = X['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n#X['hasgarage'] = X['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n#X['hasbsmt'] = X['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n#X['hasfireplace'] = X['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nX = pd.get_dummies(X)","f2b1fe8d":"X_train1 = X.loc[train.index]\nX_test_sub = X.loc[test.index]\n\noverfit = []\nfor i in X_train1.columns:\n    counts = X_train1[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\nX_train1 = X_train1.drop(overfit, axis=1).copy()\nX_test_sub = X_test_sub.drop(overfit, axis=1).copy()\n\ntrain_size = 0.8\nseparator = round(len(X_train1.index)*train_size)\n\n\nX_train, y_train = X_train1.iloc[0:separator], y.iloc[0:separator]\nX_test, y_test = X_train1.iloc[separator:], y.iloc[separator:]","82efd7e0":"gbr = ensemble.GradientBoostingRegressor(learning_rate=0.02, n_estimators=2000,\n                                           max_depth=5, min_samples_split=2,\n                                           loss='ls', max_features=35)\n\nxgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                       max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror', nthread=-1,\n                       scale_pos_weight=1, seed=27,\n                       reg_alpha=0.00006)\n\nlgbm = LGBMRegressor(objective='regression',\n                         num_leaves=4,\n                         learning_rate=0.01,\n                         n_estimators=5000,\n                         max_bin=200,\n                         bagging_fraction=0.75,\n                         bagging_freq=5,\n                         bagging_seed=7,\n                         feature_fraction=0.2,\n                         feature_fraction_seed=7,\n                         verbose=-1,\n                                       )\n\ncb = CatBoostRegressor(loss_function='RMSE', logging_level='Silent')","3bf1302c":"def mean_cross_val(model, X, y):\n    score = cross_val_score(model, X, y, cv=5)\n    mean = score.mean()\n    return mean\n\ngbr.fit(X_train, y_train)   \npreds = gbr.predict(X_test) \npreds_test_gbr = gbr.predict(X_test_sub)\nmae_gbr = mean_absolute_error(y_test, preds)\nrmse_gbr = np.sqrt(mean_squared_error(y_test, preds))\nscore_gbr = gbr.score(X_test, y_test)\ncv_gbr = mean_cross_val(gbr, X_train1, y)\n\nlgbm.fit(X_train, y_train)   \npreds = lgbm.predict(X_test) \npreds_test_lgbm = lgbm.predict(X_test_sub)\nmae_lgbm = mean_absolute_error(y_test, preds)\nrmse_lgbm = np.sqrt(mean_squared_error(y_test, preds))\nscore_lgbm = lgbm.score(X_test, y_test)\ncv_lgbm = mean_cross_val(lgbm, X_train1, y)\n\ncb.fit(X_train, y_train)   \npreds = cb.predict(X_test) \npreds_test_cb = cb.predict(X_test_sub)\nmae_cb = mean_absolute_error(y_test, preds)\nrmse_cb = np.sqrt(mean_squared_error(y_test, preds))\nscore_cb = cb.score(X_test, y_test)\ncv_cb = mean_cross_val(cb, X_train1, y)","f18b65fa":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Gradient Boosting Regression\", \"XGBoost\", \"LGBM\", \"CatBoost\"],\n    \"CV(5)\" : [str(cv_gbr)[0:5], 'NaN', str(cv_lgbm)[0:5], str(cv_cb)[0:5]],\n    \"MAE\" : [str(mae_gbr)[0:5], 'NaN', str(mae_lgbm)[0:5], str(mae_cb)[0:5]],\n    \"RMSE\" : [str(rmse_gbr)[0:5], 'NaN', str(rmse_lgbm)[0:5], str(rmse_cb)[0:5]],\n    \"Score\" : [str(score_gbr)[0:5], 'NaN', str(score_lgbm)[0:5], str(score_cb)[0:5]]\n})\n\nprint(\"Sorted by Score:\")\nprint(model_performances.sort_values(by=\"Score\", ascending=False))\n\nsub_1 = pd.read_csv('..\/input\/support-data\/House_Prices_submit.csv')\n\n\ndef blend_models_predict(X, a, b, c, d):\n    cb_pred = cb.predict(X)\n    sb = np.log(np.array(sub_1.iloc[:,1]))\n\n    \n    return ((a * gbr.predict(X)) +  (b * sb) +  (c * lgbm.predict(X)) + (d * cb_pred))\n\"\"\"\nf = open('Comb.txt', 'w')\nfor i1 in np.arange(0, 1.1, 0.05):\n    iter=0\n    print('I1=', round(i1,2))\n    for i2 in np.arange(0, 1.1-i1, 0.05):\n        for i3 in np.arange(0, 1.1-i1-i2, 0.05):\n            i1 = round(i1, 2)\n            i2 = round(i2, 2)\n            i3 = round(i3, 2)\n            i4 = 1 - i1 - i2 - i3\n            i4 = round(i4, 2)\n            if (i1+i2+i3+i4<=1) & (i4>=-0.0):\n                subm = (blend_models_predict(X_test, i1, i2, i3, i4))\n                mae_comb = mean_absolute_error(y_test, subm)\n                rmse_comb = np.sqrt(mean_squared_error(y_test, subm))\n                f.write('I1 ='+str(i1)+' I2 ='+str(i2)+' I3 ='+str(i3)+' I4 ='+str(i4)+'\\n')\n                f.write('MAE ='+ str(mae_comb)+'\\n')\n                f.write('RMSE ='+ str(rmse_comb)+'\\n')\n                f.write('-----------------\\n')\n                iter=iter+1\n    print('Iteration ', iter)\n    print('-----------------')\nf.close()  \n\"\"\"","b2f69076":"subm = np.exp(blend_models_predict(X_test_sub, 0.1, 0.4, 0.1, 0.4))\n\nsubmission = pd.DataFrame({'Id': X_test_sub.index,\n                       'SalePrice': subm})\n\nq1 = submission['SalePrice'].quantile(0.0042)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(\"submission.csv\", index=False)","075ccf91":"Here I will describe how in a very short time you can rise in the **Top 1%**. Already there are enough beautiful kernels, so I will try not to stretch much, briefly tell and show you my code.\n\n*At first I used LinRegressor, but it did not give satisfactory results. Since I was still studying, I decided to read about new mechanics and decided to try blending. So, let's go:\n*\n\n**Step 1. Log conversion of the dependent variable.** There are several reasons for predicting a logarithmic value, rather than an ordinary one: many currency units have a distribution close to a logarithmically normal one, i.e. it\u2019s better to predict it, which has not so heavy tails (I tried to predict the usual values, and the results were worse).\n\n**Do not forget to take the predict exponent to remove the logarithm!\n**\n\n**Step 2. Work with passes and outliers.** Obviously, I had to fill in the missing values \u200b\u200b- gaps in the numeric predictors, which I filled with zeros when it made sense or the median of this parameter (for example, if there are no bathtubs in the basement, then this parameter should be zero), omissions in categorical predictors, which I filled with modes or other typical values. Many users advise deleting outlier observations in some predictors - this makes some sense. I advise you to play around with this, you can add good results.\n\n**Step 2.5.** Also, you should think carefully about all the outliers, I did not work very well on this.\n\n**Step 3. New predictors.** If you want to get a good result, you need to study the entire dataset and each parameter very well. Perhaps some of them can be grouped in such a way that you get a new quality, which in the end will give a good boost to your rating. There are some good kernels where this is described in great detail.\n\n**Step 4. Tuning models.** Initially, I used more models, but later reduced their number. I spent a lot of time for fine-tuning the parameters, maybe it will turn out to improve them even more. So do not spare your time for this.\n\n**Step 5. Blending.** I spent a lot of time on it, since I came across it for the first time. I wrote my own function, which sorted through all combinations of models with different weights and among them I was looking for the best result.\n\n**Step 6. Smooth the tails of the predict.** It may sound ridiculous, but I saw it on other sources and it helped me too.\n\n*Bonus: as a small cheat, I decided to try adding a nice prediction of another participant to blending. This also gave a small increase. But without this, the result is good!)))\n*\n\n\n**Some other things that I tried did not help me:** a) adding more and more new predictors (do not overdo it), b) neural networks, c) remove all outliers, d) GridSearch (I searched for parameters manually))) )","9810e65f":"# Some outliers","5297c511":"# Download data ","ea410f75":"# My models","cf8f318a":"# Log tagret","77375883":"# Detecting best weight to blending","00526571":"# Submit","325cb42c":"# Fit models","92ba7c6c":"# Preprocessing"}}