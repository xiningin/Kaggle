{"cell_type":{"fb67089a":"code","30c7cf6f":"code","98359039":"code","0b08f20f":"code","00f24a6a":"code","a210761b":"code","9deb6b01":"code","201e2573":"code","721e8624":"code","98ad4004":"code","e1cf6a7c":"code","e57cb6a4":"code","94e031f9":"code","7fe9d96f":"code","e72a2c44":"code","38fc34fb":"code","871be7bb":"code","a44c9bdd":"code","03d371ff":"code","43513f5a":"code","330386f5":"code","db686d1d":"code","60e7306e":"code","9d1fc8db":"code","de1e3f08":"code","c19aa2bb":"code","70c0d363":"code","a6e02ae2":"code","818900c0":"code","f3baa58d":"code","33954401":"markdown","74b5393f":"markdown","161f4695":"markdown","d199c48f":"markdown","b2681900":"markdown","4bfa9783":"markdown","03c68586":"markdown","ac2bf67e":"markdown","e2449a27":"markdown","28a94244":"markdown","60ccaac7":"markdown","9c62512d":"markdown","53cbadfa":"markdown","3463f267":"markdown","2a2700ce":"markdown","523acaff":"markdown","b6732fda":"markdown","a6d91dc7":"markdown","4d432e3c":"markdown","28441c3c":"markdown","f6056d23":"markdown","fc0f25a3":"markdown","2a8697bf":"markdown","603cd443":"markdown","b7c05f6f":"markdown","82c16c91":"markdown","4572cf4c":"markdown","cc3bdb06":"markdown","ec7592c7":"markdown","662511e4":"markdown","ce6ccb82":"markdown","d537c0a5":"markdown","4d9f0d15":"markdown","7af04595":"markdown","5001b76f":"markdown","d4878741":"markdown","ec92647b":"markdown","c4138de2":"markdown","783ee400":"markdown","2c64e969":"markdown","608d255e":"markdown","1550e8af":"markdown","1d712f98":"markdown","61e54a1e":"markdown","d51457e0":"markdown","d975a029":"markdown","236811ab":"markdown","1ee717fa":"markdown","b0019d21":"markdown","e63afc2a":"markdown","d6142d6d":"markdown","cb9600b0":"markdown","31f668eb":"markdown","178eccf0":"markdown","6c79510c":"markdown","9616d7e4":"markdown","9ceb4691":"markdown","274dca24":"markdown"},"source":{"fb67089a":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nfrom statsmodels.stats.proportion import proportions_ztest\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport matplotlib.pyplot as plt\nimport warnings","30c7cf6f":"df = pd.read_csv('..\/input\/kaggle-ml-survey-data-2018-to-2021\/Kaggle ML Survey data 2018 to 2021.csv',low_memory=False)\n\ndf[\"Q1\"].replace(dict.fromkeys(['70-79','70+'], '>70'), inplace=True)\n\ndf[\"Q6\"].replace(dict.fromkeys(['Less than  1 years','< 1 years'], '< 1 year'), inplace=True)\ndf[\"Q6\"].replace(dict.fromkeys(['30-40 years','40+ years','20-30 years'], '20+ years'), inplace=True)\ndf[\"Q6\"].replace({\"1-2 years\": \"1-3 years\"}, inplace=True)\ndf[\"Q6\"].replace(dict.fromkeys(['I have never written code but I want to learn','I have never written code and I do not want to learn'], 'I have never written code'), inplace=True)\n\ndf[\"Q2\"].replace({\"Male\": \"Man\", \"Female\": \"Woman\"}, inplace=True)\n\ndf['Income'] = df[\"Q25\"].replace(dict.fromkeys(['$0-999','2,000-2,999','1,000-1,999','4,000-4,999','3,000-3,999','5,000-7,499','7,500-9,999'], '0-10,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['10,000-14,999','15,000-19,999','10-20,000'], '10,000-20,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['20,000-24,999','25,000-29,999','20-30,000'], '20,000-30,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['30,000-39,999','30-40,000'], '30,000-40,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['40,000-49,999','40-50,000'], '40,000-50,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['50-60,000','50,000-59,999','60-70,000','60,000-69,999'], '50,000-70,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['90-100,000', '70-80,000', '80-90,000','70,000-79,999','80,000-89,999','90,000-99,999'], '70,000-100,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['100-125,000', '125-150,000', '100,000-124,999','125,000-149,999'], '100,000-150,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['150-200,000', '200-250,000', '250-300,000', '150,000-199,999','200,000-249,999','250,000-299,999'], '150,000-300,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['300-400,000', '400-500,000', '300,000-499,999'], '300,000-500,000'))\ndf['Income'] = df[\"Income\"].replace(dict.fromkeys(['> $500,000', '500,000+', '$500,000-999,999','More than $1,000,000'], '500,000+'))\n\nQ7_cols = ['Q7_Part_1','Q7_Part_2','Q7_Part_3','Q7_Part_4','Q7_Part_5','Q7_Part_6','Q7_Part_7','Q7_Part_8','Q7_Part_9','Q7_Part_10','Q7_Part_11','Q7_Part_12','Q7_OTHER']\nQ7_cols_desc = ['Python','R','SQL','C','C++','Java','JavaScript','Julia','Swift','Bash','MATLAB','None','Other']\n\nQ18_cols = ['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_OTHER']\nQ18_cols_desc = ['General purpose image\/video','Image segmentation',\n'Object detection',\n'Image classification',\n'Generative Networks',\n'None',\n'Other']\n\nQ1_cols = ['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','>70']\n\ncolors = ['blueviolet','coral','darkgreen','mediumblue','orange','purple','yellow','springgreen','lightpink','lightgray', 'red', 'rosybrown', 'royalblue','magenta','maroon','mediumaquamarine','mediumblue','plum']","98359039":"# Functions to return Z-test results:\n\ndef z_test(table1,columns):\n    table3  = table1.copy()\n    for i in columns:\n        for j in range(len(table1)):\n            stat,pval = proportions_ztest(np.array([table1.loc[j,i],table1.loc[table1['Year']=='2021',i]]),\n                                          np.array([table1.loc[j,'Total'],table1.loc[table1['Year']=='2021','Total']]), value=None, alternative='two-sided', prop_var=False)\n            table3.loc[j,i] = pval*100\n    for i in columns:\n        text = 'Significant change with year(s): '\n        year_count=0\n        for j in range(len(table3)-1):\n            if (table3.loc[j,i]<5) & (table1.loc[j,i]\/table1.loc[j,'Total']>0.01):\n                text = text + \" \" + table3.loc[j,'Year']\n                year_count=year_count+1\n        if year_count>0:\n            table3.loc[table3['Year']=='2021',i] = text\n        else:\n            table3.loc[table3['Year']=='2021',i] = 'No significant change over years'\n    table3.loc[table3['Year']!='2021',columns] = ''\n    return table3\n\n# Function for stacked column chart:\n\ndef stacked_column(table2,i,x_col,table3,table_tot,j,legend):\n    return go.Bar(x = table2[x_col],\n                     y = table2.loc[:,i],\n                     orientation='v',\n                     name = i,\n                     text= table2.loc[:,i].where(table2.loc[:,i]>3,''),\n                     textangle = 0,\n                     customdata = table3.loc[:,i],\n                     texttemplate='<b>%{text}<b>', \n                     hovertemplate = 'Year: <b>%{x}<\/b><br>Cumulative percentage users: <b>%{y}<\/b><b>%<\/b><br><b>%{customdata}<\/b>',\n                     textposition='inside',\n                     textfont_size=10,\n                     cliponaxis = False,\n                     marker = dict(color = colors[j]),\n                     offsetgroup=0\n                     ,base = table_tot,\n                     showlegend=legend\n                     )\n\n# Function for horizontal bar chart:\n\ndef bar_chart(table2,i,x_col,table3,table_tot,j,legend):\n    return go.Bar(x =table2.loc[:,i] ,\n                     y =table2[x_col],\n                     orientation='h',\n                     name = i,\n                     text= table2.loc[:,i].where(table2.loc[:,i]>3,'').astype(str),\n                     textangle = 0,\n                     customdata = table3.loc[:,i],\n                     texttemplate='<b>%{text}<b>', \n                     hovertemplate = 'Year: <b>%{y}<\/b><br>Percentage users: <b>%{x}<\/b><b>%<\/b><br><b>%{customdata}<\/b>',\n                     textposition='auto',\n                     textfont_size=10,\n                     cliponaxis = False,\n                     marker = dict(color = colors[j]),\n                     offsetgroup=j,\n                     showlegend=legend\n                     )","0b08f20f":"# Distribution of Income across countries\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \nIncome_cols = ['0-10,000','10,000-20,000','20,000-30,000','30,000-40,000','40,000-50,000','50,000-70,000','70,000-100,000','100,000-150,000','150,000-300,000','300,000-500,000','500,000+']\n\ntable1 = df.pivot_table(values='User', index=['Year','Q3'], columns=['Income'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Income_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Income_cols].sum(axis=1)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Income distribution across Countries<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=400,\n                 width = 900,legend_traceorder = \"reversed\")\n\nfig.show()","00f24a6a":"# Distribution of Income across countries\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \nIncome_cols = ['0-10,000','10,000-20,000','20,000-30,000','30,000-40,000','40,000-50,000','50,000-70,000','70,000-100,000','100,000-150,000','150,000-300,000','300,000-500,000','500,000+']\n\nAbove_100 = ['100,000-150,000','150,000-300,000','300,000-500,000','500,000+']\n\ntable1 = df.pivot_table(values='User', index=['Year','Q3'], columns=['Income'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Income_cols]\n\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Income_cols].sum(axis=1)\n\ntable2['Above_100'] = table2[Above_100].sum(axis=1)\nTop_5 = table2[(table2.Year=='2021')&(table1.Total>50)].sort_values(by = ['Above_100'],ascending=False)\nTop_5 = Top_5.iloc[0:4,:].Q3\n\nBottom_5 = table2[(table2.Year=='2021')&(table1.Total>50)].sort_values(by = ['0-10,000'],ascending=False)\nBottom_5 = Bottom_5.iloc[0:4,:].Q3\n\ncategories = list(Top_5)\n\nfig = go.Figure()\n\nfig = make_subplots(rows=2, cols=len(categories),shared_yaxes=True,subplot_titles=categories + list(Bottom_5))\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \ncategories = list(Bottom_5)\n\n\ncol=1\nlegend=False\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=2,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n    \nfig.update_layout(title =\"<b>Highest and lowest paying countries in 2021 for Data Science<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=700,\n                 width = 1000,legend_traceorder = \"reversed\")\n\nfig.show()","a210761b":"# Distribution of Age across countries\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \n    \ntable1 = df.pivot_table(values='User', index=['Year','Q3'], columns=['Q1'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Q1_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Q1_cols].sum(axis=1)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q1_cols)\n\n    for i in Q1_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Age distribution across Countries<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=400,\n                 width = 900,legend_traceorder = \"reversed\")\n\nfig.show()","9deb6b01":"# Distribution of Gender across countries\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n\nQ2_cols = ['Man', 'Woman', 'Nonbinary', 'Prefer not to say','Prefer to self-describe']\n    \ntable1 = df.pivot_table(values='User', index=['Year','Q3'], columns=['Q2'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Q2_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Q2_cols].sum(axis=1)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q2_cols)\n\n    for i in Q2_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Gender distribution<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=400,\n                 width = 1000,legend_traceorder=\"reversed\")\n\nfig.show()","201e2573":"# Distribution of Income across Demogs\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \nIncome_cols = ['0-10,000','10,000-20,000','20,000-30,000','30,000-40,000','40,000-50,000','50,000-70,000','70,000-100,000','100,000-150,000','150,000-300,000','300,000-500,000','500,000+']\n\ndf['Age_buckets'] = df['Q1'].replace(dict.fromkeys(['18-21','22-24'], '18-24'))\ndf['Age_buckets'] = df[\"Age_buckets\"].replace(dict.fromkeys(['25-29','30-34'], '25-34'))\ndf['Age_buckets'] = df[\"Age_buckets\"].replace(dict.fromkeys(['35-39','40-44','45-49'], '35-49'))\ndf['Age_buckets'] = df[\"Age_buckets\"].replace(dict.fromkeys(['50-54','55-59','60-69','>70'], '>50'))\n\ntable1 = df[df.Q3=='India'].pivot_table(values='User', index=['Year','Age_buckets'], columns=['Income'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Income_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Income_cols].sum(axis=1)\n\n\ncategories = ['18-24','25-34','35-49','>50']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=2, cols=len(categories),shared_yaxes=True,subplot_titles=categories + ['','Woman','Man'])\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Age_buckets==cat].reset_index(drop=True)\n    table1_new = table1[table1.Age_buckets==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n        \n# Plot for Gender\n        \ntable1 = df[df.Q3=='India'].pivot_table(values='User', index=['Year','Q2'], columns=['Income'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Income_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Income_cols].sum(axis=1)\n\n\ncategories = ['Woman','Man']    \n\ncol=1\nlegend=False\nfor cat in categories:\n    table2_new = table2[table2.Q2==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q2==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=2,col=col+1)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n\n\nfig.update_layout(title =\"<b>India's income distribution across Age and Gender<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=800,\n                 width = 1000,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"v\",\n    yanchor=\"bottom\",\n    y=0,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","721e8624":"# Distribution of Coding experience across countries\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \nQ6_cols = ['< 1 year','1-3 years','3-5 years','5-10 years','10-20 years','20+ years','I have never written code']\n\ntable1 = df.pivot_table(values='User', index=['Year','Q3'], columns=['Q6'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Q6_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Q6_cols].sum(axis=1)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q6_cols)\n\n    for i in Q6_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Coding or Programming Experience distribution<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=400,\n                 width = 1000,legend_traceorder=\"reversed\")\n\nfig.show()","98ad4004":"# India's income distribution by Coding and ML experience\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \nIncome_cols = ['0-10,000','10,000-20,000','20,000-30,000','30,000-40,000','40,000-50,000','50,000-70,000','70,000-100,000','100,000-150,000','150,000-300,000','300,000-500,000','500,000+']\n\nQ6_cols = ['< 1 year','1-3 years','3-5 years','5-10 years','10-20 years','20+ years','I have never written code']\n\ntable1 = df[df.Q3=='India'].pivot_table(values='User', index=['Year','Q6'], columns=['Income'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Income_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Income_cols].sum(axis=1)\n\n\ncategories = Q6_cols\n\nfig = go.Figure()\n\nfig = make_subplots(rows=2, cols=4,shared_yaxes=True,subplot_titles=categories)\n\nrow=1\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q6==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q6==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=row,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    if col>4:\n        row=2\n        col=1\n    \n\n\nfig.update_layout(title =\"<b>India's income distribution by Coding experience<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=800,\n                 width = 1000,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"v\",\n    yanchor=\"bottom\",\n    y=0,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","e1cf6a7c":"# India's income distribution by ML experience\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \nIncome_cols = ['0-10,000','10,000-20,000','20,000-30,000','30,000-40,000','40,000-50,000','50,000-70,000','70,000-100,000','100,000-150,000','150,000-300,000','300,000-500,000','500,000+']\n\ndf[\"Q15\"].replace(dict.fromkeys(['Under 1 year','< 1 years'], '< 1 year'), inplace=True)\ndf[\"Q15\"].replace(dict.fromkeys(['3-4 years','4-5 years'], '3-5 years'), inplace=True)\ndf[\"Q15\"].replace(dict.fromkeys(['10-15 years','10-20 years','20 or more years'], '10+ years'), inplace=True)\ndf[\"Q15\"].replace(dict.fromkeys(['I do not use machine learning methods','I have never studied machine learning but plan to learn in the future','I have never studied machine learning and I do not plan to'], 'I do not use ML methods'), inplace=True)\n\nQ15_cols = ['< 1 year','2-3 years','3-5 years','5-10 years','10+ years','I do not use ML methods']\n\ntable1 = df[df.Q3=='India'].pivot_table(values='User', index=['Year','Q15'], columns=['Income'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Income_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Income_cols].sum(axis=1)\n\n\ncategories = Q15_cols\n\nfig = go.Figure()\n\nfig = make_subplots(rows=2, cols=4,shared_yaxes=True,subplot_titles=categories)\n\nrow=1\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q15==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q15==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=row,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    if col>4:\n        row=2\n        col=1\n    \n\n\nfig.update_layout(title =\"<b>India's income distribution by Machine learning experience<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=800,\n                 width = 1000,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"v\",\n    yanchor=\"bottom\",\n    y=0,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","e57cb6a4":"# Distribution of Age across countries\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \ndf[\"Q21\"].replace(dict.fromkeys(['10,000 or more employees'], '> 10,000 employees'), inplace=True)\n\nQ21_cols = ['0-49 employees','50-249 employees','250-999 employees','1000-9,999 employees','> 10,000 employees']\n    \ntable1 = df.pivot_table(values='User', index=['Year','Q3'], columns=['Q21'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Q21_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Q21_cols].sum(axis=1)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q21_cols)\n\n    for i in Q21_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Size of the company<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=400,\n                 width = 900,legend_traceorder=\"reversed\")\n\nfig.show()","94e031f9":"# India's income distribution by ML experience\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n    \nIncome_cols = ['0-10,000','10,000-20,000','20,000-30,000','30,000-40,000','40,000-50,000','50,000-70,000','70,000-100,000','100,000-150,000','150,000-300,000','300,000-500,000','500,000+']\n\nQ21_cols = ['0-49 employees','50-249 employees','250-999 employees','1000-9,999 employees','> 10,000 employees']\n\ntable1 = df[df.Q3=='India'].pivot_table(values='User', index=['Year','Q21'], columns=['Income'], aggfunc=lambda x: len(x.unique()))\n\ntable1 = table1[Income_cols]\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1.div(table1.sum(axis=1), axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable1['Total'] = table1[Income_cols].sum(axis=1)\n\n\ncategories = Q21_cols\n\nfig = go.Figure()\n\nfig = make_subplots(rows=2, cols=3,shared_yaxes=True,subplot_titles=categories)\n\nrow=1\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q21==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q21==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Income_cols)\n\n    for i in Income_cols:\n        fig.add_trace(stacked_column(table2_new,i,'Year',table3,table_tot,j,legend),row=row,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    if col>3:\n        row=2\n        col=1\n    \n\n\nfig.update_layout(title =\"<b>India's income distribution by Company size<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=800,\n                 width = 900,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"v\",\n    yanchor=\"bottom\",\n    y=0,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","7fe9d96f":"# Driver analysis\n\nresult = {}\nresult1 = {}\n\ntrain_columns = ['Q1','Q2','Q4','Q5','Q6','Q7_Part_1','Q7_Part_2','Q7_Part_3','Q7_Part_4','Q7_Part_5','Q7_Part_6','Q7_Part_7','Q7_Part_8','Q7_Part_9','Q7_Part_10','Q7_Part_11','Q7_Part_12','Q7_OTHER','Q9_Part_1','Q9_Part_2','Q9_Part_3','Q9_Part_4','Q9_Part_5','Q9_Part_6','Q9_Part_7','Q9_Part_8','Q9_Part_9','Q9_Part_10','Q9_Part_11','Q9_Part_12','Q9_OTHER','Q10_Part_1','Q10_Part_2','Q10_Part_3','Q10_Part_4','Q10_Part_5','Q10_Part_6','Q10_Part_7','Q10_Part_8','Q10_Part_9','Q10_Part_10','Q10_Part_11','Q10_Part_12','Q10_Part_13','Q10_Part_14','Q10_Part_15','Q10_Part_16','Q10_OTHER','Q11','Q14_Part_1','Q14_Part_2','Q14_Part_3','Q14_Part_4','Q14_Part_5','Q14_Part_6','Q14_Part_7','Q14_Part_8','Q14_Part_9','Q14_Part_10','Q14_Part_11','Q14_OTHER','Q15','Q16_Part_1','Q16_Part_2','Q16_Part_3','Q16_Part_4','Q16_Part_5','Q16_Part_6','Q16_Part_7','Q16_Part_8','Q16_Part_9','Q16_Part_10','Q16_Part_11','Q16_Part_12','Q16_Part_13','Q16_Part_14','Q16_Part_15','Q16_Part_16','Q16_Part_17','Q16_OTHER','Q17_Part_1','Q17_Part_2','Q17_Part_3','Q17_Part_4','Q17_Part_5','Q17_Part_6','Q17_Part_7','Q17_Part_8','Q17_Part_9','Q17_Part_10','Q17_Part_11','Q17_OTHER','Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_OTHER','Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER','Q20','Q21','Q22','Q23','Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_OTHER','Q40_Part_1','Q40_Part_2','Q40_Part_3','Q40_Part_4','Q40_Part_5','Q40_Part_6','Q40_Part_7','Q40_Part_8','Q40_Part_9','Q40_Part_10','Q40_Part_11','Q40_OTHER','Q41','Q27_A_Part_1','Q27_A_Part_2','Q27_A_Part_3','Q27_A_Part_4','Q27_A_Part_5','Q27_A_Part_6','Q27_A_Part_7','Q27_A_Part_8','Q27_A_Part_9','Q27_A_Part_10','Q27_A_Part_11','Q27_A_OTHER']\ntrain_columns_cat = ['Demographics','Demographics','Formal Education','Job profile','ML\/Coding experience','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Other','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','ML\/Coding experience','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Industry\/Company size','Industry\/Company size','Industry\/Company size','Industry\/Company size','Job profile','Job profile','Job profile','Job profile','Job profile','Job profile','Job profile','Job profile','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Job profile','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge','Tools and technology knowledge']\ntrain_columns_subcat = ['Age','Gender','Formal Education','Job Title','Coding Experience','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','Programming Language','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','IDEs','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Hosted Notebook','Other','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','Visualization libraries','ML Experience','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Frameworks','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','ML Algorithms','Computer vision methods','Computer vision methods','Computer vision methods','Computer vision methods','Computer vision methods','Computer vision methods','Computer vision methods','NLP methods','NLP methods','NLP methods','NLP methods','NLP methods','NLP methods','Industry','Company Size','Company Size','Industry','Job Activities','Job Activities','Job Activities','Job Activities','Job Activities','Job Activities','Job Activities','Job Activities','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Online learning','Job Activities','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms','Cloud computing Platforms']\n\ncountries = ['India','United States of America','Brazil']\nYears = [2019,2020,2021]\nImportance_features = ['Industry\/Company size','Demographics','Job profile','ML\/Coding experience','Formal Education','Tools and technology knowledge','Online learning','Other']\n\n\nmodel = GradientBoostingClassifier(n_estimators=200,learning_rate=0.05,max_features=12,subsample=0.7)\n\nfor x in countries:\n    \n    \n    df_importance = pd.DataFrame(columns = ['Features','Categories','Sub_categories','2019','2020','2021'])\n    df_importance[\"Features\"] = train_columns\n    df_importance[\"Categories\"] = train_columns_cat\n    df_importance[\"Sub_categories\"] = train_columns_subcat\n    for y in Years:\n        train = df[(df.Q3==x) & (df.Year ==y)][train_columns+['Q25']]\n        train['Q25'] = train.Q25.replace('0',np.nan)\n        train = train[train.Q25.notna()]\n\n        for cols in train.columns:\n            train[cols] = train[cols].replace(0.0,'0')\n            train[cols] = label_encoder.fit_transform(train[cols])\n\n        X = train[train_columns]\n        Y = train.Q25\n        model.fit(X, Y)\n\n        importances = model.feature_importances_\n        df_importance[str(y)] = importances\n    result[x] = df_importance\n    result1[x] = result[x].groupby(by='Categories').sum().reset_index()","e72a2c44":"categories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories,specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"},{\"type\": \"pie\"}]])\n\ncol=1\nPlot_year = '2021'\nfor x in categories:   \n    \n    \n    fig.add_trace(go.Pie(labels = result1[x]['Categories'],\n                   values = result1[x][Plot_year],\n                   hole = .5,\n                 hoverinfo='label+percent', textinfo='percent'),row=1,col=col)\n    \n    col = col+1\n\nfig.update_layout(title =\"<b>Key Drivers in determining Income across countries<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.5,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","38fc34fb":"categories = ['2019','2020','2021']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories,specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"},{\"type\": \"pie\"}]])\n\ncol=1\nCountry = 'India'\nfor x in categories:   \n    \n    \n    fig.add_trace(go.Pie(labels = result1[Country]['Categories'],\n                   values = result1[Country][x],\n                   hole = .5,\n                 hoverinfo='label+percent', textinfo='percent'),row=1,col=col)\n    \n    col = col+1\n\nfig.update_layout(title =\"<b>Income drivers in India by year<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.5,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","871be7bb":"categories = ['India','United States of America','Brazil']\n\nresult2={}\nfor x in categories:\n    result2[x] = result[x][result[x].Categories == 'Tools and technology knowledge'].groupby(by='Sub_categories').sum().reset_index()\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories,specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"},{\"type\": \"pie\"}]])\n\ncol=1\nPlot_year = '2021'\nfor x in categories:   \n    \n    \n    fig.add_trace(go.Pie(labels = result2[x]['Sub_categories'],\n                   values = result2[x][Plot_year],\n                   hole = .5,\n                 hoverinfo='label+percent', textinfo='percent'),row=1,col=col)\n    \n    col = col+1\n\nfig.update_layout(title =\"<b>Feature importance among Tools and Technologies<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.5,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","a44c9bdd":"countries = ['India','United States of America','Brazil']\ncategories = ['2019','2020','2021']\n\nresult2={}\nfor x in countries:\n    result2[x] = result[x][result[x].Categories == 'Tools and technology knowledge'].groupby(by='Sub_categories').sum().reset_index()\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories,specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"},{\"type\": \"pie\"}]])\n\ncol=1\nCountry = 'India'\nfor x in categories:   \n    \n    \n    fig.add_trace(go.Pie(labels = result2[Country]['Sub_categories'],\n                   values = result2[Country][x],\n                   hole = .5,\n                 hoverinfo='label+percent', textinfo='percent'),row=1,col=col)\n    \n    col = col+1\n\nfig.update_layout(title =\"<b>Important Drivers among Tools and Technologies in India<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.5,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","03d371ff":"countries = ['India','United States of America','Brazil']\ncategories = ['2019','2020','2021']\n\nresult2={}\nfor x in countries:\n    result2[x] = result[x][result[x].Categories == 'Tools and technology knowledge'].groupby(by='Sub_categories').sum().reset_index()\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories,specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"},{\"type\": \"pie\"}]])\n\ncol=1\nCountry = 'United States of America'\nfor x in categories:   \n    \n    \n    fig.add_trace(go.Pie(labels = result2[Country]['Sub_categories'],\n                   values = result2[Country][x],\n                   hole = .5,\n                 hoverinfo='label+percent', textinfo='percent'),row=1,col=col)\n    \n    col = col+1\n\nfig.update_layout(title =\"<b>Important Drivers among Tools and Technologies in USA<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.5,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","43513f5a":"#Job profile\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\nQ24_cols = ['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_OTHER']\nQ24_cols_desc = ['Analyze and understand data to influence product or business decisions','Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data','Build prototypes to explore applying machine learning to new areas','Build and\/or run a machine learning service that operationally improves my product or workflows','Experimentation and iteration to improve existing ML models','Do research that advances the state of the art of machine learning','None of these activities are an important part of my role at work','Other']\ndf_n = df[['User','Year','Q3']+Q24_cols]\ndf_n.rename(columns=dict(zip(Q24_cols, Q24_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q24_cols_desc]!='0').sum(axis=1)\ndf_n[Q24_cols_desc +['Total']] = df_n[Q24_cols_desc + ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q24_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q24_cols_desc)\n\n    for i in Q24_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Job description<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=1000,\n                 width = 1000,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.3,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","330386f5":"Q24_cols = ['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_OTHER']\nQ24_cols_desc = ['Analyze and understand data to influence product or business decisions','Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data','Build prototypes to explore applying machine learning to new areas','Build and\/or run a machine learning service that operationally improves my product or workflows','Experimentation and iteration to improve existing ML models','Do research that advances the state of the art of machine learning','None of these activities are an important part of my role at work','Other']\nMulti_cols = ['Single task','2-3','>3']\n\ndf_n = df[df.Year!=2018][['User','Year','Q3']+Q24_cols]\ndf_n.rename(columns=dict(zip(Q24_cols, Q24_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q24_cols_desc]!='0').sum(axis=1)\ndf_n['Single task'] = np.int64(df_n.Total==1)\ndf_n['2-3'] = np.int64((df_n.Total>1) & (df_n.Total<4))\ndf_n['>3'] = np.int64(df_n.Total>3)\ndf_n[Q24_cols_desc + Multi_cols + ['Total']] = df_n[Q24_cols_desc+Multi_cols + ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\ntable2 = round(table1[Q24_cols_desc + Multi_cols].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable2['More_than_1'] = table2[['2-3','>3']].sum(axis=1)\nTop = table2[(table2.Year=='2021')&(table1.Total>500)].sort_values(by = ['More_than_1'],ascending=False)\nTop = Top.iloc[0,:].Q3\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q24_cols_desc + Multi_cols)\n\n    for i in Multi_cols:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Multiple tasks at Work<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=500,\n                 width = 900,legend_traceorder=\"reversed\")\n\nfig.show()","db686d1d":"#Online courses\n\nQ40_cols = ['Q40_Part_1','Q40_Part_2','Q40_Part_3','Q40_Part_4','Q40_Part_5','Q40_Part_6','Q40_Part_7','Q40_Part_8','Q40_Part_9','Q40_Part_10','Q40_Part_11','Q40_OTHER']\nQ40_cols_desc = ['Coursera','edX','Kaggle Learn Courses','DataCamp','Fast.ai','Udacity','Udemy','LinkedIn Learning','Cloud-certification programs','University Courses','None','Other']\n\ndf_n = df[df.Year!=2018][['User','Year','Q3']+Q40_cols]\ndf_n.rename(columns=dict(zip(Q40_cols, Q40_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q40_cols_desc]!='0').sum(axis=1)\ndf_n[Q40_cols_desc + ['Total']] = df_n[Q40_cols_desc + ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q40_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q40_cols_desc)\n\n    for i in Q40_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Online learning platforms<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=1000,\n                 width = 1000,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.3,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","60e7306e":"Q40_cols = ['Q40_Part_1','Q40_Part_2','Q40_Part_3','Q40_Part_4','Q40_Part_5','Q40_Part_6','Q40_Part_7','Q40_Part_8','Q40_Part_9','Q40_Part_10','Q40_Part_11','Q40_OTHER']\nQ40_cols_desc = ['Coursera','edX','Kaggle Learn Courses','DataCamp','Fast.ai','Udacity','Udemy','LinkedIn Learning','Cloud-certification programs','University Courses','None','Other']\nMulti_cols = ['Single platform','2-3','>3']\n\ndf_n = df[df.Year!=2018][['User','Year','Q3']+Q40_cols]\ndf_n.rename(columns=dict(zip(Q40_cols, Q40_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q40_cols_desc]!='0').sum(axis=1)\ndf_n['Single platform'] = np.int64(df_n.Total==1)\ndf_n['2-3'] = np.int64((df_n.Total>1) & (df_n.Total<4))\ndf_n['>3'] = np.int64(df_n.Total>3)\ndf_n[Q40_cols_desc + Multi_cols + ['Total']] = df_n[Q40_cols_desc+Multi_cols + ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\ntable2 = round(table1[Q40_cols_desc + Multi_cols].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\ntable2['More_than_1'] = table2[['2-3','>3']].sum(axis=1)\nTop = table2[(table2.Year=='2021')&(table1.Total>500)].sort_values(by = ['More_than_1'],ascending=False)\nTop = Top.iloc[0,:].Q3\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q40_cols_desc + Multi_cols)\n\n    for i in Multi_cols:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Multiple Online learning Platforms<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=500,\n                 width = 900,legend_traceorder=\"reversed\")\n\nfig.show()","9d1fc8db":"#Machine learning algorithms\n\nQ17_cols = ['Q17_Part_1','Q17_Part_2','Q17_Part_3','Q17_Part_4','Q17_Part_5','Q17_Part_6','Q17_Part_7','Q17_Part_8','Q17_Part_9','Q17_Part_10','Q17_Part_11','Q17_OTHER']\nQ17_cols_desc = ['Linear or Logistic Regression','Decision Trees or Random Forests','Gradient Boosting Machines (xgboost, lightgbm, etc)','Bayesian Approaches','Evolutionary Approaches','Dense Neural Networks (MLPs, etc)','Convolutional Neural Networks','Generative Adversarial Networks','Recurrent Neural Networks','Transformer Networks (BERT, gpt-3, etc)','None','Other']\n\ndf_n = df[df.Year!=2018][['User','Year','Q3']+Q17_cols]\ndf_n.rename(columns=dict(zip(Q17_cols, Q17_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q17_cols_desc]!='0').sum(axis=1)\ndf_n[Q17_cols_desc + ['Total']] = df_n[Q17_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q17_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q17_cols_desc)\n\n    for i in Q17_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>ML Algorithm usage across countries<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=1000,\n                 width = 1000)\n\nfig.show()","de1e3f08":"\nQ18_cols = ['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_OTHER']\nQ18_cols_desc = ['General purpose image\/video tools (PIL, cv2, skimage, etc)','Image segmentation methods (U-Net, Mask R-CNN, etc)','Object detection methods (YOLOv3, RetinaNet, etc)','Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)','Generative Networks (GAN, VAE, etc)','None','Other']\n    \ndf_n = df[df.Year!=2018][['User','Year','Q3']+Q18_cols]\ndf_n.rename(columns=dict(zip(Q18_cols, Q18_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q18_cols_desc]!='0').sum(axis=1)\ndf_n[Q18_cols_desc + ['Total']] = df_n[Q18_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q18_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q18_cols_desc)\n\n    for i in Q18_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Computer vision methods usage across countries<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=1000,\n                 width = 1000,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.3,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","c19aa2bb":"Q19_cols = ['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER']\nQ19_cols_desc = ['Word embeddings\/vectors (GLoVe, fastText, word2vec)','Encoder-decorder models (seq2seq, vanilla transformers)','Contextualized embeddings (ELMo, CoVe)','Transformer language models (GPT-3, BERT, XLnet, etc)','None','Other']\n\ndf_n = df[df.Year!=2018][['User','Year','Q3']+Q19_cols]\ndf_n.rename(columns=dict(zip(Q19_cols, Q19_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q19_cols_desc]!='0').sum(axis=1)\ndf_n[Q19_cols_desc + ['Total']] = df_n[Q19_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q19_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q19_cols_desc)\n\n    for i in Q19_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>NLP methods usage across countries<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=1000,\n                 width = 1000,legend_traceorder=\"reversed\",legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=-0.3,\n    xanchor=\"right\",\n    x=1\n))\n\nfig.show()","70c0d363":"Q27_cols = ['Q27_A_Part_1','Q27_A_Part_2','Q27_A_Part_3','Q27_A_Part_4','Q27_A_Part_5','Q27_A_Part_6','Q27_A_Part_7','Q27_A_Part_8','Q27_A_Part_9','Q27_A_Part_10','Q27_A_Part_11','Q27_A_OTHER']\nQ27_cols_desc = [' Amazon Web Services (AWS)',' Microsoft Azure ',' Google Cloud Platform (GCP) ',' IBM Cloud \/ Red Hat ',' Oracle Cloud ',' SAP Cloud ',' Salesforce Cloud ',' VMware Cloud ',' Alibaba Cloud ',' Tencent Cloud ','None','Other']\n    \ndf_n = df[(df.Year!=2018) & (df.Year!=2019) ][['User','Year','Q3']+Q27_cols]\ndf_n.rename(columns=dict(zip(Q27_cols, Q27_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q27_cols_desc]!='0').sum(axis=1)\ndf_n[Q27_cols_desc + ['Total']] = df_n[Q27_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q27_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India','United States of America','Brazil']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=len(categories),shared_yaxes=True,subplot_titles=categories)\n\n\n\ncol=1\nlegend=True\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q27_cols_desc)\n\n    for i in Q27_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Cloud computing Platforms usage across countries (2021 vs 2020)<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=800,\n                 width = 1000,legend_traceorder=\"reversed\")\n\nfig.show()","a6e02ae2":"Q27B_cols = ['Q27_B_Part_1','Q27_B_Part_2','Q27_B_Part_3','Q27_B_Part_4','Q27_B_Part_5','Q27_B_Part_6','Q27_B_Part_7','Q27_B_Part_8','Q27_B_Part_9','Q27_B_Part_10','Q27_B_Part_11','Q27_B_OTHER']\nQ27B_cols_desc = [' Amazon Web Services (AWS)',' Microsoft Azure ',' Google Cloud Platform (GCP) ',' IBM Cloud \/ Red Hat ',' Oracle Cloud ',' SAP Cloud ',' VMware Cloud ',' Salesforce Cloud ',' Alibaba Cloud ',' Tencent Cloud ','None','Other']\n\n\ndf_n = df[df.Year==2021][['User','Year']+Q27B_cols]\ndf_n.rename(columns=dict(zip(Q27B_cols, Q27B_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q27B_cols_desc]!='0').sum(axis=1)\ndf_n[Q27B_cols_desc + ['Total']] = df_n[Q27B_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q27B_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=2,shared_yaxes=True,subplot_titles=['Global','India'])\n\n\n\ntable_tot = np.zeros(table2.iloc[:,1].shape)\nj=0\ntable3= z_test(table1,Q27B_cols_desc)\nfor i in Q27B_cols_desc:\n    fig.add_trace(bar_chart(table2,i,'Year',table3,table_tot,j,True),row=1,col=1)\n    table_tot =table_tot+ table2.loc[:,i]\n    j=j+1\n\n\n\n\ndf_n = df[df.Year==2021][['User','Year','Q3']+Q27B_cols]\ndf_n.rename(columns=dict(zip(Q27B_cols, Q27B_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q27B_cols_desc]!='0').sum(axis=1)\ndf_n[Q27B_cols_desc + ['Total']] = df_n[Q27B_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q27B_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India']\n\ncol=2\nlegend=False\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q27B_cols_desc)\n\n    for i in Q27B_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Future Cloud computing Platforms usage<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=500,\n                 width = 1000,legend_traceorder=\"reversed\",hovermode=False)\n\nfig.show()","818900c0":"Q31B_cols = ['Q31_B_Part_1','Q31_B_Part_2','Q31_B_Part_3','Q31_B_Part_4','Q31_B_Part_5','Q31_B_Part_6','Q31_B_Part_7','Q31_B_Part_8','Q31_B_Part_9','Q31_B_OTHER']\nQ31B_cols_desc = ['Amazon SageMaker','Azure Machine Learning Studio','Google Cloud Vertex AI','DataRobot','Databricks','Dataiku','Alteryx','Rapidminer','None','Other']\n\ndf_n = df[df.Year==2021][['User','Year']+Q31B_cols]\ndf_n.rename(columns=dict(zip(Q31B_cols, Q31B_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q31B_cols_desc]!='0').sum(axis=1)\ndf_n[Q31B_cols_desc + ['Total']] = df_n[Q31B_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q31B_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=2,shared_yaxes=True,subplot_titles=['Global','India'])\n\n\n\ntable_tot = np.zeros(table2.iloc[:,1].shape)\nj=0\ntable3= z_test(table1,Q31B_cols_desc)\nfor i in Q31B_cols_desc:\n    fig.add_trace(bar_chart(table2,i,'Year',table3,table_tot,j,True),row=1,col=1)\n    table_tot =table_tot+ table2.loc[:,i]\n    j=j+1\n\n\n\n\ndf_n = df[df.Year==2021][['User','Year','Q3']+Q31B_cols]\ndf_n.rename(columns=dict(zip(Q31B_cols, Q31B_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q31B_cols_desc]!='0').sum(axis=1)\ndf_n[Q31B_cols_desc + ['Total']] = df_n[Q31B_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q31B_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India']\n\ncol=2\nlegend=False\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q31B_cols_desc)\n\n    for i in Q31B_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Future Cloud computing Products<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=500,\n                 width = 1000,legend_traceorder=\"reversed\",hovermode=False)\n\nfig.show()","f3baa58d":"Q37B_cols = ['Q37_B_Part_1','Q37_B_Part_2','Q37_B_Part_3','Q37_B_Part_4','Q37_B_Part_5','Q37_B_Part_6','Q37_B_Part_7','Q37_B_OTHER']\nQ37B_cols_desc = ['Google Cloud AutoML','H2O Driverless AI','Databricks AutoML','DataRobot AutoML','Amazon Sagemaker Autopilot','  Azure Automated Machine Learning ','None','Other']\n\ndf_n = df[df.Year==2021][['User','Year']+Q37B_cols]\ndf_n.rename(columns=dict(zip(Q37B_cols, Q37B_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q37B_cols_desc]!='0').sum(axis=1)\ndf_n[Q37B_cols_desc + ['Total']] = df_n[Q37B_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q37B_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India']\n\nfig = go.Figure()\n\nfig = make_subplots(rows=1, cols=2,shared_yaxes=True,subplot_titles=['Global','India'])\n\n\n\ntable_tot = np.zeros(table2.iloc[:,1].shape)\nj=0\ntable3= z_test(table1,Q37B_cols_desc)\nfor i in Q37B_cols_desc:\n    fig.add_trace(bar_chart(table2,i,'Year',table3,table_tot,j,True),row=1,col=1)\n    table_tot =table_tot+ table2.loc[:,i]\n    j=j+1\n\n\n\n\ndf_n = df[df.Year==2021][['User','Year','Q3']+Q37B_cols]\ndf_n.rename(columns=dict(zip(Q37B_cols, Q37B_cols_desc)), inplace=True)\ndf_n['Total'] = np.int64(df_n[Q37B_cols_desc]!='0').sum(axis=1)\ndf_n[Q37B_cols_desc + ['Total']] = df_n[Q37B_cols_desc+ ['Total']].replace(['0',0],np.nan)\ntable1 = df_n.groupby(by=['Year','Q3']).count()\n\ntable1 = table1.fillna(0)\n\ntable2 = round(table1[Q37B_cols_desc].div(table1.Total, axis=0)*100,1)\n\ntable2 = table2.reset_index()\ntable2['Year']=table2.Year.astype(str)\n\ntable1 = table1.reset_index()\ntable1['Year']=table1.Year.astype(str)\n\n\ncategories = ['India']\n\ncol=2\nlegend=False\nfor cat in categories:\n    table2_new = table2[table2.Q3==cat].reset_index(drop=True)\n    table1_new = table1[table1.Q3==cat].reset_index(drop=True)\n    table_tot = np.zeros(table2_new.iloc[:,1].shape)\n    j=0\n\n    table3= z_test(table1_new,Q37B_cols_desc)\n\n    for i in Q37B_cols_desc:\n        fig.add_trace(bar_chart(table2_new,i,'Year',table3,table_tot,j,legend),row=1,col=col)\n        table_tot =table_tot+ table2_new.loc[:,i]\n        j=j+1\n    col = col+1\n    if col>1:\n        legend=False\n    \n\nfig.update_layout(title =\"<b>Future Auto ML Products<b>\",\n                  title_x = 0.5,\n                  title_font = dict(size = 20, color = 'MidnightBlue'),\n                 height=500,\n                 width = 1000,legend_traceorder=\"reversed\",hovermode=False)\n\nfig.show()","33954401":"Let's also check this with USA to validate our claim","74b5393f":"## Tools and Technology - Cloud computing Platforms","161f4695":"## Key Income Drivers by Country","d199c48f":"In the previous section we have seen that Tools and Technology (such as Computer vision methods, NLP Methods, ML Algorithms), Job profile, Online learning etc. play the role of key drivers in determining the Income\n\nIn this section, we **deep dive at Product or component level for these categories** and their trend over the years\n\n**Note: Similar to Section-1, feel free to use hovering feature on 2021's data to check statistical significance of change**","b2681900":"<div class=\"alert alert-success\">Significant increase is seen in age group 18-21 in India after 2019<\/div>\n<br>\n\nPeople <mark>less than 25 years of age form 60% of Kaggle users in India<\/mark> while it is only 15-17% in USA and Brazil","4bfa9783":"## India's income distribution by Machine learning experience\n\nLet's also see the distribution by ML experience","03c68586":"<div class=\"alert alert-success\">More than 50% of people in USA & Brazil wear multiple hats at work while majority of Indians are sticking to single task<\/div>\n<br>\n","ac2bf67e":"# Section-3:  Products of importance","e2449a27":"## Cloud computing Platforms","28a94244":"## Tools and Technology - Computer vision methods","60ccaac7":"This raises the question- which countries pay the most to their Data scientists and which pay the least\n\nFor the highest paying countries, I have taken the countries where proportion of people having income greater than 100k is highest in 2021\n\nFor lowest paying countries, I have taken the countries where proportion of people having income less than 10k is highest in 2021\n\nNote: Only those countries are taken where at-least 50 people have filled the survey in 2021","9c62512d":"# Section-1: EDA around Income\n\n## Overall Income distribution\n\nHere we see the distribution of people across income bands over the years for India, USA & Brazil","53cbadfa":"To simplify a little bit, let's look at percentage of people who do just 1 task at their jobs and those who do more than 1","3463f267":" ## Job description","2a2700ce":"## Size of the company","523acaff":"<div class=\"alert alert-success\">While employees working in company sizes of less than 49 are clearly getting paid less, the trend cease to exist above company sizes of 1000 employees<\/div>\n<br>","b6732fda":"## Online Learning","a6d91dc7":"<div class=\"alert alert-success\">Importance of different Technology areas is evenly distributed in all 3 countries<\/div>\n<br>","4d432e3c":"<div class=\"alert alert-success\">Two-third people use more than 1 platform for online learning in all 3 countries. This has dropped w.r.t 2019 but is consistent with last year<\/div>\n<br>\n\n","28441c3c":"## Income distribution across Age and Gender in India","f6056d23":"## Tools and Technology - NLP Methods","fc0f25a3":"<div class=\"alert alert-success\">There is a significant increase in proportion of people earning under 10k USD per year in India after 2019<\/div>\n<br>\n","2a8697bf":"# Section-2: Driver Analysis","603cd443":"<div class=\"alert alert-success\">Most Indians plan to use AWS cloud computing Platforms in next 2 years. The percentage is also higher than overall Global percentage<\/div>\n\nAlso, <mark>fewer Indians will stay away from Cloud computing platforms<\/mark> compared to Global peers","b7c05f6f":"## Coding or Programming experience","82c16c91":"## Importance of Tools and Technology in India YoY","4572cf4c":"There is a <mark>significant increase in Kaggle Learn courses among online learning platforms<\/mark> in all 3 countries from 2019 to 2021\n\nUdemy increase is significant in India and USA but same in Brazil","cc3bdb06":"**Switzerland** has come out on top! while **Bangladesh** has the highest proportion of people getting paid less than 10k per year","ec7592c7":"<div class=\"alert alert-success\">Proportion of Kagglers with 1-3 years of programming experience is increasing every year. Change in 2021 is significant with 2018,2019 and 2020<\/div>\n<br>\n\nPeople with more than 3 years experience form a bigger part of Kaggle community in Brazil in 2021 than in 2018-19","662511e4":"<div class=\"alert alert-success\">Increase in proportion of people working in smaller companies (Under 250 employees) was seen during COVID-19 period (2020).\n<br>\nThe proportion is back to pre-covid level with no significant change w.r.t 2019 for all 3 countries<\/div>\n<br>","ce6ccb82":"## Auto ML Products","d537c0a5":"**We see a similar trend here**","4d9f0d15":"<div class=\"alert alert-success\">The Industry and company size play bigger role in determining income in India compared to other countries. Knowledge of various tools and technology is the most important aspect<\/div>\n<br>\n\n<mark>Level of formal Education has very little effect on compensation in all 3 countries<\/mark>\n\n**Online learning play a significant part in what data scientists are paid**","7af04595":"## Age","5001b76f":"<div class=\"alert alert-success\">Income increases with coding experience as expected. Over the years there has not been a significant change in income for people with more than 10 years of coding experience<\/div>\n<br>","d4878741":"# Conclusion\n\nIn this analysis, I have mainly focused on Indian data scientists and the pay gap with global peers. We have looked at how Demographics, Industry, company size, Ongoing learning and Data science knowledge impact what data scientists get paid in India. We have also looked at product and component level what continues to be the preference and what has changed over the years. The analysis was divided in 4 Sections: EDA around Income, Driver analysis, Products of Importance and Into the future.\n\nKey insights from each section are provided below:\n\n1. EDA around Income: \n    * India is taking the lead in closing the gender gap in data science community but there's still a long way to go. **Pay gap between Men and Women continues to be a worry**\n    * Income is positively correlated with Age, Coding or ML experience. Though, there seems to be an emerging trend of low paying jobs (Less than 10k USD) among employees above 35 years of age in Indian data science industry\n    * Employees working in very small companies (Less than 50 employees) get paid the least. **Company size doesn't matter above 1000 employees**\n    \n2. Driver analysis:\n    * India is on the right track in terms of adopting the latest technology that seems to matter the most in driver analysis\n    * There is an emerging trend of Cloud computing technologies, Computer vision methods and NLP methods playing more important role in deciding compensation as compared to traditional Notebook, IDEs and Programming language knowledge\n    * **Level of formal education has little effect on deciding compensation** while online learning continues to play an important role\n    \n3. Products of importance:\n    * **More people do research related to ML in India compared to USA & Brazil**. However, majority of Indians are limited to single area of work while data scientists in USA & Brazil usually wear multiple hats\n    * Two-third people have completed online courses from more than 1 platform in all 3 countries. **Kaggle learn courses participation continue to rise significantly YoY**\n    * While use of traditional ML algorithms such as Logistic or Linear regression is still dominant, Indians continue to lead on use of CNNs at their work\n    \n4. Into the Future:\n    * AWS is the preferred cloud computing platform for Indian Kagglers for the next 2 years\n    * Future adoption of cloud technologies is higher among Indians compared to Global peers. This is in line to rising importance of cloud computing in compensation   \n","ec92647b":"**Let's also see how many people are learning from more than 1 platform**","c4138de2":"Now, let's start the exciting part!\n\n### What is Driver analysis?\n\nDriver analysis is another tool used in Market research to <mark>come up with the set of features that *drive* a particular behavior<\/mark> or outcome among the population. In other words, it tells the importance of Independent variables in predicting the dependent variable. The importance is similar to variance in the data explained by that variable and the sum of all the variable importance is equal to 1\n\nHere, the <mark>dependent variable taken is Income<\/mark> and Independent variables include but not limited to **Age, Gender, Highest Formal education, Programming language knowledge, Experience** etc.\n\n\n### Methodology used\n\nThis problem can be treated as a supervised machine learning problem with dependent variable as Income which is a categorical variable thus making it a Classification problem.\n\nGradient Boosting Classifier algorithm is used to fit the data **for each Year and Country** separately and features are grouped in **8 broad categories**. 142 variables are used to fit the model\n\nAfter fitting the model on 9 different data sets (For 3 countries x 3 years), we get feature importance as output for each of the 142 variables used.\n\nThese variables are grouped in 8 categories:\n* **Tools and Technology Knowledge:** Knowledge or regular use of various ML algorithms, Programming languages, Cloud computing platforms etc.\n* **Industry\/Company size:** The industry a person works in and the size of the company\n* **Demographics:** Age, Gender\n* **Job Profile:** Work activities\n* **Online learning:** Online courses completed on various platforms\n* **ML\/Coding experience:** Experience in writing code or using ML methods\n* **Formal Education:** Highest formal education\n* **Other**\n\n","783ee400":"In this section we look at future perceptions on product usage at India and Global level as entered in the 2021 survey","2c64e969":"## Cloud computing products","608d255e":"**Horizontal bar charts:**\n\n\n<img src=\"https:\/\/imgur.com\/1pKsupg.png\" align=\"left\"\/>","1550e8af":"## Income distribution by Company size in India","1d712f98":"## India's income distribution by Coding experience","61e54a1e":"## Tools and Technology - ML Algorithms","d51457e0":"## Gender Distribution","d975a029":"<div class=\"alert alert-success\">Importance of Computer vision methods, NLP methods, Visualization libraries and Cloud computing platforms is increasing every year<\/div>\n<br>","236811ab":"<div class=\"alert alert-success\">Woman participation in Data science has increased in India from 16% in 2019 to 22% in 2021 and the change is significant! There is no significant change from last year<\/div>\n<br>\n\nNot much has changed in USA & Brazil in last 3 years in terms of Gender distribution","1ee717fa":"# Section-4: Into the Future","b0019d21":"Note: For this analysis, Z-test is performed only between 2021 data and previous year's data for all the charts wherever year on year trends are shown","e63afc2a":"Even though the importance of Tools and Technology knowledge is becoming less dominant, it still is the major factor that determines Income","d6142d6d":"## Tools and Technology Deep dive","cb9600b0":"### What is Z-test?\n\nFor those who are unfamiliar with this concept, I have provided a quick summary below:\n\n\nStatistical significance testing is a common concept in market research to check if any change observed in proportions of succesful events over total events between two samples is an underlying change in population or not. For proportion, a Z-test is used where the null hypothesis is that the proportions are the same:\n\nnull hypothesis: p1-p2 = 0\n\nwhere p1 and p2 are proportion of successful events from two different samples (For this analysis p1 and p2 will come from survey data of two different years)\n\nTo be able to reject the null hypothesis, the p-value of the z-test should be less than a threshold value. For this analysis, the threshold is set at 5%.\n\nZ-score for two proportions z-test is computed as : \n\n<img src=\"https:\/\/imgur.com\/doSqCZf.png\" align=\"center\"\/>\n<br><br>\nSo all we need are the proportions (p1,p2) and the total sample size (n1,n2) for each sample!\n\n### How to read the charts?\n\nTwo types of charts are used: **Stacked column chart** for questions with single select option and **Horizontal bar charts** for questions with multiple select options. <mark>All charts are interactive plots and reveal additional information on hovering<\/mark>\n\n**Stacked column chart:**\n\n<div>\n<img src=\"https:\/\/imgur.com\/7heTnkH.png\" align=\"left\"\/>\n<\/div>\n<br><br>\n<div>\n    \n","31f668eb":"## Key income drivers for India YoY","178eccf0":"<div class=\"alert alert-success\">The rise in proportion of people earning under 10k in India is more prominent among people above 35 years of age where the proportion has doubled in last 3 years<\/div>\n<br>\n\nPeople below 25 years of age are earning the same as in 2018","6c79510c":"<div class=\"alert alert-success\">A larger portion of Women earn less than 10k USD per year than Men. There is no significant change in this proportion over the last 2 years<\/div>\n<br>","9616d7e4":"## Analysis approach\n\nThis analysis is divided in 4 main parts:\n\n* **EDA around income:** How factors such as Age, Gender, Company size etc. affect Income among Kagglers\n* **Driver analysis:** Using supervised machine learning model, gauge importance of various factors affecting Income across countries\n* **Products of importance:** Based on Driver analysis, deep dive on specific products\/components of key drivers\n* **Into the Future:** What Kagglers will do in next 2 years\n\nResults are presented as proportion of people answering a particular question for which I have used Stacked column and horizontal bar charts for Single choice and Multiple choice questions respectively. \n\nData from 3 previous surveys (2018 - 2020) is used. I have compiled this data manually since there were difference in question mappings over the years\n\nThe good thing about having data for mulitple years is that we can see trends on what has changed. Therefore, to capture these trends, two tailed z-tests are performed for each proportion to check statistical significance of the changes observed","9ceb4691":"Let's look at some of the factors affecting Income distribution in India and abroad","274dca24":"# Introduction\n\nIndians form the biggest community on kaggle. Approximately **29% of survey participants this year are from India up from 18.5% in 2018**. \nDespite being such an important part and winning mulitple competition on Kaggle, unfortunately Indian Kagglers are not doing great on one aspect - Income\n\nBeing a part of the Indian data science community, it is of great interest for me to analyze and extract key factors causing the income disparity and come up with insights that can help in reducing this gap\n\nSo with that, let's talk Money!\n\n![](https:\/\/imgur.com\/5mIKFCz.png)"}}