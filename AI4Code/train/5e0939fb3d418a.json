{"cell_type":{"f09d0058":"code","e5890ea3":"code","8af4c12b":"code","8837514f":"code","73527c76":"code","fdb204e0":"code","b43d658c":"code","0c9300a0":"code","89bbdd68":"code","3a6557b3":"code","8f1ac013":"markdown","25540cd2":"markdown","96e0cbdb":"markdown"},"source":{"f09d0058":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5890ea3":"# import all kinds of modules - not all will be utilized right now\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nplt.style.use(\"seaborn-whitegrid\")\n\nfrom scipy.stats import norm\n\n#preprocessing tools\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import (OrdinalEncoder, StandardScaler,\n                                  MinMaxScaler, PolynomialFeatures,\n                                  PowerTransformer, Binarizer, LabelBinarizer,\n                                  OneHotEncoder)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.cluster import KMeans\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n#pipelines\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer, make_column_selector\n\nfrom sklearn_pandas import DataFrameMapper\n\n#models\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n#metrics\nfrom sklearn.metrics import mean_squared_error","8af4c12b":"#import data\n\ndf = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\", index_col=0)\ndf_test = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\", index_col=0)","8837514f":"# create a simple function to visually compare predictions and actual target values\n\ndef pred_chart(prediction, target, alpha=0.1):\n  print(\"RMSE=\", mean_squared_error(prediction, target, squared=False))\n  plt.figure(figsize=(7,7))\n  plt.scatter(target, prediction, alpha=alpha, color=\"blue\")\n  _min = min(target)\n  _max = max(target)\n  plt.plot((_min, _max), (_min, _max), color=\"red\")\n  plt.xlabel(\"target\")\n  plt.ylabel(\"prediction\")\n  plt.show()","73527c76":"# seperate numeric, object and target columns\n\nnum_cols = df.select_dtypes(exclude=\"object\").columns[:-1]\nobj_cols = df.select_dtypes(\"object\").columns\ntarget = df.columns[-1]","fdb204e0":"# build a custom Feature Union to return a DataFrame\n# full credit to https:\/\/www.kaggle.com\/adam48\nclass DFM_FeatureUnion():\n  def __init__(self, transformer_list):\n    self.transformer_list = transformer_list\n\n  def fit(self, X, y=None):\n    for (_, transformer, _) in self.transformer_list:\n      transformer.fit(X,y)\n    return self\n\n  def transform(self, X):\n    trans = []\n    for (_, transformer, _) in self.transformer_list:\n      trans.append(transformer.transform(X))\n    return pd.concat(trans, axis=1)\n  \n  def fit_transform(self, X, y=None):\n    self.fit(X, y)\n    return self.transform(X)","b43d658c":"# building the Pipeline\n\ndef buildFeaturesPipeline(num_clusters = 4, num_PCAfeatures = 4, num_Poly = 2):\n    \n    # sepearte obj_cols according to the fill-value\n    fill_none = ['MasVnrType','GarageType', 'GarageQual', 'GarageFinish',\n                'GarageCond', 'FireplaceQu', 'BsmtQual','BsmtCond','BsmtExposure',\n                'BsmtFinType1', 'BsmtFinType2']\n\n    drop_obj = [\"Alley\",\"PoolQC\",\"Fence\",\"MiscFeature\"]\n\n    fill_frequent = [col for col in obj_cols if col not in set(fill_none + drop_obj)]\n\n\n    obj_filler = DataFrameMapper(\n        [([col], SimpleImputer(strategy=\"constant\", fill_value=\"None\"))\n          for col in fill_none]\n          +\n        [([col], SimpleImputer(strategy=\"most_frequent\"))\n          for col in fill_frequent]\n        ,df_out=True\n    )\n\n    # Features to encode\n    obj_ord = ['GarageQual','GarageCond','FireplaceQu','BsmtQual','BsmtCond',\n               \"BsmtExposure\",\n               \"GarageFinish\",\n               'BsmtFinType1', 'BsmtFinType2']\n\n    obj_oh = [\"MasVnrType\", \"GarageType\"]\n\n    obj_ord_DFM = DataFrameMapper(\n          [\n           # Generate a list Comprehension for several encoders\n           # DFM suffix\/prefix attribut doesn't work\n           ([col], OrdinalEncoder(categories=[[\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"None\"]], dtype=np.int64), {'alias':'ord_'+col})\n              for col in ['GarageQual','GarageCond','FireplaceQu','BsmtQual','BsmtCond']\n            ]\n          +\n          [\n           ([\"BsmtExposure\"], OrdinalEncoder(categories=[[\"Gd\",\"Av\",\"Mn\",\"No\",\"None\"]], dtype=np.int64), {\"alias\":\"ord_BsmtExposure\"} )\n           ,\n          ([\"GarageFinish\"], OrdinalEncoder(categories=[['Fin', 'RFn', 'Unf', 'None']], dtype=np.int64), {'alias':'ord_GarageFinish'} )\n          ]\n          ,df_out=True\n    )\n    \n    # use LabelBinarizer as OneHotEncoder seems runs problems within DFM\n    # also LB does actually provide meaningful column-names, which the SKlearn OHE does not\n    obj_oh_DFM = DataFrameMapper(\n        [\n         ([\"MasVnrType\"], LabelBinarizer()), # 4 Values\n         ([\"GarageType\"], LabelBinarizer())  # 6 Values\n        ]\n        ,df_out=True\n    )\n\n    obj_encode_union = DFM_FeatureUnion(\n        transformer_list=[\n         (\"ord\", obj_ord_DFM, obj_ord),                 \n         (\"OH\", obj_oh_DFM, obj_oh)\n        ]\n    )\n\n    categorical_starter = Pipeline(\n        [\n         (\"fill\", obj_filler),\n         (\"encode\", obj_encode_union)\n        ]\n    )\n\n    # add categorical features (not implemented)\n    categorical_features = DFM_FeatureUnion(\n        transformer_list = [\n                            ('original_features',categorical_starter)\n                            ])\n\n    # starting point for numerics workflow, select the columns, and apply our custom scaler\n    fill_0 = [\"MasVnrArea\"]\n    fill_mean = [col for col in num_cols if col not in [\"MasVnrArea\"]]\n\n    numeric_impute_DFM = DataFrameMapper(\n      [(fill_0, SimpleImputer(strategy=\"constant\", fill_value=0))]\n      +\n      [([col], SimpleImputer(strategy=\"mean\")) for col in fill_mean]\n      +\n      [([col], StandardScaler(), {\"alias\":\"norm_\"+col})\n        for col in num_cols\n        ],\n        df_out=True\n    )\n\n    numeric_norm_DFM = DataFrameMapper(\n        [([col], StandardScaler(), {\"alias\":\"norm_\"+col})\n        for col in num_cols\n        ],\n        df_out=True)\n\n    numeric_prep_DFM = Pipeline(\n        [\n         (\"impute\", numeric_impute_DFM),\n         (\"scale\", numeric_norm_DFM)\n        ]\n    )\n    \n    # union the categorical and numerical pipelines\n    features_merged = DFM_FeatureUnion(\n        transformer_list = [\n                            ('categorical_pipeline', categorical_starter, obj_cols),\n                            ('numerical_pipeline', numeric_prep_DFM, num_cols)\n                            ])\n    \n    # FeatureUnion returns a numpy array, convert to dataframe. Note that we've lost the column names at this point.\n    features_pipeline = Pipeline(\n        steps=[\n               ('features', features_merged)\n               ])\n    \n    return features_pipeline\n\nmyPipeline = buildFeaturesPipeline()","0c9300a0":"X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df[\"SalePrice\"], test_size = 0.4)","89bbdd68":"model = RandomForestRegressor()\nmodel_pipeline = Pipeline([\n                           (\"prep\", myPipeline),\n                           (\"model\", model)\n])\nmodel_pipeline.fit(X_train, y_train)","3a6557b3":"prediction = model_pipeline.predict(X_test)\npred_chart(prediction, y_test)","8f1ac013":"The idea of this notebook is to build a pipeline, that bundles all preprocessing into a single object for future use.\nPlus it utilizes the DataFrameMapper in order to put out a Pandas Dataframe, while stull utilizing SKLearn modules for the preprocessing steps.\n\nFor this exercise so far, basic EDA is completed and several decision on how to clean the data were made - this step will not be included here.","25540cd2":"After building the Pipeline, I'll just use a completly generic model to test it out and get a prediction.","96e0cbdb":"With the preperation done, the next cell includes all the code to preprocess the data with the following steps:\n1. Fill missing values in all columns with custom strategies\n2. OrdinalEncoding with custom categories to preserve logical order\n3. OneHot encoding via LabelEncoder to preserve named columns\n4. bundle all into a single Pipeline\n\nFurther steps like Clustering, PCA and other are not yet realized and just placeholders."}}