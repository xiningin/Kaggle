{"cell_type":{"c4882b51":"code","f6129c80":"code","aa0179cd":"code","65e9adaf":"code","8ade5d58":"code","9d9d4ef8":"code","54a5b6a4":"code","22cadf9f":"code","e339c007":"code","a850e71a":"code","5030b201":"code","78967b90":"code","452f0d2b":"code","0d7e5ef7":"code","b10e7d38":"code","069b55cf":"code","e13c8528":"code","c442b265":"code","07677f86":"code","a4f38c04":"code","eefcd57c":"code","fd52d664":"code","bdc8c40c":"code","81172791":"code","0c00cfdd":"code","582f98d2":"code","f4ac9835":"code","0a2afbca":"code","e3e4a7a7":"code","10a82080":"code","6ca133df":"code","35759419":"code","c284cb52":"code","e8bbcbfa":"code","26846f43":"code","5e47dcf6":"code","f57793a6":"code","393c5008":"markdown","1de1fcf3":"markdown","f7bfc415":"markdown","355dc8de":"markdown","73306df5":"markdown","a32981d1":"markdown","bd16359d":"markdown","38f83994":"markdown","98edcc56":"markdown","a7e2c16a":"markdown","0adbd1f1":"markdown","4af622ab":"markdown","f3a43d40":"markdown","bfab1ac5":"markdown","29c37053":"markdown","2ebc7222":"markdown","0a3e216f":"markdown","91a5f108":"markdown","e1eb589c":"markdown","33c15c6c":"markdown","25433fbd":"markdown","8f950474":"markdown","91bb0a18":"markdown","d8c136b4":"markdown","14a7df86":"markdown"},"source":{"c4882b51":"root = '..\/input\/tabular-playground-series-jun-2021\/'\ntrain_path = root + 'train.csv'\ntest_path = root + 'test.csv'\nsubm_path = root + 'sample_submission.csv'","f6129c80":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","aa0179cd":"train_df = pd.read_csv(train_path)\n\ntrain_df.head()","65e9adaf":"test_df = pd.read_csv(test_path)\n\ntest_df.head()","8ade5d58":"samp_sub = pd.read_csv(subm_path)\n\nsamp_sub.head()","9d9d4ef8":"# Target Value Count Distribution:\ntarget_mass = train_df['target'].value_counts()\nvalues = target_mass.values.tolist()\nindexes = target_mass.index.tolist()\n\nax,fig = plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nplt.pie(values , labels = indexes)\nplt.subplot(1,2,2)\nplt.bar(indexes,values)\nplt.show()","54a5b6a4":"fet_set = train_df.drop(labels=['id','target'],axis=1)\ndef plot_diag_heatmap(data):\n    corr = data.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    f, ax = plt.subplots(figsize=(11, 9))\n    sns.heatmap(corr, mask=mask, cmap='YlGnBu', center=0,square=True, linewidths=1, cbar_kws={\"shrink\": 1.0})\nplot_diag_heatmap(fet_set)","22cadf9f":"corr = train_df.iloc[:,1:-1].corr()","e339c007":"corr","a850e71a":"plt.plot((abs(corr).sum()-1)\/len(corr))\nplt.xticks([])\nplt.plot(np.ones(len(corr))*0.06,label = 'baseline',color = 'r')\nplt.legend()\nplt.show()","5030b201":"for col in corr.columns:\n    if ((sum(corr[col])-1)\/(len(corr)-1)) <0.06:\n        print(col , (sum(corr[col])-1)\/(len(corr)-1))","78967b90":"for col in corr.columns:\n    if ((sum(corr[col])-1)\/(len(corr)-1)) <0.06:\n        train_df.drop(col,1,inplace=True)\n        test_df.drop(col,1,inplace=True)","452f0d2b":"train_df.head()","0d7e5ef7":"train_df.describe()","b10e7d38":"fig,axes = plt.subplots(1,5,figsize=(24,3))\ni=1\nfor col in train_df.columns[1:-1]:\n    plt.subplot(1,5,i)\n    sns.boxplot(train_df['target'],train_df[col])\n   # plt.yaxis('off')\n    plt.xticks([])\n    i+=1\n    if i%5==1 and col!=train_df.columns[-2]:\n        i=1\n        plt.show()\n        fig,axes = plt.subplots(1,5,figsize=(24,3))","069b55cf":"from  scipy.stats import zscore","e13c8528":"# Using zscore\n\ntemp_df = train_df\n\nfor col in temp_df.columns[1:-1]:\n    temp_df['zs'] = np.abs(zscore(temp_df[col]))\n    temp_df = temp_df[temp_df['zs'] <= 2.7]\n    temp_df.drop('zs' , 1 , inplace = True)\ntrain_df.drop('zs' , 1 , inplace = True)\nprint(train_df.shape , '--->' , temp_df.shape)","c442b265":"from scipy.stats import iqr","07677f86":"# Using interquartile range\n\ntemp_df = train_df\n\nfor col in temp_df.columns[1:-1]:\n    iqr_val = iqr(temp_df[col])\n    q1 = np.quantile(temp_df[col] , 0.03)\n    q3 = np.quantile(temp_df[col] , 0.97)\n    temp_df = temp_df[temp_df[col]>=q1-1.5*iqr_val]\n    temp_df = temp_df[temp_df[col]<=q3+1.5*iqr_val]\nprint(train_df.shape,'--->',temp_df.shape)","a4f38c04":"cleaned_train_df = temp_df","eefcd57c":"cleaned_train_df.drop('id',1,inplace=True)\nidx = test_df['id']\ntest_df.drop('id',1,inplace=True)","fd52d664":"cleaned_train_df.drop_duplicates(inplace=True)\n#cleaned_train_df = cleaned_train_df.T.drop_duplicates().T  \n#no need to apply these function .Takes to much unneccessary time","bdc8c40c":"cleaned_train_df.shape","81172791":"arr = []\nplt.figure(figsize=(10,4))\nfor i in range(1,10):\n    t_df =temp_df[temp_df['target']=='Class_'+str(i)]\n    plt.scatter(t_df['feature_0'],t_df.index,label='Class_'+str(i),s=7)\nplt.legend()\nplt.show()","0c00cfdd":"from sklearn.model_selection import train_test_split","582f98d2":"def split_data(test_size,data):\n    data = data.sample(frac=1)\n    x_train = data.drop('target',1)\n    y_1 = data['target']\n    x_train = x_train\n    y_1 = y_1.to_numpy()\n    X_train , X_val , y_1 , y_2 = train_test_split( x_train , y_1 ,\n                                                         test_size = test_size ,\n                                                        random_state =1 ,\n                                                        stratify = y_1)\n    y_train = []\n    y_val = []\n    for value in y_1:\n        y_train.append(int(value[-1])-1)\n    for value in y_2:\n        y_val.append(int(value[-1])-1)\n    return X_train , X_val , np.array(y_train) , np.array(y_val)","f4ac9835":"X_train , X_val , y_train , y_val = split_data(0.2,cleaned_train_df)\nX_test = test_df[X_train.columns]","0a2afbca":"X_train.shape , X_val.shape , y_train.shape , y_val.shape , X_test.shape","e3e4a7a7":"from sklearn.preprocessing import StandardScaler as scaler","10a82080":"def scale(train,test,validation):\n  sc = scaler()\n  columns = train.columns\n  train = sc.fit_transform(train)\n  test = sc.transform(test)\n  validation = sc.transform(validation)\n\n  train = pd.DataFrame(train , columns = columns)\n  test = pd.DataFrame(test , columns = columns)\n  validation = pd.DataFrame(validation , columns = columns)\n\n  return train , test , validation","6ca133df":"X_train , X_test , X_val = scale(X_train , X_test , X_val)","35759419":"X_train.head()","c284cb52":"# Target Value Count Distribution:\ntm = pd.DataFrame(y_train,columns=['x'])\ntarget_mass = tm['x'].value_counts()\nvalues = target_mass.values.tolist()\nindexes = target_mass.index.tolist()\n\nax,fig = plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nplt.pie(values , labels = indexes)\nplt.subplot(1,2,2)\nplt.bar(indexes,values)\nplt.show()","e8bbcbfa":"# importing models\n\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.ensemble import ExtraTreesClassifier as ext\nfrom xgboost import XGBClassifier as xgb\nfrom lightgbm import LGBMClassifier as lgb\nfrom catboost import CatBoostClassifier as cbt","26846f43":"# Function to train and visualize accuracy and predict\n\ndef train_and_predict(model , x_1  , x_2 , x_3 , y_1 , y_2):\n    labels = []\n    for i in range(9):\n        labels.append('Class_'+str(i+1))\n    model.fit(x_1 , y_1)\n    print('Training Completed..........')\n    print('Train Accuracy : ',model.score(x_1,y_1))\n    print('Validation Accuracy : ',model.score(x_2 , y_2))\n    print('Model Prediction started....')\n    y_pred = model.predict_proba(x_3)\n    final_df = pd.DataFrame(y_pred , columns = labels)\n    final_df = pd.concat([idx,final_df]  , axis = 1)    #uncomment this to find the actual submission files.\n    #idxx = pd.DataFrame(np.ones(len(idx)))\n    #final_df = pd.concat([idxx,final_df],axis=1)   # comment this line find actual submission files\n    return final_df","5e47dcf6":"clf1 = rfc(random_state = 2)\nclf2 = ext(random_state = 2)\nclf3 = xgb()\nclf4 = lgb()\nclf5 = cbt(verbose=0)\nmodels = [ clf1 , clf2 , clf3 , clf4 , clf5]\nnames = ['rfc' , 'ext' , 'xgb' , 'lgb' , 'cbt']","f57793a6":"for i in range(len(models)):\n    model = models[i]\n    print(names[i] , 'model has been opted for training...........')\n    submission = train_and_predict(model , X_train , X_val , X_test , y_train , y_val)\n    print('submission file created................................\\n\\n')\n    submission.to_csv(names[i]+'.csv',index=False)\nprint('Task Completed.............................................')","393c5008":"It changed heavily :O","1de1fcf3":"In the boxplots, we can see that most of the data is outliers. So, we need to sensitively process data and remove those outliers.\n\n#### Approach:\n\nThe approaches has been taken from [here](https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/).\n\n1. Using Interquartile Range .\n2. Zscore\n\n","f7bfc415":"#### Data Gathering :","355dc8de":"We can see that some target classes are present in a very big number and some are very few.\n\n#### Approach :\nWe can take every target class row in the same count. But choosing that will reduce the data size and as we do not know which data to remove we might even remove the important rows. So, we will skip target class equalization.","73306df5":"#### Libraries :\n\nImporting the basic data manipulation and visualization libraries.","a32981d1":"# TPS-JUNE 2021 :\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25225\/logos\/header.png?t=2021-01-27-17-34-26)\n\n\n## UPVOTE if this helps you :)\n\nThis is a quick starter for TPS-kaggle.\n\nAll the major steps have been used to find the best accuracy using the most fundamental approach.\n\nThis notebook not only holds a better way to approach any other competition, but it observes and manipulates the small and tiny changes that can lead us to better Data Engineering. ","bd16359d":"### Model Generation and Evaluation :\n\n As the model is training and predicting on a single data feature we might not get the correct accuracy metric. So, we are fitting and generating submission files.","38f83994":"On the previous versions I have used a avery small range of iqr. In this version I will be removing the 3% of the furthest outliers using IQR method.","98edcc56":"Look's like we can forward to the next step.","a7e2c16a":"Here we can see some features are light in the whole plot. So, we can indicate those as low correlated features.\nSo, we are going to drop those features from both train and test data.","0adbd1f1":"### Exploratory Data Analysis and Data Processing :\n---\n\nWe'll be trying to visualize the deeper data patterns and find out the anomalies that should be omitted to prepare the best trainable data.\n\nAlso an additional point,\n**BEST TRAINABLE DATA** is data that has no noise and duplicates and outliers.","4af622ab":"Now, we will check again the target mass distribution.","f3a43d40":"7. #### Splitting the data into train and validation :\n---\n We are going to have an 80-20 train validation split, also we are going to change the target feature(basically change that into numerical values).","bfab1ac5":"We are going to drop those features which are below that baseline.","29c37053":"## And Always ....................\n\n![](https:\/\/i.ytimg.com\/vi\/GduXLWFxKhQ\/maxresdefault.jpg)","2ebc7222":"5. #### Dropping Duplicates :\n---\nNow we are going to drop the duplicate rows and features. This will reduce the dimensionality of the train data.","0a3e216f":"The sample submission format tells us that we need to predict the likelihood of the target classes. These values are between 0.0 to 1.0.\n\n#### Approach :\n\n So, now we have to decide on the approach. \n 1. We can process the data and feed it through a [neural network](https:\/\/en.wikipedia.org\/wiki\/Neural_network) and output as a [softmax layer](https:\/\/en.wikipedia.org\/wiki\/Softmax_function).\n 2. We can use the [predict_proba()](https:\/\/discuss.analyticsvidhya.com\/t\/what-is-the-difference-between-predict-and-predict-proba\/67376) to the normal Machine Learning bagging or boosting models and prepare the submission file.","91a5f108":"1. #### Target Value Count Distribution: \n---\nAt first, we are going to check the target value mass distribution. Cause too much difference in the can led us to a bad model learning.","e1eb589c":"4. #### Dropping ID :","33c15c6c":"3. #### Outliers :\n---\nAs the outliers can not be distinguished very properly but can stay in the data as noise. We can visualize the outliers using [seaborn boxplots](https:\/\/seaborn.pydata.org\/generated\/seaborn.boxplot.html).","25433fbd":"6. #### Checking other features :\n---\n Let's check if there's any other pattern if we can find.","8f950474":"#### Data Loading :\nAfter loading the data into the data frame\/python backend, we can visualize the deeper patterns or manipulate them on our wish.","91bb0a18":" 2. #### Correlation :\n---\nNow, we must check the data correlation. In this part, we'll be visualizing features to feature correlation.","d8c136b4":"# THANK YOU for visiting !!!!\n\n## You can visit my other works at [kaggle](https:\/\/www.kaggle.com\/sagnik1511\/code) or in [Github](https:\/\/github.com\/sagnik1511?tab=repositories).","14a7df86":"8. #### Scaling :\n---\nNow we need to scale the data as the different magnitudes of data may create irregular clusters."}}