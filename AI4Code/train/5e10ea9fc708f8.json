{"cell_type":{"66a1a33d":"code","80332689":"code","ec13643d":"code","283f5095":"code","01453bb0":"code","2cf04dbe":"code","ffdb8856":"code","2417512b":"code","814e65be":"code","3e4c5c76":"code","999f4ede":"code","7ff467c5":"code","595bfc0c":"code","9d048c4c":"code","9c6837f8":"code","63c81c15":"code","185369cc":"code","4f62bef0":"code","59aaa83d":"code","d969af39":"code","3fa449b4":"code","175aef8e":"code","c1b9bd1a":"code","340d378b":"code","2700d7a9":"code","cca9c2f1":"code","4ddce7c2":"code","cac4f198":"code","d9c0915d":"code","90703eb3":"code","ce086d2f":"code","8b1b5aac":"code","56a5de10":"code","1e1a880b":"code","ac7efa0e":"code","25b49f71":"code","97b8905b":"code","54a71a90":"markdown","58a48318":"markdown","5ff58b77":"markdown","43e94ede":"markdown","65f9034d":"markdown","73dea4b6":"markdown","569c2db6":"markdown","22faa2e3":"markdown","2cc13c41":"markdown","1b2363f4":"markdown","14d23c9b":"markdown","6d74e892":"markdown","453f9ac3":"markdown","477a9483":"markdown","2906a104":"markdown","a2248f94":"markdown","57d69df5":"markdown","6f6e780e":"markdown"},"source":{"66a1a33d":"!pip install ..\/input\/pytorch-tabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","80332689":"from pathlib import Path\nimport pandas as pd\nimport glob\nimport numpy as np\nimport pyarrow.parquet\nimport pyarrow as pa\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n#import tqdm\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\nfrom pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')","ec13643d":"train_data = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntest_data = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\ntrain_data =train_data[train_data.stock_id ==0]\ndisplay(train_data.head())\ndisplay(test_data.head())","283f5095":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef calc_wap3(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2']+ df['ask_size2'])\n    return wap\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","01453bb0":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","2cf04dbe":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap3'] = calc_wap3(df)\n    df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'log_return3':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n       # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature    \n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n    ","ffdb8856":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)   \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","2417512b":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","814e65be":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","3e4c5c76":"train = pd.read_csv(data_dir + 'train.csv')\ntrain.head()","999f4ede":"train_ids = train.stock_id.unique()\ndf_train = preprocessor(list_stock_ids=train_ids, is_train = True)\ndf_train","7ff467c5":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","595bfc0c":"df_train_bk = df_train.copy()\n#df_train = df_train_bk.copy()","9d048c4c":"df_train","9c6837f8":"test = pd.read_csv(data_dir + 'test.csv')\ntest","63c81c15":"df_test_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids=df_test_ids, is_train = False)\n#df_test['stock_id'] =df_test['row_id'].str.split('-').str[0]\n#df_test['time_id'] =df_test['row_id'].str.split('-').str[1]\n#df_test =df_test.drop(['row_id'],axis=1)\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')\ndf_test","185369cc":"from sklearn.model_selection import KFold\n\n\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_ext'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n\n\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 10, shuffle=True,random_state = 77)\nfor idx_1, idx_2 in tqdm(kf.split(df_train)):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_ext'] = tmp","4f62bef0":"df_train = df_train.reset_index(drop=True)\ntarget = df_train['target']\ndf_train = df_train.drop(['target'],axis=1)\n\n\nX = df_train\ny = target","59aaa83d":"col =['log_return_realized_volatility',\n       'log_return2_realized_volatility', 'log_return3_realized_volatility',\n       'wap_balance_mean', 'price_spread_mean', 'bid_spread_mean',\n       'ask_spread_mean', 'volume_imbalance_mean', 'total_volume_mean',\n       'wap_mean', 'log_return_realized_volatility_450',\n       'log_return2_realized_volatility_450',\n       'log_return3_realized_volatility_450', 'wap_balance_mean_450',\n       'price_spread_mean_450', 'bid_spread_mean_450', 'ask_spread_mean_450',\n       'volume_imbalance_mean_450', 'total_volume_mean_450', 'wap_mean_450',\n       'log_return_realized_volatility_300',\n       'log_return2_realized_volatility_300',\n       'log_return3_realized_volatility_300', 'wap_balance_mean_300',\n       'price_spread_mean_300', 'bid_spread_mean_300', 'ask_spread_mean_300',\n       'volume_imbalance_mean_300', 'total_volume_mean_300', 'wap_mean_300',\n       'log_return_realized_volatility_150',\n       'log_return2_realized_volatility_150',\n       'log_return3_realized_volatility_150', 'wap_balance_mean_150',\n       'price_spread_mean_150', 'bid_spread_mean_150', 'ask_spread_mean_150',\n       'volume_imbalance_mean_150', 'total_volume_mean_150', 'wap_mean_150',\n       'trade_log_return_realized_volatility',\n       'trade_seconds_in_bucket_count_unique', 'trade_size_sum',\n       'trade_order_count_mean', 'trade_log_return_realized_volatility_300',\n       'trade_seconds_in_bucket_count_unique_300', 'trade_size_sum_300',\n       'trade_order_count_mean_300',  'stock_id_target_ext']","d969af39":"#scaler = MinMaxScaler(feature_range=(-1, 1))\n#X[col] = scaler.fit_transform(X[col])","3fa449b4":"X[col] = X[col].fillna(X[col].mean())","175aef8e":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","c1b9bd1a":"%%capture\n'''\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=77, shuffle=True)\noof = pd.DataFrame()               \nmodels = []                          \nscores = 0.0  \n\n\nclf = TabNetRegressor() \nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold :\", fold+1)\n    \n    # create dataset\n    X_train, y_train = X[col].loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X[col].loc[val_idx], y[val_idx]\n\n\n    clf.fit(\n       X_train.values, y_train.values.reshape(-1,1),\n       eval_set=[(X_valid.values, y_valid.values.reshape(-1,1))])\n    \n    y_pred = clf.predict(X_valid.values)\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n    \n    models.append(clf)\n'''","340d378b":"x_train, x_valid, y_train, y_valid = train_test_split(X[col], y, test_size=0.3, random_state=42)\nclf = TabNetRegressor()  #TabNetRegressor()\nclf.fit(\n  x_train.values, y_train.values.reshape(-1,1),\n  eval_set=[(x_valid.values, y_valid.values.reshape(-1,1))])\n","2700d7a9":"preds = clf.predict(x_valid.values)","cca9c2f1":"\n#rmspe_score = rmspe(y_valid, preds)\n#print(f'RMSPE is {rmspe_score}')","4ddce7c2":"#preds =[]\n#for clf in models\n#    preds.append(pred = clf.predict(test))\n    ","cac4f198":"len(x_train[col].columns)","d9c0915d":"len(df_test[col].columns)","90703eb3":"x_train[col].head()","ce086d2f":"#df_test[col] = scaler.fit_transform(df_test[col])","8b1b5aac":"df_test[col] = df_test[col].fillna(df_test[col].mean())","56a5de10":"df_test[col] = df_test[col].fillna(0)","1e1a880b":"df_test[col]","ac7efa0e":"preds = clf.predict(df_test[col].values)","25b49f71":"sub = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv')\nsub.target = preds.mean()","97b8905b":"sub.to_csv('submission.csv',index=False)\nsub","54a71a90":"### I'll do the AutoML system later.","58a48318":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">Pre-processing<\/span>","5ff58b77":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">The process of reading a trade file<\/span>","43e94ede":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">Test data create<\/span>","65f9034d":"There is a detailed explanation here. As you can see, I use it as it is. <br>\nIt seems that the parameters can also be changed. For the time being, I will use the initial value <br>\n\nWhy did you use that parameter some time ago? Q \/ A came. it's simple. Because it is an initial parameter. <br>\nhttps:\/\/dreamquark-ai.github.io\/tabnet\/generated_docs\/README.html#installation","73dea4b6":"\ud83d\ude05\u3299\ud83d\udd30\ud83d\uddd1\u2b1b\ud83d\udfe5\ud83d\udfe8\ud83d\udfe9","569c2db6":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">Dedicated function<\/span>","22faa2e3":"### \ud83d\udd144 Operate all with multiplex. (Df = Parallel (n_jobs = 4, verbose = 1) makes 4 multiplexes) Still it takes time.","2cc13c41":"![image.png](attachment:6a3054b2-ad53-44e0-bf9d-a15ba5d856b0.png)","1b2363f4":"### Create row_id to merge data","14d23c9b":"> ## TabNET\n> Deep learning model for table data <br>\n> A model that takes advantage of Tree-based and DNN <br>\n> While having the interpretability of Tree-based (decision tree-based algorithm), it has high performance like DNN for large data sets. <br>\n> Unlike the Tree-based model, it does not require any feature generation.","6d74e892":"### Please refer to the LightBGM edition for the explanation here.","453f9ac3":"\u2328Make a backup. It's hard to make a mistake and start over.","477a9483":"![image.png](attachment:4dd6d667-9a3d-4550-80f4-6ddce86ad4c6.png)","2906a104":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\"> Train data create<\/span>","a2248f94":"### Make sure that train and test have the same number of columns","57d69df5":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">TabNetRegressor<\/span>","6f6e780e":"### If you have GPU time,I recommed you to use it."}}