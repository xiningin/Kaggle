{"cell_type":{"ec4a1ff3":"code","d3ea606d":"code","8d08429e":"code","9f7bb287":"code","7c5cd881":"code","86626262":"code","bbec105e":"code","a5e1561e":"code","8c97d865":"code","9a990777":"code","30f7527b":"code","31159583":"code","4a9dff09":"code","1854b8e0":"code","e878adcf":"code","e423ece0":"code","7fbad24f":"code","c672c5d0":"markdown","85583ce5":"markdown","59c919fc":"markdown","1f1fb3a5":"markdown","9196327e":"markdown","b3b9f92a":"markdown","e55762cd":"markdown","bb78b2fe":"markdown","6ea9a5b5":"markdown","decc0760":"markdown","33b41a41":"markdown","aaaefb73":"markdown","8cf0b230":"markdown","9c854fd3":"markdown","b6f99bff":"markdown","985675ec":"markdown","5e62e4b1":"markdown","6676ff99":"markdown","ee7efca9":"markdown","374e43b1":"markdown","203d2bcc":"markdown","23eed62b":"markdown"},"source":{"ec4a1ff3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport seaborn as sns\nimport time","d3ea606d":"data = pd.read_csv('..\/input\/sentiment-analysis-for-financial-news\/all-data.csv',encoding=\"latin1\",header=None)\ndata.columns = [\"Label\",\"Text\"]\ndata.head()","8d08429e":"plt.subplots(figsize=(5,5))\nsns.countplot(data[\"Label\"])\nplt.show()","9f7bb287":"data[\"len\"] = [len(text) for text in data[\"Text\"].values]\n\ndata.groupby(\"Label\")[\"len\"].mean()","7c5cd881":"def cleanText(text):\n    \n    lemma = WordNetLemmatizer()\n    stp = stopwords.words('english')\n    \n    # This means remove everything except alphabetical and numerical characters\n    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n    \n    text = text.lower()\n    \n    # This mean split sentences by words (\"I am good\" => [\"I\",\"am\",\"good\"])\n    text = nltk.word_tokenize(text)\n    \n    # Lemmatizers convert words to their base form using dictionaries (going => go, bees => be , dog => dog)\n    text = [lemma.lemmatize(word) for word in text]\n    \n    # We should remove stopwords, stopwords are the words that has no special meaning such as I,You,Me,Was\n    text = [word for word in text if word not in stp]\n    \n    # Everything is ready, now we just need join the elements of lists ([\"feel\",\"good\"] => \"feel good\")\n    text = \" \".join(text)\n    \n    return text","86626262":"cleanText(\"Nowadays I am interested in traditional text feature extraction methods, because I want to learn foundations\")","bbec105e":"start_time = time.time()\ncleanedText = []\nfor text in data[\"Text\"]:\n    \n    cleanedText.append(cleanText(text))\nend_time = time.time()\nprocess_time = round(end_time-start_time,2)\n\nprint(\"=\"*10)\nprint(\"Texts are cleaned, this process took {} seconds \\n \\n\".format(process_time))\n\nprint(cleanedText[0])\n","a5e1561e":"data[\"Label\"].value_counts()","8c97d865":"y = []\nfor label in data[\"Label\"]:\n    \n    if label==\"negative\":\n        y.append(0)\n        \n    elif label==\"positive\":\n        y.append(2)\n        \n    elif label==\"neutral\":\n        y.append(1)\n\ny = np.asarray(y)","9a990777":"# First, we need a vectorizer object\nvectorizer = TfidfVectorizer(max_features=4000)\n# This means just consider most used 4000 words\n\nstart = time.time()\n\nx = vectorizer.fit_transform(cleanedText).toarray()\n\nprocess_time = round(time.time()-start,2)\n\nprint(\"Vectorizing cleaned text using TF-IDF approach took {} seconds\".format(process_time))\n\nx.shape","30f7527b":"x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42,test_size=0.2)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","31159583":"# We'll use 100 weak learners to build a strong learner\nclassifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100)\n\nclassifier.fit(x_train,y_train)","4a9dff09":"y_pred = classifier.predict(x_test)\n\nprint(\"Test set accuracy of our Adaboost Classifier is {}\".format(round(accuracy_score(y_pred,y_test)*100,2)))\n\nplt.subplots(figsize=(6,6))\nsns.heatmap(confusion_matrix(y_pred=y_pred,y_true=y_test),annot=True,fmt=\".1f\",linewidths=1.5,cmap=\"BuPu_r\")\nplt.show()","1854b8e0":"pickle.dump(classifier,open(\"adaboost.pickle\",\"wb\"))\npickle.dump(vectorizer,open(\"vectorizer.pickle\",\"wb\"))\n            ","e878adcf":"def analyseText(text):\n    \n    cls = pickle.load(open(\"adaboost.pickle\",\"rb\"))\n    vct = pickle.load(open(\"vectorizer.pickle\",\"rb\"))\n    \n    # First we need to clean the text given\n    text = cleanText(text)\n    \n    # Then we need to vectorize the text\n    text = vct.transform([text])\n    \n    # And let's predict results using vector\n    pred = cls.predict(text)\n    \n    decision = \"neutral\"\n    \n    if pred[0] == 0:\n        decision = \"negative\"\n        \n    elif pred[0] == 2:\n        decision = \"positive\"\n        \n    return decision\n        ","e423ece0":"analyseText(\"Rental of building equipment accounted for 88 percent of the operating income \")","7fbad24f":"analyseText(\"O'Leary 's Material Handling Services , located in Perth , is the leading company in Western Australia that supplies , installs and provides service for tail lifts .\")\n","c672c5d0":"# Implementation\nIn this main section we will implement everything that we learnt. Let's start!\n\n## Preparing Environment\nFirst, we'll import libraries, then we will load our dataset","85583ce5":"# Step 4: Building and Training Adaboost Classifier\nIn this section we'll train our Adaboost classifier using vectorized texts.","59c919fc":"* Last step before modeling is splitting dataset into train and test.","1f1fb3a5":"## Step 3: Implementing TF-IDF Vectorizing\nIn this section we'll vectorize our cleaned texts using TF-IDF approach.","9196327e":"# Introduction\nHello people, welcome to this kernel&tutorial. Nowadays I've started to learn **boosting algorithms** and wanted to write a simple (implementation-oriented) tutorial about first succesfull boosting algorithm: **Adaboost**\n\nAfter this kernel, you will have been:\n* Learnt how Adaboost and Boosting classifiers work\n* Learnt how to train a Adaboost Classifier with Decision Trees\n* Learnt what is Bag of Words and what is the disadvanteges of it\n* Learnt what is TF-IDF and how to implement it using sklearn \n* How to save a sklearn model and vectorizer using python pickle\n\n# Notebook Content\n\n## Theory\n* Simply Explained: Adaboost Classifier\n* Simply Explained: Bag of Words (BoW)\n* Simply Explained: TF-IDF Vectorization\n\n\n## Implementation\n* Preparing Environment\n* Step 1: Data Analyses\n* Step 2: Cleaning Text\n* Step 3: Implementing TF-IDF\n* Step 4: Building and Training Adaboost Classifier\n* Step 5: Saving Model and Defining Test Function\n","b3b9f92a":"First sentence is *Today I am going to study data science*, by matrix this sentence contains *today*,*I*,*am*,*go*,*study*,*data*,*science*. \n\nBag of Words is just all about this, let's move on to the TF-IDF feature extraction.","e55762cd":"* Dataset is unbalanced, most of the dataset is neutral as we can predict. \n* This may cause problems.","bb78b2fe":"* Then let's take a look at the lenghts by class.","6ea9a5b5":"* Because of our dataset is unbalanced we could'nt train our classifier great.","decc0760":"* Everything is ready let's use our function","33b41a41":"## Step 1: Data Analyses\nIn this section we'll take a look at the data.\n\n* Let's check whether are there any missing value or not","aaaefb73":"* Let's try our function","8cf0b230":"## Simply Explained: Bag of Words\nTo understand why TF-IDF feature extraction important, I must explain what is Bag of Words (BOW) feature extraction. \n\nIn bag of words approach, we'll create a sparse matrix (a matrix which most of the elements are zero) using sentences. Each future will be a word, let's take a look at the example below:\n\nWe have a dataset that contains that 6 sentences:\n* Today I am going to study data science\n* Tomorrow you will go\n* I am interested in data science\n* You was a good man\n* I just wanted to be a good man\n* Are you interested in statistics?\n\nI've said each word will be feature, so our features will be:\n\n    TODAY I AM GO STUDY DATA SCIENCE TOMORROW YOU WILL INTERESTED IN WAS GOOD MAN JUST WANT BE ARE STATISTICS\n    \nIf the sentence given contains the feature, its value will be 1 and if not its value will be 0. Let's create our Bag of Words vectors\n\n        TODAY I AM GO STUDY DATA SCIENCE TOMORROW YOU WILL INTERESTED IN WAS GOOD MAN JUST WANT BE ARE STATISTICS\n    1     1   1  1  1  1     1     1      0         0  0        0      0   0   0    0   0    0   0  0    0\n    2     0   0  0  1  0     0     0      1         1  1        0      0   0   0    0   0    0   0  0    0\n    3     0   1  1  0  0     1     1      0         0  0        1      1   0   0    0   0    0   0  0    0\n    4     0   0  0  0  0     0     0      0         1  0        0      0   1   1    1   0    0   0  0    0\n    5     0   1  0  0  0     0     0      0         0  0        0      0   0   1    1   1    1   1  0    0\n    6     0   0  0  0  0     0     0      0         1  0        0      1   0   0    0   0    0   0  1    1\n    \n","9c854fd3":"* As we can see length of sentences are unrelated with classes.\n\nLet's move on to the next section and preprocess the texts.","b6f99bff":"## Step 2: Cleaning Texts\nIn this section we'll clean the texts, in order to clean texts we will define a function.","985675ec":"## Step 5: Saving Model and Defining Test Function\nIn this section we'll save model and vectorizer using pickle object serialization and write a function that takes text as parameter and returns sentiment (neutral,positive,negative) ","5e62e4b1":"* And now let's test our classifier","6676ff99":"# Theory\nIn this main section, we will cover how things works. I won't explain everything in depth, you can learn the details from articles, main subject of this kernel will be to make you understand concepts such as TF-IDF\n\nI will explain everything with graphs that drawn on **MS Paint :)**\n\n## Simply Explained: Adaboost Classifier\n\nAdaboost is the first succesfull boosting algorithm that invented in 1996 by **Robert Schapire and Yoav Freund**. Probably you ask, great but what is a boosting algorithm?\n\nGradient boosting based classifiers are classifiers to create a strong classifier using many weak learners. I know this was a confusing sentence. Let me show you how it works with 2-D charts.\n\n![dataset.png](attachment:dataset.png)\n\n* This is our dataset contains two class, the blue ones are class A and others are class B.\n* We want to classify them just using simple vertical and horizontal lines, such as:\n\n\n![1.png](attachment:1.png)\n* But this is a bad line, let's start. As we can see, most of the left side of chart contains class B dots, so we can draw a vertical line to split most of class B dots. \n\n![wk1.png](attachment:wk1.png)\n* Yea, **weak learner** concept is just that, probably you know we can do it with a decision tree.\n* But still we need more **weak learner**, as you can see most of the points above is B, so we can draw a **weak learner** to there.\n\n![wk2.png](attachment:wk2.png)\n* Almost ready, now we just need to rescue the blue point at above.\n\n![finish.png](attachment:finish.png)\n* Finally, we finished training our classifier with three decision tree estimator (week learner). If a dot is in zone A or zone B, it must be class B\n* And if a dot is in zone C,D or E, its class must be A\n\nEverything you need to get started with Adaboost is over, you can (should) learn details from Articles, let's move on to the text feature extraction methods.\n\n","ee7efca9":"# Simply Explained: Term Frequency x Inverse Document Frequency \n\nIn previous section, we've learnt bag of words. In bag of words we've used binary features (features that show whether the sentence contain the word or not) but sometimes as you can predict it can cause real problems. \n\nIn TFxIDF each future will be a word as well, but we won't use binary. We'll use a formula to determine TF-IDF score of feature.\n\n=> The i feature's value of j sentence\n#### TF-IDF Score: TF(i,j) * IDF(i)\n\n**Term Frequency(i,j)** = How many times **i** word occurs in **j** \/ how many words are there in **j**\n\n**Inverse Document Frequency(i)** = log2(number of sentences in dataset\/ number of sentences that contains **i** word)\n\nIt's all about this, let's make an example. \n\n* We have dataset that contains 500 sentences and all the sentences contain **I** word. (What a selfish dataste ha!)\n* Let's calculate TF-IDF score of **I** feature of sentence below:\n        I am really interested in gradient boosting\n   \n* TF Score will be = 1\/6\n* Inverse Document will be = log2(500\/500) = 0\n* So it's TF-IDF score will be 0, it means **I** does not have a special meaning.\n\nI hope you understand, let's start to implement.\n","374e43b1":"# Conclusion\nThanks for your attention, if you have any question feel free to ask in comments. Also if you liked this kernel, please upvote to motivate me!\n\nHave a great day!","203d2bcc":"* Now let's clean our entire texts.","23eed62b":"* Before moving on to the next section, let's encode our labels. Let negatives be 0, neutrals be 1 and positives be 2"}}