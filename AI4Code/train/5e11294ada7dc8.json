{"cell_type":{"fcd93bf6":"code","07762caf":"code","dac1ba4e":"code","a79d0c5d":"code","d2fc5052":"code","207dcd46":"code","2f4b2a08":"code","849715fc":"code","4ed2008e":"code","8ad4d111":"code","b4910fc3":"code","0ca2ace8":"code","882b240e":"code","afccd0d5":"code","1f89508d":"code","cdf9105d":"code","3ca7417a":"code","b85fb4bd":"code","e99a6501":"code","44712d91":"code","97feaed1":"code","325af767":"code","40d6b3f3":"code","c76b705c":"code","0bb2c2b1":"code","a49d44f0":"code","e2ef0ba9":"code","7105d3b8":"code","ad668af1":"code","92973874":"markdown","b11b1c7c":"markdown","5971e78b":"markdown","ec57c699":"markdown","fc527bc8":"markdown","9134e997":"markdown","f6837d2d":"markdown","6447d897":"markdown","8f78eb37":"markdown","170ba9eb":"markdown","2487d288":"markdown","9222057f":"markdown","d2de653b":"markdown","4596646a":"markdown","99084c10":"markdown","3a88aba2":"markdown","80fafb06":"markdown","448f21e3":"markdown","3482eaef":"markdown","fe67d670":"markdown","d3e7c4ec":"markdown","1951eba7":"markdown","a9ec6561":"markdown","c84c76ad":"markdown","5d62906b":"markdown","2df1eacc":"markdown","2d6fbf43":"markdown"},"source":{"fcd93bf6":"# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nimport missingno as msno\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option(\"display.max_rows\", 2000)\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.width\", 100)\npd.set_option(\"display.max_colwidth\", 500)\npd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n\nfrom sklearn.inspection import permutation_importance\nfrom IPython.display import display, HTML, display_html\n\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\nimport matplotlib as mpl\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.ticker as mticker\nfrom lightgbm import LGBMRegressor\nimport optuna\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nimport math\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_selection import VarianceThreshold\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\ncwd = os.getcwd()\nprint(cwd)\n\n\nimport gc\n\ngc.enable()\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.models import (\n    BasicTicker,\n    ColorBar,\n    ColumnDataSource,\n    LinearColorMapper,\n    PrintfTickFormatter,\n)\nfrom bokeh.plotting import figure\nfrom bokeh.transform import transform\n\n\nrandom_state = 55\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","07762caf":"%%javascript\nIPython.OutputArea.auto_scroll_threshold = 9999;","dac1ba4e":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","a79d0c5d":"print(\n    f\"Train set contains {train.shape[0]} rows,{train.shape[1]} columns. \\nTest set contains {test.shape[0]} rows, {test.shape[1]} columns.\\n\"\n)\nprint(\n    f\"{set(train.columns) - set(test.columns)} are the fields that are IN TRAIN and NOT IN TEST.\\n {set(test.columns) - set(train.columns)} are the fields that are IN TEST and NOT IN TRAIN. \"\n)\n","d2fc5052":"train.info()","207dcd46":"display(\n    train.describe().iloc[:, 0:18].applymap(\"{:,g}\".format),\n    train.describe().iloc[:, 18:].applymap(\"{:,g}\".format),\n)\n","2f4b2a08":"sample_count = 5\n\ndisplay(\n    train.sample(sample_count, random_state=random_state)\n    .iloc[:, :30]\n    .style.hide_index(),\n    train.sample(sample_count, random_state=random_state)\n    .iloc[:, 30:60]\n    .style.hide_index(),\n    train.sample(sample_count, random_state=random_state)\n    .iloc[:, 60:]\n    .style.hide_index(),\n)\n","849715fc":"Id = \"Id\"\n\nsubmission_ID = test.loc[:, Id]\n\ntrain.drop(Id, axis=1, inplace=True)\ntest.drop(Id, axis=1, inplace=True)\n\n# For identification purposes\ntrain.loc[:, \"Train\"] = 1\ntest.loc[:, \"Train\"] = 0\n\ntest[\"SalePrice\"] = 0\n\nstacked_DF = pd.concat([train, test], ignore_index=True)\n","4ed2008e":"params = {\n    \"axes.labelsize\": 12,\n    \"axes.titlesize\": 16,\n    \"xtick.labelsize\": 11,\n    \"ytick.labelsize\": 11,\n}\nplt.rcParams.update(params)\n\n(fig, ax) = plt.subplots(nrows=2, ncols=1, figsize=[8, 10], sharex=True)\n\nax[0].set_ylabel(\"Count\")\nax[0].yaxis.label.set_color('midnightblue')\nax[0].title.set_color('midnightblue')\nax[1].set_xlabel(\"SalePrice\")\nax[1].set_ylabel(\"Count\")\nax[1].xaxis.label.set_color('midnightblue')\nax[1].yaxis.label.set_color('midnightblue')\nax[1].title.set_color('midnightblue')\nax[1].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n\nplot_X = stacked_DF.loc[stacked_DF[\"Train\"] == 1][\"SalePrice\"]\n\nplot = ax[0].hist(plot_X, bins=75, log=False)\nplot = ax[1].hist(plot_X, bins=75, log=True)\n\nax[0].set_title(\"Sale Price\")\nax[1].set_title(\"Sale Price (Log Transformed)\")\n","8ad4d111":"params = {\"axes.labelsize\": 12, \n          \"xtick.labelsize\": 11, \n          \"ytick.labelsize\": 11}\nplt.rcParams.update(params)\n\nfeatures_to_viz = [\n    \"GrLivArea\",\n    \"YearBuilt\",\n    \"WoodDeckSF\",\n    \"LotArea\",\n    \"GarageArea\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"TotalBsmtSF\",\n    \"LotFrontage\",\n    \"GarageYrBlt\",\n]\n\n# Because there are a lot of variables to vizualize,\n# sorting them helps me keep track of which variable is where\n\nfeatures_to_viz = sorted(features_to_viz)\n\nncols = 2\nnrows = math.ceil(len(features_to_viz) \/ ncols)\nunused = nrows * ncols - len(features_to_viz)\n\nfigw = ncols * 5\nfigh = nrows * 4\n\n(fig, ax) = plt.subplots(nrows, ncols, sharey=True, figsize=(figw, figh))\nfig.subplots_adjust(hspace=0.3, wspace=0.1)\nax = ax.flatten()\n\nfor i in range(unused, 0, -1):\n    fig.delaxes(ax[-i])\n\nfor (n, col) in enumerate(features_to_viz):\n    if n % 2 != 0:\n        ax[n].yaxis.label.set_visible(False)\n    ax[n].set_xlabel(col)\n    ax[n].set_ylabel(\"SalePrice\")\n    ax[n].xaxis.label.set_color('midnightblue')\n    ax[n].yaxis.label.set_color('midnightblue')\n    ax[n].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n    sns.scatterplot(\n        x=col,\n        y=\"SalePrice\",\n        data=stacked_DF.loc[stacked_DF[\"Train\"] == 1],\n        legend=False,\n        ax=ax[n],\n    )\n\nplt.show()","b4910fc3":"features_to_viz = [\n    \"BsmtQual\",\n    \"ExterQual\",\n    \"FireplaceQu\",\n    \"ExterCond\",\n    \"KitchenQual\",\n    \"LotShape\",\n    \"OverallQual\",\n    \"FullBath\",\n    \"HalfBath\",\n    \"TotRmsAbvGrd\",\n    \"Fireplaces\",\n    \"KitchenAbvGr\",\n    \"Neighborhood\",\n]\n\nncols = 2\nnrows = math.ceil(len(features_to_viz) \/ ncols)\nunused = nrows * ncols - len(features_to_viz)\n\n(figw, figh) = (ncols * 7, nrows * 6)\n\n(fig, ax) = plt.subplots(nrows, ncols, sharey=False, figsize=(figw, figh))\nfig.subplots_adjust(hspace=0.2, wspace=0.1)\n\nax = ax.flatten()\nfor i in range(unused, 0, -1):\n    fig.delaxes(ax[-i])\n\nfor (n, col) in enumerate(features_to_viz):\n    if(col==\"Neighborhood\"):\n        ordering = (\n        stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n        .groupby(by=col)[\"SalePrice\"]\n        .median()\n        .sort_values()\n        .index\n        )\n        sns.boxplot(\n        x=\"SalePrice\",\n        y=col,\n        data=stacked_DF.loc[stacked_DF[\"Train\"] == 1],\n        order=ordering,\n        ax=ax[n],\n        orient=\"h\")\n        ax[n].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n        ax[n].xaxis.label.set_color('midnightblue')\n        ax[n].yaxis.label.set_color('midnightblue')\n    else:\n        ordering = (\n            stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n            .groupby(by=col)[\"SalePrice\"]\n            .median()\n            .sort_values()\n            .index\n        )\n        if(n%2==1):\n            visibility=False\n        else:\n            visibility=True\n        ax[n].get_yaxis().set_visible(visibility)\n        sns.boxplot(\n            y=\"SalePrice\",\n            x=col,\n            data=stacked_DF.loc[stacked_DF[\"Train\"] == 1],\n            order=ordering,\n            ax=ax[n],\n            orient=\"v\",\n        )\n        ax[n].xaxis.label.set_color('midnightblue')\n        ax[n].yaxis.label.set_color('midnightblue')\n        ax[n].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n\nplt.show()","0ca2ace8":"print(\"Missing Value Counts in Train DF\")\nstacked_DF[stacked_DF[\"Train\"] == 1].isna().sum()[\n    stacked_DF[stacked_DF[\"Train\"] == 1].isna().sum() > 0\n].sort_values(ascending=False)","882b240e":"print(\"Missing Values in Test DF\")\nstacked_DF[stacked_DF[\"Train\"] == 0].isna().sum()[\n    stacked_DF[stacked_DF[\"Train\"] == 0].isna().sum() > 0\n].sort_values(ascending=False)\n","afccd0d5":"# Check missing records in train set\nna_cols = (stacked_DF.isna().sum()[stacked_DF.isna().sum() > 0]).index\nmat = msno.matrix(\n    stacked_DF.loc[:, na_cols], labels=True, figsize=(14, 10), fontsize=12, inline=False\n)","1f89508d":"# Assuming Neighborhood and MSZoning are related.\nlookup = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"Neighborhood\")[\"MSZoning\"]\n    .agg(pd.Series.mode)\n)\nstacked_DF[\"MSZoning\"] = stacked_DF[\"MSZoning\"].fillna(\n    stacked_DF[\"Neighborhood\"].map(lookup)\n)\n\n# Assuming KitchenQual and OverallQual are related.\nlookup = (\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1]\n    .groupby(by=\"OverallQual\")[\"KitchenQual\"]\n    .agg(pd.Series.mode)\n)\nstacked_DF[\"KitchenQual\"] = stacked_DF[\"KitchenQual\"].fillna(\n    stacked_DF[\"OverallQual\"].map(lookup)\n)\n\n# For these features I replace nan with a string indicator: \"missing\"\ncols_na_to_missing = {\n    \"Alley\",\n    \"BsmtCond\",\n    \"BsmtExposure\",\n    \"BsmtFinType1\",\n    \"BsmtFullBath\",\n    \"BsmtQual\",\n    \"Fence\",\n    \"FireplaceQu\",\n    \"GarageCond\",\n    \"GarageFinish\",\n    \"GarageQual\",\n    \"GarageType\",\n    \"MasVnrType\",\n    \"MiscFeature\",\n    \"PoolQC\",\n    \"BsmtFinType2\",\n}\n\n# For these features I replace nan with the integer 0\ncols_na_to_zero = {\n    # 'BsmtUnfSF',\n    \"GarageArea\",\n    \"GarageCars\",\n    \"TotalBsmtSF\",\n    \"MasVnrArea\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtFullBath\",\n    \"BsmtHalfBath\",\n    \"GarageYrBlt\",\n}\n\n# For these features I replace nan with the mode of the feature the record is missing.\ncols_na_to_mode = {\n    \"Functional\",\n    \"Electrical\",\n    \"Utilities\",\n    \"Exterior1st\",\n    \"Exterior2nd\",\n    \"SaleType\",\n}\n\nfor col in cols_na_to_missing:\n    stacked_DF[col] = stacked_DF[col].astype(object).fillna(\"Missing\")\n\nfor col in cols_na_to_zero:\n    stacked_DF[col] = stacked_DF[col].astype(object).fillna(0)\n\nfor col in cols_na_to_mode:\n    stacked_DF[col] = (\n        stacked_DF[col]\n        .astype(object)\n        .fillna(stacked_DF.loc[stacked_DF[\"Train\"] == 1, col].mode()[0])\n    )\n\n# Imputing remaining missing values with the help of iterative imputer.\nnum_features = stacked_DF.drop(columns=[\"Train\"]).select_dtypes(\"number\").columns\nimputer = IterativeImputer(\n    RandomForestRegressor(max_depth=8),\n    n_nearest_features=10,\n    max_iter=10,\n    random_state=random_state,\n)\nstacked_DF.loc[stacked_DF[\"Train\"] == 1, num_features] = imputer.fit_transform(\n    stacked_DF.loc[stacked_DF[\"Train\"] == 1, num_features].values\n)\nstacked_DF.loc[stacked_DF[\"Train\"] == 0, num_features] = imputer.transform(\n    stacked_DF.loc[stacked_DF[\"Train\"] == 0, num_features].values\n)\n","cdf9105d":"stacked_DF[\"WarmSeason\"] = np.where(\n    stacked_DF[\"MoSold\"].isin([10, 11, 12, 1, 2, 3]), 0, 1\n)\nstacked_DF[\"SqFtPerRoom\"] = stacked_DF[\"GrLivArea\"] \/ (\n    stacked_DF[\"TotRmsAbvGrd\"]\n    + stacked_DF[\"FullBath\"]\n    + stacked_DF[\"HalfBath\"]\n    + stacked_DF[\"KitchenAbvGr\"]\n)\n\n# Converting MSSubClass to categorical\nstacked_DF[\"MSSubClass\"] = stacked_DF[\"MSSubClass\"].astype(str)\n\n# I combine underrepresented categories under one umbrella and\/or with another category in the same field\next2_map = {\"AsphShn\": \"Oth1\", \"CBlock\": \"Oth1\", \"CmentBd\": \"Oth2\", \"Other\": \"Oth2\"}\nroofmatl_map = {\n    \"Roll\": \"Oth1\",\n    \"ClyTile\": \"Oth1\",\n    \"Metal\": \"Oth1\",\n    \"CompShg\": \"Oth1\",\n    \"Membran\": \"Oth2\",\n    \"WdShake\": \"Oth2\",\n}\n\ncond2_map = {\"PosA\": \"Pos\", \"PosN\": \"Pos\", \"RRAe\": \"Norm\", \"RRAn\": \"Norm\"}\n\n\nstacked_DF[\"Exterior2nd\"] = (\n    stacked_DF[\"Exterior2nd\"].map(ext2_map).fillna(stacked_DF[\"Exterior2nd\"])\n)\nstacked_DF[\"RoofMatl\"] = (\n    stacked_DF[\"RoofMatl\"].map(roofmatl_map).fillna(stacked_DF[\"RoofMatl\"])\n)\nstacked_DF[\"Condition2\"] = (\n    stacked_DF[\"Condition2\"].map(cond2_map).fillna(stacked_DF[\"Condition2\"])\n)\n\n# Dropping a handful of features as there are other variables that are perfectly correlated to these\n# I did trial and error here based on the impact of removing features on RMSE.\nstacked_DF.drop(columns=[\"GarageYrBlt\", \"Utilities\"], inplace=True)","3ca7417a":"output_notebook()\n\ndf_to_viz = stacked_DF[stacked_DF[\"Train\"] == 1].drop(columns=\"Train\")\n\nxcorr = abs(df_to_viz.corr())\nxcorr.index.name = \"Feature1\"\nxcorr.columns.name = \"Feature2\"\n\ndf = pd.DataFrame(xcorr.stack(), columns=[\"Corr\"]).reset_index()\n\nsource = ColumnDataSource(df)\n\ncolors = [\n    \"#75968f\",\n    \"#a5bab7\",\n    \"#c9d9d3\",\n    \"#e2e2e2\",\n    \"#dfccce\",\n    \"#ddb7b1\",\n    \"#cc7878\",\n    \"#933b41\",\n    \"#550b1d\",\n]\n\nmapper = LinearColorMapper(palette=colors, low=df.Corr.min(), high=df.Corr.max())\n\nf1 = figure(\n    plot_width=800,\n    plot_height=800,\n    title=\"Correlation Heat Map\",\n    x_range=list(sorted(xcorr.index)),\n    y_range=list(reversed(sorted(xcorr.columns))),\n    toolbar_location=None,\n    tools=\"hover\",\n    x_axis_location=\"above\",\n)\n\nf1.rect(\n    x=\"Feature2\",\n    y=\"Feature1\",\n    width=1,\n    height=1,\n    source=source,\n    line_color=None,\n    fill_color=transform(\"Corr\", mapper),\n)\n\ncolor_bar = ColorBar(\n    color_mapper=mapper,\n    location=(0, 0),\n    ticker=BasicTicker(desired_num_ticks=len(colors)),\n    formatter=PrintfTickFormatter(format=\"%d%%\"),\n)\nf1.add_layout(color_bar, \"right\")\n\nf1.hover.tooltips = [\n    (\"Feature1\", \"@{Feature1}\"),\n    (\"Feature2\", \"@{Feature2}\"),\n    (\"Corr\", \"@{Corr}{1.1111}\"),\n]\n\nf1.axis.axis_line_color = None\nf1.axis.major_tick_line_color = None\nf1.axis.major_label_text_font_size = \"12px\"\nf1.axis.major_label_standoff = 2\nf1.xaxis.major_label_orientation = 1.0\n\nshow(f1)\n","b85fb4bd":"# Obtaining a list of categorical, numerical, and boolean - like features.\nbool_features = [\n    col\n    for col in stacked_DF.select_dtypes(include=[\"number\"]).columns\n    if np.array_equal(\n        np.sort(stacked_DF[col].unique(), axis=0), np.sort([0, 1], axis=0)\n    )\n]\n\ncat_features = [col for col in stacked_DF.select_dtypes(exclude=[\"number\"]).columns]\nnum_features = [\n    col\n    for col in stacked_DF.select_dtypes(include=[\"number\"]).columns\n    if col not in (bool_features) and col != \"SalePrice\"\n]\n\n# Holding these two DF 's on the side.\n# Will need to concatenate later with the preprocessed(scaled and oh encoded) DF.\nbool_features.remove(\"Train\")\nX_train_bool = stacked_DF.loc[stacked_DF[\"Train\"] == 1, bool_features]\nX_test_bool = stacked_DF.loc[stacked_DF[\"Train\"] == 0, bool_features]\n\n# This list contains features that has the same set of values between train - test\nohe_cols_a = []\n\n# This list contains features that has different set of values between train - test\nohe_cols_b = []\n\nfor col in cat_features:\n    if set(stacked_DF.loc[stacked_DF[\"Train\"] == 1, col].unique()) != set(\n        stacked_DF.loc[stacked_DF[\"Train\"] == 0, col].unique()\n    ):\n        ohe_cols_b.append(col)\n\nohe_cols_a = list(set(cat_features) - set(ohe_cols_b))","e99a6501":"X_train = stacked_DF.loc[stacked_DF[\"Train\"] == 1].drop(\n    labels=[\"SalePrice\", \"Train\"], axis=1\n)\n# Applying log transformation to the target variable\ny_train = stacked_DF.loc[stacked_DF[\"Train\"] == 1, \"SalePrice\"].apply(np.log)\nX_test = stacked_DF.loc[stacked_DF[\"Train\"] == 0].drop(\n    labels=[\"SalePrice\", \"Train\"], axis=1\n)\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"onehota\", OneHotEncoder(sparse=False, drop=\"first\"), ohe_cols_a),\n        (\"onehotb\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"), ohe_cols_b),\n        (\"scaler\", StandardScaler(), num_features),\n    ],\n    remainder=\"drop\",\n)\n\n\npipeline = Pipeline(\n    [(\"Preprocessor\", preprocessor), (\"VarThreshold\", VarianceThreshold(0.01))]\n)\n\nX_train_preprocessed = pipeline.fit_transform(X_train)\nX_test_preprocessed = pipeline.transform(X_test)\n\n# Get the list of one hot encoded columns and combine them\noh_encoded_a = list(\n    preprocessor.named_transformers_.onehota.get_feature_names(ohe_cols_a)\n)\noh_encoded_b = list(\n    preprocessor.named_transformers_.onehotb.get_feature_names(ohe_cols_b)\n)\noh_encoded_cols = oh_encoded_a + oh_encoded_b\n\nfeature_names = np.array(oh_encoded_cols + num_features, order=\"K\")\n\n# Filtering out the features dropped by variance threshold\nfeature_names = feature_names[pipeline.named_steps.VarThreshold.get_support()]\n\n# Putting back the column names to help with analysis\nX_train_preprocessed = pd.DataFrame(data=X_train_preprocessed, columns=feature_names)\nX_test_preprocessed = pd.DataFrame(\n    data=X_test_preprocessed, columns=feature_names, index=X_test_bool.index\n)\n\n# Combine the DF's back together\nX_train = pd.concat([X_train_bool, X_train_preprocessed], axis=1)\nX_test = pd.concat([X_test_bool, X_test_preprocessed], axis=1)\n","44712d91":"model = Lasso(alpha=0.01)\nmodel.fit(X_train, y_train)\n\n\nfeature_imp = permutation_importance(\n    model, X_train, y_train, n_repeats=10, n_jobs=-1, random_state=random_state\n)\n\nperm_ft_imp_df = pd.DataFrame(\n    data=feature_imp.importances_mean, columns=[\"FeatureImp\"], index=X_train.columns\n).sort_values(by=\"FeatureImp\", ascending=False)\nmodel_ft_imp_df = pd.DataFrame(\n    data=model.coef_, columns=[\"FeatureImp\"], index=X_train.columns\n).sort_values(by=\"FeatureImp\", ascending=False)\n\nfig, ax = plt.subplots(2, 1, figsize=(10, 16))\n\nperm_ft_imp_df_nonzero = perm_ft_imp_df[perm_ft_imp_df[\"FeatureImp\"] != 0]\nmodel_ft_imp_df_nonzero = model_ft_imp_df[model_ft_imp_df[\"FeatureImp\"] != 0]\n\nsns.barplot(\n    x=perm_ft_imp_df_nonzero[\"FeatureImp\"],\n    y=perm_ft_imp_df_nonzero.index,\n    ax=ax[0],\n    palette=\"vlag\",\n)\nsns.barplot(\n    x=model_ft_imp_df_nonzero[\"FeatureImp\"],\n    y=model_ft_imp_df_nonzero.index,\n    ax=ax[1],\n    palette=sns.diverging_palette(10, 220, sep=2, n=80),\n)\n\nax[0].set_title(\"Permutation Feature Importance\")\nax[1].set_title(\"Lasso Feature Importance\")\n\nplt.show()","97feaed1":"# def objective(trial):\n#     _C = trial.suggest_float(\"C\", 0.1, 0.5)\n#     _epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.1)\n#     _coef = trial.suggest_float(\"coef0\", 0.5, 1)\n\n#     svr = SVR(cache_size=5000, kernel=\"poly\", C=_C, epsilon=_epsilon, coef0=_coef)\n\n#     score = cross_val_score(\n#         svr, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# svr_params = study.best_params\n# svr_best_score = study.best_value\n# print(f\"Best score:{svr_best_score} \\nOptimized parameters: {svr_params}\")","325af767":"# def objective(trial):\n\n#     _alpha = trial.suggest_float(\"alpha\", 0.5, 1)\n#     _tol = trial.suggest_float(\"tol\", 0.5, 0.9)\n\n#     ridge = Ridge(alpha=_alpha, tol=_tol, random_state=random_state)\n\n#     score = cross_val_score(\n#         ridge, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# ridge_params = study.best_params\n# ridge_best_score = study.best_value\n# print(f\"Best score:{ridge_best_score} \\nOptimized parameters: {ridge_params}\")\n","40d6b3f3":"# def objective(trial):\n\n#     _alpha = trial.suggest_float(\"alpha\", 0.0001, 0.01)\n\n#     lasso = Lasso(alpha=_alpha, random_state=random_state)\n\n#     score = cross_val_score(\n#         lasso, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# lasso_params = study.best_params\n# lasso_best_score = study.best_value\n# print(f\"Best score:{lasso_best_score} \\nOptimized parameters: {lasso_params}\")\n","c76b705c":"# def objective(trial):\n#     _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n#     _max_depth = trial.suggest_int(\"max_depth\", 5, 12)\n#     _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 8)\n#     _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 3, 6)\n#     _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n#     rf = RandomForestRegressor(\n#         max_depth=_max_depth,\n#         min_samples_split=_min_samp_split,\n#         ccp_alpha=_ccp_alpha,\n#         min_samples_leaf=_min_samples_leaf,\n#         max_features=_max_features,\n#         n_estimators=_n_estimators,\n#         n_jobs=-1,\n#         random_state=random_state,\n#     )\n\n#     score = cross_val_score(\n#         rf, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# rf_params = study.best_params\n# rf_best_score = study.best_value\n# print(f\"Best score:{rf_best_score} \\nOptimized parameters: {rf_params}\")\n","0bb2c2b1":"# def objective(trial):\n#     _num_leaves = trial.suggest_int(\"num_leaves\", 5, 20)\n#     _max_depth = trial.suggest_int(\"max_depth\", 3, 8)\n#     _learning_rate = trial.suggest_float(\"learning_rate\", 0.1, 0.4)\n#     _n_estimators = trial.suggest_int(\"n_estimators\", 50, 150)\n#     _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.2, 0.6)\n\n#     lgbm = LGBMRegressor(\n#         num_leaves=_num_leaves,\n#         max_depth=_max_depth,\n#         learning_rate=_learning_rate,\n#         n_estimators=_n_estimators,\n#         min_child_weight=_min_child_weight,\n#     )\n\n#     score = cross_val_score(\n#         lgbm, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n#     ).mean()\n#     return score\n\n\n# optuna.logging.set_verbosity(0)\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100)\n\n# lgbm_params = study.best_params\n# lgbm_best_score = study.best_value\n# print(f\"Best score:{lgbm_best_score} \\nOptimized parameters: {lgbm_params}\")\n","a49d44f0":"rf_params = {\"max_depth\": 8, \"max_features\": 40, \"n_estimators\": 132}\nsvr_params = {\n    \"kernel\": \"poly\",\n    \"C\": 0.053677105521141605,\n    \"epsilon\": 0.03925943476562099,\n    \"coef0\": 0.9486751042886584,\n}\nridge_params = {\n    \"alpha\": 0.9999189637151178,\n    \"tol\": 0.8668539399622242,\n    \"solver\": \"cholesky\",\n}\nlasso_params = {\"alpha\": 0.0004342843645993161, \"selection\": \"random\"}\nlgbm_params = {\n    \"num_leaves\": 16,\n    \"max_depth\": 6,\n    \"learning_rate\": 0.16060612646519587,\n    \"n_estimators\": 64,\n    \"min_child_weight\": 0.4453842422224686,\n}","e2ef0ba9":"cv = KFold(n_splits=4, random_state=random_state)\n\nsvr = SVR(**svr_params)\nridge = Ridge(**ridge_params, random_state=random_state)\nlasso = Lasso(**lasso_params, random_state=random_state)\nlgbm = LGBMRegressor(**lgbm_params, random_state=random_state)\nrf = RandomForestRegressor(**rf_params, random_state=random_state)\nstack = StackingCVRegressor(\n    regressors=[svr, ridge, lasso, lgbm, rf],\n    meta_regressor=LinearRegression(n_jobs=-1),\n    random_state=random_state,\n    cv=cv,\n    n_jobs=-1,\n)\n\nsvr_scores = cross_val_score(\n    svr, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n)\nridge_scores = cross_val_score(\n    ridge, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n)\nlasso_scores = cross_val_score(\n    lasso, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n)\nlgbm_scores = cross_val_score(\n    lgbm, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n)\nrf_scores = cross_val_score(\n    rf, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n)\nstack_scores = cross_val_score(\n    stack, X_train, y_train, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n)\n\nscores = [svr_scores, ridge_scores, lasso_scores, lgbm_scores, rf_scores, stack_scores]\nmodels = [\"SVR\", \"RIDGE\", \"LASSO\", \"LGBM\", \"RF\", \"STACK\"]\nscore_medians = [\n    round(np.median([mean for mean in modelscore]), 5) for modelscore in scores\n]\n","7105d3b8":"fig, ax = plt.subplots(figsize=(10, 6))\n\nvertical_offset = 0.001\n\nax.set_title(\"Model Score Comparison\")\nbp = sns.boxplot(x=models, y=scores, ax=ax)\n\n\nfor xtick in bp.get_xticks():\n    bp.text(\n        xtick,\n        score_medians[xtick] - vertical_offset,\n        score_medians[xtick],\n        horizontalalignment=\"center\",\n        verticalalignment=\"top\",\n        size=14,\n        color=\"b\",\n    )\n\nplt.show()\n","ad668af1":"stack.fit(X_train.values, y_train.values)\n\npredictions = stack.predict(X_test.values)\npredictions = np.exp(predictions)\n\nsubmission = pd.DataFrame({\"Id\": submission_ID, \"SalePrice\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","92973874":"**Checking to see data types and potential missing values**:","b11b1c7c":"# Feature Importance\n\nLasso is a good feature selection model for datasets like ours (high number of features, low sample size). <br>In addition to Lasso, I'll be using sklearn's permutation feature importance model. The pitfall of permutation importance is that it doesn't deal well with correlated variables. This is something to keep in mind when we're interpreting the results.","5971e78b":"There are a few columns with different set of categories among train and test sets\nThis causes an issue for one hot encoder's \"drop\" rule.<br> Therefore, I am applying seperate OneHot encoding to two different subset of categoricals.","ec57c699":"# Bivariate Analyis\n\nI will first plot a few numeric variables against the target variable, then I will do the same for categorical variables.\nWhat I am looking for is correlation, outliers and the distribution of the target variable with respect to dependent variables.<br>\nScatter Plot is good for numeric variable visualization. For categorical variables, I will use Box-plot.","fc527bc8":"**Looking at the graphs above, I find a few things to note:**\n\n* GrLivArea, YrBuilt, LotArea and a few others have linear-like relationship with the target variable.\n* There are a few outliers. Data source's notes mention these outliers, and recommends to remove them. I, however, don't like the idea of removing records as we're already working with limited amount of samples.\n\nMoving on with the categorical bivariate analysis.","9134e997":"Importing libraries.","f6837d2d":"# Hyperparameter optimizing with Optuna","6447d897":"# Final Submission\n\nI will first inverse log-transform then submit the final predictions.<br><br>","8f78eb37":"* OverallQual impacts SalePrice exponentially!\n* Neighborhood matters. Features regarding quality also matter.\n* **The more irregular the lot shape is, the higher the Sale Price seems**. This was a surprise to me. That being said, I can relate irregular lot shape to architectural originality, which costs money. Regular lot shape relates to just regular homes that more of us can afford. But this is only a guess.","170ba9eb":"Let's look at a sample of records...","2487d288":"# Model Comparison\n\nLet's use cross-validate-score to help us see how different models perform.","9222057f":"Features like OverallQual, Neighborhood, GrLivArea and Year built rank high in our models, as one would expect.","d2de653b":"Extracting the ID since we need to use it for submission later.<br>\nI also concatenate train and test sets into one dataframe.","4596646a":"**Descriptive statistics:**","99084c10":"# Target Variable Distribution - Univariate\n\nLet's look at the target variable distribution. I am creating an additional plot to show how log transformation impacts the variable since it's skewed.","3a88aba2":"Below I will impute missing records and do some feature engineering to clean up the data before modeling. I will also establish new features based on some of the existing features. ","80fafb06":"# Final Preprocessing Steps\n\nIn the next two sections, I will apply one hot encoding to categoricals, and scaling on numericals (except boolean-like features). Once that's done, I will have all the transformed variables go through Variance Threshold. Variance Threshold will remove any feature that shows less variance than what I want.  ","448f21e3":"# House Prices: Advanced Regression Techniques\n\n\nThe [Ames Housing](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. <br><br>\n\nI challenged myself not to copy\/peek at any piece of code from other data science notebooks for this competition (although I googled a heckload of questions, and read many Kaggle discussion threads on this competition). I ended up spending a lot hours trying to figure things out on my own. On the plus side, I ended up learning a whole lot more than I originally thought, which has been very beneficial.","3482eaef":"# Missing Values\n\nI will first look at number of missing records of each feature. Then I will utilize a visualization method to see relations between features with missing records. These relations may provide guidance dealing with missing records. For example, if there are a lot of houses where all garage associated features are missing, this most likely indicates the absence of a garage for those houses.","fe67d670":"# Imputing Missing Values and Feature Engineering","d3e7c4ec":"Having completed all preprocessing steps, let's continue with model selection and hyperparameter optimization. <br>\n<br> Due to high feature\/low sample size of our data, there's a great chance of overfitting. Models with regularization mechanism such as Lasso and Ridge do well in regards to such data. In addition Lasso and Ridge, I'll use SVR, LGBM and RandomForest regressor. \n<br><br>\nI first used Optuna's search algorithm to get a starting point of hyperparameters. Then I did a manual tweaking\/testing to further increase the accuracy. <br> What I like more about Optuna over Grid Search Algorithm is that it gives me the opportunity to set a \"search range\" instead of me having to declare values one by one for it to search from.<br> It also works faster compared to GridSearch. <br><br> I am leaving hyperparameter search section commented out as I already obtained the parameters I'll run the models with.","1951eba7":"Running the code below so that IPython shows the entire result of the code I run. \nThis becomes helpful to me while visualizing\/analyzing high number of plots in the same output.","a9ec6561":"It looks like log transformation will be relatively successful gaussianize the target variable.\nEven after the log transformation, I see some outliers on the right hand side of the spectrum. However, I am not sure if there's anything I can do for those.\n\n","c84c76ad":"# Correlation Heatmap\n\nLet's look the correlation heatmap to see which independent variables are correlated.<br> In an ideal scenario, we wouldn't see high correlation **among independent variables**. I trust our models will pick features that are important to them, without us needing to deal with multicollinearity. In fact, when I ran the model after picking and eliminating correlated features the model accuracy reduced.<br>\n\nAs useful as they are, heatmaps get messy pretty quickly dealing with high number of variables. Bokeh provides interactive plotting, which means I can hover over a red square to find out which two features have high correlation.","5d62906b":"Loading Train and Test data sets.","2df1eacc":"A general overview of the data we need to work with. \nA few lines of code going from high level overview to lower\/more detailed levels.","2d6fbf43":"When I run the models, I see that the stacked model scores slightly better than SVR.<br> Even if the accuracy of one or two individual models rated higher than stacked model, I still would have picked stacked model.<br> This is because I believe that it will do a better job at generalizing\/reduce impact of any overfitting compared to individual models. <br><br> The model trains quickly, and it is ~91% accurate, I find this quite impressive."}}