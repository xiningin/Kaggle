{"cell_type":{"c09922c7":"code","b902a028":"code","ca473a23":"code","729130bb":"code","86408abe":"code","ce88f057":"code","556911a7":"code","5f3e8637":"code","a5ef0e86":"code","62c84466":"code","fd187d82":"code","003c67cb":"code","bfe3989f":"code","b35a4939":"code","e17ce2ce":"code","b784964d":"code","04c2b38f":"code","76c518c9":"code","4b878474":"code","5b5a60e5":"code","add7a421":"code","cf66e99a":"code","56d5c739":"code","b7bb1f06":"code","24c454d4":"code","ffb70c6d":"code","1b122ee0":"code","eefafb71":"code","c6d46d40":"code","77c31345":"markdown","f0d3c634":"markdown","91243539":"markdown","61551033":"markdown","44cc57be":"markdown","f2db7a69":"markdown","3bf93fc9":"markdown","4d479295":"markdown","01fb6198":"markdown","7d2f8398":"markdown","e0aeb36b":"markdown","505727e4":"markdown","523267b2":"markdown","73dd0ca3":"markdown","397e6180":"markdown","d747b473":"markdown","fd0a3a4a":"markdown","286bfe75":"markdown"},"source":{"c09922c7":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import WordCloud\n\nnp.random.seed(100)","b902a028":"def get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embedding(file):\n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n    return embeddings_index","ca473a23":"def make_embedding_matrix(embedding, tokenizer, len_voc):\n    all_embs = np.stack(embedding.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    word_index = tokenizer.word_index\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n    \n    for word, i in word_index.items():\n        if i >= len_voc:\n            continue\n        embedding_vector = embedding.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","729130bb":"glove = load_embedding('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt')","86408abe":"df = pd.read_csv(\"..\/input\/train.csv\")\nprint(\"Number of texts: \", df.shape[0])","ce88f057":"df.head()","556911a7":"plt.figure(figsize = (10, 8))\nsns.countplot(df['target'])\nplt.show()","5f3e8637":"print(\"Class repartition : \", (Counter(df['target'])))","a5ef0e86":"len_voc = 100000","62c84466":"def make_tokenizer(texts, len_voc):\n    from keras.preprocessing.text import Tokenizer\n    t = Tokenizer(num_words=len_voc)\n    t.fit_on_texts(texts)\n    return t","fd187d82":"tokenizer = make_tokenizer(df['question_text'], len_voc)","003c67cb":"X = tokenizer.texts_to_sequences(df['question_text'])","bfe3989f":"from keras.preprocessing.sequence import pad_sequences\nX = pad_sequences(X, 70)","b35a4939":"y = df['target'].values","e17ce2ce":"index_word = {0: ''}\nfor word in tokenizer.word_index.keys():\n    index_word[tokenizer.word_index[word]] = word","b784964d":"embed_mat = make_embedding_matrix(glove, tokenizer, len_voc)","04c2b38f":"from sklearn.neighbors import NearestNeighbors\n\nsynonyms_number = 5\nword_number = 20000","76c518c9":"nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat) ","4b878474":"neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]","5b5a60e5":"synonyms = {x[0]: x[1:] for x in neighbours_mat}","add7a421":"for x in np.random.randint(1, word_number, 10):\n    print(f\"{index_word[x]} : {[index_word[synonyms[x][i]] for i in range(synonyms_number-1)]}\")","cf66e99a":"index = np.random.randint(1, word_number, 9)\nplt.figure(figsize=(20,10))\n\nfor k in range(len(index)):\n    plt.subplot(3, 3, k+1)\n    \n    x = index[k]\n    text = ' '.join([index_word[x]] + [index_word[synonyms[x][i]] for i in range(synonyms_number-1)]) \n    wordcloud = WordCloud(stopwords=[]).generate((text))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n","56d5c739":"X_pos = X[y==1]","b7bb1f06":"def modify_sentence(sentence, synonyms, p=0.5):\n    for i in range(len(sentence)):\n        if np.random.random() > p:\n            try:\n                syns = synonyms[sentence[i]]\n                sentence[i] = np.random.choice(syns)\n            except KeyError:\n                pass\n    return sentence","24c454d4":"indexes = np.random.randint(0, X_pos.shape[0], 10)","ffb70c6d":"for x in X_pos[indexes]:\n    sample =  np.trim_zeros(x)\n    sentence = ' '.join([index_word[x] for x in sample])\n    print(sentence)\n\n    modified = modify_sentence(sample, synonyms)\n    sentence_m = ' '.join([index_word[x] for x in modified])\n    print(sentence_m)\n    \n    print(' ')","1b122ee0":"n_texts = 30000","eefafb71":"indexes = np.random.randint(0, X_pos.shape[0], n_texts)","c6d46d40":"X_gen = np.array([modify_sentence(x, synonyms) for x in X_pos[indexes]])\ny_gen = np.ones(n_texts)","77c31345":"For visualization, I'm gonna need to see which index corresponds to which word","f0d3c634":"### Let us preview our function","91243539":"## Step 4 - Data Augmentation \/ Oversampling \n\n#### We work on 1 labelled texts. We apply the following algorithm to modify a sentence :\n\nFor each word in the sentence :\n* Keep it with probability $p$  (or if we don't have synonyms for it)\n* Randomly swap it with one of its synonyms with probability $1-p$","61551033":"## Step 3: Tokenizing\n\nI am using Keras' Tokenizer to apply some text processing and to limit the size of the vocabulary","44cc57be":"## Step 1 : Loading Word Embeddings","f2db7a69":"\n \n #### The next work to do is to find a good value for $p$, and a correct number of samples to generate, and then feed it into a network.\n \n  ## Thanks for reading, hope it can be helpful to anyone !\n\n ","3bf93fc9":"### Class imbalance","4d479295":"### Checking our synonyms","01fb6198":" # Using Word Embeddings for Data Augmentation\n\n###  In this kernel, I'm going to show you a way to do data augmentation for texts, when you have word embeddings.\n\nI will focus on augmenting texts labelled as 1, as this class is under-represented. Oversampling can help improve perfomances.\n\n\n#### Feel free to give any feedback, it is always appreciated.\n\n##### References :\n\nInspired by this kernel :\n> https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings#  by SRK\n\nContinuation of :\n> https:\/\/www.kaggle.com\/theoviel\/dealing-with-class-imbalance-with-smote","7d2f8398":"## Step 3 : Making a Synonym Dictionary\n\nWord vectors are made in a way that similar words have similar representation. Therefore we can use the $k$-nearest neighbours to get $k$ synonyms.\n\nAs the process takes a bit of time, I chose to compute 5 synonyms for the 20000 most frequent words.","e0aeb36b":"#### Tokenizing","505727e4":"Looks pretty good, we now generate some texts","523267b2":"#### Looks pretty good ! ","73dd0ca3":"I'm using GloVe, because that's the embedding I got the best results with inbefore, but paragram is quite good as well.\n\n## Step 2 : Loading Data","397e6180":"#### We create Synonyms for the most frequent words","d747b473":"#### Embedding Matrix","fd0a3a4a":"There is way more 0s than 1s in our dataset. As mentionned, we could use data augmentation to balance classes. Therefore the prediction task will be easier.","286bfe75":"I also apply padding, mostly to store X as an array."}}