{"cell_type":{"ea8b1302":"code","e1086878":"code","1aba7b3f":"code","d66ea323":"code","7f0789a9":"code","da26a3b5":"code","1800eed5":"code","3349e63f":"code","e0602871":"code","10a1167a":"code","1fd5ab08":"markdown","6ab217e1":"markdown","43c67979":"markdown","15eb97c9":"markdown","fd34b478":"markdown","d14e5c00":"markdown","432641d2":"markdown","c675c54e":"markdown"},"source":{"ea8b1302":"ANNOTATIONS_DIR = '..\/input\/sartorius-cell-instance-segmentation-coco\/'\nIMG_BASE_DIR = \"..\/input\/sartorius-cell-instance-segmentation\"","e1086878":"from tqdm.auto import tqdm\nimport json\nimport numpy as np\nfrom scipy.fftpack import dct, idct\nfrom matplotlib.pyplot import figure\nimport matplotlib.pylab as plt\nimport cv2\n\ndef get_class(item, data):\n    image_id = item['file_name'].split('\/')[1].split('.')[0]\n    for annot in data['annotations']:\n        if annot['image_id']==image_id: return annot['category_id']\n    return None\n\ndef convert_images(img_path, item, category, images, classes, plot=False):\n    \n    np.set_printoptions(threshold=np.inf)\n    image = cv2.imread(img_path + '\/' + item['file_name'])\n    srcImageWidth, srcImageHeight = image.shape[1], image.shape[0]\n\n    im_width = 224\n    im_height = 224\n    \n    # Extracting 6 images per original image and its classes\n    for left in range(3):\n        for top in range(2):\n            images.append(image[int(top*im_height):int(top*im_height)+im_height, left*im_width:(left*im_width)+im_width, :]\/255)\n            classes.append(np.asarray(category))\n\n            if plot:\n                figure(figsize=(20, 20), dpi=80)\n                plt.gray()\n                plt.subplot(121), plt.imshow(images[len(images)-1]), plt.axis('off'), plt.title('original image', size=20)\n                plt.subplot(122), plt.imshow(image), plt.axis('off'), plt.title('original image', size=20)\n                plt.show()\n\n    return images, classes\n\ndef convert_dataset(json_filename):\n    print(f\"Converting {json_filename}\")\n\n    # Opening JSON file\n    with open(ANNOTATIONS_DIR + '\/' + json_filename) as json_file:\n        data = json.load(json_file , encoding='utf-8')\n\n    images = []\n    classes = []\n    for item in tqdm(data['images']):\n        category = get_class(item, data)\n        if category is not None:\n            images, classes = convert_images(IMG_BASE_DIR, item, category, images, classes, plot=False) \n        #break\n\n    return np.asarray(images), np.asarray(classes)\n\n# Extracting data for training and testing\nX_train, y_train = convert_dataset('annotations_train.json')\nX_test, y_test = convert_dataset('annotations_val.json')","1aba7b3f":"# Shuffling the training set\nfrom sklearn.utils import shuffle\nX_train, y_train = shuffle(X_train, y_train, random_state=111)\nX_train.shape, y_train.shape","d66ea323":"# Imports\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport inspect\nfrom tqdm import tqdm\n\n# Set batch size for training and validation\nbatch_size = 32","7f0789a9":"# Creating a list of pre-trained available models in keras\nmodel_dictionary = {m[0]:m[1] for m in inspect.getmembers(tf.keras.applications, inspect.isfunction)}","da26a3b5":"# Setting up some parameters\nnum_train = X_train.shape[0]\nnum_validation = X_test.shape[0]\nnum_classes = 3\nnum_iterations = int(num_train\/batch_size)\n\ndef get_joined_y(y_hot):\n    #TODO: Probably can be joined with one single line\n    y_joined = []\n    for i in range(0,y_hot.shape[0],6):\n        y_joined.append( np.argmax(np.sum(y_hot[i:i+6],axis=0)) )\n\n    return np.asarray(y_joined)\n\n# One hot enconding ys\ny_train_hot = np.zeros((y_train.shape[0], num_classes))\nfor i in range(y_train.shape[0]):\n    y_train_hot[i,:] = tf.one_hot(y_train[i]-1, depth=num_classes)\n\ny_test_hot = np.zeros((y_test.shape[0], num_classes))\nfor i in range(y_test.shape[0]):\n    y_test_hot[i,:] = tf.one_hot(y_test[i]-1, depth=num_classes)   \n    \n# Check one-hot encoding correctness\nfor i in range(y_train.shape[0]):\n    #print(y_test[i], y_test_hot[i])\n    assert(y_train_hot[i][y_train[i]-1]==1)\n    \nfor i in range(y_test.shape[0]):\n    #print(y_test[i], y_test_hot[i])\n    assert(y_test_hot[i][y_test[i]-1]==1)","1800eed5":"# Loop over each model available in Keras\nimport gc\ny_join = get_joined_y(y_test_hot)\n\nmodel_benchmarks = {'model_name': [], 'num_model_params': [], 'validation_accuracy': [], 'join_6_img_val_acc': []}\nfor model_name, model in tqdm(model_dictionary.items()):\n    print(f\"Evaluating on {model_name}\")\n    # Special handling for \"NASNetLarge\" since it requires input images with size (331,331)\n    if 'NASNetLarge' in model_name:\n        continue\n        #input_shape=(331,331,3)\n        #train_processed = train_processed_331\n        #validation_processed = validation_processed_331\n    #else:\n    #    input_shape=(224,224,3)\n    #    train_processed = train_processed_224\n    #    validation_processed = validation_processed_224\n\n    input_shape=(224,224,3)\n    # load the pre-trained model with global average pooling as the last layer and freeze the model weights\n    pre_trained_model = model(include_top=False, pooling='avg', input_shape=input_shape)\n    pre_trained_model.trainable = False\n\n    # custom modifications on top of pre-trained model\n    clf_model = tf.keras.models.Sequential()\n    clf_model.add(pre_trained_model)\n    clf_model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n    clf_model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n    history = clf_model.fit(X_train, y_train_hot, epochs=3, validation_data=(X_test, y_test_hot), steps_per_epoch=num_iterations)\n\n    predictions = clf_model.predict(X_test)\n    joined_predictions = get_joined_y(predictions)\n    \n    total_acc = joined_predictions[joined_predictions==y_join].shape[0] \/ joined_predictions.shape[0]\n    print(f\"Total Accuracy: {total_acc}\")\n\n    # Calculate all relevant metrics\n    model_benchmarks['model_name'].append(model_name)\n    model_benchmarks['num_model_params'].append(pre_trained_model.count_params())\n    model_benchmarks['validation_accuracy'].append(history.history['val_accuracy'][-1])\n    model_benchmarks['join_6_img_val_acc'].append(total_acc)\n\n    del pre_trained_model, clf_model\n    gc.collect()","3349e63f":"# Convert Results to DataFrame for easy viewing\nbenchmark_df = pd.DataFrame(model_benchmarks)\nbenchmark_df.sort_values('num_model_params', inplace=True) # sort in ascending order of num_model_params column\nbenchmark_df.to_csv('benchmark_df.csv', index=False) # write results to csv file\nprint(\"Sorting the benchmark by number parameters\")\nbenchmark_df","e0602871":"# Loop over each row and plot the num_model_params vs validation_accuracy\nmarkers=[\".\",\",\",\"o\",\"v\",\"^\",\"<\",\">\",\"1\",\"2\",\"3\",\"4\",\"8\",\"s\",\"p\",\"P\",\"*\",\"h\",\"H\",\"+\",\"x\",\"X\",\"D\",\"d\",\"|\",\"_\",4,5,6,7,8,9,10,11]\nplt.figure(figsize=(7,5))\nfor row in benchmark_df.itertuples():\n    plt.scatter(row.num_model_params, row.join_6_img_val_acc, label=row.model_name, marker=markers[row.Index], s=150, linewidths=2)\nplt.xscale('log')\nplt.xlabel('Number of Parameters in Model')\nplt.ylabel('Validation Accuracy after 3 Epochs')\nplt.title('Accuracy vs Model Size')\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left'); # Move legend out of the plot","10a1167a":"print(\"Printing benchmark by score\")\nbenchmark_df.sort_values('join_6_img_val_acc')","1fd5ab08":"#### Setting up the model evaluator","6ab217e1":"#### Setting up paths","43c67979":"#### Preparing the dataset. Getting 6 images of shape (224,224,3) from every image \n","15eb97c9":"#### Plotting the results","fd34b478":"#### Evaluating on each available model","d14e5c00":"## Evaluating the results","432641d2":"## Conclusion\n\nReading the results I have 2 main conclusions:\n- `MobileNetv2` seems a good candidate and `ResNet50V2` having 10x parameters relative to `MobileNetv2` can be a good candidate as well. Maybe an ensembling of the top models can be a good approach.\n- These results are obtained with no data augmentation more than extracting 6 images per sample and only trained in 3 epochs. Probably scores can be improved.\n- I haven't tested training with images without splitting into 6. Maybe it gets good or better results.\n\nHappy to know what you think!!","c675c54e":"# Evaluate models for splitting the problem into 3 different problems.\n\n## Disclaimer. A beginner here!\nAs a beginner, I'm not even sure I got the problem we are facing for sure, but for my understanding maybe splitting the problem into three different problems could make the things easier. It seems that masks are a bit different depending of the type of cells we find, then selecting the type of cells in the first place and then apply three different models to find the masks.\n\nIf I'm right, what I was trying to do is to pick the better and faster model to classify each image into the problem we want to solve and then apply the correct 2nd step model to extract the correct masks. To do that, I was wondering what model could fit better and what is the minimum necessary size (parameters\/speed) of the model is needed to solve this first problem. \n\n**These notebook can have a totally wrong approach or probably have errors. Please, feel free to correct me on anything that can be improved or the whole point!**\n\n## What you'll find in this notebook\n\nThis notebooks gets all the training images and trains (for 3 epochs) and evaluate on up to 26 different pre-trained models to find which one could fit better to solve what would be my first step of my final pipeline, splitting the test images to be predicted by three different models.\n\n## Starting from the end... references.\nAs a beginner I'm taking advantage of the knowledge of others who has a great experience and knowledge. Here, I'm just compiling and joining the work of others.\n\n### The dataset used:\n\nI'm using the great COCO ready annotations created by [@Slawek Biel](https:\/\/www.kaggle.com\/slawekbiel) and his great series of notebooks that I'm following to just understand how the whole thing works and being my guideline to enter to this competition:\n- [Positive score with Detectron 1\/3 - input data](https:\/\/www.kaggle.com\/slawekbiel\/positive-score-with-detectron-1-3-input-data\/)\n- [Positive score with Detectron 2\/3 - Training](https:\/\/www.kaggle.com\/slawekbiel\/positive-score-with-detectron-2-3-training\/)\n- [Positive score with Detectron 3\/3 - Inference](https:\/\/www.kaggle.com\/slawekbiel\/positive-score-with-detectron-3-3-inference)\n\nThe dataset I'm using is the on [@Slawek Biel](https:\/\/www.kaggle.com\/slawekbiel) created through his first notebook: [\"Sartorius - Cell Instance Segmentation\" COCO](https:\/\/www.kaggle.com\/slawekbiel\/sartorius-cell-instance-segmentation-coco)\n\n### Evaluation models\n\nI extracted (copied) the idea of evaluating  multiple models from a great blog post by [Mario Stephan Leo](https:\/\/towardsdatascience.com\/how-to-choose-the-best-keras-pre-trained-model-for-image-classification-b850ca4428d4) at Medium.\nYou can visit [his Github here](https:\/\/github.com\/stephenleo\/keras-model-selection) \nFor further details on the evaluation models explanation, take a look at the Mario's blog post.\n\n\n## An image classification problem\n\nIn order to split the problem into 3 different problems, It is needed to classify images into different categories. This is a image classification problem. We need to solve for every image which category it contains.\n\n### Input images\n\nFor simplicity, I selected the pre-trained models that work with shape `(224, 224, 3)`. Instead of resizing the images of this competition `(702, 540)`, I decided to split images into chunks of the needed size and then having 6x images for training and 6 images (per image) for predicting and then summing up the prediction scores  for each chunk to obtain the final prediction for the whole image. Probably there're better ways to join predictions than just summing them.\n"}}