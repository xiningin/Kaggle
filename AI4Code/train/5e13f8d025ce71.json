{"cell_type":{"2013fd0b":"code","326ce338":"code","e4776e8e":"code","a39bdd2b":"code","8e749783":"code","7a4e2b4e":"code","8bb74ce0":"code","db245c95":"code","55bf92ec":"code","db390168":"code","472a2cc2":"code","2eb77009":"code","fb7e61c4":"code","8a7f183c":"code","c2f2ba72":"code","ac593855":"code","8b106c02":"code","dad76e3b":"code","4fb5bac5":"code","2071a570":"code","f35690f5":"code","00719ee7":"code","278baade":"code","ae6823df":"code","5759fadf":"code","240897d8":"code","8c4ea1cd":"code","719d7a6a":"code","3b3ffc52":"code","43c23948":"code","b2659163":"code","386ae0b1":"code","04c82f76":"code","412e9a75":"code","9f180a92":"code","a24efdf5":"code","f4ed5960":"code","e783212c":"code","5b31b84e":"code","76bee191":"code","9063cac1":"code","cb91ce88":"code","48d02823":"code","d26f30e6":"code","ee3895a5":"code","02c302d6":"code","07b88950":"code","8137aace":"code","7c42f2bf":"code","9fb82143":"code","b0b997b2":"code","b245663f":"code","8f0dab5c":"code","9bc64f3c":"code","ac0f1233":"code","ba294e08":"code","c911d858":"code","40ab4b4f":"code","a6414d3e":"code","62a1bf20":"code","2b374a1c":"code","93b94100":"code","18ebe255":"code","b8ddaa57":"code","02769ca6":"code","44551205":"code","aa04873b":"code","44f97b2b":"code","651b9ac5":"code","b925567f":"code","0d4c147c":"code","40c1d42e":"code","ab1e8156":"code","b26fffac":"code","d6b2d29c":"code","ffb8d83c":"code","c548c505":"code","ffebd9a4":"code","b16f3ade":"code","f52aa376":"code","f8070602":"markdown","556bb057":"markdown","8edf9819":"markdown","46d7955c":"markdown","368b3313":"markdown","7bb90bf5":"markdown","64897186":"markdown","704a6c1c":"markdown","2c14d1d7":"markdown","184c28a4":"markdown","85f14ed5":"markdown","c7cdcf5b":"markdown","1255d184":"markdown","8b444011":"markdown","f431e00d":"markdown","75e2dccf":"markdown","88383f05":"markdown","dc39f396":"markdown","aad3133e":"markdown","c9cfc7b7":"markdown","5ecad84f":"markdown","939a6d07":"markdown","5e0453c7":"markdown","e087d0f0":"markdown","c4ab045f":"markdown","d0b411f0":"markdown","2f0deebf":"markdown","e1f57188":"markdown","73e0fd74":"markdown","1c55a631":"markdown","597d3cff":"markdown","d378c6a2":"markdown","6fcef947":"markdown","d6153f30":"markdown","dfdbe669":"markdown","5a508058":"markdown","dd9fb6e8":"markdown"},"source":{"2013fd0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","326ce338":"raw_data = pd.read_csv(\"\/kaggle\/input\/lending-club-loan-data\/loan.csv\",low_memory=False)\nraw_data.head()","e4776e8e":"preprocess_df = raw_data[['emp_length', 'loan_status', 'home_ownership', 'issue_d',\n                          'earliest_cr_line', 'purpose', 'term', 'annual_inc', 'dti',\n                          'loan_amnt', 'int_rate', 'pub_rec_bankruptcies',]].copy()","a39bdd2b":"preprocess_df = raw_data[['emp_length', 'loan_status', 'home_ownership', 'issue_d',\n                          'earliest_cr_line', 'purpose', 'term', 'annual_inc', 'dti',\n                          'loan_amnt', 'int_rate', 'pub_rec_bankruptcies',]].copy()\n# Create a list of columns that are NOT numeric values\nnot_numeric_cols = ['emp_length', 'loan_status', 'home_ownership', 'issue_d',\n                    'earliest_cr_line', 'purpose', 'term']\n\n# Create list of columns that ARE numeric values and print\nnumeric_cols = [col for col in preprocess_df.columns if col not in not_numeric_cols]\nprint(numeric_cols)\n\n# Convert numeric cols into numeric data types\npreprocess_df[numeric_cols] = preprocess_df[numeric_cols].apply(pd.to_numeric)","8e749783":"# Create list of datetime columns\ndatetime_cols = ['earliest_cr_line', 'issue_d']\n\n# Convert to datetime\npreprocess_df[datetime_cols] = preprocess_df[datetime_cols].apply(pd.to_datetime)","7a4e2b4e":"preprocess_df.isnull().sum()","8bb74ce0":"character_df = preprocess_df[['pub_rec_bankruptcies', 'earliest_cr_line', 'issue_d']].copy()","db245c95":"print(character_df.head())\nprint(character_df.describe())\nprint(character_df.dtypes)","55bf92ec":"# fill the missing value for earliest_cr_line with most frequently occuring\ncharacter_df['earliest_cr_line'].fillna(character_df['earliest_cr_line'].value_counts().index[0], inplace=True)","db390168":"# count months between now and 'earliest_cr_line'\ncharacter_df['credit_hist_in_months'] = ((character_df['issue_d'] - character_df['earliest_cr_line'])\/np.timedelta64(1, 'M'))\ncharacter_df['credit_hist_in_months'] = character_df['credit_hist_in_months'].astype(int)\n\ncharacter_df.head()","472a2cc2":"# Create a new binary feature of whether or not there is a bankruptcy on file in customers credit history\ncharacter_df['cb_person_bk_on_file_Y'] = character_df['pub_rec_bankruptcies'].apply(lambda x: 1 if x >= 1 else 0)\ncharacter_df.head()","2eb77009":"# drop the old features from the character_df\ncharacter_df.drop(['pub_rec_bankruptcies', 'earliest_cr_line', 'issue_d'], axis=1, inplace=True)\ncharacter_df.head()","fb7e61c4":"capacity_df = preprocess_df[['annual_inc', 'dti']].copy()\ncapacity_df.head()","8a7f183c":"# fill missing values for annual income with the mean\ncapacity_df['annual_inc'] = capacity_df['annual_inc'].fillna(capacity_df['annual_inc'].mean())\n\n# fill missing values for dti with the mean\ncapacity_df['dti'] = capacity_df['dti'].fillna(capacity_df['dti'].mean())","c2f2ba72":"# describe the capacity\/cash flow proxy features\nprint(capacity_df[['annual_inc', 'dti']].describe())","ac593855":"conditions_df = preprocess_df[['loan_amnt', 'int_rate', 'term']].copy()","8b106c02":"conditions_df.term.value_counts()","dad76e3b":"# Convert values of term to 0, 1 where 0 = 36 months and 1 = 60 months\nconditions_df['term'] = conditions_df['term'].replace({' 36 months': '0',\n                                                       ' 60 months': '1'})\n\n# convert term into an integer data type \nconditions_df['term'] = conditions_df['term'].astype(int)\n\n# Rename term column\nconditions_df = conditions_df.rename(columns={'term': 'term_60'})\n\nprint(conditions_df.term_60.value_counts())\nconditions_df.head()","4fb5bac5":"collateral_df = preprocess_df[['home_ownership']].copy()\ncollateral_df.head()","2071a570":"# create emp_length dummy data frame\nhome_ownership = pd.DataFrame(pd.get_dummies(collateral_df['home_ownership'], prefix='home_ownership'))\n\n# join the loan_amnt dummy dataframe to conditions_df\ncollateral_df = pd.concat([collateral_df, home_ownership], axis=1, sort=False)\n\n# drop original emp_length feature\ncollateral_df.drop(['home_ownership'], axis=1, inplace=True)\n\ncollateral_df.head()","f35690f5":"loan_status_df = preprocess_df[['loan_status']].copy()\nloan_status_df.loan_status.value_counts()","00719ee7":"loan_status_df['loan_status'] = loan_status_df['loan_status'].replace({'Fully Paid': 0, 'Current': 0, 'Charged Off': 1,\n                                                                       'Late (31-120 days)': 1, 'In Grace Period': 1,\n                                                                       'Late (16-30 days)':1,\n                                                                       'Does not meet the credit policy. Status:Fully Paid': 0,\n                                                                       'Does not meet the credit policy. Status:Charged Off': 1,\n                                                                       'Default':1})\nloan_status_df.loan_status.value_counts()","278baade":"# Concatenate all the processed dataframes into a single one\nprocessed_df = pd.concat([character_df,\n                          capacity_df,\n                          conditions_df,\n                          collateral_df,\n                          loan_status_df,\n                          raw_data['grade']], axis=1, sort=False)\nprocessed_df.head()","ae6823df":"print(processed_df.loan_status.value_counts())","5759fadf":"ax = processed_df.loan_status.value_counts().plot(kind='bar')\nlabels = ['Non-Default', 'Default']\nax.set_xticklabels(labels, rotation='horizontal')\nax.set_ylabel('Number of Loans')","240897d8":"grade_default = pd.crosstab(processed_df['grade'], processed_df['loan_status'])\n\nfig, ax = plt.subplots()\n\ngrade_default.plot.bar(legend=True, alpha=0.7, ax=ax)\nplt.title(\"Non-Default vs Default by Grade\")\nax.legend([\"Non-Default\", \"Default\"])","8c4ea1cd":"# Sample figsize in inches\nfig, ax = plt.subplots(figsize=(20,10))\n\n# Imbalanced DataFrame Correlation\ncorr = processed_df.corr()\nsns.heatmap(corr, cmap='YlGnBu', annot_kws={'size':30}, ax=ax)\nax.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)\nplt.show()","719d7a6a":"# Shuffle the Dataset.\nrandom_data = processed_df.sample(frac=1,random_state=4)\n\n# Put all the fraud class in a separate dataset.\ndefault_df = random_data.loc[random_data['loan_status'] == 1]\n\n#Randomly select 297033 observations from the non-fraud (majority class)\nnon_default_df = random_data.loc[random_data['loan_status'] == 0].sample(n=297033, random_state=42)","3b3ffc52":"# Concatenate both dataframes again\nUS_df = pd.concat([default_df, non_default_df])\n\n#plot the dataset after the undersampling\nplt.figure(figsize=(8, 8))\nsns.countplot('loan_status', data=US_df)\nplt.title('Undersampled Balanced Loan Status')\nplt.xticks(ticks=(0,1), labels=('Non-Default', 'Default'))\nplt.show()","43c23948":"# Create X and y using undersampled dataframe\nX = US_df.drop(['loan_status', 'grade'], axis=1)\ny = US_df['loan_status']\n\nX_train_US, X_test_US, y_train_US, y_test_US = train_test_split(X, y, test_size=.4, random_state=123)","b2659163":"# Create, train, and fit a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclf_logistic = LogisticRegression(solver='lbfgs').fit(X_train_US, np.ravel(y_train_US))\n\n# Create predictions of probability for loan status using test data\n# .predict_proba creates an array of probabilities of default: [[non-defualt, default]]\nlr_preds = clf_logistic.predict_proba(X_test_US)\n\n# # Create dataframes of predictions and true labels\nlr_preds_df = pd.DataFrame(lr_preds[:,1][0:], columns = ['lr_pred_PD'])\ntrue_df = y_test_US\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), lr_preds_df], axis = 1))","386ae0b1":"# Reassign loan status based on the threshold and print the predictions\nlr_preds_df['lr_pred_loan_status_60'] = lr_preds_df['lr_pred_PD'].apply(lambda x: 1 if x > 0.60 else 0)\nprint(\"Non-Default \/ Default predictions at 60% Threshhold: \")\nprint(lr_preds_df['lr_pred_loan_status_60'].value_counts())\n\n# Print the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix at 60% Threshhold: \")\nprint(confusion_matrix(y_test_US, lr_preds_df['lr_pred_loan_status_60']))\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test_US, lr_preds_df['lr_pred_loan_status_60'], target_names=target_names))","04c82f76":"# Reassign loan status based on the threshold and print the predictions\nlr_preds_df['lr_pred_loan_status_50'] = lr_preds_df['lr_pred_PD'].apply(lambda x: 1 if x > 0.50 else 0)\nprint(\"Non-Default \/ Default t predictions at 50% Threshhold: \")\nprint(lr_preds_df['lr_pred_loan_status_50'].value_counts())\n\n# Print the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix at 50% Threshhold: \")\nprint(confusion_matrix(y_test_US, lr_preds_df['lr_pred_loan_status_50']))\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test_US, lr_preds_df['lr_pred_loan_status_50'], target_names=target_names))","412e9a75":"# Print the accuracy score the model\nprint(clf_logistic.score(X_test_US, y_test_US))\n\n# Plot the ROC curve of the probabilities of default\nfrom sklearn.metrics import roc_curve\n\nlr_prob_default = lr_preds[:, 1]\nfallout, sensitivity, thresholds = roc_curve(y_test_US, lr_prob_default)\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title(\"ROC Chart for LR on PD\")\nplt.xlabel(\"Fall-out\")\nplt.ylabel(\"Sensitivity\")\nplt.show()\n\n# Compute the AUC and store it in a variable\nfrom sklearn.metrics import roc_auc_score\n\nlr_auc = roc_auc_score(y_test_US, lr_prob_default)","9f180a92":"# Create X and y using processed_df\n\nX = processed_df.drop(['loan_status', 'grade'], axis=1)\ny = processed_df['loan_status']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)","a24efdf5":"from imblearn.over_sampling import SMOTE\n\n# Resample the minority class. You can change the strategy to 'auto' if you are not sure.\nsm = SMOTE(sampling_strategy='minority', random_state=7)\n\n# Fit the model to generate the data.\noversampled_trainX, oversampled_trainY = sm.fit_sample(X_train, y_train)\noversampled_train = pd.concat([pd.DataFrame(oversampled_trainY), pd.DataFrame(oversampled_trainX)], axis=1)\noversampled_train.columns = processed_df.drop(['grade'], axis=1).columns","f4ed5960":"# Sample figsize in inches\nfig, ax = plt.subplots(figsize=(20,10))\n\n# Imbalanced DataFrame Correlation\ncorr = oversampled_train.corr()\nsns.heatmap(corr, cmap='YlGnBu', annot_kws={'size':30}, ax=ax)\nax.set_title(\"Imbalanced Correlation Matrix after oversampling\", fontsize=14)\nplt.show()","e783212c":"# Create, train, and fit a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclf_logistic = LogisticRegression(solver='lbfgs').fit(oversampled_trainX, np.ravel(oversampled_trainY))\n\n# Create predictions of probability for loan status using test data\n# .predict_proba creates an array of probabilities of default: [[non-defualt, default]]\nlr_preds = clf_logistic.predict_proba(X_test)\n\n# # Create dataframes of predictions and true labels\nlr_preds_df = pd.DataFrame(lr_preds[:,1][0:], columns = ['lr_pred_PD'])\ntrue_df = y_test\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), lr_preds_df], axis = 1))","5b31b84e":"# Reassign loan status based on the threshold and print the predictions\nlr_preds_df['lr_pred_loan_status_60'] = lr_preds_df['lr_pred_PD'].apply(lambda x: 1 if x > 0.60 else 0)\nprint(\"Non-Default \/ Default predictions at 60% Threshhold: \")\nprint(lr_preds_df['lr_pred_loan_status_60'].value_counts())\n\n# Print the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix at 60% Threshhold: \")\nprint(confusion_matrix(y_test, lr_preds_df['lr_pred_loan_status_60']))\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, lr_preds_df['lr_pred_loan_status_60'], target_names=target_names))","76bee191":"# Reassign loan status based on the threshold and print the predictions\nlr_preds_df['lr_pred_loan_status_50'] = lr_preds_df['lr_pred_PD'].apply(lambda x: 1 if x > 0.50 else 0)\nprint(\"Non-Default \/ Default t predictions at 50% Threshhold: \")\nprint(lr_preds_df['lr_pred_loan_status_50'].value_counts())\n\n# Print the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix at 50% Threshhold: \")\nprint(confusion_matrix(y_test, lr_preds_df['lr_pred_loan_status_50']))\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, lr_preds_df['lr_pred_loan_status_50'], target_names=target_names))","9063cac1":"# Print the accuracy score the model\nprint(clf_logistic.score(X_test, y_test))\n\n# Plot the ROC curve of the probabilities of default\nfrom sklearn.metrics import roc_curve\n\nlr_prob_default = lr_preds[:, 1]\nfallout, sensitivity, thresholds = roc_curve(y_test, lr_prob_default)\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title(\"ROC Chart for LR on PD\")\nplt.xlabel(\"Fall-out\")\nplt.ylabel(\"Sensitivity\")\nplt.show()\n\n# Compute the AUC and store it in a variable\nfrom sklearn.metrics import roc_auc_score\n\nlr_auc = roc_auc_score(y_test, lr_prob_default)","cb91ce88":"from imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create an object of the classifier and fit oversampled training data to the object\nbbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0).fit(oversampled_trainX, np.ravel(oversampled_trainY))\n\n# Create predictions of probability for loan status using test data\nbbc_preds = bbc.predict_proba(X_test)","48d02823":"# Create dataframes of predictions and true labels\nbbc_preds_df = pd.DataFrame(bbc_preds[:,1][0:], columns = ['bbc_pred_PD'])\ntrue_df = y_test\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), bbc_preds_df], axis = 1))","d26f30e6":"# Reassign loan status based on the threshold and print the predictions\nbbc_preds_df['bbc_pred_loan_status_60'] = bbc_preds_df['bbc_pred_PD'].apply(lambda x: 1 if x > 0.60 else 0)\nprint(\"Non-Default \/ Default predictions at 60% Threshhold: \")\nprint(bbc_preds_df['bbc_pred_loan_status_60'].value_counts())\n\n# Print the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix at 60% Threshhold: \")\nprint(confusion_matrix(y_test, bbc_preds_df['bbc_pred_loan_status_60']))\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, bbc_preds_df['bbc_pred_loan_status_60'], target_names=target_names))\n","ee3895a5":"# Reassign loan status based on the threshold and print the predictions\nbbc_preds_df['bbc_pred_loan_status_50'] = bbc_preds_df['bbc_pred_PD'].apply(lambda x: 1 if x > 0.50 else 0)\nprint(\"Non-Default \/ Default predictions at 50% Threshhold: \")\nprint(bbc_preds_df['bbc_pred_loan_status_50'].value_counts())\n\n# Print the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix at 50% Threshhold: \")\nprint(confusion_matrix(y_test, bbc_preds_df['bbc_pred_loan_status_50']))\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, bbc_preds_df['bbc_pred_loan_status_50'], target_names=target_names))\n","02c302d6":"# Print the accuracy score the model\nprint(bbc.score(X_test, y_test))\n\n# Plot the ROC curve of the probabilities of default\nfrom sklearn.metrics import roc_curve\n\nbbc_prob_default = bbc_preds[:, 1]\nfallout, sensitivity, thresholds = roc_curve(y_test, bbc_prob_default)\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title(\"ROC Chart for BBC on PD\")\nplt.xlabel(\"Fall-out\")\nplt.ylabel(\"Sensitivity\")\nplt.show()\n\n# Compute the AUC and store it in a variable\nfrom sklearn.metrics import roc_auc_score\n\nbbc_auc = roc_auc_score(y_test, bbc_prob_default)","07b88950":"# Train a model\nimport xgboost as xgb\nclf_gbt = xgb.XGBClassifier().fit(oversampled_trainX, np.ravel(oversampled_trainY))\n\n# Predict with a model\n# .predict_proba creates an array of probabilities of default: [[non-default, default]]\ngbt_preds = clf_gbt.predict_proba(X_test)\n\n# Create dataframes of first five predictions, and first five true labels\ngbt_preds_df = pd.DataFrame(gbt_preds[:,1][0:], columns = ['gbt_pred_PD'])\ntrue_df = y_test\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), gbt_preds_df], axis = 1))","8137aace":"# Reassign loan status based on the threshold and print the predictions\ngbt_preds_df['gbt_pred_loan_status_60'] = gbt_preds_df['gbt_pred_PD'].apply(lambda x: 1 if x > 0.60 else 0)\nprint(\"Non-Default \/ Default  predictions at 60% Threshhold: \")\nprint(gbt_preds_df['gbt_pred_loan_status_60'].value_counts())\n\n# Print the confusion matrix\nprint(\"Confusion Matrix at 60% Threshhold: \")\nprint(confusion_matrix(y_test, gbt_preds_df['gbt_pred_loan_status_60']))\n\n# Print the classification report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, gbt_preds_df['gbt_pred_loan_status_60'], target_names=target_names))","7c42f2bf":"# Reassign loan status based on the threshold and print the predictions\ngbt_preds_df['gbt_pred_loan_status_50'] = gbt_preds_df['gbt_pred_PD'].apply(lambda x: 1 if x > 0.50 else 0)\nprint(\"Non-Default \/ Default predictions at 50% Threshhold: \")\nprint(gbt_preds_df['gbt_pred_loan_status_50'].value_counts())\n\n# Print the confusion matrix\nprint(\"Confusion Matrix at 50% Threshhold: \")\nprint(confusion_matrix(y_test, gbt_preds_df['gbt_pred_loan_status_50']))\n\n# Print the classification report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, gbt_preds_df['gbt_pred_loan_status_50'], target_names=target_names))","9fb82143":"# Print the accuracy score the model\nprint(clf_gbt.score(X_test, y_test))\n\n# Plot the ROC curve of the probabilities of default\nfrom sklearn.metrics import roc_curve\n\nxgb_prob_default = gbt_preds[:, 1]\nfallout, sensitivity, thresholds = roc_curve(y_test, xgb_prob_default)\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title(\"ROC Chart for XGB on PD\")\nplt.xlabel(\"Fall-out\")\nplt.ylabel(\"Sensitivity\")\nplt.show()\n\n# Compute the AUC and store it in a variable\nfrom sklearn.metrics import roc_auc_score\n\nxgb_auc = roc_auc_score(y_test, xgb_prob_default)\n","b0b997b2":"# Creating a portfolio datafram\nportfolio_5c = pd.DataFrame(gbt_preds[:,1][0:], columns = ['gbt_prob_default'])\nportfolio_5c.index = X_test.index\nportfolio_5c['bbc_prob_default'] = bbc_preds[:,1][0:]\nportfolio_5c['lr_prob_default'] = lr_preds[:,1][0:]\nportfolio_5c['lgd'] = 1 # Assumes that given a default, entire loan is a loss with no recoverable amounts\nportfolio_5c['loan_amnt'] = X_test.loan_amnt\nportfolio_5c.head()","b245663f":"# Create expected loss columns for each model using the formula\nportfolio_5c['gbt_expected_loss'] = portfolio_5c['gbt_prob_default'] * portfolio_5c['lgd'] * portfolio_5c['loan_amnt']\nportfolio_5c['bbc_expected_loss'] = portfolio_5c['bbc_prob_default'] * portfolio_5c['lgd'] * portfolio_5c['loan_amnt']\nportfolio_5c['lr_expected_loss'] = portfolio_5c['lr_prob_default'] * portfolio_5c['lgd'] * portfolio_5c['loan_amnt']\n\n# Print the total portfolio size\nprint('Portfolio size: $' + \"{:,.2f}\".format(np.sum(portfolio_5c['loan_amnt'])))\n\n# Print the sum of the expected loss for bbc\nprint('BBC expected loss: $' + \"{:,.2f}\".format(np.sum(portfolio_5c['bbc_expected_loss'])))\n\n# Print the sum of the expected loss for gbt\nprint('GBT expected loss: $' + \"{:,.2f}\".format(np.sum(portfolio_5c['gbt_expected_loss'])))\n\n# Print the sum of the expected loss for lr \nprint('LR expected loss: $' + \"{:,.2f}\".format(np.sum(portfolio_5c['lr_expected_loss'])))\n\n# Print portfolio first five rows\nportfolio_5c.head()","8f0dab5c":"preprocess_TTD_df = raw_data[['emp_length', 'chargeoff_within_12_mths',\n                              'mths_since_last_delinq', 'mths_since_last_record',\n                              'mths_since_last_major_derog', 'mths_since_recent_revol_delinq']].copy()","9bc64f3c":"preprocess_TTD_df.head()","ac0f1233":"preprocess_TTD_df.isnull().sum()","ba294e08":"preprocess_TTD_df.emp_length.value_counts()","c911d858":"# fill in missing values with the most frequently occuring employment length\npreprocess_TTD_df['emp_length'].fillna(preprocess_TTD_df['emp_length'].value_counts().index[0], inplace=True)\n\n# create a dataframe of employment length for customers in dataset\nemp_length_df = pd.DataFrame(pd.get_dummies(preprocess_TTD_df.emp_length, prefix='emp_length'))\nemp_length_df.head()\n\n# concat the emp_length dataframe with preprocess_TTD_df, dropping original columns\npreprocess_TTD_df = pd.concat([preprocess_TTD_df, emp_length_df], axis=1, sort=False)\npreprocess_TTD_df.drop(['emp_length'], axis=1, inplace=True)\npreprocess_TTD_df.head()","40ab4b4f":"# fill in missing values with the most frequently occuring chargeoff\npreprocess_TTD_df['chargeoff_within_12_mths'].fillna(preprocess_TTD_df['chargeoff_within_12_mths'].value_counts().index[0], inplace=True)","a6414d3e":"preprocess_TTD_df.mths_since_last_delinq\nprint(preprocess_TTD_df.mths_since_last_delinq.describe())","62a1bf20":"preprocess_TTD_df.mths_since_last_record \nprint(preprocess_TTD_df.mths_since_last_record .describe())","2b374a1c":"preprocess_TTD_df.mths_since_last_major_derog\nprint(preprocess_TTD_df.mths_since_last_major_derog.describe())","93b94100":"preprocess_TTD_df.mths_since_recent_revol_delinq\nprint(preprocess_TTD_df.mths_since_recent_revol_delinq.describe())","18ebe255":"preprocess_TTD_df.fillna(0, inplace=True)","b8ddaa57":"preprocess_TTD_df","02769ca6":"preprocess_TTD_df.isnull().sum()","44551205":"process_TTD_df = preprocess_TTD_df.copy()\nprocess_TTD_df","aa04873b":"# create a dataframe of employment length for customers in dataset\ngrade_df = pd.DataFrame(pd.get_dummies(processed_df.grade, prefix='grade'))\ngrade_df.head()\n\n# concat the emp_length dataframe with preprocess_TTD_df, dropping original columns\nprocessed_df = pd.concat([processed_df, grade_df], axis=1, sort=False)\nprocessed_df.drop(['grade'], axis=1, inplace=True)\nprocessed_df.head()","44f97b2b":"final_processed = processed_df.join(process_TTD_df)","651b9ac5":"final_processed.columns","b925567f":"# Go to settings tab and turn internet on to downloand lifelines and instantiate KMF \n\n!pip install lifelines\nfrom lifelines import KaplanMeierFitter","0d4c147c":"raw_data.grade.value_counts()","40c1d42e":"kmf1 = KaplanMeierFitter() ## instantiate the class to create an object\n\ntenure = final_processed['credit_hist_in_months']\nevent = final_processed['loan_status']","ab1e8156":"## Two Cohorts are compared. Cohort 1: Not Grade A Customers, and Cohort  2: Grade A CUstomers\ngrade_A = final_processed['grade_A']    \nnot_grade_A = (grade_A == 0)      ## Cohort Not Grade A Customers, having the pandas series  for the 1st cohort\nyes_grade_A = (grade_A == 1)     ## Cohort Grade A Customers, having the pandas series  for the 2nd cohort\n\n\n## fit the model for 1st cohort\nkmf1.fit(tenure[not_grade_A], event[not_grade_A], label='Not Grade A')\na1 = kmf1.plot()\n\n## fit the model for 2nd cohort\nkmf1.fit(tenure[yes_grade_A], event[yes_grade_A], label='Grade A')\nkmf1.plot(ax=a1)\n# a1.set_xlim(0, 100)","b26fffac":"type(a1)","d6b2d29c":"## Two Cohorts are compared. Cohort 1: Not Grade B Customers, and Cohort  2: Grade B CUstomers\ngrade_B = final_processed['grade_B']    \nnot_grade_B = (grade_B == 0)      ## Cohort Not Grade B Customers, having the pandas series  for the 1st cohort\nyes_grade_B = (grade_B == 1)     ## Cohort Grade B Customers, having the pandas series  for the 2nd cohort\n\n\n## fit the model for 1st cohort\nkmf1.fit(tenure[not_grade_B], event[not_grade_B], label='Not Grade B')\na1 = kmf1.plot()\n\n## fit the model for 2nd cohort\nkmf1.fit(tenure[yes_grade_B], event[yes_grade_B], label='Grade B')\nkmf1.plot(ax=a1)","ffb8d83c":"## Two Cohorts are compared. Cohort 1: Not Grade C Customers, and Cohort  2: Grade C CUstomers\ngrade_C = final_processed['grade_C']    \nnot_grade_C = (grade_C == 0)      ## Cohort Not Grade C Customers, having the pandas series  for the 1st cohort\nyes_grade_C = (grade_C == 1)     ## Cohort Grade C Customers, having the pandas series  for the 2nd cohort\n\n\n## fit the model for 1st cohort\nkmf1.fit(tenure[not_grade_C], event[not_grade_C], label='Not Grade C')\na1 = kmf1.plot()\n\n## fit the model for 2nd cohort\nkmf1.fit(tenure[yes_grade_C], event[yes_grade_C], label='Grade C')\nkmf1.plot(ax=a1)","c548c505":"## Two Cohorts are compared. Cohort 1: Not Grade D Customers, and Cohort  2: Grade D CUstomers\ngrade_D = final_processed['grade_D']    \nnot_grade_D = (grade_D == 0)      ## Cohort Not Grade D Customers, having the pandas series  for the 1st cohort\nyes_grade_D = (grade_D == 1)     ## Cohort Grade D Customers, having the pandas series  for the 2nd cohort\n\n\n## fit the model for 1st cohort\nkmf1.fit(tenure[not_grade_D], event[not_grade_D], label='Not Grade D')\na1 = kmf1.plot()\n\n## fit the model for 2nd cohort\nkmf1.fit(tenure[yes_grade_D], event[yes_grade_D], label='Grade D')\nkmf1.plot(ax=a1)","ffebd9a4":"## Two Cohorts are compared. Cohort 1: Not Grade E Customers, and Cohort  2: Grade E CUstomers\ngrade_E = final_processed['grade_E']    \nnot_grade_E = (grade_E == 0)      ## Cohort Not Grade E Customers, having the pandas series  for the 1st cohort\nyes_grade_E = (grade_E == 1)     ## Cohort Grade E Customers, having the pandas series  for the 2nd cohort\n\n\n## fit the model for 1st cohort\nkmf1.fit(tenure[not_grade_E], event[not_grade_E], label='Not Grade D')\na1 = kmf1.plot()\n\n## fit the model for 2nd cohort\nkmf1.fit(tenure[yes_grade_E], event[yes_grade_E], label='Grade D')\nkmf1.plot(ax=a1)","b16f3ade":"## Two Cohorts are compared. Cohort 1: Not Grade F Customers, and Cohort  2: Grade F CUstomers\ngrade_F = final_processed['grade_F']    \nnot_grade_F = (grade_F == 0)      ## Cohort Not Grade F Customers, having the pandas series  for the 1st cohort\nyes_grade_F = (grade_F == 1)     ## Cohort Grade F Customers, having the pandas series  for the 2nd cohort\n\n\n## fit the model for 1st cohort\nkmf1.fit(tenure[not_grade_F], event[not_grade_F], label='Not Grade F')\na1 = kmf1.plot()\n\n## fit the model for 2nd cohort\nkmf1.fit(tenure[yes_grade_F], event[yes_grade_F], label='Grade F')\nkmf1.plot(ax=a1)","f52aa376":"## Two Cohorts are compared. Cohort 1: Not Grade G Customers, and Cohort  2: Grade G CUstomers\ngrade_G = final_processed['grade_G']    \nnot_grade_G = (grade_G == 0)      ## Cohort Not Grade G Customers, having the pandas series  for the 1st cohort\nyes_grade_G = (grade_G == 1)     ## Cohort Grade G Customers, having the pandas series  for the 2nd cohort\n\n\n## fit the model for 1st cohort\nkmf1.fit(tenure[not_grade_G], event[not_grade_G], label='Not Grade G')\na1 = kmf1.plot()\n\n## fit the model for 2nd cohort\nkmf1.fit(tenure[yes_grade_G], event[yes_grade_G], label='Grade G')\nkmf1.plot(ax=a1)","f8070602":"### Defaults by Grade\n\nLoans rated as \"C\" show the highest instances of defaults","556bb057":"# Plotting AUC for Logistic Regression Classification with Undersampled Data","8edf9819":"# Creating, Training, and Fitting a GBT Model to Oversampled Data","46d7955c":"## Time to Default (or negative credit event) for Grade B rated customers","368b3313":"# Collateral: home_ownership\nWhat it is: Assets that are used to guarantee or secure a loan.\n\nWhy it matters: Collateral is a backup source if the borrower cannot repay a loan.","7bb90bf5":"# Loan Status\n\nCurrent Status of the Loan. Target variable","64897186":"## Time to Default (or negative credit event) for Grade D rated customers","704a6c1c":"## Time to Default (or negative credit event) for Grade E rated customers","2c14d1d7":"# Creating, Training, and Fitting a Balanced Bagging Classfier Model to Oversampled Data","184c28a4":"# Correlation in Credit Features AFTER oversampling","85f14ed5":"# mths_since_last_delinq, mths_since_last_record, mths_since_last_major_derog, mths_since_recent_revol_delinq\n\nLC Data Dictionary defines \n\n\n* <b>mths_since_last_delinq:<\/b> The number of months since the borrower's last delinquency.\n* <b>mthsSinceLastRecord:<\/b> The number of months since the last public record.\n* <b>mths_since_last_major_derog:<\/b> Months since most recent 90-day or worse rating\n* <b>mths_since_recent_revol_delinq:<\/b> Months since most recent revolving delinquency\n\n\nImportant to note that current consumer credit practices allows for delinquencies to stay on a consumer credit report for 7 years, so while information maybe missing, it does not mean that a delinquency has not occured.\n\nWith no addtional data given on these features, we will assume that delinquencies, records, major derogatory remarks, and months since last revolving delinquencies have since fallen off the consumer credit reports and can no longer be factored into consideration. \n\nWill fill the  missing these features with 0 months","c7cdcf5b":"## Time to Default (or negative credit event) for Grade A rated customers","1255d184":"#  Load Libraries","8b444011":"# Correlation in Credit Features","f431e00d":"# Capital\nWhat it is: The amount of money invested by the business owner or management team. It is required during periods of weak cash flow generation for an obligor to sustain itself and meeet obligations. If there isn't sufficient cash flow to meet obligations, then assets may need to be sold to meet obligations.\n\nWhy it matters: Banks are more willing to lend to owners who have invested some of their own money into the venture. It shows you have some \u201cskin in the game.\u201d\n\nHow it\u2019s assessed: From the amount of money the borrower or management team has invested in the business.","75e2dccf":"# <i>NO PROXY FEATURES USED FOR CAPITAL<i>","88383f05":"# Oversampling\n\nThe process of generating synthetic data that tries to randomly generate a sample of the attributes from observations in the minority class. There are a number of methods used to oversample a dataset for a typical classification problem. The most common technique is called SMOTE (Synthetic Minority Over-sampling Technique)","dc39f396":"# Modeling Five C's Portfolio Expected Loss using Balanced Bagging Classifier and XGB Classfier predicted probabilities of default","aad3133e":"# Combine all the processed dataframes together into a single one","c9cfc7b7":"# Intepreting the Plots\n\nThe y-axis represent the probablilty that a default\/credit negative event will occur at a specific point on the timeline (the x-axis). \n\nAs we move along the timeline, we can see the survival of loans not rated G (blue line) has a better survival probability then loans G. This is consistent with the fact that G-rated loans are the weakest in credit rating. Compare the G-rated loan plot to the loans rated A, the reverse is true. Loans rate A (higher financial and credit strength) have improved survival probability versus all other loans.","5ecad84f":"# Missing Data \/ Missing Observations","939a6d07":"# Plotting AUC for Logistic Regression Classification with Oversampled Data","5e0453c7":"# Conditions : loan_amnt, int_rate, term\nWhat it is: The condition of a business\/individual \u2014 whether it is growing or faltering \u2014 as well as what you\u2019ll use the funds for. Refers to the macro-economic and competitive environment which will impact an obligor's future performance and its ability to generate cash. \n\nWhy it matters: To ensure that loans are repaid, banks want to lend to businesses operating under favorable conditions. They aim to identify risks and protect themselves accordingly.\n\nHow it\u2019s assessed: From a review of the competitive landscape, supplier and customer relationships, and macroeconomic and industry-specific issues.","e087d0f0":"## Time to Default (or negative credit event) for Grade F rated customers","c4ab045f":"# Count total Non-Defualt and Defaulted Loans in the Data Set\n\nWhere \n\n'0' is a loan that has NOT defaulted \n\nand \n\n'1' is a loan that HAS defaulted","d0b411f0":"# Instantiate KMF object","2f0deebf":"## Time to Default (or negative credit event) for Grade C rated customers","e1f57188":"<code>'earliest_cr_line'<\/code> shows the first credit line on the customer credit report. \n\nWill create a new feature that shows the number of months between the first credit line in the customers history and the issue date of the loan.","73e0fd74":"# Notebook Goals\nThe goal of this notebook is the use the Five C's of credit framework (Character, Capacity, Cash Flow, Collateral, and Condition) to select the proxy features from the Kaggle Lending Club Dataset to predict loan default probability.\n\nUsing the predicted loan default probability, the notebook will the generate expected loss on loans within the portfolio.\n\nExpected Loss (EL) on a given is calculuated using the loan amount multiplied by a loan's probability of default (PD) mulitplied by the the loan's loss given default (LGD). Loss Given Default is the amount of the loan that is deemed unrecoverable and is represented as a number between 0 (no loss) and 1 (100% of the loan is a loss)\n\nThe equation for EL is given as:\n\n** EL = PD x LGD x Loan Amount*\n\nAfter selected features using the 5 C's of Credit Framework and adjusting for the skewedness in the data (there is a greater proportion of loans in the portfolio that have NOT defaulted versus loans that HAVE defaulted) we will model proabilities of default using classification models. \n\nWith proabilties of default we will then model Expected Loss using an assumed LGD of 1 (a LGD of 1 is a conservative assumption given the unsecured nature of the loans) ","1c55a631":"# TIME TO DEFAULT PREDICTION\n\nWe can use the following features contained in the raw data to predict time to default:\n\n*'emp_length', 'chargeoff_within_12_mths', 'mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_recent_revol_delinq'  *\n\nThese feaure provide an indication of time in months since a negative credit event (i.e. an event that has a negative impact on a customers credit history such as late payment, bankruptcy, loan default, civil lawsuit, etc).\n\nThese features will be used with the processed_df features to model time to default using Kaplan Meier Estimates and Cox Proportional Hazard Models.","597d3cff":"# Survival Analysis with KMF","d378c6a2":" # EDA","6fcef947":"# Creating, Training, and Fitting a Logistic Regression Classfier Model to Oversampled Data","d6153f30":"# Character: pub_rec_bankruptcies, earliest_cr_line, issue_d\nWhat it is: A lender\u2019s opinion of a borrower\u2019s general trustworthiness, credibility and personality. Not only the willingness of the oblgior to repay an obligation, and live up to its terms, but also honesty relative to the risk of fraud. \n\nWhy it matters: Banks want to lend to people who are responsible and keep commitments.\n\nHow it\u2019s assessed: From your work experience, credit history, credentials, references, reputation and interaction with lenders.","dfdbe669":"# Undersampling\n\nThe process where you randomly delete some of the observations from the majority class in order to match the numbers with the minority class","5a508058":"## Time to Default (or negative credit event) for Grade G rated customers","dd9fb6e8":"# Capacity\/Cash flow: annual_inc & dti\nWhat it is: Your ability to repay the loan. The ability to generate cash to repay all oblgiations, when due. Assessing this is the primary goal of credit analysis\n\nWhy it matters: Lenders want to be assured that your business generates enough cash flow to repay the loan in full."}}