{"cell_type":{"6695e8a2":"code","1a946c82":"code","168c352e":"code","8d627148":"code","fb0dec43":"code","45bc54d5":"code","8e6d7185":"code","a2e744c3":"code","ed736ea2":"code","ad07252b":"code","32b15f02":"code","b41f39e9":"markdown","b14bcabb":"markdown","f3fb10c2":"markdown","f8757a7b":"markdown","6a12e750":"markdown","dbc90b1f":"markdown","4d99c885":"markdown","612054db":"markdown","92ab2c52":"markdown","86ceaa7b":"markdown","2ff1701d":"markdown","49abc9c1":"markdown","f12edc77":"markdown","ebeab80a":"markdown","c2133b10":"markdown","9fcc7f3b":"markdown"},"source":{"6695e8a2":"!python -m pip install Faker --upgrade pip ","1a946c82":"from faker import Faker\nfrom pandas import pandas as pd\nfake = Faker()\n\nn = 100\nsynthetic_data = []\nfor i in range(n):\n            item = {\n                'company': fake.company(),\n                'job': fake.job(),\n                'country': fake.country()\n            }\n            synthetic_data.append(item)\n\nsynthetic_df = pd.DataFrame(synthetic_data)\nsynthetic_df.head()","168c352e":"import random\nimport pandas as pd\n\ndef generate_dataset(file_path, n):\n    \"\"\" \n    Create a dataset contaning n-long rows\n    \n    :param file_path: The path where the CSV file will be save \n    :param n: The amount of rows to be generated randomly\n    \n    \"\"\"\n    \n    countries = [\"Algeria\",\" Angola\",\" Benin\",\" Botswana\",\" Burkina Faso\",\\\n                     \"Burundi\",\" Cameroon\",\" Cabo Verde\",\" Central African Republic\",\\\n                     \"Chad\",\" Comoros\",\" Congo\",\" Congo\",\\\n                     \"Cote d\u2019Ivoire\",\" Djibouti\",\" Equatorial Guinea\",\" Egypt\",\" Eritrea\",\\\n                     \"Ethiopia\",\" Gabon\",\" Gambia\",\" Ghana\",\" Guinea\",\"Kenya\",\"Lesotho\",\"Liberia\",\\\n                     \"Libya\",\" Madagascar\",\" Malawi\",\" Mali\",\" Mauritania\",\" Mauritius\",\" Morocco\",\\\n                     \"Mozambique\",\" Namibia\",\" Niger\",\" Nigeria\",\" Rwanda\",\" Sao Tome and Principe\",\\\n                     \"Senegal\",\" Seychelles\",\" Sierra Leone\",\" Somalia\",\" South Africa\",\" South Sudan\",\\\n                     \"Sudan\",\" Tanzania\",\" Togo\",\" Tunisia\",\" Uganda\",\"Zimbabwe.\"]\n    conditions = [\"heart\",\"viral\",\"cancer\",\"bacteria\",\"kidney\",\"diabetes\"]\n    gender = [\"male\",\"female\"]\n\n    # Initiate random generation\n    with open(file_path, \"w\") as f:\n        # print headers \n        f.write(\"country,condition,age,gender\\n\")\n        for i in range(n):\n            country = countries[random.randint(0, len(countries)-1)]\n            condition = conditions[random.randint(0, len(conditions)-1)]\n            age = random.randint(5,110)\n            gend = gender[random.randint(0,1)]\n\n            f.write(\"{},{},{},{}\\n\".format(country, condition, age, gend))\n        ","8d627148":"file_path = '\/kaggle\/working\/dataset.csv'\nn = 10**4 # how many rows we want in the dataset\n\ngenerate_dataset(file_path, n)\ndf = pd.read_csv(file_path)\ndf.shape","fb0dec43":"def k_anonymize(dataset, columns):\n    \"\"\"\n    Anonymize certain columns of the database\n    using suppression and generalization\n    \n    :param dataset: The set of columns and rows composing the dataset\n    :param columns: Specific columns which entries are considered sensitive\n    \n    \"\"\"\n    \n    anon_dataset = dataset.copy()\n    \n    # Target all sensitive columns\n    for column in columns:\n        column_label = column['label']\n        if column['type'] == 'suppressed':\n            # Replace all characters with asterix\n            anon_dataset[column_label] = ['*' for x in anon_dataset[column_label]]\n        \n        if column['type'] == 'semi-suppressed':\n            # Replace 70% of the characters with asterix\n            anon_dataset[column_label] = [('*'*(round(len(x)*.7)) + x[(round(len(x)*.7)):]) for x in anon_dataset[column_label]]\n        \n        if column['type'] == 'generalized':\n            # Summarize the data using ranges\n            for i in range(len(anon_dataset[column_label])):\n                # convert column type from int to string\n                anon_dataset[column_label] = anon_dataset[column_label].astype(str)\n                x = int(anon_dataset[column_label][i])\n                if x <= 40: anon_dataset[column_label][i] =  \"0-40\"\n                if x > 40 and x <= 60: anon_dataset[column_label][i] =  \"40-60\"\n                if x > 60 and x <= 100: anon_dataset[column_label][i] =  \"> 60\"\n\n    return anon_dataset\n\n# Describe the columns and their respective anonymization type\ncolumns = [\n    {\"label\": \"country\", \"type\": \"semi-suppressed\"},\n    {\"label\": \"gender\", \"type\": \"suppressed\"},\n    {\"label\": \"condition\", \"type\": \"semi-suppressed\"},\n    {\"label\": \"age\", \"type\": \"generalized\"}\n]\n\ndataset = df\nanonymized_dataset = k_anonymize(dataset, columns)","45bc54d5":"df.head(1000)","8e6d7185":"anonymized_dataset.head(1000)","a2e744c3":"!pip install diffprivlib","ed736ea2":"from diffprivlib.mechanisms import LaplaceTruncated\n\nsensitivity=3\nepsilon=0.3\nmechanism = LaplaceTruncated(sensitivity=sensitivity, epsilon=epsilon, lower=5, upper=100) \nlaplace_dataset = df.copy()\nlaplace_dataset['age'] = [mechanism.randomise(laplace_dataset['age'][x]) for x in laplace_dataset['age']]\n\nlaplace_dataset.head(1000)","ad07252b":"from diffprivlib.mechanisms import Exponential\n\nexp_dataset = df.copy()\nsensitivity = 3\nepsilon = 0.3\nutility = [random.randint(0,1) for x in range(len(exp_dataset['condition']))]\ncandidates = exp_dataset['condition'].values.tolist()\n\nmechanism = Exponential(\n    sensitivity=sensitivity,\n    epsilon=epsilon,\n    utility=utility,\n    candidates=candidates\n)\n\nexp_dataset['condition'] = [mechanism.randomise() for _ in range(len(exp_dataset['condition']))]\nexp_dataset.head(1000)\n","32b15f02":"final_dataset = pd.concat([\n                            df,\n                            pd.DataFrame(exp_dataset['condition']).add_prefix('anon_'),\n                            pd.DataFrame(laplace_dataset['age']).add_prefix('anon_'),\n                          ],\n                          axis=1)\nfinal_dataset.head(1000)","b41f39e9":"### Install DiffPrivlib dependency\n","b14bcabb":"### Dataset without anonymization","f3fb10c2":"### Randomize categorical values through Exponential distribution","f8757a7b":"#### Preparing a function for random generation","6a12e750":"This Notebook provides an overview of three mainstream privacy-preserving strategies: Synthethic data generation, K-anonymization, and Differential privacy. Additionally, it features some concrete implementation of both techniques in Python. \n\n## A short intro to privacy preservation\nWith the ever growing number of datasets being published, there is an increased risk of seeing personal identifying information being leaked directly or indirectly. The [Netflix Prize dataset incident](https:\/\/www.cs.utexas.edu\/~shmat\/shmat_oak08netflix.pdf) provides a concrete illustration of how quasi-identifying information can be coupled with background knowledge to uncover people's real identities.\n\nAs regulators have been putting more emphasis on privacy preservation over the last two decades, various techniques have emerged and aim at safeguarding individual's privacy.","dbc90b1f":"### Synthetic data generation using Faker\nSynthethic data generation using Faker provides a useful way to work on a large amount of data without using identifying information of real individuals. For demonstration purpose, we'll only generate a tiny dataset and display a portion of it.","4d99c885":"### Randomize all numerical values through Laplace distribution","612054db":"### Manual random data generation\nIf we want fine-grained control over the generated dataset, we can choose a more manual approach. Focusing on the following columns: `country`, `condition`, `age`, `gender`. Let's generate a dataset containing 10,000 entries created randomly, and save the obtained data in CSV format. The CSV file will be used during the subsequent anonymization steps.","92ab2c52":"#### Install Faker library\n","86ceaa7b":"## K-anonymization in action\nPreserving privacy through anonymization may be achieved through numerous strategies, ranging from masking, swaping, encryption, hashing, to [generalizing](https:\/\/www.geeksforgeeks.org\/basic-approaches-for-data-generalization-dwdm\/). Below is an illustration of anonymization through [k-anonymity](https:\/\/en.wikipedia.org\/wiki\/K-anonymity), a well known data anonymization technique which relies on data suppression and generalization.","2ff1701d":"## Differential privacy at work\nIn a nutshell, the idea behind differential privacy is the promise to make it nearly impossible for anyone to identify private information about an individual from a dataset. This is particularly vital as large datasets are available today of which many include quasi-identifying information such as zip code, gender, and birthdate, which when combined were enough to identify [86% of US population](https:\/\/dataprivacylab.org\/projects\/identifiability\/paper1.pdf), as proven by Latanya Sweeney. \n\nA differentially private algorithm will take some dataset as input and inject some noise into the identifying pieces of information it contains. The noise will be generated randomly by levaging statistical distributions such as [Laplace](https:\/\/en.wikipedia.org\/wiki\/Laplace_distribution) or [Gaussian](https:\/\/en.wikipedia.org\/wiki\/Normal_distribution). As a result, the identifying information will be hidden behind the noise, protecting the privacy of the individuals having their identifying information in the dataset.\n\nBelow is an illustration of differential privacy in Python, using [IBM's Diffprivlib library](https:\/\/github.com\/IBM\/differential-privacy-library), and statistical distributions (Laplace and Exponential) to generate random noise.\n\n\n","49abc9c1":"## Synthetic data generation\nInstead of utilizing real dataset with actual personal identifiable information (PII), a solution would be to generate a fresh dataset. Here we'll create a new dataset through Faker, a Python library and move on a more fine-grained approach.","f12edc77":"### Render final dataset","ebeab80a":"### The same dataset after k-anonymization","c2133b10":"#### Generate a tiny dataset","9fcc7f3b":"Although k-anonymization is good at hiding some sensitive details, some obvious limitations can be obversved here. Some countries are still identifiable even if they are partially deleted. For instance `***uinea` most likely refers to Guinea. Other shortcomings of k-anonymity [have widely been documented](https:\/\/en.wikipedia.org\/wiki\/K-anonymity#Possible_attacks), including homogeineity attack and background knowledge attack."}}