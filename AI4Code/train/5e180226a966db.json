{"cell_type":{"be2a1369":"code","23e13a2d":"code","271266b7":"code","22c4c44c":"code","5c2e44d4":"code","ce736046":"code","3918ff02":"code","b77ea177":"code","02d536d6":"code","8f3bad7e":"code","e8115985":"markdown","a77a467a":"markdown","8c90cf19":"markdown","13c14e29":"markdown","9dbfb13d":"markdown"},"source":{"be2a1369":"class POP:\n    def __init__(self, session_id, session, session_timestamp):\n        item_count = {}\n        \n        # count each item\n        for sess in session:\n            for item in sess:\n                item_count[item] = item_count.get(item,0) + 1\n                \n        # sort item by their count\n        self.item = [(k,item_count[k]) for k in sorted(item_count, key=item_count.get, reverse=True)]\n    \n    def predict(self, session_id, session_items, session_timestamp, k = 20):\n        # return top k populair items\n        return self.item[:k]","23e13a2d":"import math\nfrom _operator import itemgetter\n\n\nclass STAN:\n    def __init__(self, session_id, session, session_timestamp, sample_size=0, k=500, factor1=True, l1=3.54,\n                 factor2=True, l2=20 * 24 * 3600, factor3=True, l3=3.54):\n        self.k = k\n        self.sample_size = sample_size\n        self.session_all = session\n        self.session_id_all = session_id\n        self.session_timestamp_all = session_timestamp\n        self.factor1 = factor1\n        self.factor2 = factor2\n        self.factor3 = factor3\n        self.l1 = l1\n        self.l2 = l2\n        self.l3 = l3\n\n        # cache the items for O(1) retrieval.\n        self.session_timestamp_cache = {}  # session_id: timestamp\n        self.item_session_cache = {}  # item_id: set(session_id)\n        self.session_item_cache = {}  # session_id: set(item_id)\n        self.session_item_list_cache = {}  # session_id: [item_id] for the item weights in training sessions\n        for i, session in enumerate(self.session_all):\n            sid = self.session_id_all[i]  # current session id\n            self.session_timestamp_cache.update({sid: self.session_timestamp_all[i]})\n            for item in session:\n                session_map = self.item_session_cache.get(item)\n                if session_map is None:\n                    session_map = set()\n                    self.item_session_cache.update({item: session_map})\n                session_map.add(sid)\n\n                item_map = self.session_item_cache.get(sid)\n                if item_map is None:\n                    item_map = set()\n                    self.session_item_cache.update({sid: item_map})\n                item_map.add(item)\n\n                item_list = self.session_item_list_cache.get(sid)\n                if item_list is None:\n                    item_list = []\n                    self.session_item_list_cache.update({sid: item_list})\n                item_list += [item]\n\n        self.current_session_weight_cache = {}\n        self.current_timestamp = 0\n\n    def find_neighbours(self, session_items, input_item):\n        # neighbour candidate\n        possible_neighbours = self.possible_neighbour_sessions(session_items, input_item)\n        # get top k according to similarity\n        possible_neighbours = self.cal_similarity(session_items, possible_neighbours)\n        possible_neighbours = sorted(possible_neighbours, reverse=True, key=lambda x: x[1])\n        possible_neighbours = possible_neighbours[:self.k]\n\n        return possible_neighbours\n\n    def possible_neighbour_sessions(self, session_items, input_item):\n        # any session with at least one common item as the one in the testing session is considered as the possible neighbour.\n        neighbours = set()\n        for item in session_items:\n            if item in self.item_session_cache:\n                neighbours = neighbours | self.item_session_cache.get(item)\n        return neighbours\n\n    # find recent session (not use)\n    def most_recent_sessions(self, sessions):\n        recent_session = set()\n        tuples = []\n        for session in sessions:\n            time = self.session_timestamp_cache.get(session)\n            tuples += [(session, time)]\n\n        tuples = sorted(tuples, key=itemgetter(1), reverse=True)\n        cnt = 0\n        for i in tuples:\n            cnt += 1\n            if cnt > self.sample_size:\n                break\n            recent_session.add(i[0])\n        return recent_session\n\n    # calculate similarity between target session and possible neighbors\n    def cal_similarity(self, session_items, sessions):\n        neighbours = []\n        for session in sessions:\n            neighbour_session_items = self.session_item_cache.get(session)\n            similarity = self.cosine_similarity(neighbour_session_items, session_items)\n            if self.factor2 is True:\n                similarity = similarity * math.exp(\n                    -(self.current_timestamp - self.session_timestamp_cache[session]) \/ self.l2)\n            if similarity > 0:\n                neighbours += [(session, similarity)]\n        return neighbours\n\n    # cosine similarity of two sessions\n    def cosine_similarity(self, s1, s2):\n        common_item = s1 & s2\n        similarity = 0\n        for item in common_item:\n            similarity += self.current_session_weight_cache[item]\n        l1 = len(s1)\n        l2 = len(s2)\n        return similarity \/ math.sqrt(l1 * l2)\n\n    # scoring item according to their session\n    def score_items(self, neighbours, session_items):\n        scores = {}\n        for session in neighbours:\n            items = self.session_item_cache.get(session[0])\n\n            # find the latest common_items in training session\n            common_items_idx = -1\n            for i, item in enumerate(items):\n                if item in session_items:\n                    common_items_idx = i\n            # if common_items_idx == -1:\n            #     print(\"no common item\")  # will never happen\n\n            for idx, item in enumerate(items):\n                old_score = scores.get(item)\n                if self.factor3 is True:\n                    new_score = math.exp(-math.fabs(idx - common_items_idx) \/ self.l3) * session[1]\n                else:\n                    new_score = session[1]\n                \n                if old_score is None:\n                    scores.update({item: new_score})\n                else:\n                    new_score += old_score\n                    scores.update({item: new_score})\n        return scores\n\n    def predict(self, session_id, session_items, session_timestamp, k=20):\n        # initialize\n        self.current_session_weight_cache = {}\n        length = len(session_items)\n        for idx, item in enumerate(session_items):\n            if self.factor1 is True:\n                weight = math.exp((idx + 1 - length) \/ self.l1)\n            else:\n                weight = 1\n            # if there is several identical items in one session, choose the biggest weight as the item's weight\n            if item in self.current_session_weight_cache:\n                self.current_session_weight_cache.update({item: max(weight, self.current_session_weight_cache[item])})\n            else:\n                self.current_session_weight_cache.update({item: weight})\n        self.current_timestamp = session_timestamp\n\n        last_item_id = session_items[-1]\n        neighbours = self.find_neighbours(set(session_items), last_item_id)\n        scores = self.score_items(neighbours, session_items)\n        scores_sorted_list = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n        return scores_sorted_list","271266b7":"import pandas\n\n#read the dataset\ntrain = pandas.read_csv('\/kaggle\/input\/food-com-recipes-and-user-interactions\/RAW_interactions.csv').drop(['review'],axis=1)\nrecipe = pandas.read_csv('\/kaggle\/input\/food-com-recipes-and-user-interactions\/RAW_recipes.csv')\nprint(train)","22c4c44c":"#get the year from year data\ntrain['rate_year'] = pandas.DatetimeIndex(train['date']).year\n\n#session_id defined as user_id + rate_year and each session item contains recipes the user rated, filter out session length with only 1 rated recipe\ntrain =  train.groupby(['user_id','rate_year']).filter(lambda x:len(x['recipe_id']) > 1) \nprint(train)","5c2e44d4":"import datetime\n\n\ngrouped = train.groupby(['user_id','rate_year'])\nsessions = grouped['recipe_id'].apply(list)\n\n#timestamp of the session is the timestamp of the last item in that session\ntimestamp = grouped['date'].apply(lambda x: datetime.datetime.strptime(list(sorted(list(x)))[-1],'%Y-%m-%d').timestamp())\nprint(sessions)","ce736046":"# split sessions into ID - ITEM - TIMESTAMP\nID = [key for key,_ in grouped]\nITEM = sessions.tolist()\nTIMESTAMP = list(timestamp)\nprint(ID[:5])\nprint(ITEM[:5])\nprint(TIMESTAMP[:5])\n","3918ff02":"import operator\n\nmax_timestamp = 0\nfor timestamp in TIMESTAMP:\n    max_timestamp = max(max_timestamp, timestamp)  # latest date\n\nsplit_timestamp = max_timestamp - 7 * 86400 # split data from the last week to use as the testset\n\n#all data before last week used in training\ntrain_session = filter(lambda x: x[1] < split_timestamp, enumerate(TIMESTAMP))\n\n#all data in the last week used in testing\ntest_session = filter(lambda x: x[1] > split_timestamp, enumerate(TIMESTAMP))\n\n\n# Sort sessions by date\ntrain_session = sorted(train_session, key=operator.itemgetter(1))\ntest_session = sorted(test_session, key=operator.itemgetter(1))\n\n\nITEM_test = [ITEM[index] for index,_ in test_session]\nID_test = [ID[index] for index,_ in test_session]\nTIMESTAMP_test = [TIMESTAMP[index] for index,_ in test_session]\n\nITEM_train = [ITEM[index] for index,_ in train_session]\nID_train = [ID[index] for index,_ in train_session]\nTIMESTAMP_train = [TIMESTAMP[index] for index,_ in train_session]\n\nprint(len(ITEM_test))\nprint(ITEM_test[:5])","b77ea177":"# split sequence\ndef split_seq(sid, timestamp, seq):\n    x = []\n    t = []\n    y = []\n    s_id = []\n    for sid, seq, timestamp in zip(sid, seq, timestamp):\n        temp = len(seq)\n        for i in range(1, temp):\n            y += [seq[-i]]\n            x += [seq[:-i]]\n            t += [timestamp]\n            s_id += [sid]\n    return x, t, y, s_id\n","02d536d6":"import pickle\nimport numpy as np\nimport math\nimport time\n\n# splits the train sessions into sub-sessions\ntrain_session,train_timestamp,train_predict,train_id = split_seq(ID_train,TIMESTAMP_train,ITEM_train)\n\n# add the prediction back since we doesn't need any form of deep training\nfor i, s in enumerate(train_session):\n    train_session[i] += [train_predict[i]]\n\n# splits the test session into sub-sessions\ntest_session,test_timestamp,test_predict,test_id = split_seq(ID_test,TIMESTAMP_test,ITEM_test)","8f3bad7e":"# model = SKNN(session_id=train_id, session=train_session, session_timestamp=train_timestamp, sample_size=0, k=500)\n# model = VSKNN(session_id=train_id, session=train_session, session_timestamp=train_timestamp, sample_size=0, k=500)[\n#STAN and Popularity recommender models\nmodels = [\n    STAN(session_id=train_id, session=train_session, session_timestamp=train_timestamp, k=20, factor1=True, l1 = 1,\n                 factor2=True, l2=365 * 24 * 3600, factor3=True, l3 = 2),\n    POP(session_id=train_id, session=train_session, session_timestamp=train_timestamp)\n]\n\n#Calcultae metrics Recall, MRR and NDCG for each model\nfor model in models: \n    print(\"MODEL = \" , model)\n    testing_size = len(test_session)\n    # testing_size = 10\n\n    R_5 = 0\n    R_10 = 0\n    R_20 = 0\n\n    MRR_5 = 0\n    MRR_10 = 0\n    MRR_20 = 0\n\n    NDCG_5 = 0\n    NDCG_10 = 0\n    NDCG_20 = 0\n    for i in range(testing_size):\n        if i % 1000 == 0:\n            print(\"%d\/%d\" % (i, testing_size))\n            # print(\"MRR@20: %f\" % (MRR_20 \/ (i + 1)))\n\n        score = model.predict(session_id=test_id[i], session_items=test_session[i], session_timestamp=test_timestamp[i])\n        # for s in score:\n        #     print(s)\n        # print(test_predict[i])\n        # print(\"-----------------------------------\")\n        # print(\"-----------------------------------\")\n        items = [x[0] for x in score]\n        # if len(items) == 0:\n        #     print(\"!!!\")\n        if test_predict[i] in items:\n            rank = items.index(test_predict[i]) + 1\n            # print(rank)\n            MRR_20 += 1 \/ rank\n            R_20 += 1\n            NDCG_20 += 1 \/ math.log(rank + 1, 2)\n\n            if rank <= 5:\n                MRR_5 += 1 \/ rank\n                R_5 += 1\n                NDCG_5 += 1 \/ math.log(rank + 1, 2)\n                \n\n            if rank <= 10:\n                MRR_10 += 1 \/ rank\n                R_10 += 1\n                NDCG_10 += 1 \/ math.log(rank + 1, 2)\n        \n                print(\"past recipes:\")\n                for r in test_session[i]:\n                    print(recipe[recipe['id'] == r]['name'].tolist()[0],end=\",\\n\")\n                print(\"\\n\")\n                print(\"recommended recipes:\")\n                for r in items:\n                    print(recipe[recipe['id'] == r]['name'].tolist()[0],end=\",\\n\")\n                print(\"\\n\")\n                print(\"actual next recipe:\")\n                print(recipe[recipe['id'] == test_predict[i]]['name'].tolist()[0])\n                print(\"\\n\\n\\n\\n\\n\")\n\n        \n    MRR_5 = MRR_5 \/ testing_size\n    MRR_10 = MRR_10 \/ testing_size\n    MRR_20 = MRR_20 \/ testing_size\n    R_5 = R_5 \/ testing_size\n    R_10 = R_10 \/ testing_size\n    R_20 = R_20 \/ testing_size\n    NDCG_5 = NDCG_5 \/ testing_size\n    NDCG_10 = NDCG_10 \/ testing_size\n    NDCG_20 = NDCG_20 \/ testing_size\n\n    print(\"MRR@5: %f\" % MRR_5)\n    print(\"MRR@10: %f\" % MRR_10)\n    print(\"MRR@20: %f\" % MRR_20)\n    print(\"R@5: %f\" % R_5)\n    print(\"R@10: %f\" % R_10)\n    print(\"R@20: %f\" % R_20)\n    print(\"NDCG@5: %f\" % NDCG_5)\n    print(\"NDCG@10: %f\" % NDCG_10)\n    print(\"NDCG@20: %f\" % NDCG_20)\n    print(\"training size: %d\" % len(train_session))\n    print(\"testing size: %d\" % testing_size)","e8115985":"# TRANSFORMATION","a77a467a":"# TRAIN TEST SPLIT","8c90cf19":"# PREPROCESS","13c14e29":"# Implementation of STAN and Popularity recommender","9dbfb13d":"## Pre-process the dataset of interactions_train. group the datas into user - interactions, and divide sessions based on four season of the year or per year (because of the season specific ingredients)"}}