{"cell_type":{"acdeb5b2":"code","2db6a598":"code","868c471f":"code","0884ce77":"code","0757477f":"code","5f570401":"code","6b09f357":"code","1e3b013e":"code","e29baf9c":"code","7ff4d7a1":"code","29a29de9":"code","067b8cd5":"code","cc8e3ca3":"code","db708050":"code","ec99907d":"code","9707f6cb":"markdown","0527705a":"markdown","c1bd6312":"markdown","6c11f712":"markdown","eae19854":"markdown","46a5493a":"markdown","00189867":"markdown","f03ae98e":"markdown","a08e9dae":"markdown"},"source":{"acdeb5b2":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import decomposition\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2db6a598":"train = pd.read_csv(\"\/kaggle\/input\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/test.csv\")","868c471f":"pd.set_option('max_columns', None)\ntrain.describe(include = 'all')","0884ce77":"cat_train = train.select_dtypes(include = 'object')\ncount = 0\nfor i in (cat_train.columns):\n    print(cat_train.columns[count], ': ', cat_train[i].unique())\n    print('\\n--------------------------\\n')\n    count += 1","0757477f":"cat_replace = {'OverTime': {'No': 0, 'Yes': 1}}\ntrain.replace(cat_replace, inplace = True)","5f570401":"obj = train.dtypes == 'object'\nobj_cols = list(obj[obj].index)\n# train_ohe_cols = pd.DataFrame(one_hot.fit_transform(train[obj_cols])) \n\nencoded = pd.DataFrame()\ntemp = pd.DataFrame()\nfor col in obj_cols:\n    temp = pd.get_dummies(train[col], prefix = col)\n    encoded[temp.columns] = temp\n\n# train_ohe_cols.index = train.index\ntrain_ohe = pd.concat([train, encoded], axis = 1)\ntrain_encoded = train_ohe.drop(obj_cols, axis = 1)\ntrain_encoded.head()","6b09f357":"cm = train_encoded.drop(['Id', 'Behaviour'], axis = 1).corr()\n\nplt.figure(figsize = (20, 20))\nsns.heatmap(cm, square = True, xticklabels = True, yticklabels = True, cmap = 'inferno')\n\ncorrelated = []\nfor i in range(len(cm.index)):\n    for j in range(len(cm.columns)):\n        if np.abs((cm.iloc[i, j])) >= 0.5 and i != j:\n            correlated.append((cm.index[i], cm.columns[j], cm.iloc[i, j]))\n            \nlen(correlated)\n","1e3b013e":"train_encoded.drop(['Id', 'Behaviour'], axis = 1, inplace = True)","e29baf9c":"# selecting features to scale. This includes all numerical features that aren't ordinal in nature:\nfeatures = ['Age', 'EmployeeNumber','DistanceFromHome', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', \n                    'TotalWorkingYears', 'YearsAtCompany','YearsInCurrentRole', \n                    'YearsSinceLastPromotion','YearsWithCurrManager', 'TrainingTimesLastYear']\n\nfrom sklearn.preprocessing import StandardScaler\n\n# scaling the entire training dataset now:\nss_final = StandardScaler()\n\ntrain_std = train_encoded[features]\ntrain_std = pd.DataFrame(ss_final.fit_transform(train_std))\ntrain_std_full = pd.concat([train_std, train_encoded.drop(features, axis = 1)], axis = 1)\n\nfrom sklearn.decomposition import PCA\n\npca_check = PCA(whiten = True)\npca_check.n_components = 46\npca_check.fit_transform(train_std_full.drop('Attrition', axis = 1))\nvar_retention = pca_check.explained_variance_ratio_.cumsum()\nprint(\"var_retention: \", var_retention, \"\\n ---------------------------------------------------------------------------------------------\")\n\n# pulling least value of k for which variance retention is 99% or close\nfor i in range(len(var_retention)):\n    if var_retention[i] >= 0.98:\n        print(i, \", \", var_retention[i])\n\nfull_tgt = train_std_full.Attrition\ntrain_std_full.drop('Attrition', axis = 1, inplace = True)","7ff4d7a1":"# therefore, best value of n_components would be 33\npca = PCA(n_components = 33) # could try whiten = True later.\ntrain_pca = pd.DataFrame(pca.fit_transform(train_std_full))","29a29de9":"# dropping unnecessary columns\ntest.drop(['Id', 'Behaviour'], axis = 1, inplace = True)\n\n\n# separating categorical columns, and label\/one-hot encoding them\ncat_replace = {'OverTime': {'No': 0, 'Yes': 1}}\ntest.replace(cat_replace, inplace = True)\n\n# replaced OneHotEncoder with pd.get_dummies() for legibility of dataframe before scaling and PCA.\n\nencoded_test = pd.DataFrame()\ntemp_test = pd.DataFrame()\nfor col in obj_cols:\n    temp_test = pd.get_dummies(test[col], prefix = col)\n    encoded_test[temp_test.columns] = temp_test\n    \ntest_encoded = pd.concat([test, encoded_test], axis = 1)\ntest_encoded.drop(obj_cols, axis = 1, inplace = True)\n\n# Scaling of test set according to insights from training set:\ntest_std = test_encoded[features]\ntest_std = pd.DataFrame(ss_final.transform(test_std))\ntest_std_full = pd.concat([test_std, test_encoded.drop(features, axis = 1)], axis = 1)\n\n# transforming test set via PCA:\ntest_pca = pd.DataFrame(pca.transform(test_std_full))","067b8cd5":"# checking for best RFC parameters:\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\n# param_grid_xgb = {\n#     'n_estimators': range[100000],\n#     'early_stopping rounds': [10, 20],\n#     'learning_rate': [0.0005, 0.0001],\n#     'max_depth': [7],\n#     'verbosity': [1]\n# }\n\n# xgb_final = XGBClassifier()\n# grid_search_xgb = GridSearchCV(estimator = xgb_final, param_grid = param_grid_xgb, cv = 6, n_jobs = -1)\n# grid_search_xgb.fit(train_pca, full_tgt)\n# grid_search_xgb.best_params_","cc8e3ca3":"# fitting an XGBC model\nxgbc_final = XGBClassifier(n_estimators = 125000, learning_rate = 0.0005,\n                           max_depth = 7, n_jobs = -1, verbosity = 1)\nxgbc_final.fit(train_pca, full_tgt)\npred_xgbc = xgbc_final.predict(test_pca)","db708050":"test_atts = xgbc_final.predict_proba(test_pca)\nprob_att = pd.Series(test_atts[:, 1])\nprob_att.rename('Attrition', inplace = True)","ec99907d":"test_orig = pd.read_csv(\"\/kaggle\/input\/test.csv\")\n\nsubmission = pd.concat([test_orig.Id, prob_att], axis = 1)\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","9707f6cb":"Now to actually see what the dataset columns look like. We shall also look for missing values, if any, and take necessary steps to fix that issue.","0527705a":"# ***Importing basic libraries***\nWe shall import more as and when necessary.","c1bd6312":"# ***Recap (preprocessing on test set)***\n\nLet us repeat the same operations on the test set that we done on the training set.\nThis cell shall act as a summary for the entirety of operations done on the dataset, and I might use it to develop a pipeline later on\n","6c11f712":"# ***Making the model***\n\nUtilising the best possible values from above, suitable values of the hyperparameters can be chosen and the model can be made.","eae19854":"# ***Finding the best XGBoost model***\n\nThe lines containing the GridSearchCV objects have been commented for ease of running the notebook. They can be uncommented and analyses can be run on different configurations of ***param_grid_xgb***.\n\n**WARNING: this may take considerable time and processing power. Thus, keep** ***n_jobs = -1*** **for using all cores of your processor, to make it as speedy as possible.**","46a5493a":"# ***Applying PCA for dimensionality reduction***\n\nWe see that a bunch of columns are related (either positively or negatively). Using PCA (with 99% variance retention) should automatically solve this issue, without us having to bother about individual correlations. \nHowever, this will not give us a proper idea of which features are more important than the others to the prediction. ","00189867":"There seem to be no missing values in any of the columns, which is a good sign. A couple of columns are useless from the outset, like 'Id' and 'Behaviour', but we'll take care of those later.\n\nFor now, we'll look at categorical variables.","f03ae98e":"# ***Finding correlation of features***","a08e9dae":"# ***Encoding categorical variables***"}}