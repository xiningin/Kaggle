{"cell_type":{"93ce6cb6":"code","c5cbdfdf":"code","c5afcdba":"code","0ecf6bc4":"code","2b645f82":"code","59902948":"code","8e45a45f":"code","3f06929f":"code","8bb03663":"code","9def36bb":"code","23108105":"code","d2ac4d33":"code","8ce4cecf":"code","0952c387":"code","a68fb3ed":"code","aca356f1":"code","288fc2d6":"code","095f02ce":"code","d34dbe11":"code","7d9a6546":"code","4963071e":"code","bab35fa7":"code","85bfe929":"code","f568fb27":"code","11e59196":"code","8fa9a565":"code","f2a9a030":"code","b42647f2":"code","5bcc8b20":"code","9dbea560":"code","d8f2874d":"code","4a363617":"code","d5f39a7c":"code","6b300b3d":"code","09477df9":"code","300ec71c":"code","78e0c3b4":"code","16c7f9f1":"code","361d1bbf":"code","d857ae2c":"code","509386d6":"code","5f638b04":"code","426c4e89":"code","365ee9f7":"code","cbee142f":"code","585fb443":"code","5b54d0d3":"code","4e98c780":"code","8471dfd5":"code","88af0c8c":"code","3ae566bc":"code","abb149ed":"code","322a97a2":"markdown","af426b5b":"markdown","ab70898f":"markdown","ecd674bd":"markdown","17f9d4c9":"markdown","6425c324":"markdown","c71b3bcd":"markdown","b80456ae":"markdown","d02f7e70":"markdown","b67a1c3c":"markdown","22742b5a":"markdown","0e2cd2f7":"markdown","431e6020":"markdown","3314cd2e":"markdown","648ea6d8":"markdown","642411ad":"markdown","c4b182d5":"markdown","e7a67bc2":"markdown","43e69a7e":"markdown"},"source":{"93ce6cb6":"import math\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","c5cbdfdf":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.head()","c5afcdba":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.drop(['Name', 'Ticket'], axis=1, inplace=True)\ntest.head()","0ecf6bc4":"data.drop(['Name', 'Ticket'], axis=1, inplace=True)\ndata.head()","2b645f82":"data.shape","59902948":"data.describe()","8e45a45f":"data.isnull().sum()","3f06929f":"bool_data = data.isnull()\n\nsns.countplot(bool_data[data.Survived == 0].Cabin)","8bb03663":"def fill_cabin(columns):\n    cabin = columns[0]\n    \n    try:\n        cabin = float(cabin)\n        return 0\n    except:\n        return 1\n    \ndata['cabin_details'] = data[['Cabin']].apply(fill_cabin, axis=1)\ndata.head()","9def36bb":"test['cabin_details'] = test[['Cabin']].apply(fill_cabin, axis=1)\ntest.head()","23108105":"test.drop('Cabin', axis=1, inplace=True)\ndata.drop('Cabin', axis=1, inplace=True)\ndata.head()","d2ac4d33":"sns.boxplot(data.Sex, data.Age)","8ce4cecf":"#fill the null values of the Age by mean values as the ages are not varying a lot.\nbackup_data = pd.concat([data, test], axis=0)\nmale_value = round(backup_data[backup_data.Sex == 'male'].Age.mean())\nfemale_value = round(backup_data[backup_data.Sex == 'female'].Age.mean())\n\ndef fill_age(columns):\n    age = columns[0]\n    gender = columns[1]\n    \n    if math.isnan(age):\n        if gender == 'male':\n            return male_value\n        else:\n            return female_value\n    else:\n        return age\n    \ndata['Age'] = data[['Age', 'Sex']].apply(fill_age, axis=1)\ntest['Age'] = test[['Age', 'Sex']].apply(fill_age, axis=1)","0952c387":"sns.countplot(data.Embarked)","a68fb3ed":"data.Embarked.fillna('S', inplace=True)","aca356f1":"data.head()","288fc2d6":"test.fillna(test.Fare.mean(), inplace=True)","095f02ce":"def alone(cols):\n    sib = cols[0]\n    par = cols[0]\n    \n    if (sib + par) == 0:\n        return 1\n    else:\n        return 0\n    \ndata['alone'] = data[['SibSp', 'Parch']].apply(alone, axis=1)\ntest['alone'] = test[['SibSp', 'Parch']].apply(alone, axis=1)","d34dbe11":"def encode_sex(col):\n    sex = col[0]\n    if sex == 'male':\n        return 1\n    else:\n        return 0\n    \n#1 for male and 0 for female.\ndata['Sex'] = data[['Sex']].apply(encode_sex, axis=True)\ntest['Sex'] = test[['Sex']].apply(encode_sex, axis=True)","7d9a6546":"label = LabelEncoder()\nx = data.copy()\ny = test.copy()\nx['Embarked']=label.fit_transform(x['Embarked'])\ny['Embarked'] = label.fit_transform(y['Embarked'])","4963071e":"data = x.copy()\ntest = y.copy()","bab35fa7":"sns.pairplot(data)","85bfe929":"#distribution of Fare zoomed in.\nsns.distplot(data['Fare'])","f568fb27":"#add a constant c to avoid logging zeros.\nC = 2\ndata['Fare'] = data['Fare'] + C\n\ndef log_transform(col):\n    return np.log(col[0])\n\ndata['Fare'] = data[['Fare']].apply(log_transform, axis=1)","11e59196":"corr = data.corr()\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, cmap='YlGnBu', annot=True)","8fa9a565":"sns.boxplot(data['Survived'], data['PassengerId'])","f2a9a030":"data.drop('PassengerId', axis=1, inplace=True)\ntest.drop('PassengerId', axis=1, inplace=True)","b42647f2":"test.head()","5bcc8b20":"labels = pd.DataFrame(data['Survived'], columns=['Survived'])\ndata.drop('Survived', axis=1, inplace=True)","9dbea560":"extra_tree_forest = ExtraTreesClassifier() \n  \nextra_tree_forest.fit(data, labels) \n  \nfeature_importance = extra_tree_forest.feature_importances_ \n\nplt.figure(figsize=(10, 10))\nplt.bar(data.columns, feature_importance) \nplt.xlabel('Feature Labels') \nplt.ylabel('Feature Importances') \nplt.title('Comparison of different Feature Importances') \nplt.show() ","d8f2874d":"sns.countplot(labels.Survived)","4a363617":"sns.countplot(data.Pclass, hue=labels.Survived)","d5f39a7c":"sns.boxplot(labels.Survived, data.Fare)","6b300b3d":"sns.countplot(data.Sex, hue=labels.Survived)","09477df9":"sns.countplot(data.SibSp, hue=labels.Survived)","300ec71c":"sns.countplot(data.Parch, hue=labels.Survived)","78e0c3b4":"kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n#cross validation.\ndef cross_validate(model, train, labels):\n    models = []\n    print(train.shape)\n    fold_count = 1\n\n    for train_ind, val_ind in kfold.split(train, labels):\n\n        t_data = train.loc[train_ind]\n        t_labels = labels.loc[train_ind]\n        v_data = train.loc[val_ind]\n        v_labels = labels.loc[val_ind]\n    \n        model.fit(t_data, t_labels)\n        \n        preds = model.predict(v_data)\n        \n        accuracy = accuracy_score(v_labels, preds)\n        precision = precision_score(v_labels, preds)\n        recall = recall_score(v_labels, preds)\n        f1score = f1_score(v_labels, preds)\n    \n        print(\n            ' Fold : ' + str(fold_count) +\n            ' Accuracy : ' + str(round(accuracy, 2)) +\n            ' Precision : ' + str(round(precision, 2)) +\n            ' Recall : ' + str(round(recall, 2)) + \n            ' f1score : ' + str(round(f1score, 2))\n        )\n        \n        models.append(model)\n        \n        fold_count += 1\n        \n    return models\n    \n#parameter tuning.\ndef tune(model, params, train, labels):\n    search = RandomizedSearchCV(model, params, n_iter=50, cv=5, random_state=21)\n    best_model = search.fit(train, labels)\n    pprint(best_model.best_estimator_.get_params())\n    return best_model","16c7f9f1":"model = LogisticRegression()\nlogistic_models = cross_validate(model, data, labels)","361d1bbf":"model = RandomForestClassifier()\nrf_models = cross_validate(model, data, labels)","d857ae2c":"model = xgb.XGBClassifier()\nxgb_models = cross_validate(model, data, labels)","509386d6":"model = lgb.LGBMClassifier()\nlgb_models = cross_validate(model, data, labels)","5f638b04":"params = {\n    'max_iter' : [80, 90, 100, 110, 120],\n    'random_state' : [0, 1, 42],\n    'penalty' : ['l1', 'l2']\n}\n\nmodel = LogisticRegression()\n\nbest_model = tune(model, params, data, labels)\nmodel = LogisticRegression(**best_model.best_estimator_.get_params())\ntuned_logistic_models = cross_validate(model, data, labels)","426c4e89":"params = {\n    'n_estimators' : [110, 120, 130, 140],\n    'max_depth' : [6, 7, 8],\n    'max_features' : [6, 7, 8],\n    'bootstrap' : [True],\n    'min_samples_leaf' : [2, 3]\n}\n\nmodel = RandomForestClassifier()\n\nbest_model = tune(model, params, data, labels)\nmodel = RandomForestClassifier(**best_model.best_estimator_.get_params())\ntuned_rf_models = cross_validate(model, data, labels)","365ee9f7":"params = {\n    'eta' : [0.05, 0.1],\n    'max_depth' : [6, 7],\n    'verbosity' : [1],\n    'subsample' : [0.75],\n    'n_estimators' : [110, 120, 130]\n}\n\nmodel = xgb.XGBClassifier()\nbest_model = tune(model, params, data, labels)\nmodel = xgb.XGBClassifier(**best_model.best_estimator_.get_params())\ntuned_xgb_models = cross_validate(model, data, labels)","cbee142f":"feature_imp = pd.DataFrame(sorted(zip(tuned_xgb_models[0].feature_importances_, data.columns)), columns=['Value','Feature'])\n\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('XGB Features (avg over folds)')\nplt.tight_layout()\nplt.show()","585fb443":"params = {\n    'n_estimators' : [155, 160, 165],\n    'learning_rate' : [0.05],\n    'max_depth' : [7, 8],\n    'num_leaves' : [25],\n    'min_data_in_leaf' : [15, 18, 20],\n    'bagging_fraction' : [0.5],\n    'feature_fraction' : [0.7],\n    'lambda_l2' : [0.75],\n    'subsample' : [0.5],\n}\n\nmodel = lgb.LGBMClassifier()\nbest_model = tune(model, params, data, labels)\nmodel = lgb.LGBMClassifier(**best_model.best_estimator_.get_params())\ntuned_lgb_models = cross_validate(model, data, labels)","5b54d0d3":"feature_imp = pd.DataFrame(sorted(zip(tuned_lgb_models[1].feature_importances_,data.columns)), columns=['Value','Feature'])\n\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","4e98c780":"test.head()","8471dfd5":"final_model = tuned_lgb_models[1]\nprint(final_model)","88af0c8c":"preds = final_model.predict(test)\nprint(preds)","3ae566bc":"temp = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({'PassengerId': temp.PassengerId, 'Survived': preds})\nsubmission.head()","abb149ed":"submission.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","322a97a2":"Lets fill the two null values with S as S is dominating here and its highly likely that the two persons might have boarded the ship at the same port.","af426b5b":"Importance of PassengerId seems very low. Its better to drop it to reduce the number of features by 1.","ab70898f":"Class 1 is in the minority side with almost 40% of the dataset but its fine for now.","ecd674bd":"# Correlation","17f9d4c9":"## Classes in Categorical Variables.\nI have looked at the classes in each discrete feature in both train and test set and have seen that all the classes present in the train set are also in the test set with some extra classes. By extra classes I mean there are values in test set that are not available in the train set.","6425c324":"To predict if the passanger will survive or not.","c71b3bcd":"# Model Training.\n\n### Model tested on.\n1. Logistic Regression.\n2. Random Forest.\n3. XGBoost.\n4. LightGBM.\n5. KNearstNeighbours","b80456ae":"## Feature Importance","d02f7e70":"# Predictions","b67a1c3c":"1. ### sibsp\tnumber of siblings \/ spouses aboard the Titanic\t\n2. ### parch\tnumber of parents \/ children aboard the Titanic\n\nIts most likely that a person would be worried about his\/her family and would let them off board ship before he\/she does. Like for example we have seen that woman has a high probablity of surviving than a man. Its possible that a man might have helped in any way possible his wife to off board but he could not. Hence if he is alone it may be more likely that he might have survived.","22742b5a":"# Pairplot","0e2cd2f7":"# Categorical Encoding.\n* Nominal Feature. (One Hot Encoding)\n* Ordinal Feature. (Label Encoding)","431e6020":"Most of them whose cabin is unknown hasnt survived. Hence considering a feature as cabin_known? would help the prediction.","3314cd2e":"# Check null values","648ea6d8":"## final model that I will use for predictions.","642411ad":"cabin data would not help a lot. Hence its better to remove it.","c4b182d5":"Distribution of Age is almost symmetrical if not perfectly symmetrical. Coming to Fare seems like its highly right skewed. Transforming it may help in getting some symmetry.","e7a67bc2":"# Data Balance Check","43e69a7e":"# Model Training with Parameter Tuning\nUsed RandomizedSearchCV"}}