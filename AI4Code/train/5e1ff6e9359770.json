{"cell_type":{"15279040":"code","fa8a2a54":"code","d84c494f":"code","64a31b4d":"code","459ad5f7":"code","79c91d34":"code","4d26a11e":"code","dc962ff4":"code","28e4945c":"code","b4702b70":"code","c9d0a466":"code","5671d7e1":"code","baaf1317":"code","b331744e":"code","31af5b05":"markdown","7439747a":"markdown","687bbb21":"markdown","3b744c31":"markdown","dddfc44e":"markdown","4cdef69f":"markdown","3f376957":"markdown","8d679409":"markdown","8f6061b9":"markdown","32516cc2":"markdown"},"source":{"15279040":"import pandas  as pd\nimport numpy as np\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nbold = \"\\033[1m\"","fa8a2a54":"train = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\nprint(bold + \"Training Set :\\n\")\ndisplay(train.head())\nprint(bold + str(train.shape))\n\nprint(bold + \"\\nTest Set :\\n\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\ndisplay(test.head())\nprint(bold + str(test.shape))","d84c494f":"target = \"loss\"\npredictors = [x for x in train.columns if x not in [\"id\", \"loss\"]]","64a31b4d":"X = train[predictors]\ny = train[target]\ntest = test[predictors]","459ad5f7":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","79c91d34":"from sklearn.metrics import mean_squared_error, make_scorer\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared = False)\n\nrmse_scorer = make_scorer(rmse)","4d26a11e":"kf = KFold(n_splits = 3, shuffle = True, random_state = 42)\n\ndef rmse_cv(model, X, y):    \n    return cross_val_score(model, X, y, scoring = rmse_scorer, cv = kf).mean()","dc962ff4":"import lightgbm as lgb\n\nlgb_reg = lgb.LGBMRegressor(random_state = 42, n_jobs = -1, n_estimators = 50, learning_rate = 0.3)\n\nlgb_w_subsample = lgb.LGBMRegressor(random_state = 42, n_jobs = -1, n_estimators = 50, learning_rate = 0.3, subsample = 0.5)","28e4945c":"print(bold + \"RMSE for default LightGBM model: \\t\\t\\t\" + str(rmse_cv(lgb_reg, X_train, y_train)))\n\nprint(bold + \"RMSE for LightGBM model, with setting subsample = 0.5: \\t\" + str(rmse_cv(lgb_w_subsample, X_train, y_train)))","b4702b70":"lgb_w_subsample_freq = lgb.LGBMRegressor(random_state = 42, n_jobs = -1, n_estimators = 100, learning_rate = 0.3, subsample = 0.5, subsample_freq = 1)\n\nrmse_cv(lgb_w_subsample_freq, X_train, y_train)","c9d0a466":"import xgboost as xgb\n\nxgbreg = xgb.XGBRegressor(random_state = 42, n_jobs = -1, n_estimators = 50, learning_rate = 0.5)\n\nrmse_cv(xgbreg, X_train, y_train)","5671d7e1":"xgbreg_w_subsample = xgb.XGBRegressor(random_state = 42, n_jobs = -1, n_estimators = 50, learning_rate = 0.5, subsample = 0.5)\n\nrmse_cv(xgbreg_w_subsample, X_train, y_train)","baaf1317":"from catboost import CatBoostRegressor\n\ncbr = CatBoostRegressor(random_state = 42, thread_count = 4, verbose = 0, iterations = 50, learning_rate = 0.5)\n\nrmse_cv(cbr, X_train, y_train)","b331744e":"cbr_w_subsample = CatBoostRegressor(random_state = 42, thread_count = 4, verbose = 0,  iterations = 50, learning_rate = 0.5, subsample = 0.5)\n\nrmse_cv(cbr_w_subsample, X_train, y_train)","31af5b05":"Nice, subsampling works.","7439747a":"Is there a difference? Nope.\n\n**We need to use subsample_freq to enable subsampling.**","687bbb21":"# Catboost","3b744c31":"**For XGBoost and Catboost setting subsample to a float number is enough for subsampling.**","dddfc44e":"# XGBoost","4cdef69f":"# **Conclusion**\n\n**For using subsampling;**\n\n**LightGBM**: set subsample to a float number and set subsample_freq to a positive integer.\n\n**XGBoost**: set subsample to a float number.\n\n**Catboost**: set subsample to a float number.","3f376957":"In this notebook, I want to remark the usage of subsample hyperparameter.\n\nGenerally, we use subsample or bagging fraction on hyperparameter tuning step.\n\nFor XGBoost and Catboost, we can set subsample to a float number between 0 and 1 to use subsampling. It is enough.\n\nBut, for LightGBM, it is not enough. We have to use subsample_freq additionally.\n\nLet's examine them.","8d679409":"We have 8.02 rmse with default xgboost model. If we set subsample to 0.5, this score will change, we get 8.18 rmse.","8f6061b9":"# LightGBM","32516cc2":"First one is default lightgbm regressor model.\n\nI just set a random_state for reproducibility, 50 estimator, and high learning rate for faster calculations.\n\nSecond one is default lgbm model with adding subsample.\n\nLet's look at difference"}}