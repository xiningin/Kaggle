{"cell_type":{"310562b6":"code","e477b105":"code","534e486f":"code","4bb15afb":"code","d9e5d7a5":"code","e949b4f9":"code","6a42a0ce":"code","46e78745":"code","3d51fe9b":"code","054f5ac9":"code","800d697f":"code","e7a7a060":"code","b358d5c6":"code","186651d0":"code","6a3ae069":"code","be2ba608":"code","13cc3454":"code","6cb99f24":"markdown","35c698d1":"markdown","1f7c2fce":"markdown","f2bd6a56":"markdown","de47a11f":"markdown","18dce25e":"markdown","9fb4acc6":"markdown","288b7007":"markdown","ebfa64e7":"markdown","a944976e":"markdown","dfe946d3":"markdown","096c5db0":"markdown","22f503b7":"markdown"},"source":{"310562b6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nimport csv\nimport os\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import normalized_mutual_info_score\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve","e477b105":"data = pd.read_csv('..\/input\/quora-question-pair-similarity-part-2\/nlp_features_train.csv')","534e486f":"data.head()","4bb15afb":"data.drop(columns = ['question1', 'question2'], inplace=True)","d9e5d7a5":"# after we read from sql table each entry was read it as a string\n# we convert all the features into numaric before we apply any model\ncols = list(data.columns)\nfor i in cols:\n    data[i] = data[i].apply(pd.to_numeric)\n    print(i)","e949b4f9":"y_true = data['is_duplicate']","6a42a0ce":"# https:\/\/stackoverflow.com\/questions\/7368789\/convert-all-strings-in-a-list-to-int\ny_true = list(map(int, y_true.values))","46e78745":"data.drop(columns='is_duplicate', inplace=True)","3d51fe9b":"X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)","054f5ac9":"print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)","800d697f":"print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])\/train_len,\"Class 1: \", int(train_distr[1])\/train_len)\nprint(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[1])\/test_len, \"Class 1: \",int(test_distr[1])\/test_len)","e7a7a060":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)\/(C.sum(axis=1))) = [[1\/3, 3\/7]\n    #                           [2\/3, 4\/7]]\n\n    # ((C.T)\/(C.sum(axis=1))).T = [[1\/3, 2\/3]\n    #                           [3\/7, 4\/7]]\n    # sum of row elements = 1\n    \n    B =(C\/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C\/C.sum(axis=0)) = [[1\/4, 2\/6],\n    #                      [3\/4, 4\/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","b358d5c6":"# we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https:\/\/stackoverflow.com\/a\/18662466\/4084039\n# we create a output array that has exactly same size as the CV data\npredicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","186651d0":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=0, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","6a3ae069":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=0, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","be2ba608":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","13cc3454":"predicted_y =np.array(predict_y>0.5,dtype=int)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","6cb99f24":"<h2>Random train test split( 70:30) <\/h2>","35c698d1":"<h2>Building a random model (Finding worst-case log-loss) to compare again random guess<\/h2>","1f7c2fce":"<h1>Machine Learning Models <\/h1>","f2bd6a56":"We have extracted so many features but we could still do tf-idf, word2vec over question1 and question2 but for now let's see the results based on these extracted features.","de47a11f":"## Training Model ","18dce25e":"<h2>Linear SVM<\/h2>","9fb4acc6":"#### Hope you enjoyed it if so pls. Upvote \ud83d\udc4d\ud83d\udc4d","288b7007":"<h2>XGBoost <\/h2>","ebfa64e7":"This a continuation of my Previous notebooks over Quora Oquestion Pair Similarity Analysis which you can find here - https:\/\/www.kaggle.com\/notebooks?sortBy=dateRun&group=profile&pageSize=20","a944976e":"<h2>Logistic Regression<\/h2>","dfe946d3":"<h2>Converting strings to numerics <\/h2>","096c5db0":"### Let's make a function to plot Confusion metrics ","22f503b7":"<h2>Reading data from file <\/h2>"}}