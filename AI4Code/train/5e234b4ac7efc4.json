{"cell_type":{"bc8dcb8e":"code","7ee8ec22":"code","25f9644d":"code","4a3bdfde":"code","d4db7791":"code","0f3971ca":"code","c579f434":"code","3b903225":"code","b0e21303":"code","301ebe8f":"code","56b6ec74":"code","8a22f45f":"code","29f8d8d3":"code","7be09a6e":"code","c96057ea":"code","2ca06c2e":"code","6ec4194e":"code","8e6acffc":"code","00af4f93":"code","80d23207":"code","92749b06":"code","2d26f4bb":"code","69f45e68":"code","b4add9ed":"code","b60e6953":"code","39543653":"code","0bd420bd":"code","5b031257":"code","ccf80061":"code","e8c8c671":"markdown"},"source":{"bc8dcb8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ee8ec22":"work_dir=\"..\/input\/cassava-leaf-disease-classification\"","25f9644d":"image_path=\"..\/input\/cassava-leaf-disease-classification\/train_images\"","4a3bdfde":"import os\nimport json\nimport sys\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader,Dataset\nimport albumentations as A\nfrom PIL import Image,ImageFile\nfrom tqdm import tqdm_notebook as tqdm\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nfrom sklearn import model_selection,metrics\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr\nimport torchvision.models as mdl\nimport warnings\nwarnings.simplefilter(\"ignore\")","d4db7791":"os.listdir(work_dir)","0f3971ca":"with open(work_dir+\"\/label_num_to_disease_map.json\", 'r') as file:\n    class_labels = json.load(file)\n    \nclass_labels\n","c579f434":"df=pd.read_csv(\"..\/input\/cassava-leaf-disease-classification\/train.csv\")\ndf.head()","3b903225":"plt.figure(figsize=(15,15))\ndata_sample=df.sample(16).reset_index(drop=True)\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    img = cv2.imread(work_dir+ \"\/train_images\/\" + data_sample.image_id[i])\n    image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.title(class_labels.get(str(data_sample.label[i])))\nplt.tight_layout()\nplt.show()","b0e21303":"plt.figure(figsize=(15,15))\ndata_sample1=df[df.label==0].sample(8).reset_index(drop=True)\nfor i in range(8):\n    plt.subplot(2,4,i+1)\n    img = cv2.imread(work_dir+ \"\/train_images\/\" + data_sample1.image_id[i])\n    image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.title(class_labels.get(str(data_sample1.label[i])))\nplt.tight_layout()\nplt.show()\n\n","301ebe8f":"plt.figure(figsize=(15,15))\ndata_sample2=df[df.label==1].sample(8).reset_index(drop=True)\nfor i in range(8):\n    plt.subplot(2,4,i+1)\n    img = cv2.imread(work_dir+ \"\/train_images\/\" + data_sample2.image_id[i])\n    image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.title(class_labels.get(str(data_sample2.label[i])))\nplt.tight_layout()\nplt.show()\n","56b6ec74":"plt.figure(figsize=(15,15))\ndata_sample3=df[df.label==2].sample(8).reset_index(drop=True)\nfor i in range(8):\n    plt.subplot(2,4,i+1)\n    img = cv2.imread(work_dir+ \"\/train_images\/\" + data_sample3.image_id[i])\n    image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.title(class_labels.get(str(data_sample3.label[i])))\nplt.tight_layout()\nplt.show()\n\n","8a22f45f":"plt.figure(figsize=(15,15))\ndata_sample4=df[df.label==3].sample(8).reset_index(drop=True)\nfor i in range(8):\n    plt.subplot(2,4,i+1)\n    img = cv2.imread(work_dir+ \"\/train_images\/\" + data_sample4.image_id[i])\n    image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.title(class_labels.get(str(data_sample4.label[i])))\nplt.tight_layout()\nplt.show()\n","29f8d8d3":"plt.figure(figsize=(15,15))\ndata_sample5=df[df.label==4].sample(8).reset_index(drop=True)\nfor i in range(8):\n    plt.subplot(2,4,i+1)\n    img = cv2.imread(work_dir+ \"\/train_images\/\" + data_sample5.image_id[i])\n    image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.title(class_labels.get(str(data_sample5.label[i])))\nplt.tight_layout()\nplt.show()\n","7be09a6e":"df_train,df_valid=model_selection.train_test_split(df,test_size=0.05,random_state=42,stratify=df.label.values)","c96057ea":"df_train.shape,df_valid.shape","2ca06c2e":"df_train=df_train=df_train.reset_index(drop=True)\ndf_valid=df_valid.reset_index(drop=True)","6ec4194e":"train_image_path=[os.path.join(image_path,k) for k in df_train.image_id.values]\nvalid_image_path=[os.path.join(image_path,k) for k in df_valid.image_id.values]","8e6acffc":"train_image_path[:5]","00af4f93":"valid_image_path[:5]","80d23207":"train_targets=df_train.label.values\nvalid_targets=df_valid.label.values","92749b06":"train_targets","2d26f4bb":"valid_targets","69f45e68":"class CassavaDataset(Dataset):\n    def __init__(self,image_ids,labels,dimension=None,augmentations=None):\n        super().__init__()\n        self.image_ids=image_ids\n        self.labels=labels\n        self.dim=dimension\n        self.augmentations=augmentations\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self,idx):\n        imge=cv2.imread(self.image_ids[idx])\n        imge=cv2.cvtColor(imge,cv2.COLOR_BGR2RGB)\n        \n        if self.dim:\n            imge=cv2.resize(imge,self.dim)\n            \n        if self.augmentations:\n            aug_img=self.augmentations(image=imge)\n            imge=aug_img[\"image\"]\n            \n        return {\"image\":transforms.ToTensor()(imge),\n                \"label\":torch.tensor(self.labels[idx])}    \n        ","b4add9ed":"image_size = 256\ntrain_aug= A.Compose([A.RandomCrop(height = 500, width = 500 ) ,\n                          A.Transpose(p=0.3) , \n                          A.VerticalFlip(p=0.5),\n                          A.HorizontalFlip(p=0.5),\n                          A.RandomContrast(limit=0.05, p=0.5),\n                          A.OneOf([ A.MedianBlur(blur_limit=3),\n                                    #A.GaussianBlur(blur_limit=3),\n                                    A.GaussNoise(var_limit=(5.0, 30.0)) ,], p=0.6),\n                          A.OneOf([ A.OpticalDistortion(distort_limit=0.7), \n                                    A.GridDistortion(num_steps=2, distort_limit=0.2),\n                                    A.ElasticTransform(alpha=3),  ], p=0.7),\n                          A.CLAHE(clip_limit=4.0, p=0.7),\n                          A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5) , \n                          A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.8),\n                          A.Cutout(max_h_size=int(image_size * 0.2), max_w_size=int(image_size * 0.2), num_holes=1, p=0.5), \n                          A.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=3, p=0.5), \n                          A.Resize(image_size ,image_size )])\n\n\n\n","b60e6953":"valid_aug = A.Compose([A.RandomCrop(height = 500, width = 500 ) ,\n                          A.Transpose(p=0.3) , \n                          A.VerticalFlip(p=0.5),\n                          A.HorizontalFlip(p=0.5),\n                          A.RandomContrast(limit=0.05, p=0.5),\n                          A.OneOf([ A.MedianBlur(blur_limit=3),\n                                    #A.GaussianBlur(blur_limit=3),\n                                    A.GaussNoise(var_limit=(5.0, 30.0)) ,], p=0.6),\n                          A.OneOf([ A.OpticalDistortion(distort_limit=0.7), \n                                    A.GridDistortion(num_steps=2, distort_limit=0.2),\n                                    A.ElasticTransform(alpha=3),  ], p=0.7),\n                          A.CLAHE(clip_limit=4.0, p=0.7),\n                          A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5) , \n                          A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.8),\n                          A.Cutout(max_h_size=int(image_size * 0.2), max_w_size=int(image_size * 0.2), num_holes=1, p=0.5), \n                          A.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=3, p=0.5), \n                          A.Resize(image_size ,image_size )])\n","39543653":"train_dataset=CassavaDataset(image_ids=train_image_path,labels=train_targets,dimension=None,augmentations=train_aug)\nvalid_dataset=CassavaDataset(image_ids=valid_image_path,labels=valid_targets,dimension=None,augmentations=valid_aug)\n","0bd420bd":"plt.figure(figsize=(15,15))\n\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    image,label=train_dataset[i][\"image\"],train_dataset[i][\"label\"]\n    plt.imshow(image.permute(1, 2, 0))\n    plt.axis('off')\n    plt.title(class_labels.get(str(label.item())))\n\n\nplt.tight_layout()\nplt.show()\n\n    \n    ","5b031257":"train_loader = torch.utils.data.DataLoader(\n                                           train_dataset,\n                                           batch_size=16,\n                                           num_workers=4,\n                                           shuffle=False,\n                                           pin_memory=False,\n                                           drop_last=False,\n                                           )\n","ccf80061":"valid_loader = torch.utils.data.DataLoader(\n                                           valid_dataset,\n                                           batch_size=16,\n                                           num_workers=4,\n                                           shuffle=False,\n                                           pin_memory=False,\n                                           drop_last=False,\n                                           )","e8c8c671":"**IMAGE VISUALIZATION**"}}