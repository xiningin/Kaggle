{"cell_type":{"77e9925b":"code","3dd92a20":"code","f95a2997":"code","66f68cc1":"code","e5c614a3":"code","8b624c10":"code","5cbe4f23":"code","0e4000ef":"code","ee2e6cc3":"code","edcb6911":"code","296c8aad":"code","b5579a00":"code","25553f5c":"code","09369c53":"code","9bca2450":"code","61b723c2":"code","5adb99f8":"code","9d18cb75":"code","e3c9deeb":"code","232a1dbc":"code","3e64cda5":"code","b0d80ce4":"code","a7b26188":"code","2041d269":"code","73168f55":"code","f1e4bbdc":"code","afe0b9ba":"code","bb45ae1f":"code","01033cf5":"code","50a027c6":"code","7bca2d04":"code","2e856b3b":"code","ef6bd95c":"code","3fc16dd1":"code","c832aea9":"code","e3c9c14f":"code","9d98661a":"code","90e2b8e4":"code","cc7e0f35":"code","7ef93c98":"code","7dd53c2a":"code","474944e4":"code","491a5ba3":"code","c3ffb14e":"code","b1fbe75d":"code","f7cd6605":"code","8f5bacf0":"markdown","1289a341":"markdown","3f1826ee":"markdown","aaaaaf1a":"markdown","2494db1d":"markdown","825f5cf2":"markdown","66153278":"markdown","549bdced":"markdown","82b1e2aa":"markdown","9caf1867":"markdown","e1f156f8":"markdown","3d520613":"markdown","1f9cc98a":"markdown","cb68a283":"markdown","d8cfe627":"markdown","6c91abd2":"markdown","2f7bc1fb":"markdown","339d13ec":"markdown","9b4a814c":"markdown","7213e4f6":"markdown","51b66f27":"markdown","1a920a9e":"markdown","0aa1a559":"markdown","1f1b1313":"markdown","fc2f28b5":"markdown","e0ac4622":"markdown","71d5076b":"markdown","b7de754c":"markdown","06d0b1ea":"markdown","2dcf6029":"markdown","2756ff02":"markdown","65ab10e4":"markdown","a0db7ded":"markdown","c0a3ba4e":"markdown","c04ae157":"markdown","fcfc9846":"markdown","62503c87":"markdown","3fb088b7":"markdown","c429a00e":"markdown","320fe179":"markdown","908f45b1":"markdown","2f451c6e":"markdown","a45f4afe":"markdown","6b581320":"markdown","4645e68c":"markdown","736dcb8d":"markdown","9f98e540":"markdown","cef9aa79":"markdown"},"source":{"77e9925b":"import pandas as pd\nimport numpy as np\nimport matplotlib\nmatplotlib.use('nbagg')\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nimport re\nimport scipy\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nimport math\nimport joblib","3dd92a20":"print(\"Reading Data\")\n\ntrain = pd.read_csv('..\/input\/mercari-price-suggestion-challenge\/train.tsv', sep='\\t')\ntest = pd.read_csv('..\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv', sep='\\t')\n\nprint(\"Shape of train data: \",train.shape)\nprint(\"Shape of test data: \",test.shape)","f95a2997":"y_train = np.log1p(train[\"price\"])","66f68cc1":"NUM_BRANDS = 2500\nNAME_MIN_DF = 10\nMAX_FEAT_DESCP = 50000","e5c614a3":"print('No of duplicates in train: {}'.format(sum(train.duplicated())))\nprint('No of duplicates in test : {}'.format(sum(test.duplicated())))","8b624c10":"train.isnull().any()","5cbe4f23":"print('We have {} NaN\/Null values in train'.format(train.isnull().values.sum()))\nprint('We have {} NaN\/Null values in test'.format(test.isnull().values.sum()))","0e4000ef":"train[\"category_name\"] = train[\"category_name\"].fillna(\"Other\").astype(\"category\")\ntrain[\"brand_name\"] = train[\"brand_name\"].fillna(\"unknown\")\n\ntest[\"category_name\"] = test[\"category_name\"].fillna(\"Other\").astype(\"category\")\ntest[\"brand_name\"] = test[\"brand_name\"].fillna(\"unknown\")\n\ntop_brands = train[\"brand_name\"].value_counts().index[:NUM_BRANDS]\ntrain.loc[~train[\"brand_name\"].isin(top_brands), \"brand_name\"] = \"Other\"\ntest.loc[~test[\"brand_name\"].isin(top_brands), \"brand_name\"] = \"Other\"\n\ntrain[\"item_description\"] = train[\"item_description\"].fillna(\"None\")\ntrain[\"brand_name\"] = train[\"brand_name\"].astype(\"category\")\n\ntest[\"item_description\"] = test[\"item_description\"].fillna(\"None\")\ntest[\"brand_name\"] = test[\"brand_name\"].astype(\"category\")","ee2e6cc3":"train_cond_id = Counter(list(train['item_condition_id']))\ntest_cond_id = Counter(list(test['item_condition_id']))\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n\nax1.bar(train_cond_id.keys(), train_cond_id.values(), width=0.2, align='edge', label='Train')\nax1.set_xticks([1,2,3,4,5])\nax1.set_xlabel('item_condition_id')\nax1.legend()\n\nax2.bar(test_cond_id.keys(), test_cond_id.values(), width=-0.2, align='edge', label='Test')\nax2.set_xticks([1,2,3,4,5])\nax2.set_xlabel('item_condition_id')\nax2.legend()\n\nfig.show()","edcb6911":"print(train['category_name'].describe())\ncategory_nam = Counter(list(train['category_name']))","296c8aad":"print(\"Top 15 category in train data: \")\ncategory_nam.most_common(15)","b5579a00":"print(test['category_name'].describe())\ncategory_nam = Counter(list(test['category_name']))","25553f5c":"print(\"Top 15 category in test data: \")\ncategory_nam.most_common(15)","09369c53":"print(train['brand_name'].describe())\nbrand_nam = Counter(list(train['brand_name']))\n","9bca2450":"print(\"Top 15 brands in train data: \")\nbrand_nam.most_common(15)","61b723c2":"print(test['brand_name'].describe())\nbrand_nam = Counter(list(test['brand_name']))\n","5adb99f8":"print(\"Top 15 brands in test data: \")\nbrand_nam.most_common(15)","9d18cb75":"train.price.describe()","e3c9deeb":"fig, ax = plt.subplots( figsize = (10, 5))\nax.hist(train.price, bins = 100, color = \"blue\")\nax.set_title(\"\\n \\n  Histogram \", fontsize = 15)\nax.set_xlabel(\" Price\", fontsize = 10)\nplt.title(\"Distribution of Price\")\nplt.show()","232a1dbc":"train_ship = Counter(list(train['shipping']))\ntest_ship = Counter(list(test['shipping']))\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n\nax1.bar(train_ship.keys(), train_ship.values(), width=0.1, align='edge', label='Train')\nax1.set_xticks([0,1])\nax1.set_xlabel('shipping')\nax1.legend()\n\nax2.bar(test_ship.keys(), test_ship.values(), width=-0.1, align='edge', label='Test')\nax2.set_xticks([1,0])\nax2.set_xlabel('shipping')\nax2.legend()\n\nfig.show()","3e64cda5":"print(\"Encodings\")\ncount_nm = CountVectorizer(min_df=NAME_MIN_DF)\ntrain_name_vec = count_nm.fit_transform(train[\"name\"])\ntest_name_vec = count_nm.transform(test[\"name\"])\nprint(\"Shape of train Name feature: \",train_name_vec.shape)\nprint(\"Shape of test Name feature: \",test_name_vec.shape)","b0d80ce4":"print(\"Descp encoders\")\ncount_desc = TfidfVectorizer(max_features = MAX_FEAT_DESCP, \n                              ngram_range = (1,3),\n                              stop_words = \"english\")\ntrain_desc_vec = count_desc.fit_transform(train[\"item_description\"])\ntest_desc_vec = count_desc.transform(test[\"item_description\"])\nprint(\"Shape of train Name feature: \",train_desc_vec.shape)\nprint(\"Shape of test Name feature: \",test_desc_vec.shape)","a7b26188":"print(\"Category Encoders\")\nunique_categories = pd.Series(\"\/\".join(train[\"category_name\"].unique().astype(\"str\")).split(\"\/\")).unique()\ncount_category = CountVectorizer()\nencoder_cat_train = count_category.fit_transform(train[\"category_name\"])\nencoder_cat_test= count_category.transform(test[\"category_name\"])","2041d269":"print(encoder_cat_train.shape)\nprint(encoder_cat_test.shape)","73168f55":"from sklearn.preprocessing import LabelBinarizer\n\nprint(\"Brand encoders\")\nvect_brand = LabelBinarizer(sparse_output=True)\n\nencoder_brnd_train = vect_brand.fit_transform(train[\"brand_name\"])\nencoder_brnd_test= vect_brand.transform(test[\"brand_name\"])","f1e4bbdc":"print(encoder_brnd_train.shape)\nprint(encoder_brnd_test.shape)","afe0b9ba":"X_train = scipy.sparse.hstack((\n                         train_desc_vec,\n                         encoder_brnd_train,\n                         encoder_cat_train,\n                         train_name_vec,\n                         np.array(train['item_condition_id']).reshape(-1,1),\n                         np.array(train['shipping']).reshape(-1,1)\n                        )).tocsr()\nprint(X_train.shape)","bb45ae1f":"X_test = scipy.sparse.hstack((\n                         test_desc_vec,\n                         encoder_brnd_test,\n                         encoder_cat_test,\n                         test_name_vec,\n                         np.array(test['item_condition_id']).reshape(-1,1),\n                         np.array(test['shipping']).reshape(-1,1)\n)).tocsr()\nprint(X_test.shape)","01033cf5":"from xgboost import XGBRegressor\n# from sklearn.model_selection import GridSearchCV\n\n# params = { \n#           'gamma':[i\/10.0 for i in range(3,8,2)],  \n#           'max_depth': [4,8,16]}\n\n# xgb = XGBRegressor() \n\n# grid = GridSearchCV(estimator=xgb, param_grid=params, n_jobs=-1, cv=2, verbose=3)\n# grid.fit(X_train, y_train)\n# print(\"Best estimator : \", grid.best_estimator_)\n# print(\"Best Score : \", grid.best_score_)","50a027c6":"xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bytree=1, gamma=0.7, learning_rate=0.1, max_delta_step=0,\n             max_depth=16, min_child_weight=1, missing=None, n_estimators=100,\n             n_jobs=-1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, \n             seed=None, silent=True, subsample=1)\n\nprint(\"Fitting Model 1\")\nxgb.fit(X_train, y_train)","7bca2d04":"y_pred = xgb.predict(X_test)    ","2e856b3b":"from sklearn.linear_model import RidgeCV\n\nmodel = RidgeCV(fit_intercept=True, alphas=[5.0], normalize=False, cv = 2, scoring='neg_mean_squared_error')\n\n\nprint(\"Fitting Model\")\nmodel.fit(X_train, y_train)","ef6bd95c":"preds = model.predict(X_test)","3fc16dd1":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom keras.preprocessing.text import Tokenizer","c832aea9":"full_df = pd.concat([train, test])","e3c9c14f":"print(\"Processing categorical data...\")\nle = LabelEncoder()\n\nle.fit(full_df.category_name)\nfull_df.category_name = le.transform(full_df.category_name)\n\nle.fit(full_df.brand_name)\nfull_df.brand_name = le.transform(full_df.brand_name)\n\ndel le","9d98661a":"print(\"Transforming text data to sequences...\")\nraw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\n\nprint(\"   Transforming text to sequences...\")\nfull_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\nfull_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n\ndel tok_raw","90e2b8e4":"# Define constants to use when define RNN model\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([\n    np.max(full_df.seq_name.max()),\n    np.max(full_df.seq_item_description.max()),\n]) + 4\nMAX_CATEGORY = np.max(full_df.category_name.max()) + 1\nMAX_BRAND = np.max(full_df.brand_name.max()) + 1\nMAX_CONDITION = np.max(full_df.item_condition_id.max()) + 1","cc7e0f35":"def get_keras_data(df):\n    X = {\n        'name': pad_sequences(df.seq_name, maxlen=MAX_NAME_SEQ),\n        'item_desc': pad_sequences(df.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n        'brand_name': np.array(df.brand_name),\n        'category_name': np.array(df.category_name),\n        'item_condition': np.array(df.item_condition_id),\n        'num_vars': np.array(df[[\"shipping\"]]),\n    }\n    return X\n# Calculate number of train\/dev\/test examples.\nn_trains = train.shape[0]\nn_tests = test.shape[0]\n\ntrain = full_df[:n_trains]\ntest = full_df[n_trains:]\n\nX_train = get_keras_data(train)\nX_test = get_keras_data(test)","7ef93c98":"from keras import optimizers\ndef new_rnn_model(lr=0.001, decay=0.0):    \n    # Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n\n    # Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 20)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n\n    # rnn layers\n    rnn_layer1 = GRU(16) (emb_item_desc)\n    rnn_layer2 = GRU(8) (emb_name)\n\n    # main layers\n    main_l = concatenate([\n        Flatten() (emb_brand_name),\n        Flatten() (emb_category_name),\n        item_condition,\n        rnn_layer1,\n        rnn_layer2,\n        num_vars,\n    ])\n\n    main_l = Dense(256)(main_l)\n    main_l = Activation('elu')(main_l)\n\n    main_l = Dense(128)(main_l)\n    main_l = Activation('elu')(main_l)\n\n    main_l = Dense(64)(main_l)\n    main_l = Activation('elu')(main_l)\n\n    # the output layer.\n    output = Dense(1, activation=\"linear\") (main_l)\n\n    model = Model([name, item_desc, brand_name , category_name, item_condition, num_vars], output)\n\n    optimizer = optimizers.Adam(lr=lr, decay=decay)\n    model.compile(loss=\"mse\", optimizer=optimizer)\n\n    return model\n\nmodel = new_rnn_model()\nmodel.summary()\ndel model","7dd53c2a":"# Set hyper parameters for the model.\nBATCH_SIZE = 1024\nepochs = 2\n\n# Calculate learning rate decay.\nexp_decay = lambda init, fin, steps: (init\/fin)**(1\/(steps-1)) - 1\nsteps = int(n_trains \/ BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.007, 0.0005\nlr_decay = exp_decay(lr_init, lr_fin, steps)\n\nrnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n\nprint(\"Fitting RNN model to training examples...\")\nrnn_model.fit(\n        X_train, y_train, epochs=epochs, batch_size=BATCH_SIZE, verbose=2)","474944e4":"preds = rnn_model.predict(X_test, batch_size=BATCH_SIZE)","491a5ba3":"test[\"price\"] = np.expm1(preds)\ntest[\"test_id\"] = pd.to_numeric(test[\"test_id\"], downcast='integer')\ntest[[\"test_id\", \"price\"]].to_csv(\"submission.csv\", index = False)","c3ffb14e":"res = pd.read_csv(\"submission.csv\")\nres.head()","b1fbe75d":"type(res[\"test_id\"][0])","f7cd6605":"from prettytable import PrettyTable\ntable = PrettyTable()\ntable.title = \"Comparison of Models\"\ntable.field_names = [ \"Model\",\" RMLSE \"]\ntable.add_row([\"XGBRegressor\",\"0.52\"])\ntable.add_row([\"RidgeCV Regressor\",\"0.46\"])\ntable.add_row([\"RNN Model\",\"0.43\"])\nprint(table)","8f5bacf0":"<h2> 3.1 Reading Data<\/h2> ","1289a341":"<h3> 2.2.1 Type of Machine Learning Problem <\/h3>\n\n","3f1826ee":"<h1> 3. Exploratory Data Analysis <\/h1>","aaaaaf1a":"<h2> 1. Business Problem<\/h2>","2494db1d":">> <h4> 3.4.1.2 Name feature<\/h4>","825f5cf2":"<h2> 3.3 Univariate Data Analysis <\/h2>","66153278":"<h2>3.2 Data Cleaning<\/h2>","549bdced":"> <h3>4.2 Ridge Regressor<\/h3>","82b1e2aa":"<h3> 2.1.1 Data Overview <\/h3>","9caf1867":"<h3> 3.2.1 Check for Duplicates<\/h3>\n","e1f156f8":"<h3> 3.3.4 Feature : price <\/h3>","3d520613":"<h2> <font color='grey'>1.  Business Problem: <\/font><\/h2>\nIt covers the basic details which should be known before solving the case study.<br>\n<p>\n**1.1. Problem Description:** describes the background details of the Mercari shopping app which is must to know to get the insights.<br>\n**1.2. Problem Statemtent:** describes the problem which we are intended to solve.<br>\n**1.3. Business Objectives and Constraints:** describes the objectives which we have to keep in mind while solving the problem. We need to give proper attention towards the constraints stated under this.\n<\/p>","1f9cc98a":">> <h4> 3.4.1.2 Description feature<\/h4>","cb68a283":"<h3> 2.2.2 Performance metric <\/h3>","d8cfe627":"<h2> <font color='grey'> 4. Machine Learning Models<\/font><\/h2>\n<br>\n*4.1. Applying GridSearchCV for XGBRegressor*<br>\n*4.2. Applying RidgeCV Regressor*<br>\n*4.2. Applying RNN Model*<br>","6c91abd2":"<h2> <font color='grey'>3. Exploratory Data Analysis:<\/font><\/h2>\n<p>\n**3.1 Reading the Data **<br>\n**3.2 Data Cleaning**<br>\n*3.2.1 Checking for duplicates:* removing any duplicates if present<br>\n*3.2.2 Checking for NaN\/null values*<br>\n<\/p>\n<p>\n**3.3 Univariate Data Analysis **<br>\n*3.3.1 Feature : item_condition_id  *<br>\n*3.3.2 Feature : category_name* <br>\n*3.3.3 Feature : brand_name  * <br>\n*3.3.4 Feature : price * <br>\n*3.3.5 Feature : shipping* \n<\/p>\n<p>\n**3.4 Preprocessing** <br>\n*3.4.1 Preprocessing Textual Features *<br>\n*3.4.2 Preprocessing Categorical Features* <br>\n*3.4.3 Numerical Features* <br>\n<\/p>","2f7bc1fb":"<h1>4. Modelling<\/h1>","339d13ec":"> <h2> 4.3 RNN Model<\/h2>","9b4a814c":"<h3>1.3 Business Objective and constraint<\/h3>\n\n__Objectives__:\n1. Predict the the right product prices based on product category, brand name, etc.\n2. Minimize the RMSLE.\n\n__Constraints__:\n1. Some form of interpretability.\n","7213e4f6":"<h3> 3.3.1 Feature : item_condition_id <\/h3>","51b66f27":"> <h3> 3.4.1 Preprocessing Textual Features<\/h3>","1a920a9e":"<h1> 2. Machine Learning Problem <\/h1>","0aa1a559":"### Observation:\n* This RidgeCV model gave the LB Score of 0.46610","1f1b1313":"#### Observation:\nData of item_condition_id is equally distributed across train and test data","fc2f28b5":"<p> Get the data from : https:\/\/www.kaggle.com\/c\/mercari-price-suggestion-challenge\/data <\/p>\n<p> Data files : \n<ul> \n<li> train.tsv: It has 1,482,535 rows and 8 columns. <\/li>\n<li> test.tsv: It has 693,359 rows and 7 columns ('price' is excluded). <\/li>\n<\/ul>\n<br>\nThe files consist of a list of product listings. These files are tab-delimited.\n<br>\n<pre>\n1. train_id or test_id : the id of the listing\n\n2. name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]\n\n3. item_condition_id - the condition of the items provided by the seller\n\n4. category_name - category of the listing\n\n5. brand_name\n\n6. price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict.\nshipping - 1 if shipping fee is paid by seller and 0 by buyer\n\n7. item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]\n<\/pre>","e0ac4622":"__ Reference :__\n\nhttps:\/\/www.kaggle.com\/c\/mercari-price-suggestion-challenge\/overview\n\n","71d5076b":"<h3>Importing required libraries<\/h3>","b7de754c":"The problem that is to be solved is to predict the valid price for the products sold online. Thus, this is a __Regression Problem__.","06d0b1ea":"<h3> 1.1 Problem Description:<\/h3>\n<p>\nIt is hard to interpret a product's price as small details can mean big differences in pricing.\n<\/p>\n<p>\nProduct pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.\n<\/p>\n<p>\nMercari, Japan\u2019s biggest community-powered shopping app, knows this problem deeply. They\u2019d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace.\n<\/p>\n<p>\nSo, our task is to implement an algorithm which could automatically predict the prices of the products.\n<\/p>","2dcf6029":"> <h3>4.1 XGB Regressor<\/h3>","2756ff02":"<h2>3.4 Data Preprocessing<\/h2>","65ab10e4":"<h3> 3.3.2 Feature : category_name <\/h3>","a0db7ded":"<h3>1.2 Problem Statement<\/h3>\n<p>\nWe are given user-inputted text descriptions of the products, including details like product category name, brand name, and item condition. Objective is to build an algorithm that automatically suggests the right product prices. \n<\/p>","c0a3ba4e":"<h2> <font color='blue'> Procedure to solve the problem <\/font><\/h2>","c04ae157":"<h3> 3.3.5 Feature : shipping <\/h3>","fcfc9846":"<h2>2.2 Mapping the real world problem to a Machine Learning Problem <\/h2>","62503c87":"> <h3> 3.4.2 Preprocessing Categorical Features<\/h3>","3fb088b7":"<h1><font color = 'blue'> Mercari Price Suggestion Challenge<\/font><\/h1>","c429a00e":">> <h4> category_name<\/h4>","320fe179":"<h2> <font color='grey'>2. Machine Learning problem:<\/font><\/h2>\nLooking into the problem as a Machine learning problem.\n<p>\n**2.1 Data Overview:** Understanding the data and the data fields.<br>\n**2.2 Mapping the real-world problem to a Machine Learning Problem:** <br>\n_2.2.1 Type of Machine Learning Problem:_ Understand the type of problem i.e. classification (binary classification, Multi-class classification, Multi-label classification), regression, etc. This is a  Regression Problem.<br>\n_2.2.2 Performance Metric:_ Percieve the appropriate metric for this problem.\n<\/p>\n","908f45b1":"<h3> 3.3.3 Feature : brand_name <\/h3>","2f451c6e":"<h3> 3.2.2 Checking for NaN\/null values<\/h3>","a45f4afe":"### Observation:\n* This XGBRegressor model gave the LB Score of 0.52026 ","6b581320":"Root Mean Squared Logarithmic Error: \nhttps:\/\/www.kaggle.com\/c\/mercari-price-suggestion-challenge\/overview\/evaluation","4645e68c":"<h2>2.1 Data <\/h2>","736dcb8d":">> <h4> brand_name<\/h4>","9f98e540":"<h2> <font color='grey'> 5. Conclusion<\/font><\/h2>\n* **RNN Model** has RMLSE of 0.43176 and thus, it is a best model.<br>\n\n","cef9aa79":"<h1>5. Conclusion<\/h1>"}}