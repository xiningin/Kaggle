{"cell_type":{"638da10e":"code","79744443":"code","c2f8d487":"code","4884ddaa":"code","db191a41":"code","318c8cfe":"code","8ba11406":"code","57a0cb0c":"code","c9897bb8":"code","5bab36ff":"markdown","cff6aae9":"markdown","291340cc":"markdown","8b45a941":"markdown","0c0d04c9":"markdown","2dc1a856":"markdown","7981a539":"markdown","e3e9e4a0":"markdown","8215dc74":"markdown","aca230f4":"markdown"},"source":{"638da10e":"import pandas as pd\n\niris_df = pd.read_csv('..\/input\/iris\/Iris.csv')\niris_df = iris_df.set_index('Id')\niris_df['Species'] = iris_df['Species'].astype(str)\niris_df","79744443":"from typing import NamedTuple, List\nimport numpy as np\n\nVector = np.ndarray\n\nclass LabelPoint(NamedTuple):\n    point: Vector\n    label: str\n        \nspeciest_list: List[LabelPoint] = []\n\nfor _, row in iris_df.iterrows():\n    speciest_list.append(LabelPoint(row.iloc[:-1].to_numpy(), row.iat[-1]))","c2f8d487":"def euclidean_distance(v: Vector, w: Vector) -> float:\n    if len(w) == len(v):\n        return np.sqrt(np.sum(np.square(v-w)))\n    else: \n        raise Exception(f'Vectors lengths are not the same: {len(w)} != {len(v)}') \n        \ndef manhattan_distance(v: Vector, w: Vector) -> float:\n    if len(w) == len(v):\n        return np.sum(v-w)\n    else: \n        raise Exception(f'Vectors lengths are not the same: {len(w)} != {len(v)}') ","4884ddaa":"from typing import List\nfrom collections import Counter\nfrom enum import Enum, unique\n\n@unique\nclass DistanceType(Enum):\n    manhattan = 1\n    euclidean = 2 \n\ndef knn_classification(labeled_points: List[LabelPoint], new_point: Vector, k: int = 5, distance_type: DistanceType = DistanceType.euclidean) -> str:\n    \"\"\"\n    Classification of checking point:\n    1\u00b0Sorting all points by distance to new point\n    2\u00b0Choosing the most common label from the 'k' the nearest points \n    \"\"\"\n    \n    if distance_type == DistanceType.euclidean:\n        by_distance = sorted(labeled_points, key=lambda lp: euclidean_distance(lp.point, new_point.point))\n    else:\n        by_distance = sorted(labeled_points, key=lambda lp: manhattan_distance(lp.point, new_point.point))\n        \n    k_nearest_labels = [lp.label for lp in by_distance[:k]]\n    \n    return majority_vote(k_nearest_labels)\n    \ndef majority_vote(labels: List[str]) -> str:\n    \"\"\"\n    Choosing the most common label from the 'k' nearest points\n    if there are several most common labels, then the function is called \n    again with the 'k-1' nearest points\n    \"\"\"\n    vote_counts = Counter(labels)\n    winner, winner_count = vote_counts.most_common(1)[0]\n    num_winners = len([count for count in vote_counts.values() if count == winner_count])\n\n    if num_winners == 1:                \n        return winner\n    else:\n        return majority_vote(labels[:-1])","db191a41":"from random import shuffle\nimport math\n\nshuffle(speciest_list)\ntraining_speciest_list = speciest_list[:math.floor(len(speciest_list)*0.75)]\ntest_speciest_list = speciest_list[math.floor(len(speciest_list)*0.75):]\n\ncounter_euc_own = 0\ncounter_man_own = 0\n\nfor test_speciest in test_speciest_list:\n    predicted_label_euc = knn_classification(training_speciest_list, test_speciest)\n    predicted_label_man = knn_classification(training_speciest_list, test_speciest, distance_type=DistanceType.manhattan)\n    \n    if test_speciest.label == predicted_label_euc:\n        counter_euc_own += 1\n    if test_speciest.label == predicted_label_man:\n        counter_man_own += 1\n \nown_results = {\"own - euclidean\": counter_euc_own\/len(test_speciest_list) * 100,\n               \"own - manhattan\": counter_man_own\/len(test_speciest_list) * 100}","318c8cfe":"test_speciest_labels = []\ntraining_speciest_labels = [] \n\ntest_speciest_points = np.empty((1, len(test_speciest_list[0].point)), dtype = np.float64)\ntraining_speciest_points = np.empty((1, len(training_speciest_list[0].point)), dtype = np.float64)\n\nfor training_speciest in training_speciest_list:\n    \n    training_speciest_labels.append(training_speciest.label)\n    training_speciest_points = np.append(training_speciest_points, np.array([training_speciest.point]), axis = 0)\n    \nfor test_speciest in test_speciest_list:\n    \n    test_speciest_labels.append(test_speciest.label)\n    test_speciest_points = np.append(test_speciest_points, np.array([test_speciest.point]), axis = 0)\n \ntest_speciest_points = np.delete(test_speciest_points, 0, 0)\ntraining_speciest_points = np.delete(training_speciest_points, 0, 0) \n\n#species","8ba11406":"from sklearn import neighbors\nfrom collections import defaultdict\nimport json\nimport itertools\n\nscikit_results = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n\n\nfor weights, algorithm, metric in itertools.product([\"uniform\", \"distance\"], \n                                                    [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], \n                                                    [\"euclidean\", \"manhattan\", \"chebyshev\"]):\n    clf = neighbors.KNeighborsClassifier(n_neighbors=5, weights=weights, algorithm=algorithm, metric=metric)\n    clf.fit(training_speciest_points, training_speciest_labels)\n    prediction_results = clf.predict(test_speciest_points)\n    for test_speciest, predict_speciest in zip(test_speciest_labels, prediction_results.tolist()):\n        if test_speciest == predict_speciest:\n            scikit_results[weights][algorithm][metric] += 1\n            \nparamithers_combination = len(weights) * len(algorithm) * len(metric)\n                    \n#print(json.dumps(scikit_results, indent=2))","57a0cb0c":"from matplotlib import pyplot as plt\nimport matplotlib.colors as colors\nimport matplotlib.cm as cmx\n\nsklearn_labels = []\nsklearn_results = []\n\n#Preparing data for presentaion\nfor weights in scikit_results.keys():\n    for algorithm in scikit_results[weights].keys():\n        for metric in scikit_results[weights][algorithm].keys():\n            sklearn_labels.append(f\"{weights}-{algorithm}-{metric}\")\n            sklearn_results.append(scikit_results[weights][algorithm][metric]\/len(test_speciest_list)*100)\n            \n#Preparing colormap for sklern results          \ncm = plt.get_cmap('inferno')\ncNorm  = colors.Normalize(vmin=0, vmax=len(sklearn_results))\nscalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)\ncolorVal = [scalarMap.to_rgba(idx) for idx in range(len(sklearn_results))]\n\n#Preparing colormap for own results \ncm = plt.get_cmap('tab20c')\ncNorm  = colors.Normalize(vmin=0, vmax=2)\nscalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)\ncolorVal_own = [scalarMap.to_rgba(idx) for idx in range(len(own_results.keys()))]\n\n#Colors preparing\ncolor = colorVal\ncolor.append((0.0, 0.0, 0.0, 0.0))\ncolor += colorVal_own\n\n#Labels preparing\nlabels = sklearn_labels\nlabels.append(\"\")\nlabels += own_results.keys()\n\n#Results number\nresults = sklearn_results \nresults.append(0) \nresults += own_results.values()\n\nresults_number = len(results)","c9897bb8":"plt.figure(1)\nplt.bar(range(results_number), results, color=color)\nplt.xticks(range(results_number), labels, rotation=\"vertical\")\nplt.axis([None, None, 0, 100])\nplt.ylabel(\"Accuracy [%]\")\nplt.title(\"Compare the results for own algorithms and sklearn\")\nplt.show()","5bab36ff":"## Own KNN implementation \nIn order to define affiliation new unknown iris species the distance between known species and new one are calculated. Irises are sorted from the nearest to the farthest. Based on `k` nearest irises the affiliation of the new one is chosen.","cff6aae9":"## A measure of similarity \nKNN classification is based on similarity. The measure of similarity is the distance between point in 4D space. Thera are different types of distances:\n* Euclidean distance:\n\n$d(\\textbf{u,v})=\\sqrt{ \\sum_{j=1}^{n} (u_j-v_j)^2 } $\n* Manhattan distance:\n\n$d(\\textbf{u,v})=| \\sum_{j=1}^{n} (u_j-v_j) | $","291340cc":"## Algorithms Scikit-learn algorithms testing\nScikit-learn KNN algorithm is tested for different weights, algorithm and metrics parameters. For each combination the `KNeighborsClassifier` class object is created and then fitted to training dataset. Next, test dataset is used to check the accuracy. Results are gained in nested dict - `scikit_results`","8b45a941":"## Preparing dataset\nIris dataset consists of several dimensional parameters. Therefore a new class is created. It inherits from the NamedTuple class. For each iris data is divided into two variables. First ``point`` refers to abstract point in 4D space, where each dimension symbolizes the physical dimension of iris. The other attribute is ``label`` which refers to species of the flower.","0c0d04c9":"## Preparing data for presentation\nAll of collected results have to be prepared to plot the chart. Results are divided into two groups of colours,`'inferno'` for scikit algorithm and `'tav20c'` for own algorithms.","2dc1a856":"## Preparing data for Scikit-learn algorithm\nScikit-learn requires numpy arrays as an input to `fit` method of `KNeighborsClassifier`. The list of `LabelPoint` is changed for two lists - first labels and second `adarray` with points.","7981a539":"# Comparison results of own and scikit-learn implementation of KNN\nIn order to compare two different implementations of KNN algorithm Iris dataset is used.\n\n## Dataset presentation\n\nIris dataset includes four basic geometric dimensions:\n* sepal length\n* sepal width\n* penal length\n* petal width\n\n![IRIS](https:\/\/www.integratedots.com\/wp-content\/uploads\/2019\/06\/iris_petal-sepal-e1560211020463.png)\n","e3e9e4a0":"## Sources\nOwn algorithm based on solution proposed in the Joel Grus's book entitled \"Data Science from Scratch: First Principles with Python 2nd Edition\"","8215dc74":"## Own algorithms testing\nThe dataset is divided into two subsets - training and test one. Training subset is used as labelled irises whereas test subset is used to check the accuracy of KNN calcification. The results are collected in `own_results` dictionary.","aca230f4":"## Conclusion\nThe bar graph is shown below. Own algorithm based on euclidean distance calculation has similar performance to scikit-learn algorithms with different parameters. On the other hand own algorithm based on manhattan distance has much worse performance than any kind of scikit-learn algorithms with manhattan metrics. It is hard to compare each of the following results because depending on random division into training and test dataset the result could be different."}}