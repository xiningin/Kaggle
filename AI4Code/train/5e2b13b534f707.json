{"cell_type":{"d1fe597c":"code","c0fab6c2":"code","db8fb9ba":"code","62fc7016":"code","888c7fb3":"code","03d28e0a":"code","ceed6e16":"code","b9caf0e8":"code","fa03a645":"code","b6d5f51d":"code","7a9288fe":"code","c8f69b19":"code","2cd09cca":"code","76e9d2d2":"code","de6e8c31":"code","062fcc11":"code","773eec4c":"code","d3834b0f":"code","ec7619df":"code","86386f4f":"code","769565f3":"code","5d34375a":"code","c0dd0612":"code","1d729da7":"code","cd5a86f4":"code","01628e60":"markdown","6e95b69c":"markdown","d7bf10b4":"markdown","26568ee1":"markdown","4c93c57a":"markdown","0ab4023f":"markdown","7eee4484":"markdown","43d857fb":"markdown","26f0f041":"markdown","a3fd1e84":"markdown","d216f915":"markdown","15886f6b":"markdown","b12c30b2":"markdown","c0a18ad4":"markdown","027e74f9":"markdown","688d68fa":"markdown","c078e84f":"markdown","77909151":"markdown","d1f9ab09":"markdown","78bbeb53":"markdown","51ddc42c":"markdown","36c26d9b":"markdown","c8cd7c79":"markdown","b708b819":"markdown","27de67f0":"markdown"},"source":{"d1fe597c":"# Importing on packages\n\n!pip install pyod\n# Unbeatable pandas for data\nimport pandas as pd\n\n# Some packages for visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculus\nimport numpy as np\nfrom scipy.stats import norm, skew\nimport math\n\n# ML for the win (y)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# Outlier detector\nfrom pyod.models.knn import KNN\n\n# scalers\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import RobustScaler\n\n# Utilities for fun\nimport operator\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Face up with Dataframe. show us all what u got\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","c0fab6c2":"# read scv files\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_output = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ndf_train['SalePrice'] = df_train['SalePrice'].astype(int)","db8fb9ba":"%matplotlib inline\nfig1, axs1 = plt.subplots(ncols=2, nrows = 6, figsize=(30,30))\nfor ax in fig1.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=40)\nfig1.subplots_adjust(wspace=0.2, hspace=0.5)\nsns.barplot(x='Neighborhood', y='SalePrice', data=df_train, ax=axs1[0][0]) \nsns.barplot(x='MSSubClass', y='SalePrice', data=df_train, ax=axs1[0][1])\nsns.barplot(x='MSZoning',y='SalePrice', data=df_train, ax=axs1[1][0])\nsns.barplot(x='Condition1', y='SalePrice', data=df_train, ax=axs1[1][1])\nsns.barplot(x='Condition2', y='SalePrice', data=df_train, ax=axs1[2][0])\nsns.barplot(x='Exterior1st',y='SalePrice', data=df_train, ax=axs1[2][1])\nsns.barplot(x='Exterior2nd', y='SalePrice', data=df_train, ax=axs1[3][0])\nsns.barplot(x='Heating', y='SalePrice', data=df_train, ax=axs1[3][1])\nsns.barplot(x='GarageType',y='SalePrice', data=df_train, ax=axs1[4][0])\nsns.barplot(x='MiscFeature', y='SalePrice', data=df_train, ax=axs1[4][1])\nsns.barplot(x='SaleType', y='SalePrice', data=df_train, ax=axs1[5][0])\nsns.barplot(x='SaleCondition',y='SalePrice', data=df_train, ax=axs1[5][1])","62fc7016":"%matplotlib inline\n# sns.set(font_scale=2)\n# sns.set_context(\"paper\", rc={\"font.size\":10,\"axes.titlesize\":10,\"axes.labelsize\":10}) \nfig2, axs2 = plt.subplots(ncols=1, nrows = 3, figsize=(12,10))\nfig2.tight_layout()\n# axs.tick_params(labelrotation=45)\nsns.scatterplot(x='GrLivArea',y='SalePrice', data=df_train, ax=axs2[0], s=30)\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=df_train, ax=axs2[1], s=30)\nsns.scatterplot(x='YearBuilt', y='SalePrice', data=df_train, ax=axs2[2], s=30)","888c7fb3":"tmp = pd.get_dummies(df_train)\nscaler = StandardScaler()\ndummie_df = pd.DataFrame(scaler.fit_transform(tmp), columns = tmp.columns)\ncorr = dummie_df.corr()['SalePrice']\ndf_corr = pd.DataFrame(corr.sort_values().iloc[np.r_[0:20, -20:0]].drop('SalePrice'))\ncustom_palette = {}\nfor q , a in df_corr.iterrows():\n    if a.values < 0:\n        custom_palette[q] = 'r'\n    else:\n        custom_palette[q] = 'b'\n\nfig3, ax3 = plt.subplots(figsize=(7, 7))\nfig3.tight_layout()\nsns.barplot(x=\"SalePrice\",y = df_corr.index, data=df_corr,\n            label=\"Correlation\", palette=custom_palette, ax=ax3)","03d28e0a":"sns.countplot(x=\"ExterQual\", data=df_train)","ceed6e16":"sns.countplot(x=\"KitchenQual\", data=df_train)","b9caf0e8":"sns.countplot(x=\"BsmtQual\", data=df_train)","fa03a645":"pd.DataFrame(df_train.isna().sum(), columns = ['Df_train NaN Values']).sort_values(by='Df_train NaN Values', ascending = False).head(20).plot(kind = 'bar')","b6d5f51d":"pd.DataFrame(df_test.isna().sum(), columns = ['Df_test NaN Values']).sort_values(by='Df_test NaN Values', ascending = False).head(20).plot(kind = 'bar')","7a9288fe":"corr = df_train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c8f69b19":"tmp2 = pd.get_dummies(df_train)\nscaler2 = StandardScaler()\ndummie_df2 = pd.DataFrame(scaler2.fit_transform(tmp2), columns = tmp2.columns)\nX = dummie_df2.loc[:, dummie_df2.columns != 'SalePrice']\nY = dummie_df2.loc[:, dummie_df2.columns == 'SalePrice']\nX['LotFrontage'].fillna(60.0, inplace = True)\nX['GarageYrBlt'].fillna(1890.0, inplace = True)\nX['MasVnrArea'].fillna(0, inplace = True)\ntsne = TSNE(n_components = 2, perplexity = 30).fit_transform(X)\n\nresult_f = pd.DataFrame(tsne)\ncolor = pd.qcut(Y['SalePrice'],5,labels=False).map({0 : 'blue' , 1 : 'violet' , 2: 'deeppink', 3 : 'crimson', 4: 'darkred'})\n# color= ['red' if row == 'test' else 'blue' for row in output]\nplt.figure(figsize=(10,10))\nplt.title('TSNE 2D Resizing')\nplt.scatter(result_f[0],result_f[1], s=4, c=color)\nplt.show()","2cd09cca":"df_test['SalePrice'] = 'test'\ndf = pd.concat([df_train, df_test])\ndf.drop(['Id'], axis=1, inplace = True)","76e9d2d2":"df['PoolQC'].fillna('No Pool', inplace = True)\ndf['MiscFeature'].fillna('None', inplace = True)\ndf['Alley'].fillna('No alley access', inplace = True)\ndf['Fence'].fillna('No Fence', inplace = True)\ndf['FireplaceQu'].fillna('No Fireplace', inplace = True)\ndf['LotFrontage'].fillna(60.0, inplace = True)\ndf['GarageCond'].fillna('No Garage', inplace = True)\ndf['GarageType'].fillna('No Garage', inplace = True)\ndf['GarageYrBlt'].fillna(1890, inplace = True)\ndf['GarageFinish'].fillna('No Garage', inplace = True)\ndf['GarageQual'].fillna('No Garage', inplace = True)\ndf['BsmtExposure'].fillna('No Basement', inplace = True)\ndf['BsmtFinType2'].fillna('No Basement', inplace = True)\ndf['BsmtFinType1'].fillna('No Basement', inplace = True)\ndf['BsmtCond'].fillna('No Basement', inplace = True)\ndf['BsmtQual'].fillna('No Basement', inplace = True)\ndf['MasVnrArea'].fillna(0, inplace = True)\ndf['MasVnrType'].fillna('None', inplace = True)\n# df['Exterior2nd'].fillna('None', inplace = True)\n# df['Exterior1st'].fillna('None', inplace = True)\ndf['BsmtFullBath'].fillna(0, inplace = True) # df[df['BsmtFullBath'].isna()][['BsmtFullBath','BsmtFinType1','BsmtFinType2']]\ndf['BsmtHalfBath'].fillna(0, inplace = True) # df[df['BsmtFullBath'].isna()][['BsmtFullBath','BsmtFinType1','BsmtFinType2']]\n# df['KitchenQual'].fillna(0, inplace = True)\n  # df[(df['Neighborhood']== 'IDOTRR') | (df['Neighborhood']== 'Mitchel')]['MSZoning']\ndf.loc[(df['MSZoning'].isna()) & (df['Neighborhood'] == 'IDOTRR'), 'MSZoning'] = 'RM'\ndf.loc[(df['MSZoning'].isna()) & (df['Neighborhood'] == 'Mitchel'), 'MSZoning'] = 'RL'\ndf['Utilities'].fillna('AllPub', inplace = True) #la majorit\u00e9\n# df['BsmtHalfBath'].fillna(0.0, inplace = True) #la majorit\u00e9\n# df['BsmtFullBath'].fillna(0.0, inplace = True)\ndf['Functional'].fillna('Typ', inplace = True)\ndf['Exterior1st'].fillna('VinylSd', inplace = True)\ndf['Exterior2nd'].fillna('VinylSd', inplace = True)\ndf['TotalBsmtSF'].fillna(0, inplace = True)\ndf['BsmtUnfSF'].fillna(0, inplace = True)\ndf['BsmtFinSF2'].fillna(0, inplace = True)\ndf['GarageArea'].fillna(0, inplace = True)\ndf['GarageCars'].fillna(0, inplace = True)\ndf['KitchenQual'].fillna('TA', inplace = True)\ndf['BsmtFinSF1'].fillna(0, inplace = True)\ndf['SaleType'].fillna('WD', inplace = True)\ndf['Electrical'].fillna('SBrkr', inplace = True)\ndf['MSSubClass'] = df['MSSubClass'].astype(str)\ndf['OverallCond'] = df['OverallCond'].astype(str)\n\ndf['BsmtExposure'] = df['BsmtExposure'].map({'No Basement' : 0 ,'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4})\ndf['KitchenQual'] = df['KitchenQual'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['GarageYrBlt_Years'] =  abs(1890.0 - df['GarageYrBlt'])\ndf['YearBuilt_Years'] = abs(1872.0 - df['YearBuilt'])\ndf['GarageYrBlt'] = df['GarageYrBlt'].astype(str)\ndf['YearBuilt'] = df['YearBuilt'].astype(str)\n\ntmp = df['MoSold'].map({ 1: 0.0833, 2: 0.1666, 3: 0.25, 4: 0.3333, 5: 0.4166, 6: 0.50, 7: 0.5833, 8: 0.6666, 9: 0.75, 10: 0.8333\n      , 11: 0.9166\n      , 12: 0.9999})\ndf['YearSold'] = abs(2006.0 - (df['YrSold'] + tmp)).astype(float)\ndf['MoSold'] = df['MoSold'].astype(str)\ndf['YrSold'] = df['YrSold'].astype(str)\n\ncriteria = [df['YearRemodAdd'] == df['YearBuilt'], df['YearRemodAdd'] != df['YearBuilt']]\nvalues = [0, 1]\n\ndf['Remod'] = np.select(criteria, values, 0)\ndf['YearRemod'] = (df['YearRemodAdd'].astype(int) - df['YearBuilt'].astype(int)).astype(int)\ndf['YearRemodAdd'] = df['YearRemodAdd'].astype(str)\ndf['YearBuilt'] = df['YearBuilt'].astype(str)\n\ndf['Street'] = df['Street'].map({'Grvl': 0, 'Pave' : 1})\ndf['Alley'] = df['Alley'].map({'Grvl': 1, 'Pave' : 2 , 'No alley access' : 0 })\ndf['LotShape'] = df['LotShape'].map({'IR3': 0, 'IR2' : 1 , 'IR1' : 2 , 'Reg' : 3})\ndf['LandContour'] = df['LandContour'].map({'Low': 0, 'HLS' : 1 , 'Bnk' : 2 , 'Lvl' : 3})\ndf['Utilities'] = df['Utilities'].map({'ELO': 0, 'NoSeWa' : 1 , 'NoSewr' : 2 , 'AllPub' : 3})\n# df['LotConfig'] = df['LotConfig'].map({'ELO': 0, 'NoSeWa' : 1 , 'NoSewr' : 2 , 'AllPub' : 3})\ndf['LandSlope'] = df['LandSlope'].map({'Gtl': 2, 'Mod' : 1 , 'Sev' : 0})\n\ncriteria = [\n#AA\n((df['Condition1'] == 'Artery') | (df['Condition1'] == 'RRAn') | (df['Condition1'] == 'PosA') | (df['Condition1'] == 'RRAe'))\n&\n((df['Condition2'] == 'Artery') | (df['Condition2'] == 'RRAn') | (df['Condition2'] == 'PosA') | (df['Condition2'] == 'RRAe'))\n,\n#AB\n((df['Condition1'] == 'Artery') | (df['Condition1'] == 'RRAn') | (df['Condition1'] == 'PosA') | (df['Condition1'] == 'RRAe'))\n&\n((df['Condition2'] == 'Feedr') | (df['Condition2'] == 'RRNn') | (df['Condition2'] == 'PosN') | (df['Condition2'] == 'RRNe'))\n,\n#BA\n((df['Condition2'] == 'Artery') | (df['Condition2'] == 'RRAn') | (df['Condition2'] == 'PosA') | (df['Condition2'] == 'RRAe'))\n&\n((df['Condition1'] == 'Feedr') | (df['Condition1'] == 'RRNn') | (df['Condition1'] == 'PosN') | (df['Condition1'] == 'RRNe'))\n,\n#BB\n((df['Condition1'] == 'Feedr') | (df['Condition1'] == 'RRNn') | (df['Condition1'] == 'PosN') | (df['Condition1'] == 'RRNe'))\n&\n((df['Condition2'] == 'Feedr') | (df['Condition2'] == 'RRNn') | (df['Condition2'] == 'PosN') | (df['Condition2'] == 'RRNe'))\n,\n#A0\n((df['Condition1'] == 'Artery') | (df['Condition1'] == 'RRAn') | (df['Condition1'] == 'PosA') | (df['Condition1'] == 'RRAe'))\n&\n(df['Condition2'] == 'Norm')\n,\n#0A\n((df['Condition2'] == 'Artery') | (df['Condition2'] == 'RRAn') | (df['Condition2'] == 'PosA') | (df['Condition2'] == 'RRAe'))\n&\n(df['Condition1'] == 'Norm' )\n,\n#B0\n((df['Condition1'] == 'Feedr') | (df['Condition1'] == 'RRNn') | (df['Condition1'] == 'PosN') | (df['Condition1'] == 'RRNe'))\n&\n(df['Condition2'] == 'Norm' )\n,\n#0B\n((df['Condition2'] == 'Feedr') | (df['Condition2'] == 'RRNn') | (df['Condition2'] == 'PosN') | (df['Condition2'] == 'RRNe'))\n&\n(df['Condition1'] == 'Norm')\n,\n#00\n(df['Condition1'] == 'Norm' )\n&\n(df['Condition2'] == 'Norm' )\n]\n\nvalues = [4,3,3,2,2,2,1,1,0]\ndf['conditions'] = np.select(criteria, values, 10)\n\ndf['Exterior2nd'] = df['Exterior2nd'].map({ 'AsbShng': 'AsbShng', 'AsphShn': 'AsphShn', 'Brk Cmn': 'BrkComm', 'BrkFace': 'BrkFace', 'CBlock': 'CBlock', 'CmentBd': 'CemntBd',\n                       'HdBoard': 'HdBoard', 'ImStucc': 'ImStucc', 'MetalSd': 'MetalSd', 'Plywood': 'Plywood'\n                      , 'Stone': 'Stone'\n                      , 'Stucco': 'Stucco'\n                      ,'VinylSd': 'VinylSd'\n                      ,'Wd Sdng': 'Wd Sdng'\n                      ,'Wd Shng': 'WdShing'\n                      , 'None'  : 'None'\n                      , 'Other' :  'Other'})\n\ncriteria = [\n# Only 1\n((df['Exterior1st'] == df['Exterior2nd']) | (df['Exterior2nd'] !=  'Other'))\n,\n# No One\n(df['Exterior1st'] == 'None') &  (df['Exterior2nd'] == 'None')\n,\n# 2\n(df['Exterior1st'] !=  df['Exterior2nd']) \n]\n\nvalues = [1,0,2]\ndf['Exterior'] = np.select(criteria, values, 10)\n\ncriteria = [\n# Have veneer \n(df['MasVnrType'] != 'None')\n,\n# Havnt veneer    \n(df['MasVnrType'] == 'None')\n]\n\nvalues = [1,0]\ndf['veneer'] = np.select(criteria, values, 10)\n\ndf['ExterQual'] = df['ExterQual'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['ExterCond'] = df['ExterCond'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['BsmtQual'] = df['BsmtQual'].map({'No Basement' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5})\n\ndf['BsmtFinType1'] = df['BsmtFinType1'].map({'No Basement' : 0 ,'Unf' : 1, 'LwQ' : 2, 'Rec' : 3, 'BLQ' : 4, 'ALQ' : 5, 'GLQ': 6})\n\ndf['BsmtFinType2'] = df['BsmtFinType2'].map({'No Basement' : 0 ,'Unf' : 1, 'LwQ' : 2, 'Rec' : 3, 'BLQ' : 4, 'ALQ' : 5, 'GLQ': 6})\n\ncriteria = [\n# No Bsmt\n(df['BsmtFinType1'] == 0) & (df['BsmtFinType2'] == 0)\n,\n# 1 Bsmt   \n((df['BsmtFinType1'] != 0) & (df['BsmtFinType2'] != 0) & (df['BsmtFinType1'] == df['BsmtFinType2']))\n|\n((df['BsmtFinType1'] != 0) & (df['BsmtFinType2'] == 0))\n|\n((df['BsmtFinType1'] == 0) & (df['BsmtFinType2'] != 0)) \n,\n# 2 bsmnt\n(df['BsmtFinType1'] != 0) & (df['BsmtFinType2'] != 0) & (df['BsmtFinType1'] != df['BsmtFinType2'])\n]\n\nvalues = [0,1,2]\ndf['Bsmt'] = np.select(criteria, values, 10)\n\ndf['HeatingQC'] = df['HeatingQC'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['CentralAir'] = df['CentralAir'].map({'N' : 0 ,'Y' : 1})\n\ndf['FireplaceQu'] = df['FireplaceQu'].map({'No Fireplace' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4 , 'Ex' : 5})\n\ndf['Functional'] = df['Functional'].map({'Sal' : 0 ,'Sev' : 1, 'Maj2' : 2, 'Maj1' : 3, 'Mod' : 4 , 'Min2' : 5, 'Min1' : 6, 'Typ' : 7})\n\ncriteria = [\n# No garage\ndf['GarageType'] == 'No Garage'\n,\n#2 garages\ndf['GarageType'] == '2Types'\n,\n# only one garage\n(df['GarageType'] != '2Types') & (df['GarageType'] != 'No Garage')\n]\n\nvalues = [0,2,1]\ndf['Garage'] = np.select(criteria, values, 10)\n\ndf['GarageFinish'] = df['GarageFinish'].map({'No Garage' : 0 ,'Unf' : 1, 'RFn' : 2, 'Fin' : 3})\n\ndf['GarageQual'] = df['GarageQual'].map({'No Garage' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4 , 'Ex' :5})\n\ndf['GarageCond'] = df['GarageCond'].map({'No Garage' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4 , 'Ex' :5})\n\ndf['PavedDrive'] = df['PavedDrive'].map({'N' : 0 ,'P' : 1, 'Y' : 2})\n\ndf['PoolQC'] = df['PoolQC'].map({'No Pool' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['Fence'] = df['Fence'].map({'No Fence' : 0 ,'MnWw' : 1, 'GdWo' : 2, 'MnPrv' : 3, 'GdPrv' : 4})\n\ncriteria = [\n# No Feature\ndf['MiscFeature'] == 'None'\n,\n# with Feature\ndf['MiscFeature'] != 'None'\n]\nvalues = [0,1]\ndf['Feature'] = np.select(criteria, values, 10)\ndf['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea'] +df['GarageArea']\ndf['Bathrooms'] = df['FullBath'] + df['HalfBath']*0.5 \ndf['Year average']= (df['YearRemodAdd'].astype(int)+df['YearBuilt'].astype(int))\/2\n\ndf['HasBsmt'] = pd.Series(len(df['TotalBsmtSF']), index=df_train.index)\ndf['HasBsmt'] = 0 \ndf.loc[df['TotalBsmtSF']>0,'HasBsmt'] = 1","de6e8c31":"df_train = df[df['SalePrice'] != 'test']\ndf_test = df[df['SalePrice'] == 'test']\ndf_train['SalePrice'] = df_train['SalePrice'].astype(float)\n\n# to reduce the skewness of the saleprice it is necessary to apply the logarithm function on the saleprice. \n# More details are on this notebook: https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\ndf_train['SalePrice'] =  np.log1p(df_train['SalePrice'])\ndf_train['GrLivArea'] = np.log1p(df_train['GrLivArea'])\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log1p(df_train['TotalBsmtSF'])\n\ndf_test['GrLivArea'] = np.log1p(df_test['GrLivArea'])\ndf_test.loc[df_test['HasBsmt']==1,'TotalBsmtSF'] = np.log1p(df_test['TotalBsmtSF'])","062fcc11":"scaler1 = StandardScaler()\ndf_test.drop(['SalePrice'], axis = 1, inplace = True)\ndum =  pd.get_dummies(df_train, drop_first  = True)\ndum_test =  pd.get_dummies(df_test, drop_first  = True)\n\ndum1 = dum.copy()\ndum_test1 = dum_test.copy()\ncol = []\nfor i in dum.columns:\n    if i not in dum_test.columns :\n        col.append(i)\n        \nfor i in dum_test.columns:\n    if i not in dum.columns :\n        col.append(i)\n        \nfor i in dum1.columns:\n    if i in col :\n        dum1.drop(i, axis = 1, inplace = True)\nfor i in dum_test1.columns:\n    if i in col :\n        dum_test1.drop(i, axis = 1, inplace = True)","773eec4c":"x = scaler1.fit_transform(dum1)\n\nknn = KNN()\noutliers = knn.fit_predict(x)","d3834b0f":"dum1['outliers_flag'] = outliers\ndum1['SalePrice'] = df_train['SalePrice']\n\nfinal_outlier = dum1[dum1.outliers_flag != 1]\nX = final_outlier.loc[:, final_outlier.columns != 'outliers_flag']\nX = X.loc[:, X.columns != 'SalePrice']\ny = final_outlier['SalePrice']\n\npca = PCA(n_components=2)\nX['pca1'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(X)))[0].tolist()\nX['pca2'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(X)))[1].tolist()","ec7619df":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X)\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\nmodel_xgb = make_pipeline(RobustScaler(),xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1))\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = make_pipeline(RobustScaler(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5))\nGBoost = make_pipeline(RobustScaler(),GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5))\nRFR = make_pipeline(RobustScaler(),RandomForestRegressor(random_state = 1))\nRidge = make_pipeline(RobustScaler(),Ridge(alpha = 17))\nlr = make_pipeline(RobustScaler(),LinearRegression())\nex_reg=make_pipeline(RobustScaler(),ExtraTreesRegressor(n_estimators=2000, max_depth=20))\nsvr = make_pipeline(RobustScaler(),SVR(kernel='rbf',C=20000,gamma=0.045))","86386f4f":"scores = {}\nscore = rmsle_cv(model_xgb)\nscores['Xgboost'] = score.mean()\nscore = rmsle_cv(lasso)\nscores['Lasso'] = score.mean()\nscore = rmsle_cv(ENet)\nscores['ElasticNet'] = score.mean()\nscore = rmsle_cv(KRR)\nscores['Kernel Ridge'] = score.mean()\nscore = rmsle_cv(GBoost)\nscores['Gradient Boosting'] = score.mean()\nscore = rmsle_cv(RFR)\nscores['Random Forest'] = score.mean()\nscore = rmsle_cv(Ridge)\nscores['Ridge'] = score.mean()\nscore = rmsle_cv(lr)\nscores['lr'] = score.mean()\nscore = rmsle_cv(ex_reg)\nscores['svr'] = score.mean()\nscore = rmsle_cv(svr)\nscores['svr'] = score.mean()","769565f3":"sorted(scores.items(), key=operator.itemgetter(1))","5d34375a":"vote_mod = make_pipeline(RobustScaler(),VotingRegressor([('Ridge', Ridge), ('Lasso', lasso), ('Elastic', ENet), ('Xgboost', model_xgb)]))\n\nscore = rmsle_cv(vote_mod)\nprint(\"vote_mod score: {:.4f} \\n\".format(score.mean()))","c0dd0612":"stregr = make_pipeline(RobustScaler(),StackingRegressor(regressors=[ENet,Ridge, lasso, vote_mod], \n                           meta_regressor=model_xgb, use_features_in_secondary=True))\n\nscore = rmsle_cv(stregr)\nprint(\"stregr score: {:.4f}\\n\".format(score.mean()))","1d729da7":"stregr.fit(X, y)\nvote_mod.fit(X, y)\n\npca = PCA(n_components=2)\ndum_test1['pca1'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(dum_test1)))[0].tolist()\ndum_test1['pca2'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(dum_test1)))[1].tolist()\n\nstack_predict  =  stregr.predict(dum_test1)\nvote_pred  =  vote_mod.predict(dum_test1)","cd5a86f4":"df_output['SalePrice'] = (2\/3) * np.expm1(vote_pred) + (1\/3) * np.expm1(stack_predict)\n\n# source : https:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1\n\nq1 = df_output['SalePrice'].quantile(0.0045)\nq2 = df_output['SalePrice'].quantile(0.99)\n\ndf_output['SalePrice'] = df_output['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\ndf_output['SalePrice'] = df_output['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nfilename = 'submission.csv'\n\ndf_output.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","01628e60":"There is a strong negative correlation between the selling price and Average quality of the material on the exterior, Kitchen quality and the height of the basement.\n\nAnd there is a strong positive relationship between the selling price and Rates the overall material and finish of the house, size of living area and size of garage in car capacity.\n\nWhy does an average quality influence the price negatively more than a mediocre quality?\n\nWe can rely on the positive correlation but on the negative one.\n\nMaybe the number of houses with average quality is more important than others? lets check.","6e95b69c":"I took several classification algorithm and I compared their RMSE on the train data in cross validation.","d7bf10b4":"<b>Correlation matrix (heatmap style)<\/b>","26568ee1":"In this part I keep the common columns between the test and the train and I delete the outliers in the train data.","4c93c57a":"### Data Engineering","0ab4023f":"### Done !\n\nI hope you enjoyed. If so, put an <b>Upvote!<\/b> =)","7eee4484":"### Machine Learning","43d857fb":"### Sumary\n\n<ul>\n  <li>EDA<\/li>\n  <li>Data Engineering<\/li>\n  <li>ML<\/li>\n  <li>Vote and Stack<\/li>\n  <li>Blending<\/li>  \n<\/ul>\n\n<b> Upvote if you enjoy it =)<\/b>","26f0f041":"<b>Correlation between values and SalePrice<\/b>","a3fd1e84":"### EDA\n\n<b>Relationship with categorical features<\/b>","d216f915":"<b>Missing values<\/b>","15886f6b":"A red square that interests me is the positive correlation between total square feet of basement area and size of garage in car capacity.\nI can deduce that the garage can be part of the basement because the bigger it is, the larger the area of the basement is important.\n\nAnother red square that may be interesting is the positive correlation between Year garage was built and Size of garage in square feet.\nAe can say that over time people have more cars and cars have become bigger, by conceiving the size of the garage is more important and greenhouse gas production too and it's not good for nature Well, that's not the subject. protect nature =).\n\nAoncerning the blue squares there are two that have the darkest color.\n\nThe strong negative correlation between the original construction date, the people are disinterested porches in their houses and the new architchtures of houses do not include porches, it is old.\n\nConcerning the blue squares there are two that have the darkest color.\n\nThe strong negative correlation between the original construction date, the people are disinterested porches in their houses and the new architchtures of houses do not include porches, it is old.\n\nAnother strong negative correlation between Unfinished square feet of basement area and Type 1 finished square feet, it is very logical the two features are oposed and contradictory.\n\nThere is still much to say but if talk about everything it will not be a notebook but a parchment.","b12c30b2":"Lasso, ElasticNet, Ridge and Xgboost gave the best score, lets test voting on them.","c0a18ad4":"<b>Let's see<\/b>\n<ul>\n<li> Well the Neighborhoods Noridge, Nridght and StoneBr are the most expensive they surely have a starbucks and a yoga club.<\/li>\n\n<li>The partial payment is more expensive because it is a partial payment that it is expensive or it is the houses that are expensive then the buyers are obliged to pay partially? Hmm ... you know what, I will wait for the sherlock's notebook I will have answers.<\/li>\n\n<li>The Floating Village Residential and Residential Low Density are the most expensive, people love peace and be at the lake.<\/li>\n<\/ul>","027e74f9":"### Importing Packages ","688d68fa":"The scatter plot represents a resizing of the test data in two dimentions with a color nuanced compared to the sale price to see the attribution of the information.\n\nThrough graph we can see that there are 3 clusters of points, it may be interesting for clustring but it is a regression = \/.\n\nWe can also see that the arrangement of colors differs, where there is blue there is not much darkred and where there is pink there is not much purple etc. As if there is an intensification of the blue color towards the darkred in a specific direction.\n\nThis is good news because it means that there is something that sets the prices high and low.\n\nWe can distinguish divergent points, it means that there are outliers that I have to deal with later","c078e84f":"<img src=\"https:\/\/www.cursio.com.au\/wp-content\/uploads\/2016\/04\/house-prices-blog-post-financial-advice-melbourne.jpg\" class=\"center\" width=\"700px\">","77909151":"There are many NaN values that I have to deal with later","d1f9ab09":"<b>Relationship with numerical variables<\/b>","78bbeb53":"There is a positive linear relation between living area size, basement area size and Original construction date. \nNormal i want to say, the bigger the house is and the more nine are price increases.","51ddc42c":"# House prices: Voting, Stacking and Belnding","36c26d9b":"Voting and stacking have almost the same score, use blending between them and applying some adjustment to the result of the blending.","c8cd7c79":"Everything is explained, the TA values are more numerous than the other values, the union is the strength.","b708b819":"<b>Resizing and viewing<\/b>","27de67f0":"Lets prepar data fo ML.\n\nThe data preprocess that I have done is based on my own logic, a case that I will mention with a comment.\n\nIn this part I will deal with the missing values and transform the categorical values and add some features that I found useful."}}