{"cell_type":{"35a24ab6":"code","994048b9":"code","fc3de752":"code","78e92665":"code","f62abb48":"code","c338b236":"code","ac9cf57f":"code","0525043a":"code","53130460":"code","cd664d92":"code","a5dfab19":"code","61ec85bc":"code","09d27bd3":"code","5a28ab91":"code","46a18839":"code","0786ce2d":"code","59b9afaf":"code","f98e412e":"code","25e89241":"code","fc05b093":"code","9323ba9f":"code","2dbff815":"code","79f51a41":"code","9c021fc2":"code","44f9c26b":"code","994b12c4":"code","6c5427cc":"code","85017930":"code","b14f573a":"code","b4e275ac":"code","e4b1be0a":"code","3f406dd7":"code","2ce62bd6":"code","25389b32":"markdown","72998b47":"markdown","66407f24":"markdown","ebd41676":"markdown","c68f9ac4":"markdown","d2ed8117":"markdown","42e7332e":"markdown","f1752b6e":"markdown","166eb63a":"markdown","562401d8":"markdown","097dda1c":"markdown","8ef93d9f":"markdown","1e22d2b6":"markdown","eb57660d":"markdown","0cfef99c":"markdown","221bebd6":"markdown","69099f86":"markdown","48e58ea5":"markdown","64128ba6":"markdown","645172e4":"markdown","b44fef16":"markdown","97b69781":"markdown","7bcf0456":"markdown","b04eef43":"markdown","9bacaf01":"markdown","c4c36e2f":"markdown","61e586a9":"markdown","54860d6e":"markdown","be7e8774":"markdown","f74042eb":"markdown","92d3ea09":"markdown","3695aed8":"markdown","23c2a599":"markdown","00ebc12e":"markdown","99f3be48":"markdown"},"source":{"35a24ab6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\n\nsns.set_style(\"darkgrid\")\n%matplotlib inline","994048b9":"from sklearn.datasets import load_iris\n\ndata = load_iris()\ncols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n\nfeature_set = pd.DataFrame(data.data, columns=cols)\n\nspecies = pd.Series(data.target, name=\"species\").map({\n    0: 'Iris-setosa',\n    1: 'Iris-versicolor',\n    2: 'Iris-virginica'\n})\n\n\niris_data = pd.merge(feature_set, species, left_index=True, right_index=True)","fc3de752":"# iris_data = pd.read_csv('\/kaggle\/input\/iris-flower-dataset\/IRIS.csv')\n# iris_data.head()","78e92665":"# print a concise summary of a DataFrame.\n\niris_data.info()","f62abb48":"# Generate descriptive statistics.\n# Descriptive statistics include those that summarize the central tendency, \n# dispersion and shape of a dataset\u2019s distribution, excluding NaN values.\n\niris_data.describe()","c338b236":"# check class distribution\n\n_ = sns.countplot(iris_data.species)\n_ = plt.title(\"Class distribution => Balanced Dataset\", fontsize=14)","ac9cf57f":"iris_setosa = iris_data.loc[iris_data.species == \"Iris-setosa\"]\niris_versicolor = iris_data.loc[iris_data.species == \"Iris-versicolor\"]\niris_virginica = iris_data.loc[iris_data.species == \"Iris-virginica\"]","0525043a":"print(\"Setosa mean: \", np.mean(iris_setosa['petal_length']))\nprint(\"Setosa corrupted mean: \", np.mean(np.append(iris_setosa['petal_length'], 50)))\nprint(\"versicolor mean: \", np.mean(iris_versicolor['petal_length']))\nprint(\"virginica mean: \",  np.mean(iris_virginica['petal_length']))\nprint()\nprint(\"Setosa variance: \", np.var(iris_setosa['petal_length']))\nprint(\"Setosa corrupted variance: \", np.var(np.append(iris_setosa['petal_length'], 50)))\nprint(\"versicolor variance: \", np.var(iris_versicolor['petal_length']))\nprint(\"virginica variance: \", np.var(iris_virginica['petal_length']))\nprint() \nprint(\"Setosa std: \", np.std(iris_setosa['petal_length']))\nprint(\"Setosa corrupted std: \", np.std(np.append(iris_setosa['petal_length'], 50)))\nprint(\"versicolor std: \", np.std(iris_versicolor['petal_length']))\nprint(\"virginica std: \", np.std(iris_virginica['petal_length']))","53130460":"print(\"Median: \")\nprint(\"Setosa Median: \", np.median(iris_setosa['petal_length']))\nprint(\"Setosa corrupted Median: \", np.median(np.append(iris_setosa['petal_length'], 50)))\nprint(\"versicolor Median: \", np.median(iris_versicolor['petal_length']))\nprint(\"virginica Median: \",  np.median(iris_virginica['petal_length']))\n\nprint(\"\\nQuantiles: [0, 25, 50, 75]\")\nprint(\"Setosa Quantile: \", np.percentile(iris_setosa['petal_length'], np.arange(0, 100, 25)))\nprint(\"Setosa corrupted Quantile: \", np.percentile(np.append(iris_setosa['petal_length'], 50), np.arange(0, 100, 25)))\nprint(\"versicolor Quantile: \", np.percentile(iris_versicolor['petal_length'], np.arange(0, 100, 25)))\nprint(\"virginica Quantile: \", np.percentile(iris_virginica['petal_length'], np.arange(0, 100, 25)))\n\nprint(\"\\n90th Percentiles\")\nprint(\"Setosa Percentile: \", np.percentile(iris_setosa['petal_length'], 90))\nprint(\"Setosa corrupted Percentile: \", np.percentile(np.append(iris_setosa['petal_length'], 50), 90))\nprint(\"versicolor Percentile: \", np.percentile(iris_versicolor['petal_length'], 90))\nprint(\"virginica Percentile: \", np.percentile(iris_virginica['petal_length'], 90))\n\nfrom statsmodels import robust\nprint(\"\\nMedian Absolute Deviation\")\nprint(\"Setosa MAD: \", robust.mad(iris_setosa['petal_length']))\nprint(\"Setosa corrupted MAD: \", robust.mad(np.append(iris_setosa['petal_length'], 50)))\nprint(\"versicolor MAD: \", robust.mad(iris_versicolor['petal_length']))\nprint(\"virginica MAD: \", robust.mad(iris_virginica['petal_length']))","cd664d92":"fig = plt.figure(figsize=(20, 10))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor no, column in enumerate(iris_data.columns[:-1], 1):\n    ax = fig.add_subplot(2, 2, no)\n    sns.distplot(iris_data.loc[iris_data.species == 'Iris-setosa', f\"{column}\"], label=\"Setosa\")\n    sns.distplot(iris_data.loc[iris_data.species == 'Iris-versicolor', f\"{column}\"], label=\"Versicolor\")\n    sns.distplot(iris_data.loc[iris_data.species == 'Iris-virginica', f\"{column}\"], label=\"Virginica\")\n    ax.legend()\n\nplt.tight_layout(pad=2.0)\nplt.show()","a5dfab19":"counts, bin_edges = np.histogram(iris_data.loc[iris_data.species == 'Iris-setosa', 'petal_length'],\n                                bins=10, density=True)\n\npdf = counts\/sum(counts)\nprint(\"PDF: \", pdf)\nprint(\"CDF: \", bin_edges)\n\n# cdf \ncdf = np.cumsum(pdf)\n_ = plt.plot(bin_edges[1:], pdf)\n_ = plt.plot(bin_edges[1:], cdf)","61ec85bc":"fig = plt.figure(figsize=(10, 6))\n\nfor i, cls in enumerate(iris_data.species.unique(), 1):\n    counts, bin_edges = np.histogram(iris_data.loc[iris_data.species == f'{cls}', 'petal_length'],\n                                    bins=10, density=True)\n\n    pdf = counts\/sum(counts)\n    # print(\"PDF: \", pdf)\n    # print(\"CDF: \", bin_edges)\n\n    # cdf \n    cdf = np.cumsum(pdf)\n    _ = plt.plot(bin_edges[1:], pdf)\n    _ = plt.plot(bin_edges[1:], cdf, label=f'{cls}')\n\nplt.title(f\"{cls}: PDF & CDF plot\")\nplt.xlabel(\"petal_length\")\nplt.legend()\nplt.show()","09d27bd3":"import matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(9, 40))\nouter = gridspec.GridSpec(4, 1, wspace=0.2, hspace=0.2)\n\nfor i, col in enumerate(iris_data.columns[:-1]):\n    inner = gridspec.GridSpecFromSubplotSpec(2, 1,\n                    subplot_spec=outer[i], wspace=0.2, hspace=0.4)\n\n    ax = plt.Subplot(fig, inner[0])\n    _ = sns.boxplot(y=\"species\", x=f\"{col}\", data=iris_data, ax=ax)\n    _ = sns.stripplot(y=\"species\", x=f\"{col}\", data=iris_data,  jitter=True, dodge=True, linewidth=1, ax=ax)\n    _ = ax.set_title(\"Box Plot\")\n    fig.add_subplot(ax)\n\n    ax = plt.Subplot(fig, inner[1])\n    _ = sns.violinplot(y=\"species\", x=f\"{col}\", data=iris_data, inner='quartile', ax=ax)\n    # _ = sns.stripplot(x=\"species\", y=\"petal_length\", data=iris_data, jitter=True, dodge=True, linewidth=1, ax=ax)\n    _ = ax.set_title(\"Violin Plot\")\n    fig.add_subplot(ax)\nfig.show()","5a28ab91":"# experimenting\n\ndef simple_rule(subset):\n    cls = []\n    for idx, row in subset.iterrows():\n        if row['petal_length'] <= 2:\n            cls.append(\"Iris-setosa\")\n        elif row['petal_length'] > 2 and row['petal_length'] <=4.6:\n            cls.append(\"Iris-versicolor\")\n        else:\n            cls.append(\"Iris-virginica\")\n    # accuracy\n    cls = np.array(cls)\n        \n    return accuracy_score(cls, subset.species.values)","46a18839":"iris_data.sample(frac=1)\n\nrandom_idx = np.random.choice(range(0, 150), 20)\nsimple_rule(iris_data.sample(frac=1).iloc[random_idx])","0786ce2d":"plt.figure(figsize=(8, 6))\n_ = sns.heatmap(iris_data.corr(), vmin=-1, vmax=1, annot=True, cmap='afmhot')","59b9afaf":"_ = sns.relplot(x='petal_length', y='petal_width', hue='species', data=iris_data, height=7)\n_ = plt.title(\"Scatter plot\", fontsize=14)","f98e412e":"# From the scatterplot we can clearly see that Iris-setosa can be easily identified\/linearly seperated using sepal_length and sepal_width\n# whereas Iris-versicolor and Iris-virginia have almost the same distribution and rather difficult to seperate\n\n\n_ = plt.figure(figsize=(15, 10))\n_ = sns.pairplot(iris_data, hue=\"species\", height=3, diag_kind=\"kde\")","25e89241":"g = sns.jointplot(x=\"petal_length\", y=\"petal_width\", data=iris_setosa, kind=\"kde\")\ng.plot_joint(plt.scatter, c=\"k\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"$Petal$ $length$\", \"$Petal$ width$\");","fc05b093":"import plotly.express as px\ndf = px.data.iris()\nfig = px.scatter_3d(iris_data, x='petal_length', y='petal_width', z='sepal_length',\n                    color='species')\nfig.show()","9323ba9f":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.decomposition import PCA","2dbff815":"# Seperating X and y\nX = iris_data.drop(['species'], axis=1)\ny = iris_data['species'].map({\n    'Iris-setosa': 0,\n    'Iris-versicolor': 1,\n    'Iris-virginica': 2\n})\nprint(X.shape)\nprint(y.shape)","79f51a41":"models = []\n\nmodels.append((\"LogisticRegression\", LogisticRegression(max_iter=1000)))\nmodels.append((\"SVC\", SVC(kernel=\"rbf\", gamma=5, C=0.001, max_iter=1000)))\n\nmodels.append((\"KNeighbors\", KNeighborsClassifier(n_neighbors=12)))\nmodels.append((\"DecisionTree\", DecisionTreeClassifier()))\nmodels.append((\"RandomForest\", RandomForestClassifier()))\nrf2 = RandomForestClassifier(n_estimators=100, criterion='gini',\n                                max_depth=10, random_state=42, max_features=None)\nmodels.append((\"RandomForest2\", rf2))\nmodels.append(('NB', GaussianNB()))\nmodels.append((\"MLPClassifier\", MLPClassifier(hidden_layer_sizes=(10, 10), solver='adam', max_iter=2000, learning_rate='adaptive', random_state=42)))","9c021fc2":"# naive feature selection\n\nfor i in range(1, 5):\n    cols = X.columns[:i]\n    X_temp = X[cols].values\n    results = []\n    names = []\n    for name, model in models:\n        try:\n            result = cross_val_score(model, X[cols], y, cv=5, scoring='accuracy')\n        except:\n            result = cross_val_score(model, X[cols].reshape(-1, 1), y, cv=5, scoring='accuracy')\n        \n        names.append(name)\n        results.append(result)\n    \n    print(f\"Using features: {cols}\")\n    \n    for i in range(len(names)):\n        # f\"{'1':0>8}\n        print(f\"Algo: {names[i]}, Result: {round(results[i].mean(), 2)}\")\n    print()","44f9c26b":"single_feature_models = models[:]\nsingle_feature_models.pop(2)\nsingle_feature_models.insert(2, (\"KNeighbors\", KNeighborsClassifier(n_neighbors=3)))\n\ntwo_feature_models = models[:]\ntwo_feature_models.pop(2)\ntwo_feature_models.insert(2, (\"KNeighbors\", KNeighborsClassifier(n_neighbors=5)))\n\nX_selected_1 = X[['petal_length']].values\nX_selected_2 = X[['petal_length', 'petal_width']].values\n\nX_ = [X_selected_1, X_selected_2]\ny = y.ravel()\n\nmods = [single_feature_models, two_feature_models]\nfor i in range(2):   \n    curr_models = mods[i]\n    names = []\n    results = []\n    for name, mod in curr_models:    \n        if i == 0:\n            result = cross_val_score(mod, X_selected_1.reshape(-1, 1), y, cv=5, scoring='accuracy')        \n        else:\n            result = cross_val_score(mod, X_selected_2, y, cv=5, scoring='accuracy')            \n        \n        names.append(name)\n        results.append(result) \n\n    print(f\"Features: {X_[i].shape[1]}\")\n    for j in range(len(names)):\n        print(f\"Algo: {names[j]}, Result: {round(results[j].mean(), 2)}\")\n    print()","994b12c4":"import tensorflow as tf\nfrom tensorflow.keras import Sequential, Input\nfrom tensorflow.keras.layers import Dense","6c5427cc":"# Seperating X and y\nX = iris_data.drop(['species'], axis=1)\ny = iris_data['species'].map({\n    'Iris-setosa': 0,\n    'Iris-versicolor': 1,\n    'Iris-virginica': 2\n})\nprint(X.shape)\nprint(y.shape)","85017930":"x_train, x_val, y_train, y_val = train_test_split(X, y, shuffle=True, stratify=y, \n                                                  random_state=42, test_size=0.1)","b14f573a":"model = Sequential([\n                    Input(shape=(4,)),\n                    Dense(10, activation='relu'),\n                    Dense(10, activation='relu'),\n                    Dense(3, activation='softmax'),\n])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)","b4e275ac":"plt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 101), history.history['loss'], label=\"Loss\")\nplt.plot(range(1, 101), history.history['val_loss'], label=\"validation_loss\")\nplt.legend()\nplt.title(\"Epoch Vs. loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"loss\")\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 101), history.history['accuracy'], label=\"accuracy\")\nplt.plot(range(1, 101), history.history['val_accuracy'], label=\"validation_accuracy\")\nplt.legend()\nplt.title(\"Epoch Vs. accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"accuracy\")\n\nplt.show()","e4b1be0a":"# Seperating X and y\nX = iris_data.drop(['species'], axis=1)\ny = iris_data['species'].map({\n    'Iris-setosa': 0,\n    'Iris-versicolor': 1,\n    'Iris-virginica': 2\n})\nprint(X.shape)\nprint(y.shape)","3f406dd7":"pca = PCA(n_components=0.95)\nX_transformed = pca.fit_transform(X)\nprint(X_transformed.shape)\nprint(pca.n_components_)\nprint(pca.explained_variance_ratio_)","2ce62bd6":"x_train, x_val, y_train, y_val = train_test_split(X_transformed, y, shuffle=True, stratify=y, test_size=0.1)\n\nlog = LogisticRegression(max_iter=500)\nlog.fit(x_train, y_train)\npred = log.predict(x_val)\naccuracy_score(y_val, pred)","25389b32":"## Using sklearn","72998b47":"**Scatter plot**","66407f24":"* From the scatterplot we can clearly see that Iris-setosa can be easily identified\/linearly seperated using sepal_length and sepal_width, whereas Iris-versicolor and Iris-virginia have almost the same distribution and rather difficult to seperate.\n\n* Using pairplot we can see that using features *petal_length* or *petal_width*.\n    * Setosa can be easily distinguised.\n    * Versicolor and Virginica can also be seperated almost linearly from others though there will be a little miss-classification.\n\n* We arrived at the same thing conclusion using distplot","ebd41676":"## Using tensorflow","c68f9ac4":"```python\nplt.figure(figsize=(15,10))    \nfor i, j in enumerate(iris_data.columns[:-1], 1):\n    plt.subplot(2, 2, i)\n    _ = sns.boxplot(x=\"species\", y=f\"{j}\", data=iris_data)\n    _ = sns.stripplot(x=\"species\", y=f\"{j}\", data=iris_data,  jitter=True, dodge=True, linewidth=1)\n    _ = plt.title(\"Box Plot\")    \n\nplt.figure(figsize=(15,10))    \nfor i, j in enumerate(iris_data.columns[:-1], 1):\n    plt.subplot(2, 2, i)\n    _ = sns.violinplot(y=\"species\", x=f\"{col}\", data=iris_data, inner='quartile')\n    # _ = sns.stripplot(x=\"species\", y=\"petal_length\", data=iris_data, jitter=True, dodge=True, linewidth=1, ax=ax)\n    _ = plt.title(\"Violin Plot\")\nplt.tight_layout(pad=2)\n```","d2ed8117":"We can see that even after using differnt complex models we are still at average 5% off.","42e7332e":"Using a 3D plot we can see that using a little amount of flexibility in the decision boundary we can correctly classify datapoints for Iris-versicolor and Iris-virginica","f1752b6e":"```python\ng = sns.jointplot(x=\"sepal_length\", y=\"petal_width\", data=iris_data, kind=\"kde\")\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"$Sepal$ $length$\", \"$Petal$ $width$\");\n\np = sns.jointplot(x=\"sepal_length\", y=\"petal_length\", data=iris_data, kind=\"kde\")\np.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\np.ax_joint.collections[0].set_alpha(0)\np.set_axis_labels(\"$Sepal$ $length$\", \"$Petal$ $length$\");\n```","166eb63a":"Observations:\n\n* It's not suprising to see that using only one or two features gave almost the same amount accuracy as using all the features. \n* Performing **EDA** earlier gave us better insights about different features and which would yield better results.\n* EDA helps us to understand the data much better and also helps to identify and remove unwanted features.\n* It's always better to spend some time with the data rather than just randomly using models with all the available data and hoping for the best.\n* Though Iris is a small dataset and performance speed wouldn't have improved much but EDA is really helpful in situations where there are 100 or 1000's of features, we cannot just blindly use all features.","562401d8":"# Results\n* During the univariate analysis we wrote a simple if-else rule of petal_length and got almost 95% accuracy.\n* A simple if-else was on par with complex machine learning algorithms such as Logistic Regression, KNNneighbors, SVM, Gaussian Naive Bayes, Decision Tree, Random forests and MLP's\n* Feature extraction and feature selection methods can be further used to produce rich set of features for the models.","097dda1c":"**Countour probablity plot or KDE plot**\n\nUsing KDE plots, we get the sense of the distribution of density of the data, KDE Plot is used for visualizing the Probability Density of a continuous variable. It depicts the probability density at different values in a continuous variable. \n\n","8ef93d9f":"Using distplot we can clearly see that\n* petal_length and petal_width has the best value seperation among different classses\n* sepal_length and sepal_width are scary to look at as there is so much overlapping between different classes\n* feature petal_length >(better than) petal_width > sepal_length > sepal_width","1e22d2b6":"There are number of other feature selection methods we could try, but that's for another time.\n* [sample feature selection NB](https:\/\/gist.github.com\/veb-101\/d747d5841eba9ae9fe51ff9bacddfa42)\n* [Comprehensive guide](https:\/\/heartbeat.fritz.ai\/hands-on-with-feature-selection-techniques-an-introduction-1d8dc6d86c16)","eb57660d":"# Conclusion\n* Always perform EDA to have a better understanding of the data\n* Always make note about different analysis or obervations you come across while exploring the data\n* Visualization can help to better understand various statistic and information about the data. They can be used to easily convey the relevant information about the data to others.\n* Start with simple approach and make a baseline accuracy measure that all rules and models are atleast able to achieve, then start building more complex from.","0cfef99c":"```python\n# using matplotlib\n%matplotlib notebook\nfig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111, projection='3d')\n\n\nfor i, j in iris_data.groupby(\"species\"):\n    temp = j[['sepal_length', 'sepal_width', 'petal_length']]\n    ax.scatter(temp['sepal_length'], temp['sepal_width'], \\\n           temp['petal_length'], s=40, edgecolor='k')\n\nax.set_xlabel(\"sepal_length\")\nax.set_ylabel(\"sepal_width\")\nax.set_zlabel(\"petal_length\")\nplt.title(\"3D plot to check for seperation\")\nplt.show()\n```","221bebd6":"Let's try 2 more runs using:\n* Only **\"petal_length\"**.\n* Two features **\"petal_length\" and \"petal_width\"**. \n\nAs they both provide better seperation between classes as seen during the **EDA**.","69099f86":"## Univariate analysis","48e58ea5":"Box Plots\n* [Khan academy](https:\/\/www.khanacademy.org\/math\/statistics-probability\/summarizing-quantitative-data\/box-whisker-plots\/a\/box-plot-review)","64128ba6":"**Pair Plot**","645172e4":"* Using a if-else rule and testing on random 20 data points we on an average were able to get 95% accuracy.\n* Using a simple univariate analysis we were able to come up with a simple rule that doesn't require training and will run faster than almost all machine learning algorithms.","b44fef16":"From the plot we can see that most of the values for Iris-setosa densely populated near petal_length = 0.19 to 0.24 and petal_width = 1.3 to 1.55.","97b69781":"* Using 2-dimensions it's not possible to visualize how Iris-versicolor and Iris-virginica can be seperated. So we can use a 3D plot to visualize this","7bcf0456":"\nWhy and iterpretation of Box plot for petal length\n\n* Using distplot although we can get the idea of the mean and spread of the data, it's quite difficult to interpret what is the 75th percentile value. This is where Box and whisker plots come in.\n* A box and whisker plot\u2014also called a box plot \u2014 displays the five-number summary of a set of data. \n* The five-number summary is the minimum, first quartile, median, third quartile, and maximum.\n* In a box plot, we draw a box from the first quartile to the third quartile. \n* A vertical line goes through the box at the median. The whiskers go from each quartile to the minimum or maximum.\n* In seaborn whisker represent the **1.5 x IQR**.\n* Most of the points will lie within the whiskers.\n* In a single plot we can clearly see the mean, spread of petal length and gather information about the outliers plus the quartile values.\n* We can see that if we create a simple if-else rule for vesicolor such as if we set petal_length range between **2 and 5** to be classified as versicolor we can immediately see **5** value overlaps with the 25th percentile for virginica i.e. 25% of points are below 5 so we we'll have 25% error due to the rule.\n\n\nViolin Plots\n* [youtube](https:\/\/www.youtube.com\/watch?v=M6Nu59Fsyyw)\n* [geeksforgeeks](https:\/\/www.geeksforgeeks.org\/violin-plot-for-data-analysis\/)\n* A vilon plot combines the benefits of the **histograms or PDf and Box Plot** and simpliflies them.\n* Denser regions of the data are fatter and sparser ones are thinner in a violin plot","b04eef43":"# Objective\n\nTo classify data-points in classes Iris-Setosa, Iris-Versicolor, Iris-Virginica","9bacaf01":"Using the Cumulative density function\n* We can see using a simple if-else statement we can seperate differnt classes efficiently.\n* For eg. using setting *petal_length < 2*, we can differntiate between Iris-setosa and other classes easily\n* Setting petal_length in the interval between *>2 and <=5* we can seperate out Iris-versicolor from Iris-setosa and virginica with accuracy of around 95%; as the CDF value at 5 is around 0.95.\n\n* If we set *petal_length > 5*, we can classify Iris-virginica correctly all the time\n\n","c4c36e2f":"Usage of PDF's and CDF's","61e586a9":"**Correlation plot**\n* we can clearly see that the sepal_length have high +ve correlation with petal_length and petal_width\n* Similary petal_length and petal_width have high +ve correlation.\n* Later we'll see how to use PCA to reduce the number of dimension and still get good results","54860d6e":"* Percentiles go from 0 to 100.\n* Quartiles go from 1 to 4 (or 0 to 4).\n* Quantiles can go from anything to anything.\n* Percentiles and quartiles are examples of quantiles.\n\n\n* 0 quartile = 0 quantile = 0 percentile\n* 1 quartile = 0.25 quantile = 25 percentile\n* 2 quartile = .5 quantile = 50 percentile (median)\n* 3 quartile = .75 quantile = 75 percentile\n* 4 quartile = 1 quantile = 100 percentile","be7e8774":"# Exploratory data analysis","f74042eb":"## Multi-variate analysis","92d3ea09":"## Bivariate analysis","3695aed8":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Objective<\/a><\/span><\/li><li><span><a href=\"#Exploratory-data-analysis\" data-toc-modified-id=\"Exploratory-data-analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Exploratory data analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-analysis\" data-toc-modified-id=\"Univariate-analysis-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Univariate analysis<\/a><\/span><\/li><li><span><a href=\"#Bivariate-analysis\" data-toc-modified-id=\"Bivariate-analysis-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Bivariate analysis<\/a><\/span><\/li><li><span><a href=\"#Multi-variate-analysis\" data-toc-modified-id=\"Multi-variate-analysis-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Multi-variate analysis<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-building\" data-toc-modified-id=\"Model-building-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Model building<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Using-sklearn\" data-toc-modified-id=\"Using-sklearn-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Using sklearn<\/a><\/span><\/li><li><span><a href=\"#Using-tensorflow\" data-toc-modified-id=\"Using-tensorflow-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Using tensorflow<\/a><\/span><\/li><li><span><a href=\"#Feature-Extraction-using-PCA\" data-toc-modified-id=\"Feature-Extraction-using-PCA-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Feature Extraction using PCA<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Results<\/a><\/span><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","23c2a599":"Now, \n* Mean, Variance and Std. Devitaion values can easily be corrupter by a single extreme outlier\n* To overcome these, we can use Median, Percentile, Quantile, IQR, MAD","00ebc12e":"# Model building","99f3be48":"## Feature Extraction using PCA\n* Reducing the number of features"}}