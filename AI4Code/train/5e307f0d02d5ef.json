{"cell_type":{"56c1adad":"code","e357939b":"code","17ebaad0":"code","5e4b673d":"code","89643125":"code","91a51770":"code","96f31eb0":"code","184a1a1e":"code","1a3ee3ae":"code","2d40797a":"code","8f2a0d64":"code","da68e838":"code","1e7961a7":"code","c81bda15":"code","ef7faa72":"code","a0d6f7db":"code","aba4f69d":"code","52ab6073":"code","1d3d53bf":"code","77d334ee":"code","076f7759":"code","edc69587":"code","fd173610":"code","96584492":"code","d9390fa5":"code","c36d93ee":"code","24259ed3":"code","96792d17":"markdown","0976fa6d":"markdown","45d2a15a":"markdown","0975f99b":"markdown","b0894837":"markdown","ba136e1b":"markdown","f1b0bed8":"markdown","82cf1e85":"markdown","6de3d760":"markdown","cafb4508":"markdown","9b932b7d":"markdown","d21341cc":"markdown","60282f0b":"markdown","f04e3dfa":"markdown","933217d2":"markdown","61985283":"markdown","c0667ac4":"markdown","01a4bcbf":"markdown","4fa35b9e":"markdown","9b5b776d":"markdown","e3f31c7c":"markdown","0a31024d":"markdown","5c6889ab":"markdown","93ab68dc":"markdown","fbaba857":"markdown","bc36bbe6":"markdown"},"source":{"56c1adad":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder , OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e357939b":"data = pd.read_csv(\"\/kaggle\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv\")","17ebaad0":"data.head(),data.shape","5e4b673d":"data.Survived.value_counts()","89643125":"data.isnull().sum()","91a51770":"data.drop([\"PassengerId\",\"Firstname\",\"Lastname\"],axis = 1,inplace = True)","96f31eb0":"plt.figure(figsize=(15,10))\nsns.barplot(x = data.Country.unique(),y = data.Country.value_counts()) , plt.title(\"Passenger count by country\")","184a1a1e":"plt.figure(figsize=(15,10))\nsns.barplot(x = data.Sex.unique(),y = data[data.Survived == 1][\"Sex\"].value_counts() * 100 \/ data.Sex.value_counts()) , plt.title(\"survival percentage by gender\")","1a3ee3ae":"plt.figure(figsize=(15,10))\nsns.barplot(x = list(np.sort(data[data.Survived == 1][\"Country\"].unique())),y = data[data[\"Survived\"] == 1][\"Country\"].value_counts().sort_index())\nplt.title(\"How many passengers survived by country\")","2d40797a":"pd.cut(data[data[\"Survived\"] == 1][\"Age\"],bins= 8, labels = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"]).value_counts().sort_index()","8f2a0d64":"fig , axs = plt.subplots(1,2,figsize = (15,8))\nsns.distplot(data.Age , bins = 8, ax = axs[0]).set_title('Passengers age distribution')\nsns.barplot(x = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"], y= pd.cut(data[data[\"Survived\"] == 1][\"Age\"],bins= 8, labels = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"]).value_counts().sort_index()\n            * 100 \/ pd.cut(data.Age,bins= 8, labels = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"]).value_counts().sort_index(), ax = axs[1]).set_title('Passengers age distribution')\nplt.title(\"Survive percentage by age.\" )\ndata.Age.mean()","da68e838":"fig , axss = plt.subplots(1,2,figsize = (15,8))\nsns.barplot(data.Category.unique(),data[data[\"Survived\"] == 1][\"Category\"].value_counts() *100 \/ data.Category.value_counts(),ax = axss[0]).set_title(\"Survive percentage by category.(C=Crew, P=Passenger)\")\nsns.barplot(data.Category.unique(),data.Category.value_counts(),ax = axss[1]).set_title(\"Passenger and Crew count\")","1e7961a7":"data.head()","c81bda15":"le = LabelEncoder()\nohe = OneHotEncoder(sparse = False)\ndata.Sex = le.fit_transform(data.Sex) # Label Encoding\ndata.Category = le.fit_transform(data.Category)\ndata = pd.concat((data ,pd.DataFrame(ohe.fit_transform(data.Country.to_frame()),columns = ohe.get_feature_names())),axis = 1) #I concatenate the data and one hot encoded column here.\ndata.drop([\"Country\"],inplace = True , axis = 1) # After concatenation i dropped country column.","ef7faa72":"data.head()","a0d6f7db":"plt.figure(figsize=(15,10))\nsns.heatmap(data.corr() , annot = True) # Correlation matrix","aba4f69d":"X = data.drop(columns = [\"Survived\"])\ny = data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=10)","52ab6073":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_predlr = lr.predict(X_test)\ncmlr = confusion_matrix(y_test , y_predlr)\nprint(f\"Accuracy: {accuracy_score(y_test, y_predlr)} Sensitivity: {cmlr[0,0] \/ (cmlr[0,0]+cmlr[1,0])} Specificity: {cmlr[1,1] \/ (cmlr[1,1]+cmlr[0,1])}\")","1d3d53bf":"pip install imblearn","77d334ee":"from imblearn.over_sampling import SMOTE\nsm = SMOTE()\nX_sm, y_sm = sm.fit_resample(X, y)\ny.value_counts() , y_sm.value_counts()","076f7759":"X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm,y_sm,test_size = 0.2, random_state=10)","edc69587":"lr = LogisticRegression()\nlr.fit(X_train_sm, y_train_sm)\ny_predlr2 = lr.predict(X_test_sm)\ncmlr2 = confusion_matrix(y_test_sm , y_predlr2)\nprint(f\"Accuracy: {accuracy_score(y_test_sm, y_predlr2)} Sensitivity: {cmlr2[0,0] \/ (cmlr2[0,0]+cmlr2[1,0])} Specificity: {cmlr2[1,1] \/ (cmlr2[1,1]+cmlr2[0,1])}\")","fd173610":"svc = SVC() #Assignment\nrf = RandomForestClassifier()\nknn = KNeighborsClassifier()","96584492":"svc.fit(X_train_sm, y_train_sm) # Traning\nrf.fit(X_train_sm, y_train_sm)\nknn.fit(X_train_sm, y_train_sm)","d9390fa5":"y_predsvc = svc.predict(X_test_sm) #Prediction\ny_predrf = rf.predict(X_test_sm)\ny_predknn = knn.predict(X_test_sm)","c36d93ee":"cmsvc = confusion_matrix(y_test_sm , y_predsvc) #Confusion Matrix\ncmrf = confusion_matrix(y_test_sm , y_predrf)\ncmknn = confusion_matrix(y_test_sm , y_predknn)","24259ed3":"print(f\"Accuracy of Logistic Regression: {accuracy_score(y_test_sm, y_predlr2)} Sensitivity: {cmlr2[0,0] \/ (cmlr2[0,0]+cmlr2[1,0])} Specificity: {cmlr2[1,1] \/ (cmlr2[1,1]+cmlr2[0,1])}\")\nprint(f\"Accuracy of Support Vector Machine Classifier: {accuracy_score(y_test_sm, y_predsvc)} Sensitivity: {cmsvc[0,0] \/ (cmsvc[0,0]+cmsvc[1,0])} Specificity: {cmsvc[1,1] \/ (cmsvc[1,1]+cmsvc[0,1])}\")\nprint(f\"Accuracy of Random Forest Classifier: {accuracy_score(y_test_sm, y_predrf)} Sensitivity: {cmrf[0,0] \/ (cmrf[0,0]+cmrf[1,0])} Specificity: {cmrf[1,1] \/ (cmrf[1,1]+cmrf[0,1])}\")\nprint(f\"Accuracy of KNeighbor Classifier: {accuracy_score(y_test_sm, y_predknn)} Sensitivity: {cmknn[0,0] \/ (cmknn[0,0]+cmknn[1,0])} Specificity: {cmknn[1,1] \/ (cmknn[1,1]+cmknn[0,1])}\")","96792d17":"Lets check the dataset","0976fa6d":"Lets check null values","45d2a15a":"<a id=\"part6\"><\/a>\n# 6.Training Again:\n\nBefore training we have to split data again","0975f99b":"<a id=\"part3\"><\/a>\n# 3.Encoding:","b0894837":"We are going to use Smote for over-sampling.First we have to download imblearn library","ba136e1b":"Our accuracy decreased %15 but as you can see now we started to predict 1 values from y variable.The variable y was filled with zeros before smote.Now we can try different classifiers.","f1b0bed8":"<a id=\"part4\"><\/a>\n# 4.Traning:","82cf1e85":"<a id=\"part0\"><\/a>\n# 0.Intro:\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1a\/MS_Estonia_model.jpg)\n\nWhat i did at this notebook let me summarize it for you. Dataset has 9 columns and 989 rows. 3 column(\"PassengerId\",\"Firstname\",\"Lastname\") needless for training so i dropped these columns at the begining.After dropping i check nan values. Lucky for me dataset has 0 nan values :). I drew 8 plot to understand column releationships.It looks like 0-20 age range ,crew and males has more chance to survive from disaster.To analyze data i have to transform all categorical data to numerical data.To do that i applied OneHotEncode and Label Encode to 3 column. At disaster %80 of passengers died. That means we have unbalanced dataset here. I used smote for balancing.I didn't tune any hyperparameter but i compared 4 different classification method success rates.\n\n**Table of Contents **\n* [0.Intro:](#part0)\n* [1.Deleting unnecessary columns:](#part1)\n* [2.Exploratory Data Analysis:](#part2)\n* [3.Encoding:](#part3)\n* [4.Traning:](#part4)\n* [5.Data Augmentation:](#part5)\n* [6.Training Again:](#part6)\n* [7.Evaluation:](#part7)","6de3d760":"![](https:\/\/miro.medium.com\/max\/712\/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n* Sensitivity - True Positive Rate = TP \/ (TP + FN)\n* Specificity - True Negative Rate = TN \/ (TN + FP)","cafb4508":"I'am going to label encode \"Sex\" and \"Category\" columns because these columns has 2 unique values.But \"Country\" column has unique values more than 2 and Country value is nominal data.We are going to apply OneHotEncoding to Country column.","9b932b7d":"Accually model predict very bad. Specificity 0 means we predicted nothing at survived = 1. This is happening because of imbalanced dataset.lets balance data.","d21341cc":"Before training lets split the dataset first.","60282f0b":"We have 989 rows and 8 columns.But we are going to drop passengerId , firstname and lastname columns because we dont need these for training.","f04e3dfa":"<a id=\"part5\"><\/a>\n# 5.Data Augmentation:","933217d2":"<a id=\"part2\"><\/a>\n## 2.Exploratory Data Analysis: ","61985283":"<a id=\"part7\"><\/a>\n# 7.Evaluation:","c0667ac4":"Crew members have more chance to survive than passengers but still its around 20%","01a4bcbf":"Seems like males survival rate 4 times bigger than females","4fa35b9e":"No missing values. Perfect!","9b5b776d":"We don't need PassengerId , Firstname and Lastname columns for training so we can drop this columns.","e3f31c7c":"Mean value of passengers age is 44. We have less passengers at 0-20 and +70 age range also survive rate is higher at 0-20 and 76-90 age range.","0a31024d":"<a id=\"part1\"><\/a>\n# 1.Deleting unnecessary columns:","5c6889ab":"Estonian and Swedish passengers survived most. Because most of our passengers from Sweden and Estonia.","93ab68dc":"Most of our passengers from Sweden and Estonia.","fbaba857":"Before we had 137 survivors. Now we have 852. Perfect!Lets train again and evaluate results.","bc36bbe6":"Without any hyperparameter tuning random forest classifier gave us best results. Thanks for your time. What you think about dataset and my analyze? If you have any questions or suggestions let me know. Have a nice day."}}