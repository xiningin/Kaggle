{"cell_type":{"6a64af83":"code","483c96ef":"code","3beadd84":"code","8c706a12":"code","b351fae7":"code","d7a46611":"code","d470b7bc":"code","177ac965":"code","854ef3b5":"code","75573d8e":"code","bb9a472d":"code","880c17d0":"code","b705b46e":"code","0ca07883":"code","6f690454":"code","1d11298a":"code","6e78ec41":"code","16b36e6d":"code","13c3dfa5":"code","cf81d2de":"code","2653561e":"code","84811de6":"code","0911d579":"code","644783e1":"code","b48df4d8":"code","cf5ef902":"code","1d40c10f":"code","df11c077":"code","82fa306a":"code","20a5b2d7":"code","5f5dcd1c":"code","75fedd61":"code","afd0dfa8":"code","2855f201":"code","93e52ff9":"code","704e2fe5":"code","fda0dd7d":"code","1f63b4dd":"code","382df881":"code","75fc92d3":"code","cc2c6a32":"code","6d606d45":"code","54f3af9b":"code","151d116e":"code","b91a9832":"code","f71b728c":"code","d0b833be":"code","26545af9":"code","e7927801":"code","345fda61":"code","d97e5eaf":"code","5605a45f":"code","762d4c1d":"code","ca388dea":"code","d3b187e4":"code","afa7c37f":"code","05432de3":"code","be5d6420":"code","8dd83795":"code","d5add26e":"code","7a5c245f":"code","62931a05":"code","1c92c637":"markdown","575dc89b":"markdown","b21d70c2":"markdown","09dab83b":"markdown","3d3d944c":"markdown","5f189ec6":"markdown","2b93a392":"markdown","04bd9935":"markdown","1695af9d":"markdown","d4f9306b":"markdown","7c9c2d1b":"markdown","9ccaea82":"markdown","89d5db37":"markdown","d6a3a6c0":"markdown","4a95839e":"markdown","3530e151":"markdown","f90b1202":"markdown","4e7a32d0":"markdown","ea913099":"markdown","e993db71":"markdown","573f4eb3":"markdown","9f9d6bb7":"markdown","5e49fabd":"markdown","60d3ba98":"markdown","42aadac7":"markdown","7e0a3991":"markdown","36863317":"markdown","c40a81e2":"markdown","8c360ea0":"markdown","5f62c3e3":"markdown","71b49dff":"markdown","96cea967":"markdown"},"source":{"6a64af83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","483c96ef":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","3beadd84":"warnings.filterwarnings(\"ignore\")","8c706a12":"health_df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv', header=0)\nhealth_df.head()","b351fae7":"health_df.info()","d7a46611":"health_df.describe().T","d470b7bc":"plt.figure(figsize=[20,12])\nax1 = plt.subplot(3,2,1)\nax2 = plt.subplot(3,2,2)\nax3 = plt.subplot(3,2,3)\nax4 = plt.subplot(3,2,4)\nax5 = plt.subplot(3,2,5)\nsns.histplot(data= health_df, x='Glucose', kde=True, ax=ax1)\nsns.histplot(data= health_df, x='SkinThickness', kde=True, ax=ax2)\nsns.histplot(data= health_df, x='BloodPressure', kde=True, ax=ax3)\nsns.histplot(data= health_df, x='Insulin', kde=True, ax=ax4)\nsns.histplot(data= health_df, x='BMI', kde=True, ax=ax5)\nplt.show()","177ac965":"#replacing 0 with NaN for Glucose, BMI, BloodPressure\n\nhealth_df.replace({'Glucose': 0, 'BloodPressure': 0, 'BMI': 0, 'SkinThickness' : 0, 'Insulin' : 0}, np.nan, inplace=True)","854ef3b5":"#percentage of missing value\n\nhealth_df.isna().sum() * 100 \/ health_df.shape[0]","75573d8e":"health_df['BloodPressure'].fillna(health_df['BloodPressure'].mean(), inplace = True)\nhealth_df['Glucose'].fillna(health_df['Glucose'].median(), inplace = True)\nhealth_df['BMI'].fillna(health_df['BMI'].median(), inplace = True)\nhealth_df['SkinThickness'].fillna(health_df['SkinThickness'].median(), inplace = True)\nhealth_df['Insulin'].fillna(health_df['Insulin'].median(), inplace = True)","bb9a472d":"health_df.isna().sum()","880c17d0":"plt.figure(figsize=[8,5])\nsns.countplot(health_df.dtypes.map(str))\nplt.show()","b705b46e":"health_df.Outcome.value_counts()","0ca07883":"plt.figure(figsize=[15,8])\nsns.pairplot(data=health_df, kind='reg', hue='Outcome')\nplt.show()","6f690454":"#Checking below features in more details\n\nplt.figure(figsize=[16,6])\nplt.subplot(1,3,1)\nsns.scatterplot(data= health_df, x= 'Glucose', y='BloodPressure', hue='Outcome')\nplt.subplot(1,3,2)\nsns.scatterplot(data= health_df, x= 'BMI', y='DiabetesPedigreeFunction', hue='Outcome')\nplt.subplot(1,3,3)\nsns.scatterplot(data= health_df, x= 'SkinThickness', y='Age', hue='Outcome')\nplt.show()","1d11298a":"plt.figure(figsize=[12,8])\nsns.heatmap(health_df.corr(), annot=True, cmap='RdYlGn', vmin=-1, vmax=1, center= 0)\nplt.show()","6e78ec41":"label= health_df.iloc[:,-1]\nlabel","16b36e6d":"data= health_df.iloc[:,:-1]\ndata","13c3dfa5":"ss= StandardScaler()\ndata_scaled= pd.DataFrame(ss.fit_transform(data))\ndata_scaled.head()","cf81d2de":"# Using StratifiedKFold for cross validation to find best performing model\n\nkf= StratifiedKFold(n_splits= 7, random_state=None)","2653561e":"# Testing with 6 Models\n\nlr= LogisticRegression(solver='liblinear') # as dataset is small\nsvc= SVC()\nknn= KNeighborsClassifier()\ndt= DecisionTreeClassifier()\nrf= RandomForestClassifier()\nxgb= XGBClassifier()","84811de6":"lr_accuracy= []\nsvc_accuracy= []\nknn_accuracy= []\ndt_accuracy= []\nrf_accuracy= []\nxgb_accuracy= []","0911d579":"for train_idx, test_idx in kf.split(data,label):\n    X_train, X_test= data_scaled.iloc[train_idx,:], data_scaled.iloc[test_idx,:]\n    y_train, y_test= label[train_idx], label[test_idx]\n    \n    # Logistic Regression\n    lr.fit(X_train, y_train)\n    lr_prediction= lr.predict(X_test)\n    lr_acc= accuracy_score(lr_prediction, y_test)\n    lr_accuracy.append(lr_acc)\n    \n    # SVC\n    svc.fit(X_train, y_train)\n    svc_prediction= svc.predict(X_test)\n    svc_acc= accuracy_score(svc_prediction, y_test)\n    svc_accuracy.append(svc_acc)\n    \n    # KNN\n    knn.fit(X_train, y_train)\n    knn_prediction= knn.predict(X_test)\n    knn_acc= accuracy_score(knn_prediction, y_test)\n    knn_accuracy.append(knn_acc)\n    \n    # Decision Tree\n    dt.fit(X_train, y_train)\n    dt_prediction= dt.predict(X_test)\n    dt_acc= accuracy_score(dt_prediction, y_test)\n    dt_accuracy.append(dt_acc)\n    \n    # Random Forest\n    rf.fit(X_train, y_train)\n    rf_prediction= rf.predict(X_test)\n    rf_acc= accuracy_score(rf_prediction, y_test)\n    rf_accuracy.append(rf_acc)\n    \n    # XGB Classifier\n    xgb.fit(X_train, y_train)\n    xgb_prediction= xgb.predict(X_test)\n    xgb_acc= accuracy_score(xgb_prediction, y_test)\n    xgb_accuracy.append(xgb_acc)","644783e1":"print('Logistic Regression- Accuracy of each fold:',*lr_accuracy)\nprint('Average accuracy of Logistic Regression: ', np.mean(lr_accuracy))\nprint('Standard deviation of accuracy:', np.std(lr_accuracy))\nprint('='*50)\nprint('SVC- Accuracy of each fold:',*svc_accuracy)\nprint('Average accuracy of SVC: ', np.mean(svc_accuracy))\nprint('Standard deviation of accuracy:', np.std(svc_accuracy))\nprint('='*50)\nprint('KNN- Accuracy of each fold:',*knn_accuracy)\nprint('Average accuracy of KNN: ', np.mean(knn_accuracy))\nprint('Standard deviation of accuracy:', np.std(knn_accuracy))\nprint('='*50)\nprint('Decision Tree- Accuracy of each fold:',*dt_accuracy)\nprint('Average accuracy of Decision Tree: ', np.mean(dt_accuracy))\nprint('Standard deviation of accuracy:', np.std(dt_accuracy))\nprint('='*50)\nprint('Random Forest- Accuracy of each fold:',*rf_accuracy)\nprint('Average accuracy of Random Forest: ', np.mean(rf_accuracy))\nprint('Standard deviation of accuracy:', np.std(rf_accuracy))\nprint('='*50)\nprint('XGB Classifier- Accuracy of each fold:',*xgb_accuracy)\nprint('Average accuracy of XGB Classifier: ', np.mean(xgb_accuracy))\nprint('Standard deviation of accuracy:', np.std(xgb_accuracy))","b48df4d8":"#Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(data_scaled, label, test_size= 0.2, stratify= label, random_state= 42)","cf5ef902":"log_reg = LogisticRegression(solver='liblinear')\nparam_grid= {\"C\": np.logspace(-5,5,22), \"penalty\": [\"l1\",\"l2\"]}\nlog_reg_grid = GridSearchCV(log_reg, param_grid= param_grid, cv= 25, verbose= True, n_jobs= -1)\nlog_reg_grid.fit(X_train, y_train)","1d40c10f":"# Checking the best score on train data\n\nprint(log_reg_grid.best_score_)\nprint(log_reg_grid.best_params_)","df11c077":"# Testing on test data\nlog_reg_opt = LogisticRegression(solver='liblinear', C= 1.7301957388458944, penalty= 'l1')\nlog_reg_opt.fit(X_train, y_train)\nlog_reg_opt.score(X_test, y_test)","82fa306a":"y_pred= log_reg_opt.predict(X_test)","20a5b2d7":"#Confusion Matrix\n\ntn, fp, fn, tp= confusion_matrix(y_test, y_pred).ravel()\n\nprint('True Negative:', tn)\nprint('False Positive:', fp)\nprint('False Negative:', fn)\nprint('True Positive:', tp)","5f5dcd1c":"# Classification Report\n\nprint(classification_report(y_test, y_pred))","75fedd61":"predict_pr = log_reg_opt.predict_proba(data_scaled)\npredict_pr = predict_pr[:, 1]\nauc = roc_auc_score(label, predict_pr)\nprint('AUC:', round(auc, 4))\nfpr, tpr, thresholds = roc_curve(label, predict_pr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.show()","afd0dfa8":"rf_model= RandomForestClassifier()\n\nparam_grid= {'n_estimators': list(range(20,41,1)),\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth': [3,4,5,6,7,8],\n    'criterion': ['gini', 'entropy']}\n\nrf_grid= GridSearchCV(rf_model, param_grid= param_grid, cv= 25, verbose= True, n_jobs= -1)\nrf_grid.fit(X_train, y_train)","2855f201":"# Checking the best score on train data\n\nprint(rf_grid.best_score_)\n#print(rf_grid.best_params_)","93e52ff9":"rf_model_opt= RandomForestClassifier(criterion= 'gini', max_depth= 6, max_features= 'auto', n_estimators= 32)\nrf_model_opt.fit(X_train, y_train)\nrf_model_opt.score(X_test, y_test)","704e2fe5":"y_pred= rf_model_opt.predict(X_test)","fda0dd7d":"#Confusion Matrix\n\ntn, fp, fn, tp= confusion_matrix(y_test, y_pred).ravel()\n\nprint('True Negative:', tn)\nprint('False Positive:', fp)\nprint('False Negative:', fn)\nprint('True Positive:', tp)","1f63b4dd":"# Classification Report\n\nprint(classification_report(y_test, y_pred))","382df881":"probs = rf_model_opt.predict_proba(data_scaled)\nprobs ","75fc92d3":"predict_pr = rf_model_opt.predict_proba(data_scaled)\npredict_pr = predict_pr[:, 1]\nauc = roc_auc_score(label, predict_pr)\nprint('AUC:', round(auc, 4))\nfpr, tpr, thresholds = roc_curve(label, predict_pr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.show()","cc2c6a32":"knn_model= KNeighborsClassifier()\nparam_grid= {'n_neighbors': list(range(1,20)), 'weights': [\"uniform\", \"distance\"], 'metric': ['minkowski','manhattan','euclidean']}\nknn_grid= GridSearchCV(knn_model, param_grid= param_grid, cv= 25, verbose= True, n_jobs= -1)\nknn_grid.fit(X_train, y_train)","6d606d45":"# Checking the best score on train data\n\nprint(knn_grid.best_score_)\nprint(knn_grid.best_params_)","54f3af9b":"# Testing on test data\n\nknn_opt = KNeighborsClassifier(n_neighbors= 15, weights= 'uniform', metric= 'minkowski')\nknn_opt.fit(X_train, y_train)\nknn_opt.score(X_test, y_test)","151d116e":"y_pred= knn_opt.predict(X_test)","b91a9832":"#Confusion Matrix\n\ntn, fp, fn, tp= confusion_matrix(y_test, y_pred).ravel()\n\nprint('True Negative:', tn)\nprint('False Positive:', fp)\nprint('False Negative:', fn)\nprint('True Positive:', tp)","f71b728c":"# Classification Report\n\nprint(classification_report(y_test, y_pred))","d0b833be":"predict_pr = knn_opt.predict_proba(data_scaled)\npredict_pr = predict_pr[:, 1]\nauc = roc_auc_score(label, predict_pr)\nprint('AUC:', round(auc, 4))\nfpr, tpr, thresholds = roc_curve(label, predict_pr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.show()","26545af9":"data_ap2 = data.drop('Insulin', axis=1)\ndata_ap2","e7927801":"ss1= StandardScaler()\ndata_ap2_scaled= pd.DataFrame(ss1.fit_transform(data))\ndata_ap2_scaled.head()","345fda61":"X_train, X_test, y_train, y_test = train_test_split(data_ap2_scaled, label, test_size= 0.2, stratify= label, random_state= 42)","d97e5eaf":"rf_model= RandomForestClassifier()\n\nparam_grid= {'n_estimators': list(range(20,41,1)),\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth': [3,4,5,6,7,8],\n    'criterion': ['gini', 'entropy']}\n\nrf_grid= GridSearchCV(rf_model, param_grid= param_grid, cv= 25, verbose= True, n_jobs= -1)\nrf_grid.fit(X_train, y_train)","5605a45f":"# Checking the best score on train data\n\nprint(rf_grid.best_score_)\n#print(rf_grid.best_params_)","762d4c1d":"rf_model_opt2= RandomForestClassifier(criterion= 'entropy', max_depth= 4, max_features= 'auto', n_estimators= 30)\nrf_model_opt2.fit(X_train, y_train)\nrf_model_opt2.score(X_test, y_test)","ca388dea":"y_pred= rf_model_opt2.predict(X_test)","d3b187e4":"#Confusion Matrix\n\ntn, fp, fn, tp= confusion_matrix(y_test, y_pred).ravel()\n\nprint('True Negative:', tn)\nprint('False Positive:', fp)\nprint('False Negative:', fn)\nprint('True Positive:', tp)","afa7c37f":"# Classification Report\n\nprint(classification_report(y_test, y_pred))","05432de3":"predict_pr = rf_model_opt2.predict_proba(data_ap2_scaled)\npredict_pr = predict_pr[:, 1]\nauc = roc_auc_score(label, predict_pr)\nprint('AUC:', round(auc, 4))\nfpr, tpr, thresholds = roc_curve(label, predict_pr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.show()","be5d6420":"knn_model= KNeighborsClassifier()\nparam_grid= {'n_neighbors': list(range(1,20)), 'weights': [\"uniform\", \"distance\"], 'metric': ['minkowski','manhattan','euclidean']}\nknn_grid= GridSearchCV(knn_model, param_grid= param_grid, cv= 25, verbose= True, n_jobs= -1)\nknn_grid.fit(X_train, y_train)","8dd83795":"# Checking the best score on train data\n\nprint(knn_grid.best_score_)\nprint(knn_grid.best_params_)","d5add26e":"# Testing on test data\n\nknn_opt2 = KNeighborsClassifier(n_neighbors= 15, weights= 'uniform', metric= 'minkowski')\nknn_opt2.fit(X_train, y_train)\nknn_opt2.score(X_test, y_test)","7a5c245f":"y_pred= knn_opt2.predict(X_test)","62931a05":"# Classification Report\n\nprint(classification_report(y_test, y_pred))","1c92c637":"### Data types and the count of variables","575dc89b":"## Problem Statement:\n\nBuild a model to accurately predict whether the patients in the dataset have diabetes or not.\nThe dataset consists of several medical predictor variables and one target variable, Outcome. Predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n\n1. Pregnancies: Number of times pregnant\n2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n3. BloodPressure: Diastolic blood pressure (mm Hg)\n4. SkinThickness: Triceps skin fold thickness (mm)\n5. Insulin: 2-Hour serum insulin (mu U\/ml)\n6. BMI: Body mass index (weight in kg\/(height in m)^2)\n7. DiabetesPedigreeFunction: Diabetes pedigree function\n8. Age: Age (years)\n9. Outcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0","b21d70c2":"# Data Exploration:","09dab83b":"#### Receiver Operating Characteristics Curve","3d3d944c":"## Finding the best performing model\n1. Logistic Regression\n2. Support Vector Classifier\n3. K Neighbors Classifier\n4. Decision Tree Classifier\n5. Random Forest Classifier\n6. XGBoost Classifier","5f189ec6":"### 1. Logistic Regression","2b93a392":"### 1. Random Forest Classifier","04bd9935":"#### ROC Curve","1695af9d":"It can be seen that our dataset has imbalanced class. We have 500 observations of claas 0 and 268 observations for Class 1. So there is a moderate imbalance in data with 35% data in minority class (1) and 65% in majority class (0).","d4f9306b":"After Droping Insulin column from the feature the model is giving slighly bad performance.","7c9c2d1b":"### Scatter charts between the pair of variables","9ccaea82":"## Approach 1 :","89d5db37":"### 2. Random Forest Classifier","d6a3a6c0":"### Calculating percentage of missing values in these features","4a95839e":"There are 9 Numeric variables, aot of that 3 are intiger and 6 are of float type.","3530e151":"There are 9 variables in this dataset. Outcome is our target\/ dependent variable. All are numeric variables. Outcome is a categorical variable and can have value either 0 or 1. It can be seen that there is no NaN value for any of the variables.\n\nBut we can see Glucose, BloodPressure, SkinThickness, Insulin, BMI Features have minimum value 0. That does not make any sense. We will check each feature one by one.","f90b1202":"# Tableau Dashboard Link\nhttps:\/\/public.tableau.com\/profile\/anik.chakraborty#!\/vizhome\/Healthcare-DiabetesAnalysis_16205897996520\/Dashboard","4e7a32d0":"## Approach 2: (Droping Insulin and checking model performance)","ea913099":"We can see that there is no very strong linear relations between any of the variables. There are medium linear relations between Age and Number of Pregnancies, BMI and SkinThickness, Insulin and Glucose.","e993db71":"It can be seens that BloodPressure has almost normal distribution if missing values are ignored, so hear mean imputation should be ok. BMI and Glucose, SkinThickness have bit skewness, so median imputation can be used.\n\nFor Insulin, percentage of missing value is very high (48.7%). I'll go with median imputation. I'll build my predictive models in two different ways.\n\n##### Approach 1: Including SkinThickness and Insulin in our model.\n\n##### Approach 2: Another approach is, building models after excluding Insulin.","573f4eb3":"# Data Modeling:","9f9d6bb7":"### 3. KNN","5e49fabd":"#### ROC Curve","60d3ba98":"### 2. KNN","42aadac7":"# Importing required packages","7e0a3991":"## Hyperparameters Tunning and Comparing best 2 models with KNN","36863317":"It can be seen taht there is somewhat positive linear relation between Insulin and Glucose. Same with Age and Pregnancies. If this relation is very strong or not that we can see later on using correlation heatmap.\n\nFrom the scatterplots it can be seen that observations of outcome 0 and 1 are almost overplapping with each other in case of most of the features. Only For Glucose, we can see if Glucose is below 90 then there is very low chance of outcome 1 and if Glucose is above 150 then there is a very high chance of outcome to be 1.\n\nAgain if BMI is below 25 then there is almost no observation having outcome =1 where if BMI is more than 25 then we can see both 0 and 1 in outcome.\n\nAlso it can be seen that the probablity of outcome =1 when age<25 is lesser than the when age > 25.","c40a81e2":"Overall Accuracy of the model is : .70\n\nFraction of positives that were correctly identified (Recall) for class 0 is good, .81. But for class 1 the recall value is not that good. Recall for class 1 = TP\/(TP+FN) = 27\/(27+27) = .5\n\nPrecision is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision For class 1: TP\/(TP + FP) = 27\/(27+19) = .59 For class 0 it's .75\n\nF1 score can be calculated as: 2 (precision recall) \/ (precision + recall). F1 score of class 0 is .78 and for class 1 is .54","8c360ea0":"### Correlation Analysis","5f62c3e3":"## Visually explore these variables using histograms and treayting missing values","71b49dff":"There is no change in classification report of KNN, whether we include or exclude Insulin feature in our model.","96cea967":"We can see Glucose, BMI, BloodPressure have few number of 0 value, where SkinThickness and Insulin have very higher number of 0 values."}}