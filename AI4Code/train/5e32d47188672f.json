{"cell_type":{"1a080a58":"code","4c949613":"code","df16f331":"code","a3548990":"code","ca80d887":"code","77c67096":"code","556a5003":"code","7f4cc593":"code","445dba87":"code","5cb1d4cd":"code","dbfe5002":"code","ce081794":"code","69ccc643":"code","e1075e30":"code","b37ae8db":"code","a2c52899":"code","4ef981a0":"code","294acb90":"code","52aa7c8a":"code","e163fe01":"code","724b688c":"code","d7c20b8e":"code","31712386":"code","88a0eda6":"code","ee40ce0d":"code","9cd9d185":"code","76332baf":"code","d0adc428":"code","a90d3552":"code","22a4867e":"code","a68b5204":"code","5eae0fbe":"code","6fe5689b":"code","3935efe0":"code","ab66d993":"code","284a7320":"code","2b067623":"code","8057bfcb":"code","360740ff":"code","7ab26acf":"code","02d69276":"code","dad8aed0":"code","70d95cc2":"code","4dd42111":"code","ead003bb":"code","d552dd05":"code","6c39e731":"code","8d0d6294":"code","e8535cee":"code","772e47d0":"markdown","4185d648":"markdown","49d48aee":"markdown","efa41b9a":"markdown","bb25b804":"markdown","37c44023":"markdown","cca79cb1":"markdown","3b5cc2e3":"markdown","6025b364":"markdown","37fa0d6b":"markdown","1bbd8f67":"markdown","c75ccf7c":"markdown","7e9a5368":"markdown","bfce9923":"markdown","bbddbe4a":"markdown","70ddda83":"markdown","0726066a":"markdown","cd2b4eaf":"markdown","dabb91f6":"markdown","e12d7356":"markdown","9262d921":"markdown","f58dd63c":"markdown","fa00d197":"markdown","0574270d":"markdown","11e17aa4":"markdown","22deff89":"markdown","1d497e8f":"markdown","467df316":"markdown","754f5762":"markdown","62c6687a":"markdown","71cce228":"markdown","e2ec0945":"markdown","663f5d0b":"markdown","a86d6bba":"markdown","304a7e26":"markdown","2535bd84":"markdown","771cfa60":"markdown","40c58b39":"markdown","b25cb422":"markdown","596a4773":"markdown","46f4d72c":"markdown","6570b322":"markdown","079d857f":"markdown","e549c15a":"markdown","3e96adc8":"markdown","959726d5":"markdown","21745361":"markdown","941b162b":"markdown","effee1be":"markdown","082152f3":"markdown","0760aa29":"markdown","0012f3b2":"markdown","a5090baa":"markdown","604ad6e9":"markdown","7ff747df":"markdown","8aa1c409":"markdown"},"source":{"1a080a58":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\nimport numpy as np\nimport random\nimport sklearn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n!pip install --upgrade kaleido\nimport kaleido\n\n!pip uninstall -y transformers\n!pip install transformers\n\nimport transformers\nimport tokenizers\n\n# Hugging Face new library for datasets (https:\/\/huggingface.co\/nlp\/)\n!pip install nlp\nimport nlp\n\nimport datetime\nimport json\nimport IPython\nfrom collections import Counter\nfrom IPython.display import display, HTML, IFrame\n\nstrategy = None","4c949613":"LOAD_MNLI = True\nLOAD_XNLI = True\n\n# seed used for training\/validation splitting\nSEED = 2020\n\n# seed used for `get_raw_dataset()` - which samples examples from panda frames (in particular, MNLI training dataset for MLM finetuning)\nSEED_2 = 17","df16f331":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a3548990":"original_train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\n\nlang_abvs = original_train['lang_abv'].values\nlangs = original_train['language'].values\n\noriginal_train = sklearn.utils.shuffle(original_train, random_state=SEED)\noriginal_train = sklearn.utils.shuffle(original_train, random_state=SEED)\n\nvalidation_ratio = 0.2\nnb_valid_examples = max(1, int(len(original_train) * validation_ratio))\n\noriginal_valid = original_train[:nb_valid_examples]\noriginal_train = original_train[nb_valid_examples:]","ca80d887":"print(f\"original - training: {len(original_train)} examples\")\noriginal_train.head(10)","77c67096":"print(f\"original - validation: {len(original_valid)} examples\")\noriginal_valid.head(10)","556a5003":"original_test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\nprint(f\"original - test: {len(original_test)} examples\")\noriginal_test.head(10)","7f4cc593":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=original_train['language'], histnorm='probability density', name=\"train\"))\nfig.add_trace(go.Histogram(x=original_valid['language'], histnorm='probability density', name=\"valid\"))\nfig.add_trace(go.Histogram(x=original_test['language'], histnorm='probability density', name=\"test\"))\n\n# The 3 histograms are grouped\n# Hint: Use 'categoryorder':'array' and 'categoryarray' to have a custom sorting.\n# fig.update_layout(barmode='group', xaxis={'categoryorder':'category ascending'})\nfig.update_layout(barmode='group', xaxis={'categoryorder':'array', 'categoryarray':np.unique(langs).tolist()})\nfig.show()\n\nfig.write_image('training lang distributions.png')","445dba87":"if LOAD_MNLI:\n\n    mnli = nlp.load_dataset(path='glue', name='mnli')\n\n    print('The number of training examples in mnli dataset:', mnli['train'].num_rows)\n    print('The number of validation examples in mnli dataset - part 1:', mnli['validation_matched'].num_rows)\n    print('The number of validation examples in mnli dataset - part 2:', mnli['validation_mismatched'].num_rows, '\\n')\n\n    print('The class names in mnli dataset:', mnli['train'].features['label'].names)\n    print('The feature names in mnli dataset:', list(mnli['train'].features.keys()), '\\n')\n\n    for elt in mnli['train']:\n\n        print('premise:', elt['premise'])\n        print('hypothesis:', elt['hypothesis'])\n        print('label:', elt['label'])\n        print('label name:', mnli['train'].features['label'].names[elt['label']])\n        print('idx', elt['idx'])\n        print('-' * 80)\n\n        if elt['idx'] >= 10:\n            break\n\n    # convert to a dataframe and view\n    mnli_train_df = pd.DataFrame(mnli['train'])\n    mnli_valid_1_df = pd.DataFrame(mnli['validation_matched'])\n    mnli_valid_2_df = pd.DataFrame(mnli['validation_mismatched'])\n\n    mnli_train_df = mnli_train_df[['premise', 'hypothesis', 'label']]\n    mnli_valid_1_df = mnli_valid_1_df[['premise', 'hypothesis', 'label']]\n    mnli_valid_2_df = mnli_valid_2_df[['premise', 'hypothesis', 'label']]\n\n    mnli_train_df['lang_abv'] = 'en'\n    mnli_valid_1_df['lang_abv'] = 'en'\n    mnli_valid_2_df['lang_abv'] = 'en'","5cb1d4cd":"if LOAD_MNLI:\n    \n    df = mnli_train_df.head(10)\n    IPython.display.display(IPython.display.HTML(df.to_html()))","dbfe5002":"if LOAD_XNLI:\n\n    xnli = nlp.load_dataset(path='xnli')\n\n    print('The number of validation examples in xnli dataset:', xnli['validation'].num_rows, '\\n')\n\n    print('The class names in xnli dataset:', xnli['validation'].features['label'].names)\n    print('The feature names in xnli dataset:', list(xnli['validation'].features.keys()), '\\n')\n\n    for idx, elt in enumerate(xnli['validation']):\n\n        print('premise:', elt['premise'])\n        print('hypothesis:', elt['hypothesis'])\n        print('label:', elt['label'])\n        print('label name:', xnli['validation'].features['label'].names[elt['label']])\n        print('-' * 80)\n\n        if idx >= 3:\n            break\n\n    # convert to a dataframe and view\n    buffer = {\n        'premise': [],\n        'hypothesis': [],\n        'label': [],\n        'lang_abv': []\n    }\n\n\n    for x in xnli['validation']:\n        label = x['label']\n        for idx, lang in enumerate(x['hypothesis']['language']):\n            hypothesis = x['hypothesis']['translation'][idx]\n            premise = x['premise'][lang]\n            buffer['premise'].append(premise)\n            buffer['hypothesis'].append(hypothesis)\n            buffer['label'].append(label)\n            buffer['lang_abv'].append(lang)\n\n    # convert to a dataframe and view\n    xnli_valid_df = pd.DataFrame(buffer)\n    xnli_valid_df = xnli_valid_df[['premise', 'hypothesis', 'label', 'lang_abv']]","ce081794":"if LOAD_XNLI:\n\n    df = xnli_valid_df.head(10)\n    IPython.display.display(IPython.display.HTML(df.to_html()))","69ccc643":"if not LOAD_MNLI:\n    \n    mnli_train_df = pd.DataFrame(columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n    mnli_valid_1_df = pd.DataFrame(columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n    mnli_valid_2_df = pd.DataFrame(columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n\nif not LOAD_XNLI:\n    \n    xnli_valid_df = pd.DataFrame(columns=['premise', 'hypothesis', 'label', 'lang_abv'])  \n\nraw_ds_mapping = {\n    'original train': original_train,\n    'original valid':  original_valid,\n    'mnli train': mnli_train_df,\n    'mnli valid 1': mnli_valid_1_df,\n    'mnli valid 2': mnli_valid_2_df,\n    'xnli valid': xnli_valid_df,\n    'original test': original_test\n}\n\ndef get_raw_dataset(ds_spec, langs=None, shuffle=False, nb_examples=None):\n\n    if type(ds_spec) == list:\n        ds_spec = {k: None for k in ds_spec}\n    \n    all_ds = []\n    for ds_name, _nb_examples in ds_spec.items():\n        \n        raw_ds = raw_ds_mapping[ds_name]\n        \n        if not _nb_examples:\n            _nb_examples = len(raw_ds)\n        \n        ds = raw_ds\n\n        if shuffle:\n            ds = sklearn.utils.shuffle(ds, random_state=SEED_2)\n\n        if langs is not None:\n            ds = ds[ds['lang_abv'].isin(langs)]\n\n        _nb_examples = max(1, min(_nb_examples, len(ds)))\n        ds = ds[:_nb_examples]\n            \n        all_ds.append(ds)\n        \n    ds = pd.concat(all_ds)\n\n    if shuffle:\n        ds = sklearn.utils.shuffle(ds, random_state=SEED_2)    \n    \n    if not nb_examples:\n        nb_examples = len(ds)\n\n    ds = ds[:nb_examples]\n\n    return ds","e1075e30":"def get_unbatched_dataset(ds_spec, tokenizer_name, langs=None, shuffle=False, nb_examples=None, max_len=64, token_counter=None, return_raw_ds=False):\n    \"\"\"\n    Get a combined `pandas.DataFrame` from the raw datasets specified in `ds_names`, then perform tokenization\n    and create a unbatched `tf.data.Dataset` dataset.\n    \n    Args: \n        ds_spec: dict. Keys are the keys in `raw_ds_mapping` and values are the number of examples to use\n            from the corresponding datasets.\n        tokenizer_name: str, the name of a Hugging Face's tokenizer name, e.g. `distilbert-base-uncased`.\n        langs: list, a list of language abbreviations. Only examples in these languages will be included.\n        shuffle: bool, if to shuffle the raw datasets before sampling from them.\n        nb_examples: int, how many examples from the combined raw dataset to be included in the final dataset.\n            If `None`, all examples are included.\n        max_len: int, the maximal length for tokenization. Padding and truncation are performed.\n        \n    \"\"\"\n    \n    ds = get_raw_dataset(ds_spec, langs=langs, shuffle=shuffle, nb_examples=nb_examples)\n    \n    sentence_pairs = list(zip(ds['premise'].tolist(), ds['hypothesis'].tolist()))\n    \n    if 'label' not in ds:\n        ds['label'] = -1\n    labels = ds['label'].tolist()\n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n\n    # `transformers.tokenization_utils_base.BatchEncoding` object -> `dict`\n    r = {'input_ids': [], 'attention_mask': []}\n    if len(sentence_pairs) > 0:\n        r = dict(tokenizer.batch_encode_plus(batch_text_or_text_pairs=sentence_pairs, max_length=max_len, padding='max_length', truncation=True))\n\n    if token_counter is not None:\n        for tokens in r['input_ids']:\n            token_counter.update(tokens)\n\n    # This is very slow\n    dataset = tf.data.Dataset.from_tensor_slices({'inputs': r, 'labels': labels})\n\n    result = (dataset, len(ds))\n    if return_raw_ds:\n        result = (dataset, len(ds), ds)\n    \n    return result\n\ndef get_training_dataset(unbatched_dataset, nb_examples, batch_size=16, shuffle_buffer_size=None, repeat=False):\n    \n    dataset = unbatched_dataset\n    if repeat:\n        dataset = dataset.repeat()\n    \n    if not shuffle_buffer_size:\n        shuffle_buffer_size = nb_examples\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    \n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    \n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset\n\ndef get_prediction_dataset(dataset, batch_size=16):\n    \n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset","b37ae8db":"ds, nb_examples = get_unbatched_dataset(\n    ds_spec = {\n        'original valid': None,  # Take all examples from the validation dataset.\n        'mnli train': 200  # Take (at most) 200 examples from the MNLI training dataset.\n    },\n    tokenizer_name='distilbert-base-uncased',\n    langs=['en', 'fr'],  # Take only examples in enligh and french\n    shuffle=True,  # shuffle before sampling in each raw dataset  \n    nb_examples=None,  # Use all examples from the combined raw dataset.\n    max_len=64\n)\n\nfor x in ds.take(1):\n    print(x)","a2c52899":"for x in get_training_dataset(\n    ds, nb_examples,\n    batch_size=32, shuffle_buffer_size=16, repeat=True\n).take(1):\n    \n    print(x)","4ef981a0":"for k in raw_ds_mapping.keys():\n\n    ds, nb_examples = get_unbatched_dataset(ds_spec={k: None}, tokenizer_name='distilbert-base-uncased', langs=None, shuffle=True, nb_examples=100, max_len=64)    \n    ds_batched = get_training_dataset(ds, nb_examples, batch_size=16, shuffle_buffer_size=1, repeat=True)\n    print('{} - select {} examples'.format(k, nb_examples))\n    \n    for x in ds_batched.take(1):\n        pass","294acb90":"def sample_without_replacement(prob_dist, nb_samples):\n    \"\"\"Sample integers in the range [0, N), without replacement, according to the probability\n       distribution `prob_dist`, where `N = prob_dist.shape[0]`.\n    \n    Args:\n        prob_dist: 1-D tf.float32 tensor.\n    \n    Returns:\n        selected_indices: 1-D tf.int32 tensor\n    \"\"\"\n\n    nb_candidates = tf.shape(prob_dist)[0]\n    logits = tf.math.log(prob_dist)\n    z = -tf.math.log(-tf.math.log(tf.random.uniform(shape=[nb_candidates], minval=0, maxval=1)))\n    _, selected_indices = tf.math.top_k(logits + z, nb_samples)\n\n    return selected_indices","52aa7c8a":"for i in range(10):\n    print('sample {}: {}'.format(i + 1, sample_without_replacement(tf.constant([0.1, 0.2, 0.3, 0.4]), nb_samples=3).numpy()))","e163fe01":"def get_masked_lm_fn(tokenizer, mlm_mask_prob=0.15, mask_type_probs=(0.8, 0.1, 0.1), token_counts=None, predict_special_tokens=False, mlm_smoothing=0.7):\n    \"\"\"\n    Prepare the batch: from the input_ids and the lenghts, compute the attention mask and the masked label for MLM.\n\n    Args:\n\n        tokenizer: A Hugging Face tokenizer.  \n        \n        token_counts: A list of integers of length `tokenizer.vocab_size`, which is the token counting in a dataset\n            (usually, the huge dataset used for pretraing a LM model). This is used for giving higher probability\n            for rare tokens to be masked for prediction. If `None`, each token has the same probability to be masked.\n\n        mlm_mask_prob:  A `tf.float32` scalar tensor. The probability to <mask> a token, inclding\n            actually masking, keep it as it is (but to predict it), and randomly replaced by another token.\n        \n        mask_type_probs: A `tf.float32` tensor of shape [3]. Among the sampled tokens to be <masked>, \n\n            mask_type_probs[0]: the proportion to be replaced by the mask token\n            mask_type_probs[1]: the proportion to be kept as it it\n            mask_type_probs[2]: the proportion to be replaced by a random token in the tokenizer's vocabulary\n        \n        predict_special_tokens: bool, if to mask special tokens, like cls, sep or padding tokens. Default: `False`\n        \n        mlm_smoothing: float, smoothing parameter to emphasize more rare tokens (see `XLM` paper, similar to word2vec).\n        \n    Retruns:\n\n        prepare_masked_lm_batch: a function that masks a batch of token sequences.\n    \"\"\"\n\n    if token_counts is None:\n        \"\"\"\n        Each token has the same probability to be masked.\n        \"\"\"\n        token_counts = [1] * tokenizer.vocab_size\n\n    # Tokens with higher counts will be masked less often.\n    # If some token has count 1, it will have freq 1.0 in this frequency list, which is the highest value.\n    # However, since it never appears in the corpus used for pretraining, there is no effect of this high frequency.\n    token_mask_freq = np.maximum(token_counts, 1) ** -mlm_smoothing\n\n    # NEVER to mask\/predict padding tokens.\n    token_mask_freq[tokenizer.pad_token_id] = 0.0\n    \n    if not predict_special_tokens:\n        for special_token_id in tokenizer.all_special_ids:\n            \"\"\"\n            Do not to predict special tokens, e.g. padding, cls, sep and mask tokens, etc.\n            \"\"\"\n            token_mask_freq[special_token_id] = 0.0\n\n    # Convert to tensor.\n    token_mask_freq = tf.constant(token_mask_freq, dtype=tf.float32)        \n\n    mlm_mask_prob = tf.constant(mlm_mask_prob)\n    mask_type_probs = tf.constant(mask_type_probs)\n    \n    vocab_size = tf.constant(tokenizer.vocab_size)\n    pad_token_id = tf.constant(tokenizer.pad_token_id)\n    mask_token_id = tf.constant(tokenizer.mask_token_id)\n    \n    def prepare_masked_lm_batch(inputs):\n        \"\"\"\n        Prepare the batch: from the input_ids and the lenghts, compute the attention mask and the masked label for MLM.\n\n        Args:\n            \n            inputs: a dictionary of tensors. Format is:\n            \n                {\n                    'input_ids': `tf.int32` tensor of shape [batch_size, seq_len] \n                    : `tf.int32` tensor of shape [batch_size, seq_len] \n                }            \n                \n                Optionally, it could contain extra keys 'attention_mask' and `token_type_ids` with values being\n                `tf.int32` tensors of shape [batch_size, seq_len] \n             \n        Returns:\n        \n            result: a dictionary. Format is as following:\n\n                {\n                    'inputs': A dictionary of tensors, the same format as the argument `inputs`.\n                    'mlm_labels': shape [batch_size, seq_len]\n                    'mask_types': shape [batch_size, seq_len]\n                    'original_input_ids': shape [batch_size, seq_len]\n                    'nb_tokens': shape [batch_size]\n                    'nb_non_padding_tokens': shape [batch_size]\n                    'nb_tokens_considered': shape [batch_size]\n                    'nb_tokens_masked': shape [batch_size]\n                }\n                \n                The tensors associated to `number of tokens` are the toekn countings in the whole batch, not\n                in individual examples. They are actually constants, but reshapped to [batch_size], because\n                `tf.data.Dataset` requires the batch dimension to be consistent. These are used only for debugging,\n                except 'nb_tokens_masked, which is used for calculating the MLM loss values.\n        \"\"\"\n\n        input_ids = inputs['input_ids']\n        \n        batch_size, seq_len = input_ids.shape\n\n        attention_mask = None\n        if 'attention_mask' in inputs:\n            attention_mask = inputs['attention_mask']\n\n        # Compute `attention_mask` if necessary\n        if attention_mask is None:\n            attention_mask = tf.cast(input_ids != pad_token_id, tf.int32)            \n\n        # The number of tokens in each example, excluding the padding tokens. \n        # shape = [batch_size]\n        lengths = tf.reduce_sum(attention_mask, axis=-1)\n                \n        # The total number of tokens, excluding the padding tokens.\n        nb_non_padding_tokens = tf.math.reduce_sum(lengths)\n\n        # For each token in the batch, get its frequency to be masked from the 1-D tensor `token_mask_freq`.\n        # We keep the output to remain 1-D, since it's easier for using sampling method `sample_without_replacement`.\n        # shape = [batch_size * seq_len], 1-D tensor.\n        freq_to_mask = tf.gather(params=token_mask_freq, indices=tf.reshape(input_ids, [-1]))\n\n        # Normalize the frequency to get a probability (of being masked) distribution over tokens in the batch.\n        # shape = [batch_size * seq_len], 1-D tensor.\n        prob_to_mask = freq_to_mask \/ tf.reduce_sum(freq_to_mask)\n\n        tokens_considered = tf.cast(attention_mask, tf.bool)\n        if not predict_special_tokens:\n            for special_token_id in tokenizer.all_special_ids:\n                tokens_considered = tf.logical_and(tokens_considered, input_ids != special_token_id)\n        nb_tokens_considered = tf.reduce_sum(tf.cast(tokens_considered, dtype=tf.int32))\n        \n        # The number of tokens to be masked.\n        # type = tf.float32\n        # nb_tokens_to_mask = tf.math.ceil(mlm_mask_prob * tf.cast(nb_non_padding_tokens, dtype=tf.float32))\n        nb_tokens_to_mask = tf.math.ceil(mlm_mask_prob * tf.cast(nb_tokens_considered, dtype=tf.float32))\n        \n        # round to an integer\n        nb_tokens_to_mask = tf.cast(nb_tokens_to_mask, tf.int32)\n\n        # Sample `nb_tokens_to_mask` of different indices in the range [0, batch_size * seq_len).\n        # The sampling is according to the probability distribution `prob_to_mask`, without replacement.\n        # shape = [nb_tokens_to_mask]\n        indices_to_mask = sample_without_replacement(prob_to_mask, nb_tokens_to_mask)\n\n        # Create a tensor of shape [batch_size * seq_len].\n        # At the indices specified in `indices_to_mask`, it has value 1. Otherwise, the value is 0.\n        # This is a mask (after being reshaped to 2D tensor) for masking\/prediction, where `1` means that, at that place,\n        # the token should be masked for prediction. \n        # (For `tf.scatter_nd`, check https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/scatter_nd)\n        pred_mask = tf.scatter_nd(\n            indices=indices_to_mask[:, tf.newaxis],  # This is necessary for making `tf.scatter_nd` work here. Check the documentation.\n            updates=tf.cast(tf.ones_like(indices_to_mask), tf.bool),\n            shape=[batch_size * seq_len]\n        )\n\n        # Change to 2-D tensor.\n        # The mask for masking\/prediction.\n        # shape = [batch_size, seq_len]\n        pred_mask = tf.reshape(pred_mask, [batch_size, seq_len])\n\n        # Get token ids at the places where to mask tokens.\n        # 1-D tensor, shape = [nb_tokens_to_mask].\n        _input_ids_real = input_ids[pred_mask]\n\n        # randomly select token ids from the range [0, vocab_size)\n        # 1-D tensor, shape = [nb_tokens_to_mask]\n        _input_ids_rand = tf.random.uniform(shape=[nb_tokens_to_mask], minval=0, maxval=vocab_size, dtype=tf.int32)\n\n        # A constant tensor with value `mask_token_id`.\n        # 1-D tensor, shape = [nb_tokens_to_mask]\n        _input_ids_mask = mask_token_id * tf.ones_like(_input_ids_real, dtype=tf.int32)\n\n        # For each token to be masked, we decide which type of transformations to apply:\n        #     0: masked, 1: keep it as it is, 2: replaced by a random token\n        \n        # Detail: we need to pass log probability (logits) to `tf.random.categorical`,\n        #    and it has to be 2-D. The output is also 2-D, and we just take the 1st row.\n        # shape = [nb_tokens_to_mask]\n        mask_types = tf.random.categorical(logits=tf.math.log([mask_type_probs]), num_samples=nb_tokens_to_mask)[0]\n\n        # These are token ids after applying masking.\n        # shape = [nb_tokens_to_mask]\n        masked_input_ids = (\n            _input_ids_mask * tf.cast(mask_types == 0, dtype=tf.int32) + \\\n            _input_ids_real * tf.cast(mask_types == 1, dtype=tf.int32) + \\\n            _input_ids_rand * tf.cast(mask_types == 2, dtype=tf.int32)\n        )\n\n        # Put the masked token ids into a 2-D tensor (initially zeros) of shape [batch_size, seq_len].\n        # remark: `tf.where(pred_mask)` is of shape [nb_tokens_to_mask, 2].\n        token_ids_to_updates = tf.scatter_nd(indices=tf.where(pred_mask), updates=masked_input_ids, shape=[batch_size, seq_len])\n\n        # At the places where we don't mask, just keep the original token ids.\n        # shape = [batch_size, seq_len]\n        token_ids_to_keep = input_ids * tf.cast(~pred_mask, tf.int32)\n        \n        # The final masked token ids used for training\n        # shape = [batch_size, seq_len]\n        masked_input_ids = token_ids_to_updates + token_ids_to_keep\n        \n        # At the places where we don't predict, change the labels to -100\n        # shape = [batch_size, seq_len]\n        mlm_labels = input_ids * tf.cast(pred_mask, dtype=tf.int32) + -100 * tf.cast(~pred_mask, tf.int32)\n\n        masked_lm_batch = {\n            'input_ids': masked_input_ids,\n            'attention_mask': attention_mask\n        }\n        if 'token_type_ids' in inputs:\n            masked_lm_batch['token_type_ids'] = inputs['token_type_ids']\n\n        # The total number of tokens\n        nb_tokens = tf.reduce_sum(tf.cast(input_ids > -1, dtype=tf.int32))\n\n        # Used for visualization\n        # 0: not masked, 1: masked, 2: keep it as it is, 3: replaced by a random token, 4: padding - (not masked)\n        # shape = [batch_size, seq_len]\n        _mask_types = tf.scatter_nd(tf.where(pred_mask), updates=mask_types + 1, shape=[batch_size, seq_len])\n        _mask_types = tf.cast(_mask_types, dtype=tf.int32)\n        _mask_types += 4 * tf.cast(input_ids == pad_token_id, tf.int32)\n\n        result = {\n            'inputs': masked_lm_batch,\n            'mlm_labels': mlm_labels,\n            'mask_types': _mask_types,\n            'original_input_ids': input_ids,\n            'nb_tokens': nb_tokens * tf.constant(1, shape=[batch_size]),\n            'nb_non_padding_tokens': nb_non_padding_tokens * tf.constant(1, shape=[batch_size]),\n            'nb_tokens_considered': nb_tokens_considered  * tf.constant(1, shape=[batch_size]),\n            'nb_tokens_masked': nb_tokens_to_mask * tf.constant(1, shape=[batch_size])        \n        }\n\n        return result\n\n    return prepare_masked_lm_batch","724b688c":"tokenizer_name = 'distilbert-base-uncased'\ntokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n\nprepare_masked_lm_batch = get_masked_lm_fn(\n    tokenizer=tokenizer,\n    mlm_mask_prob=0.35,\n    mask_type_probs=[0.5, 0.25, 0.25],\n)\n\nds, nb_examples = get_unbatched_dataset(ds_spec={'original valid': 100}, tokenizer_name=tokenizer_name, max_len=32)    \nds_batched = get_training_dataset(ds, nb_examples, batch_size=16)\n\nmlm_dataset = ds_batched.map(lambda batch: prepare_masked_lm_batch(batch['inputs']))\n\nfor x in mlm_dataset.take(1):\n    print(x)","d7c20b8e":"dummy_sentence_pairs = [\n    ('i am so hungry', 'need some food'),\n    ('kaggle once a day', 'keep the doctors away'),\n    ('training is too slow?', 'try tensor processing unit!'),\n    ('nlp is interesting', 'join us and become a data scientist'),\n    ('want to learn bert?', 'let\\'s play with masked language model')\n]","31712386":"colors = np.array(\n    [\n        [ 44, 255, 119],   # green - unchanged tokens        \n        [192, 192, 192],  #  gray - masked tokens\n        [255, 255, 102],  # yellow - kept as it is, but to predict        \n        [255, 169, 242],  #  pink - randomly replaced tokens\n        # [137 ,209, 254 ],  #  blue - padding - unchanged tokens\n        [207, 238, 250 ],  #  blue - padding - unchanged tokens\n    ]) \/ 255\ncmap = matplotlib.colors.ListedColormap(colors)\nmask_categories = ['not to predict', 'masked', 'to predict as it is', 'randomly replaced', 'padding']\n\ndef set_ax(ax, vocab, input_ids, mask_types, batch_size, seq_len, title='', show_title=True, show_text=True, show_legend=True):\n\n    ax.set_xticks(range(-1, seq_len + 1))\n    ax.set_yticks(range(-1, batch_size + 1))\n    \n    ax.grid(color='k', linewidth=4)\n    \n    if show_text:\n        for (row_id, col_id), token_id in np.ndenumerate(input_ids):\n            ax.text(\n                col_id,\n                row_id,\n                '{}'.format(vocab[token_id]),\n                ha='center', va='center',\n                position=(0.5 * (2 * col_id + 1), 0.5 * (2 * row_id + 1)),\n                fontsize=14\n            )\n\n    extent = (0, seq_len, batch_size, 0)\n    ax.imshow(mask_types, cmap=cmap, extent=extent)\n    \n    ax.tick_params(\n        axis='both',          # changes apply to the x-axis\n        which='both',         # both major and minor ticks are affected\n        bottom=False,         # ticks along the bottom edge are off\n        top=False,            # ticks along the top edge are off\n        left=False,           # ticks along the bottom edge are off\n        right=False,          # ticks along the top edge are off    \n        labelbottom=False,\n        labeltop=False,\n        labelleft=False,\n        labelright=False\n    )\n    \n    if show_title:\n        ax.set_title(title, color='black', fontsize=20, position=(0.5, 1.05))\n    \n    if show_legend:\n        \n        # Used for legend\n        patches = [matplotlib.patches.Patch(color=colors[i], label=\"{}\".format(mask_categories[i]) ) for i in range(len(colors))]\n        # put those patched as legend-handles into the legend\n        ax.legend(handles=patches, bbox_to_anchor=(0, 1.20, 1, 0.2), loc=2, borderaxespad=0., fontsize=16, edgecolor='black')\n    \ndef plot_mlm_batch(vocab, original_input_ids, masked_input_ids, mask_types, scaling=1.0, plot_original=False, show_title=True, show_tokens=True, show_legend=True):\n        \n    original_input_ids = original_input_ids.numpy()\n    masked_input_ids = masked_input_ids.numpy()\n    mask_types = mask_types.numpy()\n    \n    batch_size, seq_len = original_input_ids.shape\n    \n    nb_axes = 1\n    if plot_original:\n        nb_axes = 2\n    \n    # size is (width, height)    \n    fig = plt.figure(figsize=(round(seq_len * scaling), round(nb_axes * batch_size * scaling)))\n    \n    gs = matplotlib.gridspec.GridSpec(nb_axes, 1)\n\n    ax_id = 0\n    if plot_original:\n        ax = fig.add_subplot(gs[ax_id, 0])\n        set_ax(ax, vocab, original_input_ids, mask_types, batch_size, seq_len, title='original tokens', show_title=show_title, show_text=show_tokens, show_legend=show_legend)\n        ax_id += 1\n    \n    ax = fig.add_subplot(gs[ax_id, 0])\n    set_ax(ax, vocab, masked_input_ids, mask_types, batch_size, seq_len, title='masked tokens', show_title=show_title, show_text=show_tokens, show_legend=show_legend and not plot_original)\n\n    plt.show()\n    \nmask_categories_abv = ['n\/a', 'masked', 'kept', 'rand. repl.', 'n\/a: padding']\ncolorscale = [\n     'rgb(44, 255, 119)',\n     'rgb(192, 192, 192)',\n     'rgb(255, 255, 102)',\n     'rgb(255, 169, 242)',\n     'rgb(207, 238, 250)'\n]\ndef plot_mlm_batch_plotly(vocab, original_input_ids, masked_input_ids, mask_types, scaling=1.0, title=''):\n    \"\"\"Use plotly to display the mlm batch when it is too large: No text is displayed, but information is shown when hovering on it.\n       This is also much faster.\n    \"\"\"\n\n    original_input_ids = original_input_ids.numpy()\n    masked_input_ids = masked_input_ids.numpy()\n    mask_types = mask_types.numpy()\n\n    original_tokens = np.array(\n        [[vocab[x] for x in token_ids] for token_ids in original_input_ids[::-1]]\n    )\n    masked_tokens = np.array(\n        [[vocab[x] for x in token_ids] for token_ids in masked_input_ids[::-1]]\n    )\n    mask_type_text = np.array(\n        [[mask_categories_abv[x] for x in mask_type] for mask_type in mask_types[::-1]]\n    )    \n    \n    customdata = np.dstack([original_tokens, masked_tokens, mask_type_text])    \n    \n    batch_size, seq_len = original_input_ids.shape\n\n    side_min = 300\n    width_max = 1280\n    \n    if seq_len >= batch_size:\n        \n        width = max(100 * seq_len, side_min)\n        width = min(width, width_max)\n        \n        height = min(100, width \/ seq_len) * batch_size            \n        height += 200 * (1 - batch_size \/ seq_len)\n                        \n    elif seq_len < batch_size:\n        \n        width = max(100 * seq_len, side_min)\n        width = min(width, width_max)        \n        \n        height = min(100, width \/ seq_len) * batch_size            \n        height += 200 * (1 - batch_size \/ seq_len)\n    \n    data = mask_types[::-1]\n\n    fig = go.Figure(go.Heatmap(\n        z=data,\n        customdata=customdata,\n        colorscale=colorscale,\n        hovertemplate=' original: %{customdata[0]}<br>   masked: %{customdata[1]}<br>mask type: %{customdata[2]}',\n        name='', showscale=False, xgap=3, ygap=3))\n\n    title_text = None\n    if title is not None:\n        title_text = title\n        \n    # https:\/\/stackoverflow.com\/questions\/54826436\/how-to-remove-axes-and-numbers-from-plotly\n    fig.update_layout(\n        title_text=title_text,\n        plot_bgcolor=\"black\",\n        xaxis=dict(zeroline=False, constrain='domain', constraintoward='left', showticklabels=False, showgrid=False,\n                   visible=False, scaleanchor='y', scaleratio=1),\n        yaxis=dict(zeroline=False, constrain='domain', constraintoward='top', showticklabels=False, showgrid=False,\n                   visible=False),\n        width=width,\n        height=height,\n        autosize=False,\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=50,\n            pad=0\n        ),\n        hoverlabel=dict(\n            bgcolor=\"black\", \n            font_size=16, \n            font_family=\"Courier New, Monospace\"\n        )        \n    )\n\n    fig.show()","88a0eda6":"tokenizer_name = 'distilbert-base-uncased'\ntokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\nvocab = {v: k for k, v in tokenizer.get_vocab().items()}\n\nmlm_mask_prob = 0.35\nmask_type_probs=[0.5, 0.25, 0.25]\n\nprepare_masked_lm_batch = get_masked_lm_fn(\n    tokenizer=tokenizer,    \n    mlm_mask_prob=mlm_mask_prob,\n    mask_type_probs=mask_type_probs,\n)","ee40ce0d":"dummy_inputs = tokenizer.batch_encode_plus(dummy_sentence_pairs, max_length=16, padding='max_length', truncation=True)\ndummy_inputs = dict(dummy_inputs)\ndummy_ds = tf.data.Dataset.from_tensor_slices({'inputs': dummy_inputs}).batch(len(dummy_sentence_pairs))\ndummy_batch = next(iter(dummy_ds))\ndummy_batch","9cd9d185":"r = prepare_masked_lm_batch(dummy_batch['inputs'])\noriginal_input_ids, masked_input_ids, mask_types = r['original_input_ids'], r['inputs']['input_ids'], r['mask_types']\nplot_mlm_batch(vocab, original_input_ids, masked_input_ids, mask_types, scaling=1.75, plot_original=True)","76332baf":"plot_mlm_batch_plotly(vocab, original_input_ids, masked_input_ids, mask_types, title='Hover the grid to see information.')","d0adc428":"tokenizer_name = 'jplu\/tf-xlm-roberta-base'\ntokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\nvocab = {v: k for k, v in tokenizer.get_vocab().items()}\n\nmlm_mask_prob = 0.35\nmask_type_probs=[0.5, 0.25, 0.25]\n\nprepare_masked_lm_batch = get_masked_lm_fn(\n    tokenizer=tokenizer,    \n    mlm_mask_prob=mlm_mask_prob,\n    mask_type_probs=mask_type_probs\n)","a90d3552":"dummy_inputs = tokenizer.batch_encode_plus(dummy_sentence_pairs, max_length=14, padding='max_length', truncation=True)\ndummy_inputs = dict(dummy_inputs)\ndummy_ds = tf.data.Dataset.from_tensor_slices({'inputs': dummy_inputs}).batch(len(dummy_sentence_pairs))\ndummy_batch = next(iter(dummy_ds))\ndummy_batch","22a4867e":"r = prepare_masked_lm_batch(dummy_batch['inputs'])\noriginal_input_ids, masked_input_ids, mask_types = r['original_input_ids'], r['inputs']['input_ids'], r['mask_types']\nplot_mlm_batch(vocab, original_input_ids[:, :], masked_input_ids[:, :], mask_types[:, :], scaling=1.75, plot_original=True)","a68b5204":"class Classifier(tf.keras.Model):\n\n    def __init__(self, transformer, use_mask=True):\n        \n        super(Classifier, self).__init__()\n        \n        self.transformer = transformer\n        self.dropout = tf.keras.layers.Dropout(rate=0.05)\n        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n        self.classifier = tf.keras.layers.Dense(3)\n        self.use_mask = use_mask\n\n    def call(self, inputs, training=False):\n\n        # Sequence outputs\n \n        mask = tf.cast(inputs['attention_mask'], tf.bool)\n        \n        x = self.transformer(inputs, training=training)[0]        \n        x = self.dropout(x, training=training)\n        if not self.use_mask:\n            mask = None\n        x = self.global_pool(x, mask=mask)\n        \n        return self.classifier(x)\n\ndef get_models(model_name, lr=1e-5, verbose=False):\n    \n    with strategy.scope():\n\n        lm_model = transformers.TFAutoModelForPreTraining.from_pretrained(model_name)\n\n        # False = transfer learning, True = fine-tuning\n        lm_model.trainable = True\n\n        # Just run a dummy batch, not necessary\n        dummy = lm_model(\n            inputs={\n                'input_ids':tf.constant(1, shape=[1, 64])\n            }\n        )\n\n        if verbose:\n            \n            print('Sample output from the masked LM model:\\n')\n            print(dummy)\n            \n            print('\\nMasked LM model\\n')\n            lm_model.summary()\n\n        transformer = lm_model.layers[0]\n        model = Classifier(transformer, use_mask=True)\n       \n        # Just run a dummy batch, not necessary\n        dummy = model(\n            inputs={\n                'input_ids':tf.constant(1, shape=[1, 64]),\n                'attention_mask':tf.constant(1, shape=[1, 64])\n            }\n        )\n\n        if verbose:\n            \n            print('Sample output from the classification model:\\n')\n            print(dummy)\n            \n            print('\\nClassification model\\n')\n            model.summary()    \n    \n        # Instiate an optimizer with a learning rate schedule\n        optimizer = tf.keras.optimizers.Adam(lr=lr)\n\n        # Only `NONE` and `SUM` are allowed, and it has to be explicitly specified.\n        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n\n        # Instantiate metrics\n        metrics = {\n            'mlm loss': tf.keras.metrics.Sum(),\n            'mlm acc': tf.keras.metrics.SparseCategoricalAccuracy(),\n            'train loss': tf.keras.metrics.Sum(),\n            'train acc': tf.keras.metrics.SparseCategoricalAccuracy()\n        }\n\n        return lm_model, model, loss_fn, optimizer, metrics","5eae0fbe":"def get_routines(lm_model, model, loss_fn, optimizer, metrics, batch_size):\n\n    def mlm_fine_tune_step(batch):\n\n        # The batch here is the batch received by each replica.\n        # However, The number of masked tokens `batch['nb_tokens_masked']` is the number of masked tokens\n        # in the whole batch before being distributed to TPU replicas.\n        inputs, mlm_labels, nb_tokens_masked = batch['inputs'], batch['mlm_labels'], batch['nb_tokens_masked']\n        \n        with tf.GradientTape() as tape:\n        \n            # sequence outputs\n            # shape = [batch_size, seq_len, vocab_size]\n            logits = lm_model(inputs, training=True)[0]\n\n            # get the places where the tokens should be predicted (masked \/ replaced \/ )\n            # shape = [batch_size, seq_len]\n            mlm_mask = (mlm_labels > -1)\n\n            # shape = [nb_masked_tokens]\n            labels_at_masked_tokens = tf.boolean_mask(mlm_labels, mlm_mask)\n\n            # shape = [nb_masked_tokens, vocab_size]\n            logits_at_masked_tokens = tf.boolean_mask(logits, mlm_mask)\n\n            # the mlm loss values are calculated only for the masked tokens\n            loss_mlm = loss_fn(\n                labels_at_masked_tokens,\n                logits_at_masked_tokens\n            )\n\n            # divide the number of masked tokens in the global batch, i.e. the whole batch that is distributed to different replicas.\n            loss_mlm = loss_mlm \/ tf.cast(nb_tokens_masked[0], dtype=tf.float32)\n        \n        gradients = tape.gradient(loss_mlm, lm_model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, lm_model.trainable_variables))\n        \n        metrics['mlm loss'].update_state(loss_mlm)\n        metrics['mlm acc'].update_state(labels_at_masked_tokens, logits_at_masked_tokens)\n        \n    def train_step(batch):\n\n        inputs, labels = batch['inputs'], batch['labels']\n        \n        with tf.GradientTape() as tape:\n\n            # shape = [batch_size, 3]\n            logits = model(inputs, training=True)\n\n            loss = loss_fn(labels, logits)\n\n            # divide by the global batch size, rather than the per replica batch size\n            loss = loss \/ batch_size\n        \n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        \n        metrics['train loss'].update_state(loss)\n        metrics['train acc'].update_state(labels, logits)\n\n    @tf.function\n    def dist_mlm_fine_tune_1_epoch(data_iter, steps_per_epoch):\n        \"\"\"\n        Iterating inside `tf.function` to optimized training time.\n        \"\"\"\n        \n        for _ in tf.range(steps_per_epoch):\n            strategy.run(mlm_fine_tune_step, args=(next(data_iter),))        \n        \n    @tf.function\n    def dist_train_1_epoch(data_iter, steps_per_epoch):\n        \"\"\"\n        Iterating inside `tf.function` to optimized training time.\n        \"\"\"\n        \n        for _ in tf.range(steps_per_epoch):\n            strategy.run(train_step, args=(next(data_iter),))\n            \n    @tf.function                \n    def predict_step(batch):\n\n        inputs = batch['inputs']\n\n        logits = model(inputs, training=False)\n        return logits\n\n    def predict_fn(dist_pred_ds):\n\n        all_logits = []\n        for batch in dist_pred_ds:\n\n            # PerReplica object\n            logits = strategy.run(predict_step, args=(batch,))\n\n            # Tuple of tensors\n            logits = strategy.experimental_local_results(logits)\n\n            # tf.Tensor\n            logits = tf.concat(logits, axis=0)\n\n            all_logits.append(logits)\n\n        # tf.Tensor\n        logits = tf.concat(all_logits, axis=0)\n\n        return logits\n            \n    return dist_mlm_fine_tune_1_epoch, dist_train_1_epoch, predict_fn","6fe5689b":"def get_datasets(\n        ds_spec,\n        tokenizer_name,\n        batch_size,\n        prediction_batch_size,\n        max_len,\n        mlm_fine_tuning_ds_spec=None\n    ): \n    \n        token_counter = Counter()\n\n        train_ds, nb_train_examples = get_unbatched_dataset(\n            ds_spec=ds_spec,\n            tokenizer_name=tokenizer_name,\n            shuffle=True,\n            max_len=max_len,\n            token_counter=token_counter,\n        )\n        train_ds = get_training_dataset(\n            train_ds, nb_train_examples, batch_size=batch_size, repeat=True\n        )\n\n        if mlm_fine_tuning_ds_spec is None:\n            mlm_fine_tuning_ds_spec = ds_spec\n        mlm_fine_tuning_ds, nb_mlm_fine_tuning_examples, mlm_fine_tuning_raw_ds = get_unbatched_dataset(\n            ds_spec=mlm_fine_tuning_ds_spec,\n            tokenizer_name=tokenizer_name,\n            shuffle=True,\n            max_len=max_len,\n            token_counter=token_counter,\n            return_raw_ds=True\n        )\n        mlm_fine_tuning_ds = get_training_dataset(\n            mlm_fine_tuning_ds, nb_mlm_fine_tuning_examples, batch_size=batch_size, repeat=True\n        )\n        \n        valid_ds, nb_valid_examples = get_unbatched_dataset(\n            ds_spec=['original valid'], tokenizer_name=tokenizer_name, max_len=max_len, token_counter=token_counter\n        )\n        valid_ds = get_prediction_dataset(valid_ds, prediction_batch_size)\n        valid_labels = next(iter(valid_ds.map(lambda batch: batch['labels']).unbatch().batch(len(original_valid))))\n        \n        test_ds, nb_test_examples = get_unbatched_dataset(\n            ds_spec=['original test'], tokenizer_name=tokenizer_name, max_len=max_len, token_counter=token_counter\n        )\n        test_ds = get_prediction_dataset(test_ds, prediction_batch_size)\n        \n        datasets = {\n            'train': train_ds,\n            'mlm_fine_tuning': mlm_fine_tuning_ds,\n            'valid': valid_ds,\n            'test': test_ds,\n            'valid labels': valid_labels,            \n            'nb_train_examples': nb_train_examples,\n            'nb_mlm_fine_tuning_examples': nb_mlm_fine_tuning_examples,\n            'nb_valid_examples': nb_valid_examples,\n            'nb_test_examples': nb_test_examples,\n            'mlm_fine_tuning_raw_ds': mlm_fine_tuning_raw_ds\n        }\n\n        return datasets, token_counter","3935efe0":"def init_tpu():\n\n    global strategy\n\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    except ValueError:\n        strategy = tf.distribute.get_strategy() # for CPU and single GPU  ","ab66d993":"def average_histories(histories):\n\n    if len(histories) == 0:\n        return {}\n    \n    averaged = {}\n    \n    history = histories[0]\n    for epoch in history:\n        averaged[epoch] = {}\n        for metric in history[epoch]:\n            averaged[epoch][metric] = sum([h[epoch][metric] for h in histories]) \/ len(histories)\n            \n    return averaged\n\ndef average_logits(logits):\n    \"\"\"Average the logits on the same set of examples over different predictions.\n\n    Args:\n        logits: A 4-D `numpy.array`. Each logits[i] is of shape [epochs, nb_examples, 3].\n        \n    Returns:\n        avg_logits: A 3-D `numpy.array` of shape [epochs, nb_examples, 3].\n    \"\"\"\n\n    return tf.math.reduce_mean(logits, axis=0)\n\ndef average_results(\n        valid_labels, all_mlm_fine_tuning_histories, all_histories,\n        all_valid_logits, all_test_logits):\n\n    nb_valid_examples = valid_labels.shape[0]\n    # shape = [1, nb_valid_examples]\n    valid_labels = tf.constant(valid_labels)[tf.newaxis, :]\n    # [epochs, nb_valid_examples]\n    valid_labels = tf.broadcast_to(valid_labels, shape=[epochs, nb_valid_examples])      \n    \n    avg_mlm_fine_tuning_history = average_histories(all_mlm_fine_tuning_histories)\n    avg_history = average_histories(all_histories)\n\n    # inputs: shape = [runs, epochs, nb_examples, 3]\n    # outputs: shape = [epochs, nb_examples, 3]\n    avg_valid_logits = average_logits(all_valid_logits)\n    avg_test_logits = average_logits(all_test_logits)\n\n    # shape = [epochs]\n    valid_loss = tf.reduce_mean(\n        tf.keras.losses.sparse_categorical_crossentropy(\n            valid_labels, avg_valid_logits,\n            from_logits=True,\n            axis=-1\n        ),\n        axis=1\n    ).numpy()\n\n    valid_acc = tf.reduce_mean(\n        tf.keras.metrics.sparse_categorical_accuracy(valid_labels, avg_valid_logits),\n        axis=1\n    ).numpy()\n\n    for epoch in range(len(avg_history)):\n        avg_history[epoch]['valid loss'] = float(valid_loss[epoch])\n        avg_history[epoch]['valid acc'] = float(valid_acc[epoch])\n\n    return avg_mlm_fine_tuning_history, avg_history, avg_valid_logits, avg_test_logits\n\ndef save_history(train_name, mlm_fine_tuning_history, history):\n\n    with open(f'mlm-fine-tuning-history-{train_name}.json', 'w', encoding='UTF-8') as fp:\n        json.dump(mlm_fine_tuning_history, fp, ensure_ascii=False, indent=4)            \n\n    with open(f'history-{train_name}.json', 'w', encoding='UTF-8') as fp:\n        json.dump(history, fp, ensure_ascii=False, indent=4) \n\ndef save_logits(train_name, valid_logits, test_logits):\n\n    np.save(f'valid-logits-{train_name}.npy', valid_logits.numpy())\n    np.save(f'test-logits-{train_name}.npy', test_logits.numpy())","284a7320":"class Trainer:\n\n    def __init__(\n        self,\n        ds_spec,\n        tokenizer_name,\n        mlm_fine_tuning_ds_spec=None,\n        batch_size_per_replica=16,\n        prediction_batch_size_per_replica=64,\n        max_len=64,\n        token_counts=None,\n        count_tokens=False,\n        mlm_mask_prob=0.15,\n        mask_type_probs=(0.8, 0.1, 0.1),\n        predict_special_tokens=False):\n        \"\"\"\n        Args:\n            ds_spec: See `get_unbatched_dataset`.\n            tokenizer_name: The name for a Hugging Face tokenizer.            \n            mlm_fine_tuning_ds_spec: See `get_unbatched_dataset`.\n            batch_size_per_replica: int\n            prediction_batch_size_per_replica: int\n            max_len: int, max length used for padding\/truncation.\n            token_counts: tf.float32 tensor of shape [vocab_size]. Could be `None`.\n            count_tokens: bool, If to count tokens in the datasets when `token_counts` is None.\n            mlm_mask_prob: See `get_masked_lm_fn`.\n            mask_type_probs: See `get_masked_lm_fn`.\n            predict_special_tokens: See `get_masked_lm_fn`.\n        \"\"\"\n\n        init_tpu()\n        \n        self.batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n        self.prediction_batch_size = prediction_batch_size_per_replica * strategy.num_replicas_in_sync        \n                \n        tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n        self.vocab = {v: k for k, v in tokenizer.get_vocab().items()}\n\n        self.mlm_mask_prob = mlm_mask_prob\n        self.mask_type_probs = mask_type_probs\n        \n        self.datasets, token_counter = get_datasets(\n            ds_spec,\n            tokenizer_name,\n            self.batch_size,\n            self.prediction_batch_size,\n            max_len,\n            mlm_fine_tuning_ds_spec=mlm_fine_tuning_ds_spec\n        )        \n\n        self.token_counts = token_counts\n        if token_counts is None and count_tokens:\n\n            self.token_counts = [0] * tokenizer.vocab_size\n            \n            for token_id, count in token_counter.items():\n                self.token_counts[token_id] = count\n\n        with open('token_counting.json', 'w', encoding='UTF-8') as fp:\n            json.dump(token_counter.most_common(), fp, indent=4, ensure_ascii=False)\n\n        prepare_masked_lm_batch = get_masked_lm_fn(tokenizer, self.mlm_mask_prob, self.mask_type_probs, token_counts=token_counts, predict_special_tokens=predict_special_tokens)\n        self.datasets['mlm_fine_tuning'] = self.datasets['mlm_fine_tuning'].map(lambda x: prepare_masked_lm_batch(x['inputs']))\n\n    def mlm_fine_tuning(self, dist_mlm_fine_tuning_1_epoch, epochs, metrics):\n\n        dist_train_ds_mlm = strategy.experimental_distribute_dataset(self.datasets['mlm_fine_tuning'])\n        dist_train_iter_mlm = iter(dist_train_ds_mlm)\n        steps_per_epoch = self.datasets['nb_mlm_fine_tuning_examples'] \/\/ self.batch_size\n        \n        print(f'\\nstart mlm finetuning for {epochs} epochs ...')\n\n        history = {}    \n        for epoch in range(epochs):\n\n            s = datetime.datetime.now()\n            \n            dist_mlm_fine_tuning_1_epoch(iter(dist_train_iter_mlm), steps_per_epoch)\n            \n            mlm_loss = metrics['mlm loss'].result() \/ steps_per_epoch\n            mlm_acc = metrics['mlm acc'].result()\n            \n            print(f'\\nmlm finetuning epoch: {epoch + 1}\\n')\n            print(f'mlm loss: {mlm_loss}')\n            print(f'mlm acc: {mlm_acc}')\n\n            metrics['mlm loss'].reset_states()\n            metrics['mlm acc'].reset_states()\n\n            e = datetime.datetime.now()\n            elapsed = (e - s).total_seconds()             \n            \n            print('train timing: {}'.format(elapsed))\n            print('-' * 40)\n            \n            history[epoch] = {\n                'mlm loss': float(mlm_loss),\n                'mlm acc': float(mlm_acc),\n                'train timing': elapsed\n            }            \n            \n        return history\n\n    def train(self, train_name, model_name, epochs, mlm_fine_tuning_epochs=0, lr=1e-5, runs=1, verbose=False):\n        \"\"\"Run the same configuration `runs` times.\n        \n        The validation loss and accuracy are computed using the averaged logits over runs.\n        The training loss and accuracy are averaged directly over runs.\n        \"\"\"\n        \n        all_mlm_fine_tuning_histories = []\n        all_histories = []\n        all_valid_logits = []\n        all_test_logits = []\n        \n        for run in range(runs):\n            \n            if run > 0:\n                verbose = False\n\n            mlm_fine_tuning_history, history, valid_logits, test_logits = \\\n                self._train(train_name + '-' + f'run-{run+1}', model_name, epochs, mlm_fine_tuning_epochs, lr=lr, verbose=verbose)\n\n            all_mlm_fine_tuning_histories.append(mlm_fine_tuning_history)\n            all_histories.append(history)\n            all_valid_logits.append(valid_logits)\n            all_test_logits.append(test_logits)\n\n        all_valid_logits = tf.stack(all_valid_logits)\n        all_test_logits = tf.stack(all_test_logits)\n        valid_labels = self.datasets['valid labels']\n        avg_mlm_fine_tuning_history, avg_history, avg_valid_logits, avg_test_logits = \\\n            average_results(valid_labels, all_mlm_fine_tuning_histories, all_histories, all_valid_logits, all_test_logits)\n        \n        save_history(train_name, avg_mlm_fine_tuning_history, avg_history)\n        save_logits(train_name, avg_valid_logits, avg_test_logits)\n\n        return avg_mlm_fine_tuning_history, avg_history, avg_valid_logits, avg_test_logits\n        \n    def _train(self, train_name, model_name, epochs, mlm_fine_tuning_epochs=0, lr=1e-5, verbose=False):\n        \"\"\"\n        Args:\n            dataset: tf.data.Dataset. Each batch is a dictionary containing at least `input_ids` and `attention_mask` as keys.\n        \"\"\"\n\n        init_tpu()\n\n        lm_model, model, loss_fn, optimizer, metrics = get_models(model_name, lr=lr, verbose=verbose)        \n        dist_mlm_fine_tuning_1_epoch, dist_train_1_epoch, predict_fn = get_routines(lm_model, model, loss_fn, optimizer, metrics, self.batch_size)\n\n        mlm_fine_tuning_history = self.mlm_fine_tuning(dist_mlm_fine_tuning_1_epoch, epochs=mlm_fine_tuning_epochs, metrics=metrics)        \n        \n        dist_train_ds = strategy.experimental_distribute_dataset(self.datasets['train'])        \n        dist_valid_ds = strategy.experimental_distribute_dataset(self.datasets['valid'])\n        dist_test_ds = strategy.experimental_distribute_dataset(self.datasets['test'])\n        \n        dist_train_iter = iter(dist_train_ds)\n\n        steps_per_epoch = self.datasets['nb_train_examples'] \/\/ self.batch_size\n\n        print(f'\\nstart training for {epochs} epochs ...\\n')    \n            \n        history = {}\n\n        valid_logits = []\n        test_logits = []\n\n        for epoch in range(epochs):\n            \n            s = datetime.datetime.now()\n            \n            dist_train_1_epoch(dist_train_iter, steps_per_epoch) \n\n            train_loss = metrics['train loss'].result() \/ steps_per_epoch\n            train_acc = metrics['train acc'].result()\n\n            print(f'epoch: {epoch + 1}\\n')\n            print(f'train loss: {train_loss}')\n            print(f'train acc: {train_acc}')\n            \n            metrics['train loss'].reset_states()\n            metrics['train acc'].reset_states()\n            \n            e = datetime.datetime.now()\n            elapsed = (e - s).total_seconds()             \n\n            logits = predict_fn(dist_valid_ds)\n            valid_logits.append(logits)\n\n            valid_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(self.datasets['valid labels'], logits, from_logits=True, axis=-1))\n            valid_acc = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(self.datasets['valid labels'], logits))\n\n            logits = predict_fn(dist_test_ds)\n            test_logits.append(logits)            \n            \n            print('\\nvalid loss: {}'.format(valid_loss))\n            print('valid acc: {}\\n'.format(valid_acc))\n            \n            print('train timing: {}'.format(elapsed))\n            print('-' * 40)           \n\n            history[epoch] = {\n                'train loss': float(train_loss),\n                'train acc': float(train_acc),\n                'valid loss': float(valid_loss),\n                'valid acc': float(valid_acc),                \n                'train timing': elapsed\n            }\n            \n        # shape = [epochs, nb_examples, 3]\n        valid_logits = tf.stack(valid_logits)\n        test_logits = tf.stack(test_logits)\n\n        save_history(train_name, mlm_fine_tuning_history, history)\n        save_logits(train_name, valid_logits, test_logits)        \n        \n        return mlm_fine_tuning_history, history, valid_logits, test_logits","2b067623":"# model_name = 'bert-base-multilingual-cased'\nmodel_name = 'jplu\/tf-xlm-roberta-base'\n# model_name = 'jplu\/tf-xlm-roberta-large'\n\nmax_len = 96\n\nbatch_size_per_replica=16\nprediction_batch_size_per_replica=128\n\nepochs = 10\nlr = 1e-5\n\nds_spec = {\n    'original train': None\n}\n\nmlm_fine_tuning_ds_spec = {\n    'original train': None,\n    'original valid': None,\n    'original test': None,\n    'mnli train': 30000,\n    'mnli valid 1': None,\n    'mnli valid 2': None,\n    'xnli valid': None\n}\n\ntoken_counts=None\ncount_tokens = False\nmlm_mask_prob=0.15\nmask_type_probs = (0.8, 0.1, 0.1)\n\npredict_special_tokens = False\n\nruns = 3","8057bfcb":"trainer = Trainer(\n    ds_spec=ds_spec,\n    tokenizer_name=model_name,\n    mlm_fine_tuning_ds_spec=mlm_fine_tuning_ds_spec,\n    batch_size_per_replica=batch_size_per_replica,\n    prediction_batch_size_per_replica=prediction_batch_size_per_replica,\n    max_len=max_len,\n    token_counts=token_counts,\n    count_tokens=count_tokens,\n    mlm_mask_prob=mlm_mask_prob,\n    mask_type_probs=mask_type_probs,\n    predict_special_tokens=predict_special_tokens\n)","360740ff":"print(json.dumps({k: str(v) for k, v in trainer.datasets.items() if k != 'mlm_fine_tuning_raw_ds'}, indent=4))","7ab26acf":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=trainer.datasets['mlm_fine_tuning_raw_ds']['language'], histnorm='probability density', name=\"MLM finetuning\"))\nfig.add_trace(go.Histogram(x=original_train['language'], histnorm='probability density', name=\"train\"))\nfig.add_trace(go.Histogram(x=original_valid['language'], histnorm='probability density', name=\"valid\"))\nfig.add_trace(go.Histogram(x=original_test['language'], histnorm='probability density', name=\"test\"))\n\nfig.update_layout(barmode='group', xaxis={'categoryorder':'array', 'categoryarray':np.unique(langs).tolist()})\nfig.show()\n\nfig.write_image('mlm finetuning lang distributions.png')","02d69276":"ds_iter = iter(trainer.datasets['mlm_fine_tuning'])","dad8aed0":"batch = next(ds_iter)\nprint(batch)\noriginal_input_ids, masked_input_ids, mask_types = batch['original_input_ids'], batch['inputs']['input_ids'], batch['mask_types']","70d95cc2":"plot_mlm_batch_plotly(trainer.vocab, original_input_ids, masked_input_ids, mask_types, title='Hover the grid to see information.')","4dd42111":"mlm_fine_tuning_epochs = 0\n\ntrain_name = 'mlm finetuning 0'\n\nmlm_fine_tuning_history_0, history_0, valid_logits_0, test_logits_0 = trainer.train(\n    train_name,\n    model_name, epochs=epochs, mlm_fine_tuning_epochs=mlm_fine_tuning_epochs, lr=lr, runs=runs, verbose=True\n)","ead003bb":"mlm_fine_tuning_epochs = 3\n\ntrain_name = 'mlm finetuning 3'\n\nmlm_fine_tuning_history_3, history_3, valid_logits_3, test_logits_3 = trainer.train(\n    train_name,\n    model_name, epochs=epochs, mlm_fine_tuning_epochs=mlm_fine_tuning_epochs, lr=lr, runs=runs, verbose=True\n)","d552dd05":"def plot(history, metric, desc='history'):\n    \"\"\"\n    metric: 'loss' or 'acc'\n    \"\"\"\n    \n    h = {\n        f'train {metric}': [history[epoch][f'train {metric}'] for epoch in history],\n        f'valid {metric}': [history[epoch][f'valid {metric}'] for epoch in history]\n    }\n        \n    fig = px.line(\n        h, x=range(1, len(history) + 1), y=[f'train {metric}', f'valid {metric}'], \n        title=f'model {metric}', labels={'x': 'Epoch', 'value': metric},\n        color_discrete_sequence=['deepskyblue', 'limegreen']\n    )\n    fig.show()\n    fig.write_image(f'{desc}.png')\n    \ndef plot_2(history1, history2, metric, desc1, desc2):\n\n    h = {\n        f'train {metric} - {desc1}': [history1[epoch][f'train {metric}'] for epoch in history1],\n        f'valid {metric} - {desc1}': [history1[epoch][f'valid {metric}'] for epoch in history1],\n        f'train {metric} - {desc2}': [history2[epoch][f'train {metric}'] for epoch in history2],\n        f'valid {metric} - {desc2}': [history2[epoch][f'valid {metric}'] for epoch in history2]\n    }\n\n    fig = px.line(\n        h, x=range(1, len(history1) + 1), y=[f'train {metric} - {desc1}', f'valid {metric} - {desc1}', f'train {metric} - {desc2}', f'valid {metric} - {desc2}'], \n        title=f'model {metric}', labels={'x': 'Epoch', 'value': metric},\n        color_discrete_sequence=['deepskyblue', 'limegreen', 'goldenrod', 'hotpink']\n    )\n    fig.show()\n    \n    fig.write_image(f'{metric}-{desc1}-vs-{desc2}.png')\n    fig.write_html(f'{metric}-{desc1}-vs-{desc2}.html')","6c39e731":"plot_2(history_0, history_3, 'loss', desc1='mlm finetuning 0', desc2='mlm finetuning 3')\nplot_2(history_0, history_3, 'acc', desc1='mlm finetuning 0', desc2='mlm finetuning 3')","8d0d6294":"! cp '\/kaggle\/input\/public-results-for-contradictory-my-dear-watson\/mbert-loss-mlm finetuning 0-vs-mlm finetuning 3.html' 'mbert_loss.html'\n! cp '\/kaggle\/input\/public-results-for-contradictory-my-dear-watson\/mbert-acc-mlm finetuning 0-vs-mlm finetuning 3.html' 'mbert_acc.html'\n\nfn = 'mbert_loss.html'\ndisplay(IFrame(src=fn, width=720, height=500))\n\nfn = 'mbert_acc.html'\ndisplay(IFrame(src=fn, width=720, height=500))","e8535cee":"!ls -l","772e47d0":"### Mask tokens<a id='mask-tokens'><\/a>\n\n#### This is the main method that peforms token masking for training MLM language models. It is a translation into [TensorFlow](https:\/\/www.tensorflow.org\/) from [Hugging Face](https:\/\/github.com\/huggingface\/transformers\/blob\/390c1285925dd119705e69a266202ef04490d012\/examples\/distillation\/distiller.py)'s code, which is originally in [PyTorch](https:\/\/pytorch.org\/).\n\nThe `mlm_smoothing` argument is used to emphasize more rare tokens, see [Cross-lingual Language Model Pretraining (XLM)](https:\/\/arxiv.org\/pdf\/1901.07291.pdf). If `token_counts` is `None`, it will be a uniform distribution over the vocabulary, except for some special tokens.","4185d648":"### MLM finetuning 3 epochs<a id='mlm-fine-tuning-3-epochs'><\/a>","49d48aee":"#### Get a combined pandas.DataFrame","efa41b9a":"### Visualize the original and masked tokens<a id='visualize-mlm-batch'><\/a>","bb25b804":"sanity check","37c44023":"### The Cross-Lingual NLI Corpus (XNLI)<a id='xnli-datasets'><\/a>\n\n#### The [MNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) contains only english sentences. Let's load the [Cross-lingual NLI Corpus (XNLI)](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/) dataset. It contains only validation and test dataset, not training examples.","cca79cb1":"### MLM finetuning: 0 vs. 3 epochs<a id='0-vs-3'><\/a>","3b5cc2e3":"#### Visualize a batch of MLM finetuning dataset<a id='check-mlm-fine-tuning-batch'><\/a>","6025b364":"# Masked Language Model<a id='mlm'><\/a>","37fa0d6b":"### History plotting methods","1bbd8f67":"batched dataset","c75ccf7c":"## Models<a id='models'><\/a>\n\n#### Keep in mind that anything that creates variables that will be used in a distributed way must be created inside [strategy.scope](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/experimental\/TPUStrategy#scope), this includes:\n\n  * model creation\n  * optimizer\n  * metrics\n  * sometimes, checkpoint restore\n  * any custom code that creates distributed variables\n  \n#### We use [transformers.TFAutoModelForPreTraining](https:\/\/huggingface.co\/transformers\/model_doc\/auto.html#tfautomodelforpretraining) instead of [transformers.TFAutoModel](https:\/\/huggingface.co\/transformers\/model_doc\/auto.html#tfautomodel) to load Hugging Face [transformers](https:\/\/huggingface.co\/transformers\/index.html)' models equipped with language model (LM) heads for pretraining. ","7e9a5368":"### Create a tokenizer and get the masking method\n\n#### Let's try with Bert tokenizer first.","bfce9923":"### Tokenize the examples","bbddbe4a":"## Routines<a id='routines'><\/a>\n\n#### We use [strategy.run](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#run) to perform distributed computations on TPU replicas.","70ddda83":"#### From pandas.DataFrame to tf.data.Dataset","0726066a":"# Datasets<a id='datasets'><\/a>","cd2b4eaf":"## Training configurations<a id='training-config'><\/a>\n\n#### The configuration for experiments.\n\nFor each setting, we will train 3 times and take the averaged logtis over the runs on the validation dataset to compute the predictions. ","dabb91f6":"### Language distributions","e12d7356":"## Visualize mask language model<a id='visualize-mlm'><\/a>\n\n#### Let's visualize MLM with a simple example!","9262d921":"#### Language distributions - MLM finetuning dataset","f58dd63c":"unbatched dataset","fa00d197":"### Results for Multilingual Bert model<a id='results-multilingual-bert'><\/a>\n#### Here is the comparision obtained from training with [bert-base-multilingual-cased](https:\/\/github.com\/google-research\/bert\/blob\/master\/multilingual.md) model (using [Hugging Face](https:\/\/huggingface.co\/) pretrained model).","0574270d":"## Implementation of masking tokens<a id='imp-mlm'><\/a>","11e17aa4":"### Sample without replacement in TensorFlow<a id='sample-no-replacement'><\/a>\n\nIn PyTorch, we can use\n\n```\n    torch.multinomial(..., replacement=False)\n```\n\nto perform sampling without replacement. In TensorFlow, there is no direct API to perform this operation.\n\nFollowing the discussion [here on GitHub](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/9260#issuecomment-437875125),\nwe use the [Gumbel-max trick]( https:\/\/timvieira.github.io\/blog\/post\/2014\/07\/31\/gumbel-max-trick\/) to implement it.","22deff89":"### Visualization helpers","1d497e8f":"# Models and training procedures<a id='models-procedures'><\/a>","467df316":"#### Bert<a id='visualize-mlm-batch-bert'><\/a>","754f5762":"### Get datasets for trainer","62c6687a":"#### example usage & sanity check","71cce228":"### Initialize TPU<a id='init-tpu'><\/a>\n\n#### We reinitialize TPU to free memory after each training.","e2ec0945":"### MLM loss calculation<a id='mlm-loss-calculation'><\/a>\n\n 1. We use the indices in `mlm_labels` where the values are non-negative to determine if a token is masked its original token (if masked). The MLM loss value is computed over the masked tokens, not over the whole batch.\n \n 2. A batch of token sequences is distributed to different replicas, however the number of masked tokens received by each replica might be different. When we compute the MLM loss value on a replica, we take the sum of the per example MLM loss values, and divide it by the number of masked tokens over the whole batch before distributed to the replicas (which is encoded in `batch['nb_tokens_masked']`), rather than the number of masked tokens over the batch received on that replica.\n \n 3. This is because the gradients are synchronized over the replicas by summing them before the optimizer updates the model parameters. Therefore, our way of calculating MLM loss value gives the gradient corresponded to the averaged MLM loss value over the whole distributed batch, which is the correct loss value.\n \n**References**:\n\n  1. [Custom training with tf.distribute.Strategy - Define the loss function](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training#define_the_loss_function)\n\n  2. [Hugging Face's distillation code - 1](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/distillation\/distiller.py#L112) and [Hugging Face's distillation code - 2](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/distillation\/distiller.py#L423)","663f5d0b":"# Configuration\n\n#### If to load external datasets.","a86d6bba":"## Training<a id='training'><\/a>\n\n#### With the same training configuration, we finetune models for 0, 1, 2 and 3 epochs on the competitioin and external datasets using the MLM objective, then continue the traiing on the labeled competition training dataset using the classification objective.\n\n#### Each setting is running for 3 times, and the validation and test predictions are calculated based on the averaged logits over the 3 runs.","304a7e26":"# Table of Contents\n\n1. [Datasets](#datasets)\n  * [Competition datasets](#competition-datasets)\n  * [External datasets](#external-datasets)\n    - [The Multi-Genre NLI Corpus (MNLI)](#mnli-datasets)\n    - [The Cross-Lingual NLI Corpus (XNLI)](#xnli-datasets)\n  * [Working with tf.data.Dataset](#tf-dataset)\n2. [Masked Language Model](#mlm)\n  * [Implementation of masking tokens](#imp-mlm)\n    - [Sample without replacement in TensorFlow](#sample-no-replacement)\n    - [Mask tokens](#mask-tokens)\n    - [Check outputs](#check-outputs)\n  * [Visualize mask language model](#visualize-mlm)\n    - [Bert](#visualize-mlm-bert)\n    - [Roberta](#visualize-mlm-batch-roberta)\n3. [Models and training procedures](#models-procedures) \n  * [Models](#models)\n  * [Routines](#routines)\n    - [MLM loss calculation](#mlm-loss-calculation)\n  * [Trainer](#trainer)\n    - [Initialize TPU](#init-tpu)\n4. [Train](#train)\n  * [Training configurations](#training-config)\n  * [Visualize a batch of MLM finetuning dataset](#check-mlm-fine-tuning-batch)\n  * [Training](#training)\n    - [No MLM finetuning](#no-mlm-fine-tuning)\n    - [MLM finetuning 3 epochs](#mlm-fine-tuning-3-epochs)\n  * [Comparison](#comparison)\n    - [MLM finetuning: 0 vs. 3 epochs](#0-vs-3)\n    - [Results for Multilingual Bert model](#results-multilingual-bert)\n5. [Conclusion](#conclusion)","2535bd84":"# Conclusion <a id='conclusion'><\/a>\n\nIn this notebook, we implement token masking in TensorFlow which is used for training models like [Bert](https:\/\/arxiv.org\/pdf\/1810.04805.pdf) or [ (XLM)-Roberta](https:\/\/arxiv.org\/pdf\/1907.11692.pdf). We also visualize some masked batches of tokens to verify the effect of masking.\n\nWe finetune the loaded models using MLM objective on a combination of the competition datasets (including the test dataset) and some external datasets: [MNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) and [XNLI](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/), which are loaded by using [Hugging Face](https:\/\/huggingface.co)'s recent [nlp](https:\/\/huggingface.co\/nlp\/) library. In this stage, we don't use the labels from these datasets - only the original (non-masked) tokens as labels for the MLM objective.\n\nAfter MLM finetuning, we train NLI classifiers. We compare the performances of NLI classifiers with and without using MLM finetuning. The results show that MLM finetuning before training a classifier increases the model accuracy by 1 ~ 2 % (after a few epochs).\n\nThere are some potential enhancements:\n* dynamic whole word masking\n* cluster tokens into languages, and for random replacement, only use the tokens in the same language as an input example. \n\nHope you enjoy this notebook!","771cfa60":"# Train<a id='train'><\/a>\n\n#### Now, let's perform training!","40c58b39":"## Comparison<a id='comparison'><\/a>","b25cb422":"### Check with another tokenizer\n\n#### Roberta tokenizer","596a4773":"<center><img src=\"https:\/\/raw.githubusercontent.com\/chiapas\/kaggle\/master\/competitions\/contradictory-my-dear-watson\/header.png\" width=\"1000\"><\/center>\n<br>\n<center><h1>Detecting contradiction and entailment in multilingual text using TPUs<\/h1><\/center>\n<br>\n<center><h2>Masked Language Model (MLM)<\/h2><\/center>\n<br>\n\n#### Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\n#### In this notebook, we implement the Masked Language Model (MLM), which is used for training [Bert model](https:\/\/arxiv.org\/pdf\/1810.04805.pdf) and [(XLM)-Roberta model](https:\/\/arxiv.org\/pdf\/1907.11692.pdf). The implemention is a translation into [TensorFlow](https:\/\/www.tensorflow.org\/) from [Hugging Face](https:\/\/github.com\/huggingface\/transformers\/blob\/390c1285925dd119705e69a266202ef04490d012\/examples\/distillation\/distiller.py)'s code, which is originally in [PyTorch](https:\/\/pytorch.org\/). We also visualize the effects of token masking.\n\n#### Here are some features of the implementation:\n  * <p style=\"color:blue\">dynamic token masking in TensorFlow operations, therefore it could be used as a tf.data.Dataset transformation.<\/p>\n  * <p style=\"color:blue\">the number of tokens to mask is calculated based on non-padding tokens (and optionally, excluding other special tokens)<\/p>\n  * <p style=\"color:blue\">including a smoothing option to mask rare tokens more frequently<\/p>\n\n#### [Dezs\u0151 Ribli](https:\/\/www.kaggle.com\/riblidezso) published a notebook [Finetune XLM-Roberta on Jigsaw test data with MLM](https:\/\/www.kaggle.com\/riblidezso\/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm) that also uses MLM finetuning. However, there are some differences between his notebook and this notebook:\n  * Dezs\u0151 Ribli's [notebook](https:\/\/www.kaggle.com\/riblidezso\/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm) implements token masking in numpy operations, and the tokens are statically masked before MLM finetuning. In this notebook, the maksing is implemented in pure TensorFlow operations, and the tokens are masked dynamically during MLM finetuning as tf.data.Dataset transformation.\n  * In Dezs\u0151 Ribli's [notebook](https:\/\/www.kaggle.com\/riblidezso\/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm), the number of tokens to be maksed is calculated before ignoring special tokens (in particular, the [PAD] token). However, in the case where we have a lot of padding tokens due to short texts, we will get a higher ratio of token being masked. This notebook carefully determines the number of tokens to be maksed based on the number the actual (non-special) tokens.\n  * The [loss calculation](https:\/\/www.kaggle.com\/riblidezso\/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm#Define-stuff-for-the-masked-language-modelling,-and-the-custom-training-loop) in that notebook uses `tf.nn.compute_average_loss` with the number of sentences in the whole distributed batch for the argument `global_batch_size`. However, the MLM loss should be the averaged per example loss over the masked tokens for MLM prediction in the whole distributed batch. This notebook implements the [correct loss value calculation](#mlm-loss-calculation).\n\n#### We include external NLI datasets, along with the competition datasets, to perform MLM finetuning the pretrained models (e.g. Bert or Roberta) using MLM, before training on the competition (labeled) dataset. These includes\n\n* [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/)\n* [Cross-lingual NLI Corpus (XNLI)](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/)\n\n#### We use [Hugging Face](https:\/\/huggingface.co\/) recent library [nlp](https:\/\/huggingface.co\/nlp\/) to work with these datasets, which is quite easy to use. Note that these external datasets are only used for MLM finetuning. They are excluded durining training the NLI classifier later, since some examples in the competition test dataset comes from [MNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/).","46f4d72c":"## Competition datasets<a id='competition-datasets'><\/a>","6570b322":"#### From the figures, we see that:\n* After a few epochs (usually 3-6 epochs), the classifier using a MLM finetuned LM model outperforms the one without using MLM finetuned LM model by 1 ~ 2 %.\n* For loss values, despite we always have higher training loss with MLM finetuning 3 epochs, its validation loss eventually becomes smaller than that of without mlm finetuning. Also, from their validation loss values, the degree of overfitting when using MLM finetuning is less than without using MLM finetuning, and it also starts at a later epoch.\n* At the first few epochs, MLM finetuning gives worse results. A possible explanation is that MLM finetuning makes a LM model lose some of its overall language knowledge, therefore in an early stage of training on labeled datasets, it can't extract good features for predicting labels. However, once the training epochs increases, the knowledge gained from MLM finetuning can help it to better predict a downstream task labels.\n* However, in some different notebook runnings, I saw the same performance for training with\/without MLM finetuning. Probably the training\/validation split and the sampling from external datasets (for MLM finetuning) also play some role here.","079d857f":"#### Roberta<a id='visualize-mlm-batch-roberta'><\/a>\n\nDue to some missing glyphs in the font, we get some tokens rendered as\n\n![missing.PNG](attachment:missing.PNG)","e549c15a":"#### Import","3e96adc8":"### The Multi-Genre NLI Corpus (MNLI)<a id='mnli-datasets'><\/a>\n\n#### First, let's load the [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) dataset. It contains $433000$ sentence pairs annotated with textual entailment information.","959726d5":"### Create dummy examples","21745361":"## External datasets<a id='external-datasets'><\/a>\n\n#### Let's use Hugging Face new library [nlp](https:\/\/huggingface.co\/nlp\/), to get more NLI datasets.","941b162b":"## Working with tf.data.Dataset<a id='tf-dataset'><\/a>","effee1be":"# Preview token masking in Masked Language Models (MLM)\n\n#### Here is how a masked batch of tokens looks like:\n\nIt consists:\n* tokens not to predict\n  1. word tokens: green\n  2. (optional) special tokens: green (we can choose to predict or not speical tokens like `[CLS]` or `[SEP]`, etc.)\n  3. padding tokens: blue\n* tokens to predict:\n  1. the token is masked: gray\n  2. the token is kept as it is: yellow\n  3. the token is replaced by a random tokens in the vocabulary: pink\n\nHere we use a higher masking frequency (`0.35`), so you may find that too many tokens are masked for training MLM. This is only for visualization. In the experiment, we will use a frequency of `0.15`, which is used for traininig [Bert](https:\/\/arxiv.org\/pdf\/1810.04805.pdf).\n\nTo visualize (in an interactive way) a real batch of masked tokens used for training, see [Visualize a batch of MLM finetuning dataset](#check-mlm-fine-tuning-batch). The tokens are not displayed directly, but you can hover the grid to see the token information.\n\n### ![mlm.png](attachment:mlm.png)","082152f3":"### No MLM finetuning<a id='no-mlm-fine-tuning'><\/a>","0760aa29":"### Trainer<a id='trainer-class'><\/a>","0012f3b2":"## Create a trainer<a id='create'><\/a>","a5090baa":"#### Interactive\n\nWhen the batch becomes large, it is impossible to display the tokens directly as in the above figure. In this case, we will use [Plotly](https:\/\/plotly.com\/) to draw the batch over which you can hover to see several information, just like the figure in the next cell.\n\n![Annotation%202020-08-21%20152241.png](attachment:Annotation%202020-08-21%20152241.png)","604ad6e9":"#### check outputs of masking tokens<a id='check-outputs'><\/a>","7ff747df":"## Trainer<a id='trainer'><\/a>\n\n#### Just a custom class to make training easier. ","8aa1c409":"#### Sanity check"}}