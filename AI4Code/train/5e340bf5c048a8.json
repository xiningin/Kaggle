{"cell_type":{"0371de5a":"code","2b94e13c":"code","0bf90e18":"code","7f00d3d2":"code","96a6dfa6":"code","81b72c7e":"code","a4abf278":"code","2c3103e7":"code","54cc260a":"markdown","b4f591d4":"markdown"},"source":{"0371de5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b94e13c":"# Import necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\n%matplotlib inline","0bf90e18":"# Load the dataset\nregdf = pd.read_csv('\/kaggle\/input\/polynomial-regression\/regression_table.csv')\nregdf.head(10)","7f00d3d2":"# Get statistical summary from dataset\n\nprint('The shape of the dataframe is - {}'.format(regdf.shape))\nprint('The number of entries - {}'.format(regdf.size))\nprint('-'*60)\nprint('The basic statistics -\\n{}'.format(regdf.describe()))\nprint('-'*60)\nprint('Unique values per column -\\n{}'.format(regdf.nunique()))\nprint('-'*60)\nprint('Null values per column -\\n{}'.format(regdf.isnull().sum()))\nprint('-'*60)\nprint('Data types per column -\\n{}'.format(regdf.dtypes))","96a6dfa6":"# Generate a Scatter Plot to Visualize the Pattern of the Data\n\nplt.figure(figsize=(18, 9))\nplt.style.use('seaborn')\nplt.scatter(regdf.X, regdf.Y)\nplt.title('Scatter Plot')\nplt.xlabel('X Values')\nplt.ylabel('Y Values')\nplt.show()","81b72c7e":"# A Quick Implementation of Different Order Polynomial Regression Models. \n# Choose the highest order of polynomials (degree) upto which we want to fit our model & visualize. Then plot the predicted values using trained polynomial regression models along with depicting their  \ud835\udc5f2  scores\n\n\ndegree = 20\nfig, ax = plt.subplots(math.ceil(degree\/2),2, figsize=(12, 20))\nax=ax.flatten()           # Converting multidimensional array into a regular \"single\" dimensional array.\npoly_x = []\nmodel = []\nfor i in range(degree):\n                          # Create and store each model into \"model\" and polynomial features into \"poly_x\"\n    poly_reg = PolynomialFeatures(degree=i+1)\n    x = poly_reg.fit_transform(regdf[['X']])\n    poly_x.append(x)\n    model.append(LinearRegression())\n    model[i].fit(x, regdf['Y'])\n    # Plot the predicted regression curve on the scatterplot of feature and target variable\n    ax[i].scatter(regdf.X, regdf.Y)\n    ax[i].plot(regdf['X'], model[i].predict(x), color='r')\n    ax[i].set_xlabel('X Values')\n    ax[i].set_ylabel('Y Values')\n    # Include r2 score in title\n    ax[i].set_title('Degree {} polynomial, r2 score = {:.3f}'.format(i+1, r2_score(regdf['Y'], model[i].predict(x))))\n\nfig.tight_layout()  ","a4abf278":"degree = 20\nfig, ax = plt.subplots(math.ceil(degree\/2),2, figsize=(12, 20), sharey=True)\nax=ax.flatten()\nplt.style.use('seaborn')\ntrain_sizes = [150, 300, 450, 600, 800, 950]  # Specify absolute sizes of the training sets for calculating scores\nK = 20                                        # Choosing K for K-Fold Cross-Validation\nestimator = LinearRegression()\nrmseval = []\nrmsetrain = []\nfor i in range(20):\n    # Generate \"i+1\"th degree polynomial features to train model\n    poly_reg = PolynomialFeatures(degree=i+1)\n    x = poly_reg.fit_transform(regdf[['X']])\n    # Get scores on training and validation sets at predetermined training sizes\n    train_sizes, train_scores, validation_scores = learning_curve(estimator = estimator, X = x, y = regdf['Y'], cv = K, train_sizes=train_sizes, scoring = 'neg_mean_squared_error')\n    # Get the mean training and validation scores of different training or validation sets for each training size\n    train_scores_mean = np.sqrt(-train_scores.mean(axis = 1))           # Also MSE is converted to RMSE\n    validation_scores_mean = np.sqrt(-validation_scores.mean(axis = 1)) # Also MSE is converted to RMSE\n    # Store the mean training and validation scores for the final(6th) training size\n    rmseval.append(validation_scores_mean[5])\n    rmsetrain.append(train_scores_mean[5])\n    # Display error metrics for different regression models\n    print('Degree={}'.format(i+1))\n    print('train sizes = {}'.format(train_sizes))\n    print('train error = {}'.format(train_scores_mean))\n    print('validation error = {}'.format(validation_scores_mean))\n    print('-'*30)\n    # Plot learning curve\n    ax[i].plot(train_sizes, train_scores_mean, label = 'Training error')\n    ax[i].plot(train_sizes, validation_scores_mean, label = 'Validation error')\n    ax[i].set_yscale('log')\n    ax[i].set_ylabel('RMSE', fontsize = 14)\n    ax[i].set_xlabel('Training set size', fontsize = 14)\n    ax[i].legend()\n    ax[i].set_title('Learning curve for degree {} polynomial'.format(i+1))\n\nfig.tight_layout()","2c3103e7":"n = 10\nintercept = model[n-1].intercept_\ncoefficient = model[n-1].coef_[1:]\nprint('Intercept : {}\\nCoefficient : {}'.format(intercept, coefficient))","54cc260a":"**Get the Parameters of Predicted  \ud835\udc5b\ud835\udc61\u210e  Degree Polynomial Function**\n\nSince the results show best performance for using  10\ud835\udc61\u210e  order polynomial, we find the coefficients and intercept terms for the model. 10th degree polynomial function with one parameter is -\n\ny=f(x)=ax+b \ud835\udc652 +c \ud835\udc653 +d \ud835\udc654 +e \ud835\udc655 +f \ud835\udc656 +g \ud835\udc657 +h \ud835\udc658 +i \ud835\udc659 +j \ud835\udc6510 +k\n\nHere, the coefficients are from a-j and intercept is k.\n\n[Note: The coefficients and intercept are collected from a previously trained model using all the sample data.]","b4f591d4":"**Comparing Performances of Different Estimators**\n\nK-fold Cross-Validation is used to train & validate each model. Learning curve was plotted for each model to understand the training and validation error changes with increasing training set size. Also, learning curves would help to diagnose if there is overfitting or underfitting issues. 20 different models were generated and 20-fold cross-validation was chosen to fit the models and calculate the error metrics. Root Mean Squared Error (RMSE) is used as the error metric in calculation. Large value of k is used because -\n\nThe sample size is small\nTo keep the training set large enough in order to get a good fit."}}