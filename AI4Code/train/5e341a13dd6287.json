{"cell_type":{"903b87cc":"code","b45e4096":"code","c029e57e":"code","e4724ef2":"code","98cefe44":"code","070914d2":"code","4ed83c8b":"code","b437e7b7":"code","12eb2c5f":"code","6e4aa727":"code","12255a0c":"code","30486f08":"code","ffe3ea5b":"markdown","1127f732":"markdown"},"source":{"903b87cc":"# !pip install numpy\n# !pip install tensorflow\n!pip install spektral","b45e4096":"import numpy as np\nimport tensorflow as tf\nimport spektral","c029e57e":"#load images with spektral (original code doesnt work, see Daniel Boyle's comment for the working variant)\n# adj, features, labels, train_mask, val_mask, test_mask = spektral.datasets.citation.load_data(dataset_name='cora')\n#train mask tells us which nodes belong to the training set, validation set, and test set\n\nfrom spektral.datasets.citation import Cora #can be trained quickly on CPU, good for research\ndataset = Cora()\ngraph = dataset[0]\nadj, features, labels = graph.a, graph.x, graph.y\ntrain_mask, val_mask, test_mask = dataset.mask_tr, dataset.mask_va, dataset.mask_te\n\n#get data in sparse format so we convert to a dense representation (sparse given in case of a large graph loaded)\n# features = features.todense()\/\n#apparently with the adjustment the loaded dataset is in npy array so it doesn't need to be made dense\n\nadj = adj.todense() + np.eye(adj.shape[0])\nfeatures = features.astype('float32')\nadj = adj.astype('float32') #plays nice in TF\n\nprint(features.shape) #verify size of graph and features\nprint(adj.shape)\nprint(labels.shape)\n\nprint(np.sum(train_mask)) #gives 0 1 array of whether they are in training or test so sum will tell us amount of each\nprint(np.sum(val_mask))\nprint(np.sum(test_mask))\n#works","e4724ef2":"def masked_softmax_cross_entropy(logits, labels, mask):\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels) #compute loss in every node\n    mask = tf.cast(mask, dtype=tf.float32)#use mask to only use the nodes that belong in given dataset also set to correct dtype\n    mask \/= tf.reduce_mean(mask)#divide the mask by average value\n    loss *= mask #get average of the mask with the loss\n    return tf.reduce_mean(loss)\n\ndef masked_accuracy(logits, labels, mask):\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) # compares argmax of the logits with the argmax of the labels across the feature axis to predict at every node\n    accuracy_all = tf.cast(correct_prediction, tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mask \/= tf.reduce_mean(mask)\n    accuracy_all *= mask\n    return tf.reduce_mean(accuracy_all) #gives accuracy at the nodes we care about (mask elements)\n    #DTYPE ISSUE\n    ","98cefe44":"#the following model is not expressive enough for message passing networks\ndef gnn(fts, adj, transform, activation): #fts = node feature matrix\n    seq_fts = transform(fts) #transform each node individually\n    ret_fts = tf.matmul(adj, seq_fts) #matrix multiply with some approrpiate adjacency matrix to recombine it across neighbors\n    return activation(ret_fts)\n","070914d2":"def train_cora(fts, adj, gnn_fn, units, epochs, lr): #features, adjacency matrix, gnn function, how many dimensions in latent features, learning rate\n    lyr_1 = tf.keras.layers.Dense(units) #transform with dense layer(1st layer is hidden layer)\n    lyr_2 = tf.keras.layers.Dense(7) #7 outputs;multiclass identification\n\n    def cora_gnn(fts, adj):\n        hidden = gnn_fn(fts, adj, lyr_1, tf.nn.relu) #applies nonlinearity function(relu) to hidden layer using given features and adjacency matrix\n        logits =  gnn_fn(hidden, adj, lyr_2, tf.identity) #define logits by applying seond transformation\n        return logits\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n    \n    best_accuracy = 0.0\n    for ep in range(epochs + 1):\n        with tf.GradientTape() as t:#what is gradient tape?\n            logits = cora_gnn(fts, adj)\n            loss = masked_softmax_cross_entropy(logits, labels, train_mask)\n            \n        variables = t.watched_variables()\n        grads = t.gradient(loss, variables)\n        optimizer.apply_gradients(zip(grads, variables))\n        \n        logits = cora_gnn(fts, adj)\n        val_accuracy = masked_accuracy(logits, labels, val_mask)\n        test_accuracy= masked_accuracy(logits, labels, test_mask)\n        \n        \n        if val_accuracy > best_accuracy:\n            best_accuracy = val_accuracy\n            print('Epoch', ep, '| Training loss:', loss.numpy(), '|val_accuracy:',\n                 val_accuracy.numpy(), '|test_accuracy:', test_accuracy.numpy())\n        \n        #skips model saving steps in this colab, could quickly be added ","4ed83c8b":"train_cora(features, adj, gnn, 32, 200, 0.01) #32 features of 200 epochs with 0.01 lr\n#raw adjacency matrix; 0 and 1. should have sum pooling but have issues \n#with the scale of the features ergo not great results","b437e7b7":"train_cora(features, tf.eye(adj.shape[0]), gnn, 32, 200, 0.01) \n#testing the model using a pointwise nlp(mlp?) in the form of the \n#identity matrix to confirm that we need our special matrix at all","12eb2c5f":"deg = tf.reduce_sum(adj, axis=-1)\ntrain_cora(features, adj\/deg, gnn, 32, 200, 0.01)\n#deals with exploding points by normalizing","6e4aa727":"#Thomas Kpif (cgnn) half normalized degree matrix multiplied the adjacency matrix on both sides\nnorm_deg = tf.linalg.diag(1.0 \/ tf.sqrt(deg))\nnorm_adj = tf.matmul(norm_deg, tf.matmul(adj, norm_deg))\ntrain_cora(features, norm_adj, gnn, 32, 200, 0.01)","12255a0c":"#additional research\/practice resource: https:\/\/cnvrg.io\/graph-neural-networks\/","30486f08":"#jumpstart for adding the model saving attribute\n'''\n    checkpoint_filepath = f'best_model.h5'\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    save_freq='epoch')\n    \n    '''","ffe3ea5b":"TODO:\n1. get the accuracy output and also implement saving the best model really quickly into the train loop (a few lines of boiler plate tensorflow code\n","1127f732":"<h1><center>Burning through an intro to graph neural networks<\/center><\/h1>\n\n____________________________________________________________________________________________\n<h3>read along or code it yourself here;<\/h3>\n\n> [source](https:\/\/www.youtube.com\/watch?v=8owQBFAHw7E&t=2992s&ab_channel=TensorFlow)\n____________________________________________________________________________________________\n\n\n![image.png](attachment:fc4bc704-e5f5-4758-9a9b-0d9855974741.png)"}}