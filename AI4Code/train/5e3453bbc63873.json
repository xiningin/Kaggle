{"cell_type":{"e75e6f72":"code","b00571f2":"code","75b5a513":"code","21e398b4":"code","803069da":"code","0bf4b3ac":"code","6f2ab6a3":"code","fcc046f9":"code","a0772c5d":"code","2d70c1f2":"code","6529f3cb":"code","e27d4f31":"code","3fd43833":"code","38ebc861":"code","16da7a1d":"code","5fb0fbf9":"code","bcba3256":"code","9302ef1e":"code","5d687614":"code","dc5c86bb":"code","ac428e11":"code","f1100095":"code","6dad0cd0":"code","6d08882d":"code","a0054c18":"markdown","e8bfe7fe":"markdown","8bd0ffff":"markdown","c9e91d35":"markdown","a9d2fe34":"markdown","696caba4":"markdown","d2ec80a0":"markdown","65ec6bfe":"markdown","af92b41c":"markdown","f07e3e20":"markdown"},"source":{"e75e6f72":"##### Importing modules #####\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport pickle\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","b00571f2":"##### Retrive and Reading Data #####\nfile = '\/kaggle\/input\/mushroom-classification\/mushrooms.csv'\n\ndata = pd.read_csv(file)\ndata.columns = data.columns.str.replace('-','_')\ndata.head()","75b5a513":"##### Remove Rare Labels #####\n# Arbitrary value\ndata.drop(\"veil_type\", axis=1, inplace=True)  ","21e398b4":"##### Variable separation #####\n\ntarget = 'class'\nfeatures = np.array([col for col in data.columns\n                     if col != target])","803069da":"##### Train test split #####\n\ntrain, test = train_test_split(data,\n                               random_state=1,\n                               test_size=0.30, \n                               stratify=data[target])\n\nprint(\"No. of data points in training set : \", len(train))\nprint(\"No. of data points in testing set : \", len(test))","0bf4b3ac":"##### Independent and dependent features #####\n\nX_train = train.drop(\"class\", axis=1)\ny_train = train['class']\n\nX_test = test.drop(\"class\", axis=1)\ny_test = test['class']","6f2ab6a3":"##### Feature engineering #####\n\n### 1. Ordinal Encoding ###\n\nenc = OrdinalEncoder(dtype=int)\n_ = enc.fit(X_train)\n\nX_train_encoded = pd.DataFrame(enc.transform(X_train),\n                               index=X_train.index,\n                               columns=X_train.columns)\n\nX_test_encoded = pd.DataFrame(enc.transform(X_test),\n                              index=X_test.index,\n                              columns=X_train.columns)\n\n### 2. Label Encoding ###\n\nlabel_enc = LabelEncoder()\n_ = label_enc.fit(y_train)\n\ny_train_encoded = pd.Series(label_enc.transform(y_train),\n                            index=y_train.index, \n                            name=\"y_train\")\n\ny_test_encoded = pd.Series(label_enc.transform(y_test),\n                           index=y_test.index, \n                           name=\"y_test\")","fcc046f9":"### categories - mapping by ordinal encoder ###\n\nfor i, feat in enumerate(enc.categories_):\n    label_map = dict(zip(feat, range(len(feat))))\n    \n    print(f\"{i+1}. {features[i]}\\n{label_map}\\n\")","a0772c5d":"X_train_encoded.head()","2d70c1f2":"### index => class ###\n\nlabel_enc.classes_ \n# 0 => e or edible \n# 1 => p or poisionous","6529f3cb":"y_train_encoded.head()","e27d4f31":"##### Feature selection #####\n\nfeature_selector = SelectKBest(chi2, k=8) # k= 5, 6, 8\n_ = feature_selector.fit(X_train_encoded,\n                         y_train_encoded)\n\nscores = sorted(zip(features,\n                    feature_selector.scores_,\n                    feature_selector.get_support()),\n                key=lambda x: x[1],\n                reverse=True)\n\nprint(\"FEATURE\\t\\t\\t    SCORE   KEEP\")\n\nfor feat, score, res in scores:\n    print(f\"{feat:<25}  {score:7.2f}  {res}\")","3fd43833":"##### Final features from Feature selection #####\nnew_features = features[feature_selector.get_support()]\nprint(f\"Selected features:\\n {new_features}\")","38ebc861":"##### Creating New dataset with selected features #####\n\nnew_data = data[np.append(new_features, 'class')]\nnew_data.to_csv('final_data.csv', index=False)\n\nnew_data.head()","16da7a1d":"##### Train test split #####\n\ntrain, test = train_test_split(new_data,\n                               random_state=1,\n                               test_size=0.30,\n                               stratify=data[target])\n\nprint(\"No. of data points in training set : \", len(train))\nprint(\"No. of data points in testing set : \", len(test))\n\n##### Independent and dependent features #####\n\nX_train = train.drop(\"class\", axis=1)\ny_train = train['class']\n\nX_test = test.drop(\"class\", axis=1)\ny_test = test['class']","5fb0fbf9":"##### Feature engineering #####\n\n### 1. Ordinal Encoding ###\n\nenc = OrdinalEncoder(dtype=int)\n_ = enc.fit(X_train)\n\nX_train_encoded = pd.DataFrame(enc.transform(X_train),\n                               index=X_train.index,\n                               columns=X_train.columns)\n\nX_test_encoded = pd.DataFrame(enc.transform(X_test),\n                              index=X_test.index,\n                              columns=X_train.columns)\n\n### 2. Label Encoding ###\n\nlabel_enc = LabelEncoder()\n_ = label_enc.fit(y_train)\n\ny_train_encoded = pd.Series(label_enc.transform(y_train),\n                            index=y_train.index,\n                            name=\"y_train\")\n\ny_test_encoded = pd.Series(label_enc.transform(y_test),\n                           index=y_test.index,\n                           name=\"y_test\")","bcba3256":"for i, col in enumerate(enc.categories_):\n    print(f\"'{new_features[i]}' : {list(col)},\")\n\n# index = numerical representation \n# Ex: for bruises, 'f' => 0 and 't' => 1","9302ef1e":"X_train_encoded.head()","5d687614":"##### Machine Learning Algorithm Selection #####\n\n### Model Selection ###\n\nmodels = [\n    # GLM\n    LogisticRegression(random_state=1),\n    # Nearest Neighbor\n    KNeighborsClassifier(),\n    # SVM\n    SVC(probability=True, random_state=1),\n    # Trees\n    DecisionTreeClassifier(random_state=1),\n    # Ensemble\n    RandomForestClassifier(random_state=1),\n\n    AdaBoostClassifier(random_state=1),\n    \n    XGBClassifier(random_state=1)\n]\n\n##### K-FOld Cross validation #####\ncv_split = ShuffleSplit(n_splits=10,\n                        test_size=.3,\n                        train_size=.7,\n                        random_state=0)\n\n##### create dataframe to compare model metrics #####\n\ncolumns = ['Name',\n           'Parameters',\n           'TrainAccuracyMean',\n           'TestAccuracyMean',\n           'AvgTrainingTime'\n           ]\n\ncompare = pd.DataFrame(columns=columns)\n\n##### Model selection process #####\n\nfor row_index, model in enumerate(models):\n\n    model_name = model.__class__.__name__\n\n    print(f\"Training started for {model_name}\")\n    \n    # model cross validation results\n    cv_results = cross_validate(model,\n                                X_train_encoded,\n                                y_train_encoded,\n                                cv=cv_split,\n                                scoring='accuracy',\n                                return_train_score=True,\n                                n_jobs=-1)\n    \n    ##### Add cv results to comparision dataframe #####\n    compare.loc[row_index, 'Name'] = model_name\n    \n    compare.loc[row_index,\n                'Parameters'] = str(model.get_params())\n    \n    compare.loc[row_index, \n                'TrainAccuracyMean'] = cv_results['train_score'].mean()\n    \n    compare.loc[row_index,\n                'TestAccuracyMean'] = cv_results['test_score'].mean()\n    \n    compare.loc[row_index,\n                'AvgTrainingTime'] = cv_results['fit_time'].mean()\n\n    print(f\"Training completed!!\\n\")\n    \n    compare.sort_values(by=['TestAccuracyMean'],\n                        ascending=False,\n                        inplace=True)","dc5c86bb":"compare","ac428e11":"##### Plot Results #####\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x='TestAccuracyMean', y='Name', data=compare)\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Model Name')\nplt.tight_layout()","f1100095":"##### Final Model => Decision Tree Classifier #####\n\n### Hyperparameter tuning ###\n\nparam_dist = {\n    'max_depth': [4, 6, 8, 10, 12, 14, 16, 20],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [5, 10, 20, 30, 40, 50],\n    'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n    'max_leaf_nodes': [8, 16, 32, 64, 128, 256],\n}\n\n\ntree = DecisionTreeClassifier(random_state=1)\n\nsearch = RandomizedSearchCV(estimator=tree,\n                             param_distributions=param_dist,\n                             cv=3,\n                             n_iter=30,\n                             verbose=1,\n                             random_state=1)\n\n\n_ = search.fit(X_train_encoded, y_train_encoded)\n\n### best model and params ###\n\nbest_model = search.best_estimator_\nbest_params = search.best_params_\nbest_score = search.best_score_\n\nprint(f\"\\nTuned Decision Tree Parameters:\\n{best_params}\")\nprint(f\"\\nBest score: {best_score}\")","6dad0cd0":"##### Final Pipeline #####\n\nordinal_encoder = OrdinalEncoder(dtype=int)\n\nestimator = DecisionTreeClassifier(**best_params)\n\n\npipeline = Pipeline([\n    (\"ordinal_encoder\", ordinal_encoder), \n    (\"estimator\", estimator)])\n\n\nprint(\"Training Started..\\n\")\n\ntic = time.time()\n_ = pipeline.fit(X_train, y_train_encoded)\ntac = time.time()\n\nprint(\"Training completed!\\n\")\nprint(f\"Training time :{tac-tic} seconds\\n\")\n\ny_train_preds = pipeline.predict(X_train)\ny_test_preds = pipeline.predict(X_test)\n\nprint(\"Train accuracy :\",\n      accuracy_score(y_train_preds, y_train_encoded))\n\nprint(\"Test accuracy :\",\n      accuracy_score(y_test_preds, y_test_encoded))\n\npickle.dump(pipeline, open(\"pipeline.pkl\", \"wb\"))\nprint(\"\\nPipeline saved!\")","6d08882d":"##### Evaluation #####\n\n### Classification Report ###\n\nprint(classification_report(y_test_encoded, y_test_preds))\n\n### Confusion matrix ###\n\ncm = confusion_matrix(y_true=y_test_encoded, \n                      y_pred=y_test_preds)\n\nclassNames = ['Edible', 'Poision']\ntick_marks = np.arange(len(classNames))\ns = [['TN', 'FP'], ['FN', 'TP']]\n\nplt.figure(figsize=(5, 5))\nplt.imshow(cm, interpolation='nearest', \n           cmap=plt.cm.Wistia)\n\nplt.title('Edible or Poisionous Mushroom Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n\nplt.xticks(tick_marks, classNames, rotation=0)\nplt.yticks(tick_marks, classNames)\n\nfor i in range(2):\n    for j in range(2):\n        plt.text(j, i, str(s[i][j])+\" = \"+str(cm[i][j]))","a0054c18":"## Upvote my notebook.\ud83d\udc9b\n### Thank you..!!","e8bfe7fe":"### Hyperparameter tuning ###","8bd0ffff":"# Phase 2\n### Final data with selected features","c9e91d35":"## Evaluation","a9d2fe34":"# Import Libraries","696caba4":"# Phase 1","d2ec80a0":"# Phase 3","65ec6bfe":"## Final Pipeline","af92b41c":"## Model Selection","f07e3e20":"## Feature Selection"}}