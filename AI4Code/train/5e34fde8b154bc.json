{"cell_type":{"244ad3af":"code","4310345a":"code","3e239355":"code","fbe54031":"code","aea734ca":"code","1dedaaf0":"code","91479bd1":"code","476cf43e":"code","75d19480":"code","defd1aa7":"code","26e9d1ea":"code","401d9094":"code","aa623c83":"code","55d9aacb":"code","262a6335":"code","5d2b0836":"code","0b2be1a8":"code","43956f58":"code","fceaaffa":"code","c87f72e7":"code","a45dfd42":"code","066cf4c1":"code","47ffb0ab":"code","0e26ac22":"code","152e81a5":"code","6e0371f9":"markdown"},"source":{"244ad3af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4310345a":"!pip install rich","3e239355":"from sklearn.model_selection import train_test_split","fbe54031":"def uniform_split_dev_df(raw_dev_df, test_size=0.2, random_state=42):\n    n_rare_class = raw_dev_df['label'].value_counts().min()\n    classes = raw_dev_df['label'].unique()\n    classes_df = [raw_dev_df[raw_dev_df['label']==c].sample(n=n_rare_class, random_state=random_state) for c in classes]\n    splitted_classes = [train_test_split(df, test_size=test_size, random_state=random_state) for df in classes_df]\n    train_df = pd.concat([l[0] for l in splitted_classes], axis=0)\n    valid_df = pd.concat([l[1] for l in splitted_classes], axis=0)\n    return train_df, valid_df","aea734ca":"raw_dev_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nraw_test_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\nraw_train_df, raw_valid_df = uniform_split_dev_df(raw_dev_df, test_size=0.2, random_state=42)\n\nraw_x_train = raw_train_df.drop(columns=['label'])\nraw_y_train = raw_train_df['label']\n\nraw_x_valid = raw_valid_df.drop(columns=['label'])\nraw_y_valid = raw_valid_df['label']","1dedaaf0":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nimport torchvision","91479bd1":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","476cf43e":"class MNISTData:\n    def __init__(self, x_df, y_df, device, name):\n        x_array = x_df.to_numpy().reshape(-1, 1, 28, 28) # channels first\n        y_array = y_df.to_numpy()\n        self.x = torch.tensor(x_array\/254, dtype=torch.float32, device=device)\n        self.y = torch.tensor(y_array, device=device)\n        self.length = self.x.shape[0]\n        self.device = device\n        self.name = name\n    def __len__(self):\n        return self.length\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]","75d19480":"trainset = MNISTData(raw_x_train, raw_y_train, device, 'trainset')\nvalidset = MNISTData(raw_x_valid, raw_y_valid, device, 'validset')","defd1aa7":"import matplotlib.pyplot as plt","26e9d1ea":"def random_visualize(dataset):\n    print(f'Randomly visualizing an image from {dataset.name}')\n    idx = np.random.randint(0, len(dataset))\n    plt.figure(figsize=(5, 5))\n    plt.imshow(dataset[idx][0].cpu().numpy().reshape(28, 28), cmap='gray_r')\n    plt.axis('off')\n    plt.title(f'Label: {dataset[idx][1].item()}')\n    plt.show()","401d9094":"random_visualize(trainset)\nrandom_visualize(validset)","aa623c83":"batch_size = 64\ntrain_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)","55d9aacb":"from sklearn.metrics import accuracy_score\nfrom rich.progress import Progress","262a6335":"def get_progressbar_description(train_loss, train_accuracy, valid_loss, valid_accuracy):\n    body = (f'TL: {train_loss:.2e}, TA: {train_accuracy:.2%}, '\n             f'VL: {valid_loss:.2e}, VA: {valid_accuracy:.2%}')    \n    return  '{' + body + '}'","5d2b0836":"class Trainer:\n    def __init__(\n        self, model, criterion, optimizer, scheduler, train_loader, valid_loader, device\n    ):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n        self.device = device\n        self.history = {\"train_loss\": [], \"valid_loss\": []}\n        self.__trained_epochs = 0\n\n    def __train_loop(self, progress, task):\n        self.model.train()\n        n_mini_batch = len(self.train_loader)\n        acm_loss = 0\n        progress.reset(task)\n        progress.update(task, visible=True)\n        for i, (x, y) in enumerate(self.train_loader):\n            pred = self.model(x.to(device))\n            loss = self.criterion(pred.squeeze(), y.to(device))\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            acm_loss += loss.item()\n            msg = (\n                \"{\"\n                f't_b: \"{i+1}\/{n_mini_batch}\", '\n                f\"t_l: {(acm_loss\/(i+1))**0.5:.2e}\"\n                \"}\"\n            )\n            progress.update(task, description=msg, completed=i + 1)\n        progress.update(task, visible=False)\n        mean_loss = acm_loss \/ n_mini_batch\n        return mean_loss\n\n    @torch.no_grad()\n    def predict(self, x):\n        self.model.eval()\n        return self.model(x.to(device))\n\n    @torch.no_grad()\n    def __valid_loop(self, progress, task):\n        self.model.eval()\n        n_mini_batch = len(self.valid_loader)\n        acm_loss = 0\n        progress.reset(task)\n        progress.update(task, visible=True)\n        for i, (x, y) in enumerate(self.valid_loader):\n            pred = self.model(x.to(device))\n            loss = self.criterion(pred.squeeze(), y.to(device))\n            acm_loss += loss.item()\n            msg = (\n                \"{\"\n                f'v_b: \"{i+1}\/{n_mini_batch}\", '\n                f\"v_l: {(acm_loss\/(i+1))**0.5:.2e}\"\n                \"}\"\n            )\n            progress.update(task, description=msg, completed=i + 1)\n        progress.update(task, visible=False)\n        mean_loss = acm_loss \/ n_mini_batch\n        return mean_loss\n\n    def train(self, epochs):\n        with Progress() as progress:\n            epoch_task = progress.add_task(\"Training...\", total=epochs)\n            train_batch_task = progress.add_task(\n                \"\", total=len(self.train_loader), visible=False\n            )\n            valid_batch_task = progress.add_task(\n                \"\", total=len(self.valid_loader), visible=False\n            )\n            for epoch in range(1, epochs + 1):\n                train_loss = self.__train_loop(progress, train_batch_task)\n                valid_loss = self.__valid_loop(progress, valid_batch_task)\n                scheduler.step(train_loss)\n                self.history[\"train_loss\"].append(train_loss)\n                self.history[\"valid_loss\"].append(valid_loss)\n                self.__trained_epochs += 1\n                msg = (\n                    \"{\"\n                    f'e: \"{self.__trained_epochs}\/{self.__trained_epochs + epochs - epoch}\", '\n                    f\"t_l: {train_loss**0.5:.2e}, \"\n                    f\"v_l: {valid_loss**0.5:.2e}\"\n                    \"}\"\n                )\n                progress.update(epoch_task, description=msg, completed=epoch)\n","0b2be1a8":"def calculate_padding(in_size, out_size, kernel_size, stride, dilation):\n    return (dilation*(kernel_size-1) - in_size + stride*(out_size-1) + 1) \/ 2","43956f58":"class ResidualBlock(nn.Module):\n    def __init__(self, in_size, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n        super().__init__()\n        padding = calculate_padding(in_size, in_size, kernel_size, stride, dilation)\n        if not padding.is_integer():\n            raise Exception('Invalid combination of in_size, kernel_size, stride, dilation')\n        padding = (int(padding), int(padding))\n        self.direct_path = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=dilation,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(\n                out_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=dilation,\n            ),\n            nn.BatchNorm2d(out_channels),\n        )\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        return self.relu(self.direct_path(x) + x)","fceaaffa":"class MyMNISTResNet(nn.Module):\n    def __init__(self, n_layers, n_conv_channels=8):\n        super().__init__()\n        self.resblock_stack = nn.Sequential(\n            ResidualBlock(28, 1, 8, kernel_size=3),\n            *[ResidualBlock(28, 8, 8, kernel_size=3) for _ in range(n_layers-2)],\n        )\n        self.fc = nn.Linear(28*28*n_conv_channels, 10)\n    def forward(self, x):\n        x = self.resblock_stack(x)\n        x = torch.flatten(x, start_dim=1, end_dim=-1)\n        return self.fc(x)","c87f72e7":"model = MyMNISTResNet(4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.5, patience=2)\ntrainer = Trainer(\n    model, \n    criterion,\n    optimizer,\n    scheduler,\n    train_loader, \n    valid_loader,\n    device,\n)","a45dfd42":"trainer.train(10)","066cf4c1":"import torch","47ffb0ab":"test_tensor = torch.tensor(raw_test_df.to_numpy().reshape(-1, 1, 28, 28)\/254, dtype=torch.float32, device=device)\n\nlogits = trainer.predict(test_tensor)\n\nsoftmax = torch.nn.LogSoftmax(dim=1)\n\npreds = softmax(logits).argmax(dim=1).cpu().numpy()\n\nsubmit_df = pd.DataFrame({'ImageId': pd.RangeIndex(start=1, stop=preds.shape[0]+1), 'Label': preds})","0e26ac22":"from datetime import datetime","152e81a5":"timestamp = int(datetime.now().timestamp())\nsubmit_df.to_csv(f'mnist_{timestamp}.csv', index=False)","6e0371f9":"# Submit"}}