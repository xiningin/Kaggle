{"cell_type":{"6851d6a5":"code","fa448970":"code","b62129c3":"code","eb1d5b0f":"code","ff2a1349":"code","3581df56":"code","93836990":"code","c7127b75":"code","e1ba7990":"code","ebf56537":"code","e23c9e53":"code","1572d3f8":"code","73796e40":"code","2b371ea7":"markdown","076328d8":"markdown","eef01271":"markdown","43881fc3":"markdown","b0cf1bb1":"markdown","672ea213":"markdown","c50239d3":"markdown","ffceeb78":"markdown","38437ac9":"markdown","22b948e0":"markdown","a462853c":"markdown"},"source":{"6851d6a5":"text = \"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\"","fa448970":"import sys\n\ntry:\n    from nltk.tokenize import wordpunct_tokenize # RE-based tokenizer which splits text on whitespace and punctuation (except for underscore)\nexcept ImportError:\n    print('[!] You need to install nltk (http:\/\/nltk.org\/index.html)')","b62129c3":"test_tokens = wordpunct_tokenize(text)\ntest_tokens","eb1d5b0f":"from nltk.corpus import stopwords\nstopwords.readme().replace('\\n', ' ') # Since this is raw text, we need to replace \\n's with spaces for it to be readable.","ff2a1349":"stopwords.fileids() # Most corpora consist of a set of files, each containing a piece of text. A list of identifiers for these files is accessed via fileids().","3581df56":"stopwords.raw('greek')","93836990":"stopwords.raw('greek').replace('\\n', ' ') # Better","c7127b75":"stopwords.words('english')[:10]","e1ba7990":"stopwords.sents('greek')","ebf56537":"len(stopwords.words(['english', 'greek'])) # There is a total of 444 Greek and English stop words","e23c9e53":"language_ratios = {}\n\ntest_words = [word.lower() for word in test_tokens] # lowercase all tokens\ntest_words_set = set(test_words)\n\nfor language in stopwords.fileids():\n    stopwords_set = set(stopwords.words(language)) # For some languages eg. Russian, it would be a wise idea to tokenize the stop words by punctuation too.\n    common_elements = test_words_set.intersection(stopwords_set)\n    language_ratios[language] = len(common_elements) # language \"score\"\n    \nlanguage_ratios","1572d3f8":"most_rated_language = max(language_ratios, key=language_ratios.get) # The key parameter to the max() function is a function that computes a key. In our case, we already have a key so we set key to languages_ratios.get which actually returns the key.\nmost_rated_language","73796e40":"test_words_set.intersection(set(stopwords.words(most_rated_language))) # We can see which English stop words were found.","2b371ea7":"Based on [Detecting Text Language With Python and NLTK by Alejandro Nolla](http:\/\/blog.alejandronolla.com\/2013\/05\/15\/detecting-text-language-with-python-and-nltk\/)\n\n*Stop words* are words which are filtered out before processing because they are mostly grammatical as opposed to semantic in nature e.g. search engines remove words like 'want'.","076328d8":"There are other tokenizers e.g. `RegexpTokenizer` where you can enter your own regexp, `WhitespaceTokenizer` (similar to Python's `string.split()`) and `BlanklineTokenizer`.","eef01271":"The erro is because the `stopwords` corpus reader is of type `WordListCorpusReader` so there are no sentences.\nIt's the same for `.paras()`.","43881fc3":"# Detecting Text Language by Counting Stop Words","b0cf1bb1":"Corpus readers provide a variety of methods to read data from the corpus:","672ea213":"## 2. Exploring NLTK's stop words corpus","c50239d3":"We can also use `.sents()` which returns sentences. However, in our particular case, this will cause an error:","ffceeb78":"NLTK comes with a corpus of stop words in various languages.","38437ac9":"## 1. Tokenizing","22b948e0":"## 3. The classification","a462853c":"We loop through the list of stop words in all languages and check how many stop words our test text contains in each language. The text is then classified to be in the language in which it has the most stop words."}}