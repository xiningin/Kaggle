{"cell_type":{"edadb41b":"code","58028b6d":"code","d78eb3b7":"code","e61a2e73":"code","5aab6256":"code","442ed49c":"code","a116641b":"code","b4eefef9":"code","3d09a5f9":"code","7339d982":"code","8ff009ff":"code","62355717":"markdown","ce90aaeb":"markdown","2134c275":"markdown","4d931c31":"markdown","1dab4f74":"markdown","16c41e75":"markdown","b1e935e1":"markdown","2eba146b":"markdown","538a1d83":"markdown"},"source":{"edadb41b":"import pandas as pd\n\nfrom bs4 import BeautifulSoup\n\nimport nltk, re, torch\n\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import sent_tokenize\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.pipeline import Pipeline\n\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\n\nfrom zipfile import ZipFile","58028b6d":"with ZipFile(\"\/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\",\"r\") as file:\n    file.extractall(\"input\")","d78eb3b7":"df = pd.read_csv(\".\/input\/labeledTrainData.tsv\",delimiter='\\t')\ndf.head()","e61a2e73":"df['sentiment'].value_counts()","5aab6256":"analyzer = SentimentIntensityAnalyzer()\ndef predict_sentiment(review):\n    sentences = sent_tokenize(review)\n    scores = [analyzer.polarity_scores(sentence)['compound'] for sentence in sentences]\n    return 1 if pd.Series(scores).mean() > 0 else 0\n    \npredictions = df['review'].apply(predict_sentiment)\nprint(classification_report(df['sentiment'], predictions))","442ed49c":"alphanum_re = re.compile(r\"\\W+\")\nstemmer =  nltk.stem.snowball.SnowballStemmer(\"english\")\nstop_words = set(stopwords.words(\"english\"))\n\ndef preprocess(review):\n    text = BeautifulSoup(review).get_text()\n    text = alphanum_re.sub(\" \", text)\n    return text\n\ndef tokenize(review):\n    tokens = nltk.tokenize.word_tokenize(review)\n    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n    return tokens\n\ndef evaluate(model):\n    results = cross_validate(\n        model, \n        df['review'], \n        df['sentiment'], \n        scoring=['precision_macro','recall_macro','accuracy']\n    )\n    return results","a116641b":"vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=tokenize, min_df=10)\nrf_model = Pipeline([\n    ('bow', vectorizer),\n    ('cls', RandomForestClassifier())\n])\nresults = evaluate(rf_model)\npd.DataFrame(results).mean(axis=0)","b4eefef9":"vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=tokenize, min_df=10)\nnb_model = Pipeline([\n    ('bow', vectorizer),\n    ('cls', MultinomialNB())\n])\nresults = evaluate(rf_model)\npd.DataFrame(results).mean(axis=0)","3d09a5f9":"class ReviewDataSet(torch.utils.data.Dataset):\n\n    def __init__(self, X, y):\n        self.classes = y\n        reviews = X.apply(preprocess)\n        tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n        \n        self.items = [tokenizer(\n            review, \n            max_length = 128, \n            truncation=True, \n            padding='max_length',\n            return_tensors='pt'\n        ) for review in reviews]\n\n    def classes(self):\n        return self.classes\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        return self.items[idx], self.classes.iloc[idx]","7339d982":"train_x, test_x, train_y, test_y = train_test_split(df['review'],df['sentiment'], shuffle=True, random_state=1)\ndataset = ReviewDataSet(train_x, train_y)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)","8ff009ff":"cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\n\nlog_interval = 10\nepochs = 3\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 2,\n    output_attentions = False,\n    output_hidden_states = False,\n)\nmodel.train()\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nif cuda:\n    model.cuda()\n    loss_fn.cuda()\n\nfor epoch in range(epochs):\n    running_loss = 0.0\n    \n    for i, batch in enumerate(dataloader):\n        inputs, labels = batch\n        \n        optimizer.zero_grad()\n        \n        mask = inputs['attention_mask'].to(device)\n        input_id = inputs['input_ids'].squeeze(1).to(device)\n        labels = labels.to(device)\n        \n        outputs = model(input_id,attention_mask=mask, labels=labels)\n        \n        loss = outputs[0]\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % log_interval == log_interval - 1:\n            print(f\"Epoch {epoch+1} Batch {i+1} avg. loss: {running_loss\/log_interval}\")\n            running_loss = 0.0","62355717":"The dataset is perfectly balanced, so no need for data augmentation or oversampling.","ce90aaeb":"## Vader Baseline\nA first attempt will be to use unsupervised sentiment classification from Vader as provided by nltk, to provide a baseline for our metrics.","2134c275":"## Count Vectorization\nAnother attempt might be to use count vectorization for the reviews, with manual preprocessing and tokenization. We'll try Random Forest and Multinomial Naive Bayes","4d931c31":"Vader seems to perform pretty well! It seems to struggle a bit with positive reviews, it might be because the dataset is labeled positive on a rating >= 7\/10, so there are many non-negative reviews classified as negative. We might be able to account for this by adjusting the classification threshold from 0 to a positive value, e.g. 0.3.","1dab4f74":"# Review Sentiment Prediction\nIn this notebook, we will attempt to predict whether a review has a positive or a negative sentiment.","16c41e75":"## BERT Classification\nNow we'll try fine-tuning BERT for our classification task","b1e935e1":"This is very slow on the CPU, so we'll do the computations on the GPU which is significantly faster","2eba146b":"We will remove the HTML entities from the review text, as well as any non-word characters, and then tokenize and stem the reviews before passing them to our models.","538a1d83":"We can already see an improvement over the unsupervised classification. Random Forest seems to perform slightly better."}}