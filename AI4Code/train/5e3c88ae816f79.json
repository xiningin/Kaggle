{"cell_type":{"fb1938c3":"code","1106d88d":"code","dc3534d6":"code","b0e0c241":"code","1f4c7fa0":"code","68902721":"code","ad594d8b":"code","f4e69dc7":"code","37822d5a":"code","d1ab1170":"code","a44616ac":"code","89b123d8":"code","715696ca":"code","b18cbde7":"code","d5981058":"code","cefa580b":"code","39e9c9d1":"code","1857369e":"code","150b59db":"code","b793e1ae":"code","b6a8d72f":"code","5a8556ee":"code","36a4765e":"code","fb0e4230":"code","ddf8549d":"code","77a43ecf":"code","854af135":"code","7ab59429":"code","9cc12302":"code","f25d7fbf":"code","dcffb39a":"code","3261c651":"code","3aadc64a":"code","74bdd45b":"code","e3024d13":"code","99f12a37":"code","3f2ae137":"code","4ee5148a":"code","59c9be8d":"code","f5d7d8b4":"code","6fcabe6d":"code","9d914c67":"code","b8e3304f":"code","f600e06d":"code","c0e9a1e0":"code","78979148":"code","11f5fc10":"code","79abb623":"code","599516d4":"code","588ff53e":"code","4d7db01d":"code","a7540512":"code","7a631714":"code","545e940b":"code","69cf7a56":"code","3de1ca49":"code","b6bd1923":"code","de02a11d":"code","b47f0f1c":"code","911b2934":"code","901f0944":"code","96d41efb":"code","1d11b896":"markdown","6ceadc03":"markdown","77f0be9c":"markdown","cc714c89":"markdown","61e799de":"markdown","e3118ae5":"markdown","ee19980d":"markdown"},"source":{"fb1938c3":"!pip install torch --upgrade --quiet","1106d88d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom tqdm.auto import tqdm\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc3534d6":"import fastai\nimport torch\nfastai.__version__","b0e0c241":"import glob\nimport pathlib\n\nhead = pathlib.Path(\"..\/input\/g2net-gravitational-wave-detection\")\n\ntrain_files = sorted(glob.glob(\"..\/input\/g2net-gravitational-wave-detection\/train\/*\/*\/*\/*.npy\"))","1f4c7fa0":"wave = np.load(train_files[0])","68902721":"import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt","ad594d8b":"from numba import njit, jit, cuda, guvectorize\n\n@njit(nogil=True)\ndef min_max_scaler(wave):\n    for i in range(len(wave)):\n        wave[i] = (wave[i] - min(wave[i])) \/ (max(wave[i]) - min(wave[i]))\n        wave[i] = 2 * wave[i] - 1\n        \n    return wave","f4e69dc7":"wave1 = min_max_scaler(wave)","37822d5a":"plt.figure(dpi=120)\nfor i in range(len(wave)):\n    plt.plot(range(len(wave[i])), wave[i], label=f\"label_{i}\")\nplt.legend()","d1ab1170":"from scipy.signal import butter, filtfilt, sosfiltfilt\n# from torchaudio.functional import bandpass_biquad\n\nT = 2 # sample period, s\nfs = 2048.0  # sample rate, Hz\ncutoff = 2.5  # desired cutoff frequency, slightly higher than actual 3 sine wave \/ 2 s = 1.5\n\nnyq = 0.5 * fs  # Nyquist frequency\n\norder = 3  # sine wave approx as quadratic\nn = int(T * fs)\nnormal_cutoff = cutoff \/ nyq","a44616ac":"def butter_bandpass_filter_torch(data, lowcut, highcut, fs):\n    return bandpass_biquad(data, fs, (highcut + lowcut) \/ 2, (highcut - lowcut) \/ (highcut + lowcut))","89b123d8":"# normal_cutoff = (21.83\/fs, 500\/fs)\n# def butter_bandpass_filter(data, normal_cutoff, fs, order=2):\n#     b, a = butter(order, normal_cutoff, btype=\"bandpass\", analog=False)\n#     y = filtfilt(b, a, data)\n#     return y","715696ca":"def butter_bandpass_filter(data, low, high, fs, order):\n    sos = butter(order, [low, high], btype=\"bandpass\", output=\"sos\", fs=fs)\n    normalization = np.sqrt((high - low) \/ (fs \/ 2))\n    return sosfiltfilt(sos, data) \/ normalization","b18cbde7":"def butter_lowpass_filter(data, normal_cutoff, fs, order):\n    \n    # Get filter coeff\n    b, a = butter(order, normal_cutoff, btype=\"lowpass\", analog=False)\n    y = filtfilt(b, a, data)\n    \n    return y","d5981058":"# y = min_max_scaler(butter_bandpass_filter(wave, normal_cutoff, fs, 3))\ndata = torch.from_numpy(wave)\ny = butter_bandpass_filter(data, 21.83, 500, fs, 4)","cefa580b":"plt.figure(dpi=120)\nplt.plot(range(len(wave[0])), y[0])","39e9c9d1":"plt.figure(dpi=120)\nplt.plot(range(len(wave[0])), y[0])","1857369e":"from scipy.signal import spectrogram\n\nplt.figure(dpi=120)\nfor i in range(len(wave)):\n    f, t, Sxx = spectrogram(wave1[i], fs=10)\n    plt.pcolormesh(t, f, Sxx, shading=\"gouraud\")","150b59db":"# plt.figure(dpi=120)\n# f, t, Sxx = spectrogram(wave1[0], fs=4096)\n# plt.pcolormesh(t, fftshift(f), fftshift(Sxx), shading=\"gouraud\")","b793e1ae":"def wrapper_plot(m):\n    plt.figure(dpi=120)\n    m()\n    plt.show()","b6a8d72f":"stacked = []\nfor j in range(len(wave1)):\n    melspec = librosa.feature.melspectrogram(wave1[j], sr=4096, n_mels=128, fmin=21.83, fmax=2048)\n    melspec = librosa.power_to_db(melspec)\n    melspec = melspec.transpose((1, 0))\n    stacked.append(melspec)\nimage = np.vstack(stacked)\nwrapper_plot(lambda: plt.imshow(image))","5a8556ee":"t.min()","36a4765e":"# X = np.load(\"..\/input\/g2net-n-mels-128-train-images-aggregated\/X.npy\")","fb0e4230":"# y = np.load(\"..\/input\/g2net-n-mels-128-train-images-aggregated\/y.npy\")","ddf8549d":"# X.shape","77a43ecf":"!pip install -q nnAudio","854af135":"from nnAudio.Spectrogram import *\nimport torch","7ab59429":"# @njit(nogil=True)\n# def min_max_scaler_hstack(wave):\n#     for i in range(len(wave)):\n#         wave[i] = (wave[i] - min(wave[i])) \/ (max(wave[i]) - min(wave[i]))\n        \n#     wave = np.hstack(wave)\n#     return wave","9cc12302":"import gc\ngc.collect()\n# import torch\n# torch.cuda.empty_cache()","f25d7fbf":"normal_cutoff = (20\/nyq, 500\/nyq)","dcffb39a":"from scipy.signal import cwt, ricker","3261c651":"wave.shape","3aadc64a":"import time","74bdd45b":"# Taken from https:\/\/www.kaggle.com\/anjum48\/continuous-wavelet-transform-cwt-in-pytorch\n\nclass CWT(nn.Module):\n    def __init__(\n        self,\n        widths,\n        wavelet=\"ricker\",\n        channels=1,\n        filter_len=2000,\n        bs=1,\n    ):\n        \"\"\"PyTorch implementation of a continuous wavelet transform.\n\n        Args:\n            widths (iterable): The wavelet scales to use, e.g. np.arange(1, 33)\n            wavelet (str, optional): Name of wavelet. Either \"ricker\" or \"morlet\".\n            Defaults to \"ricker\".\n            channels (int, optional): Number of audio channels in the input. Defaults to 3.\n            filter_len (int, optional): Size of the wavelet filter bank. Set to\n            the number of samples but can be smaller to save memory. Defaults to 2000.\n        \"\"\"\n        super().__init__()\n        self.widths = torch.from_numpy(widths)\n        self.wavelet = getattr(self, wavelet)\n        self.filter_len = filter_len\n        self.bs = bs\n        self.channels = channels\n        self.wavelet_bank = self._build_wavelet_bank()\n\n    def ricker(self, points, a):\n        # https:\/\/github.com\/scipy\/scipy\/blob\/v1.7.1\/scipy\/signal\/wavelets.py#L262-L306\n        a = torch.Tensor([a])\n        A = 2 \/ (torch.sqrt(3 * a) * (np.pi ** 0.25))\n        wsq = a ** 2\n        vec = torch.arange(0, points) - (points - 1.0) \/ 2\n        xsq = vec ** 2\n        mod = 1 - xsq \/ wsq\n        gauss = torch.exp(-xsq \/ (2 * wsq))\n        total = A * mod * gauss\n        return total\n\n    def morlet(self, points, s):\n        s = torch.Tensor([s])\n        x = torch.arange(0, points) - (points - 1.0) \/ 2\n        x = x \/ s\n        # https:\/\/pywavelets.readthedocs.io\/en\/latest\/ref\/cwt.html#morlet-wavelet\n        wavelet = torch.exp(-(x ** 2.0) \/ 2.0) * torch.cos(5.0 * x)\n        output = torch.sqrt(1 \/ s) * wavelet\n        return output\n\n    def cmorlet(self, points, s, wavelet_width=1, center_freq=1):\n        # https:\/\/pywavelets.readthedocs.io\/en\/latest\/ref\/cwt.html#complex-morlet-wavelets\n        s = torch.Tensor([s])\n        x = torch.arange(0, points) - (points - 1.0) \/ 2\n        x = x \/ s\n        norm_constant = torch.sqrt(torch.Tensor([np.pi * wavelet_width]))\n        exp_term = torch.exp(-(x ** 2) \/ wavelet_width)\n        kernel_base = exp_term \/ norm_constant\n#         kernel = kernel_base * torch.exp(1j * 2 * np.pi * center_freq * x)\n        kernel_real = kernel_base * torch.cos(2 * np.pi * center_freq * x)\n        kernel_imag = kernel_base * torch.sin(2 * np.pi * center_freq * x)\n        return kernel_real, kernel_imag\n\n    def _build_wavelet_bank(self):\n        wavelet_bank_real = []\n        wavelet_bank_imag = []\n        for w in self.widths:\n            wavelet_bank = self.wavelet(self.filter_len, w)\n            wavelet_bank_real.append(wavelet_bank[0])\n            wavelet_bank_imag.append(wavelet_bank[1])\n#         wavelet_bank = [self.wavelet(self.filter_len, w) for w in self.widths]\n        wavelet_bank_real = torch.stack(wavelet_bank_real)\n        wavelet_bank_imag = torch.stack(wavelet_bank_imag)\n        wavelet_bank_real = wavelet_bank_real.view(\n            wavelet_bank_real.shape[0], 1, 1, wavelet_bank_real.shape[1]\n        )\n        wavelet_bank_imag = wavelet_bank_imag.view(\n            wavelet_bank_imag.shape[0], 1, 1, wavelet_bank_imag.shape[1]\n        )\n        wavelet_bank_real = torch.cat([wavelet_bank_real] * self.channels, 2)\n        wavelet_bank_imag = torch.cat([wavelet_bank_imag] * self.channels, 2)\n#         wavelet_bank_real = torch.cat([wavelet_bank_real] * self.bs, 1)\n#         wavelet_bank_imag = torch.cat([wavelet_bank_imag] * self.bs, 1)\n        return wavelet_bank_real, wavelet_bank_imag\n        \n\n#     def _build_wavelet_bank(self):\n#         \"\"\"This function builds a 2D wavelet filter using wavelets at different scales\n\n#         Returns:\n#             tensor: Tensor of shape (num_widths, 1, channels, filter_len)\n#         \"\"\"\n#         wavelet_bank = [\n#             torch.conj(torch.flip(self.wavelet(self.filter_len, w), [-1]))\n#             for w in self.widths\n#         ]\n#         wavelet_bank = torch.stack(wavelet_bank)\n#         wavelet_bank = wavelet_bank.view(\n#             wavelet_bank.shape[0], 1, 1, wavelet_bank.shape[1]\n#         )\n#         wavelet_bank = torch.cat([wavelet_bank] * self.channels, 2)\n#         return wavelet_bank\n\n    def forward(self, x):\n        \"\"\"Compute CWT arrays from a batch of multi-channel inputs\n\n        Args:\n            x (torch.tensor): Tensor of shape (batch_size, channels, time)\n\n        Returns:\n            torch.tensor: Tensor of shape (batch_size, channels, widths, time)\n        \"\"\"\n        x = x.unsqueeze(1)\n#         if self.wavelet_bank.is_complex():\n        if type(self.wavelet_bank) == tuple:\n#             wavelet_real = self.wavelet_bank.real.to(device=x.device, dtype=x.dtype)\n#             wavelet_imag = self.wavelet_bank.imag.to(device=x.device, dtype=x.dtype)\n            wavelet_real = self.wavelet_bank[0].to(device=x.device, dtype=x.dtype)\n            wavelet_imag = self.wavelet_bank[1].to(device=x.device, dtype=x.dtype)\n\n            output_real = nn.functional.conv2d(x, wavelet_real, padding=\"same\")\n            output_imag = nn.functional.conv2d(x, wavelet_imag, padding=\"same\")\n            output_real = torch.transpose(output_real, 1, 2)\n            output_imag = torch.transpose(output_imag, 1, 2)\n#             return torch.complex(output_real, output_imag)\n            return torch.sqrt(output_real**2 + output_imag**2)\n        else:\n            self.wavelet_bank = self.wavelet_bank.to(device=x.device, dtype=x.dtype)\n            output = nn.functional.conv2d(x, self.wavelet_bank, padding=\"same\")\n            return torch.transpose(output, 1, 2)","e3024d13":"widths = np.arange(25, 89)\npycwt = CWT(widths, \"cmorlet\", 3, 4096)","99f12a37":"wavelet_bank_real = pycwt.wavelet_bank[0]\nwavelet_bank_real.shape","3f2ae137":"bs = 16\ntorch.cat([wavelet_bank_real] * bs, 1).shape","4ee5148a":"imgs = []\nfor i in range(1, 5): imgs.append(np.load(train_files[i]))\nimgs = torch.from_numpy(np.array(imgs))\nimgs.shape","59c9be8d":"%timeit our_imgs = pycwt(imgs)","f5d7d8b4":"imgs = []\nfor i in range(1, 9): imgs.append(np.load(train_files[i]))\nimgs = torch.from_numpy(np.array(imgs))\nimgs.shape","6fcabe6d":"%timeit _ = pycwt(imgs)","9d914c67":"our_imgs = pycwt(imgs)","b8e3304f":"@njit(nogil=True)\ndef min_max_scaler_int8(wave):\n    return (wave - wave.min()) \/ (wave.max() - wave.min()) * 255","f600e06d":"def image_to_int8(image): \n#     g = (image - image.min()) \/ (image.max() - image.min())\n    return np.round_(min_max_scaler_int8(image)).astype(np.uint8)","c0e9a1e0":"m = our_imgs[0, 2].numpy().copy()","78979148":"our_imgs[1, 1].numpy()","11f5fc10":"from PIL import Image\nImage.fromarray(image_to_int8(our_imgs[1, 1].numpy())).convert(\"RGB\").resize((400, 300)).save(\"data.png\")","79abb623":"%timeit pycwt(torch.from_numpy(wave).view(1, 3, 4096))","599516d4":"min_max_scaler(butter_bandpass_filter(wave, 20, 500, fs, 4))","588ff53e":"def apply_qtransform(waves, transform=None, cuda=False):\n#     waves *= scipy.signal.tukey(4096, 0.2)\n    waves = min_max_scaler(butter_bandpass_filter(waves, 27.5, 466.16, fs, 4))\n    waves = np.ascontiguousarray(waves)\n#     waves = np.hstack(waves)\n    waves = torch.from_numpy(waves).float().view(1, 3, 4096)\n    if cuda: waves = waves.cuda()\n    image = torch.abs(pycwt(waves))\n    image = torch.mean(image, dim=1).squeeze()  # Get mean of all 3 different waves.\n#     image = transform(waves)\n    return image\n\n\nimgs = []\nfor i in tqdm(range(10)):\n    wave = np.load(train_files[i])\n#     img = apply_qtransform(wave, transform=CQT1992v2(sr=2048, fmin=21.83, fmax=1024, hop_length=64))\n    img = apply_qtransform(wave, cuda=True if torch.cuda.is_available() else False)\n    imgs.append(img)\nprint(img.shape)","4d7db01d":"for i in range(10):\n    plt.figure(dpi=150)\n    plt.imshow(imgs[i].cpu().numpy().squeeze(), aspect=\"auto\")","a7540512":"del imgs\ngc.collect()","7a631714":"os.mkdir(\"train\/\")\nOUT_DIR = \"train\/\"\n\nlabels = pd.read_csv(\"..\/input\/g2net-gravitational-wave-detection\/training_labels.csv\")\nlabels[\"file_path\"] = train_files\n\npd.set_option(\"display.max_colwidth\", None)\nlabels.head()","545e940b":"ones_train = labels[labels[\"target\"] == 1][\"file_path\"].to_numpy()","69cf7a56":"def save_images(file_path, out_dir):\n    file_name = file_path.split('\/')[-1].split('.npy')[0]\n    waves = np.load(file_path).astype(np.float32) # (3, 4096)\n    image = apply_qtransform(wave, cuda=True).cpu()\n    plt.imsave(out_dir + file_name + \".png\", image.cpu().numpy().squeeze())","3de1ca49":"# Saving all the 1's in the 1's folder. \nimport joblib\nfrom tqdm.auto import tqdm\n\nfolder_name = \"train\/ones\/\"\n\nos.makedirs(folder_name, exist_ok=True)\n\n_ = joblib.Parallel(n_jobs=8, prefer=\"threads\")(\n    joblib.delayed(save_images)(file_path, out_dir=folder_name) for file_path in tqdm(ones_train)\n)","b6bd1923":"folder_name = \"train\/zero\/\"\nzeroes_train = labels[labels[\"target\"] == 0][\"file_path\"].to_numpy()\n\nos.makedirs(folder_name, exist_ok=True)\n\n_ = joblib.Parallel(n_jobs=8, prefer=\"threads\")(\n    joblib.delayed(save_images)(file_path, out_dir=folder_name) for file_path in tqdm(zeroes_train)\n)","de02a11d":"import os\nimport shutil\n\ndef move_to_destination(origin, destination, percentage_split):\n    num_images = int(len(os.listdir(origin))*percentage_split)\n    for image_name, image_number in zip(sorted(os.listdir(origin)), range(num_images)):\n        shutil.move(os.path.join(origin, image_name), destination)","b47f0f1c":"os.makedirs(\".\/valid\/ones\")\nos.makedirs(\".\/valid\/zero\")\nmove_to_destination(\".\/train\/ones\", \".\/valid\/ones\", 0.2)\nmove_to_destination(\".\/train\/zero\", \".\/valid\/zero\", 0.2)","911b2934":"%%time\nimport shutil\n\nshutil.make_archive(\"train\/\", 'zip', \"train\/\")\nshutil.rmtree(\"train\/\")\n\nshutil.make_archive(\"valid\/\", \"zip\", \"valid\/\")\nshutil.rmtree(\"valid\/\")","901f0944":"OUT_DIR = \"test\/\"\nos.mkdir(\"test\/\")\ntest_files = sorted(glob.glob(\"..\/input\/g2net-gravitational-wave-detection\/test\/*\/*\/*\/*.npy\"))\n\n_ = joblib.Parallel(n_jobs=8, prefer=\"threads\")(\n    joblib.delayed(save_images)(file_path, out_dir=OUT_DIR) for file_path in tqdm(test_files)\n)","96d41efb":"%%time\nshutil.make_archive(\"test\/\", 'zip', \"test\/\")\nshutil.rmtree(\"test\/\")","1d11b896":"# Continuation","6ceadc03":"With numpy: 2.19s.  \nWith pytorch (no GPU) also around 2.18s.  \nWithout using complex numbers: 740ms. (but with slightly different output).","77f0be9c":"**There some difference in the `fmin` value**. Originally it was 20Hz and I slightly raise it to **21.83Hz** for this version. Lowering the value slightly will make bright part of the image dimmer (speaking in terms of image rather than frequency since easy to visualize) while raising the frequency slightly will brighten the strongest part, and some of the background noise on the RHS of the picture will also brighten into existence. \n\nIf you'd like to make your own dataset consider tuning this value to which you see fit. It might or might not fit better with brighter or dimmer value. \n\nSecond thing is `n_bins`. Tuning this too high will cause it to exceed the nyquist limit, while too low might have some bright image darkens. Consider tuning this as well. One changes it **from 55 to 63** to try out the difference. \n\nOf course, this is not a confirmation. Some of the bright image will dim out when increasing `fmin` and\/or `n_bins`, hence this requires some experimentation. ","cc714c89":"## Finish playing\nNow is time to use dataset created by Y. Nakama and continue. ","61e799de":"# Bandpass filter","e3118ae5":"with all: $2.16 s\\pm 16.4 ms$  \nwithout norm-const: $2.2 s \\pm 108 ms$  \nwithout exponential: $2.18 s \\pm 61.9 ms$  \nwithout kernel calc: $1.07 s \\pm 4.39 ms$","ee19980d":"Currently we are taking mean of all 3 waves. Perhaps there are other methods. "}}