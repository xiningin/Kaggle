{"cell_type":{"4fefc607":"code","1401211c":"code","c5bdf207":"code","0b7d73c6":"code","0ddb2d77":"code","4ce412bd":"code","fd5d64b1":"code","2a4da34c":"code","8aa787cf":"code","06214262":"code","efddca1c":"code","62750f9a":"code","89a3ebbd":"code","52780f60":"code","c97f17b1":"markdown","84d433f3":"markdown","ca7a4ead":"markdown","6c0e2882":"markdown","0af8bd8e":"markdown","6f1251f3":"markdown","f70263ff":"markdown","237f0aad":"markdown","2bdf0cd1":"markdown","768a2528":"markdown","7fb320b0":"markdown","7d4ac5ed":"markdown","7dd563ab":"markdown","e7015aea":"markdown","dd9b189e":"markdown","f062b953":"markdown","88f69c3d":"markdown","c0f0ed26":"markdown","248ddf49":"markdown"},"source":{"4fefc607":"import torch\nfrom torch import nn\n\nimport math\nimport matplotlib.pyplot as plt\nimport torchvision\nimport torchvision.transforms as transforms","1401211c":"torch.manual_seed(111)","c5bdf207":"device = \"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"gpu available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"gpu not aviailable\")","0b7d73c6":"transform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n)","0ddb2d77":"train_set = torchvision.datasets.MNIST(\n    root=\".\", train=True, download=True, transform=transform\n)","4ce412bd":"batch_size = 32\ntrain_loader = torch.utils.data.DataLoader(\n    train_set, batch_size=batch_size, shuffle=True\n)","fd5d64b1":"real_samples, mnist_labels = next(iter(train_loader))\nfor i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    plt.imshow(real_samples[i].reshape(28, 28), cmap=\"gray_r\")\n    plt.xticks([])\n    plt.yticks([])","2a4da34c":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(784, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = x.view(x.size(0), 784)\n        output = self.model(x)\n        return output","8aa787cf":"discriminator = Discriminator().to(device=device)","06214262":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(100, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 784),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        output = output.view(x.size(0), 1, 28, 28)\n        return output\n\ngenerator = Generator().to(device=device)","efddca1c":"lr = 0.0001\nnum_epochs = 50\nloss_function = nn.BCELoss()\n\noptimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\noptimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)","62750f9a":"for epoch in range(num_epochs):\n    for n, (real_samples, mnist_labels) in enumerate(train_loader):\n        # Data for training the discriminator\n        real_samples = real_samples.to(device=device)\n        real_samples_labels = torch.ones((batch_size, 1)).to(\n            device=device\n        )\n        latent_space_samples = torch.randn((batch_size, 100)).to(\n            device=device\n        )\n        generated_samples = generator(latent_space_samples)\n        generated_samples_labels = torch.zeros((batch_size, 1)).to(\n            device=device\n        )\n        all_samples = torch.cat((real_samples, generated_samples))\n        all_samples_labels = torch.cat(\n            (real_samples_labels, generated_samples_labels)\n        )\n\n        # Training the discriminator\n        discriminator.zero_grad()\n        output_discriminator = discriminator(all_samples)\n        loss_discriminator = loss_function(\n            output_discriminator, all_samples_labels\n        )\n        loss_discriminator.backward()\n        optimizer_discriminator.step()\n\n        # Data for training the generator\n        latent_space_samples = torch.randn((batch_size, 100)).to(\n            device=device\n        )\n\n        # Training the generator\n        generator.zero_grad()\n        generated_samples = generator(latent_space_samples)\n        output_discriminator_generated = discriminator(generated_samples)\n        loss_generator = loss_function(\n            output_discriminator_generated, real_samples_labels\n        )\n        loss_generator.backward()\n        optimizer_generator.step()\n\n        # Show loss\n        if n == batch_size - 1:\n            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")","89a3ebbd":"latent_space_samples = torch.randn(batch_size, 100).to(device=device)\ngenerated_samples = generator(latent_space_samples)","52780f60":"generated_samples = generated_samples.cpu().detach()\nfor i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    plt.imshow(generated_samples[i].reshape(28, 28), cmap=\"gray_r\")\n    plt.xticks([])\n    plt.yticks([])","c97f17b1":"After fifty epochs of training, there are several generated digits that resemble the real ones. You can improve the results by considering more training epochs. As with the previous example, by using a fixed latent space samples tensor and feeding it to the generator at the end of each epoch during the training process, you can visualize the evolution of the training:","84d433f3":"In line 12, you use the [hyperbolic](https:\/\/mathworld.wolfram.com\/HyperbolicTangent.html) tangent function Tanh() as the activation of the output layer since the output coefficients should be in the interval from -1 to 1. In line 20, you instantiate the generator and send it to device to use the GPU if one is available.\n\nNow that you have the models defined, you\u2019ll train them using the training data.\n\n### Training the Models\nTo train the models, you need to define the training parameters and optimizers like you did in the previous example:","ca7a4ead":"As you can see, there are digits with different handwriting styles. As the GAN learns the distribution of the data, it\u2019ll also generate digits with different handwriting styles.\n\nNow that you\u2019ve prepared the training data, you can implement the discriminator and generator models.","6c0e2882":"Since this example uses images in the training set, the models need to be more complex, with a larger number of parameters. This makes the training process slower, taking about two minutes per epoch when running on CPU. You\u2019ll need about fifty epochs to obtain a relevant result, so the total training time when using a CPU is around one hundred minutes.\n\nTo reduce the training time, you can use a GPU to train the model.","0af8bd8e":"To obtain a better result, you decrease the learning rate from the previous example. You also set the number of epochs to 50 to reduce the training time.\n\nThe training loop is very similar to the one you used in the previous example. In the highlighted lines, you send the training data to device to use the GPU if available:","6f1251f3":"Since the generator is going to generate more complex data, it\u2019s necessary to increase the dimensions of the input from the latent space. In this case, the generator is going to be fed a 100-dimensional input and will provide an output with 784 coefficients, which will be organized in a 28 \u00d7 28 tensor representing an image.\n\nHere\u2019s the complete generator model code:","f70263ff":"To input the image coefficients into the MLP neural network, you [vectorize](https:\/\/en.wikipedia.org\/wiki\/Vectorization_(mathematics)) them so that the neural network receives vectors with 784 coefficients.\n\nThe vectorization occurs in the first line of .forward(), as the call to x.view() converts the shape of the input tensor. In this case, the original shape of the input x is 32 \u00d7 1 \u00d7 28 \u00d7 28, where 32 is the batch size you\u2019ve set up. After the conversion, the shape of x becomes 32 \u00d7 784, with each line representing the coefficients of an image of the training set.\n\nTo run the discriminator model using the GPU, you have to instantiate it and send it to the GPU with .to(). To use a GPU when there\u2019s one available, you can send the model to the device object you created earlier:","237f0aad":"You can use Matplotlib to plot some samples of the training data. To improve the visualization, you can use cmap=gray_r to reverse the color map and plot the digits in black over a white background:","2bdf0cd1":"### Implementing the Discriminator and the Generator\nIn this case, the discriminator is an MLP neural network that receives a 28 \u00d7 28 pixel image and provides the probability of the image belonging to the real training data.\n\nYou can define the model with the following code:","768a2528":"####  The function has two parts:\n\n1. transforms.ToTensor() converts the data to a PyTorch tensor.\n2. transforms.Normalize() converts the range of the tensor coefficients.\n\n\n- The original coefficients given by transforms.ToTensor() range from 0 to 1, and since the image backgrounds are black, most of the coefficients are equal to 0 when they\u2019re represented using this range.\n\n- transforms.Normalize() changes the range of the coefficients to -1 to 1 by subtracting 0.5 from the original coefficients and dividing the result by 0.5. With this transformation, the number of elements equal to 0 in the input samples is dramatically reduced, which helps in training the models.\n\n- The arguments of transforms.Normalize() are two tuples, (M\u2081, ..., M\u2099) and (S\u2081, ..., S\u2099), with n representing the number of channels of the images. Grayscale images such as those in MNIST dataset have only one channel, so the tuples have only one value. Then, for each channel i of the image, transforms.Normalize() subtracts M\u1d62 from the coefficients and divides the result by S\u1d62.\n\n- Now you can load the training data using torchvision.datasets.MNIST and perform the conversions using transform:","7fb320b0":"![](https:\/\/files.realpython.com\/media\/fig_gan_mnist.5d8784a85944.gif)","7d4ac5ed":"- we are going to need torchvision and transforms to obtain the training data and perform image conversions.","7dd563ab":"- Preparing the Training Data\n\nThe MNIST dataset consists of 28 \u00d7 28 pixel grayscale images of handwritten digits from 0 to 9. To use them with PyTorch, you\u2019ll need to perform some conversions. For that, you define transform, a function to be used when loading the data:\n\n","e7015aea":"The argument download=True ensures that the first time you run the above code, the MNIST dataset will be downloaded and stored in the current directory, as indicated by the argument root.\n\nNow that you\u2019ve created train_set, you can create the data loader as you did before:","dd9b189e":"### What Are Generative Adversarial Networks?\nGenerative adversarial networks are machine learning systems that can learn to mimic a given distribution of data. They were first proposed in a 2014 [NeurIPS paper](https:\/\/papers.nips.cc\/paper\/5423-generative-adversarial-nets.pdf) by deep learning expert Ian Goodfellow and his colleagues.\n\nGANs consist of two neural networks, one trained to generate data and the other trained to distinguish fake data from real data (hence the \u201cadversarial\u201d nature of the model). Although the idea of a structure to generate data isn\u2019t new, when it comes to image and video generation, GANs have provided impressive results such as:\n\n- Style transfer using [CycleGAN](https:\/\/github.com\/junyanz\/CycleGAN\/), which can perform a number of convincing style transformations on images\n- Generation of human faces with [StyleGAN](https:\/\/en.wikipedia.org\/wiki\/StyleGAN), as demonstrated on the website [This Person Does Not Exist](https:\/\/www.thispersondoesnotexist.com\/)\n\n Structures that generate data, including GANs, are considered generative models in contrast to the more widely studied discriminative models.","f062b953":"- importing the necessary libraries:","88f69c3d":"Some of the tensors don\u2019t need to be sent to the GPU explicitly with device. This is the case with generated_samples in **line 11**, which will already be sent to an available GPU since latent_space_samples and generator were sent to the GPU previously.\n\nSince this example features more complex models, the training may take a bit more time. After it finishes, you can check the results by generating some samples of handwritten digits.\n\n### Checking the Samples Generated by the GAN\nTo generate handwritten digits, you have to take some random samples from the latent space and feed them to the generator:","c0f0ed26":"![](https:\/\/www.mdpi.com\/remotesensing\/remotesensing-12-01149\/article_deploy\/html\/images\/remotesensing-12-01149-g001-550.jpg)","248ddf49":"You can see that at the beginning of the training process, the generated images are completely random. As the training progresses, the generator learns the distribution of the real data, and at about twenty epochs, some generated digits already resemble real data."}}