{"cell_type":{"ed3ced66":"code","d33c2cf0":"code","8f7151b2":"code","b4d77854":"code","cbb6b18e":"code","c08a0fa3":"code","ab3d7c15":"code","f96f5113":"code","000723ea":"code","d82047ea":"code","87b82591":"code","7e6f894e":"code","0c5ef82e":"code","72bd77af":"code","9bff969c":"code","bea8c2ea":"markdown"},"source":{"ed3ced66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom keras.layers import *\nfrom keras.activations import *\nfrom keras.models import *\nfrom keras.optimizers import *\nfrom keras.initializers import *\nfrom keras.callbacks import *\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d33c2cf0":"# data class\nclass MNIST:\n    def __init__(self):\n        \n        #load data\n        self.train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\n        self.test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n         \n        self.width = 28\n        self.height = 28\n        self.depth = 1\n        self.test_size = 0.2\n        self.num_classes = 10\n\n        #train\n        self.pd_x_train = self.train.drop([\"label\"], axis = 1)\n        self.pd_y_train = self.train[\"label\"].values\n\n        self.np_x_train = np.array(self.pd_x_train)\n        self.np_x_train = self.np_x_train.reshape(self.pd_x_train.shape[0], self.width, self.height, self.depth)\n\n        self.np_y_train = np.array(self.pd_y_train)\n\n        #test\n        self.pd_x_test = self.test\n        self.np_x_test = np.array(self.test)\n        self.np_x_test = self.np_x_test.reshape(self.pd_x_test.shape[0], self.width, self.height, self.depth)\n        \n        # train test split\n        self.x_train, self.x_validation, self.y_train, self.y_validation = train_test_split(self.np_x_train, self.np_y_train, test_size = self.test_size)\n        self.x_test = self.np_x_test\n        \n        #Normalization\n        #self.x_train\n        #self.x_validation\n        #self.x_test\n        \n        #One hot encoding\n        self.y_train = to_categorical(self.y_train, num_classes=self.num_classes)\n        self.y_validation = to_categorical(self.y_validation, num_classes=self.num_classes)\n        \n    def info(self):\n        print(\"x_train: \", self.x_train.shape)\n        print(\"x_validation: \", self.x_validation.shape)\n        print(\"x_test: \", self.x_test.shape)\n        print(\"y_train: \", self.y_train.shape)\n        print(\"y_validation: \", self.y_validation.shape)","8f7151b2":"# Define the CNN\ndef create_model(optimizer, lr, width, height, depth, num_classes):\n    \n    input_img = Input(shape=(width, height, depth))\n\n    x = Conv2D(filters=16, kernel_size=3, strides=(1, 1), padding=\"same\")(input_img)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x) \n    x = MaxPool2D()(x)\n    \n    x = Conv2D(filters=16, kernel_size=6, strides=(1, 1), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Dropout(rate = 0.1)(x)\n    x = Activation(\"relu\")(x) \n    x = MaxPool2D()(x)\n    \n    x = Conv2D(filters=16, kernel_size=9, strides=(1, 1), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x) \n    x = Dropout(rate = 0.1)(x)\n    x = MaxPool2D()(x)\n    \n    x = Flatten()(x)\n    x = Dense(128)(x)\n    x = Activation(\"relu\")(x)\n    x = Dense(64)(x)\n    x = Activation(\"relu\")(x)\n    x = Dense(num_classes)(x)\n    output_pred = Activation(\"softmax\")(x)\n   \n    optimizer = optimizer(lr=lr)\n    model = Model(inputs=input_img, outputs=output_pred)\n    model.compile(\n        loss=\"categorical_crossentropy\", \n        optimizer=optimizer, \n        metrics=[\"accuracy\"])\n    model.summary()\n    \n    return model","b4d77854":"data = MNIST()\ndata.info()","cbb6b18e":"#show example images\n\nrows = 5\ncols = 5\n\nfig, axs = plt.subplots(rows,cols, figsize = (25,25))\n\nfor i in range(rows):\n    for j in range(cols):      \n        axs[i,j].imshow(data.x_train[rows*i+j].reshape(28,28))\n        axs[i,j].set_title(np.argmax(data.y_train[rows*i+j]))\nfig.show()","c08a0fa3":"# hyperparameter\nlr = 1e-2\noptimizer = Adam\nbatch_size = 32\nepochs = 15","ab3d7c15":"# create model\nmodel = create_model(optimizer, lr, data.width, data.height, data.depth, data.num_classes)","f96f5113":"# define learning rate sheduler\ndef schedule(epoch):\n    lr = 0.01\/(epoch+1)\n\n    return lr\n\nlrs = LearningRateScheduler(\n    schedule=schedule,\n    verbose=1)","000723ea":"# training\nhistory = model.fit(\n    x=data.x_train, \n    y=data.y_train, \n    verbose=1, \n    #batch_size=batch_size, \n    epochs=epochs, \n    validation_data=(data.x_validation, data.y_validation),\n    callbacks=[lrs])","d82047ea":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","87b82591":"# store wrong predictions\n\npred = model.predict(data.x_validation)\n\n# init\nwrong = np.array([])\n\nfor k in range(pred.shape[0]):\n    ClassId_pred = np.argmax(pred[k])\n    ClassId_true= np.argmax(data.y_validation[k])\n    if ClassId_pred != ClassId_true: \n        wrong = np.append(wrong, k)\n        \nprint(\"Number of wrong predictions: \", wrong.size)\nprint(\"Percentage if wrong predictions: {0:.3f}\".format((wrong.size\/pred.shape[0])*100), \"%\")","7e6f894e":"#show examples of wrong predictions\n\nrows = 5\ncols = 5\n\nlabel_true = \"true\"\n\nfig, axs = plt.subplots(rows,cols, figsize = (25,25))\n\nfor i in range(rows):\n    for j in range(cols):      \n        axs[i,j].imshow(data.x_validation[int(wrong[rows*i+j])].reshape(28,28))\n        axs[i,j].set_title(\"true: \"+str(np.argmax(data.y_validation[int(wrong[rows*i+j])]))+\"\\n pred: \"+str(np.argmax(pred[int(wrong[rows*i+j])])))\nfig.show()","0c5ef82e":"# predict results of x_test\nresults = model.predict(data.x_test)\nresults = np.argmax(results,axis = 1)\n\n#create data frame\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)","72bd77af":"submission.head(15)","9bff969c":"submission.to_csv('submission.csv', index = False)","bea8c2ea":"<a href=\".\/submission.csv\"> Download File <\/a>"}}