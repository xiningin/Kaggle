{"cell_type":{"b1894d4c":"code","02098821":"code","dd97242a":"code","2cab5d7a":"code","fa5064e9":"code","a25da41a":"code","2c080e13":"code","c8c4d0f8":"code","94e26d0e":"code","0186a802":"code","eb2ad831":"code","c86c0ae7":"code","b7604630":"code","4465fe92":"code","8aada143":"code","8ef24f85":"code","25144d29":"code","701e68d2":"code","373efa84":"code","50b9f560":"code","d2593ac3":"code","31407c1e":"code","ff9e3c2c":"code","a7a82cd4":"code","9213ddbe":"markdown","f35c7bfc":"markdown","fb232b10":"markdown","a69b82c4":"markdown","1f075cc9":"markdown","d7a1e460":"markdown","687a9d8d":"markdown","92e05ed3":"markdown","437c7c38":"markdown","eeaccc32":"markdown","942b49b1":"markdown","e11b4f66":"markdown","79c01df0":"markdown","c1cce271":"markdown"},"source":{"b1894d4c":"!pip install imutils\n","02098821":"import numpy as np\nimport pandas as pd\nfrom scipy.spatial import distance as dist\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport seaborn as sns\nfrom tqdm import tqdm \nfrom sklearn.utils import shuffle\nfrom sklearn import decomposition\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport tensorflow as tf\nimport keras\nfrom keras.applications.vgg16 import VGG16 \nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Sequential, Model \nfrom keras.applications import DenseNet201\nfrom keras.initializers import he_normal\nfrom keras.layers import Lambda, SeparableConv2D, BatchNormalization, Dropout, MaxPooling2D, Input, Dense, Conv2D, Activation, Flatten \nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport imutils\n","dd97242a":"def findEdges(image):\n    # find edges in image\n    gray = cv2.GaussianBlur(image, (1, 1), 0)\n    edged = cv2.Canny(gray, 100, 400)\n    edged = cv2.dilate(edged, None, iterations=1)\n    edged = cv2.erode(edged, None, iterations=1)\n    return edged\n","2cab5d7a":"def getImgContours(edged):\n    # find contours in the edge map\n    contours = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    contours = imutils.grab_contours(contours)\n    contours = sorted(contours, key=lambda x: cv2.contourArea(x))\n    return contours","fa5064e9":"def getBoxes(contours, orig):\n    # get the boxes\n    boxes = []\n    centers = []\n    for contour in contours:\n        box = cv2.minAreaRect(contour)\n        box = cv2.cv.BoxPoints(box) if imutils.is_cv2() else cv2.boxPoints(box)\n        box = np.array(box, dtype=\"int\")\n        (tl, tr, br, bl) = box\n        if (dist.euclidean(tl, bl)) > 0 and (dist.euclidean(tl, tr)) > 0:\n            boxes.append(box)\n    return boxes","a25da41a":"class_names = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\nnb_classes = len(class_names)\nimage_size = (120,120)","2c080e13":"def load_data():\n\n    datasets = ['\/kaggle\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TRAIN','\/kaggle\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TEST' ]\n    images = []\n    labels = []\n\n    # iterate through training and test sets\n    count =0\n    for dataset in datasets:\n\n        # iterate through folders in each dataset\n        for folder in os.listdir(dataset):\n\n            if folder in ['EOSINOPHIL']: label = 0\n            elif folder in ['LYMPHOCYTE']: label = 1\n            elif folder in ['MONOCYTE']: label = 2\n            elif folder in ['NEUTROPHIL']: label = 3\n\n            # iterate through each image in folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n\n                # get pathname of each image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n\n                # Open \n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # add padding to the image to better detect cell at the edge\n                image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[198, 203, 208])\n                \n                #thresholding the image to get the target cell\n                image1 = cv2.inRange(image,(80, 80, 180),(180, 170, 245))\n                \n                # openning errosion then dilation\n                kernel = np.ones((3, 3), np.uint8)\n                kernel1 = np.ones((5, 5), np.uint8)\n                img_erosion = cv2.erode(image1, kernel, iterations=2)\n                image1 = cv2.dilate(img_erosion, kernel1, iterations=5)\n                \n                #detecting the blood cell\n                edgedImage = findEdges(image1)\n                edgedContours = getImgContours(edgedImage)\n                edgedBoxes =  getBoxes(edgedContours, image.copy())\n                if len(edgedBoxes)==0:\n                    count +=1\n                    continue\n                # get the large box and get its cordinate\n                last = edgedBoxes[-1]\n                max_x = int(max(last[:,0]))\n                min_x = int( min(last[:,0]))\n                max_y = int(max(last[:,1]))\n                min_y = int(min(last[:,1]))\n                \n                # draw the contour and fill it \n                mask = np.zeros_like(image)\n                cv2.drawContours(mask, edgedContours, len(edgedContours)-1, (255,255,255), -1) \n                \n                # any pixel but the pixels inside the contour is zero\n                image[mask==0] = 0\n                \n                # extract th blood cell\n                image = image[min_y:max_y, min_x:max_x]\n\n                if (np.size(image)==0):\n                    count +=1\n                    continue\n                # resize th image\n                image = cv2.resize(image, image_size)\n\n                # Append the image and its corresponding label to the output\n                images.append(image)\n                labels.append(label)\n\n    images = np.array(images, dtype = 'float32')\n    labels = np.array(labels, dtype = 'int32')\n\n    return images, labels","c8c4d0f8":"images, labels = load_data()","94e26d0e":"images, labels = shuffle(images, labels, random_state=10)\n\ntrain_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size = 0.2)\ntest_images, val_images, test_labels, val_labels = train_test_split(test_images, test_labels, test_size = 0.5)","0186a802":"n_train = train_labels.shape[0]\nn_val = val_labels.shape[0]\nn_test = test_labels.shape[0]\n\nprint(\"Number of training examples: {}\".format(n_train))\nprint(\"Number of validation examples: {}\".format(n_val))\nprint(\"Number of testing examples: {}\".format(n_test))\n\nprint(\"Training images are of shape: {}\".format(train_images.shape))\nprint(\"Training labels are of shape: {}\".format(train_labels.shape))\nprint(\"Validation images are of shape: {}\".format(val_images.shape))\nprint(\"Validation labels are of shape: {}\".format(val_labels.shape))\nprint(\"Test images are of shape: {}\".format(test_images.shape))\nprint(\"Test labels are of shape: {}\".format(test_labels.shape))\n","eb2ad831":"_, train_counts = np.unique(train_labels, return_counts = True)\n_, val_counts = np.unique(val_labels, return_counts = True)\n_, test_counts = np.unique(test_labels, return_counts = True)\n\npd.DataFrame({'train': train_counts, \"val\": val_counts, \"test\": test_counts}, index = class_names).plot.bar()\n\nplt.show()","c86c0ae7":"plt.pie(train_counts,\n        explode=(0, 0, 0, 0) , \n        labels=class_names,\n        autopct='%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of each observed category')\nplt.show()","b7604630":"train_images = train_images \/ 255.0 \nval_images = val_images \/ 255.0\ntest_images = test_images \/ 255.0","4465fe92":"def display_random_image (class_names, images, labels):\n    print(len(images))\n    index = np.random.randint(images.shape[0])\n    plt.figure()\n    plt.imshow(images[index])\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.title('Image #{}: '.format(index) + class_names[labels[index]])\n    plt.show()\n    \ndisplay_random_image (class_names, train_images, train_labels)","8aada143":"def display_examples(class_names, images, labels):\n    fig = plt.figure(figsize = (10,10))\n    fig.suptitle(\"Examples of images in the dataset\", fontsize=16)\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(images[i], cmap=plt.cm.binary)\n        plt.xlabel(class_names[labels[i]])\n    plt.show()\n    \ndisplay_examples(class_names, train_images, train_labels)","8ef24f85":"model1 = Sequential()\n\n# First Conv block\nmodel1.add(Conv2D(16 , (3,3) , padding = 'same' , activation = 'relu' , input_shape = (120,120,3)))\nmodel1.add(Conv2D(16 , (3,3), padding = 'same' , activation = 'relu'))\nmodel1.add(MaxPooling2D(pool_size = (2,2)))\n\n# Second Conv block\nmodel1.add(SeparableConv2D(32, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(SeparableConv2D(32, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPooling2D(pool_size = (2,2)))\n\n# Third Conv block\nmodel1.add(SeparableConv2D(64, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(SeparableConv2D(64, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPooling2D(pool_size = (2,2)))\n\n# Forth Conv block\nmodel1.add(SeparableConv2D(128, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(SeparableConv2D(128, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPooling2D(pool_size = (2,2)))\nmodel1.add(Dropout(0.2))\n\n# Fifth Conv block \nmodel1.add(SeparableConv2D(256, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(SeparableConv2D(256, (3,3), activation = 'relu', padding = 'same'))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPooling2D(pool_size = (2,2)))\nmodel1.add(Dropout(0.2))\n\n\n# FC layer \nmodel1.add(Flatten())\nmodel1.add(Dense(units = 512 , activation = 'tanh'))\nmodel1.add(Dropout(0.7))\nmodel1.add(Dense(units = 128 , activation = 'tanh'))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(units = 64 , activation = 'tanh'))\nmodel1.add(Dropout(0.3))\n\n# Output layer\nmodel1.add(Dense(units = 4 , activation = 'softmax'))\n\n# Compile\nmodel1.compile(optimizer = \"adam\" , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])\nmodel1.summary()\n\n# Implement callbacks \ncheckpoint = ModelCheckpoint(filepath='best_model.hdf5', save_best_only=True, save_weights_only=False)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=3, verbose = 1, mode='min', restore_best_weights = True)\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor = 'val_accuracy', \n    patience = 2, \n    verbose = 1, \n    factor = 0.3, \n    min_lr = 0.000001)\n\n# Train\nhistory1 = model1.fit(\n    train_images, \n    train_labels, \n    batch_size = 32, \n    epochs = 30, \n    validation_data=(val_images, val_labels), \n    callbacks=[learning_rate_reduction])","25144d29":"def plot_accuracy_loss_chart(history):\n    epochs = [i for i in range(30)]\n    fig , ax = plt.subplots(1,2)\n    train_acc = history.history['accuracy']\n    train_loss = history.history['loss']\n    val_acc = history.history['val_accuracy']\n    val_loss = history.history['val_loss']\n    fig.set_size_inches(20,10)\n    ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n    ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n    ax[0].set_title('Training & Validation Accuracy')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"Accuracy\")\n\n    ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n    ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n    ax[1].set_title('Training & Validation Loss')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    ax[1].set_ylabel(\"Training & Validation Loss\")\n    plt.show()","701e68d2":"plot_accuracy_loss_chart(history1)","373efa84":"\nresults = model1.evaluate(test_images, test_labels)\n\nprint(\"Loss of the model  is - test \", results[0])\nprint(\"Accuracy of the model is - test\", results[1]*100, \"%\")\n\n\nresults = model1.evaluate(val_images, val_labels)\n\nprint(\"Loss of the model  is - val \", results[0])\nprint(\"Accuracy of the model is - val\", results[1]*100, \"%\")\n\nresults = model1.evaluate(train_images, train_labels)\n\nprint(\"Loss of the model  is - train \", results[0])\nprint(\"Accuracy of the model is - train\", results[1]*100, \"%\")","50b9f560":"model1.save('Bloodcell_Classification_Model1_Custom_Build_10_epochs.h5')","d2593ac3":"from sklearn.metrics import classification_report\n\npredictions = model1.predict(test_images)\npredictions = np.argmax(predictions,axis=1)\npredictions[:15]","31407c1e":"print(classification_report(\n    test_labels, \n    predictions, \n    target_names = ['EOSINOPHIL (Class 0)', 'LYMPHOCYTE (Class 1)', 'MONOCYTE (Class 2)', 'NEUTROPHIL (Class 3)']))","ff9e3c2c":"cm = confusion_matrix(test_labels, predictions)\ncm = pd.DataFrame(cm, index = ['0', '1', '2', '3'], columns = ['0', '1', '2', '3'])\ncm","a7a82cd4":"def plot_confusion_matrix (cm):\n    plt.figure(figsize = (10,10))\n    sns.heatmap(\n        cm, \n        cmap = 'Blues', \n        linecolor = 'black', \n        linewidth = 1, \n        annot = True, \n        fmt = '', \n        xticklabels = class_names, \n        yticklabels = class_names)\n    \nplot_confusion_matrix(cm)","9213ddbe":"# Data Exploration\n\n- How many training, validation and testing examples do we have ? What shape are the images and labels ? \n\n- What is the proportion of each observed category ? Are the datasets balanced ? ","f35c7bfc":"# References\n\nSome code was adapted from [kbrans](https:\/\/www.kaggle.com\/kbrans\/cnn-91-6-acc-with-new-train-val-test-splits),[Vincee](https:\/\/www.kaggle.com\/vincee\/intel-image-classification-cnn-keras) and [Abhinav Sagar](https:\/\/towardsdatascience.com\/deep-learning-for-detecting-pneumonia-from-x-ray-images-fc9a3d9fdba8)","fb232b10":"# Evaluating performance\n\nFollowing model training I emplemented a helper function to plot Accuracy vs Epoch and Loss vs Epoch for both the Training and Validation sets\n","a69b82c4":"Another sanity check: Visualise the data by displaying a single random image and 25 random images from the training set with corresponding labels. \n","1f075cc9":"Lets evaluate the model on test data to find the loss and accuracy:\n","d7a1e460":"Relevant packages and libraries are imported ","687a9d8d":"# Custom built CNN\n\nThis model contains a sequence of five Conv blocks containing combinations of SeparableConv2D, BatchNormalization, MaxPooling and Dropout layers. The output of the final Conv block is flattened and followed by three Fully Connected (FC) layers each with its own Dropout layer. A final FC layer is added with four units and a softmax activation for multiclass classification. ","92e05ed3":"A second helper function is created to plot confusion matrix. which will give us a better visualization of these results. ","437c7c38":"# Blood Cell Subtype Classification \n\nwe will  process and load the images, and build  Convolutional Neural Network using keras. The evaluation will be done using SkLearn's Classification Reports, Accuracy\/Loss charts and Confusion Matrices. \n","eeaccc32":"All data (Training and Test combined) is shuffled and split into Training (80%), Validation (10%), Test (10%) sets","942b49b1":"A quick note on preparing the data: Pixel values in RGB images are represented by integers between 0..255. Data is normalised so pixel values are between 0..1 to increase computational speed. ","e11b4f66":"# Loading Data:\n\nLoad_data function that loads images and labels from the Train and Test folders and combines the datasets to return Image-Label pairs \ncombining the Train\/Test folders and recreate the dataset .","79c01df0":"# Object detection:\n\ndid object detection using opencv by findig edges in the images the extract contours and from contours get image boxes","c1cce271":"Model (and weights) are saved after training. The best model and its weights can also be saved using the ModelCheckpoint callback if \"EarlyStopping\" is activated in the Model.fit phase. [More on this here](https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/)\n\nI decided against using EarlyStopping, but I will try to include it in future work on this dataset. "}}