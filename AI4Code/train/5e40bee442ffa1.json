{"cell_type":{"90784ca0":"code","bc2d7ed9":"code","c061316a":"code","1fed275b":"code","8fd32639":"code","36e3b93a":"code","b9476ef4":"code","7fdaa593":"code","b7257208":"code","35b4f4c8":"code","d6d5b52f":"code","62b4a3b0":"code","6702626e":"code","420e4760":"code","756a75f8":"code","f919c173":"code","d8a664f9":"code","3b38cbc5":"code","946b7260":"code","43a58d0e":"code","b94fa84e":"code","1dc06451":"code","38f0df89":"code","d3586fe4":"code","14f3494a":"code","ff9ed924":"code","d0e5363e":"code","d2d1db3e":"code","7100b454":"code","9284b75d":"code","1e91d0bf":"code","9d5aba10":"code","8a8c2544":"code","bfb60446":"code","1874e770":"code","b6a862dd":"code","e7db7e0e":"code","a182f759":"code","cc4e9ce8":"code","e7a7b8a0":"code","e4349a34":"code","d8ac0ca6":"code","b59c320b":"code","dd9a5568":"code","530e4463":"code","6a195684":"code","7f28564b":"code","dbca31f1":"code","1ed8a5f5":"code","e9cfc137":"code","50b76529":"code","2c01463b":"code","655ba514":"code","11102deb":"code","9de5955d":"code","355fe3d8":"code","4f90d856":"code","ce6540e4":"code","b91d85e4":"code","eab8f37c":"code","779dfde0":"code","bb48d76c":"code","92564955":"code","b087a269":"code","65531699":"code","9d7a28b4":"code","4a8c26a2":"code","625f2d44":"code","c7bf3254":"code","69befe2e":"code","23495788":"code","8d22935d":"code","f4e5684e":"code","5d8e4e6f":"code","0bbca3fa":"code","483a4cac":"code","1fc96d6d":"code","1658165c":"code","8ae384a4":"code","e256cf88":"code","bf944409":"code","609e1b6a":"code","9f4c75a7":"code","b9aa21c1":"code","fff80be4":"code","e5ebd61c":"code","1680168c":"code","0cc5dd35":"code","5553c50b":"code","d929f0b4":"code","002fec7c":"code","c77aa4e3":"code","a56f2f57":"code","902b89f7":"code","2108095e":"code","3cebc4ad":"code","d5d3975b":"code","7b7db7ce":"code","605abb12":"code","c3ca8ff8":"code","dadbec82":"code","c8dfa03a":"code","c1ede8ec":"code","fd00b0c8":"code","ca3423a8":"code","e656f6b1":"code","a4bb276e":"code","202d0a39":"code","be719eff":"code","938e2ee2":"code","1d631e12":"code","c73879ad":"code","c726e8a0":"code","c71da48c":"code","943dfbf3":"code","d776ad5a":"code","052939fe":"code","18a050d8":"code","aa686918":"code","aa14716c":"code","dc475ad2":"code","befef976":"code","333d9868":"code","40daedc3":"code","cf558caf":"markdown","8a489e3b":"markdown","c496c672":"markdown","eca48939":"markdown","209cb953":"markdown","ef9b7a7f":"markdown","9d8d897d":"markdown","98f540fc":"markdown","db867ae4":"markdown","981e0b48":"markdown","a28ab4e6":"markdown","5543d7c7":"markdown","ddca565d":"markdown","0b85b2b1":"markdown","cedadd28":"markdown","78107dcf":"markdown","9cb7080f":"markdown","9df7fa9e":"markdown","d5973c03":"markdown","8e55a444":"markdown","0046d305":"markdown","df897374":"markdown","a222b5bc":"markdown","d7217f1c":"markdown","669238c5":"markdown","17573f1c":"markdown","78e84f2d":"markdown","5ff66e3d":"markdown","607351a7":"markdown","b8f135b4":"markdown","2097edec":"markdown","61499bd6":"markdown","d29a9ab1":"markdown","a0ebdc8c":"markdown","824e9bfd":"markdown","8c8f314c":"markdown","cd7a80ae":"markdown","7134cc93":"markdown","6325b591":"markdown","df3fcb1c":"markdown","f2a44b34":"markdown","17c9e1be":"markdown","36b06fbb":"markdown","44127a0d":"markdown","6622c6ed":"markdown","0d9a4734":"markdown","1ec6d8e2":"markdown","df980387":"markdown","71cbc956":"markdown","22e21fd9":"markdown","ee1431df":"markdown","09d8ec26":"markdown","48736440":"markdown","270c7115":"markdown","ab3b9ebb":"markdown","1ad6a9b4":"markdown","f4b17275":"markdown","8a58fa18":"markdown","bd80379a":"markdown","2b4d98c3":"markdown","f4a3ab99":"markdown","32d67e81":"markdown","fc0be8dc":"markdown","b51e69ff":"markdown","14419738":"markdown","731d3ec5":"markdown","aeaacefb":"markdown","1004ce1b":"markdown","f46105f8":"markdown","75e36748":"markdown","06357f6a":"markdown","6aab7c87":"markdown","6d320e85":"markdown","b20cc948":"markdown","e5d2d488":"markdown","53dccee8":"markdown","e65ce7cc":"markdown","b2055e56":"markdown","abfcae5f":"markdown","402e742f":"markdown","14488782":"markdown","bdf38380":"markdown","3c8e684b":"markdown","0d948771":"markdown","7152af93":"markdown","98171764":"markdown","32fbd761":"markdown","e63aa106":"markdown","b8d7ec2d":"markdown","f2ec319c":"markdown","f426529a":"markdown","d56f0e4d":"markdown","0bf8a9d8":"markdown","ed282493":"markdown","421b455c":"markdown","f0729313":"markdown","6c7cf356":"markdown","092db0ce":"markdown","1239b8c7":"markdown","038e800d":"markdown","2c6d3c73":"markdown","0b89fdf7":"markdown","98e863f2":"markdown","50e7f01a":"markdown","a3beb689":"markdown","c6817897":"markdown","21f1b4a7":"markdown","9548d13b":"markdown","dc1be317":"markdown","7f085d22":"markdown","bc9efe5b":"markdown","65193428":"markdown","e078ddc3":"markdown","640604f2":"markdown","cb9b57b5":"markdown","29de73aa":"markdown","c49433c7":"markdown","51127e17":"markdown","39024107":"markdown","a981e40b":"markdown","8657d185":"markdown","e8365741":"markdown","c6310a9c":"markdown","99aa23cd":"markdown","628a4da3":"markdown","a92810a2":"markdown","604d5fff":"markdown","15e95bf2":"markdown","64f811bb":"markdown","ee32b72d":"markdown","70fea99e":"markdown"},"source":{"90784ca0":"# Import necessary modules for data analysis and data visualization. \n# Data analysis modules\n# Pandas is probably the most popular and important modules for any work related to data management. \nimport pandas as pd\n\n\"\"\"# numpy is a great library for doing mathmetical operations. \nimport numpy as np\n\n# Some visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n## Some other snippit of codes to get the setting right \n## This is so that the chart created by matplotlib can be shown in the jupyter notebook. \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \n\"\"\"\nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning\n\nimport os ## imporing os\nprint(os.listdir(\"..\/input\/\")) ","bc2d7ed9":"## Importing the datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","c061316a":"## Take a look at the overview of the dataset. \ntrain.sample(5)","1fed275b":"test.sample(5)","8fd32639":"print (\"The shape of the train data is (row, column):\"+ str(train.shape))\nprint (train.info())\nprint (\"The shape of the test data is (row, column):\"+ str(test.shape))\nprint (test.info())","36e3b93a":"%%HTML\n<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='An Overview of Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='Titanic_data_mining&#47;Dashboard1' \/><param name='tabs' value='no' \/><param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1516349898238');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","b9476ef4":"## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n## We will drop PassengerID and Ticket since it will be useless for our data. \n#train.drop(['PassengerId'], axis=1, inplace=True)\n#test.drop(['PassengerId'], axis=1, inplace=True)\n\nprint (train.info())\nprint (\"*\"*40)\nprint (test.info())","7fdaa593":"# Let's write a functin to print the total percentage of the missing values.(this can be a good exercise for beginners to try to write simple functions like this.)\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","b7257208":"missing_percentage(train)","35b4f4c8":"missing_percentage(test)","d6d5b52f":"def percent_value_counts(df, feature):\n    \"\"\"This function takes in a dataframe and a column and finds the percentage of the value_counts\"\"\"\n    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\n    ## creating a df with th\n    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n    ## concating percent and total dataframe\n\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return pd.concat([total, percent], axis = 1)\n    ","62b4a3b0":"percent_value_counts(train, 'Embarked')","6702626e":"train[train.Embarked.isnull()]","420e4760":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=train, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 18)\nax2.set_title('Test Set',  fontsize = 18)\nfig.show()","756a75f8":"## Replacing the null values in the Embarked column with the mode. \ntrain.Embarked.fillna(\"C\", inplace=True)","f919c173":"print(\"Train Cabin missing: \" + str(train.Cabin.isnull().sum()\/len(train.Cabin)))\nprint(\"Test Cabin missing: \" + str(test.Cabin.isnull().sum()\/len(test.Cabin)))","d8a664f9":"## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)","3b38cbc5":"all_data.Cabin = [i[0] for i in all_data.Cabin]","946b7260":"percent_value_counts(all_data, \"Cabin\")","43a58d0e":"all_data.groupby(\"Cabin\")['Fare'].mean().sort_values()","b94fa84e":"with_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()","1dc06451":"def cabin_estimator(i):\n    \"\"\"Grouping cabin feature by the first letter\"\"\"\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n    ","38f0df89":"##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))","d3586fe4":"## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers","14f3494a":"test[test.Fare.isnull()]","ff9ed924":"missing_value = test[(test.Pclass == 3) & (test.Embarked == \"S\") & (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)","d0e5363e":"print (\"Train age missing value: \" + str((train.Age.isnull().sum()\/len(train))*100)+str(\"%\"))\nprint (\"Test age missing value: \" + str((test.Age.isnull().sum()\/len(test))*100)+str(\"%\"))","d2d1db3e":"pal = {'male':\"green\", 'female':\"Pink\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Sex\", \n            y = \"Survived\", \n            data=train, \n            palette = pal,\n            linewidth=2 )\nplt.title(\"Survived\/Non-Survived Passenger Gender Distribution\", fontsize = 25)\nplt.ylabel(\"% of passenger survived\", fontsize = 15)\nplt.xlabel(\"Sex\",fontsize = 15);\n\n","7100b454":"pal = {1:\"seagreen\", 0:\"gray\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.countplot(x = \"Sex\", \n                   hue=\"Survived\",\n                   data = train, \n                   linewidth=2, \n                   palette = pal\n)\n\n## Fixing title, xlabel and ylabel\nplt.title(\"Passenger Gender Distribution - Survived vs Not-survived\", fontsize = 25)\nplt.xlabel(\"Sex\", fontsize = 15);\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\n\n## Fixing xticks\n#labels = ['Female', 'Male']\n#plt.xticks(sorted(train.Sex.unique()), labels)\n\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title(\"Survived\")\nlegs = leg.texts\nlegs[0].set_text(\"No\")\nlegs[1].set_text(\"Yes\")\nplt.show()","9284b75d":"\nplt.subplots(figsize = (15,10))\nsns.barplot(x = \"Pclass\", \n            y = \"Survived\", \n            data=train, \n            linewidth=2)\nplt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25)\nplt.xlabel(\"Socio-Economic class\", fontsize = 15);\nplt.ylabel(\"% of Passenger Survived\", fontsize = 15);\nlabels = ['Upper', 'Middle', 'Lower']\n#val = sorted(train.Pclass.unique())\nval = [0,1,2] ## this is just a temporary trick to get the label right. \nplt.xticks(val, labels);","1e91d0bf":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(train.Pclass[train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived')\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Passenger Class\", fontsize = 15)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(train.Pclass.unique()), labels);","9d5aba10":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Fare\", fontsize = 15)\n\n","8a8c2544":"train[train.Fare > 280]","bfb60446":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 25)\nplt.xlabel(\"Age\", fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15);","1874e770":"pal = {1:\"seagreen\", 0:\"gray\"}\ng = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Survived\", margin_titles=True, hue = \"Survived\",\n                  palette=pal)\ng = g.map(plt.hist, \"Age\", edgecolor = 'white');\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)\n","b6a862dd":"g = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Embarked\", margin_titles=True, hue = \"Survived\",\n                  palette = pal\n                  )\ng = g.map(plt.hist, \"Age\", edgecolor = 'white').add_legend();\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)","e7db7e0e":"g = sns.FacetGrid(train, size=5,hue=\"Survived\", col =\"Sex\", margin_titles=True,\n                palette=pal,)\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\ng.fig.suptitle(\"Survived by Sex, Fare and Age\", size = 25)\nplt.subplots_adjust(top=0.85)","a182f759":"## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]\n## factor plot\nsns.factorplot(x = \"Parch\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title(\"Factorplot of Parents\/Children survived\", fontsize = 25)\nplt.subplots_adjust(top=0.85)","cc4e9ce8":"sns.factorplot(x =  \"SibSp\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title('Factorplot of Sibilings\/Spouses survived', fontsize = 25)\nplt.subplots_adjust(top=0.85)","e7a7b8a0":"# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)","e4349a34":"train.describe()","d8ac0ca6":"train.describe(include =['O'])","b59c320b":"train[['Pclass', 'Survived']].groupby(\"Pclass\").mean().reset_index()","dd9a5568":"# Overview(Survived vs non survied)\nsurvived_summary = train.groupby(\"Survived\")\nsurvived_summary.mean().reset_index()","530e4463":"survived_summary = train.groupby(\"Sex\")\nsurvived_summary.mean().reset_index()","6a195684":"survived_summary = train.groupby(\"Pclass\")\nsurvived_summary.mean().reset_index()","7f28564b":"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))","dbca31f1":"## get the most important variables. \ncorr = train.corr()**2\ncorr.Survived.sort_values(ascending=False)","1ed8a5f5":"## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nimport numpy as np\nmask = np.zeros_like(train.corr(), dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(), \n            annot=True,\n            #mask = mask,\n            cmap = 'RdBu_r',\n            linewidths=0.1, \n            linecolor='white',\n            vmax = .9,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20);","e9cfc137":"male_mean = train[train['Sex'] == 1].Survived.mean()\n\nfemale_mean = train[train['Sex'] == 0].Survived.mean()\nprint (\"Male survival mean: \" + str(male_mean))\nprint (\"female survival mean: \" + str(female_mean))\n\nprint (\"The mean difference between male and female survival rate: \" + str(female_mean - male_mean))","50b76529":"# separating male and female dataframe. \nmale = train[train['Sex'] == 1]\nfemale = train[train['Sex'] == 0]\n\n# getting 50 random sample for male and female. \nimport random\nmale_sample = random.sample(list(male['Survived']),50)\nfemale_sample = random.sample(list(female['Survived']),50)\n\n# Taking a sample means of survival feature from male and female\nmale_sample_mean = np.mean(male_sample)\nfemale_sample_mean = np.mean(female_sample)\n\n# Print them out\nprint (\"Male sample mean: \" + str(male_sample_mean))\nprint (\"Female sample mean: \" + str(female_sample_mean))\nprint (\"Difference between male and female sample mean: \" + str(female_sample_mean - male_sample_mean))","2c01463b":"import scipy.stats as stats\n\nprint (stats.ttest_ind(male_sample, female_sample))\nprint (\"This is the p-value when we break it into standard form: \" + format(stats.ttest_ind(male_sample, female_sample).pvalue, '.32f'))","655ba514":"# Creating a new colomn with a \ntrain['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n## Here \"map\" is python's built-in function. \n## \"map\" function basically takes a function and \n## returns an iterable list\/tuple or in this case series. \n## However,\"map\" can also be used like map(function) e.g. map(name_length_group) \n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \n## However, here we don't need to use parameter(\"size\") for name_length_group because when we \n## used the map function like \".map\" with a series before dot, we are basically hinting that series \n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \n\n\n## cuts the column by given bins based on the range of name_length\n#group_names = ['short', 'medium', 'good', 'long']\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)","11102deb":"## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\ntest[\"title\"] = [i.split('.')[0] for i in test.Name]\ntest[\"title\"]= [i.split(',')[1] for i in test.title]","9de5955d":"#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## test data\ntest['title'] = [i.replace('Ms', 'Miss') for i in test.title]\ntest['title'] = [i.replace('Dr', 'rare') for i in test.title]\ntest['title'] = [i.replace('Col', 'rare') for i in test.title]\ntest['title'] = [i.replace('Dona', 'rare') for i in test.title]\ntest['title'] = [i.replace('Rev', 'rare') for i in test.title]","355fe3d8":"## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1","4f90d856":"def family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a","ce6540e4":"train['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)","b91d85e4":"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]","eab8f37c":"train.Ticket.value_counts().sample(10)","779dfde0":"train.drop(['Ticket'], axis=1, inplace=True)\n\ntest.drop(['Ticket'], axis=1, inplace=True)","bb48d76c":"## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare\/train.family_size\ntest['calculated_fare'] = test.Fare\/test.family_size","92564955":"def fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)","b087a269":"train.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)","65531699":"\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)","9d7a28b4":"## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)","4a8c26a2":"## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test);","625f2d44":"## Let's look at the his\nplt.subplots(figsize = (22,10),)\nsns.distplot(train.Age, bins = 100, kde = True, rug = False, norm_hist=False);","c7bf3254":"## create bins for age\ndef age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\n\n\"\"\"train.drop('Age', axis=1, inplace=True)\ntest.drop('Age', axis=1, inplace=True)\"\"\"","69befe2e":"# separating our independent and dependent variable\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]\n\n\n#age_filled_data_nor = NuclearNormMinimization().complete(df1)\n#Data_1 = pd.DataFrame(age_filled_data, columns = df1.columns)\n#pd.DataFrame(zip(Data[\"Age\"],Data_1[\"Age\"],df[\"Age\"]))","23495788":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)","8d22935d":"train.sample(5)","f4e5684e":"headers = X_train.columns \n\nX_train.head()","5d8e4e6f":"# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\nX_train = sc.fit_transform(X_train)\n## transforming \"test_x\"\nX_test = sc.transform(X_test)\n\n## transforming \"The testset\"\ntest = sc.transform(test)","0bbca3fa":"pd.DataFrame(X_train, columns=headers).head()","483a4cac":"train.calculated_fare = train.calculated_fare.astype(float)","1fc96d6d":"plt.subplots(figsize = (12,10))\nplt.scatter(train.Age, train.Survived);\nplt.xlabel(\"Age\")\nplt.ylabel('Survival Status');","1658165c":"# import LogisticRegression model in python. \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n## call on the model object\nlogreg = LogisticRegression(solver='liblinear')\n\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(X_train,y_train)\n\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"test_x\" portion of the data(this data was not used to fit the model) to predict model outcome. \ny_pred = logreg.predict(X_test)\n\n## Once predicted we save that outcome in \"y_pred\" variable.\n## Then we compare the predicted value( \"y_pred\") and actual value(\"test_y\") to see how well our model is performing. \n\nprint (\"So, Our accuracy Score is: {}\".format(round(accuracy_score(y_pred, y_test),4)))","8ae384a4":"from sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import confusion_matrix\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\n\nclass_names = np.array(['not_survived','survived'])\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","e256cf88":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","bf944409":"from sklearn.metrics import roc_curve, auc\n#plt.style.use('seaborn-pastel')\ny_score = logreg.decision_function(X_test)\n\nFPR, TPR, _ = roc_curve(y_test, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()","609e1b6a":"from sklearn.metrics import precision_recall_curve\n\ny_score = logreg.decision_function(X_test)\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","9f4c75a7":"## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\n\n## saving the feature names for decision tree display\ncolumn_names = X.columns\n\nX = sc.fit_transform(X)\naccuracies = cross_val_score(LogisticRegression(solver='liblinear'), X,y, cv  = cv)\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))","b9aa21c1":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\n## C_vals is the alpla value of lasso and ridge regression(as alpha increases the model complexity decreases,)\n## remember effective alpha scores are 0<alpha<infinity \nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\n## Choosing penalties(Lasso(l1) or Ridge(l2))\npenalties = ['l1','l2']\n## Choose a cross validation strategy. \ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n## setting param for param_grid in GridSearchCV. \nparam = {'penalty': penalties, 'C': C_vals}\n\nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n## Fitting the model\ngrid.fit(X, y)","fff80be4":"## Getting the best of everything. \nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)\n\n","e5ebd61c":"### Using the best parameters from the grid-search.\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)","1680168c":"## Importing the model. \nfrom sklearn.neighbors import KNeighborsClassifier\n## calling on the model oject. \nknn = KNeighborsClassifier(metric='minkowski', p=2)\n## knn classifier works by doing euclidian distance \n\n\n## doing 10 fold staratified-shuffle-split cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\n\naccuracies = cross_val_score(knn, X,y, cv = cv, scoring='accuracy')\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),3)))","0cc5dd35":"## Search for an optimal value of k for KNN.\nk_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X,y, cv = cv, scoring = 'accuracy')\n    k_scores.append(scores.mean())\nprint(\"Accuracy scores are: {}\\n\".format(k_scores))\nprint (\"Mean accuracy score: {}\".format(np.mean(k_scores)))\n","5553c50b":"from matplotlib import pyplot as plt\nplt.plot(k_range, k_scores)","d929f0b4":"from sklearn.model_selection import GridSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n## Fitting the model. \ngrid.fit(X,y)","002fec7c":"print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)\n","c77aa4e3":"### Using the best parameters from the grid-search.\nknn_grid= grid.best_estimator_\nknn_grid.score(X,y)","a56f2f57":"from sklearn.model_selection import RandomizedSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \n## for RandomizedSearchCV, \ngrid = RandomizedSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\n## Fitting the model. \ngrid.fit(X,y)","902b89f7":"print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","2108095e":"### Using the best parameters from the grid-search.\nknn_ran_grid = grid.best_estimator_\nknn_ran_grid.score(X,y)","3cebc4ad":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(X_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)","d5d3975b":"from sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\ngrid_search.fit(X,y)","7b7db7ce":"print(grid_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)","605abb12":"# using the best found hyper paremeters to get the score. \nsvm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)","c3ca8ff8":"from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) ","dadbec82":"print( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)","c8dfa03a":"dectree_grid = grid.best_estimator_\n## using the best found hyper paremeters to get the score. \ndectree_grid.score(X,y)","c1ede8ec":"from sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\nfrom IPython.display import Image\ndot_data = StringIO()  \nexport_graphviz(dectree_grid, out_file=dot_data,  \n                feature_names=column_names,  class_names = ([\"Survived\" if int(i) is 1 else \"Not_survived\" for i in y.unique()]),\n                filled=True, rounded=True,\n                proportion=True,\n                special_characters=True)  \n(graph,) = pydot.graph_from_dot_data(dot_data.getvalue())\n\n## alternative tree\n#import graphviz\n#from sklearn import tree\n#dot_data = tree.export_graphviz(decision_tree=dectree_grid, out_file=None, feature_names=column_names, )\n#graph = graphviz.Source(dot_data)\n#graph.render(\"house\")\n#graph\n\nImage(graph.create_png())","fd00b0c8":"## feature importance\nfeature_importances = pd.DataFrame(dectree_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)","ca3423a8":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) ","e656f6b1":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)","a4bb276e":"rf_grid = grid.best_estimator_\nrf_grid.score(X,y)","202d0a39":"from sklearn.metrics import classification_report\n# Print classification report for y_test\nprint(classification_report(y_test, y_pred, labels=rf_grid.classes_))","be719eff":"## feature importance\nfeature_importances = pd.DataFrame(rf_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)","938e2ee2":"from sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) ","1d631e12":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)","c73879ad":"bagging_grid = grid.best_estimator_\nbagging_grid.score(X,y)","c726e8a0":"from sklearn.ensemble import AdaBoostClassifier\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                     ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) ","c71da48c":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)","943dfbf3":"adaBoost_grid = grid.best_estimator_\nadaBoost_grid.score(X,y)","d776ad5a":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X, y)\ny_pred = gradient_boost.predict(X_test)\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gradient_accy)","052939fe":"from xgboost import XGBClassifier\nXGBClassifier = XGBClassifier()\nXGBClassifier.fit(X, y)\ny_pred = XGBClassifier.predict(X_test)\nXGBClassifier_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(XGBClassifier_accy)","18a050d8":"from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(X_test)\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(extraTree_accy)\n","aa686918":"from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(X_test)\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gau_pro_accy)","aa14716c":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lr_grid', logreg_grid),\n    ('svc', svm_grid),\n    ('random_forest', rf_grid),\n    ('gradient_boosting', gradient_boost),\n    ('decision_tree_grid',dectree_grid),\n    ('knn_classifier', knn_grid),\n    ('XGB_Classifier', XGBClassifier),\n    ('bagging_classifier', bagging_grid),\n    ('adaBoost_classifier',adaBoost_grid),\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\n    ('gaussian_classifier',gaussian),\n    ('gaussian_process_classifier', GaussianProcessClassifier)\n],voting='hard')\n\n#voting_classifier = voting_classifier.fit(train_x,train_y)\nvoting_classifier = voting_classifier.fit(X,y)","dc475ad2":"y_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)","befef976":"#models = pd.DataFrame({\n#    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n#              'Random Forest', 'Naive Bayes', \n#              'Decision Tree', 'Gradient Boosting Classifier', 'Voting Classifier', 'XGB Classifier','ExtraTrees Classifier','Bagging Classifier'],\n#    'Score': [svc_accy, knn_accy, logreg_accy, \n#              random_accy, gaussian_accy, dectree_accy,\n#               gradient_accy, voting_accy, XGBClassifier_accy, extraTree_accy, bagging_accy]})\n#models.sort_values(by='Score', ascending=False)","333d9868":"all_models = [logreg_grid,\n              knn_grid, \n              knn_ran_grid,\n              svm_grid,\n              dectree_grid,\n              rf_grid,\n              bagging_grid,\n              adaBoost_grid,\n              voting_classifier]\n\nc = {}\nfor i in all_models:\n    a = i.predict(X_test)\n    b = accuracy_score(a, y_test)\n    c[i] = b\n    \n","40daedc3":"test_prediction = (max(c, key=c.get)).predict(test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": test_prediction\n    })\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)","cf558caf":"Some standard approaches of dealing with null values are mean, median and mode. However, we will take a different approach since **~20% data in the Age column is missing** in both train and test dataset. The age variable seems to be promising for determining survival rate. Therefore, It would be unwise to replace the missing values with median, mean or mode. We will use machine learning model Random Forest Regressor to impute missing value instead of  Null value. We will keep the age column unchanged for now and work on that in the feature engineering section. ","8a489e3b":"## Pros and cons of boosting\n\n---\n\n### Pros\n\n- Achieves higher performance than bagging when hyper-parameters tuned properly.\n- Can be used for classification and regression equally well.\n- Easily handles mixed data types.\n- Can use \"robust\" loss functions that make the model resistant to outliers.\n\n---\n\n### Cons\n\n- Difficult and time consuming to properly tune hyper-parameters.\n- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n- More risk of overfitting compared to bagging.\n\n<h3>Resources: <\/h3>\n<ul>\n    <li><a href=\"http:\/\/mccormickml.com\/2013\/12\/13\/adaboost-tutorial\/\">AdaBoost Tutorial-Chris McCormick<\/a><\/li>\n    <li><a href=\"http:\/\/rob.schapire.net\/papers\/explaining-adaboost.pdf\">Explaining AdaBoost by Robert Schapire(One of the original author of AdaBoost)<\/a><\/li>\n<\/ul>","c496c672":"**Missing values in *test* set.**","eca48939":"## 7j. XGBClassifier\n<a id=\"XGBClassifier\"><\/a>\n***","209cb953":"Let's take a look at the histogram of the age column. ","ef9b7a7f":"## age","9d8d897d":"## Gaussian Naive Bayes\n<a id=\"gaussian_naive\"><\/a>\n***","98f540fc":"**Train info**","db867ae4":"![title](https:\/\/cdn-images-1.medium.com\/max\/400\/1*hFJ-LI7IXcWpxSLtaC0dfg.png)","981e0b48":"# Part 3. Visualization and Feature Relations\n<a id=\"visualization_and_feature_relations\" ><\/a>\n***\nBefore we dive into finding relations between different features and our dependent variable(survivor) let us create some predictions about how the relations may turnout among features.\n\n**Predictions:**\n- Gender: More female survived than male\n- Pclass: Higher socio-economic status passenger survived more than others. \n- Age: Younger passenger survived more than other passengers. \n\nNow, let's see how the features are related to each other by creating some visualizations. \n\n","a28ab4e6":"We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of $80, are of Pclass 1 and female Sex. Let's see how the **Fare** is distributed among all **Pclass** and **Embarked** feature values","5543d7c7":"Here the data points are not continuous; rather categorical. The two horizontal dot lines represent the survival status in the y-axis and age in the x-axis. This is probably not the best graph to explain logistic regression. For the convenience of understanding the model, let's look at a similar scatter plot with some characteristics.\n\n<img src=\"https:\/\/sds-platform-private.s3-us-east-2.amazonaws.com\/uploads\/39_blog_image_3.png\" width=\"600\">\n<h5 align=\"right\">SuperDataScience team<\/h5>\n\nThis chart clearly divides the binary categorical values in the x-axis, keeping most of the 0's on the left side, and 1's on the right side. So, now that the distinction is apparent, we can use our knowledge of linear regression and come up with a regression line. So, how can we apply a regression line to explain this data?\n\n<img src=\"https:\/\/sds-platform-private.s3-us-east-2.amazonaws.com\/uploads\/39_blog_image_4.png\" width=\"800\">\n<h5 align=\"right\">SuperDataScience team<\/h5>\n\nAs you can see from the chart above, The linear regression is probably not the best approach to take for categorical data. The Linear regression line barely aligns with the data points, and even if in some best-case scenario we were to use straight regression line, we would end up with a considerable error rate, which is super inconvenient. This is where logistic regression comes in. \n\n #### This part of the kernel is a working progress. Please check back again for future updates.####","ddca565d":"# Part 2: Overview and Cleaning the Data\n<a id=\"cleaningthedata\"><\/a>\n***\n## 2a. Overview","0b85b2b1":"All the cabin names start with an English alphabet following by multiple digits. It seems like there are some passengers that had booked multiple cabin rooms in their name. This is because many of them travelled with family. However, they all seem to book under the same letter followed by different numbers. It seems like there is a significance with the letters rather than the numbers. Therefore, we can group these cabins according to the letter of the cabin name. ","cedadd28":"## 3a. Gender and Survived\n<a id=\"gender_and_survived\"><\/a>\n***","78107dcf":"<h3><font color=\"$5831bc\" face=\"Comic Sans MS\">Before Scaling<\/font><\/h3>","9cb7080f":"This KDE plot is pretty self-explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second. \n\n**Summary**\n***\nThe first class passengers had the upper hand during the tragedy than second and third. You can probably agree with me more on this, in the next section of visualizations where we look at the distribution of ticket fare and survived column. ","9df7fa9e":"<h3>Why use Bagging? (Pros and cons)<\/h3>\nBagging works best with strong and complex models(for example, fully developed decision trees). However, don't let that fool you to thinking that similar to a decision tree, bagging also overfits the model. Instead, bagging reduces overfitting since a lot of the sample training data are repeated and used to create base estimators. With a lot of equally likely training data, bagging is not very susceptible to overfitting with noisy data, therefore reduces variance. However, the downside is that this leads to an increase in bias.","d5973c03":"As I promised before, we are going to use Random forest regressor in this section to predict the missing age values. Let's see how many missing values do we have now","8e55a444":"You can see how the features have transformed above.","0046d305":"It looks like there are only two null values( ~ 0.22 %) in the Embarked feature, we can replace these with the mode value \"S\". However, let's dig a little deeper. \n\n**Let's see what are those two null values**","df897374":"we have our confusion matrix. How about we give it a little more character. ","a222b5bc":"<a id=\"introduction\" ><\/a><br>\nThis kernel is for all aspiring data scientists to learn from and to review their knowledge. We will have a detailed statistical analysis of Titanic data set along with Machine learning model implementation. I am super excited to share my first kernel with the Kaggle community. As I go on in this journey and learn new topics, I will incorporate them with each new updates. So, check for them and please <b>leave a comment<\/b> if you have any suggestions to make them better!! Going back to the topics of this kernel, I will do more in-depth visualizations to explain the data, and the machine learning classifiers will be used to predict passenger survival status. So, let's get started.\n\n\n\n<div style=\"text-align: left\">This notebook goes indepth in classifier models since we are trying to solve a classifier problem here. If you want to learn more about Advanced Regression models, please check out <a href=\"https:\/\/www.kaggle.com\/masumrumi\/a-stats-analysis-and-ml-workflow-of-house-pricing\">this<\/a> kernel.<\/div>\n","d7217f1c":"***\n\nIf you like to discuss any other projects or have a chat about data science topics, I'll be more than happy to connect with you on:\n\n**LinkedIn:** https:\/\/www.linkedin.com\/in\/masumrumi\/ \n\n**My Website:** http:\/\/masumrumi.com\/ \n\n*** This kernel is a work in progress. I will always incorporate new concepts of data science as I master them. This journey of learning is worth sharing as well as collaborating. Therefore any comments about further improvements would be genuinely appreciated.***\n***\n## If you have come this far, Congratulations!!\n\n## If this notebook helped you in any way, please upvote!!\n\n","669238c5":"## 7h. AdaBoost Classifier\n<a id=\"AdaBoost\"><\/a>\n***\nAdaBoost is another <b>ensemble model<\/b> and is quite different than Bagging. Let's point out the core concepts. \n> AdaBoost combines a lot of \"weak learners\"(they are also called stump; a tree with only one node and two leaves) to make classifications.\n\n> This base model fitting is an iterative process where each stump is chained one after the other; <b>It cannot run in parallel.<\/b>\n\n> <b>Some stumps get more say in the final classifications than others.<\/b> The models use weights that are assigned to each data point\/raw indicating their \"importance.\" Samples with higher weight have a higher influence on the total error of the next model and gets more priority. The first stump starts with uniformly distributed weight which means, in the beginning, every datapoint have an equal amount of weights. \n\n> <b>Each stump is made by talking the previous stump's mistakes into account.<\/b> After each iteration weights gets re-calculated in order to take the errors\/misclassifications from the last stump into consideration. \n\n> The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. \n\nTo illustrate what we have talked about so far let's look at the following visualization. \n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*paPv7vXuq4eBHZY7.png\">\n<h5 align=\"right\"> Source: Diogo(Medium)<\/h5>\n\n\n\n\nLet's dive into each one of the nitty-gritty stuff about AdaBoost:\n***\n> <b>First<\/b>, we determine the best feature to split the dataset using Gini index(basics from decision tree). The feature with the lowest Gini index becomes the first stump in the AdaBoost stump chain(the lower the Gini index is, the better unmixed the label is, therefore, better split).\n***\n> <b>Secondly<\/b>, we need to determine how much say a stump will have in the final classification and how we can calculate that.\n* We learn how much say a stump has in the final classification by calculating how well it classified the samples (aka calculate the total error of the weight).\n* The <b>Total Error<\/b> for a stump is the sum of the weights associated with the incorrectly classified samples. For example, lets say, we start a stump with 10 datasets. The first stump will uniformly distribute an weight amoung all the datapoints. Which means each data point will have 1\/10 weight. Let's say once the weight is distributed we run the model and find 2 incorrect predicitons. In order to calculate the total erorr we add up all the misclassified weights. Here we get 1\/10 + 1\/10 = 2\/10 or 1\/5. This is our total error. We can also think about it\n### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n* Since the weight is uniformly distributed(all add up to 1) among all data points, the total error will always be between 0(perfect stump) and 1(horrible stump).\n* We use the total error to determine the amount of say a stump has in the final classification using the following formula\n \n### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n\nWhere $\\epsilon_t$ is the misclassification rate for the current classifier:\n\n### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n\nHere...\n* $\\alpha_t$ = Amount of Say\n* $\\epsilon_t$ = Total error\n\n\n\nWe can draw a graph to determine the amount of say using the value of total error(0 to 1)\n\n<img src=\"http:\/\/chrisjmccormick.files.wordpress.com\/2013\/12\/adaboost_alphacurve.png\">\n<h5 align=\"right\"> Source: Chris McCormick<\/h5>\n\n* The blue line tells us the amount of say for <b>Total Error(Error rate)<\/b> between 0 and 1. \n* When the stump does a reasonably good job, and the <b>total error<\/b> is minimal, then the <b>amount of say(Alpha)<\/b> is relatively large, and the alpha value is positive. \n* When the stump does an average job(similar to a coin flip\/the ratio of getting correct and incorrect ~50%\/50%), then the <b>total error<\/b> is ~0.5. In this case the <b>amount of say<\/b> is <b>0<\/b>.\n* When the error rate is high let's say close to 1, then the <b>amount of say<\/b> will be negative, which means if the stump outputs a value as \"survived\" the included weight will turn that value into \"not survived.\"\n\nP.S. If the <b>Total Error<\/b> is 1 or 0, then this equation will freak out. A small amount of error is added to prevent this from happening. \n \n ***\n> <b>Third<\/b>, We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account. The pseducode for calculating the new sample weight is as follows. \n### $$ New Sample Weight = Sample Weight + e^{\\alpha_t}$$\nHere the $\\alpha_t(AmountOfSay)$ can be positive or negative depending whether the sample was correctly classified or misclassified by the current stump. We want to increase the sample weight of the misclassified samples; hinting the next stump to put more emphasize on those. Inversely, we want to decrease the sample weight of the correctly classified samples; hinting the next stump to put less emphasize on those. \n\nThe following equation help us to do this calculation. \n### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n\nHere, \n* $D_{t+1}(i)$ = New Sample Weight. \n* $D_t(i)$ = Current Sample weight.\n* $\\alpha_t$ = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and \n* $y_i h_t(x_i)$ = place holder for 1 if stump correctly classified, -1 if misclassified. \n\nFinally, we put together the combined classifier, which is \n### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$ \n\nHere, \n\n$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n\n$T$ is the set of \"weak learners\"\n\n$\\alpha_t$ is the contribution weight for weak learner $t$\n\n$h_t(X)$ is the prediction of weak learner $t$\n\nand $y$ is binary **with values -1 and 1**\n\n\nP.S. Since the stump barely captures essential specs about the dataset, the model is highly biased in the beginning. However, as the chain of stumps continues and at the end of the process, AdaBoost becomes a strong tree and reduces both bias and variance.\n\n<h3>Resources:<\/h3>\n<ul>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=LsK-xG1cLYA\">Statquest<\/a><\/li>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=-DUxtdeCiB4\">Principles of Machine Learning | AdaBoost(Video)<\/a><\/li>\n<\/ul>","17573f1c":"## fare_group","78e84f2d":"## Grid Search on Logistic Regression\n* What is grid search? \n* What are the pros and cons?\n\n**Gridsearch** is a simple concept but effective technique in Machine Learning. The word **GridSearch** stands for the fact that we are searching for optimal parameter\/parameters over a \"grid.\" These optimal parameters are also known as **Hyperparameters**. **The Hyperparameters are model parameters that are set before fitting the model and determine the behavior of the model.**. For example, when we choose to use linear regression, we may decide to add a penalty to the loss function such as Ridge or Lasso. These penalties require specific alpha (the strength of the regularization technique) to set beforehand. The higher the value of alpha, the more penalty is being added. GridSearch finds the optimal value of alpha among a range of values provided by us, and then we go on and use that optimal value to fit the model and get sweet results. It is essential to understand those model parameters are different from models outcomes, for example, **coefficients** or model evaluation metrics such as **accuracy score** or **mean squared error** are model outcomes and different than hyperparameters.\n\n#### This part of the kernel is a working progress. Please check back again for future updates.####","5ff66e3d":"This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n**Summary**\n***\n- As we suspected, female passengers have survived at a much better rate than male passengers. \n- It seems about right since females and children were the priority. ","607351a7":"There is nothing out of the ordinary of about this plot, except the very left part of the distribution. It proves that children and infants were the priority, therefore, a good chunk of infant\/children were saved. ","b8f135b4":"## 1c. A Glimpse of the Datasets. \n<a id=\"glimpse\"><\/a>\n***","2097edec":"### Hypothesis testing\n#### Formulating a well developed researched question: \nRegarding this dataset, we can formulate the null hypothesis and alternative hypothesis by asking the following questions. \n> * **Is there a significant difference in the mean sex between the passenger who survived and passenger who did not survive?**. \n> * **Is there a substantial difference in the survival rate between the male and female passengers?**\n#### The Null Hypothesis and The Alternative Hypothesis\nWe can formulate our hypothesis by asking questions differently. However, it is essential to understand what our end goal is. Here our dependent variable or target variable is **Survived**. Therefore, we say\n\n> ** Null Hypothesis($H_0$)** There is no difference in the survival rate between the male and female passengers. or the mean difference between male and female passenger in the survival rate is zero.  \n>  ** Alternative Hypothesis($H_A$):** There is a difference in the survival rate between the male and female passengers. or the mean difference in the survival rate between male and female is not zero. \n\n#### Determine the test statistics\n> This will be a two-tailed test since the difference between male and female passenger in the survival rate could be higher or lower than 0. \n> Since we do not know the standard deviation($\\sigma$) and n is small, we will use the t-distribution. \n\n#### Specify the significance level\n> Specifying a significance level is an important step of the hypothesis test. It is an ultimate balance between type 1 error and type 2 error. We will discuss more in-depth about those in another lesson. For now, we have decided to make our significance level($\\alpha$) = 0.05. So, our confidence interval or non-rejection region would be (1 - $\\alpha$) =   95%. \n\n#### Computing T-statistics and P-value\nLet's take a random sample and see the difference.","61499bd6":"## 7m. Voting Classifier\n<a id=\"voting_classifer\"><\/a>\n***","d29a9ab1":"## ticket","a0ebdc8c":"<h3>Resources: <\/h3>\n<ul>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=sDv4f4s2SB8\">Gradient Descent(StatQuest)<\/a><\/li>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Main Ideas)(StatQuest)<\/a><\/li>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Calculation)(StatQuest)<\/a><\/li>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=jxuNLH5dXCs\">Gradient Boost(Classification Main Ideas)(StatQuest)<\/a><\/li>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=StWY5QWMXCw\">Gradient Boost(Classification Calculation)(StatQuest)<\/a><\/li>\n    <li><a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\">Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python<\/a><\/li>\n<\/ul>\n","824e9bfd":"## 7a. Logistic Regression\n<a id=\"logistic_regression\"><\/a>\n***\nWe will start with one of the most basic but effective machine learning model, **Logistic Regression**. Logistic regression is a famous classifier still used today frequently despite its age. It is a regression similar to **Linear regression**, yet operates as a classifier. To understand logistic regression, we should have some idea about linear regression. Let's have a look at it. \n\nHopefully, we all know that any linear equation can be written in the form of...\n\n# $$ {y} = mX + b $$\n\n* Here, m = slope of the regression line. it represents the relationship between X and y. \n* b = y-intercept. \n* x and y are the points location in x_axis and y_axis respectively. \n<br\/>\n\nIf you want to know how, check out this [video](https:\/\/www.khanacademy.org\/math\/algebra\/two-var-linear-equations\/writing-slope-intercept-equations\/v\/graphs-using-slope-intercept-form). So, this slope equation can also be written as...\n\n## $$ y = \\beta_0 + \\beta_1 x + \\epsilon \\\\ $$\n\nThis is the equation for a simple linear regression.\nhere,\n* y = Dependent variable. \n* $\\beta_0$ = the intercept, it is constant. \n* $\\beta_1$ = Coefficient of independent variable. \n* $x$ = Indepentent variable. \n* $ \\epsilon$ = error or residual. \n\n\nWe use this function to predict the value of a dependent variable with the help of only one independent variable. Therefore this regression is called **Simple Linear Regression.** \n\nSimilar to **Simple Linear Regression**, there is **Multiple Linear Regression** which can be used to predict dependent variable using multiple independent variables. Let's look at the equation for **Multiple Linear Regression**, \n\n## $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n\n\nIf you would like to know more about **Linear Regression** checkout this [kernel](https:\/\/www.kaggle.com\/masumrumi\/a-stats-analysis-and-ml-workflow-of-house-pricing). \n\nSo, we know\/reviewed a bit about linear regression, and therefore we know how to deal with data that looks like this, \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/3a\/Linear_regression.svg\/1200px-Linear_regression.svg.png\" width=\"600\">\n\nHere the data point's in this graph is continuous and therefore the problem is a regression one. However, what if we have data that when plotted in a scatter graph, looks like this...\n","8c8f314c":"This grid unveils a couple of interesting insights. Let's find out.\n* The facet grid above clearly demonstrates the three outliers with Fare of over \\$500. At this point, I think we are quite confident that these outliers should be deleted.\n* Most of the passengers were with in the Fare range of \\$100. ","cd7a80ae":"\n#### Using the best parameters from the grid-search. ","7134cc93":"## calculated_fare","6325b591":"### Fare Feature\n***","df3fcb1c":"## 1e. Tableau Visualization of the Data\n<a id='tableau_visualization'><\/a>\n***\nI have incorporated a tableau visualization below of the training data. This visualization... \n* is for us to have an overview and play around with the dataset. \n* is done without making any changes(including Null values) to any features of the dataset.\n***\nLet's get a better perspective of the dataset through this visualization.\n","f2a44b34":"## 2b. Dealing with Missing values\n<a id=\"dealwithnullvalues\"><\/a>\n***\n**Missing values in *train* dataset.**","17c9e1be":"## 7l. Gaussian Process Classifier\n<a id=\"GaussianProcessClassifier\"><\/a>\n***","36b06fbb":"Approximately 77% of Cabin feature is missing in the training data and 78% missing on the test data. \nWe have two choices, \n* we can either get rid of the whole feature, or \n* we can brainstorm a little and find an appropriate way to put them in use. For example, We may say passengers with cabin record had a higher socio-economic-status then others. We may also say passengers with cabin record were more likely to be taken into consideration when loading into the boat.\n\nLet's combine train and test data first and for now, will assign all the null values as **\"N\"**","44127a0d":"<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">Evaluating the model<\/font><\/h2>\nWhile we try to evaluate the model, we want to focus on a couple of things. \n\n<ul>\n    <li>Which are the most importnat features(relatively) of a project ?(<b>Relative Feature Importance<\/b>)<\/li>\n    <li>Which features have the biggest impact on the project on the project success ? (<b>Permutation Importance<\/b>) <\/li>\n    <li>How does changes in those featues affact the project success? (<b>Partial Dependencies<\/b>)<\/li>\n    <li>Digging deeper into the decisions made by the model(<b>SHAP values<\/b>)\n<\/ul>\n\n<h3>Explaining the results of the model.<\/h3>\n<ul>\n    <li>How well is the model ?<\/li>\n    <li>What are the most important features ?<\/li>\n<\/ul>\n\n<h3>Introducting Confusion Matrix<\/h3>\n\nSo, what is accuracy score? what does it tell us? \n\nIntroducing <b>confusion matrix<\/b>, a table that <b>describes the performance of a classification model<\/b>. We use the classification model by using data where we already know the true outcome and compare it with the model predicted an outcome. Confusion Matrix tells us how many our model predicted correctly and incorrectly in terms of binary\/multiple outcome classes. For example, in terms of this dataset, our model is trying to classify whether the passenger survived or not survived. Let's introduce ourselves with some of the terminologies of the confusion matrix. \n\n\n<ul style=\"list-style-type:square;\">\n    <li><b>True Positive<\/b><\/li>\n    <li><b>True Negative<\/b><\/li>\n    <li><b>False Positive<\/b><\/li>\n    <li><b>False Negative<\/b><\/li>\n<\/ul>\n\nLet's find out the confusion matrix for titanic dataset. ","6622c6ed":"### Age Feature\n***","0d9a4734":"I admire working with decision trees because of the potential and basics they provide towards building a more complex model like Random Forest(RF). RF is an ensemble method (combination of many decision trees) which is where the \"forest\" part comes in. One crucial details about Random Forest is that while using a forest of decision trees, RF model <b>takes random subsets of the original dataset(bootstrapped)<\/b> and <b>random subsets of the variables(features\/columns)<\/b>. Using this method, the RF model creates 100's-1000's(the amount can be menually determined) of a wide variety of decision trees. This variety makes the RF model more effective and accurate. We then run each test data point through all of these 100's to 1000's of decision trees or the RF model and take a vote on the output. \n\n","1ec6d8e2":"Some people have travelled in groups like family or friends. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger, therefore calculated fare will be much handy in this situation. ","df980387":"#### Using best estimator from grid search using KNN. ","71cbc956":"<img src=\"http:\/\/data.freehdw.com\/ships-titanic-vehicles-best.jpg\"  Width=\"800\">","22e21fd9":"#### Compare P-value with $\\alpha$\n> It looks like the p-value is very small compared to our significance level($\\alpha$)of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is \"**There is a significant difference in the survival rate between the male and female passengers.\"**","ee1431df":"This plot shows something impressive..\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let's check...","09d8ec26":"I have yet to figureout how to best manage ticket feature. So, any suggestion would be truly appreciated. For now, I will get rid off the ticket feature.","48736440":"I have gathered a small summary from the statistical overview above. Let's see what they are...\n- This data set has 891 raw and 9 columns. \n- only 38% passenger survived during that tragedy.\n- ~74% female passenger survived, while only ~19% male passenger survived. \n- ~63% first class passengers survived, while only 24% lower class passenger survived.\n\n","270c7115":" ## 1d. About This Dataset\n<a id=\"aboutthisdataset\"><\/a>\n***\nThe data has split into two groups:\n\n- training set (train.csv)\n- test set (test.csv)\n\n***The training set includes our target variable(dependent variable), passenger survival status***(also known as the ground truth from the Titanic tragedy) along with other independent features like gender, class, fare, and Pclass. \n\nThe test set should be used to see how well our model performs on unseen data. When we say unseen data, we mean that the algorithm or machine learning models have no relation to the test data. We do not want to use any part of the test data in any way to modify our algorithms; Which are the reasons why we clean our test data and train data separately. ***The test set does not provide passengers survival status***. We are going to use our model to predict passenger survival status.\n\nNow let's go through the features and describe a little. There is a couple of different type of variables, They are...\n\n***\n**Categorical:**\n- **Nominal**(variables that have two or more categories, but which do not have an intrinsic order.)\n   > - **Cabin**\n   > - **Embarked**(Port of Embarkation)\n            C(Cherbourg)\n            Q(Queenstown) \n            S(Southampton)\n        \n- **Dichotomous**(Nominal variable with only two categories)\n   > - **Sex**\n            Female\n            Male\n- **Ordinal**(variables that have two or more categories just like nominal variables. Only the categories can also be ordered or ranked.)\n   > - **Pclass** (A proxy for socio-economic status (SES)) \n            1(Upper)\n            2(Middle) \n            3(Lower)\n***\n**Numeric:**\n- **Discrete**\n  >  - **Passenger ID**(Unique identifing # for each passenger)\n  >  - **SibSp**\n  >  - **Parch**\n  >  - **Survived** (Our outcome or dependent variable)\n            0\n            1\n- **Continous**\n>  - **Age**\n>  - **Fare**\n***\n**Text Variable**\n> - **Ticket** (Ticket number for passenger.)\n> - **Name**(  Name of the passenger.) \n\n","ab3b9ebb":"## 7k. Extra Trees Classifier\n<a id=\"extra_tree\"><\/a>\n***","1ad6a9b4":"## name_length\n***Creating a new feature \"name_length\" that will take the count of letters of each name***","f4b17275":"It seems like <i>PassengerId<\/i> column only works as an id in this dataset without any significant effect on the dataset. Let's drop it.","8a58fa18":"Above is a full-grown decision tree. I think having a tree shown like that can help a lot in understanding how the decision tree works.","bd80379a":" #### This part of the kernel is a working progress. Please check back again for future updates.####\n \n Resources: \n * [Confusion Matrix](https:\/\/www.youtube.com\/watch?v=8Oog7TXHvFY)\n### Under-fitting & Over-fitting: \nSo, we have our first model and its score. But, how do we make sure that our model is performing well. Our model may be overfitting or underfitting. In fact, for those of you don't know what overfitting and underfitting is, Let's find out.\n\n![](https:\/\/cdncontribute.geeksforgeeks.org\/wp-content\/uploads\/fittings.jpg)\n\nAs you see in the chart above. **Underfitting** is when the model fails to capture important aspects of the data and therefore introduces more bias and performs poorly. On the other hand, **Overfitting** is when the model performs too well on the training data but does poorly in the validation set or test sets.  This situation is also known as having less bias but more variation and perform poorly as well. Ideally, we want to configure a model that performs well not only in the training data but also in the test data. This is where **bias-variance tradeoff** comes in. When we have a model that overfits, meaning less biased and more of variance, we introduce some bias in exchange of having much less variance. One particular tactic for this task is regularization models (Ridge, Lasso, Elastic Net).  These models are built to deal with the bias-variance tradeoff. This [kernel](https:\/\/www.kaggle.com\/dansbecker\/underfitting-and-overfitting) explains this topic well. Also, the following chart gives us a mental picture of where we want our models to be. \n![](http:\/\/scott.fortmann-roe.com\/docs\/docs\/BiasVariance\/biasvariance.png)\n\nIdeally, we want to pick a sweet spot where the model performs well in training set, validation set, and test set. As the model gets complex, bias decreases, variance increases. However, the most critical part is the error rates. We want our models to be at the bottom of that **U** shape where the error rate is the least. That sweet spot is also known as **Optimum Model Complexity(OMC).**\n\nNow that we know what we want in terms of under-fitting and over-fitting, let's talk about how to combat them. \n\nHow to combat over-fitting?\n<ul>\n    <li>Simplify the model by using less parameters.<\/li>\n    <li>Simplify the model by changing the hyperparameters.<\/li>\n    <li>Introducing regularization models. <\/li>\n    <li>Use more training data. <\/li>\n    <li>Gatter more data ( and gather better quality data). <\/li>\n    <\/ul>\n #### This part of the kernel is a working progress. Please check back again for future updates.####","2b4d98c3":"<h3><font color=\"#5831bc\" face=\"Comic Sans MS\">After Scaling<\/font><\/h3>","f4a3ab99":"## Using Cross-validation:\nPros: \n* Helps reduce variance. \n* Expends models predictability. \n","32d67e81":"## 1b. Loading Datasets\n<a id=\"load_data\"><\/a>\n***","fc0be8dc":"## 3c. Fare and Survived\n<a id=\"fare_and_survived\"><\/a>\n***","b51e69ff":"**While, passenger who traveled in small groups with sibilings\/spouses had more survival rate than other passengers.**","14419738":"### Cabin Feature\n***","731d3ec5":"#### Manually find the best possible k value for KNN","aeaacefb":"## 7f. Random Forest Classifier\n<a id=\"random_forest\"><\/a>","1004ce1b":"# Part 8: Submit test predictions\n<a id=\"submit_predictions\"><\/a>\n***","f46105f8":"<h3>Why Random Forest?(Pros and Cons)<\/h3>","75e36748":"This bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived.","06357f6a":"### Embarked feature\n***","6aab7c87":"## age_group\nWe can create a new feature by grouping the \"Age\" column","6d320e85":"# Part 1: Importing Necessary Libraries and datasets\n***\n<a id=\"import_libraries**\"><\/a>\n## 1a. Loading libraries\n\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate. ","b20cc948":"**Passenger who traveled in big groups with parents\/children had less survival rate than other passengers.**","e5d2d488":"** Sex is the most important correlated feature with *Survived(dependent variable)* feature followed by Pclass.** ","53dccee8":"Here, We can take the average of the **Fare** column with all the values to fill in for Nan Fare value. However, that might not be the best way to fill in this value. We can be a little more specific and take the average of the values where**Pclass** is ***3***, **Sex** is ***male*** and **Embarked** is ***S***","e65ce7cc":"## PassengerId","b2055e56":"Let's apply <b>cabin_estimator<\/b> function in each unknown cabins(cabin with <b>null<\/b> values). Once that is done we will separate our train and test to continue towards machine learning modeling. ","abfcae5f":"- It looks like ...\n    - ~ 63% first class passenger survived titanic tragedy, while \n    - ~ 48% second class and \n    - ~ only  24% third class passenger survived. \n\n","402e742f":"## 3d. Age and Survived\n<a id=\"age_and_survived\"><\/a>\n***","14488782":"**> Sample train dataset**","bdf38380":"***\n<h2>Introducing Ensemble Learning<\/h2>\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. \n\nThere are two types of ensemple learnings. \n\n**Bagging\/Averaging Methods**\n> In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\n**Boosting Methods**\n> The other family of ensemble methods are boosting methods, where base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\n<h4 align=\"right\">Source:GA<\/h4>\n\nResource: <a href=\"https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\">Ensemble methods: bagging, boosting and stacking<\/a>\n***\n## 7g. Bagging Classifier\n<a id=\"bagging\"><\/a>\n***","3c8e684b":"We see that in both **train**, and **test** dataset have missing values. Let's make an effort to fill these missing values starting with \"Embarked\" feature. ","0d948771":"## 6b. Splitting the training data\n<a id=\"split_training_data\" ><\/a>\n***\nThere are multiple ways of splitting data. They are...\n* train_test_split.\n* cross_validation. \n\nWe have separated dependent and independent features; We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set usually two-thirds of the train data. Once we train our algorithm using 2\/3 of the train data, we start to test our algorithms using the remaining data. If the model performs well we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts, **X_train**, **X_test**, **y_train**, **y_test**.  \n* **X_train** and **y_train** first used to train the algorithm. \n* then, **X_test** is used in that trained algorithms to predict **outcomes. **\n* Once we get the **outcomes**, we compare it with **y_test**\n\nBy comparing the **outcome** of the model with **y_test**, we can determine whether our algorithms are performing well or not. As we compare we use confusion matrix to determine different aspects of model performance.\n\nP.S. When we use cross validation it is important to remember not to use **X_train, X_test, y_train and y_test**, rather we will use **X and y**. I will discuss more on that. ","7152af93":"## Creating dummy variables\n\nYou might be wondering what is a dummy variable? \n\nDummy variable is an important **prepocessing machine learning step**. Often times Categorical variables are an important features, which can be the difference between a good model and a great model. While working with a dataset, having meaningful value for example, \"male\" or \"female\" instead of 0's and 1's is more intuitive for us. However, machines do not understand the value of categorical values, for example, in this dataset we have gender male or female, algorithms do not accept categorical variables as input. In order to feed data in a machine learning model, we  ","98171764":"## Support Vector Machines(SVM)\n<a id=\"svm\"><\/a>\n***","32fbd761":"## 6c. Feature Scaling\n<a id=\"feature_scaling\" ><\/a>\n***\nFeature scaling is an important concept of machine learning models. Often times a dataset contain features highly varying in magnitude and unit. For some machine learning models, it is not a problem. However, for many other ones, its quite a problem. Many machine learning algorithms uses euclidian distances to calculate the distance between two points, it is quite a problem. Let's again look at a the sample of the **train** dataset below.","e63aa106":"## 4a. Correlation Matrix and Heatmap\n<a id=\"heatmap\"><\/a>\n***\n### Correlations","b8d7ec2d":"<h1>AUC & ROC Curve<\/h1>","f2ec319c":"#### Positive Correlation Features:\n- Fare and Survived: 0.26\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\n\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\n\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is. ","f426529a":"As we assumed, it looks like an outlier with a fare of $512. We sure can delete this point. However, we will keep it for now. ","d56f0e4d":"According to the samples our male and female mean measured difference is 0.58, keeping in mind that...\n* We randomly select 50 people to be in the male group and 50 people to be in the female group. \n* We know our sample is selected from a broader population(whole dataset of titanic). \n* We know we could have ended up with a different random sample of males or females from the total dataset. \n***\nWith all three points above in mind, how confident are we that, the measured difference is real or statistically significant? we can perform a **t-test** to evaluate that. When we perform a **t-test** we are usually trying to find out **an evidence of significant difference between population mean with hypothesized mean(1 sample t-test) or in our case difference between two population means(2 sample t-test).** \n\n\n\nThe **t-statistics** is the measure of a degree to which our groups differ standardized by the variance of our measurements. In order words, it is basically the measure of signal over noise. Let us describe the previous sentence a bit more for clarification. I am going to use [this post](http:\/\/blog.minitab.com\/blog\/statistics-and-quality-data-analysis\/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen) as reference to describe the t-statistics here. \n\n\n#### Calculating the t-statistics\n# $$t = \\frac{\\bar{x}-\\mu}{\\frac{S} {\\sqrt{n}} }$$\n\nHere..\n* $\\bar{x}$ is the sample mean. \n* $\\mu$ is the hypothesized mean. \n* S is the standard devaition. \n* n is the sample size. \n\n\nNow, the denominator of this fraction $(\\bar{x}-\\mu)$ is basically the strength of the signal. where we calculate the difference between hypothesized mean and sample mean. If the mean difference is higher, then the signal is stronger. \n\nthe numerator of this fraction ** ${S}\/ {\\sqrt{n}}$ ** calculates the amount of variation or noise of the data set. Here S is standard deviation, which tells us how much variation is there in the data. n is the sample size. \n\nSo, according to the explanation above, the t-value or t-statistics is basically measures the strength of the signal(the difference) to the amount of noise(the variation) in the data and that is how we calculate the t-value in one sample t-test. However, in order to calculate between two sample population mean or in our case we will use the follow equation. \n\n# $$t = \\frac{\\bar{x}_M - \\bar{x}_F}{\\sqrt {s^2 (\\frac{1}{n_M} + \\frac{1}{n_F})}}$$\n\nThis equation may seem too complex, however, the idea behind these two are similar. Both of them have the concept of signal\/noise. The only difference is that we replace our hypothesis mean with another sample mean and the two sample sizes repalce one sample size. \n\nHere..\n* $\\bar{x}_M$ is the mean of our male group sample measurements. \n* $ \\bar{x}_F$ is the mean of female group samples. \n* $ n_M$ and $n_F$ are the sample number of observations in each group. \n* $ S^2$ is the sample variance.\n\nIt is good to have an understanding of what going on in the background. However, we will use **scipy.stats** to find the t-statistics. \n","0bf8a9d8":"# Part 4: Statistical Overview\n<a id=\"statisticaloverview\"><\/a>\n***","ed282493":"Here **Age** and **Calculated_fare** is much higher in magnitude compared to others machine learning features. This can create problems as many machine learning models will get confused thinking **Age** and **Calculated_fare** have higher weight than other features. Therefore, we need to do feature scaling to get a better result. \nThere are multiple ways to do feature scaling. \n<ul>\n    <li><b>MinMaxScaler<\/b>-Scales the data using the max and min values so that it fits between 0 and 1.<\/li>\n    <li><b>StandardScaler<\/b>-Scales the data so that it has mean 0 and variance of 1.<\/li>\n    <li><b>RobustScaler<\/b>-Scales the data similary to Standard Scaler, but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers.<\/b>\n <\/ul>\nI will discuss more on that in a different kernel. For now we will use <b>Standard Scaler<\/b> to feature scale our dataset. \n\nP.S. I am showing a sample of both before and after so that you can see how scaling changes the dataset. ","421b455c":"# Kernel Goals\n<a id=\"aboutthiskernel\"><\/a>\n***\nThere are three primary goals of this kernel.\n- <b>Do a statistical analysis<\/b> of how some group of people was survived more than others. \n- <b>Do an exploratory data analysis(EDA)<\/b> of titanic with visualizations and storytelling.  \n- <b>Predict<\/b>: Use machine learning classification models to predict the chances of passengers survival.\n\nP.S. If you want to learn more about regression models, try this [kernel](https:\/\/www.kaggle.com\/masumrumi\/a-stats-analysis-and-ml-workflow-of-house-pricing\/edit\/run\/9585160). ","f0729313":"So, We still haven't done any effective work to replace the null values. Let's stop for a second here and think through how we can take advantage of some of the other features here.  \n* We can use the average of the fare column We can use pythons ***groupby*** function to get the mean fare of each cabin letter. ","6c7cf356":"Fare group was calculated based on <i>calculated_fare<\/i>. This can further help our cause. ","092db0ce":"These are the top 10 features determined by **Decision Tree** helped classifing the fates of many passenger on Titanic on that night.","1239b8c7":"This is a sample of train and test dataset. Lets find out a bit more about the train and test dataset. ","038e800d":"After loading the necessary modules, we need to import the datasets. Many of the business problems usually come with a tremendous amount of messy data. We extract those data from many sources. I am hoping to write about that in a different kernel. For now, we are going to work with a less complicated and quite popular machine learning dataset.","2c6d3c73":"# Credits\n\n* To [Brandon Foltz](https:\/\/(www.youtube.com\/channel\/UCFrjdcImgcQVyFbK04MBEhA) for being a fantastic statistics teacher. Love all those inspirational intro's. \n* To [Khan Academy](https:\/\/www.khanacademy.org), Amazing place to keep track of my mathematics journey. \n* To [General Assambly](https:\/\/generalassemb.ly); Where I started my data science journey. \n* To [Corey Schafer](https:\/\/www.youtube.com\/channel\/UCCezIgC97PvUuR4_gbFUs5g); Corey explains programming terms incredibly well. To all the newcomers, please check out his style of teaching.\n\n# Resources\nHere are some of the links I found helpful while writing this kernel. I do not assume them to be great articles; neither do I recommend them. I mentioned them because I have found them to be helpful. \n\n## Statistics\n* [What Is a t-test? And Why Is It Like Telling a Kid to Clean Up that Mess in the Kitchen?](https:\/\/blog.minitab.com\/blog\/statistics-and-quality-data-analysis\/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen)\n* [What Are T Values and P Values in Statistics?](https:\/\/blog.minitab.com\/blog\/statistics-and-quality-data-analysis\/what-are-t-values-and-p-values-in-statistics)\n* [What is p-value? How we decide on our confidence level.](https:\/\/www.youtube.com\/watch?v=E4KCfcVwzyw)\n\n","0b89fdf7":"\n**Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.** ","98e863f2":"Datasets in the real world are often messy, However, this dataset is almost clean. Lets analyze and see what we have here.","50e7f01a":"# Part 7: Modeling the Data\n<a id=\"modelingthedata\"><\/a>\n***\nSince the problem we are trying to solve is a classification problem, we are going to use a bunch of classification model to get the best prediction possible. Let's start with Logistic Regression. ","a3beb689":"## 7b. K-Nearest Neighbor classifier(KNN)\n<a id=\"knn\"><\/a>\n***","c6817897":"<a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html\">Bagging Classifier<\/a>(Bootstrap Aggregating) is the ensemble method that involves manipulating the training set by resampling and running algorithms on it. Let's do a quick review:\n* Bagging classifier uses a process called bootstrapped dataset to create multiple datasets from one original dataset and runs algorithm on each one of them. Here is an image to show how bootstrapped dataset works. \n<img src=\"https:\/\/uc-r.github.io\/public\/images\/analytics\/bootstrap\/bootstrap.png\" width=\"600\">\n<h4 align=\"center\">Resampling from original dataset to bootstrapped datasets<\/h4>\n<h4 align=\"right\">Source: https:\/\/uc-r.github.io<\/h4>\n\n\n* After running a learning algorithm on each one of the bootstrapped datasets, all models are combined by taking their average. the test data\/new data then go through this averaged classifier\/combined classifier and predict the output. \n\nHere is an image to make it clear on how bagging works, \n<img src=\"https:\/\/prachimjoshi.files.wordpress.com\/2015\/07\/screen_shot_2010-12-03_at_5-46-21_pm.png\" width=\"600\">\n<h4 align=\"right\">Source: https:\/\/prachimjoshi.files.wordpress.com<\/h4>\nPlease check out [this](https:\/\/www.kaggle.com\/masumrumi\/bagging-with-titanic-dataset) kernel if you want to find out more about bagging classifier. ","21f1b4a7":"## 3e. Combined Feature Relations\n<a id='combined_feature_relations'><\/a>\n***\nIn this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let's get to it. ","9548d13b":"Let's look at the visual of your Decision Tree.","dc1be317":"Now, these means can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean's above. Let's write a simple function so that we can give cabin names based on the means. ","7f085d22":"Now, we have to understand that those two means are not  **the population mean**.  *The population mean is a statistical term statistician uses to indicate the actual average of the entire group. The group can be any gathering of multiple numbers such as animal, human, plants, money, stocks.* For example, To find the age population mean of Bulgaria; we have to account for every single person's age and take their age. Which is almost impossible and if we were to go that route; there is no point of doing statistics in the first place. Therefore we approach this problem using sample sets. The idea of using sample set is that; if we take multiple samples of the same population and take the mean of them and put them in a distribution; eventually our distribution start to look more like a **normal distribution**. The more samples we take and the more sample means with be added and the closer the normal distribution with reach towards population mean. This is where **Central limit theory** comes from. We will go into this topic later on. \n\nGoing back to our dataset, like we are saying these means above are part of the whole story. We were given part of the data to train our machine learning models, and the other part of the data was held back for testing. Therefore, It is impossible for us to know the population means of survival for male and females. Situation like this calls for a statistical approach. We will use the sampling distribution approach to do the test. let's take 50 random sample of male and female from our train data.","bc9efe5b":"# Part 6: Pre-Modeling Tasks\n## 6a. Separating dependent and independent variables\n<a id=\"dependent_independent\"><\/a>\n***\nBefore we apply any machine learning models, It is important to separate dependent and independent variables. Our dependent variable or target variable is something that we are trying to find, and our independent variable is the features we use to find the dependent variable. The way we use machine learning algorithm in a dataset is that we train our machine learning model by specifying independent variables and dependent variable. To specify them, we need to separate them from each other, and the code below does just that.\n\nP.S. In our test dataset, we do not have a dependent variable feature. We are to predict that using machine learning models. ","65193428":"Now let's look at the value counts of the cabin features and see how it looks. ","e078ddc3":"You are probably wondering why two datasets? Also, Why have I named it \"train\" and \"test\"?  To explain that I am going to give you an overall picture of the supervised machine learning process. \n\n\"Machine Learning\" is simply \"Machine\" and \"Learning\". Nothing more and nothing less. In a supervised machine learning process, we are giving machine\/computer\/models specific inputs or data(text\/number\/image\/audio) to learn from aka we are training the machine to learn certain thing based on the data and the output. Now, how do we know what we are teaching is what they are learning? That is where the test set comes to play. We withhold part of the data where we know the output\/result of the algorithms, and we use this data to test the trained machine learning model.  We then compare the outcomes to determine machines performance. If you are a bit confused thats okay. I will explain more as we keep reading. Let's take a look at sample datasets.","640604f2":"#### Using RandomizedSearchCV\nRandomized search is a close cousin of grid search. It doesn't  always provide the best result but its fast. ","cb9b57b5":"Here, in both training set and test set, the average fare closest to $80 are in the <b>C<\/b> Embarked values. So, let's fill in the missing values as \"C\" ","29de73aa":"## 4b. Statistical Test for Correlation\n<a id=\"statistical_test\"><\/a>\n***\n\nStatistical tests are the scientific way to prove the validation of theories. In any case, when we look at the data, we seem to have an intuitive understanding of where data is leading us. However, when we do statistical tests, we get a scientific or mathematical perspective of how significant these results are. Let's apply some of the trials and see how we are doing with our predictions.\n\n###  Hypothesis Testing Outline\n\nA hypothesis test compares the mean of a control group and experimental group and tries to find out whether the two sample means are different from each other and if they are different, how significant that difference is.\n \nA **hypothesis test** usually consists of multiple parts: \n\n1. Formulate a well-developed research problem or question: The hypothesis test usually starts with a concrete and well-developed researched problem. We need to ask the right question that can be answered using statistical analyses. \n2. The null hypothesis ($H_0$) and Alternating hypothesis($H_1$):\n> * The **null hypothesis($H_0$)** is something that is assumed to be true. It is the status quo. In a null hypothesis, the observations are the result of pure chance. When we set out to experiment, we form the null hypothesis by saying that there is no difference between the means of the control group and the experimental group.\n> *  An **Alternative hypothesis($H_A$)** is a claim and the opposite of the null hypothesis.  It is going against the status quo. In an alternative theory, the observations show a real effect combined with a component of chance variation.\n    \n3. Determine the **test statistic**: test statistic can be used to assess the truth of the null hypothesis. Depending on the standard deviation we either use t-statistics or z-statistics. In addition to that, we want to identify whether the test is a one-tailed test or two-tailed test. \n\n4. Specify a **Significance level**: The significance level($\\alpha$) is the probability of rejecting a null hypothesis when it is true. In other words, we are ***comfortable\/confident*** with rejecting the null hypothesis a significant amount of times even though it is true. This considerable amount is our Significant level. In addition to that significance level is one minus our Confidence interval. For example, if we say, our significance level is 5%, then our confidence interval would be (1 - 0.05) = 0.95 or 95%. \n\n5. Compute the **T-statistics**: Computing the t-statistics follows a simple equation. This equation slightly differs depending on one sample test or two sample test  \n\n6. Compute the **P-value**: P-value is the probability that a test statistic at least as significant as the one observed would be obtained assuming that the null hypothesis was correct. The p-value is known to be unintuitive, and even many professors are known to explain it wrong. I think this [video](https:\/\/www.youtube.com\/watch?v=E4KCfcVwzyw) explains the p-value well. **The smaller the P-value, the stronger the evidence against the null hypothesis.**\n\n7. **Describe the result and compare the p-value with the significance value($\\alpha$)**: If p<= $\\alpha$, then the observed effect is statistically significant, the null hypothesis is ruled out, and the alternative hypothesis is valid. However if the p> $\\alpha$, we say that, we fail to reject the null hypothesis. Even though this sentence is grammatically wrong, it is logically right. We never accept the null hypothesis just because we are doing the statistical test with sample data points.\n\nWe will follow each of these steps above to do your hypothesis testing below.\n\n***","c49433c7":"This is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\n* The column represents Sex(left being male, right stands for female)\n* The row represents Embarked(from top to bottom: S, C, Q)\n***\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \n* Most passengers seem to be boarded on Southampton(S).\n* More than 60% of the passengers died boarded on Southampton. \n* More than 60% of the passengers lived boarded on Cherbourg(C).\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \n* There were very few females boarded on Queenstown, however, most of them survived. ","51127e17":"<h4>Random Forest VS. Bagging Classifier<\/h4>\n\nIf some of you are like me, you may find Random Forest to be similar to Bagging Classifier. However, there is a fundamental difference between these two which is **Random Forests ability to pick subsets of features in each node.** I will elaborate on this in a future update.","39024107":"We want to see how the left bar(with green and red) changes when we filter out unique values of a feature. We can use multiple filters to see if there are any correlations among them. For example, if we click on **upper** and **Female** tab, we would see that green color dominates the bar with a ratio of 91:3 survived and non survived female passengers; a 97% survival rate for females. We can reset the filters by clicking anywhere in the whilte space. The age distribution chart on top provides us with some more info such as, what was the age range of those three unlucky females as the red color give away the unsurvived once. If you would like to check out some of my other tableau charts, please click [here.](https:\/\/public.tableau.com\/profile\/masum.rumi#!\/)","a981e40b":"## 3b. Pclass and Survived\n<a id=\"pcalss_and_survived\"><\/a>\n***","8657d185":"## Feature Importance","e8365741":"## 7i. Gradient Boosting Classifier\n<a id=\"gradient_boosting\"><\/a>\n***","c6310a9c":"**> Sample test dataset**","99aa23cd":"## Decision Tree Classifier\n\nDecision tree works by breaking down the dataset into small subsets. This breaking down process is done by asking questions about the features of the datasets. The idea is to unmix the labels by asking fewer questions necessary. As we ask questions, we are breaking down the dataset into more subsets. Once we have a subgroup with only the unique type of labels, we end the tree in that node. If you would like to get a detailed understanding of Decision tree classifier, please take a look at [this](https:\/\/www.kaggle.com\/masumrumi\/decision-tree-with-titanic-dataset) kernel. ","628a4da3":"### Grid search on KNN classifier","a92810a2":"# Part 5: Feature Engineering\n<a id=\"feature_engineering\"><\/a>\n***\nFeature Engineering is exactly what its sounds like. Sometimes we want to create extra features from with in the features that we have, sometimes we want to remove features that are alike. Features engineering is the simple word for doing all those. It is important to remember that we will create new features in such ways that will not cause **multicollinearity(when there is a relationship among independent variables)** to occur. ","604d5fff":"It looks like, the features have unequal amount of data entries for every column and they have many different types of variables. This can happen for the following reasons...\n* We may have missing values in our features.\n* We may have categorical features. \n* We may have alphanumerical or\/and text features. \n","15e95bf2":"Facetgrid is a great way to visualize multiple variables and their relationships at once. From section 3a we have a bright idea of females being more of a priority then males. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky","64f811bb":"## is_alone","ee32b72d":"## family_size\n***Creating a new feature called \"family_size\".*** ","70fea99e":"## title\n**Getting the title of each name as a new feature. **"}}