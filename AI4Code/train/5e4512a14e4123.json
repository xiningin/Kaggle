{"cell_type":{"1ed0b882":"code","7d4d3b88":"code","3e4e8a7f":"code","3b89026a":"code","c83d9ee1":"code","186357ce":"code","f5fa024a":"code","f4a29879":"code","19f5cf83":"code","59a10c6d":"code","5142eaec":"code","c5d13a1f":"code","7feb088f":"code","6bf27290":"code","6195122e":"code","c544e234":"code","adaf752e":"code","aece4b52":"code","a121b757":"code","008a556b":"code","05b67431":"code","f71b819f":"code","79662fdd":"code","07080940":"code","d2be4be5":"code","1cd0d7e6":"code","53757c9c":"code","9f8c4370":"code","455a88e6":"code","611c53ed":"code","f870416a":"code","28d1e14d":"code","028f7ab7":"code","012e1173":"code","fddc554e":"code","37f926e7":"markdown","478b4c0d":"markdown","12dd349c":"markdown","92f05ad9":"markdown","2fec637a":"markdown","9ab5c6cb":"markdown","8fa968fc":"markdown","4e177cef":"markdown","12734f60":"markdown","76cf9725":"markdown","eb018d64":"markdown","a9b69e51":"markdown"},"source":{"1ed0b882":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport math\nimport plotly.graph_objects as go\nfrom ipywidgets import widgets\nfrom tqdm import tqdm\n\nplt.style.use('ggplot')\n\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.models import Model\nimport scipy.signal as signal\n\nimport gc\nINPUT_DIR = '..\/input\/xinwang-userbehaviour-sensor-data\/'\n","7d4d3b88":"def save_pred(train_pred, test_pred, ver=4):\n    INPUT_DIR = '..\/input\/xinwang-userbehaviour-sensor-data\/'\n    sub_df = pd.read_csv(f'{INPUT_DIR}submission_v4.csv')\n    pd.DataFrame(train_pred, index=['Fragment '+ str(i) for i in range(train_pred.shape[0])]).to_csv(f'train_pred_v{str(ver)}.csv', index=False)\n    pd.DataFrame(test_pred, index=['Fragment '+ str(i) for i in range(test_pred.shape[0])]).to_csv(f'test_pred_v{str(ver)}.csv', index=False)\n    # identify the index at which the maximum value occurs \n    # from https:\/\/towardsdatascience.com\/keras-accuracy-metrics-8572eb479ec7#:~:text=Categorical%20Accuracy%20calculates%20the%20percentage,yTrue%2C%20it%20is%20considered%20accurate.\n    sub_df.behavior_id = np.argmax(test_pred, axis=1)\n    sub_df.to_csv(f'submission_v{str(ver)}.csv', index=False)\n    \n    \n    \ndef acc_combo(y, y_pred):\n    # \u6570\u503cID\u4e0e\u884c\u4e3a\u7f16\u7801\u7684\u5bf9\u5e94\u5173\u7cfb\n    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n    # \u5c06\u884c\u4e3aID\u8f6c\u4e3a\u7f16\u7801\n    code_y, code_y_pred = mapping[y], mapping[y_pred]\n    if code_y == code_y_pred: #\u7f16\u7801\u5b8c\u5168\u76f8\u540c\u5f97\u52061.0\n        return 1.0\n    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #\u7f16\u7801\u4ec5\u5b57\u6bcd\u90e8\u5206\u76f8\u540c\u5f97\u52061.0\/7\n        return 1.0\/7\n    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #\u7f16\u7801\u4ec5\u6570\u5b57\u90e8\u5206\u76f8\u540c\u5f97\u52061.0\/3\n        return 1.0\/3\n    else:\n        return 0.0\n    \ndef acc_combo_all(y_train, train_pred):\n    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n    \n    num_examples =  y_train.shape[0]\n    \n    digit_score = 0\n    num_score = 0\n    total_score = 0\n    \n    idx_true = []\n    idx_true_digit = []\n    idx_true_num = []\n    \n    for idx, (y, y_pred) in enumerate(zip(y_train, train_pred)):\n        code_y, code_y_pred = mapping[y], mapping[y_pred]\n        if code_y == code_y_pred: #\u7f16\u7801\u5b8c\u5168\u76f8\u540c\u5f97\u52061.0\n            total_score += 1.0\n            idx_true.append(idx)\n        elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #\u7f16\u7801\u4ec5\u5b57\u6bcd\u90e8\u5206\u76f8\u540c\u5f97\u52061.0\/7\n            digit_score += 1.0\n            idx_true_digit.append(idx)\n        elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #\u7f16\u7801\u4ec5\u6570\u5b57\u90e8\u5206\u76f8\u540c\u5f97\u52061.0\/3\n            num_score += 1.0\n            idx_true_num.append(idx)\n            \n        else:\n            pass\n    \n    num_examples_tn_tp = total_score\n    #num_examples_fn_fp = num_examples - num_examples_tn_tp\n    total_score += num_score \/ 3 + digit_score \/ 7\n    \n    \n        \n    return {'total_score':round(total_score \/ num_examples,5), \n            'digit_score': round((digit_score+num_examples_tn_tp) \/ num_examples,5),\n            'num_score': round((num_score+num_examples_tn_tp) \/ num_examples,5),\n            'index_true': idx_true,\n            'index_true_digit': idx_true_digit,\n            'index_true_num': idx_true_num}\n\n\n\n\n\n\ndef generator(data_x, data_y, batch_size, dataAugment, shuffle=False):\n    \n    # \u5148shuffle\u6253\u4e71\n    if shuffle == True:\n        data_x, data_y = shuffle_data(data_x, data_y)\n    \n    # \u4ea7\u751fbatch\n    while True:\n        data_num = data_x.shape[0]\n        range_num = data_num \/\/ batch_size + 1\n        for i in range(range_num):\n            begin_index, end_index = i * batch_size, (i + 1) * batch_size\n\n            list_batch_x, list_batch_y = [], []\n            if begin_index < data_num:\n                batch_x, batch_y = data_x[begin_index:end_index], data_y[begin_index:end_index]\n                list_batch_x.append(batch_x), list_batch_y.append(batch_y)\n\n                # \u6570\u636e\u589e\u5f3a \n                if dataAugment == True:\n                    # \u6570\u636e\u589e\u5f3a mixup\n                    batch_x_mixup, batch_y_mixup = mixup(batch_x, batch_y, random_max=0.05)\n                    list_batch_x.append(batch_x_mixup), list_batch_y.append(batch_y_mixup)\n\n                    # \u6570\u636e\u589e\u5f3a \u566a\u58f0\n                    batch_x_noise, batch_y_noise = noise(batch_x, batch_y, random_max=0.01)\n                    list_batch_x.append(batch_x_noise), list_batch_y.append(batch_y_noise)\n\n                    # \u5176\u4ed6\n\n                batch_x_yield, batch_y_yield = np.vstack(list_batch_x), np.vstack(list_batch_y)\n                yield batch_x_yield, batch_y_yield\n","3e4e8a7f":"def data_enhance(method, train_data, train_labels):\n    if method == 'noise':\n        noise = train_data + np.random.normal(0, 0.1, size=train_data.shape)\n        return noise, train_labels\n    \n    elif method == 'mixup':\n        index = [i for i in range(len(train_labels))]\n        np.random.shuffle(index)\n\n        x_mixup = np.zeros(train_data.shape)\n        y_mixup = np.zeros(train_labels.shape)\n\n        for i in range(len(train_labels)):\n            x1 = train_data[i]\n            x2 = train_data[index[i]]\n            y1 = train_labels[i]\n            y2 = train_labels[index[i]]\n\n            factor = np.random.beta(0.2, 0.2)\n\n            x_mixup[i] = x1 * factor + x2 * (1 - factor)\n            y_mixup[i] = y1 * factor + y2 * (1 - factor)\n\n        return x_mixup, y_mixup\n\n    \ndef set_data_enhance(val):\n    if not isinstance(val, list):\n        val = [val]\n    global data_enhance_method\n    data_enhance_method = val","3b89026a":"seq_len = 61","c83d9ee1":"# # -------------------Aggregate signals for each fragment & unify the sequence length: 60--------------------(DEPRECATED)\n# # get numpy array with shape: [num_examples, seq_len 61, num_features:8]\n# def agg_func(x):\n#     list_x = list(x)\n#     if len(list_x) <= seq_len:\n#         # padding 0 in the begining\n#         list_x = [0] * (seq_len-len(list_x)) + list_x\n#     else:\n#         list_x = list_x[:seq_len]\n#     return list_x\n\n# def get_train_test2():\n#     train_df = pd.read_csv(f'{INPUT_DIR}\/sensor_train.csv')\n#     test_df = pd.read_csv(f'{INPUT_DIR}\/sensor_test.csv')\n#     map_agg_func = {    \n#         'time_point' : agg_func,\n\n#         'acc_all' : agg_func,\n#         'acc_allg' : agg_func,\n\n#         'acc_x' : agg_func,\n#         'acc_y' : agg_func,\n#         'acc_z' : agg_func,\n\n#         'acc_xg' : agg_func,\n#         'acc_yg' : agg_func,\n#         'acc_zg' : agg_func\n#     }\n#     train_df = train_df.groupby(['fragment_id']).agg(map_agg_func).reset_index()\n#     test_df = test_df.groupby(['fragment_id']).agg(map_agg_func).reset_index()\n\n\n\n#     # ----------------transform the dimension of each fragment to list of [60, 8]--------------------(DEPRECATED)\n#     list_features = []\n#     for index, row in tqdm(train_df.iterrows()):\n#         # [seq_len:60, feature_len:8]\n#         acc_all = np.array(row['acc_all'])\n#         acc_allg = np.array(row['acc_allg'])\n#         acc_x = np.array(row['acc_x'])\n#         acc_y = np.array(row['acc_y'])\n#         acc_z = np.array(row['acc_z'])\n#         acc_xg = np.array(row['acc_xg'])\n#         acc_yg = np.array(row['acc_yg'])\n#         acc_zg = np.array(row['acc_zg'])\n#         # [8, 60]\n#         features = np.stack([acc_all,acc_allg,acc_x,acc_y,acc_z,acc_xg,acc_yg,acc_zg])\n#         # [8, 60] -> [60, 8]\n#         features = features.T\n#         list_features.append(features)\n#     X_train = np.stack(list_features)\n\n#     list_features = []\n#     for index, row in tqdm(test_df.iterrows()):\n#         # [seq_len:60, feature_len:8]\n#         acc_all = np.array(row['acc_all'])\n#         acc_allg = np.array(row['acc_allg'])\n#         acc_x = np.array(row['acc_x'])\n#         acc_y = np.array(row['acc_y'])\n#         acc_z = np.array(row['acc_z'])\n#         acc_xg = np.array(row['acc_xg'])\n#         acc_yg = np.array(row['acc_yg'])\n#         acc_zg = np.array(row['acc_zg'])\n#         # [8, 60]\n#         features = np.stack([acc_all,acc_allg,acc_x,acc_y,acc_z,acc_xg,acc_yg,acc_zg])\n#         # [8, 60] -> [60, 8]\n#         features = features.T\n#         list_features.append(features)\n#     X_test = np.stack(list_features)\n\n#     X_train = X_train.reshape([X_train.shape[0], X_train.shape[1], X_train.shape[2], 1])\n#     X_test = X_test.reshape([X_test.shape[0], X_test.shape[1], X_test.shape[2], 1])\n    \n#     y_train = train_df.groupby('fragment_id', sort=True)['behavior_id'].first()\n\n#     return X_train, y_train, X_test\n","186357ce":"# def resample_train_test():\n#     train_df = pd.read_csv(f'{INPUT_DIR}\/sensor_train.csv')\n#     test_df = pd.read_csv(f'{INPUT_DIR}\/sensor_test.csv')\n\n\n#     y_train = train_df.groupby('fragment_id', sort=True)['behavior_id'].first()\n\n#     # --------------------add integrated accelerator--------------------------------------------------------\n#     train_df['acc_all'] = (train_df['acc_x'] ** 2 + train_df['acc_y'] ** 2 + train_df['acc_z'] ** 2) ** 0.5\n#     train_df['acc_allg'] = (train_df['acc_xg'] ** 2 + train_df['acc_yg'] ** 2 + train_df['acc_zg'] ** 2) ** 0.5\n#     test_df['acc_all'] = (test_df['acc_x'] ** 2 + test_df['acc_y'] ** 2 + test_df['acc_z'] ** 2) ** 0.5\n#     test_df['acc_allg'] = (test_df['acc_xg'] ** 2 + test_df['acc_yg'] ** 2 + test_df['acc_zg'] ** 2) ** 0.5\n\n\n#     # -------------------Resample signals with the sequence length: 60----------------------------------------\n#     # # resample the original sample signals so that they have the same interval\n#     X_train = np.zeros((7292, 60, 8, 1))\n#     X_test = np.zeros((7500, 60, 8, 1))\n#     for i in tqdm(range(7292)):\n#         tmp = train_df[train_df.fragment_id == i][:60]\n#         X_train[i,:,:, 0] = signal.resample(x=tmp.drop(['fragment_id', 'behavior_id', 'time_point'], axis=1), num=60, t=np.array(tmp.time_point), axis=0, window=None)[0]\n\n#     for i in tqdm(range(7500)):\n#         tmp = test_df[test_df.fragment_id == i][:60]\n#         X_test[i,:,:, 0] = signal.resample(tmp.drop(['fragment_id', 'time_point'],\n#                                         axis=1), 60, np.array(tmp.time_point))[0]\n#     return X_train, y_train, X_test","f5fa024a":"# from https:\/\/github.com\/blueloveTH\/xwbank2020_baseline_keras\ndef handle_features(data):\n    data.drop(columns=['time_point'], inplace=True)\n\n    data['acc'] = (data.acc_x ** 2 + data.acc_y ** 2 + data.acc_z ** 2) ** 0.5\n    data['acc_g'] = (data.acc_xg ** 2 + data.acc_yg ** 2 + data.acc_zg ** 2) ** 0.5\n\n    return data\n\n# \u6784\u9020numpy\u7279\u5f81\u77e9\u9635\ndef handle_mats(grouped_data):\n    mats = [i.values for i in grouped_data]\n    # padding\n    for i in range(len(mats)):\n        padding_times = 61 - mats[i].shape[0]\n        for j in range(padding_times):\n            mats[i] = np.append(mats[i], [[0 for _ in range(mats[i].shape[1])]], axis=0)\n\n    mats_padded = np.zeros([len(mats), 61, mats[0].shape[1]])\n    for i in range(len(mats)):\n        mats_padded[i] = mats[i]\n\n    return mats_padded\n\n\n\ndef get_test_data(use_scaler=True):\n    test_df = pd.read_csv(f'{INPUT_DIR}\/sensor_test.csv')\n    data = handle_features(test_df)\n    if use_scaler:\n        with open('scaler.pkl', 'rb') as f:\n            scaler = pickle.load(f)\n        data[src_names] = scaler.transform(data[src_names].values)\n\n    grouped_data = [i.drop(columns='fragment_id') for _, i in data.groupby('fragment_id')]\n    return handle_mats(grouped_data)\n\ndef get_train_data(use_scaler=True, shuffle=True, pseudo_labels_file=None):\n    df = pd.read_csv(f'{INPUT_DIR}\/sensor_train.csv')\n    # \u7b80\u5355\u62fc\u63a5\u4f2a\u6807\u7b7e\n    if pseudo_labels_file != None:\n        df = df.append(pd.read_csv(pseudo_labels_file))\n    data = handle_features(df)\n\n    # \u6807\u51c6\u5316\uff0c\u5e76\u5c06\u7edf\u8ba1\u503c\u4fdd\u5b58\n    if use_scaler:\n        scaler = StandardScaler()\n        scaler.fit(data[src_names].values)  \n        with open('scaler.pkl', 'wb') as f:\n            pickle.dump(scaler, f)\n        data[src_names] = scaler.transform(data[src_names].values)\n\n    grouped_data = [i.drop(columns='fragment_id') for _, i in data.groupby('fragment_id')]\n    train_labels = np.array([int(i.iloc[0]['behavior_id']) for i in grouped_data])\n    for i in range(len(grouped_data)):\n        grouped_data[i].drop(columns='behavior_id', inplace=True)\n    train_data = handle_mats(grouped_data)\n    \n    if shuffle:\n        index = [i for i in range(len(train_labels))]\n        np.random.seed(2020)\n        np.random.shuffle(index)\n\n        train_data = train_data[index]\n        train_labels = train_labels[index]\n\n    return train_data, train_labels\n\ndef get_train_test_data(use_scaler=False, shuffle=True, pseudo_labels_file=None):\n    train_data, train_lables = get_train_data(use_scaler, shuffle, pseudo_labels_file=pseudo_labels_file)\n    test_data = get_test_data(use_scaler)\n    return train_data, train_lables, test_data\n\ndef shuffle(data, labels, seed=None):\n    index = [i for i in range(len(labels))]\n    if seed != None:\n        np.random.seed(seed)\n    np.random.shuffle(index)\n    return data[index], labels[index]","f4a29879":"# train_data shape: (7292, 60, 8, 1); train_lables shape: (7292,); test_data shape: (7500, 60, 8, 1)\n# X_train, y_train, X_test = resample_train_test()\n# X_train = np.load('..\/input\/xinwang-userbehaviour-sensor-data\/X_train.npy')\n# y_train = np.load('..\/input\/xinwang-userbehaviour-sensor-data\/y_train.npy')\n# X_test = np.load('..\/input\/xinwang-userbehaviour-sensor-data\/X_test.npy')\n# -----------------------Check resample signal------------------------------------------------------(TEST USE)\n# resample_signals = signal.resample(x=tmp.drop(['fragment_id', 'behavior_id', 'time_point'], axis=1), num=60, t=np.array(tmp.time_point), axis=0, window=None)\n# plt.plot(tmp.time_point,tmp['acc_x'], 'red')\n# plt.plot(resample_signals[1],resample_signals[0][:,0], 'blue')\n# plt.show()\n\n\nX_train, y_train, X_test = get_train_test_data()\n# X_train = np.load('..\/input\/xinwang-userbehaviour-sensor-data\/X_train1.npy')\n# y_train = np.load('..\/input\/xinwang-userbehaviour-sensor-data\/y_train1.npy')\n# X_test = np.load('..\/input\/xinwang-userbehaviour-sensor-data\/X_test1.npy')\n\nprint(f'train_data shape: {X_train.shape}; train_lables shape: {y_train.shape}; test_data shape: {X_test.shape}' )\n# np.save('X_train.npy', X_train)\n# np.save('y_train.npy', y_train)\n# np.save('X_test.npy', X_test)","19f5cf83":"seed = 2020\nfolds = 10\nver = 'v7'\n\nmapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n    4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n    8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n    12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n    16: 'C_2', 17: 'C_5', 18: 'C_6'}\n\n# reversed_mapping = {value: key for key, value in mapping.items()}\n\nkfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\ndata_enhance_method = []\n# \u8bbe\u7f6e\u6570\u636e\u589e\u5f3a\u65b9\u5f0f (noise, mixup or both)\ndata_enhance_method = []\nset_data_enhance(['noise'])","59a10c6d":"\n\n# # Custom Loss Function\n# def custom_sparse_categorical_ce(target, output):\n#     '''\n#     https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.2.0\/tensorflow\/python\/keras\/backend.py#L4667-L4700\n#     def binary_crossentropy(target, output, from_logits=False):\n#     y_true: [b, 1]\n#     y_pred: [b,1]\n#     -y_true * log(y_pred) - (1-y_true) * log(1-y_pred)\n#     >>>y_true = [1, 2]\n#     >>>y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n#     '''\n#     target = tf.dtypes.cast(target, tf.int32)\n\n#     target = tf.one_hot(target, 19) # shape=(b, 19)\n#     # http:\/\/www.adeveloperdiary.com\/data-science\/deep-learning\/neural-network-with-softmax-in-python\/\n#     # for one sample\n#     cost = -tf.math.reduce_mean(target * tf.math.log(output + 1e-18), axis=-1) #shape: (b, 19)\n    \n#     # refer to: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/2b96f3662bd776e277f86997659e61046b56c315\/tensorflow\/python\/keras\/backend.py#L4519\n# #     epsilon = 0.01\n# #     output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n# #     return -math_ops.reduce_sum(target * math_ops.log(output), axis)\n\n#     return cost\n\n\n\n# Custom Metric\ndef get_acc_combo():\n    def combo(y, y_pred):\n        # \u6570\u503cID\u4e0e\u884c\u4e3a\u7f16\u7801\u7684\u5bf9\u5e94\u5173\u7cfb\n        mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3',\n            4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5',\n            8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6',\n            12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6',\n            16: 'C_2', 17: 'C_5', 18: 'C_6'}\n        # \u5c06\u884c\u4e3aID\u8f6c\u4e3a\u7f16\u7801\n        \n        code_y, code_y_pred = mapping[int(y)], mapping[int(y_pred)]\n        if code_y == code_y_pred: #\u7f16\u7801\u5b8c\u5168\u76f8\u540c\u5f97\u52061.0\n            return 1.0\n        elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #\u7f16\u7801\u4ec5\u5b57\u6bcd\u90e8\u5206\u76f8\u540c\u5f97\u52061.0\/7\n            return 1.0\/7\n        elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #\u7f16\u7801\u4ec5\u6570\u5b57\u90e8\u5206\u76f8\u540c\u5f97\u52061.0\/3\n            return 1.0\/3\n        else:\n            return 0.0\n        \n    confusionMatrix=np.zeros((19,19))\n    for i in range(19):\n        for j in range(19):\n            confusionMatrix[i,j]=combo(i,j)\n    confusionMatrix=tf.convert_to_tensor(confusionMatrix)\n\n    def acc_combo(y, y_pred):\n        y=tf.argmax(y,axis=1)\n        y = tf.reshape(tf.cast(y,tf.int32),[-1])\n        y_pred = tf.cast(tf.argmax(y_pred, axis=1), tf.int32)\n        indices=tf.stack([y,y_pred],axis=1)\n        scores=tf.gather_nd(confusionMatrix,tf.cast(indices,tf.int32))\n        return tf.reduce_mean(scores)\n    return acc_combo\n\n\n\n# def Conv2DNet():\n#     base_channel=64\n#     return Sequential(\n#         [\n#             #1\n#             Conv2D(filters=base_channel, kernel_size=(3,3), padding='same'),\n#             #BatchNormalization(axis=-1 ),#, center=True , scale=True\n#             ReLU(),\n\n#             #2\n#             Conv2D(filters=base_channel*2, kernel_size=(3,3), padding='same'),\n#             #BatchNormalization(axis=-1),\n#             ReLU(),\n#             MaxPooling2D(pool_size=(2, 2)),\n\n\n#             #3\n#             Conv2D(filters=base_channel*4, kernel_size=(3,3), padding='same'),\n#             #BatchNormalization(axis=-1),\n#             ReLU(),\n#             Conv2D(filters=base_channel*8, kernel_size=(3,3), padding='same'),\n#             #BatchNormalization(axis=-1),\n#             ReLU(),\n\n#             # last\n#             GlobalMaxPooling2D(),\n#             Dropout(0.3),\n#             Dense(19, activation='softmax')\n#         ]\n\n#     )\n\ndef Conv2DNet(input_shape=(None, 60, 8, 1), num_classes=19):\n    base_channel=64\n    model = Sequential()\n    model.add(Conv2D(filters=base_channel, kernel_size=(3,3), padding='same', activation='relu', input_shape=input_shape[1:]))\n    model.add( Conv2D(filters=base_channel*2, kernel_size=(3,3), padding='same', activation='relu'))\n    model.add( MaxPooling2D(pool_size=(2, 2)))\n    model.add( Conv2D(filters=base_channel*4, kernel_size=(3,3), padding='same', activation='relu'))\n    model.add( Conv2D(filters=base_channel*8, kernel_size=(3,3), padding='same', activation='relu'))\n    model.add( GlobalMaxPooling2D())\n    model.add(Dropout(0.3))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.build(input_shape)\n\n\n    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), #custom_sparse_categorical_ce\n                  optimizer=tf.optimizers.Adam(),\n                  metrics=['acc',get_acc_combo()])\n    \n    \n    return model","5142eaec":"def BLOCK(seq, filters, kernal_size):\n    cnn = keras.layers.Conv1D(filters, 1, padding='SAME', activation='relu')(seq)\n    cnn = keras.layers.LayerNormalization()(cnn)\n\n    cnn = keras.layers.Conv1D(filters, kernal_size, padding='SAME', activation='relu')(cnn)\n    cnn = keras.layers.LayerNormalization()(cnn)\n\n    cnn = keras.layers.Conv1D(filters, 1, padding='SAME', activation='relu')(cnn)\n    cnn = keras.layers.LayerNormalization()(cnn)\n\n    seq = keras.layers.Conv1D(filters, 1)(seq)\n    seq = keras.layers.Add()([seq, cnn])\n    return seq\n\ndef BLOCK2(seq, filters=128, kernal_size=5):\n    seq = BLOCK(seq, filters, kernal_size)\n    seq = keras.layers.MaxPooling1D(2)(seq)\n    seq = keras.layers.SpatialDropout1D(0.3)(seq)\n    seq = BLOCK(seq, filters\/\/2, kernal_size)\n    seq = keras.layers.GlobalAveragePooling1D()(seq)\n    return seq\n\ndef ComplexConv1D(input_shape, num_classes):\n    inputs = keras.layers.Input(shape=input_shape[1:])\n    seq_3 = BLOCK2(inputs, kernal_size=3)\n    seq_5 = BLOCK2(inputs, kernal_size=5)\n    seq_7 = BLOCK2(inputs, kernal_size=7)\n    seq = keras.layers.concatenate([seq_3, seq_5, seq_7])\n    seq = keras.layers.Dense(512, activation='relu')(seq)\n    seq = keras.layers.Dropout(0.3)(seq)\n    seq = keras.layers.Dense(128, activation='relu')(seq)\n    seq = keras.layers.Dropout(0.3)(seq)\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(seq)\n\n    model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n\n    model.compile(optimizer=tf.optimizers.Adam(1e-3),\n            loss=tf.losses.CategoricalCrossentropy(label_smoothing=0.1),           \n            metrics=['accuracy',get_acc_combo()])\n\n    return model\n\n\n\n                                               \n# # \u5982\u679c\u60f3\u8c61\u62103D \u7a7a\u95f4\u4e2d\u7684\u4e00\u6761\u7ebf\u7684\u8bdd\uff1f\uff1f\uff1f\uff1f Conv3D()\n# def Net():\n#     input = Input(shape=(60, 8))\n#     X = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input)\n#     X = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(X)\n#     X = MaxPooling1D()(X)\n#     X = Conv1D(filters=256, kernel_size=1, activation='relu', padding='same')(X)\n#     X = Conv1D(filters=512, kernel_size=1, activation='relu', padding='same')(X)\n#     X = GlobalMaxPooling1D(),\n#     X = Dropout(0.3),\n#     X = Dense(19, activation='softmax'),\n#     return Model(inputs=[input], outputs=X)\n","c5d13a1f":"def kfcv_fit_predict(builder, X_train, y_train, X_test, checkpoint_path, epochs=100,batch_size=512, \n                      verbose=0, evaluate=False, train=True, from_weights=None):\n    \n    histories = []\n    evals = []\n    \n    \n    test_pred = np.zeros((7500, 19))\n    train_pred = np.zeros((X_train.shape[0], 19))\n\n    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, np.argmax(y_train, axis=-1))):\n        print('Processing fold: %d (%d, %d)' % (fold, len(train_idx), len(val_idx)))\n        model = builder()\n        \n        XX_train = X_train[train_idx]\n        yy_train = y_train[train_idx]\n        \n        XX_val = X_train[val_idx]\n        yy_val = y_train[val_idx]\n        \n        if len(data_enhance_method) > 0:\n                X_train_copy = np.copy(XX_train)\n                y_train_copy = np.copy(yy_train)\n                for method in data_enhance_method:\n                    x_, y_ = data_enhance(method, X_train_copy, y_train_copy)\n                    \n                    XX_train = np.r_[XX_train, x_]\n                    yy_train = np.r_[yy_train, y_]\n                    del x_, y_\n                    \n                del X_train_copy, y_train_copy\n                gc.collect()\n                XX_train, yy_train = shuffle(XX_train, yy_train)\n\n#         if len(data_enhance_method) > 0:\n#                 X_train_copy = np.copy(X_train)\n#                 y_train_copy = np.copy(y_train)\n#                 for method in data_enhance_method:\n#                     x_, y_ = data_enhance(method, X_train_copy, y_train_copy)\n                    \n#                     XX_train = np.r_[X_train, x_]\n#                     yy_train = np.r_[y_train, y_]\n#                     del x_, y_\n                    \n#                 del X_train_copy, y_train_copy\n#                 gc.collect()\n#                 X_train, y_train = shuffle(X_train, y_train)\n            \n#                 print('Data enhanced (%s) => %d' % (' '.join(data_enhance_method), len(X_train)))\n\n        if train == True:\n\n            plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n                                        verbose=verbose,\n                                        mode='max',\n                                        factor=0.1,\n                                        patience=6)\n            early_stopping = EarlyStopping(monitor='val_acc',\n                                           verbose=verbose,\n                                           mode='max',\n                                           patience=20)\n            checkpoint = ModelCheckpoint(checkpoint_path+f'fold{fold}.h5',\n                                         monitor='val_acc',\n                                         verbose=verbose,\n                                         mode='max',\n                                         save_best_only=True)\n\n            h = model.fit(XX_train, yy_train,\n                      epochs=epochs,\n                      batch_size=batch_size,\n                      verbose=1,\n                      shuffle=True,\n                      validation_data=(XX_val, yy_val),\n                      callbacks=[plateau, early_stopping, checkpoint])\n            \n#             h = model.fit(X_train[train_idx], y_train[train_idx],\n#                       epochs=epochs,\n#                       batch_size=batch_size,\n#                       verbose=1,\n#                       shuffle=True,\n#                       validation_data=(X_train[val_idx], y_train[val_idx]),\n#                       callbacks=[plateau, early_stopping, checkpoint])\n            #histories.append(h)\n        \n        if evaluate == True:\n            print('Add evaluate')\n            evals.append(model.evaluate(x=x[val], y=y[val]))\n\n        #model.load_weights(checkpoint_path+f'fold{fold}.h5')\n        \n        if from_weights != None:\n            if train == False:\n                print('Unconsistent arguments: from_weights and train arguments')\n                model.load_weights(f'{INPUT_DIR}v4_fold{fold}.h5')\n            \n            \n        train_pred[val_idx] = model.predict(XX_train[val_idx])\n        test_pred += model.predict(X_test, verbose=0, batch_size=1024) \/ folds\n        \n#         del model, h, XX_train, XX_val, yy_train, yy_val\n#         gc.collect()\n        \n    return train_pred, test_pred, histories, evals","7feb088f":"X_train = tf.cast(X_train, tf.float32).numpy()\ny_train = tf.one_hot(y_train, 19).numpy()","6bf27290":"X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], -1))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], -1))","6195122e":"train_pred, test_pred, histories, _ = kfcv_fit_predict(lambda : Conv2DNet(), X_train,  y_train, X_test, checkpoint_path = '')#models_{ver}\/","c544e234":"# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], -1))\n# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], -1))\n# train_pred, test_pred, histories, _ = kfcv_fit_predict(lambda : Conv2DNet(), X_train,  y_train, X_test, checkpoint_path = '')#models_{ver}\/\n\ntrain_pred, test_pred, histories, _ = kfcv_fit_predict(lambda : ComplexConv1D(X_train.shape, 19), X_train,  y_train, X_test, checkpoint_path = '')#models_{ver}\/","adaf752e":"acc_scores = round(accuracy_score( np.argmax(y_train, axis=-1), np.argmax(train_pred, axis=1)), 5)\nacc_combo_scores = acc_combo_all( np.argmax(y_train,axis=-1), np.argmax(train_pred, axis=1))\n\n\nprint('--------')\nprint(' acc : ', acc_scores, 'acc_combo : ', list(acc_combo_scores.items())[:3])","aece4b52":"save_pred(train_pred, test_pred, ver=ver)","a121b757":"# y_train_model1 = y_train.copy()\n# y_train_model2 = y_train.copy()\n# # Train\n# y_mapping[y_train_model1[0]].split('_')[0]","008a556b":"test_pred = np.zeros((7500, 19))\ntrain_pred = np.zeros((X_train.shape[0], 19))\n\n# for Conv2D: [b, 60, 8] =>  [b, 60, 8, 1]\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train)):\n    \n    \n    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n                                verbose=0,\n                                mode='max',\n                                factor=0.1,\n                                patience=6)\n    early_stopping = EarlyStopping(monitor='val_acc',\n                                   verbose=0,\n                                   mode='max',\n                                   patience=20)\n    checkpoint = ModelCheckpoint(,\n                                 monitor='val_acc',\n                                 verbose=0,\n                                 mode='max',\n                                 save_best_only=True)\n\n    model.fit(X_train[train_idx], y_train[train_idx],\n              epochs=100,\n              batch_size=512,\n              verbose=1,\n              shuffle=True,\n              validation_data=(X_train[val_idx], y_train[val_idx]),\n              callbacks=[plateau, early_stopping, checkpoint])\n    \n\n    model.load_weights(f'{ver}_fold{fold}.h5')\n#     model.build((None, 60, 8, 1))\n#     model.load_weights(f'{INPUT_DIR}v4_fold{fold}.h5')\n    val_pred = model.predict(X_train[val_idx])\n    train_pred[val_idx] = val_pred\n    test_pred += model.predict(X_test, verbose=0, batch_size=1024) \/ 5.\n   ","05b67431":"# load dataset\ntrain_pred = pd.read_csv(f'{INPUT_DIR}train_pred_v4.csv')\ntest_pred = pd.read_csv(f'{INPUT_DIR}test_pred_v4.1.csv')\ny_train = np.load('..\/input\/xinwang-userbehaviour-sensor-data\/y_train.npy')\ntrain_pred = train_pred.values\ntest_pred = test_pred.values","f71b819f":"# concate training data including predicted probabilities for each class, and whether the final predictions are correct\ndf_train_pred = pd.concat([pd.DataFrame(train_pred,columns=range(19)), \n                       pd.DataFrame(y_train, columns=['y_train']), \n                       pd.DataFrame(np.argmax(train_pred, axis=1), columns=[ 'pred_class']),\n                       pd.DataFrame(np.max(train_pred, axis=1), columns=[ 'pred_class_prob']),\n                        pd.DataFrame(np.argsort(train_pred,axis=1)[:,-2], columns=[ 'pred_class_2']),\n                        pd.DataFrame(np.sort(train_pred, axis=1)[:,-2], columns=[ 'pred_class_prob_2']),\n                        pd.DataFrame(np.argsort(train_pred,axis=1)[:,-3], columns=[ 'pred_class_3']),\n                        pd.DataFrame(np.sort(train_pred, axis=1)[:,-3], columns=[ 'pred_class_prob_3'])], axis=1)\n\ndf_train_pred['Is_true'] = df_train_pred['y_train'] == df_train_pred['pred_class']\ndf_train_pred['Is_true_2'] = df_train_pred['y_train'] == df_train_pred['pred_class_2']\ndf_train_pred['Is_true_3'] = df_train_pred['y_train'] == df_train_pred['pred_class_3']\n\n# check distribution of true positive\/negative samples and false predicted samples\ntrue_probs = []\nfalse_probs = []\nfor i, (is_true, pred) in enumerate(zip(df_train_pred['Is_true'],df_train_pred['pred_class'] )):\n    if is_true == True:\n        true_probs.append(train_pred[i, pred])\n        \n    if is_true == False:\n        false_probs.append(train_pred[i, pred])\n   \nplt.hist(true_probs)\nplt.show()\n# np.quantile(true_probs, [0.15, 0.2,0.41, 0.5, 0.7, 0.9])\n# np.quantile(false_probs, [0.15, 0.2,0.5, 0.7, 0.9, 0.95, 0.99])\nplt.hist(false_probs)\nplt.show()","79662fdd":"# Select test samples only when its predicted values is larger than a fixed threshold\n# This is for set fixed sample weights for all selected test examples: get the index of test samples which have predicted probability larger than threshold(0.788)\n# threshold = 0.788\n# idx_lst = []\n# for idx, val in enumerate(np.max(test_pred, axis=1) > threshold):\n#     if val == True:\n#         idx_lst.append(len(y_train) + idx)\n\n# get X,y of selected samples\n# X_test_select = X_test[idx_lst]\n# y_test_select = np.argmax(test_pred, axis=1)[idx_lst]\n# print('Selected X_test_select shape: ', X_test_select.shape, '\\n', 'y_test_select shape: ', y_test_select.shape)\n\n# X_train_test = np.concatenate([X_train, X_test_select],  axis=0)\n# y_train_test = np.concatenate([y_train, y_test_select],  axis=0)\n# sample_weight = np.ones(shape=(len(y_train_test),))\n# sample_weight[idx_lst] = 0.9","07080940":"# sample_weight_train = np.ones(shape=(len(y_train),))\n# sample_weight = np.concatenate([sample_weight_train, sample_weight_test],  axis=0)\n# print('Sample Weight shape: ', sample_weight.shape,)\n\n# fit()\n#               sample_weight=sample_weight[train_idx],","d2be4be5":"def get_confidence(df, class_order = 1):\n    '''\n    input:  dataframe with two columns: 1. predicted probabilities for the predicted class; 2. Whether this class is the true label\n    return: confidence for the corresponding predicted probabilities\n    '''\n    confidence_lst = []\n    probs = [i\/100 for i in range(0, 100, 1)] \n    if class_order == 1:\n            class_order =  ''\n    else:\n        class_order = '_' + str(class_order)\n            \n    for prob in probs:\n        \n        belief_samples = df[df['pred_class_prob'+class_order] > prob]\n        confidence_lst.append(sum(belief_samples['Is_true'+class_order]==True)\/(len(belief_samples)+0.1))\n        \n    return probs, confidence_lst\n\n\ndef get_poly_deriv_func(x, y):\n    '''\n    return: differential function of fitted a polynomial function\n    '''\n  \n\n    probs = x\n    confidence_lst = y\n    print('Curve Fitting for x: Predicted Probability; y:Confidence(Proportion of True Predicted samples when predicted probability is larger than x) ....')\n    z = np.polyfit(probs, confidence_lst, 3)\n    p_func = np.poly1d(z)\n    xp = np.linspace(0, 1, 100)\n    _ = plt.plot(probs, confidence_lst, '.', xp, p_func(xp), '-')\n    plt.xlabel('Predicted Probability')\n    plt.title('Confidence(Proportion of True Predicted samples)')\n    plt.show()\n\n    print('Calculate derivatives of predicted probabilities....')\n    p_deriv = p_func.deriv()\n    #deriv = [0 if deriv < 0 else deriv for deriv in p_deriv(df['pred_class_prob'+class_order] ) ]\n    \n    return p_deriv\n\n\n\n# Used to assign weights or just use as smoothed labels according to predicted probability for semi-supervised learning\n    \n   \n    \n\nprint(df_train_pred.shape)\n\n#  calculating weights for the sample input calculated by derivatives times predicted probability\nprobs, confidence_lst = get_confidence(df_train_pred[['pred_class_prob', 'Is_true']])\np_deriv = get_poly_deriv_func(probs, confidence_lst)\n# sample_weight_train = pred_prob * deriv\n\n\n# call for plotting of second possible and third possible probabilities\nprobs, confidence_lst = get_confidence(df_train_pred[['pred_class_prob_2', 'Is_true_2']], class_order=2)\n_ = get_poly_deriv_func(probs, confidence_lst )\nprobs, confidence_lst = get_confidence(df_train_pred[['pred_class_prob_3', 'Is_true_3']], class_order=3)\n_ = get_poly_deriv_func(probs, confidence_lst)\n\n","1cd0d7e6":"# construct the labels with all three possible classes\n\nindices1 = np.expand_dims(np.argsort(test_pred,axis=1)[:,-1], axis=1)\nindices2 = np.expand_dims(np.argsort(test_pred,axis=1)[:,-2], axis=1)\nindices3 = np.expand_dims(np.argsort(test_pred,axis=1)[:,-3], axis=1)\n\npred_prop1 = np.sort(test_pred, axis=1)[:,-1]\n\nderiv1 = [0 if deriv < 0 else deriv for deriv in p_deriv(pred_prop1) ]\n\nfirst_label = pred_prop1 * deriv1\nfirst_label = np.array([1 if i>1 else i for i in first_label + np.mean(pred_prop1)])\nsecond_label = (1 - first_label) * (5\/8) # 5\/8 and 3\/8 are determined from graphs above\nthird_label =(1 - first_label) * (5\/8)\n\nfirst_label = np.expand_dims(first_label, axis=1)\nsecond_label = np.expand_dims(second_label, axis=1)\nthird_label = np.expand_dims(third_label, axis=1)\n\n\ny_test = np.zeros(test_pred.shape)\nnp.put_along_axis(y_test,indices1, first_label, axis=1)\nnp.put_along_axis(y_test,indices2, second_label, axis=1)\nnp.put_along_axis(y_test,indices3, third_label, axis=1)","53757c9c":"X_train_test = np.concatenate([X_train, X_test],  axis=0)\nprint('X_train_test shape: ', X_train_test.shape)\n\ny_train_test = np.concatenate([y_train, np.argsort(y_test,axis=1)[:,-1]],  axis=0)\nprint('y_train shape: ', y_train.shape, 'y_test shape: ', y_test.shape, 'y_train_test shape: ', y_train_test.shape)\n\n\ny_train_test_one_hot = np.concatenate([ tf.one_hot(y_train, 19).numpy(), y_test],  axis=0)\nprint(  'y_train_test_one_hot shape: ', y_train_test_one_hot.shape)","9f8c4370":"# Train with train_test samples, \nseed = 2020\nfolds = 10\nkfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n\n# save prediction for both training and test set\ntest_pred = np.zeros((7500, 19))\ntrain_pred = np.zeros((X_train_test.shape[0], 19))\n\n\n\n    \n    \n# check model architecture\n# print(X_train[0].shape)\n# model =  Conv2DNet2(X_train[0].shape)\n# model.summary()\n\n\n# for Conv2D: [b, 60, 8] =>  [b, 60, 8, 1]\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_test, y_train_test)):\n    \n    model = Conv2DNet(input_shape=X_train_test[0].shape)\n    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), #custom_sparse_categorical_ce\n                  optimizer=tf.optimizers.Adam(),\n                  metrics=['acc',get_acc_combo()])\n    \n    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n                                verbose=0,\n                                mode='max',\n                                factor=0.1,\n                                patience=6)\n    early_stopping = EarlyStopping(monitor='val_acc',\n                                   verbose=0,\n                                   mode='max',\n                                   patience=20)\n    checkpoint = ModelCheckpoint(f'train_believetest_v4_fold{fold}.h5',\n                                 monitor='val_acc',\n                                 verbose=0,\n                                 mode='max',\n                                 save_best_only=True)\n    print('X_train_test[train_idx] shape: ', X_train_test[train_idx].shape)\n    print('y_train_test_one_hot[train_idx] shape: ', y_train_test_one_hot[train_idx].shape)\n    print('X_train_test[val_idx] shape: ', X_train_test[val_idx].shape)\n    print('y_train_test_one_hot[val_idx] shape: ',  y_train_test_one_hot[val_idx].shape)\n    model.fit(X_train_test[train_idx], y_train_test_one_hot[train_idx],\n              epochs=100,\n              batch_size=512,\n              verbose=1,\n              shuffle=True,\n              validation_data=(X_train_test[val_idx], y_train_test_one_hot[val_idx]),\n              callbacks=[plateau, early_stopping, checkpoint])\n\n\n    model.load_weights(f'train_believetest_v4_fold{fold}.h5')\n    \n    \n    # save prediction for both training and test set\n    train_pred[val_idx] = model.predict(X_train_test[val_idx])\n    test_pred += model.predict(X_test, verbose=0, batch_size=1024) \/ 5.\n","455a88e6":"acc_scores = round(accuracy_score(y_train_test, np.argmax(train_pred, axis=1)), 5)\nacc_combo_scores = acc_combo_all(y_train_test, np.argmax(train_pred, axis=1))\n\n\nprint('--------')\nprint(' acc : ', acc_scores, 'acc_combo : ', list(acc_combo_scores.items())[:3])","611c53ed":"save_pred(train_pred, test_pred, ver=6)","f870416a":"# For those test samples below threshold, check possible labels\n# def get_possible_labels(pred):\n#     labels = []\n#     # input: [b, num_class] numpy array\n#     # return: dict with index(key) and value(softmax output)\n#     for i,idx in enumerate(np.argsort(pred, axis=1)[:,-3:]):\n#         d = {}\n#         for j in idx:\n#             d[j] = pred[i][j]\n#         labels.append(d)\n        \n#     return labels\n\n# get_possible_labels(train_pred)","28d1e14d":"\nmapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3',\n            4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5',\n            8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6',\n            12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6',\n            16: 'C_2', 17: 'C_5', 18: 'C_6'}\n\nfor behav in range(19):\n    num_dist = df_result_False[df_result_False['train_pred'] == behav]['y_train']\n    plt.hist(num_dist, density =True)\n    plt.title('prediction is '+ str(behav) + '  ' + mapping[behav])\n    plt.show()","028f7ab7":"np.argmax(train_pred, axis=1)","012e1173":"mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n\n\ny_train_model3 = y_train.copy()\ndelivery_idx = (y_train == 4)\ny_train_model3[delivery_idx] = 1\ny_train_model3[~delivery_idx] = 0\nprint(f'There are {sum(y_train_model3)} positive examples.')\n\n\ndef Conv2DNet():\n    base_channel=64\n    return Sequential(\n        [\n            #1\n            Conv2D(filters=base_channel, kernel_size=(3,3), padding='same'),\n            #BatchNormalization(axis=-1 ),#, center=True , scale=True\n            ReLU(),\n\n            #2\n            Conv2D(filters=base_channel*2, kernel_size=(3,3), padding='same'),\n            #BatchNormalization(axis=-1),\n            ReLU(),\n            MaxPooling2D(pool_size=(2, 2)),\n\n\n            #3\n            Conv2D(filters=base_channel*4, kernel_size=(3,3), padding='same'),\n            #BatchNormalization(axis=-1),\n            ReLU(),\n            Conv2D(filters=base_channel*8, kernel_size=(3,3), padding='same'),\n            #BatchNormalization(axis=-1),\n            ReLU(),\n\n            # last\n            GlobalMaxPooling2D(),\n            Dropout(0.3),\n            Dense(1, activation='sigmoid')\n        ]\n\n    )\n\n\nseed = 2020\nfolds = 5\nkfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n\n\n# when use categorical_crossentropy\n# y_train_ = to_categorical(y_train, num_classes=19)\n# y_train_\ntest_pred = np.zeros((7500, 19))\ntrain_pred = np.zeros((X_train.shape[0], 19))\n\n# for Conv2D: [b, 60, 8] =>  [b, 60, 8, 1]\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train)):\n#     yy = to_categorical(yy, num_classes=19)\n    model = Conv2DNet()\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                  optimizer=tf.optimizers.Adam(),\n                  metrics=[tf.keras.metrics.Recall()])\n    \n    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n                                verbose=0,\n                                mode='max',\n                                factor=0.1,\n                                patience=6)\n    early_stopping = EarlyStopping(monitor='val_acc',\n                                   verbose=0,\n                                   mode='max',\n                                   patience=10)\n    checkpoint = ModelCheckpoint(f'fold{fold}.h5',\n                                 monitor='val_acc',\n                                 verbose=0,\n                                 mode='max',\n                                 save_best_only=True)\n\n    model.fit(X_train[train_idx], y_train_model3[train_idx],\n              epochs=100,\n              batch_size=512,\n              verbose=1,\n              shuffle=True,\n              validation_data=(X_train[val_idx], y_train_model3[val_idx]),\n              callbacks=[plateau, early_stopping, checkpoint])\n    \n\n    #model.load_weights(f'fold{fold}.h5')\n    val_pred = model.predict(X_train[val_idx])\n    train_pred[val_idx] = val_pred\n    test_pred += model.predict(X_test, verbose=0, batch_size=1024) \/ 5.","fddc554e":"\n# -------------------------V1: TUNING kernel_size: 3->3->1->1 for Conv2d---------------------------------\n# loss: 0.4993 - acc: 0.8221 - val_loss: 0.9728 - val_acc: 0.6838\n# test acc :  0.6802 acc_combo :  0.72686\n# (3377\u4e5f\u5f88\u5dee, \u6682\u5b9a3333\uff09\n\n# -------------------------V2: Conv2d-------------------------------------------------------------------\n#  acc :  0.69199 acc_combo :  0.73634\n\n\n\n# -------------------------V3: TUNING for BN - Conv2d + BatchNorm(BatchNormalization(axis=-1))-------------------------\n#  acc :  0.69007 acc_combo :  0.73421\n# (\u611f\u89c9\u6ca1\u592a\u5927\u53d8\u5316\uff0c \u6682\u65f6\u5c31\u7528\u4e0a\u5148)\n\n\n# -------------------------V4.1: based on V3, TRY 'ReduceLROnPlateau'-------------------------------------------------\n#  acc :  0.70488 acc_combo :  0.74628\n# \u770b\u7740\u5b83\u572870\u9644\u8fd1\u7a33\u5b9a\u66ff\u8eab\uff0c\u771f\u7684\u633a\u9999\u7684\uff1b\u4e0d\u8fc7\u589e\u52a0\u4e0bepochs\u5230150\uff0c\u4e5f\u9700\u8981\u80fd\u66f4\u597d\u7684fine-tuning, \u6240\u4ee5\u5728\u6539\u53d8epochs\u540e\n# acc :  0.7105 acc_combo :  0.75028\n\n# -------------------------v4.2: TRY SGD, SGD with momentum 0.9\u540e-----------------------------------\n# (\u8fd8\u662fAdam\u66f4\u597d\u4e9b)\n\n\n\n# -------------------------V4.3:ADD Custom Metric------------------------------------------------\n#  acc :  0.70118 acc_combo :  0.74536\n\n\n# -------------------------V5: Add test samples which have predicted probability larger than threshold(0.788) and its predicted labels--------------------\n# acc :  0.79387 acc_combo :  [('total_score', 0.82375), ('digit_score', 0.88447), ('num_score', 0.84468)]\n# public score: 0.7028253968253968\n\n\n#-------------------------V5.1: Based on V5, add sample_weights (train sample = 1, test sample = 0.9)---------------------------------------\n# acc :  0.80527 acc_combo :  [('total_score', 0.83352), ('digit_score', 0.89297), ('num_score', 0.85243)]\n# public score: 0.7104\n\n\n\n#-------------------------V5.2: label smoothing for semi-supervised methods( by assigning sample weights according to predicted probability)---------------------------------------\n# public score: 0.711\n\n\n#-------------------------V4.2: fold: 5->20 ------------------------------------------------------------------------\n# acc :  0.71242 acc_combo :  [('total_score', 0.75347), ('digit_score', 0.84325), ('num_score', 0.77948)]\n\n\n#-------------------------V4.3: fold: 5->10----------------------------------------------------------------------------\n# Early Stop: 20\n# acc :  0.70831 acc_combo :  [('total_score', 0.7503), ('digit_score', 0.83873), ('num_score', 0.77839)]\n# public score:0.712\n# Early Stop: 200(wrong operation)\n# acc :  0.70612 acc_combo :  [('total_score', 0.7497), ('digit_score', 0.83681), ('num_score', 0.78086)]\n\n\n# ------------------------------------------Ver6: Directly define labels as probabilities: smoothed_y * log(y_pred)--------------------------------\n# Epoch 57\/100\n# 26\/26 [==============================] - 1s 55ms\/step - loss: 0.4159 - acc: 0.9098 - acc_combo: 0.9232 - val_loss: 0.8598 - val_acc: 0.8027 - val_acc_combo: 0.8330 - lr: 1.0000e-07\n# Epoch 56\/100\n# 26\/26 [==============================] - 1s 55ms\/step - loss: 0.7296 - acc: 0.8615 - acc_combo: 0.8815 - val_loss: 1.3778 - val_acc: 0.7993 - val_acc_combo: 0.8299 - lr: 1.0000e-07\n# Epoch 73\/100\n# 27\/27 [==============================] - 1s 54ms\/step - loss: 0.7712 - acc: 0.7555 - acc_combo: 0.7975 - val_loss: 0.7817 - val_acc: 0.7437 - val_acc_combo: 0.7821 - lr: 1.0000e-07\n# Epoch 62\/100\n# 27\/27 [==============================] - 1s 53ms\/step - loss: 0.7372 - acc: 0.7719 - acc_combo: 0.8109 - val_loss: 0.8034 - val_acc: 0.7471 - val_acc_combo: 0.7857 - lr: 1.0000e-08\n# Epoch 60\/100\n# 27\/27 [==============================] - 1s 53ms\/step - loss: 0.7712 - acc: 0.7559 - acc_combo: 0.7724 - val_loss: 0.8182 - val_acc: 0.7356 - val_acc_combo: 0.7756 - lr: 1.0000e-08                         \n# Epoch 79\/100\n# 27\/27 [==============================] - 1s 52ms\/step - loss: 0.5855 - acc: 0.8275 - acc_combo: 0.8580 - val_loss: 0.7626 - val_acc: 0.7715 - val_acc_combo: 0.8096 - lr: 1.0000e-07\n# Epoch 64\/100\n# 27\/27 [==============================] - 1s 53ms\/step - loss: 0.8244 - acc: 0.7366 - acc_combo: 0.7820 - val_loss: 0.8874 - val_acc: 0.7255 - val_acc_combo: 0.7641 - lr: 1.0000e-08\n# Epoch 100\/100\n# 27\/27 [==============================] - 1s 52ms\/step - loss: 0.6854 - acc: 0.7943 - acc_combo: 0.8294 - val_loss: 0.7420 - val_acc: 0.7694 - val_acc_combo: 0.8074 - lr: 1.0000e-08\n# Epoch 74\/100\n# 27\/27 [==============================] - 1s 53ms\/step - loss: 0.8075 - acc: 0.7406 - acc_combo: 0.7858 - val_loss: 0.9393 - val_acc: 0.7072 - val_acc_combo: 0.7531 - lr: 1.0000e-08                           \n# Epoch 80\/100\n# 27\/27 [==============================] - 1s 53ms\/step - loss: 0.5484 - acc: 0.8423 - acc_combo: 0.8700 - val_loss: 0.7861 - val_acc: 0.7890 - val_acc_combo: 0.8220 - lr: 1.0000e-07\n\n# acc :  0.76318 acc_combo :  [('total_score', 0.79737), ('digit_score', 0.86797), ('num_score', 0.82085)]\n# public score:0.7057\n\n\n\n    # adjust sample weights for test samples to 0.9(confidence)\n#-------------------------\u5168\u5206\u7c7b\u6982\u7387\u4f4e\u7684\u76f4\u63a5\u653e\u5f03\u5168\u5206\u7c7b\u505a\u5206\u573a\u666f\u9884\u6d4b------------------------------\n\n# To-Do\n# 1. \u5173\u4e8e\u8fc7\u62df\u5408\uff0c\u5377\u79ef\u7684\u6700\u540e\u4e00\u5c42\u5168\u8fde\u63a5\u4e0e\u5168\u5c40\u6c60\u5316\u4e4b\u5c42\u7684\uff0cdropout\u53ef\u4ee5\u52a0\u5927\u4e00\u70b9\u3002\u57fa\u7ebf\u662f0.3\u7684\uff0c\u6211\u968f\u624b\u8bbe\u4e86\u4e00\u4e2a0.5\uff0c\u63d0\u9ad81-2\u4e2a\u70b9\u3002\n# 2. \u5c0f\u7a97\u53e3mask,\u6211\u60f3\u5e94\u8be5\u662f\u7c7b\u4f3c\u56fe\u50cf\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f,\u5c31\u662f\u628a\u56fe\u7247\u7684\u67d0\u4e2a\u6216\u8005\u67d0\u51e0\u4e2a\u533a\u57df\u7528\u9ed1\u8272\u77e9\u5f62\u906e\u853d; \n# 3.\n# \u5e8f\u5217\u957f\u5ea6\u662f60\u7279\u5f81\u4e3a8\u7684\u4e00\u4e2a\u6837\u672c\n# \u770b\u6210\u662f60*8\u7684\u56fe\u50cf\n# \u7136\u540e\u5728\u4e0a\u9762\u968f\u673a\u628an*m(n<60, m<8)\u7684\u7247\u6bb5\u7f6e\u4e3a0, n,m: 3-4","37f926e7":"### Try to improve accuracy for the behavior: D_4 ","478b4c0d":"# Summary","12dd349c":"### Train 1 single model for detecting the combined behavior info.","92f05ad9":"\n    \n### 2.1 Use sample weights for test data(Training examples are 1) : label_y * log(y_pred) * sample_weights\n","2fec637a":"# Utility","9ab5c6cb":"### Train 3 models for human setting(A, B, C), activity(0,1,2,3,5,6) and phone delivery(D_4) respectively","8fa968fc":"\n### 2.2 Directly define labels as probabilities: smoothed_y * log(y_pred)\nTake the top 3 class probabilities as labels for each example, however we add the model bias information by multiplying the condidence of correct prediction i.e. First_label = prop * derivative.\n\nAnd also the classes with second and third largest predicted probabilities are probably correct labels. However, considering the final probabilities for the most possible class, instead of directly using probabilities of predicted ones, I construct them based on First_label","4e177cef":"# Data Augmentation\nfrom https:\/\/github.com\/blueloveTH\/xwbank2020_baseline_keras","12734f60":"# Modelling Functions","76cf9725":"### ADD test data for training, predicted probabilities of test data would be used as smoothed labels for training.\n1. When use softmax cross entropy as loss function, there are two ways to smoothe labels:\n\n    2.1 Directly define labels as probabilities: smoothed_y * log(y_pred)\n    \n    2.2 Use sample weights: label_y * log(y_pred) * sample_weights\n    \n    \n    \n2. However, if I wanna use multi-label smooth, then I have to use the first method.","eb018d64":"To-do: add v\uff1d\u2211ai*\u0394t","a9b69e51":"# Data Processing"}}