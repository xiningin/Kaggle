{"cell_type":{"84479c86":"code","346b9898":"code","dbbf1a25":"code","a8c6fd04":"code","f50365ba":"code","ec693f56":"code","7c608021":"code","714524d7":"code","0fb2af70":"code","901295ee":"code","347b49b1":"code","257862e3":"code","a560c07e":"code","e4d73f04":"code","7d720fa0":"code","a7e3c7da":"code","b92a8f53":"code","d8969659":"code","c8faec3a":"code","205cf5a8":"code","c942b2a7":"code","ee143b7f":"code","ef1ca73d":"code","2586cd45":"code","a6b3dd19":"code","d35fbaf6":"code","6c8d4d4c":"code","2459aae7":"code","2f31b52e":"code","4a812b6a":"code","03211368":"code","6e9be62f":"code","0a4622b4":"code","11cb39b3":"code","526f6365":"code","aefd295d":"code","02c8cf3a":"code","64956ca8":"code","f11d2533":"code","439ec03c":"code","0a3e9cdd":"code","eeab83e2":"code","bc996921":"code","1589ce41":"code","055544d2":"code","e65adbb0":"code","878f1ec4":"code","a1f50d36":"code","9d7e55ba":"code","1087df04":"code","94b5beee":"code","7dd592c1":"code","202c5de0":"code","816ae74a":"code","a74ac4d7":"code","339796dd":"code","9fe5f844":"code","59ddec83":"code","483f0ce4":"code","a1c67bd5":"code","049c28c7":"code","1771f74c":"code","2aefa851":"code","b5969cc6":"code","63ae5c44":"code","2b5c8d3b":"code","2d6b4dc4":"code","6648bc66":"code","980d432b":"code","21c8590e":"code","a8a8ef16":"code","6f998b70":"code","57b0d191":"code","50d4c14a":"code","e3497fb2":"code","742457be":"code","6c6c6c7e":"code","8f4cb902":"code","3276fa80":"code","2b23b517":"code","686100ae":"code","5abd3202":"code","58aab9e8":"code","ecbd81c5":"code","5842d3fd":"code","cd252ef0":"code","5c043ac9":"code","662a78cd":"code","8d84993f":"code","f2305666":"code","c3e07552":"code","692f3a52":"code","9c6686fa":"code","ebdf7782":"code","6a48d33a":"code","d4b44b99":"code","d3945a7a":"code","9f7808ab":"code","ff296f45":"code","6a183ebe":"code","fa3e8df8":"code","f63d967e":"code","f853cc3f":"code","c5c0ee1e":"code","6df1d5af":"code","672a7b2a":"code","2cacd795":"code","47d4e8d2":"code","00877e9a":"code","490ba622":"code","a5c626d2":"code","93cb4f99":"code","61446a01":"code","b64c07dd":"code","9328b729":"code","6a6f9905":"code","da169548":"code","ec39dacf":"code","b14cd01e":"code","7d84a602":"code","ed034f36":"code","f46652ce":"code","10270f98":"code","682fbb83":"markdown","60a75420":"markdown","418b7ea9":"markdown","34f587dc":"markdown","9b7ffef3":"markdown","c2611bca":"markdown","11501543":"markdown","524c83b1":"markdown","5f1a7dd6":"markdown","43f4a494":"markdown","cd7ec271":"markdown","5565aab2":"markdown","2027934d":"markdown","f2a73414":"markdown","936ad7ac":"markdown","2ec5a738":"markdown","4b437d2e":"markdown","40b5e334":"markdown","b8304a18":"markdown","274863a9":"markdown","e2e31359":"markdown","57c7855c":"markdown","23e3c69d":"markdown","7d9b1693":"markdown","050647aa":"markdown","b37088a1":"markdown","f1d65808":"markdown","86d33869":"markdown","7d6e97bb":"markdown","3d4bf6ca":"markdown","755539bc":"markdown","53dd0a09":"markdown","24675a55":"markdown","fd575615":"markdown","70e409d3":"markdown","07cc0943":"markdown","49517217":"markdown","adf93899":"markdown","efa7ecea":"markdown","76a1a87b":"markdown","462f80e1":"markdown","e4f5b5e9":"markdown","6c130b86":"markdown","9913c34b":"markdown","3823f9e6":"markdown","7b2e165d":"markdown","a90f95c1":"markdown","ca99597f":"markdown","96bea4ae":"markdown","34c25552":"markdown","3d9f009e":"markdown","0e3f6d12":"markdown","e5a9e955":"markdown","72439ce1":"markdown","a3a8c295":"markdown","c4d48247":"markdown","ad8584f8":"markdown","96b28ccc":"markdown","d6d476a1":"markdown","6e194eb9":"markdown","2e15e576":"markdown","d5448d26":"markdown","198b4bdd":"markdown","fc25e9f6":"markdown","3fd5caf4":"markdown","0c693c26":"markdown","33d368cd":"markdown","2ed250d8":"markdown","086e4ae2":"markdown","03be838a":"markdown","25954059":"markdown","a82c9002":"markdown","336bbda7":"markdown","dde09e6e":"markdown","d83195ee":"markdown","267e7030":"markdown","2ced46fc":"markdown","3e196cbc":"markdown","c5743347":"markdown","697915ca":"markdown","29a21d50":"markdown","6328dec4":"markdown","4625036c":"markdown","fe94841d":"markdown","2353c9a2":"markdown","1485f9aa":"markdown","2b39f9e5":"markdown","cb412303":"markdown","cbd52f8b":"markdown","c7593637":"markdown","ac117566":"markdown","673920ef":"markdown","91df6204":"markdown","6b7b3bc4":"markdown","76afec45":"markdown","ece3f5e5":"markdown","ba69835c":"markdown","b60a04c3":"markdown","07c32a0e":"markdown","2ff29da4":"markdown","7f8ca8a5":"markdown","3701f935":"markdown","df558345":"markdown","01ac35bf":"markdown","84235c38":"markdown","978a30d2":"markdown","7be9413a":"markdown","fe358bcd":"markdown","944204ca":"markdown","8f12f86c":"markdown","5e41eda8":"markdown","1a7864fb":"markdown","ad2585d3":"markdown","7468a4eb":"markdown","ae7a044a":"markdown","8ac1f994":"markdown","1270344f":"markdown","f1e01443":"markdown"},"source":{"84479c86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","346b9898":"!pip install pycaret-nightly\n!pip install missingno\n!pip install captum","dbbf1a25":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport scipy.stats as stats\n\nfrom matplotlib import rcParams\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\nfrom captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation","a8c6fd04":"PATH = '..\/input\/house-prices-advanced-regression-techniques'\ntrain = pd.read_csv(PATH+'\/train.csv')\nX_test = pd.read_csv(PATH+'\/test.csv')\nsubmission_data = pd.read_csv(PATH+'\/sample_submission.csv')","f50365ba":"house_df = pd.concat([train,X_test],ignore_index = True, sort = False)\ntr_idx = house_df['SalePrice'].notnull()","ec693f56":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","7c608021":"def outlier_iqr(data):\n    q1,q3 = np.percentile(data,[25,75])\n    iqr = q3-q1\n    lower = q1-(iqr*2)\n    upper = q3+(iqr*2)\n    return np.where((data>upper)|(data<lower))","714524d7":"house_df.head(3).style.set_properties(**{'background-color': 'Grey',\n                           'color': 'white',\n                           'border-color': 'darkblack'})","0fb2af70":"house_df.drop('Id',axis=1,inplace=True,errors='ignore')","901295ee":"plt.figure(figsize = (8,6))\nax = house_df.dtypes.value_counts().plot(kind='bar',grid = False,fontsize=20,color='grey')\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+ p.get_width() \/ 2., height + 1, height, ha = 'center', size = 25)\nsns.despine()","347b49b1":"categorical_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns]\n\n# Select numerical columns\nint_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if \n                house_df[cname].dtype in ['int64']]\nfloat_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if \n                house_df[cname].dtype in ['float64']]\n\nnumerical_cols = int_cols + float_cols","257862e3":"import missingno as msno\nmsno.matrix(house_df[tr_idx])","a560c07e":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\nhouse_df.loc[:,numerical_cols] = imputer.fit_transform(house_df.loc[:,numerical_cols])","e4d73f04":"isnull_series = house_df.loc[:,:'SaleCondition'].isnull().sum()\nisnull_series[isnull_series > 0].sort_values(ascending=False)\n\nplt.figure(figsize = (20,10))\nax = isnull_series[isnull_series > 0].sort_values(ascending=False).plot(kind='bar',\n                                                                        grid = False,\n                                                                        fontsize=20,\n                                                                        color='grey')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+ p.get_width() \/ 2., height + 5, height, ha = 'center', size = 20)\nsns.despine()","7d720fa0":"plt.figure(figsize = (20,5))\nax1 = plt.subplot(1,3,1)\nsns.histplot(house_df['SalePrice'],palette='Blues_r',kde=True)\nax1.axvline(x=house_df['SalePrice'].mean(), color='g', linestyle='--', linewidth=3)\nax1.text(house_df['SalePrice'].mean(), 125, \"Mean\", horizontalalignment='left', size=20, color='black', weight='semibold')\nax1.set_title('Original Sale Price Histogram',fontsize=20)\nax2 = plt.subplot(1,3,2)\nsns.regplot(data=house_df, y=\"SalePrice\", x=\"GrLivArea\",ax=ax2,color='Blue')\nax2.set_title('Orignal Sale Price Scatter plot',fontsize=20)\nax3 = plt.subplot(1,3,3)\nstats.probplot(house_df['SalePrice'],dist = stats.norm, plot = ax3)\nax3.set_title('Orignal Sale Price Q-Q plot',fontsize=20)\nsns.despine()\n\nmean = house_df['SalePrice'].mean()\nstd = house_df['SalePrice'].std()\nskew = house_df['SalePrice'].skew()\nprint('SalePrice : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","a7e3c7da":"house_df['SalePriceCpy'] = np.log1p(house_df['SalePrice'])","b92a8f53":"plt.figure(figsize = (20,5))\nax1 = plt.subplot(1,3,1)\nsns.histplot(house_df['SalePriceCpy'],palette='Blues_r',kde=True)\nplt.axvline(x=house_df['SalePriceCpy'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(house_df['SalePriceCpy'].mean(), 125, \"Mean\", horizontalalignment='left', size=20, color='black', weight='semibold')\nplt.title('Log Scaled Sale Price Histogram',fontsize=20)\nax2 = plt.subplot(1,3,2)\nsns.regplot(data=house_df, y=\"SalePriceCpy\", x=\"GrLivArea\",ax=ax2,color='Blue')\nax2.set_title('Log Scaled Sale Price scatter plot',fontsize=20)\nax3 = plt.subplot(1,3,3)\nstats.probplot(house_df['SalePriceCpy'],plot = ax3)\nax3.set_title('Log Scaled  Sale Price Q-Q plot',fontsize=20)\nsns.despine()\n\nmean = house_df['SalePriceCpy'].mean()\nstd = house_df['SalePriceCpy'].std()\nskew = house_df['SalePriceCpy'].skew()\nprint('SalePrice : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","d8969659":"house_df.drop('SalePriceCpy',axis=1,inplace=True,errors='ignore')","c8faec3a":"i = 1\nsns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nplt.figure(figsize=(20, 20))\nplt.subplots_adjust(hspace=1)\nfor feature in float_cols:\n    plt.subplot(6,2,i)\n    sns.histplot(house_df[feature], palette='Blues_r')\n    i = i +1","205cf5a8":"i = 1\nsns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nplt.figure(figsize=(20,20))\nplt.subplots_adjust(hspace=1)\nfor feature in float_cols:\n    plt.subplot(6,2,i)\n    sns.regplot(data=house_df, x=feature,y='SalePrice')\n    i = i +1","c942b2a7":"i = 1\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nplt.figure(figsize=(40, 40))\nplt.subplots_adjust(hspace=1)\nfor feature in int_cols:\n    plt.subplot(9,4,i)\n    sns.histplot(house_df[feature], palette='Blues_r')\n    i = i +1","ee143b7f":"house_df['MSSubClass'] = house_df['MSSubClass'].apply(str)\nhouse_df['YrSold'] = house_df['YrSold'].astype(str)\nhouse_df['MoSold'] = house_df['MoSold'].astype(str)","ef1ca73d":"sns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nplt.figure(figsize=(16,5))\nsns.lineplot(data=house_df, x='YearBuilt',y='SalePrice')\nsns.despine()","2586cd45":"house_yearbuilt_ds = pd.to_datetime(house_df[tr_idx].YearBuilt,format='%Y')\nhouse_yearbuild_df = pd.concat([house_df.SalePrice[tr_idx],house_yearbuilt_ds],axis=1)\nhouse_yearbuild_df.rename(columns={\"SalePrice\": \"y\",\"YearBuilt\":\"ds\"},inplace=True)","a6b3dd19":"from fbprophet import Prophet \nm = Prophet()\nm.fit(house_yearbuild_df)","d35fbaf6":"future = m.make_future_dataframe(periods=2000)\nforecast = m.predict(future)","6c8d4d4c":"from fbprophet.plot import plot_plotly, plot_components_plotly\nplot_plotly(m, forecast)","2459aae7":"plot_components_plotly(m, forecast)","2f31b52e":"sns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nplt.figure(figsize=(16,5))\nsns.lineplot(data=house_df, x='YearRemodAdd',y='SalePrice')\nsns.despine()","4a812b6a":"house_YearRemodAdd_ds = pd.to_datetime(house_df[tr_idx].YearRemodAdd,format='%Y')\nhouse_YearRemodAdd_df = pd.concat([house_df.SalePrice[tr_idx],house_YearRemodAdd_ds],axis=1)\nhouse_YearRemodAdd_df.rename(columns={\"SalePrice\": \"y\",\"YearRemodAdd\":\"ds\"},inplace=True)","03211368":"m = Prophet()\nm.fit(house_YearRemodAdd_df)","6e9be62f":"future = m.make_future_dataframe(periods=2000)\nforecast = m.predict(future)","0a4622b4":"plot_plotly(m, forecast)","11cb39b3":"plot_components_plotly(m, forecast)","526f6365":"sns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nplt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.subplots_adjust(wspace=0.3)\nax1 = sns.regplot(data=house_df, x='GrLivArea',y='SalePrice')\nax1.set_title('Outliers in GrLivArea',fontsize=20)\nplt.axhline(y=250000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=4000, color='Green', linestyle='--', linewidth=3)\nplt.text(4500, 150000, 'Outliers',color='red')\nplt.subplot(1,2,2)\nax2 = sns.regplot(data=house_df, x='TotalBsmtSF',y='SalePrice')\nax2.set_title('Outliers in TotalBsmtSF',fontsize=20)\nplt.axhline(y=250000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=4500, color='Green', linestyle='--', linewidth=3)\nplt.text(5000, 200000, 'Outliers',color='red')\nsns.despine()","aefd295d":"features = [\"GarageArea\",\"OverallQual\",\"TotalBsmtSF\",\"GrLivArea\"]","02c8cf3a":"house_copy = house_df[tr_idx].copy()\ny_copy = house_copy.pop(\"SalePrice\")\nX_copy = house_copy.loc[:, features]\n\nX_copy.isnull().sum()\npca, X_pca, loadings = apply_pca(X_copy)","64956ca8":"import plotly.express as px\nfig = px.histogram(X_pca[['PC1','PC2']].melt(), color=\"variable\", \n                   marginal=\"box\",\n                   barmode =\"overlay\",\n                   histnorm ='density'\n                  )  \nfig.update_layout(\n    title_font_color=\"white\",\n    legend_title_font_color=\"yellow\",\n    title={\n        'text': \"PCA Histogram\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    paper_bgcolor=\"black\",\n    plot_bgcolor='black',\n    font_color=\"white\",\n)","f11d2533":"pc1_outlier_idx = list(outlier_iqr(X_pca['PC1'])[0])","439ec03c":"component = \"PC1\"\n\npc1_idx = X_pca[component].sort_values(ascending=False).index\ndf_temp_pc1 = house_df.loc[pc1_idx, [\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + features]\n\ndef highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\ndf_temp_pc1.iloc[pc1_outlier_idx,:].style.set_properties(**{'background-color': 'Grey',\n                            'color': 'white',\n                            'border-color': 'darkblack'})","0a3e9cdd":"sns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nplt.figure(figsize=(20,30))\nplt.subplots_adjust(hspace=0.3)\nplt.subplot(4,2,1)\nax1 = sns.regplot(data=house_df, x='GrLivArea',y='SalePrice')\nax1.set_title('Outliers in GrLivArea',fontsize=20)\nplt.axhline(y=250000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=4000, color='Green', linestyle='--', linewidth=3)\nplt.text(4500, 200000, 'Outliers',color='red')\nplt.subplot(4,2,2)\nax2 = sns.regplot(data=house_df.iloc[pc1_outlier_idx,:], x='GrLivArea',y='SalePrice')\nax2.set_title('Outliers After PCA',fontsize=20)\nplt.axhline(y=300000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=4000, color='Green', linestyle='--', linewidth=3)\nplt.text(4500, 200000, 'Outliers',color='red')\nplt.subplot(4,2,3)\nax3 = sns.regplot(data=house_df, x='TotalBsmtSF',y='SalePrice')\nax3.set_title('Outliers in TotalBsmtSF',fontsize=20)\nplt.axhline(y=450000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=4500, color='Green', linestyle='--', linewidth=3)\nplt.text(5000, 220000, 'Outliers',color='red')\nplt.subplot(4,2,4)\nax4 = sns.regplot(data=house_df.iloc[pc1_outlier_idx,:], x='TotalBsmtSF',y='SalePrice')\nax4.set_title('Outliers After PCA',fontsize=20)\nplt.axhline(y=450000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=3500, color='Green', linestyle='--', linewidth=3)\nplt.text(5000, 250000, 'Outliers',color='red')\nplt.subplot(4,2,5)\nax5 = sns.regplot(data=house_df, x='OverallQual',y='SalePrice')\nax5.set_title('Outliers in OverallQual',fontsize=20)\nplt.axhline(y=650000, color='Green', linestyle='--', linewidth=3)\nplt.text(6, 700000, 'Outliers',color='red')\nplt.subplot(4,2,6)\nax6 = sns.regplot(data=house_df.iloc[pc1_outlier_idx,:], x='OverallQual',y='SalePrice')\nax6.set_title('Outliers After PCA',fontsize=20)\nplt.axhline(y=650000, color='Green', linestyle='--', linewidth=3)\nplt.text(6, 700000, 'Outliers',color='red')\nplt.subplot(4,2,7)\nax7 = sns.regplot(data=house_df, x='GarageArea',y='SalePrice')\nax7.set_title('Outliers in GarageArea',fontsize=20)\nplt.axhline(y=700000, color='Green', linestyle='--', linewidth=3)\nplt.text(400, 720000, 'Outliers',color='red')\nplt.subplot(4,2,8)\nax8 = sns.regplot(data=house_df.iloc[pc1_outlier_idx,:], x='GarageArea',y='SalePrice')\nax8.set_title('Outliers After PCA',fontsize=20)\nplt.axhline(y=700000, color='Green', linestyle='--', linewidth=3)\nplt.text(1000, 720000, 'Outliers',color='red')\nsns.despine()","eeab83e2":"pc2_outlier_idx = list(outlier_iqr(X_pca['PC2'])[0])","bc996921":"component = \"PC2\"\n\npc2_idx = X_pca[component].sort_values(ascending=False).index\ndf_temp_pc2 = house_df.loc[pc2_idx, [\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + features]\n\ndef highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\ndf_temp_pc2.iloc[pc2_outlier_idx,:].style.set_properties(**{'background-color': 'Grey',\n                            'color': 'white',\n                            'border-color': 'darkblack'})","1589ce41":"house_df.drop(pc1_outlier_idx,axis=0,inplace=True,errors='ignore')\nhouse_df.drop(pc2_outlier_idx,axis=0,inplace=True,errors='ignore')","055544d2":"sns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nplt.figure(figsize=(20,15))\nplt.subplots_adjust(hspace=0.2,wspace=0.4)\nplt.subplot(2,2,1)\nsns.regplot(data=house_df, x='GrLivArea',y='SalePrice')\nplt.axhline(y=250000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=4000, color='Green', linestyle='--', linewidth=3)\nplt.text(4010, 200000, 'Outliers',color='red')\nplt.subplot(2,2,2)\nsns.regplot(data=house_df, x='TotalBsmtSF',y='SalePrice')\nplt.axhline(y=450000, color='Green', linestyle='--', linewidth=3)\nplt.axvline(x=4500, color='Green', linestyle='--', linewidth=3)\nplt.text(4800, 250000, 'Outliers',color='red')\nplt.subplot(2,2,3)\nsns.regplot(data=house_df, x='YearRemodAdd',y='SalePrice')\nplt.axhline(y=650000, color='Green', linestyle='--', linewidth=3)\nplt.text(2000, 680000, 'Outliers',color='red')\nplt.subplot(2,2,4)\nsns.regplot(data=house_df, x='GarageArea',y='SalePrice')\nplt.axhline(y=680000, color='Green', linestyle='--', linewidth=3)\nplt.text(400, 720000, 'Outliers',color='red')\nsns.despine()","e65adbb0":"house_df[\"AllArea\"] = house_df.GrLivArea + house_df.TotalBsmtSF\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nsns.regplot(data=house_df, x='AllArea',y='SalePrice')\nplt.title('AllArea-SalePrice',fontsize=20)\nsns.despine()","878f1ec4":"house_df[\"NewBsmtSF\"] = house_df['YearRemodAdd'] + house_df['TotalBsmtSF']\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nsns.regplot(data=house_df, x='NewBsmtSF',y='SalePrice')\nplt.title('NewBsmtSF-SalePrice',fontsize=20)\nsns.despine()","a1f50d36":"house_df[\"HighQualSF\"] = house_df[\"1stFlrSF\"] + house_df[\"2ndFlrSF\"]\nsns.set_style(\"white\")\nsns.set_palette(\"Blues_r\")\nsns.regplot(data=house_df, x='NewBsmtSF',y='SalePrice')\nplt.title('HighQualSF-SalePrice',fontsize=20)\nsns.despine()","9d7e55ba":"# Select numerical columns\nint_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if \n                house_df[cname].dtype in ['int64']]\nfloat_cols = [cname for cname in house_df.loc[:,:'SaleCondition'].columns if \n                house_df[cname].dtype in ['float64']]\n\nnumerical_cols = int_cols + float_cols","1087df04":"from scipy.stats import skew\nplt.figure(figsize=(20, 10))\nskew_features = house_df[numerical_cols].apply(lambda x : skew(x))\nskew_features = skew_features[skew_features > 1].sort_values(ascending=False)\nax = sns.barplot( x =skew_features.index,y=skew_features.values,color='grey')\nfor p in ax.patches:\n    height = p.get_height().round(1)\n    ax.text(p.get_x()+ p.get_width()\/\/2, height, height, ha = 'left', size = 20)\nplt.xticks(rotation=45)\nplt.text(5, 1.2, 'Threshold',color='red')\nplt.axhline(y=1, color='green', linestyle='--', linewidth=3)\nplt.title('Skewness',fontsize=30)\nsns.despine()","94b5beee":"house_df[skew_features.index] = np.log1p(house_df[skew_features.index])","7dd592c1":"from scipy.stats import skew\nplt.figure(figsize=(10, 5))\nskew_features = house_df[numerical_cols].apply(lambda x : skew(x))\nskew_features = skew_features[skew_features > 1].sort_values(ascending=False)\nax = sns.barplot( x =skew_features.index,y=skew_features.values,color='grey')\nfor p in ax.patches:\n    height = p.get_height().round(1)\n    ax.text(p.get_x()+ p.get_width()\/\/2, height, height, ha = 'left', size = 20)\nplt.xticks(rotation=45)\nplt.axhline(y=1, color='green', linestyle='--', linewidth=3)\nplt.text(4, 1.2, 'Threshold',color='red')\nplt.title('Skewness',fontsize=30)\nsns.despine()","202c5de0":"i = 1\nsns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"gist_yarg_r\")\nplt.figure(figsize=(20,30))\nplt.subplots_adjust(hspace=1)\nfor feature in skew_features.index:\n    plt.subplot(10,2,i)\n    sns.histplot(house_df[feature], color='blue',palette='Blues_r')\n    i = i +1","816ae74a":"categorical_cols = [cname for cname in house_df.columns if\n                    house_df[cname].dtype == \"object\"]","a74ac4d7":"house_df['HasPoolQC'] = house_df['PoolQC'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasPoolQC',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasPoolQC',y='SalePrice',palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasPoolQC',y='SalePrice',color='black')","339796dd":"house_df['HasMiscFeature'] = house_df['MiscFeature'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasMiscFeature',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasMiscFeature',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasMiscFeature',y='SalePrice',color='black')","9fe5f844":"house_df['HasAlley'] = house_df['Alley'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasAlley',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasAlley',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasAlley',y='SalePrice',color='black')","59ddec83":"house_df['HasFence'] = house_df['Fence'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasFence',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasFence',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasFence',y='SalePrice',color='black')","483f0ce4":"house_df['HasFireplaceQu'] = house_df['FireplaceQu'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasFireplaceQu',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasFireplaceQu',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasFireplaceQu',y='SalePrice',color='black')","a1c67bd5":"house_df['HasGarageQual'] = house_df['GarageQual'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasGarageQual',y='SalePrice', palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasFireplaceQu',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasGarageQual',y='SalePrice',color='black')","049c28c7":"house_df['HasBsmtQual'] = house_df['BsmtQual'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasBsmtQual',y='SalePrice', palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasBsmtQual',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasBsmtQual',y='SalePrice',color='black')","1771f74c":"house_df['HasMasVnrType'] = house_df['MasVnrType'].notnull().astype(int)\n\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='HasMasVnrType',y='SalePrice', palette='gist_gray_r')\nsns.stripplot(data=house_df, x='HasMasVnrType',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='HasMasVnrType',y='SalePrice',color='black')","2aefa851":"house_df['Alley'] = house_df['Alley'].fillna('missing')\nhouse_df['MasVnrType'] = house_df['MasVnrType'].fillna('None')\nhouse_df['GarageType'] = house_df['GarageType'].fillna('missing')\nhouse_df['GarageCond'] = house_df['GarageCond'].fillna('missing')\nhouse_df['Fence'] = house_df['Fence'].fillna('missing')\nhouse_df['Street'] = house_df['Street'].fillna('missing')\nhouse_df['LotShape'] = house_df['LotShape'].fillna('missing')\nhouse_df['LandContour'] = house_df['LandContour'].fillna('missing')\nhouse_df['BsmtFinType1'] = house_df['BsmtFinType1'].fillna('missing')\nhouse_df['BsmtFinType2'] = house_df['BsmtFinType2'].fillna('missing')\nhouse_df['CentralAir'] = house_df['CentralAir'].fillna('missing')\nhouse_df['MiscFeature'] = house_df['MiscFeature'].fillna('missing')\nhouse_df['Utilities'] = house_df['Utilities'].fillna('missing')\nhouse_df['SaleCondition'] = house_df['SaleCondition'].fillna('missing')\nhouse_df[\"Functional\"] = house_df[\"Functional\"].fillna(\"Typ\")","b5969cc6":"house_df['MSZoning'] = house_df['MSZoning'].fillna(house_df['MSZoning'].mode()[0])\nhouse_df['PoolQC'] = house_df['PoolQC'].fillna(house_df['PoolQC'].mode()[0])\nhouse_df['BsmtQual'] = house_df['BsmtQual'].fillna(house_df['BsmtQual'].mode()[0])\nhouse_df['BsmtCond'] = house_df['BsmtCond'].fillna(house_df['BsmtCond'].mode()[0])\nhouse_df['FireplaceQu'] = house_df['FireplaceQu'].fillna(house_df['FireplaceQu'].mode()[0])\nhouse_df['GarageFinish'] = house_df['GarageFinish'].fillna(house_df['GarageFinish'].mode()[0])\nhouse_df['GarageQual'] = house_df['GarageQual'].fillna(house_df['GarageQual'].mode()[0])\nhouse_df['BsmtExposure'] = house_df['BsmtExposure'].fillna(house_df['BsmtExposure'].mode()[0])\nhouse_df['Electrical'] = house_df['Electrical'].fillna(house_df['Electrical'].mode()[0])\nhouse_df['Exterior1st'] = house_df['Exterior1st'].fillna(house_df['Exterior1st'].mode()[0])\nhouse_df['Exterior2nd'] = house_df['Exterior2nd'].fillna(house_df['Exterior2nd'].mode()[0])    \nhouse_df['KitchenQual'] = house_df['KitchenQual'].fillna(house_df['KitchenQual'].mode()[0])\nhouse_df['SaleType'] = house_df['SaleType'].fillna(house_df['SaleType'].mode()[0])","63ae5c44":"house_df['MSZoning'] = house_df['MSZoning'].replace({'C (all)':1,'RM':2,'RH':3,'RL':4,'FV':5})","2b5c8d3b":"house_df['Condition1'] = house_df['Condition1'].replace({'Artery':1,\n                                                         'RRAe':1,\n                                                         'RRNn':1,\n                                                         'Feedr':1,\n                                                         'RRNe':1,\n                                                         'RRAn':1,\n                                                         'Norm':2,\n                                                         'PosA':3,\n                                                         'PosN':3})","2d6b4dc4":"house_df['Condition2'] = house_df['Condition2'].replace({'RRNn':1,\n                                                         'Artery':2, \n                                                         'Feedr':2,\n                                                         'RRAn':2,\n                                                         'RRAe':2,    \n                                                         'Norm':2,\n                                                         'PosA':3,\n                                                         'PosN':3})","6648bc66":"def HouseStyleToInt(x):\n    if(x=='1.5Unf'):\n        r = 0\n    elif(x=='SFoyer'):\n        r = 1\n    elif(x=='1.5Fin'):\n        r = 2\n    elif(x=='2.5Unf'):\n        r = 3\n    elif(x=='SLvl'):\n        r = 4\n    elif(x=='1Story'):\n        r = 5\n    elif(x=='2Story'):\n        r = 6  \n    elif(x==' 2.5Fin'):\n        r = 7          \n    else:\n        r = 8\n    return r\n\nhouse_df['HouseStyle'] = house_df['HouseStyle'].apply(HouseStyleToInt)","980d432b":"def MasVnrTypeToInt(x):\n    if(x=='Stone'):\n        r = 3\n    elif(x=='BrkFace'):\n        r = 2\n    elif(x=='BrkCmn'):\n        r = 1        \n    else:\n        r = 0\n    return r\n\nhouse_df['MasVnrType'] = house_df['MasVnrType'].apply(MasVnrTypeToInt)","21c8590e":"foundation_label = {'Slab':1,'BrkTil':2,'Stone':2,'CBlock':3,'Wood':4,'PConc':5}\nhouse_df['Foundation'] = house_df['Foundation'].replace(foundation_label)","a8a8ef16":"garagetype_label = {'CarPort':1,'Basment':2,'Detchd':2,'Attchd':3,'2Types':3,'BuiltIn':4}\nhouse_df['GarageType'] = house_df['GarageType'].replace(garagetype_label)","6f998b70":"house_df['GarageFinish'] = house_df['GarageFinish'].replace({'Unf':1,'RFn':2,'Fin':3})","57b0d191":"house_df['PavedDrive'] = house_df['PavedDrive'].replace({'N':1,'P':2,'Y':3})","50d4c14a":"salecon_label = {'AdjLand':1,'Abnorml':2,'Family':2,'Alloca':2,'Normal':3,'Partial':4}\nhouse_df['SaleCondition'] = house_df['SaleCondition'].replace(salecon_label)","e3497fb2":"ext_lable = {'AsbShng':1,'AsphShn':1,\n             'MetalSd':2,'Wd Sdng':2,'WdShing':2, 'Wd Shng':2,'Stucco':2,'CBlock':2,\n             'HdBoard':3,'BrkFace':3,'Plywood':3,'Other':3,\n             'VinylSd':4,'CemntBd':4,'BrkComm':4,'CmentBd':4,'Brk Cmn':4,\n             'Stone':5,'ImStucc':5 }\nhouse_df['Exterior1st'] = house_df['Exterior1st'].replace(ext_lable)\nhouse_df['Exterior2nd'] = house_df['Exterior2nd'].replace(ext_lable)","742457be":"def BsmtExposureToInt(x):\n    if(x=='Gd'):\n        r = 4\n    elif(x=='Av'):\n        r = 3\n    elif(x=='Mn'):\n        r = 2\n    elif(x=='No'):\n        r = 1\n    else:\n        r = 0\n    return r\n\nhouse_df['BsmtExposure'] = house_df['BsmtExposure'].apply(BsmtExposureToInt)","6c6c6c7e":"def BsmtFinType1ToInt(x):\n    if(x=='GLQ'):\n        r = 6\n    elif(x=='ALQ'):\n        r = 5\n    elif(x=='BLQ'):\n        r = 4\n    elif(x=='Rec'):\n        r = 3   \n    elif(x=='LwQ'):\n        r = 2\n    elif(x=='Unf'):\n        r = 1        \n    else:\n        r = 0\n    return r\n\nhouse_df['BsmtFinType1_int'] = house_df['BsmtFinType1'].apply(BsmtFinType1ToInt)","8f4cb902":"quality_label = {'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}\n\nhouse_df['ExterQual'] = house_df['ExterQual'].replace(quality_label)\nhouse_df['ExterCond'] = house_df['ExterCond'].replace(quality_label)\nhouse_df['KitchenQual'] = house_df['KitchenQual'].replace(quality_label)\nhouse_df['HeatingQC'] = house_df['HeatingQC'].replace(quality_label)\nhouse_df['BsmtQual'] = house_df['BsmtQual'].replace(quality_label)\nhouse_df['BsmtCond'] = house_df['BsmtCond'].replace(quality_label)\nhouse_df['FireplaceQu'] = house_df['FireplaceQu'].replace(quality_label)\nhouse_df['GarageQual'] = house_df['GarageQual'].replace(quality_label)\nhouse_df['PoolQC'] = house_df['PoolQC'].replace(quality_label)","3276fa80":"categorical_cols = [cname for cname in house_df.columns if\n                    house_df[cname].dtype == \"object\"]","2b23b517":"house_df['Total_Home_Quality'] = (house_df['OverallQual'] + house_df['OverallCond'])**2\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='Total_Home_Quality',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='Total_Home_Quality',y='SalePrice', palette='Blues_r')\nplt.xticks(rotation=90)\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='Total_Home_Quality',y='SalePrice',color='black')\n","686100ae":"house_df['Total_Bathrooms'] = (house_df['FullBath'] + (0.5 * house_df['HalfBath']) + (house_df['BsmtFullBath'] + (0.5 * house_df['BsmtHalfBath'])))\nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxplot(data=house_df, x='Total_Bathrooms',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='Total_Bathrooms',y='SalePrice', palette='Blues_r')\nplt.xticks(rotation=90)\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='Total_Bathrooms',y='SalePrice',color='black')","5abd3202":"house_df['total_condition'] = house_df['Condition1'] + house_df['Condition2'] \nplt.figure(figsize=(20,6))\nplt.subplots_adjust(wspace=0.3)\nplt.subplot(1,2,1)\nsns.boxenplot(data=house_df, x='total_condition',y='SalePrice',palette='gist_gray_r')\nsns.stripplot(data=house_df, x='total_condition',y='SalePrice', palette='Blues_r')\nplt.subplot(1,2,2)\nsns.regplot(data=house_df, x='total_condition',y='SalePrice',color='black')","58aab9e8":"house_df[\"SqFtPerRoom\"] = house_df[\"GrLivArea\"] \/ (house_df[\"TotRmsAbvGrd\"] +\n                                                       house_df[\"FullBath\"] +\n                                                       house_df[\"HalfBath\"] +\n                                                       house_df[\"KitchenAbvGr\"])\nplt.figure(figsize=(8,8))\nsns.regplot(data=house_df, x='SqFtPerRoom',y='SalePrice',color='blue')","ecbd81c5":"import missingno as msno\nmsno.matrix(house_df[tr_idx])","5842d3fd":"corr=house_df.corr().round(1)\nsns.set(font_scale=1)\nplt.figure(figsize=(10, 15))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nabs(corr['SalePrice']).sort_values()[:-1].plot.barh()\nplt.gca().set_facecolor('#FFFFFF')\nplt.title('Correlation with SalePrice',fontsize=20)","cd252ef0":"abs_corr = abs(corr['SalePrice']).sort_values()[:-1]\nlow_corr_list = abs_corr[abs_corr.values == 0].index\nhouse_df.drop(low_corr_list, axis=1,inplace=True)","5c043ac9":"house_df = pd.get_dummies(house_df, drop_first=True)","662a78cd":"house_df['SalePriceCpy'] = np.log1p(house_df['SalePrice'])\nhouse_df.drop('SalePrice',axis=1,inplace=True)\nhouse_df['SalePrice'] = house_df['SalePriceCpy'].copy()\nhouse_df.drop('SalePriceCpy',axis=1,inplace=True)","8d84993f":"X_train = house_df[tr_idx].drop('SalePrice',axis=1,errors='ignore')\ny_train = house_df[tr_idx].pop('SalePrice')\nX_test = house_df[~tr_idx].drop('SalePrice',axis=1,errors='ignore')\ny_test = house_df[~tr_idx].pop('SalePrice')","f2305666":"high_corr_list = abs_corr[abs_corr.values > 0.5].index\nhigh_corr_list = high_corr_list[::-1]","c3e07552":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.inspection import plot_partial_dependence\n\nclf = GradientBoostingRegressor(n_estimators=100).fit(X_train, y_train)\nfig,ax = plt.subplots(figsize=(12,20))\nplot_partial_dependence(clf, X_train, high_corr_list,ax=ax)","692f3a52":"all_cols = [cname for cname in house_df.loc[:,:'SaleType_WD'].columns if \n                house_df[cname].dtype in ['int64','float64','uint8']]","9c6686fa":"import umap\nimport umap.plot\nmapper = umap.UMAP().fit(X_train)\numap.plot.points(mapper, labels=y_train.round(1), theme='fire')","ebdf7782":"import plotly\nimport plotly.express as px\nfrom umap import UMAP\n\numap_3d = UMAP(n_components=3, init='random', random_state=0)\nx_umap = umap_3d.fit_transform(X_train)\numap_df = pd.DataFrame(x_umap)\ntrain_y_sr = pd.Series(y_train,name='price')\nprint(type(x_umap))\nnew_df = pd.concat([umap_df,train_y_sr],axis=1)\nfig = px.scatter_3d(\n    new_df, x=0, y=1, z=2,\n    color='price', labels={'color': 'number'}\n)\nfig.update_traces(marker_size=1.5)\nfig.show()","6a48d33a":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","d4b44b99":"X_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train.values).view(-1, 1).float()\n\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test.values).view(-1, 1).float()\n\ndatasets = torch.utils.data.TensorDataset(X_train, y_train)\ntrain_iter = torch.utils.data.DataLoader(datasets, batch_size=10, shuffle=True)","d3945a7a":"batch_size = 50\nnum_epochs = 400\nlearning_rate = 0.01\nsize_hidden1 = 600\nsize_hidden2 = 300\nsize_hidden3 = 300\nsize_hidden4 = 10\nsize_hidden5 = 1\ninput_size = 202","9f7808ab":"class HouseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin1 = nn.Linear(input_size, size_hidden1)\n        self.act1 = nn.ReLU()\n        self.BatchNorm1d1 = nn.BatchNorm1d(size_hidden1, affine=False)\n        self.drop1 = nn.Dropout(0.3)\n        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n        self.act2 = nn.ReLU()\n        self.BatchNorm1d2 = nn.BatchNorm1d(size_hidden2, affine=False)\n        self.drop2 = nn.Dropout(0.3)\n        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n        self.act3 = nn.ReLU()\n        self.BatchNorm1d3 = nn.BatchNorm1d(size_hidden3, affine=False)\n        self.drop3 = nn.Dropout(0.3)\n        self.lin4 = nn.Linear(size_hidden3, size_hidden4)\n        self.act4 = nn.ReLU()\n        self.BatchNorm1d4 = nn.BatchNorm1d(size_hidden4, affine=False)\n        self.lin5 = nn.Linear(size_hidden4, size_hidden5)\n\n    def forward(self, input):\n        lin1_out = self.lin1(input)\n        lin1_out = self.BatchNorm1d1(lin1_out)\n        lin1_out = self.drop1(lin1_out)\n        act1_out = self.act1(lin1_out)\n        lin2_out = self.lin2(act1_out)\n        lin2_out = self.BatchNorm1d2(lin2_out)\n        lin2_out = self.drop2(lin2_out)\n        act2_out = self.act2(lin2_out)\n        lin3_out = self.lin3(act2_out)\n        lin3_out = self.BatchNorm1d3(lin3_out)\n        lin3_out = self.drop3(lin3_out)\n        act3_out3 = self.act3(lin3_out)\n        lin4_out = self.lin4(act3_out3)\n        lin4_out = self.BatchNorm1d4(lin4_out)\n        act4_out4 = self.act4(lin4_out)\n        lin5_out = self.lin5(act4_out4)\n        return lin5_out","ff296f45":"model = HouseNet()\nmodel.train()","6a183ebe":"criterion = nn.MSELoss(reduction='sum')","fa3e8df8":"def train(model_inp, num_epochs = num_epochs):\n    optimizer = torch.optim.Adam(model_inp.parameters(), lr=learning_rate)\n    for epoch in range(num_epochs):  # loop over the dataset multiple times\n        running_loss = 0.0\n        for inputs, labels in train_iter:\n            # forward pass\n            outputs = model_inp(inputs)\n            # defining loss\n            loss = criterion(outputs, labels)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # computing gradients\n            loss.backward()\n            # accumulating running loss\n            running_loss += loss.item()\n            # updated weights based on computed gradients\n            optimizer.step()\n        if epoch % 20 == 0:    \n            print('Epoch [%d]\/[%d] running accumulative loss across all batches: %.3f' %\n                  (epoch + 1, num_epochs, running_loss))\n        running_loss = 0.0","f63d967e":"train(model)","f853cc3f":"model.eval()\noutputs = model(X_test).squeeze()","c5c0ee1e":"preds_test_nn = np.expm1(outputs.detach().numpy())","6df1d5af":"submission_data.loc[:,'SalePrice'] = preds_test_nn\nsubmission_data.to_csv('submission_nn.csv', index=False)","672a7b2a":"submission_data.head(5)","2cacd795":"ig = IntegratedGradients(model)\nig_attr_test = ig.attribute(X_test, n_steps=50)","47d4e8d2":"# prepare attributions for visualization\nx_axis_data = np.arange(X_test.shape[1])\nx_axis_data_labels = list(map(lambda idx: all_cols[idx], x_axis_data))\n\nig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\nig_attr_test_norm_sum = ig_attr_test_sum \/ np.linalg.norm(ig_attr_test_sum, ord=1)\n\nlin_weight = model.lin1.weight[0].detach().numpy()\ny_axis_lin_weight = lin_weight \/ np.linalg.norm(lin_weight, ord=1)\n\nFONT_SIZE = 20\nplt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\nplt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\nplt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\nplt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n\nstart = 0\nwidth = 50\nfor i in range(4):\n    plt.figure(figsize=(25, 5))\n    ax = plt.subplot()\n    ax.set_title(f'Input feature importances part-{i}')\n    ax.set_ylabel('Attributions')\n    ax.bar(x_axis_data[start:start+width], ig_attr_test_norm_sum[start:start+width], align='center', color='blue')\n    ax.autoscale_view()\n    plt.tight_layout()\n\n    ax.set_xticks(x_axis_data[start:start+width])\n    ax.set_xticklabels(x_axis_data_labels[start:start+width],fontsize=20)\n    start = start + width + 1\n    plt.xticks(rotation=90)\n    plt.show()   \n","00877e9a":"# Compute the attributions of the output with respect to the inputs of the 5th linear layer\nlc = LayerConductance(model, model.lin5)\nlc_attr_test = lc.attribute(X_test, n_steps=100, attribute_to_layer_input=True)\n\n# shape: test_examples x size_hidden\nlc_attr_test = lc_attr_test[0]\n\n# weights from 5th linear layer\n# shape: size_hidden5 x size_hidden4\nlin5_weight = model.lin5.weight","490ba622":"plt.figure(figsize=(15, 8))\n\nx_axis_data = np.arange(lc_attr_test.shape[0])\n\ny_axis_lc_attr_test = lc_attr_test.detach().numpy()\ny_axis_lc_attr_test = y_axis_lc_attr_test \/ np.linalg.norm(y_axis_lc_attr_test, ord=1)\n\ny_axis_lin5_weight = lin5_weight[0].detach().numpy()\ny_axis_lin5_weight = y_axis_lin5_weight \/ np.linalg.norm(y_axis_lin5_weight, ord=1)\n\nwidth = 0.25\nlegends = ['Attributions','Weights']\nx_axis_labels = [ 'Neuron {}'.format(i) for i in range(len(y_axis_lin5_weight))]\n\nax = plt.subplot()\nax.set_title('Aggregated neuron importances and learned weights in the last linear layer of the model')\n\nax.bar(x_axis_data + width, y_axis_lc_attr_test, width, align='center', color='red')\nax.bar(x_axis_data + 2 * width, y_axis_lin5_weight, width, align='center', color='blue')\nplt.legend(legends, loc=2, prop={'size': 20})\nax.autoscale_view()\nplt.tight_layout()\n\nax.set_xticks(x_axis_data + 0.5)\nax.set_xticklabels(x_axis_labels,fontsize=20)\nplt.xticks(rotation=90)\nplt.show()","a5c626d2":"from pycaret.regression import *","93cb4f99":"reg = setup(data = house_df[tr_idx],\n            target = 'SalePrice',\n            normalize = False,\n            preprocess = False,\n            numeric_features = all_cols,\n            verbose = False,\n            silent = True)","61446a01":"catboost = create_model('catboost')\nbr = create_model('br')\nridge = create_model('ridge')\ngbr = create_model('gbr')\nlightgbm = create_model('lightgbm')","b64c07dd":"tuned_catboost = tune_model(catboost,early_stopping=True,optimize='RMSE',search_library='optuna')\ntuned_br = tune_model(br,early_stopping=True,optimize='RMSE',search_library='optuna')\ntuned_gbr = tune_model(gbr,early_stopping=True,optimize='RMSE',search_library='optuna')\ntuned_ridge = tune_model(ridge,early_stopping=True,optimize='RMSE',search_library='optuna')\ntuned_lightgbm = tune_model(lightgbm,early_stopping=True,optimize='RMSE',search_library='optuna')","9328b729":"interpret_model(tuned_catboost)","6a6f9905":"interpret_model(tuned_lightgbm)","da169548":"blend_soft = blend_models(estimator_list = [catboost,br,ridge,gbr,lightgbm], \n                          optimize = 'RMSE')","ec39dacf":"final_model = finalize_model(blend_soft)","b14cd01e":"plot_model(final_model, plot='residuals')","7d84a602":"plot_model(final_model, plot='error')","ed034f36":"X_test_df = house_df[~tr_idx].drop('SalePrice',axis=1)\npreds_test_pycaret = np.expm1(final_model.predict(X_test_df))","f46652ce":"submission_data.loc[:,'SalePrice'] = preds_test_pycaret\nsubmission_data.to_csv('submission.csv', index=False)","10270f98":"submission_data.head(5)","682fbb83":"<span style=\"color:Blue\"> Observation:\n    \nLooking at the figure above, points that are clearly judged as outliers in the figures on the left are also judged as outliers in PC1 after PCA.\nThat is, outlier judgment using PCA seems very effective.","60a75420":"11 outliers were detected as the first principal component.","418b7ea9":"-------------------------------------------------------------\n# Let's explain Neural Netwok\n\n![](https:\/\/media0.giphy.com\/media\/1iUixiLvgO2jyBtZoy\/giphy.gif)\n\nPicture Credit: https:\/\/media0.giphy.com\/media\n\n\n**neural network\/neurons**\n\n> A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Neural_network","34f587dc":"--------------------------------------------------------------\n## Utility Functions","9b7ffef3":"----------------------------------------------------------\n# Checking Target\n\nThe problem is a regression problem. Therefore, we analyze the distribution of the target and check whether there is necessary preprocessing based on this. If it is a classification problem\nWe need to check target imbalance.","c2611bca":"## Setting Hyperparameters","11501543":"-------------------------------------------------------------------------------\n# Soft Voting\n\n![](https:\/\/miro.medium.com\/max\/806\/1*bliKQZGPccS7ho9Zo6uC7A.jpeg)\n\nPicture Credit: https:\/\/miro.medium.com","524c83b1":"<span style=\"color:Blue\"> Observation:\n    \n* PoolQC, MiscFeature, Alley, Fence, and FireplaceQu features have too many missing values.\n* Garage-related features have 157 to 159 missing values. It is unknown whether the houses lacked garages or were intentionally omitted.\n* Basement-related features also have 79 to 82 missing values.\nIt seems that we need to focus more on the process of filling in the missing values \u200b\u200bof the corresponding Garage and Basement features.","5f1a7dd6":"<span style=\"color:Blue\"> Observation:\n    \nLooking at the figure above, the neural network has determined that the following features are important.\n\n* YearBuilt, YearRemodAdd, GarageYrBlt, NewBsmtSF, Total_Home_Quaility, SqFtPerRoom, HeatingGasW, CentralAir_Y, Functional_Typ, GarageCond_TA","43f4a494":"![](https:\/\/blog.paperspace.com\/content\/images\/2018\/06\/optimizers7.gif)\n\nPicture Credit: https:\/\/blog.paperspace.com\n\n> * Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\n> * Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n> * Adam is relatively easy to configure where the default configuration parameters do well on most problems.\n\nRef: https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/","cd7ec271":"<span style=\"color:Blue\"> Observation:\n    \nThere is only one outlier detected as the second Principal Component.","5565aab2":"## Tuning Hyperparameters\n\n> This function tunes the hyperparameters of a given estimator. The output of this function is a score grid with CV scores by fold of the best selected model based on optimize parameter.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html\n\nWe will use the optuna library to tune the hyperparameters.","2027934d":"## GarageType\nGarage location","f2a73414":"## Foundation\nType of foundation","936ad7ac":"## FireplaceQu: Fireplace quality\n\n**Question: Is there a difference in house prices with and without Fireplace?**","2ec5a738":"The number of skewed features is reduced from 14 to 9. The remaining 4 skewness was greatly reduced.","4b437d2e":"### Plotting after dimensionality reduction to 2D","40b5e334":"----------------------------------------------------\n# Scaling\n\nThere are various scaling methods for numerical features. However, we did log scaling, which is a non-linear scaling of our target. Therefore, other numerical features are also subjected to log scaling.","b8304a18":"In the regression problem, discrete and non-order features must be converted to one-hot encoding. ","274863a9":"## BsmtFinType1\nRating of basement finished area","e2e31359":"-----------------\n## Encoding nominal data using one-hot encoding.","57c7855c":"## Selecting Features","23e3c69d":"Even looking at the 3D scaled-down picture, it is difficult to observe the special rules. If we look at it in 202 dimensions, We may be able to find some rules. However, we cannot draw a 202-dimensional picture. \n\n**Now, all we can do is create a good model and make the model learn well.**","7d9b1693":"----------------------------------------------------------------\n# Doing EDA for Categorical Features\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*wYbTRM0dgnRzutwZq63xCg.png)\n\nPicture Credit: https:\/\/miro.medium.com\n\nCategorical data can be classified into ordinal data and nominal data. In the case of an ordinal type, there is a difference in importance for each level. This value plays an important role in the case of regression, so encode it with care.\n\nIt is difficult to encode categorical features compared to numeric features. For ordinal data, it is more difficult.","050647aa":"> Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.\n\n![](https:\/\/i.imgur.com\/UWOyg4a.png)\n\n> In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.\n> \n> In some cases, this will meaningfully improve results. In other cases, it doesn't help at all.\n\nRef: https:\/\/www.kaggle.com\/alexisbcook\/missing-values","b37088a1":"# Setting up","f1d65808":"## HouseStyle:\nHouseStyle: Style of dwelling","86d33869":"------------------------------------------------------------------------\n## Checking Outliers","7d6e97bb":"> **R-squared** is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n> \n> The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n> \n> * R-squared = Explained variation \/ Total variation\n> \n> R-squared is always between 0 and 100%:\n> \n> * 0% indicates that the model explains none of the variability of the response data around its mean.\n> * 100% indicates that the model explains all the variability of the response data around its mean.\n> In general, the higher the R-squared, the better the model fits your data. However, there are important conditions for this guideline that I\u2019ll talk about both in this post and my next post.\n\nRef: https:\/\/blog.minitab.com\/en","3d4bf6ca":"**Good derivative features come from good questions. Good questions come from a lot of domain-knowledge.**\n","755539bc":"------------------------------------------------------------------------\n## MSZoning\n\nIdentifies the general zoning classification of the sale.","53dd0a09":"## Creating Models\n\n> This function trains and evaluates the performance of a given estimator using cross validation. The output of this function is a score grid with CV scores by fold.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","24675a55":"<hr style=\"border: solid 3px blue;\">\n\n# Deep learnig\n\n![](https:\/\/cdn.dribbble.com\/users\/1373613\/screenshots\/5436349\/plexus____-.gif)\n\nPicture Credit: https:\/\/cdn.dribbble.com\n\nOne of the advantages of DL is that the preprocessing process is simpler than that of classic ML.\nAnd, one of the disadvantages is that it is difficult to explain the model.\nIn particular, this notebook summarizes the methods for explaining DL.\n\nCompared to classic ML in DL, modeling and hyperparameter selection are more important than preprocessing.","fd575615":"## Interpreting Models\n\n> SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions\n\nRef: https:\/\/shap.readthedocs.io\/en\/latest\/index.html","70e409d3":"We trained by taking the logarithm of the target values. Therefore, since predicted values \u200b\u200bare also in the logarithm state, they take the process of converting back to actual values \u200b\u200bthrough the exponential function.","07cc0943":"<span style=\"color:Blue\"> Observation:\n    \nIt seems that there are outliers that deviate from the regression line. Let's check some more.","49517217":"**Id feature is simply unique ID, so it is not helpful for learning. Let's remove it.**","adf93899":"## Missing values","efa7ecea":"<hr style=\"border: solid 3px blue;\">\n\n# Machine Learning (Ensemble)\n\n![](https:\/\/media0.giphy.com\/media\/26xBvMWzk7FQr54Sk\/giphy.gif)\n\nPicture Credit: https:\/\/media0.giphy.com\n\n","76a1a87b":"-------------------------------------\n## The Second Principal Component","462f80e1":"## Plotting after dimensionality reduction to 3D","e4f5b5e9":"## PavedDrive\nPaved driveway","6c130b86":"## MiscFeature: Miscellaneous feature not covered in other categories\n\n**Question: Is there a difference in house price with and without miscellaneous features?**","9913c34b":"-------------------------------------------\n# Submitting Result","3823f9e6":"<span style=\"color:Blue\"> Observation:\n\nIf you look at the picture above, you can see an outlier. Regression models are sensitive to outliers, so it is better to remove them. Outlier is like gravity. It pulls the regression line. Therefore, it is better to remove outliers.\nHowever, it is not easy to judge an outlier. Domain knowledge may also be required to remove outliers.","7b2e165d":"## Question 2: If you recently remodeled and have a large basement, will your sale price increase?\n\n* TotalBsmtSF: Total square feet of basement area\n* YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)","a90f95c1":"----------------------------------------------------------------\n## Explaining features with partial dependence\n\n> Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the \u2018complement\u2019 features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest\n\nRef: https:\/\/scikit-learn.org\/stable\/modules\/partial_dependence.html#partial-dependence","ca99597f":"--------------------------------------------------------\n# Making Derived Features for Categorical Data","96bea4ae":"## Question 3: Can Total Condition Affect Sale Price?","34c25552":"<span style=\"color:Blue\"> Observation:\n\n* Newly created derived variables play an important role in model learning.\n* When the models are different, the feature importance is also slightly different. This diversity is the power of ensemble. ","3d9f009e":"-----------------------------","0e3f6d12":"<span style=\"color:Blue\"> Observation:\n    \n* GarageCars, BsmtHalfBath, BsmtFullBath and GarageCars are discrete variables.\n* Some features have a skewed shape to one side.","e5a9e955":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction","72439ce1":"<span style=\"color:Blue\"> Observation:\n    \nThe order of distribution size for each PC is PC1->PC2. Since the process of finding a PC through PCA is to find a direction vector with the largest variance, this result is self-evident.\n    \nBased on the boxplot, there are outliers for each PC. Let's check further if it makes sense to remove those values.","a3a8c295":"<span style=\"color:Blue\"> Observation:\n\nThe skewness was about 1.88. Also, since the metric is RMSLE, we will perform log scaling.","c4d48247":"First, let's check skewness. A skewness greater than 1 is generally judged to be skewed, so check mainly those greater than 1.","ad8584f8":"## Modeling HouseNet","96b28ccc":"## Condition1\/Condition2\nProximity to various conditions","d6d476a1":"## Question 1: Is total house quality correlated with sale price?","6e194eb9":"## Training HouseNet\n\n![](https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2019\/10\/Backpropagation.gif)\n\nPicture Credit: https:\/\/machinelearningknowledge.ai\n\n> In the case of DL, the weight of each neuron (node) is adjusted through error backpropagation. This process is the most important part of the process of learning. Through this gradient descent process, we can understand which neurons play an important role and which features are important.","2e15e576":"## Quality Features\n\n* Ex Excellent\n* Gd Good\n* TA Average\/Typical\n* Fa Fair\n* Po Poor","d5448d26":"## Encoding Target using Log Scaling","198b4bdd":"## BsmtExposure\nRefers to walkout or garden level walls","fc25e9f6":"## PoolQC: Pool quality \n\n**Question: Does the lack of pool quality mean that the house does not have a pool? If so, is there a relationship between the missing value and the house price?**","3fd5caf4":"Some features still have skewness greater than 1, but further improvement seems difficult.","0c693c26":"------------------------------------------------------\n# Adding New Derived Features using Numerical Feature\n\nLet's create a new derived variable so that our model can learn better.\n\n* TotalBsmtSF: Total square feet of basement area\n* GrLivArea: Above grade (ground) living area square feet\n* YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\n* 1stFlrSF: First Floor square feet \n* 2ndFlrSF: Second floor square feet","33d368cd":"<span style=\"color:Blue\"> Observation:\n\n* R squared is 97.5%. This means that our model can explain the dataset by 97.5%. ","2ed250d8":"<span style=\"color:Blue\"> Observation:\n    \nThe more recently remodeled the house, the higher the house price.","086e4ae2":"------------------------------------\n# Finalizing the last model\n> This function trains a given estimator on the entire dataset including the holdout set.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html\n\n\n**The blend model seems to be stable. Let's use this model as our final model.**","03be838a":"<span style=\"color:Blue\"> Observation:\n\n* The newly created derivative variable AllArea feature and house price have a high correlation!\n* Pool-related features and fireplaces with many missing values have a low correlation with house price. \n","25954059":"We normalize and visualize the attributions and learned model weights for all 10 neurons in the fifth hidden layer.\n\nThe weights represent the weight matrix of the fifth linear layer. The attributions are computed with respect to the inputs of the fifth linear layer.","a82c9002":"Since it is a regression problem, filling it with KNN seems to be a wise choice.\n\n> Each sample\u2019s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.\n\nRef: https:\/\/scikit-learn.org\/stable","336bbda7":"# Categorizing Columns","dde09e6e":"----------------------------------------------------------\n# Attributing to the layers and comparing with model weights","d83195ee":"## Checking Missing Values","267e7030":"Let's remove features that have a correlation coefficient of 0 with the sale price!","2ced46fc":"<span style=\"color:Blue\"> Observation:\n\n* When the AllArea feature changes, the value of house price changes the most. That is, the AllArea feature will be one of the most important features in the GradientBoostingRegressor model.\n* The importance of features such as ExterQual, BsmtQual, GrLivArea, FullBath, NewBsmtSF, and TotalBsmtSF with high correlation in correlation is not important in the GradientBoostingRegressor model.\n* It is advantageous to check feature importance from various viewpoints rather than judging only by correlation. In particular, it can be effective to receive feedback from the designed model. ","3e196cbc":"<span style=\"color:Blue\"> Observation:\n\nThere are 37 numerical features and 43 object (string) types.\nAmong the numeric features, 25 are int types and 12 are float types.\nThere must be a reason for using a different type like this. Let's check some more.","c5743347":"## Alley: Type of alley access to property\n\n**Question: Is there a difference in house price with and without Alley access?**","697915ca":"<span style=\"color:Blue\"> Observation:\n    \nIf you look at the picture above, the more recently built the house, the higher the price.","29a21d50":"## Checking Categorical Missing Values\n\nNumerical missing values were filled in. Let's check again what the remaining missing values are.","6328dec4":"## GarageFinish\nInterior finish of the garage","4625036c":"<span style=\"color:Blue\"> Observation:\n    \nIt is conformed that 14 features are skewed. We will do log transformation for these features.","fe94841d":"-------------------------------------------\n## Discrete Features\n\nDiscrete variables are numeric variables that have a countable number of values between any two values. A discrete variable is always numeric. For example, the number of customer complaints or the number of flaws or defects.","2353c9a2":"-----------------------------------------------------------------------\n# Detecting Outliers by PCA\n\n![](https:\/\/joachim-gassen.github.io\/images\/ani_sim_vert_outlier.gif)\n\nPicture Credit: https:\/\/joachim-gassen.github.io\/images\/ani_sim_vert_outlier.gif\n\nThe more features, the higher the dimension. When projecting to a lower dimension through PCA, new insights can be gained. PCA can effectively detect outliers. Let's apply PCA to this problem and check whether the detection actually confirmed by the eye is also detected through PCA.\n\nIn other words, let's reduce the dimension, check the things that are judged as outliers in the reduced dimension, and check whether the row is an outlier for some reason.\n\n**Looking at the figure above, PC 1 has the largest variance in the dataset distribution. That is, the outlier in PC 1 is very likely to be an outlier that needs to be removed from the dataset.\nFor this reason, there is a high possibility that an outlier to be removed is detected in PC 2 as well.**","1485f9aa":"## Basement features\n\n**Question: Is there a difference in the house price with and without a Basement?**","2b39f9e5":"----------------------------------------------------------------\n## Checking Outliers after removing outliers","cb412303":"<hr style=\"border: solid 3px blue;\">","cbd52f8b":"If we ask a good question and generate good derivatives from it, we will train our model further. \n\n**However, I'm not a real estate agent, and I'm not very knowledgeable about it.**","c7593637":"## Fence: Fence quality\n\n**Question: Is there a difference in house price with and without fence?**","ac117566":"## SaleCondition\nCondition of sale","673920ef":"## Preprecessing for DL\n\nCompared to Classic ML, the process of Feature Engineering is simple.\n\n**In most cases, the following steps are required.**\n* Handling missing values\n* Encoding for categorical features (one-hot encoding is mainly used).\n* Standard or Min-Max Scaling","91df6204":"## Imputing Numerical Missing Valuse","6b7b3bc4":"---------------------------------------------------------------------------------------\n# Checking Dataset before Modeling","76afec45":"## MasVnrType: Masonry veneer type\n\n**Question: Is there a difference in house price with and without Masonry veneer?**","ece3f5e5":"<span style=\"color:Blue\"> Observation:\n\nObserving the above figures, it can be seen that many of the points previously judged to be outliers have disappeared. Assuming that the outlier has been removed to some extent, let's try another feature engineer.\nOf course, we can delete outliers directly by looking at each graph, but in doing so, our model reads a lot of generality. The method using PCA is reasonable and can be used generally.\nIt can be used for this problem as well as other problems.\n\nAlso, as the outliers are removed, the regression line seems to be well-fitted to more general data.","ba69835c":"-----------------------------------------------------\n# Checking Ordinal Features\nIn some cases, it is easy to judge that there is an order on a commonsense level. However, there are many cases where it is difficult to judge that there is an order. The method used in this notebook to determine whether the features are ordinal or not was determined to have a certain order through visualization. However, if you have real estate knowledge, you will be able to determine the order of each level by classifying ordinal features smarter than me.\n\n\n\n","b60a04c3":"## MasVnrType\nMasonry veneer type","07c32a0e":"## Setting Up\n\n> This function initializes the training environment and creates the transformation pipeline. Setup function must be called before executing any other function. It takes two mandatory parameters: data and target. All the other parameters are optional.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","2ff29da4":"------------------------------------------------------------------------\n## Filling missing values\n\nA good way to fill in the missing values of categorical features in the absence of domain-knowledge is to take the most-frequent strategy.","7f8ca8a5":"## Question 2: Is the total number of bathrooms correlated with the sale price?","3701f935":"## Garage Features\n\n**Question: Is there a difference in house price with and without Garage?**","df558345":"----------------------------------\n# Let's predict future house price\n\n![](https:\/\/insightimi.files.wordpress.com\/2020\/07\/on-de793_201909_g_20190830121038.gif)\n\nPicture Credit: https:\/\/insightimi.files.wordpress.com\n\nNow, let's use the YearBuilt and YearRemodAdd features to predict the future house price using prophet.","01ac35bf":"--------------------------------------------------\n# EDA","84235c38":"## Question 3: Can the combined area of the 1st and 2nd floors affect the sale price?\n* 1stFlrSF: First Floor square feet \n* 2ndFlrSF: Second floor square feet","978a30d2":"-------------------------------------------------------\n## Visualizing Training Dataset after Dimension Reduction\nThe preprocessed training dataset is 202 dimensions. After reducing the dimensions to 2D and 3D, let's check the distribution for each price.\n\nBy drawing this picture, we can approximately verify that our preprocessing was done correctly.","7be9413a":"> Logarithm function increases the spacing between small numbers and reduces the spacing between large numbers. When certain features are dense with values in small values, by increasing these intervals, our models increase the intervals for small values, and we can improve the performance of the model when training and testing using these values.\n\nRef: https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-linear-nonlinear-scaling\n\n<span style=\"color:Blue\"> Observation:\n    \n* If you look at the regression plot between GrLivArea and SalePrice, you can see that a clearer regression line is drawn after conversion. Log transform will definitely help with learning.\n* Skewness was also improved.    ","fe358bcd":"## Question 1: Does the combination of underground and above-ground area have a high correlation with the Sale Price?\n\n* TotalBsmtSF: Total square feet of basement area\n* GrLivArea: Above grade (ground) living area square feet","944204ca":"**Let's remove the outliers detected above.**","8f12f86c":"-----------------------------------------------------------------------------\n## Continous Features\n\nContinuous variables are numeric variables that have an infinite number of values between any two values. A continuous variable can be numeric or date\/time. For example, the length of a part or the date and time a payment is received.","5e41eda8":"## Question 4: Can area per room affect the sale price?","1a7864fb":"-----------------------------------------------------------------\n## The Fist Principal Component\n\nAfter PCA, if you draw histogram and box plot for each comment, IQR based outliers are visible.\n\n","ad2585d3":"# What is Integrated Gradients\nIntegrated gradients represents the integral of gradients with respect to inputs along the path from a given baseline to input. The integral can be approximated using a Riemann Sum or Gauss Legendre quadrature rule. Formally, it can be described as follows:\n\n![](https:\/\/captum.ai\/img\/IG_eq1.png)\n\nRef: https:\/\/captum.ai\/img\n\nIn this case, let's check what features are important for making a decision in our designed neural network.","7468a4eb":"## Exterior1st \/ Exterior2nd\nExterior covering on house","ae7a044a":"**In this notebook, we would like to organize them in the following order:**\n> **Detailed EDA and preprocessing:** During this process, it also includes the process of outlier detection using PCA and prediction of future house price using prophet.\n>\n> **Deep Learning:** Design a model using a neural network, train and predict. Also, check the feature importance from the point of view of the designed model.\n>\n> **Classic ML:** Learn classic ML models using the ensemble model and make predictions using them. After that, check the feature importance.","8ac1f994":"---------------------------\n## Removing Outlier using PCA","1270344f":"![](https:\/\/i.gifer.com\/9JKP.gif)\n\nPicture Credit: https:\/\/i.gifer.com","f1e01443":"---------------------------------------------------------------------------\n# Doing EDA for Numerical Features\n\n![](https:\/\/static-assets.codecademy.com\/Courses\/Hypothesis-Testing\/Intro_to_variable_types_4.png)\n\nPicture Credit: https:\/\/t3.ftcdn.net"}}