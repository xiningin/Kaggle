{"cell_type":{"bf3c33c6":"code","20e08ca0":"code","50fa4114":"code","93120dcb":"code","0afd5f47":"code","d90f96a2":"code","f5c4737c":"code","3727ca5b":"code","6713f48c":"code","2f807677":"code","fd748ab5":"code","5e51b7cf":"code","19671f0a":"code","b37de63a":"code","0336481a":"code","a58955a3":"code","526bf109":"code","f77a5f2a":"code","53754e27":"code","47a00e0e":"code","5b0df596":"code","45d86eea":"code","799e4021":"code","e803a7e0":"markdown","f6ed629c":"markdown","35f8a325":"markdown","a4c42275":"markdown","8e2f8674":"markdown","0ea6cad2":"markdown","5945083a":"markdown","00ac09b0":"markdown","62488e57":"markdown","b7661b09":"markdown","641d1c55":"markdown","7206d2ee":"markdown","777224db":"markdown","384b37bb":"markdown","ff5c00fe":"markdown"},"source":{"bf3c33c6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras import backend as K\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.utils import plot_model\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","20e08ca0":"dataset = pd.read_csv('..\/input\/uci-credit-approval-data-set\/UCI_crx.csv')\ndataset.shape","50fa4114":"dataset.head()","93120dcb":"dataset.dtypes","0afd5f47":"for col in ['A1', 'A2','A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13', 'A14', 'A16']:\n    dataset[col] = pd.Categorical(dataset[col])\n    dataset[col] = dataset[col].cat.codes","d90f96a2":"dataset.dtypes","f5c4737c":"dataset.head()","3727ca5b":"# create scaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndataset = pd.DataFrame(scaler.fit_transform(dataset))\n\ndataset.describe()","6713f48c":"X=dataset.iloc[:,0:15].values   #0:15\nY=dataset.iloc[:,15:16].values","2f807677":"np.random.seed(42) # Makes the random numbers predictable for easy comparison of models","fd748ab5":"## Set these parameter before calling create_model function\ndepthOfNetwork = 3\nneuronCountInEachLayer = [16, 9, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['sigmoid', 'relu', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\n\ndef create_model(verbose=False):\n  model = tf.keras.models.Sequential()\n  \n  if verbose:\n        print('Network configuration ',neuronCountInEachLayer)\n  \n  model.add(tf.keras.layers.Dense(neuronCountInEachLayer[0], input_dim=15, activation = activationFuncEachLayer[0], kernel_regularizer=regularizerFunc)) # First Layer\n    \n  for x in range(1, depthOfNetwork-1):\n      model.add(tf.keras.layers.Dense(neuronCountInEachLayer[x], activation = activationFuncEachLayer[x],kernel_regularizer=regularizerFunc))         # Second layer onwards\n \n  model.add(tf.keras.layers.Dense(neuronCountInEachLayer[depthOfNetwork-1], activation = activationFuncEachLayer[depthOfNetwork-1]))  # Output layer\n    \n  model.compile(loss = lossFunction , optimizer = 'adam' , metrics = ['accuracy'] ) \n        \n  return model","5e51b7cf":"depthOfNetwork = 3\nneuronCountInEachLayer = [17, 8, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['sigmoid', 'relu', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nmodel=create_model()\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","19671f0a":"def evaluateTheModel(verbose=False):\n    n_split=5\n    f1_scores = []\n\n    for train_index,test_index in StratifiedKFold(n_split).split(X, Y):      # StratifiedKFold, KFold\n        x_train,x_test=X[train_index],X[test_index]\n        y_train,y_test=Y[train_index],Y[test_index]\n\n        model=create_model(verbose)\n        model.fit(x_train, y_train,epochs=100, verbose=0)\n        evaluationMetrics = model.evaluate(x_test,y_test, verbose=0)\n        \n        if verbose:\n            print('Model evaluation ',evaluationMetrics)   # This returns metric values for the evaluation\n\n        y_pred = np.where(model.predict(x_test) > 0.5, 1, 0)\n        f1 = f1_score(y_test, y_pred , average=\"macro\")\n\n        if verbose:\n            print('F1 score is ', f1)\n        \n        f1_scores.append(f1)\n    \n    return np.mean(f1_scores)","b37de63a":"depthOfNetwork = 2\nneuronCountInEachLayer = [2, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['sigmoid', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nevaluateTheModel(True)","0336481a":"depthOfNetwork = 2\nneuronCountInEachLayer = [15, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['sigmoid', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nfor i in range (3, 20):\n    neuronCountInEachLayer = [i, 1]\n    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel())) ","a58955a3":"depthOfNetwork = 2\nneuronCountInEachLayer = [15, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['relu', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nfor i in range (3, 20):\n    neuronCountInEachLayer = [i, 1]\n    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel())) ","526bf109":"depthOfNetwork = 2\nneuronCountInEachLayer = [15, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nfor i in range (3, 20):\n    neuronCountInEachLayer = [i, 1]\n    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel())) ","f77a5f2a":"depthOfNetwork = 2\nneuronCountInEachLayer = [15, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'mean_squared_error'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nfor i in range (3, 20):\n    neuronCountInEachLayer = [i, 1]\n    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel())) ","53754e27":"depthOfNetwork = 3\nneuronCountInEachLayer = [18, 9, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['tanh', 'tanh', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nfor i in range (15, 16):\n    for j in range (3, 20):\n        neuronCountInEachLayer = [i, j, 1]\n        print(\"'Neurons [% 3d, % 3d], Mean F1 score : % 10.5f\" %(i, j, evaluateTheModel())) ","47a00e0e":"depthOfNetwork = 4\nneuronCountInEachLayer = [15, 8, 5, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['tanh', 'tanh', 'tanh','sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nprint(\"'Neurons [% 3d, % 3d, % 3d], Mean F1 score : % 10.5f\" %(3, 4, 3, evaluateTheModel())) ","5b0df596":"depthOfNetwork = 2\nneuronCountInEachLayer = [15, 1]                                 # try different depth and width\nactivationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nprint(\"'Mean F1 score : % 10.5f\" %(evaluateTheModel())) ","45d86eea":"depthOfNetwork = 2\nneuronCountInEachLayer = [15,1]                                 # try different depth and width\nactivationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l1(0)                       # try l1 and l2 with different lambda\n\nfor i in range(-5,5):\n    regularizerFunc = tf.keras.regularizers.l1(10**i)\n    print(\"'Regularizor : l1 with lambda : % 10.5f , Mean F1 score : % 10.5f\" %(10**i, evaluateTheModel()))","799e4021":"depthOfNetwork = 2\nneuronCountInEachLayer = [15,1]                                 # try different depth and width\nactivationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\nlossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\nregularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n\nfor i in range(-5,5):\n    regularizerFunc = tf.keras.regularizers.l2(10**i)\n    print(\"'Regularizor : l2 with lambda : % 10.5f , Mean F1 score : % 10.5f\" %(10**i, evaluateTheModel()))","e803a7e0":"**L1 regularizer with different lambda values**","f6ed629c":"**Experiment with chaning 2nd layer neuron count while keeping width to 15 neurons**","35f8a325":"**Normalizing input and output vectors**\n\nScale and normalize to make the magnitude of the features similar. Most popular methods for neumerical data are Min Max normalization and Standard normalization (z score). This will improve the model performance and convergence.\n\nhttps:\/\/visualstudiomagazine.com\/articles\/2014\/01\/01\/how-to-standardize-data-for-neural-networks.aspx\nhttps:\/\/machinelearningmastery.com\/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling\/","a4c42275":"**Define the evaluator function**","8e2f8674":"**Network configuration selected from the above is as below**","0ea6cad2":"**Imports and workspace setting**","5945083a":"**L2 regularizer with different lambda values**","00ac09b0":"**Converting data types to tensor supported data types**","62488e57":"**Verbose mode of the function to see F1 scores of each fold**","b7661b09":"**Visualizing a single model for ease of understanding**","641d1c55":"**Create the model generator function**\n\nDeciding neuron count in each layer and number of hidden layers is difficult and there is no straightforward answer to this.\n\nhttps:\/\/www.heatonresearch.com\/2017\/06\/01\/hidden-layers.html\n\nhttps:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw","7206d2ee":"**Experiment with chaning the first layer neuron count**","777224db":"**Loading data**","384b37bb":"**Trying 3 layer setup**","ff5c00fe":"**Adding regularization to minimize overfitting**\n\nhttps:\/\/machinelearningmastery.com\/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras\/\n\nhttps:\/\/stats.stackexchange.com\/questions\/431898\/l2-lambdas-in-keras-regularizers"}}