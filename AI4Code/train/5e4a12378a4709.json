{"cell_type":{"6df5190f":"code","4d1d9db6":"code","f8965109":"code","aa90d0f9":"code","dd68a1a8":"code","8ee34a8c":"code","ba3474a1":"code","b069e6fe":"code","4e9a3567":"code","1a2ec4d8":"code","50bd56bf":"code","9475164b":"code","41b0883e":"code","873d641b":"code","754c8656":"code","26776ace":"code","325f58af":"code","92464352":"code","3844f0d4":"code","a20b2a57":"code","b4b5e63f":"code","fe9b82e5":"code","d5f33d63":"code","6b848a14":"code","666ef06c":"code","c4c30fb2":"code","dcd37de3":"code","0ad915e7":"code","0cebd01a":"code","ddc75b47":"code","32c947b9":"code","fe9e80b5":"code","297ba9cb":"code","d14033e8":"code","07b00974":"code","2c4b3b90":"code","37cd9a73":"code","cbe14805":"code","c4b55c22":"code","2d13dc33":"code","796479dd":"code","976f9953":"code","0507fd6a":"code","283a08c6":"code","14289358":"code","dfbeb189":"code","bba7a1f5":"markdown","097aa274":"markdown","5f4db519":"markdown","feb23d24":"markdown","b08bfcdc":"markdown","81a22c64":"markdown","0f0c573b":"markdown","de95875d":"markdown","0fb92bbe":"markdown","b53128b9":"markdown","f6910189":"markdown","daccfd45":"markdown","5452641a":"markdown","b0483972":"markdown","7f3c9f85":"markdown","14d37375":"markdown","b129bc23":"markdown","16ff282b":"markdown","2a70c819":"markdown","02c78cd8":"markdown","b7e3e17d":"markdown","d3a33867":"markdown","61865175":"markdown","274d708c":"markdown","76a001b9":"markdown","26a24354":"markdown","a7238292":"markdown","6ef250be":"markdown","d164462b":"markdown","3edae0af":"markdown","88995483":"markdown","62e2ed44":"markdown","aea0b738":"markdown","024e60d3":"markdown"},"source":{"6df5190f":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom catboost import Pool, CatBoostClassifier, cv\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import precision_score,make_scorer,roc_auc_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/creditcardfraud\/creditcard.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4d1d9db6":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","f8965109":"pd.set_option(\"display.float\", \"{:.2f}\".format)\ndf.describe()","aa90d0f9":"miss = df.isnull().sum()\nmiss_percnt = (miss\/len(df))*100\npd.concat([miss,miss_percnt], axis=1, keys=['Missing Values','Missing Percentage'])","dd68a1a8":"print('There are 492 frauds and the rest are genuine transactions in the dataset. \\n1 represents fraud and 0 represents  Genuine')\ndf['Class'].value_counts()","8ee34a8c":"fraud = df[(df['Class'] != 0)]\nnormal = df[(df['Class'] == 0)]\n\ntrace = go.Pie(labels = ['Normal', 'Fraud'], values = df['Class'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightskyblue','gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of normal and fraud tansactions')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","ba3474a1":"timedelta = pd.to_timedelta(df['Time'], unit='s')\ndf['Time_min'] = (timedelta.dt.components.minutes).astype(int)\ndf['Time_hour'] = (timedelta.dt.components.hours).astype(int)","b069e6fe":"class_0 = df.loc[df['Class'] == 0][\"Time_hour\"]\nclass_1 = df.loc[df['Class'] == 1][\"Time_hour\"]\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time\/Hour Density Plot', xaxis=dict(title='Time [Hr]'))\npy.iplot(fig, filename='dist_only_Hr')","4e9a3567":"class_0 = df.loc[df['Class'] == 0][\"Time_min\"]\nclass_1 = df.loc[df['Class'] == 1][\"Time_min\"]\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time\/Hour Density Plot', xaxis=dict(title='Time [Min]'))\npy.iplot(fig, filename='dist_only_min')","1a2ec4d8":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(time_val, ax=ax[0], bins=200, color='darkslategray')\nax[0].set_title('Distribution of Transaction Time', fontsize=14)\nax[0].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(amount_val, ax=ax[1], bins=85, color='darkred')\nax[1].set_title('Distribution of Transaction Amount', fontsize=14)\nax[1].set_xlim([min(amount_val - 1000), max(amount_val)])\n\nplt.show()","50bd56bf":"fig, axs = plt.subplots(ncols=2, figsize=(18,4))\n\nsns.distplot(df[(df['Class'] == 1)]['Time'], bins=100, color='saddlebrown', ax=axs[0])\naxs[0].set_title(\"Distribution of Fraud Transactions\")\n\nsns.distplot(df[(df['Class'] == 0)]['Time'], bins=100, color='purple', ax=axs[1])\naxs[1].set_title(\"Distribution of Genuine Transactions\")\n\nplt.show()","9475164b":"fig, axs = plt.subplots(nrows=2,sharex=True,figsize=(16,6))\n\nsns.scatterplot(x='Time',y='Amount', data=df[df['Class']==1], ax=axs[0])\naxs[0].set_title(\"Distribution of Fraud Transactions\")\n\nsns.scatterplot(x='Time',y='Amount', data=df[df['Class']==0], ax=axs[1])\naxs[1].set_title(\"Distribution of Genue Transactions\")\n\nplt.show()","41b0883e":"sns.set_style('whitegrid')\nax = sns.lmplot(y=\"Amount\", x=\"Time_min\", fit_reg=False,aspect=1.8,\n                data=df, hue='Class')\nplt.title(\"Amounts by Minutes of Frauds and Normal Transactions\",fontsize=16)\nplt.show()","873d641b":"ax = sns.lmplot(y=\"Amount\", x=\"Time_hour\", fit_reg=False,aspect=1.8,\n                data=df, hue='Class')\nplt.title(\"Amounts by Hour of Frauds and Normal Transactions\", fontsize=16)\n\nplt.show()\ndf.drop(['Time_hour', 'Time_min'], axis=1, inplace=True)","754c8656":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16,7))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=False)\nplt.show();","26776ace":"# cols plot  \ncols_names = df.drop(['Class', 'Amount', 'Time'], axis=1)\nidx = 0\n\n# Spliting classes\nfraud = df[df['Class']==1]\nnormal = df[df['Class']==0]\n\n# figure plot  \nfig, ax = plt.subplots(nrows=7, ncols=4, figsize=(18,18))\nfig.subplots_adjust(hspace=1, wspace=1)\n\nfor col in cols_names:\n    idx += 1\n    plt.subplot(7, 4, idx)\n    sns.kdeplot(fraud[col], label=\"Normal\", color='blue', shade=True)\n    sns.kdeplot(normal[col], label=\"Fraud\", color='orange', shade=True)\n    plt.title(col, fontsize=11)\n    plt.tight_layout()","325f58af":"f, (ax1, ax2) = plt.subplots(1,2,figsize =( 18, 8))\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap((df.loc[df['Class'] ==1]).corr(), vmax = .8, square=True, ax = ax1, cmap = 'afmhot', mask=mask);\nax1.set_title('Fraud')\nsns.heatmap((df.loc[df['Class'] ==0]).corr(), vmax = .8, square=True, ax = ax2, cmap = 'YlGnBu', mask=mask);\nax2.set_title('Normal')\nplt.show()","92464352":"rob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time', 'Amount'], axis=1, inplace=True)\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\ntarget = ['Class']\nfeat_cols = ['scaled_time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'scaled_amount']","3844f0d4":"sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\nfor train_index, test_index in sss.split(X, y):\n  X_train_full, X_test = X.iloc[train_index], X.iloc[test_index]\n  y_train_full, y_test = y.iloc[train_index], y.iloc[test_index]\n\nsss2 = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\nfor train_idx, test_idx in sss2.split(X_train_full, y_train_full):\n  X_train, X_valid = X_train_full.iloc[train_idx], X_train_full.iloc[test_idx]\n  y_train, y_valid = y_train_full.iloc[train_idx], y_train_full.iloc[test_idx]","a20b2a57":"print('X_train shape: ', X_train.shape)\nprint('y_train shape: ', y_train.shape)\nprint('X_test shape: ', X_test.shape)\nprint('y_test shape: ', y_test.shape)","b4b5e63f":"## Naive Bayes\n\nModel = GaussianNB()\nModel.fit(X_train[feat_cols], y_train)\ny_pred = Model.predict(X_test[feat_cols])\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","fe9b82e5":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfit_params = {\"early_stopping_rounds\" : 50, \n             \"eval_metric\" : 'binary', \n             \"eval_set\" : [(X_test[feat_cols].values, y_test)],\n             'eval_names': ['valid'],\n             'verbose': 0,\n             'categorical_feature': 'auto'}\n\nparam_test = {'learning_rate' : [0.001, 0.005, 0.03, 0.05, 0.1],\n              'n_estimators' : [100, 300, 500, 800, 1000],\n              'num_leaves': sp_randint(6, 50), \n              'min_child_samples': sp_randint(100, 500), \n              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e2],\n              'subsample': sp_uniform(loc=0.2, scale=0.8), \n              'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n              'reg_alpha': [0, 1e-1, 1, 5, 7, 10],\n              'reg_lambda': [0, 1e-1, 1, 5, 10]}\n\n#number of combinations\nn_iter = 5 #(replace 2 by 200, 90 minutes)\n\n#intialize lgbm and lunch the search\nlgbm_clf = lgb.LGBMClassifier(random_state=42, silent=True, metric='None', n_jobs=4)\ngrid_search = RandomizedSearchCV(\n    estimator=lgbm_clf, param_distributions=param_test, \n    n_iter=n_iter,\n    scoring='accuracy',\n    cv=4, n_jobs=-1,\n    refit=True,\n    random_state=42,\n    verbose=True)\n\ngrid_search.fit(X_train[feat_cols].values, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(grid_search.best_score_, grid_search.best_params_))\n\nopt_parameters =  grid_search.best_params_\n\nclf_sw = lgb.LGBMClassifier(**lgbm_clf.get_params())\n#Optimal parameter\nclf_sw.set_params(**opt_parameters)","d5f33d63":"lgbm_clf = lgb.LGBMClassifier(colsample_bytree=0.6247240713084175, max_depth=6, metric='None',\n               min_child_samples=206, min_child_weight=1, n_estimators=1000,\n               n_jobs=4, num_leaves=44, random_state=42, reg_alpha=0.1,\n               reg_lambda=1, subsample=0.27997993265440235)\n\nlgbm_clf.fit(X_train, y_train)\nlgbm_clf.fit(X_train, y_train)\ny_pred = lgbm_clf.predict(X_test)\ny_score = lgbm_clf.predict_proba(X_test)[:,1]\nprint(classification_report(y_test,y_pred.round()))","6b848a14":"\nclf = RandomForestClassifier(max_depth=5, min_samples_split=5,min_samples_leaf=4, random_state=42)\n                            \nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","666ef06c":"Model=CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42,task_type='GPU')\nModel.fit(X_train[feat_cols],y_train,eval_set=(X_test[feat_cols],y_test))","c4c30fb2":"## CatBoost\ny_pred=Model.predict(X_test[feat_cols])\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","dcd37de3":"tmp = pd.DataFrame({'Feature': feat_cols, 'Feature importance': Model.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (10,6))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()","0ad915e7":"\nparams = {\n        'learning_rate': [0.001, 0.01, 0.05, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['binary:logistic'],\n    }\n\nclf = xgb.XGBClassifier(random_state=42, tree_method='gpu_hist')\nmodel = GridSearchCV(estimator=clf, param_grid=params, cv=3, n_jobs=-1)\nmodel.fit(X_train[feat_cols].values, y_train)\n","0cebd01a":"y_pred=model.predict(X_test[feat_cols].values)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","ddc75b47":"from imblearn.over_sampling import ADASYN\n\nads = ADASYN( random_state=42)\n\nX_ads, y_ads = ads.fit_resample(X_train_full[feat_cols].values, y_train_full)","32c947b9":"sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\nfor train_index, test_index in sss.split(X_ads, y_ads):\n  X_train_ads, X_valid_ads = X_ads[train_index], X_ads[test_index]\n  y_train_ads, y_valid_ads = y_ads[train_index], y_ads[test_index]","fe9e80b5":"## Naive Bayes\n\nModel = GaussianNB()\nModel.fit(X_train_ads, y_train_ads)\ny_pred = Model.predict(X_test[feat_cols])\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","297ba9cb":"params = {\n        'learning_rate': [0.001, 0.01, 0.05, 0.1],\n        'max_depth': [3, 5, 7],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['binary:logistic']\n    }\n\nclf = xgb.XGBClassifier(random_state=42, tree_method='gpu_hist')\nmodel = GridSearchCV(estimator=clf, param_grid=params, cv=3, n_jobs=-1)\nmodel.fit(X_train_ads, y_train_ads)","d14033e8":"y_pred=model.predict(X_test[feat_cols].values)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","07b00974":"\nclf = RandomForestClassifier(max_depth=3, min_samples_split=4,min_samples_leaf=4, random_state=42)\nclf.fit(X_train_ads, y_train_ads)\n\ny_pred = clf.predict(X_test)\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","2c4b3b90":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfit_params = {\"early_stopping_rounds\" : 50,\n             \"eval_metric\" : 'binary', \n             \"eval_set\" : [(X_valid[feat_cols].values, y_valid)],\n             'eval_names': ['valid'],\n             'verbose': 0,\n             'categorical_feature': 'auto'}\n\nparam_test = {'learning_rate' : [0.001, 0.005, 0.03, 0.05, 0.1],\n              'n_estimators' : [100, 300, 500, 800],\n              'num_leaves': sp_randint(6, 40),\n              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1e2],\n              'subsample': sp_uniform(loc=0.2, scale=0.8), \n              'max_depth': [ 3, 4, 5, 6, 7],\n              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n              'reg_alpha': [0, 1, 5, 7, 10],\n              'reg_lambda': [0, 1, 5, 10]}\n\n#number of combinations\nn_iter = 4 #(replace 2 by 200, 90 minutes)\n\n#intialize lgbm and lunch the search\nlgbm_clf = lgb.LGBMClassifier(random_state=42, silent=True, metric='None')\ngrid_search = RandomizedSearchCV(\n    estimator=lgbm_clf, param_distributions=param_test, \n    scoring='accuracy',\n    cv=4, n_jobs=-1)\n\ngrid_search.fit(X_train_ads, y_train_ads, **fit_params)\nprint('Best score reached: {} with params: {} '.format(grid_search.best_score_, grid_search.best_params_))\n\nopt_parameters =  grid_search.best_params_\n\nclf_sw = lgb.LGBMClassifier(**lgbm_clf.get_params())\n#Optimal parameter\nclf_sw.set_params(**opt_parameters)","37cd9a73":"lgbm_clf = lgb.LGBMClassifier(colsample_bytree=0.5339868613881635, learning_rate=0.05,\n               max_depth=7, metric='None', min_child_weight=0.1,\n               n_estimators=800, num_leaves=36, random_state=42, reg_alpha=0,\n               reg_lambda=1, subsample=0.9901662910177844)\n\nlgbm_clf.fit(X_train, y_train)\ny_pred = lgbm_clf.predict(X_test)\ny_score = lgbm_clf.predict_proba(X_test)[:,1]\nprint(classification_report(y_test,y_pred.round()))","cbe14805":"Model=CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42, task_type='GPU')\nModel.fit(X_train_ads,y_train_ads,eval_set=(X_valid_ads,y_valid_ads))","c4b55c22":"## CatBoost\n\ny_pred=Model.predict(X_test[feat_cols].values)\n\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","2d13dc33":"X_train_d = np.expand_dims(X_train, -1)\nX_test_d = np.expand_dims(X_test, -1)\nX_valid_d = np.expand_dims(X_valid, -1)\ny_valid_d = np.expand_dims(y_valid, -1)\ny_train_d = np.expand_dims(y_train, -1)","796479dd":"model = Sequential()\nmodel.add(Conv1D(32, 2, activation='selu', kernel_initializer=\"lecun_normal\", input_shape=X_train_d[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv1D(64, 2, activation='selu', kernel_initializer=\"lecun_normal\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(128, 2, activation='selu', kernel_initializer=\"lecun_normal\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512, activation='selu', kernel_initializer=\"lecun_normal\"))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))","976f9953":"model.summary()","0507fd6a":"optimizer = keras.optimizers.Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[keras.metrics.AUC()])\nm = model.fit(X_train_d, y_train, \n              validation_data=(X_valid_d, y_valid_d),\n              batch_size=500, \n              epochs=20,\n             )","283a08c6":"score = model.evaluate(X_test_d, y_test)\nprint(score)","14289358":"# predict probabilities for test set\nyhat_probs = model.predict(X_test_d, verbose=0)\n# predict crisp classes for test set\nyhat_classes = model.predict_classes(X_test_d, verbose=0)\n\nprecision = precision_score(y_test, yhat_classes)\nprint('\\nPrecision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_test, yhat_classes)\nprint('\\nRecall: %f' % recall)\n\nf1 = f1_score(y_test, yhat_classes)\nprint('\\nF1 score: %f' % f1)\n\nprint(\"_______________________________________________\")\nauc = roc_auc_score(y_test, yhat_probs)\nprint('\\nROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat_classes)\nprint(\"_______________________________________________\\n\")\nprint(\"Confusion Matrix\\n\")\nprint(matrix)","dfbeb189":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(m.history['loss'], label='Loss')\nplt.plot(m.history['val_loss'], label='val_Loss')\nplt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(m.history['auc_1'], label='auc_1')\nplt.plot(m.history['val_auc_1'], label='val_auc_1')\nplt.legend()","bba7a1f5":"## Random Forest","097aa274":"## Scaling and Splitting the data","5f4db519":"### Distribution of Time and Amount","feb23d24":"## Naive Bayes","b08bfcdc":"## Random Forest","81a22c64":"#### Lets check how much frauds occur per hour and per minute","0f0c573b":"## Exploratory Data Analysis","de95875d":"## Catboost","0fb92bbe":"Excluding Naive Bayes, all the models have showed promising accuracy, precision, recall, f1-score. Now, for that unbalanced problem in the dataset there is a solution, we can use undersampling or oversampling method and there is library for that. Next we will use imblearn's ADASYN over-sampler to solve the unbalance problem.","b53128b9":"## LightGBM","f6910189":"### Amount of transactions according to time","daccfd45":"## Loading the Data","5452641a":"### Density Plots of all Features","b0483972":"## Fraud Detection with CNN","7f3c9f85":"We should not be happy for high accuracy of test set. If the precision\/recall score is low then there is grave problem in the model.\n\n1. Low precision and high recall means the model detects most of the features as fraud including the genuine transactions.\n2. High precision and low recall means the model precisely detects the non-fruads but labels the frauds as genuine too.\n\nBy considering these issues, We have to build a model that can have highly balanced precision-recall\/ f1 score, for that we have to check beyond accuracy score.","14d37375":"Lets try some models and see how they can predict the frauds.","b129bc23":"## Catboost","16ff282b":"In This kernel I will be showing the steps to detect frauds.","2a70c819":"### Both Transactions Relative to Time","02c78cd8":"### Distribution of Fraud and Genuine Transactions","b7e3e17d":"## Introduction:\n\nCredit fraud is the manipulation of unauthorized card that are stolen or theft from the cardholder. It can be the card itself or the personal details from the owner. In this kernel multiple solutions is demonstrated.\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nThe dataset has pre-processed with PCA transformation except Time, Amount and Class feature.\n\nDue to confidentiality issues, there are not provided the original features and more background information about the data.\n\n* Features V1, V2, \u2026 V28 are the principal components obtained with PCA.\n* Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount.\n* Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","d3a33867":"recall is high but precision is very low.","61865175":"## Feature Importance","274d708c":"## XGBoost","76a001b9":"0% precision and 3% recall is not acceptable.","26a24354":"## Using Over-Sampling (ADASYN) Method","a7238292":"## LightGBM","6ef250be":"### Check for missing value","d164462b":"## XGBoost","3edae0af":"Both LightGBM and XGBoost showed great results with oversampler. Next we will try Convolutional Neural Network to detect the frauds.","88995483":"## Correlation Matrix","62e2ed44":"## Naive Bayes","aea0b738":"We can see from that pie chart, that relatively the fraud transaction is very very small. So, here is the problem arises because the high disparity of classes gives low precision and recall score. We have to choose our path wisely so that our model can detect the frauds wisely","024e60d3":"### Distribuiton of Amount vs Class"}}