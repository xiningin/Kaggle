{"cell_type":{"47d17748":"code","994304c5":"code","83a70299":"code","468c8e42":"code","7a79d28e":"code","1a9d535f":"code","d1960ae1":"code","c4228d7d":"code","23ae9ab6":"code","7444cb83":"code","ed6e468d":"code","a2f20330":"code","1d3a932d":"code","927ca599":"code","df8e44a4":"code","f222d90e":"code","8de3ed5b":"code","44f055af":"code","a72d0fb5":"code","39017eb2":"code","efee10bc":"code","055f5b89":"code","43ca8939":"code","470ff8f6":"code","b074386c":"code","5f584d61":"code","642765ea":"code","66c55fdf":"code","d1e0dad1":"code","901452cd":"code","5e7eb8cc":"code","11da6850":"code","3233f2fb":"code","4121409f":"code","eaa0bf9a":"code","259af500":"code","76ec1edb":"code","93fa4525":"code","4f3e2872":"code","ade38125":"code","ea4accf2":"code","48c8cc59":"code","c50680b2":"code","9c3e6a65":"code","fd6d3d1a":"code","1c49ca84":"code","984e1462":"code","2b665e3a":"code","3310bb70":"code","d3ce4ca2":"code","083cba79":"code","0c74bc1d":"code","4afdedd6":"code","8195497c":"code","b928fc18":"code","061fe98d":"code","3d33ad72":"code","58f8735c":"code","dfe0bffd":"code","32c39819":"code","776aa40f":"code","4b6fb4e0":"code","3f07e74a":"code","c3d5a6bb":"code","e1be825e":"code","cce2568c":"code","37975735":"code","595334f9":"code","a1251bb4":"code","3c4d7dfe":"code","c2244d66":"code","fd0b6285":"code","88488882":"code","cd9ad48e":"code","e43fba6b":"code","9d43abef":"code","4b4646d8":"code","da3ddf0f":"code","148403df":"code","c95beb33":"code","d1a47b63":"code","db7ae54b":"code","9a43a4ec":"code","fa8018c2":"code","6c5b7da2":"code","d90bc2a4":"code","35546510":"code","67768107":"code","8ba4f6ae":"code","6085dba2":"code","056552b3":"code","054054a8":"code","ad35d040":"code","cb282d92":"code","099c40d2":"code","ae222280":"code","2c82edf0":"code","c49039e6":"code","75f6d47a":"code","e0858c0a":"code","4d61be49":"code","44fba289":"code","4e5b00f4":"code","68a41a6f":"code","cd52701d":"code","b94c8649":"code","4fef15ba":"code","d925daea":"code","fc2f524d":"code","ff1af998":"code","c3e970c4":"code","3aaea9c5":"code","4f3b4f77":"code","20f39158":"code","96bb56a6":"code","aac30cbe":"code","ab7d011a":"code","ec268730":"code","85c8394f":"code","6f0dc270":"code","81ce74e6":"code","3a2eb726":"code","52703f5a":"code","304b5936":"code","1e5dd6ee":"code","0be9ef11":"code","509a2f8f":"code","af0f7727":"code","4381832a":"code","585fa9ca":"code","f4b0227f":"code","aec18e38":"code","b003831f":"code","85ea9b7e":"code","0586aace":"code","219cf9d7":"code","2c2652f7":"code","f3df4b11":"code","d875f233":"code","733ba894":"code","6253ec2a":"code","67e31fbb":"code","db024180":"code","aee53ba5":"code","2cedc914":"code","92b6595d":"code","9af25abc":"code","05dcdb45":"code","3e8160aa":"code","81c06899":"code","045ab278":"code","1d5933f1":"code","f0fff97f":"code","4cd4800b":"code","78751aaa":"code","954a3680":"markdown","9fd364c5":"markdown","7080e879":"markdown","27d92274":"markdown","97ef58d2":"markdown","0c1548d8":"markdown","a4c14825":"markdown","86752bb4":"markdown","b2d1b0d1":"markdown","9e692a39":"markdown","70902090":"markdown","f4ea72e1":"markdown","4edab1fa":"markdown","28b7a469":"markdown","337de255":"markdown","79fbbe1b":"markdown","cdd065c6":"markdown","1716fa02":"markdown","3b51206c":"markdown","84dd1e5d":"markdown","16a8b705":"markdown","c1989a90":"markdown","6b1165f3":"markdown","21023f5d":"markdown","087cf363":"markdown","241ead4c":"markdown","21f6c0fd":"markdown","4309d62b":"markdown","128fddf7":"markdown","ee5fcbc3":"markdown","2b24119f":"markdown","ee811cde":"markdown","30f7591b":"markdown","7be3d52f":"markdown","0288567d":"markdown","b1080994":"markdown","612de192":"markdown","93c2b449":"markdown","793a3b29":"markdown","19a561be":"markdown","7c5da012":"markdown","e2240b6a":"markdown","bb64278a":"markdown","4e3331cb":"markdown","64573ceb":"markdown","d62f10fe":"markdown","092d4cd9":"markdown","50758a33":"markdown","d123f7c4":"markdown","5762eb8d":"markdown","2af15247":"markdown","fb219579":"markdown","5343132f":"markdown","bdcaecbc":"markdown","843e4ff9":"markdown","7be282aa":"markdown","bb700b2b":"markdown","be60fb48":"markdown","7ae78007":"markdown","0f3aa286":"markdown","66cf3c41":"markdown","3025e887":"markdown","6283752e":"markdown","90cc0fd7":"markdown","e61a4bef":"markdown","53269763":"markdown","1315e167":"markdown","948690f0":"markdown","a74587b4":"markdown","4d5737fb":"markdown","2c6e4c49":"markdown","d4d479cd":"markdown"},"source":{"47d17748":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nplt.rcParams['figure.figsize'] = [15, 8]\n\nfrom scipy import stats\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","994304c5":"\ndf = pd.read_csv('..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndf.head()","83a70299":"df.shape","468c8e42":"df.info()","7a79d28e":"df.rename(columns={\"default.payment.next.month\":\"default\"}, inplace=True)\ndf.rename(columns={\"LIMIT_BAL\":\"limit_bal\"}, inplace=True)\ndf.rename(columns={\"SEX\":\"sex\"}, inplace=True)\ndf.rename(columns={\"EDUCATION\":\"education\"}, inplace=True)\ndf.rename(columns={\"MARRIAGE\":\"marriage\"}, inplace=True)\ndf.rename(columns={\"AGE\":\"age\"}, inplace=True)\ndf.rename(columns={\"PAY_0\":\"pay_1\"}, inplace=True)\ndf.rename(columns={\"PAY_2\":\"pay_2\"}, inplace=True)\ndf.rename(columns={\"PAY_3\":\"pay_3\"}, inplace=True)\ndf.rename(columns={\"PAY_4\":\"pay_4\"}, inplace=True)\ndf.rename(columns={\"PAY_5\":\"pay_5\"}, inplace=True)\ndf.rename(columns={\"PAY_6\":\"pay_6\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT1\":\"bill_amt1\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT2\":\"bill_amt2\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT3\":\"bill_amt3\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT4\":\"bill_amt4\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT5\":\"bill_amt5\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT6\":\"bill_amt6\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT1\":\"pay_amt1\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT2\":\"pay_amt2\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT3\":\"pay_amt3\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT4\":\"pay_amt4\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT5\":\"pay_amt5\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT6\":\"pay_amt6\"}, inplace=True)","1a9d535f":"\ndf.describe().T","d1960ae1":"#check missing values\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","c4228d7d":"## Unique values in each categorical columns\n\nprint(\"Sex\", df.sex.unique())\nprint('Education', df.education.unique())\nprint('Pay_0', df.pay_1.unique())\nprint('Marriage', df.marriage.unique())\nprint('Default', df.default.unique())","23ae9ab6":"fill = (df.education == 0) | (df.education == 5) | (df.education == 6)\ndf.loc[fill, 'education'] = 4","7444cb83":"#Replace 1 by 0 for male and 2 by 1 in Sex deature\ndf.sex.replace(1,0,inplace=True)\ndf.sex.replace(2,1,inplace=True)\nprint('Sex', df.sex.unique())","ed6e468d":"df.head()","a2f20330":"## Map categorical data\n\ndf.sex = df.sex.map({0:'Male', 1:'Female'})\ndf.default = df.default.map({0:'No', 1:'Yes'})\n\ndf.education = df.education.map({1:'Graduate School',\n                                                          2:'University',\n                                                          3:'High School',\n                                                          4:'Others'})\n\ndf.marriage = df.marriage.map({1:'Married',\n                                                        2:'Single',\n                                                        3:'Divorced',\n                                                        0: 'Others'})","1d3a932d":"def plot_categorical_count(df, col, hue=None, hue_order=None, is_pct=True, figsize=(12,6)):\n    \n    plt.figure(figsize=figsize)\n    g = sns.countplot(data=df, x=col, hue=hue, hue_order=None)\n    for rect in g.patches:\n        h = rect.get_height()\n        w = rect.get_width()\n        x = rect.get_x()\n        y = rect.get_y()\n        g.annotate(f\"{h}\", (x+w\/2, h), va='bottom', ha='center', size=14)\n        \n    g.spines['top'].set_visible(False)\n    g.spines['left'].set_visible(False)\n    g.spines['right'].set_visible(False)\n    \n    plt.show()\n    \n    if is_pct:\n        print()\n        print(\"Percentage share of each category:\")\n        print(df[col].value_counts(normalize=True)*100)","927ca599":"plot_categorical_count(df, 'default')","df8e44a4":"plot_categorical_count(df, 'sex')","f222d90e":"plot_categorical_count(df, 'education')","8de3ed5b":"plot_categorical_count(df, 'marriage')","44f055af":"def default_df(df, col):\n    df1 = pd.crosstab(index=df[col], columns=df.default, margins=True)\n    df1.rename(columns={'No':'total_no_default', 'Yes':'total_yes_default', 'All':'total_default'}, inplace=True)\n    \n    df2 = pd.crosstab(index=df[col], columns=df.default, normalize='index', margins=True)\n    df2.rename(columns={'No':'pct_no_default', 'Yes':'pct_yes_default'}, inplace=True)\n\n    df3=pd.crosstab(index=df[col], columns=df.default, normalize='all', margins=True)\n    df3.rename(columns={'No':'pct_total_no_default', 'Yes':'pct_total_yes_default'}, inplace=True)\n    final_df = pd.concat([df1, df2,df3], axis=1)\n    \n    return final_df","a72d0fb5":"plot_categorical_count(df, col='sex', hue='default', is_pct=False)","39017eb2":"default_df(df, 'sex').drop(labels='All', axis=0).sort_values(by=['pct_total_yes_default','pct_yes_default'],ascending=False)","efee10bc":"plot_categorical_count(df, col='education', hue='default', is_pct=False)\n\ndefault_df(df, 'education').sort_values(by='pct_yes_default',ascending=False)","055f5b89":"plot_categorical_count(df, col='marriage', hue='default', is_pct=False)\n\ndefault_df(df, 'marriage')","43ca8939":"pd.crosstab(df.pay_1, df.default, margins=True)","470ff8f6":"default_df(df, 'pay_1').sort_values(by='pct_total_yes_default',ascending=False)","b074386c":"## Balance Limit\n\nsns.histplot(data=df, x='limit_bal', hue='default', \n             kde=True, line_kws={'ls':'--', 'lw':2})\nplt.show()","5f584d61":"sns.boxplot(data=df, x='default', y='limit_bal')\nplt.show()","642765ea":"df.groupby('default')['limit_bal'].agg(['mean', 'median', 'std'])","66c55fdf":"## hypothesis test to check whether average balance for dafaulters and non-defaulters are same\n\nres = stats.ttest_ind(df.limit_bal.loc[df.default=='Yes'], \n                df.limit_bal.loc[df.default=='No'])\n\nprint(f\"P-Value: {res[1]:.3f}\")","d1e0dad1":"## Age \n\nsns.histplot(data=df, x='age', hue='default', \n            kde=True, line_kws={'ls':'--', 'lw':2})\nplt.show()","901452cd":"sns.boxplot(data=df, x='default', y='age')\nplt.show()","5e7eb8cc":"## hypothesis test to check whether average age for dafaulters and non-defaulters are same\n\nres = stats.ttest_ind(df.age.loc[df.default=='Yes'], \n                df.age.loc[df.default=='No'])\n\nprint(f\"P-Value: {res[1]:.3f}\")","11da6850":"age_df = pd.crosstab(df.age, df.default, normalize='index')\n\nplt.bar(x=age_df.index, height=age_df.No, label='Non-Defaulter')\nplt.bar(x=age_df.index, height=age_df.Yes, bottom=age_df.No, label='Defaulter')\n\nplt.xticks(ticks=range(20,81))\nplt.xlabel(\"Age\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percentage of Defaulters and Non-Defaulters\")\n\nplt.legend()\nplt.show()","3233f2fb":"corr = df.corr(method='spearman')\nplt.subplots(figsize=(30,10))\nsns.heatmap( corr, square=True, annot=True, fmt=\".1f\" )","4121409f":"corr_matrix=df.corr(method='spearman')\ncorr_matrix.iloc[:-1, -1].plot.bar(color='orange')\nplt.xlabel(\"Features\")\nplt.ylabel(\"Correlation\")\nplt.title(\"Correlation of default feature with all other features\", fontdict={'size':16})\nplt.show()\n","eaa0bf9a":"df = pd.read_csv('..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndf.head()","259af500":"df.rename(columns={\"default.payment.next.month\":\"default\"}, inplace=True)\ndf.rename(columns={\"LIMIT_BAL\":\"limit_bal\"}, inplace=True)\ndf.rename(columns={\"SEX\":\"sex\"}, inplace=True)\ndf.rename(columns={\"EDUCATION\":\"education\"}, inplace=True)\ndf.rename(columns={\"MARRIAGE\":\"marriage\"}, inplace=True)\ndf.rename(columns={\"AGE\":\"age\"}, inplace=True)\ndf.rename(columns={\"PAY_0\":\"pay_1\"}, inplace=True)\ndf.rename(columns={\"PAY_2\":\"pay_2\"}, inplace=True)\ndf.rename(columns={\"PAY_3\":\"pay_3\"}, inplace=True)\ndf.rename(columns={\"PAY_4\":\"pay_4\"}, inplace=True)\ndf.rename(columns={\"PAY_5\":\"pay_5\"}, inplace=True)\ndf.rename(columns={\"PAY_6\":\"pay_6\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT1\":\"bill_amt1\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT2\":\"bill_amt2\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT3\":\"bill_amt3\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT4\":\"bill_amt4\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT5\":\"bill_amt5\"}, inplace=True)\ndf.rename(columns={\"BILL_AMT6\":\"bill_amt6\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT1\":\"pay_amt1\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT2\":\"pay_amt2\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT3\":\"pay_amt3\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT4\":\"pay_amt4\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT5\":\"pay_amt5\"}, inplace=True)\ndf.rename(columns={\"PAY_AMT6\":\"pay_amt6\"}, inplace=True)","76ec1edb":"fill = (df.education == 0) | (df.education == 5) | (df.education == 6)\ndf.loc[fill, 'education'] = 4","93fa4525":"#Replace 1 by 0 for male and 2 by 1 in Sex deature\ndf.sex.replace(1,0,inplace=True)\ndf.sex.replace(2,1,inplace=True)\nprint('Sex', df.sex.unique())","4f3e2872":"df.sex=df.sex.astype('category')\ndf.education=df.education.astype('category')\ndf.marriage=df.marriage.astype('category')\n","ade38125":"df.pay_2.value_counts()","ea4accf2":"rows_pay = df.apply(lambda x: True if x['pay_1']>=3 or x['pay_2']>=3 or x['pay_3']>=3 or x['pay_4']>=3 or x['pay_5']>=3 or x['pay_6']>=3 else False , axis=1)\nrows_pay_count = len(rows_pay[rows_pay == True].index)\nprint(\"number of rows that have values higher or equal to 3\", rows_pay_count)","48c8cc59":"pay = ['pay_1','pay_2','pay_3','pay_4','pay_5','pay_6']\nfor pay_i in pay:\n  df.drop(df.loc[df[pay_i]>=3].index, inplace = True)\nprint('Pay_1', df.pay_1.unique())\nprint('Pay_2', df.pay_2.unique())\nprint('Pay_3', df.pay_3.unique())\nprint('Pay_4', df.pay_4.unique())\nprint('Pay_5', df.pay_5.unique())\nprint('Pay_6', df.pay_6.unique())","c50680b2":"#Visulize Corrolation Between features\ncorr = df.corr('spearman')\nplt.subplots(figsize=(40,20))\nsns.heatmap( corr, square=True, annot=True, fmt=\".1f\" )","9c3e6a65":"pca = PCA().fit(df[['bill_amt1','bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5']])\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n","fd6d3d1a":"pca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(df[['bill_amt1','bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5']])\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['bill_amt_pc1', 'bill_amt_pc2'])\nprincipalDf","1c49ca84":"df_dep=df.copy()\ndf.drop(columns=['bill_amt1','bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5','bill_amt6'],inplace=True)","984e1462":"df = pd.concat([df,principalDf], axis = 1)","2b665e3a":"df = df[['limit_bal', 'sex', 'education', 'marriage', 'age', 'pay_1', 'pay_2',\n       'pay_3', 'pay_4', 'pay_5', 'pay_6', 'pay_amt1', 'pay_amt2', 'pay_amt3',\n       'pay_amt4', 'pay_amt5', 'pay_amt6', 'bill_amt_pc1',\n       'bill_amt_pc2', 'default']]\n","3310bb70":"df","d3ce4ca2":"#check for duplicates per row: features + target\ndf.duplicated(keep='first').to_frame(name='isduplicated').value_counts().to_frame(name='Nb').plot.pie(y='Nb',autopct='%1.1f%%',title='duplicates per row')\n#check for duplicates in features\ndf.iloc[:, :-1].duplicated(keep='first').to_frame(name='isduplicated').value_counts().to_frame(name='Nb').plot.pie(y='Nb',autopct='%1.1f%%',title='duplicates in features')","083cba79":"#remove duplicates per row\ndf.drop_duplicates(inplace=True)\n#remove duplicates in features\ncol = df.iloc[:,:-1].columns\ndf.drop_duplicates(subset=col,keep='first',inplace=True,ignore_index=True)","0c74bc1d":"df=df.dropna()","4afdedd6":"#get dummies for education and marriage\ndf_dc = pd.get_dummies(df, columns=['education','marriage'],drop_first=True)\ndf_dc","8195497c":"df_dc = df_dc[['limit_bal', 'sex', 'age', 'pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5',\n       'pay_6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5',\n       'pay_amt6', 'bill_amt_pc1', 'bill_amt_pc2', 'education_2',\n       'education_3', 'education_4', 'marriage_1', 'marriage_2', 'marriage_3', 'default']]","b928fc18":"df_dc","061fe98d":"# Check Data unbalance\ntemp = df_dc[\"default\"].value_counts()\ndf_ud = pd.DataFrame({'default': temp.index,'values': temp.values})\nplt.figure(figsize = (6,6))\nplt.title('Default Credit Card Clients - target value - data unbalance\\n (Default = 0, Not Default = 1)')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'default', y=\"values\", data=df_ud)\nlocs, labels = plt.xticks()\nplt.show()\nprint('balance Ratio:',\"{:.2f}\".format(df_ud['values'].min()\/df_ud['values'].max()*100))","3d33ad72":"from sklearn.utils import resample","58f8735c":"features=['limit_bal', 'sex', 'age', 'pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5',\n       'pay_6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5',\n       'pay_amt6', 'bill_amt_pc1', 'bill_amt_pc2', 'education_2',\n       'education_3', 'education_4', 'marriage_1', 'marriage_2', 'marriage_3']\ny = df_dc['default'].copy() # target\nX = df_dc[features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","dfe0bffd":"# create the training df by remerging X_train and y_train\ndf_train = X_train.join(y_train)\ndf_train.sample(10)","32c39819":"from sklearn.utils import resample","776aa40f":"# Separate majority and minority classes\ndf_majority = df_train[df_train.default==0]\ndf_minority = df_train[df_train.default==1]\n\nprint(df_majority.default.count())\nprint(\"-----------\")\nprint(df_minority.default.count())\nprint(\"-----------\")\nprint(df_train.default.value_counts())","4b6fb4e0":"# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=17603,    # to match majority class\n                                 random_state=587) # reproducible results\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n# Display new class counts\ndf_upsampled.default.value_counts()","3f07e74a":"# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=4541,     # to match minority class\n                                 random_state=587) # reproducible results\n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n# Display new class counts\ndf_downsampled.default.value_counts()","c3d5a6bb":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train=scaler.fit(X_train)","e1be825e":"from sklearn.neighbors import KNeighborsClassifier","cce2568c":"error = []\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(i,metric='manhattan')\n    knn_model = knn.fit(X_train, y_train)\n    pred_i = knn_model.predict(X_test)\n    error.append(knn_model.score(X_test, y_test))\nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error rate for diffrent values of k')\nplt.xlabel('K ')\nplt.ylabel('Accuracy')","37975735":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors= 25,metric='manhattan')\nknn.fit(X_train,y_train)\nknn_pred=knn.predict(X_test)\nprint('training Error for K-nearest neighbor '+ \": {:.2f}\".format(1-knn.score(X_train,y_train)))\nprint('validation Error for K-nearest neighbor '+ \": {:.2f}\".format(1-knn.score(X_test,y_test)))","595334f9":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,knn_pred))","a1251bb4":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for LR.\nlda_matrix = confusion_matrix(y_test,knn_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for K-nearest neighbor');","3c4d7dfe":"y_upsampled = df_upsampled.default\nX_upsampled = df_upsampled.drop(['default'], axis= 1)","c2244d66":"error = []\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(i,metric='manhattan')\n    knn_model = knn.fit(X_upsampled , y_upsampled )\n    pred_i = knn_model.predict(X_test)\n    error.append(knn_model.score(X_test, y_test))\nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error rate for diffrent values of k')\nplt.xlabel('K ')\nplt.ylabel('Accuracy')","fd0b6285":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors= 25,metric='manhattan')\nknn.fit(X_upsampled,y_upsampled)\nknn_pred=knn.predict(X_test)\nprint('training Error for K-nearest neighbor '+ \": {:.2f}\".format(1-knn.score(X_upsampled,y_upsampled)))\nprint('validation Error for K-nearest neighbor '+ \": {:.2f}\".format(1-knn.score(X_test,y_test)))","88488882":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,knn_pred))","cd9ad48e":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for LR.\nlda_matrix = confusion_matrix(y_test,knn_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for K-nearest neighbor');","e43fba6b":"y_downsampled = df_downsampled.default\nX_downsampled = df_downsampled.drop(['default'], axis = 1)","9d43abef":"error = []\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(i,metric='manhattan')\n    knn_model = knn.fit(X_downsampled , y_downsampled )\n    pred_i = knn_model.predict(X_test)\n    error.append(knn_model.score(X_test, y_test))\nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error rate for diffrent values of k')\nplt.xlabel('K ')\nplt.ylabel('Accuracy')","4b4646d8":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors= 25,metric='manhattan')\nknn.fit(X_downsampled,y_downsampled)\nknn_pred=knn.predict(X_test)\nprint('training Error for K-nearest neighbor '+ \": {:.2f}\".format(1-knn.score(X_downsampled,y_downsampled)))\nprint('validation Error for K-nearest neighbor '+ \": {:.2f}\".format(1-knn.score(X_test,y_test)))","da3ddf0f":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,knn_pred))","148403df":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for LR.\nlda_matrix = confusion_matrix(y_test,knn_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for K-nearest neighbor');","c95beb33":"#LR\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_train,y_train)\nLR_pred=LR.predict(X_test)\nprint('training Error for Logistic Regression '+ \": {:.2f}\".format(1-LR.score(X_train,y_train)))\nprint('validation Error for Logistic Regression '+ \": {:.2f}\".format(1-LR.score(X_test,y_test)))","d1a47b63":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,LR_pred))","db7ae54b":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for LR.\nlda_matrix = confusion_matrix(y_test,LR_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Logistic Regression');","9a43a4ec":"#LR\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_upsampled,y_upsampled)\nLR_pred=LR.predict(X_test)\nprint('training Error for Logistic Regression '+ \": {:.2f}\".format(1-LR.score(X_upsampled,y_upsampled)))\nprint('validation Error for Logistic Regression '+ \": {:.2f}\".format(1-LR.score(X_test,y_test)))","fa8018c2":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,LR_pred))","6c5b7da2":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for LR.\nlda_matrix = confusion_matrix(y_test,LR_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Logistic Regression');","d90bc2a4":"#LR\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_downsampled,y_downsampled)\nLR_pred=LR.predict(X_test)\nprint('training Error for Logistic Regression '+ \": {:.2f}\".format(1-LR.score(X_downsampled,y_downsampled)))\nprint('validation Error for Logistic Regression '+ \": {:.2f}\".format(1-LR.score(X_test,y_test)))","35546510":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,LR_pred))","67768107":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for LR.\nlda_matrix = confusion_matrix(y_test,LR_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Logistic Regression');","8ba4f6ae":"#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train, y_train)\nlda_pred=lda.predict(X_test)\nprint('training Error for Discriminant analysis '+ \": {:.2f}\".format(1-lda.score(X_train,y_train)))\nprint('validation Error for Discriminant analysis '+ \": {:.2f}\".format(1-lda.score(X_test,y_test)))","6085dba2":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,lda_pred))","056552b3":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,lda_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Discriminant analysis');","054054a8":"#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_upsampled, y_upsampled)\nlda_pred=lda.predict(X_test)\nprint('training Error for Discriminant analysis '+ \": {:.2f}\".format(1-lda.score(X_upsampled,y_upsampled)))\nprint('validation Error for Discriminant analysis '+ \": {:.2f}\".format(1-lda.score(X_test,y_test)))","ad35d040":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,lda_pred))","cb282d92":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,lda_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Discriminant analysis');","099c40d2":"#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_downsampled, y_downsampled)\nlda_pred=lda.predict(X_test)\nprint('training Error for Discriminant analysis '+ \": {:.2f}\".format(1-lda.score(X_downsampled,y_downsampled)))\nprint('validation Error for Discriminant analysis '+ \": {:.2f}\".format(1-lda.score(X_test,y_test)))","ae222280":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,lda_pred))","2c82edf0":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,lda_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Discriminant analysis');","c49039e6":"from sklearn.naive_bayes import GaussianNB\nNB = GaussianNB()\nNB.fit(X_train, y_train)\nNB_pred=NB.predict(X_test)\nprint('training Error for Naive Bayesian '+ \": {:.2f}\".format(1-NB.score(X_train,y_train)))\nprint('validation Error for Naive Bayesian '+ \": {:.2f}\".format(1-NB.score(X_test,y_test)))","75f6d47a":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,NB_pred))","e0858c0a":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,NB_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Naive Bayesian');","4d61be49":"from sklearn.naive_bayes import GaussianNB\nNB = GaussianNB()\nNB.fit(X_upsampled, y_upsampled)\nNB_pred=NB.predict(X_test)\nprint('training Error for Naive Bayesian '+ \": {:.2f}\".format(1-NB.score(X_upsampled,y_upsampled)))\nprint('validation Error for Naive Bayesian '+ \": {:.2f}\".format(1-NB.score(X_test,y_test)))","44fba289":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,NB_pred))","4e5b00f4":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,NB_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Naive Bayesian');","68a41a6f":"from sklearn.naive_bayes import GaussianNB\nNB = GaussianNB()\nNB.fit(X_downsampled, y_downsampled)\nNB_pred=NB.predict(X_test)\nprint('training Error for Naive Bayesian '+ \": {:.2f}\".format(1-NB.score(X_downsampled,y_downsampled)))\nprint('validation Error for Naive Bayesian '+ \": {:.2f}\".format(1-NB.score(X_test,y_test)))","cd52701d":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,NB_pred))","b94c8649":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,NB_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Naive Bayesian');","4fef15ba":"from sklearn.neural_network import MLPClassifier\nMLP = MLPClassifier(max_iter=1000)\nMLP.fit(X_train, y_train)\nMLP_pred=MLP.predict(X_test)\nprint('training Error for Neural networks '+ \": {:.2f}\".format(1-MLP.score(X_train,y_train)))\nprint('validation Error for Neural networks '+ \": {:.2f}\".format(1-MLP.score(X_test,y_test)))","d925daea":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,MLP_pred))","fc2f524d":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,MLP_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Neural networks');","ff1af998":"from sklearn.neural_network import MLPClassifier\nMLP = MLPClassifier(max_iter=1000)\nMLP.fit(X_upsampled, y_upsampled)\nMLP_pred=MLP.predict(X_test)\nprint('training Error for Neural networks '+ \": {:.2f}\".format(1-MLP.score(X_upsampled,y_upsampled)))\nprint('validation Error for Neural networks '+ \": {:.2f}\".format(1-MLP.score(X_test,y_test)))","c3e970c4":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,MLP_pred))","3aaea9c5":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,MLP_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Neural networks');","4f3b4f77":"from sklearn.neural_network import MLPClassifier\nMLP = MLPClassifier(max_iter=1000)\nMLP.fit(X_downsampled, y_downsampled)\nMLP_pred=MLP.predict(X_test)\nprint('training Error for Neural networks '+ \": {:.2f}\".format(1-MLP.score(X_downsampled,y_downsampled)))\nprint('validation Error for Neural networks '+ \": {:.2f}\".format(1-MLP.score(X_test,y_test)))","20f39158":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,MLP_pred))","96bb56a6":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,MLP_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Neural networks');","aac30cbe":"from sklearn.tree import DecisionTreeClassifier\nCT= DecisionTreeClassifier()\nCT.fit(X_train, y_train)\nCT_pred=CT.predict(X_test)\nprint('training Error for Classification trees '+ \": {:.2f}\".format(1-CT.score(X_train,y_train)))\nprint('validation Error for Classification trees '+ \": {:.2f}\".format(1-CT.score(X_test,y_test)))","ab7d011a":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,CT_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Classification trees');","ec268730":"from sklearn.tree import DecisionTreeClassifier\nCT= DecisionTreeClassifier()\nCT.fit(X_upsampled, y_upsampled)\nCT_pred=CT.predict(X_test)\nprint('training Error for Classification trees '+ \": {:.2f}\".format(1-CT.score(X_upsampled,y_upsampled)))\nprint('validation Error for Classification trees '+ \": {:.2f}\".format(1-CT.score(X_test,y_test)))","85c8394f":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,CT_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Classification trees');","6f0dc270":"from sklearn.tree import DecisionTreeClassifier\nCT= DecisionTreeClassifier()\nCT.fit(X_downsampled, y_downsampled)\nCT_pred=CT.predict(X_test)\nprint('training Error for Classification trees '+ \": {:.2f}\".format(1-CT.score(X_downsampled,y_downsampled)))\nprint('validation Error for Classification trees '+ \": {:.2f}\".format(1-CT.score(X_test,y_test)))","81ce74e6":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,CT_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Classification trees');","3a2eb726":"from sklearn.model_selection import GridSearchCV","52703f5a":"param_grid = {'max_depth': np.arange(3, 10),\n             'criterion' : ['gini','entropy'],\n             'max_leaf_nodes': [5,10,20,100],\n             'min_samples_split': [2, 5, 10, 20]}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 5, scoring= 'accuracy')\ngrid_tree.fit(X_train, y_train)\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))","304b5936":"from sklearn.tree import DecisionTreeClassifier\nCT= DecisionTreeClassifier(max_depth=4, max_leaf_nodes=5)\nCT.fit(X_train, y_train)\nCT_pred=CT.predict(X_test)\nprint('training Error for Classification trees '+ \": {:.3f}\".format(1-CT.score(X_train,y_train)))\nprint('validation Error for Classification trees '+ \": {:.3f}\".format(1-CT.score(X_test,y_test)))","1e5dd6ee":"print(classification_report(y_test,CT_pred))","0be9ef11":"import graphviz\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn import tree\n# DOT data\ndot_data = tree.export_graphviz(CT, out_file=None, \n                                feature_names=features,  \n                                class_names=\"default\",\n                                filled=True)\n\n# Draw graph\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph","509a2f8f":"param_grid = {\n              'penalty' : ['l1','l2','elasticnet','none'], \n              'C'       : np.logspace(-3,3,7),\n              'solver'  : ['newton-cg', 'lbfgs', 'liblinear','sag','saga']}\nlogreg = LogisticRegression(random_state=42)\ngrid_lr = GridSearchCV(logreg, \n                   param_grid = param_grid,\n                   scoring = 'accuracy', \n                   cv = 5)\ngrid_lr.fit(X_train,y_train)","af0f7727":"print(grid_lr.best_estimator_)","4381832a":"#LR\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(penalty='l1', solver='liblinear', C=10,random_state=42)\nLR.fit(X_train,y_train)\nLR_pred=LR.predict(X_test)\nprint('training Error for Logistic Regression '+ \": {:.3f}\".format(1-LR.score(X_train,y_train)))\nprint('validation Error for Logistic Regression '+ \": {:.3f}\".format(1-LR.score(X_test,y_test)))","585fa9ca":"print(classification_report(y_test,LR_pred))","f4b0227f":"from sklearn.model_selection import GridSearchCV","aec18e38":"param_grid = {\n    'solver': ['lbfgs','sgd', 'adam'], \n    'max_iter': [800,1000,1500,2000], \n    'alpha': 10.0 ** -np.arange(1, 5), \n    'hidden_layer_sizes':[(100,),(100,),(200,),(400,)]}\nMLP = MLPClassifier()\ngrid_mlp= GridSearchCV(MLP, \n                   param_grid = param_grid,\n                   scoring = 'accuracy', \n                   cv = 5)\ngrid_mlp.fit(X_train, y_train)","b003831f":"print(grid_mlp.best_estimator_)","85ea9b7e":"print(np.abs(grid_mlp.best_score_))","0586aace":"MLP_pred=grid_mlp.predict(X_test)\nprint(classification_report(y_test,MLP_pred))","219cf9d7":"from sklearn.neural_network import MLPClassifier\n\nMLP=MLPClassifier(activation='logistic',hidden_layer_sizes=13,max_iter=700,random_state=2,solver='adam',alpha=0.0001)\nMLP.fit(X_train,y_train)\nprint(MLP.score(X_test,y_test))\nprint(MLP.score(X_train,y_train))","2c2652f7":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid= {\n        'algorithm': ['SAMME','SAMME.R'],\n    'n_estimators': [10,50,100,500],\n    'learning_rate': [0.1, 0.01, 0.001,0.0001,1.0]\n        }\nada = AdaBoostClassifier()\ngrid_ada=GridSearchCV(ada,param_grid,cv=5)\ngrid_ada.fit(X_train, y_train)","f3df4b11":"print(grid_ada.best_estimator_)\nprint(np.abs(grid_ada.best_score_))","d875f233":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(learning_rate=0.1,algorithm='SAMME',n_estimators=10)\nada.fit(X_train, y_train)\nada_pred=ada.predict(X_test)\nprint(ada.score(X_train, y_train))\nprint(ada.score(X_test, y_test))","733ba894":"from sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test,ada_pred))","6253ec2a":"from sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n# Plot confusion matrix for lda.\nlda_matrix = confusion_matrix(y_test,ada_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(lda_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Classification trees');","67e31fbb":"from xgboost import XGBClassifier\n\nparam_grid_XG = {\n        'max_depth': range (2, 10, 1),\n    'n_estimators': range(60, 220, 40),\n    'learning_rate': [0.1, 0.01, 0.05]\n        }\ngridxg=GridSearchCV(XGBClassifier(),param_grid_XG,cv=5)\ngridxg.fit(X_train, y_train)","db024180":"from xgboost import XGBClassifier\nxg=XGBClassifier(learning_rate=0.1,max_depth=2,n_estimators=100,tree_method=\"gpu_hist\", enable_categorical=True, use_label_encoder=False)\nxg.fit(X_train, y_train)\nprint(xg.score(X_train, y_train))\nprint(xg.score(X_test, y_test))\ny_pred_xg=xg.predict(X_train)","aee53ba5":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc,roc_auc_score\n#knn\ny_score_knn=knn.predict_proba(X_test)\nfpr_knn,tpr_knn,theshold_knn=roc_curve(y_test,y_score_knn[:,1])\nroc_auc_knn=auc(fpr_knn,tpr_knn)\n#LR\ny_score_lr=LR.predict_proba(X_test)\nfpr_lr,tpr_lr,theshold_lr=roc_curve(y_test,y_score_lr[:,1])\nroc_auc_lr=auc(fpr_lr,tpr_lr)\n#DA\ny_score_lda=lda.predict_proba(X_test)\nfpr_lda,tpr_lda,theshold_lda=roc_curve(y_test,y_score_lda[:,1])\nroc_auc_lda=auc(fpr_lda,tpr_lda)\n#NB\ny_score_nb=NB.predict_proba(X_test)\nfpr_nb,tpr_nb,theshold_nb=roc_curve(y_test,y_score_nb[:,1])\nroc_auc_nb=auc(fpr_nb,tpr_nb)\n#NN\ny_score_mlp=MLP.predict_proba(X_test)\nfpr_mlp,tpr_mlp,theshold_mpl=roc_curve(y_test,y_score_mlp[:,1])\nroc_auc_mlp=auc(fpr_mlp,tpr_mlp)\n#CT\ny_score_ct=CT.predict_proba(X_test)\nfpr_ct,tpr_ct,theshold_ct=roc_curve(y_test,y_score_ct[:,1])\nroc_auc_ct=auc(fpr_ct,tpr_ct)\n#ADA\ny_score_ada=ada.predict_proba(X_test)\nfpr_ada,tpr_ada,theshold_ada=roc_curve(y_test,y_score_ada[:,1])\nroc_auc_ada=auc(fpr_ada,tpr_ada)\n#XGBOOST\ny_score_xg=xg.predict_proba(X_test)\nfpr_xg,tpr_xg,theshold_xg=roc_curve(y_test,y_score_xg[:,1])\nroc_auc_xg=auc(fpr_xg,tpr_xg)","2cedc914":"plt.figure(figsize=(10,5))\nplt.plot(fpr_knn,tpr_knn, 'red', label = 'AUC KNN = %0.2f' % roc_auc_knn)\nplt.plot(fpr_lr,tpr_lr, 'purple', label = 'AUC LR = %0.2f' % roc_auc_lr)\nplt.plot(fpr_lda,tpr_lda, 'yellow', label = 'AUC DA = %0.2f' % roc_auc_lda)\nplt.plot(fpr_nb,tpr_nb, 'black', label = 'AUC NB = %0.2f' % roc_auc_nb)\nplt.plot(fpr_mlp,tpr_mlp, 'orange', label = 'AUC MLP = %0.2f' % roc_auc_mlp)\nplt.plot(fpr_ct,tpr_ct, 'blue', label = 'AUC CT = %0.2f' % roc_auc_ct)\nplt.plot(fpr_ada,tpr_ada, 'green', label = 'AUC AdaBoost = %0.2f' % roc_auc_ada)\nplt.plot(fpr_xg,tpr_xg, 'grey', label = 'AUC XGBOOST = %0.2f' % roc_auc_xg)\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of final model')\nplt.grid(True)\nplt.show()","92b6595d":"df_dep","9af25abc":"#get dummies for education and marriage\ndf_dc1 = pd.get_dummies(df_dep, columns=['education','marriage'],drop_first=True)\ndf_dc1","05dcdb45":"features=['limit_bal', 'sex', 'age', 'pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5',\n       'pay_6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5',\n       'pay_amt6', 'bill_amt1', 'bill_amt2','bill_amt3','bill_amt4','bill_amt5','bill_amt6', 'education_2',\n       'education_3', 'education_4', 'marriage_1', 'marriage_2', 'marriage_3']\ny = df_dc1['default'].copy() # target\nX = df_dc1[features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","3e8160aa":"# create the training df by remerging X_train and y_train\ndf_train = X_train.join(y_train)\ndf_train.sample(10)","81c06899":"from sklearn.tree import DecisionTreeClassifier\nXGBoost= DecisionTreeClassifier(max_depth=4, max_leaf_nodes=5)\nCT.fit(X_train, y_train)\nCT_pred=CT.predict(X_test)\nprint('training Error for Classification trees '+ \": {:.3f}\".format(1-CT.score(X_train,y_train)))\nprint('validation Error for Classification trees '+ \": {:.3f}\".format(1-CT.score(X_test,y_test)))","045ab278":"import pickle \n","1d5933f1":"filename = \"trainned_model.sav\"\npickle.dump(XGboost , open(filename,'wb'))","f0fff97f":"loaded_model = pickle.load(open(\"trainned_model.sav\",\"rb\"))","4cd4800b":"features = ['limit_bal', 'sex', 'age', 'pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5',\n       'pay_6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5',\n       'pay_amt6', 'bill_amt_pc1', 'bill_amt_pc2', 'education_2',\n       'education_3', 'education_4', 'marriage_1', 'marriage_2', 'marriage_3']\ninput_data= []\nfor i in features:\n  print(f'Donner  {i} :')\n  a=input()\n  input_data.append(float(a))\nprint(input_data)\n# 50000\t1\t2\t1\t57\t-1\t0\t-1\t0\t0\t0\t8617\t5670\t35835\t20940\t19146\t19131\t2000\t36681\t10000\t9000\t689\t679","78751aaa":"input_data_as_numpy_array= np.asarray(input_data)\n\ninput_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\nprediction = loaded_model.predict(input_data_reshaped)\nprint(prediction)\n\nif(prediction[0]==0):\n  print(\"the person is not going to default\")\nelse:\n  print(\"the person is going to default\")","954a3680":"- we can see that the maximum count of defaults falls under subcategory 2: a payment delay for the last 2 months. This implies that a customer who has missed payments for 2 continuous months has a high probability of default.","9fd364c5":"### K-nearest neighbor","7080e879":"### Handeling duplicates","27d92274":"#### DownSampled dataset","97ef58d2":"For the case the logitic regression the best training set is the simple dataset","0c1548d8":"The accuracy score for the Adaboost model is 0.833","a4c14825":"# Validation Projet Machine Learning","86752bb4":"#### UpSampled dataset","b2d1b0d1":"# <font color=darkblue>Machine Learning Project: Credit Card Default Prediction <\/font>\n\n\n*Goals:*\n* Implementing different Default Credit Card prediction models using Default Credit Card Clients dataset\n* Comparing the different models   \n* Presenting the most efficient model\n\n*Team Members:*\n* Houssem RIAHI\n* Baha Eddine TOUMI\n* Mouheb BEN ABDELKADER\n\n*Class:* \n4DS3","9e692a39":"So now we have 3 training sets:\n\n* X_train, y_train, with their unbalance and their authenticity.\n* df_upsample, which is balanced but at overfitting risk.\n* df_downsample, which will be fast but also potentially useless in terms of predictability (the test set is even bigger than it).\n","70902090":"For the case the Classification trees we are facing overfitting problem which will make us use the grid search method to find the best parameters ","f4ea72e1":"#### XGBoost","4edab1fa":"#### Simples Training dataset\nManhattan distance is a good measure to use because our  data are not of the same type (example: Age, sex, bill_pay, etc ...).","28b7a469":"The new accuracy score for the Neural Network model is 0.795","337de255":"#### DownSample dataset","79fbbe1b":"### Naive Bayesian","cdd065c6":"#### UpSample dataset","1716fa02":"#### Bivariate Analysis","3b51206c":"#### UpSample dataset","84dd1e5d":"**Imbalanced datasets** mean that the number of observations differs for the classes in a classification dataset. This imbalance can lead to inaccurate results especially  for a predictive model.\nan ideal balanced dataset will have 1:1 ratio of class distrubution \n\nAs we can see, our data is inbalanced with balance ratio equales to 25.67","16a8b705":"#### Simple training dataset","c1989a90":"### Dummies Variables","6b1165f3":"## V. Evaluation\n#### V\u00e9rifier le(s) mod\u00e8le(s) ou les connaissances obtenues afin de s\u2019assurer qu\u2019ils r\u00e9pondent aux objectifs formul\u00e9s au d\u00e9but du processus.\n","21023f5d":"For the case the Discriminant analysis the best training set is the simple dataset","087cf363":"### Loading Data ","241ead4c":"We can clearly i see a high coorealatiion between the bill_amt features and we can see also moderately correlated between pay_amt and bills amt \nand the reason of this correlation is :\nit seems that it goes like that:\n\nI have a BILL of X, I pay Y\n\nThe month after I have to pay X-Y + X', being X' my new expenses, I pay Y'\n\nThe month after I have to pay X+X' - Y - Y' + X'' , I pay Y''\n\nSo on so forth\n\nOn top of that I may or may not have months of delay.\n\nIt seems that if by september I have a bill too close to my limit, I generally fail. However, I can already see some dramatic exceptions.\n\nMoreover, I can spot some clients that joined our dataset at a later month: they have 0 in BILL and PAY AMT for a while and then they start. I have to keep that in mind as well.","21f6c0fd":"The new accuracy score for the decision tree model is 0.83","4309d62b":"As a conclusion the best model we can use to predict if the client will default the next month or not is the XGBoost model in the next section we will try to use this model to predict if a new client will default or not ","128fddf7":"For the case the Naive Bayesian the best training set is the simple dataset","ee5fcbc3":"#### Simples training dataset","2b24119f":"#### DownSample dataset","ee811cde":"### Libreries Importation","30f7591b":"#### Simple training dataset","7be3d52f":"### Classification trees","0288567d":"### GridSearchCV","b1080994":"The accuracy score for the XGBoost model is 0.831","612de192":"###  EDA\n","93c2b449":"## III. Data preparation \n\nOrganiser les donn\u00e9es pour la mod\u00e9lisation","793a3b29":"#### UpSample dataset","19a561be":"#### logistic regression ","7c5da012":" ## VI. Deploiement (optionnel)","e2240b6a":"### Building a Profile of a High-Risk Customer\n\n- A male customer is more likely to default than a female customer.\n- People with a relationship status of divorced are more likely to default than married or single people.\n- The Paymenet in September is the most feature corrolated to default.\n- We can see that the count of {3,4,5,6,7,8} in the pay_i is too low and with a high probability of default so we will delete the rows with those records to maintain more accuracy to the model. \n- A customer whose highest educational qualification is a high-school diploma is more likely to default than a customer who has gone to graduate school or university.\n- A customer who has delayed payment for 2 consecutive months has a higher probability of default.\n- A customer who is 22 years of age has a higher probability of defaulting on payments than any other age group.\n- Bill_amt feautures are highly corrolated to eachothers which will let's us transform it to one feature. \n- The Pay_amt feautures are moderatly corrolated with the bill_amt features. \n- We will transfor the sex feature to {0,1} instead of {1,2} so it will be a binary feature.\n- The education feature and the Mariage feature will convert from categorical data into dummy or indicator variables.\n\n\n","bb64278a":"#### DownSample dataset","4e3331cb":"#### AdaBoost","64573ceb":"* The EDUCATION column has 7 unique values, but as per our data description, we have only 4 unique values, so we are going to club categories 0, 5, and 6 with category 4","d62f10fe":"#### Univariate Analysis","092d4cd9":"#### Neural networks","50758a33":"#### UpSamples dataset","d123f7c4":"- we can infer that customers with higher balances have a lower likelihood of default than customers with lower balance amounts.","5762eb8d":"Pearson vs Spearman correlation?\nBoth Pearson and Spearman are used for measuring the correlation but the difference between them lies in the kind of analysis we want.\n\nPearson correlation: Pearson correlation evaluates the linear relationship between two continuous variables.\n\nSpearman correlation: Spearman correlation evaluates the monotonic relationship. The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data.","2af15247":"### Logistic Regression","fb219579":"### Discriminant analysis","5343132f":"### Handeling Data unbalance","bdcaecbc":"For the case the Neural networks the best training set is the upsample dataset","843e4ff9":"As we can see in this heatmap and as we mentioned in previous EDA, bill_atm's features are highly Correlated. \nSo in order to optimize our features we opted for PCA ","7be282aa":"#### Classification trees","bb700b2b":"#### Neural Network","be60fb48":"#### Simple training dataset","7ae78007":"## II. Data understanding\n\n#### D\u00e9terminer pr\u00e9cis\u00e9ment les donn\u00e9es \u00e0 analyser, \u00e0 identifier la qualit\u00e9 des donn\u00e9es disponibles. ","0f3aa286":"#### DownSamples dataset","66cf3c41":" Our main goal is to Build a good accurate model able to predict default credit card of individual customers to reduce the credit risk ,damage and uncertainty.\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. There are 25 variables:\n\n- ID: ID of each client\n- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family\/supplementary credit\n- SEX: Gender (1=male, 2=female)\n- EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n- MARRIAGE: Marital status (1=married, 2=single, 3=others)\n- AGE: Age in years\n- PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, \u2026 8=payment delay for eight months, 9=payment delay for nine months and above)\n- PAY_2: Repayment status in August, 2005 (scale same as above)\n- PAY_3: Repayment status in July, 2005 (scale same as above)\n- PAY_4: Repayment status in June, 2005 (scale same as above)\n- PAY_5: Repayment status in May, 2005 (scale same as above)\n- PAY_6: Repayment status in April, 2005 (scale same as above)\n- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n- default.payment.next.month: Default payment (1=yes, 0=no)","3025e887":"- around 24% of male customers have defaulted and around 20% of female customers have defaulted.","6283752e":"#### UpSamples dataset","90cc0fd7":"#### Handeling Correlation in bill_amt features using PCA","e61a4bef":"#### Simple training dataset","53269763":"#### NB: People with high pay amount in month September are less likely to default than people with less pay amount.","1315e167":"## I. Business understanding:\n#### Expliquer les \u00e9l\u00e9ments m\u00e9tiers et probl\u00e9matiques \u00e0 r\u00e9soudre","948690f0":"#### DownSample dataset","a74587b4":"## IV. Modeling \n#### Expliquer et justifier le choix, le param\u00e9trage et le test de diff\u00e9rents algorithmes ","4d5737fb":"As the plot showed, 2 components explains around 98% variance in bill_atm Features which is good as a result.","2c6e4c49":"The new accuracy score for the logistic regression model is 0.82","d4d479cd":"For the case the k-nearst neighbor the best training set is the simple dataset"}}