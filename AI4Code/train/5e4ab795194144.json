{"cell_type":{"798f6e16":"code","53a37ba1":"code","4449a1d3":"code","82a4593e":"code","7e31ec7c":"code","e1cb7416":"code","cf6b8f44":"code","bf6d3947":"code","41312693":"code","a34ef4ce":"code","3b55f0c8":"code","54633698":"code","6eba227f":"code","4fc1efbd":"code","87979698":"code","d8d48a0f":"code","057ca178":"code","2a2df56a":"code","b63d7c98":"code","b611936b":"code","340a4d15":"code","06131e25":"code","26c0619d":"code","e7f65908":"code","74e9b126":"code","af5e6e24":"code","2b377fe3":"code","3ea7580b":"code","7219481c":"code","768e1a05":"code","300ad2a9":"code","eadb5d86":"markdown","91bcae28":"markdown","95e4501b":"markdown","f91f9f84":"markdown","684c3ef9":"markdown","b122ece9":"markdown","7ef576e2":"markdown","22213c45":"markdown","0d09f199":"markdown","fa9eff39":"markdown","a60f5e10":"markdown","6e4881a5":"markdown","45d058de":"markdown","9789774c":"markdown","bb5610f0":"markdown","3d6a240a":"markdown","9058fc9a":"markdown","9b9dfcd4":"markdown","b98a04e3":"markdown","25e51c8d":"markdown"},"source":{"798f6e16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53a37ba1":"import matplotlib.pyplot as plt  # for visualization \n%matplotlib inline\nimport seaborn as sns           # for visualization ","4449a1d3":"# read the data\ndf = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\ndf.head()","82a4593e":"df.head()","7e31ec7c":"df.info()","e1cb7416":"df.describe().T","cf6b8f44":"df.nunique()","bf6d3947":"def missing_percentage(df):\n\n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n\nmissing_data = missing_percentage(df)\n\nfig, ax = plt.subplots( figsize=(16, 6))\n\nsns.barplot(x=missing_data.index,\n            y='Percent',\n            data=missing_data)\n\n\nax.set_title('Missing Values')\nplt.show()","41312693":"plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nsns.distplot(df.MINIMUM_PAYMENTS, color='#fdc029')\nplt.subplot(1,2,2)\nsns.distplot(df.CREDIT_LIMIT, color='#fdc029')\nplt.show()","a34ef4ce":"df.MINIMUM_PAYMENTS.fillna(df.MINIMUM_PAYMENTS.median(),inplace=True)","3b55f0c8":"print('MINIMUM_PAYMENTS FEATURE HAS',df.MINIMUM_PAYMENTS.isna().sum(),'MISSING VALUE')","54633698":"df.CREDIT_LIMIT.fillna(df.CREDIT_LIMIT.median(),inplace=True)","6eba227f":"print('CREDIT_LIMIT FEATURE HAS',df.CREDIT_LIMIT.isna().sum(),'MISSING VALUE')","4fc1efbd":"g = sns.PairGrid(df)\ng.map(plt.scatter)\nplt.title('relations between features')\nplt.show()","87979698":"def scatter_purchases(x):\n    sns.scatterplot(y='PURCHASES',x=x,data = df,color='#171820',alpha=0.7)","d8d48a0f":"scatter_purchases('BALANCE')","057ca178":"plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nsns.lineplot(x='TENURE',y='PURCHASES',data=df)\nplt.title('The Purchases based on Tenure of credit card service for use')\nplt.subplot(1,2,2)\nscatter_purchases('TENURE')\n","2a2df56a":"plt.hist(df.CREDIT_LIMIT)\nplt.title('credit limit distribution')\nplt.show()","b63d7c98":"col = list(df.drop('CUST_ID',axis=1).columns)","b611936b":"plt.figure(figsize=(30,30))\nfor idx,val in enumerate(col):\n    plt.subplot(6,3,idx+1)\n    sns.boxplot(x=val,data=df)","340a4d15":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nX = scale.fit_transform(df.drop('CUST_ID',axis=1))","06131e25":"from sklearn.cluster import KMeans\nn_clusters=30\ncost=[]\nfor i in range(1,n_clusters):\n    kmean= KMeans(i)\n    kmean.fit(X)\n    cost.append(kmean.inertia_)  ","26c0619d":"plt.plot(cost, 'bx-')","e7f65908":"n_clusters=10\ncost=[]\nfor i in range(1,n_clusters):\n    kmean= KMeans(i)\n    kmean.fit(X)\n    cost.append(kmean.inertia_)  ","74e9b126":"plt.plot(cost, 'gx-')\nplt.title('Elbow Criterion')\nplt.show()","af5e6e24":"kmean= KMeans(6)\nkmean.fit(X)\nlabels=kmean.labels_","2b377fe3":"clusters=pd.concat([df, pd.DataFrame({'cluster':labels})], axis=1)\nclusters.head()","3ea7580b":"clusters.info()","7219481c":"for c in clusters.iloc[:,1:]:\n    grid= sns.FacetGrid(clusters.iloc[:,1:], col='cluster')\n    grid.map(plt.hist, c)","768e1a05":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import PCA\ndist = 1 - cosine_similarity(X)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\nX_PCA.shape","300ad2a9":"x, y = X_PCA[:, 0], X_PCA[:, 1]\n\ncolors = {0: 'red',\n          1: 'blue',\n          2: 'green', \n          3: 'yellow', \n          4: 'orange',  \n          5:'purple'}\n\nnames = {0: 'who make all type of purchases', \n         1: 'more people with due payments', \n         2: 'who purchases mostly in installments', \n         3: 'who take more cash in advance', \n         4: 'who make expensive purchases',\n         5:'who don\\'t spend much money'}\n  \ndf = pd.DataFrame({'x': x, 'y':y, 'label':labels}) \ngroups = df.groupby('label')\n\nfig, ax = plt.subplots(figsize=(20, 13)) \n\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5,\n            color=colors[name],label=names[name], mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n    \nax.legend()\nax.set_title(\"Customers Segmentation based on their Credit Card usage bhaviour.\")\nplt.show()\n","eadb5d86":"<a id='cluster'><\/a>\n\n# Clustering","91bcae28":"# Credit Card Dataset for Clustering\n## Table of Contents\n<ul>\n<li><a href=\"#Dictionary\">Data Dictionary<\/a><\/li>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#wrangling\"> Data wrangling<\/a><\/li>\n<li><a href=\"#eda\">Exploratory Data Analysis<\/a><\/li>\n<li><a href=\"#cluster\">clustering <\/a><\/li>\n<\/ul> ","95e4501b":"## Checking Variables Before Imputing","f91f9f84":"**But! What if we divide them into clusters; Outliers will be in Private cluster**","684c3ef9":"**Following is the Data Dictionary for Credit Card dataset :-**\n\n- **CUSTID** : Identification of Credit Card holder (Categorical)\n- **BALANCE**: Balance amount left in their account to make purchases (\n- **BALANCEFREQUENCY** : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n- **PURCHASES** : Amount of purchases made from account\n- **ONEOFFPURCHASES**: Maximum purchase amount done in one-go\n- **INSTALLMENTSPURCHASES** : Amount of purchase done in installment\n- **CASHADVANCE**: Cash in advance given by the user\n- **PURCHASESFREQUENCY** : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n- **ONEOFFPURCHASESFREQUENCY**: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n- **PURCHASESINSTALLMENTSFREQUENCY** : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n- **CASHADVANCEFREQUENCY** : How frequently the cash in advance being paid\n- **CASHADVANCETRX** : Number of Transactions made with \"Cash in Advanced\"\n- **PURCHASESTRX**: Numbe of purchase transactions made\n- **CREDITLIMIT**: Limit of Credit Card for user\n- **PAYMENTS** : Amount of Payment done by user\n- **MINIMUM_PAYMENTS** : Minimum amount of payments made by user\n- **PRCFULLPAYMENT** : Percent of full payment paid by user\n- **TENURE** : Tenure of credit card service for user","b122ece9":"<a id='intro'><\/a>\n## Introduction\n\n","7ef576e2":"# Visualization of Clusters","22213c45":"## filling missing values ","0d09f199":"<a id='wrangling'><\/a>\n## Data wrangling","fa9eff39":"**Now we can confirm that with the increase in the period of use of the card, the purchase price increases, especially for a year, because there is a big difference between it and the rest**","a60f5e10":"- **Cluster0** People with average to high credit limit who make all type of purchases\n- \n- **Cluster1** This group has more people with due payments who take advance cash more often\n- \n- **Cluster2** Less money spenders with average to high credit limits who purchases mostly in installments\n- \n- **Cluster3** People with high credit limit who take more cash in advance\n- \n- **Cluster4** High spenders with high credit limit who make expensive purchases\n- \n- **Cluster5** People who don't spend much money and who have average to high credit limit","6e4881a5":"<a id='eda'><\/a>\n## EDA","45d058de":"# Missing Values","9789774c":"## Kmeans","bb5610f0":"<a id='Dictionary'><\/a>\n## Data Dictionary","3d6a240a":"Just wanted to check variable distribution before we impute the missing ones","9058fc9a":"**This case requires to develop a customer segmentation to define marketing strategy. The\nsample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.**\n","9b9dfcd4":"## We can see that the data contains a lot of outliers which we have to deal with","b98a04e3":"**After discovered the data with missing value and knowing its distribution, the best way to fill missing values is median**","25e51c8d":"## 6 clusters are good"}}