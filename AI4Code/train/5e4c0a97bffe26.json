{"cell_type":{"52f0ac44":"code","35ae3269":"code","bba50421":"code","6c239fc1":"code","1113364b":"code","523983b3":"code","b8920e16":"code","f723fc31":"code","beeeed0f":"code","ac2a1400":"code","5ec8569f":"code","7780ed1d":"code","9c657e9a":"code","0959a4df":"code","8c84babd":"code","187d5e41":"code","219510e8":"code","1682a672":"code","c640cae6":"code","cdef4621":"code","871b7249":"code","0eec0082":"code","b081203f":"code","524da7f1":"markdown","c7e6a801":"markdown","183711da":"markdown","1b22aa47":"markdown","781c4e55":"markdown","25396052":"markdown","4a54dd72":"markdown","e20499c5":"markdown","2c1eb6d3":"markdown"},"source":{"52f0ac44":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot","35ae3269":"train = pd.read_csv('..\/input\/fake-news\/train.csv')\ntest = pd.read_csv('..\/input\/fake-news\/test.csv')","bba50421":"train.head()","6c239fc1":"train['label'].value_counts()","1113364b":"fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(12,4), dpi=100)\nsns.countplot(train['label'], ax=axes[0])\naxes[1].pie(train['label'].value_counts(),\n            labels=['reliable', 'unreliable'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Fake News', fontsize=24)\nplt.show()","523983b3":"train.isnull().sum()","b8920e16":"test.isnull().sum()","f723fc31":"train = train.fillna('')\ntest=test.fillna('')","beeeed0f":"train['total'] = train['title']+' '+train['author']\ntest['total']=test['title']+' '+test['author']","ac2a1400":"X = train.drop('label',axis=1)\ny=train['label']\nprint(X.shape)\nprint(y.shape)","5ec8569f":"voc_size = 6000\nmsg = X.copy()\nmsg_test = test.copy()","7780ed1d":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","9c657e9a":"from nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []","0959a4df":"for i in range(len(msg)):\n    review = re.sub('[^a-zA-Z]',' ',msg['total'][i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","8c84babd":"corpus_test = []\nfor i in range(len(msg_test)):\n    review = re.sub('[^a-zA-Z]',' ',msg_test['total'][i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus_test.append(review)","187d5e41":"onehot_rep = [one_hot(words,voc_size)for words in corpus]\nonehot_rep_test = [one_hot(words,voc_size)for words in corpus_test]","219510e8":"embedded_docs = pad_sequences(onehot_rep,padding='pre',maxlen=25)\nembedded_docs_test = pad_sequences(onehot_rep_test,padding='pre',maxlen=25)","1682a672":"#We have used embedding layers with LSTM\nmodel = Sequential()\nmodel.add(Embedding(voc_size,40,input_length=25))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","c640cae6":"X_final = np.array(embedded_docs)\ny_final = np.array(y)\ntest_final = np.array(embedded_docs_test)\nX_final.shape,y_final.shape,test_final.shape","cdef4621":"history = model.fit(X_final,y_final,epochs=20,batch_size=64)","871b7249":"plt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')","0eec0082":"y_pred = model.predict_classes(test_final)","b081203f":"res = pd.DataFrame()\nres['id']=test['id']\nres['label'] = y_pred\nres.to_csv('result.csv',index=False)","524da7f1":"Filling NULL values with empty string\n","c7e6a801":"Choosing vocabulary size to be 6000 and copying data to msg for further cleaning","183711da":"# Cleaning Text\n\n* Removed urls, emojis and punctuations\n* Tokenized base text and title\n* Lower cased clean text\n* Removed stopwords\n* Applying word Stemming\n* Converted tokenized text to string again","1b22aa47":"# Data preprocessing and cleaning","781c4e55":"Padding Sentences to make them of same size","25396052":"# Model Training","4a54dd72":"We will be using Stemming here\nStemming map words to their root forms","e20499c5":"We will be only using title and author name for prediction\nCreating new coolumn total concatenating title and author\n","2c1eb6d3":"Converting into numpy array"}}