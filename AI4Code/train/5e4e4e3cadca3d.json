{"cell_type":{"d299bc5b":"code","c460c6ee":"code","e55e0f5f":"code","2769415b":"code","53d75378":"code","3a2e2317":"code","7ff2914c":"code","cb753fef":"code","7d732121":"code","56a47626":"code","de1d3253":"code","0e3d4d2f":"code","7a3b34e0":"code","81c5e867":"code","c81ed144":"code","24154d39":"code","07dcfa95":"code","8bcf0bff":"code","84262d55":"code","7824ca5c":"markdown","8ba8add8":"markdown","2b76361a":"markdown","0aaae095":"markdown","b7e62219":"markdown","4dfffd75":"markdown","88b3ba93":"markdown","d66c2e9b":"markdown","adf56693":"markdown","8940bf6f":"markdown"},"source":{"d299bc5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","c460c6ee":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\nprint('Train size: ', df_train.shape)\nprint('Test size: ', df_test.shape)\ndf_train.head()","e55e0f5f":"train_data = df_train.drop('label', axis=1).values\n# train_data.shape\n# print(train_data.max())\ntrain_mean = train_data.mean()\/255.\ntrain_std = train_data.std()\/255.\n# train_std\nprint('Mean: ', train_mean)\nprint('Std: ', train_std)","2769415b":"# Train-Val split\nmask = np.random.rand(len(df_train)) < 0.8\ndf_val = df_train[~mask]\ndf_train = df_train[mask]\nprint('Train size: ', df_train.shape)\nprint('Val size: ', df_val.shape)\nprint('Test size: ', df_test.shape)\ndf_train.head()","53d75378":"import matplotlib.pyplot as plt\nind = np.random.randint(0, df_train.shape[0]-1)\nplt.imshow(df_train.iloc[ind].values[1:].reshape((28,28)), cmap='gray')\nplt.title(str(df_train.iloc[ind][0]))","3a2e2317":"# Create dataset class for PyTorch\nclass MNISTDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, n):\n        data = self.df.iloc[n]\n        image = data[1:].values.reshape((28,28)).astype(np.uint8)\n        label = data[0]\n        if self.transform:\n            image = self.transform(image)\n        return (image, label)","7ff2914c":"# Initialize transformation, datasets, and loaders\nbatch_size = 16\nclasses = range(10)\ntrain_transform = transforms.Compose(\n                    [\n                    transforms.ToPILImage(),\n#                     transforms.RandomRotation(30),\n                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n                    ])\n# don't (really) need the data augmentation in validation\nval_transform = transforms.Compose(\n                    [\n                    transforms.ToPILImage(),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n                    ])\ntest_transform = val_transform\n\ntrain_dataset = MNISTDataset(df_train, transform = train_transform)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                batch_size=batch_size,shuffle = True)\nval_dataset = MNISTDataset(df_val, transform = val_transform)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                batch_size=batch_size,shuffle = False)\n\n","cb753fef":"# sanity check for training data\nimgs, lbls = next(iter(train_loader))\nimgs[7].data.shape\nprint(imgs.data.min())\nprint(imgs.data.max())\nprint(imgs.data.mean())\nprint(imgs.data.std())\nprint(classes[lbls[0]])\nplt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\")","7d732121":"# sanity check for validation data\nimgs, lbls = next(iter(val_loader))\nimgs[0].data.shape\nprint(imgs.data.min())\nprint(imgs.data.max())\nprint(imgs.data.mean())\nprint(imgs.data.std())\nprint(classes[lbls[0]])\nplt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\")","56a47626":"# CNN model definition\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n## deeper model adapted from https:\/\/www.kaggle.com\/gustafsilva\/cnn-digit-recognizer-pytorch\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128, 10)\n        )\n                \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        \n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n\n\n## my old model which gets ~99%\n# class Model(nn.Module):\n#     def __init__(self):\n#         super(Model, self).__init__()\n#         self.conv1 = nn.Sequential(\n#                         nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1),\n#                         nn.BatchNorm2d(4),\n#                         nn.ReLU(inplace=True),\n#                         nn.MaxPool2d(kernel_size=2, stride=2))\n#         self.conv2 = nn.Sequential(\n#                         nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, padding=1),\n#                         nn.BatchNorm2d(16),\n#                         nn.ReLU(inplace=True),\n#                         nn.MaxPool2d(kernel_size=2, stride=2))\n#         self.conv3 = nn.Sequential(\n#                         nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=1),\n#                         nn.BatchNorm2d(64),\n#                         nn.ReLU(inplace=True),\n#                         nn.MaxPool2d(kernel_size=3, stride=2))\n#         self.fc = nn.Sequential(\n#                         nn.Dropout(p=0.5),\n#                         nn.Linear(in_features=3*3*64, out_features=128),\n#                         nn.BatchNorm1d(128),\n#                         nn.ReLU(inplace=True),\n#                         nn.Dropout(p=0.5),\n#                         nn.Linear(in_features=128, out_features=32),\n#                         nn.BatchNorm1d(32),\n#                         nn.ReLU(inplace=True),\n#                         nn.Linear(in_features=32, out_features=10))\n    \n#     def forward(self, x):\n#         x = self.conv1(x)\n#         x = self.conv2(x)\n#         x = self.conv3(x)\n#         x = x.view((x.shape[0],-1))\n#         x = self.fc(x)\n#         x = F.log_softmax(x, dim=1)\n#         return x","de1d3253":"# initialize CNN, cost, and optimizer\nmodel = Model()\nmodel.to(device)\ncriterion = nn.NLLLoss()   # with log_softmax() as the last layer, this is equivalent to cross entropy loss\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","0e3d4d2f":"model","7a3b34e0":"# Training Time!\nimport time\nimport copy\n\n# Some initialization work first...\nepochs = 100\ntrain_losses, val_losses = [], []\ntrain_accu, val_accu = [], []\nstart_time = time.time()\nearly_stop_counter = 10   # stop when the validation loss does not improve for 10 iterations to prevent overfitting\ncounter = 0\nbest_val_loss = float('Inf')\n\nfor e in range(epochs):\n    epoch_start_time = time.time()\n    running_loss = 0\n    accuracy=0\n    # training step\n    model.train()\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        log_ps = model(images)\n        \n        ps = torch.exp(log_ps)                \n        top_p, top_class = ps.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy += torch.mean(equals.type(torch.FloatTensor))\n        \n        loss = criterion(log_ps, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # record training loss and error, then evaluate using validation data\n    train_losses.append(running_loss\/len(train_loader))\n    train_accu.append(accuracy\/len(train_loader))\n    val_loss = 0\n    accuracy=0\n    model.eval()\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            log_ps = model(images)\n            val_loss += criterion(log_ps, labels)\n\n            ps = torch.exp(log_ps)\n            top_p, top_class = ps.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy += torch.mean(equals.type(torch.FloatTensor))\n    val_losses.append(val_loss\/len(val_loader))\n    val_accu.append(accuracy\/len(val_loader))\n\n    print(\"Epoch: {}\/{}.. \".format(e+1, epochs),\n          \"Time: {:.2f}s..\".format(time.time()-epoch_start_time),\n          \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n          \"Training Accu: {:.3f}.. \".format(train_accu[-1]),\n          \"Val Loss: {:.3f}.. \".format(val_losses[-1]),\n          \"Val Accu: {:.3f}\".format(val_accu[-1]))\n\n#     print('Epoch %d \/ %d took %6.2f seconds' % (e+1, epochs, time.time()-epoch_start_time))\n#     print('Total training time till this epoch was %8.2f seconds' % (time.time()-start_time))\n    \n    if val_losses[-1] < best_val_loss:\n        best_val_loss = val_losses[-1]\n        counter=0\n        best_model_wts = copy.deepcopy(model.state_dict())\n    else:\n        counter+=1\n        print('Validation loss has not improved since: {:.3f}..'.format(best_val_loss), 'Count: ', str(counter))\n        if counter >= early_stop_counter:\n            print('Early Stopping Now!!!!')\n            model.load_state_dict(best_model_wts)\n            break\n        ","81c5e867":"# plot training history\nplt.figure(figsize=(12,12))\nplt.subplot(2,1,1)\nax = plt.gca()\nax.set_xlim([0, e + 2])\nplt.ylabel('Loss')\nplt.plot(range(1, e + 2), train_losses[:e+1], 'r', label='Training Loss')\nplt.plot(range(1, e + 2), val_losses[:e+1], 'b', label='Validation Loss')\nax.grid(linestyle='-.')\nplt.legend()\nplt.subplot(2,1,2)\nax = plt.gca()\nax.set_xlim([0, e+2])\nplt.ylabel('Accuracy')\nplt.plot(range(1, e + 2), train_accu[:e+1], 'r', label='Training Accuracy')\nplt.plot(range(1, e + 2), val_accu[:e+1], 'b', label='Validation Accuracy')\nax.grid(linestyle='-.')\nplt.legend()\nplt.show()","c81ed144":"# prepare to predict test data - REMEMBER PRE-PROCESSING!\n# I originally forgot to scale and normalize, which caused problems....\n\n# some sanity check to make sure\nx_test = df_test.values\nx_test = x_test.reshape([-1, 28, 28]).astype(np.float)\nx_test = x_test\/255.\nx_test = (x_test-train_mean)\/train_std\nprint(x_test.min())\nprint(x_test.max())\nprint(x_test.mean())\nprint(x_test.std())","24154d39":"# x_test = df_test.values\n# x_test = x_test.reshape([-1, 28, 28]).astype(np.float)\n# x_test = x_test\/255.\n# x_test = (x_test-train_mean)\/train_std\nx_test = np.expand_dims(x_test, axis=1)\nx_test = torch.from_numpy(x_test).float().to(device)\n# x_test.shape\nx_test.type()","07dcfa95":"# prediction time!\nmodel.eval()   # this is needed to disable dropouts\nwith torch.no_grad():    # turn off gradient computation because we don't need it for prediction\n    ps = model(x_test)\n    prediction = torch.argmax(ps, 1)\n    print('Prediction',prediction)","8bcf0bff":"# prepare output file\ndf_export = pd.DataFrame(prediction.cpu().tolist(), columns = ['Label'])\ndf_export['ImageId'] = df_export.index +1\ndf_export = df_export[['ImageId', 'Label']]\ndf_export.head()","84262d55":"df_export.to_csv('output.csv', index=False)","7824ca5c":"### Split training data into training-validation","8ba8add8":"### Model Training","2b76361a":"> ### Load Data","0aaae095":"### Prediction","b7e62219":"### Sanity check to make sure data is of normal distribution (zero mean and unit standard dev)","4dfffd75":"### Define data augmentation and data loaders","88b3ba93":"### Define a PyTorch Dataset","d66c2e9b":"### Calculate mean and std of training data - used for normalization later","adf56693":"### Define CNN Architecture: \nI used 3 conv layers plus 3 fully connected layers with ReLU activation. Dropout and batch normalization were also used.\n\nUpdate: I borrowed a deeper model architecture from [this kernel](https:\/\/www.kaggle.com\/gustafsilva\/cnn-digit-recognizer-pytorch).","8940bf6f":"### Visualize Example"}}