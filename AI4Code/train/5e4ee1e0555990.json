{"cell_type":{"113ac791":"code","0d8164c1":"code","f4732590":"code","95d96767":"code","5481045f":"code","b0a11c75":"code","12e0d3a9":"code","009cab71":"code","da5eb1e6":"code","5cff1da6":"code","8e4691f2":"code","37eb0f9e":"code","e5700cc5":"code","a5e45baa":"code","246f613e":"code","0d0b8991":"code","47cb9e35":"code","7e090400":"code","8732ead5":"code","47e036ec":"code","0e4a6f2e":"code","16d406e9":"code","1c00b108":"code","ac7b0632":"code","2d02d314":"code","d6e0483a":"code","a0431604":"code","25443e1e":"code","e38cfe5b":"code","a460f657":"code","ada68ae4":"code","88aead15":"code","2dce227c":"code","00292cbe":"code","a57af269":"code","7da776b5":"code","2dd93e72":"code","314dd4f6":"code","cbcc748b":"code","edf93d34":"code","1049f51e":"code","e34aacd9":"code","456017d0":"code","cd2d8cab":"code","42403cca":"code","5b81e5f1":"code","7dad026e":"code","0ada36e1":"code","c203d8c6":"code","bdf9f53a":"code","f3724c30":"code","dca1a407":"code","266c4d22":"code","b7d877c1":"code","c75e0a75":"code","146b0d39":"code","7b2003e6":"code","738c8bf9":"code","d702fabd":"code","3d6464de":"code","c9c3bac4":"code","e95ad79a":"code","3cceab7b":"code","2a85cc25":"code","e64984f3":"code","c08018c5":"code","c2dcb9f4":"code","ea8c3476":"markdown","e9c10e69":"markdown","2efbef65":"markdown","ff801a07":"markdown","75860571":"markdown","1d6c3424":"markdown","43be63f4":"markdown","4ed21d7b":"markdown","d5dd8a06":"markdown","0c3343c6":"markdown","eba491b6":"markdown","b2698124":"markdown","1164a7e8":"markdown","4f4ed36d":"markdown","00be1cff":"markdown","84adef24":"markdown","49acfc0e":"markdown","2096d744":"markdown","f2f96762":"markdown","976d365a":"markdown","d8813a0d":"markdown","6b707407":"markdown","02e82802":"markdown","55491972":"markdown","f5ca21ad":"markdown","da08e25c":"markdown","d0eeb90f":"markdown","01189d50":"markdown"},"source":{"113ac791":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport os\nfrom datetime import datetime\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nimport math\nimport pickle\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nnltk.download('stopwords')\n\n! pip install pandasql\nfrom pandasql import sqldf","0d8164c1":"f = open('\/kaggle\/input\/employer-review-about-their-organization\/results.json',)\ndata = json.load(f)\ndf = pd.DataFrame(data)\ndf.head()","f4732590":"df.info()","95d96767":"print('--------')\ndisplay(df.ReviewTitle.unique())\ndisplay(print('Unique values: ', len(df.ReviewTitle.unique())))\n\nprint('--------')\ndisplay(df.URL.unique())\ndisplay(print('Unique values: ', len(df.URL.unique())))\n\nprint('--------')\ndisplay(df.ReviewDetails.unique())\ndisplay(print('Unique values: ', len(df.ReviewDetails.unique())))","5481045f":"# Extracting Company name from URL\ndf['Company'] = df.URL.str.split('\/')[:].str[4]","b0a11c75":"# Checking unique values in Company\ndisplay(df.Company.unique())\ndisplay(print('Unique values: ', len(df.Company.unique())))","12e0d3a9":"# Extracting Date & Time from ReviewDetails\ndf['Timestamp'] = df['ReviewDetails'].str.split('-', expand=True)[2]","009cab71":"# Spliting Year, Month, Day\ndf['Year'] = df['Timestamp'].str.split(',', expand=True)[1]\ndf['Month'] = df['Timestamp'].str.split(',', expand=True)[0].str.split(' ', expand=True)[2]\ndf['Day'] = df['Timestamp'].str.split(',', expand=True)[0].str.split(' ', expand=True)[3]","da5eb1e6":"# Checking missing values\ndisplay(df['Day'].isnull().sum())\ndisplay(df['Month'].isnull().sum())\ndisplay(df['Year'].isnull().sum())","5cff1da6":"# Dropping missing values\ndf = df.dropna()","8e4691f2":"# Removing unecessary blank spaces\ndf['Year'] = df['Year'].str.replace(' ', '')","37eb0f9e":"# Merging and adding new Datetime column\ntemp_date = df[['Day', 'Month', 'Year']]\ndf['Timestamp'] = pd.to_datetime(temp_date.astype(str).agg('-'.join, axis=1), format='%d-%B-%Y')","e5700cc5":"# Dropping unecessary columns\ndf.drop(['Year', 'Month', 'Day'], axis=1, inplace=True)","a5e45baa":"df.head()","246f613e":"# Extract EmployeeType from ReviewDetails\ndf['EmployeeType'] = df['ReviewDetails'].str.split('-', expand=True)[0]","0d0b8991":"# Checking Unique values\ndisplay(df.EmployeeType.unique())\ndisplay(print('Unique values: ', len(df.EmployeeType.unique())))","47cb9e35":"def get_employee_type(value):\n  return 'Current Employee' if 'Current' in value else 'Former Employee'","7e090400":"# Add new column EmployeeTpe\ndf['EmployeeType'] = df.apply(lambda row: get_employee_type(row['EmployeeType']),axis=1)","8732ead5":"df.head()","47e036ec":"df['Location'] = df['ReviewDetails'].str.split('-', expand=True)[1]","0e4a6f2e":"display(df.Location.unique())\ndisplay(print('Unique values: ', len(df.Location.unique())))","16d406e9":"df['Review'] = df['ReviewTitle'] + ' ' + df['CompleteReview']","1c00b108":"# Drop uneccsary columns\ndf.drop(['ReviewTitle', 'CompleteReview', 'URL', 'ReviewDetails'], axis=1, inplace=True)","ac7b0632":"df.Location = df.Location.str.strip()\ndf.Location = df.Location.str.lower()","2d02d314":"sqldf(\"\"\"select Location, count(*) from df group by Location order by count(*) desc limit 10\"\"\")","d6e0483a":"df.Location[df['Location'] == ''] = 'Unknown'","a0431604":"df.Location.unique()","25443e1e":"df.describe()","e38cfe5b":"plt.figure(figsize = (15,8))\nsns.countplot(x ='EmployeeType', data = df)\nplt.xticks(rotation=90)","a460f657":"sns.histplot(df['Rating'])","ada68ae4":"plt.figure(figsize=(8,4))\nsns.countplot(x='Rating', hue='EmployeeType',data=df,palette='viridis')","88aead15":"plt.figure(figsize = (20,8))\nsns.countplot(x ='Company', data = df)\nplt.xticks(rotation=90)","2dce227c":"dftop_10 = sqldf(\"\"\"select Company, count(*) from df group by Company order by count(*) desc limit 10\"\"\")\ndftop_10 = sqldf(\"\"\"select * from df where Company in (select Company from dftop_10)\"\"\")","00292cbe":"plt.figure(figsize = (30,8))\nsns.countplot(x=\"Company\", hue=\"Rating\", data=dftop_10)","a57af269":"dfbot_10 = sqldf(\"\"\"select Company, count(*) from df group by Company order by count(*) asc limit 10\"\"\")\ndfbot_10 = sqldf(\"\"\"select * from df where Company in (select Company from dfbot_10)\"\"\")\ndfbot_10.Rating = pd.to_numeric(dfbot_10.Rating)","7da776b5":"plt.figure(figsize = (30,8))\nsns.countplot(x=\"Company\", hue=\"Rating\", data=dfbot_10)","2dd93e72":"df_g1 = df\ndf_g1['Year'] = df_g1.Timestamp.astype(str).str[:4]\ndf_g1 = sqldf(\"\"\"select Company, Rating, Year, count(*) as count from df_g1 group by Company, Rating, Year\"\"\")\n\nfor i, company_name in enumerate(list(df.Company.unique())):\n  plt.figure(i)\n  plt.figure(figsize = (10,8))\n  sns.lineplot(x=df_g1.Year, y=\"count\", hue=\"Rating\", data=df_g1[df_g1['Company'] == company_name]).set_title(company_name)","314dd4f6":"#Dropping unecessary columns\ndf.drop(['Company', 'EmployeeType', 'Timestamp'], axis=1, inplace=True)","cbcc748b":"# Coverting 5 ratings in 3 classes\n\n# 0    Positive  (5-4 Stars)\n# 1    Neural    (2-3 Stars)\n# 2    Negative  (1 Stars)\n\ndf.Rating.replace({'1.0': 3, '2.0': 2, '3.0': 2, '4.0': 1, '5.0': 1}, inplace=True)","edf93d34":"# Converting Rating field from float to int\ndf.Rating = df.Rating.astype(float).astype(int)","1049f51e":"df = df.sample(frac=0.45)","e34aacd9":"ros = RandomOverSampler()\nX_sample, y_sample = ros.fit_resample(df[['Review']], df['Rating'])\ndf = pd.concat([pd.DataFrame(X_sample), pd.DataFrame(y_sample)], axis=1)\n\ndf.columns = ['Review', 'Rating']","456017d0":"vocab_size = 10000\nsentence_length = 50\nX = df.drop('Rating',axis=1)\ny = df['Rating']\nsentences = X.copy()","cd2d8cab":"# Resetting so that we don't get error during extracting process \nsentences.reset_index(inplace=True)","42403cca":"def generate_corpus(sentences):\n    ps = PorterStemmer()\n    corpus = []\n    sen_len = len(sentences)\n    for i in range(0, sen_len):\n        review = re.sub('[^a-zA-Z]', ' ', sentences['Review'][i]).lower().split()\n        review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n        review = ' '.join(review)\n        corpus.append(review)\n        if i%10000 == 0:\n            print('Processing sentence {0}\/{1}'.format(i, sen_len))\n    return corpus","5b81e5f1":"def convert_ohe(corpus):\n  return [one_hot(words,vocab_size) for words in corpus] ","7dad026e":"def add_padding(onehot_repr):\n  return pad_sequences(onehot_repr,padding='pre',maxlen=sentence_length)","0ada36e1":"def save_pickle(data, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(corpus, f)","c203d8c6":"def load_pickle(name):\n    with open(name, 'rb') as f:\n        return pickle.load(f)","bdf9f53a":"# Generating corpus\ncorpus = generate_corpus(sentences)","f3724c30":"# Creating \nonehot_repr = convert_ohe(corpus)","dca1a407":"# Creating word embedding. \nembedded_docs = add_padding(onehot_repr)","266c4d22":"y = pd.get_dummies(df[\"Rating\"])","b7d877c1":"# LSTM Model\nembedding_vector_features=80\nmodel=Sequential()\nmodel.add(Embedding(vocab_size,embedding_vector_features,input_length=sentence_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","c75e0a75":"X_final=np.array(embedded_docs)\ny_final=np.array(y)","146b0d39":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)","7b2003e6":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=18,batch_size=64)","738c8bf9":"# Saving model for future use\nmodel.save('model.h5')","d702fabd":"y_pred = model.predict(X_test)","3d6464de":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred,axis=1))","c9c3bac4":"from sklearn.metrics import accuracy_score\nprint('Model Accuracy', accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred,axis=1)))","e95ad79a":"data = pd.DataFrame([{'Review': 'Great place to work. Nice work culture'}, {'Review': 'Very less salary. bad work culture'}, {'Review': 'No fix working hours. Good salary hike'}])","3cceab7b":"corpus = generate_corpus(data)\nonehot_repr = convert_ohe(corpus)\nembedded_docs = add_padding(onehot_repr)\ny_pred = model.predict(embedded_docs)","2a85cc25":"predict_text = { 0:'Positive', 1:'Neural', 2:'Negative'}","e64984f3":"pd.concat([data, pd.DataFrame(np.argmax(y_pred,axis=1), columns=['Prediction']).replace(predict_text)], axis=1)","c08018c5":"# model.save('model.h5')\n\n# from keras.models import load_model\n# model = load_model('model.h5')","c2dcb9f4":"#Saving corpus for future use\n\n# with open('corpus.pkl', 'wb') as f:\n#   pickle.dump(corpus, f)\n\n# with open('corpus.pkl', 'rb') as f:\n#   corpus = pickle.load(f)","ea8c3476":"**Extracting Company names**","e9c10e69":"EmployeeType seems fairly balanced","2efbef65":"# Extracting and Preprocessing","ff801a07":"**As we can see, company name is mentioned in the URL & ReviewDetails has Datetime, Employeee type and Location.**\n\n**We will try to extract them as much as possible.**","75860571":"**Rating Distribution for Bottom 10 Companies (Review Count)**","1d6c3424":"**Timestamp-Rating Analysis**","43be63f4":"# Cleaning","4ed21d7b":"**Extracting Location from Review Details**","d5dd8a06":"# Evaluation Metrics","0c3343c6":"**For unknown reason there's a spike in reviews, especially for 5-4-3 rating stars, during the period 2017-2019**","eba491b6":"Model was trained with 45% of random sample size of the entire dataset on Google Colab. Please feel free to work with full dataset using this notebook, if you have the resources.\n\n**Github Repository:** https:\/\/github.com\/shreyas-jk\/Employer-Review-About-Their-Organisation-Kaggle","b2698124":"# Making Prediction","1164a7e8":"# Build, Train, Test\n","4f4ed36d":"# Extras","00be1cff":"# Import Libraries","84adef24":"**Locations are entered manually so they are inconsistent and unrealiable. We may not be able to use it until we clean it manually. My Bad**","49acfc0e":"**Employer Review About Their Organisation**\n\n\n**Context**\n\nEvery organization has their pros and cons which their employees feel that it should be made public so that other people who wants to join this organization make decisions based on reviews from the people.\n\n\n**Content**\n\nThis is a textual data in the form of json file. Its has more than 145k records. Each record has attributes such as\n\nReview Title\nReview Body\nReview Rating\nReviewed Company\nReview description\nAcknowledgements\nAll thanks to indeed.com to make this data public and easily available.\n\n\n**Tasks**\n\nYou task would be to predict the rating\/predict the text is positive or negative whether based on the review text. Also do some analysis of the text to get some insights and trends based on individual company\/organization.","2096d744":"**Extracting Datetime**","f2f96762":"# Load Dataset","976d365a":"# Over Sampling","d8813a0d":"Here we can see that the dataset is imbalance which can effect the model performance","6b707407":"**Merging ReviewTitle + CompleteReview, cause we don't want to lose any data.**","02e82802":"# Selecting Sample","55491972":"# EDA","f5ca21ad":"Large number of reviews belongs to TCS, IBM, Accenture, Infosys, HDFC\n","da08e25c":"**After 2017-2018, 4 & 5 Star reviews started to fall down for all companies.**","d0eeb90f":"**Rating Distribution for Top 10 Companies (Review Count)**","01189d50":"**Extracting type of employees who submitted the review**"}}