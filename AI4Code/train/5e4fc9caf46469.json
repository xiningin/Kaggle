{"cell_type":{"8b1a39ec":"code","2f3a721c":"code","bca1eba9":"code","eec9efa4":"code","c995aa30":"code","594f3593":"code","d9dfa2a4":"code","830ccfe4":"code","ac319f24":"code","a7b01780":"code","8aa317cb":"code","80f48af1":"code","cdd394cb":"code","fdac957d":"code","4d12921c":"code","baa19f16":"code","e636105c":"code","df877283":"code","189053e8":"code","bead2c92":"code","3d6ab60a":"code","7719d93e":"code","3b4e51ea":"code","4a3674f7":"code","9a1fed23":"code","cce555d7":"code","3ef11db2":"code","2324428c":"code","b9bff3e6":"code","70259d39":"code","1679bfa4":"code","7d5cd924":"code","e6f8cb3a":"code","be99adcb":"code","d6276231":"code","91a9c6f5":"code","e73c690e":"code","2495ad68":"code","ec148c80":"code","98cd3646":"code","2f5dac10":"code","e6c41ca2":"code","c61ee9fb":"code","ae570c18":"code","be87ee80":"code","fb4b2cd4":"markdown","7014bf1b":"markdown","91a2bee2":"markdown","67802098":"markdown","b3992e3f":"markdown","c1c3c955":"markdown","ad1f1675":"markdown","c08a5ffb":"markdown","48293379":"markdown","58995fed":"markdown","c0316304":"markdown","67c03f6e":"markdown","35e94ea0":"markdown","33ee5f7d":"markdown","cce8532d":"markdown","db1e0c54":"markdown","4b3941b9":"markdown","e3f8b466":"markdown","1a855226":"markdown","7ff0cc9c":"markdown","f5034d87":"markdown","6b37c6ab":"markdown","3dff653f":"markdown","8738742d":"markdown"},"source":{"8b1a39ec":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","2f3a721c":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","bca1eba9":"data.head()","eec9efa4":"data.shape","c995aa30":"col = data.columns\nprint(col)","594f3593":"y = data.diagnosis                                # Target or label\ndrop_col = ['Unnamed: 32','id', 'diagnosis']\nx = data.drop(drop_col, axis=1)                   # features           \nx.head()","d9dfa2a4":"x.shape","830ccfe4":"ax = sns.countplot(y, label = 'counts')\nB, M = y.value_counts()\nprint('number of Belign tumor', B)\nprint('number of Melignant tumor', M)","ac319f24":"x.describe()","a7b01780":"# Take first 10 features\n\ndata = x\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std.iloc[:,0:10]],axis= 1)                # take first 10 features to make 1 group of viloinplot of features\ndata = pd.melt(data, id_vars = 'diagnosis',\n               var_name='features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x= 'features', y='value',hue='diagnosis',data = data, split= True, inner='quart')\nplt.xticks(rotation=45)","8aa317cb":"# Take next 10 features\n\ndata = pd.concat([y ,data_std.iloc[:,10:20]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features', \n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x = 'features', y = 'value', data = data, hue = 'diagnosis', split = True, inner = 'quart')\nplt.xticks(rotation = 45)","80f48af1":"# Take last 10 features\n\ndata = pd.concat([y ,data_std.iloc[:,20:30]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features', \n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x = 'features', y = 'value', data = data, hue = 'diagnosis', split = True, inner = 'quart')\nplt.xticks(rotation = 45)","cdd394cb":"# As from the viloin plot concavity_worst and concave points_worst are seems to be somewhat identical\n\nsns.jointplot(x.loc[:, 'concavity_worst'], x.loc[:, 'concave points_worst'],\n              kind = 'reg')\n\n# this shows that both the features has high value of correlation between them\n# as scattering is very much close","fdac957d":"# Swarm plots\n# Take fisrt 10 features\n\nsns.set(style = 'whitegrid', palette= 'muted')\ndata = x\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std.iloc[:, 0:10]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x = 'features', y = 'value', data = data, hue = 'diagnosis')\nplt.xticks(rotation = 45)","4d12921c":"# Take next 10 features\n\nsns.set(style = 'whitegrid', palette= 'muted')\ndata = x\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std.iloc[:, 10:20]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x = 'features', y = 'value', data = data, hue = 'diagnosis')\nplt.xticks(rotation = 45)","baa19f16":"# Take last 10 features\n\nsns.set(style = 'whitegrid', palette= 'muted')\ndata = x\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std.iloc[:, 20:30]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x = 'features', y = 'value', data = data, hue = 'diagnosis')\nplt.xticks(rotation = 45)","e636105c":"a, ax = plt.subplots(figsize = (18,18))\nsns.heatmap(x.corr(), annot= True, fmt= '.1f', linewidths= 0.5, ax = ax)\n# these heatmap shows the relation between the correlation of each of the features with each another by heatmap","df877283":"drop_cols = ['perimeter_mean', 'radius_mean', 'compactness_mean', 'concave points_mean', 'radius_se', 'perimeter_se', \n             'radius_worst', 'perimeter_worst','compactness_worst', 'concave points_worst', 'compactness_se',\n             'concave points_se','texture_worst','area_worst']\ndf = x.drop(drop_cols, axis=1)\ndf.head()","189053e8":"df.shape","bead2c92":"f, ax = plt.subplots(figsize = (14,14))\nsns.heatmap(df.corr(), annot=True, fmt = '.1f', linewidths=0.5, ax = ax)","3d6ab60a":"## get the original feature and label set then spilt again newly\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size =0.3, random_state = 42)\n\nx_train_norm = (x_train - x_train.mean())\/ (x_train.max() - x_train.min())\nx_test_norm = (x_test - x_test.mean())\/ (x_test.max() - x_test.min())\n\nfrom sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(x_train_norm)","7719d93e":"x_train_norm.shape","3b4e51ea":"\n# here we gonna see the commulative sum varience ratio vs number of feeatures we gonna take to get that much percent of ratio\/accuarcy\n\nplt.figure(1, figsize=(10,8))\nsns.lineplot(data = np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components\/features')\nplt.ylabel('Cummualtive explained varience')\n\n# these shows that to get around 99% of accuaracy we have to take around 16-17 features for prediction","4a3674f7":"from sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score","9a1fed23":"x_train, x_test, y_train, y_test = train_test_split(df, y, test_size = 0.3, random_state = 42)\n\nclf_1 = XGBClassifier(random_state=42)\nclf_1 = clf_1.fit(x_train, y_train)","cce555d7":"y_pred_1 = clf_1.predict(x_test)\nprint('accuracy is : ', accuracy_score(y_test, y_pred_1)) \ncm = confusion_matrix(y_test, y_pred_1)\nsns.heatmap(cm, annot= True, fmt = 'd')","3ef11db2":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","2324428c":"select_feature = SelectKBest(chi2, k =10).fit(x_train, y_train)\n\nprint('score list: ', select_feature.scores_)\nprint('feature list: ', x_train.columns)","b9bff3e6":"x_train.shape","70259d39":"# from selectkbest function we gonna select the features of top 10 values\/scores of k\n\nx_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n\nclf_2 = XGBClassifier()\nclf_2.fit(x_train_2, y_train)\n\ny_pred_2 = clf_2.predict(x_test_2)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_2))\ncm = confusion_matrix(y_test, y_pred_2)\nsns.heatmap(cm, annot= True, fmt = 'd')","1679bfa4":"x_train_2.shape","7d5cd924":"from sklearn.feature_selection import RFECV\n\nclf_3 = XGBClassifier()\nrfecv = RFECV(estimator = clf_3, step = 1, cv = 5, scoring = 'accuracy', n_jobs = -1).fit(x_train, y_train)       # step = 1, means eliminate 1 feature at each step, cv = cross validation folds\n\nprint('optimal number of features: ', rfecv.n_features_)\nprint('best features: ', x_train.columns[rfecv.support_])","e6f8cb3a":"print('accuracy is: ', accuracy_score(y_test, rfecv.predict(x_test)))","be99adcb":"from sklearn.linear_model import LogisticRegression\nclassifier_2 = LogisticRegression(max_iter= 200)\nclassifier_2.fit(x_train,y_train)","d6276231":"y_pred_lg = classifier_2.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_lg))\ncm = confusion_matrix(y_test, y_pred_lg)\nsns.heatmap(cm, annot= True, fmt = 'd')","91a9c6f5":"from sklearn.svm import SVC\nclassifier_3 = SVC(kernel = 'rbf')\nclassifier_3.fit(x_train,y_train)","e73c690e":"y_pred_svm = classifier_3.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_svm))\ncm = confusion_matrix(y_test, y_pred_svm)\nsns.heatmap(cm, annot= True, fmt = 'd')","2495ad68":"from sklearn.naive_bayes import GaussianNB\nclassifier_4 = GaussianNB()\nclassifier_4.fit(x_train,y_train)","ec148c80":"y_pred_nb = classifier_4.predict(x_test)\nprint('Accuracy of model: ', accuracy_score(y_test,y_pred_nb))\ncm = confusion_matrix(y_test,y_pred_nb)\nsns.heatmap(cm, annot= True, fmt= 'd')","98cd3646":"from sklearn.tree import DecisionTreeClassifier\nclassifier_5 = DecisionTreeClassifier(min_samples_split=2)\nclassifier_5.fit(x_train,y_train)","2f5dac10":"y_pred_dt = classifier_5.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_dt))\ncm = confusion_matrix(y_test, y_pred_dt)\nsns.heatmap(cm, annot= True, fmt = 'd')","e6c41ca2":"from sklearn.ensemble import RandomForestClassifier\nclassifier_6 = RandomForestClassifier()\nclassifier_6.fit(x_train,y_train)","c61ee9fb":"y_pred_rf = classifier_6.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_rf))\ncm = confusion_matrix(y_test, y_pred_rf)\nsns.heatmap(cm, annot= True, fmt = 'd')","ae570c18":"# from sklearn.feature_selection import SelectKBest\n# from sklearn.feature_selection import chi2\n\n# select_feature = SelectKBest(chi2, k =10).fit(x_train, y_train)\n\n# print('score list: ', select_feature.scores_)\n# print('feature list: ', x_train.columns)\n\n# x_train_2 = select_feature.transform(x_train)\n# x_test_2 = select_feature.transform(x_test)","be87ee80":"# from selectkbest function we gonna select the features of top 10 values\/scores of k\n\nclf_7 = RandomForestClassifier()\nclf_7.fit(x_train_2, y_train)\n\ny_pred_rf10 = clf_2.predict(x_test_2)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_rf10))\ncm = confusion_matrix(y_test, y_pred_rf10)\nsns.heatmap(cm, annot= True, fmt = 'd')","fb4b2cd4":"### Plot diagnosis distribution","7014bf1b":"### 2_Univariate feature selection and XGBoost","91a2bee2":"### Droping correlated columns from the feature list","67802098":"## 5) Decision Tree classifier","b3992e3f":"## 2) Logistic Regression classifier","c1c3c955":"## 4) Naive Bayes classifier","ad1f1675":"### Visualize standardised data with Seaborn","c08a5ffb":"### 2_univariate feature selection","48293379":"### Using Joint plots for feature comparison","58995fed":"## Breast Cancer Diagnosis by Machine Learning (Project)","c0316304":"we have allready performed above steps for selectkbest algorithm in XGBoost classifier","67c03f6e":"### Seperate Target from features","35e94ea0":"## 6) Random forest classifier","33ee5f7d":"### 1_classification using XGBoost (minimal feature selection)","cce8532d":"### Loading libraries and dataset","db1e0c54":"## 3) SVM classifier","4b3941b9":"# Feature Selection Techniques to get the prediction and highest accuracy","e3f8b466":"### Obsorving the distribution of the values and their varience with Swarm plots","1a855226":"### 1_minimal feature selection - 16 features","7ff0cc9c":"#### Breast cancer diagnosis classification project based on EDA (exploratory data analysis) and different machine learning classification algorithm for finding the best classifier fit in order to dignosis and classify the Benign (noncancerous) and Malignant (cancerous) type of breast cancer ","f5034d87":"### Observing all pairwise correlation","6b37c6ab":"### Feature extraction using principle componant analysis(PCA)","3dff653f":"## 1) XGBoost Classifier","8738742d":"### 3_Recursive feature elemination with cross validation"}}