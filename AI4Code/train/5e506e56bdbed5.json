{"cell_type":{"cdd653bc":"code","e245485a":"code","d01f66f8":"code","1e378c0a":"code","e10e32cf":"code","18c21b15":"code","4e33bc37":"code","e00776ee":"code","96560485":"code","476492ca":"code","211d252a":"code","33331dad":"code","fbf31ef9":"code","d6c7a286":"code","20478ee6":"markdown","5d19c901":"markdown","849ab274":"markdown","a93bc36d":"markdown","2b768e8b":"markdown","381f9b53":"markdown","ce61c579":"markdown","57c7bc72":"markdown","9f94defd":"markdown","460e6003":"markdown","cf980a1e":"markdown","4deb03b6":"markdown","5ad135b8":"markdown","7d04b832":"markdown","7f297d43":"markdown","12102c28":"markdown","09285361":"markdown","2c07c746":"markdown","bc31ddc1":"markdown","1ce0e740":"markdown","dfb17014":"markdown"},"source":{"cdd653bc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))","e245485a":"data = pd.read_csv(\"..\/input\/column_2C_weka.csv\")","d01f66f8":"data.info()","1e378c0a":"data.head()","e10e32cf":"data['class'].value_counts()","18c21b15":"data['class'] = [1 if each == 'Abnormal' else 0 for each in data['class']]","4e33bc37":"data.head()","e00776ee":"f, ax = plt.subplots(figsize = (10,10))\nsns.heatmap(data.corr(), annot=True, linewidths=.4, fmt= '.2f',ax=ax)\nplt.show()","96560485":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(data, alpha = 0.8, figsize = (15,15))\nplt.show()","476492ca":"y = data['class']\nx = data.drop(['class'], axis = 1)","211d252a":"x = (x - np.min(x))\/(np.max(x) - np.min(x))","33331dad":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","fbf31ef9":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\nknn.fit(x_train, y_train)","d6c7a286":"score_list = []\n\nfor each in range (1,15):\n    knn_o = KNeighborsClassifier(n_neighbors = each)\n    knn_o.fit(x_train, y_train)\n    score_list.append(knn_o.score(x_test, y_test))\n\nplt.figure(figsize = (10,10))\nplt.plot(range(1,15), score_list)\nplt.xlabel('k values')\nplt.ylabel('accuracy')\nplt.show()","20478ee6":"<a id=14><\/a>\n### Test and Train Variables\nWe have to split our data to create train and test variables. To do so, we will be using `sklearn`'s `train_test_split method`.  \n\nWe set the`test_size` parameter to `0.2`, so the train values will be randomly 80% of the data.  Let's briefly describe what all four values correspond to:\n* `x_train` : randomly 80% of data with features of `x` (`pelvic_incidence`, `pelvic_tilt_numeric`, etc.)\n* `x_test` : randomly 20% (the rest) of data with features of `x`\n* `y_train` : randomly 80% of data with feature of `y` (`class`, the target feature)\n* `y_test` : randomly 20% (the rest) of data with feature of `y`  \n\nLet's visualize what I mean:\n\n![train_test](http:\/\/i64.tinypic.com\/v5agox.jpg)","5d19c901":"<a id=4><\/a>\n## Data Munging","849ab274":"<a id=1><\/a>\n## Introduction\nHere in this kernel, firstly I have to explore the [Biomechanical features of orthopedic patients dataset](https:\/\/www.kaggle.com\/uciml\/biomechanical-features-of-orthopedic-patients) by using data science techniques. Then I will be explaining what is k-Nearest Neighbors method (which we will be using in this kernel for learning), and then implement it on the dataset and get the results.","a93bc36d":"<a id=2><\/a>\n## Importing Libraries","2b768e8b":"<a id=6><\/a>\n## k Nearest Neighbors\nWe will be using the method named **k Nearest Neighbors** for learning, so let's dive in to see what is under the hood!\n<a id=7><\/a>\n### What is kNN?\nIn pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.  \n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. [Source](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)","381f9b53":"<a id=8><\/a>\n### How it Works?\nLet's go through a visual example to have the full understanding of it!\n\nWe have two types of data in the dataset (`class 1` and `class 2`), and a test data which is unlabeled yet. These are how they look like:\n![knn_overview](http:\/\/i68.tinypic.com\/2q2nx1w.jpg)\n<a id=9><\/a>\n### Overview of the Features\nAnd our data looks like this: (ignore circles for now)\n![knn](http:\/\/i67.tinypic.com\/64nwib.jpg)\n<a id=10><\/a>\n### k=3\nIf we pick `k=3`, which means our test data wil be classified by nearest three neighbours, that are 2 red squares (`class 1`) and 1 blue triangle (`class 2`). The majority is obviously `class 1`, so the test data will be classified as `class 1`, which is the red square.  \nHere the schema below visualizes what I mean, where the inner circle is the test space.\n![knn3](http:\/\/i63.tinypic.com\/2uz9jww.jpg)\n<a id=11><\/a>\n### k=7\nLet's try `k=7` instead. Now there are 3 red squares and 4 blue triangles, which means blue guys are the winner of voting system.  \nThe outer circle is the new test space as we change `k` to `7`.\n![knn7](http:\/\/i64.tinypic.com\/2ut26tg.jpg)","ce61c579":"# kNN on Biomechanical Features of Orthopedic Patients\n<br><\/br>\n![orthopedic-img-rep](https:\/\/kaggle2.blob.core.windows.net\/datasets-images\/2374\/3987\/4a58a17df89fda0afe579dde6b7f25fa\/dataset-cover.jpg)\n\n## Contents\n* [Introduction](#1)\n* [Importing Libraries](#2)\n* [Fetching Dataset](#3)\n* [Data Munging](#4)\n* [Correlations](#5)\n* [K Nearest Neighbors](#6)\n    * [What is kNN?](#7)\n    * [Overview of the Features](#8)\n    * [How it Works?](#9)\n    * [k=3](#10)\n    * [k=7](#11)\n* [Learning with kNN](#12)\n    * [Normalizing](#13)\n    * [Test and Train Variables](#14)\n    * [Initialize and Train the Classifier](#15)\n    * [Best k Values](#16)\n* [Conclusions](#17)\n","57c7bc72":"<a id=3><\/a>\n## Fetching Dataset","9f94defd":"Dataset consists of 310 entries, each indicates an individual patient; and 7 attributes, where six of them are biomechanical numeric features and the last one (`class`) is the target feature.  \n\nEach patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):  \n\n* `pelvic incidence`\n* `pelvic tilt`\n* `lumbar lordosis angle`\n* `sacral slope`\n* `pelvic radius`\n* `grade of spondylolisthesis`","460e6003":"<a id=17><\/a>\n## Conclusions\n\n* We are getting the h\u0131ghest accuracy score as we choose `k` as `5` or `7`.\n* The highest score we could get is `0.79`, which is not a pretty good score.","cf980a1e":"Firstly create our `x` and `y` variables:  \n* `y` \u2192  target feature (label)\n* `x` \u2192  all the features for training, excepting label","4deb03b6":"<a id=5><\/a>\n## Correlations\n\nIt's perfect time to examine correlations, since we just converted our target feature to numeric-type.","5ad135b8":"Our label seems like an object-type, which we do not prefer, so let's convert it to binary numeric-type.  \n\nAfter conversion:\n* `Abnormal`  \u2192 `1`\n* `Normal` \u2192 `0`","7d04b832":"Let's investigate our target feature (`class`) a bit more.","7f297d43":"<a id=16><\/a>\n### Best k Value\nNow let's create a list named `score_list`, and keep all the accuracy values of the algorithms trained in a range of (1,15), which is the number of `k`.  \nThen visualize the accuracy values with a line plot.","12102c28":"Let's take a glimpse of data by using `head` method.","09285361":"<a id=15><\/a>\n### Initialize and Train the Classifier\nAnd initialize the classifier object from `sklearn.neighbors`, then train it by using `fit` method.  \n(`n_neighbors` parameter (k size) is set to `5` by default, if we do not indicate it implicitly)","2c07c746":"<a id=13><\/a>\n### Normalizing\nNormalizing variables is vital for the sake of healthy learning. To scale all the values between 0 and 1, we have to use the following simple formula:  \n$$\\large x = \\frac{x - min(x)}{max(x) - min(x)} $$  \nSo you can think $\\large53$ as $\\large0.53$ after normalization, if the minimum is $\\large0$ and the maximum is $\\large100$ for a column.","bc31ddc1":"To visualize the correlations in more details, let's use `scatter_matrix`:","1ce0e740":"Correlations between `class` feature and other features:  \n\n* `pelvic incidence` : `0.35`\n* `pelvic tilt` : `0.33`\n* `lumbar lordosis angle` : `0.31`\n* `sacral slope` : `0.21`\n* `pelvic radius` : `-0.31`\n* `grade of spondylolisthesis` : `0.44`  \n\nSeems like all the features are *slightly correlated* with label, excepting `pelvic radius`.","dfb17014":"<a id=12><\/a>\n## Learning with kNN\n\nNow we know what kNN means and how it works. Let's implement in on our dataset with `scikit-learn` library."}}