{"cell_type":{"4e43e7d2":"code","1a719850":"code","7188f1ea":"code","7c3d7b43":"code","c1aa28df":"code","1e9e12ba":"code","e9db6cac":"code","3c47ce67":"code","2c06683c":"code","75ae2791":"code","65aadd41":"code","1ebac80b":"code","e1fdb720":"code","6336b29a":"code","e014de9d":"code","4634f21b":"code","a245c4b7":"code","e2838b85":"code","17392c89":"code","5be131af":"code","7f87b818":"code","9095966b":"code","ba8e672d":"code","ab490bb0":"code","78affa4c":"code","98d96fe7":"code","aec5298f":"code","a66d5e5f":"code","a9e76da0":"code","231239c0":"code","6c79cd8e":"code","a6b5d981":"code","ccedc382":"markdown","dd28fd47":"markdown","1b3efefc":"markdown","1fe7258d":"markdown","2eed3662":"markdown","74aec91c":"markdown","c5b5e51c":"markdown","73c3be8a":"markdown","3f55c46b":"markdown","c6d1bbe3":"markdown","b9a20891":"markdown"},"source":{"4e43e7d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport time\nimport datetime\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\n\nimport gc\nimport scipy.fftpack\nfrom scipy.signal import butter,filtfilt,freqz\n\nimport scipy as sp\nimport scipy.fftpack\n\nfrom sklearn import *\nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import Pool,CatBoostRegressor\n\nfrom sklearn.model_selection import KFold","1a719850":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","7188f1ea":"train = import_data('..\/input\/liverpool-ion-switching\/train.csv')\ntest = import_data('..\/input\/liverpool-ion-switching\/test.csv')\nss = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')","7c3d7b43":"display(train.head())\ndisplay(test.head())","c1aa28df":"def butter_lowpass_filter(data, cutoff, fs, order):\n    normal_cutoff = cutoff \/ nyq\n    # Get the filter coefficients \n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    y = filtfilt(b, a, data)\n    return y\n\ndef signaltonoise(a, axis=0, ddof=0):\n    a = np.asanyarray(a)\n    m = a.mean(axis)\n    sd = a.std(axis=axis, ddof=ddof)\n    return np.where(sd == 0, 0, ((m*m)\/(sd*sd))) ## Slight change over here","1e9e12ba":"batch_size = 500000\nnum_batches = 10\nres = 1000 # Resolution of signal plots\n\nfs = 10000       # sample rate, 10kHz\nnyq = 0.5 * fs  # Nyquist Frequency\ncutoff_freq_sweep = range(250,5000,10) # Sweeping from 250 to 4750 Hz for SNR measurement","e9db6cac":"train_copy = train.copy()","3c47ce67":"plt.figure(figsize=(20,10));\n\n# Filter requirements.\norder = 20  \nSNR = np.zeros(len(cutoff_freq_sweep))\n\nfor batch in range(num_batches):\n    for index,cut in enumerate(cutoff_freq_sweep): \n        signal_lpf = butter_lowpass_filter(train.signal[batch_size*(batch):batch_size*(batch+1)], cut, fs, order)\n        SNR[index] = signaltonoise(signal_lpf)\n    \n    plt.plot(cutoff_freq_sweep,SNR)\n\nplt.title('Signal-to-Noise Ratio Per Batch with Low Pass Filter')    \nplt.xlabel('Frequency')\nplt.ylabel('SNR')\nplt.legend(['Batch 1','Batch 2','Batch 3','Batch 4','Batch 5','Batch 6','Batch 7','Batch 8','Batch 9','Batch 10',])","2c06683c":"cutoff_freq_sweep = range(250,700,10) # Expanding a bit\n\nplt.figure(figsize=(25,10));\n\n# Filter requirements.\norder = 20  \nSNR = np.zeros(len(cutoff_freq_sweep))\n\nfor batch in range(num_batches):\n    for index,cut in enumerate(cutoff_freq_sweep): \n        signal_lpf = butter_lowpass_filter(train.signal[batch_size*(batch):batch_size*(batch+1)], cut, fs, order)\n        SNR[index] = signaltonoise(signal_lpf)\n    \n    plt.plot(cutoff_freq_sweep,SNR)\n\nplt.title('Expansion of Signal-to-Noise Ratio Per Batch with Low Pass Filter')    \nplt.xlabel('Frequency')\nplt.ylabel('SNR')\nplt.legend(['Batch 1','Batch 2','Batch 3','Batch 4','Batch 5','Batch 6','Batch 7','Batch 8','Batch 9','Batch 10',])","75ae2791":"lpf_cutoff = 600 # Sticking with 600\nbatch = 1\n\nsignal_lpf_batch_1 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_1[::res])\n\nax[0].legend(['Batch-1: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","65aadd41":"batch = 2\n\nsignal_lpf_batch_2 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_2[::res])\n\nax[0].legend(['Batch-2: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","1ebac80b":"batch = 3\n\nsignal_lpf_batch_3 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_3[::res])\n\nax[0].legend(['Batch-3: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","e1fdb720":"batch = 4\n\nsignal_lpf_batch_4 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_4[::res])\n\nax[0].legend(['Batch-4: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","6336b29a":"batch = 5\n\nsignal_lpf_batch_5 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_5[::res])\n\nax[0].legend(['Batch-5: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","e014de9d":"batch = 6\n\nsignal_lpf_batch_6 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_6[::res])\n\nax[0].legend(['Batch-6: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","4634f21b":"batch = 7\n\nsignal_lpf_batch_7 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_7[::res])\n\nax[0].legend(['Batch-7: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","a245c4b7":"batch = 8\n\nsignal_lpf_batch_8 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_8[::res])\n\nax[0].legend(['Batch-8: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","e2838b85":"batch = 9\n\nsignal_lpf_batch_9 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_9[::res])\n\nax[0].legend(['Batch-9: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","17392c89":"batch = 10\n\nsignal_lpf_batch_10 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_10[::res])\n\nax[0].legend(['Batch-10: open_channels'])\nax[1].legend(['signal', 'filtered signal'])","5be131af":"batch = 1\ntrain['signal'][batch_size*(batch-1):batch_size*batch] = signal_lpf_batch_1\ntrain['signal_undrifted'] = train['signal']","7f87b818":"batch = 2\ntrain['signal'][batch_size*(batch-1):batch_size*batch] = signal_lpf_batch_2\ntrain['signal_undrifted'] = train['signal']","9095966b":"batch = 3\ntrain['signal'][batch_size*(batch-1):batch_size*batch] = signal_lpf_batch_3\ntrain['signal_undrifted'] = train['signal']","ba8e672d":"batch = 7\ntrain['signal'][batch_size*(batch-1):batch_size*batch] = signal_lpf_batch_7\ntrain['signal_undrifted'] = train['signal']","ab490bb0":"# Test Data\ntest['signal_undrifted'] = test['signal']","78affa4c":"%%time\ndef features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] \/ d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) \/ 2\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n\n    \n    # add shifts_1\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    # add shifts_2\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan\n        \n      \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    gc.collect()\n    return df\n\ntrain = features(train)\ntest = features(test)","98d96fe7":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","aec5298f":"def f1_score_calc(y_true, y_pred): \n    return f1_score(y_true, y_pred, average=\"macro\")\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('KaggleMetric', score, True)\n\n\ndef train_model_classification(X, X_test, y, params, model_type='lgb', eval_metric='f1score',\n                               columns=None, plot_feature_importance=False, model=None,\n                               verbose=50, early_stopping_rounds=200, n_estimators=2000):\n\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {\n                    'f1score': {'lgb_metric_name': lgb_Metric,}\n                   }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X) )\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test)))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    '''for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]'''\n            \n    if True:        \n        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state=7)    \n            \n        if model_type == 'lgb':\n            #model = lgb.LGBMClassifier(**params, n_estimators=n_estimators)\n            #model.fit(X_train, y_train, \n            #        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n            #       verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            model = lgb.train(params, lgb.Dataset(X_train, y_train),\n                              n_estimators,  lgb.Dataset(X_valid, y_valid),\n                              verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds, feval=lgb_Metric)\n            \n            \n            preds = model.predict(X, num_iteration=model.best_iteration) #model.predict(X_valid) \n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_set = xgb.DMatrix(X_train, y_train)\n            val_set = xgb.DMatrix(X_valid, y_valid)\n            model = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                                     verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            preds = model.predict(xgb.DMatrix(X)) \n\n            y_pred = model.predict(xgb.DMatrix(X_test))\n            \n\n        if model_type == 'cat':\n            # Initialize CatBoostRegressor\n            model = CatBoostRegressor(params)\n            # Fit model\n            model.fit(X_train, y_train)\n            # Get predictions\n            y_pred_valid = np.round(np.clip(preds, 0, 10)).astype(int)\n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n\n \n        oof = preds\n        \n        scores.append(f1_score_calc(y, np.round(np.clip(preds,0,10)).astype(int) ) )\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    #prediction \/= folds.n_splits\n    \n    print('FINAL score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    result_dict['model'] = model\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] \/= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict","a66d5e5f":"good_columns = [c for c in train.columns if c not in ['time', 'signal','open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n\nX = train[good_columns].copy()\ny = train['open_channels']\nX_test = test[good_columns].copy()\n\n# del train, test","a9e76da0":"%%time\nparams_xgb = {'colsample_bytree': 0.375, 'learning_rate': 0.05,  'max_depth':10, 'subsample': 1, \n              'colsample_bylevel':0.5, 'colsample_bynode':0.5, 'tree_method' : 'hist',\n              'objective':'reg:squarederror', 'eval_metric':'logloss'}\n\nresult_dict_xgb = train_model_classification(X=X[0:500000*10-1], X_test=X_test, y=y[0:500000*10-1], params=params_xgb, model_type='xgb', eval_metric='logloss', plot_feature_importance=True,\n                                                      verbose=50, early_stopping_rounds=200)  ","231239c0":"%%time\nparams_lgb = {'learning_rate': 0.1, 'max_depth': 10, 'num_leaves': 400, 'boosting_type': 'dart', 'colsample_bytree': 0.7, \n              'max_bin': 300, 'metric': 'logloss', 'random_state': 7, 'n_jobs':-1}  \n\nresult_dict_lgb = train_model_classification(X=X[0:500000*10-1], X_test=X_test, y=y[0:500000*10-1], params=params_lgb, model_type='lgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=100, n_estimators=2000)\n","6c79cd8e":"preds_ensemble = 0.50 * result_dict_lgb['prediction'] + 0.50 * result_dict_xgb['prediction']    ","a6b5d981":"sub = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsub['open_channels'] =  np.array(np.round(preds_ensemble,0), np.int) \n\nsub.to_csv('submission.csv', index=False, float_format='%.4f')\nsub.head(10)","ccedc382":"## 2.1. Low Pass Filter & SNR <a class=\"anchor\" id=\"2.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","dd28fd47":"# Acknowledgements\n* https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-xgb-confmatrix\n* https:\/\/www.kaggle.com\/nxrprime\/more-low-pass-filtering\n* https:\/\/www.kaggle.com\/binaicrai\/fe-and-ensemble-mlp-and-lgbm\/edit","1b3efefc":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Libraries & Reading Files](#1)\n1. [Feature Engineering](#2)\n    -  [Low Pass Filter and SNR](#2.1)\n    -  [Batch wise preview](#2.2)\n    -  [Adding shifts till 3](#2.3)\n1. [Model](#3)\n1. [Submission](#4)","1fe7258d":"* ## 2.3. Addings shifts <a class=\"anchor\" id=\"2.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","2eed3662":"## 2.2. Batch wise preview <a class=\"anchor\" id=\"2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","74aec91c":"## 4. Submission <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","c5b5e51c":"## 2. Feature Engineering <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","73c3be8a":"## 3. Model <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","3f55c46b":"# Complete debacle of an attempt. Bummer....","c6d1bbe3":"# Commit Summary\n### Commit 1\n* Ignore - commited for reference only\n\n### Commit 2 - Public Score: 0.936\n* XGB: 'colsample_bytree': 0.375, 'learing_rate': 0.1,  'max_depth': 20,  'subsample': 1\n* LGB:  'learning_rate': 0.1, 'max_depth': 20, 'num_leaves': 2**7+1\n* For prediction: 0.6LGB + 0.4XGB\n\n### Commit 3 -  Public Score: 0.935\n* XGB: 'colsample_bytree': 0.375, 'learing_rate': 0.1,  'max_depth': 20,  'subsample': 1\n* LGB:  'learning_rate': 0.1, 'max_depth': 20, 'num_leaves': 2**7+1\n* For prediction: 0.3LGB + 0.7XGB\n\n### Commit 4 - Exceeded Time Limit\n* XGB: 'colsample_bytree': 0.375, 'learing_rate': 0.1,  'max_depth':10, 'subsample': 0.5, 'scale_pos_weight': 0.5\n* LGB:  'learning_rate': 0.1, 'max_depth': 12, 'num_leaves': 3000, 'boosting_type': 'dart', 'colsample_bytree': 0.68\n* For prediction: 0.5LGB + 0.5XGB\n* early_stopping_rounds=200\n\n### Commit 5 - Public Score: 0.936\n* XGB: 'colsample_bytree': 0.375, 'learing_rate': 0.1,  'max_depth':10, 'subsample': 0.5, 'scale_pos_weight': 0.5\n* LGB:  'learning_rate': 0.1, 'max_depth': 7, 'num_leaves': 100, 'boosting_type': 'dart', 'colsample_bytree': 0.68\n* For prediction: 0.5LGB + 0.5XGB\n* early_stopping_rounds=200\n\n### Commit 6 - Public Score: 0.936\n* XGB: 'colsample_bytree': 0.375, 'learing_rate': 0.05,  'max_depth':10, 'subsample': 1, 'eval_metric':'logloss'\n* LGB:  'learning_rate': 0.05, 'max_depth': 7, 'num_leaves': 200, 'boosting_type': 'dart', 'colsample_bytree': 0.68\n* For prediction: 0.5LGB + 0.5XGB\n* early_stopping_rounds=200\n\n### Commit 7 - was cancelled\n\n### Commit 8 - not sure what happened\n\n### Commit 9 - Exceeded Time Limit\n* XGB: 'colsample_bytree': 0.5, 'learing_rate': 0.05,  'max_depth':10, 'subsample': 0.7, 'eval_metric':'logloss'\n* LGB:  'learning_rate': 0.05, 'max_depth': 8, 'num_leaves': 200, 'boosting_type': 'dart', 'colsample_bytree': 0.68, 'max_bin': 480\n* Shifts till 2\n* For prediction: 0.5LGB + 0.5XGB\n* early_stopping_rounds=200\n\n### Commit 10 - 0.934\n* XGB: 'colsample_bytree': 0.5, 'learning_rate': 0.05,  'max_depth':10, 'subsample': 0.7, 'eval_metric':'logloss'\n* LGB:  'learning_rate': 0.05, 'max_depth': 8, 'num_leaves': 150, 'boosting_type': 'dart', 'colsample_bytree': 0.68, 'max_bin': 300\n* Shifts till 2\n* For prediction: 0.5LGB + 0.5XGB\n* early_stopping_rounds=100\n\n### Commit 11 - 0.932\n* XGB: 'colsample_bytree': 0.375, 'learning_rate': 0.2,  'max_depth':10, 'subsample': 0.9, 'eval_metric':'logloss'\n* LGB:  'learning_rate': 0.1, 'max_depth': 8, 'num_leaves': 150, 'boosting_type': 'dart', 'colsample_bytree': 0.68, 'max_bin': 350\n* Shifts till 2\n* For prediction: 0.6LGB + 0.4XGB\n* early_stopping_rounds=300\n\n### Commit 12 - ...\n* XGB: 'colsample_bytree': 0.375, 'learning_rate': 0.05,  'max_depth':10, 'subsample': 1,'colsample_bylevel':0.5, 'colsample_bynode':0.5, 'tree_method' : 'hist', 'eval_metric':'logloss'\n* LGB:  'learning_rate': 0.1, 'max_depth': 10, 'num_leaves': 400, 'boosting_type': 'dart', 'colsample_bytree': 0.7\n* For prediction: 0.5LGB + 0.5XGB\n* early_stopping_rounds=200","b9a20891":"## 1. Libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}