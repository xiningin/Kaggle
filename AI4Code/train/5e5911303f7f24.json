{"cell_type":{"7080f51b":"code","3c1ab8e0":"code","4ae0653a":"code","d9679d64":"code","c082055b":"code","0d81924c":"code","e4cc244a":"code","843e2e4d":"code","8d4b33de":"code","ff99623f":"code","6060226c":"code","758cb3d6":"code","eb1a33dc":"code","1e6f1f6e":"code","4b53ece6":"code","3accaaeb":"code","b3dafc9e":"code","e8e7c174":"code","7da93ab8":"markdown","48951054":"markdown","48092e11":"markdown","50e61a0c":"markdown","0ae95641":"markdown"},"source":{"7080f51b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n!ls ..\/input\n!ls ..\/input\/*\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping","3c1ab8e0":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ny_train = train[\"Survived\"]\ntrain.drop(\"Survived\",axis=1,inplace=True)\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","4ae0653a":"def search_title(name):\n    match = re.search(\"([A-Za-z]+)\\.\",name)\n    if match: return match.group(1)\n    else: return \"\"\ntrain.Name = train.Name.apply(search_title)\ntest .Name = test .Name.apply(search_title)","d9679d64":"train[\"Name\"].unique(), test[\"Name\"].unique()","c082055b":"train[\"Age\"].fillna(train[\"Age\"].median(), inplace=True)\ntest [\"Age\"].fillna(train[\"Age\"].median(), inplace=True)\ntrain[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True)\ntest [\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True)\ntrain[\"Cabin\"] = train[\"Cabin\"].apply((lambda x: 0 if type(x) == float else 1))\ntest [\"Cabin\"] = test [\"Cabin\"].apply((lambda x: 0 if type(x) == float else 1))\ntrain[\"TotalFamily\"] = train[\"SibSp\"] + train[\"Parch\"]\ntest [\"TotalFamily\"] = test [\"SibSp\"] + test [\"Parch\"]\ntest [\"Fare\"].fillna(train[\"Fare\"].median(), inplace=True)","0d81924c":"oneHot = OneHotEncoder()\noneHotName = OneHotEncoder(handle_unknown=\"ignore\")\nordinal = OrdinalEncoder()\ntrain = pd.concat([train,\n                   pd.DataFrame(oneHot.fit_transform(train[[\"Embarked\"]]).toarray()),\n                   pd.DataFrame(ordinal.fit_transform(train[[\"Sex\"]])), \n                   pd.DataFrame(oneHotName.fit_transform(train[[\"Name\"]]).toarray())], axis=1)\ntrain.drop(columns=[\"Sex\",\"Embarked\",\"Name\",\"Ticket\",\"PassengerId\"],inplace=True)\n\ntest  = pd.concat([test,\n                   pd.DataFrame(oneHot.transform(test[[\"Embarked\"]]).toarray()),\n                   pd.DataFrame(ordinal.transform(test[[\"Sex\"]])), \n                   pd.DataFrame(oneHotName.transform(test[[\"Name\"]]).toarray())], axis=1)\ntest.drop(columns=[\"Sex\",\"Embarked\",\"Name\",\"Ticket\",\"PassengerId\"],inplace=True)","e4cc244a":"print(\"Train shape: {}\\nTest  shape: {}\".format(train.shape, test.shape))","843e2e4d":"forest = RandomForestClassifier(min_samples_leaf=3, random_state=42)\nboost  = GradientBoostingClassifier(min_samples_leaf=3, random_state=42)\n#I did not fine tune the models so there are many space for improvements\nforest.fit(train, y_train)\nboost .fit(train, y_train)\ntrain_preds = forest.predict(train)\ntrain_preds_boost = boost.predict(train)\nprint(\"Train Accuracy\\nRandomForest: {}\\nGBDT:         {}\".format(round(precision_score(y_train,train_preds),3),round(precision_score(y_train,train_preds_boost),3)))","8d4b33de":"test.describe()","ff99623f":"scaler = StandardScaler()\ntrain_scaled = np.hstack([scaler.fit_transform(train.iloc[:,[1,4]]), train.iloc[:,[0,2,3] + [i for i in range(5,len(train.columns))]].values])\ntest_scaled  = np.hstack([scaler.transform(test.iloc[:,[1,4]]), test.iloc[:,[0,2,3] + [i for i in range(5,len(test.columns))]].values])\n#scaling the numeric features and concatenating them with the categorical ones\n#scaling does have a significant performance impact on the result (but I didn't leave out a validation set :< , feel free to fork and play around)","6060226c":"svc = SVC(random_state=42)\nsvc.fit(train_scaled, y_train)\ntrain_preds_svc = svc.predict(train_scaled)\nlog_reg = LogisticRegression(random_state=True,n_jobs=-1)\nlog_reg.fit(train_scaled, y_train)\ntrain_preds_log = log_reg.predict(train_scaled)\nprint(\"Train Accuracy\\nLogisticRegression: {}\\nSVC:                {}\".format(round(precision_score(y_train,train_preds_log),3),round(precision_score(y_train,train_preds_svc),3)))\n#0.801, 0.818","758cb3d6":"model = None\nmodel = tf.keras.Sequential()\nmodel.add(layers.Input((len(train.columns),)))\nmodel.add(layers.Dense(64,activation=tf.nn.elu))\nmodel.add(layers.Dense(32,activation=tf.nn.elu))\nmodel.add(layers.Dense(1))\nmodel.compile(loss=\"binary_crossentropy\",metrics=[\"accuracy\"],optimizer=\"adam\")\nmodel.summary()","eb1a33dc":"es = EarlyStopping(monitor=\"accuracy\",patience=5,restore_best_weights=True)\nval = int(len(train) * 0.1)\n#I left some validation data out for early stopping\nhistory = model.fit(train_scaled[:-val], y_train[:-val].values, validation_data=(train_scaled[-val:], y_train[-val:].values), callbacks=[es], epochs=40, verbose=0)","1e6f1f6e":"train_preds_ann = model.predict(train_scaled)\ntrain_preds_ann = [int(round(i[0])) for i in train_preds_ann]\nprint(\"Train Accuracy\\nANN: {}\".format(round((np.array(train_preds_ann) == y_train).sum() \/ len(train),3)))","4b53ece6":"X_preds = np.hstack([forest.predict(test).reshape(-1,1), \n                     log_reg.predict(test_scaled).reshape(-1,1), \n                     boost.predict(test).reshape(-1,1), \n                     svc.predict(test_scaled).reshape(-1,1), \n                     np.array([int(round(i[0])) for i in model.predict(test_scaled)]).reshape(-1,1)])","3accaaeb":"#I tried to use xgboost as a second-layer model but it did not go well\n#It turns out that random forest is a very good predictor so I decided to use it as the main model and change its prediction only if all the other four disagrees\npreds = []\nfor row in X_preds:\n    pred = row[0]\n    if np.array([row[1:] != pred]).sum() >= 4:\n        pred = 0 if pred == 1 else 1\n    preds.append(pred)","b3dafc9e":"sample_sub[\"Survived\"] = preds\nsample_sub.to_csv(\"submission.csv\",index=False)","e8e7c174":"print(\"\\\"Predicted survival rate\\\": {}\".format(round((sample_sub[\"Survived\"] == 1).sum() \/ len(test),3)))\nprint(\"Train survival rate:       {}\".format(round(((y_train == 1).sum() \/ len(train)),3)))\n#distribution looks good-ish","7da93ab8":"<h3>ANN<\/h3>","48951054":"<h3>Linear Models<\/h3>","48092e11":"<h3>Tree-based Models<\/h3>","50e61a0c":"<h3>Feature Preprocessing and Extraction<\/h3>","0ae95641":"<h3>Submission<\/h3>"}}