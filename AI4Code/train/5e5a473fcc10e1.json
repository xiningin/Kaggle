{"cell_type":{"0cd2a845":"code","f2e0b729":"code","45e236cc":"code","175d43ec":"code","c006cf59":"code","717fd062":"code","c24feae8":"code","983e6c27":"code","fbc0f9c3":"code","6b26a574":"code","ece96226":"code","6abdd202":"code","05e5a74b":"code","2ae38d66":"code","535c3c78":"code","fbe17d44":"code","31dd71d8":"code","19b84773":"code","300c5517":"code","4ed1ee8d":"code","f7da273f":"code","cb481734":"code","595274ce":"code","6e3414b6":"code","3104304c":"code","69428768":"code","ae9aefc2":"code","a782e855":"code","7de37274":"code","bb090b00":"code","eef2538c":"code","afcac2df":"code","b10af5cc":"code","cfa8a70b":"code","7cc3d298":"code","213a3408":"code","4dc2ef2f":"code","1de8463f":"code","859d70ad":"code","0a920668":"code","33461570":"code","a827bcd9":"code","a3f7b7d4":"code","518b0bde":"code","af717769":"code","5ab47f9e":"code","e5b7fe0e":"code","a823cced":"code","f84f49a9":"code","8df7412e":"code","4e4dc72f":"code","1f65656d":"code","c78aba6b":"code","30e1a63f":"code","7d575901":"code","fe28958a":"code","610e00dc":"code","f26c9caf":"markdown","88890d71":"markdown","ae0f1f3e":"markdown","68493b6f":"markdown","948569b1":"markdown","d910e172":"markdown","0b44cce1":"markdown","99f7defc":"markdown","153f069b":"markdown","fe49fee0":"markdown","211d37e6":"markdown","a8d0c41e":"markdown","05a46ae3":"markdown","8a035e77":"markdown","3587a8c5":"markdown","69dfeea4":"markdown","22502182":"markdown","ad46fb53":"markdown","25c919c1":"markdown","589314ab":"markdown","e3405d50":"markdown","d92b79bc":"markdown","c7fcf437":"markdown"},"source":{"0cd2a845":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f2e0b729":"def df_nan_filter(df, filter_cols):\n    row_nans_mask = df[filter_cols].isnull().any(axis=1)\n    dropped_rows = df.loc[row_nans_mask]\n    filtered_df = df.loc[~row_nans_mask]\n    return filtered_df, dropped_rows\n","45e236cc":"with open('\/kaggle\/input\/CORD-19-research-challenge\/json_schema.txt', 'r') as f:\n    print(f.read())","175d43ec":"#Uncomment if online mode is turned on\n!pip install tika","c006cf59":"# Tika is useful for reading PDFs in Python\nfrom tika import parser\nraw = parser.from_file('\/kaggle\/input\/CORD-19-research-challenge\/COVID.DATA.LIC.AGMT.pdf')\nprint(raw['content'])","717fd062":"with open('\/kaggle\/input\/CORD-19-research-challenge\/metadata.readme', 'r') as f:\n    print(f.read())","c24feae8":"meta_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv') ","983e6c27":"meta_df.shape","fbc0f9c3":"meta_df.columns","6b26a574":"meta_df.isnull().sum()","ece96226":"meta_df, dropped_rows = df_nan_filter(meta_df, ['title'])","6abdd202":"meta_df.shape","05e5a74b":"meta_df['title']","2ae38d66":"# Look at the distribution of article titles in terms of total count of each\nmeta_df['title'].value_counts()","535c3c78":"def count_ngrams(df,column,begin_ngram,end_ngram):\n    # adapted from https:\/\/stackoverflow.com\/questions\/36572221\/how-to-find-ngram-frequency-of-a-column-in-a-pandas-dataframe\n    word_vectorizer = CountVectorizer(ngram_range=(begin_ngram,end_ngram), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(df['title'].dropna())\n    frequencies = sum(sparse_matrix).toarray()[0]\n    most_common = pd.DataFrame(frequencies, \n                               index=word_vectorizer.get_feature_names(), \n                               columns=['frequency']).sort_values('frequency',ascending=False)\n    most_common['ngram'] = most_common.index\n    most_common.reset_index()\n    return most_common\n\ndef word_cloud_function(df,column,number_of_words):\n    # adapted from https:\/\/www.kaggle.com\/benhamner\/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    word_string=str(popular_words_nonstop)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                          background_color='white',\n                          max_words=number_of_words,\n                          width=1000,height=1000,\n                         ).generate(word_string)\n    plt.clf()\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\ndef word_bar_graph_function(df,column,title):\n    # adapted from https:\/\/www.kaggle.com\/benhamner\/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()","fbe17d44":"plt.figure(figsize=(10,10))\nword_bar_graph_function(meta_df,'title','Most common words in titles of papers in CORD-19 dataset')","31dd71d8":"value_counts = meta_df['journal'].value_counts()\nvalue_counts_df = pd.DataFrame(value_counts)\nvalue_counts_df['journal_name'] = value_counts_df.index\nvalue_counts_df['count'] = value_counts_df['journal']\nfig = px.bar(value_counts_df[0:20], \n             x=\"count\", \n             y=\"journal_name\",\n             title='Most Common Journals in CORD-19 Dataset',\n             orientation='h')\nfig.show()","19b84773":"value_counts = meta_df['publish_time'].value_counts()\nvalue_counts_df = pd.DataFrame(value_counts)\nvalue_counts_df['which_year'] = value_counts_df.index\nvalue_counts_df['count'] = value_counts_df['publish_time']\nfig = px.bar(value_counts_df[0:5], \n             x=\"count\", \n             y=\"which_year\",\n             title='Most Common Dates of Publication',\n             orientation='h')\nfig.show()","300c5517":"# Idea is to gather all most popular articles that have most popular words, from the most popular journal from the most popular year, with the most \n# common n-gram phrase and then data mine these articles (and then extend this to top 3 words\/journals\/years\/etc?)","4ed1ee8d":"# Observe value distribution of publish times\nmeta_df['publish_time'].value_counts()","f7da273f":"# Observe value distribution of journals\nmeta_df['journal'].value_counts()","cb481734":"# Create a DataFrame where title and publish_time are the most popular values\nvirus_2020_df = meta_df.loc[(meta_df['title'].str.contains(\"virus\")) & (meta_df['publish_time'] == '2020')]","595274ce":"virus_2020_df['journal'].value_counts()","6e3414b6":"virus_2020_df.loc[virus_2020_df['journal'] == 'PLoS One']","3104304c":"biorxiv_df = pd.read_csv(\"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/biorxiv_clean.csv\")","69428768":"biorxiv_df.shape","ae9aefc2":"clean_commerical_df = pd.read_csv(\"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv\")","a782e855":"clean_commerical_df.shape","7de37274":"clean_noncommerical_df = pd.read_csv(\"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv\")","bb090b00":"clean_noncommerical_df.shape","eef2538c":"clean_pmc_df = pd.read_csv(\"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv\")","afcac2df":"clean_pmc_df.shape","b10af5cc":"clean_df = pd.concat([biorxiv_df, clean_commerical_df, clean_noncommerical_df, clean_pmc_df])","cfa8a70b":"clean_df.shape","7cc3d298":"clean_df.columns","213a3408":"clean_df.isnull().sum()","4dc2ef2f":"clean_abstract_df, abstract_dropped_rows = df_nan_filter(clean_df, ['abstract'])","1de8463f":"clean_abstract_df.shape","859d70ad":"smoking_abstracts_df = clean_abstract_df.loc[clean_abstract_df['abstract'].str.contains(\"smoking\")].reset_index(drop=True)","0a920668":"smoking_abstracts_df","33461570":"smoking_abstract_text = ''.join(str(elem) for elem in list(smoking_abstracts_df['abstract']))","a827bcd9":"smoking_abstract_text","a3f7b7d4":"regExSmokingAbstractResults = re.findall(r\"([^.]*?smoking[^.]*\\.)\",smoking_abstract_text)    ","518b0bde":"regExSmokingAbstractResults","af717769":"regExSmokingAbstractResultsText = ''.join(str(elem) for elem in regExSmokingAbstractResults)\nregExSmokingAbstractResultsText","5ab47f9e":"#Uncomment if online mode is turned on\n!pip install pytextrank","e5b7fe0e":"import pytextrank","a823cced":"#spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used \n# in real products. spaCy comes with pretrained statistical models and word vectors, and currently supports tokenization for 50+ languages.\nimport spacy\n# load a spaCy model, depending on language, scale, etc.\nnlp = spacy.load(\"en_core_web_sm\")\nnlp","f84f49a9":"# add PyTextRank to the spaCy pipeline\ntr = pytextrank.TextRank()\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)","8df7412e":"doc = nlp(regExSmokingAbstractResultsText)\nsmoking_strings = [x for x in doc._.phrases if 'smoking' in str(x)] ","4e4dc72f":"!pip install git+https:\/\/github.com\/boudinfl\/pke.git","1f65656d":"import pke\n\n# initialize keyphrase extraction model, here TopicRank\nextractor = pke.unsupervised.TopicRank()\n\n# load the content of the document, here document is expected to be in raw\n# format (i.e. a simple text file) and preprocessing is carried out using spacy\nextractor.load_document(input=regExSmokingAbstractResultsText, language='en')\n\n# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n# and adjectives (i.e. `(Noun|Adj)*`)\nextractor.candidate_selection()\n\n# candidate weighting, in the case of TopicRank: using a random walk algorithm\nextractor.candidate_weighting()\n\n\n# N-best selection, keyphrases contains the 10 highest scored candidates as\n# (keyphrase, score) tuples\nkeyphrases = extractor.get_n_best(n=10)\n\n# print the n-highest (10) scored candidates\nfor (keyphrase, score) in extractor.get_n_best(n=10):\n    print(keyphrase, score)","c78aba6b":"!pip install rake-nltk","30e1a63f":"from rake_nltk import Rake\n\nrAbstract = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n\nrAbstract.extract_keywords_from_text(regExSmokingAbstractResultsText)\n\nrAbstract.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.","7d575901":"abstractRankedPhrases = rAbstract.get_ranked_phrases()","fe28958a":"for phrase in abstractRankedPhrases:\n    if 'smoking' in phrase:\n        print(phrase)","610e00dc":"from rake_nltk import Rake\n\nrMainText = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n\nrMainText.extract_keywords_from_text(''.join(str(elem) for elem in re.findall(r\"([^.]*?smoking[^.]*\\.)\",''.join(str(elem) for elem in list(clean_abstract_df['text'])))))\n\nrMainText.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n\nmaintextRankedPhrases = rMainText.get_ranked_phrases()\n\nfor phrase in maintextRankedPhrases:\n    if 'smoking' in phrase:\n        print(phrase)","f26c9caf":"### Most Common Words in Titles","88890d71":"# Using PKE (https:\/\/github.com\/boudinfl\/pke) Paper = http:\/\/aclweb.org\/anthology\/C16-2015","ae0f1f3e":"# Trying to use Rake (https:\/\/pypi.org\/project\/rake-nltk\/) from the natural language toolkit to find key phrases","68493b6f":"### Ok it looks like none of the top journals are PLoS One... do any articles come from this journal, though?","948569b1":"# Exploratory Data Analysis (EDA) on Meta Data","d910e172":"### Reading about Metadata","0b44cce1":"\n## Helper Functions","99f7defc":"The topics listed above demonstrate that smoking status (most likely how often someone smokes) is considered an 'important topic' in the article abstracts, which are focused on understanding COVID-19.","153f069b":"### Reading the license agreement","fe49fee0":"# Using Pytextrank to summarize text about smoking (taken from https:\/\/medium.com\/@aneesha\/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5)","211d37e6":"### Most Common Dates of Publication","a8d0c41e":"### Info about json schema","05a46ae3":"**Sources:**\n* CZI: Chan Zuckerberg Initiative, a company aimed to \"advance human potential and promote equality in areas such as health, education, scientific research and energy\".\n* PMC: PubMed Central, a free full-text archive of biomedical and life sciences journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH\/NLM).\n* bioRxiv: an open access preprint repository for the biological sciences. As preprints, papers hosted on bioRxiv are not peer-reviewed, but undergo basic screening and checked against plagiarism.\n* medRxiv: a preprint service for the medicine and health sciences and provides a free online platform for researchers to share, comment, and receive feedback on their work. \n","8a035e77":"As we can see, certain risk factors, based on the text mining results, most likely include: \n* tension \n* smoking \n* alcohol \n* salt \n* animal fats \n* body weight \n","3587a8c5":"### Looks like PLoS One is no where to be found in this subset of data...","69dfeea4":"### Let's see if PLoS One articles can be found in the set of articles that contain 'virus' in the title and that were published in 2020...","22502182":"### See which columns have NaNs and then remove the NaNs if the column is of interest","ad46fb53":"Using RegEx to find all sentences that contain the word 'Smoking'","25c919c1":"Let's remove all rows with NaNs in the **Abstract** column so that we can search for information on risk factors (i.e. smoking) in the abstracts","589314ab":"### Functions used to find most commonly used words in article titles taken from https:\/\/www.kaggle.com\/paultimothymooney\/most-common-words-in-the-cord-19-dataset","e3405d50":"# Data on potential risks factors\n","d92b79bc":"Since we don't care about which of these sources the risk factor info comes from... I will combine all of these data sources to analyze (since columns are the same).","c7fcf437":"### Most Common Journals"}}