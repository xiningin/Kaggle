{"cell_type":{"81b62f96":"code","a3e5fc2f":"code","89bc8c87":"code","9644ee23":"code","51aea675":"code","87d9f9f8":"code","ac7726c4":"code","05d4f92d":"code","4dfb000a":"code","e48abc25":"code","3fa87d3e":"code","f3e3115c":"code","443c2c02":"code","a4932a28":"code","26261bb3":"code","0f73fd20":"code","c7cc02cc":"code","2ab09438":"code","83c3675b":"code","3297a773":"code","63623e19":"code","f7498951":"code","e6ec615b":"code","d91741c6":"code","1f3a43e8":"code","74076cc0":"code","f3d12667":"code","a883c2e2":"code","d95a06b5":"code","7dd9ed08":"code","81a38984":"code","cbc4678f":"code","af0c86e5":"code","10519594":"code","8665ad53":"code","fed8867c":"code","ce2af42b":"code","aa58498f":"code","f0ee9ad1":"code","ee382e11":"code","c76b3b2a":"code","47441bbf":"code","e3569f91":"code","0b568436":"code","a0a9f280":"code","1684b901":"code","32a5999e":"code","4fac8eb1":"code","96a8fad0":"code","18958761":"code","cde26df5":"code","903338e4":"code","2fad2367":"code","d868b908":"code","87bfe1d6":"code","63240da7":"code","59497527":"code","80b82bbb":"code","9b09301b":"code","63de670b":"code","0c841804":"code","b67a9699":"code","f6564960":"code","5888d697":"code","052bb9fc":"code","0ea505b2":"code","21defba6":"code","77c7fdcf":"code","5d21f72f":"code","88e38d2f":"code","306ff2cc":"code","b1118472":"code","88643468":"code","b6bcf454":"code","faa5cdc2":"code","66ed0c25":"code","86b5f51f":"code","3c525ffa":"code","b8c80e08":"code","1d9c6d44":"code","d04631e8":"code","71c56e19":"code","2a12f089":"code","ca7ca3f5":"code","8413edef":"code","5897694e":"code","30c5445e":"code","c9da82af":"code","604ed019":"code","ce45fece":"markdown","1f559814":"markdown","45fdb1dc":"markdown","128d225e":"markdown","f8bcbd8e":"markdown","2312cdc6":"markdown","fedd069e":"markdown","ff900e9d":"markdown","4cf7c183":"markdown","a646740a":"markdown","74f8ec44":"markdown","762ae824":"markdown","127ed62b":"markdown","4c7632e7":"markdown","fbc54ac0":"markdown","b743bd51":"markdown","3d25f476":"markdown","73932d93":"markdown","b15c1a30":"markdown","6b969de8":"markdown","3b67192d":"markdown","405adad1":"markdown","ed8cb2cf":"markdown","6dc45da9":"markdown","fc87e6f3":"markdown","9d076dbd":"markdown","31b998c9":"markdown","e72bc085":"markdown","a808ad53":"markdown","486b8b66":"markdown","c8b99680":"markdown","dcdef905":"markdown","81d82ccf":"markdown","2dd451df":"markdown","76fb901f":"markdown","9bed1e50":"markdown","46f0a324":"markdown","a8e188a8":"markdown","3dabf670":"markdown","8d2534a4":"markdown","34ca7c36":"markdown","bde725b9":"markdown","ece73f56":"markdown","78b9b063":"markdown","9c363c78":"markdown","75ce6500":"markdown","298a0f9a":"markdown","99363ee0":"markdown","73477d14":"markdown","5b6df462":"markdown","3a0eb8c1":"markdown","7440d870":"markdown","579f9356":"markdown","2f9c81a8":"markdown","7633eaf6":"markdown","b5381188":"markdown","c949dc54":"markdown","909b9a31":"markdown","403eaffd":"markdown","87421f47":"markdown","b6b83981":"markdown","330ae11f":"markdown","b5dd3767":"markdown","59376a3c":"markdown","a9609a9b":"markdown","31f7cd9f":"markdown","d38d8741":"markdown","eb297dc4":"markdown","f4e8d802":"markdown"},"source":{"81b62f96":"!pip install plotly\n!pip install chart_studio\n!pip install cufflinks\n!pip install textblob","a3e5fc2f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","89bc8c87":"import plotly as py\nimport cufflinks as cf\nfrom plotly.offline import iplot","9644ee23":"py.offline.init_notebook_mode(connected=True)\ncf.go_offline()","51aea675":"df = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\", index_col=0)\ndf.head()","87d9f9f8":"df.drop([\"Clothing ID\", \"Title\"], axis=1, inplace=True)\ndf.head()","ac7726c4":"df.isnull().sum()","05d4f92d":"df.dropna(subset=[\"Review Text\", \"Division Name\"], inplace=True)","4dfb000a":"df.isnull().sum()","e48abc25":"df[\"Review Text\"].tolist()","3fa87d3e":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","f3e3115c":"def decontracted(x):\n    if type(x) is str:\n        x = x.replace('\\\\', '')\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key, value)\n        return x\n    else:\n        return x","443c2c02":"df[\"Review Text\"] = df[\"Review Text\"].apply(lambda x: decontracted(x))","a4932a28":"import string\nstring.punctuation","26261bb3":"punctuation = '\"#$%&\\'()*+-\/:;<=>?@[\\\\]^_`{|}~'\nnumbers = \"0123456789\"\n\ndef clean_text(text):\n    clean_list = [x for x in text if x not in punctuation]\n    clean_list = [x for x in clean_list if x not in numbers]\n    clean_list = [x.lower() for x in clean_list]\n    cleaned_text = ''.join(clean_list)\n    return cleaned_text","0f73fd20":"df[\"Review Text\"] = df[\"Review Text\"].apply(clean_text)\ndf[\"Review Text\"].tolist()","c7cc02cc":"from textblob import TextBlob\ndf.head()","2ab09438":"df[\"Polarity\"] = df[\"Review Text\"].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf[\"Review Length\"] = df[\"Review Text\"].apply(lambda x: len(x))\ndf[\"Word Count\"] = df[\"Review Text\"].apply(lambda x: len(x.split()))","83c3675b":"def average_word_length(x):\n    words = x.split()\n    word_length = 0\n    for word in words:\n        word_length += len(word)\n        \n    return word_length\/len(words)","3297a773":"df[\"Average Word Length\"] = df[\"Review Text\"].apply(lambda x: average_word_length(x))","63623e19":"df.head()","f7498951":"df[\"Polarity\"].iplot(kind=\"hist\", colors=\"blue\", bins=50,\n                    xTitle = \"Sentiment Polarity\",\n                    yTitle = \"Count\",\n                    title = \"Sentiment Polarity Distribution\")","e6ec615b":"df[\"Age\"].iplot(kind=\"hist\", colors=\"red\", bins=50,\n                xTitle = \"Age\",\n                yTitle = \"Count\",\n                title = \"Age Distribution\",\n                linecolor = 'black')","d91741c6":"plt.figure(figsize=(8,8))\nlabels = [\"5 stars\", \"4 stars\", \"3 stars\", \"2 stars\", \"1 star\"]\ncmap = plt.get_cmap(\"tab20c\")\ndf[\"Rating\"].value_counts().plot.pie(autopct='%1.1f%%', shadow=True, labels=labels, colors = cmap(np.arange(5)*2))","1f3a43e8":"rocket = plt.get_cmap(\"rocket\")\nfig, axes = plt.subplots(nrows=2, ncols=3,figsize=(12, 8))\none = df[df[\"Rating\"] == 1][\"Age\"]\ntwo = df[df[\"Rating\"] == 2][\"Age\"]\nthree = df[df[\"Rating\"] == 3][\"Age\"]\nfour = df[df[\"Rating\"] == 4][\"Age\"]\nfive = df[df[\"Rating\"] == 5][\"Age\"]\n\nax1 = sns.distplot(one, ax=axes[0][0], kde=False, bins=20, color=rocket(100))\nax1.set_title('One Star')\n\nax2 = sns.distplot(two, ax=axes[0][1], kde=False, bins=20, color=rocket(120))\nax2.set_title('Two Stars')\n\nax3 = sns.distplot(three, ax=axes[0][2], kde=False, bins=20, color=rocket(140))\nax3.set_title('Three Stars')\n\nax4 = sns.distplot(four, ax=axes[1][0], kde=False, bins=20, color=rocket(160))\nax4.set_title('Four Stars')\n\nax5 = sns.distplot(five, ax=axes[1][1], kde=False, bins=20, color=rocket(180))\nax5.set_title('Five Stars')\n\naxes[-1, -1].axis(\"off\")\n\nplt.tight_layout()","74076cc0":"df[\"Review Length\"].iplot(kind=\"hist\", colors=\"green\",\n                          xTitle = 'Review Length',\n                          yTitle = \"Count\",\n                          title = \"Review Length Distribution\")","f3d12667":"df[\"Word Count\"].iplot(kind=\"hist\", colors=\"#B6E880\",\n                          xTitle = 'Review Length',\n                          yTitle = \"Count\",\n                          title = \"Review Length Distribution\")","a883c2e2":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 5))\nax=df[\"Department Name\"].value_counts().plot.pie(ax = axes[0], shadow=True, colors=rocket(np.arange(5)*50))\nax1=df[\"Division Name\"].value_counts().plot.pie(ax = axes[1], shadow=True, colors=rocket(np.arange(5)*100))","d95a06b5":"df[\"Class Name\"].value_counts().iplot(kind=\"bar\", colors='rgb(95, 70, 144)',\n                                           xTitle = 'Class',\n                                           yTitle = \"Count\",\n                                           title = \"Class Distribution\")","7dd9ed08":"x = [\"This is a list of words, which are words that are in a list.\"]","81a38984":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer().fit(x)\nbag_of_words = vectorizer.transform(x)\nsum_of_words = bag_of_words.sum(axis=0)\nword_frequency = [(key, sum_of_words[0, value]) for key, value in vectorizer.vocabulary_.items()]\nword_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\nword_frequency","cbc4678f":"word_frequency[:5]","af0c86e5":"def top_n_words(x, n):\n    vectorizer = CountVectorizer().fit(x)\n    bag_of_words = vectorizer.transform(x)\n    sum_of_words = bag_of_words.sum(axis=0)\n    word_frequency = [(key, sum_of_words[0, value]) for key, value in vectorizer.vocabulary_.items()]\n    word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n    return word_frequency[:n]","10519594":"top_n_words(df[\"Review Text\"], 20)","8665ad53":"def top_n_bigrams(x, n):\n    vectorizer = CountVectorizer(ngram_range=(2,2)).fit(x)\n    bag_of_words = vectorizer.transform(x)\n    sum_of_words = bag_of_words.sum(axis=0)\n    word_frequency = [(key, sum_of_words[0, value]) for key, value in vectorizer.vocabulary_.items()]\n    word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n    return word_frequency[:n]","fed8867c":"top_n_bigrams(df[\"Review Text\"], 20)","ce2af42b":"def top_n_trigrams(x, n):\n    vectorizer = CountVectorizer(ngram_range=(3,3)).fit(x)\n    bag_of_words = vectorizer.transform(x)\n    sum_of_words = bag_of_words.sum(axis=0)\n    word_frequency = [(key, sum_of_words[0, value]) for key, value in vectorizer.vocabulary_.items()]\n    word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n    return word_frequency[:n]","aa58498f":"top_n_trigrams(df[\"Review Text\"], 20)","f0ee9ad1":"def top_n_words(x, n):\n    vectorizer = CountVectorizer(stop_words='english').fit(x)\n    bag_of_words = vectorizer.transform(x)\n    sum_of_words = bag_of_words.sum(axis=0)\n    word_frequency = [(key, sum_of_words[0, value]) for key, value in vectorizer.vocabulary_.items()]\n    word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n    return word_frequency[:n]","ee382e11":"def top_n_bigrams(x, n):\n    vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english').fit(x)\n    bag_of_words = vectorizer.transform(x)\n    sum_of_words = bag_of_words.sum(axis=0)\n    word_frequency = [(key, sum_of_words[0, value]) for key, value in vectorizer.vocabulary_.items()]\n    word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n    return word_frequency[:n]","c76b3b2a":"def top_n_trigrams(x, n):\n    vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english').fit(x)\n    bag_of_words = vectorizer.transform(x)\n    sum_of_words = bag_of_words.sum(axis=0)\n    word_frequency = [(key, sum_of_words[0, value]) for key, value in vectorizer.vocabulary_.items()]\n    word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n    return word_frequency[:n]","47441bbf":"top_unigrams = top_n_words(df[\"Review Text\"], 20)\ndf_unigrams = pd.DataFrame(top_unigrams)\ntop_unigrams","e3569f91":"from wordcloud import WordCloud\nwordcloud = WordCloud(background_color='white').generate_from_frequencies(df_unigrams.set_index(0)[1])\nplt.figure(figsize=(14,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","0b568436":"top_bigrams = top_n_bigrams(df[\"Review Text\"], 20)\ndf_bigrams = pd.DataFrame(top_bigrams)\ntop_bigrams","a0a9f280":"wordcloud = WordCloud(background_color='white').generate_from_frequencies(df_bigrams.set_index(0)[1])\nplt.figure(figsize=(14,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","1684b901":"top_trigrams = top_n_trigrams(df[\"Review Text\"], 20)\ndf_trigrams = pd.DataFrame(top_trigrams)\ntop_trigrams","32a5999e":"wordcloud = WordCloud(background_color='white').generate_from_frequencies(df_trigrams.set_index(0)[1])\nplt.figure(figsize=(14,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","4fac8eb1":"positive = (df[\"Rating\"] >= 4)\nneutral = (df[\"Rating\"] == 3)\nnegative = (df[\"Rating\"] < 3)\n\ndf[\"Review Type\"] = \" \"\ndf[\"Review Type\"][positive] = \"Positive\"\ndf[\"Review Type\"][neutral] = \"Neutral\"\ndf[\"Review Type\"][negative] = \"Negative\"\n\ndf.head()","96a8fad0":"review_type = {\"Positive\": 2, \"Neutral\": 1, \"Negative\": 0}\ndf[\"Review Type\"] = df[\"Review Type\"].map(review_type)","18958761":"df.head()","cde26df5":"X = df.iloc[:, 1].values\ny = df.iloc[:, -1].values","903338e4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)","2fad2367":"vect = CountVectorizer()\nX_train_vect = vect.fit_transform(X_train)\nX_test_vect = vect.transform(X_test)","d868b908":"from sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n    \ndummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train_vect, y_train)\ny_dummy_predictions = dummy_majority.predict(X_test_vect)\nprint('Dummy Classifier Accuracy: {:.2f}'.format(accuracy_score(y_test, y_dummy_predictions)))","87bfe1d6":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train_vect, y_train)","63240da7":"y_pred = classifier.predict(X_test_vect)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))","59497527":"vect = CountVectorizer(min_df=3, ngram_range=(1,2))\nX_train_vect = vect.fit_transform(X_train)\nX_test_vect = vect.transform(X_test)","80b82bbb":"classifier.fit(X_train_vect, y_train)\ny_pred = classifier.predict(X_test_vect)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\nprint('Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))","9b09301b":"df_modified = df[df[\"Review Type\"] != 1]\ndf_modified.head()","63de670b":"X = df_modified.iloc[:, 1]\ny = df_modified.iloc[:, -1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)","0c841804":"vect = CountVectorizer(min_df=3, ngram_range=(1,2))\nX_train_vect = vect.fit_transform(X_train)\nX_test_vect = vect.transform(X_test)","b67a9699":"from sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n    \ndummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train_vect, y_train)\ny_dummy_predictions = dummy_majority.predict(X_test_vect)\nprint('Dummy Classifier Accuracy: {:.2f}'.format(accuracy_score(y_test, y_dummy_predictions)))","f6564960":"from sklearn.metrics import classification_report\nclassifier.fit(X_train_vect, y_train)\ny_pred = classifier.predict(X_test_vect)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","5888d697":"val = [\"The dress I ordered looked good online, but disappointing when I received it. Material is not bad but design needs improvement.\", \n       \"I had read bad reviews about their satin underwear, but it turned out to be great! Happy with my purchase.\"]\nclassifier.predict(vect.transform(val))","052bb9fc":"reverse_map = {2: \"Positive\", 0: \"Negative\"}\ndf_modified[\"Review Type\"] = df_modified[\"Review Type\"].map(reverse_map)\ndf_modified.head()","0ea505b2":"df_positive = df_modified[df_modified[\"Review Type\"] == \"Positive\"]\ndf_negative = df_modified[df_modified[\"Review Type\"] == \"Negative\"]","21defba6":"plt.figure(figsize=(10, 8))\nsns.stripplot(x=\"Division Name\", y=\"Polarity\", data=df_modified, palette='coolwarm', hue='Review Type')\nplt.tight_layout()","77c7fdcf":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 8), sharex=True, sharey=True)\nax = sns.boxplot(x=\"Division Name\", y=\"Polarity\", data=df_positive, ax=axes[0], palette='rocket')\nax.set_title(\"Positive Reviews\")\nax.set_xlabel(\" \")\n\nax1 = sns.boxplot(x=\"Division Name\", y=\"Polarity\", data=df_negative, ax=axes[1], palette='rocket')\nax1.set_title(\"Negative Reviews\")\nax1.set_xlabel(\" \")\nax1.set_ylabel(\" \")\nplt.tight_layout()","5d21f72f":"plt.figure(figsize=(10, 8))\nsns.stripplot(x=\"Department Name\", y=\"Polarity\", data=df_modified)\nplt.tight_layout()","88e38d2f":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 8), sharex=True, sharey=True)\nax = sns.boxplot(x=\"Department Name\", y=\"Polarity\", data=df_positive, ax=axes[0])\nax.set_title(\"Positive Reviews\")\nax.set_xlabel(\" \")\n\nax1 = sns.boxplot(x=\"Department Name\", y=\"Polarity\", data=df_negative, ax=axes[1])\nax1.set_title(\"Negative Reviews\")\nax1.set_xlabel(\" \")\nax1.set_ylabel(\" \")\nplt.tight_layout()","306ff2cc":"import nltk\ndf_positive = df_positive[df_positive[\"Polarity\"] >= 0.25]\ndf_negative = df_negative[df_negative[\"Polarity\"] < 0]","b1118472":"positive_words = []\nfor word in df_positive[\"Review Text\"]:\n    sen = nltk.word_tokenize(word)\n    postag = nltk.pos_tag(sen)\n    for postag in postag:\n        if postag[1] == 'NN':\n            positive_words.append(postag[0])\n        elif postag[1] == 'NNP':\n            positive_words.append(postag[0])\n        elif postag[1] == 'JJ':\n            positive_words.append(postag[0])\n        elif postag[1] == 'JJR':\n            positive_words.append(postag[0])\n        elif postag[1] == 'JJS':\n            positive_words.append(postag[0])","88643468":"positive_unigrams = top_n_words(positive_words, 20)\ndf_unigrams = pd.DataFrame(positive_unigrams)\n\nwordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(df_unigrams.set_index(0)[1])\nplt.figure(figsize=(14,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","b6bcf454":"negative_words = []\nfor word in df_negative[\"Review Text\"]:\n    sen = nltk.word_tokenize(word)\n    postag = nltk.pos_tag(sen)\n    for postag in postag:\n        if postag[1] == 'NN':\n            negative_words.append(postag[0])\n        elif postag[1] == 'NNP':\n            negative_words.append(postag[0])\n        elif postag[1] == 'JJ':\n            negative_words.append(postag[0])\n        elif postag[1] == 'JJR':\n            negative_words.append(postag[0])\n        elif postag[1] == 'JJS':\n            negative_words.append(postag[0])","faa5cdc2":"negative_unigrams = top_n_words(negative_words, 20)\ndf_unigrams = pd.DataFrame(negative_unigrams)\n\nwordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(df_unigrams.set_index(0)[1])\nplt.figure(figsize=(14,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","66ed0c25":"import plotly.express as px\nimport plotly.graph_objects as go","86b5f51f":"recommended = df[df[\"Recommended IND\"] == 1]\nnot_recommended = df[df[\"Recommended IND\"] == 0]","3c525ffa":"recommended_graph = go.Histogram(x=recommended[\"Polarity\"], name=\"Recommended\", opacity=0.8)\nnot_recommended_graph = go.Histogram(x=not_recommended[\"Polarity\"], name=\"Not Recommended\", opacity=0.8)","b8c80e08":"data = [recommended_graph, not_recommended_graph]\nlayout = go.Layout(barmode=\"overlay\", title = \"Distribution of Polarity based on Recommendations Ind\")\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(\n    autosize=False,\n    width=1200,\n    height=800,\n    xaxis_title=\"Sentiment Polarity\",\n    yaxis_title=\"Count\")\niplot(fig)","1d9c6d44":"sns.set()\nplt.figure(figsize=(10, 8))\nsns.barplot(x=\"Rating\", y=\"Recommended IND\", data=df, palette=\"coolwarm\", edgecolor=\".2\", ci=None)\nplt.tight_layout()","d04631e8":"recommended[recommended[\"Rating\"] < 3][\"Review Text\"].tolist()[:3]","71c56e19":"filtered = ((df_modified[\"Rating\"] >= 4) & (df_modified[\"Recommended IND\"] == 1)) | ((df_modified[\"Rating\"] < 3) & (df_modified[\"Recommended IND\"] == 0))\ndf_filtered = df_modified[filtered]","2a12f089":"X = df_filtered.iloc[:, 1]\ny = df_filtered.iloc[:, -1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)","ca7ca3f5":"vect = CountVectorizer(min_df=3, ngram_range=(1,2))\nX_train_vect = vect.fit_transform(X_train)\nX_test_vect = vect.transform(X_test)","8413edef":"from sklearn.metrics import classification_report\nclassifier.fit(X_train_vect, y_train)\ny_pred = classifier.predict(X_test_vect)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","5897694e":"test = ['i got this top to wear with shorts as the color goes with a lot of different prints. the quality is excellent. this top runs very large, as in three  sizes too large.',\n 'i loved this dress when i saw it. however the fit was way off. i am   lbs and the small was way too big from the waist down. when the xs arrived i was sure it would be perfect. unfortunately the waist hit way too high, above my rib cage and the dress was too short. it was as if it was a petite size. i was very disappointed as this is such a pretty, easy dress to just throw on for school. unfortunately neither size looked right on me and i had to return both.']\nclassifier.predict(vect.transform(test))","30c5445e":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.pipeline import Pipeline\n\ntext_clf = Pipeline([('vect', CountVectorizer(min_df=3, ngram_range=(1,2))),\n                     ('ROS', RandomOverSampler()),\n                     ('clf', MultinomialNB())])\n\ntext_clf = text_clf.fit(X_train, y_train)\ny_pred = text_clf.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","c9da82af":"from sklearn.neural_network import MLPClassifier\ntext_clf = Pipeline([('vect', CountVectorizer(min_df=3, ngram_range=(1,2))),\n                     ('ROS', RandomOverSampler()),\n                     ('clf', MLPClassifier((100,3), verbose=3, early_stopping=True))])\ntext_clf = text_clf.fit(X_train, y_train)\ny_pred = text_clf.predict(X_test)","604ed019":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","ce45fece":"Let's take a look at our top 20 words in the reviews.","1f559814":"# Importing Libraries and Data","45fdb1dc":"# Unigram, Bigram and Trigram Analysis","128d225e":"First, we will make use of a neat process known as Random Oversampling. This process duplicates the \"Negative\" reviews in our dataset randomly until the number of \"Negative reviews\" is the same as the number of \"Positive Reviews\". The rationale behind this is that an imbalanced dataset might lead to poorer predictive performance. Hence, equalizing the dataset might improve performance.","f8bcbd8e":"![](https:\/\/glassboxmedicine.files.wordpress.com\/2019\/02\/confusion-matrix.png)","2312cdc6":"![](https:\/\/miro.medium.com\/max\/1872\/1*pOtBHai4jFd-ujaNXPilRg.png)","fedd069e":"There are many ways to deal with contractions such as tokenization, stemming and lemmatization with different libraries (such as NLTK). But I will just use a modified version of the list of contractions from wikipedia (https:\/\/en.wikipedia.org\/wiki\/Wikipedia:List_of_English_contractions) for convenience sake.","ff900e9d":"Results improved, but only marginally. One likely cause is that neutral reviews have a mix of positive and negative sentiments, confusing the algorithm. In that case, let's focus on only the positive and negative reviews as they are generally more important.","4cf7c183":"# Modified Sentiment Analysis","a646740a":"Next, we will look at the features we created. We have already analyzed the polarity, so let's focus on the review text length and word length.","74f8ec44":"Seems like \"Clothing ID\" and the \"Title\" is not that useful for us. So let's drop them from the dataset.","762ae824":"Now, there are two ways we can optimize our Sentiment Classifier even further: by increasing Precision or by increasing Recall of our \"Negative\" review predictions. Remember, we will want a higher value of Precision if we want to be sure that when the classifier predicts a \"Negative\" review, it is actually a \"Negative\" review. We will want a higher value of Recall if we want the classifier to identify more \"Negative\" reviews correctly. \n\nHowever, there is always something to keep in mind: the precision-recall tradeoff. If we optimize one value, the other other value will inevitably drop. That is, there is no way to increase both precision and recall at the same time past a certain value. You can see this phenomena in the diagram below.","127ed62b":"Majority of our review is good, with over 77% 4\/5 stars. It also seems that our reviews, regardless of positive or negative, is distributed similarly across age groups (i.e. no age group seem to favour our clothes more than others). ","4c7632e7":"From here, we can see some useful key words such as \"dress\", \"material\", \"fabric\", and \"color\". But this is mostly not that useful as there are too many stopwords (commonly occuring words that have no context value, such as \"this, it, the, is\" etc). Let's remove the stopwords and see if our analysis turns up with something more useful.","fbc54ac0":"In this final section, we will analyze the rest of the features we have not yet gone through. Let's start with the \"Recommended IND\" (whether customers recommended the product or not).","b743bd51":"Let's first create a function that can read in a list of words and return us the top n number of words and their frequencies.","3d25f476":"Let's use a unigram vectorizer with no stopwords filtered as a baseline.","73932d93":"And if we want the top 5 words:","b15c1a30":"It's clear that some of these reviews are contradictory, which makes it difficult for our sentiment classifier to predict accurately. Let's remove them from the dataset. We will set a filter that if a review has a rating of 4 or more, it has to be recommended. If a review has a rating of less than 3, it cannot be recommended.","6b969de8":"Looks like our hypothesis is true. Next, let's convert our \"Review Text\" into one large corpus in the form of a list.","3b67192d":"# Data Cleaning and Preparation","405adad1":"Let's check for missing values.","ed8cb2cf":"Let's first take a look at the distribution of sentiment polarity in this dataset. To be clear, a polarity of 1 is overwhelmingly positive, a polarity of -1 is overwhelmingly negative and a polarity of 0 is neutral. ","6dc45da9":"Seems like the most common complaint revolves around the size and fitting of the clothes. Sometimes it's too big, sometimes it's too small. It doesn't look like it does on the model, and it's too short etc.","fc87e6f3":"If you realized, earlier I had already utilized a confusion matrix. However, I did not explain what it was as I used it for multi-class classification, which was unsuitable for this diagram (binary classification). That being said, once you understand the concept behind a confusion matrix, it is easy to extrapolate your interpretation to as many classes as you want.\n\n1. Top left of the confusion matrix: True Positives --> Labels which the algorithm predicted as Positive and are actually Positive.\n\n2. Top right of the confusion matrix: False Positives --> Labels which the algorithm predicted as Positive but are actually Negative.\n\n3. Bottom left of the confusion matrix: False Negatives --> Labels which the algorithm predicted as Negatives but are actually Positive.\n\n4. Bottom right of the confusion matrix: True Negatives --> Labels which the algorithm predicted as Negative and are actually Negative.\n\nIn this case, an ideal confusion matrix would be one where the values along the right diagonal (False Positives and False Negatives) are 0.","9d076dbd":"Great! Now we can be sure that any new review can be classified with a 94% accuracy. Let's dive deeper into the positive and negative reviews.","31b998c9":"In terms of positive reviews, most departments score the same except for Jackets and Trend. They are also the departments with the least number of reviews, so this is unsurprising. In terms of negative reviews, we can see that the Intimate department attracts more negative reviews than the others, with lower quartiles compared to others, which is unexpected.\n\nFinally, we will take a look at the 20 most occuring words for positive and negative reviews. However, we will only want to look at nouns and adjectives, so we will use the Natural Language Tool Kit (NLTK) library to help us do this. Additionally, we will impose an additional filter that requires positive reviews to have a polarity of >= 0.25 and negative reviews to have a polarity of <0.","e72bc085":"We want to remove all punctuation except for fullstops, commas, and exclamation marks.","a808ad53":"OK, so this is quite consistent with our findings. Better Sentiment Polarity equates to higher ratings and more recommendations. Let's see how recommendations are related to review ratings next.","486b8b66":"Unsurprisingly, higher ratings lead to more instances of the product being recommended. However, there are still some products that have low rating yet are still recommended. Let's explore some of these reviews.","c8b99680":"As you can see, for both models, when the either precision or recall increase, the other decreased. That's it! Thanks for reading through this notebook and don't forget to upvote if you liked it!","dcdef905":"Seems like there are many contractions in this corpus of text. Let's replace them in the next portion of our notebook.","81d82ccf":"Let's take a look at the top 20 bigrams and trigrams too.","2dd451df":"![](https:\/\/c1.wallpaperflare.com\/preview\/191\/476\/40\/fashion-clothing-shop-clothes.jpg)","76fb901f":"Since we will be working on Sentiment Analysis based on the \"Review Text\", there is no way for us to fill in the missing \"Review Text\" data. Let's drop it from the dataframe.\n\nAdditionally, it seems that the missing values for \"Division Name\", \"Department Name\" and \"Class Name\" is the same. Let's test that hypothesis by dropping null values for \"Division Name\".","9bed1e50":"Seems like the length of reviews also tend towards the high side (94+ words). As most of our reviews are positive, we can infer that positive reviews are likely to be long and have many word\/characters.","46f0a324":"Great, now let's put this into a function.","a8e188a8":"# Text Cleaning","3dabf670":"Looking at the positive reviews, it seems that each Division performs almost equally well. The General Division performs minimally better with higher quartiles and less outliers with negative polarity. On the other hand, the Initmate Division seems to perform worse than the others in terms of negative reviews. It clearly has lower quartiles and with few outliers to offset this observation. Thus, we can be sure in our conclusion.","8d2534a4":"Here we can see that even a classifier that just predicts the most frequent value (\"Positive\"\/1) has an accuracy of 77%. This is due to the imbalanced nature of the dataset where most of the reviews are positive. Therefore, accuracy alone is not a reliable metric, and we have to take into account precision and recall as well.","34ca7c36":"# Positive\/Negative Review Analysis","bde725b9":"# Data Visualization","ece73f56":"Seems like it didn't fare much better than the dummy classifier. The problem probably stems from the fact that we used a default unigram vectorizer. Let's tune the paramters a bit. We will tune the vectorizer to take into account unigrams and bigrams that only occur in 3 reviews or more.","78b9b063":"A recap that Tops and Dresses occupy the majority of reviews here, followed closely by bottoms. It seems they are distributed quite similarly. However, there is an apparent trend that as the number of reviews increase, the number of polarizing reviews (more positive and more negative) reviews increase too.","9c363c78":"# Content","75ce6500":"The objective of our analysis today is to look through the reviews, determine if they are positive or negative (sentiment analysis) and find out what the customers like and dislike about the clothing. We will also find out what is the most popular and least popular items, as well as look at the distribution of the customers according to their age groups.\n\nThis will help us optimize the product and market strategy for this e-commerce store. Please upvote if you liked this notebook!","298a0f9a":"Let's do the same for Departments.","99363ee0":"We can see that most reviews have 500 or more characters. This can be a useful for sorting out authentic reviews from fake reviews (bots).","73477d14":"This makes sense, positive reviews tend to have greater polarity than negative reviews. However, it seems like there are more polarized negative reviews in the General Division as well as General Petite Division. This is possibly because there is a larger sample size in those two divisions.","5b6df462":"# Building a Sentiment Classifier","3a0eb8c1":"What exactly are accuracy, precision and recall? These are metrics to measure the performance of a predictive model. In essence:\n\n1. Precision --> For all labels that were predicted positive by our algorithm, what % of them are actually positive? (i.e. We want an algorithm where not all true positive labels are predicted, but when it does predict a positive label, we can be confident that it's right.)\n\n2. Recall --> For all labels that were predicted by our algorithm, if they were truly positive or classified wrongly as negative, what % of them are positive? (i.e. We want an algorithm that rarely fails to detect true positive labels, thereby minimizing false negatives.)\n\n3. Accuracy --> For all labels, what % of them did our algorithm predict correctly?\n\nPrecision is important in customer-facing cases, where people tend to remember the failure of an algorithm even if it performs well most of the time. For example, a query suggestion in a web search interface.\n\nRecall is especially important in the healthcare industry, where we want to be sure that if the AI predicts a tumor, it is actually a tumor.","7440d870":"![](https:\/\/bbsmax.ikafan.com\/static\/L3Byb3h5L2h0dHBzL2ltZzIwMTguY25ibG9ncy5jb20vYmxvZy8xMDEyNTkwLzIwMTkwMy8xMDEyNTkwLTIwMTkwMzI3MTIyMTEwNjE4LTk5MzY2Nzg4OS5wbmc=.jpg)","579f9356":"Let's use the filtered data to optimize the performance of our Sentiment Classifier.","2f9c81a8":"# Distribution of Department, Division and Class","7633eaf6":"# Miscellaneous Features Analysis","b5381188":"As you can see, our algorithm is quite balanced with a Precision of 76% and a Recall of 80% for \"Negative\" review predictions. To optimize Precision or Recall any further will lead to skewed values. This is demonstrated below.","c949dc54":"Neat. Now let's map the Review Type from categorical data to numerical data. Positive = 1, Neutral = 0, and Negative = -1","909b9a31":"We can see that there is a normal distribution centered around 0.175 polarity. Most of the reviews were positive, and a small fraction was negative. Now let's explore the distribution of review ratings (how good or bad were the reviews exactly?) and the reviewers age (do certain age groups tend to have a better opinion of our clothes?)","403eaffd":"From here, we are able to identify several key features of the clothes which customers liked. For example, the quality\/material of the clothes.. How soft and comfortable the clothes are.. The fabric and color etc.","87421f47":"From this, we can tell that the median of our age groups is around 38 years old, with younger people constituting to a larger proportion of our customers. However, a sizeable amount of customers still come from age 40+.","b6b83981":"Now, let's try one of the most well known classifiers for text data, Naive Bayes. But before that, we will create a dummy classifier that predicts the most frequent value as a benchmark.","330ae11f":"This is much more useful. We can see that the dresses are very well liked, and many of the reviews praise the fitting of the clothes as well as the aesthetics.","b5dd3767":"Even though we can get a rough judgement on the polarity of reviews based on the TextBlob sentiment polarity function, let's create a classifier based on our own terms. For this purpose, I will classify reviews with 4 and 5 stars as positive reviews, 3 as neutral, and below 3 as negative. Let's reflect this in a new feature column.","59376a3c":"Great. Now let's engineer some text features before moving on to data visualization.","a9609a9b":"![](https:\/\/miro.medium.com\/max\/2246\/1*o_KfyMzF7LITK2DlYm_wHw.png)","31f7cd9f":"From here, we can see that the most popular type of item are dresses, followed by an assortment of tops.","d38d8741":"# Analyzing Engineered Features","eb297dc4":"This dataset includes 23486 rows and 10 feature variables. Each row corresponds to a customer review, and includes the variables:\n\n**Clothing ID**: Integer Categorical variable that refers to the specific piece being reviewed.\n\n**Age**: Positive Integer variable of the reviewers age.\n\n**Title**: String variable for the title of the review.\n\n**Review Text**: String variable for the review body.\n\n**Rating**: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n\n**Recommended IND**: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n\n**Positive Feedback Count**: Positive Integer documenting the number of other customers who found this review positive.\n\n**Division Name**: Categorical name of the product high level division.\n\n**Department Name**: Categorical name of the product department name.\n\n**Class Name**: Categorical name of the product class name.","f4e8d802":"# Feature Engineering"}}