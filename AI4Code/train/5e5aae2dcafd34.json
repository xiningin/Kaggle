{"cell_type":{"4d0c71f1":"code","980944fe":"code","3ac9237e":"code","45b18780":"code","bc0fedad":"code","292b3bf8":"code","f61cba3d":"code","6bb049da":"code","0d1edf18":"code","517ead63":"code","d3833552":"code","a56eb473":"code","84c207b3":"code","1e55fa8c":"code","f874d3a9":"code","d7f710d2":"markdown","e44f259d":"markdown","b1e72441":"markdown","1150fa4f":"markdown","cf6ff118":"markdown","8469ec48":"markdown","274cdb60":"markdown","d62fa185":"markdown","e098156b":"markdown","29db404b":"markdown","8f28e425":"markdown"},"source":{"4d0c71f1":"#Used for navigating files\nimport os\n\n#Used for handling raw data\nimport numpy as np\nimport pandas as pd\n\n#Used for visualizations\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n#Used for scraping tweets about Chris D'Elia\n#import GetOldTweets3 as got\n\n#Used for handling dates\/datetime objects\nimport time\nfrom datetime import datetime, timedelta\n\n#Used for assessing model performance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n#Used for saving model\nimport pickle\n\n#Used for preprocessing\/modelling of text data\nimport spacy\nimport random","980944fe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3ac9237e":"#Import chris's follower count since 2016\nfollowers_2016 = pd.read_csv('\/kaggle\/input\/chris-delia-twitter-info\/Chris_Followers_From_2016.csv')\n\n\n#Plot the historical follower count\nplt.figure(figsize=(22,10))\nsns.lineplot(x='Date', y='Followers', data=followers_2016, markers=True, linewidth=3, marker='o', markersize=10)\nplt.title(\"Chris D'Elia Follower Count From Oct. 2016\", size = 30, fontweight='bold')\nplt.xlabel('Date', size=20, fontweight='bold')\nplt.ylabel('Followers', size=20, fontweight='bold')\nplt.xticks(rotation=65, size=18, horizontalalignment='right')\nplt.yticks(size=20)\nplt.axvspan(xmin=44, xmax=46, color='red', alpha=0.5)\nplt.axvspan(xmin=32, xmax=34, color='green', alpha=0.5)\nplt.show()","45b18780":"#Make a function to extract tweets using GOT3\ndef download_tweets(StartDate, FinishDate, Query, num_tweets, wait_time):\n    '''\n    Downloads tweets within a certain time period and returns a dataframe of the tweets.\n    Note: to avoid an error, approximately less than 10k tweets should be scraped every 10 minutes.\n    \n    Startdate and Finishdate should be strings. String of date, formatted: 'yyyy-mm-dd'. \n    Query = string of query Ex: \"Chris D'Elia\"\n    num_tweets = integer value for number of tweets to be extracted per day\n    wait_time = integer value for number of seconds to wait before extracting tweets for the next day\n    '''\n    #Convert the strings into datetime objects (for tracking time)\n    start = datetime.strptime(StartDate, '%Y-%m-%d')\n    finish = datetime.strptime(FinishDate, '%Y-%m-%d')\n      \n    #Create storage place for tweets - dataframe\n    df = pd.DataFrame(columns=['date', 'username','tweet', 'favorites', 'retweets', 'hashtags'])\n    \n    while start < finish:       \n        #Setup criteria for tweet extraction - 7500 tweets per day\n        tweets_crit = got.manager.TweetCriteria().setQuerySearch(Query).setSince(start.strftime('%Y-%m-%d'))\\\n                                                 .setUntil((start + timedelta(days=1)).strftime('%Y-%m-%d'))\\\n                                                 .setMaxTweets(num_tweets)\n        \n        #Query\/download the tweets\n        daily_tweets = got.manager.TweetManager().getTweets(tweets_crit)\n        \n        #Extract the tweets and concatenate the tweets to the storage df\n        tweets = [[tweet.date, tweet.username, tweet.text, tweet.favorites, tweet.retweets, tweet.hashtags] for \\\n                                                                                            tweet in daily_tweets]\n        df_tweets = pd.DataFrame.from_records(tweets, columns=['date', 'username','tweet', 'favorites', 'retweets', 'hashtags'])\n        df = pd.concat([df, df_tweets], axis = 0)\n        \n        #Puts program to sleep - prevents error of too many requests at one time\n        time.sleep(wait_time)\n        \n        #Update StartDate\n        start = start + timedelta(days=1)\n    \n    #Return the df of tweets\n    return df\n\n#Scrape the tweets from the two time periods being compared: Save a version of the raw data \n#tweets_2020 = download_tweets(\"2020-06-12\",\"2020-07-27\", \"Chris DElia\", 7500, 600)\n#tweets_2020.to_csv('summer_2020_tweets.csv')\n#tweets_2019 = download_tweets(\"2019-06-01\",\"2019-08-01\", \"Chris DElia\", 1000, 60)\n#tweets_2019.to_csv('summer_2019_tweets.csv')","bc0fedad":"#Read in the tweets, reset index and drop entries where there is no tweet\ntweets_2020 = pd.read_csv('\/kaggle\/input\/chris-delia-twitter-info\/summer_2020_tweets.csv', index_col='Unnamed: 0')\ntweets_2019 = pd.read_csv('\/kaggle\/input\/chris-delia-twitter-info\/summer_2019_tweets.csv', index_col='Unnamed: 0')\n\n#Cleans tweets that have not been categorized yet, fresh from download_tweets function\ndef clean_tweets(dataframe):\n    \"\"\"\n    Input is a dataframe from the tweet scraping function, it returns a model-ready version of the data.\n    \"\"\"\n    \n    #Fill missing values for hashtags - replace null values with string 'None'\n    dataframe['hashtags'].fillna('None', inplace = True)\n    \n    #Remove datapoints where there is a missing value for tweet - it will be useless for the sentiment anlaysis\n    dataframe.dropna(inplace = True)\n    \n    #Convert datetime to string and extract the date in format (YYYY-MM-DD)\n    dataframe['date'] = dataframe['date'].apply(lambda date: date.split(' ')[0])\n    dataframe.reset_index(drop=True,inplace=True)\n    \n    return dataframe\n\n#Use function on extracted tweets\ntweets_2020 = clean_tweets(tweets_2020)\ntweets_2019 = clean_tweets(tweets_2019)\n\ntweets_2020.sample(3)   #Example of df","292b3bf8":"#Reading in sentiment140 tweets\nsent140_filepath = 'Sentiment140.csv'\nsent140_cols= [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nsent140_encoding = \"ISO-8859-1\"\nsentiment140 = pd.read_csv(sent140_filepath, encoding=sent140_encoding, names=sent140_cols)\n\n#Drop unneccessary data and convert labels into 'positive' or 'negative'\nsentiment140.drop(['ids', 'date', 'flag', 'user'], axis=1, inplace=True)\nsentiment140['target'].replace({0:'negative',4:'positive'}, inplace=True)","f61cba3d":"#Create blank spacy model that will be trained on sentiment140 data\nnlp = spacy.blank('en')\n\n#Create pipeline in spaCy (spaCy pipes) to create preprocessing\/transforming object for tokens\ntext_cat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\":True})\n\n#Add the pipeline to the spacy model\nnlp.add_pipe(text_cat)\n\n#Add labels (target categories) to the classifier\ntext_cat.add_label('negative')\ntext_cat.add_label('positive')","6bb049da":"#Define inputs and targets\ninputs = sentiment140['text']\ntargets = sentiment140['target']\n\n#split the training set into train\/validation sets\nX_train, X_val, y_train, y_val = train_test_split(inputs, targets, test_size=0.20, random_state=42)\nprint(X_train.shape, X_val.shape)\n\n#Convert dataframes into numpy arrays - used for training\/predictions\nX_train = X_train.values\nX_val = X_val.values\ny_train = y_train.values","0d1edf18":"#Import library\/objects used for making the function\nimport random\nfrom spacy.util import minibatch\n\n#Making a function to train the model. \n#Arguments will be: x_train, y_train, optimizer, number of epochs\ndef train(x_train, y_train, optimizer, num_epochs):\n    \"\"\"\n    Function to train the spaCy model, takes four arguements defined in the following manner.\n    \n    x_train = numpy array of texts\/documents\n    y_train = numpy array of labels (positive\/negative) for corresponding texts\/documents\n    optimizer = spaCy object used to initialize weights for the CNN.\n    num_epochs = Integer defining number of epochs used to train the model.\n    \"\"\"\n    #Define the random seed so the results are reproducable\n    random.seed(42)\n    spacy.util.fix_random_seed(42)\n    \n    #Setup training data for nlp.update(). Each label is a dictionary as described above.\n    train_labels = [{'cats':{'negative':label=='negative', 'positive':label=='positive'}} for label in y_train]\n    train_data = list(zip(x_train, train_labels))\n    \n    #Define a dictionary to store losses for each epoch\n    losses={}\n    for epoch in range(num_epochs):     \n        #Create batch generator \n        batches = minibatch(train_data, size=1024)\n\n        #Iterate through minibatches\n        for batch in batches:\n            #Each batch is a list of (text, label) tuples.\n            #But we need to send separate lists for texts and labels to update().\n            texts, labels = zip(*batch)\n            nlp.update(texts, labels, drop=0.1, sgd=optimizer, losses=losses)\n        print(losses)\n\n#Defining arguments to enter into the function\noptimizer = nlp.begin_training() #Initialize the model weights randomly\nepochs = 10\n\n#Call the function with the defined parameters to train the nlp model\n#train(X_train, y_train, optimizer, epochs)","517ead63":"#Optional: save the model\n\n#Use the open() function to open a file. Set the file mode to 'wb' to open the file for writing in binary mode. \n#Wrap it in a with statement to ensure the file is closed automatically when you\u2019re done with it.\n#The dump() function in the pickle module takes a serializable Python data structure, serializes it into a binary, \n#Python-specific format using the latest version of the pickle protocol, and saves it to an open file.\n\n\n#with open('spaCy_sent140_model_v3', 'wb') as file:\n    #pickle.dump(nlp, file)","d3833552":"#Loading in the model\n\n#The pickle module uses a binary data format,so you should always open pickle files in binary mode.\n#The pickle.load() function takes a stream object, reads the serialized data from the stream, \n#creates a new Python object, recreates the serialized data in the new Python object, and returns the new Python object.\nwith open('spaCy_sent140_model_v2', 'rb') as file:\n    nlp = pickle.load(file)\n    ","a56eb473":"#Make a function to predict sentiment\nimport random\ndef sentiment_predict(pred_data, sample=False):  \n    \"\"\"\n    Function used to predict sentiment of text documents, returns a pandas dataframe of predictions.\n        \n    pred_data = A pandas series\/dataframe or numpy array of texts\/documents\n    sample: Boolean value that will return an example of the sentiment prediction for a randomly chosen tweet when True.\n            It returns the tweet, probabilities for a 'negative' label and 'positive' label and the choosen label.\n    \"\"\"  \n    #Tokenize the documents\/tweets\n    tokenized_data = [nlp.tokenizer(tweet) for tweet in pred_data]\n    \n    #Use the textcategorizer object from the trained model to make predictions\n    textcat = nlp.get_pipe('textcat')\n    \n    #Use textcat to get the score for each tweet's label\n    scores,_ = textcat.predict(tokenized_data)\n    \n    #Get model predictions from the score\n    predicted_labels = scores.argmax(axis=1)\n    \n    #Pick a random tweet from the df\n    number = random.randrange(1,pred_data.shape[0])\n    \n    #Show an example of what the score looks like for each label\n    if sample == True:\n        print(f'Random tweet:  \\n{pred_data[number]}\\n')\n        print('Example of scores for the random tweet:\\nnegative    positive')\n        print(scores[number])\n        print('The tweets label is: ', textcat.labels[predicted_labels[number]])\n    \n    #Convert the predicted labels from integers to negative and positive labelsto match the format of the validation labels\n    predicted_labels = pd.DataFrame(predicted_labels)\n    predicted_labels.replace({0:'negative', 1:'positive'}, inplace=True)\n    \n    #return the predicted labels to assess the models performance\n    return predicted_labels","84c207b3":"#Make predictions for the sentiment140 validation set\nval_preds = sentiment_predict(X_val, sample=True)\n\n#Compare validation predictions to the labels\nprint('\\n', confusion_matrix(val_preds,y_val))\nprint(classification_report(val_preds,y_val))","1e55fa8c":"#Predict the sentiment of the tweets mentioning Chris D'Elia\ntweets_2020['Sentiment'] = sentiment_predict(tweets_2020['tweet'])\ntweets_2019['Sentiment'] = sentiment_predict(tweets_2019['tweet'])\n\n#Save the results\n#tweets_2020.to_csv('2020_sentiment.csv')\n#tweets_2019.to_csv('2019_sentiment.csv')\n\n#Example\ntweets_2020[['Sentiment', 'tweet']].sample(10)","f874d3a9":"#Create the subplots\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,7))\n\n#Plot the sentiment distribution for 2019 tweets\nsns.countplot(tweets_2019['Sentiment'], ax=axes[0], order=['positive','negative'])\naxes[0].set_title('Sentiment Distribution - 2019 Tweets', size=15, fontweight='bold')\n\n#Plot the sentiment distribution for 2020 tweets\nsns.countplot(tweets_2020['Sentiment'], ax=axes[1], order=['positive','negative'])\naxes[1].set_title('Sentiment Distribution - 2020 Tweets', size=15, fontweight='bold')\nplt.show()\n\n#Positive tweets\npos_2019 = tweets_2019[tweets_2019['Sentiment']=='positive']['Sentiment'].count()\npos_2020 = tweets_2020[tweets_2020['Sentiment']=='positive']['Sentiment'].count()\n\n#Negative tweets\nneg_2019 = tweets_2019[tweets_2019['Sentiment']=='negative']['Sentiment'].count()\nneg_2020 = tweets_2020[tweets_2020['Sentiment']=='negative']['Sentiment'].count()\n\n#Print the ratio of negative to positive tweets\nprint(f'Ratio of negative:positive tweets for Summer 2019 \\n{neg_2019\/neg_2019} : {(pos_2019\/neg_2019).round(2)}')\nprint(f'\\nRatio of negative:positive tweets for Summer 2020 \\n1 : {(pos_2020\/neg_2020).round(2)}')","d7f710d2":"# 3.0 - Data Cleaning\n\n### 3.1- Chris D'Elia Tweets \nThe tweets extracted from the GetOldTweets3 library contain the following 6 fields. <br>\n1) **date:** the date of the tweet (2020-06-06 23:56:08+00:00)<br>\n2) **username:** the user that tweeted (Ex: savechrisdelia)<br>\n3) **tweet:** the text of the tweet <br>\n4) **favorites:** The number of favorites that tweet received.<br>\n5) **retweets:** The number of retweets that tweet received. <br>\n6) **hashtags:** The hashtags included with that tweet (Ex: #chrisdelia)\n","e44f259d":"### 4.4 - Validating the TextCategorizer\nTo make predictions using the TextCategorizer object the documents\/tweets can simply be entered as a numpy array. After the predictions are made the model can be assessed using a number of metrics. The classification report is a general summary that provides information on metrics like accuracy, recall, sensitivity etc.","b1e72441":"# Setup","1150fa4f":"# 2.0 - Tweet Scraping - GetOldTweets3\nUse the GetOldTweets3 API to extract tweets that mention Chris D'Elia. The following useful information about the tweets were also included.\n* Date of tweet\n* Username\n* Num. of favorites\n* Num. of retweets\n* hashtags\n","cf6ff118":"The spaCy TextCategorizer has performed reasonably well on the sentiment140 validation set - a score of 85% in the majority of classification metrics. Now that it has been trained to analyze tweets, it can be used to predict the sentiment of tweets that mention Chris D'Elia.\n\n# 5.0 - Sentiment Analysis\nThe next step is to use the spaCy model we created to predict the sentiment of tweets that mention Chris from 2019 and 2020.","8469ec48":"# 4.0 - spaCy Modelling\nRecall that the sentiment140 dataset is used to train\/validate the spaCy model which will be used to predict the sentiment of tweets that mention Chris D'Elia. \n\n### 4.1 - spaCy Background\nspaCy is a popular, free, open-source library for Natural Language Processing (NLP). A few of the many possible NLP tasks spaCy can be be used for: \n* Tokenization: Segments a text document into words (tokens)\n* Lemmatization: Puts a word in its base form (Ex: running --> run)\n* Part-of-speech (POS): Determines if the word is a pronoun, noun, verb, adverb, etc. \n* Sentence boundary detection (SBD): The model can detect when a sentence comes to an end.\n\nAt the center of spaCy models is the object which is usually called \"nlp\". The \"nlp\" object uses a pipeline which takes in a text document and processes it accordingly. Every \"nlp\" object can be used to load a premade model provided from spaCy or a blank model that can be used for your specific purpose. For example, there are premade models that are available from spaCy that come in different languages, that can be used for general-purpose or vocab, analyzes syntax and entities, trained on web text or news text, etc. These premade models can be trained with new data to make them perform better on a specific text type.\n\n**Note**: Every spaCy processing pipeline has a tokenizer. It is possible to add more components to the pipeline such as a tagger, parser, entity recognizer, textcategorizer, etc.\n\nFor the purposes of the sentiment analysis, the pipeline will consist of a tokenizer and a TextCategorizer component to perform the classification. The default model algorithm in the TextCategorizer is a stacked ensemble of a bag-of-words model and a neural network model that uses a CNN with mean pooling and attention.\n\n\n### 4.2 - Defining the Model\nIn this notebook, I use a blank spaCy model because we are analyzing tweets. This means that the spaCy model has to be trained specifically on text from tweets. Other text such as news articles or blogs have a very different syntax than tweets and therefore the model would not perform well. As mentioned above, the pipeline in the \"nlp\" object will consist of a tokenizer and a TextCategorizer object that will classify negative\/positive tweets.\n\n**TIP:** It is a HUGE time saver to train the model using a bag-of-words architecture instead of the standard ensemble - this will likely yield lower accuracy. The model type can be changed in the config. arguement when creating the TextCategorizer pipe.","274cdb60":"# 1.0 - Introduction\nThis notebook contains a sentiment analysis of tweets that mention Chris D'Elia. He's has had a successful career boasting multiple Netflix specials, goes on tour for standup frequently, a popular podcast and a couple successful tv shows. His following has consistently grown over the past few years until his recent controversy in mid-June. The following graph shows his growth over the years and his most recent decline, where he lost approximately 50,000 followers. The sentiment analysis focuses on tweets extracted over that time and a similar period from 2019. This notebook explores Chris's follower count and the sentiment of tweets that mention him during his growth and decline, highlighted in green and red, respectively.\n\n**NOTES**\n* The tweets mentioning Chris D'Elia were retrieved using the GetOldTweets3 library. \n* The classification of a tweet - either positive or negative - is performed using the spaCy library. The spaCy TextCategorizer is trained on a set of tweets from kaggle - the sentiment 140 dataset - it contains 1,600,000 tweets extracted using the twitter api. \n* Follower count was obtained from the website www.socialblade.com","d62fa185":"### 3.2 - Sentiment140 Tweets\nThe dataset has the following 6 fields: <br>\n1) **target:** the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)<br>\n2) **ids:** The id of the tweet ( 2087)<br>\n3) **date:** the date of the tweet (Sat May 16 23:58:44 UTC 2009)<br>\n4) **flag:** The query (lyx). If there is no query, then this value is NO_QUERY.<br>\n5) **user:** the user that tweeted (robotickilldozr)<br>\n6) **text:** the text of the tweet (Lyx is cool)\n\nRecall that the reason this dataset is included is to train the model that will be used to determine the sentiment of Chris D'Elia tweets. Therefore, only the tweets and targets are needed as they are the only features\/targets for training the model. Therefore the rest of the columns will be dropped, to save memory.","e098156b":"### 4.3 - Training the TextCategorizer\nThe next step is to train the nlp TextCategorizer, but first split the data into training\/validation sets and randomly shuffle the data. <br>","29db404b":"**Summary of what model does:** <br>\nFirst, it initialize the weights randomly, this is done using the method .begin_training(). Next, we batch the samples and call nlp.update() which creates predictions for that batch and compares them against their respective labels. Loss values are calculated and these are used to update the current weights, this process repeats for all the batches until the epoch is complete. This is done continuously for the defined number of epochs and can be a timely process so be careful with the hyperparameters used.\n\n**TIP:** Larger batches make for quicker processing times but are less accurate. Smaller batches are more accurate but dramatically increase processing time.\n\n**NOTE:** To use the TextCategorizer later, the nlp object must be trained on data that has a specific format. It requires a dictionary of boolean values for each class. The model looks for this dictionary in another dictionary with the key 'cats'.<br>\nFinally, zip the tweets and labels (negative\/positive) used for training together to produce a list of [(tweet, label), (tweet, label), ...] tuples.","8f28e425":"**Observations** \n* In both 2019 and 2020, there were more positive tweets than negative tweets.\n* In 2020 there was a signigicant increase in the number of negatively classified tweets with respect to the number of positiely classified tweets. \n\n**Conclusions** <br>\nAll publicity is not good publicity. After his controversy, the overall public opinion of Chris has become more negative. This is likely the cause of his follower loss, perhaps \"casual fans\" who saw this controversy unravel are the main reason behind his follower loss. This brings up an interesting situation, if I had access to who follows him I could run a clustering algorithm based on the number of interactions and if the person followed\/unfollowed him. With this I could determine if there are specific groups that are more likely to follow\/unfollow him. (Ex: \"Casual fans\" may tweet about him once or twice and unfollow). After gaining a understanding of his follwer base, a PR strategy could be implemented to change his overall image."}}