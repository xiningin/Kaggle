{"cell_type":{"53da5633":"code","f0da4295":"code","2bee2815":"code","37a287f5":"code","4eb5b288":"code","3a582df8":"code","a7cb3d40":"code","a5965c5e":"code","f218a410":"code","87dbe297":"code","38396a50":"code","9fbd5e8a":"code","eb5036b8":"code","7c5e775c":"code","3e3fc47c":"code","49a375dd":"code","7dd97eb0":"code","e9a7df1d":"code","2cd93629":"code","17f51580":"code","8112113d":"code","4a138ab9":"code","ac9065ee":"code","74e5b0b9":"code","f423c31e":"code","885c5565":"code","47dd81ad":"code","eb0d3120":"code","74475be0":"code","29f1b4e0":"code","c7cf2dae":"code","d9aa49ec":"code","24bd1b24":"code","bae6f45d":"code","ca1a848b":"code","64b90ce9":"code","d7cadc5d":"code","15ab67e4":"code","7751be61":"code","4a3f8bff":"code","b4dacb3f":"code","bd4b44fa":"code","cfff732c":"code","56e82e82":"code","32faef64":"code","bfe0e532":"code","6e88a07d":"code","ffed8c85":"code","c15eca38":"code","47b7ae66":"code","b34f2251":"code","5ecb4e20":"code","2ff06cfe":"code","7e2d190d":"code","808cd772":"code","013e8df8":"markdown","30ffb5a7":"markdown","cb154294":"markdown","d61b79da":"markdown","567a7184":"markdown","fe27f65a":"markdown","7cf4e2e7":"markdown","4daf6318":"markdown","d4cbd16e":"markdown","1ef90858":"markdown","8910dcee":"markdown","7ae7d3c1":"markdown","290f6644":"markdown","3f86a879":"markdown","7e702760":"markdown","ee095771":"markdown","fad3dcde":"markdown","cc9ec932":"markdown","eace0f5e":"markdown","f11bbc17":"markdown","0c19c798":"markdown","20a96cb0":"markdown","cd8efbb2":"markdown","ae8453bc":"markdown","05a86245":"markdown","4ea5de48":"markdown","9f59dafe":"markdown","79939937":"markdown","d19fbf33":"markdown","f822f20a":"markdown","0592bc60":"markdown","de9d74cd":"markdown","146c3650":"markdown","66fb52c0":"markdown"},"source":{"53da5633":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nfrom time import time\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import accuracy_score\n# Import the three supervised learning models from sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA    \n\n# Pretty display for notebooks\n%matplotlib inline","f0da4295":"# Add column names to data set\ncolumns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n           'relationship', 'race','sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n\n# Read in train data\nadult_train = pd.read_csv('adult.data', header=None, names=columns, skipinitialspace=True)\n\n# Drop the fnlwgt column which is useless for later analysis\nadult_train = adult_train.drop('fnlwgt', axis=1)\n\n# Display several rows and shape of data set\ndisplay(adult_train.head())\ndisplay(adult_train.shape)\n","2bee2815":"# Read in test data\nadult_test = pd.read_csv('adult.test', header=None, skiprows=1, names=columns, skipinitialspace=True)\n\n# Drop the fnlwgt column which is useless for later analysis\nadult_test = adult_test.drop('fnlwgt', axis=1)\n\n# Remove '.' in income column\nadult_test['income'] = adult_test['income'].apply(lambda x: '>50K' if x=='>50K.' else '<=50K')\n\n# Review several rows and shape of data set\ndisplay(adult_test.head())\ndisplay(adult_test.shape)","37a287f5":"# Examine if there are missing value\nadult_train.info()","4eb5b288":"# Check missing value code and convert to NaNs\nobject_col = adult_train.select_dtypes(include=object).columns.tolist()\nfor col in object_col:\n    print(adult_train[col].value_counts(dropna=False)\/adult_train.shape[0],'\\n')","3a582df8":"# Convert '?' to NaNs and remove the entries with NaN value\nfor col in object_col:\n    adult_train.loc[adult_train[col]=='?', col] = np.nan\n    adult_test.loc[adult_test[col]=='?', col] = np.nan\n\n# Perform an mssing assessment in each column of the dataset.\ncol_missing_pct = adult_train.isna().sum()\/adult_train.shape[0]\ncol_missing_pct.sort_values(ascending=False)\n","a7cb3d40":"# Remove data entries with missing value\nadult_train = adult_train.dropna(axis=0, how='any')\nadult_test = adult_test.dropna(axis=0, how='any')\n\n# Show the results of the split\nprint(\"After removing the missing value:\")\nprint(\"Training set has {} samples.\".format(adult_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(adult_test.shape[0]))","a5965c5e":"# Combine the data\nadult_data = pd.concat([adult_train, adult_test])","f218a410":"# Overview of the data\nsns.catplot('income', 'capital-gain', hue='sex', data=adult_data, kind='bar', col='race', row='relationship')","87dbe297":"fig, ax =plt.subplots(2,1, figsize = (8, 12))\n# fig = plt.figure(figsize = (10,13));\n\n# Initiate plot\nsns.countplot(x = 'age', hue = 'income', data = adult_data[adult_data.sex=='Female'], saturation=1, ax=ax[0])\nsns.countplot(x = 'age', hue = 'income', data = adult_data[adult_data.sex=='Male'], saturation=1, ax=ax[1])\n\n# Add titles\nax[0].set_title('Female', loc='center', fontsize = 14)\nax[1].set_title('Male', loc='center', fontsize = 14)\n\n# Add labels\nax[0].set_xlabel(\"Age\")\nax[1].set_xlabel(\"Age\")\nax[0].set_ylabel(\"Proportion of Records\")\nax[1].set_ylabel(\"Proportion of Records\")\n\n# Add x_axis ticks\nnew_ticks = [i.get_text() for i in ax[0].get_xticklabels()]\nax[0].set_xticks(range(0, len(new_ticks), 10))\nax[0].set_xticklabels(new_ticks[::10])\nax[1].set_xticks(range(0, len(new_ticks), 10))\nax[1].set_xticklabels(new_ticks[::10])\n\n# Optimize y_axis ticks\ntotal_F = adult_data[adult_data.sex=='Female'].shape[0]*1.\ntotal_M = adult_data[adult_data.sex=='Male'].shape[0]*1.\nax[0].set_yticklabels(map('{:.1f}%'.format, 100*ax[0].yaxis.get_majorticklocs()\/total_F))\nax[1].set_yticklabels(map('{:.1f}%'.format, 100*ax[1].yaxis.get_majorticklocs()\/total_M))\n\n# Change legend location\nax[0].legend(loc=1, title='Income')\nax[1].legend(loc=1, title='Income')\n\n# Set suptitle\nfig.suptitle(\"Income by Age\", fontsize = 16, y = 1.03)\n\nfig.tight_layout()\nfig.show()","38396a50":"# Use occupation percentage of '>50K' as order of the plot\norder = (adult_data.occupation[adult_data.income=='>50K'].value_counts()\/adult_data.shape[0]).index\n\n# Plotting the income by age\nplt.figure(figsize=(8,6))\nax = sns.countplot(x = 'occupation', hue = 'income', order = order, data = adult_data, saturation=1)\nax.set_title('Income by Occupation', fontsize = 14)\nax.set_xlabel(\"Occupation\")\nax.set_ylabel(\"Proportion of Records\")\n\n# new_ticks = [i.get_text() for i in ax.get_xticklabels()]\n# plt.xticks(range(0, len(new_ticks), 10), new_ticks[::10])\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n\ntotal = adult_data.shape[0]*1.\nax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()\/total))\nax.legend(loc=1, title='Income')\n\nplt.tight_layout()\nplt.show()","9fbd5e8a":"# Use occupation percentage of '>50K' as order of the plot\norder = (adult_data.race[adult_data.income=='>50K'].value_counts()\/adult_data.shape[0]).index\n\n# Plotting the income by age\nplt.figure(figsize=(8,6))\nax = sns.countplot(x = 'race', hue = 'income', order = order, data = adult_data, saturation=1)\nax.set_title('Income by Race', fontsize = 14)\nax.set_xlabel(\"Race\")\nax.set_ylabel(\"Proportion of Records\")\n\n# new_ticks = [i.get_text() for i in ax.get_xticklabels()]\n# plt.xticks(range(0, len(new_ticks), 10), new_ticks[::10])\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n\ntotal = adult_data.shape[0]*1.\nax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()\/total))\nax.legend(loc=1, title='Income')\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100.0),\n            ha=\"center\") \n    \nplt.tight_layout()\nplt.show()","eb5036b8":"# Use occupation percentage of '>50K' as order of the plot\norder = (adult_data.education[adult_data.income=='>50K'].value_counts()\/adult_data.shape[0]).index\n\n# Plotting the income by age\nplt.figure(figsize=(8,6))\nax = sns.countplot(x = 'education', hue = 'income', order = order, data = adult_data, saturation=1)\nax.set_title('Income by Education', fontsize = 14)\nax.set_xlabel(\"Education\")\nax.set_ylabel(\"Proportion of Records\")\n\n# new_ticks = [i.get_text() for i in ax.get_xticklabels()]\n# plt.xticks(range(0, len(new_ticks), 10), new_ticks[::10])\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n\ntotal = adult_data.shape[0]*1.\nax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()\/total))\nax.legend(loc=1, title='Income')\n\nplt.tight_layout()\nplt.show()","7c5e775c":"# Total number of records\nn_records = adult_data.shape[0]\n\n# Number of records where individual's income is more than $50,000\nn_greater_50k = np.sum(adult_data.income=='>50K')\n\n# Number of records where individual's incomre is less than $50,000\nn_at_most_50k = np.sum(adult_data.income=='<=50K')\n\n# Percentage of indiciduals whose income is more than $50,000\ngreater_percentage = round(np.mean(adult_data.income=='>50K')*100.00, 2)\n\n# Print the results\nprint(\"Total number of records: {}\".format(n_records))\nprint(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\nprint(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\nprint(\"Percentage of individuals making more than $50,000: {}%\".format(greater_percentage))","3e3fc47c":"# Check the skewness of numerical variables in data set\nnum_col = adult_train.dtypes[adult_train.dtypes != 'object'].index\n\n# Create figure\nfig = plt.figure(figsize = (10,13));\n\n# Skewed feature plotting\nfor i, feature in enumerate(adult_train[num_col]):\n    ax = fig.add_subplot(3, 2, i+1)\n    ax.hist(adult_train[feature], bins = 25, color = '#00A0A0')\n    ax.set_title(\"'%s' Feature Distribution\"%(feature), fontsize = 14)\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Number of Records\")\n    ax.set_ylim((0, 2000))\n    ax.set_yticks([0, 500, 1000, 1500, 2000])\n    ax.set_yticklabels([0, 500, 1000, 1500, \">2000\"])\n\n# Plot aesthetics\nfig.suptitle(\"Skewed Distributions of Continuous Census Data Features\", fontsize = 16, y = 1.03)\n\nfig.tight_layout()\nfig.show()","49a375dd":"# Calculate skew and sort\nskew_feats = adult_train[num_col].skew().sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skew_feats})\nskewness","7dd97eb0":"# Split the data into features and target label\nincome_raw = adult_train['income']\nfeature_raw = adult_train.drop('income', axis=1)\n\nincome_raw_test = adult_test['income']\nfeature_raw_test = adult_test.drop('income', axis=1)\n\n# Log transform the skewed feature highly-skewed feature 'capital-gain' and 'capital-loss'. \nskewed = ['capital-gain', 'capital-loss']\nfeatures_log_transformed = pd.DataFrame(data=feature_raw)\nfeatures_log_transformed[skewed] = feature_raw[skewed].apply(lambda x: np.log(x + 1))\n\nfeatures_log_transformed_test = pd.DataFrame(data=feature_raw_test)\nfeatures_log_transformed_test[skewed] = feature_raw_test[skewed].apply(lambda x: np.log(x + 1))\n","e9a7df1d":"# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\n\nfeatures_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\nfeatures_log_minmax_transform[num_col] = scaler.fit_transform(features_log_transformed[num_col])\n\n# Transform the test data set\nfeatures_log_minmax_transform_test = pd.DataFrame(data = features_log_transformed_test)\nfeatures_log_minmax_transform_test[num_col] = scaler.transform(features_log_transformed_test[num_col])\n\n# Show an example of a record with scaling applied\ndisplay(features_log_minmax_transform.head())\ndisplay(features_log_minmax_transform_test.head())","2cd93629":"# One-hot encode the 'features_log_minmax_transform' data using sklearn.OneHotEncoder\n\n# Categorical columns' names\ncat_feats = features_log_minmax_transform.dtypes[features_log_minmax_transform.dtypes=='object'].index.tolist()\ncat_idx = [features_log_minmax_transform.columns.get_loc(col) for col in cat_feats]\n\n# Create the encoder.\nencoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\n# Fit and transform the encoder on categorical features\nencoded_cat_feats = encoder.fit_transform(features_log_minmax_transform.loc[:,cat_feats])\n\n# Extract one-hot-encoder's feature names\ncat_col_name = features_log_minmax_transform.columns.get_values()[cat_idx].tolist()\nencoded_cat_feats_name = encoder.get_feature_names(cat_col_name)\n\n# Generate OHE dataframe and concatenate it with the numerical dataframe later\nencoded_cat_feats_df = pd.DataFrame(encoded_cat_feats, columns=encoded_cat_feats_name)\nencoded_cat_feats_df.head()","17f51580":"# Apply OHE above to transform the test dataset\nencoded_cat_feats_test = encoder.transform(features_log_minmax_transform_test.loc[:,cat_feats])\n\n# Generate OHE dataframe and concatenate it with the numerical dataframe later\nencoded_cat_feats_df_test = pd.DataFrame(encoded_cat_feats_test, columns=encoded_cat_feats_name)\nencoded_cat_feats_df_test.head()","8112113d":"# Extract the dataframe with only numerical features\nnum_feats_df = features_log_minmax_transform[num_col].reset_index()\n\n# Concatenate numerical and encoded categorical features together\nX_train = pd.merge(num_feats_df, encoded_cat_feats_df, left_index=True, right_index=True).drop('index', axis=1)\n\n# Encode the 'income_raw' to numerical values\ny_train = income_raw.apply(lambda x: 1 if x == '>50K' else 0)\n\nprint(\"{} total features after one-hot encoding.\".format(len(X_train.columns)))\n\n# Display several rows of processed dataframe\nX_train.head()","4a138ab9":"# Do the same transformation on test data\n# Extract the dataframe with only numerical features\nnum_feats_df_test = features_log_minmax_transform_test[num_col].reset_index()\n\n# Concatenate numerical and encoded categorical features together\nX_test = pd.merge(num_feats_df_test, encoded_cat_feats_df_test, left_index=True, right_index=True)\\\n            .drop('index', axis=1)\n\n# Encode the 'income_raw' to numerical values\ny_test = income_raw_test.apply(lambda x: 1 if x == '>50K' else 0)\n\nprint(\"{} total features after one-hot encoding.\".format(len(X_test.columns)))\n\n# Display several rows of processed dataframe\nX_test.head()","ac9065ee":"'''\nTP = np.sum(income) # Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data \nencoded to numerical values done in the data preprocessing step.\nFP = income.count() - TP # Specific to the naive case\n\nTN = 0 # No predicted negatives in the naive case\nFN = 0 # No predicted negatives in the naive case\n'''\n\n# Calculate accuracy, precision and recall\naccuracy = np.sum(y_train)\/ y_train.count()\nrecall = np.sum(y_train) \/ np.sum(y_train)\nprecision = np.sum(y_train) \/ y_train.count()\n\n# Calculate F-score using beta = 0.5 and correct values for precision and recall.\nfscore = (1 + 0.5*0.5)* precision* recall\/ (0.5*0.5*precision + recall)\n\n# Print the results \nprint(\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))","74e5b0b9":"def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data using slicing with 'sample_size' \n    start = time() # Get start time\n    learner =  learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() # Get end time\n    \n    # Calculate the training time\n    results['train_time'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    # then get predictions on the first 300 training samples(X_train) using .predict()\n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:300])\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['pred_time'] = end - start\n            \n    # Compute accuracy on the first 300 training samples which is y_train[:300]\n    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n        \n    # Compute accuracy on test set using accuracy_score()\n    results['acc_test'] = accuracy_score(y_test, predictions_test)\n    \n    # Compute F-score on the the first 300 training samples using fbeta_score()\n    results['f_train'] = fbeta_score(y_train[:300], predictions_train, average = 'binary', beta = 0.5)\n        \n    # Compute F-score on the test set which is y_test\n    results['f_test'] = fbeta_score(y_test, predictions_test, average = 'binary', beta = 0.5)\n       \n    # Success\n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n        \n    # Return the results\n    return results","f423c31e":"# Initialize the three models\nclf_A = LogisticRegression(random_state = 42)\nclf_B = RandomForestClassifier(random_state=42)\nclf_C = AdaBoostClassifier(random_state = 42)\nclf_D = SVC(random_state = 42)\n\n\n# Calculate the number of samples for 1%, 10%, and 100% of the training data\nsamples_100 = int(len(X_train))\nsamples_10 = int(len(X_train) \/ 10)\nsamples_1 = int(len(X_train) \/ 100)\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C, clf_D]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = \\\n        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n","885c5565":"# Quick view on three model's performance\nfor i in results.items():\n    print (i[0])\n    display(pd.DataFrame(i[1]).rename(columns={0:'1%', 1:'10%', 2:'100%'}))","47dd81ad":"# Run metrics visualization for the three supervised learning models chosen\n\n# Create figure\nfig, ax = plt.subplots(3, 2, figsize = (12,18))\n\n# Constants\nbar_width = 0.2\ncolors =  ['#ff9408', '#3b638c', '#fe46a5', '#90b134']\n\n# Super loop to plot four panels of data\nfor k, learner in enumerate(results.keys()):\n    for j, metric in enumerate(['train_time','pred_time', 'acc_train', 'acc_test', 'f_train', 'f_test']):\n        for i in np.arange(3):\n                \n            # Creative plot code\n            ax[j\/\/2, j%2].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n            ax[j\/\/2, j%2].set_xticks([0.45, 1.45, 2.45])\n            ax[j\/\/2, j%2].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n            ax[j\/\/2, j%2].set_xlabel(\"Training Set Size\")\n            ax[j\/\/2, j%2].set_xlim((-0.1, 3.0))\n    \n# Add y-labels\nax[0, 0].set_ylabel(\"Time (in seconds)\")\nax[0, 1].set_ylabel(\"Time (in seconds)\" )\nax[1, 0].set_ylabel(\"Accuracy Score\")\nax[1, 1].set_ylabel(\"Accuracy Score\")\nax[2, 0].set_ylabel(\"F-score\")\nax[2, 1].set_ylabel(\"F-score\")\n    \n# Add titles\nax[0, 0].set_title(\"Model Training Time\")\nax[0, 1].set_title(\"Model Predicting Time\")\nax[1, 0].set_title(\"Accuracy Score on Training Set\")\nax[1, 1].set_title(\"Accuracy Score on Testing Set\")\nax[2, 0].set_title(\"F-score on Training Set\")\nax[2, 1].set_title(\"F-score on Testing Set\")\n    \n# Add horizontal lines for naive predictors\nax[1, 0].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\nax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\nax[2, 0].axhline(y = fscore, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\nax[2, 1].axhline(y = fscore, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    \n# Set y-limits for score panels\nax[1, 0].set_ylim((0, 1))\nax[1, 1].set_ylim((0, 1))\nax[2, 0].set_ylim((0, 1))\nax[2, 1].set_ylim((0, 1))\n\n# Create patches for the legend\npatches = []\nfor i, learner in enumerate(results.keys()):\n    patches.append(mpatches.Patch(color = colors[i], label = learner))\nplt.legend(handles = patches, bbox_to_anchor = (-0.08, 3.68), \\\n            loc = 'upper center', borderaxespad = 0., ncol = 2, fontsize = 'x-large')\n    \n# Aesthetics\nplt.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 0.96)\n\nfig.tight_layout()\nfig.show()","eb0d3120":"# Initialize the classifier\nclf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), random_state=42)\n\n# Create the parameters list \nparameters = {'n_estimators':[50,75,100,200], \n              'learning_rate':[0.05,0.1,0.3,1], \n              'base_estimator__min_samples_split' : np.arange(2, 8, 2),\n              'base_estimator__max_depth' : np.arange(1, 4, 1)}\n\n# Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(fbeta_score, beta = 0.5)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, parameters, scoring=scorer)\n\nstart = time()\n# Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\nend = time()\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n\nprint(\"--- Run time: %s mins ---\" % np.round(((end - start)\/60),2))","74475be0":"print(best_clf)","29f1b4e0":"# Train the supervised model on the training set using .fit(X_train, y_train)\nmodel = AdaBoostClassifier(random_state=42).fit(X_train, y_train)\n\n# Extract the feature importances using .feature_importances_ \nimportances = model.feature_importances_\n\n# Display the five most important features\nindices = np.argsort(importances)[::-1]\ncolumns = X_train.columns.values[indices[:5]]\nvalues = importances[indices][:5]\n\n# Creat the plot\nfig = plt.figure(figsize = (7,5))\nplt.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize = 16)\nplt.bar(np.arange(5), values, width = 0.6, align=\"center\", color = '#00A000', label = \"Feature Weight\")\nplt.bar(np.arange(5) - 0.3, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n        label = \"Cumulative Feature Weight\")\nplt.xticks(np.arange(5), columns)\nplt.xlim((-0.5, 4.5))\nplt.ylabel(\"Weight\", fontsize = 12)\nplt.xlabel(\"Feature\", fontsize = 12)\n    \nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()  ","c7cf2dae":"# Import functionality for cloning a model\nfrom sklearn.base import clone\n\n# Reduce the feature space\nX_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\nX_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]\n\n# Train on the \"best\" model found from grid search earlier\nclf = (clone(best_clf)).fit(X_train_reduced, y_train)\n\n# Make new predictions\nreduced_predictions = clf.predict(X_test_reduced)\n\n# Report scores from the final model using both versions of data\nprint(\"Final Model trained on full data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\nprint(\"\\nFinal Model trained on reduced data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))","d9aa49ec":"feature_all = adult_data.drop('income', axis=1)\nfeatures_log_transformed_all = pd.DataFrame(data=feature_all)\nfeatures_log_transformed_all[skewed] = feature_all[skewed].apply(lambda x: np.log(x + 1))\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\n\nfeatures_log_minmax_transform_all = pd.DataFrame(data = features_log_transformed_all)\nfeatures_log_minmax_transform_all[num_col] = scaler.fit_transform(features_log_transformed_all[num_col])\n\n# Apply OHE above to transform the test dataset\nencoded_cat_feats_all = encoder.transform(features_log_minmax_transform_all.loc[:,cat_feats])\n\n# Generate OHE dataframe and concatenate it with the numerical dataframe later\nencoded_cat_feats_df_all = pd.DataFrame(encoded_cat_feats_all, columns=encoded_cat_feats_name)\n\n# Extract the dataframe with only numerical features\nnum_feats_df_all = features_log_minmax_transform_all[num_col].reset_index()\n\n# Concatenate numerical and encoded categorical features together\nadult_data_processed = pd.merge(num_feats_df_all, encoded_cat_feats_df_all, left_index=True, right_index=True)\\\n                        .drop('index', axis=1)\n\n# Encode the 'income_raw' to numerical values\n# y_train = income_raw.apply(lambda x: 1 if x == '>50K' else 0)\n\nprint(\"{} total features after one-hot encoding.\".format(len(X_all.columns)))\n\nscaler2 = StandardScaler()\nadult_data_processed[X_train.columns] = scaler2.fit_transform(adult_data_processed[X_train.columns].as_matrix())\nadult_data_processed.head()\n","24bd1b24":"# Apply PCA to the data\npca = PCA()\nmodel = pca.fit_transform(adult_data_processed)","bae6f45d":"# Investigate the variance accounted for by each principal component.\n\ndef plot_pc(pca):\n    '''\n    Creates a scree plot associated with the principal components \n    \n    INPUT: pca - the result of instantian of PCA in scikit learn\n            \n    OUTPUT: None\n    '''\n    num_components=len(pca.explained_variance_ratio_)\n    ind = np.arange(num_components)\n    vals = pca.explained_variance_ratio_\n \n    plt.figure(figsize=(18, 8))\n    ax = plt.subplot(111)\n    cumvals = np.cumsum(vals)\n    ax.bar(ind, vals)\n    ax.plot(ind, cumvals)\n    for i in range(num_components):\n        ax.annotate(r\"%s%%\" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n \n    ax.xaxis.set_tick_params(width=0)\n    ax.yaxis.set_tick_params(width=2, length=12)\n \n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Variance Explained (%)\")\n    plt.title('Explained Variance Per Principal Component')","ca1a848b":"plot_pc(pca)","64b90ce9":"# Find appropriate number of components to retain\nstart = time()\nfor i in np.arange(20, len(pca.explained_variance_ratio_), 3):\n    print('For {} components, explained variance:'.format(i), \n          pca.explained_variance_ratio_[:i].sum())\nend = time()\nprint(end - start)","d7cadc5d":"pca = PCA(n_components=80)\npca_80 = pca.fit_transform(adult_data_processed)","15ab67e4":"# Define a function to show the weight of each features by dimension\ndef show_weight(full_dataset, pca, comp_n, feat_n):\n    '''\n    Display the weight of each feature in dimension i\n    INPUT: \n        full_dataset: dataset\n        pca: PCA model fitted with data\n        comp_n: index of component\n        feat_n: feature number\n    OUTPUT: weight of each feature \n    '''\n    components = pd.DataFrame(np.round(pca.components_, 4), columns=full_dataset.keys()).iloc[comp_n - 1]\n    components.sort_values(ascending=False, inplace=True)\n    top2n_components = pd.concat([components.head(feat_n), components.tail(feat_n)])\n\n    # Plot the result\n    top2n_components.plot(kind='bar', \n                          title='Top {} weighted features for PCA component {}'.format(feat_n*2, comp_n),\n                          figsize=(12, 6))\n    plt.show()\n#     top_components = pd.concat([components.iloc[:5,:],components.iloc[-5:]]).reset_index()\n    return top2n_components","7751be61":"# Map weights for the first principal component to corresponding feature names\n# and then print the linked values, sorted by weight.\n\ncomponents1 = show_weight(adult_data_processed, pca, 1, 5)\ncomponents1","4a3f8bff":"# Map weights for the second principal component to corresponding feature names\n# and then print the linked values, sorted by weight.\n\ncomponents2 = show_weight(adult_data_processed, pca, 2, 5)\ncomponents2","b4dacb3f":"# Map weights for the third principal component to corresponding feature names\n# and then print the linked values, sorted by weight.\n\ncomponents3 = show_weight(adult_data_processed, pca, 3, 5)\ncomponents3","bd4b44fa":"# Investigate the change in within-cluster distance across number of clusters.\ndef get_kmeans_score(data, center):\n    '''\n    return the kmeans score regarding SSE for points to centers\n    INPUT:\n        data - the dataset you want to fit kmeans to\n        center - the number of centers you want\n    OUTPUT:\n        score - the SSE score for the kmeans model fit to the data\n    '''\n    # instantiate kmeans\n    kmeans = KMeans(n_clusters=center)\n    \n    # Then fit the model to you data using the fit mothod\n    model = kmeans.fit(data)\n    \n    # Obtain a score related to the model fit\n    score = np.abs(model.score(data))\n    \n    return score","cfff732c":"scores = []\ncenters = list(range(1, 12))\n\nstart = time()\n\nfor center in centers:\n    print('Fitting k = {} '.format(center))\n    scores.append(get_kmeans_score(pca_80, center))\nend = time()\n\nprint(\"--- Run time: %s mins ---\" % np.round(((end - start)\/60),2))","56e82e82":"# Plot the SSE value to decide the K value\nplt.plot(centers, scores, linestyle='--', marker='o', color='b')\nplt.xlabel('K')\nplt.ylabel('SSE')\nplt.title('SSE vs. K')\n","32faef64":"# Re-fit the k-means model with the selected number of clusters and obtain\n# cluster predictions for the general population demographics data.\nkmeans = KMeans(n_clusters=2)\nmodel = kmeans.fit(pca_80)","bfe0e532":"# Store the prediction\nadult_predict = pd.DataFrame(np.array(model.predict(pca_80)), columns=['Prediction'])\n\n# Count the cluster number\ncluster_cnt = adult_predict['Prediction'].value_counts().sort_index()\ndisplay(cluster_cnt)\n\ncluster_prop = pd.DataFrame((cluster_cnt\/cluster_cnt.sum()), columns=['Prediction']).reset_index()\ncluster_prop.set_index('index', inplace=True)\ndisplay(cluster_prop)\n","6e88a07d":"cluster_prop = cluster_prop.reset_index()\ncluster_prop.plot(x = 'index', y = 'Prediction', kind = 'bar', figsize = (18,8 ))\nplt.title('Cluster Distribution')\nplt.xlabel('Cluster')\nplt.ylabel('Proportion of persons in cluster')\nplt.show()","ffed8c85":"# Investigate top components of cluster 0\ncluster0_components = pd.Series(kmeans.cluster_centers_[0])\ncluster0_components.sort_values(ascending=False, inplace=True)\ncluster0_components.head(5)","c15eca38":"# To get the detail of cluster, transfer cluster back to analyze the principle component\ncluster0 = scaler2.inverse_transform(pca.inverse_transform(pca_80))[adult_predict.Prediction==0]\ncluster0_features = pd.DataFrame(data = np.round(cluster0), columns= adult_data_processed.columns)\n# cluster0_features.head()","47b7ae66":"# Chenck top features in first component\nshow_weight(cluster0_features, pca, 1, 5)","b34f2251":"# Chenck top features in first component\nshow_weight(cluster0_features, pca, 5, 5)","5ecb4e20":"# Investigate top components of cluster 1\ncluster1_components = pd.Series(kmeans.cluster_centers_[1])\ncluster1_components.sort_values(ascending=False, inplace=True)\ncluster1_components.head(5)","2ff06cfe":"# To get the detail of cluster, transfer cluster back to analyze the principle component\ncluster1 = scaler2.inverse_transform(pca.inverse_transform(pca_80))[adult_predict.Prediction==1]\ncluster1_features = pd.DataFrame(data = np.round(cluster1), columns= adult_data_processed.columns)\n# cluster1_features.head()","7e2d190d":"# Chenck top features in top component\nshow_weight(cluster1_features, pca, 0, 5)","808cd772":"# Chenck top features in top component\nshow_weight(cluster1_features, pca, 4, 5)","013e8df8":"Observations from detailed investigation of the first few principal components generated. Can we interpret positive and negative values from them in a meaningful way?\n\nFrom the analysis above, it shows detail of how each feature distibute within each dimension. Analyse the first three dimensions along with top 10 features in details.\n\n#### Dimension 1\n- In the first dimension, it's positively affected by  \n    - sex_Female                              0.3330\n    - marital-status_Never-married            0.2760\n    - relationship_Own-child                  0.1861\n    - relationship_Not-in-family              0.1710\n    - relationship_Unmarried                  0.1546\n- In the first dimension, it's positively affected by  \n    - age                                    -0.1849\n    - hours-per-week                         -0.1864\n    - sex_Male                               -0.3330\n    - marital-status_Married-civ-spouse      -0.3984\n    - relationship_Husband                   -0.4160\n\n\n#### Dimension 2\n- In the second dimension, it's positively affected by   \n    - workclass_Private                    0.2139\n    - native-country_Mexico                0.1939\n    - sex_Male                             0.1930\n    - education_HS-grad                    0.1756\n    - occupation_Craft-repair              0.1601\n- In the second dimension, it's positively affected by  \n    - sex_Female                          -0.1930\n    - education_Masters                   -0.1939\n    - education_Bachelors                 -0.2301\n    - occupation_Prof-specialty           -0.2858\n    - education-num                       -0.4595\n\n#### Dimension 3\n- In the third dimension, it's positively affected by   \n    - race_Asian-Pac-Islander                 0.4008\n    - native-country_Philippines              0.2248\n    - race_Black                              0.1603\n    - native-country_India                    0.1562\n    - native-country_China                    0.1535\n- In the third dimension, it's positively affected by  \n    - workclass_Private                      -0.0902\n    - marital-status_Never-married           -0.1466\n    - relationship_Own-child                 -0.1615\n    - race_White                             -0.3682\n    - native-country_United-States           -0.4475","30ffb5a7":"## Unsupervised clustering\nAfter I build the model predicting whether the individual has the income >50K, I'm gonna try to use unsupervised learning to cluster the population into groups. And see if I could get any insight from the clusters. Since in this case we don't need test data, I'll combine train and test to train clustering model.","cb154294":"### Transforming Skewed Continuous Features\nSkewness may violate model assumptions or may impair the interpretation of feature importance. Therefore, here I will apply logarithmic transformation on the skewed data.","d61b79da":"### Assessing missing data\nHave a quick check on whether there's any huge missing value in columns or rows which may largely affect later analysis.","567a7184":"### Extracting Feature Importance\n\nThe code cell below will implement the following:\n - Train the supervised model on the entire training set.\n - Extract the feature importances using `'.feature_importances_'`.","fe27f65a":"## Preparing the Data\n\nBefore data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured. After processing the missing entries, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.","7cf4e2e7":"### Apply Clustering\nWe've assessed and cleaned the demographics data, then scaled and transformed them. Now, it's time to see how the data clusters in the principal components space. In this substep, I will apply k-means clustering to the dataset and use the average within-cluster distances from each point to their assigned cluster's centroid to decide on a number of clusters to keep.\n\n- Use sklearn's KMeans class to perform k-means clustering on the PCA-transformed data.\n- Then, compute the average difference from each point to its assigned cluster's center. \n- Perform the above two steps for a number of different cluster counts.","4daf6318":"## Evaluating Model Performance\n\nIn this section, I will investigate five different algorithms, and determine which is best at modeling the data. Four of these algorithms will be supervised learners, and the fifth algorithm is known as a *naive predictor*.","d4cbd16e":"### Feature Selection\n\nFrom the visualization above, we see that the top five most important features contribute more than half of the importance of **all** features present in the data. This hints that we can attempt to *reduce the feature space* and simplify the information required for the model to learn. The code cell below will use the same optimized model found earlier, and train it on the same training set *with only the top five important features*. ","1ef90858":"# AF Data Science Project\n__Author: Fan Yuan__  \n__Created: 07\/28\/2019__  \n","8910dcee":"### Initial review on data set\n\nThe goal is to identify if an individual has an income over 50k or not, so first have an overlook at how the income ditributes in the data set. A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than \\$50,000. \n\nHere I'll generate some variables to help analysis as below:\n- The total number of records, `'n_records'`\n- The number of individuals making more than \\$50,000 annually, `'n_greater_50k'`.\n- The number of individuals making at most \\$50,000 annually, `'n_at_most_50k'`.\n- The percentage of individuals making more than \\$50,000 annually, `'greater_percent'`.\n\n__Note:__\nSince in EDA process, we don't need the test data, I'll combine the train and test data in this section just for getting a better and more general distribution of data\n","7ae7d3c1":"## Project Description\n\nIn this project we analyze a U.S. census data taken from the [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income). The goal of this project is to profile people in the above dataset based on available demographic attributes.\n\n1) Construct a model that accurately predicts whether an individual makes more than $50,000.  \n2) What are the key factors contributing to high vs. low income?  \n3) Are there any significant gaps in these Census attributes by gender or race?  \n4) Any underneath clusters (group) based on census data?","290f6644":"As shown in the graph, there seems skewness in 'capital-gain' and 'capital-loss' features. Use quantitative result to confirm if I need to transform skewness in these two variables.","3f86a879":"### Final Model Evaluation\n\n|     Metric     |  Benchmark Model  |Unoptimized Model | Optimized Model |\n| :------------: | :---------------: |:---------------: | :-------------: | \n| Accuracy Score |      0.2478       |      0.8280      |     0.8701      |\n| F-score        |      0.2917       |      0.6518      |     0.7518      |\n\nThe optimized model's accuracy on testing data is 0.8701 and F-score is 0.7518. Both of those scores are better than the unoptimized model. Also, the optimized model performs much better than the benchmarks","7e702760":"Since the missing data '?' is in a small volumn, here I choose to just remove the unknown data which flagged as '?' here. But if the missing data is in a large volumn, need to consider imputing NaNs with more advanced methods.","ee095771":"### Data Preprocessing\n\nThere are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called *categorical variables*) be converted. Here convert categorical variables by using the **one-hot encoding** scheme.\n\nAdditionally, as with the non-numeric features, I need to convert the non-numeric target label, `'income'` to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can simply encode these two categories as `0` and `1`, respectively. The code cell below implement the following:\n - Use `sklearn.OneHotEncoder` to perform one-hot encoding on the `'features_log_minmax_transform'` data.  \n     - Note: Since the test data is separate, in case there are unseen categories in test data which will fail the model, here use sklearn.OneHotEncoder rather the pd.get_dummies()\n - Convert the target label `'income_raw'` to numerical entries.\n   - Set records with \"<=50K\" to `0` and records with \">50K\" to `1`.","fad3dcde":"Try different K value range from 6 to 30, store SSE and compare to decide the final K value. As shown in plot above, there's no clear elbow, here choose 6 as cluster number since there's a relatively dramatice decrease at that point.","cc9ec932":"### Creating a Training and Predicting Pipeline\nTo properly evaluate the performance of each model chosen above more efficiently, it's helpful to create a training and predicting pipeline that can quickly and effectively train models using various sizes of training data and perform predictions on the testing data.\n\nThe code block below will implement the following:\n - Fit the learner to the sampled training data and record the training time.\n - Perform predictions on the test data `X_test`, and also on the first 300 training points `X_train[:300]`.\n   - Record the total prediction time.\n - Calculate the accuracy score for both the training subset and testing set.\n - Calculate the F-score for both the training subset and testing set.","eace0f5e":"### Naive Predictor\n\nGenerate a naive predictor to show what a base model without any intelligence would look like. That is if we chose a model that always predicted an individual made more than $50,000, what would  that model's accuracy and F-score be on this dataset? Here assum that we consider more about to correctly predict individual who has incomre over 50K. \n\n\nTherefore, a model's ability to precisely predict those that make more than \\$50,000 is *more important* than the model's ability to **recall** those individuals. We can use **F-beta score** as a metric that considers both precision and recall:\n\n$$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$\n\nIn particular, when $\\beta = 0.5$, more emphasis is placed on precision, which is also called the **F$_{0.5}$ score** (or F-score for simplicity).\n\n__Note__:\n* When we have a model that always predicts '1' (i.e. the individual makes more than 50k) then our model will have no True Negatives(TN) or False Negatives(FN) as we are not making any negative('0' value) predictions. Therefore our Accuracy in this case becomes the same as our Precision(True Positives\/(True Positives + False Positives)) as every prediction that we have made with value '1' that should have '0' becomes a False Positive; therefore our denominator in this case is the total number of records we have in total. \n* Our Recall score(True Positives\/(True Positives + False Negatives)) in this setting becomes 1 as we have no False Negatives.","f11bbc17":"From the result, the featuer importance put 'capital-loss' the most important feature. It's probably because the bigger capital loss means that the person has to have that volume of money to invest. The 'age' ranks the second one which may because the elder the people the more salary they will have to donor. The 'hours-per-week' and 'sex_Female' ranks the forth and fifth which probably because it's not that sure cases. It's true since maybe the person works longer but have lower unit salary.","0c19c798":"### Process unknown\/missing data\nThe result above shows there's no `null` value in dataset. But according to data notes provided, unknown data was converted into '?'. Therefore, next we'll convert '?' to NaNs.","20a96cb0":"### Interpret Principal Components\n\nNow that we have our transformed principal components, it's a nice idea to check out the weight of each variable on the first few components to see if they can be interpreted in some fashion.\n\n- To investigate the features, map each weight to their corresponding feature name, then sort the features according to weight. The most interesting features for each principal component, then, will be those at the beginning and end of the sorted list. ","cd8efbb2":"As result above show, the model performs a bit worse if I only used important features. The accuracy is 5% lower and the f-score is 7% lower, so in this case I'll still choose to use all feature to build the model unless when the time for fitting model matters a lot.","ae8453bc":"## Improving Results\nIn this section, I will choose from the four supervised learning models the *best* model to use on the test data. I will then perform a grid search optimization for the model over the entire training set (`X_train` and `y_train`) to improve upon the untuned model's F-score. \n\nAccording to the model performance graph above, although the Random Forest performs best on training set but the AdaBoost Classifier finally predicts best on testing data. Although the accuracy of AdaBoost Classifier is quiet similar to the performance of other models, F-score of AdaBoost is better on both training and testing data when the model is applied to the whole data set. Also, in contrast to Support Vector Classfier which takes dramatically more time to train and predict, the AdaBoost is faster. In terms of binary classification, AdaBoost will also performs good in this case.","05a86245":"### Apply Feature Scaling\n\nBefore we apply __dimensionality reduction techniques__ to the data, we need to perform __feature scaling__ so that the __principal component vectors are not influenced by the natural differences in scale for features.__","4ea5de48":"### Initial Model Evaluation\n\nIn the next section:\n- Initialize the four models and store them in `'clf_A'`, `'clf_B'`, `'clf_C'` and `'clf_D'`.\n  - **Note:** Here use the default settings for each model \u2014 will tune one specific model in a later section.\n- Calculate the number of records equal to 1%, 10%, and 100% of the training data.\n  - Store those values in `'samples_1'`, `'samples_10'`, and `'samples_100'` respectively.\n","9f59dafe":"__Data Information__  \n`age`: continuous.  \n`workclass`: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.  \n`fnlwgt`: final weight, continuous.  \n`education`: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.  \n`education-num`:  continuous.  \n`marital-status`: Represents the responding unit\u2019s role in the family. Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.  \n`occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.  \n`relationship`: Represents the responding unit\u2019s role in the family. Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.  \n`race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.  \n`sex`: Female, Male.  \n`capital-gain`: income from investment sources, apart from wages\/salary, continuous.  \n`capital-loss`: losses from investment sources, apart from wages\/salary, continuous.  \n`hours-per-week`: continuous.  \n`native-country`: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.  ","79939937":"### Normalizing Numerical Features\nIn addition to performing transformations on features that are highly skewed, here will perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as 'capital-gain' or 'capital-loss' above); however, it is useful to scale the input attributes for a model that relies on the magnitude of values, such as distance measures used in k-nearest neighbors and in the preparation of coefficients in regression.","d19fbf33":"### Model Tuning\nFine tune the chosen model. Use grid search (`GridSearchCV`). Use the entire training set for this. The code cell below will implement the following:\n- Initialize the classifier I've chosen and store it in `clf`.\n- Create a dictionary of parameters to tune for the chosen model.\n    - `n_estimators` and `learning_rate` of AdaBoostClassifier\n    - `max_depth` and `min_samples_split` of base_estimator\n- Use `make_scorer` to create an `fbeta_score` scoring object (with $\\beta = 0.5$).\n- Perform grid search on the classifier `clf` using the `'scorer'`, and store it in `grid_obj`.\n- Fit the grid search object to the training data (`X_train`, `y_train`), and store it in `grid_fit`.\n","f822f20a":"## Feature Importance\n\nGenerally, it's useful to know which features provide the most predictive power when performing supervised learning on a dataset like the census data here. In this case, it means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than \\$50,000.\n\nHere will choose a scikit-learn classifier (e.g., adaboost, random forests) that has a `feature_importance_` attribute. Fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset.","0592bc60":"The largest missing percentage by column level is 5% in dataset, and most columns are complete enough. Therefore, here I'll remove the NaN values instead of manually imputing.","de9d74cd":"### Supervised Lerning Models\n\nApart from bench mark model, I'll chose four other models Logistic Regression, Random Forest, Ensemble Methods (AdaBoost) and Support Vector Machines (SVM) as a candidate to build the predictive model.\n\n- Logistic Regression  \n\n    - Logistic regression doesn't need too many computational resources. 's highly interpretable, and doesn't require input features to be scaled. It doesn't require any tuning, and is wasy to regularize.\n    - However, it can't solve non-linear problem with logistic regression since its secision surface is linear. It won't perform well with independent variables that are not correlated to the target variable or the variables correlated to each others\n    - In this case, the model is simply doing binary classification and most of features are correlated to the target variable.\n\n- Random Forest  \n\n    - RF can be used in data sets with large number of features and instances. It trains fast due to parallel tree generation. It has low variance compared to single decision tree due to uncorrelated trees.  \n    - Prediction time can be higher for complex models with large number of trees. Possible issues with diagonal decision boundaries  \n    - In this case, Given large number of training data and one hot encoded categorical features will be a good fit for the given problem.\n\n- Ensemble Methods -- AdaBoost  \n\n    - AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. AdaBoost is not prone to overfitting.  \n    - AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. AdaBoost is slower compared to XGBoost. Reference.  \n    - In this case, the final goal is to predict if the individual will have salary above 50K or not. AdaBoost, as one of the boosting algorithm, it focuses on classification problems and aims to convert a set of weak classifiers into a strong one. Reference\n\n\n\n- Support Vector Machines (SVM)  \n\n    - SVM has a regularisation parameter, which makes the user avoid over-fitting SVM usese the kernel trick, can build in expert knowledge about the problem via engineering the kernel It's defined by a convex optimisation problem for which there are efficient methods Reference  \n    - SVM only really covers the determination of the parameters for a given value of the regularisation and kernel parameters and choice of kernel. Therefore, the biggest disadvantages are choosing appropriately hyper parameters of the SVM that will allow for sufficient generalization performance Reference  \n    - In this case, the data is not highly skewed\/ imbalanced which would be good to use SVM model. The number of features in this case is not too many so that SVM can still work well. And the problem is binary classification which is suitable for SVM. Reference\n\n","146c3650":"## Exploring Data\n\n### Load necessry Python libraries and the census data","66fb52c0":"### Perform Dimensionality Reduction\nOn scaled data, now ready to apply dimensionality reduction techniques.\n\n- Use sklearn's PCA class to apply principal component analysis on the data, thus finding the vectors of maximal variance in the data.\n- Check out the ratio of variance explained by each principal component as well as the cumulative variance explained. - Plot the cumulative or sequential values. Based on what I find, select a value for the number of transformed features I'll retain for the clustering part of the project."}}