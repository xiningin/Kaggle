{"cell_type":{"2206c746":"code","35804808":"code","2587b161":"code","0d4b35ef":"code","e2cff208":"code","b4107ea6":"code","003f142d":"code","22a33a27":"code","e2e5935e":"code","f93f422a":"code","b476cdb3":"code","2284a96e":"markdown"},"source":{"2206c746":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom keras.models import load_model\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('..\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nprint(os.listdir('..\/input'))\n\n# Any results we write to the current directory are saved as output\n","35804808":"import tensorflow_hub as hub\nimport os\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.applications.xception import Xception\nfrom keras.layers import GlobalMaxPooling2D, Dense, GlobalAveragePooling2D\nfrom keras.applications.xception import preprocess_input\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","2587b161":"%load_ext tensorboard\n!rm -rf .\/logs\/ \nimport tensorflow as tf\nfrom tensorboard.plugins.hparams import api as hp","0d4b35ef":"train_dir = \"..\/input\/gtsrb-german-traffic-sign\/Train\"\ndatagen = ImageDataGenerator(validation_split=0.2, rescale=1.\/255)\ntrain_generator = datagen.flow_from_directory(\n    train_dir,\n    subset='training',\n    target_size=(75,75),\n    batch_size=32,\n    color_mode='rgb',    \n    shuffle=True,\n    seed=42,\n    class_mode='categorical')\n\nval_generator = datagen.flow_from_directory(\n    train_dir,\n    subset='validation',\n    target_size=(75,75),\n    batch_size=32,\n    color_mode='rgb',    \n    shuffle=True,\n    seed=42,\n    class_mode='categorical')","e2cff208":"HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([64,128]))\nHP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.25, 0.5))\nHP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n\nMETRIC_ACCURACY = 'accuracy'\n\nwith tf.summary.create_file_writer('logs\/hparam_tuning').as_default():\n  hp.hparams_config(\n    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n  )","b4107ea6":"weightpath = \"model-3x3 (1).h5\"\ncheckpoint = ModelCheckpoint(weightpath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]","003f142d":"STEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VAL =val_generator.n\/\/val_generator.batch_size","22a33a27":"from keras import optimizers\n\nconv_base = Xception(include_top = False, weights = 'imagenet',\n                          input_shape = (75,75,3))\ndef train_test_model(hparams):\n  model = conv_base.output\n  model = layers.GlobalAveragePooling2D()(model)\n  model = layers.Dropout(hparams[HP_DROPOUT])(model)\n  model = layers.Dense(hparams[HP_NUM_UNITS], activation = \"relu\")(model)\n  model = layers.Dense(43, activation = \"softmax\")(model)\n  model = models.Model(conv_base.input, model)\n  \n  model.compile(loss='categorical_crossentropy',\n        optimizer=hparams[HP_OPTIMIZER],\n        metrics=['accuracy'])\n  history = model.fit(train_generator,steps_per_epoch=STEP_SIZE_TRAIN,epochs=10, validation_data=val_generator, validation_steps=STEP_SIZE_VAL,callbacks=[checkpoint])\n  _, accuracy = model.evaluate(val_generator)\n  return accuracy","e2e5935e":"def run(run_dir, hparams):\n  with tf.summary.create_file_writer(run_dir).as_default():\n    hp.hparams(hparams)  # record the values used in this trial\n    accuracy = train_test_model(hparams)\n    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)","f93f422a":"session_num = 0\n\nfor num_units in HP_NUM_UNITS.domain.values:\n  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n    for optimizer in HP_OPTIMIZER.domain.values:\n      hparams = {\n          HP_NUM_UNITS: num_units,\n          HP_DROPOUT: dropout_rate,\n          HP_OPTIMIZER: optimizer,\n      }\n      run_name = \"run-%d\" % session_num\n      print('--- Starting trial: %s' % run_name)\n      print({h.name: hparams[h] for h in hparams})\n      run('logs\/hparam_tuning\/' + run_name, hparams)\n      session_num += 1","b476cdb3":"%tensorboard --logdir logs\/hparam_tuning","2284a96e":"# \ud83d\udce5 Importing needed libraries"}}