{"cell_type":{"ec1d788b":"code","35dafc70":"code","4d8fffcf":"code","57d6fbc8":"code","fd3699be":"code","d91dcfe3":"code","42386d84":"code","15301f11":"code","87c58cda":"code","71312e1d":"code","e6bb853b":"code","6228a439":"code","b8504e05":"code","2d89597a":"code","5ce8aaec":"code","0ffe557e":"code","affb7743":"code","fafe61ba":"code","a8e89a1f":"code","95d361e0":"code","767ae32a":"code","fd972a17":"code","6b906c6c":"code","aad6cca2":"code","505f8a9f":"code","d03217ac":"code","5a40061c":"code","c1e4365b":"code","89a7dac1":"code","837fda54":"code","98557ec4":"code","d22d0c27":"code","18ab307c":"code","23fab306":"code","b5be7c85":"code","b988788f":"code","d3df5174":"code","82afb9bf":"code","812ad37a":"code","7c2261c9":"code","32800360":"code","0905e111":"code","7a39c976":"code","690050d7":"code","9759f93b":"code","4b531695":"code","7b1f7c83":"code","376bd5b3":"code","99a3f0b2":"code","ef0ad180":"code","582c9f67":"code","a0238df9":"code","ec656d7f":"code","f107e676":"code","aa971655":"code","8adb508d":"code","40471920":"code","14a6a5ea":"code","a0f71190":"code","74a8b1c1":"code","c2c9db92":"code","678d3d5a":"code","bb359934":"code","1dd417a4":"code","8563e1da":"code","2a77973e":"code","cbfadc5b":"code","1b4c0c43":"code","e3c5cd94":"code","50a45f63":"code","8c65c9fd":"code","cd715a77":"markdown","9d292ebb":"markdown","70f627d1":"markdown","7e68e007":"markdown","64134d52":"markdown","a9fd8314":"markdown","ccfd32a5":"markdown","d194b9c0":"markdown","1864dd88":"markdown","38b0adc9":"markdown","12d2ca46":"markdown","3682bc21":"markdown","19c8a77e":"markdown"},"source":{"ec1d788b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nnp.random.seed(42)","35dafc70":"train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntrain.head()","4d8fffcf":"test = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\ntest.head()","57d6fbc8":"test['song_popularity'] = -1\ndf = pd.concat((train, test))\ndf.tail()","fd3699be":"cols = [c for c in df.columns if c not in('id','song_popularity')]","d91dcfe3":"it_imputer = IterativeImputer(max_iter=100)\ndf[cols] = it_imputer.fit_transform(df[cols])","42386d84":"# Normalize the data\n\nnormalizers = {} # saving them for standardizing testing data\nfor col in cols:\n    scaler = RobustScaler()\n    df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n    normalizers[col] = scaler","15301f11":"# cdf = pd.DataFrame(data=train_iterimp, columns=cols)\n# cdf['song_popularity'] = df['song_popularity'].values\n# cdf['id'] = df['id'].values\n# cdf.tail()","87c58cda":"test_df = df[df['song_popularity'] == -1]\ntrain_df = df[df['song_popularity'] != -1]\n\ntest_df.to_csv(\"processed_test.csv\", index=False)","71312e1d":"def create_folds(df, target=\"target\"):\n\n    # We create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n\n    # randomizing the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # Fetch targets || Required in Stratified KFold\n    y = df[target].values\n\n    # init the kfold class from model selection module\n    kf = model_selection.StratifiedKFold(n_splits=5) # k = 5\n\n    for fold, (trn_, val_) in enumerate(kf.split(X=df, y=y)):\n\n        df.loc[val_, 'kfold'] = fold\n\n    # save the new csv with kfol column\n    df.to_csv(\"train_folds.csv\", index=False)","e6bb853b":"create_folds(train_df, 'song_popularity')","6228a439":"def run_adb(fold, target=\"song_popularity\", adb_params=None, ret_model=False):\n\n    # load the full training data with folds\n    df = pd.read_csv('.\/train_folds.csv')\n\n    # all columns are features excpet target and kfold columns\n    features = [\n        f for f in df.columns if f not in (target, \"kfold\", \"id\")\n    ]\n\n    # get the training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n\n    # get the validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    # transform training data\n    x_train = df_train[features]\n\n    # transform validation data\n    x_valid = df_valid[features]\n\n    # init the AdaBoost model\n    model = AdaBoostClassifier(**adb_params)\n\n    # fit model on training data\n    model.fit(x_train, df_train[target].values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC\n    # we will use the probability of 1s\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get the roc auc score\n    auc = metrics.roc_auc_score(df_valid[target].values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    df_valid.loc[:, \"adb_pred\"] = valid_preds\n    \n    if ret_model:\n        return df_valid[[\"id\", target, \"kfold\", \"adb_pred\"]], auc, model\n    \n    return df_valid[[\"id\", target, \"kfold\", \"adb_pred\"]], auc","b8504e05":"best_models = {}","2d89597a":"adb_params = {'n_estimators': 215, \n              'learning_rate': 0.2383622479638699, \n              'algorithm': 'SAMME.R'}\n\ndfs = []\nbest_model = None\nbest_score = 0\nfor f in range(5):\n    temp_df, scr, model = run_adb(f, adb_params=adb_params, ret_model=True)\n    if best_score < scr:\n        best_score = scr\n        best_model = model\n    dfs.append(temp_df)\n    \nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.shape)\nfin_valid_df.to_csv(\"adb_preds.csv\", index=False)\n\nbest_models['adb'] = best_model","5ce8aaec":"def run_xgb(fold, target=\"song_popularity\", xgb_params=None, ret_model=False):\n\n    # load the full training data with folds\n    df = pd.read_csv('.\/train_folds.csv')\n\n    # all columns are features excpet target and kfold columns\n    features = [\n        f for f in df.columns if f not in (target, \"kfold\", \"id\")\n    ]\n\n    # get the training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n\n    # get the validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    # transform training data\n    x_train = df_train[features]\n\n    # transform validation data\n    x_valid = df_valid[features]\n\n    # init the XGBoost model\n    model = xgb.XGBClassifier(**xgb_params)\n\n    # fit model on training data\n    model.fit(x_train, df_train[target].values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC\n    # we will use the probability of 1s\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get the roc auc score\n    auc = metrics.roc_auc_score(df_valid[target].values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    df_valid.loc[:, \"xgb_pred\"] = valid_preds\n    \n    if ret_model:\n        return df_valid[[\"id\", target, \"kfold\", \"xgb_pred\"]], auc, model\n    \n    return df_valid[[\"id\", target, \"kfold\", \"xgb_pred\"]], auc","0ffe557e":"xgb_params = {\n     'n_estimators': 947, \n     'booster': 'gbtree', \n     'lambda': 6.8369801149592085e-06, \n     'alpha': 3.777062056768887e-05, \n     'reg_alpha': 0, 'reg_lambda': 0, \n     'learning_rate': 0.008458805233233854, \n     'colsample_bytree': 0.41000000000000003, \n     'max_depth': 4, 'eta': 4.0969289659837145e-05, \n     'gamma': 1.5225673369335376e-06, \n     'grow_policy': 'lossguide'\n}\n\ndfs = []\nbest_model = None\nbest_score = 0\n\nfor f in range(5):\n    temp_df, scr, model = run_xgb(f, xgb_params=xgb_params, ret_model=True)\n    if best_score < scr:\n        best_score = scr\n        best_model = model\n    dfs.append(temp_df)\n    \nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.shape)\nfin_valid_df.to_csv(\"xgb_preds.csv\", index=False)\n\nbest_models['xgb'] = best_model","affb7743":"def run_lgbm(fold, target=\"song_popularity\", lgbm_params=None, ret_model=False):\n\n    # load the full training data with folds\n    df = pd.read_csv('.\/train_folds.csv')\n\n    # all columns are features excpet target and kfold columns\n    features = [\n        f for f in df.columns if f not in (target, \"kfold\", \"id\")\n    ]\n\n    # get the training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n\n    # get the validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    # transform training data\n    x_train = df_train[features]\n\n    # transform validation data\n    x_valid = df_valid[features]\n\n    lgb_train = lgb.Dataset(x_train, df_train[target].values)\n    lgb_test = lgb.Dataset(x_valid, df_valid[target].values)\n    \n    model = lgb.train(params = lgbm_params,\n                     train_set = lgb_train,\n                     valid_sets = lgb_test,\n                     early_stopping_rounds = 300,\n                     verbose_eval = False)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC\n    # we will use the probability of 1s\n    valid_preds = model.predict(x_valid, num_iteration=model.best_iteration)\n\n    # get the roc auc score\n    auc = metrics.roc_auc_score(df_valid[target].values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    df_valid.loc[:, \"lgbm_pred\"] = valid_preds\n    \n    if ret_model:\n        return df_valid[[\"id\", target, \"kfold\", \"lgbm_pred\"]], auc, model\n    \n    return df_valid[[\"id\", target, \"kfold\", \"lgbm_pred\"]], auc","fafe61ba":"lgbm_params = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 3,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':5000,\n    'colsample_bytree':0.1107\n    }\n\ndfs = []\nbest_model = None\nbest_score = 0\n\nfor f in range(5):\n    temp_df, scr, model = run_lgbm(f, lgbm_params=lgbm_params, ret_model=True)\n    if best_score < scr:\n        best_score = scr\n        best_model = model\n    dfs.append(temp_df)\n    \nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.shape)\nfin_valid_df.to_csv(\"lgbm_preds.csv\", index=False)\n\nbest_models['lgbm'] = best_model","a8e89a1f":"import glob\nimport pandas\nfrom sklearn import metrics\nimport numpy as np","95d361e0":"files = glob.glob(\".\/*_preds.csv\")\n\ndf = None\nfor f in files:\n    if df is None:\n        df = pd.read_csv(f)\n    else:\n        temp_df = pd.read_csv(f)\n        df = df.merge(temp_df, on=\"id\", how=\"left\")\n        \ndf.head()","767ae32a":"pred_cols = [\"adb_pred\", \"lgbm_pred\", \"xgb_pred\"]\ntargets = df['song_popularity'].values\n\nfor col in pred_cols:\n    auc = metrics.roc_auc_score(targets, df[col].values)\n    print(f\"pred_col={col}, overall_auc={auc}\")","fd972a17":"print(\"Average\")\navg_pred = np.mean(df[pred_cols].values, axis=1)\nprint(metrics.roc_auc_score(targets, avg_pred))","6b906c6c":"print(\"Weighted Average\")\n\na = df.adb_pred.values\nb = df.lgbm_pred.values\nc = df.xgb_pred.values\n\nw_avg_pred = (1 * a + 1.5 * b + 1.5 * c) \/ 4\nprint(metrics.roc_auc_score(targets, w_avg_pred))","aad6cca2":"print(\"Rank Averaging\")\na = df.adb_pred.values\nb = df.lgbm_pred.values\nc = df.xgb_pred.values\n\nr_avg_pred = (a + b + c) \/ 3\nprint(metrics.roc_auc_score(targets, r_avg_pred))","505f8a9f":"print(\"Weighted Rank Averaging\")\na = df.adb_pred.values\nb = df.lgbm_pred.values\nc = df.xgb_pred.values\n\nwr_avg_pred = (2 * a + 3 * b + 3 * c) \/ 8\nprint(metrics.roc_auc_score(targets, wr_avg_pred))","d03217ac":"files = glob.glob(\".\/*_preds.csv\")\n\ndf = None\nfor f in files:\n    if df is None:\n        df = pd.read_csv(f)\n    else:\n        temp_df = pd.read_csv(f)\n        df = df.merge(temp_df, on=\"id\", how=\"left\")\n        \ndf.head()","5a40061c":"pred_cols = [\"adb_pred\", \"lgbm_pred\", \"xgb_pred\"]\ntargets = df['song_popularity'].values","c1e4365b":"from scipy.optimize import fmin\nfrom functools import partial\n\nclass OptimizeAUC:\n    \n    def __init__(self):\n        self.coef_ = 0\n        \n    def _auc(self, coef, X, y):\n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        auc_score = metrics.roc_auc_score(y, predictions)\n        return -1.0 * auc\n        \n    def fit(self, X, y):\n        partial_loss = partial(self._auc, X=X, y=y)\n        init_coef  = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n        \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions","89a7dac1":"def run_training(pred_df, fold):\n    \n    train_df = pred_df[pred_df.kfold != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n\n    pred_cols = [\"adb_pred\", \"lgbm_pred\", \"xgb_pred\"]\n    xtrain = train_df[pred_cols].values\n    xvalid = valid_df[pred_cols].values\n    opt = OptimizeAUC()\n    opt.fit(xtrain, train_df['song_popularity'].values)\n    preds = opt.predict(xvalid)\n    auc = metrics.roc_auc_score(valid_df['song_popularity'].values, preds)\n    print(f\"fold={fold}, auc={auc}\")\n    \n    return opt.coef_","837fda54":"coefs = []\nfor f in range(5):\n    coefs.append(run_training(df, f))\n    \ncoefs = np.array(coefs)\nprint(coefs)","98557ec4":"coefs = np.mean(coefs, axis=0)\nprint(coefs)","d22d0c27":"print(\"Optimal auc\")\na = df.adb_pred.values\nb = df.lgbm_pred.values\nc = df.xgb_pred.values\n\nwt_avg = (coefs[0] * a + coefs[1] * b + coefs[2] * c)\n\nmetrics.roc_auc_score(targets, wt_avg)","18ab307c":"tdf = pd.read_csv(\".\/processed_test.csv\")\ntdf.head()","23fab306":"xtest = tdf[cols].values\nxtest.shape","b5be7c85":"preds = (\n    (best_models['adb'].predict_proba(xtest) * coefs[0])[:, 1] +\n    best_models['lgbm'].predict(xtest) * coefs[1] +\n    (best_models['xgb'].predict_proba(xtest) * coefs[2])[:, 1]\n)\n\nprint(preds.shape)","b988788f":"sub = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nsub['song_popularity'] = preds\nsub.to_csv(\"sub_blend.csv\", index=False)\nsub.head()","d3df5174":"from sklearn.linear_model import LogisticRegression","82afb9bf":"def run_training_lr(pred_df, fold):\n    \n    train_df = pred_df[pred_df.kfold != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n\n    pred_cols = [\"adb_pred\", \"lgbm_pred\", \"xgb_pred\"]\n    xtrain = train_df[pred_cols].values\n    xvalid = valid_df[pred_cols].values\n    \n    scl = RobustScaler()\n    xtrain = scl.fit_transform(xtrain)\n    xvalid = scl.transform(xvalid)\n    \n    opt = LogisticRegression()\n    opt.fit(xtrain, train_df['song_popularity'].values)\n    preds = opt.predict_proba(xvalid)[:, 1]\n    auc = metrics.roc_auc_score(valid_df['song_popularity'].values, preds)\n    print(f\"fold={fold}, auc={auc}\")\n    \n    return opt.coef_","812ad37a":"coefs = []\nfor f in range(5):\n    coefs.append(run_training_lr(df, f)[0])\n    \ncoefs = np.array(coefs)\nprint(coefs)","7c2261c9":"coefs = np.mean(coefs, axis=0)\nprint(coefs)","32800360":"print(\"Optimal auc\")\na = df.adb_pred.values\nb = df.lgbm_pred.values\nc = df.xgb_pred.values\n\nwt_avg = (coefs[0] * a + coefs[1] * b + coefs[2] * c)\n\nmetrics.roc_auc_score(targets, wt_avg)","0905e111":"tdf = pd.read_csv(\".\/processed_test.csv\")\ntdf.head()","7a39c976":"xtest = tdf[cols].values\nxtest.shape","690050d7":"preds = (\n    (best_models['adb'].predict_proba(xtest) * coefs[0])[:, 1] +\n    best_models['lgbm'].predict(xtest, num_iteration=best_models['lgbm'].best_iteration) * coefs[1] +\n    (best_models['xgb'].predict_proba(xtest) * coefs[2])[:, 1]\n)\n\nprint(preds.shape)","9759f93b":"sub = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nsub['song_popularity'] = preds\nsub.to_csv(\"sub_blend_LR.csv\", index=False)\nsub.head()","4b531695":"from sklearn.linear_model import LinearRegression","7b1f7c83":"def run_training_lreg(pred_df, fold):\n    \n    train_df = pred_df[pred_df.kfold != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n\n    pred_cols = [\"adb_pred\", \"lgbm_pred\", \"xgb_pred\"]\n    xtrain = train_df[pred_cols].values\n    xvalid = valid_df[pred_cols].values\n    \n    scl = RobustScaler()\n    xtrain = scl.fit_transform(xtrain)\n    xvalid = scl.transform(xvalid)\n    \n    opt = LinearRegression()\n    opt.fit(xtrain, train_df['song_popularity'].values)\n    preds = opt.predict(xvalid)\n    auc = metrics.roc_auc_score(valid_df['song_popularity'].values, preds)\n    print(f\"fold={fold}, auc={auc}\")\n    \n    return opt.coef_","376bd5b3":"coefs = []\nfor f in range(5):\n    coefs.append(run_training_lreg(df, f))\n    \ncoefs = np.array(coefs)\nprint(coefs)","99a3f0b2":"coefs = np.mean(coefs, axis=0)\nprint(coefs)","ef0ad180":"print(\"Optimal auc\")\na = df.adb_pred.values\nb = df.lgbm_pred.values\nc = df.xgb_pred.values\n\nwt_avg = (coefs[0] * a + coefs[1] * b + coefs[2] * c)\n\nmetrics.roc_auc_score(targets, wt_avg)","582c9f67":"tdf = pd.read_csv(\".\/processed_test.csv\")\ntdf.head()","a0238df9":"xtest = tdf[cols].values\nxtest.shape","ec656d7f":"preds = (\n    (best_models['adb'].predict_proba(xtest) * coefs[0])[:, 1] +\n    best_models['lgbm'].predict(xtest, num_iteration=best_models['lgbm'].best_iteration) * coefs[1] +\n    (best_models['xgb'].predict_proba(xtest) * coefs[2])[:, 1]\n)\n\nprint(preds.shape)","f107e676":"sub = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nsub['song_popularity'] = preds\nsub.to_csv(\"sub_blend_LREG.csv\", index=False)\nsub.head()","aa971655":"from sklearn.ensemble import AdaBoostClassifier\nimport optuna","8adb508d":"def run_training_adaboost(pred_df, fold, params, ret_model=False):\n    \n    train_df = pred_df[pred_df.kfold != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n\n    pred_cols = [\"adb_pred\", \"lgbm_pred\", \"xgb_pred\"]\n    xtrain = train_df[pred_cols].values\n    xvalid = valid_df[pred_cols].values\n    \n    clf = AdaBoostClassifier(**params)\n    clf.fit(xtrain, train_df['song_popularity'].values)\n    preds = clf.predict_proba(xvalid)[:, 1]\n    auc = metrics.roc_auc_score(valid_df['song_popularity'].values, preds)\n    print(f\"{xtrain.shape}, fold={fold}, auc={auc}\")\n    \n    if ret_model:\n        return auc, clf\n    \n    return auc","40471920":"def objective(trial):\n    \n    params = {\n        'n_estimators' : trial.suggest_int('n_estimators', 50, 1000),\n        'learning_rate':trial.suggest_loguniform('learning_rate',0.005, 1.0),\n        'algorithm' : trial.suggest_categorical(\"algorithm\", [\"SAMME\", \"SAMME.R\"]),\n        'random_state' : 42\n    }\n    \n    all_auc = []\n    for f_ in range(5):\n        temp_auc = run_training_adaboost(df, f_, params, ret_model=False)\n        all_auc.append(temp_auc)\n        \n    return np.mean(all_auc)","14a6a5ea":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=40)","a0f71190":"print(\"best trial:\")\ntrial_ = study.best_trial\n\nprint(trial_.values)\nprint(trial_.params)\n\nscores = 0\nbest_score = 0\nbest_model = 0\nfor f_ in range(5):\n    scr, model = run_training_adaboost(df, f_, trial_.params, ret_model=True)\n    if scr > best_score:\n        best_score = scr\n        best_model = model\n    scores += scr\n    \nprint(scores \/ 5)","74a8b1c1":"tdf = pd.read_csv(\".\/processed_test.csv\")\ntdf.head()","c2c9db92":"xtest = tdf[cols].values\nxtest.shape","678d3d5a":"a = np.array(best_models['adb'].predict_proba(xtest)[:, 1]).reshape(-1,)\nb = np.array(best_models['lgbm'].predict(xtest, num_iteration=best_models['lgbm'].best_iteration)).reshape(-1,) # model.predict(x_valid, num_iteration=model.best_iteration)\nc = np.array(best_models['xgb'].predict_proba(xtest)[:, 1]).reshape(-1,)","bb359934":"xtest_xgb = pd.DataFrame(columns=pred_cols)\nxtest_xgb['adb_pred'] = a\nxtest_xgb['lgbm_pred'] = b\nxtest_xgb['xgb_pred'] = c\nxtest_xgb.head()","1dd417a4":"preds = best_model.predict_proba(xtest_xgb.values)[:, 1]\npreds.shape","8563e1da":"# print(max(xtest_xgb['adb_pred']), min(xtest_xgb['adb_pred']))\n# print(max(xtest_xgb['lgbm_pred']), min(xtest_xgb['lgbm_pred']))\n# print(max(xtest_xgb['xgb_pred']), min(xtest_xgb['xgb_pred']))\n# print(max(xtest_xgb['xgb_boost']), min(xtest_xgb['xgb_boost']))","2a77973e":"# print(np.median(xtest_xgb['adb_pred']))\n# print(np.median(xtest_xgb['lgbm_pred']))\n# print(np.median(xtest_xgb['xgb_pred']))","cbfadc5b":"# preds = []\n# for a, b, c in xtest_xgb.values:\n    \n#     if a >= 0.498 or b > 0.45 or c > 0.45:\n#         preds.append(max(a, b, c) + 0.25)\n    \n#     else:\n#         preds.append(min(a, b, c))\n        \n# preds = np.array(preds)","1b4c0c43":"np.sum(preds > 0.5)","e3c5cd94":"sub = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nsub['song_popularity'] = preds\nsub.to_csv(\"sub_stack_ada_tune.csv\", index=False)\nsub.head()","50a45f63":"a = preds + 0.10\nnp.sum(a>0.5)","8c65c9fd":"sub = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nsub['song_popularity'] = a\nsub.to_csv(\"sub_stack_ada_tune_a.csv\", index=False)\nsub.head()","cd715a77":"# BLENDING","9d292ebb":"## Using Logistic Regression for optimal weights","70f627d1":"## Model 1","7e68e007":"# Bruhh.. Save the models also","64134d52":"### Saving preds for test set, using these weights","a9fd8314":"## Using Linear Regression for optimal weights","ccfd32a5":"## Model 3","d194b9c0":"### How to adjust these weights ?\n### **Optimal Weights**","1864dd88":"## Using Adaboost for optimal weights","38b0adc9":"## Predictions","12d2ca46":"## Model 2","3682bc21":"### Saving preds for test set, using these weights","19c8a77e":"### Saving preds for test set, using these weights"}}