{"cell_type":{"117f1d91":"code","927505ae":"code","3175bb5e":"code","aa6b5dda":"code","2570058e":"code","db122bee":"code","872924a2":"code","5c48adf7":"code","e0acd3e5":"code","410e4ee5":"code","e3914864":"code","4c0c16d6":"code","e57d1681":"code","fc97c71e":"code","9059f515":"code","896b340c":"code","5304988e":"code","a27d4cbe":"code","331ae211":"code","3b50c81b":"code","d52c4303":"code","5c57b940":"code","6a322afc":"code","2dfd77ce":"code","fa25e30f":"code","739e7567":"code","240f3967":"code","5ff7ebe1":"code","bdaf3cd9":"code","36fe028f":"code","65cf7434":"code","ccf65d00":"code","4b3dfaed":"code","dc5b6970":"code","52603e84":"code","a83d82bb":"code","17dbf334":"code","33348bee":"code","785ad4dd":"code","45275d13":"code","bfd2bcc8":"code","c346b5e6":"code","c35bf8f4":"code","61084c93":"code","beafc0ec":"markdown","f6d2d968":"markdown","21e09121":"markdown","6f2aa4fa":"markdown","b5159a98":"markdown","9c4ea052":"markdown","035477a7":"markdown","c9212d6f":"markdown","88f5988f":"markdown","5c58a200":"markdown","264a2a22":"markdown","0213872f":"markdown","8ece10cb":"markdown","d5ba6455":"markdown","b166f435":"markdown","c64267f1":"markdown","9f1f27b3":"markdown","680cee23":"markdown","2232d6f4":"markdown","f78ca825":"markdown","e0c05327":"markdown","857a0d73":"markdown","ada9aae7":"markdown","78ae7b20":"markdown","5802a1ce":"markdown","90a8ed01":"markdown","6430fe7a":"markdown","c6575867":"markdown","ecea0056":"markdown","b60c57bf":"markdown","703a60e9":"markdown","2deacbac":"markdown","d420159e":"markdown","340c5e47":"markdown","94ff0eb9":"markdown","a087311c":"markdown","8719a864":"markdown"},"source":{"117f1d91":"#Bibliotecas para cria\u00e7\u00e3o e manipula\u00e7\u00e3o de DATAFRAMES e Algebra \nimport pandas as pd\nimport numpy as np\n\n#Bibliotecas para gera\u00e7\u00e3o de gr\u00e1ficos\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#Bibliotecas para execu\u00e7\u00e3o das metricas e modelo\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_score, accuracy_score, recall_score\nfrom sklearn.metrics import precision_recall_curve, roc_curve , roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import scale\nimport missingno as msno","927505ae":"data = pd.read_csv('..\/input\/titanic_data.csv')\nprint ('This one dataset it has %s rows e %s columns' % (data.shape[0], data.shape[1]))","3175bb5e":"data.info()","aa6b5dda":"# Compute the correlation matrix\ncorr = data.loc[:, ['Pclass', 'Age', 'SibSp','Parch', 'Fare', 'Survived']].corr(method='spearman')\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin= -1.0, vmax=1.0, annot=True)","2570058e":"potentialFeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']","db122bee":"# check how the features are correlated with the overall ratings\n\nfor f in potentialFeatures:\n    related = data['Survived'].corr(data[f], method='spearman')\n    print(\"%s: %f\" % (f,related))","872924a2":"cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']","5c48adf7":"# create a list containing Pearson's correlation between 'overall_rating' with each column in cols\ncorrelations = [ data['Survived'].corr(data[f], method='spearman') for f in cols ]","e0acd3e5":"len(cols), len(correlations)","410e4ee5":"# create a function for plotting a dataframe with string columns and numeric values\n\ndef plot_dataframe(data, y_label):  \n    color='coral'\n    fig = plt.gcf()\n    fig.set_size_inches(9, 7)\n    plt.ylabel(y_label)\n\n    ax = data.correlation.plot(linewidth=3.3, color=color)\n    ax.set_xticks(data.index)\n    ax.set_xticklabels(data.attributes, rotation=75); #Notice the ; (remove it and see what happens !)\n    plt.show()","e3914864":"# create a dataframe using cols and correlations\n\ndata2 = pd.DataFrame({'attributes': cols, 'correlation': correlations}) ","4c0c16d6":"# let's plot above dataframe using the function we created\n    \nplot_dataframe(data2, 'Surviving Overall Rating')","e57d1681":"#is any row NULL ?\ndata.isnull().any().any(), data.shape","fc97c71e":"msno.matrix(data.loc[:, ['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', \n                         'Embarked']],figsize=(11,9))","9059f515":"print(data.loc[:, ['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].count())","896b340c":"data.Age.fillna(data['Age'].mean(), inplace=True)","5304988e":"sns.countplot(data['Survived'], label=\"Count\")","a27d4cbe":"data['Age'].describe()","331ae211":"data.Age.hist (bins=50, figsize=(5,3), color = \"blue\")\nplt.show","3b50c81b":"data['SibSp'].describe()","d52c4303":"data.SibSp.hist (bins=50, figsize=(5,3), color = \"blue\")\nplt.show","5c57b940":"data['Parch'].describe()","6a322afc":"data.Parch.hist (bins=50, figsize=(5,3), color = \"blue\")\nplt.show","2dfd77ce":"data['Fare'].describe()","fa25e30f":"data.Fare.hist (bins=50, figsize=(5,3), color = \"blue\")\nplt.show","739e7567":"data.drop(['PassengerId', 'Pclass', 'Name', 'Cabin', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)","240f3967":"data.head()","5ff7ebe1":"data.tail()","bdaf3cd9":"#def X and Y\nY = np.array(data.Survived.tolist())\npredictors = data.drop('Survived', axis=1)\nX = np.array(predictors.as_matrix())\nseed=42","36fe028f":"Y.shape","65cf7434":"X.shape","ccf65d00":"print(X)","4b3dfaed":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\nfor train_index, test_index in skf.split(X, Y):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    Y_train, Y_test = Y[train_index], Y[test_index]","dc5b6970":"X_train.shape","52603e84":"X_test.shape","a83d82bb":"Y_train.shape","17dbf334":"Y_test.shape","33348bee":"# ROC curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","785ad4dd":"# precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();","45275d13":"#feature importance plot\ndef plot_feature_importance(model):\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,5))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(y='Feature',x='Feature importance',data=tmp)\n    s.set_yticklabels(s.get_yticklabels(),rotation=360)\n    plt.show()","bfd2bcc8":"# Ajustar o modelo usando X como dados de treinamento e y como valores de destino\nrf_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed)\n                                \nrf_clf = rf_clf.fit(X_train, Y_train)\n\n# Modelo prevendo os valores para o conjunto de teste\nY_pred = rf_clf.predict(X_test)\n\n# Prever as probabilidades de classe para o conjunto de teste\nY_score_rf = rf_clf.predict_proba(X_test)[:,1]\n\n# Matriz de confus\u00e3o\nprint(\"Confusion matrix:\")\nprint(confusion_matrix(Y_test, Y_pred))\n\n# Calculo Accuracy \nprint(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n\n# Reportar para outras medidas de classifica\u00e7\u00e3o\nprint(\"Classification report:\")\nprint(classification_report(Y_test, Y_pred))\n\n# ROC curve\nfpr_rfc, tpr_rfc, thresholds = roc_curve(Y_test, Y_score_rf) #Test and probability\nplot_roc_curve(fpr_rfc, tpr_rfc)\n\nauc = roc_auc_score(Y_test, Y_score_rf)\nprint('AUC: %.2f' % auc)\n\n# Precision-recall curve\nprint('Plot the Precision-Recall curve')\nprecision, recall, thresholds = precision_recall_curve(Y_test, Y_score_rf) #Test and probability\nplot_precision_recall()\n\nplot_feature_importance(rf_clf)","c346b5e6":"param_grid = {\n            'n_estimators': [50, 100, 200, 300, 500, 800, 1000, 2000],\n            'max_features': [2, 3, 4],\n            'min_samples_leaf': [1, 2, 4],\n            'min_samples_split': [2, 5, 10, 100]\n            }","c35bf8f4":"gs_rf = GridSearchCV(estimator = rf_clf, param_grid = param_grid, scoring = 'f1', verbose = 10, n_jobs=-1)\ngs_rf.fit(X_train, Y_train)\n\nbest_parameters = gs_rf.best_params_\nprint(\"The best parameters for using this model is\", best_parameters)","61084c93":"# Ajustar o modelo usando X como dados de treinamento e y como valores de destino\nrf_clf = RandomForestClassifier(max_features=2, min_samples_leaf=1, min_samples_split=10, \n                                n_estimators=1000, n_jobs=-1)\n                                \nrf_clf = rf_clf.fit(X_train, Y_train)\n\n# Modelo prevendo os valores para o conjunto de teste\nY_pred = rf_clf.predict(X_test)\n\n# Prever as probabilidades de classe para o conjunto de teste\nY_score_rf = rf_clf.predict_proba(X_test)[:,1]\n\n# Matriz de confus\u00e3o\nprint(\"Confusion matrix:\")\nprint(confusion_matrix(Y_test, Y_pred))\n\n# Calculo Accuracy \nprint(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n\n# Reportar para outras medidas de classifica\u00e7\u00e3o\nprint(\"Classification report:\")\nprint(classification_report(Y_test, Y_pred))\n\n# ROC curve\nfpr_rfc, tpr_rfc, thresholds = roc_curve(Y_test, Y_score_rf) #Test and probability\nplot_roc_curve(fpr_rfc, tpr_rfc)\n\nauc = roc_auc_score(Y_test, Y_score_rf)\nprint('AUC: %.2f' % auc)\n\n# Precision-recall curve\nprint('Plot the Precision-Recall curve')\nprecision, recall, thresholds = precision_recall_curve(Y_test, Y_score_rf) #Test and probability\nplot_precision_recall()\n\nplot_feature_importance(rf_clf)","beafc0ec":"<h1 style=\"font-size:2em;color:#2467C0\">Predicting: 'overall_rating' of a persons<\/h1>\nNow that our data cleaning step is reasonably complete and we can trust and understand the data more, we will start diving into the dataset further. ","f6d2d968":"<h1 style=\"font-size:1.5em;color:#FB41C4\">Analysis of Findings<\/h1>\n\nNow it's time for you to analyze what we plotted. Suppose you have to predict an overall rating that has person who survived. What person attributes would you ask for?\n<br> <br>\n<b> Hint: <\/b> What are the five features with the highest correlation coefficients?","21e09121":"<h6>Curve ROC","6f2aa4fa":"### Let's take a look at top few rows.\n\nWe will use the head function for data frames for this task. This gives us every column in every row.","b5159a98":"<h4>Drop Variables","9c4ea052":"<h1 style=\"font-size:2em;color:#2467C0\">Exploring Data<\/h1>\n\nWe will start our data exploration by generating simple statistics of the data. \n<br><br> \nLet us look at what the data columns are using a pandas attribute called \"info\".","035477a7":"<h1>Thank you very much!!!","c9212d6f":"<h6>knowledge do dataset: ","88f5988f":"<h4>M\u00e9trics","5c58a200":"## Fixing Null Values by Mean Them\n\nIn our next two lines, we will mean the null values by going through each row.\n","264a2a22":"We see that Spearman's Correlation Coefficient for columns fare and survived is 0.32, parch and survived 0.14. <br><br>\nPearson goes from -1 to +1. A value of 0 would have told there is no correlation, so we shouldn\u2019t bother looking at that attribute. A value of 0.32 shows some correlation, although it could be stronger. <br><br>\nAt least, we have these attributes which are slightly correlated. This gives us hope that we might be able to build a meaningful predictor using these \u2018weakly\u2019 correlated features.<br><br>\nNext, we will create a list of features that we would like to iterate the same operation on for correlation Spearman's.","0213872f":"<h6>Overview","8ece10cb":"## Import Libraries\n<br> We will start by importing the Python libraries we will be using in this analysis. These libraries include:\n<ul>\n<li><b>pandas<\/b> and <b>numpy<\/b> for data ingestion and manipulation.<\/li>\n<li><b>matplotlib<\/b> and <b>seaborn<\/b> for data visualization<\/li>\n<li>specific methods from <b>sklearn<\/b> for Machine Learning and <\/li>\n<li><b>missingno<\/b>, which contains functions missing values visualization<\/li>\n\n<\/ul>","d5ba6455":"Next we will start plotting the correlation coefficients of each feature with \"Survived\". We start by selecting the columns and creating a list with correlation coefficients, called \"correlations\".","b166f435":"#### Ingest Data\n\nNow, we will need to read the dataset using the commands below. \n\n<b>Note:<\/b> Make sure you run the import cell above (shift+enter) before you run the data ingest code below.\n\n<b>data<\/b> is a variable pointing to a pandas data frame.","c64267f1":"<br><br><center><h1 style=\"font-size:4em;color:#2467C0\">Titanic Data Analysis<\/h1><\/center>\n<br>","9f1f27b3":"<h1 style=\"font-size:2em;color:#2467C0\">Data preparation<\/h1>","680cee23":"<h6>Precision and Recall","2232d6f4":"<h6>Tuning:","f78ca825":"<h1 style=\"font-size:2em;color:#2467C0\">Data Visualization<\/h1>\n\n## Feature Correlation Analysis ","e0c05327":"## Create a list of potential Features that you want to measure correlation with","857a0d73":"Available in a monotonic relationship between two continuous or ordinal variables. In a monotonic relationship, how variables can change joints, but are not subject to a constant rate. Spearman's correlation coefficient is based on the classified values of each variable rather than the raw data.","ada9aae7":"<h4>Feature Importance","78ae7b20":"<h6>Model RandomForestClassifier","5802a1ce":"Next will display simple statistics of our dataset. You need to run each cell to make sure you see the outputs.","90a8ed01":"### Let's take a look at last rows.\n\nWe will use the tail function for data frames for this task. This gives us every column in every row.","6430fe7a":"<h1><center>Titanic Survived Detection - RandomForestClassifier : F1_score = 0.50 (Precision = 0.54 - Recall = 0.47)<\/h1>\nAdriano Fonseca\n\nOut 2019","c6575867":"<h6>Spearman's","ecea0056":"We make sure that the number of selected features and the correlations calculated are the same, e.g., both 5 in this case. Next couple of cells show some lines of code that use pandas plaotting functions to create a 2D graph of these correlation vealues and column names. ","b60c57bf":"<h1 style=\"font-size:2em;color:#2467C0\">Data Cleaning: Handling Missing Data<\/h1>\nReal data is never clean. We need to make sure we clean the data by converting or getting rid of null or missing values.<br>\nThe next code cell will show you if any of the 891 rows have null value in one of the 11 columns.","703a60e9":"# Are these correlated (using Speraman's correlation coefficient)?","2deacbac":"<h6>Variables","d420159e":"## Which features have the highest correlation with overall_rating?\n\nLooking at the values printed by the previous cell, we notice that the to two are \"Parch\" (0.14) and \"Fare\" (0.32). So these two features seem to have higher correlation with \"overall_rating\".\n","340c5e47":"* Now let's try to find how many data points in each column are null.","94ff0eb9":"<h6>GridSearch","a087311c":"<h4>StratifiedKFold","8719a864":"<h6>Model RandomForestClassifier and Best parameters"}}