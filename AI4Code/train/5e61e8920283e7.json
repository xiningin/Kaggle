{"cell_type":{"5b950978":"code","51340287":"code","c89b265c":"code","3db0b981":"code","66985894":"code","804e766f":"code","b47a2af5":"code","995167d5":"code","921e3ac5":"code","019fddcc":"code","06ebbe15":"code","bfe31196":"code","03f948eb":"code","af7a240e":"code","aa5fd19f":"code","b644a37e":"code","bf53df20":"code","e9a29f95":"code","05086740":"code","e7bb3691":"code","cdbc66eb":"code","9d0bf7a0":"code","60a61c4d":"code","1051dd06":"code","1609a7b3":"code","e508e76b":"code","535e471b":"code","529aec48":"code","71236325":"code","8c69f017":"code","955e3bec":"code","76bd2eb2":"code","adf79acf":"code","4b13a998":"code","f0bdc9ca":"code","497abac1":"code","f52330a0":"code","617fea0c":"code","61d0a729":"code","58b097e6":"code","12dbef9d":"code","2257ffbd":"code","1e942e80":"markdown","7cbaaec2":"markdown","158e190e":"markdown","b7525da0":"markdown","f606fb70":"markdown","1c3e85e5":"markdown","047866c2":"markdown","b9f8d634":"markdown","3d18114f":"markdown","1153b3c3":"markdown","8f65364c":"markdown","c103f1ba":"markdown","4b3c3375":"markdown","8d719d24":"markdown","07cb59d8":"markdown","33e699a6":"markdown","99893673":"markdown","c562b1d6":"markdown","ef8b8008":"markdown","d83b3962":"markdown","5d1d2b3d":"markdown","65e7a794":"markdown","77178b7c":"markdown","c2546a90":"markdown","9f3e42e1":"markdown","fd737c64":"markdown","d8c4b09d":"markdown","b26fdcb3":"markdown","51c72999":"markdown","4427137c":"markdown","5bebea62":"markdown"},"source":{"5b950978":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","51340287":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c89b265c":"import numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","3db0b981":"#Training data\ndf_train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n","66985894":"df_train.head()","804e766f":"df_test.head()","b47a2af5":"print(df_train.shape,df_test.shape)","995167d5":"df_train.info()","921e3ac5":"df_train.isnull().sum()\n","019fddcc":"null_columns=df_train.columns[df_train.isnull().any()]\n# print all rows with atleast one null values\nprint(df_train[df_train.isnull().any(axis=1)][null_columns])","06ebbe15":"df_train.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n","bfe31196":"# Moving on to test dataset.","03f948eb":"df_test.isna().sum()","af7a240e":"# Nothing to worry about. ","aa5fd19f":"# Contains positive tweets \n# Here we have a glimpse of positive cases\ndf_pos = df_train[df_train['sentiment']=='positive']\ndf_pos['text'].head()","b644a37e":"df_neg = df_train[df_train['sentiment']=='negative']\ndf_neg['text'].head()","bf53df20":"df_neu = df_train[df_train['sentiment']=='neutral']\ndf_neu['text'].head()","e9a29f95":"# Now lets check out whether tha dataset is distributed equally or not","05086740":"df_train['sentiment'].value_counts()\n","e7bb3691":"sns.barplot(df_train['sentiment'].value_counts().index,df_train['sentiment'].value_counts(),palette='rocket')","cdbc66eb":"# Lets check out for tes dataset what is the proportions","9d0bf7a0":"df_test['sentiment'].value_counts()","60a61c4d":"sns.barplot(df_test['sentiment'].value_counts().index,df_test['sentiment'].value_counts(),palette='rocket')","1051dd06":"import torch","1609a7b3":"from transformers import BertForQuestionAnswering\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","e508e76b":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","535e471b":"def answer_question(question, answer_text):\n    '''\n    Takes a `question` string and an `answer_text` string (which contains the\n    answer), and identifies the words within the `answer_text` that are the\n    answer. Prints them out.\n    '''\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = tokenizer.encode(question, answer_text)\n\n    # Report how long the input sequence is.\n    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example question through the model.\n    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n\n    return answer","529aec48":"text ='Spent the entire morning in a meeting w\/ a vendor, and my boss was not happy w\/ them. Lots of fun.I had other plans for my morning'\nquestion = 'What text is neutral?'","71236325":"ans = answer_question(question, text)\nprint(ans)","8c69f017":"# Initialise the text\ndf_train['Bert_answers'] = ''","955e3bec":"df_train.head()","76bd2eb2":"positive_question = 'What text is positive ?' # In case of positive sentiment\nnegative_question = 'What text is negative ?'  # In case of negative sentiment\nneutral_question = 'What text is neutral?' # In case of neutral sentiment","adf79acf":"df_test.shape[0]","4b13a998":"\ndf_test['selected_text'] = ''\ni = 0\nwhile(i!=df_test.shape[0]):\n    if (df_test['sentiment'].iloc[i]== 'positive'):\n        df_test['selected_text'].iloc[i] = answer_question(positive_question,df_test['text'][i])\n    elif (df_test['sentiment'].iloc[i]== 'negative'):\n        df_test['selected_text'].iloc[i] = answer_question(negative_question,df_test['text'][i])\n    else :\n        df_test['selected_text'].iloc[i] = answer_question(neutral_question,df_test['text'][i])\n    print(df_test['selected_text'].iloc[i])\n    print(i)\n    i = i+1\n    \n     \n","f0bdc9ca":"df_test.head()","497abac1":"df_test = df_test.drop(['text','sentiment'],axis = 1)\ndf_test.head()","f52330a0":"df_test.set_index('textID',inplace = True)\ndf_test.head()","617fea0c":"df_test.to_csv('submission.csv')","61d0a729":"# So we try to make efficent code","58b097e6":"# def soc_iter(sentiment,text):\n#     if (sentiment == 'positive'):\n#         result = answer_question(positive_question,text)\n#     elif (sentiment == 'negative'):\n#         result = answer_question(negative_question,text)\n#     else :\n#         result = answer_question(neutral_question,text)\n        \n    ","12dbef9d":"# %%timeit\n# df_test['selected_text'] = ''\n# draw_series = []\n# for index, row in df_test.iterrows():\n#     draw_series.append(soc_iter(row['sentiment'],row['text']))\n#     print(index)\n    \n# df_test['selected_text'] = draw_series","2257ffbd":"# We can see this method even though 321 times faster but still is gonna take an hour to complete","1e942e80":"From my intution we can see the test dataset is in same proportion !!","7cbaaec2":"# No preprocessing! Let's do this bare-handed","158e190e":"For Question Answering we use the BertForQuestionAnswering class from the transformers library.","b7525da0":"* # Evaluation Metric","f606fb70":"*This is slighly modified problem than the sentiment analysis. Here we have to extract the sentiment sentences which cause the specific nature of senytence i.e.** which line of the sentences implies whether the sentiment is positive or negative.** This seems a little more intresting problem than the classical problem of sentiment classification.*","1c3e85e5":"# Reading all datasets available\n\nWe have total of three csv file : train , test and submission","047866c2":"# About the competition","b9f8d634":"So the dataset which we have got is uneven with more more neutral text than positive and negative.","3d18114f":"## Demo shot : Just in case for understanding","1153b3c3":"# We gonna go all the way down,guys!!","8f65364c":"### We will ask BERT three questions","c103f1ba":"Without further delay let's dive in completely.","4b3c3375":"Well this metric is quite easy to understand from the code give above. If you don't get it below is one example for your refrence","8d719d24":"A = {0,1,2,5,6}\n\nB = {0,2,3,4,5,7,9}\n\nSolution: \n                 \n                 = J(A,B) = |A\u2229B| \/ |A\u222aB| \n\n                 = |{0,2,5}| \/ |{0,1,2,3,4,5,6,7,9}|\n                 \n                 = 3\/9 = 0.33.","07cb59d8":"Well we both nan value is in same column. I think we have all right to dump it ;)","33e699a6":"# Let't get our guns ready \n***(Import Libraries)***","99893673":"## Since this is a pretrained model we don't need to train model on the text.So we will directly apply the dataset\n\nThis may seem bizarre to few people but it's kinda cool to try.","c562b1d6":"Its strange that out of 27k rows only **1 text and 1 selected_text** is empty. \n\nThis make me curious. Let's dig a bit more","ef8b8008":"# Data distribution based on Sentiment Column","d83b3962":"For more please refer to this :\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/jaccard-index\/","5d1d2b3d":"## Load Fine-Tuned BERT-large\n","65e7a794":"# Voila! This does amazingly well!!","77178b7c":"So we can see select_text column is missing in the testing dataset ! ","c2546a90":"From what I have understand this problem the sentiment extraction does not need any text preprocessing as even the extracted sentence do have stopwords and all this!","9f3e42e1":"Our dataset is of nominal size for training ","fd737c64":"##  Install huggingface transformers library \n\nPlease be sure to keep internet on!","d8c4b09d":"# BERT is HERE!!","b26fdcb3":"## This was gonna take foreever to see.We need to amp up our cpu power which is rather impossible","51c72999":"This is need for something special!! And wkt i.e. BERT( for now ;) )","4427137c":"## For Easy-Pissy reproducible code we can use this amazing function","5bebea62":"I will be using pytorch and Hugging face transformer for this task. Its quite easy to implement and use."}}