{"cell_type":{"f56bb7fb":"code","6b6029d3":"code","4582c17a":"code","00595793":"code","4449200c":"code","53d6ecbc":"code","054e7c46":"code","993941fd":"code","ec4d182b":"code","1cca6b43":"code","6a57cc96":"code","27fb872d":"markdown","bb757b54":"markdown","a3bd8897":"markdown","719090db":"markdown","01daac49":"markdown","5dd0a21e":"markdown","0f5194a1":"markdown","14d47855":"markdown","bf8b3a5b":"markdown","0b25847a":"markdown","d3a2085b":"markdown","68e06cbf":"markdown","08d84734":"markdown","a49bfea6":"markdown","57dc3107":"markdown","bb9e7f98":"markdown","296c1ae4":"markdown","a2512fd1":"markdown","3bf9d802":"markdown","c80e41df":"markdown","39579594":"markdown","2d648a66":"markdown","7f355fa8":"markdown","207d7ffe":"markdown","fcf4df4b":"markdown","2e9d4ca6":"markdown","0fc90369":"markdown","8194ada4":"markdown","990c2bbf":"markdown","2ea12075":"markdown","0141bc52":"markdown","b95cda98":"markdown","03e19cf9":"markdown","1bc0735f":"markdown","4bdc4603":"markdown"},"source":{"f56bb7fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6b6029d3":"iris = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\n","4582c17a":"iris.head()","00595793":"iris.info()","4449200c":"X = iris[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n\nX.head()","53d6ecbc":"y = iris['Species']\n\ny.head()","054e7c46":"from sklearn.preprocessing import LabelEncoder\n\nle=LabelEncoder()\n\ny=le.fit_transform(y)","993941fd":"# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","ec4d182b":"# Import the AdaBoost classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# Create adaboost classifer object\nabc = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)\n\n# Train Adaboost Classifer\nmodel1 = abc.fit(X_train, y_train)\n\n\n#Predict the response for test dataset\ny_pred = model1.predict(X_test)","1cca6b43":"#import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import accuracy_score\n\n\n# calculate and print model accuracy\nprint(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(y_test, y_pred))","6a57cc96":"# load required classifer\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# import Support Vector Classifier\nfrom sklearn.svm import SVC\n\n\n# import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import accuracy_score\nsvc=SVC(probability=True, kernel='linear')\n\n\n# create adaboost classifer object\nabc =AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1, random_state=0)\n\n\n# train adaboost classifer\nmodel2 = abc.fit(X_train, y_train)\n\n\n# predict the response for test dataset\ny_pred = model2.predict(X_test)\n\n\n# calculate and print model accuracy\nprint(\"Model Accuracy with SVC Base Estimator:\",accuracy_score(y_test, y_pred))\n","27fb872d":"<a class=\"anchor\" id=\"0\"><\/a>\n# **AdaBoost Classifier Tutorial in Python**\n\n\nHello friends,\n\n\n\nIn recent years, boosting algorithms have gained massive popularity in kaggle competitions. The winners of these competitions use boosting algorithms to achieve high performance. Boosting algorithms such as AdaBoost, Gradient Boosting, and XGBoost are widely used machine learning algorithms. In this kernel, we will discuss **AdaBoost  algorithm**.\n\n\nSo, let's get started.","bb757b54":"- In this case, we have got a classification rate of 91.11%, which is considered as a very good accuracy.\n\n\n- In this case, SVC Base Estimator is getting better accuracy then Decision tree Base Estimator.","a3bd8897":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Notebook Contents**\n\n1. [Intro to Ensemble Machine Learning](#1)\n    - [1.1. Bagging](#1.1)\n    - [1.2. Boosting](#1.2)\n    - [1.3. Stacking](#1.3)\n1. [How are base-learners classified](#2)\n1. [AdaBoost Classifier](#3)\n1. [AdaBoost algorithm intuition](#4)\n1. [Difference between AdaBoost and Gradient Boosting model](#5)\n1. [AdaBoost implementation in Python](#6)\n    - [6.1 Import libraries](#6.1)\n    - [6.2 Load dataset](#6.2)\n    - [6.3 EDA](#6.3)\n    - [6.4 Split dataset into training and test set](#6.4)\n    - [6.5 Build the AdaBoost model](#6.5)\n    - [6.6 Evaluate model](#6.6)\n    - [6.7 Further evaluation with SVC base estimator](#6.7)\n1. [Advantages and disadvantages of AdaBoost](#7)\n1. [Results and Conclusion](#8)","719090db":"### 6.5 Build the AdaBoost model <a class=\"anchor\" id=\"6.5\"><\/a>","01daac49":"### **1.1 Bagging** <a class=\"anchor\" id=\"1.1\"><\/a>\n\n- **Bagging** stands for **bootstrap aggregation**. \n\n- It combines multiple learners in a way to reduce the variance of estimates. \n\n- For example, random forest trains N Decision Trees where we will train N different trees on different random subsets of the data and perform voting for final prediction. \n\n- **Bagging ensembles** methods are **Random Forest** and **Extra Trees**.","5dd0a21e":"### 6.2 Load dataset <a class=\"anchor\" id=\"6.2\"><\/a>","0f5194a1":"### 6.6 Evaluate Model <a class=\"anchor\" id=\"6.6\"><\/a>\n\nLet's estimate, how accurately the classifier or model can predict the type of cultivars.","14d47855":"**If this helped in your learning, then please <font color=\"red\"><b>UPVOTE<\/b><\/font>  \u2013 because they are the source of motivation!**","bf8b3a5b":"### ** 1.2 Boosting** <a class=\"anchor\" id=\"1.2\"><\/a>\n\n- **Boosting** algorithms are a set of the weak classifiers to create a strong classifier. \n\n- Strong classifiers offer error rate close to 0. \n\n- Boosting algorithm can track the model who failed the accurate prediction. \n\n- Boosting algorithms are less affected by the overfitting problem. \n\n- The following three algorithms have gained massive popularity in data science competitions.\n\n  - AdaBoost (Adaptive Boosting)\n  - Gradient Tree Boosting (GBM)\n  - XGBoost\n  \n- We will discuss AdaBoost in this kernel and GBM and XGBoost in future kernels.","0b25847a":"# **5. Difference between AdaBoost and Gradient Boosting** <a class=\"anchor\" id=\"5\"><\/a>\n\n\n[Back to Notebook Contents](#0.1)\n\n\n- **AdaBoost** stands for **Adaptive Boosting**. It works on sequential ensemble machine learning technique. The general idea of boosting algorithms is to try predictors sequentially, where each subsequent model attempts to fix the errors of its predecessor.\n\n\n- **GBM or Gradient Boosting** also works on sequential model. Gradient boosting calculates the gradient (derivative) of the Loss Function with respect to the prediction (instead of the features). Gradient boosting increases the accuracy by minimizing the Loss Function (error which is difference of actual and predicted value) and having this loss as target for the next iteration.\n\n\n- Gradient boosting algorithm builds first weak learner and calculates the Loss Function. It then builds a second learner to predict the loss after the first step. The step continues for third learner and then for fourth learner and so on until a certain threshold is reached.\n\n\n- So, the question arises in mind that how AdaBoost is different than Gradient Boosting algorithm since both of them works on Boosting technique.\n\n\n- Both AdaBoost and Gradient Boosting build weak learners in a sequential fashion. Originally, AdaBoost was designed in such a way that at every step the sample distribution was adapted to put more weight on misclassified samples and less weight on correctly classified samples. The final prediction is a weighted average of all the weak learners, where more weight is placed on stronger learners.\n\n\n- Later, it was discovered that AdaBoost can also be expressed as in terms of the more general framework of additive models with a particular loss function (the exponential loss).\n\n\n- So, the main differences between AdaBoost and GBM are as follows:-\n\n\n  1. The main difference therefore is that Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function (Exponential loss function). Hence, gradient boosting is much more flexible.\n\n\n  2. AdaBoost can be interepted from a much more intuitive perspective and can be implemented without the reference to gradients by reweighting the training samples based on classifications from previous learners.\n\n\n  3. In Adaboost, shortcomings are identified by high-weight data points while in Gradient Boosting, shortcomings of existing weak learners are identified by gradients.\n\n\n  4. Adaboost is more about \u2018voting weights\u2019 and Gradient boosting is more about \u2018adding gradient optimization\u2019. \n\n\n  5. Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model. At each iteration, Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances.","d3a2085b":"![AdaBoost Classifier](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1542651255\/image_3_nwa5zf.png)","68e06cbf":"# **3. AdaBoost Classifier** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Notebook Contents](#0.1)\n\n\n- **AdaBoost or Adaptive Boosting** is one of the ensemble boosting classifier proposed by Yoav Freund and Robert Schapire in 1996. \n\n- It combines multiple weak classifiers to increase the accuracy of classifiers. \n\n- AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. \n\n- The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. \n\n- Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. \n\n- **AdaBoost** should meet two conditions:\n\n   1. The classifier should be trained interactively on various weighed training examples.\n  \n   2. In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.","08d84734":"# **7. Advantages and disadvantages of AdaBoost** <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Notebook Contents](#0.1)\n\n\n- The advantages are as follows:\n\n   1. AdaBoost is easy to implement. \n  \n   2. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. \n  \n   3. We can use many base classifiers with AdaBoost. \n  \n   4. AdaBoost is not prone to overfitting. ","a49bfea6":"### 6.7 Further evaluation with SVC base estimator <a class=\"anchor\" id=\"6.7\"><\/a>\n\n\n- For further evaluation, we will use SVC as a base estimator as follows:","57dc3107":"# **1. Intro to Ensemble Machine Learning** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Notebook Contents](#0.1)\n\n\n- An ensemble model is a composite model which combines a series of low performing or weak classifiers with the aim of creating a strong classifier. \n\n- Here, individual classifiers vote and final prediction label returned that performs majority voting. \n\n- Now, these individual classifiers are combined according to some specific criterion to create an ensemble model.\n\n- These ensemble models offer greater accuracy than individual or base classifiers. \n\n- These models can parallelize by allocating each base learner to different mechanisms. \n\n- So, we can say that ensemble learning methods are meta-algorithms that combine several machine learning algorithms into a single predictive model to increase performance. \n\n- Ensemble models are created according to some specific criterion as stated below:-\n\n  - **Bagging** - They can be created to decrease model variance using bagging approach.\n  \n  - **Boosting** - They can be created to decrease model bias using a boosting approach. \n  \n  - **Stacking** - They can be created to improve model predictions using stacking approach.\n  \n  \n- It can be depicted with the help of following diagram.","bb9e7f98":"### Create Adaboost Classifier\n\n- The most important parameters are `base_estimator`, `n_estimators` and `learning_rate`.\n\n- **base_estimator** is the learning algorithm to use to train the weak models. This will almost always not needed to be changed because by far the most common learner to use with AdaBoost is a decision tree \u2013 this parameter\u2019s default argument.\n\n- **n_estimators** is the number of models to iteratively train.\n\n- **learning_rate** is the contribution of each model to the weights and defaults to 1. Reducing the learning rate will mean the weights will be increased or decreased to a small degree, forcing the model train slower (but sometimes resulting in better performance scores).\n\n- **loss** is exclusive to AdaBoostRegressor and sets the loss function to use when updating weights. This defaults to a linear loss function however can be changed to square or exponential.","296c1ae4":"# **4. AdaBoost algorithm intuition** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Notebook Contents](#0.1)\n\n\n- It works in the following steps:\n\n   1. Initially, Adaboost selects a training subset randomly.\n  \n   2. It iteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training.\n  \n   3. It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability for classification.\n  \n   4. Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The more accurate classifier will get high weight.\n  \n   5. This process iterate until the complete training data fits without any error or until reached to the specified maximum number of estimators.\n  \n   6. To classify, perform a \"vote\" across all of the learning algorithms you built.\n  \n  \n- The intuition can be depicted with the following diagram:","a2512fd1":"- In this case, we got an accuracy of 86.67%, which will be considered as a good accuracy.","3bf9d802":"We can see that there are no missing values in the dataset.","c80e41df":"So, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n","39579594":"# **8. Results and Conclusion** <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Notebook Contents](#0.1)\n\n\n- In this kernel, we have discussed AdaBoost classifier.\n\n- We have discussed how the base-learners are classified.\n\n- Then, we move on to discuss the intuition behind AdaBoost classifier.\n\n- We have also discuss the differences between AdaBoost classifier and GBM.\n\n- Then, we present the implementation of AdaBoost classifier using iris dataset.\n\n- Lastly, we have discussed the advantages and disadvantages of AdaBoost classifier.","2d648a66":"[Go to Top](#0)","7f355fa8":"### 6.4 Split dataset into training set and test set <a class=\"anchor\" id=\"6.4\"><\/a>","207d7ffe":"- The disadvantages are as follows:\n\n   1. AdaBoost is sensitive to noise data. \n  \n   2. It is highly affected by outliers because it tries to fit each point perfectly. \n  \n   3. AdaBoost is slower compared to XGBoost.","fcf4df4b":"### 6.1 Import libraries <a class=\"anchor\" id=\"6.1\"><\/a>","2e9d4ca6":"### **1.3 Stacking** <a class=\"anchor\" id=\"1.3\"><\/a>\n\n- **Stacking** (or stacked generalization) is an ensemble learning technique that combines multiple base classification models predictions into a new data set. \n\n- This new data are treated as the input data for another classifier. \n\n- This classifier employed to solve this problem. Stacking is often referred to as blending.","0fc90369":"### Declare feature vector and target variable","8194ada4":"- Please refer to my previous kernel - [Bagging vs Boosting](https:\/\/www.kaggle.com\/prashant111\/bagging-vs-boosting?scriptVersionId=24194759)  for a more detailed discussion on on **Bagging** and **Boosting**.","990c2bbf":"# **6. AdaBoost implementation in Python** <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Notebook Contents](#0.1)\n\n\n- Now, we come to the implementation part of AdaBoost algorithm in Python.\n\n- The first step is to load the required libraries.","2ea12075":"### 6.3 EDA <a class=\"anchor\" id=\"6.3\"><\/a>","0141bc52":"### Preview dataset","b95cda98":"# **2. How are base-learners classified** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Notebook Contents](#0.1)\n\n\n- Base-learners are classified into two types.\n\n\n- On the basis of the arrangement of base learners, ensemble methods can be divided into two groups. \n\n  - In parallel ensemble methods, base learners are generated in parallel for example - Random Forest. \n  \n  - In sequential ensemble methods, base learners are generated sequentially for example AdaBoost.\n  \n\n- On the basis of the type of base learners, ensemble methods can be divided into two groups.\n\n  - homogenous ensemble method uses the same type of base learner in each iteration. \n  \n  - heterogeneous ensemble method uses the different type of base learner in each iteration.","03e19cf9":"- To build a AdaBoost classifier, imagine that as a first base classifier we train a Decision Tree algorithm to make predictions on our training data. \n\n- Now, following the methodology of AdaBoost, the weight of the misclassified training instances is increased. \n\n- The second classifier is trained and acknowledges the updated weights and it repeats the procedure over and over again.\n\n- At the end of every model prediction we end up boosting the weights of the misclassified instances so that the next model does a better job on them, and so on.\n\n- AdaBoost adds predictors to the ensemble gradually making it better. The great disadvantage of this algorithm is that the model cannot be parallelized since each predictor can only be trained after the previous one has been trained and evaluated.\n\n- Below are the steps for performing the AdaBoost algorithm:\n\n  1. Initially, all observations are given equal weights.\n  \n  2. A model is built on a subset of data.\n  \n  3. Using this model, predictions are made on the whole dataset.\n  \n  4. Errors are calculated by comparing the predictions and actual values.\n  \n  5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n  \n  6. Weights can be determined using the error value. For instance,the higher the error the more is the weight assigned to the observation.\n  \n  7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.","1bc0735f":"### View summary of dataframe","4bdc4603":"![Ensemble Machine Learning](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1542651255\/image_1_joyt3x.png)"}}