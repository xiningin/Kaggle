{"cell_type":{"975347fc":"code","32ab3cc4":"code","1c961b0f":"code","fba1b45d":"code","5e89418a":"code","da9e7cb9":"code","0231d770":"code","1ec08cfe":"code","43079441":"code","361f8060":"code","da1c3ab1":"code","728cfeda":"code","55a34360":"code","1cbb6fe6":"code","10422620":"code","7b832cbd":"code","8f2d4de2":"code","d52d21a2":"code","b7a5fa02":"code","0bda5676":"code","84a23d71":"code","533e37ce":"code","8ea15f64":"markdown","793185d6":"markdown","b3d402d3":"markdown","d8a7635d":"markdown","9e9ba874":"markdown"},"source":{"975347fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport h2o\nimport seaborn as sns\nimport json\n%matplotlib inline\nfrom pandas.io.json import json_normalize\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold","32ab3cc4":"from sklearn.metrics import mean_squared_error\ndef rmse(y_true, y_pred):\n    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)","1c961b0f":"def load_df(filename):\n    json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    df = pd.read_csv(filename, converters={column: json.loads for column in json_cols}, \n                     dtype={'fullVisitorId': 'str'})\n    \n    for column in json_cols:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    return df","fba1b45d":"%%time\ntrain_df = load_df('..\/input\/train.csv')\ntest_df = load_df('..\/input\/test.csv')\ntrain_ind = train_df['fullVisitorId'].copy()\ntest_ind = test_df['fullVisitorId'].copy()","5e89418a":"def feature_design(df):\n    df['date'] = pd.to_datetime(df['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))\n    for col in ['visitNumber', 'totals.hits', 'totals.pageviews']:\n        df[col] = df[col].astype(float)\n        \n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    df['month_unique_user_count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n    df['day_unique_user_count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n    df['weekday_unique_user_count'] = df.groupby('weekday')['fullVisitorId'].transform('nunique')\n    df['weekofyear_unique_user_count'] = df.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n    \n    df['browser_category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n    df['browser_operatingSystem'] = df['device.browser'] + '_' + df['device.operatingSystem']\n    df['source_country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n    \n    df['visitNumber'] = np.log1p(df['visitNumber'])\n    df['totals.hits'] = np.log1p(df['totals.hits'])\n    df['totals.pageviews'] = np.log1p(df['totals.pageviews'].fillna(0))\n    \n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n    \n    df['mean_hits_per_day'] = df.groupby(['day'])['totals.hits'].transform('mean')\n    df['sum_hits_per_day'] = df.groupby(['day'])['totals.hits'].transform('median')\n\n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('median')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n\n    df['sum_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('median')\n    df['mean_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('mean')\n\n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('median')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\n    df['sum_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('sum')\n    df['count_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('median')\n    df['mean_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('mean')\n\n    df['sum_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('sum')\n    df['count_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('median')\n    df['mean_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('mean')\n\n    df['user_pageviews_sum'] = df.groupby('fullVisitorId')['totals.pageviews'].transform('sum')\n    df['user_hits_sum'] = df.groupby('fullVisitorId')['totals.hits'].transform('sum')\n\n    df['user_pageviews_count'] = df.groupby('fullVisitorId')['totals.pageviews'].transform('count')\n    df['user_hits_count'] = df.groupby('fullVisitorId')['totals.hits'].transform('count')\n\n    df['user_pageviews_sum_to_mean'] = df['user_pageviews_sum'] \/ df['user_pageviews_sum'].mean()\n    df['user_hits_sum_to_mean'] = df['user_hits_sum'] \/ df['user_hits_sum'].mean()\n\n    df['user_pageviews_to_region'] = df['user_pageviews_sum'] \/ df['mean_pageviews_per_region']\n    df['user_hits_to_region'] = df['user_hits_sum'] \/ df['mean_hits_per_region']\n    \n    useless_columns = ['sessionId', 'visitId', 'fullVisitorId', 'date', 'visitStartTime','user_pageviews_sum', 'user_hits_sum',\n                      'user_pageviews_count', 'user_hits_count']\n    df = df.drop(useless_columns, axis = 1)\n    \n    return df","da9e7cb9":"target = train_df['totals.transactionRevenue'].fillna(0).astype(float)\ntarget = target.apply(lambda x: np.log(x) if x > 0 else x)\ndel train_df['totals.transactionRevenue']\n\ncolumns = [col for col in train_df.columns if train_df[col].nunique() > 1]\n\ntrain_df = train_df[columns].copy()\ntest_df = test_df[columns].copy()\n\ntrain_df =  feature_design(train_df)\ntest_df =  feature_design(test_df)","0231d770":"cat_cols = train_df.select_dtypes(exclude=['float64', 'int64']).columns\n\nfor col in cat_cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))","1ec08cfe":"import lightgbm","43079441":"def lgb_train(X, y, test_X, params):\n    kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n    pred_test = 0\n    pred_train = 0\n    for dev_index, val_index in kf.split(X):\n        train_x, valid_x = X.iloc[dev_index,:], X.iloc[val_index,:]\n        train_y, valid_y = y[dev_index], y[val_index]\n        lgtrain = lightgbm.Dataset(train_x, train_y,categorical_feature=list(cat_cols))\n        lgvalid = lightgbm.Dataset(valid_x, valid_y,categorical_feature=list(cat_cols))\n        model = lightgbm.train(params, lgtrain, 2000, valid_sets=[lgvalid], early_stopping_rounds=100, verbose_eval=100)\n        pred_test_iter = model.predict(test_X, num_iteration=model.best_iteration)\n        pred_test_iter[pred_test_iter<0]=0\n        pred_test+=pred_test_iter\n        pred_train_iter = model.predict(X, num_iteration=model.best_iteration)\n        pred_train_iter[pred_train_iter<0]=0\n        pred_train+=pred_train_iter\n    pred_test \/= 5.\n    pred_train  \/= 5.\n    return pred_test, pred_train","361f8060":"params_lgb = {'objective': 'regression', \n          'metric': 'rmse', \n          'num_leaves': 49, \n          'max_depth': 14, \n          'lambda_l2': 0.01931081461346337, \n          'lambda_l1': 0.007163878762237125, \n          'num_threads': 4, \n          'min_child_samples': 40, \n          'learning_rate': 0.01, \n          'bagging_fraction': 0.7910460446769023, \n          'feature_fraction': 0.5046791892199741, \n          'subsample_freq': 5, \n          'bagging_seed': 42, \n          'verbosity': -1}","da1c3ab1":"sub_lgb_test, sub_lgb_train = lgb_train(train_df, target, test_df, params_lgb)","728cfeda":"import xgboost as xgb","55a34360":"def xgb_train(X, y, test_X, params):\n    kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n    pred_test_xgb = 0\n    pred_train_xgb = 0\n    for dev_index, val_index in kf.split(train_df):\n        train_x, valid_x = X.loc[dev_index,:], X.loc[val_index,:]\n        train_y, valid_y = y[dev_index], y[val_index]\n        xgb_train_data = xgb.DMatrix(train_x, train_y)\n        xgb_val_data = xgb.DMatrix(valid_x, valid_y)\n        xgb_submit_data = xgb.DMatrix(test_X)\n        xgb_submit_data_train = xgb.DMatrix(X)\n        xgb_model = xgb.train(params, xgb_train_data, \n                          num_boost_round=2000, \n                          evals= [(xgb_train_data, 'train'), (xgb_val_data, 'valid')],\n                          early_stopping_rounds=100, \n                          verbose_eval=500\n                         )\n        pred_test = xgb_model.predict(xgb_submit_data, ntree_limit=xgb_model.best_ntree_limit)\n        pred_train = xgb_model.predict(xgb_submit_data_train, ntree_limit=xgb_model.best_ntree_limit)\n        pred_test[pred_test<0]=0\n        pred_train[pred_train<0]=0\n        pred_test_xgb += pred_test\n        pred_train_xgb += pred_train\n    pred_test_xgb \/= 5.\n    pred_train_xgb \/= 5.\n    return pred_test_xgb, pred_train_xgb","1cbb6fe6":"params_xgb = {\n            'objective': 'reg:linear',\n            'eval_metric': 'rmse',\n            'eta': 0.001,\n            'max_depth': 7,\n            'gamma': 1.3250360141843498, \n            'min_child_weight': 13.0958516960316, \n            'max_delta_step': 8.88492863796954, \n            'subsample': 0.9864199446951019, \n            'colsample_bytree': 0.8376539278239742,\n            'subsample': 0.6,\n            'colsample_bytree': 0.8,\n            'alpha':0.001,\n            \"num_leaves\" : 40,\n            'random_state': 42,\n            'silent': True,\n            }","10422620":"sub_xgb_test, sub_xgb_train = xgb_train(train_df, target, test_df, params_xgb)","7b832cbd":"from catboost import CatBoostRegressor","8f2d4de2":"def cat_train(X, y, test_X):\n    kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n    pred_test_cat = 0\n    pred_train_cat = 0\n    for dev_index, val_index in kf.split(train_df):\n        train_x, valid_x = X.loc[dev_index,:], X.loc[val_index,:]\n        train_y, valid_y = y[dev_index], y[val_index]\n        model = CatBoostRegressor(iterations=1000,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='RMSE',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)\n        model.fit(train_x, train_y, eval_set=(valid_x, valid_y),use_best_model=True,verbose=True, \n                  cat_features= [i for i in range(len(train_df.columns)) if train_df.columns[i] in cat_cols])\n        pred_test = model.predict(test_X)\n        pred_test[pred_test<0]=0\n        pred_test_cat += pred_test\n        pred_train = model.predict(X)\n        pred_train[pred_train<0]=0\n        pred_train_cat += pred_train\n    pred_test_cat \/= 5.\n    pred_train_cat \/= 5.\n    return pred_test_cat, pred_train_cat","d52d21a2":"sub_cat_test, sub_cat_train = cat_train(train_df, target, test_df)","b7a5fa02":"last = pd.DataFrame()\nlast['fullVisitorId'] = train_ind\nlast['lgbm'] = sub_lgb_train\nlast['xbm'] = sub_xgb_train\nlast['cat'] = sub_cat_train","0bda5676":"last_test = pd.DataFrame()\nlast_test['fullVisitorId'] = test_ind\nlast_test['lgbm'] = sub_lgb_test\nlast_test['xbm'] = sub_xgb_test\nlast_test['cat'] = sub_cat_test","84a23d71":"from sklearn.linear_model import Ridge\nmodel = Ridge().fit(last, target)\npred = model.predict(last_test)","533e37ce":"pred[pred<0] = 0\nsubmission = pd.DataFrame()\nsubmission['fullVisitorId'] = test_ind\nsubmission['PredictedLogRevenue'] = pred\nsubmission = submission.groupby('fullVisitorId').sum()['PredictedLogRevenue'].fillna(0).reset_index()\nsubmission.to_csv('submit_stack1.csv', index=False)","8ea15f64":"## XGBoost","793185d6":"## LGBM","b3d402d3":"## Stacking","d8a7635d":"Categorical columns","9e9ba874":"## CatBoost"}}