{"cell_type":{"bc3cd6a7":"code","28792895":"code","7d501374":"code","1a82a2c4":"code","d50daeb8":"code","66be654c":"code","cae0d202":"code","dca87205":"code","d5dfcbdb":"code","83d1a81f":"code","e5f06a64":"code","65efbcf0":"code","a973d36c":"code","0fe3e3bd":"code","055dcac9":"code","d641ef8c":"code","22a660a3":"code","03ee7cfe":"code","61bf8b81":"code","3344bc21":"code","75561f62":"code","c8e5dbf2":"code","1e74cee8":"code","b274ed42":"code","3d2de4c8":"code","20a9415c":"code","b866e1ed":"code","8f687166":"code","7ae1a3cb":"code","41ffdcd1":"code","913d1abd":"code","ae0dbf13":"code","a5247a28":"code","68b09193":"code","d4371e94":"code","18ca29e2":"code","a87b58a2":"code","1cfb7b30":"code","1dc8dd01":"code","5a892fe9":"code","8965d564":"code","e6f76504":"code","756fb9f5":"code","e4c8d520":"code","75cef74c":"code","84291f0e":"code","cb6f0b4d":"code","a4f23ad9":"code","4c37fbc2":"code","111a5a05":"code","979a20a6":"code","c9c4be0c":"code","1640efce":"code","23e190f3":"code","05d04b43":"code","c484aff8":"code","eb55b3f2":"code","4bc0cbc1":"code","e6e491ac":"code","56394deb":"code","0a6bae86":"code","1bbc2348":"code","e92345ef":"code","c8d59f9b":"code","c8d64241":"code","f1202e88":"code","3e5c9d20":"code","72a51c69":"code","e3e1dedc":"code","0c62218c":"code","6011fa27":"code","200e84d3":"code","f9ba515a":"code","db7e7b28":"code","fd77fe3d":"code","09fcd379":"code","ba4ea9f8":"code","cfa2075e":"code","e82ad6fe":"code","8e51581f":"code","eab64aaa":"code","6d29b696":"code","9d6df58c":"code","e1e78495":"code","13312333":"code","e66a7646":"code","2a190723":"code","50c0162d":"code","3e6de7ed":"code","44fbed35":"code","fb56190f":"code","9b7c3cdc":"code","6855294f":"code","d117875f":"code","7fcc4db5":"code","58490a28":"code","8545b2cb":"code","3429c9fd":"code","4ac5e9b6":"markdown","203d8b31":"markdown","3a2df7ed":"markdown","0c5b5aaa":"markdown","2bd246d4":"markdown","0a0b074b":"markdown","e6fd5da7":"markdown","775b9fe0":"markdown","5be51908":"markdown","6c87b478":"markdown","1b427e1d":"markdown","cc660260":"markdown"},"source":{"bc3cd6a7":"!pip install lime","28792895":"!pip install vit-keras","7d501374":"!pip install tensorflow_addons","1a82a2c4":"!pip install wandb","d50daeb8":"# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n# tf.config.experimental_connect_to_cluster(resolver)\n# # This is the TPU initialization code that has to be at the beginning.\n# tf.tpu.experimental.initialize_tpu_system(resolver)\n# print(\"All devices: \", tf.config.list_logical_devices('TPU'))","66be654c":"# strategy = tf.distribute.TPUStrategy(resolver)","cae0d202":"# !pip install keras","dca87205":"import numpy as np\n# random seeds must be set before importing keras & tensorflow\nmy_seed = 123\nnp.random.seed(my_seed)\nimport random \nrandom.seed(my_seed)\nimport tensorflow as tf\ntf.random.set_seed(my_seed)\n\n# import libraries\nfrom PIL import Image\nimport PIL\nimport matplotlib.pyplot as plt\nimport pathlib\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport keras.backend as K\nimport cv2\n\nfrom lime import lime_image\n\nimport tensorflow_addons as tfa","d5dfcbdb":"import wandb\nfrom wandb.keras import WandbCallback\n\nwandb.login()","83d1a81f":"# connect to google drive to access data\n# from google.colab import drive\n# drive.mount('\/content\/drive')","e5f06a64":"# define data file paths\nROOT_train = '..\/input\/kneeoa-2\/kneeKL224_2\/train'\nROOT_val = '..\/input\/kneeoa-2\/kneeKL224_2\/val'\nROOT_test = '..\/input\/kneeoa-2\/kneeKL224_2\/test'\nknee_data_dir_train = pathlib.Path(ROOT_train)\nknee_data_dir_val = pathlib.Path(ROOT_val)\nknee_data_dir_test = pathlib.Path(ROOT_test)","65efbcf0":"# define some hyperparameters\nbatch_size = 32\nimg_height = 224\nimg_width = 224\n\nimg_shape = (img_height, img_width, 3)\n\n# if set to True, image data will be augmented during training (rescaling, shear, zoom, flip)\n\ndef data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k = 3) # rotate 270\u00ba\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k = 2) # rotate 180\u00ba\n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k = 1) # rotate 90\u00ba\n        \n    # Pixel-level transforms\n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower = .8, upper = 1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta = .1)\n        \n    return image\n\ndata_augmentation = True\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        preprocessing_function=data_augment)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","a973d36c":"# load train data \n\nif not data_augmentation:\n    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n      knee_data_dir_train,\n      seed=123,\n      image_size=(img_height, img_width),\n      batch_size=batch_size)\n\nelse:\n    train_ds = train_datagen.flow_from_directory(\n      knee_data_dir_train,\n      target_size=(img_height, img_width),\n      batch_size=batch_size,\n      class_mode='sparse',\n      shuffle=True\n      # , color_mode='grayscale'\n      )","0fe3e3bd":"# load val data\n\nif not data_augmentation:\n    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n      knee_data_dir_val,\n      seed=123,\n      image_size=(img_height, img_width),\n      batch_size=batch_size)\n\nelse:\n    val_ds = train_datagen.flow_from_directory(\n      knee_data_dir_val,\n      target_size=(img_height, img_width),\n      batch_size=batch_size,\n      class_mode='sparse',\n      shuffle=True\n      # , color_mode='grayscale'\n      )","055dcac9":"# load test data\n\nif not data_augmentation:\n    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n      knee_data_dir_test,\n      seed=123,\n      image_size=(img_height, img_width),\n      batch_size=batch_size)\n    \nelse: \n    test_ds = test_datagen.flow_from_directory(\n      knee_data_dir_test,\n      target_size=(img_height, img_width),\n      batch_size=batch_size,\n      class_mode='sparse',\n      shuffle=True\n      # , color_mode='grayscale'\n      )","d641ef8c":"def show_k_images(k):\n  plt.figure(figsize=(10, 10))\n  for images, labels in test_ds.take(1):\n    for i in range(k):\n      ax = plt.subplot(3, 3, i + 1)\n      plt.imshow(images[i].numpy().astype(\"uint8\"))\n      plt.title(class_names[labels[i]])\n      plt.axis(\"off\")","22a660a3":"# try binary classification [0, 1] vs [2, 3, 4]","03ee7cfe":"# train_labels = np.concatenate([y for x, y in train_ds], axis=0)\n# test_labels = np.concatenate([y for x, y in test_ds], axis=0)\n# val_labels = np.concatenate([y for x, y in val_ds], axis=0)\n\n# uniquetr, countstr = np.unique(train_labels, return_counts=True)\n# print(dict(zip(uniquetr, countstr)))\n\n# uniquete, countste = np.unique(test_labels, return_counts=True)\n# print(dict(zip(uniquete, countste)))\n\n# uniqueva, countsva = np.unique(val_labels, return_counts=True)\n# print(dict(zip(uniqueva, countsva)))","61bf8b81":"# load DenseNet169\n\nbase_model = tf.keras.applications.DenseNet169(\n    include_top=False,\n    weights='imagenet',\n    input_shape=img_shape,\n    pooling='max',\n)\n\nbase_model.trainable = False\n\nmodel = tf.keras.Sequential([\n        base_model,\n#         tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(15, activation=tfa.activations.gelu),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(5, activation='softmax')\n    ])","3344bc21":"model = tf.keras.applications.DenseNet169(\n    include_top=True,\n    weights=None,\n    input_shape=img_shape,\n    pooling='max',\n    classes=5\n)\n","75561f62":"model = tf.keras.applications.resnet.ResNet152(\n    include_top=True, weights=None, input_tensor=None,\n    input_shape=img_shape, pooling=None, classes=5,\n)","c8e5dbf2":"try:\n  from vit_keras import vit, utils\nexcept:\n  !pip install vit_keras","1e74cee8":"image_size = 224\nvit_model = vit.vit_b16(\n    image_size=image_size,\n    activation='softmax',\n    pretrained=True,\n    include_top=False,\n    pretrained_top=False,\n    classes=5\n)","b274ed42":"model = tf.keras.Sequential([\n        vit_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.BatchNormalization(),\n#         tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(15, activation = tfa.activations.gelu),\n        tf.keras.layers.BatchNormalization(),\n#         tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(5, 'softmax')\n    ],\n    name = 'vision_transformer')","3d2de4c8":"model.summary()","20a9415c":"LEARNING_RATE = 1e-4\nEPOCHS = 75","b866e1ed":"optimizer = tfa.optimizers.RectifiedAdam(learning_rate = LEARNING_RATE)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n                                                 factor = 0.2,\n                                                 patience = 3,\n                                                 verbose = 1,\n                                                 min_delta = 1e-4,\n                                                 min_lr = 1e-6,\n                                                 mode = 'max')","8f687166":"run = wandb.init(project=\"dsp\", entity=\"dsp-quin\",\n           config={\n               \"learning_rate\":LEARNING_RATE,\n               \"epochs\":EPOCHS,\n               \"batch_size\":batch_size\n           })\n\nconfig = wandb.config","7ae1a3cb":"# compile the model\n# optimizer = tf.keras.optimizers.Adam(config.learning_rate)\n\nmodel.compile(\n  optimizer=optimizer,\n  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n  metrics=['accuracy', tfa.metrics.CohenKappa(num_classes=5, sparse_labels=True, weightage='linear')]\n  )","41ffdcd1":"# define callbacks\n\ndef scheduler(epoch, lr):\n  if epoch < 15:\n    return lr\n  else:\n    return lr * tf.math.exp(-0.1)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                              patience=5, verbose=1, \n                                              min_delta=1e-4, restore_best_weights = True)\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)","913d1abd":"# fit the model\n\nh = model.fit(train_ds,\n              steps_per_epoch = 7304 \/\/ batch_size,\n              validation_data = val_ds,\n              validation_steps = 826 \/\/ batch_size, \n              epochs = config.epochs, verbose = 1, \n              callbacks=[early_stop, reduce_lr, WandbCallback()]\n              )","ae0dbf13":"def plot_performance(h):\n  plt.plot(h.history['accuracy'])\n  plt.plot(h.history['val_accuracy'])\n  plt.title('model accuracy')\n  plt.ylabel('accuracy')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'validation'], loc='upper left')\n  plt.show()\n\ndef plot_loss(h):\n  plt.plot(h.history['loss'])\n  plt.plot(h.history['val_loss'])\n  plt.title('training loss')\n  plt.ylabel('loss')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'validation'], loc='upper left')\n  plt.show()","a5247a28":"plot_performance(h)","68b09193":"plot_loss(h)","d4371e94":"score = model.evaluate_generator(test_ds)\nprint(\"Test loss: \", score[0])\nprint(\"Test accuracy: \", score[1])","18ca29e2":"import seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\npredicted_classes = np.argmax(model.predict(test_ds, steps = test_ds.n \/\/ test_ds.batch_size + 1), axis = 1)\ntrue_classes = test_ds.classes\nclass_labels = list(test_ds.class_indices.keys())  \n\nconfusionmatrix = confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize = (16, 16))\nsns.heatmap(confusionmatrix, cmap = 'Blues', annot = True, cbar = True)\n\nprint(classification_report(true_classes, predicted_classes))","a87b58a2":"acc = h.history['accuracy']\nval_acc = h.history['val_accuracy']\n\nloss = h.history['loss']\nval_loss = h.history['val_loss']","1cfb7b30":"base_model.trainable = True\nprint(\"Number of layers in the base model: \", len(base_model.layers))","1dc8dd01":"# Fine-tune from this layer onwards\nfine_tune_at = 400\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False\n    \n# compile the model\noptimizer = tfa.optimizers.RectifiedAdam(learning_rate = config.learning_rate\/10)\nmodel.compile(\n  optimizer=optimizer,\n  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n  metrics=['accuracy', tfa.metrics.CohenKappa(num_classes=5, sparse_labels=True, weightage='linear')]\n  )","5a892fe9":"h_fine = model.fit(train_ds,\n              steps_per_epoch = 7304 \/\/ batch_size,\n              validation_data = val_ds,\n              validation_steps = 826 \/\/ batch_size, \n              epochs = config.epochs + 35,\n              initial_epoch = h.epoch[-1],\n              verbose = 1, \n              callbacks=[early_stop, reduce_lr, WandbCallback()]\n              )","8965d564":"score = model.evaluate_generator(test_ds)\nprint(\"Test loss: \", score[0])\nprint(\"Test accuracy: \", score[1])","e6f76504":"acc += h_fine.history['accuracy']\nval_acc += h_fine.history['val_accuracy']\n\nloss += h_fine.history['loss']\nval_loss += h_fine.history['val_loss']","756fb9f5":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","e4c8d520":"wandb.log({'Model': 'RN152',\n           'Random Seed': my_seed,\n           'Data': 'with_auto_test',\n           'Dropout': 'True',\n           'Augmentation': str(data_augmentation),\n           'Test loss': score[0],\n           'Test accuracy': score[1]})","75cef74c":"model.save('\/kaggle\/working\/RN152.h5')","84291f0e":"model.save_weights('\/kaggle\/working\/DN169_w.h5')","cb6f0b4d":"wandb.finish()","a4f23ad9":"model = tf.keras.models.load_model('..\/input\/densenet169-01\/model_01.h5')","4c37fbc2":"y_prob = model.predict(test_ds)\ny_pred = y_prob.argmax(axis=-1)","111a5a05":"y_test = test_ds.classes","979a20a6":"#Confution Matrix and Classification Report\n# Y_pred = model.predict_generator(validation_generator, num_of_test_samples \/\/ batch_size+1)\n# y_pred = np.argmax(Y_pred, axis=1)\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# class_names = ['0', '1', '2', '3', '4']\n# print('Confusion Matrix')\n# print(confusion_matrix(test_ds.classes, y_pred))\n# print('Classification Report')\n# print(classification_report(test_ds.classes, y_pred, target_names=class_names))","c9c4be0c":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, y_pred)","1640efce":"print(classification_report(y_test, y_pred, target_names=class_names))","23e190f3":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_gt, y_prob, average=\"weighted\", multi_class=\"ovr\")","05d04b43":"from sklearn.metrics import cohen_kappa_score\n\ncohen_kappa_score(y_gt, y_pred)","c484aff8":"from sklearn.metrics import matthews_corrcoef\n\nmatthews_corrcoef(y_gt, y_pred)\n","eb55b3f2":"model.save('\/content\/drive\/MyDrive\/DSP-H3-Quin\/model_4.h5')","4bc0cbc1":"# model 1: early stopping, 200 epochs, batch size 32, avg pooling; train accuracy 0.62; overfitting a lot\n# model 2: early stopping + LR scheduler, 200 epochs, bs 32, avg pooling; train accuracy 0.73; overfitting (but less)\n# model 3: early stopping + LR scheduler, 200 epochs, bs 32, max pooling; train accuracy 0.40; overfitting a lot\n# model 4: D201, early stopping + LR scheduler, 200 epochs, bs 32, avg pooling; train accuracy","e6e491ac":"# load model\nmodel = tf.keras.models.load_model('\/content\/drive\/MyDrive\/DSP-H3-Quin\/model_2.h5')","56394deb":"# inference with loaded model\nscore = model.evaluate_generator(test_ds)\nprint(\"Test loss: \", score[0])\nprint(\"Test accuracy: \", score[1])","0a6bae86":"# l = []\n# yy = []\n# for images, labels in test_ds.take(1):\n#   for i in range(2):\n#     l.append(images[i].numpy())\n#     yy.append(class_names[labels[i]])","1bbc2348":"# load test image\n# \/content\/drive\/MyDrive\/DSP-H3-Quin\/data_set\/kneeKL224\/auto_test\/4\/9012867_1.png\nimage_test = cv2.imread('..\/input\/knee-osteoarthritis-dataset-with-severity\/auto_test\/4\/9012867_1.png')\nimage_test = image_test.reshape((224, 224, 3))","e92345ef":"def plot_image(img):\n  plt.imshow(img.astype(\"uint8\"))","c8d59f9b":"# instantiate LIME explainer\nexplainer = lime_image.LimeImageExplainer()","c8d64241":"explanation = explainer.explain_instance(image_test.astype('uint8'), model.predict,  \n                                         top_labels=3, hide_color=0, num_samples=1000)","f1202e88":"explanation.top_labels","3e5c9d20":"# get model prediction for loaded test image\nimage_test_temp = image_test.reshape((1, 224, 224, 3))\npred = model.predict(image_test_temp.astype('uint8'))\nprint(np.argmax(pred[0]))","72a51c69":"# plot image with LIME mask\n\nfrom skimage.segmentation import mark_boundaries\n\n# explanation.top_labels[0]\n\ntemp_1, mask_1 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, negative_only=False, num_features=3, hide_rest=False)\ntemp_2, mask_2 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, negative_only=False, num_features=3, hide_rest=False)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15))\nax1.imshow(mark_boundaries(temp_1, mask_1))\nax2.imshow(mark_boundaries(temp_2, mask_2))\nax1.axis('off')\nax2.axis('off')","e3e1dedc":"from tensorflow.keras.models import Model\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nclass GradCAM:\n    def __init__(self, model, classIdx=None, layerName=None):\n        # store the model, the class index used to measure the class\n        # activation map, and the layer to be used when visualizing\n        # the class activation map\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n        # if the layer name is None, attempt to automatically find\n        # the target output layer\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n\n    def find_target_layer(self):\n        # attempt to find the final convolutional layer in the network\n        # by looping over the layers of the network in reverse order\n        for layer in reversed(self.model.layers):\n            # check to see if the layer has a 4D output\n            if len(layer.output_shape) == 4:\n                return layer.name\n        # otherwise, we could not find a 4D layer so the GradCAM\n        # algorithm cannot be applied\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n\n\n    def compute_heatmap(self, image, eps=1e-8):\n        # construct our gradient model by supplying (1) the inputs\n        # to our pre-trained model, (2) the output of the (presumably)\n        # final 4D layer in the network, and (3) the output of the\n        # softmax activations from the model\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n\n        # record operations for automatic differentiation\n        with tf.GradientTape() as tape:\n            # cast the image tensor to a float-32 data type, pass the\n            # image through the gradient model, and grab the loss\n            # associated with the specific class index\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            \n            loss = predictions[:, tf.argmax(predictions[0])]\n    \n        # use automatic differentiation to compute the gradients\n        grads = tape.gradient(loss, convOutputs)\n\n        # compute the guided gradients\n        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n        castGrads = tf.cast(grads > 0, \"float32\")\n        guidedGrads = castConvOutputs * castGrads * grads\n        # the convolution and guided gradients have a batch dimension\n        # (which we don't need) so let's grab the volume itself and\n        # discard the batch\n        convOutputs = convOutputs[0]\n        guidedGrads = guidedGrads[0]\n\n        # compute the average of the gradient values, and using them\n        # as weights, compute the ponderation of the filters with\n        # respect to the weights\n        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n        # grab the spatial dimensions of the input image and resize\n        # the output class activation map to match the input image\n        # dimensions\n        (w, h) = (image.shape[2], image.shape[1])\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n        # normalize the heatmap such that all values lie in the range\n        # [0, 1], scale the resulting values to the range [0, 255],\n        # and then convert to an unsigned 8-bit integer\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer \/ denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n        # return the resulting heatmap to the calling function\n        return heatmap\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n                        colormap=cv2.COLORMAP_VIRIDIS):\n        # apply the supplied color map to the heatmap and then\n        # overlay the heatmap on the input image\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        # return a 2-tuple of the color mapped heatmap and the output,\n        # overlaid image\n        return (heatmap, output)","0c62218c":"# load test image\nimage_test = cv2.imread('..\/input\/knee-osteoarthritis-dataset-with-severity\/auto_test\/4\/9012867_1.png')\nimage_test = image_test.reshape((1, 224, 224, 3))","6011fa27":"# pred_ = loaded_model.predict(image_test.astype('uint8'))\n# print(np.argmax(pred_[0]))","200e84d3":"i = np.argmax(pred[0])","f9ba515a":"# get layer names\nfor idx in range(len(model.layers)):\n  print(model.get_layer(index = idx).name)","db7e7b28":"image = cv2.imread('..\/input\/knee-osteoarthritis-dataset-with-severity\/auto_test\/4\/9012867_1.png')\nimage = cv2.resize(image, (224, 224))\n\nimage_re = image.reshape((1, 224, 224, 3))","fd77fe3d":"print(image.shape, image_re.shape)","09fcd379":"# compute heatmap\nicam = GradCAM(model, layerName='conv5_block32_concat') \nheatmap = icam.compute_heatmap(image_re)\nheatmap = cv2.resize(heatmap, (224, 224))","ba4ea9f8":"print(heatmap.shape, image_test.shape)","cfa2075e":"# overlay heatmap onto test image\n(heatmap, output) = icam.overlay_heatmap(heatmap, image, alpha=0.7)\n\nfig, ax = plt.subplots(1, 3, figsize=(15,15))\n\nax[0].imshow(heatmap)\nax[1].imshow(image)\nax[2].imshow(output)\n","e82ad6fe":"import cv2\n\n# Grayscale then Otsu's threshold\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\n# Find contours\ncnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    x,y,w,h = cv2.boundingRect(c)\n    cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n\ncv2.imshow('thresh', thresh)\ncv2.imshow('image', image)\ncv2.waitKey()","8e51581f":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nimport gc\n\ntf.compat.v1.disable_eager_execution()\n\ndef normalize(x):\n        \"\"\"Utility function to normalize a tensor by its L2 norm\"\"\"\n        return (x + 1e-10) \/ (K.sqrt(K.mean(K.square(x))) + 1e-10)\n\ndef GradCam(model, img_array, layer_name):\n    cls = np.argmax(model.predict(img_array))\n    \n    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n    y_c = model.output[0, cls]\n    conv_output = model.get_layer(layer_name).output\n    grads = tf.gradients(y_c, conv_output)[0]\n    # grads = normalize(grads)\n\n    gradient_function = K.function([model.input], [conv_output, grads])\n    output, grads_val = gradient_function([img_array])\n    output, grads_val = output[0, :], grads_val[0, :, :, :]\n    weights = np.mean(grads_val, axis=(0, 1))\n\n    cam = np.dot(output, weights)\n    cam = np.maximum(cam, 0)  # Passing through ReLU\n    cam \/= np.max(cam)  # scale 0 to 1.0  \n\n    return cam\n\ndef GradCamPlusPlus(model, img_array, layer_name):\n    cls = np.argmax(model.predict(img_array))\n    y_c = model.output[0, cls]\n    conv_output = model.get_layer(layer_name).output\n    grads = tf.gradients(y_c, conv_output)[0]\n    # grads = normalize(grads)\n\n    first = K.exp(y_c)*grads\n    second = K.exp(y_c)*grads*grads\n    third = K.exp(y_c)*grads*grads*grads\n\n    gradient_function = K.function([model.input], [y_c,first,second,third, conv_output, grads])\n    y_c, conv_first_grad, conv_second_grad,conv_third_grad, conv_output, grads_val = gradient_function([img_array])\n    global_sum = np.sum(conv_output[0].reshape((-1,conv_first_grad[0].shape[2])), axis=0)\n\n    alpha_num = conv_second_grad[0]\n    alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum.reshape((1,1,conv_first_grad[0].shape[2]))\n    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, np.ones(alpha_denom.shape))\n    alphas = alpha_num\/alpha_denom\n\n    weights = np.maximum(conv_first_grad[0], 0.0)\n    alpha_normalization_constant = np.sum(np.sum(alphas, axis=0),axis=0)\n    alphas \/= alpha_normalization_constant.reshape((1,1,conv_first_grad[0].shape[2]))\n    deep_linearization_weights = np.sum((weights*alphas).reshape((-1,conv_first_grad[0].shape[2])),axis=0)\n\n    cam = np.sum(deep_linearization_weights*conv_output[0], axis=2)\n    cam = np.maximum(cam, 0)  # Passing through ReLU\n    cam \/= np.max(cam) # scale 0 to 1.0  \n\n    return cam\n\ndef softmax(x):\n    f = np.exp(x)\/np.sum(np.exp(x), axis = 1, keepdims = True)\n    return f\n\ndef ScoreCam(model, img_array, layer_name, max_N=-1):\n\n    cls = np.argmax(model.predict(img_array))\n    act_map_array = Model(inputs=model.input, outputs=model.get_layer(layer_name).output).predict(img_array)\n    \n    # extract effective maps\n    if max_N != -1:\n        act_map_std_list = [np.std(act_map_array[0,:,:,k]) for k in range(act_map_array.shape[3])]\n        unsorted_max_indices = np.argpartition(-np.array(act_map_std_list), max_N)[:max_N]\n        max_N_indices = unsorted_max_indices[np.argsort(-np.array(act_map_std_list)[unsorted_max_indices])]\n        act_map_array = act_map_array[:,:,:,max_N_indices]\n\n    input_shape = model.layers[0].output_shape[0][1:]  # get input shape\n    # 1. upsampled to original input size\n    act_map_resized_list = [cv2.resize(act_map_array[0,:,:,k], input_shape[:2], interpolation=cv2.INTER_LINEAR) for k in range(act_map_array.shape[3])]\n    # 2. normalize the raw activation value in each activation map into [0, 1]\n    act_map_normalized_list = []\n    for act_map_resized in act_map_resized_list:\n        if np.max(act_map_resized) - np.min(act_map_resized) != 0:\n            act_map_normalized = act_map_resized \/ (np.max(act_map_resized) - np.min(act_map_resized))\n        else:\n            act_map_normalized = act_map_resized\n        act_map_normalized_list.append(act_map_normalized)\n    # 3. project highlighted area in the activation map to original input space by multiplying the normalized activation map\n    masked_input_list = []\n    for act_map_normalized in act_map_normalized_list:\n        masked_input = np.copy(img_array)\n        for k in range(3):\n            masked_input[0,:,:,k] *= act_map_normalized\n        masked_input_list.append(masked_input)\n    masked_input_array = np.concatenate(masked_input_list, axis=0)\n    # 4. feed masked inputs into CNN model and softmax\n    pred_from_masked_input_array = softmax(model.predict(masked_input_array))\n    # 5. define weight as the score of target class\n    weights = pred_from_masked_input_array[:,cls]\n    # 6. get final class discriminative localization map as linear weighted combination of all activation maps\n    cam = np.dot(act_map_array[0,:,:,:], weights)\n    cam = np.maximum(0, cam)  # Passing through ReLU\n    cam \/= np.max(cam)  # scale 0 to 1.0\n    \n    return cam\n\ndef superimpose(original_img_path, cam, emphasize=False):\n    \n    img_bgr = cv2.imread(original_img_path)\n\n    heatmap = cv2.resize(cam, (img_bgr.shape[1], img_bgr.shape[0]))\n    if emphasize:\n        heatmap = sigmoid(heatmap, 50, 0.5, 1)\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    \n    hif = .8\n    superimposed_img = heatmap * hif + img_bgr\n    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n    superimposed_img_rgb = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n    \n    return superimposed_img_rgb\n\nimport tensorflow.keras\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\ndef build_guided_model(build_model_function):\n    \"\"\"Function returning modified model.\n    \n    Changes gradient function for all ReLu activations according to Guided Backpropagation.\n    \"\"\"\n    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n        @ops.RegisterGradient(\"GuidedBackProp\")\n        def _GuidedBackProp(op, grad):\n            dtype = op.inputs[0].dtype\n            return grad * tf.cast(grad > 0., dtype) * \\\n                   tf.cast(op.inputs[0] > 0., dtype)\n\n    g = tf.compat.v1.get_default_graph()\n    with g.gradient_override_map({'Relu': 'GuidedBackProp'}):\n        new_model = build_model_function()\n    return new_model\n\ndef GuidedBackPropagation(model, img_array, layer_name):\n    model_input = model.input\n    layer_output = model.get_layer(layer_name).output\n    max_output = K.max(layer_output, axis=3)\n    grads = tf.gradients(max_output, model_input)[0]\n    get_output = K.function([model_input], [grads])\n    saliency = get_output([img_array])\n    saliency = np.clip(saliency[0][0], 0.0, 1.0)  # scale 0 to 1.0  \n    return saliency\n\ndef sigmoid(x, a, b, c):\n    return c \/ (1 + np.exp(-a * (x-b)))\n\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef read_and_preprocess_img(path, size=(224,224)):\n    img = load_img(path, target_size=size)\n    x = img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","eab64aaa":"img_path = '\/content\/drive\/MyDrive\/DSP-H3-Quin\/data_set\/kneeKL224\/test\/4\/9012867R.png'\norig_img = np.array(load_img(img_path),dtype=np.uint8)\nimg_array = read_and_preprocess_img(img_path, size=(224,224))\n\nlayer_name = \"conv5_block32_concat\"\n\ngrad_cam=GradCam(model,img_array,layer_name)\ngrad_cam_superimposed = superimpose(img_path, grad_cam)\ngrad_cam_emphasized = superimpose(img_path, grad_cam, emphasize=True)\n\ngrad_cam_plus_plus=GradCamPlusPlus(model,img_array,layer_name)\ngrad_cam_plus_plus_superimposed = superimpose(img_path, grad_cam_plus_plus)\ngrad_cam_plus_plus_emphasized = superimpose(img_path, grad_cam_plus_plus, emphasize=True)\n\nscore_cam=ScoreCam(model,img_array,layer_name)\nscore_cam_superimposed = superimpose(img_path, score_cam)\nscore_cam_emphasized = superimpose(img_path, score_cam, emphasize=True)\n\nfaster_score_cam=ScoreCam(model,img_array,layer_name, max_N=10)\nfaster_score_cam_superimposed = superimpose(img_path, faster_score_cam)\nfaster_score_cam_emphasized = superimpose(img_path, faster_score_cam, emphasize=True)\n\nsaliency = GuidedBackPropagation(model, img_array, layer_name)\nsaliency_resized = cv2.resize(saliency, (orig_img.shape[1], orig_img.shape[0]))\n\ngrad_cam_resized = cv2.resize(grad_cam, (orig_img.shape[1], orig_img.shape[0]))\nguided_grad_cam = saliency_resized * grad_cam_resized[..., np.newaxis]\n\ngrad_cam_plus_plus_resized = cv2.resize(grad_cam_plus_plus, (orig_img.shape[1], orig_img.shape[0]))\nguided_grad_cam_plus_plus = saliency_resized * grad_cam_plus_plus_resized[..., np.newaxis]\n\nscore_cam_resized = cv2.resize(score_cam, (orig_img.shape[1], orig_img.shape[0]))\nguided_score_cam = saliency_resized * score_cam_resized[..., np.newaxis]\n\nfaster_score_cam_resized = cv2.resize(faster_score_cam, (orig_img.shape[1], orig_img.shape[0]))\nguided_faster_score_cam = saliency_resized * faster_score_cam_resized[..., np.newaxis]\n\nimg_gray = cv2.imread(img_path, 0)\ndx = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\ndy = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\ngrad = np.sqrt(dx ** 2 + dy ** 2)  #Get image gradient\ngrad = cv2.dilate(grad,kernel=np.ones((5,5)), iterations=1)  #Fattening process\ngrad -= np.min(grad)\ngrad \/= np.max(grad)  # scale 0. to 1.\n\ngrad_times_grad_cam = grad * grad_cam_resized\ngrad_times_grad_cam_plus_plus = grad * grad_cam_plus_plus_resized\ngrad_times_score_cam = grad * score_cam_resized\ngrad_times_faster_score_cam = grad * faster_score_cam_resized\n\nfig, ax = plt.subplots(nrows=4,ncols=5, figsize=(18, 16))\nax[0,0].imshow(orig_img)\nax[0,0].set_title(\"input image\")\nax[0,1].imshow(grad_cam_superimposed)\nax[0,1].set_title(\"Grad-CAM\")\nax[0,2].imshow(grad_cam_plus_plus_superimposed)\nax[0,2].set_title(\"Grad-CAM++\")\nax[0,3].imshow(score_cam_superimposed)\nax[0,3].set_title(\"Score-CAM\")\nax[0,4].imshow(faster_score_cam_superimposed)\nax[0,4].set_title(\"Faster-Score-CAM\")\nax[1,0].imshow(orig_img)\nax[1,0].set_title(\"input image\")\nax[1,1].imshow(grad_cam_emphasized)\nax[1,1].set_title(\"Grad-CAM emphasized\")\nax[1,2].imshow(grad_cam_plus_plus_emphasized)\nax[1,2].set_title(\"Grad-CAM++ emphasized\")\nax[1,3].imshow(score_cam_emphasized)\nax[1,3].set_title(\"Score-CAM emphasized\")\nax[1,4].imshow(faster_score_cam_emphasized)\nax[1,4].set_title(\"Faster-Score-CAM emphasized\")\nax[2,0].imshow(saliency_resized)\nax[2,0].set_title(\"Guided-BP\")\nax[2,1].imshow(guided_grad_cam)\nax[2,1].set_title(\"Guided-Grad-CAM\")\nax[2,2].imshow(guided_grad_cam_plus_plus)\nax[2,2].set_title(\"Guided-Grad-CAM++\")\nax[2,3].imshow(guided_score_cam)\nax[2,3].set_title(\"Guided-Score-CAM\")\nax[2,4].imshow(guided_faster_score_cam)\nax[2,4].set_title(\"Guided-Faster-Score-CAM\")\nax[3,0].imshow(grad, 'gray')\nax[3,0].set_title(\"grad\")\nax[3,1].imshow(grad_times_grad_cam, 'gray')\nax[3,1].set_title(\"grad * Grad-CAM\")\nax[3,2].imshow(grad_times_grad_cam_plus_plus, 'gray')\nax[3,2].set_title(\"grad * Grad-CAM++\")\nax[3,3].imshow(grad_times_score_cam, 'gray')\nax[3,3].set_title(\"grad * Score-CAM\")\nax[3,4].imshow(grad_times_faster_score_cam, 'gray')\nax[3,4].set_title(\"grad * Faster-Score-CAM\")\nfor i in range(4):\n    for j in range(5):\n        ax[i,j].axis('off')\nplt.show()","6d29b696":"import os\nimport torch\nimport torchvision\nimport tarfile\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor","9d6df58c":"ROOT_train = '\/content\/drive\/MyDrive\/DSP-H3-Quin\/data_set\/kneeKL224\/train'\nROOT_val = '\/content\/drive\/MyDrive\/DSP-H3-Quin\/data_set\/kneeKL224\/val'\nROOT_test = '\/content\/drive\/MyDrive\/DSP-H3-Quin\/data_set\/kneeKL224\/test'\nROOT_auto_test = '\/content\/drive\/MyDrive\/DSP-H3-Quin\/data_set\/kneeKL224\/auto_test'","e1e78495":"model = torch.load('\/content\/drive\/MyDrive\/DSP-H3-Quin\/model_stuff\/densenet169\/densenet-169-SGD-0\/6-0.665-0.668-0.522.pth')","13312333":"# model.eval()","e66a7646":"test_dataset = ImageFolder(ROOT_auto_test, transform=ToTensor())","2a190723":"classes = test_dataset.classes","50c0162d":"def show_example(img, label):\n    print('Label: ', test_dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))\nshow_example(*test_dataset[0])","3e6de7ed":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","44fbed35":"from torchvision import transforms\ntransform = transforms.Compose([            #[1]\n transforms.Resize(224),                    #[2]\n transforms.CenterCrop(224),                #[3]\n#  transforms.ToTensor(),                     #[4]\n transforms.Normalize(                      #[5]\n mean=[0.485, 0.456, 0.406],                #[6]\n std=[0.229, 0.224, 0.225]\n )])\n","fb56190f":"img, label = test_dataset[0]","9b7c3cdc":"img_t = transform(img)\nbatch_t = torch.unsqueeze(img_t, 0)","6855294f":"model2 = torch.load('\/content\/drive\/MyDrive\/DSP-H3-Quin\/model_stuff\/vgg16\/vgg-16-SGD-0\/5-0.64-0.671-0.585.pth')","d117875f":"model2.eval()","7fcc4db5":"output = model2(batch_t)\n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\nprint(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)","58490a28":"out = model2(batch_t)","8545b2cb":"classes = [test_dataset.classes] # change to labels probably\n_, index = torch.max(out, 1)\npercentage = torch.nn.functional.softmax(out, dim=1)[0] * 10\nprint(labels[index[0]], percentage[index[0]].item())","3429c9fd":"_, indices = torch.sort(out, descending=True)\n[(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]","4ac5e9b6":"#### GradCam Impl","203d8b31":"#### Evaluation Sandbox","3a2df7ed":"#### Score-CAM","0c5b5aaa":"#### DenseNet finetuning","2bd246d4":"### XAI","0a0b074b":"#### log and finish run","e6fd5da7":"### Keras Model Training","775b9fe0":"### Load Model","5be51908":"### Data Loading","6c87b478":"#### GradCam.py","1b427e1d":"#### LIME","cc660260":"### PyTorch Pre-trained Model (ignore this part for now)"}}