{"cell_type":{"ecd06c60":"code","36688047":"code","62aa51c6":"code","773d1080":"code","a2d5900c":"code","fe0fb642":"code","c69d8b9e":"code","1c78fba4":"code","05dc7176":"code","33e4894c":"code","46902bca":"code","735064a0":"code","2d22d7a5":"code","fa32a791":"code","fce5b7ad":"markdown"},"source":{"ecd06c60":"import numpy as np\nimport pandas as pd\nimport collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","36688047":"# I like to take a first look this way too\ndf = train\ndescri = df.describe(include='all')\nDF_OB = df.select_dtypes(include=['object', 'bool']).copy()\ndf_oo = DF_OB.copy()\ndf_oo.iloc[:, :] = \" \"\nfor L in DF_OB.columns:\n    C = collections.Counter(DF_OB[L])\n    CC = sorted(C.items(), key=lambda x: x[1], reverse=True)\n    CCC = collections.OrderedDict(CC)\n    for i in range(0, len(CCC.keys())):\n        df_oo[L][i] = str(round(list(CCC.values())[i] \/ len(DF_OB[L]) * 100.0, 2))","62aa51c6":"# Outliers\n# Graph\nfig, ax = plt.subplots()\nax.scatter(x=train['GrLivArea'], y=train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n# Deleting two outliers points\ntrain = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index)\n# Graph\nfig, ax = plt.subplots()\nax.scatter(x=train['GrLivArea'], y=train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","773d1080":"# Normalization\n\n# Histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\n\n# Applying log(1+x) transformation\ntrain['SalePrice'] = np.log1p(train['SalePrice'])","a2d5900c":"# Transformed histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","fe0fb642":"# Concatenate train and test\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","c69d8b9e":"# Correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n\n# Saleprice correlation matrix\nk = 10\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values,\n                 xticklabels=cols.values)\nplt.show()\n\n# Scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], height=2.5)\nplt.show()","1c78fba4":"# Missing data\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","05dc7176":"# Numerical to categorical\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","33e4894c":"# Encoder\ncols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',\n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',\n        'YrSold', 'MoSold']\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n    \n# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","46902bca":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\nskewness = skewness[abs(skewness) > 0.75]\nskewness = skewness[abs(skewness.Skew) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)    \n# Same to do this: all_data[skewed_features] = np.log1p(all_data[skewed_features])","735064a0":"# Dummies \nall_data = pd.get_dummies(all_data, drop_first=True)","2d22d7a5":"# Until now, my favoryte model\nfrom xgboost import XGBRegressor\nregressor = XGBRegressor(objective='reg:squarederror')\n\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\n\n# With params from this guy kernel's: kaggle.com\/serigne\nmodel_xgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread=-1)\n# Fitting in the Train\ny_train = np.ravel(y_train)\nmodel_xgb.fit(train, y_train)\n\n# Predicting the Test\ny_pred = model_xgb.predict(test)","fa32a791":"# Exporting\noutt = pd.DataFrame()\noutt['Id'] = test_ID.values\noutt['SalePrice'] = np.expm1(y_pred)\noutt.to_csv('Submission0.csv', index=False)","fce5b7ad":"Created on Mon Sep 23 09:26:32 2019\n\n@author: walter using spyder and very inspired on kaggle.com\/serigne\n\nHi guys, this is my second kernel, nothing special; as an kaggle beginner, I took almost \neverything from serigne's methods, made a few code ajustments, aplied my stuff and used only\none model to predict. The main idea here is to study the data cleaning and \nfeature engineering. I think that there is a lot more to improve and I've made some, wait for more! \n"}}