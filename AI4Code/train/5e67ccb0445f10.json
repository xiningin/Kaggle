{"cell_type":{"d99ce5a8":"code","58291647":"code","4bfb8675":"code","7e39ab1c":"code","737753bb":"code","f1719fc4":"code","1aae4c06":"code","08590422":"code","9b729157":"code","9ef92e86":"code","65127c68":"code","a406daa4":"code","78869efd":"code","189c0f57":"code","3fd1e839":"code","c83c34d0":"code","db8799cb":"code","51de53f9":"code","5e07fe4d":"code","286cd1ea":"code","779cbcda":"code","f8210e2e":"code","03e608f5":"code","b965dfe9":"code","e4729602":"code","e837a452":"code","9628cc06":"code","a767c9e2":"code","093b8094":"code","23643f21":"code","0694d036":"code","596aa7b0":"code","18f44c67":"code","58d84f1c":"code","0e372c67":"code","695d12d5":"code","ec55f2a9":"code","64d72876":"code","43812718":"code","aaa8b763":"code","2f50e5d8":"code","293a89ad":"code","6a625299":"code","78db56b6":"code","073dec62":"code","5a8b02ba":"code","0a207adf":"code","bb5c2b84":"code","227a5516":"code","d23d8f76":"code","4dd48f64":"code","2cfafef8":"code","3063f26c":"code","f387a518":"code","173987e9":"code","d4b4ca6a":"code","a68ab431":"code","6d2dd59c":"code","fb01aec4":"code","3db11b6d":"code","4f1b7f07":"code","148be963":"code","e6508e2e":"markdown","d5ac129b":"markdown","f78352bf":"markdown","a79a2191":"markdown","ad1b21fc":"markdown","f9e94d15":"markdown","309c7abc":"markdown","2b05dc44":"markdown","88fb9fcb":"markdown","81fa5886":"markdown","3c52f7d6":"markdown","5abfa935":"markdown","ec72867e":"markdown","020ea246":"markdown","9734f290":"markdown","f57e594b":"markdown"},"source":{"d99ce5a8":"import math\nimport pydot\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy.random import seed\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD, NMF\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout \nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.random import set_seed\npd.set_option('display.max_columns', None)","58291647":"data = pd.read_csv('..\/input\/should-this-loan-be-approved-or-denied\/SBAnational.csv')\ndata.head()","4bfb8675":"data.drop(data[data['MIS_Status'].isnull()].index, axis = 0, inplace = True)","7e39ab1c":"data","737753bb":"df, validation_df  = train_test_split(data,\n                                test_size=0.35,\n                                random_state = 101)","f1719fc4":"df_train, df_test  = train_test_split(df,\n                                test_size=0.25,\n                                random_state = 101)","1aae4c06":"del data","08590422":"df_train.info()","9b729157":"df_train.describe()","9ef92e86":"df.isnull().sum()","65127c68":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"NewExist\", hue=\"MIS_Status\", data=df_train)\nplt.legend()\nplt.show()","a406daa4":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"RevLineCr\", hue=\"MIS_Status\", data=df_train)\nplt.legend()\nplt.show()","78869efd":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"LowDoc\", hue=\"MIS_Status\", data=df_train)\nplt.legend()\nplt.show()","189c0f57":"g = sns.FacetGrid(df_train, hue='MIS_Status', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'Term')\nplt.legend()\nplt.title('Term factor')\nplt.show()","3fd1e839":"plt.figure(figsize = (12, 8))\njob_survey_data = df_train[['CreateJob', 'RetainedJob', 'MIS_Status']]\njob_survey_data[['CreateJob', 'RetainedJob']] = np.sqrt(job_survey_data[['CreateJob', 'RetainedJob']])\nsns.scatterplot(data = job_survey_data, x = 'CreateJob', y = 'RetainedJob', hue = 'MIS_Status', palette = 'magma')\nplt.show()\ndel job_survey_data","c83c34d0":"count = df_train['City'].value_counts()\nprint(f'Unique values: {len(count)}')\ncount_f = count[count>500]\nmore_popular_Cities = set(count_f.index)\nprint(f'Unique values after values grouped: {len(count_f)}')\ncount_f","db8799cb":"count = df_train['Bank'].value_counts()\nprint(f'Unique values: {len(count)}')\ncount_f = count[count>500]\nmore_popular_Banks = set(count_f.index)\nprint(f'Unique values after values grouped: {len(count_f)}')\ncount_f","51de53f9":"df_train['ChgOffDate'].isnull().value_counts()","5e07fe4d":"count = df_train['NAICS'].value_counts()\nprint(f'Unique values: {len(count)}')\ncount_f = count[count>500]\nmore_popular_NAICS = set(count_f.index)\nprint(f'Unique values after values grouped: {len(count_f)}')\ncount_f","286cd1ea":"def unknown_filling_text(val):\n    if pd.isna(val):\n        return 'no data'\n    else:\n        return str(val)\n    \ndef proc_col_City(val):\n    if val not in more_popular_Cities:\n        return 'other'\n    else:\n        return val\n    \ndef proc_col_Bank(val):\n    if val not in more_popular_Banks:\n        return 'other'\n    else:\n        return val\n    \ndef proc_col_NAICS(val):\n    if val not in more_popular_NAICS:\n        return str(val)[:3]\n    else:\n        return str(val)\n\ndef proc_col_MIS_Status(val):\n    if val == 'CHGOFF':\n        return 1\n    elif val == 'P I F':\n        return 0\n    else:\n        raise ValueError('Incorrect MIS_Status value')\n    \ndef check_na(df):\n    if len(df[df.isnull().any(axis=1)])!= 0:\n        raise ValueError('N\\A in data')\n    \ndef pre_dumm_proc(df):\n    df = df.copy()\n    to_drop = [\n        'LoanNr_ChkDgt', 'ChgOffDate', 'Name', 'Zip', 'ApprovalDate',\n        'ApprovalFY', 'DisbursementDate', 'DisbursementGross',\n        'BalanceGross', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv'\n    ]\n    df.drop(columns = to_drop, axis = 1, inplace = True)\n    \n    df['City'] = df['City'].apply(proc_col_City)\n    df['State'] = df['State'].apply(unknown_filling_text)\n    df['Bank'] = df['Bank'].apply(proc_col_Bank)\n    df['BankState'] = df['BankState'].apply(unknown_filling_text)\n    df['NAICS'] = df['NAICS'].apply(proc_col_NAICS)\n    df['NewExist'] = df['NewExist'].apply(unknown_filling_text)\n    df['RevLineCr'] = df['RevLineCr'].apply(unknown_filling_text)\n    df['LowDoc'] = df['LowDoc'].apply(unknown_filling_text)\n    df['MIS_Status'] = df['MIS_Status'].apply(proc_col_MIS_Status)\n    check_na(df)\n    return df\n\ndef dummification(df):\n    dummy_df = pd.DataFrame()\n    object_cols = df.columns[df.dtypes == object]\n    for col in object_cols:\n        dummy_df = pd.concat([dummy_df, create_dummy(col, df)], axis = 1)\n    \n    df_out = pd.concat([df.drop(columns = object_cols), dummy_df], axis = 1)\n    return df_out.sort_index(ascending=False, axis=1)\n        \n\ndef create_dummy(col, df):\n    df_dummy = pd.get_dummies(df[col], drop_first = True)\n    df_dummy.columns = ['dum: ' + col + ': ' + str(name) for name in df_dummy.columns]\n    return df_dummy\n\ndef data_preparation(df):\n    return dummification(pre_dumm_proc(df))\n\ndf_train_d = data_preparation(df_train)\ndf_train_c = pre_dumm_proc(df_train)\ndf_train_d","779cbcda":"df_train_c","f8210e2e":"columns_needed = set(df_train_d.columns)\n\ndef columns_standardization(df):\n    df = df.copy()\n    for col in columns_needed:\n        if col not in set(df.columns):\n            df.insert(loc = len(df.columns), column = col, value = 0, allow_duplicates=False)\n    \n    for col in set(df.columns):\n        if col not in columns_needed:\n            df.drop(columns = col, axis = 1, inplace = True)\n    \n    return df.sort_index(ascending=False, axis=1)","03e608f5":"df_test_d = columns_standardization(data_preparation(df_test))\ndf_test_c = pre_dumm_proc(df_test)\ndf_test_d","b965dfe9":"df_test_c.head()","e4729602":"df_test_d[df_test_d.isnull().any(axis=1)]","e837a452":"df_train_d[df_train_d.isnull().any(axis=1)]","9628cc06":"X_train = df_train_d.drop('MIS_Status', axis = 1)\ny_train = df_train_d['MIS_Status']\nX_test = df_test_d.drop('MIS_Status', axis = 1)\ny_test = df_test_d['MIS_Status']","a767c9e2":"del df, df_train_d, df_train_c, df_test_d, df_test_c","093b8094":"scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_sc = scaler.transform(X_train.values)\nX_test_sc = scaler.transform(X_test.values)","23643f21":"pca = PCA(n_components=2, random_state = 1)\ndf_pca_train = pca.fit_transform(X_train_sc)\ndf_pca_test = pca.transform(X_test_sc)","0694d036":"df_pca_vis = pd.DataFrame(df_pca_train)\ndf_pca_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_pca_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","596aa7b0":"pca_variance = pca.explained_variance_\n\nplt.figure(figsize=(6, 6))\nplt.bar(['0', '1'], pca_variance, align='center', label='individual variance')\nplt.legend()\nplt.ylabel('Variance ratio')\nplt.xlabel('Principal components')\nplt.show()","18f44c67":"lgbr_pca = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_pca.fit(df_pca_train, y_train)\npred = lgbr_pca.predict(df_pca_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","58d84f1c":"svd = TruncatedSVD(n_components=2, random_state = 1)\ndf_svd_train = svd.fit_transform(X_train_sc)\ndf_svd_test = svd.transform(X_test_sc)","0e372c67":"df_svd_vis = pd.DataFrame(df_svd_train)\ndf_svd_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_svd_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","695d12d5":"lgbr_svd = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_svd.fit(df_svd_train, y_train)\npred = lgbr_svd.predict(df_svd_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","ec55f2a9":"nmf = NMF(n_components=2, random_state = 1)\ndf_nmf_train = nmf.fit_transform(X_train_sc, y_train)\ndf_nmf_test = nmf.transform(X_test_sc)","64d72876":"df_nmf_vis = pd.DataFrame(df_nmf_train)\ndf_nmf_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_nmf_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","43812718":"lgbr_nmf = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_nmf.fit(df_nmf_train, y_train)\npred = lgbr_nmf.predict(df_nmf_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","aaa8b763":"lda = LinearDiscriminantAnalysis(n_components=1)\ndf_lda_train = lda.fit_transform(X_train_sc, y_train)\ndf_lda_test = lda.transform(X_test_sc)","2f50e5d8":"df_lda_vis = pd.DataFrame(df_lda_train)\ndf_lda_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_lda_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","293a89ad":"lgbr_lda = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_lda.fit(df_lda_train, y_train)\npred = lgbr_lda.predict(df_lda_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","6a625299":"input_width = len(X_train.columns)\ninput_width","78db56b6":"def dim_red_analysis(n_epochs = None):\n    seed(101)\n    set_seed(101)\n\n    encoder = Sequential()\n    encoder.add(Dense(units = 256, activation = 'relu', input_shape = [input_width]))\n    encoder.add(Dropout(0.2))\n    encoder.add(Dense(units = 16, activation = 'relu'))\n    encoder.add(Dense(units = 2, activation = 'relu'))\n\n    decoder = Sequential()\n    decoder.add(Dense(units = 16, activation = 'relu', input_shape = [2]))\n    decoder.add(Dense(units = 256, activation = 'relu'))\n    decoder.add(Dense(units = input_width, activation = 'relu'))\n\n    autoencoder = Sequential([encoder, decoder])\n\n    autoencoder.compile(loss = 'mse', optimizer = SGD(lr = 12))\n    \n    autoencoder.summary()\n    \n    if n_epochs is None:\n        es = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n        n_epochs = 100\n    else:\n        es = []\n    \n    autoencoder.fit(\n            X_train_sc,\n            X_train_sc,\n            epochs = n_epochs,\n            validation_data=(X_test_sc, X_test_sc), \n            callbacks=[es]\n             )\n    \n    if n_epochs > 1:\n        histo = pd.DataFrame(autoencoder.history.history)\n        for metric in ['loss', 'val_loss']:\n            plt.title(metric)\n            histo[metric].plot()\n            plt.show()\n        \n    encoded_2dim = encoder.predict(X_train_sc)\n    encoded_2dim = pd.DataFrame(encoded_2dim)\n    encoded_2dim['y'] = y_train.values\n\n    plt.figure(figsize = (12, 8))\n    sns.scatterplot(data = encoded_2dim, x = 0, y = 1, hue = 'y', palette = 'magma')\n    plt.show()\n    \n    return encoder","073dec62":"encoder = dim_red_analysis(0)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\n","5a8b02ba":"encoder = dim_red_analysis(1)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","0a207adf":"encoder = dim_red_analysis(2)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","bb5c2b84":"encoder = dim_red_analysis(5)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndel encoder, enc_train, enc_test, lgbr_enc, pred","227a5516":"encoder = dim_red_analysis()\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","d23d8f76":"dtc = DecisionTreeClassifier(random_state = 101)\ndtc.fit(X_train, y_train)\npred = dtc.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","4dd48f64":"rfc = RandomForestClassifier(random_state = 101, n_jobs = -1)\nrfc.fit(X_train, y_train)\npred = rfc.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':rfc.feature_importances_}).sort_values('Importance', ascending=False).head(10))","2cfafef8":"gbr = GradientBoostingClassifier(random_state = 101)\ngbr.fit(X_train, y_train)\npred = gbr.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':gbr.feature_importances_}).sort_values('Importance', ascending=False).head(10))","3063f26c":"lgbr = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr.fit(X_train.values, y_train)\npred = lgbr.predict(X_test.values)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':lgbr.feature_importances_}).sort_values('Importance', ascending=False).head(10))","f387a518":"xgbr = XGBClassifier(random_state = 1, n_jobs=- 1)\nxgbr.fit(X_train, y_train)\npred = xgbr.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':xgbr.feature_importances_}).sort_values('Importance', ascending=False).head(10))","173987e9":"def ANN_model_classification(model, X_train_sc, y_train, X_test_sc, y_test):\n    \n    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    \n    model.fit(\n        x = X_train_sc,\n        y = y_train,\n        epochs = 100,\n        validation_data=(X_test_sc, y_test), \n        batch_size = 128,\n        callbacks=[es]\n             )\n\n\n    histo = pd.DataFrame(model.history.history)\n    \n    for metric in ['loss', 'val_loss', 'accuracy', 'val_accuracy']:\n        plt.title(metric)\n        histo[metric].plot()\n        plt.show()\n    \n    pred_test_values = model.predict_classes(X_test_sc)\n\n    print('test')\n    print(classification_report(y_test,pred_test_values))\n    print(confusion_matrix(y_test,pred_test_values))\n    \n    return model","d4b4ca6a":"seed(101)\nset_seed(101)\n\nann_model1 = Sequential()\n\nann_model1.add(Dense(units=128, activation = 'relu'))\nann_model1.add(Dropout(0.3))\nann_model1.add(Dense(units=1,activation='sigmoid'))\nann_model1 = ANN_model_classification(ann_model1, X_train_sc, y_train, X_test_sc, y_test)","a68ab431":"seed(101)\nset_seed(101)\n\nann_model2 = Sequential()\n\nann_model2.add(Dense(units=128, activation = 'relu'))\nann_model2.add(Dropout(0.3))\nann_model2.add(Dense(units=16, activation = 'relu'))\nann_model2.add(Dense(units=1,activation='sigmoid'))\nann_model2 = ANN_model_classification(ann_model2, X_train_sc, y_train, X_test_sc, y_test)","6d2dd59c":"val_data_for_model = columns_standardization(data_preparation(validation_df))\nval_data_for_model","fb01aec4":"val_data_for_model[val_data_for_model.isnull().any(axis=1)]","3db11b6d":"X_validation = val_data_for_model.drop('MIS_Status', axis = 1)\ny_validation = val_data_for_model['MIS_Status']","4f1b7f07":"pred = xgbr.predict(X_validation)\nprint(classification_report(y_validation, pred))\nprint(confusion_matrix(y_validation, pred))","148be963":"accuracy_score(y_validation,pred)","e6508e2e":"# EDA","d5ac129b":"N\\A values","f78352bf":"# Dimensionality reduction using PCA","a79a2191":"Looks like XGB model performs better then other.","ad1b21fc":"Model showed quite good result on validation data","f9e94d15":"# Validation","309c7abc":"# Data preparation","2b05dc44":"No null value left","88fb9fcb":"# Dimensionality reduction using Singular Value Decomposition","81fa5886":"# Splits","3c52f7d6":"# Dimensionality reduction using Linear Discriminant Analysis","5abfa935":"# Dimensionality reduction using autoencoder","ec72867e":"# Dimensionality reduction using Non-Negative Matrix Factorization (NMF)","020ea246":"I think it will be interesting to predict MIS_Status value (Loan status charged off = CHGOFF, Paid in full = PIF)","9734f290":"Looks like this column is better to ignore at all","f57e594b":"# Modelling without DR"}}