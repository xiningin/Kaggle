{"cell_type":{"816db217":"code","88ecadd7":"code","4a496224":"code","2d51db83":"code","25c32990":"code","09f10bf6":"code","25b50087":"code","729ecd6b":"code","a56d286e":"code","81f691ca":"code","d0d0826a":"code","3c8067bc":"code","83b1899e":"code","1f4fc7ca":"code","4035cd91":"code","50098ba1":"code","27e3bbe5":"code","d394d197":"code","adf068c8":"code","401315df":"code","b00c8c12":"code","c1775f3c":"code","d132d44d":"code","755c4568":"code","b082d4d7":"code","db0476c5":"code","dd95ae62":"code","c100e2b5":"code","150e3b99":"code","74ecd21a":"code","8bf3ffb0":"code","a53a3469":"code","987c49ba":"code","34d5ac09":"code","9493c145":"code","fe155cec":"code","042a5a19":"code","c14a363e":"code","ce250db4":"code","bab801f6":"markdown","bf1c2b74":"markdown","e3312235":"markdown","9a979ed9":"markdown","5d3526e4":"markdown","2b54b41c":"markdown","fb8da291":"markdown","733fd6ac":"markdown","387e3029":"markdown","d5ac217a":"markdown","d7528747":"markdown","26cb3c91":"markdown","6fd36eb1":"markdown","916441ba":"markdown","2b6227a3":"markdown","7301fa19":"markdown","22809290":"markdown","271b2a37":"markdown","d0a4a753":"markdown","720120ec":"markdown","edcbdc3f":"markdown"},"source":{"816db217":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#from sklearn.neighbors import DistanceMetric\nfrom math import radians\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score","88ecadd7":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\naus_town_gps = pd.read_csv(\"..\/input\/aus-town-gps\/aus_town_gps.csv\",sep=\",\")\nclimatsaus = pd.read_csv(\"..\/input\/climatsaus-v2\/climatsAUS_v2.csv\",sep=\";\")","4a496224":"df.info()","2d51db83":"# Pour simplifier, on regroupe les climats en 4 cat\u00e9gories : chaud_humide, temp\u00e9r\u00e9_froid, sec et m\u00e9diterran\u00e9en. On pourra ainsi faire des visualisations plus facilement.\n\nclimats_type = {'Am':'chaud_humide',\n                'Aw':'chaud_humide',\n                'Cfa':'chaud_humide',\n                'Cfb':'temp\u00e9r\u00e9_froid', \n                'Cfc':'temp\u00e9r\u00e9_froid', \n                'BSh':'sec',\n                'BSk':'sec',\n                'Bsk':'sec', \n                'Bwh':'sec',\n                'Csa':'m\u00e9diterran\u00e9en',\n                'Csb':'m\u00e9diterran\u00e9en'              \n               }\n\nclimatsaus['Clim_type']=climatsaus['Climat_Koppen'].map(climats_type)","25c32990":"#Fusion des dataframes\ndf = pd.merge(df, aus_town_gps, how='left', left_on=\"Location\",right_on=\"Location\")\ndf = pd.merge(df, climatsaus, how='left', left_on=\"Location\",right_on=\"Location\")\ndf.head(10)","09f10bf6":"#cr\u00e9ation de quelques variables de date et conversion de raintoday et raintomorrow en num\u00e9riques\ndf['RainToday_Num'] = (df['RainToday'] ==  'Yes')*1\ndf['RainTomorrow_Num'] = (df['RainTomorrow'] ==  'Yes')*1\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Mois'] = df['Date'].dt.month\ndf['Trimestre'] = df['Date'].dt.quarter\ndf['Annee'] = df['Date'].dt.year","25b50087":"#cr\u00e9ation d'un dictionnaire associant la direction du vent \u00e0 l'angle correspondant (en degr\u00e9s) sur le cercle trigonom\u00e9trique (ie. E=0\u00b0 et rotation dans le sens direct)\nangles = {'E':0, \n          'ENE':22.5, \n          'NE':45, \n          'NNE':67.5, \n          'N':90, \n          'NNW':112.5, \n          'NW':135, \n          'WNW':157.5, \n          'W':180, \n          'WSW':202.5, \n          'SW':225, \n          'SSW':247.5, \n          'S':270, \n          'SSE':292.5, \n          'SE':315, \n          'ESE':337.5}\n\n#ajout des variables indiquant l'angle du vent au DF\ndf['WindGust_Ang']=df['WindGustDir'].map(angles)\ndf['Wind9am_Ang'] = df['WindDir9am'].map(angles)\ndf['Wind3pm_Ang'] = df['WindDir3pm'].map(angles)\n\n#ajout de variables correspondant au cosinos de l'angle (abscisse des coordonn\u00e9es trigo). Un cosinus n\u00e9gatif correspond \u00e0 un vent d'ouest, un cosinus positif \u00e0 un vent d'est.\ndf['WindGust_cos'] = np.cos(np.radians(df['WindGust_Ang']))\ndf['Wind9am_cos'] = np.cos(np.radians(df['Wind9am_Ang']))\ndf['Wind3pm_cos'] = np.cos(np.radians(df['Wind3pm_Ang']))\n\n#ajout de variables correspondant au sinus de l'angle (ordonn\u00e9e des coordonn\u00e9es trigo). Un sinus n\u00e9gatif correspond \u00e0 un vent de sud, un sinus positif \u00e0 un vent de nord.\ndf['WindGust_sin'] = np.sin(np.radians(df['WindGust_Ang']))\ndf['Wind9am_sin'] = np.sin(np.radians(df['Wind9am_Ang']))\ndf['Wind3pm_sin'] = np.sin(np.radians(df['Wind3pm_Ang']))","729ecd6b":"df.info()","a56d286e":"#D\u00e9compte des valeurs manquantes\npercent_na = df.isna().sum() * 100 \/ len(df)\npercent_na = percent_na.sort_values(ascending=False)\n\nplt.figure(figsize=(16,4))\npercent_na[percent_na > 0].plot(kind='bar')\nplt.title(\"Part de valeurs manquantes par crit\u00e8re\");","81f691ca":"df20 = df.dropna()\ndf20.info()","d0d0826a":"df21 = df20.drop(['Date', 'Location','WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow', 'Trimestre', 'Annee', 'WindGust_Ang', 'Wind9am_Ang', 'Wind3pm_Ang', 'Climat_Koppen'], axis=1)\ndf21.info()","3c8067bc":"df21.Clim_type = LabelEncoder().fit_transform(df21.Clim_type)","83b1899e":"df21.info()","1f4fc7ca":"y = df21.RainTomorrow_Num\nX = df21.drop('RainTomorrow_Num', axis=1)","4035cd91":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","50098ba1":"std_scal = StandardScaler()\nX_train_std = std_scal.fit_transform(X_train)\nX_test_std = std_scal.transform(X_test)","27e3bbe5":"lr1 = LogisticRegression(max_iter=1000)","d394d197":"lr1.fit(X_train_std, y_train)","adf068c8":"y_pred_train = lr1.predict(X_train_std)\ny_pred_test = lr1.predict(X_test_std)","401315df":"print(\"accuracy :\", lr1.score(X_train_std, y_train))","b00c8c12":"print(\"performances sur train : \\n\", classification_report(y_train, y_pred_train))","c1775f3c":"print(\"Matrice de confusion lr1 - train\")\npd.crosstab(y_train, y_pred_train, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'])","d132d44d":"print(\"Matrice de confusion lr1 - train, Rappel\")\npd.crosstab(y_train, y_pred_train, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'], normalize='index')","755c4568":"print(\"Matrice de confusion lr1 - train, Precision\")\npd.crosstab(y_train, y_pred_train, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'], normalize='columns')","b082d4d7":"print(\"performances sur test : \\n\", classification_report(y_test, y_pred_test))","db0476c5":"print(\"Matrice de confusion lr1 - test\")\npd.crosstab(y_test, y_pred_test, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'])","dd95ae62":"print(\"Matrice de confusion lr1 - test, Rappel\")\npd.crosstab(y_test, y_pred_test, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'], normalize='index')","c100e2b5":"print(\"Matrice de confusion lr1 - test, Precision\")\npd.crosstab(y_test, y_pred_test, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'], normalize='columns')","150e3b99":"from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.metrics import classification_report_imbalanced, geometric_mean_score ","74ecd21a":"rus1 = RandomUnderSampler() ","8bf3ffb0":"X_ru, y_ru = rus1.fit_resample(X_train_std, y_train)","a53a3469":"lr2 = LogisticRegression(max_iter=1000)","987c49ba":"lr2.fit(X_ru, y_ru)","34d5ac09":"y_pred_ru = lr2.predict(X_test_std)","9493c145":"print(\"accuracy :\", lr2.score(X_train_std, y_train))","fe155cec":"print(\"performances sur test : \\n\", classification_report_imbalanced(y_test, y_pred_ru))","042a5a19":"print(\"Matrice de confusion lr2 - test\")\npd.crosstab(y_test, y_pred_ru, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'])","c14a363e":"print(\"Matrice de confusion lr2 - test, Rappel\")\npd.crosstab(y_test, y_pred_ru, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'], normalize='index')","ce250db4":"print(\"Matrice de confusion lr2 - test, Precision\")\npd.crosstab(y_test, y_pred_ru, rownames=['R\u00e9alit\u00e9'], colnames=['Pr\u00e9dictions'], normalize='columns')","bab801f6":"- Liste des variables avec des valeurs manquantes tri\u00e9s par ordre d\u00e9croissant de valeurs manquantes : 4 variables avec une forte proportion de valeurs manquantes (Sunshine\/Evaporation\/Cloud3pm\/Cloud9am). A voir comment on les traite.\n- Les autres variables avec moins de 10% de valeurs maquantes peuvent \u00eatre trait\u00e9s avec la m\u00e9thode interpolate","bf1c2b74":"## Num\u00e9risation des vents","e3312235":"### Sous-\u00e9chantillonnage de la classe 0","9a979ed9":"### Import des fonctions imblearn","5d3526e4":"## Cr\u00e9ation des nouvelles variables","2b54b41c":"### Entrainement d'un mod\u00e8le simple de r\u00e9gression logistique","fb8da291":"### Score d'accuracy","733fd6ac":"### Evaluation du mod\u00e8le sur train","387e3029":"### Suppression des variables superflues (climat : conservation des grandes cat\u00e9gories)","d5ac217a":"## Import des fichiers de travail","d7528747":"# 1e mod\u00e8le simple (r\u00e9gression logistique) sur les observations sans NA","26cb3c91":"Conclusion : perte de precision pour un gain de rappel.\nComme on pouvait s'y attendre, la classe 1 est mieux d\u00e9tect\u00e9e (moins de FN), mais le mod\u00e8le est moins fiable (plus de FP et precision plus faible).","6fd36eb1":"### Evaluation du mod\u00e8le sur test","916441ba":"### Suppression des observations avec NA","2b6227a3":"### Evaluation du mod\u00e8le","7301fa19":"**Chargements des jeux de donn\u00e9es :**\n - df : donn\u00e9es m\u00e9t\u00e9o en australie sur 10 ans\n - aus_town_gps : localisation des stations m\u00e9t\u00e9o (x,y) => ce jeu de donn\u00e9es va nous permettre de repr\u00e9senter les indicateurs sur une carte et de calculer des distances entre stations m\u00e9t\u00e9o\n - climatsaus : climat des stations m\u00e9teo ","22809290":"### Pr\u00e9paration des jeux de donn\u00e9es d'entrainement et de test (seuil = 20%)","271b2a37":"# Visualisation des NA","d0a4a753":"Conclusions :\n\nLe mod\u00e8le est peu performant sur la classe minoritaire. La classe 1 est mal d\u00e9tect\u00e9e mais lorsqu'elle l'est, le mod\u00e8le est fiable (peu de FP).\n\nPeu de diff\u00e9rence de performances entre train et test -> pas d'effet de surapprentissage (?)\n\nHypoth\u00e8se :\nLe jeu de donn\u00e9es est trop d\u00e9s\u00e9quilibr\u00e9 -> on peut proc\u00e9der \u00e0 un r\u00e9\u00e9chantillonage.","720120ec":"### Num\u00e9risation des climats","edcbdc3f":"### Entrainement du mod\u00e8le de r\u00e9gression logistique sur le jeu de donn\u00e9es r\u00e9\u00e9chantillonn\u00e9"}}