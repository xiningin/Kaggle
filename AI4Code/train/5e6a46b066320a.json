{"cell_type":{"de089a01":"code","4f2fd38d":"code","d5a338df":"code","5ace5b0f":"code","731ed4f2":"code","a8f1c3c7":"code","2322449f":"code","608b2012":"code","e074db0d":"code","d7fb226c":"code","9e1a3430":"code","81a6f32b":"code","4b31b4e1":"code","1476f67a":"code","e856913a":"code","7b1df7d7":"code","c2deed5b":"code","688a759b":"code","6ec85a2a":"code","a48750ba":"code","52b94d9f":"code","573f790d":"code","59f7e6f5":"code","b4919130":"code","25d91da4":"code","0e60af89":"code","853f337f":"code","86bf1044":"code","e21dd04f":"code","09c3ef2d":"code","edc87048":"code","a561a3d8":"code","4feadc6b":"code","197c518c":"code","931bd166":"code","b66ad6ee":"code","403cb6db":"code","1c9350dd":"code","0ed64115":"markdown","7fbd9306":"markdown","81233599":"markdown","4664f62c":"markdown","a911219d":"markdown","18ec39bf":"markdown"},"source":{"de089a01":"#loading the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')","4f2fd38d":"#loading the data\ndata = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","d5a338df":"data.info()","5ace5b0f":"pd.set_option(\"display.float\",\"{:.2f}\".format)\ndata.describe()","731ed4f2":"data.isnull().sum().sum()","a8f1c3c7":"data.columns","2322449f":"Labels = ['Normal', 'Fraud']\n\ncount_classes = pd.value_counts(data['Class'], sort=True)\ncount_classes.plot(kind=\"bar\", rot=0)\nplt.title(\"Transaction class distribution\")\nplt.xticks(range(2), Labels)\nplt.xlabel(\"labels\")\nplt.ylabel(\"frequency\")","608b2012":"data.Class.value_counts()","e074db0d":"fraud = data[data['Class']==1]\nnormal = data[data['Class']==0]","d7fb226c":"print(\"shape of fraud transaction:: {}\".format(fraud.shape))\nprint(\"shape of normal transaction:: {}\".format(normal.shape))","9e1a3430":"\npd.concat([fraud.Amount.describe(), normal.Amount.describe()], axis=1)","81a6f32b":"\npd.concat([fraud.Time.describe(), normal.Time.describe()], axis=1)","4b31b4e1":"\nplt.figure(figsize=(10,8))\n\nplt.subplot(2,2,1)\nplt.title(\"Time Distribution(seconds)\")\nsns.distplot(data['Time'], color=\"red\")\n\nplt.subplot(2,2,2)\nplt.title(\"Distribution of Amount\")\nsns.distplot(data['Amount'], color=\"green\")","1476f67a":"\nplt.figure(figsize=(10,8))\n\nplt.subplot(2,2,1)\ndata[data.Class==1].Time.hist(bins=35, color = 'purple', alpha=0.5, label = \"Fraudulant Transaction\")\nplt.legend()\n\nplt.subplot(2,2,2)\ndata[data.Class==0].Time.hist(bins=35, color = 'indigo', alpha=0.5, label = \"Non Fraudulant Transaction\")\nplt.legend()","e856913a":"data.hist(figsize=(20,20))","7b1df7d7":"\nplt.figure(figsize=(10,8))\nsns.heatmap(data.corr(), cmap=\"seismic\")\nplt.show()","c2deed5b":"#preprocessing the data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = data.drop('Class', axis=1)\ny = data.Class\n\nX_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train,X_validate, y_train, y_validate = train_test_split(X_train_v, y_train_v, test_size=0.2, random_state=42)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_validate_scaled = scaler.transform(X_validate)","688a759b":"preprocessed_dict = {}\npreprocessed_dict['X_train_scaled'] = X_train_scaled\npreprocessed_dict['X_test_scaled'] = X_test_scaled\npreprocessed_dict['X_validate_scaled'] = X_validate_scaled\npreprocessed_dict['y_train'] = y_train\npreprocessed_dict['y_test'] = y_test\npreprocessed_dict['y_validate'] = y_validate","6ec85a2a":"# from sklearn.externals import joblib\nimport pickle\npickle.dump(preprocessed_dict, open(\"preprocessed_data.pkl1\", \"wb\"))","a48750ba":"#loading the data\npreprocessed_data = pickle.load(open(\"preprocessed_data.pkl1\", \"rb\"))","52b94d9f":"X_train_scaled = preprocessed_data['X_train_scaled']\nX_test_scaled = preprocessed_data['X_test_scaled']\nX_validate_scaled = preprocessed_data['X_validate_scaled']\ny_train = preprocessed_data['y_train']\ny_test = preprocessed_data['y_test']\ny_validate = preprocessed_data['y_validate']","573f790d":"import numpy as np\nimport matplotlib.pyplot as plt\n\nX_train_d = np.expand_dims(X_train_scaled, -1)\nX_test_d = np.expand_dims(X_test_scaled, -1)\nX_validate_d = np.expand_dims(X_validate_scaled, -1)","59f7e6f5":"print(\"TRAINING DATA: X_train_d: {}, y_train: {}\".format(X_train_d.shape, y_train.shape))\nprint(\"-\"*60)\nprint(\"VALIDATION DATA: X_validate_d: {}, y_validate: {}\".format(X_validate_d.shape, y_validate.shape))\nprint(\"-\"*60)\nprint(\"TESTING DATA: X_test_d: {}, y_test: {}\".format(X_test_d.shape, y_test.shape))","b4919130":"from tensorflow import keras","25d91da4":"model = keras.models.Sequential([\n        keras.layers.Conv1D(32, 2, activation='relu', input_shape = X_train_d[0].shape),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.1),\n    \n        keras.layers.Conv1D(64, 2, activation=\"relu\"),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.2),\n    \n        keras.layers.Conv1D(128, 2, activation=\"relu\"),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.3),\n    \n        keras.layers.Flatten(),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(512, activation=\"relu\"),\n        keras.layers.Dropout(0.5),\n        \n        keras.layers.Dense(1, activation=\"sigmoid\")\n])","0e60af89":"model.summary()","853f337f":"model.compile(optimizer=keras.optimizers.Adam(0.0001), loss = \"binary_crossentropy\", metrics=['accuracy'])","86bf1044":"fit1 = model.fit(X_train_d, y_train, validation_data=(X_validate_d, y_validate), batch_size=500, epochs=20)","e21dd04f":"model.evaluate(X_validate_d, y_validate)","09c3ef2d":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(fit1.history['loss'], label='Loss')\nplt.plot(fit1.history['val_loss'], label='val_Loss')\nplt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(fit1.history['accuracy'], label='auc_1')\nplt.plot(fit1.history['val_accuracy'], label='val_acc_1')\nplt.legend()","edc87048":"model.save('CNN_model1.h5')","a561a3d8":"model1 = keras.models.Sequential([\n        keras.layers.Conv1D(32, 2, activation='relu', input_shape = X_train_d[0].shape),\n        keras.layers.BatchNormalization(),\n        keras.layers.MaxPool1D(2),\n        keras.layers.Dropout(0.1),\n    \n        keras.layers.Conv1D(64, 2, activation=\"relu\"),\n        keras.layers.BatchNormalization(),\n        keras.layers.MaxPool1D(2),\n        keras.layers.Dropout(0.2),\n    \n        keras.layers.Conv1D(128, 2, activation=\"relu\"),\n        keras.layers.BatchNormalization(),\n        keras.layers.MaxPool1D(2),\n        keras.layers.Dropout(0.3),\n    \n        keras.layers.Flatten(),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(512, activation=\"relu\"),\n        keras.layers.Dropout(0.5),\n        \n        keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nr1 = model1.fit(X_train_d, y_train, \n              validation_data=(X_validate_d, y_validate),\n              batch_size=50, \n              epochs=20, \n             )","4feadc6b":"model1.evaluate(X_validate_d, y_validate)","197c518c":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r1.history['loss'], label='Loss')\nplt.plot(r1.history['val_loss'], label='val_Loss')\nplt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(r1.history['accuracy'], label='auc_1')\nplt.plot(r1.history['val_accuracy'], label='val_acc_1')\nplt.legend()","931bd166":"model1.save('CNN_model2.h5')","b66ad6ee":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score","403cb6db":"y_predict1 = model.predict_classes(X_test_d)\ny_predict2 = model.predict_classes(X_test_d)","1c9350dd":"print(\"for model 1\")\nprint(\"accuracy score:: {}\".format(accuracy_score(y_test, y_predict1)))\nprint(\"-\"*50)\nprint(\"confusion matrix\")\nprint(confusion_matrix(y_test, y_predict1))\nprint(\"*\"*60)\nprint(\"for model 2\")\nprint(\"accuracy score:: {}\".format(accuracy_score(y_test, y_predict2)))\nprint(\"-\"*50)\nprint(\"confusion matrix\")\nprint(confusion_matrix(y_test, y_predict2))\n","0ed64115":"\n# Exploratory Data Analysis","7fbd9306":"Saving the model","81233599":"***We can see that model1 and model2 both performed almost same. It will be wise to save the data and model in valuable steps so that the data can be reused. Please give an upvote and comment if you find this useful. And point me for any mistakes.***","4664f62c":"Adding Maxpool","a911219d":"# Model Creation","18ec39bf":"# OK, Now making a dictionary to save all the datas"}}