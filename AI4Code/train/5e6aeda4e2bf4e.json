{"cell_type":{"f143ce3d":"code","e4a3069f":"code","4ab9bc23":"code","d5592f5e":"code","cc2ec7bb":"code","dddd91f4":"code","334fccd7":"code","c0aa43e5":"code","9e238cfd":"code","104232e8":"code","5e32aebf":"code","89bbadd4":"code","c76c1a50":"code","6072adf7":"code","167e8dad":"code","d9200d21":"code","99b22f48":"code","d509b8a6":"code","e1c2a242":"code","4598225d":"code","cf7b8dce":"code","7f4d6171":"markdown","b120c6fb":"markdown","2bf33c60":"markdown","558574c7":"markdown","7cf166da":"markdown","00964198":"markdown","0811b4a4":"markdown","b3b539de":"markdown","309eadc4":"markdown","0a00a994":"markdown","b5d9a6c5":"markdown","af19e6fb":"markdown","646ce0cb":"markdown","df3b4aab":"markdown","dc1fd74b":"markdown","2dd724a2":"markdown","aa6593f5":"markdown","4a75b5b1":"markdown","6b78f942":"markdown","3f661dfc":"markdown","eed8a38c":"markdown","2273874d":"markdown","92e954a0":"markdown","4e393c1e":"markdown","04aadb64":"markdown","402ccdbc":"markdown","9ad23862":"markdown"},"source":{"f143ce3d":"!pip install MiniSom","e4a3069f":"from IPython.display import Image\n\nfrom minisom import MiniSom\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nfrom matplotlib import colors\nimport random\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport pandas as pd\n\n%matplotlib inline\n%load_ext autoreload","4ab9bc23":"# Helper Code\ndef draw_som():\n    fig, ax = plt.subplots(figsize=(6, 6))\n    w = som.get_weights().T\n    w = np.moveaxis(w, 0, -1)\n    ax.imshow(w \/ 256, origin=\"lower\")\n    plt.grid(False)\n    \ndef draw_data(data):\n    fig, ax = plt.subplots(figsize=(25, 20))\n    ax.imshow([x \/ 256 for x in np.reshape([data], (-1, len(data), 3))], origin=\"lower\")\n    plt.axis('off')\n    \ndef draw_datapoint(datapoint):\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow([x \/ 256 for x in np.reshape([datapoint], (-1, len(datapoint), 3))], origin=\"lower\")\n    plt.axis('off')","d5592f5e":"test_data = [\n    [245,101,101], #red\n    [237,137,54], #orange\n    [236,201,75], #yellow\n    [72,187,120], #green\n    [56,178,172], #teal\n    [66,153,225], #blue\n    [102,126,234], #indigo\n    [159,122,234], #purple\n    [237,100,166], #pink\n    [0, 0, 0],\n    [255, 255, 255]\n]\n\ntest_data_df = pd.DataFrame({\"label\": [\"red\", \"orange\", \"yellow\", \"green\", \n                                     \"teal\", \"blue\", \"indigo\", \"purple\", \n                                     \"pink\", \"black\", \"white\"],\n                             \"rgb\": test_data,\n                             \"quant_error\": np.zeros(len(test_data))})","cc2ec7bb":"\ntrain_data = [\n    [254,215,215], [254,178,178], [252,129,129], [229, 62, 62], [197, 48, 48], [155, 44, 44], [116,42,42], # red\n    [254,235,200], [251,211,141], [246,173,85], [221,107,32], [192,86,33], [221,107,32], [123,52,30], #orange\n    [254,252,191], [250,240,137], [246,224,94], [214,158,46], [183,121,31], [151,90,22], [116,66,16], # yellow\n    [198,246,213], [154,230,180], [104,211,145], [56,161,105], [47,133,90], [39,103,73], [34,84,61], # green\n    [178,245,234], [129,230,217], [79,209,197], [49,151,149], [44,122,123], [40,94,97], [35,78,82], #teal\n    [190,227,248], [144,205,244], [99,179,237], [49,130,206], [43,108,176], [44,82,130], [42,67,101], # blue\n    [195,218,254], [163,191,250], [127,156,245], [90,103,216], [76,81,191], [67,65,144], [60,54,107], #indigo\n    [233,216,253], [214,188,250], [183,148,244], [128,90,213], [107,70,193], [85,60,154], [68,51,122], #purple\n    [254,215,226], [251,182,206], [246,135,179], [213,63,140], [184,50,128], [151,38,109], [112,36,89]  # pink\n]\n\n\ndraw_data(train_data)","dddd91f4":"som = MiniSom(10, 10, 3)\nsom.random_weights_init(data=train_data) # Uses random training data\ndraw_som()","334fccd7":"random_data_point = random.choice(train_data)\ndraw_datapoint([random_data_point])","c0aa43e5":"def draw_bmu_compare(datapoint, winner_coords):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n    axes[0].imshow([x \/ 256 for x in np.reshape([datapoint], (-1, len(datapoint), 3))], origin=\"lower\")\n    axes[0].axis('off')\n\n    w = som.get_weights().T\n    w = np.moveaxis(w, 0, -1)\n    axes[1].imshow(w \/ 256, origin=\"lower\")\n    plt.plot(winner_coords[0], winner_coords[1], 'x', markeredgecolor=\"black\", markersize=12, markeredgewidth=2)","9e238cfd":"winner_coords = som.winner(random_data_point)\ndraw_bmu_compare([random_data_point], winner_coords)\n\nprint(som.get_weights()[winner_coords[0]][winner_coords[1]])\nprint(random_data_point)","104232e8":"def draw_updated_som():\n    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n    w = som.get_weights().T\n    w = np.moveaxis(w, 0, -1)\n    axes[0].imshow(w \/ 256, origin=\"lower\")\n    axes[0].plot(winner_coords[0], winner_coords[1], 'x', markeredgecolor=\"black\", markersize=12, markeredgewidth=2)\n    \n    som.update(random_data_point, winner_coords, 0, 1)\n    w = som.get_weights().T\n    w = np.moveaxis(w, 0, -1)\n    axes[1].imshow(w \/ 256, origin=\"lower\")\n    axes[1].plot(winner_coords[0], winner_coords[1], 'x', markeredgecolor=\"black\", markersize=12, markeredgewidth=2)\n    \n    axes[0].grid(False)\n    axes[1].grid(False)","5e32aebf":"draw_updated_som()","89bbadd4":"def draw_som_and_distance_map():\n    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n    axes[0].imshow([x \/ 256 for x in som.get_weights()], origin=\"lower\")\n    axes[1].pcolor(som.distance_map().T, cmap='bone_r')\n    axes[0].grid(False)\n    axes[0].set_xticks([])\n    axes[0].set_yticks([])","c76c1a50":"som = MiniSom(10, 10, 3, learning_rate=0.001, sigma=1.2) # learning_rate und sigma von Hand gesetzt\nsom.random_weights_init(data=train_data)\n\nnum_iter = 100000 # 1, 10, 25, 50, 100, 500, 1000, 10000\nsom.train_batch(train_data, num_iter)\ndraw_som_and_distance_map()","6072adf7":"def plot_quant_errors():\n    sns.set(rc={'figure.figsize':(12, 6)})\n    quant_errors = [som.quantization_error([datapoint_test]) for datapoint_test in train_data]\n    quant_errors_compl = pd.DataFrame({\"observation\": range(len(quant_errors)), \"Quantization Error\": quant_errors})\n    sns.lineplot('observation', 'Quantization Error', data=quant_errors_compl, label=\"Train Data\")","167e8dad":"plot_quant_errors()\nprint(\"Topographic Error: %s\" % som.topographic_error(train_data))","d9200d21":"quant_errors = [som.quantization_error([datapoint]) for datapoint in train_data] # Berechnen des Quantization Errors f\u00fcr alle Trainingsdaten\nthreshold = 0.95 * np.amax(quant_errors) # Der Wert ist frei w\u00e4hlbar\n\nsns.distplot(quant_errors, axlabel='Quantization Error')\nplt.axvline(threshold, color='red')\nprint(\"The Threshold is %s\" % threshold)","99b22f48":"test_data = [\n    [245,101,101], #red\n    [237,137,54], #orange\n    [236,201,75], #yellow\n    [72,187,120], #green\n    [56,178,172], #teal\n    [66,153,225], #blue\n    [102,126,234], #indigo\n    [159,122,234], #purple\n    [237,100,166], #pink\n    [0, 0, 0], #black -> anomaly\n    [255, 255, 255] #white -> anomaly\n]\n\ndraw_data(test_data)","d509b8a6":"def draw_quant_errors():\n    quant_errors_test = [som.quantization_error([datapoint_test]) for datapoint_test in test_data]\n    test_data_df[\"quant_error\"] = quant_errors_test\n\n    sns.set(rc={'figure.figsize':(12, 6)})\n    palette = ['#%02x%02x%02x' % (rgb[0], rgb[1], rgb[2]) for rgb in test_data_df[\"rgb\"].to_list()]\n    ax = sns.scatterplot(x=range(11), y=test_data_df[\"quant_error\"], palette=palette,\n                         hue=test_data_df[\"label\"], legend=False, s=100, edgecolor='black')\n    ax.set_xticks(range(11))\n    ax.set_xticklabels(test_data_df['label'])\n    plt.axhline(threshold, color='r')","e1c2a242":"draw_quant_errors()","4598225d":"# random_test_data = random.choice(test_data)\nrandom_test_data = [10, 10, 10]\n\nwinner_coords = som.winner(random_test_data)\ndraw_bmu_compare([random_test_data], winner_coords)\nprint(\"Quantization Error: %s | Threshold: %s\" % (som.quantization_error([random_test_data]), threshold))","cf7b8dce":"# You can test the SOM with any random colors\n\nrandom_test_data = random.choice(test_data)\n# random_test_data = [25, 56, 29]\n\nwinner_coords = som.winner(random_test_data)\ndraw_bmu_compare([random_test_data], winner_coords)","7f4d6171":"![anomaly_detection.PNG](attachment:anomaly_detection.PNG)","b120c6fb":"*These Colors are the base training data which the SOM should cluster*","2bf33c60":"Image Source: Alexander von Birgelen, Davide Buratti, Jens Mager und Oliver Niggemann. *Self-Organizing Maps for Anomaly Localization and Predictive Maintenance in Cyber-Physical Production Systems*","558574c7":"### Berechnung der Nachbarschaft\n\nRadius in the Beginning: \n$ r_0 = max(somWidth, somHeight) * 0.5 $\n\nRadius during Training: \n$ r_t = r_0 - \\frac{epoch * (r_0 - 1)}{max\\_epochs - 1} $.\n\n&rarr; Radius decreases in the course of training, so that only the neurons are directly influenced by the BMU","7cf166da":"# Detect Anomalies in the color Example","00964198":"# Predictive Maintenance without Labels: Self-Organizing maps for Anomaly Detection","0811b4a4":"# Step 1: Select a random data point from the training data","b3b539de":"There are several different methods for anomaly detection. You can use both supervised and unsupervised machine learning Models. **The Advantage of using unsupervised Models is that you only need data from the new condition of the machine**. For supervised Methods, you need data from new machines and data from previous Failures. But sensor data of machine failure is rare and expensive, so unsupervised models are often cheaper and faster to start with.","309eadc4":"# Usage for Predictive Maintenance\n\n* SOM is trained on data from the new state  \n&rarr; Clusters are formed with the normal states of the machine\n\n* For new data points, the distance between the new data point and the BMU is examined  \n&rarr; If the distance is greater than a certain threshold, this data point is marked as an anomaly (cluster = normal state; large distance from normal state = anomaly)","0a00a994":"0. Initialize weights randomly\n1. Select a random data point from the training data\n2. Calculate the Best Matching Unit (BMU)\n3. Calculation of the BMU's neighborhood\n4. Adjusting the weights\n\n_Repeat  for N Iterations_","b5d9a6c5":"## Background","af19e6fb":"# Schritt 3: Calculating the neighborhood of the BMU","646ce0cb":"# Demo Projekt: Color Clustering\nThis project demonstrates how you could use SOMs for anomaly detection by clustering colors and detecting outliers in the colors","df3b4aab":"One Example of an unsupervised Model used for anomaly detection is the [Self-Organizing Map](https:\/\/en.wikipedia.org\/wiki\/Self-organizing_map).","dc1fd74b":"# Step 2: Find the BMU","2dd724a2":"The first colors should be close enough to the training data so that they are not considered anomalies. Black and white are so different that the should be recognized as anomalies.","aa6593f5":"![som_radius.PNG](attachment:som_radius.PNG)","4a75b5b1":"# Results after Training","6b78f942":"# Step 0: Init Weights","3f661dfc":"# Training","eed8a38c":"_Best Matching Unit_ is the neuron with the greatest similarity to the data point\n\nQuantization Error:  \n$ dist = \\sqrt{\\sum_{i=0}^{n}(V_i - W_i)^2} $ (euklidsche Distanz)\n\n&rarr; Die BMU ist das Neuron, f\u00fcr das *dist* minimal ist","2273874d":"*Notation: Each square is a neuron, the color indicates the weight vector*","92e954a0":"The Steps 1 - 4 are now repeated for *N* Iterations","4e393c1e":"# Quality features\n\n* Quantization Error: If the average quantization error is too high, this may indicate a too low resolution\n* Topographic Error: Indicates the proportion of data points where the two BMUs are not directly adjacent to each other. Can be between 0 and 1, should be as close to 0 as possible.\n\n*The quality characteristics are checked using the training data*","04aadb64":"# Step 4: Adjust Weights\n\n$ W(t + 1) = W(t) + \\Theta(t) * \\lambda(t) * (V(t) - W(t)) $\n\n$ \\Theta = exp(-\\frac{dist^{2}}{2 r^{2} (t)}) $  \n$ \\Theta $ depends on the time and the distance of the neuron to the BMU &arr; Ensures that the weights are adjusted particularly strongly for neurons close to the BMU\n\n$ \\lambda $ is the learning rate and can be freely chosen","402ccdbc":"# Classification of new Data","9ad23862":"Right: Distance map, shows the sum of the distance to the adjacent neurons"}}