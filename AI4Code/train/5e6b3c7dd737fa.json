{"cell_type":{"eacaaf76":"code","818efdfd":"code","244ab3c1":"code","020c1dd7":"code","cf5ebc4b":"code","19f1a419":"code","d6d320ca":"code","96bce752":"code","1ae56218":"code","afd3a348":"code","b6405060":"code","b869a6df":"code","8a089aa9":"code","b542a2bc":"code","4b3f1e7f":"code","ee5cdda0":"markdown","4d8e48c8":"markdown","fac62fce":"markdown","f3aa2b69":"markdown","3ea39b94":"markdown","e8914efa":"markdown"},"source":{"eacaaf76":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","818efdfd":"import pathlib\ndata_dir = \"..\/input\/flowers-recognition\/flowers\/flowers\"\n\ndata_dir = pathlib.Path(data_dir)\nimage_count = len(list(data_dir.glob('*\/*.jpg')))\nprint(image_count)","244ab3c1":"roses = list(data_dir.glob(\"rose\/*\"))\nPIL.Image.open(str(roses[0]))","020c1dd7":"tulips = list(data_dir.glob('tulip\/*'))\nPIL.Image.open(str(tulips[0]))","cf5ebc4b":"batch_size = 32\nimg_height = 180\nimg_width = 180\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=42,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=41,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","19f1a419":"class_names = train_ds.class_names\nprint(class_names)","d6d320ca":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","96bce752":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","1ae56218":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","afd3a348":"num_classes = 5\n\nmodel = Sequential([\n  data_augmentation,\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","b6405060":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","b869a6df":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=4, min_lr=0.0001)\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)","8a089aa9":"epochs = 50\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[reduce_lr,early_stop]\n)","b542a2bc":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","4b3f1e7f":"sunflower_url = \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/592px-Red_sunflower.jpg\"\nsunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n\nimg = keras.preprocessing.image.load_img(\n    sunflower_path, target_size=(img_height, img_width)\n)\nimg_array = keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0) # Create a batch\n\npredictions = model.predict(img_array)\nscore = tf.nn.softmax(predictions[0])\n\nprint(\n    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n    .format(class_names[np.argmax(score)], 100 * np.max(score))\n)","ee5cdda0":"# 2) Download and explore the dataset","4d8e48c8":"# 3) Visualize the data","fac62fce":"# 6) Test model","f3aa2b69":"# 4) Create the model","3ea39b94":"# 1) Import TensorFlow and other libraries","e8914efa":"# 5) Visualize training results"}}