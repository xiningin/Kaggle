{"cell_type":{"5ea81249":"code","28ed3e27":"code","b08dd927":"code","e8ffabad":"code","b466e8f9":"code","a772c27b":"code","bcf599c4":"code","8d3a9a4d":"code","00878992":"code","879e928b":"code","3c757931":"code","f87e1673":"code","17b8f07a":"code","5b0b1508":"code","98881c21":"code","cb8bc125":"code","6b81fedd":"code","3ac8cefe":"code","d56c0c61":"code","25c1b395":"code","b13c3622":"code","5d1bc41c":"code","f403d689":"code","c6b6355b":"code","45b60a17":"code","1856c72e":"code","cb7ca020":"code","0d3a070a":"code","d4ee80c8":"code","bb0adf7c":"code","c7eaf37a":"code","7ff48dd3":"code","3ec515fb":"code","207edd30":"code","7b28bb4f":"code","6d329029":"code","648ac17c":"code","6b777dac":"code","96b0bb07":"code","040ff72e":"code","6d95ea06":"code","2f0808ef":"code","f4f73a38":"code","a69aaed9":"code","fbfb2314":"code","c346677b":"code","d1deeac0":"code","d052bcc0":"code","c4311292":"code","418979a0":"code","1db6b9cb":"code","206d9f58":"code","392a7a65":"code","df0f0241":"code","6fe147f8":"code","4fc10ba6":"code","f6f26f86":"code","57d6478f":"code","4afcff82":"code","b58573bd":"code","38952c51":"code","3a20f9f6":"code","9589572d":"code","a0c4f26f":"code","4fc7033d":"code","222b5cff":"code","1628c3e9":"code","0e488dc5":"code","07efae8c":"code","4e29a83a":"code","859f47df":"code","4a2190ee":"code","547ebb1d":"code","d591f63d":"code","4fa497fb":"code","32f2b93b":"code","99bc8e36":"code","9b204801":"code","81a27b20":"code","f0deee9a":"code","cb18d451":"code","42a4df58":"code","c5da2ef3":"code","3f2a6fba":"code","9e618c2b":"code","7f3d6b7f":"code","c638346d":"code","6a3e6a26":"code","8ac8e225":"code","d5b6a94f":"code","63ec6df6":"code","c91d8d14":"code","8a3f7f5b":"code","ab0dd624":"code","1e3aa728":"code","9748ab7b":"code","2855c851":"code","1c61b377":"code","3e180fbf":"code","8fd4ae1e":"markdown","db1d12ff":"markdown","406854b9":"markdown","7df1bb7c":"markdown","d5ad81d7":"markdown","7c1989f6":"markdown","0cafc584":"markdown","fc42436c":"markdown","6c107477":"markdown","1d6c53da":"markdown","6372f1ec":"markdown","23e541cf":"markdown","5b2eec58":"markdown","76c8fb49":"markdown","5ccf6168":"markdown","90fc69ce":"markdown","8fbb51db":"markdown","bdacdc20":"markdown","6abec97d":"markdown","69ad0a61":"markdown","95016b13":"markdown","a3192c2c":"markdown","3848e834":"markdown","6a8a0324":"markdown","b0e289d6":"markdown","9dbf8615":"markdown"},"source":{"5ea81249":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28ed3e27":"# Read the train and test Data\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","b08dd927":"# print top 5 rows\nprint('Top 5 Rows')\ntrain.head()\n","e8ffabad":"#print top 5 row of test data\nprint('Top 5 Rows')\ntest.head()\n","b466e8f9":"# Train data set size\nprint(\"Train Data Set Size:\",train.shape)\nprint(\"Total no of data points in Train Data:\",train.shape[0])","a772c27b":"#print Test dataset size\nprint(\"Test Data Set Size:\",test.shape)\nprint(\"Total no of data points in Test Data:\",test.shape[0])","bcf599c4":"Train_Survived_Distribution=train[\"Survived\"].value_counts()\n","8d3a9a4d":"Train_Survived_Distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in train data')\nplt.grid()\nplt.show()\nsorted_yi = np.argsort(-Train_Survived_Distribution.values)\nprint(sorted_yi )\nprint(Train_Survived_Distribution.values )\nfor i in sorted_yi:\n    print('Number of data points in class', i, ':',Train_Survived_Distribution.values[i], '(', np.round((Train_Survived_Distribution.values[i]\/train.shape[0]*100), 3), '%)')\nprint('Observation:','Dataset is imbalanced Data')   ","00878992":"print('--'*10,\"Verify null values in Train Data set\",'--'*10) \nprint(train.isnull().sum())\nprint('--'*40)\nprint(' '*40)\nprint('--'*40)\nprint('--'*10,\"Verify null values in Test Data set\",'--'*10) \nprint(test.isnull().sum())\nprint('--'*40)\nprint('Observation:','Age, Cabin and Embarked features are  having Null values in Train Data')\nprint('Observation:','Age, Cabin and Fari features are  having Null values in Test Data')","879e928b":"def data_clean(data):\n    Total =data.isnull().sum().sort_values()\n    Percent=((data.isnull().sum()\/(data.isnull().count())*100)).sort_values().round(2)\n    d1=pd.concat([Total,Percent],axis=1,keys=['Total','Percent'])\n    d1=d1[d1['Percent']>0]\n    fig=plt.subplots(figsize=(8,5))\n    fig=sns.barplot(d1.index,d1.Percent)\n    plt.ylabel(\"Percentage of  Missing values \",fontsize=15)\n    plt.xlabel(\"Features\",fontsize=15)\n    plt.title(\"Percentage Of Missing values in  Data\",fontsize=15)\n    return d1\n    ","3c757931":"data_clean(train)","f87e1673":"data_clean(test)","17b8f07a":"# drop the Cabin variable in both datasets\ntrain.drop(['Cabin'],axis=1,inplace=True)\ntest.drop([\"Cabin\"],axis=1,inplace=True)","5b0b1508":"train.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\ntest.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)","98881c21":"train['Age'].fillna(train['Age'].mean(),inplace=True)\ntest['Age'].fillna(train['Age'].mean(),inplace=True)\ntest['Fare'].fillna(test['Fare'].mean(),inplace=True)\ntrain['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)","cb8bc125":"print('--'*10,\"Verify null values in Train Data set\",'--'*10) \nprint(train.isnull().sum())\nprint('--'*40)\nprint('--'*40)\nprint('--'*10,\"Verify null values in Test Data set\",'--'*10) \nprint(test.isnull().sum())\nprint('--'*40)\nprint('Observation:','No Null values in Train Data')\nprint('Observation:','No Null values in Test Data')","6b81fedd":"unique_sex = train['Sex'].value_counts()\nprint(\"There are\", unique_sex.shape[0] ,\"different categories of sex feature in the train data\",)\nprint(unique_sex)\nsns.countplot(x='Sex',data=train)\nplt.show()","3ac8cefe":"unique_sex = test['Sex'].value_counts()\nprint(\"There are\", unique_sex.shape[0] ,\"different categories of sex feature in the test data\",)\nprint(unique_sex)\nsns.countplot(x='Sex',data=test)\nplt.show()","d56c0c61":"train=pd.get_dummies(train,columns=['Sex'],prefix=['Sex'])","25c1b395":"(train.head())","b13c3622":"test = pd.get_dummies(test, columns = [\"Sex\"],prefix=[\"Sex\"])","5d1bc41c":"unique_embarked = test['Embarked'].value_counts()\nprint(\"There are\", unique_embarked.shape[0] ,\"different categories of Embarked feature in the test data\",)\nprint(unique_embarked)\nsns.countplot(x='Embarked',data=test)\nplt.show()","f403d689":"unique_embarked = train['Embarked'].value_counts()\nprint(\"There are\", unique_embarked.shape[0] ,\"different categories of Embarked feature in the train data\",)\nprint(unique_embarked)\nsns.countplot(x='Embarked',data=train)\nplt.show()","c6b6355b":"train=pd.get_dummies(train,columns=['Embarked'],prefix=['Embarked'])\ntest=pd.get_dummies(test,columns=['Embarked'],prefix=['Embarked'])","45b60a17":"train.head()","1856c72e":"test.head()","cb7ca020":"print(train['Age'].max())\nprint(train['Age'].min())","0d3a070a":"print(test['Age'].max())\nprint(test['Age'].min())","d4ee80c8":"train['Age']=pd.cut(train['Age'],bins=[0,12,20,40,100],labels=['Childen','Teenage','Adilt','Elder'])\ntest['Age']=pd.cut(test['Age'],bins=[0,12,20,40,100],labels=['Children','Teenage','Adult','Elder'])","bb0adf7c":"test.head()","c7eaf37a":"train.head()","7ff48dd3":"unique_age=train.Age.value_counts()\nprint(\"There are\", unique_age.shape[0] ,\"different categories of age feature in the train data\")\nprint(unique_age)\nsns.countplot(x='Age',data=train)\nplt.title('Bins of Age feature in Train data')\nplt.show()","3ec515fb":"unique_age=test.Age.value_counts()\nprint(\"There are\", unique_age.shape[0] ,\"different categories of age feature in the test data\")\nprint(unique_age)\nsns.countplot(x='Age',data=test)\nplt.title('Bin of Age feature in Test data')\nplt.show()","207edd30":"train=pd.get_dummies(train,columns=['Age'],prefix=['Age'])\ntest=pd.get_dummies(test,columns=['Age'],prefix=['Age'])","7b28bb4f":"train.head()","6d329029":"test.head()","648ac17c":"print(train['Fare'].min())\nprint(train['Fare'].max())","6b777dac":"print(test['Fare'].min())\nprint(test['Fare'].max())","96b0bb07":"train['Fare']=pd.cut(train['Fare'],bins=[0,14.45,31,60,513],labels=['Low Fare','median Fare','Average Fare','high Fare'])\ntest['Fare']=  pd.cut(test['Fare'],bins=[0,14.45,31,60,513],labels=['Low Fare','median Fare','Average Fare','high Fare'])","040ff72e":"(train.head())","6d95ea06":"unique_fare=train.Fare.value_counts()\nprint(\"There are\", unique_fare.shape[0] ,\"different categories of Fare feature in the train data\")\nprint(unique_fare)\nsns.countplot(x='Fare',data=train)\nplt.title('Bin of Fare feature in Train data')\nplt.show()","2f0808ef":"unique_Fare=test.Fare.value_counts()\nprint(\"There are\", unique_Fare.shape[0] ,\"different categories of Fare feature in the test data\")\nprint(unique_age)\nsns.countplot(x='Fare',data=test)\nplt.title('Bin of Fare feature in Test data')\nplt.show()","f4f73a38":"train=pd.get_dummies(train,columns=['Fare'],prefix=['Fare'])\ntest=pd.get_dummies(test,columns=['Fare'],prefix=['Fare'])","a69aaed9":"train.head()","fbfb2314":"train.head()","c346677b":"test.head(10)","d1deeac0":"data=train\nprint(\"Train Data Set Size:\",data.shape)\nprint(\"Total no of data points in Train Data:\",data.shape[0])","d052bcc0":"print(\"features:\",data.columns.values)","c4311292":"Y=data[\"Survived\"]\nX=data.drop(\"Survived\",axis=1)","418979a0":"print(Y)","1db6b9cb":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n#from sklearn.grid_search import GridSearchCV","206d9f58":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3)","392a7a65":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","df0f0241":"lr=LogisticRegression()\nlr.fit(X_train,Y_train)","6fe147f8":"turned_parameters=[{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]","4fc10ba6":"lrgscv=GridSearchCV(lr,turned_parameters,cv=10,scoring = 'accuracy')\n","f6f26f86":"lrgscv.fit(X_train,Y_train)\npredict_val1=lrgscv.predict(X_test)\npredict_val=lr.predict(X_test)","57d6478f":"lr_score=cross_val_score(lr,X,Y,cv=10,scoring='accuracy')\npredictlr=cross_val_predict(lr,X,Y,cv=10)","4afcff82":"\nprint('--------------The Accuracy of the model----------------------------')\nprint(\"Accuracy of Logistic Regression is :\",round(accuracy_score(Y_test,predict_val)*100,2),'%')\nprint(\"Cross validation score for Logistic Regression Accuracy is:\",round(lr_score.mean()*100,2),'%')\nprint(\"Grid Search CV score for Logistic Regression Accuracy is:\",round(accuracy_score(Y_test,predict_val1)*100,2),'%')","b58573bd":"sns.heatmap(confusion_matrix(Y,predictlr),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","38952c51":"print(lrgscv.best_estimator_)\n#print(lrgscv.score(X,Y))\nprint(lrgscv.best_score_)","3a20f9f6":"knn=KNeighborsClassifier()\nknn.fit(X_train,Y_train)\nknnpredict=knn.predict(X_test)","9589572d":"knn_score=cross_val_score(knn,X,Y,cv=10,scoring='accuracy')\npredictknn=cross_val_predict(knn,X,Y,cv=10)","a0c4f26f":"param_grid={'n_neighbors':[3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51],'p':[1,2],\n           'weights': ['uniform', 'distance']}\nmodel = GridSearchCV(knn, param_grid, scoring = 'accuracy', cv=10)\nmodel.fit(X_train, Y_train)\nknngrcv=model.predict(X_test)\nprint(model.best_estimator_)\nprint(model.score(X_test, Y_test))","4fc7033d":"print('--------------The Accuracy of the model----------------------------')\nprint(\"Accuracy of KNN is :\",round(accuracy_score(Y_test,knnpredict)*100,2),'%')\nprint(\"Cross validation score for KNN Accuracy is:\",round(knn_score.mean()*100,2),'%')\nprint(\"Grid Search CV score for KNN Accuracy is:\",round(accuracy_score(Y_test,knngrcv)*100,2),'%')\n","222b5cff":"sns.heatmap(confusion_matrix(Y,predictknn),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","1628c3e9":"svc=SVC()\nsvcclf=svc.fit(X_train,Y_train)\nsvcpredict=svcclf.predict(X_test)\nsvcpredict1=svcclf.predict(test)","0e488dc5":"svc_score=cross_val_score(svc,X,Y,cv=10,scoring='accuracy')\npredictsvc=cross_val_predict(svc,X,Y,cv=10)","07efae8c":"\"\"\"turn_perameters={'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid','linear']}\nsvmclf=GridSearchCV(SVC(),turn_perameters,cv=10)\nsvmclf.fit(X_train,Y_train)\nsvmpredict=svmclf.predict(X_test)\nprint(svmclf.best_estimator_)\nprint(svmclf.best_score_)\nprint(svmclf.score(X_test, Y_test))\"\"\"","4e29a83a":"sns.heatmap(confusion_matrix(Y,predictsvc),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","859f47df":"nb=GaussianNB()\nnb.fit(X_train,Y_train)\nnbclf=nb.predict(X_test)\n","4a2190ee":"nb_score=cross_val_score(nb,X,Y,scoring='accuracy',cv=10)\npredictnb=cross_val_predict(nb,X,Y,cv=10)\n#print(nb.best_score_)\n#print(nb.best_estimator_)","547ebb1d":"print('--------------------The Accuracy of the model--------------------------')\nprint(\"Accuracy of Naive Bayes is :\",round(accuracy_score(Y_test,nbclf)*100,2),'%')\nprint(\"Cross validation score for Naive Bayes is:\",round(nb_score.mean()*100,2),'%')\n#print(\"Best Cross validation score for SVM Accuracy is:\",round((nb.best_score_)*100,2),'%')","d591f63d":"sns.heatmap(confusion_matrix(Y,predictnb),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","4fa497fb":"dt=DecisionTreeClassifier()\ndt.fit(X_train,Y_train)\ndtpredict=dt.predict(X_test)","32f2b93b":"dt_score=cross_val_score(dt,X,Y,cv=10,scoring='accuracy')\npredictdt=cross_val_predict(dt,X,Y,cv=10)","99bc8e36":"print('--------------------The Accuracy of the model--------------------------')\nprint(\"Accuracy of Decision Tree is :\",round(accuracy_score(Y_test,dtpredict)*100,2),'%')\nprint(\"Cross validation score for Decision Tree is:\",round(dt_score.mean()*100,2),'%')\n","9b204801":"sns.heatmap(confusion_matrix(Y,predictdt),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","81a27b20":"rfclf=RandomForestClassifier()\nrfclf.fit(X_train,Y_train)\nrfpredict=rfclf.predict(X_test)","f0deee9a":"rf_score=cross_val_score(rfclf,X,Y,cv=10,scoring='accuracy')\npredictdt=cross_val_predict(rfclf,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,predictdt),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","cb18d451":"print('--------------------The Accuracy of the model--------------------------')\nprint(\"Accuracy of Random Forest  is :\",round(accuracy_score(Y_test,rfpredict)*100,2),'%')\nprint(\"Cross validation score for Random Forest is:\",round(rf_score.mean()*100,2),'%')\n","42a4df58":"xgb=XGBClassifier()\nxgb.fit(X_train,Y_train)\nxgbpridect=xgb.predict(X_test)","c5da2ef3":"xgb_score=cross_val_score(xgb,X,Y,cv=10,scoring='accuracy')\npredictxgb=cross_val_predict(xgb,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,predictxgb),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","3f2a6fba":"print('--------------------The Accuracy of the model--------------------------')\nprint(\"Accuracy of XGBoosting is :\",round(accuracy_score(Y_test,xgbpridect)*100,2),'%')\nprint(\"Cross validation score for XGBoosting is:\",round(xgb_score.mean()*100,2),'%')\n","9e618c2b":"adbclf=AdaBoostClassifier()\nadbclf.fit(X_train,Y_train)\nadbpredict=adbclf.predict(X_test)","7f3d6b7f":"adb_score=cross_val_score(adbclf,X,Y,cv=10,scoring='accuracy')\npredictadbclf=cross_val_predict(adbclf,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,predictadbclf),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","c638346d":"print('--------------------The Accuracy of the model--------------------------')\nprint(\"Accuracy of Adaboosting is :\",round(accuracy_score(Y_test,adbpredict)*100,2),'%')\nprint(\"Cross validation score for Adaboosting  is:\",round(adb_score.mean()*100,2),'%')\n","6a3e6a26":"gbclf=GradientBoostingClassifier()\ngbclf.fit(X_train,Y_train)\ngbclfpredict=gbclf.predict(X_test)\ngbclfpredict1=gbclf.predict(test)","8ac8e225":"gb_score=cross_val_score(gbclf,X,Y,cv=10,scoring='accuracy')\npredictadbclf=cross_val_predict(gbclf,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,predictadbclf),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")\nplt.show()","d5b6a94f":"print('--------------------The Accuracy of the model--------------------------')\nprint(\"Accuracy of Gradient Boosting is :\",round(accuracy_score(Y_test,gbclfpredict)*100,2),'%')\nprint(\"Cross validation score for Gradient Boosting is:\",round(gb_score.mean()*100,2),'%')\n","63ec6df6":"\"\"\"sclf=StackingClassifier(classifiers=[knn,rfclf,nb,svc,dt,xgb,adbclf,gbclf],meta_classifier=lr)\nfor clf, label in zip([knn,rfclf,nb,svc,dt,xgb,adbclf,gbclf,sclf],\n                      ['KNN',\n                       'Random Forest',\n                       'Naive Bayes',\n                       'SVM',\n                       'Decision Tree',\n                       'XGBoost',\n                       'Adaboosting',\n                       'GradientBoosting',\n                       'StackingClassifier']):\n    scores=cross_val_score(clf,X,Y,cv=10,scoring='accuracy')\n    print(\"Accuracy:%0.2f  [%s]\" %(scores.mean(),label))\n    \"\"\"","c91d8d14":"sclf=StackingClassifier(classifiers=[knn,rfclf,nb,svc,dt,xgb,adbclf,gbclf],meta_classifier=lr)\nsclf.fit(X_train,Y_train)\nsclfpredict=sclf.predict(X_test)\nstck_score=cross_val_score(sclf,X,Y,cv=10,scoring='accuracy')\npredictadbclf=cross_val_predict(sclf,X,Y,cv=10)\nprint('--------------------The Accuracy of the model--------------------------')\nprint(\"Accuracy of Stacking is :\",round(accuracy_score(Y_test,sclfpredict)*100,2),'%')\nprint(\"Cross validation score for Stacking is:\",round(stck_score.mean()*100,2),'%')\n\nsns.heatmap(confusion_matrix(Y,predictadbclf),annot=True,fmt='3.0F')\nplt.title(\"Confusion Matrix\")","8a3f7f5b":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'AdaBoostClassifier', \n              'Gradient Decent', 'XGBoosting', \n              'Decision Tree','Stacking Classifier'],\n    'Score': [svc_score.mean(), knn_score.mean(), lr_score.mean(), \n              rf_score.mean(), nb_score.mean(), adb_score.mean(), \n              gb_score.mean(), xgb_score.mean(), dt_score.mean(),stck_score.mean()]})\nmodels.sort_values(by='Score',ascending=False)","ab0dd624":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'AdaBoostClassifier', \n              'Gradient Decent', 'XGBoosting', \n              'Decision Tree','Stacking Classifier'],\n    'Accuracy': [accuracy_score(Y_test,svcpredict), accuracy_score(Y_test,knnpredict),accuracy_score(Y_test,predict_val), \n              accuracy_score(Y_test,rfpredict), accuracy_score(Y_test,nbclf), accuracy_score(Y_test,adbpredict), \n              accuracy_score(Y_test,gbclfpredict), accuracy_score(Y_test,xgbpridect),accuracy_score(Y_test,dtpredict),accuracy_score(Y_test,sclfpredict)]})\nmodels.sort_values(by='Accuracy',ascending=False)","1e3aa728":"pred=pd.DataFrame(gbclfpredict1)\nsubmsdf=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n","9748ab7b":"submsdf.head()","2855c851":"print(pred.shape)\nprint(submsdf.shape)","1c61b377":"datasets=pd.concat([submsdf['PassengerId'],pred],axis=1)\ndatasets.columns=['PassengerId','Survived']\ndatasets.to_csv('resultdf3.csv',index=False)\n","3e180fbf":"#d=pd.read_csv('C:\/\/Users\/\/user\/\/Desktop\/\/resultdf3.csv')\n#print(d.head())","8fd4ae1e":"  Decision Tree","db1d12ff":"Random Forest","406854b9":"Age feature has more than 17% missing values in both train and test data, so i have filling age fature with median","7df1bb7c":"# Spliting  Dataset","d5ad81d7":"Cabin feature has more than 77% of   missing values in both Train and Test Dataset. So I can Remove the cabin feature ","7c1989f6":"1. How to featurize this Sex feature?\n\nAns:Using Get_dummies(), we can Featurize this variable","0cafc584":"3. Age Feature \n\n   Age,What type of feature it is?\n   \n   ans: Age is  Continuous fature.\n   \nFor improve the accuracy purpose, Age variable values are divied into bins using cut method\n\n","fc42436c":"Logistic Regression","6c107477":"# Feature engineering and EDA","1d6c53da":"KNN","6372f1ec":"# Models","23e541cf":" SVM ","5b2eec58":"StackingClassifier","76c8fb49":" Naive Bayes","5ccf6168":"1. Sex Feature\n\nSex,What type of feature,it is?\n\nAns: Sex is Categorical variable","90fc69ce":"2. Embarked Feature\n\nEmbarked,What type of feature,it is?\n\nAns: Embarked is Categorical variable","8fbb51db":"# Data Analysis","bdacdc20":"2.1 How to featurize this Embarked feature?\n\nAns:Using Get_dummies(), we can Featurize this variable","6abec97d":"# Data preprocessing","69ad0a61":"# Load Dataset","95016b13":"4.1 How to featurize this Embarked feature?\n\nAns:Using Get_dummies(), we can Featurize this variable","a3192c2c":"3.1 How to featurize this Embarked feature?\n\nAns:Using Get_dummies(), we can Featurize this variable","3848e834":"I will remove the PassengerId ,Name and Ticket, since i will be useless for our data","6a8a0324":" Adaboosting","b0e289d6":"XGBoosting","9dbf8615":"4. Fare Feature \n\n   Fare,What type of feature it is?\n   \n   ans: Fare is  Continuous fature.\n   \nFor improve the accuracy purpose, Age variable values is divied into bins using cut method"}}