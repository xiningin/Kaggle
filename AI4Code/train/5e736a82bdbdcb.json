{"cell_type":{"06125299":"code","0f23959e":"code","2f0de432":"code","74140d3c":"code","5dfc7239":"code","b80b40fc":"code","ac98ba26":"code","dd8158da":"code","ee8492fb":"code","88339aa9":"code","4bc51e98":"code","617bb5b4":"code","20e2bb6a":"code","a45aa613":"code","2e68ada2":"code","8a9217c9":"code","da16b194":"code","b222be6c":"code","800c3c46":"code","d9e27c43":"code","5ad1006d":"code","097a1ea1":"code","eccf3eef":"code","80197fc1":"code","6400aae6":"code","16f7085f":"code","a5f57853":"code","e1f385f7":"code","df37af62":"code","96e8263d":"code","91194bff":"code","a8bfdc1c":"code","14947428":"code","86c2f43f":"code","1a5c3b14":"code","f7fee644":"code","57ecea4f":"code","9b432087":"code","3b46ccb0":"code","5b24338a":"code","7309a89f":"code","dbaa6c1a":"code","e1ddc2d2":"code","5ce2133b":"code","ff81e4dc":"code","42b6866c":"code","f44912f3":"code","c2945af4":"code","4b3ab9f0":"code","668c02b2":"code","3357b9b6":"code","0c353452":"code","cca1c289":"code","663bacc8":"code","aa272a72":"code","30127b2c":"code","5ea4ae69":"code","a0434428":"code","3ac2e920":"code","a4c3934c":"code","1b97cca7":"code","4e45b03c":"code","a7f04b1f":"code","d3820a87":"code","23b01769":"code","6922840c":"code","f24daef6":"code","03a62efd":"code","d2183abe":"code","3e22c351":"code","db27b5f6":"code","9b9d0a48":"code","7c7e0ee6":"code","d60e9446":"code","d8f5779d":"code","5a606c99":"code","2f7dc31d":"code","be396835":"code","753eaf9d":"code","de6ba1b2":"code","77fce5db":"code","b73da2ea":"code","59e62018":"code","14155db7":"code","f326fc30":"code","a2e8d997":"code","a8c7bfb6":"code","81b577ab":"code","c3202975":"code","81fcc054":"code","d164b452":"code","c912ab35":"code","52981e61":"code","8f791e64":"code","7b8f5a41":"code","8b203b46":"code","b030ef39":"code","b75b8c5f":"code","ba242204":"code","4a4b45a7":"code","c3d557dc":"code","70379a1d":"code","0301ccab":"code","61ab3afe":"code","f619461f":"code","3961f577":"code","97028638":"code","b4517230":"code","90f63065":"code","bf663322":"code","68b69e28":"code","39a16e20":"code","c7eeb2c3":"code","3bb2ce68":"code","f114ac65":"code","affb59b1":"code","443641a7":"code","ddcee83a":"code","7977d3eb":"code","e34358bc":"markdown","448bee1c":"markdown","86b53877":"markdown","f314988d":"markdown","82bf2c48":"markdown","35b56b45":"markdown","16797df5":"markdown","30f64f24":"markdown","8a0eb2a5":"markdown","f2b04419":"markdown","b3a2ceba":"markdown","0d3db7f2":"markdown","13eaefb0":"markdown","f05e3c57":"markdown","1bd9099a":"markdown","69a3a999":"markdown","f738828e":"markdown","0c7ec5a6":"markdown","dc1ad777":"markdown","8c1837f7":"markdown","06ca43bb":"markdown","4c780ea3":"markdown","40175320":"markdown","ff238910":"markdown","5c004a74":"markdown","231c25db":"markdown","5acf1f8f":"markdown","8fd514a1":"markdown","5035784c":"markdown","91ceac49":"markdown","5e8e8d1a":"markdown","3e2d2196":"markdown","e03113c9":"markdown","b6765eff":"markdown","cd9025f4":"markdown","971177b9":"markdown","79729bc6":"markdown","b1ebf030":"markdown","52cf538c":"markdown","9080c682":"markdown","f704360e":"markdown","cb7d00fe":"markdown","bafb8d33":"markdown","aea86400":"markdown","0dc213cc":"markdown","11daeac6":"markdown","1892a2fa":"markdown","88909673":"markdown","0476d98c":"markdown","aa261836":"markdown","f6624e11":"markdown","234fd036":"markdown","cc889a12":"markdown","261ba0e8":"markdown","1e7715e0":"markdown","e03bc2da":"markdown","5e6be775":"markdown","c4508a02":"markdown","ec0e7faa":"markdown","83576f44":"markdown","30577560":"markdown","bb95c085":"markdown","33bf9c46":"markdown","3cdbaa0f":"markdown","7578b6cd":"markdown","bbec9ef9":"markdown","55712a2a":"markdown","2e37bf96":"markdown"},"source":{"06125299":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f23959e":"train = pd.read_csv('..\/\/input\/itea-goal-prediction\/\/goal_train.csv', encoding='utf-8')\ntest = pd.read_csv('..\/\/input\/itea-goal-prediction\/\/goal_test.csv', encoding='utf-8')","2f0de432":"train.info()","74140d3c":"test.info()","5dfc7239":"len(train.columns)","b80b40fc":"train.head()","ac98ba26":"train.sample(10)","dd8158da":"y = train['is_goal']\ny","ee8492fb":"train = train.drop(columns='is_goal')\ntrain.head()","88339aa9":"train['part'] = 1\ntest['part'] = 2\ndf = train.append(test, ignore_index=True)\ndf.head()","4bc51e98":"df.sample(10)","617bb5b4":"df.info()","20e2bb6a":"df.describe()","a45aa613":"df.isna().any()[lambda x: x]","2e68ada2":"df['foot'] = df['foot'].fillna(df['foot'].mode()[0])","8a9217c9":"df['weight'] = df['weight'].fillna(df['weight'].mean())","da16b194":"df['passportArea'] = df['passportArea'].fillna(df['passportArea'].mode()[0])","b222be6c":"df['firstName'] = df['firstName'].fillna(df['firstName'].mode()[0])","800c3c46":"df['middleName'].isna().value_counts()","d9e27c43":"df = df.drop(columns='middleName')","5ad1006d":"df['lastName'] = df['lastName'].fillna(df['lastName'].mode()[0])","097a1ea1":"df['currentTeamId'] = df['currentTeamId'].fillna(df['currentTeamId'].mode()[0])","eccf3eef":"df['currentNationalTeamId'] = df['currentNationalTeamId'].fillna(df['currentNationalTeamId'].mode()[0])","80197fc1":"df['birthDate'] = df['birthDate'].fillna(df['birthDate'].mode()[0])","6400aae6":"df['shortName'] = df['shortName'].fillna(df['shortName'].mode()[0])","16f7085f":"df['height'] = df['height'].fillna(df['height'].mean())","a5f57853":"df['role'] = df['role'].fillna(df['role'].mode()[0])","e1f385f7":"df['birthArea'] = df['birthArea'].fillna(df['birthArea'].mode()[0])","df37af62":"df['eventSec'] = np.round(df['eventSec'], decimals=2)","96e8263d":"df[['area', 'birthArea', 'league']]","91194bff":"df['playingInMotherland'] = df['area'] == df['birthArea']\ndf['playingInMotherland']","a8bfdc1c":"df['flang'] = np.where (((df['y_1'] > 50) & (df['foot'] == 'left')), 1, \n                           np.where((df['y_1'] <= 50) & (df['foot'] == 'right'), 1, 0))","14947428":"df['legioner'] = np.where((df['league'] == \"IT\") & (df['passportArea'] == \"Italy\"), 0,\n                            np.where((df['league'] == \"SP\") & (df['passportArea'] == \"Spain\"), 0,\n                                    np.where((df['league'] == \"GE\") & (df['passportArea'] == \"Germany\"), 0,\n                                            np.where((df['league'] == \"FR\") & (df['passportArea'] == \"France\"), 0,\n                                                    np.where((df['league'] == \"EN\") & (df['passportArea'] == \"England\"), 0, 1)))))","86c2f43f":"from datetime import datetime","1a5c3b14":"df['age'] = df['birthDate'].map( lambda date: np.round((datetime.strptime('2020-09-20', '%Y-%m-%d') - datetime.strptime(date, '%Y-%m-%d')).days \/ 365.25, decimals=2))","f7fee644":"df.plot.scatter(x='x_1', y='y_1', grid=True, legend=True)","57ecea4f":"from sklearn.neighbors import DistanceMetric\ndist = DistanceMetric.get_metric('euclidean')","9b432087":"df['distanceToGates'] = df.apply(lambda row: np.round(dist.pairwise([[row['x_1'], row['y_1']], [50, 50]])[0][1], decimals=2), axis=1)","3b46ccb0":"import numpy as np\n\ndef angle_between(p1, p2):\n    ang1 = np.arctan2(*p1[::-1])\n    ang2 = np.arctan2(*p2[::-1])\n    return np.rad2deg((ang2 - ang1) % (2 * np.pi))","5b24338a":"import category_encoders as ce","7309a89f":"df['angleToGates'] = df.apply(lambda row: np.round(angle_between((row['x_1'], row['y_1']), (50, 50)), decimals=2), axis=1)","dbaa6c1a":"df._get_numeric_data()","e1ddc2d2":"df.columns[df.isna().any()].tolist()","5ce2133b":"import matplotlib.pyplot as plt","ff81e4dc":"def histograms_plot(dataframe, features, rows, cols, figsize=(20, 20)):\n    fig = plt.figure(figsize=figsize)\n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(rows,cols, i+1)\n        dataframe[feature].hist(bins=20, ax=ax,facecolor='#56cfe1')\n        ax.set_title(feature, color='#e76f51')\n\n    fig.tight_layout()  \n    plt.show()\n","42b6866c":"num_columns = ['eventSec', 'y_1', 'x_1', 'weight', 'height', 'age', 'distanceToGates', 'angleToGates']\n    \nhistograms_plot(df, num_columns, 4, 2)","f44912f3":"for col_name in num_columns:\n    print('Skew of :', col_name, ':', df[col_name].skew())\n    print('Kurtosis of :', col_name, ':', df[col_name].kurtosis())\n    print('-------------------')","c2945af4":"fixed_x_1 = np.log(np.abs(df['x_1']))\nprint('Skew of  x_1 : ', fixed_x_1.skew())\nprint('Kurtosis of x_1 : ', fixed_x_1.kurtosis())","4b3ab9f0":"fixed_x_1 = np.square(df['x_1'])\nprint('Skew of  x_1 : ', fixed_x_1.skew())\nprint('Kurtosis of x_1 : ', fixed_x_1.kurtosis())\ndf['x_1'] = fixed_x_1","668c02b2":"def sigmoid(x):\n    return 1\/(1 + np.exp(-x)) ","3357b9b6":"fixed_eventSec = np.sqrt(np.abs(df['eventSec']))\nprint('Skew of  eventSec : ', fixed_eventSec.skew())\nprint('Kurtosis of eventSec : ', fixed_eventSec.kurtosis())\ndf['eventSec_1'] = fixed_eventSec\nhistograms_plot(df, ['eventSec', 'eventSec_1'], 1, 2, figsize=(10, 5))\ndf['eventSec'] = df['eventSec_1']\ndf = df.drop(columns='eventSec_1')","0c353452":"fixed_weight= np.square(np.abs(df['weight']))\nprint('Skew of  weight : ', fixed_weight.skew())\nprint('Kurtosis of weight : ', fixed_weight.kurtosis())\ndf['weight_1'] = fixed_weight\nhistograms_plot(df, ['weight', 'weight_1'], 1, 2, figsize=(10, 5))\ndf['weight'] = df['weight_1']\ndf = df.drop(columns='weight_1')","cca1c289":"fixed_height= np.square(np.abs(df['height']))\nprint('Skew of  height : ', fixed_height.skew())\nprint('Kurtosis of height : ', fixed_height.kurtosis())\ndf['height_1'] = fixed_height\nhistograms_plot(df, ['height', 'height_1'], 1, 2, figsize=(10, 5))\ndf['height'] = df['height_1']\ndf = df.drop(columns='height_1')","663bacc8":"# fixed_age= np.log(np.abs(df['age']))\n# print('Skew of  age : ', fixed_age.skew())\n# print('Kurtosis of age : ', fixed_age.kurtosis())\n# df['age_1'] = fixed_age\n# histograms_plot(df, ['age', 'age_1'], 1, 2, figsize=(10, 5))\n# df['age'] = df['age_1']\n# df = df.drop(columns='age_1')\n# Result\n# Skew of  age :  -0.03771203652443697\n# Kurtosis of age :  -0.47623913319654587\n# Was\n# Skew of : age : 0.2632797465060076\n# Kurtosis of : age : -0.2891482093613438","aa272a72":"# fixed_distanceToGates = np.square(np.abs(df['distanceToGates']))\n# print('Skew of  distanceToGates : ', fixed_distanceToGates.skew())\n# print('Kurtosis of distanceToGates : ', fixed_distanceToGates.kurtosis())\n# df['distanceToGates_1'] = fixed_distanceToGates\n# histograms_plot(df, ['distanceToGates', 'distanceToGates_1'], 1, 2, figsize=(10, 5))\n# df['distanceToGates'] = df['distanceToGates_1']\n# df = df.drop(columns='distanceToGates_1')\n# Result\n# Skew of  distanceToGates :  -0.2726414219391746\n# Kurtosis of distanceToGates :  -0.56980014668585\n# Was\n# Skew of : distanceToGates : -0.7252659051840858\n# Kurtosis of : distanceToGates : 0.11196854368607045","30127b2c":"fixed_angleToGates= np.sqrt(np.sqrt(np.abs(df['angleToGates'])))\nprint('Skew of  angleToGates : ', fixed_angleToGates.skew())\nprint('Kurtosis of angleToGates : ', fixed_angleToGates.kurtosis())\ndf['angleToGates_1'] = fixed_angleToGates\nhistograms_plot(df, ['angleToGates', 'angleToGates_1'], 1, 2, figsize=(10, 5))\ndf['angleToGates'] = df['angleToGates_1']\ndf = df.drop(columns='angleToGates_1')","5ea4ae69":"from scipy.stats import shapiro\nfrom scipy.stats import normaltest\nfrom scipy.stats import anderson","a0434428":"def normal_test(test, dataframe, features, p_threshold = 0.05):\n    test_results = []\n    for feature in features:\n        test_res = test(dataframe[feature])\n        print(feature, 'is distributted', 'normally' if test_res[1] > p_threshold else 'not normally')\n        test_results.append(test_res)\n    return test_results","3ac2e920":"normal_test(shapiro, df, num_columns)","a4c3934c":"normal_test(normaltest, df, num_columns)","1b97cca7":"def anderson_test(dataframe, features):\n    for feature in features:\n        test_res = anderson(dataframe[feature])\n        stat = test_res.statistic\n        critical_values = test_res.critical_values\n        significance_level = test_res.significance_level\n        for i in range(len(critical_values)):\n            sl, cv = significance_level[i], critical_values[i]\n            print(feature, 'is distributted', 'normally' if stat < cv else 'not normally', 'at the', sl, '% level')","4e45b03c":"anderson_test(df, num_columns)","a7f04b1f":"histograms_plot(df, num_columns, 4, 2, figsize=(10, 10))","d3820a87":"correlations = df.corr()\ncorrelations","23b01769":"import seaborn as sns;","6922840c":"sns.heatmap(correlations)","f24daef6":"correlations.unstack().sort_values(ascending=False).drop_duplicates()","03a62efd":"df = df.drop(columns=['shot_id', 'matchId', 'teamId', 'playerId', 'birthDate'])\nce_bin = ce.BinaryEncoder(cols=['passportArea', 'currentTeamId', 'currentNationalTeamId', 'birthArea', 'city', 'officialName', 'shortName', 'name'])\ndf = ce_bin.fit_transform(df)\ndf = pd.get_dummies(df, columns=['matchPeriod', 'is_CA', 'body_part', 'foot', 'league', 'role', 'area', 'playingInMotherland', 'flang', 'legioner', 'firstName', 'lastName'])","d2183abe":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score","3e22c351":"df_train = df[df['part'] == 1]\ndf_train = df_train.drop(columns='part')","db27b5f6":"df_test = df[df['part'] == 2]\ndf_test = df_test.drop(columns='part')","9b9d0a48":"from sklearn.model_selection import KFold","7c7e0ee6":"def perform_cross_validation(model, X, y, threshold=0.5, n_splits=5):\n    kf = KFold(shuffle=True, n_splits=n_splits, random_state=42)\n    scores = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.loc[train_index], X.loc[test_index]\n        y_train, y_test = y.loc[train_index], y.loc[test_index]\n        model.fit(X_train, y_train)\n        scores.append(f1_score(y_test, (model.predict_proba(X_test)[:,1] > threshold).astype('int')))\n    return scores","d60e9446":"def find_best_f1_threshold(model, train_X, train_y, thresholds=[0.5]):\n    overall_scores = []\n    for threshold in thresholds:\n        scores = perform_cross_validation(model, train_X, train_y, threshold)\n        overall_scores.append(np.mean(scores))\n\n    best_threshold_index = np.argmax(overall_scores)\n    return thresholds[best_threshold_index]","d8f5779d":"find_threshold_model = LogisticRegression(random_state=42, max_iter=10000).fit(df_train, y)","5a606c99":"best_threshold = find_best_f1_threshold(find_threshold_model, df_train, y, thresholds=[0.15, 0.32, 0.18, 0.23, 0.005, 0.42])","2f7dc31d":"best_threshold","be396835":"# best_threshold = 0.18","753eaf9d":"classifier = LogisticRegression(random_state=42, max_iter=10000).fit(df_train, y)","de6ba1b2":"scores = perform_cross_validation(classifier, df_train, y, best_threshold)\nnp.mean(scores)","77fce5db":"# 0.3596028179371231","b73da2ea":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","59e62018":"standart_classifier = make_pipeline(StandardScaler(), LogisticRegression(random_state=42, max_iter=10000)).fit(df_train, y)","14155db7":"scores = perform_cross_validation(standart_classifier, df_train, y, best_threshold)\nnp.mean(scores)","f326fc30":"# 0.3513648067597932","a2e8d997":"from sklearn.ensemble import RandomForestClassifier","a8c7bfb6":"# from sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\n\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1300, num = 100)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [1, 3, 5, 7, 10, 13, 15, 20]\n# # Minimum number of samples required to split a node\n# min_samples_split = [1, 2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n\n# random_forest_clf = RandomForestClassifier();\n\n# rf_random = RandomizedSearchCV(estimator = random_forest_clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# rf_random.fit(df_train, y)","81b577ab":"# rf_random.best_params_","c3202975":"random_forest_clf = RandomForestClassifier(random_state=42, n_estimators=1255, max_depth=15, min_samples_leaf=1, min_samples_split=2, bootstrap=True, max_features='sqrt').fit(df_train, y)","81fcc054":"scores_random_forest = perform_cross_validation(random_forest_clf, df_train, y, best_threshold)\nnp.mean(scores_random_forest)","d164b452":"# 0.3063723046112142","c912ab35":"pd.DataFrame({\n    'variable': df_train.columns,\n    'importance': random_forest_clf.feature_importances_\n}).sort_values('importance', ascending=False)","52981e61":"from sklearn.ensemble import BaggingClassifier","8f791e64":"bagging_logistic_classifier = BaggingClassifier(classifier, max_samples=0.85, max_features=0.85, random_state=42).fit(df_train, y)","7b8f5a41":"scores = perform_cross_validation(bagging_logistic_classifier, df_train, y, best_threshold)\nnp.mean(scores)","8b203b46":"# 0.3518289228036299","b030ef39":"from sklearn.ensemble import VotingClassifier","b75b8c5f":"voting_classifier = VotingClassifier(\n    weights=[1.1, 0.1],\n    estimators=[('lr', classifier), ('rf', random_forest_clf)],\n    voting='soft'\n).fit(df_train, y)","ba242204":"scores = perform_cross_validation(voting_classifier, df_train, y, best_threshold)\nnp.mean(scores)","4a4b45a7":"# 0.36217970352147233","c3d557dc":"import xgboost as xgb","70379a1d":"!nvidia-smi","0301ccab":"# xg_class_grid = XGBClassifier(objective='binary:logistic', missing = None)\n# param_grid = {\n#         'learning_rate': [0, 0.01, 0.03, 0.1],\n#         'min_split_loss': [0, 0.01, 0.2, 0.5, 1],\n#         'max_depth': [3, 4, 14, 17],\n#         'min_child_weight': [1, 3, 7, 8],\n#         'subsample': [0.5, 0.7, 0.9, 1],\n#         'n_estimators': [300, 500, 600, 700],\n#         'seed' : [42],\n#         'tree_method': ['gpu_hist'],\n#         'early_stopping_rounds': [15, 22, 30, 70, 100]\n#         }\n\n# xg_grid = RandomizedSearchCV(estimator=xg_class_grid, param_distributions=param_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n# xg_grid.fit(df_train, y)","61ab3afe":"# xg_grid.best_params_","f619461f":"xg_classifier = xgb.XGBClassifier(\n    colsample_bytree=0.7,\n    gamma=3,\n    learning_rate=0.02,\n    max_delta_step=0,\n    max_depth=2,\n    min_child_weight=8,\n    n_estimators=600,\n    reg_alpha=0.04,\n    reg_lambda=0.04,\n    scale_pos_weight=1,\n    seed=0,\n    subsample=0.8,\n    missing=None, \n    nthread=4,\n    objective='binary:logistic', \n    silent=False\n).fit(df_train, y)","3961f577":"scores = perform_cross_validation(xg_classifier, df_train, y, best_threshold)\nnp.mean(scores)","97028638":"# 0.4059622546375321","b4517230":"voting_classifier_with_xgb = VotingClassifier(\n    weights=[1.3, 0.9, 0.05],\n    estimators=[('xgboost', xg_classifier), ('lr', classifier), ('rf', random_forest_clf)],\n    voting='soft'\n).fit(df_train, y)","90f63065":"scores = perform_cross_validation(voting_classifier_with_xgb, df_train, y, n_splits=2, threshold=best_threshold)\nnp.mean(scores)","bf663322":"# defining various steps required for the genetic algorithm\n# import random\n# from sklearn.metrics import accuracy_score\n# def initilization_of_population(size,n_feat):\n#     population = []\n#     for i in range(size):\n#         chromosome = np.ones(n_feat,dtype=np.bool)\n#         chromosome[:int(0.3*n_feat)]=False\n#         np.random.shuffle(chromosome)\n#         population.append(chromosome)\n#     return population\n\n# def fitness_score(population, model, X_train, X_test, y_train, y_test):\n#     scores = []\n#     for chromosome in population:\n#         model.fit(X_train.iloc[:,chromosome],y_train)\n#         predictions = model.predict(X_test.iloc[:,chromosome])\n#         scores.append(accuracy_score(y_test,predictions))\n#     scores, population = np.array(scores), np.array(population) \n#     inds = np.argsort(scores)\n#     return list(scores[inds][::-1]), list(population[inds,:][::-1])\n\n# def selection(pop_after_fit,n_parents):\n#     population_nextgen = []\n#     for i in range(n_parents):\n#         population_nextgen.append(pop_after_fit[i])\n#     return population_nextgen\n\n# def crossover(pop_after_sel):\n#     population_nextgen=pop_after_sel\n#     for i in range(len(pop_after_sel)):\n#         child=pop_after_sel[i]\n#         child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\n#         population_nextgen.append(child)\n#     return population_nextgen\n\n# def mutation(pop_after_cross,mutation_rate):\n#     population_nextgen = []\n#     for i in range(0,len(pop_after_cross)):\n#         chromosome = pop_after_cross[i]\n#         for j in range(len(chromosome)):\n#             if random.random() < mutation_rate:\n#                 chromosome[j]= not chromosome[j]\n#         population_nextgen.append(chromosome)\n#     #print(population_nextgen)\n#     return population_nextgen\n\n# def generations(size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, y_train, y_test, model):\n#     best_chromo= []\n#     best_score= []\n#     population_nextgen=initilization_of_population(size,n_feat)\n#     for i in range(n_gen):\n#         scores, pop_after_fit = fitness_score(population_nextgen, model, X_train, X_test, y_train, y_test)\n#         print(scores[:2])\n#         pop_after_sel = selection(pop_after_fit,n_parents)\n#         pop_after_cross = crossover(pop_after_sel)\n#         population_nextgen = mutation(pop_after_cross,mutation_rate)\n#         best_chromo.append(pop_after_fit[0])\n#         best_score.append(scores[0])\n#     return best_chromo,best_score\n\n# predictions = logmodel.predict(X_test.iloc[:,chromo[-1]])","68b69e28":"# len(df_train.columns)","39a16e20":"# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size=0.20, random_state=40)","c7eeb2c3":"# chromo, score = generations(size=200, n_feat=len(df_train.columns), n_parents=100, mutation_rate=0.10, n_gen=10, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, model=classifier)\n# classifier.fit(df_train.iloc[:,chromo[-1]], y)","3bb2ce68":"predictions = voting_classifier_with_xgb.predict_proba(df_test)\npredictions","f114ac65":"predictions[:,1]","affb59b1":"f1_predictions = (predictions[:,1] > best_threshold).astype('int')\nf1_predictions","443641a7":"f1_predictions.sum()","ddcee83a":"submissions = pd.read_csv('..\/\/input\/itea-goal-prediction\/\/goal_submission.csv')\nsubmissions.head()","7977d3eb":"submissions['is_goal'] = f1_predictions\nsubmissions.to_csv('submission21.csv', index = False)","e34358bc":"best threshold appears to be 0.18","448bee1c":"obvious that standartized classifier works worse than the ordinary LogisticRegression. Now lets try training RandomForest classifier, using the same metrics","86b53877":"convert `birthDate` to `age` and delete `birthDate` (later `age` can be also used for some features)","f314988d":"unfortunately all tests show that data is not distributted normally(","82bf2c48":"lets write `is_goal` as Series into `y` variable and drop it from `train` dataset","35b56b45":"now lets design our baseline model. It will be LogisticRegresion as data was prepared first of all to be used with it.\n`f1` score will be used for evaluation","16797df5":"lets crete LogisticsRegression as our baseline model","30f64f24":"`legioner` - seems to be oposite to the `playingInMotherland` but citizenship can be changed (`passportArea`), but `birthArea` - no (\u043c\u043e\u0436\u043d\u0430 \u0432\u0441\u0435 \u043d\u0430 \u0441\u0432\u0456\u0442\u0456 \u0432\u0438\u0431\u0438\u0440\u0430\u0442\u0438 \u0441\u0438\u043d\u0443 \u0432\u0438\u0431\u0440\u0430\u0442\u0438 \u043d\u0435 \u043c\u043e\u0436\u043d\u0430 \u0442\u0456\u043b\u044c\u043a\u0438 \u0431\u0430\u0442\u044c\u043a\u0456\u0432\u0449\u0438\u043d\u0443 - \u0412.\u0421\u0438\u043c\u043e\u043d\u0435\u043d\u043a\u043e)","8a0eb2a5":"and write our own predictions ito a file to submit","f2b04419":"so it was decided to use it here as well. But, because it still takes a lot of time, hyperparameters of xgboost from the notebook (https:\/\/www.kaggle.com\/ligala\/xg-boost) of this great data scientist (https:\/\/www.kaggle.com\/ligala) have been taken:\n\n`{\n    'colsample_bytree': 0.7,\n    'gamma': 3,\n    'learning_rate': 0.02,\n    'max_delta_step': 0,\n    'max_depth': 2,\n    'min_child_weight': 8,\n    'n_estimators': 600,\n    'reg_alpha': 0.04,\n    'reg_lambda': 0.04,\n    'scale_pos_weight': 1,\n    'seed': 0,\n    'subsample': 0.8\n}`","b3a2ceba":"next some trick will be introduced. As `f1` needs to receive classes, but not probabilities, by default 0.5 is used, but it can be super inefficient. So the best threshold is needed to be selected. Next function that performs KFold division of the dataset and then performs `f1` scoring on these chanks is defined","0d3db7f2":"`playingInMotherland` - if player is playing in the country he\/she was born","13eaefb0":"seaborn will be used to show nice correlation matrix","f05e3c57":"`describe` is the another usefull method that shows usefull statistical values of the dataset","1bd9099a":"looks like `y_1` fits into Normal distribution, `weight` is shifted to the right, `age` has some right skew and `distanceToGates` - left, `x_1` also has left skew and smething bad with kurtosis. It is hard to say if others can even fit to normal distribution. Lets check this skew and kurtosis of this columns (skew should be 0 for Normal distribution as well as kurtosis)","69a3a999":"lets get all numerical columns","f738828e":"these commented lines are inspired with this article (https:\/\/datascienceplus.com\/genetic-algorithm-in-machine-learning-using-python\/) where author uses genetic algorythm to select only those features from the dataset that produce the best result (aka automated feature engineering) unfortunately after 9 hours of training and only 10 gemnerations result was very poor. So this try to use feature engineering with genetic algorythm on Logistic Regression can be considered as the possible improvement that is needed to be tested on a computer with better hardware","0c7ec5a6":"`middleName` will be dropped as it is fully empty","dc1ad777":"after analysing and creating columns data we can say that `['eventSec', 'y_1', 'x_1', 'weight', 'height', 'age', 'distanceToGates', 'angleToGates']` are not categorical","8c1837f7":"just to check if the number of goals looks adequate","06ca43bb":"the same with distanceToGates","4c780ea3":"they best parameters are:\n`{'n_estimators': 1255,\n 'min_samples_split': 2,\n 'min_samples_leaf': 1,\n 'max_features': 'sqrt',\n 'max_depth': 15,\n 'bootstrap': True}`\n \nand apply them to the `real` classifier and evaluate `f1` against it using the same method as for the LogisticRegression above","40175320":"lets find numerical columns","ff238910":"read the submission file example","5c004a74":"lets analyse them with histograms","231c25db":"seems that LogisticRegression is much-much better(","5acf1f8f":"next feature engineering part comes, where dataset is being prepared for the modelling","8fd514a1":"and check which features are correlating highly","5035784c":"and here function to iterate over various threshold options and select the one, that produces best score","91ceac49":"ordinary LogisticRegression seems to give the best results, so in VotingClassifier will use it","5e8e8d1a":"`eventSec` will be rounded to two decimals","3e2d2196":"`is_goal` - target value that is needed to be predicted","e03113c9":"`flang` - whether shot was done from the flang","b6765eff":"lets check if all values in columns are not empty and `fix` them setting column `mode` if this is a categorical column or `mean` otherwise (as LogisticRegression will be used as a baseline model and it needs data to fit as much to Normal distribution as it is possible)","cd9025f4":"now lets check correlation, the easiest way is to use `corr` method of DataFrame that by default uses Pearson correlation","971177b9":"after that create new column `part` in both `test` and `train` datasets that specifies origin train\/test(), this will allow us to perform same transformations on both datasets and only before training separate them","79729bc6":"also lets check feature importances","b1ebf030":"RandomizedSearchCV will be used to find best fitting parameters as it works faster then GridSearchCV","52cf538c":"here train and test datasets are separated, using that `part` column, that was created during datasets join","9080c682":"also we can get columns and columns length","f704360e":"first 5 rows can be received with `head` method","cb7d00fe":"also usefull is `sample` method that will show one random row from the dataset or an argument, that describes the needed rows that are needed to be randomly selected is passed","bafb8d33":"if RandomForest is so bad lets try to improve our LogisticRegression classifier and get more relevant scores using Bagging","aea86400":"for the experiment `purity` lets create one more LogisticRegression","0dc213cc":"enable GPU boosting","11daeac6":"mmmm, nice result!!!!\nNow lets go with a dark magic and try boosting, using xgboost","1892a2fa":"count `distanceToGates` from `x_1` and `y_1` assuming that gates are in the (50, 50) coordinates (we can see this from the graph below)","88909673":"there is no need to ran any tests before `fixing` these values (as a thought `y_1` can be tested right now, but lets try to fix others). We can try to fix skew with `log` or `square`","0476d98c":"results are far from ideal but much better, now we can check with test","aa261836":"To get info about data lets use `info` function of train part","f6624e11":"transformation of `age` above gives the best result, but tradeoff between the results is nearly simmilar, so not sure if it is needed","234fd036":"AWFULL, lets try square","cc889a12":"now all data is stored in the `df` DataFrame","261ba0e8":"the only difference is `is_goal` column that is not present in the test dataset, because it will be used to submit predictions","1e7715e0":"as we can see, this result is the best one! Lets try to use Blending of Xgboost, Logistic Regression and Random Forest classifiers to achieve even better result","e03bc2da":"now lets check what are the best params","5e6be775":"and also `angleToGates`","c4508a02":"lets create some new features from the existing ones that probably may increase model efficiency","ec0e7faa":"lets drop differrent `id` features and `birthDate` as it has been already converted into age. Lets use Binary encoder for the features with high cardinality, to avoid too many new features creted with One Hot encoding, and rest will encode with One Hot.\nThe interesting part is that dropping `firstName`, `lastName` columns results in worse result then encoding them with Binary Encoder, but ! encoding them with One Hot encoding producec even better result! ","83576f44":"As a task for course goal was decided to select dataset from itea-goal-prediction contest). \n\nFile descriptions:\n\n    goal_train.csv - the training set\n    goal_test.csv - the test set\n    goal_submission.csv - a sample submission file in the correct format","30577560":"converted into `f1` suitable format using previously defined threshold","bb95c085":"to ensure that test and train data sets are simmilar, the same `info` method can be used","33bf9c46":"Much better, lets try the same with the others, sometimes there is a small tradeoff when some one prop becomes worse, but the other largerly improves, also sometimes normalization (for example with sigmoid) helps","3cdbaa0f":"here final predictions on test dataset using best classifier are made","7578b6cd":"lets use this result for the evaluation even taking into account that it is slightly worse than xgboost alone","bbec9ef9":"next step will include combining RandomForest and LogisticRegression classifiers using Blending (VotingClassifier)","55712a2a":"and try the same using StandartScaler to check if even more standartization (despite of the one that we have already made) of features improves the model accuracy","2e37bf96":"there are two most popular options for tuning hyperparameters: RandomSearchCV (that has been already used for RandomForestClassifier) and GridSearchCV. The second one can produce better results as it performs all the combinations between passed hyperparameters values, but it is slow, and RandomSearchCV makes random selections and evaluates model only on them. So its results can be not so accurate, but can be received much faster"}}