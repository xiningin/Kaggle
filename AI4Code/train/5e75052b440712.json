{"cell_type":{"cdfe815a":"code","e51ef41f":"code","2010d1f5":"code","6c2ec97f":"code","4574723c":"code","59b69af7":"code","3fec7eae":"code","702a2b04":"code","70f8e6c6":"code","8a30911d":"code","0ad20c8f":"code","650fdbf1":"code","8c451bf3":"code","86f0853b":"code","dbf25fed":"code","7ca062bc":"code","aee205f9":"code","11272d7e":"code","915a5b1c":"code","1a6813c8":"code","2f67bdd8":"code","107f7ac6":"code","aef2becd":"code","7c73b1a6":"code","99e3d042":"code","7901c48b":"code","5dbceb20":"code","74955e97":"code","05a03d97":"code","1118e28f":"code","83d9ae36":"code","2bcca988":"markdown","00fe5e88":"markdown","78ee3f95":"markdown","5b1b0af3":"markdown","52abd414":"markdown","b6491223":"markdown","f8d6ea22":"markdown","14e3574f":"markdown","c78c2d92":"markdown","4e96daa1":"markdown","59a39421":"markdown","3b5add8b":"markdown","a9820e86":"markdown"},"source":{"cdfe815a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e51ef41f":"# import tensorflow as tf\n# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# # instantiating the model in the strategy scope creates the model on the TPU\n","2010d1f5":"df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')","6c2ec97f":"df","4574723c":"test = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\ntest","59b69af7":"df.describe()","3fec7eae":"df.isna().any()","702a2b04":"df.info() ","70f8e6c6":"X = df.drop('pressure', axis=1)","8a30911d":"y = df['pressure']","0ad20c8f":"# X = X[:5000]\n# y = y[:5000]","650fdbf1":"# i.shape\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33)","8c451bf3":"# !pip install lightgbm --install-option=--gpu\n# !pip install lightgbm --install-option=--cuda\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import Normalizer","86f0853b":"xgb = XGBRegressor(seed = 2021, n_estimators=500,verbosity=1, eval_metric=\"mae\", tree_method=\"gpu_hist\",gpu_id=0)\nxgb.fit(X_train, y_train)","dbf25fed":"from numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(xgb, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","7ca062bc":"\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(xgb, X_valid, y_valid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","aee205f9":"pred = xgb.predict(test)\nsub = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsub['pressure'] = pred\nsub.to_csv('submission.csv', index = 0)","11272d7e":"lgbm = LGBMRegressor(random_state=2021,n_estimators=10, metric=\"mae\", device_type=\"gpu\",gpu_platform_id = 0, gpu_device_id = 0)\nlgbm.fit(X_train, y_train)","915a5b1c":"cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(lgbm, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","1a6813c8":"\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(lgbm, X_valid, y_valid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","2f67bdd8":"cat = CatBoostRegressor(iterations=50, depth=3,eval_metric=\"MAE\", learning_rate=0.1, task_type=\"GPU\",devices=\"0\")\ncat.fit(X_train, y_train)","107f7ac6":"cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(cat, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","aef2becd":"\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(cat, X_valid, y_valid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","7c73b1a6":"from tensorflow.keras.wrappers.scikit_learn import KerasRegressor \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, Flatten\n\ndef baseline_model():\n        model = Sequential()\n        model.add(GRU(32, kernel_initializer='normal', activation='relu',return_sequences=True, input_shape=(1, 7)))\n        model.add(Dropout(0.2))\n        model.add(GRU(32, kernel_initializer='normal', return_sequences=True,activation='relu'))\n        model.add(Flatten())\n        model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(1,kernel_initializer='normal'))\n        model.compile(loss='mean_absolute_error', optimizer='adam')\n        return model\n\nestimator = KerasRegressor(build_fn=baseline_model, batch_size=16, epochs=2, verbose=1) # increase the epoch to around 20 for better result\nkfold = KFold(n_splits=2)  # replace the split with 5\nresults = cross_val_score(estimator, np.expand_dims(X, axis=1), y,scoring='neg_mean_absolute_error', cv=kfold)\nprint(\"Baseline: %.2f (%.2f) MAE\" % (results.mean(), results.std()))","99e3d042":"# train with the same configuration as below\n# from tensorflow.keras.callbacks import EarlyStopping\n# early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True )\n# history = estimator.fit(np.expand_dims(X, axis=1),y,validation_split=0.2, epochs=20,callbacks=[early_stopping])","7901c48b":"# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import KFold\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, Flatten\n\n# def baseline_model():\n#     with tpu_strategy.scope():\n#         model = Sequential()\n#         model.add(GRU(32, kernel_initializer='normal', activation='relu',return_sequences=True, input_shape=(1, 7)))\n#         model.add(Dropout(0.2))\n#         model.add(GRU(32, kernel_initializer='normal', return_sequences=True,activation='relu'))\n#         model.add(Flatten())\n#         model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n#         model.add(Dense(1,kernel_initializer='normal'))\n#         model.compile(loss='mean_absolute_error', optimizer='adam')\n#         return model\n\n# estimator = KerasRegressor(build_fn=baseline_model, batch_size=16, verbose=1)\n# kfold = KFold(n_splits=5)\n# results = cross_val_score(estimator, np.expand_dims(X_train, axis=1), y_train,scoring='neg_mean_absolute_error', cv=kfold)\n# print(\"Baseline: %.2f (%.2f) MAE\" % (results.mean(), results.std()))","5dbceb20":"# from tensorflow.keras.callbacks import EarlyStopping\n# early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True )\n# history = estimator.fit(np.expand_dims(X, axis=1),y,validation_split=0.2, epochs=20,callbacks=[early_stopping])","74955e97":"# history.history['loss']","05a03d97":"# history.history['val_loss']","1118e28f":"# import matplotlib.pyplot as plt\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n# plt.show()","83d9ae36":"# Thanks!","2bcca988":"Train with TPU","00fe5e88":"## CatBoost","78ee3f95":"performance","5b1b0af3":"## GRU TPU","52abd414":"## XGBoost","b6491223":"performance","f8d6ea22":"# Ventilator pressure prediction\nwe'll be using XGBoost regressor, catboost, lgbm libraries to predict our output. But I found XGBoost worked best for me without any finetuning. Other models will too work well if implemented properly.","14e3574f":"## GRU RNN","c78c2d92":"## LGBM","4e96daa1":"Load data and analyse it","59a39421":"[amitnikhade.com](https:\/\/amitnikhade.com)","3b5add8b":"split data into features and label","a9820e86":"performance"}}