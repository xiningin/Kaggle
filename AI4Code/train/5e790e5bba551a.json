{"cell_type":{"43eb20f1":"code","b1ee8d4f":"code","c953b65c":"code","413e12a0":"code","5965564c":"code","f245ad99":"code","0bcf1133":"code","7dbaa341":"code","2ca75b89":"code","26c641d5":"code","0183ae53":"code","8255c2b1":"code","2564c6b7":"code","fc415bb2":"code","85f13c86":"code","7d1fa17a":"code","e2566999":"code","d4d570d2":"code","b0472f57":"code","6426b79c":"code","4878a38b":"code","b2f88ad7":"code","f1f76b20":"code","68612114":"code","dbe9c47a":"code","dd735f11":"code","256eb5ec":"code","eed8d9ea":"code","d36242a9":"code","6be55fbf":"code","f8e66a73":"code","44fbb95f":"code","7001de3d":"markdown","871d00cc":"markdown","edffa1ef":"markdown","4492d0af":"markdown","9b5ee192":"markdown","c07a4f40":"markdown","5e5faabf":"markdown"},"source":{"43eb20f1":"import pandas as pd               # For manipulation of data\nimport numpy as np                # For mathematical computation of data\nimport matplotlib.pyplot as plt   # For visualization\nimport seaborn as sns             # Better visualization as built on top of matplolib","b1ee8d4f":"# Read the dataset stored in github repo\npharma_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/pharma_data\/Training_set_begs.csv')\ndataset = pharma_data.copy()","c953b65c":"# let's observe first five rows of dataset\ndataset.head()","413e12a0":"# Basic info\ndataset.info()","5965564c":"# Let's have a look at columns and their datatypes\nfrom tabulate import tabulate\n\ndef get_col_names(dataset):\n    col_list = list(dataset.columns)\n    dtype_list = list(dataset.dtypes)\n    table = dict()\n    table[\"cols\"] = col_list\n    table[\"dtype\"] = dtype_list\n    return print(tabulate(table, showindex=\"always\", tablefmt=\"grid\"))\n        \nget_col_names(dataset)","f245ad99":"# Let's look at missing values\nimport missingno\nmissingno.matrix(dataset);","0bcf1133":"# Percentage of missing values of dataset\nmissing = dataset.isna().sum()\npercent = (missing.sum() \/ len(dataset)) * 100\npercent","7dbaa341":"# Deleting records of people whose age is greater than 100\nnew_dataset = dataset[dataset[\"Patient_Age\"] < 100]\nlen(new_dataset[new_dataset[\"Patient_Age\"] > 100])","2ca75b89":"# Filling the missing values is other columns with mode\ncols = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"Z\", \"Number_of_prev_cond\"]\nnew_dataset[cols] = new_dataset[cols].fillna(new_dataset.mode().iloc[0])","26c641d5":"# Dropping columns which are not needed for model\ncols_to_drop = [\"ID_Patient_Care_Situation\", \"Patient_ID\", \"Patient_mental_condition\"]\nnew_dataset = new_dataset.drop(columns=cols_to_drop, axis=1)","0183ae53":"# Label encoding the Treated_with_drugs column\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nnew_dataset[\"Treated_with_drugs\"] = le.fit_transform(new_dataset[\"Treated_with_drugs\"])","8255c2b1":"# One hot encoding the Patient_Smoker and Patient_Rural_Urban columns and dropping the originals\ndummy = pd.get_dummies(new_dataset[\"Patient_Smoker\"], prefix=\"smoker_\")\nnew_dataset = pd.concat([new_dataset, dummy], axis=1)\n\ndummy_2 = pd.get_dummies(new_dataset[\"Patient_Rural_Urban\"], prefix=\"patient_\")\nnew_dataset = pd.concat([new_dataset, dummy_2], axis=1)\n\nnew_dataset = new_dataset.drop(columns=[\"Patient_Smoker\", \"Patient_Rural_Urban\"], axis=1)","2564c6b7":"# Splitting the dataset\nX = new_dataset.drop(columns=\"Survived_1_year\")\ny = new_dataset[\"Survived_1_year\"]","fc415bb2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7, shuffle=True)\nprint(f\"X_train={X_train.shape}  X_test={X_test.shape}\")","85f13c86":"# Initializing the model\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nmodel = LGBMClassifier()\nmodel.fit(X_train, y_train)","7d1fa17a":"# Performing cross-validation\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')","e2566999":"np.mean(n_scores)","d4d570d2":"from sklearn.metrics import f1_score\n\ny_preds_train = model.predict(X_train)\nprint(f1_score(y_train, y_preds_train))\n\ny_preds_test = model.predict(X_test)\nprint(f1_score(y_test, y_preds_test))","b0472f57":"# Visualizing the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_train, y_preds_train)\nsns.heatmap(cm, annot=True, fmt=\"d\");","6426b79c":"cm = confusion_matrix(y_test, y_preds_test)\nsns.heatmap(cm, annot=True, fmt=\"d\");","4878a38b":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(model, X_test, y_test)\nplt.show()","b2f88ad7":"# Tuning the model\nfrom sklearn.model_selection import GridSearchCV\n\nparameter_space = {\n    'boosting_type': [\"gbdt\", \"dart\"],\n    'num_leaves': [31, 62],\n    'max_depth': [3, 5, 10, 15],\n    'n_estimators': [200, 300, 500]\n}\n\nfrom sklearn.model_selection import GridSearchCV\nlgbm_gscv = GridSearchCV(model, parameter_space, scoring = 'f1')\nlgbm_gscv.fit(X_train, y_train)","f1f76b20":"lgbm_gscv.best_params_","68612114":"# Fitting the tuned model, making predictions & calculating f1 score\nlgbm_grid = LGBMClassifier(boosting_type=\"gbdt\", max_depth=3, n_estimators=500, num_leaves=31)\nlgbm_grid.fit(X_train, y_train)\n\ny_train_pred_2 = lgbm_grid.predict(X_train)\ny_test_pred_2 = lgbm_grid.predict(X_test)\n\nf1_tuned_train = f1_score(y_train, y_train_pred_2)\nf1_tuned_test = f1_score(y_test, y_test_pred_2)\nprint(f\"F1 of tuned model on train data = {f1_tuned_train}\")\nprint(f\"F1 of tuned model on validation data = {f1_tuned_test}\")","dbe9c47a":"cm = confusion_matrix(y_train, y_train_pred_2)\nsns.heatmap(cm, annot=True, fmt=\"d\");","dd735f11":"cm = confusion_matrix(y_test, y_test_pred_2)\nsns.heatmap(cm, annot=True, fmt=\"d\");","256eb5ec":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(lgbm_grid, X_test, y_test)\nplt.show()","eed8d9ea":"# Getting the test dataset\ntest_new = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/pharma_data\/Testing_set_begs.csv')\ntest_new.head()","d36242a9":"# Dropping irrelevant columns\ncols_to_drop = [\"ID_Patient_Care_Situation\", \"Patient_ID\", \"Patient_mental_condition\"]\ntest_new = test_new.drop(columns=cols_to_drop, axis=1)\ntest_new.head()","6be55fbf":"# Label encoding and one hot encoding the columns\nle = LabelEncoder()\ntest_new[\"Treated_with_drugs\"] = le.fit_transform(test_new[\"Treated_with_drugs\"])\n\ndummy = pd.get_dummies(test_new[\"Patient_Smoker\"], prefix=\"smoker_\")\ntest_new = pd.concat([test_new, dummy], axis=1)\n\ndummy_2 = pd.get_dummies(test_new[\"Patient_Rural_Urban\"], prefix=\"patient_\")\ntest_new = pd.concat([test_new, dummy_2], axis=1)","f8e66a73":"# Dropping the original columns\ntest_new = test_new.drop(columns=[\"Patient_Smoker\", \"Patient_Rural_Urban\"], axis=1)\ntest_new.shape","44fbb95f":"# Making predictions on test data and saving\npredictions = lgbm_grid.predict(test_new)\nres = pd.DataFrame(predictions) \nres.index = test_new.index\nres.columns = [\"prediction\"]\nres.to_csv(\"prediction_results_HP.csv\")","7001de3d":"We have 23097 entries with 18 columns","871d00cc":"## Data Description\n\n    ID_Patient_Care_Situation: Care situation of a patient during treatment\n    Diagnosed_Condition: The diagnosed condition of the patient\n    ID_Patient: Patient identifier number\n    Treatment_with_drugs: Class of drugs used during treatment\n    Survived_1_year: If the patient survived after one year (0 means did not survive; 1 means survived)\n    Patient_Age: Age of the patient\n    Patient_Body_Mass_Index: A calculated value based on the patient\u2019s weight, height, etc.\n    Patient_Smoker: If the patient was a smoker or not\n    Patient_Rural_Urban: If the patient stayed in Rural or Urban part of the country\n    Previous_Condition: Condition of the patient before the start of the treatment ( This variable is splitted into 8 columns - A, B, C, D, E, F, Z and Number_of_prev_cond. A, B, C, D, E, F and Z are the previous conditions of the patient. Suppose for one patient, if the entry in column A is 1, it means that the previous condition of the patient was A. If the patient didn't have that condition, it is 0 and same for other conditions. If a patient has previous condition as A and C , columns A and C will have entries as 1 and 1 respectively while the other column B, D, E, F, Z will have entries 0, 0, 0, 0, 0 respectively. The column Number_of_prev_cond will have entry as 2 i.e. 1 + 0 + 1 + 0 + 0 + 0 + 0 + 0 = 2 in this case. )\n","edffa1ef":"### Importing Libraries & fetching the data","4492d0af":"## Target\nThe dataset contains the patient records collected from a hospital in Greenland. The \"Survived_1_year\" column is a target variable which has binary entries (0 or 1).\n\n    Survived_1_year == 0, implies that the patient did not survive after 1 year of treatment\n    Survived_1_year == 1, implies that the patient survived after 1 year of treatment\n","9b5ee192":"# Objective\nA hospital in the province of Greenland has been trying to improve its care conditions by looking at historic survival of the patients. They tried looking at their data but could not identify the main factors leading to high survivals.\n\nYou are the best data scientist in Greenland and they've hired you to solve this problem. Now you are responsible for developing a model that will predict the chances of survival of a patient after 1 year of treatment (Survived_1_year).","c07a4f40":"## Test Data","5e5faabf":"We can see that there are few missing values across few columns. We have to take care of it!"}}