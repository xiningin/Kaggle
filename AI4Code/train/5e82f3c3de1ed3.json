{"cell_type":{"e2a3f988":"code","a7ce5979":"code","d4ce7a64":"code","e0b62c10":"code","008a1fe9":"code","1eada850":"code","b50715ce":"code","d4450202":"code","cec8151b":"code","b5fb6472":"code","16b7ffa0":"code","7a107691":"code","1b8a190a":"code","2e09f08f":"code","5bc73518":"code","09283255":"code","8bb40e73":"code","747533dc":"code","d8052f31":"code","9fa36e22":"code","15028d2f":"code","2598af03":"code","4ce30834":"code","2343859b":"code","7099f68e":"code","9c29ec12":"code","df2b4d3c":"code","aa609e3b":"code","b6068786":"code","f4923332":"code","fc7bfd90":"code","4c7638d4":"code","16bdeaff":"code","c899c22c":"code","7e75ffc4":"code","9aeefeda":"code","3fe8796a":"code","0c41b736":"code","04c8e8cd":"code","6a60c685":"code","4c355309":"code","a7c32c44":"code","f6d4297b":"code","92eccd69":"code","42cd6e40":"code","8c4e7424":"code","81ec9a65":"code","d2ab759e":"code","24f14f75":"code","23c35f8f":"code","cb75e576":"code","16e6abbd":"code","119e89e8":"code","9c7db9c6":"code","e9b5be77":"markdown","3abdd319":"markdown","145451e3":"markdown","91cdaaea":"markdown","a7fb7e6b":"markdown","f1ad7873":"markdown","4e19ee28":"markdown","7177c85e":"markdown","be569f6b":"markdown","9232ae6b":"markdown","44973a7b":"markdown","572c96fc":"markdown","85ff8fd1":"markdown","abc46a87":"markdown","0630d804":"markdown","2f58d488":"markdown","eca6e56d":"markdown","b9654b18":"markdown","9670c4eb":"markdown","19f8916d":"markdown","ce38a45f":"markdown","1e34b242":"markdown","edeed1af":"markdown","68543d4f":"markdown","0e2ac042":"markdown","e8f60ff8":"markdown"},"source":{"e2a3f988":"from fastai.text import *\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a7ce5979":"path = \"\/kaggle\/input\/nlp-getting-started\/\"\nbase_path=\"..\/output\"\ntext_columns=['text']\nlabel_columns=['target']\nBATCH_SIZE=128","d4ce7a64":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","e0b62c10":"train_df.head()","008a1fe9":"test_df.head()","1eada850":"list(train_df[train_df[\"target\"] == 1][\"text\"].values[0:10])","b50715ce":"list(train_df[train_df[\"target\"] == 0][\"text\"].values[0:10])","d4450202":"from nltk.tokenize import TweetTokenizer\ntwt = TweetTokenizer(strip_handles=True)\ndef tweets(r):\n    s = ' '.join(twt.tokenize(r['text']))\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub(r'https\\S+', '', s)    \n    return s","cec8151b":"train_df['ptext'] = train_df.apply(tweets, axis=1)\ntest_df['ptext'] = test_df.apply(tweets, axis=1)","b5fb6472":"# use the new preprocessed text column\ntext_columns = ['ptext']","16b7ffa0":"tweets = pd.concat([train_df[text_columns], test_df[text_columns]])\nprint(tweets.shape)","7a107691":"tweets.head()","1b8a190a":"data_lm = (TextList.from_df(tweets)\n           #Inputs: all the text files in path\n            .split_by_rand_pct(0.15)\n           #We randomly split and keep 10% for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=BATCH_SIZE))\ndata_lm.save('tmp_lm')\ndata_lm.show_batch()","2e09f08f":"data = TextClasDataBunch.from_csv(path, csv_name='train.csv', valid_pct=0.2, test='test.csv', text_cols='text', label_cols='target')\ndata.show_batch()","5bc73518":"data.vocab.itos[:15]","09283255":"data.train_ds[0][0]","8bb40e73":"data.train_ds[0][0].data[:10]","747533dc":"data_lm.show_batch()","d8052f31":"data_lm.train_ds","9fa36e22":"data_lm.valid_ds","15028d2f":"# no test set\ndata_lm.test_ds","2598af03":"# download a model\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","4ce30834":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","2343859b":"learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))\nlearn.save('fit_head')","7099f68e":"learn.load('fit_head');","9c29ec12":"learn.unfreeze()","df2b4d3c":"learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))","aa609e3b":"learn.save('fine_tuned')","b6068786":"learn.load('fine_tuned');","f4923332":"TEXT = \"There is a fire \"\nN_WORDS = 40\nN_SENTENCES = 2\n\nprint(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","fc7bfd90":"TEXT = \"I love this movie \"\nN_WORDS = 40\nN_SENTENCES = 2\n\nprint(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","4c7638d4":"learn.save_encoder('fine_tuned_enc')","16bdeaff":"data_clas = (TextList.from_df(train_df, cols=text_columns, vocab= data_lm.vocab)\n             # create a vaildation set from 15% of the training df\n            .split_by_rand_pct(0.15)\n             # identify the label columns\n            .label_from_df(label_columns)\n             # add the test df\n            .add_test(test_df[text_columns])\n            .databunch(bs=BATCH_SIZE))\ndata_clas.save('tmp_clas')\ndata_clas.show_batch()","c899c22c":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\nlearn.load_encoder('fine_tuned_enc')","7e75ffc4":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","9aeefeda":"learn.fit_one_cycle(1, 3e-2, moms=(0.8,0.7))","3fe8796a":"learn.save('first')","0c41b736":"learn.load('first');","04c8e8cd":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2), moms=(0.8,0.7))","6a60c685":"learn.save('second')","4c355309":"learn.load('second');","a7c32c44":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","f6d4297b":"learn.save('third')","92eccd69":"learn.load('third');","42cd6e40":"learn.unfreeze()\nlearn.fit_one_cycle(3, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","8c4e7424":"#learn.export()\nlearn.save('final')","81ec9a65":"learn.predict(\"There is a fire on the street\")","d2ab759e":"learn.predict(\"I love hot wings, they are fire\")","24f14f75":"from fastai.vision import ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6,6), dpi=60)","23c35f8f":"interp = TextClassificationInterpretation.from_learner(learn)\ninterp.show_top_losses(10)","cb75e576":"# check some actual predictions\nfor i in range(10):\n    print(test_df.loc[i,'text'])\n    print(learn.predict(test_df.loc[i,'text']))\n    print(' ')","16e6abbd":"## get test set predictions and ids\npreds, _ = learn.get_preds(ds_type=DatasetType.Test,  ordered=True)\npreds = preds.argmax(dim=-1)\n\nid = test_df['id']","119e89e8":"my_submission = pd.DataFrame({'id': id, 'target': preds})\nmy_submission.to_csv('submission.csv', index=False)","9c7db9c6":"my_submission['target'].value_counts()","e9b5be77":"# Creating a submission file\n\n![](http:\/\/)Create a submission file required for the competition","3abdd319":"It looks like our final accuracy is 81.3%. This is pretty good. Lets check it out.","145451e3":"Lets go over what the TextDataBunch function above actually did.","91cdaaea":"## Numericalization\nOnce we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n\nThe correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string).","a7fb7e6b":"The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n- the \"'s\" are grouped together in one token\n- the contractions are separated like this: \"did\", \"n't\"\n- content has been cleaned for any HTML symbol and lower cased\n- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one).","f1ad7873":"How good is our model? Well let's try to see what it predicts after a few given words.","4e19ee28":"Lets examine a couple predictions, as we can see, the first prediction correct,\n\"a fire on the street\" is a disaster.\n\nThe next text could very easily be mistaken as a disaster as it says fire.\nBut due to the context is obvioulsy is not. \n\nIt is correctly predicted as a non disaster tweet.","7177c85e":"As we can see, we have data sets for train and valid for our language model, we do not include the test set","be569f6b":"## Tokenization\nThe first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n\n- we need to take care of punctuation\n- some words are contractions of two different words, like isn't or don't\n- we may need to clean some parts of our texts, if there's HTML code for instance\n\nTo see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch.","9232ae6b":"Lets take a look at the top losses, these are the most confidently incorrect predictions","44973a7b":"## Lets get back to the original data_lm DataBunch","572c96fc":"Our language model has a accuracy of 41%, that is pretty good given such a small corpus of data.","85ff8fd1":"Lets have a look at the two types of tweets, disaster and not a disaster","abc46a87":"# **Disaster Tweets NLP (fastai V1 wikitext-103)**\n\nUsing fast.ai to predict disaster tweets.\nThis notebook walks through the steps to use a language model to predict the sentiment of disaster tweets.\nWe go through minimal preprocessing outside of the fast.ai default preprocessing techniques.\n\n**Overview:**\n* Access the data\n    * access the training and test sets and use tweet tokenization to remove urls\n    * create a df for our language model. This df simply incorporates all training and test tweets after preprocessing\n* Create a Language Model:\n    * using pre trained wikitext-103 model. Created off of a corpus of Wikipedia data\n    * The model is an RNN and we will fine tune it by unfreezing the final layers and inputting the tweets\n* Create a Text Classifier\n    * Using the language model we will train our text classifier against the training and test tweet dfs.\n    * Create a validation set from the 15% of the training df\n    * The text classifier will iteratively unfreeze the final layers of the model until finally unfreezing all the layers.\n* Analysis\n    * Take a look at the data in a couple different ways, check to see if the predictions make sense\n* Create a Submission File:\n    * Create the submission file to for Kaggle. (81.244%)","0630d804":"Convert the data into a text data bunch","2f58d488":"### Unfreezing\nIn Text Classification it has been found that it is best not to unfreeze the entire model\nSo we will unfreeze the last two layers first, fit one cycle, then train a little be more, \nthen finally unfreeze the entire model","eca6e56d":"We're not going to train a model that classifies the reviews from scratch. We will use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https:\/\/einstein.ai\/research\/blog\/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n\nWe are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the tweets left by people isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the tweets dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on.\n\nThis is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes).","b9654b18":"Fit one cycle on one epoch. \nUse the learning rate 2e-2 which is identified above in the learning rate finder.\nAlso include \"moms\" which decreases the momentum. This has been found to be useful in RNNs.","9670c4eb":"# Text Classifier\nNow, we'll create a new data object that only grabs the labelled data and keeps those labels.\nWe will constuct the text classifier by accessing the training and test df, the vocab from the language model\nspliting the data into a validation set, and add a test set from the test_df","19f8916d":"# Access the data\n\nOur data contains two files, one with the training data the other with the test data\n\nlets have a look at them","ce38a45f":"We can now create a model to classify those tweets and load the language model encoder we saved before","1e34b242":"# Language Model\n\nUsing the data_lm with the tweet url data removed from the text and all the text from both the training and test set we can create a langauge model.\nThis model will use transfer learning to learn how to predict the next word.\n","edeed1af":"We have to save not only the model, but also its encoder, the part that is responsible for creating and updating the hidden state.\nFor the next part, we dont care about the part that tries to guess the next work.","68543d4f":"# Analysis and Interpretation of Results\n\nFirst lets examine the confusion matrix.\n\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.","0e2ac042":"To complete the fine-tuning, we can then unfeeze and launch a new training.","e8f60ff8":"Now that we have removed the urls from the text strings lets use this new column in our data set"}}