{"cell_type":{"63bfe27d":"code","46e3e5c2":"code","f39337f0":"code","123eac27":"code","5f0605b6":"code","d3f58c24":"code","5d3565eb":"code","98c9f272":"code","7b2eee6e":"code","64fe199f":"code","d23f6a7a":"code","731c2b25":"code","61b17790":"code","6e3d2733":"code","146e78a8":"code","b2187175":"code","3ab3e3b1":"code","e297d549":"code","097ac22d":"code","861c2df4":"code","54c0ecc6":"code","90c412c3":"code","5766b97e":"code","5382a4b0":"code","f43c33c5":"code","3fc27dfb":"code","295dfdca":"code","b65a6bc1":"code","578ea8fa":"code","f2cf5307":"code","45bdd7b5":"code","daa10f27":"code","1e4218e8":"code","f3162212":"code","5bf2fdc8":"code","68db7940":"code","47cc2dd8":"code","6653344e":"code","57a0e4c5":"markdown","8e99d41b":"markdown","689ae562":"markdown","5d479f23":"markdown","0ed03652":"markdown","f4b1cf25":"markdown","11f3218e":"markdown","e04459f4":"markdown","855686d2":"markdown","c2bdb5bf":"markdown"},"source":{"63bfe27d":"!pip install xlrd==1.2.0","46e3e5c2":"!pip install transformers==3.5.0","f39337f0":"import os\nimport jieba\nimport numpy as np \nimport tensorflow_addons as tfa\nfrom tensorflow_addons.optimizers import AdamW\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input,Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nimport copy\nimport seaborn as sn\nimport random\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt\nfrom string import digits, punctuation\nimport re\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom tensorflow.keras.models import Model\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom transformers import AutoModel\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import BertTokenizer,BertModel,BertConfig,BertForPreTraining\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import f1_score,confusion_matrix,precision_score,recall_score\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nimport pandas as pd\nfrom gensim.models.word2vec import Word2VecKeyedVectors\nAUTO = tf.data.experimental.AUTOTUNE","123eac27":"transformers.__version__","5f0605b6":"def seed_everything(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n\n\nseed = 1024\nseed_everything(seed)","d3f58c24":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","5d3565eb":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen     \n    )\n    \n    return np.array(enc_di['input_ids'])","98c9f272":"\n#\u4f7f\u7528\u4e94\u4e2atoken\n\ndef build_model(transformer, max_len=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    res = transformer(input_word_ids,output_hidden_states = True)\n    #hideden_state\u4e00\u517112\u4e2a\uff0c\u6bcf\u4e00\u5c42\u7684\n    all_hidden_state = res[2]\n    \n    result= 0\n    for i in range(len(all_hidden_state)):\n        every_hidden_state = (all_hidden_state[i])\n        print(every_hidden_state.shape,every_hidden_state)\n        every_hidden_state_cls_token = every_hidden_state[:, 0, :]\n        result += every_hidden_state_cls_token\n    average_cls_token = result \/ len(all_hidden_state)\n \n    out = Dense(2, activation='softmax')(average_cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=5e-6), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.01), metrics=['accuracy',tfa.metrics.F1Score(num_classes=2,average='weighted')])\n    \n    return model","7b2eee6e":"#\u53ea\u4f7f\u7528\u6700\u540e\u4e00\u5c42\u7684cls_token\n# def build_model(transformer, max_len=512):\n#     \"\"\"\n#     https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n#     \"\"\"\n#     input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n#     res = transformer(input_word_ids,output_hidden_states = True)\n#     #hideden_state\u4e00\u517112\u4e2a\uff0c\u6bcf\u4e00\u5c42\u7684\n#     sequence_output, hidden_state = res[0],res[2]\n#     cls_token = sequence_output[:, 0, :]#\u6700\u540e\u4e00\u5c42\u7684cls token\n#     out = Dense(2, activation='softmax')(cls_token)\n    \n#     model = Model(inputs=input_word_ids, outputs=out)\n#     model.compile(Adam(lr=5e-6), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.01), metrics=['accuracy',tfa.metrics.F1Score(num_classes=2,average='weighted')])\n    \n#     return model","64fe199f":"# Detect hardware, return appropriate distribution strategy\n#\u8c03\u7528kaggle\u4e0a\u7684tpu\u5fc5\u8981\u4ee3\u7801\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","d23f6a7a":"AUTO = tf.data.experimental.AUTOTUNE\n\n# \u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\nkfold = KFold(n_splits=5, random_state=seed, shuffle=True)#\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\n# Configuration\nEPOCHS = 12\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nMAX_LEN = 140\nuse_external1 = False\nuse_external2 = True\nuse_pseudo = True\nuse_valid = True\nDISPLAY_PLOT = True\n# MODEL = 'roberta-base'\n# # \"roberta-base\",\"roberta-large\",\"bert-base-uncased\",\"ernie-2.0-en\"","731c2b25":"#\u6570\u636e\u9884\u5904\u7406\ndef fake_data_process(data):\n    for i,label in enumerate(data['label']):\n        if(data['label'][i] == \"real\"):\n            data['label'][i] = 1\n        else:\n            data['label'][i] = 0","61b17790":"fake_train = pd.read_excel(\"\/kaggle\/input\/aaai-fake-news\/Constraint_English_Train.xlsx\")\nfake_valid = pd.read_excel(\"\/kaggle\/input\/aaai-fake-news\/Constraint_English_Val.xlsx\")\nfake_test = pd.read_excel(\"\/kaggle\/input\/aaai-fake-news\/english_test_with_labels.xlsx\")\nfake_external1 = pd.read_excel(\"\/kaggle\/input\/aaai-fake-news\/external_1.xlsx\")\nfake_external2 = pd.read_excel(\"\/kaggle\/input\/aaai-fake-news\/external_2.xlsx\")\ndf_pseudo = pd.read_csv(\"..\/input\/aaai-fake-news\/pseudo_submission.csv\")\ndf_pseudo['label'] = np.mean(df_pseudo.iloc[:, -5:], axis=1) # label merge by mean\ndf_pseudo_cleaned = df_pseudo.loc[np.logical_or(df_pseudo['label'] > 0.95, df_pseudo['label'] < 0.05), ['tweet', 'label']].reset_index(drop=True) # select 0\ndf_pseudo_cleaned['label'] = np.round(df_pseudo_cleaned['label']).astype('int')","6e3d2733":"fake_train.label.value_counts()","146e78a8":"fake_valid.label.value_counts()","b2187175":"fake_test.label.value_counts()","3ab3e3b1":"fake_external2","e297d549":"fake_test","097ac22d":"df_pseudo_cleaned","861c2df4":"fake_train1 = pd.concat([fake_train['tweet'],fake_train['label']],axis=1)\nfake_valid1 = pd.concat([fake_valid['tweet'],fake_valid['label']],axis=1)\nfake_data_process(fake_train1)\nfake_data_process(fake_valid1)\nif (use_valid):\n    fake_train1 = pd.concat([fake_train1,fake_valid1],ignore_index=True)","54c0ecc6":"fake_train1.label.value_counts()","90c412c3":"# x_fake_train = regular_encode(fake_train1.tweet, tokenizer, maxlen=MAX_LEN)\n# x_fake_test = regular_encode(fake_valid1.tweet,tokenizer,maxlen=MAX_LEN)\n\n# y_fake_train = to_categorical(fake_train1.label,2,dtype='int32')\n# y_fake_test = to_categorical(fake_valid1.label,2,dtype='int32')\n# # y_fake_train = tf.convert_to_tensor(fake_train1.label.values,dtype=tf.int32)\n# # y_fake_valid = tf.convert_to_tensor(fake_valid1.label.values,dtype=tf.int32)","5766b97e":"def get_train_dataset(x_data,y_data):\n    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n    dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.shuffle(seed)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_valid_dataset(x_data,y_data):\n    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset","5382a4b0":"# fake_train_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((x_fake_train, y_fake_train))\n#     .repeat()\n#     .shuffle(2048)\n#     .batch(BATCH_SIZE)\n#     .prefetch(AUTO)\n# )\n\n# fake_valid_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((x_fake_valid, y_fake_valid))\n#     .batch(BATCH_SIZE)\n#     .cache()\n#     .prefetch(AUTO)\n# )","f43c33c5":"#\u5e26warm_up\u7684\u4f59\u5f26\u8870\u51cf\nLR_START = 0.00000\nLR_MAX = 0.00005 \nLR_MIN = LR_START = 0.000001\nLR_RAMPUP_EPOCHS = 6\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .4\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\n\nrng = [i for i in range(EPOCHS)]\ny_s = [lrfn(x) for x in rng]\nprint(y_s)\nplt.plot(rng, y_s)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y_s[0], max(y_s), y_s[-1]))\nlr_warm_up = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","3fc27dfb":"preds = []\nweights = []\nmodels = [\"digitalepidemiologylab\/covid-twitter-bert\",\"distilroberta-base\",\"nghuyong\/ernie-2.0-en\",\"lordtt13\/COVID-SciBERT\",\"digitalepidemiologylab\/covid-twitter-bert-v2\"]\nfor fold,(train,valid) in enumerate(kfold.split(fake_train1.tweet,fake_train1.label)):\n    print('#### FOLD',fold+1)\n    x_train,x_valid,y_train,y_valid = fake_train1.tweet[train],fake_train1.tweet[valid]\\\n    ,fake_train1.label[train],fake_train1.label[valid]\n    if (use_pseudo):\n        print(\"use pseudo\")\n        x_train = pd.concat([x_train, df_pseudo_cleaned.tweet]).reset_index(drop=True)\n        y_train = pd.concat([y_train, df_pseudo_cleaned.label]).reset_index(drop=True)\n    if (use_external2):\n        x_train = pd.concat([x_train, fake_external2.tweet]).reset_index(drop=True)\n        y_train = pd.concat([y_train, fake_external2.label]).reset_index(drop=True)\n    \n    tokenizer = AutoTokenizer.from_pretrained(models[fold])\n    \n    x_fake_train = regular_encode(x_train,tokenizer, maxlen=MAX_LEN)\n    x_fake_valid = regular_encode(x_valid,tokenizer,maxlen=MAX_LEN)\n\n    y_fake_train = to_categorical(y_train,2,dtype='int32')\n    y_fake_valid = to_categorical(y_valid,2,dtype='int32')\n    \n    train_dataset = get_train_dataset(x_fake_train,y_fake_train )\n    valid_dataset = get_valid_dataset(x_fake_valid,y_fake_valid)\n    \n    n_steps = x_fake_train.shape[0] \/\/ BATCH_SIZE\n\n    # BUILD MODEL\n    K.clear_session()\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    with strategy.scope():\n        transformer_layer = TFAutoModel.from_pretrained(models[fold])\n        model = build_model(transformer_layer, max_len=MAX_LEN)\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_f1_score', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='max', save_freq='epoch')\n    cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor = 'val_f1_score', factor = 0.1, patience = 2, verbose = 2, min_delta = 0.0001, mode = 'max')\n    \n    train_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    callbacks = [cb_lr_schedule,sv,lr_warm_up],\n    validation_data= valid_dataset,\n    epochs=EPOCHS)\n    \n    \n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)\n    \n        \n    x_full_train = regular_encode(fake_train1.tweet, tokenizer, maxlen=MAX_LEN)\n    \n    \n    weights.append(model.evaluate(valid_dataset)[2])\n    x_fake_test = regular_encode(fake_test.tweet,tokenizer,maxlen=MAX_LEN)\n    preds.append(model.predict(x_fake_test))\n    \n    \n    if DISPLAY_PLOT:\n        plt.figure(figsize=(15,5))\n        plt.plot(np.arange(EPOCHS),train_history.history['f1_score'],'-o',label='Train F1 Score',color='#ff7f0e')\n        plt.plot(np.arange(EPOCHS),train_history.history['val_f1_score'],'-o',label='Val F1 Score',color='#1f77b4')\n        x = np.argmax(train_history.history['val_f1_score']); y = np.max( train_history.history['val_f1_score'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max score\\n%.2f'%y,size=14)\n        plt.ylabel('F1 Score',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(np.arange(EPOCHS),train_history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(np.arange(EPOCHS),train_history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n        x = np.argmin( train_history.history['val_loss'] ); y = np.min( train_history.history['val_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n        plt.ylabel('Loss',size=14)\n        plt.title('FOLD %i'%(fold+1),size=18)\n        plt.legend(loc=3)\n        plt.show()  \n        \n    del model","295dfdca":"def weight_ensemble(weights,predictions):\n    weight_sum = np.sum(weights)  \n    prediction_sum = 0\n    for i in range(len(weights)):\n        prediction_sum += (weights[i]\/weight_sum)*predictions[i]\n    print(prediction_sum)\n    np.savez('model_predict_weight',prediction_sum)\n    result = np.argmax(prediction_sum,axis=1)\n    return result","b65a6bc1":"def mean_ensemble(predictions):\n    result = np.argmax(np.mean(predictions,axis=0),axis=1)\n    np.savez('model_predict_mean',np.mean(predictions,axis=0))\n    return result","578ea8fa":"weight_result= weight_ensemble(weights,preds)\n# f1_score(np.array(fake_valid1.label,dtype='int32'),weight_result,average='weighted')","f2cf5307":"data1 = np.load('.\/model_predict_weight.npz')","45bdd7b5":"# mean_result = mean_ensemble(preds)\n# f1=f1_score(np.array(fake_valid1.label,dtype='int32'),mean_result,average='weighted')\n# f1","daa10f27":"# result = np.argmax(preds[4],axis=1)\n# f1_score(np.array(fake_valid1.label,dtype='int32'),result,average='weighted')","1e4218e8":"def make_submission(result):\n    submission = pd.DataFrame(columns=['id','label'])\n    submission.label = result\n    nlist = range(1,result.shape[0]+1)\n    submission.id = nlist\n    submission.to_csv('answer.txt', index=False)","f3162212":"make_submission(weight_result)\n# make_submission(mean_result)","5bf2fdc8":"# submission = pd.DataFrame(columns=['id','label'])\n# submission.label = weight_result\n# nlist = range(1,weight_result.shape[0]+1)\n# submission.id = nlist\n# submission.to_csv('answer.txt', index=False)","68db7940":"\n# MODEL = \"lordtt13\/COVID-SciBERT\"\n# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n\n# x_fake_valid = regular_encode(fake_valid1.tweet,tokenizer,maxlen=MAX_LEN)\n# x_fake_train = regular_encode(fake_train1.tweet,tokenizer,maxlen=MAX_LEN)\n\n\n\n# y_fake_train = to_categorical(fake_train1.label,2,dtype='int32')\n# y_fake_valid = to_categorical(fake_valid1.label,2,dtype='int32')\n\n# with strategy.scope():\n#     transformer_layer = TFAutoModel.from_pretrained(MODEL)\n#     model = build_model(transformer_layer, max_len=MAX_LEN)\n# model.summary()\n# n_steps = x_fake_train.shape[0] \/\/ BATCH_SIZE\n\n# train_dataset = get_train_dataset(x_fake_train,y_fake_train)\n# valid_dataset = get_valid_dataset(x_fake_valid,y_fake_valid)\n# cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n#         monitor = 'val_f1_score', factor = 0.5, patience = 3, verbose = 1, min_delta = 0.0001, mode = 'max')\n# sv = tf.keras.callbacks.ModelCheckpoint(\n#         'best_model.h5', monitor='val_f1_score', verbose=0, save_best_only=True,\n#         save_weights_only=True, mode='max', save_freq='epoch')\n# train_history = model.fit(\n#     train_dataset,\n#     steps_per_epoch=n_steps,\n#     callbacks = [cb_lr_schedule,lr_warm_up,sv],\n#     validation_data= valid_dataset,\n#     epochs=EPOCHS\n#     )\n# print('Loading best model...')\n# model.load_weights('best_model.h5')\n# x_fake_test = regular_encode(fake_valid1.tweet,tokenizer,maxlen=MAX_LEN)\n# score = model.evaluate(valid_dataset)[2]\n# pred = model.predict(x_fake_test)\n# np.savez('single-model',pred)","47cc2dd8":"# f1_score(np.array(fake_valid1.label,dtype='int32'),np.argmax(pred,axis=1),average='weighted')","6653344e":"# submission = pd.DataFrame(columns=['id','label'])\n# submission.label = result\n# nlist = range(1,result.shape[0]+1)\n# submission.id = nlist\n# submission.to_csv('answer.txt', sep='\\t', index=False)","57a0e4c5":"## Build datasets objects","8e99d41b":"## TPU Configs","689ae562":"## Train Model","5d479f23":"## Helper Functions","0ed03652":"\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1 base\n\n\u5355\u6298     base     \n\n","f4b1cf25":"## Load text data into memory","11f3218e":"\u8fd8\u53ef\u4ee5\u5c1d\u8bd5\u7684\u70b9:\n\n\u4e94\u6298\u4ea4\u53c9\u5747\u503c\u878d\u5408 base  0.97709\n\n\u5355\u6298     base   0.974\n\n\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1,\u6743\u91cd\u878d\u5408  0.9789\n\n\u9886\u57df\u5185\u6570\u636e\u8fdb\u4e00\u6b65\u9884\u8bad\u7ec3\n\n1.\u52a0\u4e2d\u6587\u6570\u636e\u96c6 \u7528xlm-bert\n\n2.\u7cbe\u8c03warm_up    0.9794\n\n4.dropout  0.9706\n\n3.\u5229\u7528\u4f2a\u6807\u7b7e \n\n4.\u6807\u7b7e\u5e73\u6ed1    0.9790\n\n5:\u6dfb\u52a0\u5370\u5ea6\u8bed\u6570\u636e\n\n6:\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1,\u5e73\u5747\u878d\u5408  0.9814\n\n7:\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1,\u6743\u91cd\u878d\u5408  0.9789\n\n8:\u5355\u6298,\u5e73\u5747\u878d\u5408\n\n9:\u5355\u6298,\u6743\u91cd\u878d\u5408\n ,\n10:\u957f\u5ea6 130 0.978, 192 0.976\n\n","e04459f4":"## About this notebook\n\u4e3b\u8981\u91c7\u7528\u4e94\u6298","855686d2":"First, we train on the subset of the training set, which is completely in English.","c2bdb5bf":"## Load model into the TPU"}}