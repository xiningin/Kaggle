{"cell_type":{"6ae48c0d":"code","1e231d3b":"code","b3920aa0":"code","1bb27496":"code","13673ecd":"code","9645b879":"code","52e11d09":"code","824d77f4":"code","0f1a5b4c":"code","49ad4b43":"code","e40a5657":"code","1c7d6f73":"code","8b45fcd2":"code","2522fc27":"code","52044ef4":"code","f9ca4541":"code","d9465d90":"code","ace380bf":"code","951a11a8":"code","4dbab3ae":"markdown","067401c9":"markdown","5ff9c292":"markdown","5d4746ff":"markdown","04a7c9e7":"markdown","96a5a248":"markdown","34fd4a6b":"markdown","231041bc":"markdown","89f87585":"markdown"},"source":{"6ae48c0d":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport sys\nimport os\n\nimport cudf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport xgboost as xgb\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport shap\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')","1e231d3b":"N_SPLITS = 5\nN_ESTIMATORS = 80000\nLEARNING_RATE = 1e-2\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 42","b3920aa0":"# Optuna parameters\nN_TRIALS = 300\nTRAIN_TIME = 3 * 60 * 60","1bb27496":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nseed_everything(SEED)","13673ecd":"%%time\n\nINPUT = \"..\/input\/tabular-playground-series-nov-2021\/\"\n\ntrain = pd.read_csv(INPUT + \"train.csv\")\ntest = pd.read_csv(INPUT + \"test.csv\")\nsubmission = pd.read_csv(INPUT + \"sample_submission.csv\")\n\nfeatures = [col for col in test.columns if 'f' in col]\nTARGET = 'target'","9645b879":"scaler = StandardScaler()\n\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","52e11d09":"display(train.info())\ndisplay(train.head())","824d77f4":"display(test.info())\ndisplay(test.head())","0f1a5b4c":"def objective(trial, X=train[features], y=train[TARGET]):\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=SEED, stratify=train[TARGET])\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 2, 10),\n        'subsample': trial.suggest_float('subsample', 0.2, 0.9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),\n        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.2, 0.9),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-1, 1e3), \n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e2),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e2), \n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e2),\n    }\n\n    xgb_params = params\n    xgb_params['booster'] = 'gbtree'\n    xgb_params['objective'] = 'binary:logistic'\n    xgb_params['n_estimators'] = N_ESTIMATORS\n    xgb_params['seed'] = SEED\n    xgb_params['learning_rate'] = LEARNING_RATE\n    xgb_params['use_label_encoder'] = False\n    xgb_params['importance_type'] = 'gain'\n    xgb_params['tree_method'] = 'gpu_hist'\n    xgb_params['predictor'] = 'gpu_predictor'\n\n    model = xgb.XGBClassifier(**xgb_params)\n    model.fit(X_train,\n              y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='auc',\n              early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n              verbose=False\n             )\n\n    preds = model.predict_proba(X_valid)[:, -1]\n    score = roc_auc_score(y_valid, preds)\n\n    return score","49ad4b43":"study = optuna.create_study(sampler=TPESampler(), study_name='TPS09', direction='maximize')\nstudy.optimize(objective, n_trials=N_TRIALS, timeout=TRAIN_TIME, show_progress_bar=True)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","e40a5657":"optuna.visualization.plot_optimization_history(study)","1c7d6f73":"optuna.visualization.plot_parallel_coordinate(study)","8b45fcd2":"optuna.visualization.plot_slice(study)","2522fc27":"optuna.visualization.plot_param_importances(study)","52044ef4":"xgb_params = trial.params\n    \nxgb_params['booster'] = 'gbtree'\nxgb_params['objective'] = 'binary:logistic'\nxgb_params['n_estimators'] = N_ESTIMATORS\nxgb_params['seed'] = SEED\nxgb_params['learning_rate'] = LEARNING_RATE\nxgb_params['use_label_encoder'] = False\nxgb_params['importance_type'] = 'gain'\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\n\ndisplay(xgb_params)","f9ca4541":"xgb_oof = np.zeros(train.shape[0])\nxgb_pred = np.zeros(test.shape[0])\nshap_values = np.zeros((train.shape[0], train[features].shape[1]))\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=train[TARGET])):\n    print(f\"===== fold {fold} =====\")\n    X_train, y_train = train[features].iloc[trn_idx], train[TARGET].iloc[trn_idx]\n    X_valid, y_valid = train[features].iloc[val_idx], train[TARGET].iloc[val_idx]\n    X_test = test[features]\n\n    start = time.time()\n    model = xgb.XGBClassifier(**xgb_params)\n    model.fit(X_train,\n              y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='auc',\n              early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n              verbose=VERBOSE\n             )\n\n    shap_values[val_idx] = shap.TreeExplainer(model).shap_values(X_valid)\n    xgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    xgb_pred += model.predict_proba(X_test)[:, -1] \/ N_SPLITS\n\n    elapsed = time.time() - start\n    auc = roc_auc_score(y_valid, xgb_oof[val_idx])\n    print(f\"fold {fold} - xgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nprint(f\"oof xgb auc = {roc_auc_score(train[TARGET], xgb_oof)}\")\n\nnp.save(\"xgb_oof.npy\", xgb_oof)\nnp.save(\"xgb_pred.npy\", xgb_pred)","d9465d90":"if 0:\n    print(\"Feature distribution: \")\n    ncols = 5\n    nrows = int(len(features) \/ ncols + (len(features) % ncols > 0))\n\n    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 150), facecolor='#EAEAF2')\n\n    for r in range(nrows):\n        for c in range(ncols):\n            col = features[r*ncols+c]\n            sns.kdeplot(x=train[col], ax=axes[r, c], color='#58D68D', label='Train data')\n            sns.kdeplot(x=test[col], ax=axes[r, c], color='#DE3163', label='Test data')\n            axes[r, c].set_ylabel('')\n            axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n            axes[r, c].tick_params(labelsize=5, width=0.5)\n            axes[r, c].xaxis.offsetText.set_fontsize(4)\n            axes[r, c].yaxis.offsetText.set_fontsize(4)\n    plt.show()","ace380bf":"shap.summary_plot(shap_values, train[features], show=False)","951a11a8":"submission[TARGET] = xgb_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","4dbab3ae":"## Cross validation","067401c9":"# Datasets\n---","5ff9c292":"## SHAP values","5d4746ff":"# Libraries\n---","04a7c9e7":"## Hyperparameter tuning\n---","96a5a248":"# Standardization\n---","34fd4a6b":"# Parameters\n---","231041bc":"# Submission\n---","89f87585":"# XGBoost\n---"}}