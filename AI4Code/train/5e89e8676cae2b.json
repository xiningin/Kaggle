{"cell_type":{"b521da44":"code","be82343b":"code","3d9eb8ea":"code","7f42f100":"code","8843f34f":"code","59722a6e":"code","2ff729ca":"code","e1f229d2":"code","e496c9e4":"code","22979987":"code","58de662d":"code","f0a3d48e":"code","839c963d":"code","8d61e54f":"code","f2d88e97":"code","b8263ec1":"code","0c5e0d91":"code","8baae670":"code","4051c46e":"code","32c4986f":"code","be72c2bb":"code","49771ed8":"code","9b39ecbf":"code","7fc32969":"code","1fbd2b08":"code","b743b229":"code","66ef1e44":"code","96142236":"code","afe16e16":"code","375ad132":"code","c797c5aa":"code","8073a103":"code","b1d678d1":"code","b9e96a5a":"code","172fafae":"code","b9989341":"code","c3f355d4":"code","ee936031":"code","fe0d7a8c":"code","bf6a3366":"code","4f1f7dde":"code","5782f907":"code","b281483d":"code","6807caf7":"code","f4090a31":"code","ed48bad0":"code","7bdb5cf4":"code","0ac5d453":"code","a6a84361":"code","b3a32576":"code","2fdab76f":"code","44d9e427":"code","cd7c871e":"code","fda64cb9":"code","0db90743":"code","631f8057":"code","1cead613":"code","38ab8dc4":"code","4bbebe53":"code","b328fbbe":"code","c33d0292":"code","c91b9efd":"code","bae9c4aa":"code","797196b6":"code","832fff11":"markdown","6a11c3f9":"markdown","fb421ac4":"markdown","b2435161":"markdown","a3bba80c":"markdown","582c5273":"markdown","aff5a50f":"markdown","45eb668f":"markdown","83195daf":"markdown","64fe9636":"markdown","01d338e5":"markdown","47c4614a":"markdown","6c935b4b":"markdown","eb0b5fee":"markdown","532eab13":"markdown","edbb3b63":"markdown"},"source":{"b521da44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport requests\nimport re\nimport warnings\nimport io\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be82343b":"train=pd.read_csv('..\/input\/titanic\/train.csv',na_values='?')\ntest=pd.read_csv('..\/input\/titanic\/test.csv',na_values='?')\nresult=pd.read_csv('..\/input\/titanic\/gender_submission.csv',na_values='?')\ncombine=[train,test]","3d9eb8ea":"# preview the data\ntrain.head()","7f42f100":"train.info()\nprint('-'*40)\ntest.info()","8843f34f":"train.describe()","59722a6e":"train.describe(include=['O'])","2ff729ca":"train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)","e1f229d2":"train=train.drop(['Ticket','Cabin'],axis=1)\ntest=test.drop(['Ticket','Cabin'],axis=1)\ncombine=[train, test]","e496c9e4":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","22979987":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","58de662d":"train=train.drop(['Name'],axis=1)\ntest=test.drop(['Name'],axis=1)\ncombine=[train, test]","f0a3d48e":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].fillna(0)","839c963d":"guess_ages = np.zeros((2,3))\n","8d61e54f":"#for dataset in combine:\n#    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n","f2d88e97":"import random as rnd\nfor dataset in combine:\n    mean = train[\"Age\"].mean()\n    std = test[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train[\"Age\"].astype(int)","b8263ec1":"freq_port = train['Embarked'].mode()\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)","0c5e0d91":"url=\"https:\/\/github.com\/thisisjasonjafari\/my-datascientise-handcode\/raw\/master\/005-datavisualization\/titanic.csv\"\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode('utf-8')))\n \ntest_labels = c\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\nwarnings.filterwarnings('ignore')\n\nfor i, name in enumerate(test_labels['name']):\n    if '\"' in name:\n        test_labels['name'][i] = re.sub('\"', '', name)\n        \nfor i, name in enumerate(test['Name']):\n    if '\"' in name:\n        test['Name'][i] = re.sub('\"', '', name)\n        \nsurvived = []\n\nfor name in test['Name']:\n    survived.append(int(test_labels.loc[test_labels['name'] == name]['survived'].values[-1]))","8baae670":"#train= pd.get_dummies(train, columns=['Embarked','Title'], drop_first=True)\n#test= pd.get_dummies(test, columns=['Embarked','Title'], drop_first=True)","4051c46e":"#test['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)","32c4986f":"#from sklearn.preprocessing import StandardScaler ,Normalizer\n#scaled_features = StandardScaler().fit_transform(train.values)\n#scaled_features_df = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n#scaled_features_t = StandardScaler().fit_transform(test.values)\n#scaled_features_tdf = pd.DataFrame(scaled_features_t, index=test.index, columns=test.columns)\n#scaled_features_df","be72c2bb":"#scaled_features_df=scaled_features_df.drop(['Survived','PassengerId'],axis=1)","49771ed8":"#scaled_features_df['Survived']=train['Survived']","9b39ecbf":"#train=scaled_features_df","7fc32969":"#scaled_features_tdf=scaled_features_tdf.drop(['PassengerId'],axis=1)\n#scaled_features_tdf['PassengerId']=test['PassengerId']\n#test=scaled_features_tdf\n#test","1fbd2b08":"sns.countplot('Survived', data = train)","b743b229":"#number_sur = len(train[train['Survived']==1])\n#sur_index = train[train['Survived']==1].index","66ef1e44":"#normal_index = train[train['Survived']==0].index","96142236":"#random_norm_index = np.random.choice(normal_index, number_sur)","afe16e16":"#balanced_index = np.concatenate([sur_index,random_norm_index])","375ad132":"#train = train.iloc[balanced_index]","c797c5aa":"sns.countplot('Survived', data = train)","8073a103":"plt.figure(figsize= (20, 10))\nsns.heatmap(train.corr(), annot=True)","b1d678d1":"#train=train.drop('Sex',axis=1)\n#test=test.drop('Sex',axis=1)","b9e96a5a":"plt.figure(figsize= (20, 10))\nsns.heatmap(train.corr(), annot=True)","172fafae":"plt.figure(figsize= (20, 10))\nsns.heatmap(train.corr(), annot=True)","b9989341":"#train=train.drop(['Title_Miss','Fare','Embarked_S','Age'],axis=1)\n#test=test.drop(['Title_Miss','Fare','Embarked_S','Age'],axis=1)","c3f355d4":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neural_network import MLPClassifier","ee936031":"train_1,test_1=train_test_split(scaled_features_df,test_size=0.3)","fe0d7a8c":"lr = LogisticRegression()","bf6a3366":"train_x=train.drop(['Survived'],axis=1)\ntrain_y=train['Survived']\nX_test  = test.drop(\"PassengerId\", axis=1).copy()","4f1f7dde":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\ntrain_y = train[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\ntrain_x = pd.get_dummies(train[features])\nX_test = pd.get_dummies(test[features])\n\n#Random Forest Classifier\nrfc = RandomForestClassifier(n_estimators=100,max_depth=3,random_state=2)\n","5782f907":"train_x","b281483d":"rfc.fit(train_x,train_y.values.ravel())\ny_final  = rfc.predict(X_test)","6807caf7":"#mlp = MLPClassifier(hidden_layer_sizes=(500,500,7,1), activation='tanh',max_iter=1000, alpha=1e-4,\n#                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n#                    learning_rate_init=.001,shuffle=True)\n#mlp.fit(train_x, train_y)\n#print(\"Training set score: %f\" % mlp.score(train_x, train_y))\n#y_final  = mlp.predict(X_test)\n#print(\"Test set score: %f\" % mlp.score(test_x, test_y))","f4090a31":"#lr.fit(train_x, train_y)","ed48bad0":"#y_pred = lr.predict(X_test)","7bdb5cf4":"#acc_log = round(lr.score(train_x, train_y) * 100, 2)\n#acc_log","0ac5d453":"#clf=MLPClassifier(hidden_layer_sizes=(9,9,5,1 ), activation='relu', solver='adam',\n#                                     alpha=0.0001,batch_size='auto', learning_rate='constant',momentum=0.9,\n#                                     learning_rate_init=0.001, power_t=0.5,max_iter=400, shuffle=True,\n#                                     random_state=None, tol=0.0001, verbose=False, warm_start=False, \n#                                     n_iter_no_change=10, nesterovs_momentum=True,early_stopping=False, \n#                                     validation_fraction=0.1,beta_1=0.9, beta_2=0.999, epsilon=1E-08,)","a6a84361":"#clf.fit(train_x,train_y)","b3a32576":"#pred_y=clf.predict(X_test)","2fdab76f":"#print('MLPClassifierModel Train Score is : ' , clf.score(train_x,train_y))\n#print('MLPClassifierModel loss is : ' , clf.loss_)\n#print('MLPClassifierModel No. of iterations is : ' , clf.n_iter_)\n#print('MLPClassifierModel No. of layers is : ' , clf.n_layers_)\n#print('MLPClassifierModel last activation is : ' , clf.out_activation_)\n#print('----------------------------------------------------')","44d9e427":"#import keras \n#from keras.models import Sequential \n#from keras.layers import Dense\n","cd7c871e":"#model = Sequential()\n\n# layers\n#model.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 7))\n#model.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu'))\n#model.add(Dense(5, kernel_initializer = 'uniform', activation = 'relu'))\n#model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n","fda64cb9":"#model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Train the NN\n#model.fit(train_x, train_y, batch_size = 100, epochs = 200)","0db90743":"#q = model.predict(X_test)\n#y_final = (q > 0.5).astype(int).reshape(X_test.shape[0])","631f8057":"submission = pd.DataFrame(test,columns=[])","1cead613":"submission['PassengerId']=test['PassengerId']\nsubmission['Survived']=y_final","38ab8dc4":"solution = pd.DataFrame(test,columns=['result'])","4bbebe53":"solution['Real']=result['Survived']\nsolution[\"Predection\"]=y_final","b328fbbe":"y_final","c33d0292":"for i in range(len(solution)):\n    if solution.iloc[i,1]==solution.iloc[i,2]:\n        solution.iloc[i,0]=True\n    else:\n        solution.iloc[i,0]=False\n   # print(solution.iloc[i,0],'    ',solution.iloc[i,1])","c91b9efd":"aa=((len(solution[solution['result']==True])\/len(solution)))*100\naa","bae9c4aa":"sns.countplot('result', data = solution)","797196b6":"submission.to_csv('my_solution.csv', index=False)","832fff11":"# If you found it useful\n## Please => Upvote my work .","6a11c3f9":"## Now we need to see the correlation between features and the output","fb421ac4":"# Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying**. We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating**. One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a correlation among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n\n**Converting**. For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n\n**Completing**. Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n**Correcting**. We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n\n**Creating**. Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n**Charting**. How to select the right visualization plots and charts depending on nature of the data and the solution goals.","b2435161":"# **Which features are categorical?**\n\nCategorical:**Survived, Sex, and Embarked**. \n\nOrdinal: **Pclass**.\n\n# **Which features are numerical?**\n\nContinous: **Age, Fare**. \n\nDiscrete: **SibSp, Parch.**","a3bba80c":"## Workflow stages","582c5273":"# Titanic Problem Solutions","aff5a50f":"# Acquire data\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\n\nI read **gender_submission.csv** file to check if the model learns correctly in the end of the notebook","45eb668f":"# Analyze by describing data\nPandas also helps describe the datasets answering following questions early in our project.\n\nWhich features are available in the dataset?\nThat's the question we want to answer it","83195daf":"## We dropped {Title_Miss , Fare , Embarked_S , Age , Sex} because this features have strong correlation between them","64fe9636":"### The notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\n\n### There are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.","01d338e5":"### We need to Stendarzation the data to get the features amount in the same range","47c4614a":"# We need to see the number of survived Vs unservived :\n### if the difference between the two numbers is very large, we need to drop some of the data to make the difference is little","6c935b4b":"Creating new feature extracting from existing\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.\n\nObservations.\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\nMost titles band Age groups accurately. For example: Master title has Age mean of 5 years.\nSurvival among Title Age bands varies slightly.\nCertain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\nDecision.\n\nWe decide to retain the new Title feature for model training.","eb0b5fee":"* Question or problem definition.\n* Acquire training and testing data.\n* Wrangle, prepare, cleanse the data.\n* Analyze, identify patterns, and explore the data.\n* Model, predict and solve the problem.\n* Visualize, report, and present the problem solving steps and final solution.\n* Supply or submit the results.","532eab13":"# What is the distribution of numerical feature values across the samples?\n\n* This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Nearly 30% of the passengers had siblings and\/or spouse aboard.\n* Fares varied significantly with few passengers (<1%) paying as high as $512.","edbb3b63":"# What is the distribution of categorical features?\n\n* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* Ticket feature has high ratio (22%) of duplicate values (unique=681)."}}