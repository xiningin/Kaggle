{"cell_type":{"77783b64":"code","9d9f059e":"code","47204c17":"code","37bb60c4":"code","b98cf903":"code","8124caba":"code","2e165ce3":"code","bc3312c8":"code","4e7fb911":"code","d7838608":"code","ccfaba4d":"code","c221801e":"code","2d9b183a":"code","b5851a04":"code","11cd4bd2":"code","ec2b9c15":"code","1ca5b790":"code","16ee8d93":"code","d471689e":"code","ab3d4659":"code","6f39bb20":"code","bd774975":"code","81e49eb0":"code","2207f930":"code","51b02019":"code","d1c5b131":"code","274c82a0":"code","95bbe8ec":"code","2d1ed0ec":"code","7e97699a":"code","c4363960":"code","19df4d18":"code","d5f3ac90":"code","61d29a00":"code","c55e3476":"code","9aff0ab0":"code","74a8eef3":"code","bdaab10c":"code","003aaa6a":"code","f53e561f":"code","27e77a6e":"code","508aa2fc":"code","61ea855a":"code","61b90bf0":"code","87aeda57":"code","0c8e31e4":"code","1b837648":"code","65a255f8":"code","32ab31bf":"code","14c2c8fb":"code","3180932a":"code","862e8a45":"code","8b5df681":"code","5c702418":"code","47f8b953":"code","1ac38846":"code","6dd6bb4f":"code","d979eb9e":"code","b5bcd670":"code","0146bcf3":"markdown","acd32000":"markdown","1ccf7f1c":"markdown","f0956edf":"markdown","e3e7a882":"markdown"},"source":{"77783b64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d9f059e":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nboth = [train_data, test_data]","47204c17":"sns.heatmap(train_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","37bb60c4":"sns.heatmap(test_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","b98cf903":"train_data.columns[train_data.isnull().any()]","8124caba":"corrmat = train_data.corr()\nmatrix = np.triu(corrmat)\nf,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(corrmat, vmax=1.0,annot=True,fmt='.2g',cbar=False,mask=matrix, square=True)","2e165ce3":"grid = sns.FacetGrid(train_data, row='Embarked', height=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","bc3312c8":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', height=2.2, aspect=1.7)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5,color='r', ci=50)\ngrid.add_legend()","4e7fb911":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', height=2, aspect=1.5)\ngrid.map(plt.hist, 'Age',color='g', alpha=.7, bins=20)\ngrid.add_legend();","d7838608":"for dataset in both:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","ccfaba4d":"for dataset in both:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'VIP')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title']).mean()","c221801e":"for data in both:\n    data.Title = data.Title.replace(to_replace=['Master','Miss','Mr','Mrs','VIP'], value=[0.575,0.702,0.156,0.793,0.347])","2d9b183a":"train_data.head()","b5851a04":"for data in both:\n    data.drop('Name', axis=1, inplace=True)","11cd4bd2":"train_data.head()","ec2b9c15":"train_data['Cabin'].describe()","1ca5b790":"train_data.info()","16ee8d93":"train_data.drop('Cabin',axis=1, inplace=True)\ntest_data.drop('Cabin',axis=1,inplace=True)","d471689e":"ID = np.array(test_data.PassengerId)\ntrain_data.drop('PassengerId', axis=1, inplace=True)\ntest_data.drop('PassengerId', axis=1, inplace=True)","ab3d4659":"train_data.columns[train_data.isnull().any()]","6f39bb20":"test_data.columns[test_data.isnull().any()]","bd774975":"train_data['Age'].mode()","81e49eb0":"train_data.head()","2207f930":"train_data.drop('Ticket', axis=1, inplace=True)\ntest_data.drop('Ticket', axis=1, inplace=True)","51b02019":"train_data.head()","d1c5b131":"train_data['Age'].describe()","274c82a0":"train_data['Age'] = train_data['Age'].fillna(np.mean(train_data['Age']))","95bbe8ec":"train_data.columns[train_data.isnull().any()]","2d1ed0ec":"train_data[train_data['Embarked'].isnull()]","7e97699a":"train_data['Embarked'].value_counts()","c4363960":"train_data['Embarked'] = train_data['Embarked'].fillna('S')","19df4d18":"train_data.columns[train_data.isnull().any()]","d5f3ac90":"test_data['Age'] = test_data['Age'].fillna(np.mean(test_data['Age']))","61d29a00":"test_data.columns[test_data.isnull().any()]","c55e3476":"test_data[test_data.Fare.isnull()]","9aff0ab0":"test_data.Fare.describe()","74a8eef3":"test_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].mean())","bdaab10c":"train_data.columns","003aaa6a":"test_data.columns","f53e561f":"sns.heatmap(test_data.isnull(),yticklabels=False,cbar=True,cmap='viridis')","27e77a6e":"sns.heatmap(train_data.isnull(),yticklabels=False,cbar=True,cmap='viridis')","508aa2fc":"print(train_data.shape, test_data.shape)","61ea855a":"train_data = train_data.replace(to_replace=['female','male'],value=[1,0])\ntest_data = test_data.replace(to_replace=['female','male'],value=[1,0])","61b90bf0":"pd.crosstab(train_data['Embarked'], train_data['Survived'])","87aeda57":"train_data.Embarked= train_data.Embarked.replace(to_replace=['C','S','Q'], value=[1.24,0.51,0.63])\ntest_data.Embarked = test_data.Embarked.replace(to_replace=['C','S','Q'], value=[1.24,0.51,0.63])","0c8e31e4":"#num_features = [cat for cat in train_data.columns if train_data[cat].dtype in ['float64','int64']]\n#cat_features = [cat for cat in train_data.columns if train_data[cat].dtype not in ['float64','int64']]\n#print(cat_features,'\\n',num_features)","1b837648":"train_data.info()","65a255f8":"test_data.info()","32ab31bf":"train_data.shape","14c2c8fb":"valid_data = train_data[int(0.8 * train_data.shape[0]):]\nt_data = train_data[:int(0.8 * train_data.shape[0])]","3180932a":"t_data.info()","862e8a45":"train_data.info()","8b5df681":"import lightgbm as lgb\n\nfeature_cols = train_data.columns.drop('Survived')\ndtrain = lgb.Dataset(t_data[feature_cols], label=t_data['Survived'])\ndvalid = lgb.Dataset(valid_data[feature_cols], label=valid_data['Survived'])\n\nparam = {'num_leaves':64 , 'objective':'binary'}\nparam['metric'] = 'auc'\nnum_round = 1000\nbst = lgb.train(param, dtrain, num_round, valid_sets = [dvalid], early_stopping_rounds=10,verbose_eval=False)","5c702418":"y_pred = bst.predict(test_data[feature_cols])","47f8b953":"model = lgb.LGBMClassifier(learning_rate=0.05, num_leaves=61,n_estimators=1000)\nmodel.fit(train_data[feature_cols], train_data['Survived'])","1ac38846":"y_pred1 = model.predict(test_data)","6dd6bb4f":"x = [int(i+0.65) for i in y_pred]\nx[:3]","d979eb9e":"submit = pd.DataFrame({'PassengerId':ID, 'Survived':y_pred1})","b5bcd670":"submit.to_csv('submission.csv', index=False)","0146bcf3":"### We can see clearly that SibSp and Parch column have strong correlation. And Pclass and Fare , Pclass and Age, Age and SibSp columns have strong negative correlation","acd32000":"#### According to the survival rate according to the titles , We will convert this categorical values to numerical values.","1ccf7f1c":"## I don't think the Ticket column will have much effect on the Survival predictions. So We are going to drop this column.","f0956edf":"## Since Cabin have lot of null values , We are going to drop it.","e3e7a882":"### Now We can remove the Name column"}}