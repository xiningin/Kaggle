{"cell_type":{"6713eb68":"code","326b6b4f":"code","e47502e3":"code","42ab2d34":"code","4e61b67a":"code","1a5027c9":"code","a31de285":"code","3521f0ae":"code","5988f2ee":"code","ca160731":"code","1f35eeab":"code","4feb45b1":"code","8a758fe1":"code","6630e042":"code","f1b11c68":"code","3182a9e7":"code","7646c798":"code","2cf32882":"code","c1cab6c2":"code","8e8473f3":"code","d1a21762":"code","50d26a5f":"code","ba423f8d":"code","eec30283":"code","fc2e3c02":"code","9d66f5e7":"code","1f8774ac":"code","c9d92a09":"code","a2a24194":"code","45447f3c":"code","d751cd9e":"code","c9ea61a6":"code","45bdeb94":"code","f791307c":"code","040c678f":"code","dd276f9f":"markdown","c07c3074":"markdown","8d359666":"markdown","881f5779":"markdown","f95ac500":"markdown","5aa03ced":"markdown","f21efc57":"markdown","ab69c3ac":"markdown","19e6ecef":"markdown","478323f5":"markdown","d5a23b26":"markdown","ebdbeee7":"markdown","dfbcb7eb":"markdown","7c01fe51":"markdown","efb95b4b":"markdown","dfb2ac23":"markdown","bf9f834a":"markdown","1ed37aa6":"markdown","b423077d":"markdown","9d9658a4":"markdown","3bb51f86":"markdown","a320c566":"markdown","c394e441":"markdown","77b17ae2":"markdown","1714b658":"markdown","ff81eaca":"markdown","b31f41bc":"markdown","135c1d67":"markdown","e5b1a53b":"markdown","9def80fa":"markdown","90667b93":"markdown","ae855d69":"markdown","0dd0fb20":"markdown","cf2417e9":"markdown","86986f04":"markdown","5befdd3b":"markdown","239f6cbc":"markdown","231fb82f":"markdown","a87864db":"markdown","89d82e9b":"markdown","7573039c":"markdown","967b947f":"markdown","91c26396":"markdown","e2c192d9":"markdown","2ab7e0f2":"markdown","4deabdc1":"markdown","d24f2bb9":"markdown","ca3bd043":"markdown","38712b7b":"markdown","0d5dcc91":"markdown","2cf4ee90":"markdown","89d40fd0":"markdown","973416b2":"markdown","ae2c3931":"markdown"},"source":{"6713eb68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","326b6b4f":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-dataset\/heart.csv\")","e47502e3":"data.head()","42ab2d34":"data.info()","4e61b67a":"column_list = data.columns\nfor i in column_list:\n    print(\"Values of\",i,\"column\\n\",data[i].unique())\n    print(\"--------------\\n\")","1a5027c9":"g = sns.FacetGrid(data, col = \"target\")\ng.map(sns.distplot, \"age\", bins = 25)\nplt.show()","a31de285":"# Let's check the relationship of each column with heart disease\n# we cant visualize each column, that would be meaningless so I'll create new list.\ncolumn_list2 = [\"sex\",\"cp\",\"fbs\",\"restecg\",\"exang\",\"slope\",\"ca\",\"thal\"]\n\ng = sns.factorplot(x = column_list2[0], y = \"target\", data = data, kind = \"bar\",size=5)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[0])\nplt.show()","3521f0ae":"g = sns.factorplot(x = column_list2[1], y = \"target\", data = data, kind = \"bar\",size=5)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[1])\nplt.show()","5988f2ee":"g = sns.factorplot(x = column_list2[2], y = \"target\", data = data, kind = \"bar\",size=5)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[2])\nplt.show()","ca160731":"g = sns.factorplot(x = column_list2[3], y = \"target\", data = data, kind = \"bar\",size=5)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[3])\nplt.show()","1f35eeab":"g = sns.factorplot(x = column_list2[4], y = \"target\", data = data, kind = \"bar\",size=5)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[4])\nplt.show()","4feb45b1":"g = sns.factorplot(x = column_list2[5], y = \"target\", data = data, kind = \"bar\",size=5)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[5])\nplt.show()","8a758fe1":"g = sns.factorplot(x = column_list2[6], y = \"target\", data = data, kind = \"bar\",size=7)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[6])\nplt.show()","6630e042":"g = sns.factorplot(x = column_list2[7], y = \"target\", data = data, kind = \"bar\",size=6)\ng.set_ylabels(\"Heart Disase Probability\")\ng.set_xlabels(column_list2[7])\nplt.show()","f1b11c68":"plt.figure(figsize=(15,10))\nsns.heatmap(data[column_list2].corr(), annot = True, fmt = \".2f\")\nplt.show()","3182a9e7":"dummy_list = [\"sex\",\"cp\",\"restecg\",\"exang\",\"slope\",\"thal\"]\ndata = pd.get_dummies(data,columns=dummy_list)\ndata.head()","7646c798":"# Import Machine Learning Libraries\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","2cf32882":"y = data.target.values\nx_data = data.drop([\"target\"],axis=1)","c1cab6c2":"# big values can be dominated low values so we use normalization method \nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))","8e8473f3":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 42)\nprint(\"x_train\",len(x_train))\nprint(\"x_test\",len(x_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))","d1a21762":"random_state = 42\nclassifier = [KNeighborsClassifier(),\n              SVC(random_state = random_state,probability=True),\n              DecisionTreeClassifier(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             ]\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nsvm_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\n\nclassifier_param = [knn_param_grid,\n                   svm_param_grid,\n                    dt_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","50d26a5f":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(x_train,y_train)\n    cv_result.append(clf.best_score_ * 100)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","ba423f8d":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[ \"KNeighborsClassifier\", \"SVM\",\"Decision Tree Classifier\",\n             \"Random Forest Classifier\",\"LogisticRegression\",\n            ]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","eec30283":"# We must detect the best k value of knn model so we will determine best_estimators list\nbest_estimators","fc2e3c02":"# We must find predicted values of each models. After that, we will compare with real values.\nknn9 = KNeighborsClassifier(n_neighbors = 9)\nknn9.fit(x_train, y_train)\ny_head_knn = knn9.predict(x_test)\n\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\ny_head_svm = svm.predict(x_test)\n\nrf = RandomForestClassifier(n_estimators = 500, random_state = 1)\nrf.fit(x_train,y_train)\ny_head_rf = rf.predict(x_test)","9d66f5e7":"# We find confusion matrix of models below.\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_svm = confusion_matrix(y_test,y_head_svm)\ncm_rf = confusion_matrix(y_test,y_head_rf)\n\n# Let's visualize them\nplt.figure(figsize=(12,6))\nplt.suptitle(\"Confusion Matrices\",fontsize=24, color=\"red\")\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(1,3,1)\nplt.title(\"K Nearest Neighbors Confusion Matrix\",fontsize=10,color=\"blue\")\nsns.heatmap(cm_knn,annot=True, cmap=\"YlGnBu\",fmt=\"d\",cbar=False, annot_kws={\"size\": 18})\n\nplt.subplot(1,3,2)\nplt.title(\"Support Vector Machine Confusion Matrix\",fontsize=10,color=\"blue\")\nsns.heatmap(cm_svm,annot=True, cmap=\"YlGnBu\",fmt=\"d\",cbar=False, annot_kws={\"size\": 18})\n\nplt.subplot(1,3,3)\nplt.title(\"Random Forest Confusion Matrix\",fontsize=10,color=\"blue\")\nsns.heatmap(cm_svm,annot=True, cmap=\"YlGnBu\",fmt=\"d\",cbar=False, annot_kws={\"size\": 18})\n\nplt.show()\n","1f8774ac":"votingC = VotingClassifier(estimators = [(\"knn\",best_estimators[0]),\n                                        (\"svm\",best_estimators[1]),\n                                        (\"rf\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(x_train, y_train)\nprint(\"Accuracy of Ensemble: {:.2f}\".format(accuracy_score(votingC.predict(x_test),y_test)*100))","c9d92a09":"best_accuracies_each_classes = {}\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\naccuracy_lr_train = round(lr.score(x_train, y_train)*100,2) \naccuracy_lr_test = round(lr.score(x_test,y_test)*100,2)\nbest_accuracies_each_classes[\"Logistic Regression\"] = lr.score(x_test,y_test)*100\nprint(\"Training Accuracy: {}%\".format(accuracy_lr_train))\nprint(\"Testing Accuracy: {}%\".format(accuracy_lr_test))\n","a2a24194":"\nscore_list_test = []\nfor i in range(1,21):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(x_train,y_train)\n    score_list_test.append(knn.score(x_test,y_test))\n    \nbest_accuracies_each_classes[\"KNN\"] = max(score_list_test)*100\nprint(\"Best Test KNN Score accuracy is: {:.2f}\".format(max(score_list_test)*100))\n\nplt.figure(figsize=(15,5))\nplt.plot(range(1,21),score_list_test)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n","45447f3c":"svm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n\nbest_accuracies_each_classes[\"SVM\"] = svm.score(x_test,y_test)*100\nprint(\"Accuracy of SVM Algo: {:.2f}\".format(svm.score(x_test,y_test)*100))","d751cd9e":"nb = GaussianNB()\nnb.fit(x_train,y_train)\n\nbest_accuracies_each_classes[\"Naive Bayes\"] = nb.score(x_test,y_test)*100\nprint(\"Accuracy of Naive Bayes: {:.2f}\".format(nb.score(x_test,y_test)*100))","c9ea61a6":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nbest_accuracies_each_classes[\"Decision Tree\"] = dt.score(x_test,y_test)*100\nprint(\"Accuracy of Decision Tree: {:.2f}\".format(dt.score(x_test,y_test)*100))","45bdeb94":"\nrf = RandomForestClassifier(n_estimators = 500, random_state = 1)\nrf.fit(x_train,y_train)\n\nbest_accuracies_each_classes[\"Random Forest\"] = rf.score(x_test,y_test)*100\nprint(\"Accuracy of  Random Forest is: {:.2f}\".format(rf.score(x_test,y_test)*100))","f791307c":"plt.figure(figsize=(8,5))\nsns.barplot( y=list(best_accuracies_each_classes.keys()), x=list(best_accuracies_each_classes.values()))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Classification Methods\")\nplt.show()","040c678f":"# I did not sort models \u0131n an organized way so I filled by manually according to first graphic's sort.\n\ncv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[ \"KNeighborsClassifier\", \"SVM\",\"Decision Tree Classifier\",\n             \"Random Forest Classifier\",\"LogisticRegression\",\n            ]})\n\nplt.figure(figsize=(18,6))\nplt.suptitle(\"Comparing Models\",fontsize=24, color=\"red\")\nplt.subplots_adjust(wspace = 0.8, hspace= 0.4)\n\nplt.subplot(1,2,1)\nplt.title(\"With Parameter Grid\",fontsize=10,color=\"blue\")\nsns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\n\n\nplt.subplot(1,2,2)\nplt.title(\"Without Parameter Grid\",fontsize=10,color=\"blue\")\nsns.barplot( y=['KNeighborsClassifier',\n                 'SVM',\n                 'Decision Tree Classifier',\n                 'Random Forest Classifier',\n                 'LogisticRegression',\n                'Naive Bayes'] , x=[99.02597402597402,87.01298701298701,  98.05194805194806,98.05194805194806,80.84415584415584, 71.1038961038961])\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Classification Methods\")\nplt.show()","dd276f9f":"<a id=\"7\"><\/a> <br>\n## Hyperparameter Tuning & Grid Search & Cross Validation\nWe will compare 6 machine learning classifier and evaluate mean accuracy of each of them by stratified cross validation.\n\n* KNN\n* SVM\n* Naive Bayes\n* Decision Tree\n* Random Forest\n* Logistic Regression","c07c3074":"<a id=\"3\"><\/a> <br>\n# Feature Engineering","8d359666":"<a id=\"16\"><\/a> <br>\n## Decision Tree Classification","881f5779":"We investigated columns of heart diseases data and recognized that cp, restecg, exang, slope, thal are appropriate for machine learning model. \nSo we will split them as binomial.","f95ac500":"for best estimating ratings, we see that value of k is 9.","5aa03ced":"<a id=\"17\"><\/a> <br>\n## Random Forest Classification\n","f21efc57":"<a id=\"11\"><\/a> <br>\n## Also I wanted to create models without using grid parameters because I wanted to see that how much the result may change","ab69c3ac":"Our result are almost same. We can use these 3 models for ensembling.","19e6ecef":"0 is female, 1 is male so, we estimate that females have more heart disease probability according to males.","478323f5":"### As you can see, using parameter grid is important in terms of obtaining good results.","d5a23b26":"<a id=\"14\"><\/a> <br>\n## SVM (Support Vector Machine Classification)","ebdbeee7":"**SVM Algorithm**\n<img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png\"\/>","dfbcb7eb":"**Naive Bayes Algorithm**\n<img src=\"https:\/\/www.codespeedy.com\/wp-content\/uploads\/2019\/06\/Formula-of-Naive-Bayes-theory.png\"\/>","7c01fe51":"<a id=\"6\"><\/a> <br>\n## Train-Test Split","efb95b4b":"<a id=\"1\"><\/a> <br>\n# Reading Data","dfb2ac23":"<a id=\"13\"><\/a> <br>\n## KNN (K Nearest Classification)","bf9f834a":"<a id=\"8\"><\/a> <br>\n# Comparing Classifications","1ed37aa6":"## <font color=\"red\">Test Accuracy of KNN is: 99.03%","b423077d":"<a id=\"18\"><\/a> <br>\n# Comparing New Classifications","9d9658a4":"<a id=\"9\"><\/a> <br>\n# Confusion Matrix","3bb51f86":"it seems like people who have slope 2 are more likely to have a haert diseases.","a320c566":"it seems like people who have fbs 0 or 1 is not so important for having heart disease.","c394e441":"****KNN Algorithm****\n\n<img src=\"https:\/\/d1jnx9ba8s6j9r.cloudfront.net\/blog\/wp-content\/uploads\/2018\/07\/KNN-Algorithm-k3-edureka-437x300.png\"\/>","77b17ae2":"## <font color=\"blue\"> I'm new at kaggle platform and being data scientist. Please give feedback to me to improve myself.","1714b658":"<a id=\"19\"><\/a> <br>\n# Comparing Models","ff81eaca":"## <font color=\"red\">Test Accuracy of Decision Tree is: 97.08%","b31f41bc":"Generally, people who are > 60 y.o have more heart disease probability. \nMost patients are in 55-65 age range.","135c1d67":"* We must determine a threshold for best results, if we look the graph, we can select KNN, SVM and Random Forest models.\n* But, before the selection we must look the confusion matrices for this models.","e5b1a53b":"we see that people who have cp value is 1,2 or 3 have more heart disease probability.","9def80fa":"<a id=\"20\"><\/a> <br>\n# Conclusion\n\n## As a result, we found accuracy of ensemble 99.03%\n### [Accuracy of Ensemble](#10)","90667b93":"It seems like people who have thal 2are more likely to have a heart diseases.","ae855d69":"**Decision Tree Algorithm**\n<img src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/qa.png\"\/>","0dd0fb20":"# Introduction\n* This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. \n\n* The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.\n* That means we will predict target as 1 or 0\n\n\n## Context List\n1. [Reading Data](#1)\n1. [Basic Skimming on Data with Visualization](#2)\n1. [Feature Engineering](#3)\n1. [Creating Machine Learning Model](#4)\n    * [Normalization](#5)\n    * [Train - Test Split](#6)\n1. [Hyperparameter Tuning & Grid Search & Cross Validation](#7)\n1. [Comparing Classifications](#8)\n1. [Confusion Matrix](#9)\n1. [Ensemble Modelling](#10)\n1. [Different Type of Modelling](#11)\n1. [Simple Logistic Regression](#12)\n1. [KNN (K Nearest Classification](#13)\n1. [SVM (Support Vector Machine Classification](#14)\n1. [Naive Bayes Classification](#15)\n1. [Decision Tree Classification](#16)\n1. [Random Forest Classification](#17)\n1. [Comparing New Classifications](#18)\n1. [Comparing Models](#19)\n1. [Conclusion](#20)","cf2417e9":"<a id=\"2\"><\/a> <br>\n# Basic Skimming on Data","86986f04":"## <font color=\"red\">Test Accuracy of Ensemble is: 99.03%","5befdd3b":"<a id=\"12\"><\/a> <br>\n## Simple Logistic Regression","239f6cbc":"<a id=\"5\"><\/a> <br>\n## Normalization","231fb82f":"It seems like people who have ca 0 and 4 are more likely to have a heart diseases","a87864db":"# ===============================================\n# MEANS OF COLUMNS\n1. age: age \n2. sex: sex  (1 = male; 0 = female)\n3. cp: chest pain type (4 values) \n4. trestbps: resting blood pressure  \n5. chol: serum cholestoral in mg\/dl \n6. fbs: fasting blood sugar > 120 mg\/dl \n7. restecg: resting electrocardiographic results (values 0,1,2) \n8. thalach: maximum heart rate achieved \n9. exang: exercise induced angina\n10. oldpeak: oldpeak = ST depression induced by exercise relative to rest \n11. slope: the slope of the peak exercise ST segment \n12. ca: number of major vessels (0-3) colored by flourosopy \n13. thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n14. target: 1 or 0 \n\n# ===============================================","89d82e9b":"We found 99.03% Accuracy, I think that is so good rate.","7573039c":"we see that there is no nan data, but maybe we have tidy data so let's check it.","967b947f":"<a id=\"4\"><\/a> <br>\n# Creating Machine Learning Model","91c26396":"it seems like people who have exang 0 are more likely to have a haert diseases","e2c192d9":"it seems that there is no tidy data like (0, --, X) so our data is good for machine learning modelling.","2ab7e0f2":"it seems like people who have restecgvalue 0-1 are more likely to have a haert diseases (especially 1)","4deabdc1":"## <font color=\"red\">Test Accuracy of Linear Regreession is: 80.84%","d24f2bb9":"## <font color=\"red\">Test Accuracy of Random Forest is: 98.05%","ca3bd043":"**Random Forest Algorithm**\n<img src=\"https:\/\/www.researchgate.net\/publication\/316982197\/figure\/fig5\/AS:559887665303554@1510499029585\/The-structure-of-random-forest-algorithm-The-random-forest-is-composed-of-the-generated.png\"\/>","38712b7b":"It seems like there is no too much correlation between features.","0d5dcc91":"<a id=\"10\"><\/a> <br>\n# Ensemble Modeling","2cf4ee90":"### Spliting Dummy Variables ","89d40fd0":"## <font color=\"red\">Test Accuracy of SVM is: 87.01%","973416b2":"## <font color=\"red\">Test Accuracy of Naive Bayes is: 71.10%","ae2c3931":"<a id=\"15\"><\/a> <br>\n## Naive Bayes Classification"}}