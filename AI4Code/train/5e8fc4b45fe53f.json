{"cell_type":{"00c1fab1":"code","2f4ce600":"code","3d214436":"code","c4f94576":"code","934c7f7a":"code","a8045b61":"code","59590985":"code","38351ee6":"code","adc7688e":"code","330570ff":"code","8e261cd7":"code","86a512ba":"code","246a378f":"code","76dab5d6":"code","0c551045":"code","152ac0e4":"code","c70d27b6":"code","eac2f1df":"code","6d06d86b":"code","f30020bc":"code","cf0be39a":"code","88664422":"code","329c52eb":"code","1734e36c":"code","f061a45c":"code","0f68c08b":"code","0d08fe68":"code","0435b34d":"code","55d25edf":"code","5bf06c47":"code","c42d4a76":"code","ca7befcd":"code","0a227f15":"code","b4110a0c":"code","e7fd9921":"code","1e33439f":"code","101cca0b":"code","85c98d18":"code","ab1fd783":"code","2df2482d":"code","2d8beab7":"code","e4d49e5f":"code","13a47049":"code","bdd61e12":"code","d82fbcd7":"code","d97e2809":"code","5dd58605":"code","f6c9d9ff":"code","51c4be31":"code","4fa24a58":"code","5fa1f345":"code","bf05f693":"code","ac2bdb13":"code","7b1d8507":"markdown","cc078544":"markdown","f1f0b700":"markdown","9e8d6f2b":"markdown","818a588e":"markdown","5de12883":"markdown","fbb5a498":"markdown","b280cbbc":"markdown","593b01d1":"markdown","8f4c79a3":"markdown","c88e0d0d":"markdown","904781ec":"markdown","c8445e3c":"markdown","b8a34b10":"markdown"},"source":{"00c1fab1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f4ce600":"from IPython.display import Image\nimport os\n!ls ..\/input\/\nImage(\"\/kaggle\/input\/turbo-fan-picture\/turbo fan picture.PNG\")","3d214436":"import pandas as pd\ndata_train = pd.read_csv(\"\/kaggle\/input\/nasa-cmaps\/cmaps\/CMaps\/train_FD001.txt\",sep=\" \",header=None)\ndata_test = pd.read_csv(\"\/kaggle\/input\/nasa-cmaps\/cmaps\/CMaps\/test_FD001.txt\",sep=\" \",header=None)\ndata_RUL = pd.read_csv(\"\/kaggle\/input\/nasa-cmaps\/cmaps\/CMaps\/RUL_FD001.txt\",sep=\" \",header=None)\ndata_train","c4f94576":"data_test","934c7f7a":"data_RUL","a8045b61":"#delete Nan Value\n\ndata_train.drop(columns=[26,27],inplace=True)\ndata_test.drop(columns=[26,27],inplace=True)\ndata_RUL.drop(columns=[1],inplace=True)","59590985":"Image(\"\/kaggle\/input\/variable-independent\/variable 1.PNG\")","38351ee6":"Image(\"\/kaggle\/input\/variable-independent\/variable 2.PNG\")","adc7688e":"#data train Column Labelling from table\n#setting 1,2,3 turbo fan only for setup when running\n\ncolumns_train = ['unit_ID','cycles','setting_1','setting_2','setting_3','T2','T24','T30','T50','P2','P15','P30','Nf',\n           'Nc','epr','Ps30','phi','NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32' ]\ndata_train.columns = columns_train\ndata_train.describe()","330570ff":"#sort data about unit_ID and cycles\n\ndata_train_cycles = data_train.loc[:, 'unit_ID':'cycles']\ndata_train_cycles","8e261cd7":"print('unit_ID : ',data_train.unit_ID.unique())","86a512ba":"#sort data unit_ID get maximum cycles or how to know when machine failure\n\ndata_train_cycles_failure = pd.DataFrame(data_train.groupby('unit_ID')['cycles'].max()).reset_index()\ndata_train_cycles_failure.columns = ['unit_ID', 'failure']\ndata_train_cycles_failure","246a378f":"#merge failure data and RUL data to dataset\n\ndata_train=data_train.merge(data_train_cycles_failure,on=['unit_ID'],how='left')\ndata_train['RUL']=data_train['failure']-data_train[\"cycles\"]\ndata_train","76dab5d6":"#data test column labelling\n\ncolumns_test = ['unit_ID','cycles','setting_1','setting_2','setting_3','T2','T24','T30','T50','P2','P15','P30','Nf',\n           'Nc','epr','Ps30','phi','NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32' ]\ndata_test.columns = columns_test\ndata_test.describe()","0c551045":"#data RUL column labelling\n\ncolumns_RUL = ['RUL']\ndata_RUL.columns = columns_RUL\ndata_RUL.insert(0, 'unit_ID', range(1, 1 + len(data_RUL)))\ndata_RUL","152ac0e4":"#sort data unit_ID get maximum cycles\n\ndata_test_cycles_MAX = pd.DataFrame(data_test.groupby('unit_ID')['cycles'].max()).reset_index()\ndata_test_cycles_MAX.columns = ['unit_ID', 'cycles MAX']\ndata_test_cycles_MAX","c70d27b6":"data_test=data_test.merge(data_RUL,on=['unit_ID'],how='left')\ndata_test=data_test.merge(data_test_cycles_MAX,on=['unit_ID'],how='left')\ndata_test['failure']=data_test['cycles MAX']+data_test['RUL']\ndata_test","eac2f1df":"#sort data unit_ID get best performance or most long time failure from data train\n\ndata_train_cycles.loc[data_train_cycles['cycles'].idxmax()]","6d06d86b":"#sort data unit_ID 69 from start until failure\n\nunit_ID_69 = data_train.loc[13631:13992, ['unit_ID','cycles','setting_1','setting_2','setting_3','T2','T24','T30','T50','P2','P15','P30','Nf',\n           'Nc','epr','Ps30','phi','NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32', 'failure','RUL']]\nunit_ID_69","f30020bc":"#check distribution plot setting_1, setting_2, setting_2 with cycles\n\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n\nsns.set()\ncols = ['setting_1','setting_2','setting_3','failure', 'cycles']\nsns.pairplot(unit_ID_69[cols], size = 3)\nplt.show();","cf0be39a":"# check temperature correlation\n\nsns.set()\ncols = ['T2','T24','T30','T50', 'cycles']\nsns.pairplot(unit_ID_69[cols], size = 3)\nplt.show();","88664422":"# check pressure correlation\n\nsns.set()\ncols = ['P2','P15','P30','Ps30', 'cycles']\nsns.pairplot(unit_ID_69[cols], size = 3)\nplt.show();","329c52eb":"Image('\/kaggle\/input\/pressure-total\/pressure total.png')","1734e36c":"# check rotation correlation\n\nsns.set()\ncols = ['Nf','Nc','NRf','NRc','Nf_dmd','PCNfR_dmd', 'cycles']\nsns.pairplot(unit_ID_69[cols], size = 3)\nplt.show();","f061a45c":"# check coolant flow correlation\n\nsns.set()\ncols = ['W31','W32', 'cycles']\nsns.pairplot(unit_ID_69[cols], size = 3)\nplt.show();","0f68c08b":"# check correlation of variable independent without unit \n\nsns.set()\ncols = ['epr','BPR', 'farB', 'htBleed','phi', 'cycles']\nsns.pairplot(unit_ID_69[cols], size = 3)\nplt.show();","0d08fe68":"#missing data\ntotal = data_train.isnull().sum().sort_values(ascending=False)\npercent = (data_train.isnull().sum()\/data_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","0435b34d":"# create X (features) and y (response)\n# only choose feature which has a change in value with engine failure\nSelected_features = ['T24','T30','T50','P30','Ps30','Nf','Nc','NRf','NRc','W31','W32','BPR','htBleed','phi']\n\nX = data_train[Selected_features]\ny = data_train['cycles']\n\nfrom sklearn.model_selection import train_test_split\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","55d25edf":"# Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nlogistic_model = LogisticRegression(solver='liblinear')\n# Fit the classifier to the training data  \nlogistic_model.fit(X_train,y_train)\n#Training Model: Predict \nlogistic_predictions = logistic_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('logistic Accuracy train :', logistic_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('logistic absolute error:', mean_absolute_error(y_valid, logistic_predictions))\nprint('logistic square error:', mean_squared_error(y_valid, logistic_predictions)) ","5bf06c47":"# Support Vector Machines\nfrom sklearn.svm import SVC, LinearSVC\n\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)\nsvc_predictions = svc_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', svc_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, svc_predictions))\nprint('square error:', mean_squared_error(y_valid, svc_predictions)) ","c42d4a76":"# DecisionTree Regression\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Define model. Specify a number for random_state to ensure same results each run\nTree_model = DecisionTreeRegressor()\n\n# Fit model\nTree_model.fit(X, y)\nTree_predictions = Tree_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', Tree_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, Tree_predictions))\nprint('square error:', mean_squared_error(y_valid, Tree_predictions)) ","ca7befcd":"#underfiitng and overfitting\ndef get_mae(max_leaf_nodes, X_train, X_valid, y_train, y_valid):\n    Tree_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    Tree_model.fit(X_train, y_train)\n    Tree_predictions = Tree_model.predict(X_valid)\n    mae = mean_absolute_error(y_valid, Tree_predictions)\n    return(mae)","0a227f15":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5,10, 50, 250, 500]:\n    my_mae = get_mae(max_leaf_nodes, X_train, X_valid, y_train, y_valid)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","b4110a0c":"#random forest regression\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nForest_model = RandomForestRegressor(n_estimators=225, random_state=0)\nForest_model.fit(X_train, y_train)\nForest_predictions = Forest_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', Forest_model.score(X_train, y_train))\n\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, Forest_predictions))\nprint('square error:', mean_squared_error(y_valid, Tree_predictions)) ","e7fd9921":"#Xgboost Regression\n\nfrom xgboost import XGBRegressor\n\nXGB_model = XGBRegressor()\nXGB_model.fit(X_train, y_train)\nXGB_predictions = XGB_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', XGB_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, XGB_predictions))\nprint('square error:', mean_squared_error(y_valid, XGB_predictions)) ","1e33439f":"XGB_model = XGBRegressor(n_estimators=100)\nXGB_model.fit(X_train, y_train)\nXGB_predictions = XGB_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', XGB_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, XGB_predictions))\nprint('square error:', mean_squared_error(y_valid, XGB_predictions))","101cca0b":"XGB_model = XGBRegressor(n_estimators=100)\nXGB_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\nXGB_predictions = XGB_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', XGB_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, XGB_predictions))\nprint('square error:', mean_squared_error(y_valid, XGB_predictions))","85c98d18":"XGB_model = XGBRegressor(n_estimators=100, learning_rate=0.05)\nXGB_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\nXGB_predictions = XGB_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', XGB_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, XGB_predictions))\nprint('square error:', mean_squared_error(y_valid, XGB_predictions))","ab1fd783":"XGB_model = XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=100)\nXGB_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\nXGB_predictions = XGB_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', XGB_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, XGB_predictions))\nprint('square error:', mean_squared_error(y_valid, XGB_predictions))","2df2482d":"# k-Nearest Neighbors\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors = 3)\nknn_model.fit(X_train, y_train)\nknn_predictions = knn_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', knn_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, knn_predictions))\nprint('square error:', mean_squared_error(y_valid, knn_predictions))","2d8beab7":"# Naive Bayes classifier\n\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian_model = GaussianNB()\ngaussian_model.fit(X_train, y_train)\ngaussian_predictions = gaussian_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', gaussian_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, gaussian_predictions))\nprint('square error:', mean_squared_error(y_valid, gaussian_predictions))","e4d49e5f":"# Perceptron\n\nfrom sklearn.linear_model import Perceptron\n\nperceptron_model = Perceptron()\nperceptron_model.fit(X_train, y_train)\nperceptron_predictions = perceptron_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', perceptron_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, perceptron_predictions))\nprint('square error:', mean_squared_error(y_valid, perceptron_predictions))","13a47049":"# Linear SVC\n\nfrom sklearn.svm import SVC, LinearSVC\n\nLinearSVC_model = LinearSVC()\nLinearSVC_model.fit(X_train, y_train)\nLinearSVC_predictions = LinearSVC_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('Accuracy train :', LinearSVC_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('error:', mean_absolute_error(y_valid, LinearSVC_predictions))\nprint('square error:', mean_squared_error(y_valid, LinearSVC_predictions))","bdd61e12":"# Stochastic Gradient Descent\n\nfrom sklearn.linear_model import SGDClassifier\n\nSGDClassifier_model = SGDClassifier()\nSGDClassifier_model.fit(X_train, y_train)\nSGDClassifier_predictions = SGDClassifier_model.predict(X_valid)\n\n#Evaluate Model Performance\nprint('SGDClassifier Accuracy train :', SGDClassifier_model.score(X_train, y_train))\n\n# Evaluate Error\nprint('SGDClassifier error:', mean_absolute_error(y_valid, SGDClassifier_predictions))\nprint('square error:', mean_squared_error(y_valid, SGDClassifier_predictions))","d82fbcd7":"# predict the number of remaining operational cycles before failure in the test set\n# use random forest modelling with Accuracy train : 0.9456401296365164, Accuracy valid : 0.6036809414130251, error: 33.73317564947655\ndata_test","d97e2809":"#missing data\ntotal = data_test.isnull().sum().sort_values(ascending=False)\npercent = (data_test.isnull().sum()\/data_test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","5dd58605":"# create X (features) and y (response) data_test\n\nSelected_features_test =  ['T24','T30','T50','P30','Ps30','Nf','Nc','NRf','NRc','W31','W32','BPR','htBleed','phi']\nX_test = data_test[Selected_features_test]\ny_test = data_test['cycles']","f6c9d9ff":"#random forest regression data_test\n\nForest_model_test = RandomForestRegressor(n_estimators=225, random_state=0)\nForest_model_test.fit(X_test, y_test)\nForest_test_predictions = Forest_model.predict(X_test)","51c4be31":"# predictions cycle output data_test\n\noutput_test = pd.DataFrame({'unit_ID': data_test.unit_ID,\n                       'cycles': y_test})\noutput_test.to_csv('cycles_test.csv', index=False)\noutput_test","4fa24a58":"#sort data unit_ID get maximum cycles\n\noutput_test_cycles_MAX = pd.DataFrame(output_test.groupby('unit_ID')['cycles'].max()).reset_index()\noutput_test_cycles_MAX.columns = ['unit_ID', 'cycles MAX']","5fa1f345":"# predictions cycle MAX output data_test\noutput_test_cycles_MAX.to_csv('cycles_MAX_test.csv', index=False)\noutput_test_cycles_MAX","bf05f693":"# Define model. Specify a number for random_state to ensure same results each run\nimport numpy as np\nSelected_features = ['T24','T30','T50','P30','Ps30','Nf','Nc','NRf','NRc','W31','W32','BPR','htBleed','phi']\n\nX = data_train[Selected_features]\ny = data_train['failure']\n\nForest_model_train = RandomForestRegressor(n_estimators=225, random_state=0)\nForest_model_train.fit(X, y)\nForest_train_predictions = Forest_model.predict(X)\n\ntrain_output = pd.DataFrame({'unit_ID': data_train.unit_ID,\n                       'failure': Forest_train_predictions})\ntrain_output.to_csv('submission_train.csv', index=False)\ntrain_output = pd.DataFrame(train_output.groupby('unit_ID')['failure'].max()).reset_index()\ntrain_output['time_to_maintenance']=train_output['failure']*0.8\ntrain_output['time_to_maintenance']=train_output['time_to_maintenance'].apply(np.ceil)\ntrain_output","ac2bdb13":"test_output = pd.DataFrame({'unit_ID': data_test.unit_ID,\n                       'failure': data_test.failure})\ntest_output.to_csv('submission_test.csv', index=False)\ntest_output = pd.DataFrame(test_output.groupby('unit_ID')['failure'].max()).reset_index()\ntest_output['time_to_maintenance']=test_output['failure']*0.8\ntest_output['time_to_maintenance']=test_output['time_to_maintenance'].apply(np.ceil)\ntest_output","7b1d8507":"W31 and W32 have decreased this indicates that the coolant flow rate has decreased until engine failure occurs","cc078544":"- epr and farB have a stagnant distribution for cycles until the system breaks down\n- BPR and htBleed have increased until there is damage to the engine\n- phi decreased until there was damage to the engine. can be an indicator of damage to the engine","f1f0b700":"This analysis use data :\n\n* Data Set: FD001\n* Train trjectories: 100\n* Test trajectories: 100\n* Conditions: ONE (Sea Level)\n* Fault Modes: ONE (HPC Degradation)\n* \nExperimental Scenario\n\nData sets consists of multiple multivariate time series. Each data set is further divided into training and test subsets. Each time series is from a different engine \ufffd i.e., the data can be considered to be from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The data is contaminated with sensor noise.\n\nThe data are provided as a zip-compressed text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:\n1)\tunit number\n2)\ttime, in cycles\n3)\toperational setting 1\n4)\toperational setting 2\n5)\toperational setting 3\n6)\tsensor measurement  1\n7)\tsensor measurement  2\n...\n26)\tsensor measurement  26\n\nhere provided 3 types of data in the form of text. i.e. train data, test data, RUL data.\n1. train data describes the run to failure \/ breakdown. \n    if maintenance is done from here it means \"breakdown maintenance\". The disadvantage of breakdown maintenance is a loss in profit and time. where the engine was supposed to work, it suddenly stopped and had to be repaired\n\n2. test data describes the engine has stopped before failure \/ breakdown. \n    if maintenance is carried out from here it means \"preventive maintenance\". the loss of preventive maintenance is a loss in the cost of spare parts. where engine spare parts that should still be able to work and the remaining lifetime is still long must be replaced immediately\n3. RUL data describes the rest of the engine can still operate after the engine stops during test data\n\nof the three data above will be analyzed when is the right time to do predictive maintenance. so that the maintenance process can be more measurable in terms of cost of funds without losing time\n\nReference: A. Saxena, K. Goebel, D. Simon, and N. Eklund, \ufffdDamage Propagation Modeling for Aircraft Engine Run-to-Failure Simulation\ufffd, in the Proceedings of the Ist International Conference on Prognostics and Health Management (PHM08), Denver CO, Oct 2008.\n","9e8d6f2b":"**By using the Pareto principle, the engine with a lifetime of 80% is time for maintenance**","818a588e":"Static pressure is pressure on the edge of the container so it does not change with the velocity of the fluid direction. while the total pressure is in the middle of the fluid flow so that it is influenced by the fluid velocity which affects the dynamic pressure changes","5de12883":"because settings 1,2,3 and failure have no correlation with changes to the cycle, it can be ignored","fbb5a498":"- PCNfR_dmd is demanded corrected fan speed and Nf_dmd is demanded fan speed with stable value because it is only the value of rotation input to fan. so even if the engine is damaged and the system doesn't work, the input rotation on the fan is always the same\n\n- NRf is corrected fan speed and Nf famn speed, increasing rpm until engine damage occurs\n\n- NRc is corrected core speed and Nc is core spreed, the value decreases until engine failure occurs. this shows an indication of a turbo fan engine malfunction that can only be detected from the inside","b280cbbc":"# 2. by data test","593b01d1":"# 1. by data train","8f4c79a3":"![](https:\/\/mmk-web-m3scn.s3.cn-north-1.amazonaws.com.cn\/sites\/default\/files\/inline-images\/pareto-rule-explained_0.jpg)","c88e0d0d":"# schedule for maintenance according to predictions, and use pareto principle","904781ec":"- T2: no changes to the cycle. because he is the total temperature of the fan inlet, thus indicating that the temperature outside the engine system is always stable\n\n- T24, T30, T50 : changes to the cycle. because it is the total temperature of LPC outlet, HPC outlet, LPT outlet. thus indicating that the temperature of the fluid after entering the engine system, whether LPC, HPC, or LPT, has an increase in temperature\n- so it can be concluded that the peak of engine damage \/ failure \/ breakdown is the temperature has increased","c8445e3c":"reference :\n- Syahrul Bahar Hamdani,2019 : [PREDICTIVE MAINTENANCE OF AIRCRAFT ENGINES WITH MACHINE LEARNING APPROACH](https:\/\/digilib.itb.ac.id\/index.php\/gdl\/view\/35771)\n- Manav Sehgal : [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)","b8a34b10":"why between P30 and Ps30 have a different correlation distribution to the cycle? and why did P2 and P15 not change?\n\n- P2 is pressure and fan inlet, and P15 is total pressure in bypass duct. both are pressure outside the engine system and the value does not change, this shows that the environmental pressure or outside the engine system is in a stable condition where in the FD001 data at sea level.\n\n- P30 is total pressure at HPC outlet, while Ps30 is static pressure at HPC outlet. The difference between total pressure and static pressure can be explained in the following picture"}}