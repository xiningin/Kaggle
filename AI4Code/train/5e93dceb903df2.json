{"cell_type":{"8758726f":"code","9eacb0dd":"code","ab09a82a":"code","9f45eb27":"code","4b5fc728":"code","0413bba1":"code","80b7a9b4":"code","8bc0e6e1":"code","e810977c":"code","4c43fa37":"code","35611a11":"code","ad6de013":"code","b9a2a0d1":"code","cc6dee45":"code","99d4d88d":"code","06552718":"code","8ff944ef":"code","078f7e4b":"code","4ba9d5f2":"code","ff86e2e1":"code","de2570e5":"code","d2dafbb6":"code","fcd0e13f":"code","8a642956":"code","9fa79e3e":"code","5c131d22":"code","05ceacd6":"code","41d9b1c2":"code","58a99b33":"code","c1726357":"code","766b286e":"code","79b24f4a":"code","0579a7df":"code","7ff19178":"code","163eb9ae":"code","d953d469":"code","54b1f314":"code","2413d1a8":"code","0060abcb":"markdown","156cef0b":"markdown","62ae8fff":"markdown","595d9976":"markdown","451b294d":"markdown","98a5392e":"markdown","898835a4":"markdown","07a6fda5":"markdown","d973fd34":"markdown","8f014b89":"markdown","75a87722":"markdown","097cda5f":"markdown","1757aa30":"markdown","3b0ec7a6":"markdown","2629a293":"markdown"},"source":{"8758726f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9eacb0dd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ab09a82a":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf","9f45eb27":"df.shape","4b5fc728":"df.info()","0413bba1":"df.describe()","80b7a9b4":"df.isnull().sum()","8bc0e6e1":"sel_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']\n\ncorr = df[df.columns].corr()\nsns.heatmap(corr, annot=True)","e810977c":"g = sns.PairGrid(df[sel_cols])\ng.map(sns.scatterplot)","4c43fa37":"for col in df.select_dtypes(include=np.number).columns:\n    plt.figure(figsize=(6,3))\n    sns.boxplot(x=col, data=df)\n    plt.show()","35611a11":"for col in df.select_dtypes(include=np.object):\n    sns.catplot(x=col, data=df, col='HeartDisease', kind='count', height=3)","ad6de013":"for col in ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']:\n    plt.figure(figsize=(10,4))\n    plt.title(f'Heart Disease {col} Distribution\\n', fontsize=18)\n\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.xlabel(f'{col}\\n\\n', fontsize=14)\n    plt.ylabel('Count', fontsize=14)\n\n    sns.histplot(data=df, x=col, hue='HeartDisease',multiple='dodge', shrink=0.6, kde=True)","b9a2a0d1":"class Clean:\n    def __init__(self, dataframe, features):\n        self.dataframe = dataframe\n        self.columns = features\n        \n    def get_outliers(self):\n        final_indexes = list()\n        \n        for col in self.columns:\n            Q1 = self.dataframe[col].quantile(0.25)\n            Q3 = self.dataframe[col].quantile(0.75)\n            IQR = Q3 - Q1\n            \n            lower_bound = Q1 - (1.5 * IQR)\n            upper_bound = Q3 + (1.5 * IQR)\n        \n            outliers_indexes = self.dataframe.index[(self.dataframe[col] < lower_bound) | (self.dataframe[col] > upper_bound)]\n            for index in outliers_indexes:\n                if index not in final_indexes:\n                    final_indexes.append(index)\n                    \n        return final_indexes\n               \n        \n    def remove_outliers(self):\n        remove_indexes = self.get_outliers()\n        currated_dataframe = self.dataframe.drop(self.dataframe.index[[remove_indexes]])\n        currated_dataframe.reset_index(inplace=True)\n        \n        return currated_dataframe.drop('index', axis = 1)","cc6dee45":"num_feats = [col for col in df.select_dtypes(include=np.number).columns]\ncat_feats = [col for col in df.select_dtypes(include=np.object).columns]\n\ndata = Clean(df, ['RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak'])\ncleaned_data = data.remove_outliers()","99d4d88d":"cleaned_data.head()","06552718":"from sklearn.preprocessing import LabelEncoder\n\nclass Convert:\n    def __init__(self, dataframe, features):\n        self.dataframe = dataframe\n        self.features = features\n        \n    def convert_val(self):\n        le = LabelEncoder()\n        for feature in self.features:\n            self.dataframe[feature] = le.fit_transform(self.dataframe[feature])\n        \n        return self.dataframe","8ff944ef":"cat_feats = df.select_dtypes(include=np.object)\ndata = Convert(cleaned_data, cat_feats)\ncleaned_df = data.convert_val()","078f7e4b":"cleaned_df.head()","4ba9d5f2":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(cleaned_df.drop('HeartDisease', axis=1))\ndf_features = pd.DataFrame(scaled_features, columns=cleaned_data.columns[:-1])","ff86e2e1":"from sklearn.model_selection import train_test_split\n\nx = df_features\ny = cleaned_df[cleaned_df.columns[-1]]\n\nxtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=2)","de2570e5":"print(f'{ytrain.value_counts(normalize=True)}\\n')\nprint(f'{ytest.value_counts(normalize=True)}')","d2dafbb6":"from sklearn.neighbors import KNeighborsClassifier  \n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(xtrain, ytrain)","fcd0e13f":"knn_pred = knn.predict(xtest)\nknn_pred","8a642956":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(ytest, knn_pred)\nsns.heatmap(cm, annot=True, cmap=\"YlGnBu\")","9fa79e3e":"error_rates = list()\nk_range = 30\n\nfor k in range(1, k_range):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(xtrain, ytrain)\n    y_pred = knn.predict(xtest)\n    \n    error_rates.append(np.mean(y_pred != ytest))","5c131d22":"plt.figure(figsize=(13,6))\nsns.lineplot(x=range(1,k_range), y=error_rates, linestyle='--', marker='o', markerfacecolor='red', markersize=10)\n\nplt.title(\"K's Error Rates\", fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Number of K', fontsize=16)\nplt.ylabel('Error Percentage', fontsize=16)\n\nplt.show()","05ceacd6":"from sklearn.linear_model import LogisticRegression\n\nlog = LogisticRegression()\nlog.fit(xtrain, ytrain)","41d9b1c2":"log_pred = log.predict(xtest)\nlog_pred","58a99b33":"cm = confusion_matrix(ytest, log_pred)\nsns.heatmap(cm, annot=True, cmap=\"YlGnBu\")","c1726357":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier()\nforest.fit(xtrain, ytrain)","766b286e":"forest_pred = forest.predict(xtest)\nforest_pred","79b24f4a":"cm = confusion_matrix(ytest, forest_pred)\nsns.heatmap(cm, annot=True, cmap=\"YlGnBu\")","0579a7df":"print('Classification Reports\\n')\nprint(f'K-NN -------------------\\n{classification_report(ytest, knn_pred)}\\n')\nprint(f'Logistic Regression -------------------\\n{classification_report(ytest, log_pred)}\\n')\nprint(f'Random Forest Classifier -------------------\\n{classification_report(ytest, forest_pred)}\\n')","7ff19178":"print('Mean Accuracies\\n')\nprint(f'K-NN: {knn.score(xtest, ytest)}')\nprint(f'Logistic Regression: {log.score(xtest, ytest)}')\nprint(f'Random Forest Classifier: {forest.score(xtest, ytest)}')","163eb9ae":"importance = forest.feature_importances_\nimportance_feats = xtrain.columns","d953d469":"df_importance = pd.DataFrame({'Feature':importance_feats, 'Percentage':importance})\ndf_importance.sort_values('Percentage', ascending=False, inplace=True)\ndf_importance.Percentage = df_importance.Percentage.apply(lambda x: x * 100)","54b1f314":"df_importance","2413d1a8":"plt.figure(figsize=(15,6))\nsns.barplot(x='Percentage', y='Feature', data=df_importance)\n\nplt.title('Feature Importance', fontsize=18)\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel('')\nplt.ylabel('')\n\nplt.show()","0060abcb":"## Feature Importance\n> I will measure the feature importance using Random Forest model since it performed the highest accuracy","156cef0b":"# **Models Evaluation**","62ae8fff":"# **Main Dataset**","595d9976":"### **K-NN Model**","451b294d":"### *Correlation between each column*","98a5392e":"## Convert Categorical Values into Numeric Values\n1. #### **Sex** --> M = 0, F = 1\n1. #### **ChestPainType** --> ATA = 0, NAP = 1, ASY = 2, TA = 3\n1. #### **RestingECG** --> Normal = 0, ST = 1, LVH = 2\n1. #### **ExerciseAngina** --> N = 0, Y  = 1\n1. #### **ST_Slope** --> Up = 0, Flat = 1, Down = 2","898835a4":"### Error Rates per K-Model","07a6fda5":"# **Data Preprocessing**","d973fd34":"### **Random Forest**","8f014b89":"#### We can conclude that the best n_neighbors are 5, 7, and 9 because they have the least error rate[](http:\/\/)","75a87722":"### Split Train and Test Data","097cda5f":"# **Feature Engineering**","1757aa30":"### Features Scaling","3b0ec7a6":"### **Logistic Regression Model**","2629a293":"# **Exploratory Data Analysis**"}}