{"cell_type":{"8958fe33":"code","b0a5511f":"code","827bc80e":"code","12331429":"code","9b78fc5f":"code","643e122b":"code","59dbf039":"code","c75bfcc2":"code","6f7604a9":"code","be9326b9":"code","3d11ab5e":"code","70b9a615":"code","ed4118f6":"code","196d1d73":"code","7a31566b":"code","abb91aa7":"markdown","7d120303":"markdown","c1921d12":"markdown","52f2a66b":"markdown"},"source":{"8958fe33":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing","b0a5511f":"df = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")","827bc80e":"df","12331429":"df.isnull().sum()","9b78fc5f":"# Import label encoder\nfrom sklearn import preprocessing\n \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n\n# Encode labels in column Sex,ExerciseAngina\nle = ['Sex','ExerciseAngina']\nfor i in le:\n    df[i] = label_encoder.fit_transform(df[i] )\n\n# One Hot encode labels in column ChestPainType, RestingECG,ST_Slope\ndf = pd.get_dummies(df, columns=['ChestPainType','RestingECG','ST_Slope'])\n\n","643e122b":"df","59dbf039":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\ny = df['HeartDisease']\nX = df.drop(columns = 'HeartDisease',axis=1)\n...\n# transform the dataset\noversample = SMOTE()\nX_res, y_res = oversample.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)","c75bfcc2":"#Check if its imbalanced data\nimport seaborn as sns\nsns.countplot(y_train)","6f7604a9":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint('Acuracy score:',accuracy_score(y_test, y_pred))","be9326b9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint('Acuracy score:',accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","3d11ab5e":"importances = model.feature_importances_\n#\n# Sort the feature importance in descending order\n#\nsorted_indices = np.argsort(importances)[::-1]","70b9a615":"feature = X_train.columns[sorted_indices][0:13]","ed4118f6":"import matplotlib.pyplot as plt\n \nplt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]), importances[sorted_indices], align='center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)\nplt.tight_layout()\nplt.show()","196d1d73":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = XGBClassifier(use_label_encoder=False, verbosity = 0)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint('Acuracy score:',accuracy_score(y_test, y_pred))","7a31566b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(solver='lbfgs', max_iter=10000)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint('Acuracy score:',accuracy_score(y_test, y_pred))","abb91aa7":"# An easy method to check if there is missing value","7d120303":"# Random Forest seems to be the best model in this dataset","c1921d12":"# One Hot Encoding and Label Encoding is pretty important in preprocessing step","52f2a66b":"# Using SMOTE to make the dataset ot be fully balanced"}}