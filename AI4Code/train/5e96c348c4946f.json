{"cell_type":{"37a418ae":"code","fe76008e":"code","e48f42f8":"code","4b81e4af":"code","da3cfd9f":"code","8af2b959":"code","a1064c0f":"code","79d30bde":"code","7981cb33":"code","ace120c7":"code","b37d29a1":"code","741912a7":"markdown","0e983d9c":"markdown","f0ce0552":"markdown","451578fa":"markdown","d1cf9dce":"markdown","d2445ba3":"markdown","f0625335":"markdown"},"source":{"37a418ae":"from time import time\nnotebook_start_time = time()","fe76008e":"import os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader as DL\nfrom torch.nn.utils import weight_norm as WN\nfrom torchvision import models, transforms\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler","e48f42f8":"SEED = 49\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATA_PATH = \"..\/input\/petfinder-pawpularity-score\"\nFEATURE_PATH = \"..\/input\/petfinder-pf-nc-ua-all-dataset\"\nMODEL_NAME = \"densenet169\"\n\nDEBUG = False\nverbose = False\n\nsc_y = StandardScaler()","4b81e4af":"def breaker(num=50, char=\"*\") -> None:\n    print(\"\\n\" + num*char + \"\\n\")\n\n\ndef get_targets(path: str) -> np.ndarray:\n    df = pd.read_csv(os.path.join(path, \"train.csv\"), engine=\"python\")\n    targets = df[\"Pawpularity\"].copy().values\n    return targets.reshape(-1, 1)\n\n\ndef show_graphs(L: list, title=None) -> None:\n    TL, VL = [], []\n    for i in range(len(L)):\n        TL.append(L[i][\"train\"])\n        VL.append(L[i][\"valid\"])\n    x_Axis = np.arange(1, len(L) + 1)\n    plt.figure()\n    plt.plot(x_Axis, TL, \"r\", label=\"train\")\n    plt.plot(x_Axis, VL, \"b\", label=\"valid\")\n    plt.grid()\n    plt.legend()\n    if title:\n        plt.title(\"{} Loss\".format(title))\n    else:\n        plt.title(\"Loss\")\n    plt.show()","da3cfd9f":"class DS(Dataset):\n    def __init__(self, features=None, targets=None):\n        self.features = features\n        self.targets  = targets\n        \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return torch.FloatTensor(self.features[idx]), torch.FloatTensor(self.targets[idx])\n    \n\ndef build_dataloaders(tr_features: np.ndarray, va_features: np.ndarray,\n                      tr_targets: np.ndarray, va_targets: np.ndarray,\n                      batch_size: int, seed: int):\n\n    if verbose:\n        breaker()\n        print(\"Building Train and Validation DataLoaders ...\")\n    \n    tr_data_setup = DS(features=tr_features, targets=tr_targets)\n    va_data_setup = DS(features=va_features, targets=va_targets)\n    \n    dataloaders = {\n        \"train\" : DL(tr_data_setup, batch_size=batch_size, shuffle=True, generator=torch.manual_seed(seed)),\n        \"valid\" : DL(va_data_setup, batch_size=batch_size, shuffle=False)\n    }\n    \n    return dataloaders","8af2b959":"def build_model(IL: int, seed: int):\n    class ANN(nn.Module):\n        def __init__(self, IL=None):\n            super(ANN, self).__init__()\n\n            self.predictor = nn.Sequential()\n            self.predictor.add_module(\"BN\", nn.BatchNorm1d(num_features=IL, eps=1e-5))\n            self.predictor.add_module(\"FC\", WN(nn.Linear(in_features=IL, out_features=1)))\n\n        def get_optimizer(self, lr=1e-3, wd=0):\n            params = [p for p in self.parameters() if p.requires_grad]\n            return optim.Adam(params, lr=lr, weight_decay=wd)\n\n        def get_plateau_scheduler(self, optimizer=None, patience=5, eps=1e-8):\n            return optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=patience, eps=eps, verbose=True)\n\n        def forward(self, x1, x2=None):\n            if x2 is not None:\n                return self.predictor(x1), self.predictor(x2)\n            else:\n                return self.predictor(x1)\n    \n    if verbose:\n        breaker()\n        print(\"Building Model ...\")\n        print(\"\\n{} -> 1\".format(IL))\n    \n    torch.manual_seed(seed)\n    model = ANN(IL=IL)\n    \n    return model","a1064c0f":"def fit(model=None, optimizer=None, scheduler=None, \n        epochs=None, early_stopping_patience=None,\n        dataloaders=None, fold=None, verbose=False) -> tuple:\n    \n    name = \".\/Fold_{}_state.pt\".format(fold)\n    \n    if verbose:\n        breaker()\n        print(\"Training Fold {}...\".format(fold))\n        breaker()\n    else:\n        print(\"Training Fold {}...\".format(fold))\n\n    Losses = []\n    bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n\n    start_time = time()\n    for e in range(epochs):\n        e_st = time()\n        epochLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n\n        for phase in [\"train\", \"valid\"]:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n            \n            lossPerPass = []\n\n            for X, y in dataloaders[phase]:\n                X, y = X.to(DEVICE), y.to(DEVICE)\n\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == \"train\"):\n                    output = model(X)\n                    loss = torch.nn.MSELoss()(output, y)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n                lossPerPass.append(loss.item())\n            epochLoss[phase] = np.mean(np.array(lossPerPass))\n        Losses.append(epochLoss)\n\n        if early_stopping_patience:\n            if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n                bestLoss = epochLoss\n                BLE = e + 1\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optim_state_dict\": optimizer.state_dict()},\n                           name)\n                early_stopping_step = 0\n            else:\n                early_stopping_step += 1\n                if early_stopping_step > early_stopping_patience:\n                    if verbose:\n                        print(\"\\nEarly Stopping at Epoch {}\".format(e))\n                    break\n        \n        if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n            bestLoss = epochLoss\n            BLE = e + 1\n            torch.save({\"model_state_dict\": model.state_dict(),\n                        \"optim_state_dict\": optimizer.state_dict()},\n                       name)\n        \n        if scheduler:\n            scheduler.step(epochLoss[\"valid\"])\n        \n        if verbose:\n            print(\"Epoch: {} | Train Loss: {:.5f} | Valid Loss: {:.5f} | Time: {:.2f} seconds\".format(e+1, epochLoss[\"train\"], epochLoss[\"valid\"], time()-e_st))\n    \n    if verbose:\n        breaker()\n        print(\"Best Validation Loss at Epoch {}\".format(BLE))\n        breaker()\n        print(\"Time Taken [{} Epochs] : {:.2f} minutes\".format(len(Losses), (time()-start_time)\/60))\n        breaker()\n        print(\"Training Completed\")\n        breaker()\n\n    return Losses, BLE, name\n\n#####################################################################################################\n\ndef predict_batch(model=None, dataloader=None, mode=\"test\", path=None) -> np.ndarray:    \n    model.load_state_dict(torch.load(path, map_location=DEVICE)[\"model_state_dict\"])\n    model.to(DEVICE)\n    model.eval()\n\n    y_pred = torch.zeros(1, 1).to(DEVICE)\n    if re.match(r\"valid\", mode, re.IGNORECASE):\n        for X, _ in dataloader:\n            X = X.to(DEVICE)\n            with torch.no_grad():\n                output = model(X)\n            y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n    elif re.match(r\"test\", mode, re.IGNORECASE):\n        for X in dataloader:\n            X = X.to(DEVICE)\n            with torch.no_grad():\n                output = model(X)\n            y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n    \n    return y_pred[1:].detach().cpu().numpy()","79d30bde":"def train(features: np.ndarray, targets: np.ndarray,\n          n_splits: int, batch_size: int, lr: float, wd: float, \n          epochs: int, early_stopping: int, \n          patience=None, eps=None) -> list:        \n    \n    metrics = []\n        \n    KFold_start_time = time()\n    breaker()\n    print(\"Performing {} Fold CV ...\".format(n_splits))\n    breaker()\n    fold = 1\n    for tr_idx, va_idx in KFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(features):\n\n        tr_features, va_features = features[tr_idx], features[va_idx]\n        tr_targets, va_targets   = targets[tr_idx], targets[va_idx]\n\n        tr_targets = sc_y.fit_transform(tr_targets)\n        va_targets = sc_y.transform(va_targets)\n\n        dataloaders = build_dataloaders(tr_features, va_features,\n                                        tr_targets, va_targets, \n                                        batch_size, SEED)\n        model = build_model(IL=tr_features.shape[1], seed=SEED).to(DEVICE)\n        optimizer = model.get_optimizer(lr=lr, wd=wd)\n        scheduler = None\n        if isinstance(patience, int) and isinstance(eps, float):\n            scheduler = model.get_plateau_scheduler(optimizer, patience, eps)\n\n        L, _, name = fit(model=model, optimizer=optimizer, scheduler=scheduler, \n                         epochs=epochs, early_stopping_patience=early_stopping,\n                         dataloaders=dataloaders, fold=fold, verbose=verbose)\n        y_pred = predict_batch(model=model, dataloader=dataloaders[\"valid\"], mode=\"valid\", path=name)\n        RMSE = np.sqrt(mean_squared_error(sc_y.inverse_transform(y_pred), sc_y.inverse_transform(va_targets)))\n        if verbose:\n            print(\"\\nValidation RMSE [Fold {}]: {:.5f}\".format(fold, RMSE))\n            breaker()\n            show_graphs(L)\n        \n        metrics_dict = {\"Fold\" : fold, \"RMSE\" : RMSE}\n        metrics.append(metrics_dict)\n        \n        fold += 1\n    \n    breaker()\n    print(\"Total Time to {} Fold CV : {:.2f} minutes\".format(n_splits, (time() - KFold_start_time)\/60))\n    \n    return metrics, (time() - KFold_start_time)\/60","7981cb33":"def main():\n    \n    ########### Params ###########\n    \n    if DEBUG:\n        n_splits = 10\n        patience, eps = 5, 1e-8\n        epochs, early_stopping = 5, 5\n\n        batch_size = 128\n        lr = 5e-4\n        wd = 1e-1\n    else:\n        n_splits = 10\n        patience, eps = 5, 1e-8\n        epochs, early_stopping = 100, 8\n\n        batch_size = 128\n        lr = 5e-4\n        wd = 1e-1\n    \n    ##############################\n\n    complete_metrics = []\n\n    if verbose:\n        breaker()\n        print(\"Loading Data ...\")\n    \n    features = np.load(os.path.join(FEATURE_PATH, \"{}_features.npy\".format(MODEL_NAME)))\n    targets  = get_targets(DATA_PATH)\n\n    # Without Scheduler\n    metrics, _ = train(features, targets, n_splits, batch_size, lr, wd, epochs, early_stopping, patience=None, eps=None)\n\n    # # With Plateau Scheduler\n    # metrics = train(features, targets, n_splits, batch_size, lr, wd, epochs, early_stopping, patience=patience, eps=eps)\n\n    rmse = []\n    breaker()\n    for i in range(len(metrics)):\n        print(\"Fold {}, RMSE: {:.5f}\".format(metrics[i][\"Fold\"], metrics[i][\"RMSE\"]))\n        rmse.append(metrics[i][\"RMSE\"])\n    \n    best_index = rmse.index(min(rmse))\n    breaker()\n    print(\"Best RMSE : {:.5f}\".format(metrics[best_index][\"RMSE\"]))\n    print(\"Avg RMSE  : {:.5f}\".format(sum(rmse) \/ len(rmse)))\n    breaker()\n\n    with open(\"metrics.pkl\", \"wb\") as fp:\n        pickle.dump(metrics, fp)","ace120c7":"main()","b37d29a1":"breaker()\nprint(\"Notebook Rumtime : {:.2f} minutes\".format((time() - notebook_start_time)\/60))\nbreaker()","741912a7":"## Main","0e983d9c":"## Build Model","f0ce0552":"## Dataset Template and Build Dataloader","451578fa":"## Fit and Predict Helpers","d1cf9dce":"## Train","d2445ba3":"## Constants and Utilities","f0625335":"## Library Imports"}}