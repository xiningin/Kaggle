{"cell_type":{"dbbfaf24":"code","96b20353":"code","73da14f7":"code","202615e2":"code","95ec6c68":"markdown","ce5cffa9":"markdown","b0ea29f4":"markdown","46fa93e3":"markdown","eb2b787d":"markdown","4a807ba9":"markdown","1b79d054":"markdown","c926405c":"markdown","5408a3e6":"markdown","f5d41cbd":"markdown","6934a531":"markdown","d8430e33":"markdown","f907fbe0":"markdown","9f5e2233":"markdown","553f5b96":"markdown","f98ac2d0":"markdown","2d7d9227":"markdown","8c9220b5":"markdown","2708c4b8":"markdown","d72d0507":"markdown","73ff316f":"markdown","f87d78d5":"markdown","8515c21d":"markdown","9c617cba":"markdown","8c358a5c":"markdown","c472f10d":"markdown","baa9655a":"markdown"},"source":{"dbbfaf24":"num_validation_samples = 20000\n\nnp.random.shuffle(data)\n\n#Defining the validatian set.\nvalidation_data= data[:num_validation_samples]\n\ndata = data[num_validation_samples:]\n\n#Defining the training set. \ntraining_data = data[:]\n\n#Trainining the model and evaluating it. \nmodel = get_model()\nmodel.train(training_data)\nvalidation_score = model.evaluate(validation_data)\n\n#We can play (retrain,evalute, tune it again...) with our model after these steps.\n\nmodel = get_model()\nmodel.train(np.concatenate([training_data,\n                            validation_data]))\n\ntest_score = model.evaluate(test_data)","96b20353":"k = 4 #or 3,5..\nnum_validation_samples = len(data)\/\/k\n\nnp.random.shuffle(data)\n\nvalidation_scores = []\n\nfor fold in range(k):\n#Selecting and concetenation.\n    validation_data= data[num_validation_samples*fold : num_validation_samples * (fold + 1)]\n    training_data = data[:num_validation_samples*fold] + data[num_validation_samples* fold +1 :] \n\n\n    model = get_model()\n    model.train(training_data)\n    validation_score = model.evaluate(validation_data)\n    validation_score.append(validation_score)\nvalidation_score = np.average(validation_score)\n\nmodel = get_model()\nmodel.train(data)\ntest_score = model.evaluate(test_data)","73da14f7":"import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nmodel = RandomForestClassifier()\n\nN_iterations = 10\nN_folds = 5\n\n# Initialize array to store the scores of each K-fold cross-validation\nscores = np.zeros((N_iterations, N_folds))\n\n# This loop is what makes it iterated K-fold validation\nfor i in range(N_iterations):\n\n    skf = StratifiedKFold(n_splits=N_folds, shuffle=True)\n\n    # This loops is traditional K-fold cross validation\n    for j, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 0):\n\n        X_tr = X_train[train_index]\n        X_te = X_train[test_index]\n\n        y_tr = y_train[train_index]\n        y_te = y_train[test_index]\n\n        model.fit(X_tr, y_tr)\n\n        scores[i][j] = model.score(X_te, y_te)\n\n    print(\"Iteration\", i+1, \"avg: %.4f\" % np.mean(scores[i]))\n\nprint(\"\\nAvg score: %.4f\" % np.mean(np.mean(scores, axis=1)))\n\nSouce: Stackoverflow-Pablo.","202615e2":"#DATA AUGEMENTATION \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n%matplotlib inline\n\n\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n    \ngen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, \n                         shear_range=0.15, zoom_range=0.1, \n                         channel_shift_range=10., horizontal_flip=True)\n\nchosen_image = random.choice(os.listdir('example\/path'))\n\nimage_path = 'example\/path' + chosen_image\n\n\nimage = np.expand_dims(plt.imread(image_path),0)\n\nplt.imshow(image[0])\n\n\naug_iter = gen.flow(image)\n\naug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(20)]\n\nplotImages(aug_images)\n\naug_iter = gen.flow(image, save_to_dir='example\/path', save_prefix='aug-image-', \n                    save_format='jpeg')\n\n\n# Note, we can also use ImageDataGenerator.flow_from_directory() \n# instead of ImageDataGenerator.flow()","95ec6c68":"\nSources: \n\n[1] Yakup Gen\u00e7, GTU Derin \u00d6\u011frenme Ders Notlar\u0131\n\n[2] Cristina Scheau, Regularization in deep learning, https:\/\/chatbotslife.com\/regularization-in-deep-learning-f649a45d6e0\n\n[3] https:\/\/codelabs.developers.google.com\/codelabs\/cloud-tensorflow-mnist\/#12\n\n[4] https:\/\/www.linkedin.com\/pulse\/derin-%C3%B6%C4%9Frenme-uygulamlar%C4%B1nda-ba%C5%9Far%C4%B1m-iyile%C5%9Ftirme-necmettin-%C3%A7arkac%C4%B1\/\n\n[5] http:\/\/alierbey.com \n\n[6] http:\/\/deeplizard.com\n\n[7] Deep Learning Specialization, Andrew Ng\n\n[8] https:\/\/makineogrenimi.wordpress.com\/2017\/05\/31\/duzenlilestirme-regularization\/\n\n\n","ce5cffa9":"# <p style=\"color:blue;font-size:25px \" >HyperParameter <\/p> A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010parameter to a very large value, you will get an almost flat model (a slope close to zero); the learning algorithm will almost certainly not overfit the training data, but it will be less likely to find a good solution. [HANDS-ON MACHINE LEARNING BOOK]","b0ea29f4":"<a id =\"super\"><\/a> \n\n# **1. Supervised Learning**\n\nYou give input data. \n\nEx for Supervised Learning:\n\n**Sequence generation:**\n\n![image.png](attachment:88a55176-666b-478f-8b5b-825531b51d1b.png)\n\nSource: Show and Tell: A Neural Image Caption Generator, 2015\n\n\n\n\n**Syntax tree prediction**\n\n**Object detection**\n\n**Image segmentation**\n\n![image.png](attachment:5463ad72-4b75-42e9-9348-b8a2af266639.png)\n\nSource: Li, Johnson and Yeung, 2017","46fa93e3":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content!<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">Four Categories of Machine Learning<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#super\" role=\"tab\" aria-controls=\"settings\">1. Supervised Learning<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#unsuper\" role=\"tab\" aria-controls=\"settings\">2. Unsupervised Learning<span class=\"badge badge-primary badge-pill\">3<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#selfsup\" role=\"tab\" aria-controls=\"settings\">3. Self Supervised Learning<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#RL\" role=\"tab\" aria-controls=\"settings\">4.Reinforcement Learning<span class=\"badge badge-primary badge-pill\">5<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">EVALUTATION MODEL <span class=\"badge badge-primary badge-pill\">6<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#simp\" role=\"tab\" aria-controls=\"settings\">1. Simple Hold-out Validation<span class=\"badge badge-primary badge-pill\">7<\/span><\/a>  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#kfo\" role=\"tab\" aria-controls=\"settings\">2. K-fold Validation<span class=\"badge badge-primary badge-pill\">8<\/span><\/a> \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#it\" role=\"tab\" aria-controls=\"settings\">3. Iterated K-fold<span class=\"badge badge-primary badge-pill\">9<\/span><\/a> \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#not\" role=\"tab\" aria-controls=\"settings\">NOTES[Fran\u00e7ois]<span class=\"badge badge-primary badge-pill\">10<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#reg1\" role=\"tab\" aria-controls=\"settings\">Regularization-1<span class=\"badge badge-primary badge-pill\">11<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#lr\" role=\"tab\" aria-controls=\"settings\">Learning Rate<span class=\"badge badge-primary badge-pill\">12<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#zp\" role=\"tab\" aria-controls=\"settings\">Zero Padding<span class=\"badge badge-primary badge-pill\">13<\/span><\/a>\n    \n","eb2b787d":"<a id =\"RL\"><\/a>\n\n# **4. Reinforcement Learning**\n\n![image.png](attachment:7db13489-ac8a-488a-a4d4-b1502a93feba.png)\nSource: wikimedia.org\n\n    Examples: \n\n    Self- driving cars, health, gaming ( Alpha-Go..) etc. \n\n","4a807ba9":"<a id =\"zp\"><\/a>\n\n# <p style=\"color:green;font-size:25px \" >Zero Padding <\/p> \n\n\n<a ><img src=\"https:\/\/i.ibb.co\/41X31tJ\/Screenshot-from-2021-07-24-18-31-15.png\" alt=\"Screenshot-from-2021-07-24-18-31-15\" border=\"0\"><\/a>\n\nZero padding allows us to preserve original size of input \n\n <a ><img src=\"https:\/\/i.ibb.co\/pfG0S5n\/Screenshot-from-2021-07-24-18-32-14.png\" alt=\"Screenshot-from-2021-07-24-18-32-14\" border=\"0\"><\/a>","1b79d054":"# BIAS in AI\n\n* Each neuron has bias.\n* The flexibility of the model will be rised. \n\n<a ><img src=\"https:\/\/i.ibb.co\/9TSdKf6\/front.png\" alt=\"front\" border=\"0\"><\/a>\n\n# Learnable Parameters in Fully Connected Layers\n\n<a ><img src=\"https:\/\/i.ibb.co\/tBvwt9d\/learnpara.png\" alt=\"learnpara\" border=\"0\"><\/a>\n\n    How we can calculate = input * output + bias\n\n    Weight = input* output\n    \n# Learnable Parameters in CNN\n\n<a ><img src=\"https:\/\/i.ibb.co\/YfjTFk2\/Screenshot-2021-07-14-at-20-04-03-Learnable-Parameters-in-an-Artificial-Neural-Network-explained.png\" alt=\"Screenshot-2021-07-14-at-20-04-03-Learnable-Parameters-in-an-Artificial-Neural-Network-explained\" border=\"0\"><\/a>\n\n    For ex: 56 = 3 *  ( 2 * (3 * 3) ) + 2 \n\n    For ex: 2402 = (input: 20 * 20 * 3 ) 1200 * 2 + 2 \n","c926405c":"<a id =\"3\"><\/a>\n<p style=\"color:blue;font-size:20px \" >Three classic evalution model: <\/p> \n\n# EVALUTATION RECIPES\n \n1. Hold-out Validation\n2. K-fold Validation\n3. Iterated K-fold Validation\n\n<a id =\"simp\"><\/a>\n\n# 1. Simple Hold-out Validation\n\n\n    It is the simplest way, but it is convenient for big volume data. ","5408a3e6":"# **Sequential or Functional Model**","f5d41cbd":"# DATA AUGEMENTATION \n","6934a531":"<a id =\"it\"><\/a>\n\n# 3. Iterated K-fold\n\n    We can evaluate our model \"as precisely as possible\". ","d8430e33":"# Fine-tuning\n    If it has learned the knowledge of recognizing cars, it can also notice trucks later. This is called transfer learning. In fine tuning it is a way to implement or benefit from a transfer learning.\n    \n    \n# Batch Normalization\n\n<a ><img src=\"https:\/\/i.ibb.co\/LYCKPJn\/Screenshot-2021-07-19-at-13-59-06-Batch-Normalization-batch-norm-explained-You-Tube.png\" alt=\"Screenshot-2021-07-19-at-13-59-06-Batch-Normalization-batch-norm-explained-You-Tube\" border=\"0\"><\/a>\n\n","f907fbe0":"#   **BATCH SIZE**\n\n>     Number of the samples that will be passed through to the network at one time.\n\n<a ><img src=\"https:\/\/i.ibb.co\/47cPKWh\/Screenshot-2021-07-18-at-15-13-40-Regularization-in-a-Neural-Network-explained.png\" alt=\"Screenshot-2021-07-18-at-15-13-40-Regularization-in-a-Neural-Network-explained\" border=\"0\"><\/a> \n\n    Difference between batch size and epoch.\n    \n ","9f5e2233":"# Vanishing Gradient \nCHeck the code example : https:\/\/cs224d.stanford.edu\/notebooks\/vanishing_grad_example.html\nWith \"relu\" or \"rnn\" we can find a solution for these problems. (Training acc:0.97 and loss is bigger with using sigmoid. But Training acc: 0.99 with relu. )","553f5b96":"<a id =\"selfsup\"><\/a>\n\n# **3. Self-supervised Learning**\n\n    \"It is a supervised learning but without human-annotated labels.\"\n    \n   For ex: \n   \n   <img src=\"https:\/\/lilianweng.github.io\/lil-log\/assets\/images\/self-sup-by-relative-position.png\"  width=\"500\" height=\"600\"> \nSource: Doersch et al., 2015","f98ac2d0":"<a id =\"reg1\"><\/a>\n\n# <p style=\"color:purple;font-size:30px \">Regularization-1 <\/p> Regularization techniques are generally used to increase performance by preventing overfitting.\n\n<img src=\"https:\/\/media-exp3.licdn.com\/dms\/image\/C5112AQFLszdEOQX5tQ\/article-inline_image-shrink_1000_1488\/0\/1520215463785?e=1629936000&v=beta&t=oYVXLGESFy7FBi0Oo4MxWof4V8SiHHqCuVY3ygRnbJI\"  width=\"900\" height=\"600\"> \n\n\n\n> It can be defined as the process of calculating the \u201cw\u201d weights in the model in a way that will solve the problem more appropriately by adding an independent parameter to the cost function y=f(x.w)+b. \n\n> In this sense, it can be said that regularization is actually an independent parameter added to the loss function. \n\n> !!There is no regulation for the \u201cb\u201d bias values. Just \"w\" weight is affected. \n\n# <p style=\"color:purple;font-size:25px \" >Commonly used regulation techniques <\/p> \n \nHaving a large dataset prevents overfitting. If the dataset size is small, there are some methods to increase it.\n\n*   1. Synthetic data\n*   2. Inject random negative\n\n## 3. Dropout Layer\n\nSuppose a group of workers is working to construct a building, and each of them specializes in a job and does that job alone. In such a case, when one of the workers makes a mistake or gets sick and does not come to work, it will be difficult for other workers to replace him, and the situation will greatly affect the progress of the work. Instead, the employer can drop out a few workers each week and wait for other workers to fill in, so that each worker learns the minimum information that can replace other workers. Thus, when any worker makes a mistake or in his absence, a system will be built that can fill his place and prevent the system from failing.\n\n*   4. Dense-sparse-dense training\n\n*   5. Early Stoping\n\n## 6. L1 and L2 Regularization\n\nThe logic in model improvement is based on the assumption that the model with small weights is simpler and interpretable than the model with large weights.\n\nFor ex: Assume that when we apply L2 to the cost function, it give f1 = w1.x = 1.0 and f2 = w2.x = 0.25. w2 vector will be preferred because it produces lower loss value than L2.\n\n## 7. Multi-tasking Learning\n\n<img src=\"https:\/\/media-exp3.licdn.com\/dms\/image\/C5112AQEr_PpuboKLuQ\/article-inline_image-shrink_1000_1488\/0\/1520148254216?e=1629936000&v=beta&t=aQ2oNkFEXv9h4cDUbCTQXqArWg8_xRJOF1b-32VnrtQ\"  width=\"800\" height=\"600\"> \n\n\nFor example, the model represented above tries to learn two things; the first is whether the given photo is a dog or a human, the second is whether the given photo is a girl or a boy. In multi-tasking, the model will try to create a more general model as it will try to explain both tasks. In this case, it will take us a little further away from the dependency on the dataset, thus preventing overfitting.\n\n## 8. Finetuning \n\nIt is one of the most commonly used improvement methods. Initializing the parameters of the model with the weights of a problem in a similar domain rather than starting it randomly increases the performance of the model.\n\n*    9. Semi-Supervised Learning\n\n*    10. Boostrap aggration\n\n*    11. Ensemble models\n\n\n\n\n\n\n\n\n\n\n","2d7d9227":"# What is Kernel?\n\nThe Kernel helps us separate data with a non-linear decision boundary using a linear classifier. They achieve this by mapping features to higher-dimensional vector spaces using functions that represent dot products in the higher dimension without explicitly creating those feature mappings.","8c9220b5":"<a id =\"not\"><\/a>\n\n# <p style=\"color:red;font-size:20px \" >NOTES: <\/p> \n\n> 1. No information regarding the test set should have reach to our model, even if it is indirectly. \n\n> 2. Before splitting it into training and test sets, we should randomly shuffle our data.\n\n> 3. We should not randomly shuffle if we want to predict the future ( tomorrow's weather) given the past.\n\n> 4. Training set and validation set should be different,(we shouldn't see data twice.)\n\n> 5. We must first turn our data into tensors, \"data **vectorization**\" (if they didn't come in vectorized form.)\n\n 6. ![image.png](attachment:e2e16fff-3d8e-400a-9419-ebc2658f6b0b.png)\n\n{Another ex: not to evaluate pictures whether people are masked, but to set up algorithms with their coordinates.}\n\n\n> 7. Always keep this in your mind: DL tends to be good at fitting to training data, real challenge is \"**generalization**\" not fitting.\n\n\n","2708c4b8":"<a id =\"kfo\"><\/a>\n# 2. K-fold Validation\n\n    When we have too few samples for hold-out validation.","d72d0507":"<a id =\"unsuper\"><\/a>\n\n# **2. Unsupervised Learning**\n\n    \"Unsupervised Learning is the bread and butter of data analytics.\"\n\n    Example Categories of it : \n\n* Dimesionality reduction\n \n* Clustering","73ff316f":"<a id =\"lr\"><\/a>\n\n# <p style=\"color:blue;font-size:25px \" >Learning Rate <\/p> Learning Rate symbolizes the step we will take. A very small number is assigned (like 0.0001). If this number is large, it cannot reach the minimum. If it is small, the model learns slowly.\n    \n## Verbose\n    It means how many output that you want to take. \n\n## Neural Network \n\n    3 layers remember: Input, Hidden and Output. Ex. Neurans can be shown like that Dense(**\"32\"**) \n\n","f87d78d5":"# <p style=\"color:green;font-size:25px \" >Max Pooling  <\/p> <a ><img src=\"https:\/\/i.ibb.co\/5Lzy1r1\/Screenshot-from-2021-07-24-18-26-20.png\" alt=\"Screenshot-from-2021-07-24-18-26-20\" border=\"0\"><\/a>\n\n> Max pooling can be 1D ;2D; 3D, we should indicate that. \n\n> Pooling mainly helps in extracting sharp and smooth features. It is also done to reduce variance and computations. Max-pooling helps in extracting low-level features like edges, points, etc. While Avg-pooling goes for smooth features. If time constraint is not a problem, then one can skip the pooling layer and use a convolutional layer to do the same. (Andrew Ng.)\n\n\n\n<a ><img src=\"https:\/\/i.ibb.co\/pQQPTLw\/Screenshot-from-2021-07-24-18-22-00.png\" alt=\"Screenshot-from-2021-07-24-18-22-00\" border=\"0\"><\/a>\n\n> And if we checked dimension first CONV2D will be in a shape of 20,20  but MAX Pooling will be 10,10. Then the second CONV2D will be 10,10 again!\n\n> So, it reduced the dimensions of image. \n\nCheck this demo app: https:\/\/deeplizard.com\/resource\/pavq7noze3\n\n<a ><img src=\"https:\/\/i.ibb.co\/YcLchSV\/Screenshot-from-2021-07-24-18-28-47.png\" alt=\"Screenshot-from-2021-07-24-18-28-47\" border=\"0\"><\/a>\n\n\n# <p style=\"color:green;font-size:25px \" >Back Propogation  <\/p> <a ><img src=\"https:\/\/i.ibb.co\/3WCJ4hB\/Screenshot-from-2021-07-24-18-36-10.png\" alt=\"Screenshot-from-2021-07-24-18-36-10\" border=\"0\"><\/a>\n\n<a ><img src=\"https:\/\/i.ibb.co\/7yHStFk\/Screenshot-from-2021-07-24-18-37-25.png\" alt=\"Screenshot-from-2021-07-24-18-37-25\" border=\"0\"><\/a>","8515c21d":"<a id =\"1\"><\/a> \n# **Four Categories of Machine Learning** \n\n    \n \n1. Supervised Learning\n2. Unsupervised Learning\n3. Self-supervised Learning\n4. Reinforcement Learning\n\n","9c617cba":"# Leaky Relu\n\n>  **Leaky Rectified Linear Unit**, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.\n\nSource: https:\/\/paperswithcode.com\/method\/leaky-relu","8c358a5c":"![image.png](attachment:c0b9f51e-2abb-4960-bde7-bfabddee8f6e.png)","c472f10d":"# Regularization-2\n\n   ## 1. Ridge Regression\n\n<a ><img src=\"https:\/\/i.ibb.co\/y4SJq5k\/Screenshot-2021-07-14-at-21-12-19-D-zenlile-tirme-Regularization.png\" alt=\"Screenshot-2021-07-14-at-21-12-19-D-zenlile-tirme-Regularization\" border=\"0\"><\/a>\n\n    Left: Linear Regres. Right : Polynomial Regres.\n    \n   ![image.png](attachment:c65b6c1b-8b2f-4df9-a4f0-6ecdfa8a9c1d.png)\n    \n    \n   ## 2. Lasso Regression\n![image.png](attachment:de8730c5-6797-4140-b962-4971c4812b43.png)   \n   \n    An important characteristic of Lasso regression is that it eliminates the weights of the least important features (for example, making it zero).\n\n   ## 3. Elastic Net\n    \n   ![image.png](attachment:4f97ccbf-626d-46f8-be2a-338aa9c49ac4.png)\n   \n   \n   ![image.png](attachment:dce4cd4e-e5d0-4bd2-bc33-235cea0812ea.png)\n   \n       This is a mix of Rigde regression and Lasso regression. If r=0, it is Elastic Net and Ridge;  if r= it is equal to Ridge regression.\n\n    So, which regression method will we choose? \n    First we should generally avoid straight linear regression. Ridge is generally preferable, but we should choose Lasso or Elastic Net if we think that few features will be useful in the model.\n","baa9655a":"**There are two ways to build Keras models: sequential and functional.**\n \nThe sequential API allows you to create models layer-by-layer for most problems. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs.\n\nAlternatively, the functional API allows you to create models that have a lot more flexibility as you can easily define models where layers connect to more than just the previous and next layers. In fact, you can connect layers to (literally) any other layer. As a result, creating complex networks such as siamese networks and residual networks become possible."}}