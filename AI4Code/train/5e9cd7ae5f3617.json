{"cell_type":{"9d8d3334":"code","84edd2b3":"code","b5ace6e2":"code","5f2ecc88":"code","f080acae":"code","1124f4fb":"code","53653de9":"code","181a39c2":"code","d00f6c04":"code","9bec3af7":"code","958d552a":"code","2df5d807":"code","edf9ebab":"code","10bff43b":"code","f6af0251":"code","b11dc2b4":"markdown","919468c3":"markdown","4794ccbd":"markdown","e6468bdc":"markdown","3483b448":"markdown","d1f9690f":"markdown","c69bb977":"markdown","b3183413":"markdown"},"source":{"9d8d3334":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84edd2b3":"from pathlib import Path\nfrom xgboost import XGBRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV","b5ace6e2":"train_path = '..\/input\/home-data-for-ml-course\/train.csv'\ntest_path = '..\/input\/home-data-for-ml-course\/test.csv'\ntrain = pd.read_csv(train_path,index_col='Id')\ntest = pd.read_csv(test_path,index_col='Id')\n","5f2ecc88":"# Drop N\/A values for sale price\ntrain.dropna(axis=0,subset=['SalePrice'],inplace=True)\ny = train.SalePrice\nX = train.drop(['SalePrice'],axis=1)","f080acae":"X.isnull().sum().sort_values(ascending=False).head(20)","1124f4fb":"# Pool Data was very few and far between. \n#Converted the quality of the pool to whether the house had a pool or not\nX['Pool'] = X.PoolQC.notnull().astype('int')\nX = X.drop(['PoolQC'],axis=1)\ntest['Pool'] = test.PoolQC.notnull().astype('int')\ntest = test.drop(['PoolQC'],axis=1)","53653de9":"fence_map = {'MnWw':1,'GdWo':2,'MnPrv':3,'GdPrv':4} \nX['Fence'] = X.Fence.map(fence_map)\nX.Fence.fillna(value=0,inplace=True)\ntest['Fence'] = test.Fence.map(fence_map)\ntest.Fence.fillna(value=0,inplace=True)\n\n# Central Air\nairMap = {'Y':1,'N':0}\nX['CentralAir'] = X.CentralAir.map(airMap)\nX.CentralAir.fillna(value=0,inplace=True)\ntest['CentralAir'] = test.CentralAir.map(airMap)\ntest.CentralAir.fillna(value=0,inplace=True)\n\n# Garage Year Built fill NA rows\nX['GarageYrBlt'].fillna(X['YearBuilt'],inplace=True)\nX.GarageType.fillna(value='NoGarage',inplace=True)\ntest['GarageYrBlt'].fillna(test['YearBuilt'],inplace=True)\ntest.GarageType.fillna(value='NoGarage',inplace=True)\n\n# Masonry Veneer\nX['MasVnrType'].fillna(value='None',inplace=True)\nX['MasVnrArea'].fillna(value=0,inplace=True)\ntest['MasVnrType'].fillna(value='None',inplace=True)\ntest['MasVnrArea'].fillna(value=0,inplace=True)\n\n# Lot Frontage Assume square lot and take the square root of lot area if no value given\nX.LotFrontage.fillna(X['LotArea']**0.5,inplace=True)\ntest.LotFrontage.fillna(test['LotArea']**0.5,inplace=True)\n\n# Drop Misc Feature\nX.drop(columns=['MiscFeature'],inplace=True)\ntest.drop(columns=['MiscFeature'],inplace=True)\n\n# Sale Type\nX['SaleType'].fillna(value='Other',inplace=True)\ntest['SaleType'].fillna(value='Other',inplace=True)","181a39c2":"\n# Get X columns that are categorical\/numerical\ncategory_list = [cname for cname in X.columns if X[cname].dtype == 'object']\nnum_list = [cname for cname in X.columns if X[cname].dtype != 'object']","d00f6c04":"conditionSet1 = [0,'Po','Fa','TA','Gd','Ex']\nconditionSet2 = [0, 'Unf','RFn','Fin']\nconditionSet3 = [0,'N','P','Y'] #Paved Driveway\nconditionSet4 = [0, 'Mix','FuseP','FuseF','FuseA','SBrkr']\nconditionSet5 = [0,'Grvl', 'Pave'] #Access\nconditionSet6 = [0, 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']\nconditionSet7 = [0, 'Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ']\nconditionSet8 = [0,'ELO','NoSeWa','NoSewr','AllPub']\nconditionSet9 = ['TwnhsI','TwnhsE','Duplx','2FmCon','1Fam'] # House Style\nconditionSet10 = [0,'No','Mn','Av','Gd'] #BsmtExposure\nconditions = [conditionSet1, conditionSet2, conditionSet3, conditionSet4, conditionSet5, conditionSet6, conditionSet7, conditionSet8, conditionSet9, conditionSet10]","9bec3af7":"# Create a mapping system for columns that have responses fulfilling one of the conditions\ndef cond_map(df,column,condition):\n    weightings = [weight for weight in range(len(condition))]\n    condition_map = dict(zip(condition,weightings))\n    df[column].fillna(value=0,inplace=True)\n    return df[column].map(condition_map)\n\nfor column in category_list:\n    for condition in conditions:\n        if set(X[column].fillna(value=0).values).issubset(condition):\n            X[column] = cond_map(X,column,condition)\n            test[column] = cond_map(test,column,condition)\n        else:\n            continue","958d552a":"# Reload X columns that are categorical\/numerical\ncategory_list = [cname for cname in X.columns if X[cname].dtype == 'object']\nnum_list = [cname for cname in X.columns if X[cname].dtype != 'object']\nyearList = ['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']\n\n# Want to fill missing values with zero except columns that are years\nfor column in np.setdiff1d(num_list,yearList):\n    X[column].fillna(value=0,inplace=True)\n    test[column].fillna(value=0,inplace=True)","2df5d807":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_list),\n        ('cat', categorical_transformer, category_list)\n    ])\n\n# Bundle preprocessing and modeling code in a pipeline\npipeline = Pipeline(steps=[('preprocessor',preprocessor),('xgbrg', XGBRegressor())])","edf9ebab":"# param_grid = {'xgbrg__learning_rate':[0.05,0.075,0.1],\n# 'xgbrg__n_estimators':[300,450,600],\n# 'xgbrg__max_depth':[2,4,6]}\n\n# \"\"\"\n# Result:\n# 0.8907309348826631\n# {'xgbrg__learning_rate': 0.05, 'xgbrg__max_depth': 2, 'xgbrg__n_estimators': 600}\n# \"\"\"\n\n# search = GridSearchCV(pipeline, param_grid,n_jobs=-1,cv=5)\n\n# # Preprocessing of training data, fit model\n# #my_pipeline.fit(X_train,y_train)\n# search.fit(X,y)\n# print(search.best_score_)\n# print(search.best_params_)\n","10bff43b":"my_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('xgbrg', XGBRegressor(learning_rate=0.05,max_depth=2,n_estimators=600))])\nmy_pipeline.fit(X,y)\ntest_pred = my_pipeline.predict(test)","f6af0251":"output = pd.DataFrame({'Id': test.index,'SalePrice': test_pred})\noutput.to_csv('home_prices_submission.csv', index=False)","b11dc2b4":"# Data Cleaning\nI decided to do a lot of manual imputation since a lot of the categories were ordinal. I wasnt confident that Simple Imputer would be able to map the values the way that I would have wanted.","919468c3":"Made conditions sets that were responses found in the data. Condition set 1 was found in many of the features. Each set is arranged from worst to best (in my view) and then these would be mapped using the cond_map function","4794ccbd":"# GridSearchCV with Pipelines\nI used XGBRegressor for no particular reason. I'm fairly new to this and am more than welcome to any critiques to my notebook!\nThank you for checking it out.","e6468bdc":"Get a list of category columns and number columns to do further refining of the data","3483b448":"To see which columns had missing data","d1f9690f":"Category and Number Lists have now been changed... reload it and fill all values with zero that are missing (except year features)","c69bb977":"To get the \"optimal\" parameters I ran the code below to obtain the values to plug into the XBGRegressor","b3183413":"# Pipeline Creation\nUsed a pipeline as I find it easy to implement and I found with dataset if I used pd.get_dummies I would resultingly get differing number of features between the validation set and the test set which would cause an error."}}