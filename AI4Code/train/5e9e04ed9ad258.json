{"cell_type":{"ded4be0d":"code","cff3c64c":"code","02cb2222":"code","50a5ac9a":"code","2fb38db5":"code","8381e1be":"code","86e4f254":"code","c7283e7b":"code","f5efee28":"code","32993e4d":"code","d25b8fbc":"code","4b0b6310":"code","2e2794f7":"code","e2826227":"code","c148e478":"code","c5545ce4":"code","7be6d314":"markdown","d418f8be":"markdown","62ee6a7c":"markdown","62059cd4":"markdown","91c29ba2":"markdown","924cca2b":"markdown","b7d20a7a":"markdown","696cba54":"markdown","5cc6dfdf":"markdown","e7f32b64":"markdown","ca0b71e7":"markdown","90abe26b":"markdown","c785abd3":"markdown","5f5e16ec":"markdown","4684f98d":"markdown","c4dc87ba":"markdown","4c39d6a1":"markdown","09011efb":"markdown","67b4dbf6":"markdown","16d44d85":"markdown","c969106d":"markdown","9b1b0aa7":"markdown","1d9c6667":"markdown","b1d24efe":"markdown","068ff2fb":"markdown","3659d868":"markdown","91463ba6":"markdown","e098891b":"markdown","7dd0118c":"markdown","db7a16b6":"markdown","a920a533":"markdown","75bbea9b":"markdown","c04db9a8":"markdown","8c13037d":"markdown","1e8f7c9f":"markdown","1c02a88a":"markdown","f44f8412":"markdown","c2603aae":"markdown","62f94837":"markdown","08ea232e":"markdown","af0e0936":"markdown","cc5183b2":"markdown","19625ca4":"markdown","e64b7c0a":"markdown","c71e3c27":"markdown","7197c635":"markdown","0a7f41b2":"markdown","56f3c4ac":"markdown","1f95b6f9":"markdown","fa1af724":"markdown"},"source":{"ded4be0d":"import requests\nimport json\n\nheaders = {'accept': 'application\/json','Content-Type': 'text\/plain'}\nparams = (('annotationTypes', '*'),('language', 'en'))\n\ndef get_json_object(text):\n    return requests.post('<our url>', headers=headers, params=params, data=text).json()\n\n# Output: json string\ndef get_json_str(json_obj):\n    return json.dumps(json_obj)\n\n# Output: beautified json string\ndef get_pretty_json(json_str):\n    return json.dumps(json_str, indent=4)\n\n# Output: List of themes\ndef get_themes(text):\n    json_obj = get_json_object(text)\n    json_array = json_obj[\"annotationDtos\"]\n    return json_array[-1][\"themes\"]","cff3c64c":"import tensorflow_hub as hub\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")","02cb2222":"def lst_to_str(word_list):\n    return ' '.join(word_list).strip()","50a5ac9a":"import numpy as np\nimport json\nimport os\nimport csv\nimport time\nfrom nltk.tokenize import sent_tokenize\n\nroot = '\/kaggle\/input\/dataset\/CORD-19-research-challenge\/'\nfolders = ['biorxiv_medrxiv\/biorxiv_medrxiv\/', 'comm_use_subset\/comm_use_subset\/', \n           'noncomm_use_subset\/noncomm_use_subset\/', 'custom_license\/custom_license\/']\n\n\ndef collect_sentences():\n    index_in_docs = 0\n    num_files_processed = 0\n    sentences_np_array = np.empty(100000000, dtype=object)\n\n    start = time.time()\n    for folder in folders:\n        for filename in os.listdir(root+folder):\n            if filename.endswith(\".json\"): \n                input_file_path = root+folder+filename\n                with open(input_file_path) as f:\n                    data = json.load(f)\n\n                    # Collect abstract sentences\n                    abstracts = data['abstract']\n                    for content in abstracts:\n                        abstract_para = content['text']\n                        sentences = sent_tokenize(abstract_para)\n                        for sentence in sentences:\n                            sentences_np_array[index_in_docs] = sentence\n                            index_in_docs += 1\n\n                    # Collect body sentences\n                    body_texts = data['body_text']\n                    for content in body_texts:\n                        body_para = content['text']\n                        sentences = sent_tokenize(body_para)\n                        for sentence in sentences:\n                            sentences_np_array[index_in_docs] = sentence\n                            index_in_docs += 1\n                num_files_processed += 1            \n                print('Num files processed: ' + str(num_files_processed))\n                print('Time taken since beginning = ' + str(time.time()-start))\n    np.save('sentences.npy', sentences_np_array) ","2fb38db5":"import json\nimport os\nimport csv\nimport time\nfrom nltk.tokenize import sent_tokenize\n\nroot = '\/kaggle\/input\/dataset\/CORD-19-research-challenge\/'\nfolders = ['biorxiv_medrxiv\/biorxiv_medrxiv\/', 'comm_use_subset\/comm_use_subset\/', \n           'noncomm_use_subset\/noncomm_use_subset\/', 'custom_license\/custom_license\/']\n\ndef collect_json_docs():\n    docs = np.empty(100000000, dtype=np.object) \n\n    index_in_docs = 0\n    num_files_processed = 0\n    num_docs_collected = 0\n\n    start = time.time()\n    for folder in folders:\n        for filename in os.listdir(root+folder):\n            if filename.endswith(\".json\"): \n                input_file_path = root+folder+filename\n                print(input_file_path)\n                with open(input_file_path) as f:\n                    data = json.load(f)\n\n                    # Collect paper title\n                    paper_title = data['metadata']['title']\n\n                    # Collect authors' names\n                    authors = data['metadata']['authors']\n                    authors_names = []\n\n                    for author in authors:\n                        first_name = author['first']\n                        middle_name = author['middle']\n                        last_name = author['last']\n                        author_name = first_name + ' ' + lst_to_str(middle_name) + ' ' + last_name\n                        authors_names.append(author_name)\n\n                    # Collect abstract sentences\n                    abstracts = data['abstract']\n                    for content in abstracts:\n                        abstract_para = content['text']\n                        section = content['section']\n                        sentences = sent_tokenize(abstract_para)\n    #                     para_themes = get_themes(abstract_para)\n                        for sentence in sentences:\n                            new_doc = {\n                                \"sentence\": sentence,\n                                \"section\": section,\n                                \"paper_title\": paper_title,\n                                \"authors\": authors_names,\n                                \"paragraph\": abstract_para\n    #                             \"para_themes\": para_themes\n                            }\n                            print(new_doc)\n                            docs[index_in_docs] = new_doc\n                            index_in_docs += 1\n                            num_docs_collected += 1\n\n                    # Collect body sentences\n                    body_texts = data['body_text']\n                    for content in body_texts:\n                        body_para = content['text']\n                        section = content['section']\n                        sentences = sent_tokenize(body_para)\n    #                     para_themes = get_themes(body_para)\n                        for sentence in sentences:\n                            new_doc = {\n                                \"sentence\": sentence,\n                                \"section\": section,\n                                \"paper_title\": paper_title,\n                                \"authors\": authors_names,\n                                \"paragraph\": body_para\n    #                             \"para_themes\": para_themes\n                            }\n                            print(new_doc)\n                            docs[index_in_docs] = new_doc\n                            index_in_docs += 1\n                            num_docs_collected += 1\n                num_files_processed += 1\n\n    np.save('docs', docs) # by default, allow pickle = True","8381e1be":"import numpy as np\ndocs = np.load('\/kaggle\/input\/jsondocs\/docs.npy', allow_pickle=True)","86e4f254":"def collect_sentences():\n    sentences=[]\n    for jsonobject in docs:\n        sentences.append(jsonobject['sentence'])","c7283e7b":"import tensorflow as tf\nimport time\n\n \ndef generate_embeddings():\n    start = time.time()\n    index=0\n    batch_size = 3000\n    num_rows=len(sentences)\n\n\n    embeddings = np.empty((num_rows,512), dtype=np.float32)\n    while index < num_rows:\n        end_index = index+batch_size\n        if end_index > num_rows:\n            break\n        embeddings[index:end_index,:] = embed(sentences[index:end_index])\n        index += batch_size\n\n    if index < num_rows:\n        embeddings[index:num_rows,:] = embed(sentences[index:num_rows])\n\n    np.save('embeddings\/embeddings.npy', embeddings) # removed from working directory","f5efee28":"!python -m pip install --upgrade faiss faiss-gpu","32993e4d":"import numpy as np\nimport faiss\n\ndef create_index():\n    embeddings = np.load('embeddings\/embeddings.npy',mmap_mode='r')\n    index = faiss.IndexScalarQuantizer(512,faiss.ScalarQuantizer.QT_6bit) \n    index.train(embeddings)\n    index.add(embeddings)\n\n    faiss.write_index(index, \"vector_6.index\")","d25b8fbc":"import faiss\nindex = faiss.read_index(\"\/kaggle\/input\/vector\/vector_6.index\", faiss.IO_FLAG_MMAP|faiss.IO_FLAG_READ_ONLY)  # load the index","4b0b6310":"import time\nquery = [\"What has been published concerning research and development and evaluation efforts of vaccines and therapeutics for COVID-19?\"]\nquery_vector = embed(query)\nquery_vector = np.asarray(query_vector, dtype=np.float32)\n\nstart = time.time()\nD, I = index.search(query_vector, 10)  ","2e2794f7":"!pip install json2html\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \nimport pandas as pd \nfrom json2html import *\nfrom IPython.core.display import display, HTML","e2826227":"stopwords = set(STOPWORDS)\n\nfor id_index in I[0]:\n    doc = docs[id_index]\n    html = json2html.convert(doc)\n    html = html.replace(\"<td>\", \"<td style='text-align:left'>\")\n    display(HTML(html))\n    themes_list = doc['themes']\n    final_theme_string = ''\n    for theme in themes_list:\n        words = theme.replace('-', ' ').split()\n        t = '_'.join(words)\n        final_theme_string = final_theme_string + ' ' + t\n        \n    # plot the WordCloud image  \n    if doc['themes'] and doc['themes'][0]:\n        wordcloud = WordCloud(width = 700,height = 200,stopwords = stopwords,min_font_size = 8, \n                              max_font_size=20, background_color='white', \n                              prefer_horizontal=1).generate(final_theme_string)\n        plt.figure(figsize = (10, 10), linewidth=10, edgecolor=\"#04253a\")\n        plt.imshow(wordcloud, interpolation=\"bilinear\") \n        plt.axis(\"off\") \n        plt.show() \n    display(HTML(\"<hr style='height:3px; color:black'>\"))","c148e478":"import pandas as pd\nwriter = pd.ExcelWriter('QueryResult1.xlsx', engine='xlsxwriter')\n\ndef writepaperdetails(query,retrivallines,subquestion):\n    print(subquestion)\n    print(retrivallines)\n    Rpaper_title=[]\n    Rsection=[]\n    Rsentence=[]\n    Rparagraph=[]\n    Rthemes=[]\n   \n    for retrivalline in retrivallines:\n        Rpaper_title.append(docs[retrivalline]['paper_title'])\n        Rsection.append(docs[retrivalline]['section'])\n        Rsentence.append(docs[retrivalline]['sentence'])\n        Rparagraph.append(docs[retrivalline]['paragraph'])\n        Rthemes.append(docs[retrivalline]['themes'])\n       \n    df = pd.DataFrame()\n    dfquery=pd.DataFrame()\n    df['PAPER_TITLE']=Rpaper_title\n    df['SECTION']=Rsection\n    df['SENTENCE']=Rsentence\n    df['PARAGRAPH']=Rparagraph\n    df['THEMES']=Rthemes\n    dfquery['QUERY']=[query]\n   \n   \n    dfquery.to_excel(writer, sheet_name=\"QUERY_\"+str(subquestion))\n    df.to_excel(writer, sheet_name=\"QUERY_\"+str(subquestion),startrow=4)","c5545ce4":"def generate_excel_files(query, I):\n    for i,q in enumerate(query):\n        writepaperdetails(q,I.tolist()[i],i)\n    writer.save()","7be6d314":"Task4: What do we know about vaccines and therapeutics?\n\n[Task4 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_4.xlsx)","d418f8be":"# Introduction:\nWith the growing risk of fast pace spread of COVID-19 across the globe, there is an extreme need for potential way forward or approaches to break the chain if not cure. Currently, there is significant research & literature around the similar situation during previous epidemics spread which may not be specific for current situation but still very valuable. This might provide us with right approaches and improved policy measures which will aid us to fight this battle. We, as AI & NLP researchers, hope to leverage this research, ideas, reports or any data to find close to accurate and quickly actionable insights to control the spread via medical or non-Pharma interventions.\n\nWith this, we hope to bring in our approach \/ engine which can help community members to find right literature using the methods of NLP, Deep Learning & Search. The approach mainly uses Facebook AI Similarity Search (FAISS) for indexing documents and Google's Universal Sentence Encoder for the text embeddings.\n\nThe approach was designed keeping in mind the task of finding the right insights for \u201cWhat do we know about non-pharmaceutical interventions?\u201d but we expanded its reach to solve sub-questions or any relevant questions from all 10 tasks specified in CORD-19 Challenge by Kaggle community.","62ee6a7c":"## Dataset: \n\n33375 papers across 4 folders: \n\n* biorxiv_medrxiv - 1053 papers\n\n* comm_use_subset - 9315 papers\n\n* custom_license - 20657 papers\n\n* noncom_use_subset - 2350 papers\n","62059cd4":"Task6: What do we know about non-pharmaceutical interventions?\n\n[Task6 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_6.xlsx)","91c29ba2":"# Indexing stage","924cca2b":"## Search the index","b7d20a7a":"# Load embeddings from npy file and index the embeddings into FAISS, and save the index in correct directory. \n### Please note: 'embeddings.npy' file is not available in notebook - so this cell will throw an error. \n### Index was created in AWS Sagemaker and uploaded in data directory in \/kaggle\/input\/vector\/vector_6.index\n","696cba54":"# Why FAISS for indexing the embeddings?\nFaiss is a library developed by Facebook AI Research, for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. Faiss provides lightning fast search through 1 billion vectors.\n\nTotal size of the index after indexing the embeddings was around 12 GB. Faiss provides a Scalar Quantization technique using which we were able to reduce the size to around 3 GB (32 bit float to 6 bit representation).\n\nWe used L2 euclidean distance provided by FAISS, for computing distance between embeddings. Since we are computing similarity using L2 distance, loss of accuracy due to quantization does not affect query results. \n\n\nSearch operation in our case is linear and time complexity is O(n)(n is total number of embeddings indexed).","5cc6dfdf":"### Generate embeddings for sentences collected above. We ran the below code in AWS Sagemaker because of RAM issue in Kaggle (resource exhausted error). Total size of embeddings.npy was around 12 GB. It is removed from notebook because of lack of space.","e7f32b64":"## <font color='red'>RUN BELOW CELL! ---> <\/font>","ca0b71e7":"# Load docs.npy\n### docs.npy file contains json objects - each json object contains the section_name, author names, paper title, sentence, themes, and paragraph in which the sentence occurs.","90abe26b":"# Querying stage","c785abd3":"## Further scope of our project:\n* Extend our approach to work with the latest dataset - additional 40k papers approx\n* Re-rank the query results to answer specific questions by considering keywords - for eg. If a query contains 'covid-19' or 'coronavirus', it is possible in our approach that some results related to pneumonia or ebola could come up.\n* Quantitative data like time, number of patients, number of beds, etc. can be extracted from the query results\n* Experiment with other index types that FAISS supports\n* Searching can done faster by applying clustering on fiass indexes and search only the specific cluser whose centroid is closest to the query embedding.\n* Product Quantizers in FAISS can be used for further compression of the index.","5f5e16ec":"# Results for tasks in excel files:\nClick on the results corresponding to each task to see results fetched by our system for each subquestion in the task. \n\n## <font color='blue'>NOTE: Papers which do not have 'abstract' section won't have themes and thus you will be able to see empty cells in 'themes' column when such papers are returned by the system - since we obtain themes from abstract.<\/font>","4684f98d":"# Search docs matching user query","c4dc87ba":"# Team members:\nWe, the Text Analytics team, are pleased to provide to the Kaggle community, an Information Retrieval system using NLP, Machine Learning and Deep Learning techniques.  \n\n[Bharat Hegde], [Rohit Rangarajan], [Prathamesh Karmalkar], [Harsha Gurulingappa], [Gerard Megaro]","4c39d6a1":"![flowchart.png](attachment:flowchart.png)","09011efb":"# Our approach:","67b4dbf6":"# Write to excel sheet","16d44d85":"![diagram2.png](attachment:diagram2.png)","c969106d":"# Define function to join words in a list into one string\n### This method is useful when extracting author names from the provided json files","9b1b0aa7":"## <font color='red'>RUN BELOW CELL! ---> <\/font>","1d9c6667":"# Visualize the results!","b1d24efe":"Task1: What is known about transmission, incubation, and environmental stability?\n\n[Task1 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_1.xlsx)","068ff2fb":"# Collect json docs\n#### The lines calling the get_themes() method have been commented since the method will only work in our internal environment","3659d868":"Task3: What do we know about virus genetics, origin, and evolution?\n\n[Task3 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_3.xlsx)","91463ba6":"## <font color='red'>RUN BELOW CELL! ---> <\/font>","e098891b":"Task2: What do we know about COVID-19 risk factors?\n\n[Task2 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_2.xlsx)","7dd0118c":"## <font color='red'>RUN BELOW CELL! ---> <\/font>","db7a16b6":"# Algorithm for theme extraction:\nTheme extraction helps to define the context and content of a conversation providing a highly valuable combination of contextual phrases. Phrase themes provide an excellent view of the context of conversation and is useful on all content length from one line to hundred-page document. Theme identification is one of the most fundamental tasks in qualitative research. System takes unstructured data as input and extracts semantic themes based on context of the data.\n\nStep 1: Pre-process the input verbatim for identifying the themes\n\nStep 2: Identify & split verbatim into multiple sentences to identify individual themes at sentence level (Themes specific to each question)\n\nStep 3: Algorithm extracts the themes using tokenization, PoS tags, chunks, dependency parse tree as features of the input verbatim.\n\nStep 4: Extract first set of themes from input verbatim\n\nStep 5: Algorithm uses heuristic approach to prune contextually less significant chains from the extracted theme set\n\n## The code below makes use of our internal tool for extracting themes from text. This will not work in external networks. It has been provided only for documentation purposes.","a920a533":"# Extract embeddings from the 'sentence' field of docs collected above - using Google Universal Sentence Encoder - and save them!\n","75bbea9b":"## Load index from directory where it was saved. Index can be saved and loaded from disk.","c04db9a8":"### Extract the sentences from docs loaded above","8c13037d":"# Our approach:\n* We extract docs from the dataset using NLTK, Theme Extractor (proprietary algorithm to be published in September 2020) and JSON parser. Each doc is a json object of the form:\n        {\n            'sentence': '....',\n            'section': '....',\n            'paper_title': '....',\n            'authors': [],\n            'paragraph': '....',\n            'themes': []\n        }\n* The themes are identified from the abstract of the paper in which the sentence appears.        \n\n* Once all docs are extracted, we obtain the sentence embeddings for all sentences extracted from each of these docs, using Google\u2019s Universal Sentence Encoder. \n\n* We then index all the embeddings into one of the available FAISS indexes, called the ScalarQuantizer (https:\/\/github.com\/facebookresearch\/faiss\/wiki\/Faiss-building-blocks:-clustering,-PCA,-quantization).  \n\n* Then, when user makes a query, we convert the query to its text embedding using the same Universal Sentence Encoder and query our index. \n\n* The index returns the id of the embeddings (along with L2 distance) which are most similar to the query embedding. We then map those ids to the corresponding doc objects to display the details to the user.\n\n* We also provide additional information, such as the paragraph context in which the sentence appears, the paper containing the sentence, the authors of the paper, title of that paper, and section name. Therefore the  user can look into the paper for more detais regarding the query.\n\n* We also provide a ThemeCloud which gives a pictorial representation of the themes being talked about in the paper.  ","1e8f7c9f":"# Pros and cons of our approach:\n## Pros:\n* Using embeddings at a sentence level helps to capture maximum information, and thus is highly effective when searching for contextually similar sentences. Representing the documents at paragraph level (with fixed vector or average of sentence embeddings) causes loss of information. Hence sentence-level embeddings helps us.\n\n* Even though we end up in more number of embeddings due to more number of sentences (compared to number of paragraphs), the query results are very accurate.\n\n* Scalable approach to information retrieval\n\n## Cons:\n* Response time after query execution is a bit slow currently, due to the index we are using - it takes on an average 75 seconds for response\n* Hard disk and RAM space is crucial for our approach - to store embeddings, index, text documents, etc. and also to compute embeddings\n* Recommendation for Kaggle: We had to make use of AWS Sagemaker, which is a costly affair. Please provide better RAM support.\n* The dataset has now been increased to more papers, but we are not using the latest dataset due to storage constraints on Kaggle\n","1c02a88a":"# Problem statement:\nWe started with the goal of solving the task \"What do we know about non-pharmaceutical interventions?\", but ended up developing a solution which would work across all given tasks and questions.","f44f8412":"# Collect sentences as python list\n#### In the below code, we make use of NLTK's sentence tokenizer to extract sentences out of paragraphs. We also tried spaCy's tokenizer, but found this to be faster.","c2603aae":"Task10: What has been published about information sharing and inter-sectoral collaboration?\n\n[Task10 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_10.xlsx)","62f94837":"# Download Google Universal Sentence Encoder for extracting sentence embedding from text","08ea232e":"![diagram1.png](attachment:diagram1.png)","af0e0936":"Task8: What do we know about diagnostics and surveillance?\n\n[Task8 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_8.xlsx)","cc5183b2":"## <font color='red'>RUN BELOW CELL! ---> <\/font>","19625ca4":"# Why Google's Universal Sentence Encoder?\n\n    We tried Sentence Transformer for the embeddings, it had 768 dimension representation and \n    thus it occupied more size (768 * ~ 7 million sentences = 18 GB approx). Also, time taken \n    to generate the embeddings for 7 million sentences was around 3.5 hours with K80 GPU on AWS Sagemaker.\n    You can refer to Sentence Tranformer here: https:\/\/github.com\/UKPLab\/sentence-transformers\n    \n    But in case of Google Universal Sentence Encoder, it has 512 dimensional vector embedding \n    for each sentence, thereby occupying less space (512 * ~ 7 million sentences = 12 GB approx), \n    but equally effective as Sentence Transformer. Time complexity is O(n)(n is length of sentence).\n    Embeddings were computed on Sagemaker, with configuration 8 virtual CPUs, one V100 GPU of \n    16 GB memory, and 61 GB RAM. This took around 20 minutes to compute all embeddings - way faster. \n    It took 1325 seconds to compute 7191466 embeddings.    ","e64b7c0a":"## <font color='red'>RUN BELOW CELL! ---> <\/font>","c71e3c27":"## <font color='red'>RUN BELOW CELL! ---> <\/font>","7197c635":"## <font color='red'>DISCLAIMER: If you want to run our system with your own input, we have mentioned in RED, the cells you need to run. Thank you! <\/font>","0a7f41b2":"# References:\n1. FAISS: https:\/\/engineering.fb.com\/data-infrastructure\/faiss-a-library-for-efficient-similarity-search\/ \n\n2. Universal Sentence Encoder: https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4 \n\n3. ThemeCloud\/WordCloud: https:\/\/amueller.github.io\/word_cloud\/generated\/wordcloud.WordCloud.html \n\n4. spaCy: https:\/\/spacy.io\/\n\n5. NLTK: https:\/\/www.nltk.org\/\n\n6. Sentence Transformer: https:\/\/github.com\/UKPLab\/sentence-transformers","56f3c4ac":"Task9: What has been published about ethical and social science considerations?\n    \n[Task9 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_9.xlsx)    ","1f95b6f9":"# Install FAISS to use GPU","fa1af724":"Task5: What has been published about medical care?\n\n[Task5 results](https:\/\/sagemaker-eu-central-1-922643591083.s3.eu-central-1.amazonaws.com\/KaggleOut\/QueryResult_TASK_5.xlsx)"}}