{"cell_type":{"0ca2d6a6":"code","1f187aba":"code","c4228190":"code","d776ed1a":"code","06209c1b":"code","7584e09a":"code","9cbda032":"code","a0f3a98f":"code","91319b72":"code","ad8364e0":"code","12720987":"code","44b979a4":"code","8450ebd0":"code","4b2d0841":"code","3cd1416c":"code","4f3b1d86":"code","931ae76f":"code","2824eb51":"code","2f546898":"code","bd00139b":"code","eb49afbd":"code","5d72c8d5":"code","4c87d161":"code","905be666":"code","b1e27217":"code","f30fe7ed":"code","456e3982":"code","05d1911c":"code","c955bd3b":"code","6bcae57e":"code","0fea2e3d":"code","923bc300":"code","b6592ce5":"code","13a3e78a":"code","d91b7de0":"code","68ef8f15":"code","946740f6":"code","854f24de":"code","747addb6":"code","e8849dc7":"code","11165c61":"code","67b2a35d":"code","5bf0e244":"code","30923296":"code","cce375ba":"code","8578ce07":"code","439fadef":"code","4868ec9a":"code","ac868bc5":"code","b56b7c38":"code","f9303895":"code","bc41d86c":"code","329f4785":"code","823729b9":"code","4b675f3f":"code","9e68b9b0":"code","9da30dd6":"code","3a6e14f0":"code","775f564a":"markdown","c091dcee":"markdown","78e81060":"markdown","cafd2ff8":"markdown","69e472a4":"markdown","16765021":"markdown","3103d91a":"markdown","e9693aef":"markdown","bb00bd4e":"markdown","4a455d2b":"markdown","3f46c344":"markdown","1b3822f1":"markdown","a3169265":"markdown","476e30b2":"markdown","322c23cf":"markdown","0ea6beb5":"markdown","61c002ce":"markdown","023bb018":"markdown"},"source":{"0ca2d6a6":"#! pip install -q kaggle","1f187aba":"#from google.colab import files","c4228190":"#files.upload()","d776ed1a":"#!unzip \"\/content\/musicnet_midis.zip\"","06209c1b":"#!unzip \"\"","7584e09a":"#! mkdir ~\/.kaggle","9cbda032":"#! cp kaggle.json ~\/.kaggle\/","a0f3a98f":"#! chmod 600 ~\/.kaggle\/kaggle.json","91319b72":"#! kaggle datasets list","ad8364e0":"#! kaggle datasets download -d imsparsh\/musicnet-dataset","12720987":"#! unzip \/content\/musicnet-dataset.zip","44b979a4":"!pip install mido","8450ebd0":"from mido import MidiFile, MidiTrack, Message\nimport mido\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom keras.models import load_model\nimport os\nfrom tqdm import *","4b2d0841":"!pip install pygame","3cd1416c":"!pip install IPython","4f3b1d86":"!pip install music21","931ae76f":"import pygame\nimport IPython\nimport matplotlib.pyplot as plt\nimport librosa.display\nfrom IPython import *\nfrom music21 import *\nfrom music21 import converter, instrument, note, chord, stream, midi #just so that I remember what to use, you could delete this line.\nimport glob\nimport time\nimport numpy as np\nimport keras.utils as utils\nimport pandas as pd","2824eb51":"#visualise the tracks in midi file.\nmid=MidiFile('..\/input\/musicnet-dataset\/musicnet_midis\/musicnet_midis\/Beethoven\/2313_qt15_1.mid',clip=True)\n\nmid.tracks","2f546898":"#visualise each message in one track\nfor i in mid.tracks[1] :\n    print(i)","bd00139b":"!pip install --upgrade music21","eb49afbd":"!add-apt-repository ppa:mscore-ubuntu\/mscore-stable -y\n!apt-get update\n!apt-get install musescore -y","5d72c8d5":"!apt-get install xvfb -y","4c87d161":"!sh -e \/etc\/init.d\/x11-common start","905be666":"import os\nos.putenv('DISPLAY', ':99.0')","b1e27217":"!start-stop-daemon --start --pidfile \/var\/run\/xvfb.pid --make-pidfile --background --exec \/usr\/bin\/Xvfb -- :99 -screen 0 1024x768x24 -ac +extension GLX +render -noreset","f30fe7ed":"from music21 import *\nus = environment.UserSettings()\nus['musescoreDirectPNGPath'] = '\/usr\/bin\/mscore'\nus['directoryScratch'] = '\/tmp'\n","456e3982":"# Melody-RNN Format is a sequence of 8-bit integers indicating the following:\n# MELODY_NOTE_ON = [0, 127] # (note on at that MIDI pitch)\nMELODY_NOTE_OFF = 128 # (stop playing all previous notes)\nMELODY_NO_EVENT = 129 # (no change from previous event)\n# Each element in the sequence lasts for one sixteenth note.\n# This can encode monophonic music only.\n\ndef streamToNoteArray(stream):\n    \"\"\"\n    Convert a Music21 sequence to a numpy array of int8s into Melody-RNN format:\n        0-127 - note on at specified pitch\n        128   - note off\n        129   - no event\n    \"\"\"\n    # Part one, extract from stream\n    total_length = np.int(np.round(stream.flat.highestTime \/ 0.25)) # in semiquavers\n    stream_list = []\n    for element in stream.flat:\n        if isinstance(element, note.Note):\n            stream_list.append([np.round(element.offset \/ 0.25), np.round(element.quarterLength \/ 0.25), element.pitch.midi])\n        elif isinstance(element, chord.Chord):\n            stream_list.append([np.round(element.offset \/ 0.25), np.round(element.quarterLength \/ 0.25), element.sortAscending().pitches[-1].midi])\n    np_stream_list = np.array(stream_list, dtype=np.int)\n    df = pd.DataFrame({'pos': np_stream_list.T[0], 'dur': np_stream_list.T[1], 'pitch': np_stream_list.T[2]})\n    df = df.sort_values(['pos','pitch'], ascending=[True, False]) # sort the dataframe properly\n    df = df.drop_duplicates(subset=['pos']) # drop duplicate values\n    # part 2, convert into a sequence of note events\n    output = np.zeros(total_length+1, dtype=np.int16) + np.int16(MELODY_NO_EVENT)  # set array full of no events by default.\n    # Fill in the output list\n    for i in range(total_length):\n        if not df[df.pos==i].empty:\n            n = df[df.pos==i].iloc[0] # pick the highest pitch at each semiquaver\n            output[i] = n.pitch # set note on\n            output[i+n.dur] = MELODY_NOTE_OFF\n    return output\n\n\ndef noteArrayToDataFrame(note_array):\n    \"\"\"\n    Convert a numpy array containing a Melody-RNN sequence into a dataframe.\n    \"\"\"\n    df = pd.DataFrame({\"code\": note_array})\n    df['offset'] = df.index\n    df['duration'] = df.index\n    df = df[df.code != MELODY_NO_EVENT]\n    df.duration = df.duration.diff(-1) * -1 * 0.25  # calculate durations and change to quarter note fractions\n    df = df.fillna(0.25)\n    return df[['code','duration']]\n\n\ndef noteArrayToStream(note_array):\n    \"\"\"\n    Convert a numpy array containing a Melody-RNN sequence into a music21 stream.\n    \"\"\"\n    df = noteArrayToDataFrame(note_array)\n    melody_stream = stream.Stream()\n    for index, row in df.iterrows():\n        if row.code == MELODY_NO_EVENT:\n            new_note = note.Rest() # bit of an oversimplification, doesn't produce long notes.\n        elif row.code == MELODY_NOTE_OFF:\n            new_note = note.Rest()\n        else:\n            new_note = note.Note(row.code)\n        new_note.quarterLength = row.duration\n        melody_stream.append(new_note)\n    return melody_stream\n","05d1911c":"wm_mid = converter.parse(\"..\/input\/musicnet-dataset\/musicnet_midis\/musicnet_midis\/Beethoven\/2313_qt15_1.mid\")\nwm_mid.show()\nwm_mel_rnn = streamToNoteArray(wm_mid)\nprint(wm_mel_rnn)\nnoteArrayToStream(wm_mel_rnn).show()","c955bd3b":"#gettinng the note on values from the messages on 50 midi files\nnote_on=[]\nn=50\nfor m in range(n):\n    mid=MidiFile('..\/input\/musicnet-dataset\/musicnet_midis\/musicnet_midis\/Beethoven\/'+os.listdir('..\/input\/musicnet-dataset\/musicnet_midis\/musicnet_midis\/Beethoven')[m],clip=True)\n    for j in range(len(mid.tracks)):\n        for i in mid.tracks[j] :\n            if str(type(i))!=\"<class 'mido.midifiles.meta.MetaMessage'>\" :\n                x=str(i).split(' ')\n                if x[0]=='note_on':\n                    note_on.append(int(x[2].split('=')[1]))","6bcae57e":"#making data to train\ntraining_data=[]\nlabels=[]\nfor i in range(20,len(note_on)):\n    training_data.append(note_on[i-20:i])\n    labels.append(note_on[i])","0fea2e3d":"\ntraining_data[0]\nlabels[0]","923bc300":"different_labels=set(labels)","b6592ce5":"model1= Sequential()\n\nmodel1.add(LSTM(200,input_shape=(10,1),unroll=True,return_sequences=True))\nmodel1.add(Dropout(0.4))\nmodel1.add(LSTM(100))\nmodel1.add(Dense(100,'relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(1,'relu'))\n\nmodel1.compile(loss='MSE',optimizer='adam')\n","13a3e78a":"from keras.layers import *\nfrom keras.models import *\nfrom keras.callbacks import *\nimport keras.backend as K\n\n#the residual and skip connection used, only helps to converge faster. \nmodel = Sequential()\n    \n#embedding layer\nmodel.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n\nmodel.add(Conv1D(64,3, padding='causal',activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(MaxPool1D(2))\n    \nmodel.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\nmodel.add(Dropout(0.2))\nmodel.add(MaxPool1D(2))\n\nmodel.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\nmodel.add(Dropout(0.2))\nmodel.add(MaxPool1D(2))\n          \n#model.add(Conv1D(256,5,activation='relu'))    \nmodel.add(GlobalMaxPool1D())\n    \nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(1, activation='relu'))\n    \nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n\nmodel.summary()","d91b7de0":"early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n\ntraining_data=np.array(training_data)\ntraining_data=training_data.reshape((training_data.shape[0],training_data.shape[1],1))\nlabels=np.array(labels)\n\nX_train, X_test, y_train, y_test = train_test_split(training_data, labels, test_size=0.05, random_state=42)","68ef8f15":"X_train.shape","946740f6":"model.fit(X_train,y_train,epochs=200,batch_size=32 * strategy.num_replicas_in_sync,\n          validation_data=(X_test,y_test),callbacks=[early_stop])","854f24de":"model.save('musicnetgen.h5')","747addb6":"#prediction \n\nn=200\nstarter_notes=training_data[0]\nx=training_data[0].reshape(1,20,1)\ntune=list(training_data[0].reshape(-1,))\nfor i in range(n) :\n    pred=int(model.predict(x)[0][0])\n    if round(pred)==round(tune[-1]):\n        p=np.random.choice(['a','b','c'])\n        if p=='a':\n            pred=65\n        elif p=='b':\n            pred=60\n        else:\n            pred=70\n    tune.append(pred)\n    x=tune[-10:]\n    x=np.array(x)\n    x=x.reshape(1,10,1)\n    \ntune=list(np.array(tune).astype('float32'))","e8849dc7":"#encoder\n\noffset = 0\noutput_notes = []\n# create note and chord objects based on the values generated by the model\nfor patterns in tune:\n    pattern=str(patterns)\n    # pattern is a chord\n    if ('.' in pattern) or pattern.isdigit():\n        notes_in_chord = pattern.split('.')\n        notes = []\n        for current_note in notes_in_chord:\n            new_note = note.Note(int(current_note))\n            new_note.storedInstrument = instrument.Piano()\n            notes.append(new_note)\n        new_chord = chord.Chord(notes)\n        new_chord.offset = offset\n        output_notes.append(new_chord)\n    # pattern is a note\n    else:\n        new_note = note.Note(pattern)\n        new_note.offset = offset\n        new_note.storedInstrument = instrument.Piano()\n        output_notes.append(new_note)\n    # increase offset each iteration so that notes do not stack\n    offset += 0.5","11165c61":"import time\nmidi_files = glob.glob(\"..\/input\/musicnet-dataset\/musicnet_midis\/musicnet_midis\/Beethoven\/*.mid\") # this won't work, no files there.\n\ntraining_arrays = []\nfor f in midi_files:\n    start = time.perf_counter()\n    try:\n        s = converter.parse(f)\n    except:\n        continue\n#     for p in s.parts: # extract all voices\n#         arr = streamToNoteArray(p)\n#         training_arrays.append(p)\n    arr = streamToNoteArray(s.parts[0]) # just extract first voice\n    training_arrays.append(arr)\n    print(\"Converted:\", f, \"it took\", time.perf_counter() - start)\n\ntraining_dataset = np.array(training_arrays)\nnp.savez('melody_training_dataset.npz', train=training_dataset)","67b2a35d":"# Training Hyperparameters:\nVOCABULARY_SIZE = 130 # known 0-127 notes + 128 note_off + 129 no_event\nSEQ_LEN = 30\nBATCH_SIZE = 64\nHIDDEN_UNITS = 256\nEPOCHS = 30\nSEED = 2345  # 2345 seems to be good.\nnp.random.seed(SEED)\n\n## Load up some melodies I prepared earlier...\nwith np.load('..\/input\/melody-training-datasetnpz\/melody_training_dataset.npz', allow_pickle=True) as data:\n    train_set = data['train']\n\nprint(\"Training melodies:\", len(train_set))","5bf0e244":"def slice_sequence_examples(sequence, num_steps):\n    \"\"\"Slice a sequence into redundant sequences of lenght num_steps.\"\"\"\n    xs = []\n    for i in range(len(sequence) - num_steps - 1):\n        example = sequence[i: i + num_steps]\n        xs.append(example)\n    return xs\n\ndef seq_to_singleton_format(examples):\n    \"\"\"\n    Return the examples in seq to singleton format.\n    \"\"\"\n    xs = []\n    ys = []\n    for ex in examples:\n        xs.append(ex[:-1])\n        ys.append(ex[-1])\n    return (xs,ys)\n\n# Prepare training data as X and Y.\n# This slices the melodies into sequences of length SEQ_LEN+1.\n# Then, each sequence is split into an X of length SEQ_LEN and a y of length 1.\n\n# Slice the sequences:\nslices = []\nfor seq in train_set:\n    slices +=  slice_sequence_examples(seq, SEQ_LEN+1)\n\n# Split the sequences into Xs and ys:\nX, y = seq_to_singleton_format(slices)\n# Convert into numpy arrays.\nX = np.array(X)\ny = np.array(y)\n\n# Look at the size of the training corpus:\nprint(\"Total Training Corpus:\")\nprint(\"X:\", X.shape)\nprint(\"y:\", y.shape)\nprint()\n\n# Have a look at one example:\nprint(\"Looking at one example:\")\nprint(\"X:\", X[95])\nprint(\"y:\", y[95])\n# Note: Music data is sparser than text, there's lots of 129s (do nothing)\n# and few examples of any particular note on.\n# As a result, it's a bit harder to train a melody-rnn.","30923296":"# Do some stats on the corpus.\nall_notes = np.concatenate(train_set)\nprint(\"Number of notes:\")\nprint(all_notes.shape)\nall_notes_df = pd.DataFrame(all_notes)\nprint(\"Notes that do appear:\")\nunique, counts = np.unique(all_notes, return_counts=True)\nprint(unique)\nprint(\"Notes that don't appear:\")\nprint(np.setdiff1d(np.arange(0,129),unique))\n\nprint(\"Plot the relative occurences of each note:\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#plt.style.use('dark_background')\nplt.bar(unique, counts)\nplt.yscale('log')\nplt.xlabel('melody RNN value')\nplt.ylabel('occurences (log scale)')","cce375ba":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.utils.data_utils import get_file\nfrom keras.models import load_model\n\n# build the model: 2-layer LSTM network.\n# Using Embedding layer and sparse_categorical_crossentropy loss function \n# to save some effort in preparing data.\n\nprint('Build model...')\nmodel_train = Sequential()\nmodel_train.add(Embedding(VOCABULARY_SIZE, HIDDEN_UNITS, input_length=SEQ_LEN))\n\n# LSTM part\nmodel_train.add(LSTM(HIDDEN_UNITS, return_sequences=True))\nmodel_train.add(LSTM(HIDDEN_UNITS))\n\n# Project back to vocabulary\nmodel_train.add(Dense(VOCABULARY_SIZE, activation='softmax'))\nmodel_train.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\nmodel_train.summary()","8578ce07":"model_train.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS)\nmodel_train.save(\"zeldic-rnn.h5\")","439fadef":"# Build a decoding model (input length 1, batch size 1, stateful)\nmodel_dec = Sequential()\nmodel_dec.add(Embedding(VOCABULARY_SIZE, HIDDEN_UNITS, input_length=1, batch_input_shape=(1,1)))\n# LSTM part\nmodel_dec.add(LSTM(HIDDEN_UNITS, stateful=True, return_sequences=True))\nmodel_dec.add(LSTM(HIDDEN_UNITS, stateful=True))\n\n# project back to vocabulary\nmodel_dec.add(Dense(VOCABULARY_SIZE, activation='softmax'))\nmodel_dec.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\nmodel_dec.summary()\n# set weights from training model\n#model_dec.set_weights(model_train.get_weights())\nmodel_dec.load_weights(\"zeldic-rnn.h5\")","4868ec9a":"def sample(preds, temperature=1.0):\n    \"\"\" helper function to sample an index from a probability array\"\"\"\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n## Sampling function\n\ndef sample_model(seed, model_name, length=400, temperature=1.0):\n    '''Samples a musicRNN given a seed sequence.'''\n    generated = []  \n    generated.append(seed)\n    next_index = seed\n    for i in range(length):\n        x = np.array([next_index])\n        x = np.reshape(x,(1,1))\n        preds = model_name.predict(x, verbose=0)[0]\n        next_index = sample(preds, temperature)        \n        generated.append(next_index)\n    return np.array(generated)","ac868bc5":"model_dec.reset_states() # Start with LSTM state blank\no = sample_model(60, model_dec, length=127, temperature=15.0) # generate 8 bars of melody\n\nmelody_stream = noteArrayToStream(o) # turn into a music21 stream\nmelody_stream.show() # show the score.\n","b56b7c38":"print(o)","f9303895":"for i in range(len(melody_stream)):\n  print(melody_stream[i])","bc41d86c":"#sp = midi.realtime.StreamPlayer(melody_stream)\n#sp.play()","329f4785":"midi_stream = stream.Stream(melody_stream)","823729b9":"midi_stream.write('midi','melody_stream.mid')","4b675f3f":"pip install pydub","9e68b9b0":"from collections import defaultdict\nfrom mido import MidiFile\nfrom pydub import AudioSegment\nfrom pydub.generators import Sine\n\ndef note_to_freq(note, concert_A=440.0):\n  '''\n  from wikipedia: http:\/\/en.wikipedia.org\/wiki\/MIDI_Tuning_Standard#Frequency_values\n  '''\n  return (2.0 ** ((note - 69) \/ 12.0)) * concert_A\n\nmid = MidiFile(\".\/melody_stream.mid\")\noutput = AudioSegment.silent(mid.length * 1000.0)\n\ntempo = 100 # bpm\n\ndef ticks_to_ms(ticks):\n  tick_ms = (60000.0 \/ tempo) \/ mid.ticks_per_beat\n  return ticks * tick_ms\n  \n\nfor track in mid.tracks:\n  # position of rendering in ms\n  current_pos = 0.0\n\n  current_notes = defaultdict(dict)\n  # current_notes = {\n  #   channel: {\n  #     note: (start_time, message)\n  #   }\n  # }\n  \n  for msg in track:\n    current_pos += ticks_to_ms(msg.time)\n\n    if msg.type == 'note_on':\n      current_notes[msg.channel][msg.note] = (current_pos, msg)\n    \n    if msg.type == 'note_off':\n      start_pos, start_msg = current_notes[msg.channel].pop(msg.note)\n  \n      duration = current_pos - start_pos\n  \n      signal_generator = Sine(note_to_freq(msg.note))\n      rendered = signal_generator.to_audio_segment(duration=duration-50, volume=-20).fade_out(100).fade_in(30)\n\n      output = output.overlay(rendered, start_pos)\n\noutput.export(\"melody_stream.wav\", format=\"wav\")","9da30dd6":"IPython.display.Audio('.\/melody_stream.wav')","3a6e14f0":"# figure out where the user settings are kept.\n# from music21 import *\n# us = environment.UserSettings()\n# us.getSettingsPath()","775f564a":"#### Peeping into data.","c091dcee":"##### sampling from model","78e81060":"#### Convert between Midi file and numpy array in melody format.","cafd2ff8":"##### Training RNN","69e472a4":"#### Music21 and MuseScore set up","16765021":"Normal LSTM model.","3103d91a":"This implementation is for simpler models. You skip it and go for the complex one. Basically this one is just experimental.","e9693aef":"#### kaggle data collection (not needed while working in kaggle because the data is already in kaggle)","bb00bd4e":"#### Setting up Kaggle Dataset (run this section only while working in colab environment)","4a455d2b":"#### Model code that is simple. ","3f46c344":"WAVENET without residual and skip connection. (You can implement it. It was experimental for me)","1b3822f1":"This small encoder and decoder code is from Puru Behl's. Well with the model I am not using it as well. So if u want you can use it if you are using the simpler model otherwise move forward.","a3169265":"Setting up the environment so that we can visualise the notes in old school sheet style. Basically these libraries run upon linux environment. So we are setting it up so that it can run in windows as well.","476e30b2":"#### Code complex","322c23cf":"Music is more complex than regular text that we feed into sequential model. The quantisation of music will give us Notes.\n\nCombinations of notes become chords. And together in a pattern they make beautiful melody.","0ea6beb5":"This is the model I am running. ","61c002ce":"We are using Music21 library to read MIDI music files and convert to an array of notes. And also to convert the predicted note array to music stream.\n\n* All complex rythms are simplified to sixteenth note versions.\n* Chords are simplified to the highest note.\n\nThis encoding and decoding is borrowed from Melody-RNN's code. ","023bb018":"#### Showing the Notes"}}