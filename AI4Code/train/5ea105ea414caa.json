{"cell_type":{"57431ef9":"code","8ad3e86a":"code","96032250":"code","546f54e5":"code","c84c3573":"code","206603b6":"code","60513c3b":"code","1ddd5562":"code","a40cbcd3":"code","c805ed7a":"code","f0e512a8":"code","f9c59b9b":"code","b7df67bd":"code","f0905bf4":"code","ff59cdac":"code","67d7d8eb":"code","7908685b":"code","63853883":"code","2807c291":"markdown","07136429":"markdown","bd72b01a":"markdown","d98885c4":"markdown","f7e661e8":"markdown","a5bc0acc":"markdown","0dfd39aa":"markdown","7b7a0826":"markdown","80afebcf":"markdown","03c12f3e":"markdown","12a1b147":"markdown","9bf4e8cb":"markdown","09684948":"markdown"},"source":{"57431ef9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8ad3e86a":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntrain","96032250":"test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\ntest","546f54e5":"cat_features = [col for col in test.columns if test[col].dtype == object]\nprint(\"Total categorical features:\", len(cat_features))","c84c3573":"num_features = [col for col in test.columns if test[col].dtype != object]\nnum_features.remove('id') #Since ID is numerical but has no relative information to provide\nprint(\"Total numerical features:\", len(num_features))","206603b6":"train[num_features].describe()","60513c3b":"train[cat_features].describe()","1ddd5562":"import seaborn as sns\n\nfig, ax = plt.subplots(figsize=(13, 13))\ncorrelation = train.corr(method='spearman')\nsns.heatmap(correlation, cmap='vlag', center=0, annot=True, ax=ax)","a40cbcd3":"def create_features(df): \n    #Convert cat to sparse\n    df[cat_features] = df[cat_features].apply(lambda x: [ord(c) - 65 for c in x])\n    \n    return df","c805ed7a":"train = create_features(train)\ntrain","f0e512a8":"test = create_features(test)\ntest","f9c59b9b":"all_cols = test.columns.tolist()\nall_cols.remove('id')\nprint(\"Total number of features:\", len(all_cols))\nnum_features = [col for col in all_cols if not col.startswith('cat')]\ncat_features = [all_cols.index(c) for c in cat_features]","b7df67bd":"import lightgbm as lgbm\n\nseed = np.random.randint(1, 1e5)\nprint(\"Using SEED:\", seed)\n\nparams = {\n    \"objective\": \"rmse\",\n    \"metric\": \"rmse\",\n    \"boosting_type\": 'gbdt', #\"gbdt\", \"dart\"\n    #\"xgboost_dart_mode\": True,\n    'device_type': 'cpu', #'gpu', 'cuda'\n    #'drop_rate': 0.5, #For dart type only\n    'seed': seed,\n    #'deterministic': True,\n    'tree_learner': 'data',\n    'num_threads': 4,\n    'max_bin': 512,\n    'max_depth': 20,\n    'num_leaves': 15,\n    #'early_stopping_rounds': 1000,\n    'lambda_l1': 1.75,\n    'lambda_l2': 3.25,\n    'cat_l2': 1.25,\n    'learning_rate': 0.00325,\n    'feature_fraction': 0.25,\n    \"feature_fraction_seed\": seed,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 50,\n    'bagging_seed': seed,\n    'force_col_wise': True,\n    #'min_data_in_leaf': 27,\n    'path_smooth': 2.5,\n    'cat_smooth': 2.5,\n    'verbosity':0,\n  }","f0905bf4":"import time\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing  import StandardScaler\n\ndef RMSE(y_true, y_pred):\n    return np.sqrt(MSE(y_true, y_pred))\n\n#Silence LGBM warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntest_preds = np.zeros((test.shape[0]))\n\nlgbm_models = []\nlgbm_history = []\ntarget_cols = 'target'\nn_folds = 5\nprint(\"#\"*72)\n\nX = train[all_cols]\ny = train[target_cols]\n\nkf = KFold(n_splits=n_folds, random_state=seed, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print(\"Fold :\", fold+1)\n    st_time = time.time()\n    evals_result = {} #Record evaluation results to dict\n    # create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    #Normalization\n    scaler = StandardScaler().fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    X_test = scaler.transform(test[all_cols])\n    \n    \n    #LGBM Model Training ###################################################################\n    lgbm_train = lgbm.Dataset(X_train, y_train)\n    lgbm_valid = lgbm.Dataset(X_valid, y_valid, reference=lgbm_train)\n\n    model_lgbm = lgbm.train(\n        params=params,\n        train_set=lgbm_train,\n        valid_sets=[lgbm_train, lgbm_valid],\n        valid_names=['train','valid'],\n        num_boost_round=30000,\n        verbose_eval=2500,\n        #categorical_feature=cat_features,\n        evals_result=evals_result,\n    )\n\n    lgbm_y_pred1 = model_lgbm.predict(X_valid, num_iteration=model_lgbm.best_iteration)\n    score1 = RMSE(y_true=y_valid, y_pred=lgbm_y_pred1)\n    print(f'\\nPerformance of the\u3000LGBM prediction RMSE on validation data: {score1:.5}')\n    \n    #Lower RMSPE yields higher prediction weight during ensemble\n    apparent_weight = 1# max(0, 1 - score1**2)\n    print(f'\\nApparent weight of model is:', apparent_weight)\n    lgbm_models.append((apparent_weight, model_lgbm))\n    lgbm_history.append(evals_result)\n    \n    test_preds += model_lgbm.predict(X_test, num_iteration=model_lgbm.best_iteration)\n    \n    diff_time = time.time() - st_time\n    print(f\"Total time spent on fold: {int(diff_time\/60)} min {round(diff_time%60, 4)} sec\")\n    print(\"#\"*72)","ff59cdac":"fig, ax = plt.subplots(figsize=(10, 10))\nlegends = []\nfor n, model in enumerate(lgbm_history):\n    lgbm.plot_metric(model, metric='rmse', ax=ax, ylabel=f'model RMSE')\n    legends += [f'train_{n}', f'valid_{n}']\nplt.title('Training RMSE History:')\nplt.legend(legends)\nplt.savefig('mse_history_plot.png')","67d7d8eb":"divisor = sum(score for score, _ in lgbm_models)\n\ntry:\n    feature_importance = np.zeros((len(all_cols)))\n    \n    for score, model in tqdm(lgbm_models):\n        feature_importance += model.feature_importance() * score \/ divisor\n        \n    importance_df = pd.DataFrame({'Features':all_cols,\n                                  'Importance': feature_importance}\n                                ).sort_values('Importance')\n    \n    fig, ax = plt.subplots(figsize=(8, 10))\n    importance_df.plot.barh(x='Features', y='Importance', ax=ax)\n    plt.title(\"Importance of Features\")\n    plt.tight_layout()\n    plt.savefig('combined_weighted_importance.png')\n    \nexcept Exception as e:\n    print(e)","7908685b":"#Test ensemble\ntarget = np.zeros(len(train))\ncounter = 1\nfor score, model in tqdm(lgbm_models):\n    X = StandardScaler().fit_transform(train[all_cols])\n    pred = model.predict(X, num_iteration=model.best_iteration)\n    target += pred * (score\/divisor)\n    print(f\"Singular score of model {counter}: {RMSE(train[target_cols], pred):.5}\")\n    counter += 1\n\nscore = RMSE(y_true = train[target_cols], y_pred = target)\nprint(f'Performance of the Ensemble LGBM prediction RMSE: {score:.5}')","63853883":"target = test_preds \/ n_folds\n\ntest['target'] = target\ntest[['id', 'target']].to_csv('submission.csv', index=False)\n\n#Check if submission is saved\nsub = pd.read_csv('submission.csv')\ndisplay(sub)","2807c291":"# Correlation Matrix\n\nLet's see the correlation of each variables, especially towards the `target` column.","07136429":"# Prediction","bd72b01a":"We can see that there are both categorical and numerical data in our dataset. We will separate them to two groups by type:","d98885c4":"# Ensemble","f7e661e8":"We can see that numerical features are not evenly distributed and may need some normalization and scaling before being applied to the models.","a5bc0acc":"Now, let us see the summary of each numerical features:","0dfd39aa":"# Model Creation\n\nSince the categorical features are heavily imbalanced, we will be implementing a tree-based model like LightGBM and XGBoost as our models. For this exercise, we will use LightGBM as our main model.","7b7a0826":"We can see here that there is no column that is directly correlational to the `target` column. Next, we will be trying to create features that have high correlation to the `target`.","80afebcf":"There is an obvious imbalance to the category features.","03c12f3e":"# Feature Importance","12a1b147":"# Train History","9bf4e8cb":"# Begin\n\nRead train and test data:","09684948":"# Feature Engineering"}}