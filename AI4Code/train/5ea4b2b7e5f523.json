{"cell_type":{"bbc9bbb8":"code","d972ac8e":"code","4c31e928":"code","a251018a":"code","1f835c0f":"code","a35115d4":"code","0a32bfb8":"code","c91a21ff":"code","4fc88848":"code","57307e53":"code","36fdace7":"code","c863f307":"code","f3ec9a64":"code","f700575d":"code","021437bb":"code","be2f6032":"code","854277c2":"code","a7d750b1":"code","26d1cbab":"markdown","c6de62a4":"markdown","1c92303a":"markdown","29f58450":"markdown","12d5df60":"markdown","093cc663":"markdown","5042ed50":"markdown","0dda3434":"markdown","df3422ab":"markdown","ef6ca774":"markdown","6d22e206":"markdown","8bcf1f46":"markdown","bc69ad9b":"markdown","eb6c335b":"markdown","275f3559":"markdown","2f8db057":"markdown","10d60ff4":"markdown"},"source":{"bbc9bbb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d972ac8e":"import warnings\nwarnings.filterwarnings(\"ignore\")","4c31e928":"train_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv')\n\nprint(train_data.head(), \"\\n\")\nprint(test_data.head())","a251018a":"print(train_data.shape, \"\\n\")\nprint(test_data.shape)","1f835c0f":"print(train_data.info())","a35115d4":"# import needed modules\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns","0a32bfb8":"print(train_data.columns)","c91a21ff":"cat_features = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nnumerical_features = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5','cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']","4fc88848":"for i in numerical_features:\n    \n    fig, ax = plt.subplots(1,2, figsize=(18,6))\n\n    ax[0].plot(train_data[\"id\"], train_data[i])\n    ax[1].plot(test_data[\"id\"], test_data[i])\n    \n    ax[0].set(xlabel=\"id\", ylabel=i)\n    ax[0].set_title('train_data')\n\n    ax[1].set(xlabel=\"id\", ylabel=i)\n    ax[1].set_title(\"test_data\")\n\n    plt.show()","57307e53":"for i in numerical_features:\n\n    fig, ax = plt.subplots(1,2, figsize=(18,6))\n\n    sns.distplot(a = train_data[i], ax = ax[0])\n    ax[0].set(xlabel='id', ylabel=i)\n    ax[0].set_title('train_data')\n\n    ax[1].set(xlabel='id', ylabel=i)\n    ax[1].set_title(\"test_data\")\n    sns.distplot(a = test_data[i], ax = ax[1])","36fdace7":"for i in numerical_features:\n    \n    fig = plt.figure(figsize=(10,6))\n    plt.plot(train_data[i], train_data[\"target\"], linestyle = '', marker = 'x')\n    plt.title(i)\n    plt.show()","c863f307":"outlier = train_data.loc[train_data.target < 1.0]\nprint(outlier, \"\\n\")\nprint(outlier.index)","f3ec9a64":"# remove the outlier from the train_data set\ntrain_data.drop([99682], inplace = True)","f700575d":"for i in cat_features:\n\n    fig, ax = plt.subplots(1,2, figsize=(18,6))\n    \n    train_data[i].value_counts().plot(kind = 'bar', ax = ax[0])\n    ax[0].set(xlabel='id', ylabel=i)\n    ax[0].set_title('train_data')\n    \n    ax[1].set(xlabel='id', ylabel=i)\n    ax[1].set_title(\"test_data\")\n    test_data[i].value_counts().plot(kind = 'bar', ax = ax[1])\n    \n    plt.show()","021437bb":"for i in cat_features:\n    \n    sns.catplot(x = i, y=\"target\", data=train_data)\n    plt.show()","be2f6032":"from catboost import CatBoostRegressor\n\ncategorical_features = cat_features\n\ny_train = train_data[\"target\"]\n\ntrain_data.drop(columns = ['target'], inplace = True)\n\n\ntest_data_backup = test_data.copy()\n\n# dropping the id column slightly improves the score\ntrain_data.drop(columns = [\"id\"], inplace = True)\ntest_data.drop(columns = [\"id\"], inplace = True)","854277c2":"model_ctb = CatBoostRegressor(iterations = 3000, \n                               learning_rate = 0.02,\n                               od_type = 'Iter',\n                               loss_function = 'RMSE',\n                               #eval_metric='AUC',\n                               grow_policy = 'SymmetricTree',\n                               #auto_class_weights = 'Balanced',\n                               #max_depth = 8,\n                               subsample = 0.8,\n                               #colsample_bylevel = 0.9,\n                               #l2_leaf_reg = 0.80, \n                               #one_hot_max_size = 4,\n                               verbose = 3,\n                               random_seed = 17)\n\nmodel_ctb.fit(train_data, y_train, cat_features=categorical_features)\n\ny_pred = model_ctb.predict(test_data)\n\nprint(y_pred)","a7d750b1":"solution = pd.DataFrame({\"id\":test_data_backup.id, \"target\":y_pred})\n\nsolution.to_csv(\"solution.csv\", index = False)\n\nprint(\"saved successful!\")","26d1cbab":"**In this section we will simply print out some interesting properties and characteristics of our data.**","c6de62a4":"**The columns cat0, cat1 and cat2 have only 2 unique values: A and B.**\n\n**The columns cat3, cat4, cat5 have 3 unique values: A, B and C.**\n\n**The columns cat6 and cat7 have 8 unique values: A,B,C,D,E,F,G,H.**\n\n**The column cat8 and 7 unique values: A,B,C,D,E,F,G.**\n\n**The column cat9 has 15 unique values: A,B,C,D,E,F,G,H,I,J,K,L,M,N,O.**\n\n**The unique values are distributed in an imbalanced way.**\n\n**And train and test data look very similar.**","1c92303a":"# 3.) Plot data\n\n## 3.1) Plot x-axis = id, y-axis = num. feature","29f58450":"## 3.2) distplot of num. features","12d5df60":"# 1.) Load data","093cc663":"**These distributions look good for now, only a few outliers at the bottom.**\n\n**The target range goes from 0 to 10, the range of the numerical feature values goes rougly from 0 to 1.1.**\n\n**Only the feature cont1 shows some interesting properties in form of the thin stripes between x = 0.2 and 0.6.**\n\n**Let's remove that one outlier at the bottom.**","5042ed50":"# Train CatBoost model","0dda3434":"## 3.3) Plot x-axis = num. feature, y-axis = target","df3422ab":"**The data sets of this competition have the same number of rows as the previous January competition.**","ef6ca774":"# 2.) Have a first look at data","6d22e206":"# Beginner-friendly February Tabular Tutorial! (0.84358)\n\n**Hello and welcome to my beginner-friendly tutorial for the Tabular Playground series February 2021 competition!**\n\n**This tutorial is meant for anybody who is new to kaggle competitions. Doenst matter if you have absolutely no experience with kaggle competitions or if you already gained some experience in a few competitions, this tutorial should be helpful for both scenarios.**\n\n**I was very happy about this new kaggle competition series, every first day of each month a competition like this will be hosted :)**\n\n**It is very helpful for beginners, because the datasets are very friendly and nicely structured.**\n\n**link to competition:** https:\/\/www.kaggle.com\/c\/tabular-playground-series-feb-2021\n\n\n# What is going to happen in this tutorial?\n\n**In this tutorial we will first look at the data, we are going to analyze the numerical and categorical features separately.**\n\n**Afterwards we are going to train a CatBoost model and finally make a prediction.**","8bcf1f46":"**We have 24 feature columns and 1 target column.**\n\n**The 24 features columns consist of 10 categorical features and 14 numeric features.**","bc69ad9b":"# Thanks for reading this tutorial!\n\n\n# If you have any ideas or questions, feel free to ask :)","eb6c335b":"**These plots reveal some information about the relationship between the categorical features and the target.**\n\n**The first 3 plots with only 2 unique categorical values show that both unique values cover rougly the same target range.**","275f3559":"**The numerical features look good for now, train and test data look very similar, rougly the same y-range for all 14 features.**\n\n**Now let's plot the numerical features of the train data together with the target.**","2f8db057":"## 3.5) Plot x-axis = cat. feature, y-axis = target","10d60ff4":"**Now let's plot the categorical features.**\n\n**First we will simply plot the unique values of the categorical features.** \n\n**Afterwards we will plot the categorical feature together with the target.**\n\n\n## 3.4) Plot x-axis = id, y-axis = cat. feature"}}