{"cell_type":{"9673f6b8":"code","5116a22e":"code","43b08fb5":"code","06d072aa":"code","fa64cacd":"code","cf3f518d":"code","8175cedf":"code","0b567508":"code","c72146f9":"code","b3266489":"code","3522e8ca":"code","9d633496":"code","ea03870f":"code","7c812e02":"code","23c756a7":"code","f13a0962":"code","e2cade73":"code","c3975b50":"code","2661d69b":"code","94fa3866":"code","7dd9229e":"code","da4fafc9":"code","4666138a":"code","6427119c":"code","94ceb8c1":"code","00359bf0":"code","5c2dbeba":"code","cb55bf42":"code","3b559048":"code","622c3096":"code","ee035297":"code","f55a3ee1":"code","3f29583b":"code","6410bed1":"code","39406e1e":"code","21bda398":"code","33aaa9f5":"code","974b4d93":"code","fead34d5":"code","56ab6431":"code","afb6520c":"markdown","c66ab512":"markdown","068eff55":"markdown","e2a13fa4":"markdown","3c2aedc9":"markdown","ddb1b31e":"markdown","342c1279":"markdown","44d75a88":"markdown","d1b97907":"markdown","58ed4382":"markdown","0c654363":"markdown","a92c3772":"markdown","51a4eebc":"markdown","8005d594":"markdown","72939f06":"markdown","06d6e861":"markdown","a1e378f6":"markdown","a96d43f8":"markdown","6a0f9abb":"markdown","e9e0026e":"markdown"},"source":{"9673f6b8":"# ### 09\/07\/2021\n#    - Saved dataset from tf.data.experimental save method could not be read back correctly.\n#     => Fixed by upgrading tensorflow to version 2.5, can now be loaded with the load method. \n#     - Lowered saved dataset records count keeping in 5-6 GB range size for confort. \n\n\n","5116a22e":"# Upgrade to TensorFlow >= 2.5\n!pip uninstall pytorch-lightning -y\n!pip install tensorflow -U","43b08fb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n# for dirname, _, filenames in os.walk('..\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# import math\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as Layer\nfrom tensorflow.keras.preprocessing import image\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06d072aa":"print(\"Tensorflow version: \", tf.__version__)","fa64cacd":"# Base repository\nBASE_PATH = '..\/input\/g2net-gravitational-wave-detection\/'\nSPLIT_RATIO = .9\nRANDOM_STATE = 42","cf3f518d":"# \ndf = pd.read_csv(os.path.join(BASE_PATH, 'training_labels.csv'))\n#sample_submission = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\ndf.head()","8175cedf":"def apply_raw_path(row, is_train=True): \n    file_name = row[0]\n    if is_train:\n        return os.path.join(\n            BASE_PATH, 'train',\n            file_name[0],\n            file_name[1],\n            file_name[2],\n            file_name + \".npy\")\n    else:\n        return os.path.join(\n            BASE_PATH, 'test',\n            file_name[0],\n            file_name[1],\n            file_name[2],\n            file_name + \".npy\")\n\ndf['file_path'] = df.apply(apply_raw_path, args=(True,), axis=1)\ndf['target'] = df['target'].astype('int8')","0b567508":"df.head()","c72146f9":"sample_num = 2579\n\ndef plot_samples(sample_list):\n    fig,a =  plt.subplots(len(sample_list), 3, figsize=(15,2 * len(sample_list)))\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=.5, wspace=0.2)\n    cpt = 0\n    for sample_num in sample_list:\n        sample = np.load(df.loc[sample_num]['file_path'])\n        a[cpt, 0].plot(sample[0],color='red')\n        a[cpt, 1].plot(sample[1],color='green')\n        a[cpt, 2].plot(sample[2],color='cyan')\n        a[cpt, 1].set_title(\n            'sample ' + str(sample_num) +\n            ', label ' + str(df.loc[sample_num]['target']),\n            fontsize=16\n        )\n        cpt = cpt + 1\n    plt.show()\n    return sample.shape\n\nsamples = [0, 1, 2, 2547]\nplot_samples(samples)\n","b3266489":"print(\"Sample shape: \",str(np.load(df.loc[2547]['file_path']).shape))\nprint(\"Number of records:\", str(len(df)))","3522e8ca":"df.head()","9d633496":"df_train, df_valid = train_test_split(\n    df,\n    test_size=1-SPLIT_RATIO,\n    train_size=SPLIT_RATIO,\n    random_state=RANDOM_STATE)\nprint(len(df_train))\nprint(len(df_valid))\ndisplay(df_train.head())\nprint(df_train.loc[0])","ea03870f":"def npy_header_offset(npy_path):\n    with open(str(npy_path), 'rb') as f:\n        if f.read(6) != b'\\x93NUMPY':\n            raise ValueError('Invalid NPY file.')\n        version_major, version_minor = f.read(2)\n        if version_major == 1:\n            header_len_size = 2\n        elif version_major == 2:\n            header_len_size = 4\n        else:\n            raise ValueError('Unknown NPY file version {}.{}.'.format(version_major, version_minor))\n        header_len = sum(b << (8 * i) for i, b in enumerate(f.read(header_len_size)))\n        header = f.read(header_len)\n        if not header.endswith(b'\\n'):\n            raise ValueError('Invalid NPY file.')\n        return f.tell()","7c812e02":"# records chosen in train df indexes \/ raise error if was allocated to test df\nfor record_num in [0, 1, 3, 25, 2586, 54863, 258412, 559998]:\n    file_length = os.path.getsize(df_train.loc[record_num]['file_path'])\n    header_size = npy_header_offset(df_train.loc[record_num]['file_path'])\n    print(\n        \"File length:\", file_length,\n        \", Header size: \",  header_size,\n        \", Data length\", file_length - header_size\n    )","23c756a7":"ds_train_data = tf.data.FixedLengthRecordDataset(\n    df_train['file_path'],\n    98304,\n    header_bytes=128,\n    num_parallel_reads=4)\nds_train_data = ds_train_data.map(lambda s: tf.reshape(tf.io.decode_raw(s, tf.float64), (3,4096)))\nds_train_data = ds_train_data.map(lambda s: tf.cast(s, tf.float32))","f13a0962":"ds_train_label = tf.data.Dataset.from_tensor_slices(df_train['target'])\nds_train = tf.data.Dataset.zip((ds_train_data, ds_train_label)) ","e2cade73":"def plot_from_dataset(data, label):\n    fig,a =  plt.subplots(1, 3, figsize=(15,2))\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=.5, wspace=0.2)\n    a[0].plot(data[0],color='red')\n    a[1].plot(data[1],color='green')\n    a[2].plot(data[2],color='cyan')\n    a[1].set_title(', label ' + str(label), fontsize=16)\n    plt.show()\n    return data.shape","c3975b50":"for data, label in ds_train.take(3):\n    plot_from_dataset(data.numpy(), label.numpy())","2661d69b":"df_train.index[100]","94fa3866":"check_game_num = 15\nfor data, label in ds_train.skip(check_game_num).take(1):\n    data_from_ds = data.numpy()\n    print(data_from_ds)\n    print(\"label:\", label.numpy() )\n\ndata_from_file = np.load(\n    df_train.loc[df_train.index[check_game_num]]['file_path']).astype(np.float32)\nprint(data_from_file)\nprint(\"label:\", df_train.loc[df_train.index[check_game_num]]['target'])\n\nprint(\"\")\nif (data_from_ds==data_from_file).all():\n    print(\"Hello World!\")\nelse:\n    print(\"Something went wrong.\")","7dd9229e":"# We start from: \nprint(ds_train.cardinality)\nprint(ds_train_data.cardinality)","da4fafc9":"# lets leverage the tfio.audio TensorFlow tfio library\n!pip install  tensorflow_io\nimport tensorflow_io as tfio\n","4666138a":"ds_train_data = tf.data.FixedLengthRecordDataset(\n    df_train['file_path'],\n    98304,\n    header_bytes=128,\n    num_parallel_reads=4)\nds_train_data = ds_train_data.map(\n    lambda s: tf.reshape(\n        tf.io.decode_raw(\n            s, tf.float64), (3,4096)))\nds_train_data = ds_train_data.map(\n    lambda s: tf.cast(s, tf.float32))\nds_train_data = ds_train_data.map(\n    lambda s: tfio.audio.spectrogram(\n        s,\n        nfft=128,\n        window=256,\n        stride=64))\nprint(ds_train_data.cardinality)\n\n# The nfft and stride selected here allow for output size from (4096) to (256,129)\n# My guess is the transformation should consider both expected relevant signal transformation and sizing complying with models ","6427119c":"for data in ds_train_data.take(1):\n    print(data)","94ceb8c1":"# tfio.audio also provides with a melscale method might be leveraged","00359bf0":"# The below applies a log transformation\n# the main goal is transforming inputs to a [0,1] compliant with expected inputs hence the shift and scale\n# The clip_by_value prevents from log(0) nan.\n# Obviously the clip values will have to be set from consistant data analysis # TODO #  ","5c2dbeba":"ds_train_data = ds_train_data.map(\n    lambda s: (\n        tf.math.log(\n            tf.clip_by_value(s,1e-30, 1e-17)) + 60)\/25)\nprint(ds_train_data.cardinality)","cb55bf42":"# this has yet to be transposed for each wave becoming one of the RGB channels expected last by TensorFlow\nds_train_rgb = ds_train_data.map(lambda s: tf.transpose(s))\nprint(ds_train_rgb.cardinality)\n\n# include labels in dataset the zip way\nds_train = tf.data.Dataset.zip((ds_train_rgb, ds_train_label))","3b559048":"fig,a =  plt.subplots(2,5, figsize=(18,8))\ncpt = 0\nmin_value = 1\nmax_value = 0\nfor data, label in ds_train.skip(15).take(10):\n#     data = tf.math.log(data)\n    local_max = np.amax(data.numpy())\n    if  local_max > max_value:\n        max_value = local_max \n    local_min = np.amin(data.numpy())\n    if  local_min < min_value:\n        min_value = local_min \n#     print(data)\n#     a.imshow(tf.math.log(data[0]))\n    line = cpt % 5\n    col = cpt \/\/ 5\n    a[col, line].imshow(data)\n    a[col, line].set_title(\n            'Label: ' + str(label.numpy()),\n            fontsize=16)\n    cpt = cpt + 1\n\nplt.show()\nprint(\"Min value: \", min_value)\nprint(\"Max value: \", max_value)\n","622c3096":"# Build validation\nds_valid_label = tf.data.Dataset.from_tensor_slices(df_valid['target'])\nds_valid_data = tf.data.FixedLengthRecordDataset(\n    df_valid['file_path'],\n    98304,\n    header_bytes=128,\n    num_parallel_reads=4)\nds_valid_data = ds_valid_data.map(\n    lambda s: tf.reshape(tf.io.decode_raw(s, tf.float64), (3,4096)))\nds_valid_data = ds_valid_data.map(\n    lambda s: tf.cast(s, tf.float32))\nds_valid_data = ds_valid_data.map(\n    lambda s: tfio.audio.spectrogram(\n        s,\n        nfft=128,\n        window=256,\n        stride=64))\nds_valid_data = ds_valid_data.map(\n    lambda s: (tf.math.log(tf.clip_by_value(s,1e-30, 1e-17)) + 60)\/25)\nds_valid_rgb = ds_valid_data.map(\n    lambda s: tf.transpose(s))\nds_valid = tf.data.Dataset.zip((ds_valid_rgb, ds_valid_label))","ee035297":"print(ds_train.cardinality)\nprint(ds_valid.cardinality)","f55a3ee1":"# Save  \ntf.data.experimental.save(ds_train.take(120000), '.\/ds_train', compression=None) \ntf.data.experimental.save(ds_valid.take(12000), '.\/ds_valid', compression=None) \n","3f29583b":"# Load \ntrain = tf.data.experimental.load('.\/ds_train')\nvalid = tf.data.experimental.load('.\/ds_valid')","6410bed1":"from tensorflow.keras.applications.resnet_v2 import ResNet50V2\nfrom tensorflow.keras.applications.resnet_v2 import preprocess_input\n\ndef prep(img, label):\n    ret = preprocess_input(img)\n    return ret,label","39406e1e":"# Limiting to 2000 record for fast run without CPU\nBATCH_SIZE = 16\n\ntrain_data = train.take(1000).map(prep).batch(BATCH_SIZE).prefetch(buffer_size=64)\nvalid_data = valid.take(100).map(prep).cache().batch(BATCH_SIZE).prefetch(buffer_size=64)","21bda398":"base_model = ResNet50V2(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(65,64,3),\n    pooling='avg')\nbase_model.trainable=True\n# base_model.trainable=False\n\n\nfine_tune_at = 171\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","33aaa9f5":"metrics = [tf.keras.metrics.AUC(name='AUC'),\n           tf.keras.metrics.SparseCategoricalAccuracy(name='Accuracy'),\n          ]\n\ntb_callback = tf.keras.callbacks.TensorBoard(\"logs\")\n\ncallbacks= [\n    tb_callback,\n]\n\ndef create_model(alpha):\n    model = tf.keras.Sequential([\n        Layer.InputLayer(input_shape=(65,64,3)),\n        base_model,\n        Layer.Dropout(.3),\n        Layer.LeakyReLU(alpha=alpha),\n        Layer.Dense(64),\n        Layer.Dropout(.3),\n        Layer.LeakyReLU(alpha=alpha),\n        Layer.Dense(8),\n        Layer.Dropout(.3),\n        Layer.LeakyReLU(alpha=alpha),\n        Layer.Dense(1, activation='sigmoid'),\n    ])\n    return model\n","974b4d93":"model = create_model(0)\nmodel.summary()","fead34d5":"model.compile(optimizer=tf.keras.optimizers.Adam(\n    learning_rate=1e-5,\n    ),\n    loss='binary_crossentropy', metrics=metrics)","56ab6431":"model.fit(\n    train_data,\n    epochs=2,\n    validation_data=valid_data,\n    callbacks = callbacks,\n    )","afb6520c":"# Dataset transformation (Spectrogram)","c66ab512":"### First shuffle and split","068eff55":"# Fit a RESNET 50 ","e2a13fa4":"# Build Dataset \n\n## Data presentation","3c2aedc9":"To sum up we now have at disposal a shuffled train TensorFlow Dataset able to read data from provided files.\n- Data have shape (3,4096) float 32 type\n- Labels have shape (1) type int8\n\nNow imagine we'd like to leverage a convolutional neural network could be a ResNet, EfficientNet, or other famous ones expecting 2 dimensional image like inputs in RGB mode.\nThis because I saw in other notebooks here some spectrograms transformation sounds quite relevant for the classification task we target.\n\nI will below propose an implementation idea I have no clue whether it's relevant or not.<BR>\nWorth mentioning here I'm new to Machine Learning and worse in regards to signal analysis and tranformation, so please forgive the chosen values if aberrant.\n    \nSo my basic idea is why not transforming each wave into a spectrogram and leveraging one of the 3 RGB channel per wave. \n3 RGB channel xpected, 3 waves at disposal... KIS\n    \nThe data shape tranformation mechanic is the focus here.    \n","ddb1b31e":"### Let's try plotting this","342c1279":"Good looks pretty solid,\nLet's check number wise:\n\nNote: relying on df_train.index matching the right element from dataset hence the:<BR>\n    df_train.loc[df_train.index[100]]","44d75a88":"Let's add the labels to our data the zip way","d1b97907":"# Introduction\n\nThis Notebook runs for the G2Net Gravitational Wave Detection Kaggle competition started early July 2021.\n\nThere are already several interesting public EDAs for the data. Thanks these help me greatly.<BR>\nI will be kinda quick on this part here. \n\nThe main focus here is building a TensorFlow pipeline mostly relying on tf.data.Dataset and other build in mechanism preparing provided data in order to feed a Machine Learning model.\n ","58ed4382":"### Build validation dataset ","0c654363":"First thing, dataset are only working sequentially and cannot skip (there is a skip method yet it only discards, not really skip).\n\nConsidering the whole data volumetry:\n- If any full shuffling required it has to be performed on the ds_train dataframe as prior,\n- If any validation data to be reserved, rather prepare a spedcific dataset from a dedicated dataframe.\n\nTherefore we will first shuffle the df dataframe then split in train and validation.  ","a92c3772":"Let's have a look at the data","51a4eebc":"# TODO\n### TO FIX\n- Tensorflow io installation facing issues.\n- Model  training seems to heavily bottleneck on CPU and would last forever. \nLinked to tfio issue or Dataset handling here? <BR>\nSame code @home 4790K\/RTX2060 runs 5-15% SSD, 20-30%CPU and 90-100%GPU? To be investigated.\n\n### Spectrogram\n- Learning and tuning\n- In depth data analysis\n\n### ML Model architecture and tuning \n- First have to fix GPU handling\n    \n## Latest changes\n    - Few typos and code presentation\n    - Spectrogram output size lowered to (64, 65)\n    - Train dataset spectrogram plotting layout\n    - Model fit from loaded dataset\n    - Model example simplified and minimum records for fast execution w\/o GPU \n    \n### Changes history","8005d594":"So now we have acces to the data, that is\n- 560 000 records \n- Of shape (3, 4096)\nEach record includes 3 channels each containing a wave signal sampled at 2048Hz over 2 seconds.\nEach channel comes from either one of the two LIGOs either the VIRGO\n\nNow let's build a TensorFlow DataSet preparing data for futur use.","72939f06":"Does it work?<BR>\nLet's adapt previous plot function to work with dataset data:","06d6e861":"# Dataset save and load\nThis is informative and should not be necessary yet might help identifying model fitting performances issues ","a1e378f6":"## Consolidate the data in a pandas DataFrame ","a96d43f8":"### Second step the numpy format\n\nAs per my understanding TF does not provide yet an easy way feeding from numpy file type.\nStill we can work our way out handling known data structure from files leveraging:\ntf.data.FixedLengthRecordDataset\n\nThis will require providing with the specs of the data.\n\nThe below function found somewhere on the Internet will parse the numpy header returning its length.<BR>\nBtw can be found at https:\/\/stackoverflow.com\/questions\/48889482\/feeding-npy-numpy-files-into-tensorflow-data-pipeline","6a0f9abb":"Good let's rock\nwe need: \n- FixedLengthRecordDataset will read the file (can leverage threading btw preventing from cpu\/io bottleneck)\n- Next tf.io.decode_raw will unflat the data to our expected format (3, 4096)\nStill 3 x 4096 != 98304, in fact 98304 \/ ( 3x4096) = 8 ;<BR> Guess we are facing float 64 data type assuming file type is byte.\n","e9e0026e":"# TensorFlow Dataset preparation"}}