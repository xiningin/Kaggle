{"cell_type":{"35bf7c86":"code","87a3fd7d":"code","ea75b2ce":"code","6a8e525b":"code","e2921c32":"code","a41e4314":"code","12ba4997":"code","99234ca4":"code","5ee086f0":"code","0088c17d":"code","76d0ec0a":"code","0d12bd50":"code","e120eec6":"code","67267eb8":"code","b8ffcad1":"code","5ba05c94":"code","859040c7":"code","dcb414d7":"code","d0c1705a":"code","867de983":"code","4015fa79":"code","bee6057a":"code","b6e0a376":"code","9c1b3817":"code","3604bedd":"markdown","dd551a14":"markdown"},"source":{"35bf7c86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","87a3fd7d":"comments= pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')","ea75b2ce":"comments.head()","6a8e525b":"comments.isnull().sum()","e2921c32":"import seaborn as sb\n\nfrom matplotlib import pyplot as plt\n\nsb.heatmap(comments.isnull(),cbar=True)\nplt.show()","a41e4314":"# Number of classes present in each target variable\nfor i,j in comments[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].items():\n    print(i,j.value_counts(),'',sep='\\n')","12ba4997":"val = comments[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]","99234ca4":"plt.rcParams['figure.figsize']=[20,8]\nfig =plt.figure()\n\nplt.subplot(2,2,1)\nplot = plt.hist(val.toxic)\nplt.title('toxic')\nplt.subplot(2,2,2)\nplt.hist(val.severe_toxic,color='red')\nplt.title('severe_toxic')\nplt.subplot(2,2,3)\nplt.hist(val.obscene,color='orange')\nplt.title('obscene')\nplt.subplot(2,2,4)\nplt.hist(val.threat,color='green')\nplt.title('threat')\n\nplt.show()","5ee086f0":"plt.rcParams['figure.figsize']=[20,8]\nfig =plt.figure()\n\nplt.subplot(2,2,1)\nplt.hist(val.insult,color='purple')\nplt.title('insult')\nplt.subplot(2,2,2)\nplt.hist(val.identity_hate,color='black')\nplt.title('identity_hate')\nplt.show()","0088c17d":"from nltk.tokenize import word_tokenize\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","76d0ec0a":"stop_nltk = stopwords.words(\"english\")\n\nstop_updated = stop_nltk + [\"...\",\"..\",\"\\n\",\"\\t\",\"==\",\"=\",\"\/\/\",\"'\",'D',',','en wikipedia org','https en wikipedia','wikipedia org wiki'] ","0d12bd50":"contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}","e120eec6":"import re\n\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))","67267eb8":"# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n def replace(match):\n    return contractions_dict[match.group(0)]\n return contractions_re.sub(replace, text)\n","b8ffcad1":"# Function for removing the stopwords  and punctuations\nlemm = WordNetLemmatizer()\ndef clean_text(sent):\n    tokens = word_tokenize(sent.lower())\n    lemmed = [lemm.lemmatize(term) for term in tokens \\\n               if term not in stop_updated and \\\n                term not in list(punctuation) and\\\n               len(term) > 2] \n    res = \" \".join(lemmed)\n    return res","5ba05c94":"#Expanding Contractions in the comments\n\ncomments['clean_text']=comments['comment_text'].apply(lambda x:expand_contractions(x))\n\ncomments['clean_text']=comments['clean_text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n","859040c7":"comments['clean_text'] = comments.clean_text.apply(clean_text)","dcb414d7":"comments.head()","d0c1705a":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","867de983":"X = comments.clean_text.values\ntarget = comments.toxic.values","4015fa79":"X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.75,random_state=42)","bee6057a":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=3500)\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","b6e0a376":"classifier = MultinomialNB()\nclassifier.fit(X_train_tfidf, y_train)\ntraget_prediction = classifier.predict(X_test_tfidf)\nprint('Accuaracy score for toxic comment classification:',accuracy_score(y_test,traget_prediction)*100)","9c1b3817":"traget_prediction","3604bedd":"Heat map to check if any missing values are present in the dataset","dd551a14":"Missing value treatment "}}