{"cell_type":{"3e7325ac":"code","a7532e6d":"code","b79d3ab0":"code","8f4a3d8c":"code","f2dd3f96":"code","188af0c2":"code","875c9e1c":"code","2119e14b":"code","1ee898d0":"code","aa41118e":"code","efae1e2b":"code","88d2e48d":"markdown","6a43a97a":"markdown","966fff80":"markdown","0cbabc59":"markdown","69099138":"markdown","8840073a":"markdown","4b6b2125":"markdown","f773dadc":"markdown","f6f0fc3b":"markdown","d9aaed19":"markdown","e257ba1d":"markdown"},"source":{"3e7325ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom tqdm import tqdm\n\n# Any results you write to the current directory are saved as output.","a7532e6d":"# Fix seeds\nfrom numpy.random import seed\nseed(639)\nfrom tensorflow import set_random_seed\nset_random_seed(5944)\n\nfloat_data = pd.read_csv(\"..\/input\/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values","b79d3ab0":"def extract_features(z):\n     return np.c_[z.mean(axis=1), \n                  np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=1)),\n                  z.std(axis=1)]","8f4a3d8c":"def create_X(x, last_index=None, n_steps=150, step_length=1000):\n    if last_index == None:\n        last_index=len(x)\n       \n    assert last_index - n_steps * step_length >= 0\n\n    # Reshaping and approximate standardization with mean 5 and std 3.\n    # ORIGINAL: I changed this becuase I got an No OpKernel was registered to support Op 'CudnnRNN' error\n    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) \/ 3\n    # MY CHANGE: This doesn't fix things, I get the same errors\n    temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1).astype(np.float32) - 5 ) \/ 3\n    \n    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n    # of the last 10 observations. \n    return np.c_[extract_features(temp),\n                 extract_features(temp[:, -step_length \/\/ 10:]),\n                 extract_features(temp[:, -step_length \/\/ 100:])]","f2dd3f96":"# Query \"create_X\" to figure out the number of features\nn_features = create_X(float_data[0:150000]).shape[1]\nprint(\"Our RNN is based on %i features\"% n_features)\n    \n# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\ndef generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n    if max_index is None:\n        max_index = len(data) - 1\n     \n    while True:\n        # Pick indices of ending positions\n        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n         \n        # Initialize feature matrices and targets\n        samples = np.zeros((batch_size, n_steps, n_features))\n        targets = np.zeros(batch_size, )\n        \n        for j, row in enumerate(rows):\n            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n            targets[j] = data[row - 1, 1]\n        yield samples, targets\n        \nbatch_size = 32\n\n# Position of second (of 16) earthquake. Used to have a clean split\n# between train and validation\nsecond_earthquake = 50085877\nfloat_data[second_earthquake, 1]\n\n# Initialize generators\n# train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\ntrain_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\nvalid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)\n","188af0c2":"from keras.models import Sequential\nfrom keras.layers import Dense, CuDNNGRU\nfrom keras.optimizers import adam\nfrom keras.callbacks import ModelCheckpoint\n\ncb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n\nmodel = Sequential()\nmodel.add(CuDNNGRU(48, input_shape=(None, n_features)))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.summary()","875c9e1c":"model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=1000,\n                              epochs=30,\n                              verbose=0,\n                              callbacks=cb,\n                              validation_data=valid_gen,\n                              validation_steps=200)","2119e14b":"import matplotlib.pyplot as plt\n\ndef perf_plot(history, what = 'loss'):\n    x = history.history[what]\n    val_x = history.history['val_' + what]\n    epochs = np.asarray(history.epoch) + 1\n    \n    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n    plt.title(\"Training and validation \" + what)\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    plt.show()\n    return None\n\nperf_plot(history)","1ee898d0":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})","aa41118e":"for i, seg_id in enumerate(tqdm(submission.index)):\n  #  print(i)\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    x = seg['acoustic_data'].values\n    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n\nsubmission.head()","efae1e2b":"submission.to_csv('submission.csv')","88d2e48d":"# Helper functions\n## Helper function for data generator\nExtracts mean, standard deviation, and quantiles per time step.\nCan easily be extended. Expects a two dimensional array.","6a43a97a":"# Define the model","966fff80":"# Load submission file","0cbabc59":"# Introduction\nI constructed this Kernel from [MichaelMayer](https:\/\/www.kaggle.com\/mayer79)'s great [RNN Starter Kernel](https:\/\/www.kaggle.com\/mayer79\/rnn-starter\/code) code. Thanks for that! I've been using a similar Kernel with a Notebook but it's got plenty of other stuff in there which makes it more than just bare bones. Since this is not really my work, I figured I would share this Kernel with the community. It's meant to be MichaelMayer's starter code converted to a Notebook and nothing more. I had to make some changes, which I've indicated in the code.\n\n# Basic idea of the Kernel\nAt test time, we will see a time series x of length 150 000 to predict the next earthquake. The idea of this kernel is to split x into contiguous chunks of size 1000 and feed summary statistics calculated from these chunks into a current neural net with 150 000 \/ 1000 = 150 time steps. \n\n# Validation\nValidation is a very complex and crucial element of this competition. Here, the validation generator is fed with data until the second earthquake, while the training generator solely picks chunks after the second earthquake. In order to reach a better LB score, you will need to change this so that the training generator has access to the full data.\n\n# Imports","69099138":"# Visualize accuracies","8840073a":"# Setup","4b6b2125":"## Create features\nFor a given ending position \"last_index\", we split the last 150 000 values of *x* into 150 pieces of length 1000 each. So *n_steps * step_length* should equal 150 000.\nFrom each piece, a set features are extracted. This results in a feature matrix of dimension *(150 time steps x features)*.  ","f773dadc":"## Save submission file","f6f0fc3b":"# Compile and fit model","d9aaed19":"## Prepare submission data\nLoad each test data, create the feature matrix, get numeric prediction\n","e257ba1d":"## Generate features"}}