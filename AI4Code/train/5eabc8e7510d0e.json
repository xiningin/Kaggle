{"cell_type":{"1ad08cd5":"code","a637b80d":"code","84faf262":"code","ed916d65":"code","b40ce754":"code","30cef3f9":"code","a211a768":"code","8d973640":"code","50a8b6f1":"code","83f4a78b":"code","ddebf913":"code","9560ca9c":"code","8c472a0a":"code","e927c286":"markdown","e0817312":"markdown","9359bb6d":"markdown","29f3c353":"markdown","0a2fe73d":"markdown","19591959":"markdown","d2490c08":"markdown","1ff33977":"markdown","def1a5e6":"markdown","f3685c4f":"markdown","641ff434":"markdown","890d6be5":"markdown","8f369008":"markdown","f199b351":"markdown","1f636082":"markdown"},"source":{"1ad08cd5":"import tensorflow as tf\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","a637b80d":"df = pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json', lines=True)\ndf.head()","84faf262":"df.info()","ed916d65":"sentences = df['headline']\nlabels = df['is_sarcastic']","b40ce754":"plt.figure(figsize=(10,6))\nsns.set(style=\"darkgrid\")\nsns.countplot(labels)\n","30cef3f9":"# Check sentence lengths\nsentences_lengths = sentences.apply(lambda x: len(x))\n\nplt.figure(figsize=(15,6))\nplt.xlim(0, 150)\n\nax = sns.distplot(sentences_lengths, hist=False, color=\"r\")\nax.set(xlabel='Sentence Lengths')","a211a768":"from sklearn.model_selection import train_test_split\n\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2, random_state=0)\n\nprint(train_sentences.shape)\nprint(val_sentences.shape)\nprint(train_labels.shape)\nprint(val_labels.shape)","8d973640":"# Tokenize and pad\nvocab_size = 10000\noov_token = '<00V>'\nmax_length = 120\npadding_type = 'post'\ntrunc_type = 'post'\nembedding_dim = 16\nnum_epochs = 10","50a8b6f1":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nval_sequences = tokenizer.texts_to_sequences(val_sentences)\nval_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","83f4a78b":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nhistory = model.fit(train_padded, \n                    train_labels, \n                    validation_data=(val_padded, val_labels), \n                    epochs=num_epochs, \n                    verbose=2)","ddebf913":"model2 = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel2.summary()\nhistory_flatten = model2.fit(train_padded, \n                    train_labels, \n                    validation_data=(val_padded, val_labels), \n                    epochs=num_epochs, \n                    verbose=2)","9560ca9c":"model_lstm = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel_lstm.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel_lstm.summary()\nhistory_lstm = model_lstm.fit(train_padded, \n                    train_labels, \n                    validation_data=(val_padded, val_labels), \n                    epochs=num_epochs, \n                    verbose=2)","8c472a0a":"model_mul_lstm = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel_mul_lstm.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel_mul_lstm.summary()\nhistory_mul_lstm = model_mul_lstm.fit(train_padded, \n                    train_labels, \n                    validation_data=(val_padded, val_labels), \n                    epochs=num_epochs, \n                    verbose=2)","e927c286":"## GlobalAveragePooling1D layer vs Flatten layer\n\n### Flattening simply converts a multi-dimensional object to one-dimensional by re-arranging the elements.\n\n### GlobalAveragePooling is a methodology used for better representation of your vector. It can be 1D\/2D\/3D. It uses a parser window which moves across the object and pools the data by averaging it (GlobalAveragePooling) or picking max value (GlobalMaxPooling).\n\n## 2.1.1 GlobalAveragePooling1D","e0817312":"## Tokenize and Pad","9359bb6d":"<center>\n<img src=\"https:\/\/camo.githubusercontent.com\/200d24b84fb905e680fa1ebaa71af582e3d6e24e\/68747470733a2f2f64327776666f7163396779717a662e636c6f756466726f6e742e6e65742f636f6e74656e742f75706c6f6164732f323031392f30362f576562736974652d5446534465736b746f7042616e6e65722e706e67\" width=800><br><\/center>\n\n\n## I decided to create this notebook while working on [Tensorflow in Practice Specialization](https:\/\/www.coursera.org\/specializations\/tensorflow-in-practice) on Coursera. I highly recommend this course, especially for beginners. Most of the ideas here belong to this course.","29f3c353":"# 2.3 Multiple Layer LSTM","0a2fe73d":"# 2.2 LSTM","19591959":"## Import necessary libraries","d2490c08":"## 1.2 Sentence lengths distribution\n\n### This'll be useful for deciding the maxlen parameter in pad_sequence()","1ff33977":"## 2.1.2 Flatten","def1a5e6":"## 1.3 Train-Validation Split","f3685c4f":"# 1. Explore the dataset","641ff434":"# 2. Build Models ","890d6be5":"## Hyperparameters","8f369008":"## 1.1 Target value counts","f199b351":"# 2.1 Only Embedding ","1f636082":"## The second model with flatten layer slightly slower than the first model but has done a better job."}}