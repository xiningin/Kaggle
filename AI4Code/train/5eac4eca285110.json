{"cell_type":{"5ad5bb1a":"code","d8459cd7":"code","f8a5c06a":"code","7a69d058":"code","6f36908c":"code","2c2699b3":"code","584958f1":"code","23385ba8":"code","bc816e02":"code","5918705e":"code","8e90cf42":"code","2ce446d4":"code","ec910e31":"code","db4eb58d":"markdown","f09f7cb3":"markdown","b1e4f840":"markdown","d6d0e372":"markdown","cbd11c74":"markdown","2c64a895":"markdown","29f6607f":"markdown","16720992":"markdown","93585cf0":"markdown","4d17eb57":"markdown","b9b70d9e":"markdown"},"source":{"5ad5bb1a":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# LIME\nimport lime.lime_tabular\nfrom lime.explanation import Explanation\n\nimport warnings\nwarnings.filterwarnings('ignore')","d8459cd7":"# Load Titanic DataSet\ndf = pd.read_csv('..\/input\/train.csv')\ndf.isnull().sum()  # check missing data.","f8a5c06a":"# Remove Useless Attributes\ndf.drop(['PassengerId', 'Name', 'Pclass', 'SibSp',\n         'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n\n# Handle Missing Data\nage_train_mean = df.groupby('Sex').Age.mean()\ndf.loc[df['Age'].isnull() & (df['Sex'] == 'male'),\n       'Age'] = age_train_mean['male']\ndf.loc[df['Age'].isnull() & (df['Sex'] == 'female'),\n       'Age'] = age_train_mean['female']\n\ndf.dropna(subset=['Embarked'], axis=0, inplace=True)\n\nprint(df.isnull().sum())","7a69d058":"df.head()","6f36908c":"sns.catplot(data=df, kind='violin', hue='Survived',\n            x='Embarked', y='Age', col='Sex')","2c2699b3":"X_train = df.drop(['Survived'],  axis=1, inplace=False)\ny_train = df.Survived","584958f1":"X_train_lbenc = X_train.copy()\ncats = ['Sex','Embarked'] # not yet specified label encoded attributes.\n\ncat_dic = {}  # also be used at LimeTabularExplainer's parameter.\ncat_list = [] # also be used at OneHotEncoder, LimeTabularExplainer's parameter.\n\nle = LabelEncoder()\nfor s in cats:\n    i = X_train_lbenc.columns.get_loc(s)\n    X_train_lbenc.loc[:,s] = le.fit_transform(X_train_lbenc[s])\n    cat_dic[i] = le.classes_\n    cat_list.append(i)\n\nX_train_lbenc.head()","23385ba8":"print(cat_list, '\\n',  cat_dic) # check","bc816e02":"# Non-categorical features are always stacked to the right of the matrix.\noe = OneHotEncoder(sparse=False, categorical_features=cat_list)\noe_fit = oe.fit(X_train_lbenc)\nX_train_ohenc = oe_fit.transform(X_train_lbenc)\nX_train_ohenc[:5, :]  # show 5 samples.","5918705e":"parameters = {\n    'C': np.logspace(-5, 5, 10),\n    'random_state': [0]\n}\n\ngs = GridSearchCV(\n    LogisticRegression(),\n    parameters,\n    cv=5\n)\ngs.fit(X_train_ohenc, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)\n\nmodel = LogisticRegression(**gs.best_params_)\nmodel.fit(X_train_ohenc, y_train)","8e90cf42":"explainer = lime.lime_tabular.LimeTabularExplainer(X_train_lbenc.values,  # Label Encoded Numpy Format\n                                                   feature_names = X_train_lbenc.columns,\n                                                   class_names = [\n                                                       'dead', 'survive' ], # 0,1,...\n                                                   categorical_features = cat_list,\n                                                   categorical_names = cat_dic,\n                                                   mode = 'classification'\n                                                   )","2ce446d4":"def pred_fn(x):\n    return model.predict_proba(oe_fit.transform(x)).astype(float)","ec910e31":"exp = explainer.explain_instance(X_train_lbenc.values[2, :],\n                                 pred_fn,\n                                 num_features=len(X_train_lbenc.columns)\n                                 )\nexp.show_in_notebook(show_all=False)","db4eb58d":"### Create Model","f09f7cb3":"Japanese Translation with jargon style. (Please use at your own risk when making reference. :))\n- \u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3064\u3044\u3066\n - \u30bf\u30a4\u30bf\u30cb\u30c3\u30af\u306e\u751f\u5b58\u78ba\u7387\u306b\u95a2\u4fc2\u3059\u308b\u5404\u8aac\u660e\u5909\u6570\u306e\u52b9\u304d\u5177\u5408\u3092\u3001LIME\u3092\u4f7f\u3063\u3066\u884c\u5358\u4f4d\u3067\u30d3\u30b8\u30e5\u30a2\u30eb\u5316\u3002\n - LIME\u306e\u52d5\u304d\u3092\u7406\u5c48\u3067\u306a\u304f\u4f53\u611f\u7684\u306b\u611f\u3058\u3068\u308b\u3053\u3068\u3092\u76ee\u7684\u306b\u4f5c\u6210\u3002\n - \u30e2\u30c7\u30eb\u306f\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u4e8c\u5024\u5206\u985e\u554f\u984c\u3002\n - \u7279\u5fb4\u91cf\u306e\u9078\u629e\u3068\u304b\u3001\u6b20\u640d\u5024\u306e\u6271\u3044\u306f\u9069\u5f53\u3002\n - Train\u30c7\u30fc\u30bf\u306e\u307f\u4f7f\u7528\u3002\n- LIME\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\n - pip install lime\n- \u4eca\u5f8c\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u4e88\u5b9a\n - \u591a\u91cd\u5171\u7dda\uff08\u30de\u30eb\u30c1\u30b3\uff09\u3092\u8003\u616e\u3057\u305f\u5b9f\u88c5\u306b\u3059\u308b\u3002\n    - \u4f46\u3057\u3001\u3054\u308a\u62bc\u3057\u3067\u306f\u306a\u304f\u300c\u7dba\u9e97\u306b\u300d\u5b9f\u88c5\u3059\u308b\u3002\n    - \u4e00\u5fdc\u3001get_dummies \u3084 LabelBinarizer \u3067\u8a66\u307f\u305f\u304c\u3001\u3054\u308a\u62bc\u3057\u306b\u306a\u3063\u305f\u306e\u3067\u3084\u3081\u305f\u3002\n- \u53c2\u8003\u30b5\u30a4\u30c8\n - https:\/\/lime-ml.readthedocs.io\/en\/latest\/index.html\n - https:\/\/github.com\/marcotcr\/lime\n - https:\/\/towardsdatascience.com\/decrypting-your-machine-learning-model-using-lime-5adc035109b5","b1e4f840":"LIME = (L)ocal (I)nterpretable (M)odel-agnostic (E)xplanations)","d6d0e372":"- About example program.\n - Visualize the effectiveness of each explanatory variable related to the survival probability of Titanic by using LIME.\n - Created for the purpose of understanding LIME physically.\n - Program created as binary classification problem of logistic regression. \n - Selecting features and handling-missing-values are decided on my own.\n - Use only Train data.\n- Hot to install LIME.\n - pip install lime\n- Future version upgrade plans.\n - Inplement considering Multiple collinearity.\n    - However, \"Simple and High Quality\" is mandatory.\n    - Anyway, I had tried using get_dummies \/ LabelBinarizer. but implemented spaghetti code, so I gave up.\n- Reference URL.\n - https:\/\/lime-ml.readthedocs.io\/en\/latest\/index.html\n - https:\/\/github.com\/marcotcr\/lime\n - https:\/\/towardsdatascience.com\/decrypting-your-machine-learning-model-using-lime-5adc035109b5","cbd11c74":"### Visualizing explanations for each data","2c64a895":"## Overview","29f6607f":"### Visualizing pre-encoded dataframe","16720992":"## Preprocessing","93585cf0":"- Note\n - LimeTabularExplainer handles Label Encoded Data. So, it's  difficult to select get_dummies or LabelBinalizer. ","4d17eb57":"## Encoding","b9b70d9e":"# Simple Example Using LIME."}}