{"cell_type":{"7da44b21":"code","49e0a3b9":"code","fa65e72a":"code","df9bbc1e":"code","f3fba4ae":"code","6459226a":"code","27147904":"code","17cb9c09":"code","88e4ae6f":"code","4dc1a022":"code","28fbd880":"code","f0199601":"code","2ca27681":"code","d161f24c":"code","b988ba89":"code","b469697d":"code","cf7a4220":"code","92509a7a":"code","142638d1":"code","1ede5dde":"code","89efa13e":"code","e8427cd1":"code","7b9c2921":"code","999fa994":"code","32495c34":"code","928badd1":"code","2b3b4036":"code","8016848e":"code","cff03068":"code","3706290f":"code","1f456746":"code","af361e1e":"code","939ca2e7":"code","d7474401":"code","39a79b01":"code","7b8e310e":"code","6324efc1":"code","fdaa67aa":"code","fd094085":"code","32fb4774":"code","d6f1b21f":"code","73a11084":"code","7fc9476a":"code","d2618027":"markdown","844922af":"markdown","1fa8a228":"markdown","441a01b0":"markdown","b7888683":"markdown","d2eaa172":"markdown","44a98804":"markdown","b7ad422c":"markdown","8217d4f8":"markdown","0dae9e78":"markdown","cf7a0447":"markdown","a6849afe":"markdown","21ef4ed5":"markdown","3415f061":"markdown","89b56a2b":"markdown","d50bc4a2":"markdown","fc804506":"markdown","69f6a69a":"markdown","62c17db9":"markdown","25adc1da":"markdown","2eafe580":"markdown","f98fb371":"markdown"},"source":{"7da44b21":"import nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt              # visualization library\nimport numpy as np  # library for scientific computing and matrix operations \nimport random ","49e0a3b9":"nltk.download('twitter_samples')","fa65e72a":"import numpy as np\nimport pandas as pd\nfrom nltk.corpus import twitter_samples \n                           \n\n# download the stopwords for the process_tweet function\nnltk.download('stopwords')\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings","df9bbc1e":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","f3fba4ae":"print('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))","6459226a":"# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n","27147904":"# make a numpy array representing labels of the tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))","17cb9c09":"# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg","88e4ae6f":"# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)","4dc1a022":"# Print the shape train and test sets\nprint(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))","28fbd880":"# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()","f0199601":"# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])","2ca27681":"# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)","d161f24c":"print('\\201[92m' + tweet)\nprint('\\201[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\nprint(tweet2)","b988ba89":"print()\nprint('\\201[92m' + tweet2)\nprint('\\201[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)","b469697d":"#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)","cf7a4220":"print()\nprint('\\201[92m')\nprint(tweet_tokens)\nprint('\\201[94m')\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('removed stop words and punctuation:')\nprint(tweets_clean)","92509a7a":"print()\nprint('\\201[92m')\nprint(tweets_clean)\nprint('\\201[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('stemmed words:')\nprint(tweets_stem)","142638d1":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","1ede5dde":"def build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs","89efa13e":"tweets = all_positive_tweets + all_negative_tweets\nprint(\"Number of tweets: \", len(tweets))","e8427cd1":"labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))","7b9c2921":"# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')","999fa994":"print(freqs)","32495c34":"# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '\u2764', ':)', ':(', '\ud83d\ude12', '\ud83d\ude2c', '\ud83d\ude04', '\ud83d\ude0d', '\u265b',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata","928badd1":"fig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()","2b3b4036":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \n\nprint(\"Number of tweets: \", len(train_x))","8016848e":"# create frequency dictionary\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))","cff03068":"# test the function below\nprint('This is an example of a positive tweet: \\n', train_x[0])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))","3706290f":"def sigmoid(z): \n\n    h = 1\/(1+np.exp(-z))\n   \n    return h","1f456746":"def gradientDescent(x, y, theta, alpha, num_iters):\n   \n    m = x.shape[0]\n    \n    for i in range(0, num_iters):\n        \n        # get z, the dot product of x and theta\n        z = np.dot(x,theta)\n        \n        # get the sigmoid of z\n        h = sigmoid(z)\n        \n        # calculate the cost function\n        J = -(np.dot(y.T,np.log(h))+np.dot((1-y).T,np.log(1-h)))\/m\n\n        # update the weights theta\n        theta = theta - alpha*(np.dot(x.T,h-y))\/m\n        \n    J = float(J)\n    return J, theta","af361e1e":"def extract_features(tweet, freqs):\n    \n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n       \n    # loop through each word in the list of words\n    for word in word_l:\n         \n        if (word,1.0) in freqs:\n            # increment the word count for the positive label 1\n            x[0,1] += freqs[(word,1.0)]\n        if(word,0.0) in freqs:\n            # increment the word count for the negative label 0\n            x[0,2] += freqs[(word,0.0)]\n        \n    return x","939ca2e7":"# Check your function\n\n# test 1\n# test on training data\ntmp1 = extract_features(train_x[0], freqs)\nprint(tmp1)","d7474401":"# collect the features 'x' and stack them into a matrix 'X'\nX = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\n# training labels corresponding to X\nY = train_y\n\nprint(X.shape)\nprint(Y.shape)","39a79b01":"# Apply gradient descent\nJ, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n\nprint(f\"The cost after training is {J:.6f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")","7b8e310e":"# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (10, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")","6324efc1":"# Equation for the separation plane\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) \/ theta[2]","fdaa67aa":"# Equation for the separation plane\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) \/ theta[2]\nfig, ax = plt.subplots(figsize = (10, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])           # max value in x-axis\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\nplt.show()","fd094085":"def predict_tweet(tweet, freqs, theta):\n\n    # extract the features of the tweet and store it into x\n    x = extract_features(tweet, freqs)\n    \n    # make the prediction using x and theta\n    y_pred = sigmoid(np.dot(x,theta))\n    \n    return y_pred","32fb4774":"# check your own sentiment\nmy_tweet = 'I love Natural Language Processing :)'\ny_pred_temp = predict_tweet(my_tweet, freqs, theta)\nprint(y_pred_temp)\n\nif y_pred_temp > 0.5:\n    print('Positive sentiment')\nelse: \n    print('Negative sentiment')","d6f1b21f":"def test_logistic_regression(test_x, test_y, freqs, theta):\n    \"\"\" \n    test_x: a list of tweets\n    test_y: (m, 1) vector with the corresponding labels for the list of tweets\n    \"\"\"\n    \n    y_hat = []\n    \n    for tweet in test_x:\n        # get the label prediction for the tweet\n        y_pred = predict_tweet(tweet, freqs, theta)\n        \n        if y_pred > 0.5:\n            y_hat.append(1.0)\n        else:\n            y_hat.append(0.0)\n            accuracy = np.sum(np.squeeze(test_y) == np.squeeze(np.asarray(y_hat)))\/len(test_y)\n\n    return accuracy","73a11084":"test_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\nprint(f\"Logistic regression model's accuracy = {test_accuracy:.4f}\")","7fc9476a":"print('Label Predicted Tweet')\nfor x,y in zip(test_x,test_y):\n    y_hat = predict_tweet(x, freqs, theta)\n\n    if np.abs(y - (y_hat > 0.5)) > 0:\n        print('THE TWEET IS:', x)\n        print('THE PROCESSED TWEET IS:', process_tweet(x))\n        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))","d2618027":"# We will select a set of words that we would like to visualize.","844922af":"# Extracting the features","1fa8a228":"# Process tweet\n* The given function process_tweet() tokenizes the tweet into individual words, removes stop words and applies stemming.","441a01b0":"# Logistic regression\n*   Logistic regression\n*   Cost function and Gradient\n*   Update the weights","b7888683":"# Preprocess raw text for Sentiment analysis\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n* NLTK has different modules for stemming but in this kernel i will be using the PorterStemmer.\n","d2eaa172":"# Visualizing tweets","44a98804":"# Table of word counts","b7ad422c":"# Dataset","8217d4f8":"* Train test split: 20% will be in the test set, and 80% in the training set.","0dae9e78":"# Test your logistic regression","cf7a0447":"# About the Dataset\n*  The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset.\n\n","a6849afe":"# Training Your Model\nTo train the model:\n\n* Stack the features for all training examples into a matrix X.\n* Call gradientDescent, which we've implemented above.","21ef4ed5":"# concatenate the lists\n* Posirive and negatice tweets.","3415f061":"# Gradient Descent\n","89b56a2b":"#  download the stopwords","d50bc4a2":"# Prepare The Data.\n* The twitter_samples contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.","fc804506":"* Create the numpy array of positive labels and negative labels.","69f6a69a":"# Error Analysis","62c17db9":"# Visualization","25adc1da":"# Libraries","2eafe580":"# Plot the model alongside the data\n* i will draw a gray line to show the cutoff between the positive and negative regions.","f98fb371":"# Visualizing tweets and the Logistic Regression model"}}