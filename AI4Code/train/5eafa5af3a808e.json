{"cell_type":{"73197738":"code","0ddc5906":"code","db27e073":"code","53f32fb0":"code","5bb54fe2":"code","f78d402a":"code","1d117898":"code","426be746":"code","8ce16c0d":"code","658c3b34":"code","f83f9aa8":"code","69baaba0":"code","f1a1a0e9":"code","5fa6c544":"code","4e0ab500":"code","9ecfe4a9":"code","87296f7d":"code","4ad4636e":"code","1d57dae5":"code","98622816":"code","63eb57f9":"code","f201b8ab":"code","ce4b234c":"code","c323c66a":"code","fe098685":"code","b4ee8c6b":"code","a8e28d24":"code","5fa27224":"code","c630d973":"code","222e2351":"code","f4ee3dc1":"code","526f9ed2":"code","1186c25e":"code","4dd63f99":"code","30984d7e":"code","5fa82e59":"code","6eef520b":"code","564f0643":"code","96488d3d":"code","fc786408":"code","873ad247":"code","9ef72ef1":"code","a8f4c018":"code","7e8cad47":"code","ccf7be0f":"code","e0814796":"code","1e6f53e7":"code","d1e8a155":"code","4e7b948a":"code","5d88d28f":"code","92d8bd6e":"code","d458006a":"code","9eda2b3d":"code","3a641997":"code","41bfe849":"code","b7654af7":"code","1734d179":"code","15408440":"code","3ae19b17":"code","1e1bbbaf":"code","01daf80b":"code","50af10e5":"code","36a38950":"code","68fbec88":"code","75f567d7":"code","3ecabd7c":"code","6641677b":"code","e4198a18":"code","0d93f8ab":"code","191cacf2":"code","bd59f9a4":"code","6b78a6dd":"code","241be831":"code","3d21f8c6":"code","765eab23":"code","103f6faf":"code","41a37d24":"code","f52c6c4f":"markdown","10141491":"markdown","59b32471":"markdown","ada4eba3":"markdown","c5263455":"markdown","b917267b":"markdown","cffd1122":"markdown","7ca6c83d":"markdown","82439b4e":"markdown","d2aeba86":"markdown","48974ed7":"markdown","444d895e":"markdown","0af06aba":"markdown","8b9d8359":"markdown","1f36d026":"markdown","cec39b23":"markdown","c2c96fa0":"markdown","c63267ea":"markdown","7139f7b4":"markdown","8ba2eec1":"markdown","33f8a81e":"markdown","6b4d0452":"markdown","28429b9b":"markdown","6ec8bb1f":"markdown","8457f8c7":"markdown","59181b8b":"markdown","9502ffa9":"markdown","60e3e033":"markdown","0253fd97":"markdown","e2800654":"markdown","be81981b":"markdown","c2bfe127":"markdown"},"source":{"73197738":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # plotting tools\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nplt.rcParams[\"axes.grid\"] = False\n\nimport matplotlib.mlab as mlab\nfrom scipy import signal\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import butter, filtfilt, iirdesign, zpk2tf, freqz\n# Train test split\nfrom sklearn.model_selection import train_test_split\n\nfrom glob import glob\nfrom tqdm import tqdm\n\n# Import tensorflow\nimport tensorflow as tf\n\n# Model & compile arguments\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\n# Get the layers\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Flatten\n\n# Import the Efficientnet models\nfrom tensorflow.keras.applications import EfficientNetB0\n\n# TF model metrics\nfrom tensorflow.keras.metrics import AUC\n\n# \nimport librosa\nimport torch\n\n# (Install &) Import the nnAudio library for Constant Q-Transform\ntry:\n    from nnAudio.Spectrogram import CQT1992v2\nexcept:\n    !pip install -q nnAudio\n    from nnAudio.Spectrogram import CQT1992v2\n#Note: The nnAudio's CQT1992v2 is used, instead of GWpy from analysis notebook, to transform the wave data into Constant Q-Transform spectrograms because this performs the operation much faster and is GPU compatible.\n    \nfrom IPython.display import HTML","0ddc5906":"## Get the training ids\ntrain = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/training_labels.csv')\n\n# Get the subsmission file\nsample_sub = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')","db27e073":"print(f'Training labels: {train.shape[0]} | Test dataset: {sample_sub.shape[0]}')","53f32fb0":"#Checking train data contents\ntrain.head()","5bb54fe2":"train.shape","f78d402a":"train['target'].value_counts()","1d117898":"train['id'].value_counts()","426be746":"#Checking for any Null Values\ntrain.isnull().sum()","8ce16c0d":"sns.countplot(data=train, x=\"target\")\n","658c3b34":"#Obtain path of all 5,60,000 files in .\/train\ntrain_path = glob('..\/input\/g2net-gravitational-wave-detection\/train\/*\/*\/*\/*')","f83f9aa8":"print(\"The total number of files in the training set:\", len(train_path))","69baaba0":"ids = [path.split(\"\/\")[-1].split(\".\")[0] for path in train_path]\npaths_df = pd.DataFrame({\"path\":train_path, \"id\": ids})\ntrain_data = pd.merge(left=train, right=paths_df, on=\"id\")\ntrain_data.head()","f1a1a0e9":"#Load npy files into DataFrame with target 1\nfile_path = pd.DataFrame(train_data).iloc[0]\n\n#Loading any one file\nexample_strain = np.load(file_path.path)","5fa6c544":"#View file contents\nprint(example_strain)\n\n#Shape of the array\nprint (example_strain.shape)","4e0ab500":"#Plotting Raw Data from the three detectors\n\ndef plot_graph(example_strain):\n    plt.figure(figsize=(20,5))\n\n    plt.plot(example_strain[0,:], c=\"firebrick\", label=\"LIGO Hanford\")\n    plt.plot(example_strain[1,:], c=\"mediumseagreen\", label=\"LIGO Livingston\")\n    plt.plot(example_strain[2,:], c=\"slateblue\", label=\"Virgo\")\n    plt.title(\"Id: \"+file_path.id);\n    plt.grid(\"on\")\n    plt.xlabel(\"Timestamp\");\n    plt.legend();","9ecfe4a9":"#Plotting graph for strain with target 1\nplot_graph(example_strain)","87296f7d":"#Load npy files into DataFrame with target 0\nfile_path = pd.DataFrame(train_data).iloc[1]\n\n#Loading any one file\nexample_strain_negative = np.load(file_path.path)\n\n#Plotting graph for strain with target 0\nplot_graph(example_strain_negative)","4ad4636e":"#define some signal parameters\nsample_rate = 2048 #Hz (1\/seconds)\ntime_span = 2 # each signal lasts 2 s\nsignal_length = time_span\nsamples_total = time_span * sample_rate\ndt = 1\/(samples_total) #4096 points in total\ndt\n\nchannel = 1 #picking detector 1","1d57dae5":"# set of observatories\nobs_list = ('LIGO Hanford', 'LIGO Livingston', 'Virgo')","98622816":"# Gravitational wave analysis python library\ntry:\n    import gwpy\nexcept:\n    !pip install -q --user gwpy\n    import gwpy\nfrom gwpy.timeseries import TimeSeries","63eb57f9":"# function to plot the amplitude spectral density (ASD) plot\ndef plot_asd(sample_id):\n    # Get the data\n    sample = example_strain\n    \n    # we convert the data to gwpy's TimeSeries for analysis\n    for i in range(sample.shape[0]):\n        ts = TimeSeries(sample[i], sample_rate=sample_rate)\n        ax = ts.asd(signal_length).plot(figsize=(12, 5)).gca()\n        ax.set_xlim(10, 1024);\n        ax.set_title(f\"ASD plots for sample: {sample_id} from {obs_list[i]}\");","f201b8ab":"# plot ASD for sample w\/ GW\nplot_asd(file_path.id)","ce4b234c":"#Computing Power Series Density\nplt.figure(figsize=(20,5))\n\nfhat = np.fft.fft(example_strain[channel,:], samples_total)\nPSD = fhat * np.conj(fhat) \/ samples_total\nfreq = 1\/(dt*samples_total) * np.arange(samples_total)\n\n\nL = np.arange(1, np.floor(samples_total\/2), dtype=\"int\")\nplt.plot(freq[L],PSD[L], '.-')\nplt.grid(\"on\")\nplt.xlabel(\"Frequency Hz\");\nplt.title(\"Power spectral density\");\nplt.yscale(\"log\")","c323c66a":"#breaking signal into FFT components\nshow_side_effects = True\nfig, ax = plt.subplots(6,1,figsize=(20,15))\n\nax[0].plot(example_strain[channel])\nax[1].plot(np.fft.ifft((PSD>1e-38)*fhat))\nax[2].plot(np.fft.ifft(((PSD>1e-40) & (PSD <= 1e-38))*fhat))\nax[3].plot(np.fft.ifft(((PSD>1e-42) & (PSD <= 1e-40))*fhat))\nax[4].plot(np.fft.ifft(((PSD>0.5e-42) & (PSD <= 1e-42))*fhat))\nax[5].plot(np.fft.ifft((PSD<=0.5e-42)*fhat))\n\nif not show_side_effects:\n    for n in range(3,6):\n        ax[n].set_xlim(20,2000)","fe098685":"from scipy import signal\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import butter, filtfilt, iirdesign, zpk2tf, freqz","b4ee8c6b":"hp_window = 1\nhp_tukey_alpha = 0.125\nfband = [35.0, 200.0]","a8e28d24":"blackman_window = signal.blackman(int(samples_total*hp_window)) #signal.tukey(strain, alpha=1.\/8)\ntukey_window = signal.tukey(samples_total*hp_window, hp_tukey_alpha)","5fa27224":"fig, ax = plt.subplots(3,1,figsize=(20,15))\n\n#plotting raw data\nax[0].plot(example_strain[channel])\nax[0].set_title(\"Original data\")\n\n#plotting data with blackman \nax[1].plot(example_strain[channel]*blackman_window)\nax[1].set_title(\"With blackman window applied\")\n\n#plotting data with tukey window\nax[2].plot(example_strain[channel]*tukey_window)\nax[2].set_title(\"With tukey window applied\");","c630d973":"windowed_strain = example_strain[channel]*tukey_window","222e2351":"#Whitening data to make signal more uniform\ndef whiten(strain, samples_total, dt):\n    # TODO: normalization \n    \n    fhat = np.fft.fft(strain, samples_total)\n    PSD = fhat * np.conj(fhat) \/ samples_total\n    freq = 1\/(dt*samples_total) * np.arange(samples_total)\n    \n    # scipy interp1d interpolation\n    interp_psd = interp1d(freq, PSD, \"nearest\")\n    \n    w_fhat = fhat\/np.sqrt(interp_psd(freq))\n    w_strain = np.fft.ifft(w_fhat)\n    return w_strain, interp_psd(freq)","f4ee3dc1":"w_strain, ip = whiten(windowed_strain, samples_total, dt)","526f9ed2":"#plotting whitened data\nfig, ax = plt.subplots(2,1,figsize=(20,10))\nax[0].plot(np.log(ip[0:1024]), '-o')\nax[0].set_title(\"Interpolated PSD\")\nax[0].set_xlabel(\"Frequency Hz\")\nax[0].set_ylabel(\"Sn(t)\")\nax[1].plot(w_strain, '-.')\nax[1].set_ylabel(\"dw(t)\")\nax[1].set_xlabel(\"Timestamp\")\n","1186c25e":"#Defining bandpass filter\ndef bandpass(strain, fband, fs):\n    \"\"\"Bandpasses strain data using a butterworth filter.\n    \n    Args:\n        strain (ndarray): strain data to bandpass\n        fband (ndarray): low and high-pass filter values to use\n        fs (float): sample rate of data\n    \n    Returns:\n        ndarray: array of bandpassed strain data\n    \"\"\"\n    bb, ab = butter(4, [fband[0]*2.\/fs, fband[1]*2.\/fs], btype='band')\n    normalization = np.sqrt((fband[1]-fband[0])\/(fs\/2))\n    strain_bp = filtfilt(bb, ab, strain) \/ normalization\n    return strain_bp\n\n#Applying bandpass filter\nbandpassed_strain = bandpass(w_strain, fband, samples_total)\n\n#plotting bandpass filtered data\nplt.figure(figsize=(20,5))\nplt.plot(bandpassed_strain, '-')","4dd63f99":"# function to plot the Q-transform spectrogram\ndef plot_q_transform(sample_id,example_strain):\n    # Get the data\n    sample = example_strain\n    \n    # we convert the data to gwpy's TimeSeries for analysis\n    for i in range(sample.shape[0]):\n        ts = TimeSeries(sample[i], sample_rate=sample_rate)\n        ax = ts.q_transform(whiten=True).plot().gca()\n        ax.set_xlabel('')\n        ax.set_title(f\"Spectrogram plots for sample: {sample_id} from {obs_list[i]}\")\n        ax.grid(False)\n        ax.set_yscale('log');","30984d7e":"# plot the Q-transform for sample w\/ GW\nplot_q_transform(file_path.id,example_strain)","5fa82e59":"sample_gw_id = pd.DataFrame(train_data).iloc[0].id\nsample_no_gw_id = pd.DataFrame(train_data).iloc[1].id\n\n# function to plot the Q-transform spectrogram side-by-side\ndef plot_q_transform_sbs(sample_gw_id, sample_no_gw_id,example_strain,example_strain_negative ):\n    # Get the data\n    sample_gw = example_strain\n    sample_no_gw = example_strain_negative\n    \n    for i in range(len(obs_list)):\n        # get the timeseries\n        ts_gw = TimeSeries(sample_gw[i], sample_rate=sample_rate)\n        ts_no_gw = TimeSeries(sample_no_gw[i], sample_rate=sample_rate)\n        \n        # get the Q-transform\n        image_gw = ts_gw.q_transform(whiten=True)\n        image_no_gw = ts_no_gw.q_transform(whiten=True)\n\n        plt.figure(figsize=(20, 10))\n        plt.subplot(131)\n        plt.imshow(image_gw)\n        plt.title(f\"id: {sample_gw_id} | Target=1\")\n        plt.grid(False)\n\n        plt.subplot(132)\n        plt.imshow(image_no_gw)\n        plt.title(f\"id: {sample_no_gw_id} | Target=0\")\n        plt.grid(False)\n        \n        plt.show()\n","6eef520b":"# let's plot two spectrograms for sample w\/ and w\/o GW signal side-by-side\nplot_q_transform_sbs(sample_gw_id, sample_no_gw_id,example_strain,example_strain_negative )","564f0643":"# function to return the npy file corresponding to the id\ndef get_npy_filepath(id_, is_train=True):\n    path = ''\n    if is_train:\n        return f'..\/input\/g2net-gravitational-wave-detection\/train\/{id_[0]}\/{id_[1]}\/{id_[2]}\/{id_}.npy'\n    else:\n        return f'..\/input\/g2net-gravitational-wave-detection\/test\/{id_[0]}\/{id_[1]}\/{id_[2]}\/{id_}.npy'","96488d3d":"# let's define some signal parameters\nsample_rate = 2048 # data is provided at 2048 Hz\nsignal_length = 2 # each signal lasts 2 s\nfmin, fmax = 20, 1024 # filter above 20 Hz, and max 1024 Hz (Nyquist freq = sample_rate\/2)\nhop_length = 64 # hop length parameter for the stft\n\n# model compile params\nbatch_size = 250 # size in which data is processed and trained at-once in model\nepochs = 3 # number of epochs (keep low as dataset is quite large 3~5 is enough as observed)","fc786408":"# Define the Constant Q-Transform\ntransform = CQT1992v2(sr=sample_rate, fmin=fmin, fmax=fmax, hop_length=hop_length)\n\n# check if GPU enabled, then run the transform on GPU for faster execution\n# if tf.test.is_gpu_available():\n#     cq_transform = cq_transform.to('cuda')","873ad247":"# function to load the file, preprocess, return the respective Constant Q-transform\n# the Cqt function\n# preprocess function\ndef preprocess_function_cqt(path):\n    signal = np.load(path.numpy())\n    # there are 3 signal as explained before for each interferometers\n    for i in range(signal.shape[0]):\n        # normalize signal\n        signal[i] \/= np.max(signal[i])\n    # horizontal stack\n    signal = np.hstack(signal)\n    # tensor conversion\n    signal = torch.from_numpy(signal).float()\n    # getting the image from CQT transform\n    image = transform(signal)\n    # converting to array from tensor\n    image = np.array(image)\n    # transpose the image to get right orientation\n    image = np.transpose(image,(1,2,0))\n    \n    # conver the image to tf.tensor and return\n    return tf.convert_to_tensor(image)","9ef72ef1":"image = preprocess_function_cqt(tf.convert_to_tensor(train_data['path'][2]))\nprint(image.shape)\nplt.imshow(image)","a8f4c018":"# From the Constant Q-Transform that we got, get the shape\ninput_shape = (69, 193, 1)","7e8cad47":"# Get the feature ids and target\nX = train_data['id']\ny = train_data['target'].astype('int8').values","ccf7be0f":"x_train, x_valid, y_train, y_valid = train_test_split(X, y, random_state = 51, stratify = y)","e0814796":"def preprocess_function_parse_tf(path, y=None):\n    [x] = tf.py_function(func=preprocess_function_cqt, inp=[path], Tout=[tf.float32])\n    x = tf.ensure_shape(x, input_shape)\n    if y is None:\n        return x\n    else:\n        return x,y","1e6f53e7":"train_dataset = tf.data.Dataset.from_tensor_slices((x_train.apply(get_npy_filepath).values, y_train))\n# shuffle the dataset\ntrain_dataset = train_dataset.shuffle(len(x_train))\ntrain_dataset = train_dataset.map(preprocess_function_parse_tf, num_parallel_calls=tf.data.AUTOTUNE)\ntrain_dataset = train_dataset.batch(batch_size)\ntrain_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)","d1e8a155":"# valid dataset\n# Get the data filepaths as tensor_slices\nvalid_dataset = tf.data.Dataset.from_tensor_slices((x_valid.apply(get_npy_filepath).values, y_valid))\n\n# apply the map method to tf_parse_function()\nvalid_dataset = valid_dataset.map(preprocess_function_parse_tf, num_parallel_calls=tf.data.AUTOTUNE)\n\n# set batch size of the dataset\nvalid_dataset = valid_dataset.batch(batch_size)\n\n# prefetch the data\nvalid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n","4e7b948a":"#CNN Modeling\ntrain_dataset.take(1)\n\n#Instantiate the Sequential model\nmodel_cnn = Sequential(name='CNN_model')\n\n# Add the first Convoluted2D layer w\/ input_shape & MaxPooling2D layer followed by that\nmodel_cnn.add(Conv2D(filters=16,\n                     kernel_size=3,\n                     input_shape=input_shape,\n                     activation='relu',\n                     name='Conv_01'))\nmodel_cnn.add(MaxPooling2D(pool_size=2, name='Pool_01'))\n\n# Second pair of Conv1D and MaxPooling1D layers\nmodel_cnn.add(Conv2D(filters=32,\n                     kernel_size=3,\n                     input_shape=input_shape,\n                     activation='relu',\n                     name='Conv_02'))\nmodel_cnn.add(MaxPooling2D(pool_size=2, name='Pool_02'))\n\n# Third pair of Conv1D and MaxPooling1D layers\nmodel_cnn.add(Conv2D(filters=64,\n                     kernel_size=3,\n                     input_shape=input_shape,\n                     activation='relu',\n                     name='Conv_03'))\nmodel_cnn.add(MaxPooling2D(pool_size=2, name='Pool_03'))\n\n# Add the Flatten layer\nmodel_cnn.add(Flatten(name='Flatten'))\n\n# Add the Dense layers\nmodel_cnn.add(Dense(units=512,\n                activation='relu',\n                name='Dense_01'))\nmodel_cnn.add(Dense(units=64,\n                activation='relu',\n                name='Dense_02'))\n\n# Add the final Output layer\nmodel_cnn.add(Dense(1, activation='sigmoid', name='Output'))","5d88d28f":"# Display the CNN model architecture\nmodel_cnn.summary()","92d8bd6e":"# compile the model with following parameters\n# Optimizer: Adam (learning_rate=0.0001)\n# loss: binary_crossentropy\n# metrics: accuracy\/AUC\nmodel_cnn.compile(optimizer=Adam(learning_rate=0.0001),\n                  loss='binary_crossentropy',\n                  metrics=[[AUC(), 'accuracy']])\n\n# Fit the data\nhistory_cnn = model_cnn.fit(x=train_dataset,\n                            epochs=3,\n                            validation_data=valid_dataset,\n                            batch_size=batch_size,\n                            verbose=1)","d458006a":"# save the model\nmodel_cnn.save('.\/model_CNN.h5')","9eda2b3d":"sub = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\nx_test = sub[['id']]","3a641997":"x_test.tail()","41bfe849":"# test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test['id'].apply(get_npy_filepath, is_train=False).values))\ntest_dataset = test_dataset.map(preprocess_function_parse_tf, num_parallel_calls=tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(batch_size)\ntest_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)","b7654af7":"# load the trained Simple CNN model \nsaved_cnn_model = tf.keras.models.load_model('..\/input\/g2netfinal\/full_cnn_model.h5')","1734d179":"saved_cnn_model.fit(x=valid_dataset, epochs=3, batch_size=batch_size, verbose=1)","15408440":"saved_cnn_model.save('.\/model\/full_cnn_model.h5')","3ae19b17":"full_cnn_model = tf.keras.models.load_model('.\/model\/full_cnn_model.h5')","1e1bbbaf":"# predict the test dataset using CNN\npreds_cnn = saved_cnn_model.predict(test_dataset)","01daf80b":"# Function to save kaggle submissions for test prediction probabilities\ndef get_kaggle_format(prediction_probs, model='base'):\n    # load the sample submission file\n#     sub = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\n    sample_sub['target'] = prediction_probs\n    \n    # Output filename for kaggle submission\n    filename = f\"kaggle_sub_{model}.csv\"\n    \n    # Save the DataFrame to a file\n    sample_sub.to_csv(filename, index=False)\n    print(f'File name: {filename}')","50af10e5":"#save the kaggle submission file\nget_kaggle_format(preds_cnn, model='cnn')","36a38950":"# Import libraries\nimport matplotlib.pyplot as plt # plotting tools\nfrom random import shuffle\nimport math\nimport os\n\n#import keras\n!pip install -U git+https:\/\/github.com\/leondgarse\/keras_efficientnet_v2\nimport re\nimport os\nfrom scipy.signal import get_window\nfrom typing import Optional, Tuple\nimport warnings\nimport random\nimport math\nimport tensorflow as tf\nimport keras_efficientnet_v2\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import mixed_precision\nimport tensorflow_addons as tfa\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","68fbec88":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        policy = mixed_precision.Policy('mixed_bfloat16')\n        mixed_precision.set_global_policy(policy)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","75f567d7":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access (Train tf records)\nGCS_PATH1 = KaggleDatasets().get_gcs_path('g2net-tf-records-tr-bp-filter-1')\nGCS_PATH2 = KaggleDatasets().get_gcs_path('g2net-tf-records-tr-bp-filter-1')\nGCS_PATH3 = KaggleDatasets().get_gcs_path('g2net-tf-records-tr-bp-filter-3')\n# Data access (Test tf records)\nGCS_PATH4 = KaggleDatasets().get_gcs_path('g2net-tf-records-ts-bp-filter-1')\nGCS_PATH5 = KaggleDatasets().get_gcs_path('g2net-tf-records-ts-bp-filter-2')\n\n# Configuration\nEPOCHS = 30\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 2021\n# Learning rate\nLR = 0.0001\n# Verbosity\nVERBOSE = 1\n\n# Training filenames directory\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH1 + '\/train*.tfrec') + tf.io.gfile.glob(GCS_PATH2 + '\/train*.tfrec') + tf.io.gfile.glob(GCS_PATH3 + '\/train*.tfrec')\n# Testing filenames directory\nTESTING_FILENAMES = tf.io.gfile.glob(GCS_PATH4 + '\/test*.tfrec') + tf.io.gfile.glob(GCS_PATH5 + '\/test*.tfrec')","3ecabd7c":"from sklearn.model_selection import train_test_split\n\ntrain_names, valid_names = train_test_split(TRAINING_FILENAMES, test_size = 0.20, random_state = 51)\nvalid_names","6641677b":"# Function to create cqt kernel\ndef create_cqt_kernels(\n    q: float,\n    fs: float,\n    fmin: float,\n    n_bins: int = 84,\n    bins_per_octave: int = 12,\n    norm: float = 1,\n    window: str = \"tukey\",\n    fmax: Optional[float] = None,\n    topbin_check: bool = True\n) -> Tuple[np.ndarray, int, np.ndarray, float]:\n    fft_len = 2 ** _nextpow2(np.ceil(q * fs \/ fmin))\n    \n    if (fmax is not None) and (n_bins is None):\n        n_bins = np.ceil(bins_per_octave * np.log2(fmax \/ fmin))\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] \/ np.float(bins_per_octave))\n    elif (fmax is None) and (n_bins is not None):\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] \/ np.float(bins_per_octave))\n    else:\n        warnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n        n_bins = np.ceil(bins_per_octave * np.log2(fmax \/ fmin))\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] \/ np.float(bins_per_octave))\n        \n    if np.max(freqs) > fs \/ 2 and topbin_check:\n        raise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n                           please reduce the `n_bins`\")\n    \n    kernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n    \n    length = np.ceil(q * fs \/ freqs)\n    for k in range(0, int(n_bins)):\n        freq = freqs[k]\n        l = np.ceil(q * fs \/ freq)\n        \n        if l % 2 == 1:\n            start = int(np.ceil(fft_len \/ 2.0 - l \/ 2.0)) - 1\n        else:\n            start = int(np.ceil(fft_len \/ 2.0 - l \/ 2.0))\n\n        sig = get_window(window, int(l), fftbins=True) * np.exp(\n            np.r_[-l \/\/ 2:l \/\/ 2] * 1j * 2 * np.pi * freq \/ fs) \/ l\n        \n        if norm:\n            kernel[k, start:start + int(l)] = sig \/ np.linalg.norm(sig, norm)\n        else:\n            kernel[k, start:start + int(l)] = sig\n    return kernel, fft_len, length, freqs\n\n\ndef _nextpow2(a: float) -> int:\n    return int(np.ceil(np.log2(a)))\n\n# Function to prepare cqt kernel\ndef prepare_cqt_kernel(\n    sr=22050,\n    hop_length=512,\n    fmin=30,\n    fmax=1024,\n    n_bins=84,\n    bins_per_octave=12,\n    norm=1,\n    filter_scale=1,\n    window=\"hann\"\n):\n    q = float(filter_scale) \/ (2 ** (1 \/ bins_per_octave) - 1)\n    print(q)\n    return create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)\n\n# Function to create cqt image\ndef create_cqt_image(wave, hop_length=16):\n    CQTs = []\n    for i in range(3):\n        x = wave[i]\n        x = tf.expand_dims(tf.expand_dims(x, 0), 2)\n        x = tf.pad(x, PADDING, \"REFLECT\")\n\n        CQT_real = tf.nn.conv1d(x, CQT_KERNELS_REAL, stride=hop_length, padding=\"VALID\")\n        CQT_imag = -tf.nn.conv1d(x, CQT_KERNELS_IMAG, stride=hop_length, padding=\"VALID\")\n        CQT_real *= tf.math.sqrt(LENGTHS)\n        CQT_imag *= tf.math.sqrt(LENGTHS)\n\n        CQT = tf.math.sqrt(tf.pow(CQT_real, 2) + tf.pow(CQT_imag, 2))\n        CQTs.append(CQT[0])\n    return tf.stack(CQTs, axis=2)\n\nHOP_LENGTH = 6\ncqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n    sr=2048,\n    hop_length=HOP_LENGTH,\n    fmin=20,\n    fmax=1024,\n    bins_per_octave=9)\nLENGTHS = tf.constant(lengths, dtype=tf.float32)\nCQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\nCQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\nPADDING = tf.constant([[0, 0],\n                        [KERNEL_WIDTH \/\/ 2, KERNEL_WIDTH \/\/ 2],\n                        [0, 0]])","e4198a18":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# Function to prepare image\ndef prepare_image(wave):\n    # Decode raw\n    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n    normalized_waves = []\n    # Normalize\n    for i in range(3):\n        normalized_wave = wave[i] \/ tf.math.reduce_max(wave[i])\n        normalized_waves.append(normalized_wave)\n    # Stack and cast\n    wave = tf.stack(normalized_waves)\n    wave = tf.cast(wave, tf.float32)\n    # Create image\n    image = create_cqt_image(wave, HOP_LENGTH)\n    # Resize image\n    image = tf.image.resize(image, [*IMAGE_SIZE])\n    # Reshape\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\n# This function parse our images and also get the target variable\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        'wave': tf.io.FixedLenFeature([], tf.string),\n        'wave_id': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = prepare_image(example['wave'])\n    image_id = example['wave_id']\n    target = tf.cast(example['target'], tf.float32)\n    return image, image_id, target\n\n# This function parse our images and also get the target variable\ndef read_unlabeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        'wave': tf.io.FixedLenFeature([], tf.string),\n        'wave_id': tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = prepare_image(example['wave'])\n    image_id = example['wave_id']\n    return image, image_id\n\n# This function loads TF Records and parse them into tensors\ndef load_dataset(filenames, ordered = False, labeled = True):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False \n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) \n    return dataset\n\n# This function is to get our training dataset\ndef get_training_dataset(filenames, ordered = False, labeled = True):\n    dataset = load_dataset(filenames, ordered = ordered, labeled = labeled)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(filenames, ordered = False, labeled = True):\n    dataset = load_dataset(filenames, ordered = ordered, labeled = labeled)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# This function is to get our validation and test dataset\ndef get_val_test_dataset(filenames, ordered = True, labeled = True):\n    dataset = load_dataset(filenames, ordered = ordered, labeled = labeled)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) \n    return dataset\n\n# Function to count how many photos we have in\ndef count_data_items(filenames):\n    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = count_data_items(train_names)\nNUM_VALID_IMAGES = count_data_items(valid_names)\nNUM_TESTING_IMAGES = count_data_items(TESTING_FILENAMES)\nprint(f'Dataset: {NUM_TRAINING_IMAGES} training images')\nprint(f'Dataset: {NUM_VALID_IMAGES} valid images')\nprint(f'Dataset: {NUM_TESTING_IMAGES} testing images')","0d93f8ab":"# Learning rate callback function\ndef get_lr_callback():\n    lr_start   = 0.0001\n    lr_max     = 0.000015 * BATCH_SIZE\n    lr_min     = 0.0000001\n    lr_ramp_ep = 3\n    lr_sus_ep  = 0\n    lr_decay   = 0.7\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start   \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max    \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = VERBOSE)\n    return lr_callback\n\n# Function to create our EfficientNetB7 model\ndef get_model():\n    with strategy.scope():\n        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3))\n        x = keras_efficientnet_v2.EfficientNetV2XL(drop_connect_rate=0.2, num_classes=0, pretrained=\"imagenet21k-ft1k\")(inp)\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n        model = tf.keras.models.Model(inputs = [inp], outputs = [output])\n        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n        opt = tfa.optimizers.SWA(opt)\n        model.compile(\n            optimizer = opt,\n            loss = [tf.keras.losses.BinaryCrossentropy()],\n            metrics = [tf.keras.metrics.AUC()]\n        )\n        return model\n    \n# Function to train a model with 100% of the data\ndef train_and_evaluate():\n    print('\\n')\n    print('-'*50)\n    if tpu:\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n    train_dataset = get_training_dataset(train_names, ordered = False, labeled = True)\n    train_dataset = train_dataset.map(lambda image, image_id, target: (image, target))\n    \n    valid_dataset = get_validation_dataset(valid_names, ordered = False, labeled = True)\n    valid_dataset = valid_dataset.map(lambda image, image_id, target: (image, target))\n    \n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ (BATCH_SIZE * 4)\n    K.clear_session()\n    # Seed everything\n    seed_everything(SEED)\n    model = get_model()\n    es = EarlyStopping(patience = 5, restore_best_weights=True,verbose=1)\n    history = model.fit(train_dataset,\n                        validation_data = valid_dataset,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        epochs = EPOCHS,\n                        callbacks = [get_lr_callback(), es], \n                        verbose = VERBOSE)\n        \n    print('\\n')\n    print('-'*50)\n    print('Test inference...')\n    # Predict the test set \n    dataset = get_val_test_dataset(TESTING_FILENAMES, ordered = True, labeled = False)\n    image = dataset.map(lambda image, image_id: image)\n    test_predictions = model.predict(image).astype(np.float32).reshape(-1)\n    # Get the test set image_id\n    image_id = dataset.map(lambda image, image_id: image_id).unbatch()\n    image_id = next(iter(image_id.batch(NUM_TESTING_IMAGES))).numpy().astype('U')\n    # Create dataframe output\n    test_df = pd.DataFrame({'id': image_id, 'target': test_predictions})\n    # Save test dataframe to disk\n    test_df.to_csv(f'submission_efn_{IMAGE_SIZE[0]}_{SEED}.csv', index = False)\n    \ntrain_and_evaluate()","191cacf2":"# load the CNN predictions into a dataframe\ndf_preds_cnn = pd.read_csv('..\/input\/g2netassets\/kaggle_sub_cnn.csv')\ndf_preds_cnn.head()","bd59f9a4":"df_preds_cnn.shape","6b78a6dd":"df_preds_cnn[(df_preds_cnn['target'] >= 0.9) | (df_preds_cnn['target'] <= 0.1)]['target'].count()","241be831":"df_preds_cnn[(df_preds_cnn['target'] >= 0.8) | (df_preds_cnn['target'] <= 0.2)]['target'].count()","3d21f8c6":"# load the EFNet predictions into a dataframe\ndf_preds_efn = pd.read_csv('.\/submission_efn_512_2021.csv')\ndf_preds_efn.head()","765eab23":"df_preds_efn.shape","103f6faf":"df_preds_efn[(df_preds_efn['target'] >= 0.9) | (df_preds_efn['target'] <= 0.1)]['target'].count()","41a37d24":"df_preds_efn[(df_preds_efn['target'] >= 0.8) | (df_preds_efn['target'] <= 0.2)]['target'].count()","f52c6c4f":"**Constant Q-Transform**\n- The signal analysis didn't provide much insights, so let's try the second method in signal processing. Tranforming the waves into spectrograms images, i.e. frequency-domain, and then visualize them. This technique is widely used in audio analysis and since our data is a wave with bunch of frequencies, we can use the same technique as well.\n- The advantage of using a spectrogram, over a direct Fourier Transform where you lose time info, is that it captures the shift or change in frequencies over time and this removes white noise frequencies that are persistent, leaving the signals of interest. Constant Q-Transform is one way to visualize the spectrogram.","10141491":"- Here, out of the 226000 total test predictions, we can say that 74524, or ~61% of the values were predicted by the EfficientNet model with high confidence (>80% probability) for either class\n- 70680 or ~31% were predicted with more than 90% probability. ","59b32471":"**Insights**\n- The three signals originating from different detectors all look a bit different.\n- It is difficult to infer just by looking whether the given wave has GW signal or not as there is no notable difference between graphs of *target - 0* and *target - 1.*\n- The strain is of the order $10^{-20}$, which is extremely small and can be affected by many external factors. However, as seen in both the sample plots, the strain data is a combination of many frequencies and analysing the signals in frequency domain, instead of the time domain, might give us better insights.\n- Depending on the location the amplitude recorded varies ( smaller amplitudes indicating *weak signal* detection.\n- Astrophysical signals have typical amplitudes comparable to the detector background noise. Therefore, characterization and reduction of detector noise is essential to GW searches. \n- The interferometer is sensitive towards gravitational waves but unfortunately also for terrestrial forces and displacements.This may also include vibrations of the instruments themselves etc.. This kind of forces cause streching of the interferometer arms and this leads to constructive interferance and the waves we can see above.\n\n### Typical signal processing workflow\nNext, we try to implement the steps from this paper by LIGO by following these steps:\n- Plot the raw signal\n- Window the signal\n- Whiten the signal\n- Bandpass the signal\n\n## Raw Data Visulaization - ASD\nA Fourier Transform is the most commonly used method in maths and signal processing, to decompose the signals into its constituent discrete frequencies. This spectrum of frequencies can be analyzed based on average, power or energy of the signal to get a spectral density plot. As it says, one of the ways to visualize a raw signal in frequency domain is by plotting the amplitude spectral density (ASD).","ada4eba3":"\n\n![KLE TU](https:\/\/pbs.twimg.com\/media\/CEvKZ8CUsAAj3Ll.jpg)\n<h2 style=\"text-align:center;\">Data Mining and Analysis Course Project ( 2021 )<\/h2>\n\n***\n<h3 style=\"text-align:center;\"> Team - 5D03<\/h3>\n\n- [Avantika Shrivastava](https:\/\/www.kaggle.com\/avantikashrivastava) *- 01FE19BCS253* \n- [Tanmayi Shurpali](https:\/\/www.kaggle.com\/t01fe19bcs238)     *- 01FE19BCS238*\n- [Shrinidhi Kulkarni](https:\/\/www.kaggle.com\/shrinidhi05)   *- 01FE19BCS241* \n- [Bhavana Kumbar](https:\/\/www.kaggle.com\/bhavanakumbar)        *- 01FE19BCS244*\n\n***\n\n## Contents of this Notebok\n1. Imports\n2. Reading DataSet\n3. Explorative Data Analysis\n4. Preprocessing\n5. Model Training\n6. Evaluation\n7. Results and Conclusion\n\n***\n***Kaggle Challange Name:*** [G2Net Gravitational Wave Detection](https:\/\/www.kaggle.com\/c\/g2net-gravitational-wave-detection) - (Submission Deadline : 30th September, 2021)\n***\n***Introduction*** : \n*Gravitational Waves have been discussed since the beginning of the 20th century, and scientifically researched since the Einstein's General Theory of Relativity. They are caused by massive celestial bodies, like the Neutron Stars or ***Black Holes***, when they accelerate they cause gravitational waves, in the form of waves, propagating through the curvature of space-time at the speed of light. These disturbances can be felt on the other side of the observable universe, but are extremely weak as they lose energy as gravitational radiation. It can be imagined similar to throwing a pebble in the pond, the site where the pebble hits water is the source of the disturbance and the outgoing ripples, are the gravitational waves, that get weaker as they move away from the source.In February 2015, the Laser Interferometer Gravitational-wave Observatory ***(LIGO) Scientific Collaboration and the Virgo Collaboration*** announced the first observation of a Gravitational-Wave (GW) signal from a ***stellar-mass Compact Binary Coalescence (CBC) system** .Despite all the initial successes, the future of GW astronomy is facing many challenges. Because of the effectiveness of ML algorithms in identifying patterns in data, ML techniques may be harnessed to make all these searches more sensitive and robust. Applications of ML algorithms to GW searches range from building automated data analysis methods for low-latency pipelines to distinguishing terrestrial noise from astrophysical signals and improving the reach of searches.*\n\n\n![KLE TU](https:\/\/media2.giphy.com\/media\/xT9IgoYWAh5lYliiYM\/giphy.gif)\n\n\n***Problem Statement*** : *To preprocess data then build, train & evaluate binary classification model to predict if the given set of signals has Gravitational Waves in them or not.*\n\n***Objectives :*** \n1. *To understand and visulaize raw data using EDA methods*\n2. *To build and train model using Deep Learning Techniques*\n3. *To evaluate model using metrics like ROC AUC (receiver operating charateristics area under curve)*\n\n***\nNote: This notebook was developed and run in the kaggle notebook environment.","c5263455":"# 2. Reading Datasets\n\n   1. **train\/** - the training set files, one npy file per observation; labels are provided in a files shown below\n   2. **test\/** - the test set files; you must predict the probability that the observation contains a gravitational wave\n   3. **training_labels.csv** - target values of whether the associated signal contains a gravitational wave \n   4. **sample_submission.csv** - a sample submission file in the correct format","b917267b":"- The baseline model seems to be converging well only after about 3 epochs. \n- But, it takes almost about an hour to train each epoch, we can only say that the model can be improved with further training and fine-tuning the structure. \n- At the end of 3rd epoch, we see 0.83 AUC score and 0.76 accuracy for training dataset, while 0.84 AUC score and 0.77 accuracy for the validation dataset.","cffd1122":"For the analysis of transient data the use of **Tukey windows** is advantageous as signals will suffer less modification than, **Blackman windows**.","7ca6c83d":"- There are **5,60,000 rows** and **2 columns.**\n- The attributes are **'id'** and **'target'**","82439b4e":"- Apart from a few hints, we cannot say for sure that the difference between the waves with and without GW signals is obvious. \n- There can be some cleaning or filtering we can apply to remove the noise further but that's where the Deep Learning shines. \n- The things we can't detect visually, machine learning can. Next, in the modelling notebook, we build data pipelines, transform the data to spectrograms, and build models to make the predictions.\n***\n\n# 4.Preprocessing Methods\nAstrophysical signals have typical amplitudes comparable to the detector background noise. Therefore, characterization and reduction of detector noise is essential to GW searches. \nWe follow signal processing methodology to preprocess signals, converting the time domain data to frequency domain, converting to Constant Q-Transform images and using these as input to our model training step. \n\nThere are mainly two ways in which we can preprocess this type of data to train our models:\n\n1.  **Using the time series data,** and performing some cleaning steps to enhance the signal, remove noise, as described in publications by B P Abbott et al. and Daniel George et al.Typical signal processing workflow\nNext, we try to implement the steps from this paper referenced above by following these steps:\nPlot the raw signal\nWindow the signal\nWhiten the signal\nBandpass the signal\n1. **Getting the Constant Q-Transformed spectrogram image,** which is a frequency-domain fourier transformed data, while treating the sample being analyzed as a wave.","d2aeba86":"- Visibly, all three signals have different features and the above were plotted from a sample which has gravitational waves, and it shows the famous 'chirp' confirming the presence of gravitational waves. \n- This transformation removes the unwanted noise frequencies, but still some of it remains, but a signal has to be detected in all three waves to be predicted as gravitational wave.\n- Next, we can compare how the Q-Transforms look for samples with and without gravitational wave signals.","48974ed7":"***\n**References**\n1. [Enhancing gravitational-wave science with machine learning](https:\/\/iopscience.iop.org\/article\/10.1088\/2632-2153\/abb93a\/pdf)\n2. [Improving significance of binary black hole mergers in Advanced LIGO data using deep learning](https:\/\/arxiv.org\/abs\/2010.08584)\n3. [GW Tutorials](https:\/\/www.gw-openscience.org\/LVT151012data\/LOSC_Event_tutorial_LVT151012.html#Intro-to-signal-processing)\n4. [tf.data: Build TensorFlow input pipelines](https:\/\/www.tensorflow.org\/guide\/data)\n5. [Image classification via fine-tuning with EfficientNet](https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/)","444d895e":"## 1. Baseline Model : Simple CNN","0af06aba":"## Insights\n- The steep shape at low frequencies is dominated by noise related to ground motion. \n- Above roughly 100 Hz, the Advanced LIGO detectors are currently quantum noise limited, and their noise curves are dominated by shot noise. \n- High amplitude noise features are also present in the data at certain frequencies, including lines due to the AC power grid (harmonics of 60 Hz in the U.S. and 50 Hz in Europe), mechanical resonances of the mirror suspensions, injected calibration lines, and noise entering through the detector control systems. ","8b9d8359":"- The first step in many LVC analyses is to Fourier transform the time-domain data using a fast Fourier transform (FFT) . - - There are interesting kind of side effects in the beginning and the end of each wave after doing the inverse Fourier transform.\n- Since the FFT implicitly assumes that the stretch of data being transformed is periodic in time, window functions have to be applied to the data to suppress spectral leakage using e.g. a Tukey (cosine-tapered) window function. \n- Failing to window the data will lead to spectral leakage and spurious correlations in the phase between bins. \n\n### Apply Window Functions - Removing Spectral leakages","1f36d026":"These plots are plotted on a log scale for x-axis, and we see that it ranges from 10 Hz ~ 1000 Hz. Although, these limits are for visualization purposes only, it helps us see some peaks for each observatory. A particular frequency can be peculiar in one measurement but remember that the GW signal has to be detected in all three waves to be confirmed. This data here still seems a bit noisy and as showed in the tutorial, if sampled for longer periods of time (on real data), it can give some valuable insights. However, the data in this competition is simulated and we try to find other ways to visualize it.\n***\n### Power Spectral Density Plots","cec39b23":"This is the whitened signal. Next, since we know this data is from merger binary black holes, the frequency is in lower range and this we apply a bandpass filter to passthrough signals between 35 ~ 350 Hz.\n***\n### Bandpass Filter - Filtering signal for certain bandwidth","c2c96fa0":"The Simple CNN model has been trained and can be found in our this version of [Notebook](https:\/\/www.kaggle.com\/t01fe19bcs238\/g2net-eda-preprocessing-model) ","c63267ea":"# 3. Explorative Data Analysis\n","7139f7b4":"**Inference**\n- The data is evenly distributed with 50-50 division between the samples with and without gravitational waves signal.\n\n**Insights**\n- The data is binary classified.\n- target *'0'* indicates absence of GW signal i.e **only noise** where as *'1'* indicates presence of GW signal i.e **GW signal + noise.**\n\n## 3.2 Raw Data Visualization\nTo visualize raw signal, we load any one np array file as sample","8ba2eec1":"***\n# 5. Modeling\n**Strategy**\n\n- This is essentially a signal processing problem with classification task, there can be two ways in which we can build models around this data, as also mentioned in the [LIGO research paper](https:\/\/arxiv.org\/pdf\/1908.11170.pdf) - using \"raw\" signals with minimal pre-processing and using \"images\" by transforming the waves into spectrograms. \n- However, building models on raw signal data, by following the cleaning steps from respective publications, didnot yield acceptable results. It is worth mentioning that only a part of the data was used while strategy selection process, and it was concluded that more pre-processing was necessary, or rather proper pre-processing, if we were to use raw signal.\n- Eventually, the second method that we went with in this project, is used to transform the waves into the spectrogram image. We train two models to evaluate the results:\n1. Simple CNN- a simple CNN architecture that is a modified version of the model usually used in MNIST Digit Recognizer tutorials. This acts as our baseline model.\n2. EfficientNet-a EfficientNetB7 model that has been developed and pre-trained on ImageNet dataset. This model is chosen as it is known for its excellent performance with significantly fewer number of parameters, that can drastically improve the computational efficiency.","33f8a81e":"**Inference :**\n- There are **3** rows.\n- Each index(row) has **4096** columns.\n\n\n**Insights :**\n- In the competition description it is given that the observations are recorded from 3  gravitational wave interferometers (LIGO Hanford, LIGO Livingston, and Virgo)\n- The quantity in this time series is strain, which is of the order of ~$10^{-20}$, recorded for 2 sec periods sampled at 2048 Hz - 4096 data points.\n- The output of a GW detector is a temporal series of the detector strain, h(t).","6b4d0452":"**'target'** has value either *'0' or '1'*.","28429b9b":"### Whitening - Making signal more Uniform\n\n- Whitening the data is suppressing the extra noise at low frequencies and at the spectral lines, to better see the weak signals in the most sensitive band.\n- It is always one of the first steps in astrophysical data analysis (searches, parameter estimation).\n- It requires no prior knowledge of spectral lines, etc; only the data are needed.","6ec8bb1f":"# 1.Imports","8457f8c7":"# 6. Evaluation\n- We compiled both the models to keep track of  ROC AUC\n- The focus was on looking out for a good AUC value, which tells us that the model is good at separating the two classes well. \n- We compared the two models later and also see what kaggle submission scores we get from our predictions for the test dataset.\n\nThe most basic direct comparison between the two models, our simple CNN and the EfficientNet, is summarized in the following table.\n\n<table>\n    <tr>\n        <th>Model<\/th>\n        <th>Train AUC<\/th>\n        <th>Val AUC<\/th>\n        <th>Test AUC (kaggle)<\/th>\n        <th>Avg time\/epoch<\/th>\n    <\/tr>\n    <tr>\n        <td>Simple CNN<\/td>\n        <td>0.8363<\/td>\n        <td>0.8388<\/td>\n        <td>0.8435<\/td>\n        <td>3300s|55min<\/td>\n    <\/tr>\n     <tr>\n        <td>EfficientNetB7<\/td>\n        <td>0.8952<\/td>\n        <td>0.8800<\/td>\n        <td>0.8754<\/td>\n        <td>860s|15min<\/td>\n    <\/tr>\n <table>\n\n- Since the predictions made by model are predicted values for the classes, we can look at the predicted values to judge how well our model did classifying those, specifically how confidently did the model predicted those targets. \n- Closer the predicted probabilities of the target are to 0 and 1, we can say more confident the model output is.","59181b8b":"***\n## 2. Advanced Model - EfficientNet B7 Model\n\n- The baseline model performed quite well actually, but it was a very simple model which we trained for our particular dataset from scratch .\n- However, there are more advanced and pre-trained state-of-the-art models that we can try to use for our classification task. \n- EfficientNet is one such model architecture that has been researched extensively recently, and has achieved state-of-the-art level accuracy as compared to other models on ImageNet data with significantly fewer number of parameters, which means faster training times. \n- As we have a large dataset, we can use these models, with and without pretrained weights to see if we get better results than our baseline. ","9502ffa9":"There are no *null* values in the file","60e3e033":"**Creating TF Data pipeline :**\nNext, we create the TensorFlow input data pipeline. This is crucial as loading such a huge dataset can create a bottleneck on the entire workflow and can cause memory overload.","0253fd97":"- As you can see, out of the 226000 total test predictions, we can say that 74524, or ~33% of the values were predicted by the CNN model with high confidence (>80% probability) for either class\n- 48979 or ~22% were predicted with more than 90% probability. \n- Now this cannot be translated directly into good performance, without the true test values; but with further training, regularization and structure changes, we can seek to improve these values in the future.","e2800654":"**'id'** has 5,60,000 unique values.","be81981b":"***\n# 7.Results & Conclusions\n\n- Gravitational Waves are NOT EASY to detect! Once detected, they are hard to find. \n- After sifting through a varierty of preprocessing steps, we transformed the orginal strain wave data into frequency spectrograms, which are images that we then used to train deep learning models. \n- One of the biggest challenges in this project was managing such a large dataset, which was solved by using the TensorFlow's tf.data API, and streamlining the entire workflow all the way from data import to model training & prediction tasks. This helped us achieve the goal of this project of building a pipeline that is flexible and can be reused in the future.\n- Our simple CNN architecture, just after 3 epochs, was performing more than expected.\n- The Efficient Net B7 model worked quite well with AUC score of 0.8754\n- We evaluated the models for ROC AUC score, as we wanted our model to be good at separating the two classes, but also tracked accuracy scores for comparison. Overall, we achieved AUC score of 0.8754 on the test dataset from kaggle.","c2bfe127":"There are **5,60,000** records in train data and **2,26,000** records in Test data"}}