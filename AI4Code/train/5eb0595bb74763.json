{"cell_type":{"1a9c9a78":"code","cc36a262":"code","45a4d57d":"code","50f70583":"code","ca8047a6":"code","36b2d565":"code","c5f18a96":"code","c179f97d":"code","16bfb6aa":"code","010fb873":"code","eba56ae8":"code","a0e99381":"code","9e5b437a":"code","05cf4ba5":"code","f0f2f2f0":"code","9301cf60":"code","c713ef50":"code","e38f7341":"code","27d05094":"code","d8235642":"code","c2fe7090":"code","eddcb8a2":"code","9a59f933":"code","66c873c0":"code","deb5e10e":"code","df782637":"code","ea36d5ea":"code","0e510d88":"code","9b0fa9c5":"code","fa065a07":"code","1c10d137":"code","07e2116c":"code","30510d14":"code","62a3545a":"code","60435054":"code","1c58f5b5":"code","22a41244":"code","876cc58e":"code","7ee4bbb7":"code","754ab0ed":"code","a76fd663":"code","55cde4f4":"markdown","b40acf7b":"markdown","2145f6de":"markdown","5d4cc7f9":"markdown","b65c20ca":"markdown","80c172f5":"markdown","f9e0772c":"markdown","227935b9":"markdown","3f7f126e":"markdown","c06933cd":"markdown","e192c258":"markdown","beb85d53":"markdown","56fba6be":"markdown","db98b7d1":"markdown","79bff3fe":"markdown","994af145":"markdown","c365c558":"markdown","6e21d68b":"markdown","dac597a4":"markdown","9fbd709e":"markdown","91ffd890":"markdown","4f953dc2":"markdown","a83793cd":"markdown","48508fe0":"markdown"},"source":{"1a9c9a78":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cc36a262":"# to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","45a4d57d":"df = pd.read_csv(\"\/kaggle\/input\/preprocessed-titanic-dataset\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/preprocessed-titanic-dataset\/test.csv\")","50f70583":"df.head()","ca8047a6":"test.head()","36b2d565":"from sklearn.model_selection import train_test_split\nX = df.drop('Survived',axis=1)\ny = df['Survived']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)","c5f18a96":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scal = scaler.fit_transform(X_train)\nX_test_scal = scaler.transform(X_test)","c179f97d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV","16bfb6aa":"from sklearn.metrics import classification_report,confusion_matrix","010fb873":"model_log = LogisticRegression()\nmodel_log.fit(X_train_scal,y_train)\npred = model_log.predict(X_test_scal)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","eba56ae8":"param_grid = {\n    'penalty':['none', 'l1', 'l2', 'elasticnet'],\n    'C':  [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n    'solver':['ewton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'max_iter':[100,300,900],\n}\n\ngrid = GridSearchCV( LogisticRegression(),param_grid=param_grid,n_jobs=-1)\ngrid.fit(X_train_scal,y_train)\npred = grid.predict(X_test_scal)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","a0e99381":"model_svc = SVC(probability=True)\nmodel_svc.fit(X_train_scal,y_train)\npred = model_svc.predict(X_test_scal)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","9e5b437a":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_scal,y_train)\n    pred_i = knn.predict(X_test_scal)\n    error_rate.append(np.mean(pred_i != y_test))","05cf4ba5":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","f0f2f2f0":"model_knn =  KNeighborsClassifier(n_neighbors=15)\nmodel_knn.fit(X_train_scal,y_train)\npred = model_knn.predict(X_test_scal)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","9301cf60":"model_dt = DecisionTreeClassifier()\nmodel_dt.fit(X_train,y_train)\npred = model_dt.predict(X_test)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","c713ef50":"from sklearn.tree import plot_tree\nplt.figure(figsize=(100,100))\nplot_tree(model_dt,feature_names=X_train.columns.values,  \n                   class_names=[\"0\",\"1\"],\n                   filled=True);","e38f7341":"param_grid = {\n    'criterion': [\"gini\", \"entropy\"],\n    'max_depth' : [5, 8, 15, 25,None],\n    'min_samples_split' : [2, 5, 10, 15],\n    'max_features' :[\"auto\", \"sqrt\", \"log2\"],\n    'min_samples_leaf' : [1, 2, 5] \n}\n\ngrid = GridSearchCV(DecisionTreeClassifier(),param_grid=param_grid,verbose=1,n_jobs=-1)\ngrid.fit(X_train,y_train)\npred = grid.predict(X_test)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","27d05094":"grid.best_estimator_","d8235642":"model_random = RandomForestClassifier(200,n_jobs=-1)\nmodel_random.fit(X_train,y_train)\npred = model_random.predict(X_test)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","c2fe7090":"param_grid = {\n    'criterion': [\"gini\", \"entropy\"],\n    'max_depth' : [5, 8, 15, 25,None],\n    'min_samples_split' : [2, 5, 10, 15],\n    'bootstrap' : [True,False],\n    'max_features' :[\"auto\", \"sqrt\", \"log2\"],\n    'min_samples_leaf' : [1, 2, 5] \n}\n\ngrid = GridSearchCV(RandomForestClassifier(200,random_state=1),param_grid=param_grid,verbose=1,n_jobs=-1)\ngrid.fit(X_train,y_train)\npred = grid.predict(X_test)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","eddcb8a2":"model_xgb = xgb.XGBClassifier()\nmodel_xgb.fit(X_train,y_train)\npred = model_xgb.predict(X_test)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","9a59f933":"params = {'objective':\"binary:logistic\",\n          'base_score':0.005,\n          'eval_metric':'error',\n          'n_jobs':-1\n}\ntrainm = xgb.DMatrix(X_train,y_train)\nvalidm = xgb.DMatrix(X_test,y_test)","66c873c0":"from sklearn.metrics import accuracy_score\ndef accuracy(y_prob, dtrain):\n    y_true = dtrain.get_label()\n    best_mcc = accuracy_score(y_true, (y_prob>0.5)*1)\n    return 'accuracy_score', best_mcc","deb5e10e":"watchlist = [(trainm, 'train'), (validm, 'val')]\nxgb_model = xgb.train(params, trainm,\n                num_boost_round=100,\n                evals=watchlist,\n                early_stopping_rounds=20,\n                feval=accuracy,\n                maximize=True\n                )","df782637":"xgb_model = xgb.train(params, trainm,\n                num_boost_round=7,\n                feval=accuracy\n                )","ea36d5ea":"pred = xgb_model.predict(validm)\nprint(classification_report(y_test,(pred>0.5)*1))\nconfusion_matrix(y_test,(pred>0.5)*1)","0e510d88":"#feature importance graph of XGboost\nxgb.plot_importance(model_xgb)","9b0fa9c5":"np.set_printoptions(suppress=True)\nxgb_pred = xgb_model.predict(validm)\nknn_pred = model_knn.predict_proba(X_test_scal)[:,0]\nsvc_pred = model_svc.predict_proba(X_test_scal)[:,0]","fa065a07":"log_pred = model_log.predict_proba(X_test_scal)[:,0]\ndt_pred = model_dt.predict_proba(X_test)[:,0]\nrandom_pred = model_random.predict_proba(X_test)[:,0]","1c10d137":"combined_pred = 1*((knn_pred + svc_pred+ log_pred)\/3 < 0.5)\n\nprint(classification_report(y_test,combined_pred))\nconfusion_matrix(y_test,combined_pred)","07e2116c":"combined_pred = 1*((knn_pred + svc_pred+ log_pred+ xgb_pred )\/4 < 0.5)\n\nprint(classification_report(y_test,combined_pred))\nconfusion_matrix(y_test,combined_pred)","30510d14":"from sklearn.ensemble import VotingClassifier\nvote = VotingClassifier(estimators=[('svc',SVC()),\n                                    ('knn',KNeighborsClassifier(n_neighbors=15)),\n                                    ('log',LogisticRegression()),\n                                   ])\nvote.fit(X_train_scal,y_train)\npred = vote.predict(X_test_scal)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","62a3545a":"from sklearn.ensemble import VotingClassifier\nvote = VotingClassifier(estimators=[('svc',SVC()),\n                                    ('knn',KNeighborsClassifier(n_neighbors=15)),\n                                    ('log',LogisticRegression()),\n                                    ('Xgboost',xgb.XGBClassifier()),\n                                    #('dtree',DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='sqrt',min_samples_leaf=2, min_samples_split=10)),\n                                    ('randomf',RandomForestClassifier(200,n_jobs=-1))\n                                   ])\nvote.fit(X_train_scal,y_train)\npred = vote.predict(X_test_scal)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","60435054":"from sklearn.ensemble import BaggingClassifier","1c58f5b5":"bag = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=15),n_estimators=7,n_jobs=-1,random_state=4)\nbag.fit(X_train_scal,y_train)\npred = bag.predict(X_test_scal)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","22a41244":"from sklearn.ensemble import AdaBoostClassifier","876cc58e":"boost = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=200,n_jobs=-1),n_estimators=20)\nboost.fit(X_train,y_train)\npred = boost.predict(X_test)\nprint(classification_report(y_test,pred))\nconfusion_matrix(y_test,pred)","7ee4bbb7":"scaler = StandardScaler()\nscaler.fit(X)\ntest_scal = scaler.transform(test.drop(['PassengerId'],axis=1))","754ab0ed":"test['Survived'] = bag.predict(test_scal)","a76fd663":"test[['PassengerId','Survived']].to_csv(\"submission.csv\",index=False)","55cde4f4":"Now Let's plot how the decision tree was actually built. The image is large and zoom in to the maximum to see the features","b40acf7b":"We will try LogisticRegression, SVM, KNN,Decision Tree, RandomForest and xgboost algorithms.\n<br><br>\nFor each model we will try the default hyperparameters and then try tuning hyperparameters using GridSearch. You can increase the paramaters in gridsearchcv param_grid but remember it also increases the time to fit","2145f6de":"\n## 2. Voting Classifier","5d4cc7f9":"# Predict on Test Data\n\nWe find that our KNN model with bagging have given us the best score! **0.84**. So let's make predictions on the test data.","b65c20ca":"### RandomForest","80c172f5":"Since KNN has performed the best I have tried the bagging and boosting with KNN. Feel free to try for other algorithms too but remember bagging will lead to overfitting in several cases","f9e0772c":"### Support Vector Machines","227935b9":"\n**Note for Begginers:** To use the cleaned dataset click on Add data on your right of the kernel.\n![s.PNG](attachment:s.PNG)\n<br>\nSelect search by url and paste the link [Pre-Processed Titanic Dataset](https:\/\/www.kaggle.com\/aakashveera\/preprocessed-titanic-dataset) and click add button will be added to the kernel and you can access it directly similarly to the titanic dataset. (Don't forget to upvote if you like the dataset and notebook)\n ","3f7f126e":"The above image shows the the feature importance for the xgboost algorithm.<br><br>\nfeature importance can't be plotted for KNN and SVC with a rbf kernel \n<br><br>\nXGboost, KNN and SVC has performed well in the individual performance. Now lets try out ensembling techniques to improve the score further. ","c06933cd":"For Linear Models such as Logistic Regression, SVM as well as KNN data needs to be in Scaled.\n<br>For tree based models such as Decision tree, RandomForest, and other ensemble methods normal data is sufficient. Let's try both.\n<br>\n\nBefore that let's do train, validaion split to evaluate the performance of the model using train_test_split of sklearn. <br>\nrandom_state is just any number to do the same split each time you call the function and its optional. <br>\nstratify to divide the ratio of class 0 and 1 and train and test equally and it is again optional.","e192c258":"## 4.Boosting","beb85d53":"## 1. Probality based prediction voter\n\nlet's predict the probablity using different models and then average the probablities. Finally the prob>0.5 can be made 1 and less than 0.5 as 0","56fba6be":"Boosting is converting weak learners into strong learners. XGboost is based on Adaboost.\n\n**Note**: Adaboost does not support SVM and KNN.","db98b7d1":"### Logistic Regression","79bff3fe":"### XGBOOST","994af145":"# Modelling Notebook\n**Note**: This notebook has the modelling section alone. I have made another notebook [EDA and Feature Engineering Titanic](https:\/\/www.kaggle.com\/aakashveera\/eda-and-feature-engineering-titanic) which has the feature engineering and pre-processing section.<br>\n<br>\n**In this notebook I have tried out several algorithms(logistic regression, SVM, KNN, Decision Tree, RandomForest, Xgboost)**\n<br>\n\n**As well as Several Ensemble methods such as Bagging, Boosting as well as voting.**\n<br>\n\nHere, I am going to use my cleaned version of titanic data [Pre-Processed Titanic Dataset](https:\/\/www.kaggle.com\/aakashveera\/preprocessed-titanic-dataset) which is created using the notebook [EDA and Feature Engineering Titanic](https:\/\/www.kaggle.com\/aakashveera\/eda-and-feature-engineering-titanic). Check it out for the data pre-processing. <br>\n\nIf you have better prepared data try these algorithms and ensembles with your data.\n\nAlso you can use this dataset [Pre-Processed Titanic Dataset](https:\/\/www.kaggle.com\/aakashveera\/preprocessed-titanic-dataset) directly if you want to skip the data cleaning part and want to try out machine learning.\n<br>\n","c365c558":"### KNN","6e21d68b":"If you want to check image of the tuned decision tree you cannot visualize directly from the gridsearch. <br><br>\nRefit the model with the best above estimators then plot using the same method ","dac597a4":"Feel free to try out all other combinations and find whether there is a improvement in score.<br>\n\n### **But don't forgot to use scaled data for linear models (logistic, SVC and KNN) and normal data for other algorithms**","9fbd709e":"## 3. Bagging","91ffd890":"Now lets try voting classifier with only linear models as well as all models","4f953dc2":"Thanks for reading my kernel any improvements will be welcomed and if you like this kernel give a upvote !","a83793cd":"### Decision Tree","48508fe0":"We got the lowest possible errror at n_neighbours = 15 we will use it"}}