{"cell_type":{"f318f613":"code","51931a9a":"code","711b3538":"code","0308803b":"code","2f2aaf38":"code","3af76f87":"code","a3bf4905":"code","e4cb0963":"code","00e40124":"code","dff4b27f":"code","bf659f36":"code","0af88f7b":"code","9e9175db":"code","50e0002c":"code","2651b5a0":"code","80f29c7d":"code","c3165acd":"code","2722604f":"code","7dd37ac8":"code","4b87b4b1":"code","b933f368":"code","a07dea61":"code","35cae28a":"code","6acd1c17":"code","2d82b32e":"code","380ef948":"code","61a0fb21":"code","6e5b9749":"code","0665585f":"code","d81e0e1b":"code","16c551d7":"markdown","127c3236":"markdown","5042c9d7":"markdown","c62bd872":"markdown","1657c8bf":"markdown","d6f746dd":"markdown","89578462":"markdown","6ca9bdca":"markdown","768c99c2":"markdown","8f9d93b2":"markdown","2ec0adbe":"markdown","6e99f41b":"markdown","ee3bd271":"markdown","6d557663":"markdown","d844b18e":"markdown","c1b0ad65":"markdown","588d4301":"markdown","57d829c0":"markdown","033653a5":"markdown","e42ba216":"markdown","b364cc42":"markdown","daabf68e":"markdown","08fa14f7":"markdown","48e59eea":"markdown","b70519cd":"markdown","956146cb":"markdown"},"source":{"f318f613":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","51931a9a":"import pandas as pd\n\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col=\"PassengerId\")\nrows, cols = df.shape\n\nprint(f\"Original DataFrame has {rows} rows and {cols} columns\")\ndf.head()","711b3538":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n# Encode Sex column with LabelEcoder\ndf[\"Sex\"] = label_encoder.fit_transform(df[\"Sex\"])\n\n# Family Size: Sum of sibilings, parents and self\ndf[\"Family_Size\"] = df[\"SibSp\"] + df[\"Parch\"] + 1  # add self\n\n# Calculate fare per person\ndf[\"Fare_Per_Person\"] = df[\"Fare\"] \/ df[\"Family_Size\"]\n\ndf.tail()","0308803b":"set1 = df.copy()\nset1 = set1[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nset1.dropna(axis=0, inplace=True)\nrows, cols = set1.shape\n\nprint(f\"set1 DataFrame has {rows} rows and {cols} columns\")\nset1.tail()","2f2aaf38":"y = set1[\"Age\"]\nX = set1[[\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]","3af76f87":"from sklearn.model_selection import train_test_split\n\n# Split data into training and validation set\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\nprint(f\"Training features shape: {train_X.shape}\")\nprint(f\"Training labels shape: {train_y.shape}\")\nprint(f\"Testing features shape: {val_X.shape}\")\nprint(f\"Testing labels shape: {val_y.shape}\")","a3bf4905":"from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n# Function to detect underfitting and overfitting\ndef get_mae1(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    # Define model\n    age_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    # Fit model\n    age_model.fit(train_X, train_y)\n    # Predict Age on validation data\n    val_predictions = age_model.predict(val_X)\n    return mean_absolute_error(val_y, val_predictions)\n\n\nfor max_leaf_nodes in [5, 10, 15, 20, 50, 100]:\n    mae = get_mae1(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(f\"Max leaf nodes: {max_leaf_nodes}  \\t\\t Mean Absolute Error: {mae}\")\n\n# 10 Seems to be the best value","e4cb0963":"# Define model\nage_model = DecisionTreeRegressor(max_leaf_nodes=10, random_state=0)\n\n# Fit model\nage_model.fit(X, y)","00e40124":"set2 = df.copy()\ncolumns = [\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\nset2 = set2.loc[set2[\"Age\"].isnull(), columns]\nrows, cols = set2.shape\n\nprint(f\"set2 DataFrame has {rows} rows and {cols} columns\")\nset2.tail()","dff4b27f":"X = set2[[\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]\n\nset2[\"Age\"] = age_model.predict(X)\nset2.head()","bf659f36":"set1 = set1.append(set2)\nrows, cols = set1.shape\n\nprint(f\"set1 DataFrame has {rows} rows and {cols} columns\")\nset1.head()","0af88f7b":"y = set1[\"Survived\"]\nX = set1[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]]","9e9175db":"# Split data into training and validation set\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\nprint(f\"Training features shape: {train_X.shape}\")\nprint(f\"Training labels shape: {train_y.shape}\")\nprint(f\"Testing features shape: {val_X.shape}\")\nprint(f\"Testing labels shape: {val_y.shape}\")","50e0002c":"from sklearn.ensemble import RandomForestClassifier\n\n\n# Function to detect underfitting and overfitting\ndef get_mae2(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    # Define model\n    survival_model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    # Fit model\n    survival_model.fit(train_X, train_y)\n    # Predict Survived on validation data\n    val_predictions = survival_model.predict(val_X)\n    return mean_absolute_error(val_y, val_predictions)\n\n\nfor max_leaf_nodes in [5, 8, 10, 15, 20, 50, 100]:\n    mae = get_mae2(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(f\"Max leaf nodes: {max_leaf_nodes}  \\t\\t Mean Absolute Error: {mae}\")\n\n# 5 Seems to be the best value but using 15 leaf-nodes","2651b5a0":"# 15 yields Score of 0.79\nsurvival_model = RandomForestClassifier(max_leaf_nodes=15, random_state=0)\naccuracy = survival_model.fit(X, y).score(X, y)\nprint(f\"Accuracy value: {accuracy}\")","80f29c7d":"tdf = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\", index_col=\"PassengerId\")\nrows, cols = tdf.shape\n\nprint(f\"Original Test DataFrame has {rows} rows and {cols} columns\")\ntdf.head()","c3165acd":"# Encode Sex column with LabelEcoder\nlabel_encoder = LabelEncoder()\ntdf[\"Sex\"] = label_encoder.fit_transform(tdf[\"Sex\"])\n\n# Family Size: Sum of sibilings, parents and self\ntdf[\"Family_Size\"] = tdf[\"SibSp\"] + tdf[\"Parch\"] + 1  # add self\n\n# Calculate fare per person\ntdf[\"Fare_Per_Person\"] = tdf[\"Fare\"] \/ tdf[\"Family_Size\"]\n\ntdf.tail()","2722604f":"tdf[tdf[\"Fare\"].isnull()]","7dd37ac8":"test_set1 = tdf.copy()\nfiltr = (~test_set1[\"Fare\"].isnull()) & (~test_set1[\"Age\"].isnull()) & (test_set1[\"Pclass\"] == 3)\ntest_set1 = test_set1[filtr]\nrows, cols = test_set1.shape\n\nprint(f\"test_set1 DataFrame has {rows} rows and {cols} columns\")\ntest_set1.head()","4b87b4b1":"y = test_set1[\"Fare\"]\nX = test_set1[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]]\n\nfare_model = DecisionTreeRegressor(random_state=1)\nr_squared = fare_model.fit(X, y).score(X, y)\nprint(f\"R-Squared value: {r_squared}\")","b933f368":"tdf.loc[1044, [\"Fare\"]] = fare_model.predict([[3, 1, 60.5, 0, 0]])\ntdf.loc[1044, [\"Fare_Per_Person\"]] = fare_model.predict([[3, 1, 60.5, 0, 0]])\ntdf.loc[1044]","a07dea61":"test_set1 = tdf.copy()\ntest_set1 = test_set1.loc[~tdf[\"Age\"].isnull()]\nrows, cols = test_set1.shape\n\nprint(f\"test_set1 DataFrame has {rows} rows and {cols} columns\")\ntest_set1.head()","35cae28a":"y = test_set1[\"Age\"]\nX = test_set1[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]","6acd1c17":"# Split data into training and validation set\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\nprint(f\"Training features shape: {train_X.shape}\")\nprint(f\"Training labels shape: {train_y.shape}\")\nprint(f\"Testing features shape: {val_X.shape}\")\nprint(f\"Testing labels shape: {val_y.shape}\")","2d82b32e":"# Function to detect underfitting and overfitting\ndef get_mae3(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    # Define model\n    age_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    # Fit model\n    age_model.fit(train_X, train_y)\n    # Predict Age on validation data\n    val_predictions = age_model.predict(val_X)\n    return mean_absolute_error(val_y, val_predictions)\n\n\nfor max_leaf_nodes in [5, 10, 15, 20, 50, 100]:\n    mae = get_mae3(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(f\"Max leaf nodes: {max_leaf_nodes}  \\t\\t Mean Absolute Error: {mae}\")\n\n# 10 Seems to be the best value","380ef948":"# Define model\nage_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model\nage_model.fit(X, y)","61a0fb21":"test_set2 = tdf.copy()\ntest_set2 = test_set2.loc[tdf[\"Age\"].isnull()]\nrows, cols = test_set2.shape\n\nprint(f\"test_set2 DataFrame has {rows} rows and {cols} columns\")\ntest_set2.head()","6e5b9749":"X = test_set2[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]\n\ntest_set2[\"Age\"] = age_model.predict(X)\ntest_set2.head()","0665585f":"test_set1 = test_set1.append(test_set2)\nrows, cols = test_set1.shape\n\nprint(f\"test_set1 DataFrame has {rows} rows and {cols} columns\")\ntest_set1.head()","d81e0e1b":"X = test_set1[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n\npredictions = survival_model.predict(X)\noutput = pd.DataFrame({\"PassengerId\": test_set1.index, \"Survived\": predictions})\noutput.to_csv(\"my_submission.csv\", index=False)\n\nprint(\"my_submission.csv is ready to submit!\")","16c551d7":"## PART 1 - Predict missing age in training data","127c3236":"#### Predict missing `Fare` in `tdf`","5042c9d7":"### Build a model to predict the chances of survival","c62bd872":"#### [Feature engineering](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/)","1657c8bf":"#### DecisionTreeRegressor","d6f746dd":"#### Split training dataset into _train_X, val_X, train_y, val_y_","89578462":"#### Define _features_ and _label_ from training dataset","6ca9bdca":"#### Prepare data for `Survived` predictions","768c99c2":"#### Build model to predict missing `Fare` in `tdf`","8f9d93b2":"### Build model to predict missing `Age` in test data","2ec0adbe":"#### Define Features and Label","6e99f41b":"#### Define Features and Labels","ee3bd271":"#### Prepare data","6d557663":"#### `Set1`: DataFrame with records having data in all columns","d844b18e":"#### Update Dataframe with predicted `Age` using `age_medel`","c1b0ad65":"#### DecisionTreeRegressor","588d4301":"#### RandomForestClassifier","57d829c0":"## PART 3 - Predict Survivers in test data","033653a5":"### Build a model to predict the missing age in training dataset","e42ba216":"#### Import test data from csv file into a `DataFrame`","b364cc42":"## PART 2 - Predict missing age in testing data","daabf68e":"One row is missing data in `Fare` column","08fa14f7":"#### Define train_X, val_X, train_y, val_y","48e59eea":"#### [Feature engineering](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/)","b70519cd":"#### Import data from a csv file into a `DataFrame` using `pandas`","956146cb":"#### Split test dataset into train_X, val_X, train_y, val_y"}}