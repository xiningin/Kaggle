{"cell_type":{"17756abc":"code","c992bc87":"code","b4c5ace0":"code","75e345de":"code","817737d9":"code","0c95edb8":"code","b6ff6b6c":"code","df7e7e89":"code","ce9d4f34":"code","b41913af":"code","93a78cac":"code","26801133":"code","e32a76ee":"code","9e067d2b":"code","4e793389":"code","735099db":"code","9f7d3303":"code","f773923d":"code","5e58f6df":"code","07848e79":"code","045f55e7":"code","8cf3b4fb":"code","0209c410":"code","7f9073c0":"code","b4dab18b":"code","611503b1":"code","d6faa801":"code","db73c25c":"code","c3934dfc":"code","27372138":"code","96808651":"code","7bc4785d":"code","18f0e4a6":"code","767f109d":"code","b8ba9e6b":"code","0b5b0d92":"markdown","20499bdd":"markdown","7db1e192":"markdown","7f00377c":"markdown","8ee832f3":"markdown","1b0a9552":"markdown","efc54bad":"markdown","86691052":"markdown","238da2a3":"markdown","4a7813ba":"markdown","c67c093f":"markdown","5f29d304":"markdown","79ce3986":"markdown","706fd30b":"markdown","d2da8821":"markdown","eb8653f5":"markdown","f0a237e1":"markdown","98e5a7f7":"markdown","3b5bae89":"markdown","509d8af5":"markdown","853dd8a5":"markdown","5e46c5ce":"markdown","ae35cc7c":"markdown","eb431c12":"markdown","12e8a3c2":"markdown","bb4974ab":"markdown","6251ed5e":"markdown","b2801768":"markdown","98c4baf0":"markdown","f48b621c":"markdown","cc7622ec":"markdown","c1b13c43":"markdown","c52fe554":"markdown","8ae2ce0b":"markdown","7cd75722":"markdown","f765068d":"markdown","b434913c":"markdown","7c468194":"markdown","c43899f6":"markdown","73d004a9":"markdown","7bf8512b":"markdown","e3f1e1b0":"markdown","662a148c":"markdown","fb2b8a89":"markdown","18c47333":"markdown","3286947b":"markdown","8f976101":"markdown"},"source":{"17756abc":"import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import (\n    tree,linear_model, metrics, neural_network, pipeline, preprocessing, model_selection)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (mean_absolute_error,mean_squared_error )","c992bc87":"import os\nprint(os.listdir(\"..\/input\"))","b4c5ace0":"df = pd.read_csv('..\/input\/charity\/charity.csv')\nprint(\"Number of observations before dropping NA is {}\".format(len(df)))\ndf = df.dropna()\nprint(df.describe())\ndf.head()","75e345de":"sns.pairplot(df, hue = 'respond', corner=True)","817737d9":"plt.title(\"The Fraction of Respondants\", fontsize=16)\nplt.pie(df['respond'].value_counts(),autopct='%.2f',shadow=True, startangle=90)\nplt.legend([\"No\",\"Yes\"])\nplt.axis('equal') \nplt.show()","0c95edb8":"corr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","b6ff6b6c":"def kde_plot(x,hue,df):\n    sns.displot(\n    data = df, x=x, hue=hue,\n    facet_kws=dict(margin_titles=True), kind=\"kde\")","df7e7e89":"features_list = ['gift','resplast','weekslast','propresp','mailsyear','giftlast','avggift']\ndef density_plot(x,hue,df,den): # this function can plot either histogram or density\n    # den at 0 is histogram while 1 is distribution plot\n    if den ==1: \n        sns.displot(data = df, x=x, hue=hue,facet_kws=dict(margin_titles=True), kind=\"kde\",aspect = 2, multiple=\"stack\")\n        plt.show()\n    elif den == 0:\n        sns.displot(data = df, x=x, hue=hue,facet_kws=dict(margin_titles=True),aspect = 2,)\n        plt.show()\n    else:\n        print(\"Incorrect Value!\")","ce9d4f34":"# For gift, we will plot density.\ndensity_plot(features_list[0],'respond',df,0)","b41913af":"# For resplast, we will plot histogram.\ndensity_plot(features_list[1],'respond',df,0)","93a78cac":"# For weekslast, we will plot density.\ndensity_plot(features_list[2],'respond',df,1)","26801133":"# For propresp, we will plot density.\ndensity_plot(features_list[3],'respond',df,1)","e32a76ee":"# For mailsyear, we will plot density.\ndensity_plot(features_list[4],'respond',df,1)","9e067d2b":"# For giftlast, we will plot density.\ndensity_plot(features_list[5],'respond',df,1)","4e793389":"df_giftlas = df[df['giftlast']<=df['giftlast'].quantile(q=0.99)]\ndensity_plot(features_list[5],'respond',df_giftlas,1)","735099db":"# For avggift, we will plot density.\ndensity_plot(features_list[6],'respond',df,1)","9f7d3303":"df_avggift = df[df['avggift']<=df['giftlast'].quantile(q=0.99)]\ndensity_plot(features_list[5],'respond',df_avggift,1)","f773923d":"#  X is  with all avalable features\nX = df[['gift','resplast','weekslast','propresp','mailsyear','giftlast','avggift']]\n#  X1 is  with selected features \nX1 = df[['resplast','weekslast','propresp','avggift']]\ny = df['respond']","5e58f6df":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2,random_state = 999)\nX_train1, X_test1, y_train1, y_test1 = model_selection.train_test_split(X1, y, test_size=0.2,random_state = 999)","07848e79":"tree_classifer =  DecisionTreeClassifier(criterion= \"entropy\", max_depth=None)\nclf = tree_classifer.fit(X_train1,y_train1)\ny_pred = clf.predict(X_test1)\ny_score = clf.score(X_test1,y_test1)\nprint(\"Accuracy is {0:.4g}%\".format(y_score*100))\nprint(\"MAE is {0:.4g}\".format(mean_absolute_error(y_test1, y_pred)))\nprint(\"MSE is {0:.4g}\".format(mean_squared_error(y_test1, y_pred)))\nprint(\"RMSE is {0:.4g}\".format(mean_squared_error(y_test1, y_pred)**(1\/2)))","045f55e7":"report = metrics.classification_report(\n    y_test1, tree_classifer.predict(X_test1),\n    target_names=[\"Not Donate\", \"Donate\"]\n)\nprint(report)","8cf3b4fb":"logistic_model = linear_model.LogisticRegression(solver=\"lbfgs\")\nlogistic_model.fit(X_train1, y_train1)","0209c410":"report = metrics.classification_report(\n    y_test1, logistic_model.predict(X_test1),\n    target_names=[\"Not Donate\", \"Donate\"]\n)\nprint(report)","7f9073c0":"X2_avggift = df_avggift[['resplast','weekslast','propresp','avggift']]\ny_avggift = df_avggift['respond']\nX_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(X2_avggift, y_avggift, test_size=0.2,random_state = 999)","b4dab18b":"logistic_model = linear_model.LogisticRegression(solver=\"lbfgs\")\nlogistic_model.fit(X_train2, y_train2)","611503b1":"report = metrics.classification_report(\n    y_test2, logistic_model.predict(X_test2),\n    target_names=[\"Not Donate\", \"Donate\"]\n)\nprint(report)","d6faa801":"# X3 is X but excluded \"gift\"\nX3  = df[['resplast','weekslast','propresp','mailsyear','giftlast','avggift']]\ny = df['respond']\nX_train3, X_test3, y_train3, y_test3 = model_selection.train_test_split(X3, y, test_size=0.2,random_state = 999)","db73c25c":"logistic_model = linear_model.LogisticRegression(solver=\"lbfgs\")\nlogistic_model.fit(X_train3, y_train3)","c3934dfc":"report = metrics.classification_report(\n    y_test3, logistic_model.predict(X_test3),\n    target_names=[\"Not Donate\", \"Donate\"]\n)\nprint(report)","27372138":"KNN_classifier = KNeighborsClassifier(n_neighbors=3)\nKNN_classifier.fit(X_train1, y_train1)","96808651":"report = metrics.classification_report(\n    y_test1, KNN_classifier.predict(X_test1),\n    target_names=[\"Not Donate\", \"Donate\"]\n)\nprint(report)","7bc4785d":"KNN_classifier = KNeighborsClassifier(n_neighbors=3)\nKNN_classifier.fit(X_train2, y_train2)","18f0e4a6":"report = metrics.classification_report(\n    y_test2, KNN_classifier.predict(X_test2),\n    target_names=[\"Not Donate\", \"Donate\"]\n)\nprint(report)","767f109d":"KNN_classifier = KNeighborsClassifier(n_neighbors=3)\nKNN_classifier.fit(X_train3, y_train3)","b8ba9e6b":"report = metrics.classification_report(\n    y_test3, KNN_classifier.predict(X_test3),\n    target_names=[\"Not Donate\", \"Donate\"]\n)\nprint(report)","0b5b0d92":"By comparison, number of weeks since last response (weekslast) seems to have quite similar impact to \"response\", since the distribution are look alike.","20499bdd":"## 1.3 Training with decision tree","7db1e192":"#### Model with all features except gift","7f00377c":"There is a slight improvement of KNN's accuracy with X2 at 64%.","8ee832f3":"### 1.5.1 Estimation to Logistics Regression,","1b0a9552":"#### Model with 4 features ['resplast','weekslast','propresp','avggift']","efc54bad":"According to the descriptive statistics (on above), giftlast has outlier, such that, our graph has a width range of the values","86691052":"#### Model with 4 features ['resplast','weekslast','propresp','avggift'] but excluded outliter in 'avggift'","238da2a3":"This is an example of the imbalance data set. Since \"No\" and \"yes\" are respectively, 40% and 60% there is a propensity towards \"No\" much more than \"Yes\".","4a7813ba":"# Datascience II: Final Project on Charity Data","c67c093f":"The KNN with all features except gift provided 64% accuracy.","5f29d304":"Before we preceed, should we load all necessary libraries and prepare data.","79ce3986":"#### Model with all features except gift","706fd30b":"Precision is TP\/(TP + FP), meaning if we say \"Y\" 100 times, how many times we say \"Y\" that actually \"Y\".\nRecall is TP\/(TP+FN), meaning, in all actual \"Y\" 100 observations (times), how many times we can say \"Y\" correctly.\n\nFrom Dicision tree, \"Not to donate\" has 69% precision and 73% recall. Since the recall is higher, it is proned to predict correctly if it is acctually \"Not to donate. Whereas, \"Donate\" has higher precision that recall at 55% and 50%, respectively. This model is doing better in predicting correctly for \"Donate\".\n\nThe recall will consider the cases of FN more, because it doesn't want the real values were missed out in prediction (Type II error minimisation). Opposingly, the precision wants its to be most precise (Type I error minimisation).\n\nIn addition, the decision tree is doing quite good job. Since \"Not Donate\" and \"Donate\"'s precision and recall are higher than 60% and 40%, respectively. Or it is better than predicting all \"Not Donate\" 100% and recieve 60% accuracy, and vice versa.","d2da8821":"#### Model with 4 features ['resplast','weekslast','propresp','avggift'] but excluded outliter in 'avggift'","eb8653f5":"We have dealt with outlier that far exceed 99%. By the quite similar distributions, the amount of most recent gift seems to have similar effect to donate and not to donate","f0a237e1":"The accuracy of KNN with X1 is about 63%, which is much lower than the logistics counterpart.","98e5a7f7":"## 1.4 To intepret precision and recall rates","3b5bae89":"We should, at least, drop n.a. data (if have) and observe descriptive statistics to understand how the data actually behaves.","509d8af5":"## 1.2 Plot the bar graph\/distribution to represent the response with gift.","853dd8a5":"The high response rate to mailings (propresp) seems to make individual to donate more.","5e46c5ce":"By comparison with Accuracy, MAE and MSE is equivalent to the the inverse of the accurary: 1 - 0.3618 = 0.6382. Thus, RMSE is the square root of MSE, it has no direct meaning.","ae35cc7c":"In conclusion, we estimated decision tree, logistics, and KNN with different data specification. We discovered that logistics yielded the highest accuracy about 70%. But depened on your business objectives, you might want to keetp eyes on precision or recall, much more just accuracy aspect. Furthermore, you can also try with various combinations of features and see.","eb431c12":"Based on the correlation plot, we can conclude into 2 points;\n    1.For a target variable, \"respond\" has slight correlation with other feature, except \"giftlant\" and \"avglast\".\n    2.For the features, while some of them have slight linear relationship, \"weeklast\" and \"resplast\" are seemed to have high negative relationship.\n    \nWe might suspect that, to train with only one feature, it may not yield satisfied amount. Or \"weeklast\" and \"resplast\" can lead to high autocovariance in the training model due to their high correlation, for examples.","12e8a3c2":"## 1.5 Repeating with KNN and Logistics regression","bb4974ab":"### 1.5.2 Estimation to KNN,","6251ed5e":"### Visualise feature 'resplast'","b2801768":"#### Model with 4 features ['resplast','weekslast','propresp','avggift']","98c4baf0":"With X3, there is an improvement for accuracy from 70% to 71%.","f48b621c":"### With Decision tree estimation","cc7622ec":"The 4 given features, 'resplast','weekslast','propresp','avggift', in X1 seem to have have variation on 'repsond', based on correlation in 1.1 and visualisation in 1.2 (except 'avggift' due to its low correlation and variation with 'respond')","c1b13c43":"The logistics with ['resplast','weekslast','propresp','avggift'] have quite decent accuracy at 70% compared to Decision tree at 64%.","c52fe554":"Similarly, we have dealt with outlier that far exceed 99%. By the pretty similar distributions, the average of past gifts seems to have similar effect to donate and not to donate","8ae2ce0b":"From the figure, those with gift = 0 will not donate.","7cd75722":"### Visualise feature 'avggift'","f765068d":"The longer number of mailings per year (mailsyear) may have some impact on donation.","b434913c":"This case, we tried with X2 that drop 99% percentile outlier from avggift. We see no improvement on accuracy but better prediction on \"Not Donate\".","7c468194":"Since we have not too many features, we can compare a pair of features to observe how each relates.","c43899f6":"### Visualise feature 'propresp'","73d004a9":"### Visualise feature 'gift'","7bf8512b":"### Visualise feature 'mailsyear'","e3f1e1b0":"## 0.Load Data","662a148c":"### Visualise feature 'giftlast'","fb2b8a89":"### Visualise feature 'weekslast'","18c47333":"If the individual responded with recent mail (replast = 1), they tended to donate more.","3286947b":"## 1.1 Plot pie chart for the fraction. And correlation matrix between variables.","8f976101":"According to the descriptive statistics (on above), avggift also has outlier."}}