{"cell_type":{"1b3ecff6":"code","e4cfec64":"code","b9998234":"code","537cecd9":"code","540de6f0":"code","90b5c1f6":"code","b08f0364":"code","506e13b3":"markdown","c58cb459":"markdown","c03a630e":"markdown","d468e0b3":"markdown","969069d4":"markdown","f64a1fcf":"markdown","3bd073b0":"markdown","c257eb93":"markdown","28d68319":"markdown","9214c004":"markdown","d0c3703e":"markdown","fb35043d":"markdown","37216b04":"markdown","d3dec22d":"markdown","57c57855":"markdown","95277682":"markdown","91ce7e4b":"markdown","0696fddd":"markdown","440cc2aa":"markdown","9698532c":"markdown","1ad37e07":"markdown","3c5e3344":"markdown"},"source":{"1b3ecff6":"from IPython.display import YouTubeVideo\n\ndef display_youtube_video(url, **kwargs):\n    \"\"\"\n    Displays a Youtube video in a Jupyter notebook.\n    \n    Args:\n        url (string): a link to a Youtube video.\n        **kwargs: further arguments for IPython.display.YouTubeVideo\n    \n    Returns:\n        YouTubeVideo: a video that is displayed in your notebook.\n    \"\"\"\n    \n    id_ = url.split(\"=\")[-1]\n    return YouTubeVideo(id_, **kwargs)","e4cfec64":"display(display_youtube_video(\"https:\/\/www.youtube.com\/watch?v=OyFJWRnt_AY\", width=800, height=600))","b9998234":"display(display_youtube_video(\"https:\/\/www.youtube.com\/watch?v=U0s0f995w14\", width=800, height=600))","537cecd9":"import torch\nimport torch.nn as nn\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(SelfAttention, self).__init__()\n        self.embed_size = embed_size  # K\n        self.heads = heads  # H\n        self.head_dim = embed_size \/\/ heads  # K\/H\n\n        assert (\n            self.head_dim * heads == embed_size\n        ), \"Embedding size needs to be divisible by heads\"\n\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)  # W: (K\/H, K\/H)\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)  # W: (K\/H, K\/H)\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)  # W: (K\/H, K\/H)\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)  # W: (H * K\/H, K)\n\n    def forward(self, values, keys, query, mask):\n        # B (b_size): 2\n        # T (seq_len): 9\n        # K (embed_size): 512\n        # H (heads): 8\n        \n        \n        # General Pass\n        # values: (B, T_v, K)\n        # keys: (B, T_k, K)\n        # query: (B, T_q, K)\n        # mask: varies\n        \n        \n        # Encoder Pass\n        # T_s (seq_len src): 9\n        # T_s == T_v == T_k == T_q\n        # values: (B, T_s, K)\n        # keys: (B, T_s, K)\n        # query: (B, T_s, K)\n        # mask: (B, 1, 1, T_s)\n        \n        \n        # Decoder 1st Pass\n        # T_t (seq_len trg): 8\n        # (T_t - 1) == T_v == T_k == T_q\n        # values: (B, T_t - 1, K)\n        # keys: (B, T_t - 1, K)\n        # query: (B, T_t - 1, K)\n        # mask: (B, 1, T_t - 1, T_t - 1)\n        \n        \n        # Decoder 2nd Pass\n        # T_s (seq_len src): 9\n        # T_t (seq_len trg): 8\n        # T_s == T_v == T_k\n        # (T_t - 1) == T_q\n        # values: (B, T_s, K)\n        # keys: (B, T_s, K)\n        # query: (B, T_t - 1, K)\n        # mask: (B, 1, 1, T_s)\n        \n        \n        N = query.shape[0]  # B\n\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]  # T_v, T_k, T_q\n        \n        \n        # Splitting the heads.\n        values = values.reshape(N, value_len, self.heads, self.head_dim)  # (B, T_v, H, K\/H)\n        keys = keys.reshape(N, key_len, self.heads, self.head_dim)  # (B, T_k, H, K\/H)\n        query = query.reshape(N, query_len, self.heads, self.head_dim)  # (B, T_q, H, K\/H)\n\n        # Linear transformation.\n        values = self.values(values)  # (B, T_v, H, K\/H)\n        keys = self.keys(keys)  # (B, T_k, H, K\/H)\n        queries = self.queries(query)  # (B, T_q, H, K\/H)\n\n        # MatMul querys and keys.\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # (B, H, T_q, T_k)\n\n        # Mask padded indices with infty so their weights become 0. \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))  # (B, H, T_q, T_k)\n\n        # Scale and apply softmax. \n        attention = torch.softmax(energy \/ (self.embed_size ** (1 \/ 2)), dim=3)  # (B, H, T_q, T_k)\n\n        # MatMul attentions and values.\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n            N, query_len, self.heads * self.head_dim\n        )  # (B, T_q, H * K\/H)\n\n        # Last linear layer.\n        out = self.fc_out(out)  # (B, T_q, K)\n\n        return out  # (B, T_q, K)\n\n    \n# ====================================================================== #\n# ====================================================================== #\n# ====================================================================== #\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_size, heads, dropout, forward_expansion):\n        super(TransformerBlock, self).__init__()\n        self.attention = SelfAttention(embed_size, heads)  # K, H\n        self.norm1 = nn.LayerNorm(embed_size)  # K\n        self.norm2 = nn.LayerNorm(embed_size)  # K\n\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_size, forward_expansion * embed_size),  # W: (K, F * K)\n            nn.ReLU(),\n            nn.Linear(forward_expansion * embed_size, embed_size),  # W: (F * K, K)\n        )\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, value, key, query, mask):\n        # B (b_size): 2\n        # T_v (seq_len values)\n        # T_k (seq_len keys)\n        # T_q (seq_len queries) \n        # K (embed_size): 512\n        # H (heads): 8\n        # F (forward_expansion): 4\n        \n        \n        # General Pass\n        # value: (B, T_v, K)\n        # key: (B, T_k, K)\n        # query: (B, T_q, K)\n        # mask: (B, 1, 1, T_s)\n        \n        \n        # Encoder Pass\n        # T_s (seq_len src): 9\n        # T_s == T_v == T_k == T_q\n        # value: (B, T_s, K)\n        # key: (B, T_s, K)\n        # query: (B, T_s, K)\n        # mask: (B, 1, 1, T_s)\n        \n        \n        # Decoder Pass\n        # T_s (seq_len src): 9\n        # T_t (seq_len trg): 8\n        # T_s == T_v == T_k\n        # (T_t - 1) == T_q\n        # value: (B, T_s, K)\n        # key: (B, T_s, K)\n        # query: (B, T_t - 1, K)\n        # mask: (B, 1, 1, T_s)\n        \n        \n        # Attention.\n        attention = self.attention(value, key, query, mask)  # (B, T_q, K)\n        \n        # Skip connection, layer normalization, dropout FFN block.\n        x = self.dropout(self.norm1(attention + query))  # (B, T_q, K)\n        forward = self.feed_forward(x)  # (B, T_q, K)\n        out = self.dropout(self.norm2(forward + x))  # (B, T_q, K)\n        return out  # (B, T_q, K)\n\n    \n# ====================================================================== #\n# ====================================================================== #\n# ====================================================================== #\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        src_vocab_size,\n        embed_size,\n        num_layers,\n        heads,\n        device,\n        forward_expansion,\n        dropout,\n        max_length,\n    ):\n\n        super(Encoder, self).__init__()\n        self.embed_size = embed_size  # K\n        self.device = device\n        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # W: (S, K)\n        self.position_embedding = nn.Embedding(max_length, embed_size)  # W: (M, K)\n\n        self.layers = nn.ModuleList(\n            [\n                TransformerBlock(\n                    embed_size,\n                    heads,\n                    dropout=dropout,\n                    forward_expansion=forward_expansion,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        # B (b_size): 2\n        # T_s (seq_len src): 9\n        # K (embed_size): 512\n        # H (heads): 8\n        # S (src_vocab_size): 10\n        # M (max_length): 100\n        \n        \n        # x: (B, T_s)\n        # mask: (B, 1, 1, T_s)\n        \n        \n        N, seq_length = x.shape  # B, T_s\n        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)  # (B, T_s)\n                \n        # Compute word & position embeddings and apply dropout.\n        we = self.word_embedding(x)  # (B, T_s, K)\n        pe = self.position_embedding(positions)  # (B, T_s, K)\n            \n        out = self.dropout(\n            (we + pe)\n        )  # (B, T_s, K)\n\n        # N Transformer blocks.\n        for idx, layer in enumerate(self.layers):\n            out = layer(out, out, out, mask)  # (B, T_s, K)\n            \n        return out  # (B, T_s, K)\n\n    \n# ====================================================================== #\n# ====================================================================== #\n# ====================================================================== #\n    \n    \nclass DecoderBlock(nn.Module):\n    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n        super(DecoderBlock, self).__init__()\n        self.norm = nn.LayerNorm(embed_size)  # K\n        self.attention = SelfAttention(embed_size, heads=heads)  # K, H\n        self.transformer_block = TransformerBlock(\n            embed_size, heads, dropout, forward_expansion\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, value, key, src_mask, trg_mask):\n        # B (b_size): 2\n        # T_s (seq_len src): 9\n        # T_t (seq_len trg): 8\n        # K (embed_size): 512\n        # H (heads): 8\n        \n        \n        # x: (B, T_t - 1, K)\n        # value: (B, T_s, K)\n        # key: (B, T_s, K)\n        # src_mask: (B, 1, 1, T_s)\n        # trg_mask: (B, 1, T_t - 1, T_t - 1)\n        \n        \n        attention = self.attention(x, x, x, trg_mask)  # (B, T_t - 1, K)\n        query = self.dropout(self.norm(attention + x))  # (B, T_t - 1, K)\n        out = self.transformer_block(value, key, query, src_mask)  # (B, T_t - 1, K)\n        return out  # (B, T_t - 1, K)\n\n\n# ====================================================================== #\n# ====================================================================== #\n# ====================================================================== #\n    \n    \nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        trg_vocab_size,\n        embed_size,\n        num_layers,\n        heads,\n        forward_expansion,\n        dropout,\n        device,\n        max_length,\n    ):\n        super(Decoder, self).__init__()\n        self.device = device\n        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n        self.position_embedding = nn.Embedding(max_length, embed_size)\n\n        self.layers = nn.ModuleList(\n            [\n                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n                for _ in range(num_layers)\n            ]\n        )\n        self.fc_out = nn.Linear(embed_size, trg_vocab_size)  # W: (K, Z)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_out, src_mask, trg_mask):\n        # B (b_size): 2\n        # T_s (seq_len src): 9\n        # T_t (seq_len trg): 8\n        # K (embed_size): 512\n        # H (heads): 8\n        # Z (trg_vocab_size): 10\n        \n        \n        # x: (B, T_t - 1)\n        # enc_out: (B, T_s, K)\n        # src_mask: (B, 1, 1, T_s)\n        # trg_mask: (B, 1, T_t - 1, T_t - 1)\n        \n        \n        N, seq_length = x.shape  # B, T_t - 1\n        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)  # (B, T_t - 1)\n        \n        we = self.word_embedding(x)  # (B, T_t - 1, K)\n        pe = self.position_embedding(positions)  # (B, T_t - 1, K)\n        \n        x = self.dropout(\n            (we + pe)\n        )  # (B, T_t - 1, K)\n\n        for layer in self.layers:\n            x = layer(x, enc_out, enc_out, src_mask, trg_mask)  # (B, T_t - 1, K)\n\n        out = self.fc_out(x)  # (B, T_t - 1, Z)\n\n        return out  # (B, T_t - 1, Z)\n\n    \n# ====================================================================== #\n# ====================================================================== #\n# ====================================================================== #\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        src_vocab_size,\n        trg_vocab_size,\n        src_pad_idx,\n        trg_pad_idx,\n        embed_size=512,\n        num_layers=6,\n        forward_expansion=4,\n        heads=8,\n        dropout=0,\n        device=\"cpu\",\n        max_length=100,\n    ):\n\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(\n            src_vocab_size,\n            embed_size,\n            num_layers,\n            heads,\n            device,\n            forward_expansion,\n            dropout,\n            max_length,\n        )\n\n        self.decoder = Decoder(\n            trg_vocab_size,\n            embed_size,\n            num_layers,\n            heads,\n            forward_expansion,\n            dropout,\n            device,\n            max_length,\n        )\n\n        self.src_pad_idx = src_pad_idx\n        self.trg_pad_idx = trg_pad_idx\n        self.device = device\n\n    def make_src_mask(self, src):\n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n        return src_mask.to(self.device)\n\n    def make_trg_mask(self, trg):\n        N, trg_len = trg.shape\n        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n            N, 1, trg_len, trg_len\n        )\n\n        return trg_mask.to(self.device)\n\n    def forward(self, src, trg):\n        # B (b_size): 2\n        # T_s (seq_len src): 9\n        # T_t (seq_len trg): 8\n        # K (embed_size): 512\n        # Z (trg_vocab_size): 10\n        \n        \n        # src: (B, T_s)\n        # trg: (B, T_t - 1)\n        \n        \n        src_mask = self.make_src_mask(src)  # (B, 1, 1, T_s)\n                \n        trg_mask = self.make_trg_mask(trg)  # (B, 1, T_t - 1, T_t - 1)\n                \n        enc_src = self.encoder(src, src_mask)  # (B, T_s, K)\n                \n        out = self.decoder(trg, enc_src, src_mask, trg_mask)  # (B, T_t - 1, Z)\n        return out  # (B, T_t - 1, Z)\n\n    \n    \n    \n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if not torch.cuda.is_available() else \"cpu\")\n    print(device)\n    \n    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n    \n    src_pad_idx = 0\n    trg_pad_idx = 0\n    src_vocab_size = 10\n    trg_vocab_size = 10\n    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, \n                        num_layers=1,\n                        device=device).to(device)\n        \n    out = model(x, trg[:, :-1])\n    print(out.shape)","540de6f0":"display(display_youtube_video(\"https:\/\/www.youtube.com\/watch?v=dichIcUZfOw\", width=800, height=600))","90b5c1f6":"display(display_youtube_video(\"https:\/\/www.youtube.com\/watch?v=mMa2PmYJlCo\", width=800, height=600))","b08f0364":"display(display_youtube_video(\"https:\/\/www.youtube.com\/watch?v=gJ9kaJsE78k\", width=800, height=600))","506e13b3":"This is a simple utility function for embedding YouTube videos into a Jupyter Notebook. \n\n\n\ud83d\udccd **Note**: The only parameters you need to worry about are:\n\n- *the URL* \n- *width of the embed*\n- *height of the embed*","c58cb459":"1. Why is the position embedding using a max length of 100? Shouldn't it be 9? The sequences can vary.\n2. What is the purpose of the FFN in the TransformerBlock? For preserving gradients. The Linear layers in the TramsformerBlock and the linear layers elsewhere are used to compute word pair features.\n3. Why is dropout applied after the positional encoding, after the 1st and 2nd Add & Norm? This is actually specified in the paper and it is a regularization technique.\n4. Why does the FFN in the TransformerBlock not have a 2nd ReLU? I presume that this is simply to apply a simple *linear* transformation (exclude nonlinearities). The FFN has 1 ReLU to zero out the negative values.\n5. Why does the Attention block have linear transformations at the beginning? Same as question 4.\n6. Why is the masking there in the Attention block? The masking is there to prevent the decoder from cheating, essentially looking ahead in the sentence sequence.\n7. Why is there a linear layer at the end of the attention block? Same as question 4.","c03a630e":"<img src=\"attachment:9aae368d-4d73-4dea-96a5-d064c8827467.jpg\" width=\"1000px\" height=\"1000px\">","d468e0b3":"<h1><center><font color=\"#fcc63c\">\u26a1<\/font> Attention Is All You Need <font color=\"#fcc63c\">\u26a1<\/font><\/center><\/h1>","969069d4":"### 2.2. Aladdin Persson's From-Scratch Implementation [[2]](https:\/\/www.youtube.com\/watch?v=U0s0f995w14) \ud83d\udcbb","f64a1fcf":"## 1. Utility Function <font color=\"#b6b6b6\">\ud83d\udd27<\/font>","3bd073b0":"### 2.4. Hedu - Math of Intelligence Transformer Series [[4]](https:\/\/www.youtube.com\/watch?v=dichIcUZfOw) [[5]](https:\/\/www.youtube.com\/watch?v=mMa2PmYJlCo) [[6]](https:\/\/www.youtube.com\/watch?v=gJ9kaJsE78k) \ud83d\udcf9","c257eb93":"This is an experimental notebook and I'm simply using it to document one part of my learning journey! I hope you found this useful. If you have any corrections or suggestions, feel free to comment! Much appreciated and thank you! \ud83e\udd73","28d68319":"### 2.1. Pascal Poupart's Lecture [[1]](https:\/\/www.youtube.com\/watch?v=OyFJWRnt_AY) \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb","9214c004":"> This is the implementation in Aladdin Persson's video with my personal annotations. My personal annotations, in comments, simply detail what each snippet of code does and the shapes of the matrices involved in the Transformer architecture. \n\n\ud83d\udccc Some interesting notes:\n* Self-Attention here is abstracted and encapsulated away in its own class so that it can be used wherever you want. \n* The TransformerBlock class is the block (in the white container) in the encoder. \n* The Encoder class is simply a wrapper for the TransformerBlock, adding positional encoding and a word embedding.\n* The DecoderBlock class is yet another wrapper for the TransformerBlock except this models the architecture located in the decoder's white container.\n* The Decoder class is the entire decoder side of the Transformer Architecture. It includes the position encoding, word embedding, and the linear layer at the end of the Transformer architecture.\n* The Transformer class essentially combines everything (both the Encoder and Decoder classes). ","d0c3703e":"> Peter Bloem's article on Transformer's is thorough with a few applications and further research directions appended to the end of it. It walks you through both the intuition and the math (with a PyTorch implementation) of self-attention and the Transformer architecture. It is also the reference for Aladdin Persson's from-scratch implementation.","fb35043d":"#### 2.2.1. My Annotated Version of the Implementation","37216b04":"> Professor Poupart's lecture focuses heavily on the query, key, value mechanism of the attention block within the Transformer architecture. I found the hand drawn diagrams paired with the slideshow to be mostly effective in describing the gist of how the attention mechanism *operates* and how that is *quantified*. \n\n\ud83d\udccc Some lessons I took away:\n- There are many different ways at computing the similarity between the query and key matrix; one of those ways which proves to be computationally efficient and robust is a simple **scaled dot product**.\n- BERT implements a **bidirectionality** mechanism into the transformer, further boosting its performance. \n- After the work with Transformers, many subsequent papers have manipulated this architecture to sometimes include only the encoder, decoder, or a mixture of both. ","d3dec22d":"> This YouTube Channel has a *great* high-level overview tutorial on Transformers. This was where I got my foot in the door with Transformers. It walks you through the computations being performed at the matrix and vector level instead of simply showing something like \"Self-Attention block\". This aspect of this mini series proved to be *extremely* useful in getting an initial grasp on what the computation behind the architecture, attention mechanisms, or FFNs in the network might be doing.","57c57855":"\ud83d\udccd **Note**: The following non-exhaustive list of resources are a mix of research-centric and conceptual approaches, and implementation-centric approaches. Some of the resources approach the paper from a more conceptual standpoint, explaining the intuition and how the math and computation aligns with it. Some other resources may be a bit more code-based, focusing more on the implementation and modularization of it. ","95277682":"## 2. Resources <font color=\"#3a96dd\">\ud83d\udcd8<\/font>","91ce7e4b":"## 3. Questions <font color=\"#ef3f32\">\u2753<\/font>","0696fddd":"## 0. Introduction\n\n&emsp; This is a jupyter notebook detailing some resources (with an implementation!) of the [Attention Is All You Need](https:\/\/papers.nips.cc\/paper\/2017\/file\/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper. This is also my first ever notebook where I'll be experimenting with *upgrading* my kernel-ing skills! I'll try to interleave this resource notebook with *embedded visuals*, and some other cool tricks.\n\n-----------------------------------------------------------------------------------------------------------\n<div class=\"alert alert-block alert-info\">\nTable of Contents: <br>\n\n0. **Introduction** <br>\n1. **Utility Function** <br>\n2. **Resources** <br>\n&emsp;    2.1. *Pascal Poupart's Lecture* <br>\n&emsp;    2.2. *Aladdin Persson's From-Scratch Implementation* <br>\n&emsp;&emsp;&emsp;&emsp; 2.2.1. *My Annotated Version of the Implementation* <br>\n&emsp;    2.3. *Peter Bloem's Article* <br>\n&emsp;    2.4. *Hedu - Math of Intelligence Transformer Series* <br>\n3. **Questions**\n4. **References**\n<\/div>","440cc2aa":"### 2.3. Peter Bloem's Article [[3]](http:\/\/peterbloem.nl\/blog\/transformers) \ud83d\udcf0","9698532c":"> Aladdin Persson's from-scratch implementatation briefly covers the figures in the research paper. The implementation is close to the the paper however they do not use the sinusoid equations to encode positions. Instead, they use a learned embedding. The embeddings between the encoder and decoder are also not shared. Overall, I found this guide to be extremely helpful in understanding how the Transformer architecture would be put in practice and how it might be done in an object-oriented, reusable way.\n\n\ud83d\udccc Some lessons I took away:\n* Positional encoding can be done through learned positional embeddings, sinusoidal formulas, and other methods. \n* The Feed Forward Network, FFN, has a forward expansion that quadruples d_model or the number of embedding dimensions (4 x 512) and then back to d_model (512) and is both located in the encoder and decoder.","1ad37e07":"## 4. References \ud83d\udcdd\n\n\\[1\\] [Pascal Poupart's Lecture](https:\/\/www.youtube.com\/watch?v=OyFJWRnt_AY) <br>\n\\[2\\] [Aladdin Persson's From-Scratch Implementation](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/transformer_from_scratch\/transformer_from_scratch.py) <br>\n\\[3\\] [Peter Bloem's Article](http:\/\/peterbloem.nl\/blog\/transformers) <br>\n\\[4\\] [Hedu - Math of Intelligence Transformer Series Part 1](https:\/\/www.youtube.com\/watch?v=dichIcUZfOw) <br>\n\\[5\\] [Hedu - Math of Intelligence Transformer Series Part 2](https:\/\/www.youtube.com\/watch?v=mMa2PmYJlCo) <br>\n\\[6\\] [Hedu - Math of Intelligence Transformer Series Part 3](https:\/\/www.youtube.com\/watch?v=gJ9kaJsE78k) <br>\n\n\nOther References\n* [An Inspiring and Beautiful Notebook by Andrada Olteanu](https:\/\/www.kaggle.com\/vincenttu\/birdcall-recognition-eda-and-audio-fe)\n* [A Great article on markdown formatting in Jupyter](https:\/\/medium.com\/analytics-vidhya\/the-jupyter-notebook-formatting-guide-873ab39f765e)\n* [Embedding Youtube Videos into Jupyter](https:\/\/gist.github.com\/christopherlovell\/e3e70880c0b0ad666e7b5fe311320a62)\n* [How to add a certain number of spaces](https:\/\/stackoverflow.com\/questions\/40023013\/tab-space-in-markdown)\n* [How to add a spoiler for code blocks](https:\/\/stackoverflow.com\/questions\/32181339\/code-block-in-spoiler-with-markdown)\n* [How to add comments](https:\/\/stackoverflow.com\/questions\/4823468\/comments-in-markdown)\n* [Getting Emojis](https:\/\/emojipedia.org)","3c5e3344":"# OPEN TO SUGGESTIONS AND CORRECTIONS!"}}