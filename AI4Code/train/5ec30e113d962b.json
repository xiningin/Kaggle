{"cell_type":{"34c70c20":"code","da5b330c":"code","1918fa7f":"code","0ea6ced4":"code","745c298e":"code","c4e197bb":"code","a9f7f9d1":"code","745eda15":"code","834e7476":"code","d28cb7dc":"code","8f5f8076":"code","9c123cfc":"code","4baa95e4":"code","d9a897da":"code","d3d560a0":"code","9954e9a3":"code","cb084fd5":"code","b610f00a":"code","01b4e269":"code","b671e57b":"code","badf09e2":"code","2ccec2b4":"code","d8904337":"markdown","a099fa7c":"markdown","a7661a7a":"markdown","073eb0bf":"markdown","8a1acc74":"markdown","4795c0be":"markdown","45a5d29e":"markdown","df05a0f1":"markdown","8acea61f":"markdown","47190f48":"markdown","9f9f945d":"markdown","2ee23249":"markdown","aaedaa7c":"markdown","093bdafd":"markdown","f26301cb":"markdown","a783ab43":"markdown","44fd7ca1":"markdown","4d8b3112":"markdown","5a4421cf":"markdown","d4324c1d":"markdown","a855aac5":"markdown","38556711":"markdown","45cdc956":"markdown","6aef8d01":"markdown","db025b91":"markdown"},"source":{"34c70c20":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\") ## Telling Python to do not show any warnings for clean throughput","da5b330c":"## Loading csv data to Pandas DataFrame\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","1918fa7f":"df.head(5)","0ea6ced4":"df.info()","745c298e":"df['age'].describe()","c4e197bb":"df['sex'].value_counts()","a9f7f9d1":"df['target'].value_counts()\n\n## 1 --> Defective heart\n## 0 --> Healthy heart","745eda15":"# Assign target column to the variable tar\ntar = df[\"target\"] \n\n# Assign nummerical columns to the variable num (all columns in data DF - target column)\nfeature = list(set(df.columns)-set(tar))","834e7476":"plt.figure(figsize=(25,25))\nfor i in range(0,len(feature)):\n    plt.subplot(4, 4, i+1)\n    sns.boxplot(y = df[feature[i]], x = df['target'], data=df)","d28cb7dc":"# This is a neat and easiest way to visualize the correlation between the variables\n\ncorr_matrix = df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, cmap=\"PiYG\", annot=True, square=False, fmt=\".2g\")\nplt.title(\"Data: Correlations between Variables\")","8f5f8076":"X = df.drop(columns = 'target', axis = 1)\ny = df['target']","9c123cfc":"# Stratify: So that the target class will be evenly distributed to train & test set i.e. not all 0 will be assigned to train\/test and vice versa\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)","4baa95e4":"# Let's check the row number of our split sets compared to the original one\n\n(X.shape, X_train.shape, X_test.shape)","d9a897da":"## Fit & Transform Standard scaler on X_train & X_test\n\nstd = StandardScaler()\n\nX_train_sc = std.fit(X_train).transform(X_train)\nX_test_sc = std.transform(X_test)","d3d560a0":"param_grid_knn = {'n_neighbors': np.arange(1, 20),\n              'p': [1,2],\n              'weights': ['uniform','distance']}\n\ngrid_knn = GridSearchCV(KNeighborsClassifier(), \n                    param_grid_knn, scoring='roc_auc',\n                    cv=5)\n\ngrid_knn.fit(X_train, y_train)\n\nprint(\"KNN Best Parameters: \", grid_knn.best_params_)\n\nmodel_knn = grid_knn.best_estimator_\nprint(\"KNN Best Score: \", grid_knn.best_score_)\n\nmodel_knn.fit(X_train, y_train)\ny_pred_knn = model_knn.predict(X_test)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_knn))\nconfusion_matrix(y_test, y_pred_knn)","9954e9a3":"param_grid_lr = {'C': [0.001,0.01,0.1,1,10,100],\n             'penalty': ['none','l2'],\n             'fit_intercept':[True,False],\n                'solver':['newton-cg','lbfgs','sag','saga']}\n\ngrid_lr = GridSearchCV(LogisticRegression(), \n                    param_grid_lr, scoring='roc_auc',\n                    cv=5)\n\ngrid_lr.fit(X_train, y_train)\n\nprint(\"LR Best Parameters: \", grid_lr.best_params_)\n\nmodel_lr = grid_lr.best_estimator_\nprint(\"LR Best Score: \", grid_lr.best_score_)\n\nmodel_lr.fit(X_train, y_train)\ny_pred_lr = model_lr.predict(X_test)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_lr))\nconfusion_matrix(y_test, y_pred_lr)","cb084fd5":"param_grid_rf = {'max_depth': np.arange(1, 20),\n                'n_estimators':[1,10,20,50,100]}\n\ngrid_rf = GridSearchCV(RandomForestClassifier(), \n                    param_grid_rf, scoring='roc_auc',\n                    cv=5)\n\ngrid_rf.fit(X_train, y_train)\n\nprint(\"RF Best Parameters: \", grid_rf.best_params_)\n\nmodel_rf = grid_rf.best_estimator_\nprint(\"RF Best Score: \", grid_rf.best_score_)\n\nmodel_rf.fit(X_train,y_train)\ny_pred_rf = model_rf.predict(X_test)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_rf))\nconfusion_matrix(y_test, y_pred_rf)","b610f00a":"param_grid_mlp = {'hidden_layer_sizes': [(20),(20,20),(20,20,20)],\n    'activation': ['logistic', 'relu'],\n    'solver': ['adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive']}\n\ngrid_mlp = GridSearchCV(MLPClassifier(),\n                        param_grid_mlp,\n                        scoring='roc_auc',\n                        cv=5)\n\ngrid_mlp.fit(X_train, y_train)\n\nprint(\"MLP Best Parameters: \", grid_mlp.best_params_)\n\nmodel_mlp = grid_mlp.best_estimator_\nprint(\"MLP Best Score: \", grid_mlp.best_score_)\n\nmodel_mlp.fit(X_train,y_train)\ny_pred_mlp = model_mlp.predict(X_test)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_mlp))\nconfusion_matrix(y_test, y_pred_mlp)","01b4e269":"param_grid_knn_sc = {'n_neighbors': np.arange(1, 20),\n              'p': [1,2],\n              'weights': ['uniform','distance']}\n\ngrid_knn_sc = GridSearchCV(KNeighborsClassifier(), \n                    param_grid_knn_sc, scoring='roc_auc',\n                    cv=5)\n\ngrid_knn_sc.fit(X_train_sc, y_train)\n\nprint(\"KNN Best Parameters: \", grid_knn_sc.best_params_)\n\nmodel_knn_sc = grid_knn_sc.best_estimator_\nprint(\"KNN Best Score: \", grid_knn_sc.best_score_)\n\nmodel_knn_sc.fit(X_train_sc,y_train)\ny_pred_knn_sc = model_knn_sc.predict(X_test_sc)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_knn_sc))\nconfusion_matrix(y_test, y_pred_knn_sc)","b671e57b":"param_grid_lr_sc = {'C': [0.001,0.01,0.1,1,10,100],\n             'penalty': ['none','l2'],\n             'fit_intercept':[True,False],\n                'solver':['newton-cg','lbfgs','sag','saga']}\n\ngrid_lr_sc = GridSearchCV(LogisticRegression(), \n                    param_grid_lr_sc, scoring='roc_auc',\n                    cv=5)\n\ngrid_lr_sc.fit(X_train_sc, y_train)\n\nprint(\"LR Best Parameters: \", grid_lr_sc.best_params_)\n\nmodel_lr_sc = grid_lr_sc.best_estimator_\nprint(\"LR Best Score: \", grid_lr_sc.best_score_)\n\nmodel_lr_sc.fit(X_train_sc,y_train)\ny_pred_lr_sc = model_lr_sc.predict(X_test_sc)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_lr_sc))\nconfusion_matrix(y_test, y_pred_lr_sc)","badf09e2":"param_grid_rf_sc_sc = {'max_depth': np.arange(1, 20),\n                'n_estimators':[1,10,20,50,100]}\n\ngrid_rf_sc = GridSearchCV(RandomForestClassifier(), \n                    param_grid_rf_sc_sc, scoring='roc_auc',\n                    cv=5)\n\ngrid_rf_sc.fit(X_train_sc, y_train)\n\nprint(\"RF Best Parameters: \", grid_rf_sc.best_params_)\n\nmodel_rf_sc = grid_rf_sc.best_estimator_\nprint(\"RF Best Score: \", grid_rf_sc.best_score_)\n\nmodel_rf_sc.fit(X_train_sc,y_train)\ny_pred_rf_sc = model_rf_sc.predict(X_test_sc)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_rf_sc))\nconfusion_matrix(y_test, y_pred_rf_sc)","2ccec2b4":"param_grid_mlp_sc_sc = {'hidden_layer_sizes': [(20),(20,20),(20,20,20)],\n    'activation': ['logistic', 'relu'],\n    'solver': ['adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive']}\n\ngrid_mlp_sc = GridSearchCV(MLPClassifier(),\n                        param_grid_mlp_sc_sc,\n                        scoring='roc_auc',\n                        cv=5)\n\ngrid_mlp_sc.fit(X_train_sc, y_train)\n\nprint(\"MLP Best Parameters: \", grid_mlp_sc.best_params_)\n\nmodel_mlp_sc = grid_mlp_sc.best_estimator_\nprint(\"MLP Best Score: \", grid_mlp_sc.best_score_)\n\nmodel_mlp_sc.fit(X_train_sc,y_train)\ny_pred_mlp_sc = model_mlp_sc.predict(X_test_sc)\n\nprint('classification_report:\\n',classification_report(y_test, y_pred_mlp_sc))\nconfusion_matrix(y_test, y_pred_mlp_sc)","d8904337":"## Scale the X_train & X_test","a099fa7c":"## Finding early insights ","a7661a7a":"# Model Building: Without Scaler\n\nWe'll use 4 models: kNN, Logistic Regression, Random Forest Classifier, and MultiLayer Perceptron Classifier.\n\nWe'll also use Receiver Operating Characteristics-Area Under Curve (ROC_AUC) as our metric score as it measures **how well predictions** are ranked, rather than the absolute values.\n\nTo simplify the steps in searching for the best parameters for each models, we'll use **GridSearchCV** function to loop through predefined hyperparameters and fit the estimator on the training set.\n\nFor each of the model, we'll also print out the **Classification Report and Confusion Matrix** to see the models' Precision and Recall score on a bird's eye view. ","073eb0bf":"Whoa! Note the 75% of the score given by this kNN model","8a1acc74":"Note that since we do not have a highly correlated features, **we don't need to perform PCA** as a pre-processing steps in our model building to remove the highly-correlated variables.","4795c0be":"## Exploring The Variables","45a5d29e":"# Objective\n\nThe objective for this dataset is **to build a predictive model** that could best classify and predict if a patient would contract a heart disease based on associated health variables.\n\nWithout further ado, let's explore how we could accomplish the objective.","df05a0f1":"# Import Dependencies\/Libraries & Datasets\n\nFirst, we import our dependencies and libraries. I like to put all my libaries in one cell at the start so that I could detect any of the libraries that I'm missing. Of course, I could always go back and run the cell again after I add the necessary missing library.\n\nThen, we add our dataset and save it as a variable in Pandas Dataframe.","8acea61f":"### MultiLayer Perceptron","47190f48":"### Random Forest","9f9f945d":"Note that when we use the Scaled set, the models' scores are either enhanced or stay about the same. No harm done, right :)","2ee23249":"# Exploratory Data Analysis\n\nNow that we have imported our dataset, let's take a look at a few characteristics of the dataframe, as well as the the insights we could find on the features and target variables.","aaedaa7c":"We have **13 features** and **one target (0 and 1)** variables, with a total row of **303**. \n\nThere's also **no null values** in all of the columns, with all of the datapoints being integer or float datatypes. It's safe to say that we have a clean and pretty straightfoward dataset.","093bdafd":"### k-nearest neighbour (kNN)","f26301cb":"Hey! Looks like our kNN model's score **improves** when we use Scaled set.","a783ab43":"### Logistic Regression","44fd7ca1":"Given that the datapoints for Patients with No heart disease overlapped with those with a heart disease, we could say that **there is no significant difference** between the two class in all of our feature variables. ","4d8b3112":"### MultiLayer Perceptron","5a4421cf":"1. 50% of the patients are at least 55 years old\n2. We have quite an imbalance proportion of male and female patients, with male being about 2 times the number of female patients\n3. Since the distribution of our target variable is about the same AND more than 30 samples, we have a normal standard distribution and thus, we don't need to perform pre-processing steps for our model building","d4324c1d":"### k-nearest neighbours Classifier (kNN)","a855aac5":"# Conclusion\n\n**Best Model:** Random Forest without StandardScaler\n\n**Reason:**\n* We have an **excellent roc_auc score**. Our model is able to distinguish the patients with heart disease and those who don\u2019t above 90% of the time.\n\n* We have **a higher Precision & Recall score** amongst all our models, which means if we choose those scores as our metrics, Random Forest will generate a better score overall.\n\n* The **misclassified portions** (False Negative and False Positive) of datapoints in Random Forest without Scaling in the Classification Matrix is **lower than the others**. In regard to disease classification, we would want to try and avoid a misclassification as much as we could.\n\n* The model gives out the **highest True Positive and True Negative portions** amongst all the models. Back to our objective, this model would be the best to classify and predict patients with a heart disease and those that truly do not have a heart disease.","38556711":"# Model Training: With StandardScaler()","45cdc956":"### Random Forest","6aef8d01":"### Logistic Regression","db025b91":"# Splitting the Features & Target variables, Scale X and y\n\nNow, we split our features and targets into X and y for our model building process. To make it more interesting, I use StandardScaler() on X_train and X_test to remove the mean and scale each variable to their unit variance. \n\nBecause why not, right? Let's see if we could have a better model with a scaled sets."}}