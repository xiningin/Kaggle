{"cell_type":{"6756ac36":"code","1b3de1ad":"code","21e9e805":"code","a552c70d":"code","97bdf800":"code","ec44749b":"code","30a3c3c3":"code","55a2cff8":"code","843824b1":"code","854906ec":"code","86772db8":"code","1d1bd785":"code","6c8033fd":"code","18cb396e":"code","7ad70732":"code","0a7b0937":"code","efde8f8c":"code","4430f4ed":"code","9dd47db3":"code","3833ef22":"code","1a2fbb7e":"code","7ef8d928":"code","b75bf6ad":"code","7ce2f988":"code","c233cfec":"code","130ad6b4":"code","66a9c89c":"code","d2e06af9":"code","dc592d07":"code","703e2e97":"code","4e042b0d":"code","9a48ea50":"code","e4832260":"code","17982ed6":"code","ecbb588a":"code","d6551893":"code","7fd553a5":"code","0921955e":"code","c2d3fba4":"code","156ec37f":"code","6302a7a6":"code","1210073e":"code","f25a9fcf":"code","8ff91896":"code","1c80c72a":"code","026b2044":"code","93904c76":"code","3bca8b1c":"code","acaf0cf8":"code","6ccff5e2":"code","4977f3ce":"code","72996c93":"code","655c4ca3":"code","328715a6":"code","8f57da92":"code","eb0e6a70":"code","da6e39be":"code","f7ae76e7":"code","20699b37":"code","cf35f7d3":"code","4b124181":"code","02b5594b":"code","3e0ea9e1":"code","7ddb4709":"code","2fa87b7f":"code","394403ed":"code","5b8d0a60":"code","9e62dcdb":"code","c07029bd":"code","f447278b":"markdown","46c6c2f4":"markdown","d6997a84":"markdown","4951508f":"markdown","2602d08b":"markdown","204af26b":"markdown","23daffb9":"markdown","4255269d":"markdown","1f8e4e25":"markdown","6f37a742":"markdown","33cf3d59":"markdown","fbde9c3e":"markdown","e846eba9":"markdown","40fa53c5":"markdown","8af46810":"markdown","6758a93e":"markdown","e8d5daa0":"markdown","26b6a1a7":"markdown","0da4b0f8":"markdown","cca41114":"markdown","f328db36":"markdown","7934bf5d":"markdown","7d1c7726":"markdown","ae92e69e":"markdown","269d5e87":"markdown","dd00b95b":"markdown","1e82bbd8":"markdown","15957d79":"markdown","be60e9af":"markdown","7dbcc6ea":"markdown","827f4776":"markdown","2f7e05f8":"markdown","63db77aa":"markdown","92de40dc":"markdown","8b493bfd":"markdown","be3bd8e2":"markdown","1e0d7585":"markdown","22e67811":"markdown","59e78718":"markdown","6b932633":"markdown","5222aa64":"markdown","ec9cb04a":"markdown","358c2bec":"markdown","1898e73e":"markdown","798438d2":"markdown","0f43cfb0":"markdown","461795dc":"markdown","77a2b320":"markdown","2ba97b6c":"markdown","f949d510":"markdown","6c69228c":"markdown","c07d5ea3":"markdown","bf17d02f":"markdown","e1b2a63c":"markdown","9cd08b7b":"markdown","ae69c54d":"markdown","d19b1113":"markdown","6b5262ff":"markdown","afc62cb4":"markdown","bf9e86b3":"markdown","9730ab79":"markdown","cbc86461":"markdown","18a76bbc":"markdown","b2844ad3":"markdown","8224a579":"markdown","16f217f7":"markdown","3cb75436":"markdown","883f0807":"markdown","2b0883db":"markdown","4e96bc9b":"markdown","1bf574c1":"markdown","6852be37":"markdown","3cef0c4b":"markdown","b15b4c9a":"markdown","a6c71f20":"markdown","e9a95cb7":"markdown","1396286f":"markdown","d64cc87c":"markdown","e1ba3399":"markdown","4f5cce32":"markdown","7a07a0c4":"markdown","ca01b4d9":"markdown","61dacffe":"markdown","bf0cd7a2":"markdown","6a83a999":"markdown","af177895":"markdown","ce3e9c8c":"markdown","655c52ab":"markdown","69504c43":"markdown"},"source":{"6756ac36":"#Load Packages\nimport numpy as np # linear algebra\nimport matplotlib as mpl\nimport pandas as pd\nimport pandas_ml as pdml\nfrom pandas_ml import ConfusionMatrix\nimport seaborn as sns\nimport re \nfrom IPython.display import display_html\nimport itertools\nimport math\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import *\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom scipy import stats\nfrom statistics import variance, stdev, mode\nfrom scipy import interp\n#Load sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import precision_recall_curve, confusion_matrix, accuracy_score, hamming_loss\nfrom sklearn import linear_model\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold\n\n# Import Classifiers\nimport scikitplot as skplt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import  AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import classification_report, matthews_corrcoef, log_loss, hinge_loss, cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss\n#Learning curve\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict,train_test_split, cross_val_score, validation_curve\nfrom sklearn.preprocessing import StandardScaler,  LabelEncoder\nfrom IPython.display import display_html\nimport warnings\n\n\n# for inline plots\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nplt.rcParams[\"legend.fontsize\"] = 15\nplt.rcParams[\"axes.labelsize\"] = 15\nmpl.rc('xtick', labelsize = 15) \nmpl.rc('ytick', labelsize = 15)\nsns.set(style = 'whitegrid', palette = 'muted', font_scale = 2)\n    \nprint('Libraries Imported')","1b3de1ad":"# provide some statistics for numerics\ndef Stats(feature):\n    mean  = np.nanmean(X_train[feature])\n    median = np.nanmedian(X_train[feature])\n    mode_ = stats.mode(X_train[feature])\n    \n    variation = np.nanvar(X_train[feature])\n    stdv = np.nanstd(X_train[feature])\n    range_ = np.max(X_train[feature]) - np.min(X_train[feature])\n    Quantile = (X_train[feature])\n    \n    print('Stats of %s:'%(feature.upper()))\n    print('Mean: %.2f'%(mean))\n    print('Median: %.2f'%(median))\n    print('Mode: %.2f'%(mode_[0]))\n    print('Range: %.2f'%(range_))    \n    print('Variance: %.2f'%(variation))\n    print('Standard Deviation: %.2f'%(stdv))\n    print('Quantile:')\n    for val in [10, 25, 50, 75, 90, 100]:\n        perc = np.nanpercentile(X_train[feature],val)\n        print('\\t%s %%: %.2f'%(val, perc))  \n        \n        \n#sets up the parametes for plotting.. size and font\ndef PlotParams(Font, sizex, sizey):\n    mpl.rcParams['figure.figsize'] = (sizex,sizey)\n    plt.rcParams[\"legend.fontsize\"] = Font\n    plt.rcParams[\"axes.labelsize\"] = Font\n    mpl.rc('xtick', labelsize = Font) \n    mpl.rc('ytick', labelsize = Font)\n\n#sets up Seaborn parametes for plotting\ndef snsParams(font, colour_scheme):\n    #eaborn.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n    sns.set(style = 'whitegrid', palette = colour_scheme, font_scale = font)\n\n#determined ht emissing data\ndef Missing (X):\n    total = X.isnull().sum().sort_values(ascending = False)\n    percent = round(X.isnull().sum().sort_values(ascending = False)\/len(X)*100, 2)\n    missing = pd.concat([total, percent], axis = 1,keys= ['Total', 'Percent'])\n    return(missing) \n\n#plots number of dataframes side by side\ndef SideSide(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw = True)\n\n#makes heat map of correllations\ndef PlotCorr(X):\n    corr = X.corr()\n    #fig , ax = plt.figure( figsize = (6,6 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    sns.heatmap(\n        corr, cmap = cmap, square = True, cbar = False, cbar_kws = { 'shrink' : 1 }, \n     annot = True, annot_kws = { 'fontsize' : 14 }\n    )\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90) \n    \n#plot top correlatins in a heat map\ndef TopCorr(X, lim):\n    corr = X.corr()\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    #fig , ax = plt.subplots( figsize = (6,6 ) )\n    sns.heatmap(corr[(corr >= lim) | (corr <= -lim)], \n         vmax = 1.0,  cmap = cmap, vmin = -1.0, square = True, cbar = False, linewidths = 0.2, annot = True, \n                annot_kws = {\"size\": 14})\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90)","21e9e805":"# get data from csv files\ntest  = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')\n\n#determine sizes of datasets\nn_train, m_train = train.shape\nn_test, m_test = test.shape\n\n# divide into X and y data\nX_train = pd.DataFrame(train.iloc[:,1: m_train])\ny_train = pd.DataFrame(train.iloc[0:, 1])\nX_test_original = test\nX_test = test\n\nprint('Data Imported\\n\\n')","a552c70d":"print('FULL DATA')\nprint('Number of features (m): %.0f'%(m_train))\nprint('Number of traing samples (n): %.0f'%(n_train))\n\nprint('\\n\\nTest DATA')\nprint('Number of features (m): %.0f'%(m_test))\nprint('Number of traing samples (n): %.0f'%(n_test))\n\ncnt = 0\n# print out the features\nprint('\\n\\nFeatures: ')\nfor feature in X_train.columns:\n    cnt += 1\n    print('%d. '%(cnt), feature,'\\t\\t')\n","97bdf800":"# take a sample of what the data looks like\nX_train.head(20)","ec44749b":"# provide information about the types of data we are dealing with\nprint('ORIGINAL TRAINING DATA:\\n')\nX_train.info()\n\nprint('\\n\\n\\nORIGINGAL TEST DATA:\\n')\nX_test.info()\n\n#summarise the types of data\nprint('\\ndata types of features:')\n\ncnt = 0\nd_type = ['float64', 'int64','object','dtype']\nprint('\\n\\tTRAIN \\t\\t TEST')\nfor c1, c2 in zip(X_train.get_dtype_counts(), X_test.get_dtype_counts()):\n    cnt += 1\n    print(\"%s:\\t%-9s \\t%s\"%(d_type[cnt],c1, c2))\n    ","30a3c3c3":"#finds missing values\nmissing_train = Missing(X_train)\nmissing_test = Missing(X_test)\n    \nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(missing_train, missing_test)","55a2cff8":"X_train.describe(include = \"all\")","843824b1":"X_train.hist(figsize = (16,10),bins = 20)","854906ec":"sns.pairplot(X_train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked']], \n             hue = 'Survived', palette = 'muted',size = 2.2,\n             diag_kind = 'kde', dropna = True, diag_kws = dict(shade = True), plot_kws = dict(s=20) )\n","86772db8":"X_train.skew()\n\nskew_train = pd.DataFrame(X_train.skew())\nskew_test = pd.DataFrame(X_test.skew())\n    \nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(skew_train, skew_test)","1d1bd785":"#show the correlations between all the featured in a heatmap\nplt.figure(figsize = (20,6))\nPlotCorr(X_train);","6c8033fd":"# highest correlated with correlation of features with 'Survived'\nprint('Featured hights correlation with survival')\nprint('Feature\\tCorrelation')\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[1:9] # remove the 'Survived'\nSurvive_Corr= Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\nprint(Survive_Corr)","18cb396e":"\n# plot survival count for male and female\nplt.figure(figsize = (10,6))\nax = sns.barplot(x = np.arange(len(Survive_Corr)), y = np.array(Survive_Corr.values), palette = 'muted', orient= 'v');\nax.set_xlabel(\"Feature\",fontsize = 15)\nax.set_ylabel(\"Correlation Coefficient (with Survival)\",fontsize = 15)\nax.set_xticklabels(Survive_Corr.index)\n","7ad70732":"f,ax = plt.subplots(1,2,figsize =(18,8))\nX_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nplt.tight_layout()\nsns.countplot('Survived',data=X_train,ax=ax[1],palette=\"muted\")\nax[1].set_title('Survived')","0a7b0937":"snsParams(2, 'muted')\n# plot survival count for male and female\nplt.figure(figsize = (20,5))\nplt.subplot(1, 3, 1)\nax = sns.countplot(x = 'Survived',hue = 'Sex', data = X_train);\nax.set_xlabel(\"Survived\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 14)\n\n\n#survival probability of males and females\nplt.subplot(1, 3, 2)\nax = sns.barplot(x = \"Sex\", y = \"Survived\",data = X_train)\nax = ax.set_ylabel(\"Survival Probability\")\n\nplt.subplot(1, 3, 3)\nsns.violinplot(y = 'Survived', x = 'Sex', data = X_train, inner = 'quartile')\n","efde8f8c":"Stats('Age')\n\n# plot survival number for age dependandcy\nfig, axes = plt.subplots(figsize = (20,6), nrows = 1, ncols = 3)\n\nax = sns.distplot(X_train[X_train['Survived'] == 1].Age.dropna(), bins = 20, label = 'Survived')\nax = sns.distplot(X_train[X_train['Survived'] == 0].Age.dropna(), bins = 20, label = 'Not Survived')\n\nax = sns.kdeplot(X_train[\"Age\"][(X_train[\"Survived\"] == 0) & (X_train[\"Age\"].notnull())], color = \"Green\", shade = False)\nax = sns.kdeplot(X_train[\"Age\"][(X_train[\"Survived\"] == 1) & (X_train[\"Age\"].notnull())], ax = ax, color = \"Blue\", shade= False)\n\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Frequency\",fontsize = 15)\nax = ax.legend([\"Not Survived\",\"Survived\"],fontsize = 15)\nplt.xlim(0,80)\nplt.ylim(0,0.04)\nplt.grid(True)\n\nwomen = X_train[X_train['Sex'] == 'female']\nmen = X_train[X_train['Sex'] == 'male']\n\n#For womwn\nax = sns.distplot(women[women['Survived'] == 1].Age.dropna(), bins = 20, label = 'survived', ax = axes[0], kde = False)\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(), bins = 20, label = 'not survived', ax = axes[0], kde = False)\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 15)\nax.set_title('Female', fontsize = 15)\nax.set(xlim = (0, X_train['Age'].max()));\nax.set(ylim = (0, 50));\n    \n    \n#For men\nax = sns.distplot(men[men['Survived'] == 1].Age.dropna(), bins = 20, label = 'survived', ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(), bins = 20, label = 'not survived', ax = axes[1], kde = False)\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 15)\nax.set_title('Male', fontsize = 15)\nax.set(xlim = (0, X_train['Age'].max()))\nax.set(ylim = (0, 50));\n\n\n","4430f4ed":"\nplt.figure(figsize=(20,12))\nplt.subplot(2,3,1)\nsns.boxplot( y = \"Age\", x = \"Survived\",data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,2)\nsns.violinplot(\"Pclass\",\"Age\", hue = \"Survived\", data = X_train, split = True, palette = 'muted')\n\nplt.subplot(2,3,3)\nsns.violinplot(\"Sex\",\"Age\", hue = \"Survived\", data = X_train, split = True, palette = 'muted')\n\nplt.subplot(2,3,4)\nsns.boxplot(y = \"Age\", x = \"Sex\", data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,5)\nsns.boxplot(y = \"Age\", x = \"Sex\", hue = \"Pclass\", data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,6)\nsns.boxplot(y = \"Age\", x = \"Parch\", data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,4)\nsns.boxplot(y = \"Age\", x = \"SibSp\", data = X_train, palette = \"muted\")","9dd47db3":"plt.figure(figsize = (16,10))\nplt.subplot(2, 3, 1)\nsns.barplot(x = 'Pclass', y = 'Survived', data = X_train)\n\nplt.subplot(2, 3, 2)\nsns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=X_train);\n\nplt.subplot(2, 3, 3)\nsns.barplot(x = \"Pclass\", y = \"Survived\", hue = \"Sex\", data = X_train)\n\nplt.subplot(2, 3, 4)\nsns.countplot(x = 'Survived',hue = 'Pclass',data = X_train);\n\nplt.subplot(2, 3, 5)\nsns.violinplot(y = 'Survived', x = 'Pclass', data = X_train, inner = 'quartile')\nplt.subplot(2, 3, 6)\nsns.violinplot(x='Pclass', y = 'Age', hue = 'Survived', data = X_train, split = True)\n\n\nplt.subplots(figsize=(16,5))\nplt.subplot(131)\nsns.boxplot(x = \"Pclass\", y = \"Age\", hue = \"Sex\", data = X_train);\nplt.ylim(0,90)\n\nplt.subplot(132)\nsns.boxplot(y = \"Age\", x = \"Sex\", hue = \"Pclass\", data = X_train)\nplt.subplot(133)\nX_train.Age[X_train['Pclass'] == 1].plot(kind = 'kde')    \nX_train.Age[X_train['Pclass'] == 2].plot(kind = 'kde')\nX_train.Age[X_train['Pclass'] == 3].plot(kind = 'kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\", fontsize = 15)\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'), loc = 'best') ;\nplt.xlim(0,80)\nplt.ylim(0,0.04)","3833ef22":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(X_train, col = 'Survived', row = 'Pclass', size = 3, aspect = 3.2)\ngrid.map(plt.hist, 'Age', alpha = 0.8, bins=20)\ngrid.add_legend();","1a2fbb7e":"# Explore Embarked vs Survived \nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\nsns.barplot(x = \"Embarked\", y = \"Survived\",  data = X_train)\n\n# Explore Pclass vs Survived by Sex\nplt.subplot(1, 3, 2)\nsns.barplot(x = \"Embarked\", y = \"Survived\", hue = \"Sex\", data = X_train)\n#g = g.set_ylabels(\"survival probability\")\nplt.subplot(1, 3, 3)\nsns.countplot(x = 'Survived',hue = 'Embarked',data = X_train);","7ef8d928":"plt.figure(figsize = (15,5))\nsns.boxplot(y = \"Age\", x = \"Embarked\", hue = \"Pclass\", data = X_train)","b75bf6ad":"# Explore Pclass vs Embarked \nPlotParams(15, 8, 6)\nsnsParams(2,'muted')\n\ng = sns.factorplot(\"Pclass\", col = \"Embarked\",  data = X_train, size = 8, \n                   kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")\ng = sns.factorplot(\"Pclass\", col = \"Embarked\",  data = X_train,\n                   hue = \"Sex\", size = 8, kind = \"count\", palette = \"muted\")\n\ng = g.set_ylabels(\"Count\")\n","7ce2f988":"PlotParams(15, 10, 6)\nplt.figure(figsize = (16,5))\nplt.subplot(1, 2, 1)\nsns.barplot(x = \"Parch\", y = \"Survived\",  data = X_train, palette = \"muted\")\nplt.subplot(1, 2, 2)\nsns.barplot(x = \"SibSp\", y = \"Survived\",  data = X_train, palette = \"muted\")\n\nplt.figure(figsize=(20,5))\nplt.subplot(1, 2, 1)\nsns.violinplot(y = 'Survived', x = 'Parch', data = X_train, palette = \"muted\", inner = 'quartile')\nplt.subplot(1, 2, 2)\nsns.violinplot(y = 'Survived', x = 'SibSp', data = X_train, palette = \"muted\", inner = 'quartile')","c233cfec":"PlotParams(15, 8, 6)\n\nplt.figure(figsize = (20,6))\nplt.subplot(1,3,1)\nsns.kdeplot(X_train[\"Fare\"])\nplt.xlim(0,160)\nplt.ylim(0,.040)\nplt.xlabel('Fare')\nplt.ylabel('Survival Probability')\n\nplt.subplot(1,3,2)\nax = sns.distplot(X_train[X_train['Survived'] == 1].Fare.dropna(), bins = 80, label = 'Survived')\nax = sns.distplot(X_train[X_train['Survived'] == 0].Fare.dropna(), bins = 80, label = 'Not Survived')\nax = sns.kdeplot(X_train[\"Fare\"][(X_train[\"Survived\"] == 0) & (X_train[\"Fare\"].notnull())], color = \"Green\", shade = False)\nax = sns.kdeplot(X_train[\"Fare\"][(X_train[\"Survived\"] == 1) & (X_train[\"Fare\"].notnull())], ax = ax, color = \"Blue\", shade= False)\nax.set_xlabel(\"Fare\",fontsize = 15)\nax.set_ylabel(\"Frequency\",fontsize = 15)\nax = ax.legend([\"Not Survived\",\"Survived\"],fontsize = 15)\nplt.ylim(0,0.1)\nplt.xlim(0,160)\nplt.grid(True)\n\nplt.subplot(1,3,3)\nax1 = sns.boxplot(x = \"Embarked\", y = \"Fare\", hue = \"Pclass\", data = X_train);\nplt.ylim(0,200)","130ad6b4":"Stats('Fare')","66a9c89c":"# Fill empty values with NaN\nX_train = X_train.fillna(np.nan)\nX_test = X_test.fillna(np.nan)\n\n#finds missing values\nmissing_train = Missing(X_train)\nmissing_test = Missing(X_test)\n    \nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(missing_train, missing_test)\n\n#plot missing data in heatmap for visualisation\nprint('\\n\\n  MISSING TRAINING DATA \\t\\t\\t MISSING TEST DATA')\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\nplt.figure(figsize = (10,5));\nplt.subplot(1, 2, 1)\nsns.heatmap(X_train.isnull(), yticklabels = False, cbar = False, cmap = cmap)\nplt.subplot(1, 2, 2)\nsns.heatmap(X_test.isnull(), yticklabels = False, cbar = False,cmap = cmap);\n","d2e06af9":"#combine the tets and training data so that operations can be performed together\nfull_data = [X_train, X_test] ","dc592d07":"#fill in Embarked datta with S as it is the most common\nfor X in full_data:\n    X['Embarked'] = X['Embarked'].fillna(\"S\")","703e2e97":"# fill missing Fare with median fare for each Pclass\nfor X in full_data:\n    X[\"Fare\"].fillna(X.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n    X[\"Fare Group\"] = X[\"Fare\"]","4e042b0d":"PlotCorr(X_train[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]])\n\n#correlation of features with target variable\nAge_Corr = X_train.corr()[\"Age\"]\n#Age_Corr= Age_Corr[np.argsort(Age_Corr, axis = 0)[::-1]] #sort in descending order\nAge_Corr = Age_Corr[1:10] # remove the 'Survived'\nprint(Age_Corr)","9a48ea50":"from sklearn.ensemble import RandomForestRegressor\n\n#use random forest to predict age\ndef MissingAges(X, AGE_features):\n    \n    age_data = X[age_features]\n\n    known_ages = age_data[age_data['Age'].notnull()].as_matrix()\n    unknown_ages = age_data[age_data['Age'].isnull()].as_matrix()\n\n    # Create target and eigenvalues for known ages\n    target = known_ages[:, 0]\n    eigen_val = known_ages[:, 1:]\n\n    # apply random forest regressor\n    RFR_age = RandomForestRegressor(random_state = 0, n_estimators = 2000, n_jobs = -1)\n    RFR_age.fit(eigen_val, target)\n\n\n    return (RFR_age, unknown_ages)\n\n# age distribution BEFORE filling in missing values\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values')\naxis2.set_title('New Age values')\n# plot new Age Values\nX_train['Age'].hist(bins = 70, ax = axis1)\nplt.xlabel('Age')\nplt.ylabel('Counts')\nplt.xlim(0,80)\n\n#the features used to determine missing ages\nage_features = [\"Age\", \"SibSp\", \"Parch\", \"Pclass\"]\n\n# filling ing the training data\nRFR_age, unknown_ages_train = MissingAges(X_train, age_features)\nAge_predictions_train = RFR_age.predict(unknown_ages_train[:, 1::])\nX_train.loc[(X_train['Age'].isnull()), \"Age\"] = Age_predictions_train\nX_train[\"Age\"] = X_train[\"Age\"].astype(int)\n\n# filling in the test data\n_, unknown_ages_test = MissingAges(X_test, age_features)\nAge_predictions_test = RFR_age.predict(unknown_ages_test[:, 1::])\nX_test.loc[(X_test['Age'].isnull()), \"Age\"] = Age_predictions_test\nX_test[\"Age\"] = X_test[\"Age\"].astype(int)\n\n\n# age distribution AFTER filling in missing values\nX_train['Age'].hist(bins = 70, ax = axis2)\nplt.xlabel('Age')\nplt.ylabel('Counts')\nplt.xlim(0,80)\nprint('Ages filled in')","e4832260":"# cabin Vrs no cabine survival rates\nfor X in full_data:\n    X[\"CabinBool\"] = (X[\"Cabin\"].notnull().astype('int'))\n    \n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x = \"CabinBool\", y = \"Survived\", data = X_train)\nplt.show()","17982ed6":"# Extract deck \ndef extract_cabin(x):\n    return x != x and 'Other' or x[0]\n\nfor X in full_data:\n    X['Cabin'] = X['Cabin'].apply(extract_cabin)\n    X['Deck'] = X['Cabin']\n\ntrain_deck = pd.DataFrame(X_train.groupby('Deck').size())\ntest_deck = pd.DataFrame(X_test.groupby('Deck').size())\n\nprint('TRAIN \\t\\t TEST')\nSideSide(train_deck,test_deck )","ecbb588a":"snsParams(1.2, 'muted')\nplt.figure(figsize = (16,5))\n\nplt.subplot(1, 3, 1)\ng = sns.countplot(X_train[\"Cabin\"], palette = \"muted\")\nplt.subplot(1, 3, 2)\ng = sns.barplot(x = \"Deck\", y = \"Survived\",  data = X_train, palette = \"muted\")\n\nplt.subplot(1, 3, 3)\nsns.countplot(x = 'Survived',hue = 'Deck',data = X_train, palette = \"muted\");\n\nsnsParams(2, 'muted')\nplt.figure(figsize = (16,5))\ng = sns.factorplot(\"Deck\", col = \"Pclass\",  data = X_train, size = 8, \n                   kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")\ng = sns.factorplot(\"Deck\", col = \"Embarked\",  data = X_train,\n                   hue = \"Sex\", size = 8, kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")","d6551893":"PlotParams(15, 8, 6)\n# determine size of family on board\nfor X in full_data:\n    X['Family Size'] = X['SibSp'] + X['Parch'] + 1 \n    X['Alone'] = [1 if i<2 else 0 for i in X['Family Size']]\n    X['Surname'] = X['Name'].str.extract('(\\w+),', expand = False)\n    X['Large Family'] = [1 if i > 5 else 0 for i in X['Family Size']]\n    \n    #X['First Name'] = X['Name'].str.extract('(Mr\\. |Miss\\. |Master. |Mrs\\.[A-Za-z ]*\\()([A-Za-z]*)',expand = False)[1]\n    \naxes = sns.factorplot('Family Size','Survived', hue = 'Sex', data = X_train, aspect = 2)\nplt.grid(True)\naxes = sns.factorplot('Family Size','Survived',  data = X_train, aspect = 2)\nplt.grid(True)\npd.crosstab(X_train['Family Size'], X_train['Survived']).plot(kind = 'bar', stacked = True)\n    \n\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(18,6))\nsns.barplot(x = \"Family Size\", y = \"Survived\", hue = \"Sex\", data = X_train, ax = axis1);\nsns.barplot(x = \"Alone\", y = \"Survived\", hue = \"Sex\", data = X_train, ax = axis2);\nsns.barplot(x = \"Alone\", y = \"Survived\", data = X_train)\nplt.show()\n   \n","7fd553a5":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor X in full_data:\n    X['Title'] = X['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping \"Rare\"\nfor X in full_data:\n    X['Title'] = X['Title'].replace(['Lady', 'Countess', 'Don', 'Sir', 'Jonkheer', 'Dona'], 'Noble')\n    X['Title'] = X['Title'].replace(['Capt', 'Col', 'Dr', 'Major', 'Rev'], 'Officer')\n    X['Title'] = X['Title'].replace('Mlle', 'Miss')\n    X['Title'] = X['Title'].replace('Ms', 'Miss')\n    X['Title'] = X['Title'].replace('Mme', 'Mrs')\n\n    \nprint('TRAIN TITLE \\t TEST TITLES')\ntrain_titles = pd.DataFrame(X_train.Title.value_counts())\ntest_titles = pd.DataFrame(X_test.Title.value_counts())\n\nSideSide(train_titles,test_titles)\n\nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\ng = sns.barplot(x = \"Title\", y = \"Survived\",  data = X_train)\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 2)\nsns.countplot(x = 'Survived', hue = 'Title',data = X_train);\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 3)\nsns.boxplot(data = X_train, x = \"Title\", y = \"Age\");\nplt.xticks(rotation = 90)\n\ntab = pd.crosstab(X_train['Title'], X_train['Pclass'])\ntab_prop = tab.div(tab.sum(1).astype(float), axis=0)\n\ntab_prop.plot(kind = \"bar\", stacked = True)\nplt.xticks(rotation = 90)","0921955e":"#sort the ages into logical categories\n## create bins for age\ndef AgeCategory(age):\n    a = ''\n    if age <= 3:\n        a = 'Baby'\n    elif age <= 12: \n        a = 'Child'\n    elif age <= 18:\n        a = 'Teenager'\n    elif age <= 35:\n        a = 'Young Adult'\n    elif age <= 65:\n        a = 'Adult'\n    elif age == 'NaN':\n        a = 'NaN'\n    else:\n        a = 'Senior'\n    return a\n        \nfor X in full_data:\n    X['Age Group'] = X['Age'].map(AgeCategory)\n    X['Age*Class'] = X['Age'] * X['Pclass']\n\nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\ng = sns.barplot(x = \"Age Group\", y = \"Survived\",  data = X_train)\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 2)\nsns.countplot(x = 'Survived', hue = 'Age Group',data = X_train)\n\nplt.subplot(1, 3, 3)\nsns.boxplot(data = X_train, x = \"Age Group\", y = \"Age\");\nplt.xticks(rotation = 90)","c2d3fba4":"def GetPerson(X):\n    age, sex = X\n    return 'child' if age < 16 else sex\n\nfor X in full_data:\n    X['Person'] = X[['Age','Sex']].apply(GetPerson, axis = 1)","156ec37f":"\n    \nfor X in full_data:\n    X.loc[ X['Fare Group'] <= 7.91, 'Fare Group'] = 0\n    X.loc[(X['Fare Group'] > 7.91) & (X['Fare Group'] <= 14.454), 'Fare Group'] = 1\n    X.loc[(X['Fare Group'] > 14.454) & (X['Fare Group'] <= 31), 'Fare Group']   = 2\n    X.loc[(X['Fare Group'] > 31) & (X['Fare Group'] <= 99), 'Fare Group']   = 3\n    X.loc[(X['Fare Group'] > 99) & (X['Fare Group'] <= 250), 'Fare Group']   = 4\n    X.loc[X['Fare Group'] > 250, 'Fare Group'] = 5\n    X['Fare Group'] = X['Fare Group'].astype(int)   ","6302a7a6":"#map each Sex value to a numerical value\nsex_map = {\"male\": 0, \"female\": 1}\nperson_map = {'child': 0, \"male\": 1, \"female\": 2}\nEmbark_map = {\"C\": 1,\"S\": 2, \"Q\": 3}\ndeck_map = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"T\": 8, \"Other\": 9}\nage_map = {\"Baby\": 1, \"Child\": 2, \"Teenager\": 3, \"Young Adult\": 4, \"Adult\": 5, \"Senior\": 6}\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Officer\": 5, \"Noble\": 5}\n\nfor X in full_data:\n    X[\"Sex\"] = X[\"Sex\"].map(sex_map)\n    X[\"Embarked\"] = X[\"Embarked\"].map(Embark_map)\n    X[\"Person\"] = X[\"Person\"].map(person_map)\n    X[\"Deck\"] = X[\"Deck\"].map(deck_map)\n    X[\"Age Group\"] = X[\"Age Group\"].map(age_map)\n    X[\"Title\"] = X[\"Title\"].map(title_mapping)","1210073e":"X_train = X_train.drop(\"Name\", axis = 1) \nX_test = X_test.drop(\"Name\", axis = 1) \nX_train = X_train.drop(\"Ticket\", axis = 1) \nX_test = X_test.drop(\"Ticket\", axis = 1) \nX_train = X_train.drop(\"Cabin\", axis = 1) \nX_test = X_test.drop(\"Cabin\", axis = 1) \nX_train = X_train.drop(\"Surname\", axis = 1) \nX_test = X_test.drop(\"Surname\", axis = 1) \n#X_train = X_train.drop(\"Age\", axis = 1) \n#X_test = X_test.drop(\"Age\", axis = 1) \nX_test = X_test.drop(\"PassengerId\", axis = 1) ","f25a9fcf":"X_train.head()","8ff91896":"# Feature Scaling\n\ndef Norm(X):\n    X1 = (X - np.mean(X)) \/ (np.max(X) - np.min(X))\n    return(X1)\n\nX_train['Fare'] = Norm(X_train['Fare'])\nX_train['Age'] = Norm(X_train['Age'])\nX_test['Fare'] = Norm(X_test['Fare'])\nX_test['Age'] = Norm(X_test['Age'])\n\n#X_train['Fare'] = X_train['Fare'].astype(int)\n#X_train['Age'] = X_train['Age'].astype(int)\n#X_test['Fare'] = X_test['Fare'].astype(int)\n#X_test['Age'] = X_test['Age'].astype(int)\n#X_train['Age*Class'] = X_train['Age*Class'].astype(int)\n#X_test['Age*Class'] = X_test['Age*Class'].astype(int)","1c80c72a":"X_train.head(10)","026b2044":"plt.figure(figsize = (20,12))\nPlotCorr(X_train);","93904c76":"plt.figure(figsize = (20,12))\nTopCorr(X_train, 0.25)","3bca8b1c":"# highest correlated with correlation of features with 'Survived'\nprint('Featured hights correlation with survival')\nprint('Feature\\tCorrelation')\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[1:20] # remove the 'Survived'\nSurvive_Corr= Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\nprint(Survive_Corr)\n\n\ncorrelations = X_train.corr() # determines parameters that are correlated to Survival\n# most correlated featues = features with correlation to Survival >0.1\ntop_correlations = correlations.index[abs(correlations[\"Survived\"]) > 0.1]\nplt.figure(figsize=(12,10))\nsns.set(font_scale = 1.5)\ng = sns.heatmap(X_train[top_correlations].corr(), annot = True, cmap = cmap, annot_kws={\"size\": 10})\nplt.title('Features most correlated with Survival (>0.1)')\nplt.yticks(rotation = 0)\nplt.xticks(rotation = 90)\n\nsnsParams(2, 'muted')\nplt.figure(figsize = (10,6))\nax = sns.barplot(x = np.arange(len(Survive_Corr)), y = np.array(Survive_Corr.values), \n                 palette = 'muted', orient= 'v');\nax.set_xlabel(\"Feature\",fontsize = 15)\nax.set_ylabel(\"Correlation Coefficient\",fontsize = 15)\nax.set_xticklabels(Survive_Corr.index)\nplt.xticks(rotation = 90)","acaf0cf8":"# can drop a few more features\nX_train = X_train.drop(\"Age\", axis = 1) \nX_test = X_test.drop(\"Age\", axis = 1) \nX_train = X_train.drop(\"SibSp\", axis = 1) \nX_test = X_test.drop(\"SibSp\", axis = 1) \nX_train = X_train.drop(\"Parch\", axis = 1)\nX_test = X_test.drop(\"Parch\", axis = 1)\nX_train = X_train.drop(\"Family Size\", axis = 1)\nX_test = X_test.drop(\"Family Size\", axis = 1)\nX_train = X_train.drop(\"Age Group\", axis = 1)\nX_test = X_test.drop(\"Age Group\", axis = 1)\n","6ccff5e2":"# final features\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[1:9] # remove the 'Survived'\nSurvive_Corr= Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\n\nsnsParams(2, 'muted')\nplt.figure(figsize = (10,6))\nax = sns.barplot(x = np.arange(len(Survive_Corr)), y = np.array(Survive_Corr.values), \n                 palette = 'muted', orient= 'v');\nax.set_xlabel(\"Feature\",fontsize = 15)\nax.set_ylabel(\"Correlation Coefficient\",fontsize = 15)\nax.set_xticklabels(Survive_Corr.index)\nplt.xticks(rotation = 90)\n\nX_train = X_train.drop(\"Survived\", axis = 1)","4977f3ce":"sns.pairplot(X_train)","72996c93":"print('TRAINING')\nprint(X_train.info())\nprint('\\n\\nTEST')\nprint(X_train.info())\n\nX_train.head(0)\nX_test.head(0)\n\ncnt = 0\nd_type = ['float64', 'int64','object','dtype']\nprint('\\n\\tTRAIN \\t\\t TEST')\nfor c1, c2 in zip(X_train.get_dtype_counts(), X_test.get_dtype_counts()):\n    cnt += 1\n    print(\"%s:\\t%-9s \\t%s\"%(d_type[cnt],c1, c2))\n    ","655c4ca3":"    classes = ['Dead','Survived']\n    cv = ShuffleSplit(n_splits = 100, test_size = 0.25, random_state = 0)\n    train_sizes = np.linspace(.1, 1.0, 10)","328715a6":"def GridSearcher(model, GridParams, X, y):\n    print('Performing Grid Search...')\n    kfold = StratifiedKFold(n_splits = 10)\n    \n    model_GS = GridSearchCV(estimator = model, param_grid = GridParams, cv = kfold, scoring = \"accuracy\", n_jobs = 2, verbose = 1)\n\n    model_GS.fit(X, y['Survived'])\n    \n    model_best = model_GS.best_estimator_\n    model_best_score = model_GS.best_score_\n    model_best_params = model_GS.best_params_\n    \n    print(\"\\nBest Estimatot:\", model_GS.best_estimator_,\n          \"\\nBest Score:\", model_GS.best_score_, # Mean cross-validated score of the best_estimator\n          \"\\nBest parameters:\", model_GS.best_params_)\n          \n    return (model_best, model_best_score, model_best_params)\n    \ndef Confuse(y, y_pred, classes):\n    cnf_matrix1 = confusion_matrix(y, y_pred)\n    \n    cnf_matrix = cnf_matrix1.astype('float') \/ cnf_matrix1.sum(axis = 1)[:, np.newaxis] *100\n    c_train = pd.DataFrame(cnf_matrix, index = classes, columns = classes)  \n    plt.subplot(2, 3, 3)\n    ax = sns.heatmap(c_train, annot = True, cmap = cmap, square = True, cbar = False, \n                          fmt = '.2f', annot_kws = {\"size\": 20})\n    plt.title('Confusion Matrix (%)')\n    \n    return(ax, cnf_matrix1)\n\ndef FitModel(model, X, y):\n    print('Fitting Model...')\n    model.fit(X, y)\n    y_pred  = model.predict(X)\n    CV_score = round(np.median(cross_val_score(model, X, y, cv = cv)), 4) * 100\n    \n    return (model, y_pred, CV_score)\n\ndef LearningCurve(X, y, model, cv, train_sizes, title):\n    print('Evaluating Learning Curve...')\n    train_sizes, train_scores, val_scores = learning_curve(model, X, y, cv = cv, n_jobs = 4, \n                                                            train_sizes = train_sizes)\n\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std   = np.std(train_scores, axis = 1)\n    val_scores_mean  = np.mean(val_scores, axis = 1)\n    val_scores_std   = np.std(val_scores, axis = 1)\n    \n    train_Error_mean = np.mean(1- train_scores, axis = 1)\n    train_Error_std  = np.std(1 - train_scores, axis = 1)\n    val_Error_mean  = np.mean(1 - val_scores, axis = 1)\n    val_Error_std   = np.std(1 - val_scores, axis = 1)\n\n    train_sc = train_scores_mean[-1] \n    val_sc = val_scores_mean[-1]\n    \n    train_sc_std = train_scores_std [-1]\n    val_sc_std = val_scores_std[-1]\n    \n    Learn_Results = [train_sc * 100, train_sc_std * 100, val_sc * 100, val_sc_std * 100]\n    \n    plt.figure(figsize = (20,15))\n    plt.subplot(2, 3, (1,2))\n    plt.fill_between(train_sizes, train_Error_mean - train_Error_std,\n                     train_Error_mean + train_Error_std, alpha = 0.1, color = \"r\")\n    plt.fill_between(train_sizes, val_Error_mean - val_Error_std, \n                     val_Error_mean + val_Error_std, alpha = 0.1, color = \"g\")\n    plt.plot(train_sizes, train_Error_mean, 'o-', color = \"r\",label = \"Training Error\")\n    plt.plot(train_sizes, val_Error_mean, 'o-', color = \"g\",label = \"Cross-validation Error\")\n    plt.xlabel('Training Examples (m)')\n    plt.title('Learning Curve %s'%(title))\n    plt.ylabel('Error')\n    plt.legend(loc = \"best\")\n    plt.grid(True)\n     \n    return (Learn_Results)\n    \ndef PlotPrecisionRecall(model, X, y):\n\n    # getting the probabilities of our predictions\n    y_scores = model.predict_proba(X) # probability estimates\n\n    \n    y_scores = y_scores[:,1]\n    \n    precision, recall, threshold = precision_recall_curve(y, y_scores)\n\n    plt.subplot(2,3,4)\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth = 2)\n    plt.plot(threshold, recall[:-1], \"b\", label = \"Recall\", linewidth = 2)\n    plt.xlabel(\"Threshold\", fontsize = 19)\n    plt.ylabel(\"Precision or Recall\", fontsize = 19)\n    plt.title(\"Precision & Recall\", fontsize = 19)\n    plt.legend(loc = \"best\", fontsize = 19)\n    plt.ylim([0, 1])\n\n    plt.subplot(2,3,5)\n    plt.plot(recall[:-1], precision[:-1], color = \"r\", linewidth = 2)\n    plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n    plt.fill_between(recall, precision, step = 'post', alpha = 0.2,\n                 color = 'b')\n    #plt.plot(threshold,  color = \"g\", label = \"recall\", linewidth = 2)\n    plt.title(\"Precision - Recall Curve\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.ylim([0.0, 1])\n    plt.xlim([0.0, 1])\n    plt.legend(loc = \"best\")\n\ndef PlotROC(model, X, y):\n\n    print('Evaluating ROC Curve...')\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    random_state = np.random.RandomState(0)\n\n    i = 0\n    y = y['Survived']\n    \n    for train, test in cv.split(X,y):\n        prob = model.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])[:,1]\n        fpr, tpr, t = roc_curve(y[test], prob)\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        \n        i= i + 1\n        \n    plt.subplot(2, 3, 6)\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Luck', alpha=.8)\n    \n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    plt.plot(mean_fpr, mean_tpr, color='b',\n         label = r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n         lw = 2, alpha=1)\n\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='red', alpha = .3,\n                 label=r'$\\pm$ 1 std. dev.')\n\n    plt.xlim([-0.0, 1.0])\n    plt.ylim([-0.0, 1.0])\n    plt.xlabel('False Positive Rate (FPR)')\n    plt.ylabel('True Positive Rate (TPR)')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc = \"best\")\n    plt.show()\n    \n    return()\n\ndef PlotKS(model, X, y):\n    y_probas = model.predict_proba(X)\n    \n    skplt.metrics.plot_ks_statistic(y, y_probas, title = 'Kolmogorov\u2013Smirnov test', \n                                    text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n        \ndef PlotLift(model, X, y):\n    y_probas = model.predict_proba(X)\n    skplt.metrics.plot_lift_curve(y, y_probas,title = 'Lift Curve', \n                                  text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n\ndef PlotCumGain(model, X, y):\n    y_probas = model.predict_proba(X)\n    skplt.metrics.plot_cumulative_gain(y, y_probas, title = 'Cumulative Gain',\n                                       text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n\ndef PlotPR(model, X, y):\n\n    y_probas = model.predict_proba(X)\n    skplt.metrics.plot_precision_recall(y, y_probas,\n                                        text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n\ndef Classification_Analysis(model_best, title, title_abrv,X, X_test, y):\n    \n    #Fitting Model\n    (model_best, y_pred, CV_score) = FitModel(model_best, X, y)\n\n    y_train_pred = pd.Series(model_best.predict(X), name = title_abrv)\n    y_test_pred = pd.Series(model_best.predict(X_test), name = title_abrv)\n    \n    # Learning Curve Analysis\n    LearnResults = LearningCurve(X, y, model_best, cv, train_sizes, title)\n    #Confuson Matrix\n    Confuse_fig, cnf_matrix = Confuse(y, y_train_pred, classes)\n    #Precision - Recall Curve\n    PlotPrecisionRecall(model_best, X, y)\n    #plt scikit-plot\n    PlotROC(model_best, X, y)\n    PlotKS(model_best, X, y)\n    PlotLift(model_best, X, y)\n    PlotPR(model_best, X, y)\n    PlotCumGain(model_best, X, y)\n    \n    Summary = PrintResults(title, model_best,X_train,\n                          y, y_train_pred, CV_score, LearnResults, cnf_matrix)\n\n    return (Summary, y_train_pred, y_test_pred)\n\ndef TreeImportance():\n    nrows = ncols = 2\n    fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\n    names_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\n    nclassifier = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            name = names_classifiers[nclassifier][0]\n            classifier = names_classifiers[nclassifier][1]\n            indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n            g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , \n                            orient='h',ax=axes[row][col])\n            g.set_xlabel(\"Relative importance\",fontsize=12)\n            g.set_ylabel(\"Features\",fontsize=12)\n            g.tick_params(labelsize=9)\n            g.set_title(name + \" feature importance\")\n            nclassifier += 1\n            \n#def PrintResults(y, y_pred, Confuse_fig, learn_fig, CV_score, Score_mean, Scores_std):\ndef PrintResults(title, model,X, y, y_pred, CV_score, LearnResults, cnf_matrix ):\n    y_scores = model.predict_proba(X)[:, 1]\n    \n    precision = precision_score(y, y_pred, average = 'macro') * 100\n    recall = recall_score(y, y_pred,average = 'macro') * 100\n    f1score = f1_score(y, y_pred,average = 'macro') * 100\n    Accuracy = accuracy_score(y, y_pred)\n    MCC = matthews_corrcoef(y, y_pred) \n    Lg_loss = log_loss(y, y_pred)\n    Zero_one_loss= zero_one_loss(y, y_pred, normalize = False)\n    Hinge = hinge_loss(y, y_pred) \n    Cohen_kappa = cohen_kappa_score(y, y_pred) \n    Hamming = hamming_loss(y, y_pred)\n    AUC = roc_auc_score(y, y_scores)\n    Brier = brier_score_loss(y, y_scores )\n    \n    Population = np.sum(cnf_matrix)  \n    PP = np.sum(y == 1)\n    NP = np.sum(y == 0)\n    PP_t = np.sum(cnf_matrix[:,1])\n    NP_t = np.sum(cnf_matrix[:,0])\n    TP = cnf_matrix[1,1]\n    \n    TN = cnf_matrix[0,0]\n    FP = cnf_matrix[0,1]\n    FN = cnf_matrix[1,0]\n    \n    TPR = TP\/(TP + FN)\n    TNR = TN\/(TN + FP)\n    FPR = FP\/(FP + TN)\n    FNR = FN\/(FN + TP)\n    \n    P_sum = np.sum(cnf_matrix[:,1])\n    N_sum = np.sum(cnf_matrix[:,0])\n    PPV = TP\/P_sum\n    NPV = TN\/N_sum\n       \n    FDR = FP\/P_sum\n    FOR = FN\/N_sum\n    \n    Acc = (TP + TN)\/(P_sum + N_sum)\n    F1_scr = (2 * TP) \/ (2*TP + FP + FN)\n    MCC = ((TP * TN) - (FP * FN))\/np.sqrt(P_sum * N_sum * (TP + FN) * (TN + FP))\n    \n    BM = TPR + TNR - 1\n    MK = PPV + NPV - 1 \n    \n    LR_minus = FNR\/TNR\n    LR_plus = TPR\/FPR\n    DOR = LR_plus\/LR_minus\n    \n    \n    print('\\nRESULTS: %s'%(title.upper()),'CLASSIFIER')\n    print('--------------------------------')\n    print('\\n Model Settings: %s'%(title.upper()),'CLASSIFIER')\n    print('\\t %s'%(model))\n    print('--------------------------------')\n    print('\\nLearning Curve Results %s'%(title.upper()),'CLASSIFIER')\n    print('\\tTraining')\n    print('\\t\\tScore: %.2f %%'%(LearnResults[0]))\n    print('\\t\\tStdv: %.2f %%'%(LearnResults[1]))\n    print('\\tValidation')\n    print('\\t\\tScore: %.2f%%'%(LearnResults[2]))\n    print('\\t\\tStdv: %.2f %%'%(LearnResults[3]))\n    print('-------------------------------------------------------')\n    print('\\nFull Fitting Results %s'%(title.upper()),'CLASSIFIER')\n    print('\\tAccuracy Score: %.2f %%'%(Accuracy*100))\n    print('\\tCross-Validation Score: %.2f %%'%(CV_score))\n    print(\"\\tPrecision: %.2f %%\"%(precision))\n    print(\"\\tRecall: %.2f %%\"%(recall))\n    print('\\tf1-score: %.2f %%'%(f1score))\n    print('\\tC-Statistic or (AUC-ROC): %.2f %%'%(AUC * 100))\n    print('\\tCohens Kappa: %.2f %%'%(Cohen_kappa * 100))\n    print('\\tKolmogorov\u2013Smirnov (KS) Statistic: %.2f'%(00))\n    \n    print('\\nLosses: %s'%(title.upper()),'CLASSIFIER')\n    print('\\tLog Loss: %.2f'%(Lg_loss))\n    print('\\tZero-One-Loss: %.2f'%(Zero_one_loss))\n    print('\\tHamming Loss: %.2f'%(Hamming))\n    print('\\tBrier Loss: %.2f'%(Brier))\n    print('\\tHinge Loss: %.2f'%(Hinge))    \n    print('-------------------------------------------------------')\n    print('\\nConfusion Matrix: %s'%(title.upper()),'CLASSIFIER')\n    print('\\nClassification Report (weigthed results):')\n    print(classification_report(y, y_pred, digits = 4)) \n    print(' \\n\\t\\t\\t\\t\\tCounts \\t Percentage')\n    print('\\tPopulation: \\t\\t\\t%.0f'%(Population))\n    print('\\tPositive Population (P): \\t%.0f \\t%.2f %% (Prevalence)'%(PP,PP\/Population * 100))     \n    print('\\tNegative Population (N): \\t%.0f \\t%.2f %%'%(NP, NP\/Population * 100))\n    print('\\n')\n    print('\\tPositive Population (P_test): \\t%.0f \\t%.2f %%'%(PP_t,PP_t\/Population * 100))     \n    print('\\tNegative Population (N_test): \\t%.0f \\t%.2f %%'%(NP_t, NP_t\/Population * 100))\n    print('\\n')\n    print('\\tTrue Positive (TP):  \\t\\t%.0f \\t %.2f %% (Sensitivity \/ Recall \/ Hit Rate\/ True Positive Rate (TPR))'%(TP,TPR * 100))\n    print('\\tTrue Negative (TN):  \\t\\t%.0f \\t %.2f %% (Specificity \/ Selectivity \/ True Negative Rate (TNR))'%(TN, TNR * 100))\n    print('\\tFalse Positive (FP): \\t\\t%.0f \\t %.2f %% (Fall-Out \/ False Positive Rate (FPR))'%(FP,FPR * 100))\n    print('\\tFalse Negative (FN): \\t\\t%.0f \\t %.2f %% (Miss Rate \/ False Negative Rate (FNR))'%(FN,FNR * 100))\n    print('\\n')\n    print('\\tPositive Predictive Value (PPV): \\t%.2f %% (Precision)'%(PPV * 100))\n    print('\\tNegative Predictive Value (NPV): \\t%.2f %%'%(NPV * 100))\n    print('\\n')\n    print('\\tFalse Discovery Rate(FDR): \\t\\t%.2f %%'%(FDR * 100))\n    print('\\tFalse Omission Rate (FOR): \\t\\t%.2f %%'%(FOR * 100))\n    print('\\n')\n    print('\\tAccuracy (Acc): \\t\\t\\t%.2f %%'%(Acc * 100))\n    print('\\tF1-Score (F1): \\t\\t\\t\\t%.2f %%'%(F1_scr * 100))\n    print('\\tMathews Correlation Coefficient (MCC): \\t%.2f %%'%(MCC * 100))\n    print('\\n')\n    print('\\tBookmaker Informedness (BM): \\t\\t%.2f %%'%(BM * 100))\n    print('\\tMarkedness (Acc): \\t\\t\\t%.2f %%'%(MK * 100))\n    print('\\n')\n    print('\\tNegative Likelihood Ratio(LR_minus): \\t%.2f '%(LR_minus))\n    print('\\tPositive Likelihood Ratio (LR_plus): \\t%.2f '%(LR_plus))\n    print('\\tDiagnostic Odds Ratio (DOR): \\t\\t%.2f'%(DOR))\n    \n    Summary = pd.DataFrame({\n                    'Model': title,\n                    'Accuracy': Accuracy,\n                    'CV Score': CV_score,\n                    'Precision': precision, \n                    'Recall': recall, \n                    'F1-Score': f1score,\n                    'Train Score': LearnResults[0],\n                    'Train Stdv': LearnResults[1],   \n                    'Val Score': LearnResults[2],\n                    'Val std': LearnResults[3],\n                    'ROC AUC':  AUC * 100, \n                    'MCC': MCC,\n                    'Cohens Kappa':Cohen_kappa},index = [0])\n\n    return (Summary)","8f57da92":"#Logistic Regresion\ntitle = 'Logistic Regression'\ntitle_abrv = 'LR'\n\nmodel = LogisticRegression()\n\nLR_GS_Params = {'penalty': ['l1', 'l2'],\n                 'C': np.logspace(0, 10, 10)}\n\n# the grid search was run and the resutls are shown below... for now it is commented so not repet the s \n#(model_best_LR, model_best_score, model_best_params) = GridSearcher(model, LR_GS_Params, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n#          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n#          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n#          verbose=0, warm_start=False) \n#----------------------------------------------------------------------------------------------------\n\nmodel_best_LR = LogisticRegression(C = 1.0, class_weight = None, dual = False, fit_intercept = True,\n          intercept_scaling = 1, max_iter = 500, multi_class='ovr', n_jobs = -1,\n          penalty = 'l2', random_state = None, solver ='liblinear', tol = 0.0001,\n          verbose=0, warm_start = False) \n\nSummary_LR, y_train_LR, y_test_LR = Classification_Analysis(model_best_LR, title,title_abrv, \n                                                       X_train, X_test, y_train);","eb0e6a70":"#Support Vector Maachine\ntitle = 'Support Vector Machine'\ntitle_abrv = 'SVM'\nmodel = SVC(probability = True)\n\nSVM_GS_Params = {'kernel': ['rbf'], \n                  'gamma': [0.0008, 0.005],\n                  'C': [1, 50, 100, 110,125],\n                  'decision_function_shape':('ovo','ovr'),\n                 'shrinking':(True, False)}\n\n\n# the grid search was run and the resutls are shown below... for now it is commented so not repet the s \n#(model_best_SVM, model_best_score, model_best_params) = GridSearcher(model, SVM_GS_Params, X_train, y_train)\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n\n#Best Estimatot: SVC(C=125, cache_size=200, class_weight=None, coef0=0.0,\n#  decision_function_shape='ovo', degree=3, gamma=0.0008, kernel='rbf',\n#  max_iter=-1, probability=False, random_state=None, shrinking=True,\n#  tol=0.001, verbose=False) \n#Best Score: 0.8215488215488216 \n#Best parameters: {'C': 125, 'decision_function_shape': 'ovo', 'gamma': 0.0008, 'kernel': 'rbf', 'shrinking': True}\n\n#----------------------------------------------------------------------------------------------------\nmodel_best_SVM = SVC(C = 125, cache_size = 200, class_weight = None, coef0 = 0.0,\n  decision_function_shape='ovo', degree = 3, gamma = 0.0008, kernel = 'rbf',\n  max_iter = -1, probability = True, random_state = None, shrinking = True,\n  tol = 0.001, verbose = False) \n\nSummary_SVM, y_train_SVM, y_test_SVM = Classification_Analysis(model_best_SVM, title,title_abrv, \n                                                               X_train, X_test, y_train);","da6e39be":"# Random Forest\ntitle = 'Random Forest'\ntitle_abrv = 'RF'\nmodel = RandomForestClassifier()\n\nRF_GS_Params = {\"max_depth\": [None],\n              \"max_features\": [4,5,6,7],\n              \"min_samples_split\": [3,4,5],\n              \"min_samples_leaf\": [3, 4,5],\n              \"n_estimators\" :[250, 300, 300]}\n\n# the grid search was run and the resutls are shown below... for now it is commented so not repet the s \n#(model_best_RF, model_best_score, model_best_params) = GridSearcher(model, RF_GS_Params, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#            max_depth=None, max_features=5, max_leaf_nodes=None,\n#            min_impurity_decrease=0.0, min_impurity_split=None,\n#            min_samples_leaf=4, min_samples_split=4,\n#            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n#            oob_score=False, random_state=None, verbose=0,\n#            warm_start=False) \n#Best Score: 0.8462401795735129 \n#Best parameters: {'max_depth': None, 'max_features': 5, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 300} \n#----------------------------------------------------------------------------------------------------\nmodel_best_RF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=5, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=4, min_samples_split=4,\n            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\nSummary_RF, y_train_RF, y_test_RF = Classification_Analysis(model_best_RF, title,title_abrv, \n                                                            X_train, X_test, y_train);","f7ae76e7":"#Extra Tree\n\ntitle = 'Extra Tree'\ntitle_abrv = 'ET'\nmodel = ExtraTreesClassifier()\n\n## Search grid for optimal parameters\nET_param_grid = {\"max_depth\": [None],\n              \"max_features\": [7,8,9,10],\n              \"min_samples_split\": [13,14],\n              \"min_samples_leaf\": [1],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[ 600, 700, 800],\n              \"criterion\": [\"gini\"]}\n\n#(model_best_ET, model_best_score, model_best_params) = GridSearcher(model, ET_param_grid, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n#           max_depth=None, max_features=9, max_leaf_nodes=None,\n#          min_impurity_decrease=0.0, min_impurity_split=None,\n#           min_samples_leaf=1, min_samples_split=13,\n#           min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n#           oob_score=False, random_state=None, verbose=0, warm_start=False) \n#Best Score: 0.8462401795735129 \n#Best parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 9, 'min_samples_leaf': 1, 'min_samples_split': 13, 'n_estimators': 600}\n#---------------------------------------------------------------------\n\nmodel_best_ET = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n           max_depth=None, max_features=9, max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=13,\n           min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False) \n\nSummary_ET, y_train_ET, y_test_ET = Classification_Analysis(model_best_ET, title, title_abrv, X_train, X_test, y_train);\n","20699b37":"# Gradient Boosting\n\nmodel = GradientBoostingClassifier()\n\ntitle = 'Gradient Boosting'\ntitle_abrv = 'GB'\n\n#GB_param_grid = {'loss' : [\"deviance\"],\n#              'n_estimators' : [100,200,300,500],\n#              'learning_rate': [0.1, 0.05, 0.01,],\n#              'max_depth': [4,6, 8],\n#              'min_samples_leaf': [50,100,150],\n#              'max_features': [0.1, 0.3, 0.5] \n#              }\n\nGB_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [400,450],\n              'learning_rate': [0.1],\n              'max_depth': [8],\n              'min_samples_leaf': [50],\n              'max_features': [0.01, 0.02, 0.05] \n              }\n\n#(model_best_GB, model_best_score, model_best_params) = GridSearcher(model, GB_param_grid, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: GradientBoostingClassifier(criterion='friedman_mse', init=None,\n#              learning_rate=0.1, loss='deviance', max_depth=8,\n#              max_features=0.02, max_leaf_nodes=None,\n#              min_impurity_decrease=0.0, min_impurity_split=None,\n#              min_samples_leaf=50, min_samples_split=2,\n#              min_weight_fraction_leaf=0.0, n_estimators=400,\n#              presort='auto', random_state=None, subsample=1.0, verbose=0,\n#              warm_start=False) \n#Best Score: 0.8406285072951739 \n#Best parameters: {'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 8, 'max_features': 0.02, 'min_samples_leaf': 50, 'n_estimators': 400}\n\n#----------------------------------------------------------------------------------------------------\n\nmodel_best_GB = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=8,\n              max_features=0.02, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=50, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=400,\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\n              warm_start=False)\n\nSummary_GB, y_train_GB, y_test_GB = Classification_Analysis(model_best_GB, title,title_abrv, X_train, X_test, y_train);","cf35f7d3":"# KNN\ntitle = 'K-Nearest Neighbour'\ntitle_abrv = 'KNN'\nmodel = KNeighborsClassifier()\n\nKNN_param_grid = {'algorithm': ['auto'], 'n_neighbors': [1, 2, 3],\n                 'leaf_size':[1,2,3,4,5,7],\n                 'weights': ['uniform', 'distance']}\n\n#(model_best_KNN, model_best_score, model_best_params) = GridSearcher(model, KNN_param_grid, X_train, y_train)\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski',\n#           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n#           weights='uniform') \n#Best Score: 0.7822671156004489 \n#Best parameters: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 2, 'weights': 'uniform'}\n#----------------------------------------------------------------------------------------------------\n\nmodel_best_KNN = KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n           weights='uniform') \n\nSummary_KNN, y_train_KNN, y_test_KNN = Classification_Analysis(model_best_KNN, title,title_abrv, X_train, X_test, y_train);","4b124181":"# Gaussian Naive Bayes\ntitle = 'Gaussian Naive Bayes'\ntitle_abrv = 'GNB'\nmodel_best_GNB = GaussianNB()\nSummary_GNB, y_train_GNB, y_test_GNB = Classification_Analysis(model_best_GNB, title, title_abrv, \n                                                               X_train, X_test, y_train);","02b5594b":"# Decision Tree\n\ntitle = 'Decision Tree'\ntitle_abrv = 'DT'\nmodel = DecisionTreeClassifier()\n\nDT_param_grid = {'max_depth': [1, 2, 3, 4, 5],\n                  'max_features': [ 4,6, 10,11,12],\n                 'min_samples_split': [2,4,5]\n                }\n        \n#(model_best_DT, model_best_score, model_best_params) = GridSearcher(model, DT_param_grid, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------    \n#Best Estimatot: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4, max_features=12, \n#    max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, \n#     min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') \n\n#Best Score: 0.8327721661054994 \n#----------------------------------------------------------------------------------------------------   \nmodel_best_DT =    DecisionTreeClassifier(class_weight = None, criterion = 'gini', max_depth = 4,\n            max_features = 12, max_leaf_nodes = None,\n            min_impurity_decrease = 0.0, min_impurity_split=None,\n            min_samples_leaf = 1, min_samples_split = 2,\n            min_weight_fraction_leaf = 0.0, presort = False, random_state = None,\n            splitter = 'best')\n    \nSummary_DT, y_train_DT, y_test_DT = Classification_Analysis(model_best_DT, title, title_abrv, X_train, X_test, y_train);   ","3e0ea9e1":"#AdaBoost with Decision Tree Classifier\n\ntitle = 'AdaBoost - Decision Tree'\ntitle_abrv = 'ABDT'\nmodel = AdaBoostClassifier(model_best_DT, random_state=7)\n\nABDT_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\n\n#(model_best_ABDT, model_best_score, model_best_params) = GridSearcher(model, ABDT_param_grid, X_train, y_train)\n \n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------    \n#Best Estimatot: AdaBoostClassifier(algorithm='SAMME.R',\n#   base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4, max_features=12, max_leaf_nodes=None,\n#    min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, \n#    presort=False, random_state=None, splitter='random'), learning_rate=0.001, n_estimators=2, random_state=7) \n\n#Best Score: 0.8226711560044894 \n#---------------------------------------------------------------------------------------------------- \n    \nmodel_best_ABDT =    AdaBoostClassifier(algorithm = 'SAMME.R',\n    base_estimator = DecisionTreeClassifier(class_weight = None, criterion = 'gini', max_depth = 4,\n    max_features = 12, max_leaf_nodes = None, min_impurity_decrease = 0.0, min_impurity_split = None,\n    min_samples_leaf = 1, min_samples_split = 2, min_weight_fraction_leaf = 0.0, presort = False, \n    random_state = None, splitter = 'random'), learning_rate = 0.001, n_estimators = 2, random_state = 7)\n  \nSummary_ABDT, y_train_ABDT, y_test_ABDT = Classification_Analysis(model_best_ABDT, title, title_abrv, X_train, X_test, y_train);    ","7ddb4709":"#Which is the best Model ?\n\nClass_Results = pd.concat([Summary_LR, Summary_SVM, Summary_RF, Summary_GB, Summary_KNN, Summary_ET,\n                          Summary_DT, Summary_ABDT, Summary_GNB], ignore_index = True)\n    \n    \nClass_Results = Class_Results.sort_values(by = 'CV Score', ascending=False)\n#Class_Results = Class_Results.set_index('CV Score')\nClass_Results.head(12)\n","2fa87b7f":"g = sns.barplot(Class_Results[\"CV Score\"],Class_Results[\"Model\"],data = Class_Results, \n                palette = \"muted\",orient = \"h\",**{'xerr': Class_Results['Val std']})\ng.set_xlabel(\"Cross Validation Score\")\ng = g.set_title(\"Cross validation scores\")","394403ed":"# Concatenate all classifier results\ny_test_Results = pd.concat([y_test_LR, y_test_SVM, y_test_RF, y_test_ET, y_test_GB, y_test_KNN, y_test_GNB,\n                              y_test_DT, y_test_ABDT], axis = 1)\n\ny_train_Results = pd.concat([y_train_LR, y_test_SVM, y_train_RF, y_train_ET, y_test_GB, y_train_KNN, y_train_GNB,\n                               y_train_DT, y_train_ABDT], axis = 1)\n\n\nplt.figure(figsize = (14, 7))\nplt.subplot(1,2,1)\nPlotCorr(y_train_Results)\nplt.title('Training data')\nplt.subplot(1,2,2)\nPlotCorr(y_test_Results)\nplt.title('Test data')","5b8d0a60":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", model_best_ABDT),\n                     (\"Extra Trees\", model_best_ET),\n                     (\"RandomForest\",model_best_RF),\n                     (\"GradientBoosting\",model_best_GB)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        \n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , palette = 'muted', orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\")\n        g.set_ylabel(\"Features\")\n        g.set_title(name)\n        \n        nclassifier += 1\n\n","9e62dcdb":"Voting = VotingClassifier(estimators = [('RF', model_best_RF),\n                                      ('SVM', model_best_SVM),\n                                      ('ET', model_best_ET),\n                                      ('GB',model_best_GB),\n                                      #('LR',model_best_LR),\n                                      ('KNN',model_best_KNN),\n                                      ('GNB',model_best_GNB),\n                                      ('DT',model_best_DT),\n                                      ('ABDT',model_best_ABDT)], voting='soft', n_jobs = 2)\n\nVoting = Voting.fit(X_train, y_train)\n\ny_test_V = pd.Series(Voting.predict(X_test), name = \"V\")\n\n#Voting = 78.468% = highest  in Kaggle","c07029bd":"submission = pd.DataFrame({\n        \"PassengerId\": X_test_original[\"PassengerId\"],\n        \"Survived\": y_test_V\n    })\nsubmission.to_csv('Titanic Submission Voting2.csv', index = False)\n\nprint('Done')","f447278b":"VARIABLE DESCRIPTIONS:\n\n    Survived: Survived (1) or died (0)\n    Pclass: Passenger's class\n    Name: Passenger's name\n    Sex: Passenger's sex\n    Age: Passenger's age\n    SibSp: Number of siblings\/spouses aboard\n    Parch: Number of parents\/children aboard\n    Ticket: Ticket number\n    Fare: Fare\n    Cabin: Cabin\n    Embarked: Port of embarkation\n","46c6c2f4":"#### Evaluation of Classification","d6997a84":"### AdaBoost with Decision Tree Classifier","4951508f":"## Exploratory Data Analysis","2602d08b":"FEMALES: have a much higher survival count than men. there are 2 intervals with a particularly high survival count: infants 0 - 5 year, and adults 16 - 38 years old\n\nMEN: have a much lower survival count than women. again there are 2 intervals with relatively high survival counts, infants 0 - 5 year, and adults 20 - 32 years old\n\n\nWhen we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) for babies and very young childrens.\n\n\nThe age distribution for survivors and non-survivors are very similar. One notable difference is that, of the survivors, a larger proportion were children. ","204af26b":"### Feature Description","23daffb9":"\n\n1. Small families have more chance to survive, more than single (Parch 0), medium sized families (Parch 3,4) and large families (Parch 5,6 ).\n\n","4255269d":"### Family Size, Alone","1f8e4e25":"Average Age is 29 years with a standard deviation of 14 years. The average Price of a ticket is and ticket price is 32 but has a huge standard deviation (49... over 100 %). As there are 681 unique tickets and there is no way to extract a less detailed information this feature can be dropped. There are 891 unique names but we could take a look on the title of each person to understand if the survival rate of people based on title & class","6f37a742":"This first bar plot above shows the distribution of female and male survived and died. the opposite happened for men, more dies than survived. This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n\nThe second plots the count as a percentate of that group.  it shows that ~74% female passenger survived while only ~19% male passenger survived\n\n\nthe violin plot also reinforces the fact that more males die and more women survive and shows the density where more med die and womed survive.\n\n\nfrom this is is evident that  Males have less chance to survive than Female. this is probably due to the \"Women and children first\" mentality","33cf3d59":"Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.","fbde9c3e":"## Missing Data","e846eba9":"### Cabin and Deck level","40fa53c5":"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is.\n\nLets take a closer look at the correlations","8af46810":"it is important to know what types of data you are dealing with early on. For classification, all featured will have to be in integer format. below you can see that the data is made up of floats (i.e. numbers), objects (i.e string). The floats will need to be converted into int64 values, some of the objects (e.g. sex) will ned to be converted into numberics, all NaN and null values will need to be filled","6758a93e":"# Titanic Survival - Classification Voting","e8d5daa0":"\n\nAssumption: the less people was in your family the faster you were to get to the boat. The more people they are the more managment is required. However, if you had no family members you might wanted to help others and therefore sacrifice.\n\nThe females traveling with up to 2 more family members had a higher chance to survive. However, a high variation of survival rate appears once family size exceeds 4 as mothers\/daughters would search longer for the members and therefore the chanes for survival decrease.\n\nAlone men might want to sacrifice and help other people to survive.\n\n","26b6a1a7":"### Missing Embark","0da4b0f8":"### Notebook Description","cca41114":"### Survival","f328db36":"### Random Forest","7934bf5d":"the mode increases with decreasing class the third class will have a a large number of infants","7d1c7726":"Lets check for missing data. as can be seen below there are several features with missing data. A much closer look will be taken later when I will replace the missing data. below Cabine is missing most og its data (78%) an thus mostly likely will a feature which cannot be used fot modeling. the Age feature is also missing a considerable amount of data (>20%). This will be filled later by predicting the age basaed on other features (e.g. sex, class, etc). The embark feature has very littel data missing and so can be replaces easily with a mean value deermined by class.","ae92e69e":"### Age Category","269d5e87":"### Gradient Boosting","dd00b95b":"### Feature importance of trees","1e82bbd8":"Logistic regression\nNaive Bayes\nSupport Vector machine\ndecision tree\nBoosted Trees\nRandom Forest\nNeural Network\nNearest Neighbour\nPerceptron\n","15957d79":"### Survival by Embarked Lcoations","be60e9af":"### Person Type ","7dbcc6ea":"# 1. Introduction","827f4776":"### SibSP & Parch","2f7e05f8":"# Fare","63db77aa":"### Support Vector machine","92de40dc":"# Feature Engineering","8b493bfd":"RESULTS IN KAGGLE\n\nGB = 74.%\n\nRF = 78.%\n\nET = 78.46%\n\nDT = 76.55%\n\nLR = 77%","be3bd8e2":"### Null Data","1e0d7585":"Passangers embarked in three different locations. The survival rate is dependand on this. pehaps passangers from a certian embark location belong to a specific class","22e67811":"Confusion matrix: This is the matrix of the actual versus the predicted. This concept is better explained with the example of cancer prediction using the model:\n\n- True positives (TPs): True positives are cases when we predict the disease as yes when the patient actually does have the disease.\n\n\n- True negatives (TNs): Cases when we predict the disease as no when the patient actually does not have the disease.\n\n\n- False positives (FPs): When we predict the disease as yes when the patient actually does not have the disease. FPs are also considered to be type I errors.\n\n\n- False negatives (FNs): When we predict the disease as no when the patient actually does have the disease. FNs are also considered to be type II errors.\n\n\n- Accuracy:  Overall effectivness of a classifier (TP + TN)\/(TP + TN + FP + FN)\n\n\n-  precision or positive predictive value (PPV): correct positive labels? (TP)\/(TP + FP)\n\n\n- Recall\/sensitivity\/true positive rate: effectiveness to identify positive labels?\n(TP\/TP+FN)\n\n\n- F1 score (F1): This is the harmonic mean of the precision and recall.  F1 = 2PR\/(P + R)\n\n\n- specificity, selectivity or true negative rate (TNR): Effectiveness to identify negative labels (TN)\/(FP + TN)\n(TN\/TN+FP)\n\n\n-  Area under Curve (AUC): Ability to avoid false classiication\n\n\n- Receiver operating characteristic (ROC): Receiver operating characteristic curve is used to plot between true positive rate (TPR) and false positive rate (FPR), also known as a sensitivity and 1- specificity graph\n\n\n-  The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. \n\n\n\n-  Cohen Kappa: a score that expresses the level of agreement between Observed Accuracy with an Expected Accuracy (random chance)\n\n\n-  Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier\u2019s predictions.\n\n\n-  Zero-One Loss: return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.\n\n\n-  Hamming Loss: The Hamming loss is the fraction of labels that are incorrectly predicted.\n\n\n-  Hinge Loss: The cumulated hinge loss is  an upper bound of the number of mistakes made by the classifier.\n\n\n-  Brier Loss: measures the mean squared difference between the predicted probability assigned to the possible outcomes and (2) the actual outcome. \n\n\n\n","59e78718":"### Missing Cabin","6b932633":"### Fare Feature","5222aa64":"### Basic Statistics","ec9cb04a":"### Voting","358c2bec":"### Survival by Class","1898e73e":"## PREDICTION MODELS","798438d2":"ater looking closely at the cabin number, it can be seen that it is an alpha-numeric identity. The letter indicates the deck and the number represents the cabin number on this deck. We will therefore subsitute this 'Cabin' category for a 'Deck\" categor and simply extract the deck letter\n\nrecall most cabin number are missing so lets see how may people have cabine and if it is related to surviving","0f43cfb0":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.\n","461795dc":"### Compare Classification Models","77a2b320":"### Compare Predictions by Classifiers","2ba97b6c":"Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees","f949d510":"### Survival by sex","6c69228c":"### KNN","c07d5ea3":"### Missing Age","bf17d02f":"the overall survival is shown below where 0 = died, 1 = survival. as can be seen more died than survived. however, this tells us nothing about what groups of people, age, and class of people these categories are made up of... so let's have a closer look","e1b2a63c":"### Logistic Regresion","9cd08b7b":"### Survival by Age","ae69c54d":"### Import Data","d19b1113":"### Gaussian Naive Bayes","6b5262ff":"A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.","afc62cb4":"### Missing Data now","bf9e86b3":"## Data Description","9730ab79":"### Feature Correlation\n","cbc86461":"### Helper Functions","18a76bbc":"Although there now are no missing FarePP\u2019s anymore, I also noticed that 17 Fares actually have the value 0. These people are not children that might have traveled for free. I think the information might actually be correct (have people won free tickets?), but I also think that the zero-Fares might confuse the algorithm. For instance, there are zero-Fares within the 1st class passengers. To avoid this possible confusion, I am replacing these values by the median FarePP\u2019s for each Pclass.","b2844ad3":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1.\n\n\n(1.) the higher the class hte higher the rate of survival\n\n(2. & 3). for both men and women the higher the class hte higher the rate of survival, but in each class the women more than twice as much as the the men (infact it )\n\n(4. & 5.) the age range increases with increasing class. 1st clas mmen are older than 1st class women, the ranges are closer for the other classes. \n\n(6.) the mean age increases with increasing class but the density decreases, (i.ee 1st class have lees people and there mean age is older)","8224a579":"# Prepare Submission Data","16f217f7":"### Missing fare","3cb75436":"1. the age distribution of both mena and women are the same\n2. for both men and women the mean survival and death age increases with class\n3. for both men an women the mean age of survival and death is the same\n4. in generatl the low the number of siblings\/spouse a person has the larger the mean age and large distrubtion \n5. for both male and female, the the lower the higher the class the larger the age means and range.","883f0807":"### Import Packages","2b0883db":"### Title","4e96bc9b":"### Quick Visual Glance at Data","1bf574c1":"1. Embarkment from location C has the highest survival rate\n2. for the two sexes, the same patter is seem but women have a much greater chance of survival (three times)\n3. most people embarked at S, but these die the most. probably related to class.","6852be37":"### Decision Tree","3cef0c4b":"### Look at hte prepared Data","b15b4c9a":"### Extra Tree","a6c71f20":"we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n\nIt is particularly true for cabin B, C, D, E and F.\n\nmost people where the deck is unknown are from the 3rd class","e9a95cb7":"Wikipedia: Logistic Regression is a statistical model that is usually taken to apply to a binary dependent variable. More formally, a logistic model is one where the log-odds of the probability of an event is a linear combination of independent or predictor variables. The two possible dependent variable values are often labelled as \"0\" and \"1\", which represent outcomes such as pass\/fail, win\/lose, Survive\/dead or healthy\/sick. The binary logistic regression model can be generalized to more than two levels of the dependent variable: categorical outputs with more than two values are modelled by multinomial logistic regression, and if the multiple categories are ordered, by ordinal logistic regression, for example the proportional odds ordinal logistic model. Logistic regression has a high bias and a low variance error.","1396286f":"\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class.","d64cc87c":"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nAn object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.","e1ba3399":"Cabin has alot of data missing. the replacement of this feature is performed durign the feature engineering section... see below\n","4f5cce32":"as can be seen from the above data, there are several featured with NaN missing values. Lets take a look at the features that have missing values missing\n","7a07a0c4":"I created this notebook for beginners (like myself) who are interested and learning how to implement a ML algorithm from scratch, importing, exploratory data analysis, feature engineering, training algorithms, evaluating algorithms, applying algorithms. \n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","ca01b4d9":"Age, embarked and cabin have the most missing values. These terms can be used to determine the survival (see later). thus these missing values will need to be filled in based on their relationship with other featured\n\nThe Embarked feature has only 2 missing values, which can easily be filled. \nThe 'Age' feature, which has 177 missing values, will be filled with values based on its relationship with other features. ","61dacffe":"### Skewness","bf0cd7a2":"it can be seen that tha Pclass (0.34) and the Fare (0.26) have the stongest correlation with the survival rate. These parameters themselves are also highly correlated with eachother. An early repiction may therefore be that the social position (class, money) may be a good indicator of survival. In the feature engineering section new features will be generated and we will also look at these correlations","6a83a999":"### Data Types","af177895":"Above you can see that the Fare is very skewed. I know that this is not desirable for some algorithms, and can be solved by taking the logarithm or normalisation \n\nAnother option is to use Fare Groups instead of keeping the FarePerPerson as a numeric variable. ","ce3e9c8c":"It seems that passenger coming from Cherbourg (C) have more chance to survive. Below we can see that this is not related to class. perhaps it is related to Derck level... see later","655c52ab":"### Mapping","69504c43":"#### Classifiers"}}