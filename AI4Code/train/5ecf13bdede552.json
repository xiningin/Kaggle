{"cell_type":{"9dc991e4":"code","9284e867":"code","fafa5c4a":"code","9ae92e5d":"code","3699fd4b":"code","10c8cc5a":"code","dadfb6e9":"code","7f4d631a":"code","e416899d":"code","4418015a":"code","f753ef77":"code","a1fdf929":"code","99de9fa6":"code","5a97b1e5":"code","20d4f706":"code","2131692d":"code","f57c537a":"code","502787b8":"code","c47cfc3d":"code","ad5559b0":"code","7073953d":"code","a0d0263c":"code","a90f1852":"code","f446ad63":"markdown","243ca722":"markdown","5488cece":"markdown","7ece1d65":"markdown","0586ac91":"markdown","f0d7f170":"markdown","1cdcd89a":"markdown","48761107":"markdown","961878e5":"markdown","1bad08af":"markdown","8250d91c":"markdown","7b975365":"markdown"},"source":{"9dc991e4":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom ipywidgets import  interact, interactive, fixed, interact_manual,FloatSlider","9284e867":"uniform = lambda x: (np.abs(x) <= 1) and 1\/2 or 0\ntriangle = lambda x: (np.abs(x) <= 1) and  (1 - np.abs(x)) or 0\ngaussian = lambda x: (1.0\/np.sqrt(2*np.pi))* np.exp(-.5*x**2) ","fafa5c4a":"# some sample kernel that can be run with a parzen window\nplt.rcParams['figure.figsize'] = [20, 5]\nplt.subplot(1, 3, 1)\nplt.title('Uniform')\nplt.plot([uniform(i) for i in np.arange(-2, 2, 0.1)])\n\nplt.subplot(1, 3, 2)\nplt.title('Triangular')\nplt.plot([triangle(i) for i in np.arange(-2, 2, 0.1)])\n\nplt.subplot(1, 3, 3)\nplt.title('Gaussian')\nplt.plot([gaussian(i) for i in np.arange(-2, 2, 0.1)])\nplt.show()","9ae92e5d":"for i,h in enumerate([.4,.9,1,4]):\n    plt.rcParams['figure.figsize'] = [20, 10]\n    plt.subplot(221 + i)\n    plt.plot([triangle(ln\/h)  for ln in np.arange(-10, 10, 0.1)],label=\"triangle\")\n    plt.plot([gaussian(ln\/h)  for ln in np.arange(-10, 10, 0.1)],label=\"gaussian\")\n    plt.plot([uniform(ln\/h)  for ln in np.arange(-10, 10, 0.1)],label=\"uniform\")  \n    plt.legend()\n    plt.title('parzen windows normal: h: %f' % (h))\nplt.show()\n# A larger value of h samples over a larger region","3699fd4b":"plt.rcParams['figure.figsize'] = [40, 5]\ninp = np.array([np.random.normal(0, 1, 200) + np.random.rand(200) * 4,np.random.normal(5, 2, 200)+ np.random.rand(200),np.random.normal(10, 1, 200)+ np.random.rand(200)]).flatten()\nplt.hist(inp, bins=100);\nplt.show()\nfor i,h in enumerate([.05,.4,1,4]):\n    plt.rcParams['figure.figsize'] = [20, 10]\n    plt.subplot(221 + i)\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([triangle((ln - d)\/h) for d in inp]) for ln in np.arange(0, 20, 0.1)],label=\"triangle\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([gaussian((ln - d)\/h) for d in inp]) for ln in np.arange(0, 20, 0.1)],label=\"gaussian\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([uniform((ln - d)\/h) for d in inp]) for ln in np.arange(0, 20, 0.1)],label=\"uniform\")\n    plt.legend()\n    plt.title('parzen windows: h: %f' % (h))\n    \nplt.tight_layout()\nplt.show()\n# applying the window over a random distributio. \n# A large value of h will obscure a lot of major features of the structure while a small h is highly suceptible to noise.","10c8cc5a":"inp1 = np.array([np.random.normal(0, 1, 200)]).flatten()\nfor i,h in enumerate([.05,.4,1,4]):\n    plt.rcParams['figure.figsize'] = [20, 10]\n    plt.subplot(221 + i)\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([triangle((ln - d)\/h) for d in inp1]) for ln in np.arange(-10, 10, 0.1)],label=\"triangle\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([gaussian((ln - d)\/h) for d in inp1]) for ln in np.arange(-10, 10, 0.1)],label=\"gaussian\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([uniform((ln - d)\/h) for d in inp1]) for ln in np.arange(-10, 10, 0.1)],label=\"uniform\")\n    plt.legend()\n    plt.title('parzen windows normal: h: %f' % (h))\n    \nplt.show()\n# running on a simlar but single normal distribution. ","dadfb6e9":"# Read the CSV input file and show first 5 rows\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_train.head(5)","7f4d631a":"# We can't do anything with the Name, Ticket number, and Cabin, so we drop them.\ndf_train = df_train.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)","e416899d":"# To make 'Sex' numeric, we replace 'female' by 0 and 'male' by 1\ndf_train['Sex'] = df_train['Sex'].map({'female':0, 'male':1}).astype(int) ","4418015a":"# We replace 'Embarked' by three dummy variables 'Embarked_S', 'Embarked_C', and 'Embarked Q',\n# which are 1 if the person embarked there, and 0 otherwise.\ndf_train = pd.concat([df_train, pd.get_dummies(df_train['Embarked'], prefix='Embarked')], axis=1)\ndf_train = df_train.drop('Embarked', axis=1)","f753ef77":"# We normalize the age and the fare by subtracting their mean and dividing by the standard deviation\nage_mean = df_train['Age'].mean()\nage_std = df_train['Age'].std()\ndf_train['Age'] = (df_train['Age'] - age_mean) \/ age_std\n\nfare_mean = df_train['Fare'].mean()\nfare_std = df_train['Fare'].std()\ndf_train['Fare'] = (df_train['Fare'] - fare_mean) \/ fare_std","a1fdf929":"# In many cases, the 'Age' is missing - which can cause problems. Let's look how bad it is:\nprint(\"Number of missing 'Age' values: {:d}\".format(df_train['Age'].isnull().sum()))\n\n# A simple method to handle these missing values is to replace them by the mean age.\ndf_train['Age'] = df_train['Age'].fillna(df_train['Age'].mean())","99de9fa6":"# With that, we're almost ready for training\ndf_train.head()","5a97b1e5":"# Finally, we convert the Pandas dataframe to a NumPy array, and split it into a training and test set\nx_train = df_train.drop('Survived', axis=1).values\ny_train = [[(value == i) * 1 for i in range(0,2)] for value in df_train['Survived'].values]\n\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\nx_train = np.array(x_train)\nx_test = np.array(x_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n","20d4f706":"# uniform_tf = lambda x: (tf.math.abs(x) <= 1) and 1\/2 or 0\n# triangle_tf = lambda x: (np.abs(x) <= 1) and  (1 - np.abs(x)) or 0\ngaussian_tf = lambda x: (1.0\/tf.sqrt(2*np.pi))* tf.exp(-.5*x**2) ","2131692d":"def _pattern(input,name,feature_count,h):\n    with tf.variable_scope(name) as scope:\n        bias = tf.get_variable('bias',[feature_count, 1],initializer=tf.constant_initializer(0))\n        bandwidth = tf.constant(1.0\/(h * feature_count))\n        return tf.multiply(tf.reduce_sum(tf.map_fn(lambda x: (gaussian_tf(x)\/h),input + tf.transpose(bias)),axis=1),bandwidth)","f57c537a":"\ntf.reset_default_graph() \n# N number of traning example with a 28*28 size image\ninputs = tf.placeholder(tf.float32, shape=(None,x_train.shape[1]), name='inputs')\n# 0-2 survived or perished\nlabels = tf.placeholder(tf.float32, shape=(None, 2), name='labels')\n\nsurvive = _pattern(inputs,'survived',x_train.shape[1],.2)\nperished = _pattern(inputs,'perished',x_train.shape[1],.2)\nresult = tf.stack([survive,perished],axis=1)\n","502787b8":"# Loss function and optimizer\nlr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=result, labels=labels))\noptimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n\n# Prediction\npred_label = tf.argmax(result,1)\ncorrect_prediction = tf.equal(pred_label, tf.argmax(labels, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))","c47cfc3d":"# Configure GPU not to use all memory\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True","ad5559b0":"# Start a new tensorflow session and initialize variables\nsess = tf.InteractiveSession(config=config)\nsess.run(tf.global_variables_initializer())","7073953d":"# This is the main training loop: we train for 50 epochs with a learning rate of 0.05 and another \n# 50 epochs with a smaller learning rate of 0.01\nperformance = []\nfor learning_rate in [0.05, 0.01]:\n    for epoch in range(200):\n        avg_cost = 0.0\n\n        # For each epoch, we go through all the samples we have.\n        for i in range(0,x_train.shape[0]):\n            # Finally, this is where the magic happens: run our optimizer, feed the current example into X and the current target into Y\n            _, c = sess.run([optimizer, loss], feed_dict={lr:learning_rate, \n                                                          inputs: [x_train[i]],\n                                                          labels: [y_train[i]]})\n            avg_cost += c\n        avg_cost \/= x_train.shape[0]    \n        performance += [accuracy.eval(feed_dict={inputs: x_test, labels: y_test})]\n        \n        # Print the cost in this epcho to the console.\n        if epoch % 10 == 0:\n            print(\"Epoch: {:3d}    Train Cost: {:.4f}\".format(epoch, avg_cost))","a0d0263c":"acc_train = accuracy.eval(feed_dict={inputs: x_train, labels: y_train})\nprint(\"Train accuracy: {:3.2f}%\".format(acc_train*100.0))\n\nacc_test = accuracy.eval(feed_dict={inputs: x_test, labels: y_test})\nprint(\"Test accuracy:  {:3.2f}%\".format(acc_test*100.0))","a90f1852":"plt.plot(performance,label='performance')\nplt.legend()\nprint('max performance:',max(performance))","f446ad63":"# Performance","243ca722":"# Training","5488cece":"## Cleaning","7ece1d65":"## Kernels\n","0586ac91":"1.  Input Layer\ninput values feed into the PNN network. each entry is a predictor for the network\n2.  Pattern Layer\nThe euclidian distance of the input value centered around zero fed into a parzen window density estimator. non parametric method used to estimate over a distribution.\n3.  Summation Layer\nA summation of the results from the pattern layer\n4.  Output Layer\nthe max is used to denote the target class","f0d7f170":"## Splitting Data","1cdcd89a":"![](https:\/\/i.imgur.com\/vmEfX8G.png)","48761107":"# Kernel Density Estimation","961878e5":"# Titanic Dataset","1bad08af":"## Building Model","8250d91c":"each class kernel is estimated using a Parzen window or a kernel density function.\n\n$f_h(x) = \\frac{1}{n}\\sum_{i=1}^{n}{K_h(x-x_i)} = \\frac{1}{nh}\\sum_{i=1}^{n}{K(\\frac{x-x_i}{h})}$\n\nK is a kernel that is centered around zero. different functions of K can be used to change the estimation. h is used to estimate the distribution given the set of data points. h is a smooting parameter over a given value of x. A large h produces a smoother result while a smaller h is more sensitive to noise. ","7b975365":"## Sources\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/089360809090049Q\n\nhttps:\/\/stats.stackexchange.com\/questions\/244012\/can-you-explain-parzen-window-kernel-density-estimation-in-laymans-terms"}}