{"cell_type":{"2c91657f":"code","ee1c95b1":"code","89e4806d":"code","3ca3f302":"code","34675d14":"code","b26a2b07":"code","54c16f23":"code","c68451bb":"code","c86d9086":"code","a9cf9212":"code","2fa58d45":"code","d30a3c74":"code","665a848b":"code","8a95dcfc":"code","27b0740e":"code","5c486d52":"code","e9bc94f9":"code","08d18349":"code","f2b8da96":"code","0da824d8":"code","e876b371":"code","f6f8383d":"code","e444a9df":"code","6a3c2e5e":"code","86df69c7":"code","6a6fc0b7":"code","ca824f84":"code","4ead9136":"code","d47c95f1":"code","c4316183":"code","d963bbc6":"code","de6ffc27":"code","d3ad9d96":"code","5311a1d8":"code","4c48d0a4":"code","a7a14049":"code","c85b53fb":"code","93a8d938":"code","6d16e575":"code","e3429c98":"code","d4b39d33":"code","652c2ee2":"code","0dc5e96a":"code","1ee9f8d5":"code","0e7c74b7":"code","fe1c6642":"code","9fc69658":"code","18667762":"code","09429837":"markdown","15f63aa0":"markdown","a78306bd":"markdown","3a5816ab":"markdown","5a68ec69":"markdown","05238c1d":"markdown","4e39d12d":"markdown","7e568fe0":"markdown","224c84d5":"markdown","b2258d4e":"markdown","88ab87da":"markdown","99d784e2":"markdown","bb2b408b":"markdown","966149e3":"markdown","4ce7f114":"markdown","26263562":"markdown","710e32b9":"markdown"},"source":{"2c91657f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee1c95b1":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","89e4806d":"df = pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv')","3ca3f302":"df.head()","34675d14":"df.shape","b26a2b07":"df.columns","54c16f23":"df.isna().sum()","c68451bb":"plt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), cmap='Blues', annot=True)","c86d9086":"categorical_features = [col for col in df.columns if df[col].dtypes == 'O']","a9cf9212":"categorical_features","2fa58d45":"for col in categorical_features:\n    print(col, df[col].nunique())","d30a3c74":"unique_values = []\nfor col in categorical_features:\n    unique_values.append(df[col].nunique())","665a848b":"unique_values","8a95dcfc":"sns.set_style(\"white\")\nsns.barplot(unique_values, categorical_features, orient='h')\nplt.title('Unique values of each Categorical values')","27b0740e":"df.drop(['Car_Name'], axis=1, inplace=True)","5c486d52":"categorical_features = [col for col in df.columns if df[col].dtypes == 'O']\ncategorical_features","e9bc94f9":"X = df.drop(['Selling_Price'], axis=1)\ny = df['Selling_Price']","08d18349":"print(X.shape)\nprint(y.shape)","f2b8da96":"X.head()","0da824d8":"for col in categorical_features:\n    print(col, X[col].unique())","e876b371":"X.head()","f6f8383d":"X = pd.get_dummies(X, drop_first=True)","e444a9df":"X.head()","6a3c2e5e":"X['Current_Year'] = 2020\nX['Number_of_years'] = X['Current_Year'] - X['Year']\nX.drop(['Current_Year', 'Year'], axis=1, inplace=True)","86df69c7":"X.head()","6a6fc0b7":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor","ca824f84":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","4ead9136":"model = RandomForestRegressor()","d47c95f1":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n\nmin_samples_split = [2, 5, 10, 15, 100]\n\nmin_samples_leaf = [1, 2, 5, 10]","c4316183":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","d963bbc6":"rf = RandomizedSearchCV(estimator = model, \n                               param_distributions = random_grid,\n                               scoring='neg_mean_squared_error', \n                               n_iter = 10, cv = 5, verbose=2, \n                               random_state=42, n_jobs = 1)\n","de6ffc27":"rf.fit(X_train,y_train)","d3ad9d96":"predictions = rf.predict(X_test)","5311a1d8":"from sklearn import metrics\n\nprint('MAE:',round(metrics.mean_absolute_error(y_test, predictions),2))\nprint('MSE:',round(metrics.mean_squared_error(y_test, predictions),2))\nprint('RMSE:',round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2))\nprint('R2_score',round(metrics.r2_score(y_test, predictions),2))\nRandom_Forest_Regressor = { 'MAE': round(metrics.mean_absolute_error(y_test, predictions),2), 'MSE': round(metrics.mean_squared_error(y_test, predictions),2), \n                      'RMSE': round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2) , 'R2_score':round(metrics.r2_score(y_test, predictions),2)}","4c48d0a4":"plt.figure(figsize=(8,6))\nsns.scatterplot(y_test, predictions)\nplt.xlabel('y_test')\nplt.ylabel('Predictions')\nplt.title('y_test vs Predictions (RandomForestRegressor)')","a7a14049":"from sklearn.tree import DecisionTreeRegressor","c85b53fb":"tree = DecisionTreeRegressor()","93a8d938":"tree.fit(X_train, y_train)","6d16e575":"predictions = tree.predict(X_test)","e3429c98":"print('MAE:',round(metrics.mean_absolute_error(y_test, predictions),2))\nprint('MSE:',round(metrics.mean_squared_error(y_test, predictions),2))\nprint('RMSE:',round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2))\nprint('R2_score',round(metrics.r2_score(y_test, predictions),2))\n\nDecision_Tree_Regressor = { 'MAE': round(metrics.mean_absolute_error(y_test, predictions),2), 'MSE': round(metrics.mean_squared_error(y_test, predictions),2), \n                      'RMSE': round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2) , 'R2_score':round(metrics.r2_score(y_test, predictions),2)}","d4b39d33":"plt.figure(figsize=(8,6))\nsns.scatterplot(y_test, predictions)\nplt.xlabel('y_test')\nplt.ylabel('Predictions')\nplt.title('y_test vs Predictions (DecisionTreeRegressor)')","652c2ee2":"from sklearn.linear_model import LinearRegression","0dc5e96a":"LR = LinearRegression()","1ee9f8d5":"LR.fit(X_train, y_train)","0e7c74b7":"predictions = tree.predict(X_test)","fe1c6642":"print('MAE:',round(metrics.mean_absolute_error(y_test, predictions),2))\nprint('MSE:',round(metrics.mean_squared_error(y_test, predictions),2))\nprint('RMSE:',round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2))\nprint('R2_score',round(metrics.r2_score(y_test, predictions),2))\n\nLinear_Regression = { 'MAE': round(metrics.mean_absolute_error(y_test, predictions),2), 'MSE': round(metrics.mean_squared_error(y_test, predictions),2), \n                      'RMSE': round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2) , 'R2_score':round(metrics.r2_score(y_test, predictions),2)}","9fc69658":"plt.figure(figsize=(8,6))\nsns.scatterplot(y_test, predictions)\n\nplt.xlabel('y_test')\nplt.ylabel('Predictions')\nplt.title('y_test vs Predictions (LinearRegression)')","18667762":"from tomark import Tomark\n\ndata = [Random_Forest_Regressor, Decision_Tree_Regressor, Linear_Regression]\n\nmarkdown = Tomark.table(data)","09429837":"<font size=4>Splitting the data into independent and dependent features.\n<\/font>\n\nHere,\n\n**Dependent Feature** - <font color='#ff9234'>'Selling_Price'<\/font>\n\n**Independent Features** - <font color='#ff9234'>'Year', 'Present_Price', 'Kms_Driven', 'Fuel_Type','Seller_Type', 'Transmission', 'Owner'<\/font> ","15f63aa0":"## Exploring Categorical Features ","a78306bd":"## Creating Models ","3a5816ab":"<font size=3.7 color='#1b6ca8'>Performing RandomizedSearchCV<\/font>","5a68ec69":"## Importing Data","05238c1d":"###  1. RandomForestRegressor","4e39d12d":"|Model| MAE | MSE | RMSE | R2_score |\n|-----|-----|-----|-----|-----|\n|RandomForestRegressor| 0.83 | 2.92 | 1.71 | 0.89 |\n|DecisionTreeRegressor| 0.74 | 1.24 | 1.11 | 0.95 |\n|LinearRegression| 0.74 | 1.24 | 1.11 | 0.95 |\n\n","7e568fe0":"<b>This dataset contains information about used cars listed on www.cardekho.com\nThis data can be used for a lot of purposes such as price prediction to exemplify the use of linear regression in Machine Learning.<\/b> \n\nThe columns in the given dataset is as follows:\n<ol>\n    <li> Car_Name <\/li>\n    <li> Year <\/li>\n    <li> Selling_Price <\/li>\n    <li> Present_Price <\/li>\n    <li> Kms_Driven <\/li>\n    <li> Fuel_Type <\/li>\n    <li> Seller_Type <\/li>\n    <li> Transmission <\/li>\n    <li> Owner <\/li>\n<\/ol>","224c84d5":"### 3. LinearRegression ","b2258d4e":"## Final Result ","88ab87da":"## OneHotEncoding ","99d784e2":"<font size=3.7 color='#1b6ca8'>Exploring Unique values of each Categorical values<\/font>","bb2b408b":"<font size=3.7 color='#1b6ca8'>Converting Year column into (How Old the Car is?) by subtracting Year column from Current Year<\/font> ","966149e3":"<font size=3.7 color='#1b6ca8'>Here, Car_Name feature has **98** unique values so converting them into one hot encoding is not a very good idea. And also Car_Name is not much beneficial for predictions. So, we are dropping that column.<\/font>","4ce7f114":"## Fitting the Training data ","26263562":"## Importing Dependencies ","710e32b9":"### 2. DecisionTreeRegressor"}}