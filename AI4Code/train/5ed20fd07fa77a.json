{"cell_type":{"8a7b0f5e":"code","96f33ab0":"code","f5bb3ee8":"code","27aa25a7":"code","4869c5ca":"code","7200db85":"code","307f946a":"code","2e626ff6":"code","d8485a6e":"code","3598ae94":"code","8d5b1098":"code","5bb9a26a":"code","b12ca7b1":"code","581abf1b":"code","959c466a":"code","5db1afdc":"code","8ba76dd5":"code","ecf97173":"code","f6f39e49":"code","5226bba0":"code","e902e202":"code","a6ac4dd4":"code","f8c00eff":"markdown","31f6de59":"markdown","263902e1":"markdown"},"source":{"8a7b0f5e":"! pip install --quiet colored","96f33ab0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch.autograd import Variable\nimport os\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport joblib\nfrom colored import fore, style\nplt.style.use('fivethirtyeight')","f5bb3ee8":"data = pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndata.head()","27aa25a7":"data.info()","4869c5ca":"# Look at the descriptive statistics of the data\ndata.describe()","7200db85":"# Drop useless column for further visualization and also check for any null values\ndata = data.drop(['Serial No.'], axis=1)\ndata.isna().sum()","307f946a":"# A pairplot visualizes how each variable depends on other variables (If you have no idea what that is, pick a stats book for god's sake)\nsns.pairplot(data)","2e626ff6":"# This is a heatmap.\n# It shows the correlation between different variables at play\nfig = plt.figure(figsize=(8, 8))\nsns.heatmap(data.corr(), annot=True)\nfig.show()\nprint(\"Correlation in a nutshell: \")\nprint(fore.GREEN+\"More Correlation between 2 features => More closely they affect each other and vice-versa\"+style.RESET)","d8485a6e":"feature_importance = dict(data.corr()['Chance of Admit '])\nsort_orders = sorted(feature_importance.items(), key=lambda x: x[1])\nsort_orders.pop()\n\nprint(fore.GREEN+f\"Most Important feature for getting selected is: {sort_orders[-1][0]}\"+style.RESET)\nprint(fore.RED+f\"Least Important feature for getting selected is: {sort_orders[0][0]}\"+style.RESET)","3598ae94":"# Order of Most Important to Least Important Features\nprint(\"Following are the features from most important to least important (Darker Blue Shade = More Important) and (Lesser Blue Shade = Less Important)\")\ni=len(sort_orders)-1\ncolors = [fore.BLUE_VIOLET, fore.VIOLET, fore.BLUE, fore.GREEN, fore.YELLOW, fore.ORANGE_1, fore.RED][::-1]\nwhile i>=0:\n    print(colors[i]+f\"{sort_orders[i][0]}\"+style.RESET)\n    i-=1","8d5b1098":"# Split the data\nX = data.drop('Chance of Admit ', axis=1).values\ny = data['Chance of Admit '].values\n# Just taking 5% of the total data for validation\nx_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.05)","5bb9a26a":"# Define a linear regression model\nclass LinearRegressionTorch(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LinearRegressionTorch, self).__init__()\n        self.linear = torch.nn.Linear(input_size, output_size)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out","b12ca7b1":"# And Hyperparamters\ninp_dim = 7\nop_dim = 1\nlearningRate = 0.001\nepochs = 15000","581abf1b":"# See if CUDA is available\nmodel = LinearRegressionTorch(inp_dim, op_dim)\ntry:\n    model.cuda()\nexcept AssertionError:\n    print(\"GPU isn't enabled\")","959c466a":"# Define the loss and optimizer\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learningRate)","5db1afdc":"# Reshape the labels\ny_train = y_train.reshape(-1, 1)\ny_val = y_val.reshape(-1, 1)","8ba76dd5":"verbose=True\nall_loss = []\nfor epoch in range(epochs+1):\n    \n    # Convert the data into torch tensors\n    inputs = Variable(torch.from_numpy(x_train)).float()\n    label = Variable(torch.from_numpy(y_train)).float()\n    \n    # Clear the existing Gradients from the optimizer\n    optimizer.zero_grad()\n    \n    # Pass the data thorugh model and get one set of predictions\n    output = model(inputs)\n    \n    # Calculate the loss from the obtained predictions and the ground truth values\n    loss = torch.sqrt(criterion(output, label))\n    \n    # Calculate gradients by doing one step of back propagation\n    loss.backward()\n    \n    # Apply those gradients to the weights by doing one step of optimizer\n    optimizer.step()\n    \n    # Add the current loss to the list of all loses (used later for prediction)\n    all_loss.append(loss)\n    \n    # For monitoring and debugging\n    if verbose and epoch % 1000 == 0:\n        print(f\"Epoch: {epoch}  |  Loss: {loss}\")","ecf97173":"# Test the model and compute Validation Accuracy\nVAL_inp = Variable(torch.from_numpy(x_val)).float()\n\ny_pred = model(VAL_inp).detach().numpy()\n\n# Calculate R^2 Accuracy (used for regression where discrete values are absent)\nrss = sum((y_val - y_pred)**2)       # Residual Sum of Squares\ntss = sum((y_val - y_val.mean())**2) # Total Sum of Squares\n\nr2_accuracy = (1 - rss \/ tss)\nprint(f\"Validation Accuracy of the model is: {r2_accuracy.squeeze() * 100} %\")","f6f39e49":"plt.plot(all_loss)","5226bba0":"# Let's now use sklearn's Linear Regression Model\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)","e902e202":"# Measure the model accuracy (the same R^2 Accuracy) using score method\nmodel.score(x_val, y_val)","a6ac4dd4":"# Save the sklearn model\njoblib.dump(model, \"sklearn_model_college_adm.sav\")","f8c00eff":"## Obseravations from the dataset\n\n* Students in this dataset had a minimum chance of **34%** and a maximum of **97%** of getting selected\n* *75%* of students had less than **82%** chances of getting selected.  \n* *50%* of students in the data had less than **72%** chances of getting selected.\n* On an average a student had **8.5** GPA (out of a maximum 10). In this, 75% of all students have less than 9.0 GPA and 50% of the students have less than 8.5 GPA.\n* Also, the minimum present GPA present is **6.8** and the maximum present is **9.9**\n* SOP, LOR and University Rating Descriptives are fairly straight forward and you can take a look at them yourself below\n* Maximum TOEFL Score is **120** and minimum is **92**. Of this, 75% of all students have less than 112 score in TOEFL and 50% of the students have less than 107 score. On an average the student has 107 score.\n* Maximum GRE Score is **340** and minimum is **290**. Of this, 75% of all students have less than 325 score in GRE and 50% of the students have less than 317 score. On an average the student has 316 score.","31f6de59":"# Training a Model\n\nI have tried training the model using different libraries and here is what I observed;\n\n1. PyTorch: Made a Custom Linear Regression model and got ~91% accuracy on test set\n2. Scikit-learn: Just used the pre-built `LogisticRegression` class and got ~92% accuracy on test set\n\nConclusion: You can use both, however for much bigger and diverse datasets, I would rather use the pre-built Logistic Regression model from scikit-learn as it is more efficient and does steps such as Data Normalization and better weights initialization.","263902e1":"# Importing and Data Loading\nNot using Plotly for this dataset, since it weirdly keeps crashing the notebook. It would be great if anyone can give some potential reasons as to why this is happening."}}