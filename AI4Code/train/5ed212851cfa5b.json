{"cell_type":{"22e7d96e":"code","63d7d563":"code","49b7169f":"code","6a91a9b7":"code","75eb5ce6":"code","34f1b199":"code","1fb6b6d7":"code","4b09145b":"code","9fa711aa":"code","e9c320aa":"code","a8ea7150":"code","313b3931":"code","6920b472":"code","13b33dbc":"code","5d3b2383":"code","f262011c":"code","1b37d619":"code","5f03f9fe":"code","07f0bfbd":"code","b9cfd1ea":"code","16d32bd7":"code","24a96470":"code","292472b7":"code","cbc14468":"code","538b0e4a":"code","fe94fa37":"code","2bc967a8":"code","874b06ce":"code","555b5f58":"code","ca4615a3":"code","b041dbff":"code","802295c5":"code","61a3ca57":"code","a68f8852":"code","ec19902b":"code","b4dfb9c8":"code","2b1584b1":"code","046f8eda":"code","51b7859d":"code","0fd82abd":"code","1b423c68":"code","49bf9fd8":"code","fd351b05":"code","8b8593f6":"code","9d6013b7":"code","4d5d8f63":"code","68469ac2":"code","fb158fa0":"code","88a01913":"code","fb4a1637":"code","fec48fe1":"code","5647d27f":"code","e4642615":"code","f8f77863":"markdown","0f42ff41":"markdown","f1f1528f":"markdown","adbc32c3":"markdown","c196d57d":"markdown","69f5134d":"markdown","30bd1f55":"markdown","781fcf64":"markdown","0633497a":"markdown","17470e77":"markdown","0f76dd36":"markdown","e820e087":"markdown","edd5b1ca":"markdown","105a92a6":"markdown","2b0ad76e":"markdown","853afde2":"markdown","9a0930fc":"markdown","997a0bf5":"markdown","87855d4e":"markdown","db18509a":"markdown","b06366e4":"markdown","915765eb":"markdown","80c74094":"markdown","734e134f":"markdown","175b6007":"markdown","49ce1000":"markdown","1e2deb9f":"markdown","68385b53":"markdown","67339640":"markdown","34828cd9":"markdown"},"source":{"22e7d96e":"# Verifica\u00e7\u00e3o do caminho dos arquivos iniciais e fun\u00e7\u00f5es que ir\u00e3o auxiliar no decorrer do notebook\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndef get_iqr_values(df_in, col_name):\n    median = df_in[col_name].median()\n    q1 = df_in[col_name].quantile(0.25) # 25% \/ 1st quartile\n    q3 = df_in[col_name].quantile(0.75) # 7th percentile \/ 3rd quartile\n    iqr = q3-q1 #Interquartile range\n    minimum  = q1-1.5*iqr # The minimum value or the |- marker in the box plot\n    maximum = q3+1.5*iqr # The maximum value or the -| marker in the box plot\n    return median, q1, q3, iqr, minimum, maximum\n  \ndef get_iqr_text(df_in, col_name):\n    median, q1, q3, iqr, minimum, maximum = get_iqr_values(df_in, col_name)\n    text = f\"median={median:.2f}, q1={q1:.2f}, q3={q3:.2f}, iqr={iqr:.2f}, minimum={minimum:.2f}, maximum={maximum:.2f}\"\n    return text\n  \ndef remove_outliers(df_in, col_name):\n    _, _, _, _, minimum, maximum = get_iqr_values(df_in, col_name)\n    df_out = df_in.loc[(df_in[col_name] > minimum) & (df_in[col_name] < maximum)]\n    return df_out\n  \ndef count_outliers(df_in, col_name):\n    _, _, _, _, minimum, maximum = get_iqr_values(df_in, col_name)\n    df_outliers = df_in.loc[(df_in[col_name] <= minimum) | (df_in[col_name] >= maximum)]\n    return df_outliers.shape[0]\n  \ndef box_and_whisker(df_in, col_name):\n    title = get_iqr_text(df_in, col_name)\n    sns.boxplot(df_in[col_name])\n    plt.title(title)\n    plt.show()\n    \n# Fun\u00e7\u00e3o para remover os outliers \ndef remove_all_outliers(df_in, col_name):\n    loop_count = 0\n    outlier_count = count_outliers(df_in, col_name)\n    while outlier_count > 0:\n        loop_count += 1\n        if (loop_count > 100):\n            break\n        df_in = remove_outliers(df_in, col_name)\n        outlier_count = count_outliers(df_in, col_name)\n    return df_in","63d7d563":"# Importa\u00e7\u00e3o das principais libs que utiizaremos ao longo do c\u00f3digo\n\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nimport time\n\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\npd.options.mode.chained_assignment = None  # default='warn'","49b7169f":"# Descompacta\u00e7\u00e3o das bases usando o ZipFile e cria\u00e7\u00e3o dos dataframes principais\n\nfrom zipfile import ZipFile\n\nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')  \n    \nfeatures = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/features.csv\")\n\nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip') as f: \n    f.extractall(path= 'walmart-recruiting-store-sales-forecasting')\n\nsubmission = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv\")\n    \nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip') as f: \n    f.extractall(path= 'walmart-recruiting-store-sales-forecasting')\n\ntest = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/test.csv\")\n\nwith ZipFile('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip') as f: \n    f.extractall(path= 'walmart-recruiting-store-sales-forecasting')\n\ntrain = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/train.csv\")\n\nstores = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")","6a91a9b7":"# An\u00e1lise b\u00e1sica dos DataFrames\n\nprint(\"features.shape\", features.shape)\n#print(\"submission.shape\", submission.shape)\nprint(\"test.shape\", test.shape)\nprint(\"train.shape\", train.shape)\nprint(\"stores.shape\", stores.shape)","75eb5ce6":"features.head(5)","34f1b199":"features.dtypes","1fb6b6d7":"test.head(5)","4b09145b":"test.dtypes","9fa711aa":"train.head(5)","e9c320aa":"train.dtypes","a8ea7150":"stores.head(5)","313b3931":"stores.dtypes","6920b472":"df_train = train.merge(stores, how='left').merge(features, how='left')\ndf_test = test.merge(stores, how='left').merge(features, how='left')","13b33dbc":"print(\"df_train.shape\", df_train.shape)\nprint(\"train.shape\", train.shape)\n\nprint(\"df_test.shape\", df_test.shape)\nprint(\"test.shape\", test.shape) ","5d3b2383":"#Verifica\u00e7\u00e3o de valores nulos no dataframe de treino\n\nmiss_sum = pd.DataFrame(df_train.isnull().sum().sort_values(ascending=False), columns = ['Total'])\nmiss_percent = pd.DataFrame((df_train.isnull().sum()\/df_train.isnull().count()*100), columns = ['Percentage'])\nmissfulldf = pd.concat([miss_sum,miss_percent], axis=1)\n\nprint(missfulldf[missfulldf['Total']>0].sort_values(by= 'Percentage', ascending= False))\nprint('\\n********TOTALS**********\\n',missfulldf[missfulldf['Total']>0].count())","f262011c":"# Preenchimento dos valores nulos com 0\n\ndf_train[\"MarkDown1\"].fillna(0, inplace= True)\ndf_train[\"MarkDown2\"].fillna(0, inplace= True)\ndf_train[\"MarkDown3\"].fillna(0, inplace= True)\ndf_train[\"MarkDown4\"].fillna(0, inplace= True)\ndf_train[\"MarkDown5\"].fillna(0, inplace= True)","1b37d619":"df_train[\"Date\"]= pd.to_datetime(df_train[\"Date\"])","5f03f9fe":"# Como a vari\u00e1vel target \u00e9 a predi\u00e7\u00e3o do volume de vendas por semana, vamos analisar o volume de vendas por semanas do ano\n\ndf_train[\"week\"]= df_train.Date.dt.isocalendar().week\ndf_train[\"year\"]= df_train.Date.dt.year\ndf_train['Month'] = df_train.Date.dt.month\ndf_train['Day'] = df_train.Date.dt.day","07f0bfbd":"#Verifica\u00e7\u00e3o de valores nulos no dataframe de teste\n\nmiss_sum = pd.DataFrame(df_test.isnull().sum().sort_values(ascending=False), columns = ['Total'])\nmiss_percent = pd.DataFrame((df_test.isnull().sum()\/df_test.isnull().count()*100), columns = ['Percentage'])\nmissfulldf = pd.concat([miss_sum,miss_percent], axis=1)\n\nprint(missfulldf[missfulldf['Total']>0].sort_values(by= 'Percentage', ascending= False))\nprint('\\n********TOTALS**********\\n',missfulldf[missfulldf['Total']>0].count())","b9cfd1ea":"# Aplica\u00e7\u00e3o dos mesmos passos utilizados no DataFrame de treino\n\ndf_test[\"MarkDown1\"].fillna(0, inplace= True)\ndf_test[\"MarkDown2\"].fillna(0, inplace= True)\ndf_test[\"MarkDown3\"].fillna(0, inplace= True)\ndf_test[\"MarkDown4\"].fillna(0, inplace= True)\ndf_test[\"MarkDown5\"].fillna(0, inplace= True)\n\ndf_test[\"Date\"]= pd.to_datetime(df_test[\"Date\"])\n\ndf_test[\"week\"]= df_test.Date.dt.isocalendar().week\ndf_test[\"year\"]= df_test.Date.dt.year\ndf_test['Month'] = df_test.Date.dt.month\ndf_test['Day'] = df_test.Date.dt.day","16d32bd7":"# Fun\u00e7\u00e3o criada no in\u00edcio do notebook\n_= box_and_whisker(df_test, \"CPI\")","24a96470":"df_test[\"CPI\"].fillna(df_test[\"CPI\"].mean(), inplace= True)","292472b7":"# Fun\u00e7\u00e3o criada no in\u00edcio do notebook\n_= box_and_whisker(df_test, \"Unemployment\")","cbc14468":"df_test[\"Unemployment\"].fillna(df_test[\"Unemployment\"].mean(), inplace= True)","538b0e4a":"df_train.dtypes","fe94fa37":"# Altera\u00e7\u00e3o dos tipos de vari\u00e1veis\n\ndf_train= df_train.astype({\"Store\":\"int32\",\n                           \"Dept\":\"int32\",\n                           \"Type\":\"object\",\n                           \"IsHoliday\":\"bool\",\n                           \"Type\":\"object\",\n                           \"week\":\"int16\",\n                           \"year\":\"int16\",\n                           \"Month\":\"int16\",\n                           \"Day\":\"int16\"})\n\n\ndf_test= df_test.astype({\"Store\":\"int32\",\n                         \"Dept\":\"int32\",\n                         \"Type\":\"object\",\n                         \"IsHoliday\":\"bool\",\n                         \"Type\":\"object\",\n                         \"week\":\"int16\",\n                         \"year\":\"int16\",\n                         \"Month\":\"int16\",\n                         \"Day\":\"int16\"})\n\nfeatdf = pd.DataFrame(df_train.dtypes,columns=['Data_Type'])\nnunfeat = list(featdf[featdf['Data_Type']!='object'].index)\ncatfeat = list(featdf[featdf['Data_Type']=='object'].index)","2bc967a8":"df_train[nunfeat].describe().T","874b06ce":"# Vendas por semanas do ano\n\nfig, ax = plt.subplots()\n\nfig.set_size_inches(25, 10)\n\n_ = sns.lineplot(x= \"week\", \n                 y= \"Weekly_Sales\", \n                 data= df_train,\n                 hue= \"year\", \n                 ci= None)\n\nplt.show()","555b5f58":"_= sns.catplot(x= \"Store\", \n               y= \"Weekly_Sales\", \n               data= df_train,\n               kind= \"bar\",\n               hue= \"Type\", \n               height= 10, \n               aspect= 2)","ca4615a3":"plt.figure(figsize=(16,8))\n\n_= sns.scatterplot(x= \"Temperature\", \n                   y= \"Weekly_Sales\", \n                   data= df_train,\n                   hue= \"Type\")","b041dbff":"_= sns.catplot(x= \"IsHoliday\",\n               y= \"Weekly_Sales\",\n               data= df_train,\n               kind= \"violin\")","802295c5":"plt.figure(figsize=(16,8))\n\n_= sns.scatterplot(x= \"Size\",\n                   y= \"Weekly_Sales\",\n                   data= df_train,\n                   hue= \"Type\")","61a3ca57":"plt.figure(figsize=(16,8))\n\n_= sns.scatterplot(x= \"Dept\",\n                   y= \"Weekly_Sales\",\n                   data= df_train,\n                   hue= \"Type\")","a68f8852":"# Correla\u00e7\u00e3o para medir a for\u00e7a de correla\u00e7\u00e3o linear entre as vari\u00e1veis\n\nf, ax= plt.subplots(figsize= (15, 10))\n\ncorr= df_train[nunfeat].corr(method='pearson')\n\nmask= np.triu(np.ones_like(corr, dtype= bool))\n\n_= sns.heatmap(corr, \n               mask= mask,\n               cmap='coolwarm', \n               annot=True, \n               center= 0, \n               fmt= '.3f',\n               square= True)\n\n_.set_title('Correla\u00e7\u00e3o de Pearson');","ec19902b":"# Cria\u00e7ao das vari\u00e1veis X e y\n\nX= df_train.drop(columns= [\"Date\", \n                           \"Weekly_Sales\"])\n                        \ny= df_train[\"Weekly_Sales\"]","b4dfb9c8":"featdf = pd.DataFrame(X.dtypes,columns=['Data_Type'])\n\nnunfeat = list(featdf[featdf['Data_Type']!='object'].index)\ncatfeat = list(featdf[featdf['Data_Type']=='object'].index)\nencfeat= [\"IsHoliday\",\"Type\"]","2b1584b1":"#Cria\u00e7\u00e3o dos dataframes de treino, teste e valida\u00e7\u00e3o\n\nfrom sklearn.model_selection import train_test_split\n\n# Dataset de treino\nX_train, X_rem, y_train, y_rem= train_test_split(X, y, train_size= 0.8)\n\n#Dataset de valida\u00e7\u00e3o\nX_valid, X_test, y_valid, y_test= train_test_split(X_rem, y_rem, test_size= 0.5)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\nprint(X_valid.shape, y_valid.shape)","046f8eda":"# Cria\u00e7\u00e3o dos encoders para as vari\u00e1veis categ\u00f3ricas\n\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[encfeat])\nencoded_cols_train = list(enc.get_feature_names(encfeat))\n\n# Grava\u00e7\u00e3o do encoder com pickle para podermos persistir o mesmo padr\u00e3o em futuros dataframes \nfilehandler= open(\"enc.obj\", \"wb\")\npickle.dump(enc, filehandler)\nfilehandler.close()\n","51b7859d":"#Persistindo o encoder para X_train, X_test, X_valid\n\nX_train[encoded_cols_train]= enc.transform(X_train[encfeat])\nX_train.drop(columns= encfeat, inplace= True)\n\nX_test[encoded_cols_train]= enc.transform(X_test[encfeat])\nX_test.drop(columns= encfeat, inplace= True)\n\nX_valid[encoded_cols_train]= enc.transform(X_valid[encfeat])\nX_valid.drop(columns= encfeat, inplace= True)","0fd82abd":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(random_state=42, \n                     n_estimators= 25, \n                     max_depth=4)","1b423c68":"%%time\n\nmodel.fit(X_train, y_train)","49bf9fd8":"preds = model.predict(X_test)","fd351b05":"# Fun\u00e7ao para calcular o WMAE  \n\ndef WMAE(df, targets, predictions):\n    weights = df.IsHoliday_True.apply(lambda x: 1 if x==0 else 5)\n    return np.round(np.sum(weights*abs(targets-predictions))\/(np.sum(weights)), 2)","8b8593f6":"#WMAE Inicial\n\nWMAE(X_test, y_test, preds)","9d6013b7":"# Verifica\u00e7\u00e3o da import\u00e2ncia das vari\u00e1veis\n\nplt.figure(figsize=(10,7))\n_= sns.barplot(x= \"importance\",\n            y= \"feature\", \n            data= pd.DataFrame({\"feature\": X_train.columns, \"importance\": model.feature_importances_}).sort_values(by= \"importance\", ascending= False)) ","4d5d8f63":"# Melhores par\u00e2metros encontrados pelo optuna = Trial 82 finished with value: 1242.22\n\nparams= {'lambda': 1.3455233520686138, \n         'alpha': 0.316230354228637, \n         'colsample_bytree': 0.9, \n         'subsample': 1.0, \n         'learning_rate': 0.014, \n         'n_estimators': 385, \n         'max_depth': 20, \n         'min_child_weight': 8}","68469ac2":"# Rodando o modelo com os melhores par\u00e2metros \n\nmodel = XGBRegressor(**params)  \n    \nmodel.fit(X_train, y_train)\n    \npreds = model.predict(X_test)\n\nWMAE(X_test, y_test, preds)","fb158fa0":"# teste do WMAE\npreds_validation= model.predict(X_valid)\n\nWMAE(X_valid, y_valid, preds_validation)","88a01913":"# Cria\u00e7\u00e3o da vari\u00e1vel X no dataset de submiss\u00e3o do desafio\n\nX_entry= df_test[X.columns]","fb4a1637":"# Persisitindo o encoder, previamente salvo (o pickle foi aberto quando persistimos o enconder para X_train, X_test e X_valid), \n# nas vari\u00e1veis categ\u00f3ricas do X de submiss\u00e3o \n\nX_entry[encoded_cols_train]= enc.transform(X_entry[encfeat])\nX_entry.drop(columns= encfeat, inplace= True)","fec48fe1":"# Predi\u00e7\u00e3o \n\npreds_entry= model.predict(X_entry)\n\nX_entry[\"Weekly_Sales\"]= preds_entry","5647d27f":"# Verifica\u00e7\u00e3o das predi\u00e7\u00f5es X realizado em anos anteriores. \n\nplt.figure(figsize=(25, 7))\n\nweekly_sales2010 = df_train.loc[df_train['year']==2010].groupby(['week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2011 = df_train.loc[df_train['year']==2011].groupby(['week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2012 = df_train.loc[df_train['year']==2012].groupby(['week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2012_test = X_entry.loc[X_entry['year']==2012].groupby(['week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2013_test = X_entry.loc[X_entry['year']==2013].groupby(['week']).agg({'Weekly_Sales': ['mean']})\n\nsns.lineplot(weekly_sales2010['Weekly_Sales']['mean'].index, weekly_sales2010['Weekly_Sales']['mean'].values, color='gray')\nsns.lineplot(weekly_sales2011['Weekly_Sales']['mean'].index, weekly_sales2011['Weekly_Sales']['mean'].values, color='gray')\nsns.lineplot(weekly_sales2012['Weekly_Sales']['mean'].index, weekly_sales2012['Weekly_Sales']['mean'].values, color='gray')\n\nsns.lineplot(weekly_sales2012_test['Weekly_Sales']['mean'].index, weekly_sales2012_test['Weekly_Sales']['mean'].values, color='red')\nsns.lineplot(weekly_sales2013_test['Weekly_Sales']['mean'].index, weekly_sales2013_test['Weekly_Sales']['mean'].values, color='red')\n\n\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012','2012 test', '2013 test'])\nplt.show()","e4642615":"# Cria\u00e7\u00e3o do dataset de envio\n\nsubmission[\"Weekly_Sales\"]= preds_entry\nsubmission.to_csv('submission.csv',index=False)\nsubmission","f8f77863":"De antem\u00e3o, podemos ver que ocorrem 3 picos de vendas todos os anos, um pico pequeno por volta da 12-14 semana, e dois picos grandes por volta das 46-47 e 51-51.\n\nQuando olhamos as datas, vemos que os maiores picos s\u00e3o o dia de a\u00e7\u00e3o de gra\u00e7as e natal. ","0f42ff41":"**** Este passo pode levar muito tempo para ser executado e \u00e9 necess\u00e1rio que a GPU seja ativada nas prefer\u00eancias de ambiente. \n\nimport optuna\n\ndef objective(trial):\n    \n    param = {'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n             'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n             'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n             'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n             'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n             'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n             'n_estimators': trial.suggest_int(\"n_estimators\", 50, 400),\n             'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n             'random_state': 42,\n             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n            \n\n    model = XGBRegressor(**param)  \n    \n    model.fit(X_train, y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(X_test)  \n    \n    return WMAE(X_test ,y_test, preds)","f1f1528f":"# Treinamento Inicial","adbc32c3":"O gr\u00e1fico acima, mostra que existe uma correla\u00e7\u00e3o entre vendas semanais com o tipo de loja e tamanho da loja.","c196d57d":"# Predi\u00e7\u00f5es no dataset de teste","69f5134d":"Podemos ver que a var\u00edavel \"Unemployment\" est\u00e1 normalmente distribu\u00edda. Neste caso, optei preencher os valores nulos com a m\u00e9dia. ","30bd1f55":"Com o gr\u00e1fico acima, podemos ver que a m\u00e9dia de vendas semanais, tentem a aumentar quando a temperatura esta entre 40 e 60 grau e que tendem a diminuir nos extremos. Por\u00e9m n\u00e3o aparenta existir alguma correla\u00e7\u00e3o forte com esta hip\u00f3tese. ","781fcf64":"# Tratamentos no DataFrame de treino","0633497a":"# An\u00e1lise basica dos dataframes","17470e77":"Podemos ver que as vendas semanais nas semanas que tem feriados tendem a ser mais altas do que as que n\u00e3o tem. ","0f76dd36":"# EDA\n\nAlgumas perguntas que podemos fazer para os dados: \n\n    Quando a temperatura aumenta, as vendas sobem? \n    As vendas semanais costumam aumentar nos feriados? \n    O tipo de loja ou tamanho da loja influencia no volume de vendas?\n    Existe algum tipo de sazonalidade de compras?","e820e087":"Inicialmente, podemos ver que os dataframes \"submission\" e \"test\" tem a mesma quantidade de linhas. E que o DataFrame Stores, deve ser de cardinalidade 1 para muitos. ","edd5b1ca":"Vamos **juntar** os dataframes de **lojas e features** com os dataframes de **treino e teste**","105a92a6":"Estudo com inten\u00e7\u00e3o de diminuir o WMAE \n\nstudy = optuna.create_study(direction= 'minimize')\nstudy.optimize(objective, n_trials= 150)\n\nprint('Number of finished trials:', len(study.trials))","2b0ad76e":"# Ajuste de hiperpar\u00e2metros\n","853afde2":"# Cria\u00e7\u00e3o do dataset de envio","9a0930fc":"***Features***","997a0bf5":"# Feature Engineering","87855d4e":"Podemos ver pelo gr\u00e1fico acima, que existe alguma correla\u00e7\u00e3o entre o volume de vendas semanais com os departamentos. ","db18509a":"***Test***","b06366e4":"Vemos que inicialmente, algumas vari\u00e1veis tiveram pouca import\u00e2ncia para predizer as vendas semanais e que as vari\u00e1veis mais importantes foram o tamanho da loja e o departamento. ","915765eb":"Podemos ver que existe uma diferen\u00e7a quando comparamos o dataframe de treino com o dataframe de testes. \n   \n    DataFrame de treino, apenas as vari\u00e1veis de MarkDown vieram com campos nulos. \n    DataFrame de teste temos valores nulos nas vari\u00e1veis CPI e Unemployment","80c74094":"Podemos ver que as lojas do tipo \"A\" tendem a ter uma m\u00e9dia semanal de vendas maior que as outras. ","734e134f":"***Train***","175b6007":"Aparentemente, n\u00e3o existe uma correla\u00e7\u00e3o muito forte entre entre vari\u00e1vel target e outras vari\u00e1veis.\n\n","49ce1000":"# Tratamentos no DataFrame de teste","1e2deb9f":"***Stores***","68385b53":"Podemos ver que a var\u00edavel \"CPI\" est\u00e1 normalmente distribu\u00edda. Neste caso, optei preencher os valores nulos com a m\u00e9dia. ","67339640":"***Submission***","34828cd9":"Tivemos uma queda expressiva no WMAE ap\u00f3s o estudo com optuna, saindo de 6413.33 para 1289.05. "}}