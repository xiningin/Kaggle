{"cell_type":{"6ffe315c":"code","95cbab88":"code","743eb001":"code","84280281":"code","1cf661e1":"code","c32266e5":"code","acc7b282":"code","926836a5":"code","dd490854":"code","b72a20c0":"code","0da460f3":"code","b04e6855":"code","cd0191e0":"code","e2b2b1de":"code","5691c4be":"code","fa91cb70":"code","5e443ebb":"code","1c1b7b9e":"code","ada9ee71":"code","cb9f7750":"code","95044d4c":"code","de62323a":"code","8acc4010":"code","1d6306d6":"code","d05757f7":"code","0e7da589":"code","919e3e91":"code","ca4946ee":"code","159b17ed":"code","599361d8":"markdown","a11617ce":"markdown","42f326e0":"markdown","c3b88ad2":"markdown","44587cf0":"markdown","e30d0aad":"markdown","ea3af9b5":"markdown","3a608ccf":"markdown","4b7f7613":"markdown","4ce2a533":"markdown","d269fc96":"markdown","0d08d863":"markdown","6abdc158":"markdown","982cbb4d":"markdown","8ade79c3":"markdown","e6d6917b":"markdown","4ac9e5c3":"markdown","fefd5a65":"markdown","dda8fdf1":"markdown","5d24f69a":"markdown"},"source":{"6ffe315c":"# Libraries for Data Analysis\nimport numpy as np\nimport pandas as pd\n# Libraries for Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","95cbab88":"# Loading dataset from sklearn\nfrom sklearn.datasets import load_boston\n# Making the instance of the class\nboston = load_boston()\nprint(boston.keys())\nprint(boston.DESCR)","743eb001":"# Creating the feature dataframe\ndf_feat = pd.DataFrame(boston.data, columns=boston.feature_names)\n# Creating the target dataframe which is 'price'\ndf_target = pd.DataFrame(boston.target, columns=['PRICE'])\n# Concatinating the two dataframe into one for data exploration\ndf = pd.concat([df_feat, df_target], axis=1)\ndf.head()","84280281":"df.info()","1cf661e1":"df.describe()","c32266e5":"# Checking the distribution of varibales\ndf.hist(figsize=(20,15), edgecolor='black')\nplt.show()","acc7b282":"fig, axs = plt.subplots(ncols=2, figsize=(20,8))\nmatrix = np.triu(df.corr())\nsns.heatmap(df.corr(), mask=matrix, annot=True, cmap='viridis',cbar_kws = dict(use_gridspec=False,location=\"top\"), ax=axs[0])\naxs[0].set_title('Correlation matrix')\naxs[1].barh(df.columns, df.corrwith(df.PRICE))\naxs[1].set_title('features correlation with price')\nplt.show()","926836a5":"sns.set_style('whitegrid')\nsns.jointplot(x='RM', y='PRICE', data=df, color='#007b7f', edgecolor='black')\nplt.show()","dd490854":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('PRICE',axis=1), df.PRICE, test_size=0.20)","b72a20c0":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\nX_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(df_scaled[:,:-1], df_scaled[:,-1], test_size=0.20)","0da460f3":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor","b04e6855":"knn = KNeighborsRegressor().fit(X_train_scaled, y_train_scaled)\nknn_score = knn.score(X_test_scaled, y_test_scaled)\n\nlr = LinearRegression().fit(X_train, y_train)\nlr_score = lr.score(X_test, y_test)\n\nridge = Ridge().fit(X_train, y_train)\nridge_score = ridge.score(X_test, y_test)\n\nlasso = Lasso().fit(X_train, y_train)\nlasso_score = lasso.score(X_test, y_test)\n\ntree = DecisionTreeRegressor().fit(X_train, y_train)\ntree_score = tree.score(X_test, y_test)\n\nforest = RandomForestRegressor().fit(X_train, y_train)\nforest_score = forest.score(X_test, y_test)\n\nboost = GradientBoostingRegressor().fit(X_train, y_train)\nboost_score = boost.score(X_test, y_test)\n\nsvm = SVR().fit(X_train_scaled, y_train_scaled)\nsvm_score = svm.score(X_test_scaled, y_test_scaled)\n\nmlp = MLPRegressor().fit(X_train_scaled, y_train_scaled)\nmlp_score = mlp.score(X_test_scaled, y_test_scaled)\n\nbaseline_score = pd.DataFrame({'model':['KNN','Linear Regression', 'Ridge', 'Lasso', 'Decision Tree','Random Forest',\n                                     'Gradient Boost','Kernel SVM','Neural Network'], 'Score':[knn_score, lr_score,\n                                                                                              ridge_score, lasso_score,\n                                                                                              tree_score, forest_score,\n                                                                                              boost_score, svm_score,\n                                                                                              mlp_score]})\nbaseline_score.sort_values(by='Score', ascending=False)","cd0191e0":"train_accuracy = []\ntest_accuracy = []\nfor i in range(1,11):\n    knn = KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train_scaled, y_train_scaled)\n    train_accuracy.append(knn.score(X_train_scaled, y_train_scaled))\n    test_accuracy.append(knn.score(X_test_scaled, y_test_scaled))\n    \n\nscore = pd.DataFrame({'n_neighbors':range(1,11),'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('n_neighbors')\nscore.transpose()","e2b2b1de":"knn = KNeighborsRegressor(n_neighbors=2).fit(X_train_scaled, y_train_scaled)\nknn_score = knn.score(X_test_scaled, y_test_scaled)","5691c4be":"train_accuracy = []\ntest_accuracy = []\nfor i in [0.001, 0.01, 0.1, 1, 100]:\n    ridge = Ridge(alpha=i).fit(X_train, y_train)\n    train_accuracy.append(ridge.score(X_train, y_train))\n    test_accuracy.append(ridge.score(X_test, y_test))\n    \npd.DataFrame({'alpha':[0.001,0.01,0.1,1,100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('alpha').transpose()","fa91cb70":"ridge = Ridge(alpha=0.001).fit(X_train,y_train)\nridge_score = ridge.score(X_test, y_test)","5e443ebb":"train_accuracy = []\ntest_accuracy = []\nfor i in [0.001, 0.01, 0.1, 1, 100]:\n    lasso = Lasso(alpha=i).fit(X_train, y_train)\n    train_accuracy.append(lasso.score(X_train, y_train))\n    test_accuracy.append(lasso.score(X_test, y_test))\n    \npd.DataFrame({'alpha':[0.001,0.01,0.1,1,100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('alpha').transpose()","1c1b7b9e":"lasso = Lasso(alpha=0.001).fit(X_train,y_train)\nlasso_score = lasso.score(X_test, y_test)","ada9ee71":"train_accuracy = []\ntest_accuracy = []\nfor i in [1,2,3,10,100]:\n    tree = DecisionTreeRegressor(max_depth=i).fit(X_train, y_train)\n    train_accuracy.append(tree.score(X_train, y_train))\n    test_accuracy.append(tree.score(X_test, y_test))\n    \npd.DataFrame({'max_depth':[1,2,3,10,100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('max_depth').transpose()\n","cb9f7750":"tree = DecisionTreeRegressor(max_depth=2).fit(X_train, y_train)\ntree_score = tree.score(X_test, y_test)","95044d4c":"train_accuracy = []\ntest_accuracy = []\nfor i in [5, 20, 50, 75, 100]:\n    forest = RandomForestRegressor(n_estimators=i, random_state=43).fit(X_train, y_train)\n    train_accuracy.append(forest.score(X_train, y_train))\n    test_accuracy.append(forest.score(X_test, y_test))\n    \npd.DataFrame({'n_estimator':[5,20,50,75,100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('n_estimator').transpose()","de62323a":"forest = RandomForestRegressor(n_estimators=50, random_state=43).fit(X_train, y_train)\nforest_score = forest.score(X_test, y_test)","8acc4010":"train_accuracy = []\ntest_accuracy = []\nfor i in [0.001,0.01,0.1,1]:\n    boost = GradientBoostingRegressor(learning_rate=i).fit(X_train, y_train)\n    train_accuracy.append(boost.score(X_train, y_train))\n    test_accuracy.append(boost.score(X_test, y_test))\n    \npd.DataFrame({'learning_rate':[0.001,0.01,0.1,1], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('learning_rate').transpose()\n","1d6306d6":"boost = GradientBoostingRegressor(learning_rate=0.1).fit(X_train, y_train)\nboost_score = boost.score(X_test, y_test)","d05757f7":"train_accuracy = []\ntest_accuracy = []\nfor i in [1, 10 ,100 ,1000]:\n    svm = SVR(C=i).fit(X_train_scaled, y_train_scaled)\n    train_accuracy.append(svm.score(X_train_scaled, y_train_scaled))\n    test_accuracy.append(svm.score(X_test_scaled, y_test_scaled))\n    \npd.DataFrame({'C':[1,10,100,1000], 'train_accuracy':train_accuracy,'test_accuracy':test_accuracy}).set_index('C').transpose()","0e7da589":"svm = SVR(C=10).fit(X_train_scaled, y_train_scaled)\nsvm_score = svm.score(X_test_scaled, y_test_scaled)","919e3e91":"train_accuracy = []\ntest_accuracy = []\nfor i in [[10],[10,10], [20,20]]:\n    mlp = MLPRegressor(hidden_layer_sizes=i).fit(X_train_scaled, y_train_scaled)\n    train_accuracy.append(mlp.score(X_train_scaled, y_train_scaled))\n    test_accuracy.append(mlp.score(X_test_scaled, y_test_scaled))\n    \npd.DataFrame({'hidden_layers':['10','10,10','20,20'], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('hidden_layers').transpose()","ca4946ee":"mlp = MLPRegressor(hidden_layer_sizes=[20,20]).fit(X_train_scaled, y_train_scaled)\nmlp_score = mlp.score(X_test_scaled, y_test_scaled)","159b17ed":"tuning_score = pd.DataFrame([knn_score, lr_score, ridge_score , lasso_score, tree_score, forest_score, boost_score,\n                            svm_score, mlp_score], columns=['performance after tuning'])\nscores = pd.concat([baseline_score, tuning_score],axis=1)\nscores","599361d8":"# Basic Imports ","a11617ce":"# Neural Network","42f326e0":"This notebook is all about checking the performance of following ML model on the Boston Dataset.\n1. K Nearest Neighbors\n2. Linear Regression\n3. Ridge Regression\n4. Lasso Regression\n5. Decision Tree\n6. Random Forest\n7. Kernel SVM\n8. Neural Network","c3b88ad2":"## Gradient Boost","44587cf0":"# Model Performance","e30d0aad":"There are no null values.","ea3af9b5":"## Decision Tree","3a608ccf":"# Buliding Baseline Models","4b7f7613":"## Kernel SVM","4ce2a533":"## Lasso","d269fc96":"## KNN","0d08d863":"the dataset is in the form of dictionaries.","6abdc158":"# Data Imports","982cbb4d":"# Exploratory Data Analysis","8ade79c3":"## Scaling the dataset","e6d6917b":"## Ridge","4ac9e5c3":"## Random Forest","fefd5a65":"# Splitting the dataset","dda8fdf1":"# Quick Look at data","5d24f69a":"# Model Tuning"}}