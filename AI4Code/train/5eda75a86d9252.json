{"cell_type":{"eb27c314":"code","8a823125":"code","73712610":"code","2aabed77":"code","accae5d7":"code","af071111":"code","3577df8d":"code","dc6d890d":"code","c1918a37":"code","82ba557c":"code","d89e2230":"code","aff3ed95":"code","1aaa9391":"code","6810832e":"code","dc5924e3":"code","f240a981":"code","27b5c2aa":"code","6d5374f6":"code","a4b64448":"code","db7681f0":"code","9af7a8a4":"code","5a38b552":"code","1b11f078":"code","18e82ea1":"code","a5a5dcc4":"code","4911653e":"code","f365b40d":"code","9b38c0c4":"code","cc744c53":"code","bcbd9707":"code","70a64167":"code","2573de8f":"code","6126f6ad":"code","13315686":"code","30c223da":"code","c722db05":"code","ecefcd53":"code","a500efaa":"code","1601f73b":"code","cfaf77b7":"markdown","47737a5d":"markdown","43ed5b1b":"markdown","a4f7426f":"markdown","a711d05a":"markdown","a71f2579":"markdown","d2e87228":"markdown","3c8f8ebb":"markdown","7deca481":"markdown","aafc72a6":"markdown"},"source":{"eb27c314":"!pip install -q hvplot","8a823125":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\nimport optuna\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\n%matplotlib inline\n\npd.pandas.set_option('display.max_columns', None)\npd.pandas.set_option('display.max_rows', 100)","73712610":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ntrain.head()","2aabed77":"test.head()","accae5d7":"sample_submission.head()","af071111":"print(f\"Train data shape {train.shape}\")\nprint(f\"Trest data shape {test.shape}\")","3577df8d":"train.hvplot.hist(\"SalePrice\", title=\"Sales Price Distribution\")","dc6d890d":"train['SalePrice'].describe()","c1918a37":"train[train['SalePrice']>500000].shape","82ba557c":"missing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.hvplot.barh(title=\"Missing Values (Training Data)\")","d89e2230":"missing = test.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.hvplot.barh(title=\"Missing Values (Testing Data)\", height=500)","aff3ed95":"train_missing = []\nfor column in train.columns:\n    if train[column].isna().sum() != 0:\n        missing = train[column].isna().sum()\n        print(f\"{column:-<{30}}: {missing} ({missing \/ train.shape[0] * 100:.2f}%)\")\n        if missing > train.shape[0] \/ 3:\n            train_missing.append(column)","1aaa9391":"test_missing = []\nfor column in test.columns:\n    if test[column].isna().sum() != 0:\n        missing = test[column].isna().sum()\n        print(f\"{column:-<{30}}: {missing} ({missing \/ test.shape[0] * 100:.2f}%)\")\n        if missing > test.shape[0] \/ 3:\n            test_missing.append(column)","6810832e":"print(f\"{train_missing}\")\nprint(f\"{test_missing}\")","dc5924e3":"train.drop(train_missing, axis=1, inplace=True)\ntest.drop(train_missing, axis=1, inplace=True)","f240a981":"print(f\"MSZoning number of unique values (Train): {train['MSZoning'].nunique()}\")\nprint(f\"MSZoning number of unique values (Test): {test['MSZoning'].nunique()}\")\n\nprint(f\"MSSubClass number of unique values (Train): {train['MSSubClass'].nunique()}\")\nprint(f\"MSSubClass number of unique values (Test): {test['MSSubClass'].nunique()}\")","27b5c2aa":"all_columns = train.columns.to_list()","6d5374f6":"train['MSSubClass'].value_counts().hvplot.bar(title=\"MSSubClass (Trainig Data)\")","a4b64448":"test['MSSubClass'].value_counts().hvplot.bar(title=\"MSSubClass (Testing Data)\")","db7681f0":"print(f\"LotArea number of unique values (Train): {train['LotArea'].nunique()}\")\nprint(f\"LotArea number of unique values (Test): {test['LotArea'].nunique()}\")\n\nprint(f\"LotFrontage number of unique values (Train): {train['LotFrontage'].nunique()}\")\nprint(f\"LotFrontage number of unique values (Test): {test['LotFrontage'].nunique()}\")\n\nprint(f\"LotShape number of unique values (Train): {train['LotShape'].nunique()}\")\nprint(f\"LotShape number of unique values (Test): {test['LotShape'].nunique()}\")\n\nprint(f\"LotConfig number of unique values (Train): {train['LotConfig'].nunique()}\")\nprint(f\"LotConfig number of unique values (Test): {test['LotConfig'].nunique()}\")","9af7a8a4":"train.hvplot.scatter(x='LotArea', y='SalePrice')","5a38b552":"train.hvplot.scatter(x='LotFrontage', y='SalePrice')","1b11f078":"train['LotShape'].value_counts().hvplot.bar()","18e82ea1":"test['LotShape'].value_counts().hvplot.bar()","a5a5dcc4":"train['LotConfig'].value_counts().hvplot.bar()","4911653e":"test['LotConfig'].value_counts().hvplot.bar()","f365b40d":"train.hvplot.scatter(x='GrLivArea', y='SalePrice')","9b38c0c4":"train.hvplot.scatter(x='TotalBsmtSF', y='SalePrice')","cc744c53":"train.hvplot.box(by='OverallQual', y='SalePrice')","bcbd9707":"plt.figure(figsize=(12, 10))\nsns.heatmap(train.corr(), vmax=.8, square=True)","70a64167":"cols = train.corr().nlargest(15, 'SalePrice')['SalePrice'].index\nplt.figure(figsize=(10, 8))\nsns.heatmap(train[cols].corr(), annot=True, vmax=.8, square=True)","2573de8f":"print(f\"Train dataset shape before removing: {train.shape}\")\nprint(f\"Test dataset shape before removing: {test.shape}\")\n\n# from 2 features high correlated, removing the less correlated with SalePrice\ntrain.drop(['GarageArea','1stFlrSF','TotRmsAbvGrd','2ndFlrSF'], axis=1, inplace=True)\ntest.drop(['GarageArea','1stFlrSF','TotRmsAbvGrd','2ndFlrSF'], axis=1, inplace=True)\n\n# removing outliers\n# train = train[train['GrLivArea'] < 4500]\ntrain.reset_index(drop=True, inplace=True) # Important to make optuna work\n\nprint(f\"Train dataset shape after removing: {train.shape}\")\nprint(f\"Test dataset shape after removing: {test.shape}\")","6126f6ad":"missing_features = [col for col in train.columns if train[col].isna().sum()!=0]\ncategorical_col = [col for col in train.columns if train[col].dtype == object]","13315686":"print(missing_features)\nprint(categorical_col)","30c223da":"X = train.drop(['Id', 'SalePrice'], axis=1)\ny = train['SalePrice']\ntest.drop('Id', axis=1, inplace=True)\n\nimputer = SimpleImputer(strategy='most_frequent')\nX = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\ntest = pd.DataFrame(imputer.transform(test), columns=test.columns)\n\nohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n# scaler = StandardScaler()\n\ncolumn_transformer = make_column_transformer(\n    (ohe, categorical_col),\n    remainder='passthrough'\n)\n\nX = column_transformer.fit_transform(X)\ntest = column_transformer.transform(test)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"Test shape: {test.shape}\")","c722db05":"def objective(trial):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33)\n\n    param_grid = {\n        'max_depth': trial.suggest_int('max_depth', 2, 15),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1500, 50),\n        'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n        'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n#         'learning_rate': trial.suggest_discrete_uniform('leaning_rate', 0.01, 1, 0.01)\n    }\n\n    reg = xgb.XGBRegressor(tree_method='gpu_hist', **param_grid)\n    # TODO: PRUNING\n    # pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-error')\n    reg.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)], eval_metric='rmse',\n            verbose=False)\n\n#     return np.sqrt(-cross_val_score(reg, X_valid, y_valid, scoring='neg_mean_squared_error').mean())\n    return mean_squared_error(y_valid, reg.predict(X_valid), squared=False)","ecefcd53":"train_time = 1 * 10 * 60 # h * m * s\nstudy = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='XGBRegressor')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","a500efaa":"xgb_params = trial.params\n# xgb_params['eta'] = 0.01\nxgb_params['tree_method'] = 'gpu_hist'\n\nn_splits = 10\ntest_preds = None\nkf_rmse = []\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X, y)):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric='rmse', verbose=False)\n       \n    valid_pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_pred, squared=False)\n    print(f'Fold {fold+1}\/{n_splits} RMSE: {rmse:.4f}')\n    kf_rmse.append(rmse)\n    \n    if test_preds is None:\n        test_preds = model.predict(test)\n    else:\n        test_preds += model.predict(test)\n\ntest_preds \/= n_splits\nprint(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')","1601f73b":"sample_submission['SalePrice'] = test_preds\nsample_submission.to_csv('submission.csv', index=False)","cfaf77b7":"# \ud83d\udce6 Data Pre-processing","47737a5d":"# \ud83c\udfe0 Kaggle Competition for House Prices \ud83d\udcb5: Advanced Regression Techniques \n***","43ed5b1b":"## `MSSubClass`, `MSZoning`","a4f7426f":"# \ud83e\udd16 Model Building & Hyperparameter Tuning","a711d05a":"# \ud83d\udce4 Import the Libraries","a71f2579":"# \ud83d\udcbe Load Data","d2e87228":"# \ud83d\udcca Exploratory Data Analysis (EDA)\n\nIn Data Analysis We will Analyze To Find out the below stuff\n1. Missing Values\n1. All The Numerical Variables\n1. Distribution of the Numerical Variables\n1. Categorical Variables\n1. Cardinality of Categorical Variables\n1. Outliers\n1. Relationship between independent and dependent feature(SalePrice)","3c8f8ebb":"## `LotArea`, `LotFrontage`, `LotShape`, `LotConfig`","7deca481":"Only 9 houses in  training data has a sales price more then 500000.","aafc72a6":"## Missing Values"}}