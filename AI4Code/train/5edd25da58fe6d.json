{"cell_type":{"f7a211ca":"code","7357df49":"code","32a5e9bb":"code","802a379f":"code","26ea0be6":"code","1487dea3":"code","2c499c4f":"code","c4172522":"code","e3bc4302":"code","1f963b23":"code","17fd93ca":"code","c7427c73":"code","be5908a0":"code","06fcea94":"code","35918455":"code","08492133":"code","77014f60":"code","cf02dc5d":"code","6dac17ec":"code","0488b60a":"code","e13eb50f":"code","e5dd7677":"code","ccb32d32":"code","9333a78f":"code","928b94f4":"code","80a64088":"code","749f9141":"code","26ae89a9":"code","be0aeb21":"markdown","327b25cb":"markdown","2a0a85e9":"markdown","3a09f0f8":"markdown","b3622e15":"markdown"},"source":{"f7a211ca":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras import optimizers, Sequential, Model\nimport keras","7357df49":"train_df = pd.read_csv('..\/input\/beyond-analysis\/train.csv')\ntrain_df.drop(columns = ['CATEGORY_1', 'CATEGORY_2'], axis=0, inplace = True)\n\nusers = train_df['UNIQUE_IDENTIFIER'].unique()\nusers_train, users_test = train_test_split(users, test_size=0.2, shuffle=True, random_state=42)","32a5e9bb":"import json\nf = open('..\/input\/techniche-lstm-version-3-data\/x_train.json')\nX = json.load(f)\nf = open('..\/input\/techniche-lstm-version-3-data\/y_train.json')\nY = json.load(f)","802a379f":"train_df[train_df['UNIQUE_IDENTIFIER']== users[0]]","26ea0be6":"def pad(users, X=X, Y=Y, pad_size=30):\n    final_x = []\n    final_y1 = []\n    final_y2 = []\n    for user_id in tqdm(users):\n        x = np.array(X[str(user_id)]).T\n        y = np.array(Y[str(user_id)]).T\n\n        pad_array = np.zeros((x.shape[0], pad_size - x.shape[1]))\n        x = np.concatenate((x, pad_array), axis=1)\n        final_x.append(x)\n        final_y1.append(y[0])\n        final_y2.append(y[1])\n    final_x, final_y1, final_y2 = np.array(final_x), np.array(final_y1), np.array(final_y2)\n    final_x = np.transpose(final_x, [0, 2, 1])\n    return final_x, final_y1, final_y2","1487dea3":"train_x, train_y1, train_y2 = pad(users_train)\ntest_x, test_y1, test_y2 = pad(users_test)\n#  Remove columns 5, 9, 10, 11, 12. 15, 16, 17, 18, \n","2c499c4f":"train_x.shape, test_x.shape","c4172522":"cols = list(train_df.columns)\ni = 2\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 3\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 4\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 5\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 6\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 7\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 8\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 9\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 10\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 11\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 12\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 13\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 14\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 15\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 16\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 17\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 18\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 19\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 20\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))","e3bc4302":"a = train_x[..., :5]\nb = train_x[..., 6:9]\nc = train_x[..., 13:15]\ntrain_x_cat = np.concatenate((a, b, c), axis=-1)\ntrain_x_cat.shape","1f963b23":"a = test_x[..., :5]\nb = test_x[..., 6:9]\nc = test_x[..., 13:15]\ntest_x_cat = np.concatenate((a, b, c), axis=-1)\ntest_x_cat.shape","17fd93ca":"serie_size =  train_x_cat.shape[1] # 30\nn_features =  train_x_cat.shape[2] # 10\n\nlstm_model_y1 = Sequential()\nlstm_model_y1.add(L.LSTM(10, input_shape=(serie_size, n_features), return_sequences=True))\nlstm_model_y1.add(L.LSTM(6, activation='relu', return_sequences=True))\nlstm_model_y1.add(L.LSTM(1, activation='relu'))\nlstm_model_y1.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y1.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y1.add(L.Dense(1))\nlstm_model_y1.summary()","c7427c73":"epochs = 2\nbatch = 128*16*4*4\nlr = 0.001\n\nadam = optimizers.Adam(lr)\nlstm_model_y1.compile(loss='mse', optimizer=adam)","be5908a0":"lstm_history = lstm_model_y1.fit(train_x_cat, train_y1,\n                              validation_data=(test_x_cat, test_y1), \n                              batch_size=batch, \n                              epochs=epochs,\n                              verbose=1)","06fcea94":"serie_size =  train_x_cat.shape[1] # 30\nn_features =  train_x_cat.shape[2] # 10\n\nlstm_model_y2 = Sequential()\nlstm_model_y2.add(L.LSTM(10, input_shape=(serie_size, n_features), return_sequences=True))\nlstm_model_y2.add(L.LSTM(6, activation='relu', return_sequences=True))\nlstm_model_y2.add(L.LSTM(1, activation='relu', return_sequences=True))\nlstm_model_y2.add(L.Flatten())\nlstm_model_y2.add(L.Dense(100, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y2.add(L.Dense(100, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y2.add(L.Dense(50, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y2.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y2.add(L.Dense(1))\nlstm_model_y2.summary()","35918455":"epochs = 2\nbatch = 128*16*4*4\n\nlr = 0.0001\n\nadam = optimizers.Adam(lr)\nlstm_model_y2.compile(loss='mse', optimizer=adam)","08492133":"lstm_history = lstm_model_y2.fit(train_x_cat, train_y2,\n                              validation_data=(test_x_cat, test_y2), \n                              batch_size=batch, \n                              epochs=epochs,\n                              verbose=1)","77014f60":"lstm_model_y1 = keras.models.load_model(\"..\/input\/techniche-beyond-analysis-models\/1_models\/Y1_model.h5\")\nlstm_model_y2 = keras.models.load_model(\"..\/input\/techniche-beyond-analysis-models\/1_models\/Y2_model.h5\")","cf02dc5d":"lstm_model_y1.predict(train_x_cat[:2]), train_y2[:2]","6dac17ec":"lstm_model_y2.predict(train_x_cat[:2]), train_y2[:2]","0488b60a":"import json\nf = open('..\/input\/techniche-lstm-version-3-data\/x_test.json')\nX_test = json.load(f)","e13eb50f":"def test_pad(users, X=X_test, pad_size=30):\n    final_x = []\n    for user_id in tqdm(users):\n        x = np.array(X[str(user_id)]).T\n        pad_array = np.zeros((x.shape[0], pad_size - x.shape[1]))\n        x = np.concatenate((x, pad_array), axis=1)\n        final_x.append(x)\n    final_x = np.array(final_x)\n    final_x = np.transpose(final_x, [0, 2, 1])\n    return final_x","e5dd7677":"test_df = pd.read_csv('..\/input\/beyond-analysis\/test.csv')\ntest_df.drop(columns = ['CATEGORY_1', 'CATEGORY_2'], axis=0, inplace = True)\n\nusers_sub = test_df['UNIQUE_IDENTIFIER'].unique()","ccb32d32":"sub_x = test_pad(users_sub)","9333a78f":"sub_x.shape","928b94f4":"a = sub_x[..., :5]\nb = sub_x[..., 6:9]\nc = sub_x[..., 13:15]\nsub_x_cat = np.concatenate((a, b, c), axis=-1)\nsub_x_cat.shape","80a64088":"pred_y1 = lstm_model_y1.predict(sub_x_cat)\nprint(pred_y1)\npred_y1.shape","749f9141":"pred_y2 = lstm_model_y2.predict(sub_x_cat)\nprint(pred_y2)\npred_y2.shape","26ae89a9":"# sub = pd.DataFrame({'UNIQUE_IDENTIFIER': users_sub.astype(int), \n#                    'Y1': pred_y1[:, 0], 'Y2': pred_y2[:, 0]})\n\n# sub.to_csv('sub_checking.csv', index=False)\nsub = pd.DataFrame({'UNIQUE_IDENTIFIER': users_sub.astype(int), \n                   'Y2': pred_y2[:, 0]})\n\nsub.to_csv('sub_checking.csv', index=False)","be0aeb21":"### Loading our pretrained models","327b25cb":"# Y1","2a0a85e9":"We removed some features having more than 80% zeros. Also, we do not have UNIQUE_IDENTIFIER and SEQUENCE_NO in our X and Y vectors for each user.","3a09f0f8":"# Y2","b3622e15":"# Prediction"}}