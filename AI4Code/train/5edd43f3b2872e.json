{"cell_type":{"0a4cf9f4":"code","3e868e5b":"code","794f47c7":"code","077bc37a":"code","5edd883a":"code","b465d9c0":"code","80e082e0":"code","6e2ff3b4":"code","35d3ec3e":"code","7e45173d":"code","81fed9ce":"code","9ed7c969":"code","3b2a14c9":"code","6f4ff4f9":"code","9e64aa6f":"code","9ff7f537":"code","5fffcb42":"code","eeff5dc5":"code","503a2b53":"code","6f540770":"code","6c07498d":"code","11d86dab":"code","3ee9e54a":"code","6e5b9f63":"code","55cceaca":"code","2137be7a":"code","c407b99d":"code","265a5d52":"code","a8013138":"markdown","5e63ff1e":"markdown","c28b16cb":"markdown","c38ad644":"markdown","6b895463":"markdown","21ca204a":"markdown","5d9eae7d":"markdown","b893b2e8":"markdown","d02f9a64":"markdown","07d10e9d":"markdown","1e6c4a5b":"markdown","fc313961":"markdown","f94f356c":"markdown","7ec80b79":"markdown","4ce864df":"markdown","e9257459":"markdown","edb3a469":"markdown","d916dbd5":"markdown"},"source":{"0a4cf9f4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import GradientBoostingClassifier \n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import metrics\nimport seaborn as sns\n\n%matplotlib inline\n","3e868e5b":"df = pd.read_csv('..\/input\/churn-in-telecoms-dataset\/bigml_59c28831336c6604c800002a.csv')\ndf.head(3)","794f47c7":"print(df.dtypes)","077bc37a":"df.info()","5edd883a":"df.describe(include=['O'])","b465d9c0":"df.groupby('churn')['phone number'].count()","80e082e0":"y_True = df[\"churn\"][df[\"churn\"] == True]\nprint (\"Churn Percentage = \"+str( (y_True.shape[0] \/ df[\"churn\"].shape[0]) * 100 ))","6e2ff3b4":"print('\u041f\u0440\u043e\u0446\u0435\u043d\u0442 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0445 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u0433\u0440\u0443\u043f\u043f\u0443 \"\u043e\u0442\u0442\u043e\u043a\" 0.14')\ny = df[\"churn\"].value_counts()\nsns.barplot(y.index, y.values)","35d3ec3e":"df.drop([\"phone number\"], axis = 1, inplace=True)\n\nlabel_encoder = preprocessing.LabelEncoder()\n\ndf['state'] = label_encoder.fit_transform(df['state'])\ndf['international plan'] = label_encoder.fit_transform(df['international plan'])\ndf['voice mail plan'] = label_encoder.fit_transform(df['voice mail plan'])\ndf['churn'] = label_encoder.fit_transform(df['churn'])\n","7e45173d":"df.hist()","81fed9ce":"# calculate the correlation matrix\ncorr = df.corr()\n\n# plot the heatmap\nfig = plt.figure(figsize=(5,4))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n            linewidths=.75)","9ed7c969":"#we will normalize our data so the prediction on all features will be at the same scale\nX = df.iloc[:,0:19].values\ny = df.iloc[:,19].values\n#nurmalize the data\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\ndfNorm = pd.DataFrame(X_std, index=df.index, columns=df.columns[0:19])\n# # add non-feature target column to dataframe\ndfNorm['churn'] = df['churn']\ndfNorm.head(10)\n\nX = dfNorm.iloc[:,0:19].values\ny = dfNorm.iloc[:,19].values","3b2a14c9":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.15 , random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape , y_test.shape","6f4ff4f9":"# help func 1\nresults_test = {}\nresults_train = {}\ndef prdict_date(algo_name,X_train,y_train,X_test,y_test,verbose=0):\n    algo_name.fit(X_train, y_train)\n    Y_pred = algo_name.predict(X_test)\n    acc_train = round(algo_name.score(X_train, y_train) * 100, 2)\n    acc_val = round(algo_name.score(X_test, y_test) * 100, 2)\n    results_test[str(algo_name)[0:str(algo_name).find('(')]] = acc_val\n    results_train[str(algo_name)[0:str(algo_name).find('(')]] = acc_train\n    if verbose ==0:\n        print(\"acc train: \" + str(acc_train))\n        print(\"acc test: \"+ str(acc_val))\n    else:\n        return Y_pred\n\n#help func 2\ndef conf(algo_name,X_test, y_test):\n    y_pred = algo_name.predict(X_test)\n    forest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0])\n    sns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [\"1\", \"0\"] , yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.title(str(algo_name)[0:str(algo_name).find('(')])","9e64aa6f":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0, k_neighbors=4)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)","9ff7f537":"#data labels before SMOTE:\nimport collections\ncollections.Counter(y_train)","5fffcb42":"#after SMOTE:\nimport collections\ncollections.Counter(y_train_res)","eeff5dc5":"random_forest = RandomForestClassifier(n_estimators=80, random_state=0, criterion='entropy')\nprdict_date(random_forest,X_train_res,y_train_res,X_test,y_test)\nprint(classification_report(y_test, random_forest.predict(X_test)))\nconf(random_forest,X_test, y_test)","503a2b53":"# Train: Gradient Boosting\ngbc = GradientBoostingClassifier(loss='deviance', learning_rate=0.2, n_estimators=200 , max_depth=6)\nprdict_date(gbc,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, gbc.predict(X_test)))\nconf(gbc,X_test, y_test)","6f540770":"# linear svm:\n\nsvm = SVC(kernel='linear', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","6c07498d":"# linear rbf:\nsvm = SVC(kernel='rbf', probability=True, gamma=10)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","11d86dab":"#  poly svm:\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","3ee9e54a":"# Train: SVM\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, svm.predict(X_test)))\nconf(svm,X_test, y_test)","6e5b9f63":"#we will try to find witch K is the best on our data\n# first, we will look which give us the best predictions on the train:\nfrom sklearn import model_selection\n\n#Neighbors\nneighbors = [x for x in list(range(1,20)) if x % 2 == 0]\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\n#Perform 10-fold cross validation on training set for odd values of k:\nseed=0\n\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    scores = model_selection.cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    #print(\"k=%d %0.2f (+\/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\n\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint(( \"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k])))\n\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","55cceaca":"# then on the test:\ncv_preds = []\n\n#Perform 10-fold cross validation on testing set for odd values of k\nseed=0\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    preds = model_selection.cross_val_predict(knn, X_test, y_test, cv=kfold)\n    cv_preds.append(metrics.accuracy_score(y_test, preds)*100)\n    #print(\"k=%d %0.2f\" % (k_value, 100*metrics.accuracy_score(test_y, preds)))\n\noptimal_k = neighbors[cv_preds.index(max(cv_preds))]\nprint(\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_preds[optimal_k]))\n\nplt.plot(neighbors, cv_preds)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Test Accuracy')\nplt.show()","2137be7a":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 2)\nprdict_date(knn,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, knn.predict(X_test)))\nconf(knn,X_test, y_test)","c407b99d":"logr = LogisticRegression()\nprdict_date(logr,X_train,y_train,X_test,y_test)\n\nprint(classification_report(y_test, logr.predict(X_test)))\nconf(logr,X_test, y_test)","265a5d52":"df_test =pd.DataFrame(list(results_test.items()),\n                      columns=['algo_name','acc_test'])\ndf_train =pd.DataFrame(list(results_train.items()),\n                      columns=['algo_name','acc_train'])\ndf_results = df_test.join(df_train.set_index('algo_name'), on='algo_name')\ndf_results.sort_values('acc_test',ascending=False)","a8013138":"# Gradient Boosting","5e63ff1e":"We want to check how many nulls, int, objects we have in the data to do a better preprocessing","c28b16cb":"from the data description, we can see that phone number is unique - therefor it not provides us information we can learn. we will drop phone number column and enumerate all the categorial objects columns. enumeration advantage is for easier use of the algorithms witch often accept only numbers","c38ad644":"we enumerate with encoder-decoder to have a fast way to switch between the two if needed","6b895463":"# Predictions","21ca204a":"# K Neighbors Classifier","5d9eae7d":"* Reading the data into Pandas DataFrame.\n* Describing the data.\n","b893b2e8":"we can see strong correlation between the features:\n\ntotal day\/eve\/night\/intl charge - total day\/eve\/night\/intl minutes we can assume they charge per call time.\n\nanother correlation is between voice mail plan and number vmail mail massages.\n\ncorrelation with churn:\n\ninternational plan total day minutes total day charge customers service call","d02f9a64":"# smote\nwe do sentetic data only on the train: SMOTE creates synthetic observations of the minority class (churn) by:\n\nFinding the k-nearest-neighbors for minority class observations (finding similar observations) Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.\n\nwe use smote only on the training data set","07d10e9d":"# \u041c\u0435\u0442\u043e\u0434 \u043e\u043f\u043e\u0440\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 (SVM)\nwe can\u2019t know ahead witch type of kernel will predict the best results- therefor we tried multiple different kernels types.","1e6c4a5b":"See types of dataset","fc313961":"# Results Predictions","f94f356c":"Unfortunately, there are more churn = False then True. we will need to balance the data before making predictions. for balancing we will use synthetic data from SMOTE: Synthetic Minority Over-sampling Technique","7ec80b79":"# Logistic Regression","4ce864df":" We can see than not all the columns are numerical, we will need to dig dipper for the object type","e9257459":"Splitting the dataset into Train and Test data according to the dimensions needed","edb3a469":"# 1.RandomForestClassifier","d916dbd5":"We can see that there are no null columns. It is wery important where we learn data, because most of the algorithms don\u2019t know how to deal with nulls so they have to be replaced"}}