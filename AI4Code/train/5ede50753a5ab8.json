{"cell_type":{"7524e58c":"code","47226e33":"code","acefd486":"code","6dea78d9":"code","685ec796":"code","96f07be7":"code","ca12255b":"code","ea4ada30":"code","3a14c089":"code","cec75ef5":"code","88e66378":"code","8a59274c":"code","bd2b642f":"code","0415142b":"code","d5e4dea9":"code","85a5d712":"code","1d05e83a":"code","68f6a463":"code","beb20e42":"code","3e19518e":"code","2a1d7198":"code","11ce5130":"code","bbcbacaf":"code","1dd5caf7":"code","418505ff":"code","39f19e6c":"code","54e45e42":"code","9666e832":"code","426fca99":"code","1841d1c4":"code","2366db1b":"markdown","2533ef9e":"markdown","2180ac1d":"markdown","ffb97d32":"markdown","f963e5d6":"markdown","e50c3484":"markdown","d3988dcb":"markdown","985f3057":"markdown","798c8e11":"markdown","12e6a2f9":"markdown","f045209d":"markdown","00774795":"markdown","23866af0":"markdown","5334c16e":"markdown","eb02c5b1":"markdown","74304128":"markdown","1ab3a0e8":"markdown","a3e01905":"markdown","6f6c3731":"markdown","120c1b7a":"markdown","07ed7c24":"markdown","528a45c3":"markdown","681c2fb7":"markdown","9459a972":"markdown","81302ea0":"markdown"},"source":{"7524e58c":"import numpy as np \nimport pandas as pd\nfrom sklearn import *\nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import Pool,CatBoostRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport datetime\n\nsns.set_style(\"whitegrid\")\n\nfrom sklearn.model_selection import KFold\n\n#Constants\nROW_PER_BATCH = 500000","47226e33":"#Loading data\ntrain = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv')\n\n\nprint('Shape of train is ',train.shape)\nprint('Shape of test is ',test.shape)","acefd486":"train['batch'] = 0\n\nfor i in range(0, train.shape[0]\/\/ROW_PER_BATCH):\n    train.iloc[i * ROW_PER_BATCH: (i+1) * ROW_PER_BATCH,3] = i","6dea78d9":"plt.figure(figsize=(26, 22))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\nfor i, b in enumerate(train['batch'].unique()):\n    plt.subplot(5, 2, i + 1)\n    plt.plot(train.loc[train['batch'] == b, ['signal']], color='b')\n    plt.title(f'Batch: {b}')\n    plt.plot(train.loc[train['batch'] == b, ['open_channels']], color='r')\n    plt.legend(['signal', 'open_channels'], loc=(0.875, 0.9))\n    plt.grid(False)","685ec796":"plt.figure(figsize=(26, 22))\ncolors = ['red','red','blue','pink','gold','brown','blue','pink','brown','gold']\nfor i, b in enumerate(train['batch'].unique()):\n    plt.subplot(5, 2, i + 1)\n    train.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH]['open_channels'].value_counts().plot(kind='bar',color= colors[i])\n    plt.title(f'Batch: {b}')","96f07be7":"df = pd.DataFrame()\nplt.figure(figsize=(26, 22))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\ncolors = ['red','red','blue','pink','gold','brown','blue','pink','brown','gold']\nfor i in train['batch'].unique():\n    df[ f'batch:{i}' ] = train.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH].reset_index().signal \n    plt.subplot(5, 2, i + 1)\n    \n    sns.distplot(df[f'batch:{i}'], color= colors[i]).set_title(f\"(Signal) median: {df[f'batch:{i}'].median():.2f}\")\n\n    \n","ca12255b":"plt.figure(figsize=(16,10))\n\nsns.boxplot(x=\"variable\",y=\"value\",data=pd.melt(df))","ea4ada30":"test['batch'] = 0\n\nfor i in range(0, test.shape[0]\/\/ROW_PER_BATCH):\n    test.iloc[i * ROW_PER_BATCH: (i+1) * ROW_PER_BATCH,2] = i","3a14c089":"mean_signal = []\ndf = pd.DataFrame()\ntemplis = {}\nplt.figure(figsize=(26, 22))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\ncolors = ['red','red','blue','pink','gold','brown','blue','pink','brown','gold']\nfor i in test['batch'].unique():\n    df[ f'batch:{i}' ] = test.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH].reset_index().signal \n    plt.subplot(5, 2, i + 1)\n    sns.distplot(df[f'batch:{i}'], color= colors[i]).set_title(f\"(Signal) median: {df[f'batch:{i}'].median():.2f}\")","cec75ef5":"plt.figure(figsize=(8,5))\nsns.boxplot(x=\"variable\",y=\"value\",data=pd.melt(df))","88e66378":"train['type'] = 0\nfor i in range(train['batch'].nunique()):\n    median = train.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH].signal.median()\n    if (median < 0):\n        train.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, train.columns.get_loc('type')] = 0\n    else:\n        train.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, train.columns.get_loc('type')] = 1\n        \ntest['type'] = 0\n\nROW_PER_BATCH = 100000\n\nfor i in range(test['batch'].nunique()):\n    median = test.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH].signal.median()\n    if (median < 0):\n        test.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, test.columns.get_loc('type')] = 0\n    else:\n        test.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, test.columns.get_loc('type')] = 1    ","8a59274c":"plt.figure(figsize=(20,5))\nplt.plot( train.signal[500000:1000000][::100] )\nplt.show()","bd2b642f":"a = 500000; b = a *2\nprint( 'Before: mean: {} std: {} median: {}'.format( train.signal[a:b].mean(), train.signal[a:b].std(),train.signal[a:b].median() ) )\n\na=500000; b=600000\ntrain['signal_undrifted'] = train.signal\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal[a:b].values - 3*(train.time.values[a:b] - 50)\/10.\n\na = 500000; b = a *2\nprint( 'After: mean: {} std: {} median: {}'.format( train.signal_undrifted[a:b].mean(), train.signal_undrifted[a:b].std(),train.signal_undrifted[a:b].median() ) )","0415142b":"plt.figure(figsize=(20,6))\nsns.distplot(train.signal[500000:1000000],color='r')\nsns.distplot(train.signal_undrifted[500000:1000000],color='g' ).set(xlabel=\"Signal\")\nplt.legend(labels=['Original Signal','Undrifted Signal'])\n","d5e4dea9":"\ndef f(x,low,high,mid): return -((-low+high)\/625)*(x-mid)**2+high -low\n\n# CLEAN TRAIN BATCH 7\nbatch = 7; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,-1.817,3.186,325)\n# CLEAN TRAIN BATCH 8\nbatch = 8; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,-0.094,4.936,375)\n# CLEAN TRAIN BATCH 9\nbatch = 9; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,1.715,6.689,425)\n# CLEAN TRAIN BATCH 10\nbatch = 10; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,3.361,8.45,475)","85a5d712":"plt.figure(figsize=(20,5))\nsns.lineplot(train.time[::1000],train.signal[::2000],color='r').set_title('Training Batches 7-10 with Parabolic Drift')\n#plt.figure(figsize=(20,5))\ng = sns.lineplot(train.time[::1000],train.signal_undrifted[::2000],color='g').set_title('Training Batches 7-10 without Parabolic Drift')\nplt.legend(title='Train Data',loc='upper left', labels=['Original Signal', 'UnDrifted Signal'])\nplt.show(g)\n\n","1d05e83a":"test['signal_undrifted'] = test.signal\n\n# REMOVE BATCH 1 DRIFT\nstart=500\na = 0; b = 100000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)\/10.\nstart=510\na = 100000; b = 200000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)\/10.\nstart=540\na = 400000; b = 500000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)\/10.\n\n# REMOVE BATCH 2 DRIFT\nstart=560\na = 600000; b = 700000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)\/10.\nstart=570\na = 700000; b = 800000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)\/10.\nstart=580\na = 800000; b = 900000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)\/10.\n\n# REMOVE BATCH 3 DRIFT\ndef f(x):\n    return -(0.00788)*(x-625)**2+2.345 +2.58\na = 1000000; b = 1500000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - f(test.time[a:b].values)","68f6a463":"plt.figure(figsize=(20,5))\nsns.lineplot(test.time[::1000],test.signal[::1000],color='r').set_title('Training Batches 7-10 with Parabolic Drift')\n#plt.figure(figsize=(20,5))\ng = sns.lineplot(test.time[::1000],test.signal_undrifted[::1000],color='g').set_title('Training Batches 7-10 without Parabolic Drift')\nplt.legend(title='Test Data',loc='upper right', labels=['Original Signal', 'UnDrifted Signal'])\nplt.show(g)\n\n","beb20e42":"%%time\n\ndef features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 50_000\n    df['batch_index'] = df.index  - (df.batch * 50_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 5_000\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal_undrifted'].mean()\n        d['median'+c] = df.groupby([c])['signal_undrifted'].median()\n        d['max'+c] = df.groupby([c])['signal_undrifted'].max()\n        d['min'+c] = df.groupby([c])['signal_undrifted'].min()\n        d['std'+c] = df.groupby([c])['signal_undrifted'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n        df['range'+c] = df['max'+c] - df['min'+c]\n        df['maxtomin'+c] = df['max'+c] \/ df['min'+c]\n        df['abs_avg'+c] = (df['abs_min'+c] + df['abs_max'+c]) \/ 2\n    \n    #add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal_undrifted'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal_undrifted'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n\n    # add shifts_2 - https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-confmatrix\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal_undrifted'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal_undrifted'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan \n        \n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal_undrifted', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal_undrifted']\n        \n    return df\n\ntrain = features(train)\ntest = features(test)","3e19518e":"def f1_score_calc(y_true, y_pred):\n    return f1_score(y_true, y_pred, average=\"macro\")\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('KaggleMetric', score, True)\n\n\ndef train_model_classification(X, X_test, y, params, model_type='lgb', eval_metric='f1score',\n                               columns=None, plot_feature_importance=False, model=None,\n                               verbose=50, early_stopping_rounds=200, n_estimators=2000):\n\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {\n                    'f1score': {'lgb_metric_name': lgb_Metric,}\n                   }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X) )\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test)))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    '''for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]'''\n            \n    if True:        \n        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state=7)    \n            \n        if model_type == 'lgb':\n            #model = lgb.LGBMClassifier(**params, n_estimators=n_estimators)\n            #model.fit(X_train, y_train, \n            #        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n            #       verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            model = lgb.train(params, lgb.Dataset(X_train, y_train),\n                              n_estimators,  lgb.Dataset(X_valid, y_valid),\n                              verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds, feval=lgb_Metric)\n            \n            \n            preds = model.predict(X, num_iteration=model.best_iteration) #model.predict(X_valid) \n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_set = xgb.DMatrix(X_train, y_train)\n            val_set = xgb.DMatrix(X_valid, y_valid)\n            model = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                                     verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            preds = model.predict(xgb.DMatrix(X)) \n\n            y_pred = model.predict(xgb.DMatrix(X_test))\n            \n\n        if model_type == 'cat':\n            # Initialize CatBoostRegressor\n            model = CatBoostRegressor(params)\n            # Fit model\n            model.fit(X_train, y_train)\n            # Get predictions\n            y_pred_valid = np.round(np.clip(preds, 0, 10)).astype(int)\n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n\n \n        oof = preds\n        \n        scores.append(f1_score_calc(y, np.round(np.clip(preds,0,10)).astype(int) ) )\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    #prediction \/= folds.n_splits\n    \n    print('FINAL score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    result_dict['model'] = model\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] \/= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict","2a1d7198":"#Let's uses 'signal_undrifted' instead of 'signal'\ngood_columns = [c for c in train.columns if c not in ['time', 'signal','open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n\nX = train[good_columns].copy()\ny = train['open_channels']\nX_test = test[good_columns].copy()","11ce5130":"del train, test","bbcbacaf":"params_xgb = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse'}\n\nresult_dict_xgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_xgb, model_type='xgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250)","1dd5caf7":"#params_cat = {'task_type': \"CPU\",'iterations':1000,'learning_rate':0.1,'random_seed': 42,'depth':2}\n\n#result_dict_cat = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_cat, model_type='cat', eval_metric='f1score', plot_feature_importance=False)","418505ff":"params_lgb = {'learning_rate': 0.1, 'max_depth': 7, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1}\n\nresult_dict_lgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_lgb, model_type='lgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250, n_estimators=3000)","39f19e6c":"booster = result_dict_lgb['model']\n\nfi = pd.DataFrame()\nfi['importance'] = booster.feature_importance(importance_type='gain')\nfi['feature'] = booster.feature_name()\n\nbest_features = fi.sort_values(by='importance', ascending=False)[:20]\n\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features);\nplt.title('LGB Features (avg over folds)');","54e45e42":"def weight_opt(oof_lgb, oof_xgb, y_true):\n    weight_lgb = np.inf\n    best_f1 = np.inf\n    \n    for i in np.arange(0, 1.01, 0.10):\n        combined_oof = i * oof_lgb + (1-i) * oof_xgb\n        blend = np.round(np.clip(combined_oof,0,10)).astype(int) \n        f1_blend = metrics.f1_score(y_true, blend, average = 'macro' )\n        if np.mean(f1_blend) < best_f1:\n            best_f1 = np.mean(f1_blend)\n            weight_lgb = round(i, 2)\n            \n        print(str(round(i, 2)) + ' : mean F1 (Blend) is ', round(np.mean(f1_blend), 6))\n        \n    print('-'*36)\n    print('Best weight for LGB: ', weight_lgb)\n    print('Best weight for XGB: ', round(1-weight_lgb, 2))\n    print('Best mean F1 (Blend): ', round(best_f1, 6))\n    \n    return weight_lgb, round(1-weight_lgb, 2)","9666e832":"weight_lgb, weight_xgb = weight_opt(result_dict_lgb['oof'], result_dict_xgb['oof'], y[0:500000*8-1])","426fca99":"preds_ensemble = 0.50 * result_dict_lgb['prediction'] + 0.50 * result_dict_xgb['prediction']","1841d1c4":"sub = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsub['open_channels'] =  np.array(np.round(preds_ensemble,0), np.int) \n\nsub.to_csv('submission_unshifted_70p.csv', index=False, float_format='%.4f')\nsub.head(10)","2366db1b":"As we can see in figure below, the drift removal makes signal closer to a normal distribution.","2533ef9e":"### Checking if this changes the data distribution.","2180ac1d":"The model can score 0.938 on LB without further optimization. Maybe with some GridSearch for parameters tunning or more features engineering, it's possible to get to 0.94","ffb97d32":"> **Nota**: If you fork my work, please, upvote and give the credits \u263a <br>\nMade by[@AdrianoAvelar](https:\/\/www.kaggle.com\/adrianoavelar) <br>\nFork and Code.\n<h3 style=\"color:red\"> If this Kernel Helped You! Please UP VOTE! \ud83d\ude01 <\/h3>","f963e5d6":"While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.","e50c3484":"<a id=\"id5\"><\/a> <br> \n# **6. Model**\n\n> Simple Model: It's can be improved.","d3988dcb":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/18045\/logos\/header.png?t=2020-02-21-18-37-17\" width=1900px height=400px \/>","985f3057":"<a id=\"id5\"><\/a><br> \n# **7. Results**","798c8e11":"# This kernel is a modified version of the '\ud83d\udca53 Simple Ideas - LB 0.938' kernel from [adrianoavelar](https:\/\/www.kaggle.com\/adrianoavelar) \n# I have added an ensemble of models to improve CV\/LB\n\n# v13: Added Shift +- 2 features from https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-confmatrix","12e6a2f9":"# Ion Switching\n## Drift Removal + Feature Engineering + Overfitting (Train)\n\n<h3 style=\"color:red\"> If this Kernel Helped You! Please UP VOTE! \ud83d\ude01 <\/h3>\n\n<h3> Kernel description: <\/h3>\n\n\nWhat I love most about Kaggle is the sharing of solutions. For me, it is far from a competition. Kaggle is the true meaning of research, that is, the combination of several ideas from different people to find the solution to a problem.\n\nThis Kernel uses ideas from me and other authors. The main ideas are the removal of the signal Drift, presented by @cdeotte, Feature Engineering from the work of @jazivxt and implementation of @ kmat2019 of a [discussion](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/133142) that proved that the first 30% of the test data is used to calculate public score. Below, the link to the mentioned notebooks.\n\n* Drift Removal: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\n* Feature Engineering: https:\/\/www.kaggle.com\/jazivxt\/physically-possible\n* Overfitting (Train): https:\/\/www.kaggle.com\/kmat2019\/train-test-similarity-analysis\n\n","f045209d":"### Findings\n\n1) batch 0: It looks like the most stable data part. Where you can clearly see the signs and open channels for correspondence. In this graph, open_channels are binary events with low probability<br>\n2) batch 1: There also appears to be a certain correlation, even though there is a shift in signal levels after the 600k time. In this graph, open_channels are binary events with LOW probability.<br>\n3) batch 2 and 6: open_channels are binary events with HIGH probability<br>\n**4) A Drift was added to batches 6, 7, 8, 9. And the beginning of batch 1.**","00774795":"The idea here is identify some patterns and then create groups from these patterns to split the data. After that, we can make diferente models for each of patterns. For now, I'm not using this idea, but I will later.","23866af0":"<a id=\"id2\"><\/a> <br> \n# **2. Get the Data (Collect \/ Obtain):** \n> Loading all libs and necessary datasets","5334c16e":"## Remove Drift from Training Data\nfrom: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\n","eb02c5b1":"<a id=\"id1\"><\/a> <br> \n# **1. Problem Definition:** \n<img src=\"https:\/\/www.news-medical.net\/image.axd?picture=2018%2F10%2Fshutterstock_480412786.jpg\" \/>\n\nMany diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants\nIf scientists could better study ion channels, it could have a far-reaching impact.\n\nIon channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction.\n\nIn this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals.","74304128":"### Removing Drift from Batch 7-10","1ab3a0e8":"<a id=\"ref\"><\/a>\n# **8. References** \n\n[1] Deep-Channel uses deep neural networks to detect\nsingle-molecule events from patch-clamp data https:\/\/www.nature.com\/articles\/s42003-019-0729-3.pdf\n\n[2] The Patch Clamp Method: https:\/\/www.youtube.com\/watch?v=mVbkSD5FHOw\n\n[3] Electophysiology: Patch clamp method https:\/\/www.youtube.com\/watch?v=CvfXjGVNbmw\n\n[4] The Action Potential https:\/\/www.youtube.com\/watch?v=HYLyhXRp298\n\n[5] https:\/\/www.kaggle.com\/pestipeti\/eda-ion-switching\n\n[6] https:\/\/www.kaggle.com\/kmat2019\/u-net-1d-cnn-with-keras\n\n[7] https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\n\n[8] [https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-confmatrix](https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-confmatrix)","a3e01905":"## Remove Drift from Test Data","6f6c3731":"## Interesting Pattern \n\nIt can be seen a patern batches, for every batch there is another with almost identical target distribution, same color here.","120c1b7a":"# Test Data Analisys","07ed7c24":"<a id=\"id5\"><\/a> <br> \n# **5. Feature Engineering** ","528a45c3":"<a id=\"id3\"><\/a> <br> \n# **3. Understand the Data** ","681c2fb7":"<a id=\"id7\"><\/a> <br> \n# **8. Submittion** ","9459a972":"### Chacing distribution\n\nJust checking the distribution to seek for ","81302ea0":"# Table of Contents:\n\n**1. [Problem Definition](#id1)** <br>\n**2. [Get the Data (Collect \/ Obtain)](#id2)** <br>\n**3. [Load the Dataset](#id3)** <br>\n**4. [Data Pre-processing](#id4)** <br>\n**5. [Model](#id5)** <br>\n**6. [Visualization and Analysis of Results](#id6)** <br>\n**7. [Submittion](#id7)** <br>\n**8. [References](#ref)** <br>"}}