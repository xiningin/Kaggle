{"cell_type":{"7d922430":"code","9aa1a6d9":"code","aa176e2c":"code","c435642f":"code","7b6def1a":"code","1bc63c65":"code","37d5d6e6":"code","70a51fee":"code","f68fdd7f":"code","9580b113":"code","f00f93db":"code","0411f6c0":"code","ed1142d0":"code","0c431f8c":"code","aa00f55d":"code","e0f582c1":"code","49510b70":"code","24e2b410":"code","21d0e121":"code","fc2065fb":"code","5ed18a18":"code","0f8ca8a9":"code","0b49cf59":"code","140e8162":"code","a0113687":"code","7d0025de":"code","516a5582":"code","437b3a07":"code","217daf36":"code","4ad84cfb":"code","0140cab3":"code","dcce04cd":"code","6722b50d":"code","dc311704":"code","1c6ab5ec":"code","bd3b3d42":"code","65f47064":"code","f61cd9b9":"code","27233920":"code","7d2945eb":"code","0802777c":"code","6c99339d":"code","59902cc5":"code","691818cb":"code","c547796a":"code","d9cb1326":"code","0db1f2f6":"code","b3e539f4":"markdown","1cb515ca":"markdown","2987d3f0":"markdown","f06c02ea":"markdown","5e2ce8d6":"markdown","23bf89bd":"markdown","9c2d19a0":"markdown","073fa264":"markdown","eb961626":"markdown","8c45b1e6":"markdown","1411551d":"markdown","ca40fb25":"markdown","0e433a27":"markdown","80c99a26":"markdown","c3442010":"markdown","c64a403a":"markdown","30066103":"markdown","184e9413":"markdown","bcc62f6e":"markdown","f53c641d":"markdown","98799241":"markdown","40ea716d":"markdown","44041793":"markdown","cbe4d43b":"markdown","c4926d02":"markdown","e0ead6ee":"markdown","bbf0ff19":"markdown","8547266c":"markdown","abbd2160":"markdown","f6201eaa":"markdown","2156ee40":"markdown","c9a7dd98":"markdown","8a1fa196":"markdown","07e2114e":"markdown","5b2d668d":"markdown"},"source":{"7d922430":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.layers import LeakyReLU, BatchNormalization, Conv2D, Add, Layer, Conv2DTranspose, Activation, ZeroPadding2D, Input\nfrom tensorflow_addons.layers import InstanceNormalization\nfrom keras.models import Model\nfrom keras.activations import *\nfrom keras.layers import *\n\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os, random, json, PIL, shutil, re, imageio, glob\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","9aa1a6d9":"BASE_PATH = '..\/input\/gan-getting-started'\nMONET_PATH = os.path.join(BASE_PATH, 'monet_jpg')\nPHOTO_PATH = os.path.join(BASE_PATH, 'photo_jpg')\n\nimport cv2\nimport math\nimport random\n\n\ndef load_images(paths):\n    images = []\n    for img in paths:\n        try:\n            img = cv2.imread(img)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        except:\n            print(\"Could not load {}\".format(img))\n        images.append(img)\n    return images\n        \n        \ndef show_folder_info(path):\n    d_image_sizes = {}\n    for image_name in os.listdir(path):\n        image = cv2.imread(os.path.join(path, image_name))\n        d_image_sizes[image.shape] = d_image_sizes.get(image.shape, 0) + 1\n        \n    for size, count in d_image_sizes.items():\n        print(f'shape: {size}\\tcount: {count}')\n        \ndef visualize_images(images, title=None):\n    plt.figure(figsize=(16,16))\n    \n    w = int(len(images) ** .5)\n    h = math.ceil(len(images) \/ w)\n    \n    if title:\n        plt.suptitle(title)\n        \n    for idx, image in enumerate(images):\n        \n        plt.subplot(h, w, idx + 1)\n        plt.imshow(image)\n        plt.axis('off')\n    \n    \n    \n    plt.show()\n        ","aa176e2c":"MONET_IMAGES = [os.path.join(MONET_PATH, file) for file in os.listdir(MONET_PATH)]\nmonet_images = load_images(MONET_IMAGES)\n\nPHOTO_IMAGES = [os.path.join(PHOTO_PATH, file) for file in os.listdir(PHOTO_PATH)]\nphoto_images = load_images(PHOTO_IMAGES)","c435642f":"visualize_images(random.sample(monet_images,15), \"Samples of the Monet Dataset\")","7b6def1a":"visualize_images(random.sample(photo_images,15), \"Samples of the Photo Dataset\")","1bc63c65":"print(\"Monet Picture Overview\")\nshow_folder_info(MONET_PATH)\n\nprint(\"Content Picture Overview\")\nshow_folder_info(PHOTO_PATH)","37d5d6e6":"GCS_PATH_FULL_DATA = KaggleDatasets().get_gcs_path(\"gan-getting-started\")\nGCS_PATH_CLEAN_DATA = KaggleDatasets().get_gcs_path(\"cleaned-monet-data\")","70a51fee":"excluded =  ['05144e306f.jpg',\n             '9d9a4fccfb.jpg',\n             '3283442e33.jpg',\n             'b5c2fe7c4c.jpg',\n             '8ee2933868.jpg',\n             'b1ea5d5a7d.jpg',\n             'cdddf326e3.jpg',\n             'cb50326950.jpg',\n             '23d6aeb485.jpg',\n             '47a0548067.jpg',\n             '9963d64ebf.jpg',\n             '16dabe418c.jpg',\n             'c78b4fa3a9.jpg',\n             '23b07c3769.jpg',\n             '2e0d0e6e19.jpg',\n             '6a03aea8be.jpg']\n\nexcluded_images = load_images([f\"{MONET_PATH}\/{img}\" for img in excluded])","f68fdd7f":"visualize_images(excluded_images, \"Images excluded from dataset\")","9580b113":"MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH_CLEAN_DATA + '\/*.tfrec'))\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH_FULL_DATA + '\/photo_tfrec\/*.tfrec'))","f00f93db":"IMAGE_SIZE = [256, 256]\nHEIGHT_IMG = 256\nWIDTH_IMG = 256\nN_CHANNELS = 3 ","0411f6c0":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) \/ 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image","ed1142d0":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","0c431f8c":"def data_augment_crop(image):\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    if p_crop > .5:\n        height_crop = 275\n        width_crop = 275\n        image = tf.image.resize(image, [height_crop, width_crop])\n        image = tf.image.random_crop(image, size=[HEIGHT_IMG, WIDTH_IMG, N_CHANNELS])\n        if p_crop > .9:\n            height_crop = 300\n            width_crop = 300\n            image = tf.image.resize(image, [height_crop, width_crop])\n            image = tf.image.random_crop(image, size=[HEIGHT_IMG, WIDTH_IMG, N_CHANNELS])\n            \n    return image\n\ndef data_augment_mirror(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n\n    if p_spatial > .5:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        if p_spatial > .9:\n            image = tf.image.transpose(image)\n            \n    return image\n\ndef data_augment_rotate(image):\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n\n    if p_rotate > .9:\n        image = tf.image.rot90(image, k=3) # rotate 270\u00ba\n    elif p_rotate > .7:\n        image = tf.image.rot90(image, k=2) # rotate 180\u00ba\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=1) # rotate 90\u00ba\n            \n    return image\n\ndef data_augment_color(image):\n    p_color = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    x = tf.image.random_saturation(image, 0.5, 1.5)\n    \n    if p_color > .5:\n        x = tf.image.random_brightness(x, 0.05)\n    else:\n        x = tf.image.random_contrast(x, 0.5, 1.5)\n\n    return image  ","aa00f55d":"def process_dataset(filenames, augment = False, repeat = True, shuffle = False, label = True, batch_size = 1):\n    ds = load_dataset(filenames, labeled = label)\n    \n    if augment:\n        ds = ds.map(data_augment_crop, num_parallel_calls=AUTOTUNE)\n        ds = ds.concatenate(load_dataset(filenames))\n        ds = ds.map(data_augment_mirror, num_parallel_calls=AUTOTUNE)\n        ds = ds.concatenate(load_dataset(filenames))\n        ds = ds.map(data_augment_color, num_parallel_calls=AUTOTUNE)\n        ds = ds.concatenate(load_dataset(filenames))\n        ds = ds.map(data_augment_rotate, num_parallel_calls=AUTOTUNE)\n        ds = ds.concatenate(load_dataset(filenames))\n    if repeat:\n        ds = ds.repeat(count=1)\n    \n    if shuffle:\n        ds = ds.shuffle(2021)\n    \n    ds = ds.batch(batch_size,drop_remainder=True)\n    ds = ds.cache()\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds","e0f582c1":"batch_s = 32\nmonet_ds = process_dataset(MONET_FILENAMES, augment = True, shuffle = True, batch_size = batch_s)\nphoto_ds = process_dataset(PHOTO_FILENAMES, augment = False, shuffle = True, batch_size = batch_s)","49510b70":"def residual_block(x):\n    layer_block = x\n    x = Conv2D(128, (3, 3), strides=1, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(128, (3, 3), strides=1, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    m = layers.add([x, layer_block])\n    return m","24e2b410":"def effnet_block(x_in, ch_in, ch_out):\n    x = Conv2D(ch_in, kernel_size=(1, 1), padding='same', use_bias=False)(x_in) #x_in\n    x = get_post(x)\n\n    x = DepthwiseConv2D(kernel_size=(1, 3), padding='same', use_bias=False)(x)\n    x = get_post(x)\n    x = MaxPool2D(pool_size=(2, 1), strides=(2, 1))(x)\n    \n    x = DepthwiseConv2D(kernel_size=(3, 1), padding='same', use_bias=False)(x)\n    x = get_post(x)\n\n    x = Conv2D(ch_out, kernel_size=(2, 1), strides=(1, 2), padding='same', use_bias=False)(x)\n    x = get_post(x)\n\n    return x\n\ndef get_post(x_in):\n    x = LeakyReLU()(x_in)\n    x = BatchNormalization()(x)\n    return x","21d0e121":"def create_generator():\n    input_layer = layers.Input(shape=(256, 256, 3), dtype='float32')\n    \n    ef1 = effnet_block(input_layer, 32, 64)\n    ef2 = effnet_block(ef1, 64, 128)\n    \n    r1 = residual_block(ef2)\n    r2 = residual_block(r1)\n    r3 = residual_block(r2)\n    r4 = residual_block(r3)\n    r5 = residual_block(r4)\n\n    d1 = Conv2DTranspose(64, (3, 3), strides=2, padding='same', use_bias=False)(r5)\n    d1 = BatchNormalization()(d1)\n    d1 = LeakyReLU(alpha=0.05)(d1)\n\n    d2 = Conv2DTranspose(32, (3, 3), strides=2, padding='same', use_bias=False)(d1)\n    d2 = BatchNormalization()(d2)\n    d2 = LeakyReLU(alpha=0.05)(d2)\n\n    c1 = Conv2D(3, (9, 9), strides=1, padding='same', use_bias=False)(d2)\n    c1 = BatchNormalization()(c1)\n    \n    output_layer = Activation('tanh')(c1)\n\n    model = Model([input_layer], output_layer)\n    return model","fc2065fb":"def create_discriminator():\n    input_layer = layers.Input(shape=[256, 256, 3])\n    input_noise = layers.GaussianNoise(0.2)(input_layer) #0.1 #input_layer\n\n    d1 = layers.Conv2D(64, 4, strides=2, padding='same', use_bias=False)(input_noise)\n    d1 = layers.LeakyReLU(alpha=0.05)(d1)\n    d2 = layers.Conv2D(128, 4, strides=2, padding='same', use_bias=False)(d1)\n    d2 = layers.LeakyReLU(alpha=0.05)(d2)\n    d3 = layers.Conv2D(256, 4, strides=2, padding='same', use_bias=False)(d2)\n    d3 = layers.LeakyReLU(alpha=0.05)(d3)\n    z1 = layers.ZeroPadding2D()(d3)\n    \n    c1 = layers.Conv2D(512, 4, strides=1, use_bias=False)(z1)\n    c1 = layers.BatchNormalization()(c1)\n    c1 = layers.LeakyReLU()(c1)\n    z2 = layers.ZeroPadding2D()(c1)\n\n    out_layer = layers.Conv2D(1, 4, strides=1, use_bias=False)(z2)\n\n    return tf.keras.Model(inputs=input_layer, outputs=out_layer)","5ed18a18":"def create_activation_model():\n    \n    inception_model = tf.keras.applications.InceptionV3(input_shape=(256,256,3),pooling=\"avg\",include_top=False, weights = \"imagenet\", )\n    activations = []\n    \n    # choose a middle layer of the activation layers. Choose on with 160 channels for performance reasons\n    for layer in inception_model.layers:\n        if \"activation\" in layer.name and layer.output.shape[3] == 160:\n            activations.append(layer)\n            \n    # choose the middle of those activations\n    layer = inception_model.get_layer(activations[int(len(activations)\/2)].name)\n    \n    print(f'Selected layer {layer} for the activation model')\n    \n    return tf.keras.models.Model(inputs=inception_model.input, outputs=[layer.output])","0f8ca8a9":"with strategy.scope():\n    monet_generator = create_generator() # transforms photos to Monet-esque paintings\n    photo_generator = create_generator() # transforms Monet paintings to be more like photos\n\n    monet_discriminator = create_discriminator() # differentiates real Monet paintings and generated Monet paintings\n    photo_discriminator = create_discriminator() # differentiates real photos and generated photos\n    \n    activation_model = create_activation_model()","0b49cf59":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        activation_model,\n        mu,\n        lambda_cycle=10, #25\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        \n        # add the inception model to the mix\n        self.inception = activation_model\n        self.inception.trainable = False\n        # the average  monet activation map\n        self.mu_monet = mu\n        \n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        gen_loss_photo,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.gen_loss_photo = gen_loss_photo\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            #inception model output\n            inception_fake_monet = self.inception(fake_monet)\n            \n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet, inception_fake_monet, self.mu_monet, self.lambda_cycle)\n            photo_gen_loss = self.gen_loss_photo(disc_fake_photo)\n            \n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","140e8162":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5","a0113687":"with strategy.scope():\n    \n    # for the generated monet, combine BCE with our custom loss based on the activation layers\n    def generator_loss(generated, inception_fake, mu, LAMBDA):\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n        \n        return loss + LAMBDA * tf.reduce_mean(tf.abs(mu - inception_fake))\n    \n    # fpr the transformation back keep the BCE\n    def generator_loss_photo(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n        ","7d0025de":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1","516a5582":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","437b3a07":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","217daf36":"monet_ds_single_batch = process_dataset(MONET_FILENAMES, augment = False, shuffle = True, batch_size = 1)\n\nactivations = []\n\nfor batch, images in enumerate(monet_ds_single_batch):\n    pred = activation_model(images)\n    activations.append(pred)\n\n# average over all images\nmu_monet = np.mean(activations, axis=0)\n       ","4ad84cfb":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator, activation_model, mu_monet\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        gen_loss_photo = generator_loss_photo,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","0140cab3":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\ncycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=75,\n    callbacks=[callback]\n)","dcce04cd":"_, ax = plt.subplots(5, 2, figsize=(50,50))\nds_predict = load_dataset(PHOTO_FILENAMES).batch(1)\nfor i, img in enumerate(ds_predict.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque (generated)\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","6722b50d":"import PIL\n! mkdir ..\/images","dc311704":"fast_photo_ds = load_dataset(PHOTO_FILENAMES).batch(32*strategy.num_replicas_in_sync).prefetch(32)","1c6ab5ec":"%%time\ni = 1\nfor img in fast_photo_ds:\n    prediction = monet_generator(img, training=False).numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    for pred in prediction:\n        im = PIL.Image.fromarray(pred)\n        im.save(\"..\/images\/\" + str(i) + \".jpg\")\n        i += 1","bd3b3d42":"print(f\"Generated {i} images for submission\")","65f47064":"import shutil\nshutil.make_archive(\"\/kaggle\/working\/images\", 'zip', \"\/kaggle\/images\")","f61cd9b9":"monet_histograms = [[cv2.calcHist([image],[i],None,[256],[0,256]) for i in range(3)] for image in monet_images]\nphoto_histograms = [[cv2.calcHist([image],[i],None,[256],[0,256]) for i in range(3)] for image in photo_images]","27233920":"def plot_histograms(img, title):\n    plt.figure(figsize=(16,16))\n    \n    plt.title(title)\n    color = ('b','g','r')\n    \n    \n    w = 4\n    h = int(len(img)\/2)\n    \n    idxs = [i for i in range(len(img)*2)[::2]]\n    \n    for idx, image in zip(idxs,img):\n        \n        plt.subplot(h, w, idx + 1)\n        plt.imshow(image)\n        plt.axis('off')\n        \n        plt.subplot(h, w, idx + 2)\n        for i,col in enumerate(color):   \n            histr = cv2.calcHist([image],[i],None,[256],[0,256])\n            plt.plot(histr,color = col)\n            plt.xlim([0,256])","7d2945eb":"plot_histograms(random.sample(monet_images,8), \"Histogram for Monet Image\")","0802777c":"plot_histograms(random.sample(photo_images,8), \"Histogram for Photo\")","6c99339d":"color = ('b','g','r')\nmonet_means = {c: [np.mean(x[:,:,i]) for x in monet_images] for i,c in enumerate(color)}\nphoto_means = {c: [np.mean(x[:,:,i]) for x in photo_images] for i,c in enumerate(color)}\n\nmonet_std = {c: [np.std(x[:,:,i]) for x in monet_images] for i,c in enumerate(color)}\nphoto_std = {c: [np.std(x[:,:,i]) for x in photo_images] for i,c in enumerate(color)}","59902cc5":"def boxplots_for_comparison(monet, photo, title):\n    for i,c in enumerate(color):\n        fig, ax = plt.subplots()\n        ax.set_title('{} Channel: {}'.format(c.upper(), title))\n        ax.boxplot([monet[c], photo[c]])\n        ax.set_xticklabels([\"Monet\",\"Photo\"])\n        fig.show()","691818cb":"boxplots_for_comparison(monet_means, photo_means, \"Pixel Value Mean comparison - Monet and Photo\")","c547796a":"boxplots_for_comparison(monet_std, photo_std, \"Pixel Value standard deviation comparison - Monet and Photo\")","d9cb1326":"zero_values_monet = {c: [np.count_nonzero(hist[i].ravel()==0) for hist in monet_histograms] for i,c in enumerate(color)}\nzero_values_photo = {c: [np.count_nonzero(hist[i].ravel()==0) for hist in photo_histograms] for i,c in enumerate(color)}","0db1f2f6":"boxplots_for_comparison(zero_values_monet,zero_values_photo, \"Number of not used intensity values Monet and Photo\" )","b3e539f4":"<a id='eda2'><\/a>\n## Exhaustive EDA","1cb515ca":"Visualization of the results","2987d3f0":"![](http:\/\/)<a id='discriminator'><\/a>\n<div style=\"display: flex; flex-direction: column; align-items: center\">\n    <div>\n    <h3> Discriminator <\/h3>\n        <p>Our discriminator follows the following architecture:<\/p><\/div>\n<img src= \"https:\/\/imgur.com\/eJliJxm.png\" alt =\"Our Discriminator\" style='width: 300px;'><\/div>","f06c02ea":"<a id='training'><\/a>\n## Training","5e2ce8d6":"### Histogram analysis\n\nOpenCV uses the BGR channel order, thus the first histogram of an image is its blue value histogram, the second gree and the third red","23bf89bd":"Lastly, we need a function that combines all our data loading functions","9c2d19a0":"## Inception Model\n\nTo encourage our model to adapt the style of monet images, we extract features for both generated and original Monet images from middle layers of the Inception V3 model. \nAt a later point we add an additional term to our losses that requires the model to minimize the distance between the layer output of the real monet and the current training sample. \n\nThis way, we hope to capture the pure style features (what may be recognized as brush strokes, colours etc. for humans) and add an additional constraint to the generation process.","073fa264":"# My CycleGAN is somewhat of a painter itself","eb961626":"The histograms indicate that \n\n(1) The monet images seem to use on average more colours for every channel. We come to this conclusion, since the mean pixel intensity for the Monet images is higher than for the photos. However, given the amount of photo files, there are several outliers in both directions. \n\n(2) The range of intensity values used for the Monet images is smaller than for the photos. I.e. the Monet images have more \"unused\" intensity values. This may be explained by the fact that photo transitions are way more smooth than the transitions of colours in expressionistic paintings. \n\nHowever, these observations are not too surprising considering that we have paintings and images.","8c45b1e6":"<a id='eda'><\/a>\n## Exploratory Data Analysis\n","1411551d":"<a id='cycle'><\/a>\n### CycleGan Architecture","ca40fb25":"### Data Stats:","0e433a27":"<img src= \"https:\/\/imgur.com\/AhIJfHj.png\" alt =\"Titanic\" style='width: 500px;'>","80c99a26":"The images for this challenges are already cleaned. All are of size 256x256 and have three channels (RGB)","c3442010":"#### \"Frequency\" of intensity values\n\nThe boxplots only tell us the mean and the standard deviation of the intensity values. Now it would be interesting to see if both Monet and the photos make use of the full range of intensity values. That is, are there any empty bins in images? Our assumption is that Monet images might not make use of the full range of intensity values (based on the histograms)","c64a403a":"## Visualization of a few images\n\nLet's have a first look at the data we got. All the images for the challenge are of the size 256x256 and sorted by type already","30066103":"### Photos","184e9413":"We load a cleaned version of the monet dataset. The original had two round images and some really dark and not quite \"Monet-like\" images. The following images were excluded:\n","bcc62f6e":"Compute the average activation for the monet images. The inception model is just used for feature extraction which means we have to compute the activation map for the original model only once","f53c641d":"<a id='generator'><\/a>\n### Generator","98799241":"<a id='data_augmentation'><\/a>\n## Data Augmentation","40ea716d":"<a id='data_loading'><\/a>\n## Data Loading","44041793":"<a id='results'><\/a>\n## Results","cbe4d43b":"Lastly, we create a function that loads the dataset and combines the upper functions","c4926d02":"*Visual Inspection*\n\nComparing the Monet images and the photos we can obviously see which are paintings and which are photos. The photos have way smoother transitions, the monet images \"jump\" in colours. Monet was a impressionist painter, an art style which is characterized by a small but still visible brush strokes. \nSo our final images should also have this characteristic but showing the content of the content images.  \n\nFurther, the colours of photos are more natural than the colours of the mone style images. This is not too surprising, but we want to keep this in mind when judging the final output of our GAN","e0ead6ee":"First, we create the instances of our generator and discriminator","bbf0ff19":"This notebook is part of the Machine Learning in Practice Course 2021 at Radboud University. With it, we are participating in the \"I am somewhat of a painter myself\" challenge.\n\n**Objective of the challenge**: Build a GAN that generates 7,000 to 10,000 Monet-style images\n\n## Outline of this notebook\n\n0. <a href='#imports'>Imports<\/a>\n1. <a href='#eda'>Exploratory Data Analysis (EDA) (short version)<\/a>\n2. <a href='#data_loading'>Data Loading<\/a>\n3. <a href='#data_augmentation'>Data Augmentation<\/a>\n4. <a href='#model'>The model<\/a>\n    1. <a href='#generator'>Generator<\/a>\n    2. <a href='#discriminator'>Discriminator<\/a>\n    3. <a href='#cycle'>CycleGan<\/a>\n    4. <a href='#losses'>Losses<\/a>\n5. <a href='#training'>Model Training<\/a>\n6. <a href='#results'>Results<\/a>\n7. <a href='#eda2'>Exhaustive EDA<\/a>","8547266c":"### Analysis of the image intensity value distributions","abbd2160":"Data augmentation can help to improve our model. However, we need to make sure to not change the style too much, as otherwise we might decrease the performance of our model. We therefore decided to only apply augmentation strategies which will not change the image style too much. This includes:\n\n- Mirroring\n- Rotation\n- Changes in Saturation\n- Cropping\n","f6201eaa":"Below we define two functions. `read_tfrecord(example)` retrieves the images data from the tf record, `decode_image` transforms it into a `tf.image`","2156ee40":"And then combine everything in the cycleGAN architecture","c9a7dd98":"<a id='imports'><\/a>\n## Imports","8a1fa196":"<a id='losses'><\/a>\n### Losses","07e2114e":"<a id='model'><\/a>\n## The Model","5b2d668d":"### Monet Style Images"}}