{"cell_type":{"7594c096":"code","74b79e1c":"code","9187c34e":"code","839dbf93":"code","392ab934":"code","b5160a8d":"code","d21bf246":"code","4ff4319a":"code","11562b8f":"code","32d71b25":"code","1c6df30e":"code","1ae7a994":"code","9b7413be":"code","67a7d306":"code","bddce419":"code","dd45a3c7":"code","ba562411":"code","300b06d9":"code","f4526969":"code","e5943789":"code","0e7439e3":"code","d0d07f32":"code","9dae7182":"code","fd89ee83":"code","6442c115":"code","0e17b54a":"code","288ddd29":"code","8ce038b4":"markdown","244c1a1d":"markdown","537b0736":"markdown","6f28b6a9":"markdown","259410a8":"markdown","4c2ddea6":"markdown","f2c92455":"markdown","bf1dde81":"markdown","3bebfb92":"markdown"},"source":{"7594c096":"# kaggle paths\nTRAIN_DATA_DIR = \"..\/input\/10-monkey-species\/training\/training\/\"\nVALID_DATA_DIR = \"..\/input\/10-monkey-species\/validation\/validation\/\"\nLABELS_FILE = \"..\/input\/10-monkey-species\/monkey_labels.txt\"\nFILE_SAVE_PATH = \"\/kaggle\/working\/\"","74b79e1c":"import os\nimport numpy as np\n# set the seed for to make results reproducable\nnp.random.seed(2)\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n\nfrom tensorflow.random import set_seed\n# set the seed for to make results reproducable\nset_seed(2)\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","9187c34e":"img_width = 224  # since this is the size of image, that vgg19 model, as the documentation states, accepts\nimg_height = 224 # check: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/VGG19#arguments_1\nbtch_size = 16   # since the dataset is small","839dbf93":"# Since the number of example is very low, I am making some augmentation in the training data(not increasing the count of training examples),\n# to make the robust to the unseen examples.\ntrain_data_IDG = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,\n                                    zoom_range=0.2, fill_mode='nearest', horizontal_flip=True, rescale=1.0\/255,\n                                    validation_split=0.0)\n\nvalid_data_IDG = ImageDataGenerator(rescale=1.0\/255)","392ab934":"train_data_generator = train_data_IDG.flow_from_directory(directory=TRAIN_DATA_DIR, target_size=(img_width, img_height),\n                                                          class_mode='categorical', batch_size=btch_size)\n\nvalid_data_generator = valid_data_IDG.flow_from_directory(directory=VALID_DATA_DIR, target_size=(img_width, img_height),\n                                                          class_mode='categorical')","b5160a8d":"train_len = 1_098\nvalid_len = 272","d21bf246":"tr_imgs, tr_labels = train_data_generator.next()\ntr_imgs.shape, tr_labels.shape","4ff4319a":"target_data = pd.read_csv(filepath_or_buffer=LABELS_FILE)\ntarget_data","11562b8f":"i = 2\nprint(target_data.iloc[np.where(tr_labels[i] == 1)[0][0], 2])\nplt.imshow(tr_imgs[i])","32d71b25":"i = 7\nprint(target_data.iloc[np.where(tr_labels[i] == 1)[0][0], 2])\nplt.imshow(tr_imgs[i])","1c6df30e":"labels = train_data_generator.classes\nlabels.shape","1ae7a994":"sns.distplot(a=labels, bins=None, kde=False)","9b7413be":"pd.Series(labels).value_counts(normalize=True)","67a7d306":"from tensorflow.keras.applications.vgg19 import VGG19\n\n\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras import layers","bddce419":"base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling='max')\n\n\n# # let's make some layers at the last layers non-trainable, since that's where I want to make changes at\nfor layer in base_model.layers[:-6]:\n    layer.trainable = False\n\n# checking the status in number of parameters trainable\nbase_model.summary()","dd45a3c7":"for layer in base_model.layers[:]:\n    print(layer, layer.trainable)","ba562411":"model_vgg19 = Sequential()\n\nfor layer in base_model.layers:\n    model_vgg19.add(layer)\n\nmodel_vgg19.add(layers.Dense(512, activation=\"relu\"))\nmodel_vgg19.add(layers.Dropout(0.5))\nmodel_vgg19.add(layers.Dense(10, activation=\"softmax\"))\n\nmodel_vgg19.summary()","300b06d9":"import time\nfrom tensorflow.keras.callbacks import Callback\n\nclass EpochTimeHistory(Callback):\n    \"\"\"\n    a custom callback to print the time(in minutes, to console) each epoch took during.\n    \"\"\"\n    def on_train_begin(self, logs={}):\n        self.train_epoch_times = []\n        self.valid_epoch_times = []\n\n    def on_epoch_begin(self, epoch, logs={}):\n        self.epoch_time_start = time.time()\n\n    def on_epoch_end(self, epoch, logs={}):\n        cur_epoch_time = round((time.time() - self.epoch_time_start)\/60, 4)\n        self.train_epoch_times.append(cur_epoch_time)\n        print(\" ;epoch {0} took {1} minutes.\".format(epoch+1, cur_epoch_time))\n\n\n    def on_test_begin(self, logs={}):\n        self.test_time_start = time.time()\n\n    def on_test_end(self, logs={}):\n        cur_test_time = round((time.time() - self.test_time_start)\/60, 4)\n        self.valid_epoch_times.append(cur_test_time)\n        print(\" ;validation took {} minutes.\".format(cur_test_time))","f4526969":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n# saving model weights at the end of every epoch, in case something happens to my internet connection; and so i can resume training with those weights from the epoch last session stopped at.\nmodel_save_cb = ModelCheckpoint(filepath=os.path.join(FILE_SAVE_PATH, 'vgg19-weights-epoch{epoch:02d}-val_fbeta_score{val_fbeta_score:.2f}.h5'))\n\n# let's not waste resources for performance that we aren't gonna get\nearly_stop_cb = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min')\n\n# also reducing learning rate, because you know all about the gradient reaching optima in correct way.\nreduce_learning_rate_cb = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, cooldown=2, min_lr=0.00001, verbose=1)\n\n# to see how much time each epoch took to complete training\nepoch_times_cb = EpochTimeHistory()","e5943789":"from tensorflow_addons.metrics import FBetaScore\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall\n# from tensorflow.keras.losses import CategoricalCrossentropy\n\n\nmodel_vgg19.compile(loss = 'categorical_crossentropy',\n                optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.00001),\n                metrics = [FBetaScore(num_classes=10, average='macro', name='fbeta_score'),\n                           CategoricalAccuracy(name='cat_acc'),\n                           Precision(name='precision'), Recall(name='recall')])","0e7439e3":"history_VGG19 = model_vgg19.fit(train_data_generator,\n                                  steps_per_epoch=train_len \/\/ btch_size,\n                                  validation_data=valid_data_generator,\n                                  validation_steps=valid_len \/\/ btch_size,\n                                  epochs=35, verbose=1,\n                                  callbacks=[model_save_cb, early_stop_cb,\n                                             epoch_times_cb, reduce_learning_rate_cb])","d0d07f32":"model_vgg19.save(filepath=FILE_SAVE_PATH, overwrite=True, include_optimizer=True)","9dae7182":"# if training had no problems, delete all the saved weights and include the final model\n!rm \/kaggle\/working\/*.h5","fd89ee83":"plt.plot(history_VGG19.history['loss'])\nplt.plot(history_VGG19.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='center right')\nplt.show()","6442c115":"plt.plot(history_VGG19.history['fbeta_score'])\nplt.plot(history_VGG19.history['val_fbeta_score'])\nplt.title('model fbeta_score')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='center right')\nplt.show()","0e17b54a":"plt.plot(history_VGG19.history['cat_acc'])\nplt.plot(history_VGG19.history['val_cat_acc'])\nplt.title('model categorical accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='center right')\nplt.show()","288ddd29":"plt.plot(history_VGG19.history['precision'])\nplt.plot(history_VGG19.history['recall'])\nplt.plot(history_VGG19.history['val_precision'])\nplt.plot(history_VGG19.history['val_recall'])\nplt.title('model precision and recall')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['precision_train', 'recall_train', 'precision_val', 'recall_val'], loc='center right')\nplt.show()","8ce038b4":"### checking balance in the output classes","244c1a1d":"# modelling","537b0736":"## exploring data","6f28b6a9":"## VGG-19","259410a8":"# loading and preparing data","4c2ddea6":"looks quiet balanced. Also look at the percentage of examples available per class.","f2c92455":"### plotting model statistics","bf1dde81":"### making model","3bebfb92":"What I have and had done:\n1. Given the very less number of examples, training a classification model from scratch will only take us so far. So, I have opted to use transfer learning approach.\n2. I have also tried transfer learning with VGG-16. I couldn't get fbeta score of above 0.914 with that architecture. Transfer learning because the dataset is so small(only 1,098 train examples.)\n3. I had tried batch sizes from 4 to 64 and it's batch size of 16, that worked.\n\nAnd yes, I have removed all the cells in this notebook that I tried above things in, for better understanding."}}