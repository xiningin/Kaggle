{"cell_type":{"264b8c27":"code","d3ee5c91":"code","bb3914ae":"code","a6977c04":"code","536f2501":"code","9d26ded0":"code","91eb707f":"code","2ba538a9":"code","370c0e54":"code","0e9981ff":"code","ff4e5b4e":"code","c751a6c5":"code","d562d63a":"code","2e734e42":"code","11bf05e7":"code","f0c7e49f":"code","51b15182":"code","62d3d250":"code","7af61506":"code","06829051":"code","2b263a86":"code","3facb554":"code","6e0bdfb4":"code","af685b4f":"code","92886876":"code","9aef61ac":"code","653df8c3":"code","ccbe1d0a":"code","2bdf9899":"code","2eeaefb9":"code","41b837be":"code","4202e786":"code","a649e6a6":"code","b41af03d":"code","f282a33f":"code","f70126b0":"code","d9ff34e1":"code","b24c8593":"code","d0980e77":"code","ad1a452c":"code","0099654c":"markdown","ceacdbb7":"markdown","f6a9014b":"markdown","25fe9295":"markdown","5442a2fd":"markdown","a31a25b4":"markdown","a8659220":"markdown","ca947d26":"markdown","63325cb3":"markdown","3366ddaf":"markdown","868701a4":"markdown","3993df54":"markdown","ab77a3d8":"markdown","f47a8ad8":"markdown","cdb08b85":"markdown","2ea062b4":"markdown","a47e80bd":"markdown"},"source":{"264b8c27":"import os\nimport sys\nimport random\nimport subprocess\nimport cv2\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom collections import OrderedDict\n\nimport scipy\nimport scipy.ndimage as ndimage\nimport scipy.ndimage.filters as filters\nfrom scipy.ndimage import binary_dilation\nimport matplotlib.patches as patches\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.nn import functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom sklearn.metrics import roc_auc_score\nimport torch.optim as optim\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image","d3ee5c91":"random_seed= 42\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic=True\ntorch.backends.cudnn.benchmark = True\nbatch_size = 64\nvalidation_split = .34\nshuffle_dataset = True","bb3914ae":"def run_cmd(cmd, stderr=subprocess.STDOUT):\n    out = None\n    try:\n        out = subprocess.check_output(\n            [cmd], \n            shell=True,\n            stderr=subprocess.STDOUT, \n            universal_newlines=True,\n        )\n    except subprocess.CalledProcessError as e:\n        print(f'ERROR {e.returncode}: {cmd}\\n\\t{e.output}', flush=True, file=sys.stderr)\n        raise e\n    return out","a6977c04":"def clone_data(data_root):\n    clone_uri = 'https:\/\/github.com\/ieee8023\/covid-chestxray-dataset.git'\n    if os.path.exists(data_root):\n        assert os.path.isdir(data_root), \\\n        f'{data_root} should be cloned from {clone_uri}'\n    else:\n        print(\n            'Cloning the covid chestxray dataset. It may take a while\\n...\\n', \n            flush=True\n        )\n        run_cmd(f'git clone {clone_uri} {data_root}')","536f2501":"data_root = \".\/data\"\nmgpath=f'{data_root}\/images',\ncsvpath=f'{data_root}\/metadata.csv',","9d26ded0":"csvpath","91eb707f":"clone_data(data_root)","2ba538a9":"meta = pd.read_csv('data\/metadata.csv')","370c0e54":"meta['view'].value_counts(dropna=False)","0e9981ff":"for x in meta['filename']:\n    if x.split('.')[-1]=='gz':\n        meta.drop(meta.index[meta['filename']==x], \n                  inplace=True)","ff4e5b4e":"meta = meta[(meta['finding']=='COVID-19')\n            |(meta['finding']=='SARS')\n            |(meta['finding']=='Pneumocystis')\n            |(meta['finding']=='Streptococcus')\n            |(meta['finding']=='COVID-19, ARDS')\n            |(meta['finding']=='ARDS')]\nmeta = meta[meta['view']=='PA']","c751a6c5":"meta['finding'].value_counts(dropna=False)","d562d63a":"X_train_val, X_test = train_test_split( meta[meta['finding']=='COVID-19'], test_size=0.85, random_state=random_seed)\nmeta.drop(X_test.index, inplace=True)\nmeta.reset_index(drop=True, inplace=True)","2e734e42":"meta['finding'].value_counts(dropna=False)","11bf05e7":"dataset_size = len(meta)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]","f0c7e49f":"train_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)","51b15182":"Labels = np.array(meta['finding']).reshape(len(meta['finding']),1)\nencode = OneHotEncoder()\nencode.fit(Labels)\nlabels_enc = encode.transform(Labels).toarray()","62d3d250":"transform=transforms.Compose([\n                              transforms.ToPILImage(),\n                              transforms.RandomCrop(224),                              \n                              transforms.ToTensor(),                                                  \n                              transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n                                        ])","7af61506":"class_names = [\n    'COVID-19', \n    'SARS ',\n    'Pneumocystis',       \n    'Streptococcus',      \n    'COVID-19, ARDS',     \n    'ARDS',               \n]","06829051":"class ChestXrayDataSet(Dataset):\n    def __init__(self,csvpath,mgpath,labels_enc,transform=None):\n        self.meta_data = pd.read_csv(csvpath)\n        self.root_dir = mgpath\n        self.labels = self.meta_data['finding']\n        self.transform = transform\n        for x in self.meta_data['filename']:\n            if x.split('.')[-1]=='gz':\n                self.meta_data.drop(self.meta_data.index[self.meta_data['filename']==x],\n                                    inplace=True)\n    \n        self.meta_data = self.meta_data[(self.meta_data['finding']=='COVID-19')\n                                        |(self.meta_data['finding']=='SARS')\n                                        |(self.meta_data['finding']=='Pneumocystis')\n                                        |(self.meta_data['finding']=='Streptococcus')\n                                        |(self.meta_data['finding']=='COVID-19, ARDS')\n                                        |(self.meta_data['finding']=='ARDS')]\n        self.meta_data = self.meta_data[self.meta_data['view']=='PA']\n        self.meta_data.drop(X_test.index, inplace=True)\n        self.meta_data.reset_index(drop=True, inplace=True)\n    \n    def __len__(self):\n        return len(self.meta_data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        img_name = os.path.join(self.root_dir,\n                                self.meta_data.loc[idx,'filename'])\n        image = Image.open(img_name).convert('RGB')\n        image = np.array(image.resize((256,256)))\n        image = image[:,:,0]\n        image = np.uint8(((np.array(image)\/255).reshape(256,256,1))*255*255)\n        image = np.tile(image,3) \n        label = labels_enc[idx]\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label, idx","2b263a86":"dataset = ChestXrayDataSet(csvpath[0],mgpath[0],labels_enc,transform)","3facb554":"train_loader = torch.utils.data.DataLoader(dataset, \n                                           batch_size=batch_size, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(dataset, \n                                                batch_size=batch_size,\n                                                sampler=valid_sampler)","6e0bdfb4":"def img_display(img):\n    img = img*0.229+0.485   # unnormalize (inp = inp*std + mean)\n    npimg = img.numpy()[0]\n  \n    return npimg","af685b4f":"# get some random training images\ndataiter = iter(train_loader)\nimages, labels, id_ = dataiter.next()\n# Viewing data examples used for training\nfig, axis = plt.subplots(2, 4, figsize=(15, 10))\nfor i, ax in enumerate(axis.flat):\n    with torch.no_grad():\n        image, label, _ = images[i], labels[i], id_[i]\n        ax.imshow(img_display(image),cmap='gray') # add image\n        ax.set(title = f\"{meta['finding'][_.item()]}\") # add label","92886876":"# construct model\nclass DenseNet121(nn.Module):\n    def __init__(self, out_size):\n        super(DenseNet121, self).__init__()\n        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n        num_ftrs = self.densenet121.classifier.in_features\n        self.densenet121.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, out_size),\n            nn.Sigmoid()\n        ).cuda()\n    def forward(self, x):\n        x = self.densenet121(x)\n        return x","9aef61ac":"cudnn.benchmark = True\nN_CLASSES = 6","653df8c3":"def compute_AUCs(gt, pred):\n    AUROCs = []\n    gt_np = gt.cpu().numpy()\n    pred_np = pred.cpu().numpy()\n    for i in range(N_CLASSES):\n        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n    return AUROCs\n","ccbe1d0a":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\n\nprint(device)","2bdf9899":"# initialize and load the model\nmodel = DenseNet121(N_CLASSES)\nmodel = model.cuda(device)","2eeaefb9":"optimizer = optim.Adam(model.parameters(),lr=0.0007)\ncriterion = nn.BCEWithLogitsLoss()\ncriterion = criterion.cuda(device)","41b837be":"save_best = 0.0\nfor epoch in range(100):\n    print(\"Epoch:\",epoch)\n    running_loss = 0.0\n    for batch_idx, (data_, target_,_) in enumerate(train_loader):\n        target_ = target_.type(torch.float)\n        data_, target_ = data_.cuda(device), target_.cuda(device)\n        optimizer.zero_grad()\n        outputs = model(data_)\n        loss = criterion(outputs, target_)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n        # ======== validation ======== \n        # switch to evaluate mode\n        # initialize the ground truth and output tensor\n        with torch.no_grad():\n            model.eval()\n            gt = torch.FloatTensor()\n            gt = gt.cuda()\n            pred = torch.FloatTensor()\n            pred = pred.cuda()\n            for i, (data_t, target_t,_t) in enumerate(validation_loader):\n                target_t = target_t.type(torch.float)\n                data_t, target_t = data_t.cuda(device), target_t.cuda(device)\n                gt = torch.cat((gt, target_t), 0)\n                input_var = Variable(data_t.view(-1, 3, 224, 224).cuda())\n                output = model(input_var)\n                pred = torch.cat((pred, output.data), 0)\n            AUROCs = compute_AUCs(gt, pred)\n            AUROC_avg = np.array(AUROCs).mean()\n            print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n            if AUROC_avg>save_best:\n                save_best=AUROC_avg\n                torch.save(model.state_dict(), 'Covid_detection.pt')\n                print('Detected network improvement, saving current model')\n            for i in range(N_CLASSES):\n                print('The AUROC of {} is {}'.format(class_names[i], AUROCs[i]))\n        model.train()\n        # print statistics\n        #print('[%d] loss: %.3f' % (epoch + 1, running_loss \/ 715 ))\n        print(\"======================================================================\\n\")\nprint('Finished Training')","4202e786":"model.load_state_dict(torch.load('Covid_detection.pt'))","a649e6a6":"class ChestXrayDataSet_plot(Dataset):\n    def __init__(self, input_X, transform=None):\n        self.data = input_X#np.uint8(test_X*255)\n        self.transform = transform\n        self.root_dir = mgpath[0]\n        self.transform = transform\n\n    def __getitem__(self, index):\n        if torch.is_tensor(index):\n            index = index.tolist()\n        img_name = os.path.join(self.root_dir,self.data.loc[index,'filename'])\n        image = Image.open(img_name).convert('RGB')\n        image = np.array(image.resize((256,256)))\n        image = np.uint8(image*255)\n        image = self.transform(image)\n        return image\n\n    def __len__(self):\n        return len(self.data)","b41af03d":"X_test.reset_index(drop=True, inplace=True)\ntest_dataset = ChestXrayDataSet_plot(input_X = X_test,transform=transforms.Compose([\n                                        transforms.ToPILImage(),                                                                \n                                        transforms.ToTensor(),               \n                                        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n                                        ]))","f282a33f":"print(\"generate heatmap ..........\")\n# ======= Grad CAM Function =========\nclass PropagationBase(object):\n    def __init__(self, model, cuda=True):\n        self.model = model\n        self.model.eval()\n        if cuda:\n            self.model.cuda()\n        self.cuda = cuda\n        self.all_fmaps = OrderedDict()\n        self.all_grads = OrderedDict()\n        self._set_hook_func()\n        self.image = None\n\n    def _set_hook_func(self):\n        raise NotImplementedError\n\n    def _encode_one_hot(self, idx):\n        one_hot = torch.FloatTensor(1, self.preds.size()[-1]).zero_()\n        one_hot[0][idx] = 1.0\n        return one_hot.cuda() if self.cuda else one_hot\n\n    def forward(self, image):\n        self.image = image\n        self.preds = self.model.forward(self.image)\n        #self.probs = F.softmax(self.preds, dim=1)[0]\n        #self.prob, self.idx = self.preds[0].data.sort(0, True)\n\n        return self.preds.cpu().data.numpy()\n\n    def backward(self, idx):\n        self.model.zero_grad()\n        one_hot = self._encode_one_hot(idx)\n        self.preds.backward(gradient=one_hot, retain_graph=True)","f70126b0":"class GradCAM(PropagationBase):\n    def _set_hook_func(self):\n        def func_f(module, input, output):\n            self.all_fmaps[id(module)] = output.data.cpu()\n    \n        def func_b(module, grad_in, grad_out):\n            self.all_grads[id(module)] = grad_out[0].cpu()\n\n        for module in self.model.named_modules():\n            module[1].register_forward_hook(func_f)\n            module[1].register_backward_hook(func_b)\n\n    def _find(self, outputs, target_layer):\n        for key, value in outputs.items():\n            for module in self.model.named_modules():\n                if id(module[1]) == key:\n                    if module[0] == target_layer:\n                        return value\n        raise ValueError('Invalid layer name: {}'.format(target_layer))\n\n    def _normalize(self, grads):\n        l2_norm = torch.sqrt(torch.mean(torch.pow(grads, 2))) + 1e-5\n        return grads \/ l2_norm.item() \n  \n    def _compute_grad_weights(self, grads):\n        grads = self._normalize(grads)\n        self.map_size = grads.size()[2:]\n        return nn.AvgPool2d(self.map_size)(grads)\n\n    def generate(self, target_layer):\n        fmaps = self._find(self.all_fmaps, target_layer)\n        grads = self._find(self.all_grads, target_layer)\n        weights = self._compute_grad_weights(grads)\n        gcam = torch.FloatTensor(self.map_size).zero_()\n        for fmap, weight in zip(fmaps[0], weights[0]):\n            gcam += fmap * weight.data\n    \n        gcam = F.relu(Variable(gcam))\n        gcam = gcam.data.cpu().numpy()\n        gcam -= gcam.min()\n        gcam \/= gcam.max()\n        gcam = cv2.resize(gcam, (self.image.size(3), self.image.size(2)))    \n        return gcam\n    \n    def FinalImage(self, gcam, raw_image):\n        raw_image = raw_image*0.229+0.485\n        gcam = cv2.applyColorMap(np.uint8(gcam * 255.0), cv2.COLORMAP_JET)\n        gcam = np.float32(gcam) \/ (600)\n        gcam = gcam.astype(np.float) + raw_image.numpy()[0].astype(np.float).reshape(256,256,1)\n        gcam = gcam \/ gcam.max() \n  \n        return np.uint8(gcam * 255.0)","d9ff34e1":"# ======== Create heatmap ===========\n\nheatmap_output = []\nimage_id = []\noutput_class = []\n\nthresholds = 0.1\ndataiter = iter(validation_loader)\nimages = dataiter.next()\ngcam = GradCAM(model=model, cuda=True)\nfor index in range(len(test_dataset)):\n    input_img = Variable((test_dataset[index]).unsqueeze(0).cuda(), requires_grad=True)\n    probs = gcam.forward(input_img)\n    activate_classes = np.where((probs > thresholds)[0]==True)[0] # get the activated class\n    for activate_class in activate_classes:\n        gcam.backward(idx=activate_class)\n        output = gcam.generate(target_layer=\"densenet121.features.denseblock4.denselayer16.conv2\")\n        #### this output is heatmap ####\n        if np.sum(np.isnan(output)) > 0:\n            print(\"fxxx nan\")\n        img = gcam.FinalImage(output, test_dataset[index])\n        heatmap_output.append(img)\n        image_id.append(index)\n        output_class.append(activate_class)\n    print(\"test \",str(index),\" finished\")","b24c8593":"heatmap_output_1 = heatmap_output[:130]\nheatmap_output_2 = heatmap_output[130:]","d0980e77":"fig, axis = plt.subplots(26,5, figsize=(20, 125))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(heatmap_output_1[i]) # add image","ad1a452c":"fig, axis = plt.subplots(11,5, figsize=(20, 50))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(heatmap_output_2[i]) # add image","0099654c":"## Please do not leave right away, just because ......\n* I know that the dataset is completly imbalanced. But still I worked it by balancing the target labels and also utilized the unbalanced data in a better way. \nSo, please do not leave right away, just by saying \"yeah, I know this is another work on covid detection with higher accuracy over IMBALANCED DATASET..... \"\n* I know till now there are hundreds of work over the internet, with the same motive i.e. detection of covid-19 from lungs x-ray image, even there are published research papers also. But this is my way of solving the problem, so please do not leave away by saying \"Yeah! I know what this notebook is about, and what is going to be in this notebook....\".\n* Please do not tag me with the meme \"you know I'm something of a epidemologist myself\". Because.............<br>\nI do not have any reason for this, just do not do it\ud83d\ude16\ud83d\ude16.......\n![meme](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fi.redd.it%2Ffkmqligtxjh01.jpg&f=1&nofb=1)\n","ceacdbb7":"## Thank you\nThank you for scrolling down till the end of the notebook. I hope this notebook was helpful to you. And if it was helpful then please share this. <br>\nPlease let me know your opinions and suggestions in the comment section below<br>\nAlso let me know your opinion about my work through my linkedIn profile or through mail.<br>\nLinkedIn - [Victor Basu](https:\/\/www.linkedin.com\/in\/victor-basu-520958147\/)<br>\nGmail - basu369victor@gmail.com","f6a9014b":"## COVID-19 detection with Heat-map visualization\n![front image](https:\/\/0f14676b303fd91881eb-98dd17e178263eba3c55ca6434a72b9d.ssl.cf5.rackcdn.com\/img\/longform_content_images\/53hnt2dj1JWD31uWuxXKjj.jpg)","25fe9295":"## View\nwe, are going to train our model against the only \"PA\" view of the lungs X-ray images.","5442a2fd":"## Introduction\nThis notebook is demonstrates detection of COVID-19 affected lungs, and also detection of the region of lungs affected by COVID-19. The notebook also demonstrates my way of handeling and utilizing an imbalanced dataset. <br>\nBy the time we started implementing our model there were research papers published based on detection of COVID-19 from lungs X-Rays, but all these papers and articles explained a binary classification model where they classified between a COVID-19 affected and healthy lungs. \n<br>\nThe improvement and uniqueness that we have added to our project are that our model is not a binary classifier rather a multi-label classifier. we have handled the problem of classifying COVID-19 affected lungs from other different lungs disease because in real life scenario a patient might get affected by other lungs disease and as from our current situation in this pandemic, any doctor could see similar lungs symptoms like COVID and mark that patient with an affected COVID label and then he might go through wrong treatment. We have focused more on overall lungs disease classification than classifying among healthy and COVID affected.\n<br>\nBy adding more lungs disease for classification we had made our model more robust as from the latest research, a lot of respiratory tract disease shows similarly affected lungs or respiratory tract symptoms as in the case of COVID-19.","a31a25b4":"## Download Dataset","a8659220":"## Credits\n* [COVID-XPERT: ANAI POWEREDPOPULATIONSCREENING OFCOVID-19 CASES USINGCHESTRADIOGRAPHYIMAGES](https:\/\/arxiv.org\/pdf\/2004.03042v1.pdf)\n* [CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rayswith Deep Learning](https:\/\/arxiv.org\/pdf\/1711.05225v3.pdf)\n* [Grad-CAM: Visual Explanations from Deep Networksvia Gradient-based Localization](https:\/\/arxiv.org\/pdf\/1610.02391.pdf)","ca947d26":"\"X_test\" would be used later for testing detection of affected section of lungs through heat-map","63325cb3":"## Balancing the un-balanced\nAs you could clearly see that COVID affected x-ray images are ten times more in number than other classes. So in the next cell, we are going to bring down the number of images under COVID.<br>\nThe excess images of COVID that has been removed from the dataset would be used to detect the region of the lungs that has been affected by COVID-19 through heatmap visualization","3366ddaf":"## Building the model\nI have used DenseNet121, for training the model. But I would definitely suggest you to try efficient net to see If there is any possible scope for performance improvement of the model.","868701a4":"## GradCAM( Implementation + Visualization)","3993df54":"## Import libraries","ab77a3d8":"## Some of the remarkable results\nSome of the remarkable results produced by the model during prediction.\n![heat-map prediction-1](https:\/\/lh3.googleusercontent.com\/U_hImjeGdjDEN9AH6_esN8hGSif24hQvxFis8Tn63bXMDpzIf7jpGOBt1XyfSn0ebRoWVPe51ebVAmYxKvEZ-r-yWDVvP6CDpZomnrfFFnCNKpmyEP27ut6Bev_PzR97IEc3YMu3aJQBv-lTvWrp6rLq9jJHj_Sf8XeWmBuVN-NKlKi7YgQ3-r-Bjod1vW8piwp40jHsTIhTm9jvUwcx1MI8X-n-BjhiH4G27_kSR-JEiwvbHKGacQ_YFFwPWg_efifXteHQvdroshKpbNXBXEQ6OU3pFsTcvMRrX99EI0zAtEgG03AwjVIP47Fs_V7Tj-IbpfqHXLwwiFzZckkCbnkwONA6O6wSx8NvhM8liUHRuKoF3yxlKrdd4hsvtR6H2lpfMIWvLIIyEYH9yMrHt0PBF1SWRpdHuS_UhtWUAKKgT3boE-DHBAOk16EG1567hXCeKi0HMJMf1-xiRxq6V4-LUXOSOmVGbP8HxvhNRGYWKRum_LMfiO90KLWrTw3ZD0dzlmf7lZmbt-2ztSMSiPJaT-a1jCPbBgM9qB5UwvFT4oR_OsXlbcFL4epnxw36JxdoiP2wl7kyp6fY1-EKBtiyEK06GmpX6iJlsZ2Gsxqcud--kyGkSAaBoPD4YB7twwplUZVR-fKwc0e3hi6S4Gb9AS9vyS2VZk1QYCt6rJih_GMttIEIcBpvoC0H=w925-h461-no?authuser=1)\n![heat-map prediction-2](https:\/\/lh3.googleusercontent.com\/CYttLLPoErAu9s-HwdFHj1NdsUT2o3SB1jBwMSlagCR2hot6HCwPYOLQH0X8C_eDqldtw4hlLX0h0J88W5RG2dmQ4bSw7nhGBEyd3LOIbrUwWAQhJaw8Rv9uFccX4KzBnmIoC5rkNyg0-f4GleES6nkf-cdjsXKGbO0Z2a7l6ppaeOPfRqOVSFR5Ww_63hG_9xkNbKG46oJCbTi-JgnfBCz19EAeY4OiCvmT6BGA_OZRzd9B-pJZc6yQytj9PCFBBmTvQMuyiUp1oxA0rvMyAIU1Db4dIp35zMh-nvt70BZnfumwe1Ki0nevI4WXblaSmH_Q48ixABD95t4RxzRxhlvtLlCFneRJVAb7UygQyvZA1fm4nxwFgldG8EF9BkPgwXxjGUYGTNr4bjVieL2fNiGz3avmR1Ka-PkjjfIHk_dWegaVdGENDRvmwOsFJ933k64nZG2fJQbmgS98Fm56RiDPQOB88PuWFKJBBjD7CzsgiidLzrohSoEE2t38g3enpDQl9eqTkT-HPyQnjnktSo8IkaW9ffQdNbCVqCCdRrn3BcUIgyMrsRGEt6nqu7Jn8vJUkcKH0rAwdgzzaBAGpciB4HF4V4ic4HWO9QxMHL4Z-er7qeVrVt41Ddaw9LIfWbWC3EFIGPxELCdR2jLeJigslumuibmFY595AX_PPG_I8gCQSLw5IbVhIK0F=w226-h222-no?authuser=1)\n![heat-map prediction-3](https:\/\/lh3.googleusercontent.com\/AVAP1jRMC3opmun14I3EQm5zFXaLRVpK-0BwZSVJIrGgtCgN8SBI-wiG61W7CZ2sccFp2tIElK7kcKUiDYPoTfMIXD9kWCAAh2bPLEiNPaneIsJ51kfSqxPPWPSXUvtgdsAGDCyNphhqsDEDmn5MKbNqf0xT6JpEKCjeVc9ea7Tv4imyLe9IH_xe7LYloZ-MjMGGvuUACQbZEuWC1vnHCZQS7TFmWxexuk9R2jXLantitx1dVEKXc4_lNJXNT9MUt-z5rWnejuRR8vMU2_SyBLUNaamo9Dkw6cPcibD7NVDJdNzeiytFcBC-FQfMe8ngl8hMFgjKR-_NnRnEIuM6PSgG4bPEeyxESakCaNVeMVRORAZFVVclyVdgmqrMBS3F46FcBoECw5LVur6gtbvZ2snjxZiAV7AsoaRTsR6z2KnJKvkKkmcSPIw8f-y0pH9FqPuyCkKuFATv1pppuMoOPyzt4gbDSdVgEqb6Qc_Vng7nmYMwFLBHBYzx4WX3js6xttLfbcaq17_DBC-_qNpRioUO5iXosgi8enlt2O8H8dbwhJoCvjpxBioKvxGsv-jbgzZzdsFPbWJMCcvHazSncR4t9qrQoQoOVyvVY51kgpP2a0rB3LT9DpJMFFHY3UVRUPksNLAnKgetcMkUjv3Y0WwwqXujYnZab_cyzqtlKWV5NtMH1ZKTN2UG6m8O=w234-h215-no?authuser=1)\n![heat-map prediction-4](https:\/\/lh3.googleusercontent.com\/g-4_SftGZMkOPJjrxXj-2GV5CrjYXvq-wn75PHFKCfDhw9BqYrI7cQYMRodhq0_zzBNb2oOTo9bAkuwW2hUSeCzpuy1eHZs_dyDMvzE0emu4vfro2AVd063nqTOkxi5yBuRArA9tEfhaQaGgEk2GGoytLFu3VEidyHpu9Z5hxN0Yt7WGyNX_ek9cEAmKY4ZlLdGdQfJC9wPEI6PEbd_N1toul_UrIEakAKq9eb0vXJzPIeQd9O-DdaKmTuRpwOUVoBgnG1s8rXVu8U2T144FC5LktGo9O44blYy3tAFr9L-MNTkiNCFmlzSFz2FXdmysv2RB-A3wMz-DVPpHxoisob1l443Ws-Qij19FJVKa791Aykj6f2Jloqw3E8vJgpwYgnupEeP92ms_gZR8Z3BfeSE3QlsEZ6SaxCJ31zoPafZpJlCLavF1OYb9h-ClAyzuiWJGfoYAnmQ0IE-uhPcZBWWn6G4Cd2ibYHHCC9m6kQp3VZTexvu37L9bVIWqmfC_uGpI-AtVX1BIsBye1NdwLEWL1kPyNoVs6RdEMymD2EEeVc56ilsqzV-fppU8V4X52a-f7gw5O8u8Z0sRAYe2CdIUByq7ht0hyi5S4jMbzfc3cNP3n4U8CEjDxrbr-F64MSTLGdGPhZdxroVL0fUC4qvMZ9dMckDI9H0LJHhjfEcKAvzsud40H-KdE-xI=w229-h230-no?authuser=1)","f47a8ad8":"## Training the model","cdb08b85":"X_test, which was extracted before would be used as a dataset for the heat-map visualization through gradcam.","2ea062b4":"## Heatmap Visualization\nAll the images in the plot shown below are covid-19 lungs x-ray images, over which we have done heatmap visualization based on predictions done by the model.","a47e80bd":"## Visualizing the dataset"}}