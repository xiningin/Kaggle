{"cell_type":{"7b090844":"code","e0ec4018":"code","2660ca06":"code","dbdb3a1b":"code","325ca0df":"code","8fec49dd":"code","0f7545a4":"code","4610e271":"code","91681676":"code","38d41313":"code","ccf3ac23":"code","d1bf9eb4":"code","deda7b41":"code","389c8800":"code","9e9c456e":"code","07b97127":"code","62e75e2f":"markdown","22701622":"markdown","93e3c978":"markdown","461a56a7":"markdown","96b429c7":"markdown","ce576b6a":"markdown","1d34fb7d":"markdown","7fb9d11a":"markdown","4a75bdf5":"markdown","5ee43551":"markdown","ad79286b":"markdown","8c6be444":"markdown"},"source":{"7b090844":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0ec4018":"train_sample_sub = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/sample_submission.csv')\ntrain_data = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/test.csv\")\n\ntrain_data","2660ca06":"#L\u1ea5y ra t\u1eadp c\u00e2u h\u1ecfi Insincere c\u00f3 nh\u00e3n 1\ninsincere_data = train_data[train_data.target == 1]\n#L\u1ea5y ra t\u1eadp c\u00e2u h\u1ecfi Sincere c\u00f3 nh\u00e3n 0\nsincere_data = train_data[train_data.target == 0]","dbdb3a1b":"#Infor c\u1ee7a t\u1eadp train\ninsincere_data.info()","325ca0df":"train_data.target.value_counts()","8fec49dd":"import matplotlib.pyplot as plt\nimport pandas as pd\n\ntemp = train_data['target'].value_counts(normalize=True).reset_index()\n\ncolors = ['#9f32ff', '#5fff30']\nexplode = (0.05, 0.05)\n \nplt.pie(temp['target'], explode=explode, labels=temp['index'], colors=colors,\n         autopct='%1.1f%%', shadow=True, startangle=0)\n \nfig = plt.gcf()\nfig.set_size_inches(10, 5)\nplt.axis('equal')\nplt.show()\nprint( \"C\u00e2u h\u1ecfi Sincere chi\u1ebfm:\", sincere_data.shape[0] \/ train_data.shape[0] * 100)\nprint( \"C\u00e2u h\u1ecfi Insincere chi\u1ebfm:\", insincere_data.shape[0] \/ train_data.shape[0] * 100)","0f7545a4":"from wordcloud import STOPWORDS\nfrom collections import defaultdict\nimport seaborn as sns\nimport nltk\n\ndef ngram_extractor(text, n_gram):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# L\u1ea5y max_row c\u00e1c t\u1eeb d\u1ea1ng n_gram xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong d\u1eef li\u1ec7u\ndef generate_ngrams(df, col, n_gram, max_row):\n    temp_dict = defaultdict(int)\n    for question in df[col]:\n        for word in ngram_extractor(question, n_gram):\n            temp_dict[word] += 1\n    temp_df = pd.DataFrame(sorted(temp_dict.items(), key=lambda x: x[1])[::-1]).head(max_row)\n    temp_df.columns = [\"word\", \"wordcount\"]\n    return temp_df\n\ndef comparison_plot(df_1,df_2,col_1,col_2):\n    fig, ax = plt.subplots(1, 2, figsize=(20,10))\n    \n    sns.barplot(x=col_2, y=col_1, data=df_1, ax=ax[0])\n    sns.barplot(x=col_2, y=col_1, data=df_2, ax=ax[1])\n\n    ax[0].set_xlabel('Word count', size=12)\n    ax[0].set_ylabel('Words', size=12)\n    ax[0].set_title('Top words in sincere questions', size=16)\n\n    ax[1].set_xlabel('Word count', size=12)\n    ax[1].set_ylabel('Words', size=12)\n    ax[1].set_title('Top words in insincere questions', size=16)\n\n    fig.subplots_adjust(wspace=0.25)\n    \n    plt.show()","4610e271":"#L\u1ea5y ra 20 t\u1eeb d\u1ea1ng unigram xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong 2 lo\u1ea1i t\u1eadp c\u00e2u h\u1ecfi\nsincere_1gram = generate_ngrams(sincere_data, 'question_text', 1, 20)\ninsincere_1gram = generate_ngrams(insincere_data, 'question_text', 1, 20)\n\ncomparison_plot(sincere_1gram,insincere_1gram,'word','wordcount')","91681676":"#L\u1ea5y ra 20 t\u1eeb d\u1ea1ng bigram xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong 2 lo\u1ea1i t\u1eadp c\u00e2u h\u1ecfi\nsincere_2gram = generate_ngrams(sincere_data, 'question_text', 2, 20)\ninsincere_2gram = generate_ngrams(insincere_data, 'question_text', 2, 20)\n\ncomparison_plot(sincere_1gram,insincere_1gram,'word','wordcount')","38d41313":"#L\u1ea5y ra 20 t\u1eeb d\u1ea1ng trigram xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong 2 lo\u1ea1i t\u1eadp c\u00e2u h\u1ecfi\nsincere_3gram = generate_ngrams(sincere_data, 'question_text', 3, 20)\ninsincere_3gram = generate_ngrams(insincere_data, 'question_text', 3, 20)\n\ncomparison_plot(sincere_1gram,insincere_1gram,'word','wordcount')","ccf3ac23":"import re\nimport nltk\nimport string\nfrom unidecode import unidecode\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom wordcloud import STOPWORDS\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"}\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean(text):        \n    # Convert to unicode \n    text = unidecode(text).encode(\"ascii\")\n    text = str(text, \"ascii\")\n\n    # chuy\u1ec3n v\u1ec1 ch\u1eef th\u01b0\u1eddng, b\u1ecf c\u00e1c link, k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t, ch\u1eef s\u1ed1.\n    text = text.lower()\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  \n    text = re.sub('\\n', '', text)\n    text = re.sub('[\u2019\u201c\u201d\u2026]', ' ', text)  \n    text = ''.join(i for i in text if not i.isdigit())\n\n    # Chuy\u1ec3n c\u00e1c t\u1eeb vi\u1ebft t\u1eaft trong t\u1eeb \u0111i\u1ec3n v\u1ec1 d\u1ea1ng th\u01b0\u1eddng\n    tokens = word_tokenize(text)\n    tokens = [mispell_dict.get(token) if (mispell_dict.get(token) != None) else token for token in tokens]\n    text = \" \".join(tokens)\n\n    # Remove stop-words   \n    tokens = word_tokenize(text)\n    tokens_without_sw = [word for word in tokens if not word in STOPWORDS]\n\n    # Chuy\u1ec3n bi\u1ebfn th\u1ec3 ng\u1eef ph\u00e1p c\u1ee7a t\u1eeb v\u1ec1 t\u1eeb g\u1ed1c\n    text = [lemmatizer.lemmatize(word) for word in tokens_without_sw ] \n    text = \" \".join(text)\n\n    return text","d1bf9eb4":"#Insert c\u1ed9t c\u00e1c c\u00e2u h\u1ecfi \u0111\u00e3 clean v\u00e0o t\u1eadp train\ntrain_data['clean_questions'] = train_data['question_text'].apply(clean)\ntrain_data","deda7b41":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nX = train_data['clean_questions']\nY = train_data['target']\n\ncount_vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,3))\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n\ncount_train = count_vectorizer.fit(X)\n\nX_vec_train = count_train.transform(X_train)\nX_vec_test = count_train.transform(X_test)","389c8800":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, recall_score ,f1_score\nfrom sklearn.metrics import classification_report\n\nmod = LogisticRegression(n_jobs=10, solver='saga',class_weight = 'balanced', C=0.1, verbose=1)\nmod.fit(X_vec_train, Y_train)\nY_pred = mod.predict(X_vec_test)","9e9c456e":"print('Recall: ', recall_score(Y_pred, Y_test))\nprint('F1 score :', f1_score(Y_pred, Y_test), '\\n')\nprint(classification_report(Y_test, Y_pred))","07b97127":"test_data['clean_questions'] = test_data['question_text'].apply(clean)\nX_test_vec = count_vectorizer.transform(test_data['clean_questions'])\npredictions = mod.predict(X_test_vec)\nsubmission = pd.DataFrame({'qid': test_data['qid'].values})\nsubmission['prediction'] = predictions\nsubmission.to_csv('submission.csv', index=False)","62e75e2f":"Ta th\u1ea5y c\u00f3 1225312 c\u00e2u h\u1ecfi c\u00f3 g\u00e1n nh\u00e3n 0 (Sincere) l\u1edbn h\u01a1n r\u1ea5t nhi\u1ec1u so v\u1edbi 80810 c\u00e2u h\u1ecfi c\u00f3 nh\u00e3n 1 (Insincere)\n\nTa xem x\u00e9t t\u1ec9 l\u1ec7 n\u00e0y k\u1ef9 h\u01a1n \u1edf bi\u1ec3u \u0111\u1ed3 b\u00ean d\u01b0\u1edbi:","22701622":"# 4. Submission","93e3c978":"**Ph\u00e2n t\u00edch c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t b\u1eb1ng n-gram** : l\u00e0 t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a n k\u00ed t\u1ef1 ( ho\u1eb7c t\u1eeb ) li\u00ean ti\u1ebfp nhau c\u00f3 trong d\u1eef li\u1ec7u c\u1ee7a corpus.\n* Unigram, m\u00f4 h\u00ecnh v\u1edbi n=1, t\u1ee9c l\u00e0 ta s\u1ebd t\u00ednh t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a m\u1ed9t k\u00ed t\u1ef1 (t\u1eeb), nh\u01b0: \u201ck\u201d, \u201ca\u201d,\u2026\n* bigrams v\u1edbi n=2, m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u trong vi\u1ec7c ph\u00e2n t\u00edch c\u00e1c h\u00ecnh th\u00e1i cho ng\u00f4n ng\u1eef, v\u00ed d\u1ee5 v\u1edbi c\u00e1c ch\u1eef c\u00e1i ti\u1ebfng Anh, \u2018th\u2019,\u2019he\u2019,\u2019in\u2019,\u2019an\u2019,\u2019er\u2019 l\u00e0 c\u00e1c c\u1eb7p k\u00ed t\u1ef1 hay xu\u1ea5t hi\u1ec7n nh\u1ea5t.\n* trigrams v\u1edbi n=3.","461a56a7":"**\u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh:**\n* \u0110\u00e2y l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh kh\u00e1 \u0111\u01a1n gi\u1ea3n v\u1edbi k\u1ebft qu\u1ea3 nh\u1eadn \u0111\u01b0\u1ee3c \u0111\u1ec1u \u1edf m\u1ee9c trung b\u00ecnh.\n* Tuy m\u00f4 h\u00ecnh kh\u00f4ng y\u00eau c\u1ea7u d\u1eef li\u00eau l\u00e0 linearly separable, nh\u01b0ng \u0111\u01b0\u1eddng ph\u00e2n chia gi\u1eefa 2 class v\u1eabn l\u00e0 \u0111\u01b0\u1eddng tuy\u1ebfn t\u00ednh, n\u00ean c\u01a1 b\u1ea3n, b\u1ed9 d\u1eef li\u1ec7u c\u0169ng ph\u1ea3i r\u1ea5t g\u1ea7n v\u1edbi linearly separable, trong b\u00e0i to\u00e1n n\u00e0y, theo em m\u00f4 h\u00ecnh s\u1ebd ho\u1ea1t \u0111\u1ed9ng kh\u00f4ng t\u1ed1t do d\u1eef li\u1ec7u l\u00e0 c\u00e1c t\u1eeb m\u00e0 \u0111\u00f4i khi r\u1ea5t th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n \u1edf c\u1ea3 2 class h\u1eefu \u00edch v\u00e0 \u0111\u1ed9c h\u1ea1i n\u00ean r\u1ea5t kh\u00f3 \u0111\u1ec3 m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 d\u1ef1 \u0111o\u00e1n.\n* M\u00f4 h\u00ecnh c\u00f2n y\u00eau c\u1ea7u c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u l\u00e0 \u0111\u1ed9c l\u1eadp v\u1edbi nhau nh\u01b0ng trong b\u00e0i to\u00e1n n\u00e0y r\u1ea5t kh\u00f3 s\u1ea3y ra, v\u00ec c\u00e1c t\u1eeb c\u00f9ng xu\u1ea5t hi\u1ec7n \u0111\u00f4i khi \u1ea3nh h\u01b0\u1edfng l\u1eabn nhau **=> \u0110i\u1ec1u n\u00e0y d\u1eabn \u0111\u1ebfn vi\u1ec7c \u00e1p d\u1ee5ng m\u1ea1ng neural th\u00f4ng th\u01b0\u1eddng v\u1edbi \u0111\u1ea7u v\u00e0o v\u00e0 \u0111\u1ea7u ra \u0111\u1ed9c l\u1eadp v\u1edbi nhau kh\u00f4ng ph\u00f9 h\u1ee3p v\u1edbi d\u1eef li\u1ec7u d\u1ea1ng n\u00e0y**.\n* Thu\u1eadt to\u00e1n c\u0169ng ch\u1ea1y ch\u1eadm h\u01a1n so v\u1edbi NB v\u00ec m\u1ed7i l\u1ea7n c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 ta l\u1ea1i ph\u1ea3i t\u00ednh l\u1ea1i \u0111\u1ea1o h\u00e0m c\u1ee7a Loss funcion v\u1edbi t\u1eebng bi\u1ebfn.<br>\nTa c\u00f3 th\u1ec3 gi\u1ea3m th\u1eddi gian training c\u1ee7a m\u00f4 h\u00ecnh b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng c\u00e1c mini batch nh\u01b0ng \u1edf m\u00f4 h\u00ecnh n\u00e0y em s\u1ebd kh\u00f4ng \u0111\u1ec1 c\u1eadp \u0111\u1ebfn n\u00f3 do em \u0111ang h\u01b0\u1edbng \u0111\u1ebfn m\u1ed9t m\u00f4 h\u00ecnh c\u00f3 k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n.","96b429c7":"# 2. Ph\u00e2n t\u00edch v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u\nD\u1eef li\u1ec7u v\u00e0o l\u00e0 c\u00e1c c\u00e2u h\u1ecfi tr\u00ean Quora. Nhi\u1ec7m v\u1ee5 ph\u1ea3i l\u00e0m l\u00e0 ph\u00e2n lo\u1ea1i c\u00e1c c\u00e2u h\u1ecfi \u1ea5y.\nM\u1ed9t c\u00e2u h\u1ecfi bao g\u1ed3m 3 thu\u1ed9c t\u00ednh:\n* qid : id duy nh\u1ea5t nh\u1eadn di\u1ec7n c\u00e2u h\u1ecfi.\n* question_text: c\u00e2u h\u1ecfi d\u1ea1ng text.\n* target: nh\u00e3n c\u1ee7a c\u00e2u h\u1ecfi, b\u1eb1ng l\u00e0 1 n\u1ebfu l\u00e0 \"Insincere\", ng\u01b0\u1ee3c l\u1ea1i th\u00ec b\u1eb1ng 0.\n\nTa s\u1eed d\u1ee5ng question_text l\u00e0 \u0111\u1ea7u v\u00e0o X, target l\u00e0 label y\n","ce576b6a":"**S\u1ef1 d\u1ee5ng F1-score l\u00e0m metric**\n* Trung b\u00ecnh \u0111i\u1ec1u h\u00f2a gi\u1eefa precision (\u0111\u1ed9 ch\u00ednh x\u00e1c) v\u00e0 recall (\u0111\u1ed9 bao ph\u1ee7). $\\frac {2}{F_1} = \\frac {1}{Precision} + \\frac {1}{Recall}$\n* Precision: \u0111\u00e1nh gi\u00e1 bao nhi\u00eau % k\u1ebft lu\u1eadn c\u1ee7a model l\u00e0 ch\u00ednh x\u00e1c-True. $Precision = \\frac{TP}{TP+FP}$\n* Recall: \u0111\u00e1nh gi\u00e1 bao nhi\u00eau % positive samples m\u00e0 model nh\u1eadn \u0111\u01b0\u1ee3c. $Recall = \\frac{TP}{TP+FN}$","1d34fb7d":"# 3. Training v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy Logistic\n* V\u1edbi **input** t\u1eadp c\u00e2u h\u1ecfi sau khi clean v\u00e0 convert X_train_vec $\\in$ $R^d$, **output** l\u00e0 nh\u00e3n Y_train $\\in \\{0, 1\\}$.\n* M\u00f4 h\u00ecnh: $Y|X=x \\sim Ber(y|\\sigma(f(x))$ v\u1edbi $f(x) = w^Tx+w_0$\n* Cho $f(x)$ \u0111i qua h\u00e0m sigmoid: $\\sigma(z) = \\frac 1 {1+e^{-z}}$\n* Hu\u1ea5n luy\u1ec7n b\u1ed9 tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh $\\theta = (w, w_0)$: T\u00ednh Likelihood => T\u00ednh h\u00e0m l\u1ed7i Negative Loglikelihood (NLL) => Xu\u1ed1ng \u0111\u1ed3i b\u1eb1ng \u0111\u1ea1o h\u00e0m c\u1eadp nh\u1eadt b\u1ed9 tham s\u1ed1.\n\nChi ti\u1ebft m\u00f4 h\u00ecnh: https:\/\/excessive-source-1c9.notion.site\/16-09-2021-H-i-quy-Logistics-cdcc911147e5458ba9203b58e6bd0099#dd32774a49cc44ac81c61578b831ad47","7fb9d11a":"**\u0110\u00e1nh gi\u00e1 d\u1eef li\u1ec7u**\n\nT\u1eeb c\u00e1c bi\u1ec3u \u0111\u1ed3 tr\u00ean ta c\u00f3 th\u1ec3 \u0111\u01b0a ra nh\u1eefng \u0111\u00e1nh gi\u00e1 v\u1ec1 d\u1eef li\u1ec7u:\n* S\u1ed1 c\u00e2u h\u1ecfi Sincere chi\u1ebfm t\u1edbi 93,8% cao h\u01a1n r\u1ea5t nhi\u1ec1u so v\u1edbi 6,2% c\u00e1c c\u00e2u h\u1ecfi Insincere\n* C\u00e1c c\u00e2u h\u1ecfi Insincere ch\u1ee7 y\u1ebfu bao g\u1ed3m c\u00e1c t\u1eeb nh\u01b0 trump, women, white, men, indian, muslims, black, americans, girls, indians, sex, india\n* 3 c\u00e2u h\u1ecfi Insincere bigram nhi\u1ec1u nh\u1ea5t l\u00e0 \"Donald Trump\", \"White People\", \"Black People\" => Li\u00ean quan \u0111\u1ebfn ch\u1ee7ng t\u1ed9c.\n\n**K\u1ebft lu\u1eadn**\n* C\u00e1c c\u00e2u h\u1ecfi Insincere xoay quanh c\u00e1c t\u00ecnh hu\u1ed1ng gi\u1ea3 \u0111\u1ecbnh, tu\u1ed5i t\u00e1c, ch\u1ee7ng t\u1ed9c, v.v.\n* C\u00e1c c\u00e2u h\u1ecfi Sincere li\u00ean quan \u0111\u1ebfn c\u00e1c tips, l\u1eddi khuy\u00ean, g\u1ee3i \u00fd, s\u1ef1 th\u1eadt, ...","4a75bdf5":"**H\u1ecd t\u00ean**: Nguy\u1ec5n H\u1eefu H\u01b0ng\n\n**MSV**: 18020612\n\n","5ee43551":"**X\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o**\n\n\n\n***Ta c\u1ea7n x\u1eed l\u00fd nh\u1eefng v\u1ea5n \u0111\u1ec1 sau:***\n* Chuy\u1ec3n v\u1ec1 d\u1ea1ng unicode.\n* Chuy\u1ec3n v\u1ec1 ch\u1eef in th\u01b0\u1eddng (lowercase).\n* B\u1ecf d\u1ea5u, link, ch\u1eef s\u1ed1 (Remove punctuation, link, number)\n* Ph\u00e2n t\u00e1ch m\u1ed9t c\u00e2u th\u00e0nh c\u00e1c t\u1eeb, c\u1ee5m t\u1eeb,... c\u00f3 ngh\u0129a (Tokenization).\n* Chuy\u1ec3n \u0111\u1ed5i, r\u00fat g\u1ecdn c\u00e1c t\u1eeb v\u1ec1 t\u1eeb g\u1ed1c (Stemming ho\u1eb7c Lemmazation).\n* Lo\u1ea1i b\u1ecf c\u00e1c stopword nh\u01b0 (Remove stopword) nh\u01b0 a, an, or, of, the, ... \n\n\u0110\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u v\u00e0o, ta s\u1eed d\u1ee5ng Natural Language Toolkit(NLTK) - l\u00e0 b\u1ed9 c\u00f4ng c\u1ee5 ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean, l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng Python h\u1ed7 tr\u1ee3 x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. B\u1eb1ng c\u00e1ch cung c\u1ea5p c\u00e1c c\u01a1 ch\u1ebf v\u00e0 k\u1ef9 thu\u1eadt x\u1eed l\u00fd ng\u00f4n ng\u1eef ph\u1ed5 bi\u1ebfn, n\u00f3 gi\u00fap cho vi\u1ec7c x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean tr\u1edf l\u00ean d\u1ec5 d\u00e0ng, nhanh ch\u00f3ng h\u01a1n v\u00e0 c\u00f3 t\u00e1c d\u1ee5ng l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u, x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o cho c\u00e1c thu\u1eadt to\u00e1n Machine Learning.\n\nNgo\u00e0i ra c\u00f2n gi\u00fap x\u1eed l\u00fd c\u00e1c stopwords - l\u00e0 c\u00e1c t\u1eeb c\u00f3 t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u01b0 the, to... c\u00e1c t\u1eeb n\u00e0y th\u01b0\u1eddng mang \u00edt gi\u00e1 tr\u1ecb \u00fd ngh\u0129a v\u00e0 kh\u00f4ng kh\u00e1c nhau nhi\u1ec1u trong c\u00e1c v\u0103n b\u1ea3n kh\u00e1c nhau. V\u00ed d\u1ee5 t\u1eeb \"the\" hay \"to\" th\u00ec \u1edf v\u0103n b\u1ea3n n\u00e0o n\u00f3 c\u0169ng kh\u00f4ng b\u1ecb thay \u0111\u1ed5i v\u1ec1 \u00fd ngh\u0129a. \nV\u00ed d\u1ee5: i, the, my, me, ...","ad79286b":"# 1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\nQuora l\u00e0 m\u1ed9t trang web h\u1ecfi \u0111\u00e1p (Q&A) \u0111\u01b0\u1ee3c c\u1ed9ng \u0111\u1ed3ng ng\u01b0\u1eddi s\u1eed d\u1ee5ng t\u1ea1o l\u1eadp, tr\u1ea3 l\u1eddi, v\u00e0 bi\u00ean t\u1eadp. Quora t\u1eadp h\u1ee3p c\u00e1c c\u00e2u h\u1ecfi v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi cho c\u00e1c ch\u1ee7 \u0111\u1ec1 ph\u00e1t sinh trong cu\u1ed9c s\u1ed1ng hay c\u00f4ng vi\u1ec7c h\u00e0ng ng\u00e0y.\n\nM\u1ed9t v\u1ea5n \u0111\u1ec1 t\u1ed3n t\u1ea1i \u0111\u1ed1i v\u1edbi b\u1ea5t k\u1ef3 trang web l\u1edbn n\u00e0o hi\u1ec7n nay l\u00e0 l\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 x\u1eed l\u00fd n\u1ed9i dung \u0111\u1ed9c h\u1ea1i v\u00e0 g\u00e2y chia r\u1ebd. Quora mu\u1ed1n gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y tr\u1ef1c ti\u1ebfp \u0111\u1ec3 gi\u1eef cho n\u1ec1n t\u1ea3ng c\u1ee7a h\u1ecd tr\u1edf th\u00e0nh m\u1ed9t n\u01a1i m\u00e0 ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 c\u1ea3m th\u1ea5y an to\u00e0n khi chia s\u1ebb ki\u1ebfn th\u1ee9c c\u1ee7a h\u1ecd v\u1edbi th\u1ebf gi\u1edbi.\n\nTa c\u1ea7n ph\u1ea3i x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c gi\u1eefa nh\u1eefng c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh, m\u1ee5c \u0111\u00edch t\u1ed1t (Sincere) v\u1edbi nh\u1eefng c\u00e2u h\u1ecfi mang t\u00ednh ti\u00eau c\u1ef1c, kh\u00f4ng \u0111\u00fang \u0111\u1eafn \u1ea5y (Insincere).\n\nInput: C\u00e2u h\u1ecfi tr\u00ean Quora (d\u1ea1ng text)\n\nOutput: G\u1eafn nh\u00e3n Insincere (1) or not (0)","8c6be444":"**Convert d\u1eef li\u1ec7u**\n\n\u00c1p d\u1ee5ng CountVectorizre, convert d\u1eef li\u1ec7u text v\u1ec1 d\u1ea1ng vector c\u1ee7a s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c token. Bag-of-Words l\u00e0 k\u1ef9 thu\u1eadt c\u1ed1t l\u00f5i cho v\u1ea5n \u0111\u1ec1 n\u00e0y, c\u00e1c ph\u01b0\u01a1ng ph\u00e1p bao g\u1ed3m:\n* T\u00e1ch d\u1eef li\u1ec7u th\u00e0nh c\u00e1c token\n* X\u00e1c \u0111\u1ecbnh tr\u1ecdng s\u1ed1 cho m\u1ed7i token \u1ee9ng v\u1edbi s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a n\u00f3."}}