{"cell_type":{"d00264fa":"code","b3df91f6":"markdown","fe48c76e":"markdown","1240bf3f":"markdown"},"source":{"d00264fa":"X = [\n        [0], \n        [1], \n        [2], \n        [3]\n]\ny = [0, 0, 1, 1]\n\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X, y)\n\nprint('Simply predicting class: ', neigh.predict([[1.1]]))\n#[0]\n\nprint('Predicting w\/ probabilities: ', neigh.predict_proba([[0.9]]))\n#[[0.66666667 0.33333333]]","b3df91f6":"- Instead of sqashing to class labels to *whole numbers*, use majority vote as probabilities\n- Example,\n    - Consider for 10-NN\n        - **Case 1:** 1 `A`, 1`B` and 8`C`\n        - **Case 2:** 3 `A`, 2`B` and 5`C`\n    - Both predict class `C` but instead of just saying `class C`, present probabilities.\n        - **Case 1:** `[0.1, 0.1, 0.8]` instead of `[0, 0, 1]`\n        - **Case 2:** `[0.3, 0.2, 0.5]` instead of `[0, 0, 1]`\n\n**Advantages**\n\n- [Crossentropy](https:\/\/www.kaggle.com\/l0new0lf\/log-loss\/) works better w\/ **probability distributions**\n- Mainly, information is **NOT** lost","fe48c76e":"> *Majority vote by model - `9\/10` is a lot confident while vote `6\/10` comparably isn't!*","1240bf3f":"# KNN Probability Targets"}}