{"cell_type":{"83ab025a":"code","2021225e":"code","576062f7":"code","cff93abb":"code","ca49861d":"code","d0f930a1":"code","773b1d7a":"code","db4de822":"code","6772cddf":"code","8a53dfbe":"code","65b2cffa":"code","88102ea4":"code","ded88f1e":"code","a173b984":"code","112ff469":"code","3ea9e72a":"code","250c1b32":"code","315f1f58":"code","3bd5b7f7":"code","22e7a7dd":"code","2f22eb3a":"code","e746760e":"code","04893bdc":"code","54cbd660":"code","0c733fc8":"code","381b6dc3":"code","8bfbddd0":"code","98876636":"code","dc340c3d":"code","dc6030d8":"code","e8bec975":"code","e158f82b":"code","5a57d71d":"code","3a4fa2a2":"code","2fc700e2":"code","e3dbba72":"code","f0fa1614":"code","2ab699d5":"code","90e95dc0":"code","5ec3a907":"code","a979bb40":"code","f2cf0cc9":"code","741262a4":"code","14e39c63":"code","62ffca03":"code","8681375e":"code","6dd5420e":"code","de35b984":"code","b02430d8":"code","55a77df4":"code","477791c0":"code","24458f16":"code","8be2aa45":"code","e04502ba":"code","5ba9720c":"code","7a5c4614":"code","030147eb":"code","739b380a":"code","db429288":"code","a4ccf696":"code","b28b0f18":"code","d207e9c5":"code","f2d668e7":"code","3abfa47e":"code","748b7557":"code","d9fc3004":"code","aa09c7ae":"markdown","2503f24e":"markdown","576adaff":"markdown","f9524451":"markdown","60999fe5":"markdown","62216a5d":"markdown","252defd7":"markdown","e3c78696":"markdown","419188cf":"markdown","3d285140":"markdown","f8e72438":"markdown","e34cc9fe":"markdown","601c60d4":"markdown","6ecd9ed1":"markdown","2bed25fa":"markdown","faad4d4c":"markdown","adc4f650":"markdown","6f18a64e":"markdown","29c0aaea":"markdown","08ff7e69":"markdown","823e60f9":"markdown","56ff63f5":"markdown","b6eba4fd":"markdown","17927469":"markdown","c2e23640":"markdown","7e08667d":"markdown","b2201198":"markdown","43365ae6":"markdown","94e4a1e2":"markdown","d98cf87e":"markdown","e59177d1":"markdown","2345a9d3":"markdown","59a604af":"markdown","a43d3dfd":"markdown","f538b899":"markdown","92907a67":"markdown","2397a1b2":"markdown"},"source":{"83ab025a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2021225e":"# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Feature Engineering\nimport re\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Model Training\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split","576062f7":"# Data Import\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","cff93abb":"# Which features are available?\ntrain_df.columns.values","ca49861d":"# Data Preview\ntrain_df.head()","d0f930a1":"train_df.info()\nprint('_'*40)\ntest_df.info()","773b1d7a":"# Overview of distributions\ntrain_df.describe()","db4de822":"train_df.Pclass.value_counts()[:20]","6772cddf":"# Passengers with upper class tickets are more likely to survive\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8a53dfbe":"train_df.Sex.value_counts()[:20]","65b2cffa":"# Female passengers are more likely to survive\ntrain_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","88102ea4":"train_df.SibSp.value_counts()[:20]","ded88f1e":"# It seems like a SibSp greater than 0 has a correlation with survival\ntrain_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a173b984":"train_df.Parch.value_counts()[:20]","112ff469":"# It seems like a Parch greater than 0 has a correlation with survival\ntrain_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3ea9e72a":"# Data is a bit wild\ntrain_df.Ticket.value_counts()[:20]","250c1b32":"#76.4% of values in this column are unique\ntrain_df.Ticket.nunique()\/train_df.Ticket.shape[0]*100","315f1f58":"# Looks like cabin and ticket represent similar data - although there are many missing values\ntrain_df.Cabin.value_counts()[:20]","3bd5b7f7":"#16.5% of values in this column are unique\ntrain_df.Cabin.nunique()\/train_df.Cabin.shape[0]*100","22e7a7dd":"train_df.Embarked.value_counts()[:20]","2f22eb3a":"# Cherbourg seems to correlate with survival\ntrain_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e746760e":"train_df.Name.value_counts()[:20]","04893bdc":"# 100% of names are unique\ntrain_df.Name.nunique()\/train_df.Name.shape[0]*100","54cbd660":"# Looks like people in their 20s are less likely to survive\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=40)","0c733fc8":"# Higher fare correlates with higher survival rate\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)","381b6dc3":"feature_df = train_df[[\"Pclass\", \"Sex\", \"Embarked\", \"Age\", \"Fare\", \"Survived\"]]","8bfbddd0":"combine_df = pd.concat([train_df,test_df])\n\nage_mean = combine_df['Age'].mean()\nembarked_mode = combine_df['Embarked'].mode()[0]\nfare_median = combine_df['Fare'].median()","98876636":"feature_df['Age'].fillna((age_mean), inplace=True)\nfeature_df['Embarked'].fillna((embarked_mode), inplace=True)\n\nfeature_df.info()","dc340c3d":"enc = OrdinalEncoder()\nfeature_df.loc[:, ['Sex', 'Embarked']] = enc.fit_transform(feature_df[['Sex', 'Embarked']])\nfeature_df","dc6030d8":"feature_df_1 = feature_df","e8bec975":"rf_clf = RandomForestClassifier(random_state=0)\n\nX = feature_df_1.drop([\"Survived\"], axis=1)\ny = feature_df_1[\"Survived\"]\n\nX_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf_clf.fit(X_train,y_train)\n\nrf_clf.score(X_eval, y_eval)","e158f82b":"scores = cross_val_score(rf_clf, X, y, cv=5)\nscores","5a57d71d":"train_df.columns","3a4fa2a2":"family_df = train_df[[\"SibSp\", \"Parch\", \"Survived\"]]","2fc700e2":"# We define a function that identifies whether a given passenger has family\ndef has_family(row):\n    if row[\"SibSp\"] > 0:\n        return 1\n    elif row[\"Parch\"] > 0:\n        return 1\n    else:\n        return 0","e3dbba72":"family_df[\"HasFamily\"] = family_df.apply(lambda row: has_family(row), axis=1)","f0fa1614":"family_df","2ab699d5":"family_df[[\"HasFamily\", \"Survived\"]].groupby(['HasFamily'], as_index=False).mean().sort_values(by='HasFamily', ascending=False)","90e95dc0":"name_df = train_df[[\"Name\", \"Survived\"]]","5ec3a907":"name_df","a979bb40":"# We define a function that extracts the title from the name\ndef extract_title(row):\n    match = re.search(\",\\s([A-Za-z]+).\", row[\"Name\"])\n    return match.group(1)\n\n# To develop my regex I used: https:\/\/regex101.com\/","f2cf0cc9":"name_df[\"Title\"] = name_df.apply(lambda row: extract_title(row), axis=1)","741262a4":"name_df.Title.value_counts()[:30]","14e39c63":"common_titles = [\"Mr\", \"Miss\", \"Mrs\", \"Master\"]\ndef common_title(row):\n    if row[\"Title\"] in common_titles:\n        return 1\n    else:\n        return 0","62ffca03":"name_df[\"CommonTitle\"] = name_df.apply(lambda row: common_title(row), axis=1)","8681375e":"name_df[[\"CommonTitle\", \"Survived\"]].groupby(['CommonTitle'], as_index=False).mean().sort_values(by='CommonTitle', ascending=False)","6dd5420e":"feature_df[\"HasFamily\"] = family_df[\"HasFamily\"]\nfeature_df[\"CommonTitle\"] = name_df[\"CommonTitle\"]","de35b984":"feature_df","b02430d8":"feature_df.drop([\"Survived\"], axis=1)","55a77df4":"rf_clf = RandomForestClassifier(random_state=0)\n\nX = feature_df.drop([\"Survived\"], axis=1)\ny = feature_df[\"Survived\"]\n\nX_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf_clf.fit(X_train,y_train)\n\nrf_clf.score(X_eval, y_eval)","477791c0":"scores = cross_val_score(rf_clf, X, y, cv=5)\nscores","24458f16":"# The last two features that were made have relatively low feature importance\nrf_clf.feature_importances_ ","8be2aa45":"# Bagging, Boosting, Stacking","e04502ba":"# Adaboost, XGBoost, Stacking\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression","5ba9720c":"rf_clf = RandomForestClassifier(n_jobs=-1)\n\nparam_dist = {\n    \"n_estimators\": [100, 200],\n    \"max_depth\": [None, 4, 8, 16],\n    \"min_samples_split\": [2, 4, 8],\n    \"min_samples_leaf\": [1, 2, 4,] \n}\n\nrf_opt = RandomizedSearchCV(rf_clf, param_dist, n_jobs=-1)\nrf_opt.fit(X,y)","7a5c4614":"rf_opt.best_params_","030147eb":"ada_clf = AdaBoostClassifier()\n\nparam_dist = {\n    \"n_estimators\": [50, 100, 200],\n    \"learning_rate\": [1, 0.8, 0.6]\n}\n\nada_opt = RandomizedSearchCV(ada_clf, param_dist, n_jobs=-1)\nada_opt.fit(X,y)","739b380a":"ada_opt.best_params_","db429288":"xgb_clf = xgb.XGBClassifier()\n\nparam_dist = {\n    \"booster\": [\"gbtree\", \"dart\"],\n    \"eta\": [0.1, 0.3, 0.5],\n    \"gamma\": [0, 1, 10, 100],\n    \"max_depth\": [2, 4, 6, 8]\n}\n\nxgb_opt = RandomizedSearchCV(xgb_clf, param_dist, n_jobs=-1)\nxgb_opt.fit(X,y)","a4ccf696":"estimators = [('rf', RandomForestClassifier(**rf_opt.best_params_)),\n             ('ada', AdaBoostClassifier(**ada_opt.best_params_)),\n             ('xgb', xgb.XGBClassifier(**xgb_opt.best_params_))]\n\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\nclf.fit(X_train, y_train)\nclf.score(X_eval, y_eval)","b28b0f18":"clf.fit(X, y)","d207e9c5":"# Building the test feature df\ntest_feature_df = test_df[[\"Pclass\", \"Sex\", \"Embarked\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Name\"]]\n\ntest_feature_df['Age'].fillna((age_mean), inplace=True)\ntest_feature_df['Embarked'].fillna((embarked_mode), inplace=True)\ntest_feature_df['Fare'].fillna((fare_median), inplace=True)\ntest_feature_df.loc[:, ['Sex', 'Embarked']] = enc.transform(test_feature_df[['Sex', 'Embarked']])\ntest_feature_df[\"HasFamily\"] = test_feature_df.apply(lambda row: has_family(row), axis=1)\ntest_feature_df[\"Title\"] = test_feature_df.apply(lambda row: extract_title(row), axis=1)\ntest_feature_df[\"CommonTitle\"] = test_feature_df.apply(lambda row: common_title(row), axis=1)","f2d668e7":"test_feature_df","3abfa47e":"test_feature_df = test_feature_df.drop([\"SibSp\", \"Parch\", \"Name\", \"Title\"], axis=1)","748b7557":"# Predicting the values\npredictions = clf.predict(test_feature_df)","d9fc3004":"output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","aa09c7ae":"## Categorical and Ordinal Variables\n### Pclass","2503f24e":"## Fare","576adaff":"### Name","f9524451":"## Numerical Variables\n### Age","60999fe5":"\n### Ticket","62216a5d":"### Embarked","252defd7":"Missing values have now been filled in.","e3c78696":"# What are the distributions of numerical features?","419188cf":"## Which features shall we use?\n\n### Immediately useful features:\n- Pclass\n- Sex\n- Embarked\n- Age\n- Fare\n\n\n### Features that need more work:\n- SibSp\/Parch\n- Ticket\n- Name","3d285140":"### Encoding Numerical Features","f8e72438":"The title appears to be placed after the first comma, and before the first full stop.","e34cc9fe":"### Acknowledgements:\n- https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n- https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\n- https:\/\/github.com\/ageron\/handson-ml2\n- https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling","601c60d4":"### Key observations:\n- 38.4% of passengers survived\n- There is a skew towards 3rd class passengers\n- The average passenger is below age 30\n- Most passengers seem to have travelled alone\n- There is a high variance in fares","6ecd9ed1":"Note - useful functions to use:\n\n### Missing Values\n- pandas.DataFrame.fillna\n- pandas.DataFrame.dropna\n\n### Encoding Features\n- sklearn.preprocessing.OneHotEncoder\n- sklearn.preprocessing.OrdinalEncoder\n- sklearn.preprocessing.LabelEncoder\n","2bed25fa":"## Lets start with some feature engineering:\n\n### Idea #1: Combining the SibSp and Parch Features\n\nThese two features represent similar information and they seem to have similar correlations with survival. If we were to engineer these features into a column that represents a binary value of whether the passenger has family, this may be beneficial for a model.","faad4d4c":"### Filling in missing values","adc4f650":"Looks alright","6f18a64e":"## Model Selection and Hyperparemeter optimisation","29c0aaea":"## Idea #2: Performing pattern matching on the Name feature to extract the title\n\nEventhough every name is unique in the name feature, there is a small number of reoccuring titles that we can extract.","08ff7e69":"### Parch","823e60f9":"# Investigating correlations with the target variable\nWe now want to investigate how each of our variables independently relates to the target variable (Survived)","56ff63f5":"# Improving Model Performance\n\nWe have now finished the basic pipeline from EDA to model creation. The next challange is to maximise model performance. To achieve this, there are two avenues we can go down:\n- Feature Engineering\n- Model Selection \/ Hyperparameter Optimisation\n\n","b6eba4fd":"### Cabin","17927469":"# Training an initial model\nThis can be used as a reference point for optimisation of features and machine learning model.","c2e23640":"### Sex","7e08667d":"Looks good! :)","b2201198":"## Will our model perform better with these additional features?","43365ae6":"There seem to be rare and common titles, we can therefore bucket titles into two groups","94e4a1e2":"Performance does not seem to be drastically different. However, these features may increase generalisation performance.","d98cf87e":"## What have we learned?\nWe have now completed our initial investigation of variables\n\n### Categorical\/Ordinal Features:\n- **Pclass: Passengers with upper class tickets are more likely to survive**\n- **Sex: Female passengers are more likely to survive**\n- SibSp: There seems to be a correlation but this feature needs engineering\n- Parch: There seems to be a correlation but this feature needs engineering\n- Ticket: No clear value in this variable - Need to experiment with how value can be extracted from this feature\n- Cabin: No clear value in this variable - but there are many missing values (represents similar information to Ticket)\n- **Embarked: Cherbourg correlates with survival rate**\n- Name: Every value is unique - No correlation - Need to experiment with how value can be extracted from this feature\n\n\n### Numerical Features:\n- **Age: People in their 20s are less likely to survive**\n- **Fare: Higher fare correlated with survival rate**","e59177d1":"### Let us recall what we currently know about the quantity of missing values:\nThe following fields have missing values:\n- Age: 100+ missing values\n- Cabin: 300+ missing values\n- Fare: 1 missing value\n- Embarked: 2 missing values","2345a9d3":"# Submitting Results","59a604af":"### Key Observations:\nThe following features have missing values:\n- Age: 100+ missing values\n- Cabin: 300+ missing values\n- Fare: 1 missing value\n- Embarked: 2 missing values","a43d3dfd":"# Creating a basic set of features\n\n## Actions:\n1. Deal with missing values\n1. Encode non-numeric features (Sex and Embarked)\n\n\n","f538b899":"# What does the dataset look like?","92907a67":"# Are there any erroneous values?","2397a1b2":"### SibSp"}}