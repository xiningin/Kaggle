{"cell_type":{"97ca3e18":"code","382e413d":"code","2b9d3b5e":"code","6f148dd3":"code","422a7320":"code","c24d2c96":"code","56f87fed":"code","93aebd38":"code","f02ecb48":"code","54bc15e6":"code","e39f93b5":"code","de8464bd":"code","0596a73f":"code","7d7ffc35":"code","e7ee6fb2":"code","1ea694aa":"code","0c4e541d":"code","d1b13b23":"code","8e9bb238":"code","90765c33":"code","632213a7":"code","a03bdee5":"code","a1612e70":"code","10b358cf":"code","72ecbd68":"code","bf4be2b0":"code","9b64c481":"code","f3beb3c5":"code","f6860d58":"code","8e00b6ba":"code","a6c190f5":"code","25f92c20":"code","8915a422":"code","1b9d07c1":"code","654aa150":"code","53a7d32f":"code","b416d5cf":"code","1247ed59":"code","e249ad79":"code","c23dc1a4":"markdown","75e1f04c":"markdown","365a7a67":"markdown","e972d825":"markdown","8e973fcc":"markdown","0f67daeb":"markdown","593942f5":"markdown","69d6f519":"markdown","93b6a842":"markdown","cbdcba2e":"markdown","9137805e":"markdown","9f2c715c":"markdown","5ee4cafb":"markdown"},"source":{"97ca3e18":"import nltk                                # Python library for NLP\nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt            # library for visualization\nimport random                              # pseudo-random number generator\nimport numpy as np\nfrom os import getcwd\nimport pandas as pd ","382e413d":"nltk.download('twitter_samples')","2b9d3b5e":"all_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","6f148dd3":"print('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))","422a7320":"# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.2f%%',\n        shadow=True, startangle=45)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()","c24d2c96":"# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])","56f87fed":"tweet = all_positive_tweets[2277]\nprint(tweet)","93aebd38":"# download the stopwords from NLTK\nnltk.download('stopwords')","f02ecb48":"import re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings","54bc15e6":"print('\\033[92m' + tweet)\nprint('\\033[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\nprint(tweet2)","e39f93b5":"print()\nprint('\\033[92m' + tweet2)\nprint('\\033[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)","de8464bd":"#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)","0596a73f":"print()\nprint('\\033[92m')\nprint(tweet_tokens)\nprint('\\033[94m')\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('removed stop words and punctuation:')\nprint(tweets_clean)","7d7ffc35":"print()\nprint('\\033[92m')\nprint(tweets_clean)\nprint('\\033[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('stemmed words:')\nprint(tweets_stem)","e7ee6fb2":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","1ea694aa":"def build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs","0c4e541d":"tweets = all_positive_tweets + all_negative_tweets\nprint(\"Number of tweets: \", len(tweets))","d1b13b23":"labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))","8e9bb238":" #Create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')\n","90765c33":"print(freqs)","632213a7":"# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '\u2764', ':)', ':(', '\ud83d\ude12', '\ud83d\ude2c', '\ud83d\ude04', '\ud83d\ude0d', '\u265b',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata","a03bdee5":"fig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()","a1612e70":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \n\nprint(\"Number of tweets: \", len(train_x))","10b358cf":"def extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1,3)\n    '''\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # loop through each word in the list of words\n    for word in word_l:\n        # increment the word count for the positive label 1\n        x[0,1] += freqs.get((word, 1.0),0)\n        \n        # increment the word count for the negative label 0\n        x[0,2] += freqs.get((word, 0.0),0)\n        \n    ### END CODE HERE ###\n    assert(x.shape == (1, 3))\n    return x","72ecbd68":"# Check your function\n\n# test 1\n# test on training data\ntmp1 = extract_features(train_x[0], freqs)\nprint(tmp1)","bf4be2b0":"def sigmoid(z): \n    '''\n    Input:\n        z: is the input (can be a scalar or an array)\n    Output:\n        h: the sigmoid of z\n    '''\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # calculate the sigmoid of z\n    h = 1\/(1+np.exp(-z))\n    ### END CODE HERE ###\n    \n    return h","9b64c481":"theta = [7e-08, 0.0005239, -0.00055517]","f3beb3c5":"def predict_tweet(tweet, freqs, theta):\n    '''\n    Input: \n        tweet: a string\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n        theta: (3,1) vector of weights\n    Output: \n        y_pred: the probability of a tweet being positive or negative\n    '''\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # extract the features of the tweet and store it into x\n    x = extract_features(tweet,freqs)\n    \n    # make the prediction using x and theta\n    y_pred =sigmoid(np.dot(x,theta))\n    \n    ### END CODE HERE ###\n    \n    return y_pred","f6860d58":"my_tweet = \"I am not happy because I am learning NLP\"\n#my_tweet = 'happy birthday to my brother boss'\nprint(process_tweet(my_tweet))\ny_hat = predict_tweet(my_tweet, freqs, theta)\nprint(y_hat)\nif y_hat > 0.5:\n    print('Positive sentiment')\nelse: \n    print('Negative sentiment')","8e00b6ba":"#the creation of the numerical features needed for the Logistic regression model. In order not to interfere with it,\n#we have previously calculated and stored these features in a CSV file for the entire training set.\ndata = pd.read_csv('..\/input\/logistic-features1\/logistic_features.csv'); # Load a 3 columns csv file using pandas function\ndata.head(15) # Print the first 10 data entries","a6c190f5":"# Each feature is labeled as bias, positive and negative\nX = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\nY = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n\nprint(X.shape) # Print the shape of the X part\nprint(X) # Print some rows of X","25f92c20":"# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")","8915a422":"# Equation for the separation plane\n# It give a value in the negative axe as a function of a positive value\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n# s(pos, W) = (w0 - w1 * pos) \/ w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) \/ theta[2]\n\n# Equation for the direction of the sentiments change\n# We don't care about the magnitude of the change. We are only interested \n# in the direction. So this direction is just a perpendicular function to the \n# separation plane\n# df(pos, W) = pos * w2 \/ w1\ndef direction(theta, pos):\n    return    pos * theta[2] \/ theta[1]","1b9d07c1":"# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\n\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n\nplt.show()","654aa150":"data_random = pd.read_csv('..\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv', encoding=\"ISO-8859-1\") \ndata_random.head(10) # Print the first 10 data entries","53a7d32f":"rows = []\nfor row in data_random:\n        rows.append(row)\nrows","b416d5cf":"data_random","1247ed59":"data_random.full_text[123456]","e249ad79":"my_tweet = data_random.full_text[123456]\n#my_tweet = 'RT @ArbabKasho: The iron man of democracy and civil supremacy\\n#WelldoneRashidSoomro https:\/\/t.co\/DNEB9uOHVN'\nprint(process_tweet(my_tweet))\ny_hat = predict_tweet(my_tweet, freqs, theta)\nprint(y_hat)\nif y_hat > 0.5:\n    print('Positive sentiment')\nelse: \n    print('Negative sentiment')","c23dc1a4":"# Function Defined to Build Frequencies of Words using Process_Tweets Function","75e1f04c":"# Visualizations","365a7a67":"# Downloading Data 'twitter_samples'","e972d825":"# Tokenization","8e973fcc":" # Necessary Libraries","0f67daeb":"# Stemming","593942f5":"# Selecting an Example Tweet to demonstrate different operations","69d6f519":"# Stop Words Examples","93b6a842":"# Function Defined to Process Tweets","cbdcba2e":"# Pre-processing","9137805e":"# Stop Words Removal","9f2c715c":"# Testing Data's Sentiment using Already Trained Model","5ee4cafb":"# Using Our 'Random Tweets' Data"}}