{"cell_type":{"58199ac8":"code","ae1b2245":"code","9414bed6":"code","f785dd4e":"code","9dadd788":"code","9c3e4b53":"code","e3b07362":"code","97f481b3":"code","a6067581":"code","be088428":"code","ea39e71f":"code","426cd30a":"code","3039d231":"code","d4879d18":"code","51b2cd0f":"code","5c248438":"code","1be22f2d":"code","3611b0cc":"code","ad870ac7":"code","0419d9a9":"code","750a0eac":"code","19637acd":"code","145bcc7b":"code","b2164b17":"code","e842692c":"code","a83b2b8f":"code","a3c019b8":"code","919521c6":"code","8a0789a7":"code","0ed4943e":"code","2ea4fa47":"code","5bcfd07b":"code","19cf8be2":"code","be8d3fff":"code","ead508ff":"code","bac4ae20":"code","247301e4":"code","955b1231":"code","c2b05688":"code","b126f717":"code","90955344":"code","71fa3c34":"code","6f596570":"markdown","06ddcc4d":"markdown","579c67da":"markdown","166e8607":"markdown","41195cbe":"markdown","06f0411d":"markdown","1161f1a8":"markdown","f5aec522":"markdown","eafa51cd":"markdown","56a88256":"markdown","a9c3e4d0":"markdown","cf631876":"markdown","866e8c19":"markdown","abf57e07":"markdown","92d12c1e":"markdown","bb97aba0":"markdown","7fb1c6c9":"markdown","5f7f4aeb":"markdown","04cb2eed":"markdown","666c9f5d":"markdown","31612243":"markdown","aebc007b":"markdown","01b7d5f4":"markdown","d0b6cea4":"markdown","d7b47bc4":"markdown","98885ef9":"markdown","822991c9":"markdown","75956443":"markdown","4100ed7c":"markdown","b19b3383":"markdown","986baa99":"markdown","dac6b9d4":"markdown","aa6ea825":"markdown","cff4e1c2":"markdown","e44c9656":"markdown","64dd05fe":"markdown","1817e422":"markdown"},"source":{"58199ac8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.utils import shuffle\nfrom matplotlib import pyplot\nimport seaborn as sns\n!pip install pycountry-convert\n\nimport pycountry_convert as pc\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom statistics import mean\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNetCV\n\n\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ae1b2245":"train_df = pd.read_excel('\/kaggle\/input\/cs412-fall2020\/train.xlsx')\ntest_df = pd.read_excel('\/kaggle\/input\/cs412-fall2020\/test.xlsx')\n","9414bed6":"train_df = train_df[train_df.Age != 0]# One person is 0 years old (should drop)\ntrain_df = train_df[train_df.Age != 1]# One person is 1 years old (should drop)\ntest_df = test_df[test_df.Age != 0]\ntest_df = test_df[test_df.Age != 1]\n\ncount_row = train_df.shape[0]  # gives number of row count\ncount_col = train_df.shape[1] \ny = train_df.pop(\"JobSatisfaction\")#label column\n\n","f785dd4e":"train_df.isnull().mean() * 100","9dadd788":"\ntrain_df = train_df.drop(columns=[\n                \"WorkMethodsFrequencyCross-Validation\",\n                 \"WorkMethodsFrequencyDataVisualization\",\"WorkMethodsFrequencyDecisionTrees\",\n                 \"WorkMethodsFrequencyLogisticRegression\",\"WorkMethodsFrequencyNeuralNetworks\",\n                \"WorkMethodsFrequencyPCA\",\"WorkMethodsFrequencyRandomForests\",\n                 \"WorkMethodsFrequencyTimeSeriesAnalysis\"])\ntrain_df = train_df.reset_index(drop=True)\n\ntest_df = test_df.drop(columns=[\n                \"WorkMethodsFrequencyCross-Validation\",\n                 \"WorkMethodsFrequencyDataVisualization\",\"WorkMethodsFrequencyDecisionTrees\",\n                 \"WorkMethodsFrequencyLogisticRegression\",\"WorkMethodsFrequencyNeuralNetworks\",\n                \"WorkMethodsFrequencyPCA\",\"WorkMethodsFrequencyRandomForests\",\n                 \"WorkMethodsFrequencyTimeSeriesAnalysis\"])\ntest_df = test_df.reset_index(drop=True)","9c3e4b53":"def comparefornull(change,effects,df_to_change):\n    tool_df = train_df.groupby(by=[effects]).sum()   \n    ml_df = train_df[[change,effects]]\n    tools = []\n    for row in tool_df.index: \n        tools.append(row)\n    ml_dict = dict((l,0) for l in tools)\n\n\n    for i in tools:\n        sub_df = ml_df[ml_df[effects]== i]\n        frequent = sub_df[change].mode().values[0]\n        ml_dict[i] = frequent\n\n\n    fill_df = df_to_change[[change,effects]]\n    is_NaN = fill_df.isnull().any(axis=1)\n    rows_with_NaN = fill_df[is_NaN]\n\n\n    for i,row in rows_with_NaN.iterrows():\n        if pd.isna(row[change]) == True:\n            if pd.isna(row[effects]) == False:\n                df_to_change.loc[i,change] = ml_dict[row[effects]]","e3b07362":"comparefornull(\"TitleFit\",\"CurrentJobTitleSelect\",train_df)\ncomparefornull(\"CurrentJobTitleSelect\",\"DataScienceIdentitySelect\",train_df)\n\ncomparefornull(\"TitleFit\",\"CurrentJobTitleSelect\",test_df)\ncomparefornull(\"CurrentJobTitleSelect\",\"DataScienceIdentitySelect\",test_df)\n","97f481b3":"frames = [train_df, test_df] #create a list and append train data and test data\n\ntemp_df = pd.concat(frames) #merge databases in list\ntemp_df = temp_df.reset_index(drop=True)\n","a6067581":"def moder (column):\n    temp_df[column] = temp_df[column].fillna(train_df[column].mode()[0])","be088428":"moder(\"GenderSelect\")\nmoder(\"Country\")\nmoder(\"EmploymentStatus\")\nmoder(\"CodeWriter\")\nmoder(\"CurrentJobTitleSelect\")\nmoder(\"TitleFit\")\nmoder(\"CurrentEmployerType\")\nmoder(\"MLToolNextYearSelect\")\nmoder(\"MLMethodNextYearSelect\")\nmoder(\"LanguageRecommendationSelect\")\nmoder(\"DataScienceIdentitySelect\")\nmoder(\"FormalEducation\")\nmoder(\"MajorSelect\")\nmoder(\"Tenure\")\nmoder(\"PastJobTitlesSelect\")\nmoder(\"MLSkillsSelect\")\nmoder(\"MLTechniquesSelect\")\nmoder(\"EmployerIndustry\")\nmoder(\"EmployerSize\")\nmoder(\"WorkProductionFrequency\")\nmoder(\"WorkAlgorithmsSelect\")\nmoder(\"WorkDataVisualizations\")\nmoder(\"WorkInternalVsExternalTools\")\nmoder(\"WorkMLTeamSeatSelect\")\nmoder(\"RemoteWork\")\nmoder(\"Age\")\nmoder(\"CompensationScore\")\n\ntemp_df[\"CurrentEmployerType\"]= temp_df[\"CurrentEmployerType\"].astype(str)\n","ea39e71f":"DataScienceIdentitySelect_map = {'No':0,'Yes':2,'Sort of (Explain more)':1}\ntemp_df['DataScienceIdentitySelect'] = temp_df['DataScienceIdentitySelect'].replace(DataScienceIdentitySelect_map)\n\n","426cd30a":"Codewriter_map = {'No':0,'Yes':1}\ntemp_df['CodeWriter'] = temp_df['CodeWriter'].replace(Codewriter_map)\n\n\n","3039d231":"TitleFit_map = {'Fine':1,'Perfectly':2,'Poorly':0}\ntemp_df['TitleFit'] = temp_df['TitleFit'].replace(TitleFit_map)\n\n","d4879d18":"Tenure_map = {'1 to 2 years':2,'3 to 5 years':3,'6 to 10 years':4,\n          \"I don't write code to analyze data\":0,'Less than a year':1,\n          'More than 10 years':5}\ntemp_df['Tenure'] = temp_df['Tenure'].replace(Tenure_map)\n","51b2cd0f":"EmployerSize_map = {'1,000 to 4,999 employees':7,'10 to 19 employees':3,\n              '10,000 or more employees':9,\"100 to 499 employees\":5,\n              '20 to 99 employees':4,'5,000 to 9,999 employees':8,\n              '500 to 999 employees':6,'Fewer than 10 employees':2,\n              \"I don't know\":1,'I prefer not to answer':0}\ntemp_df['EmployerSize'] = temp_df['EmployerSize'].replace(EmployerSize_map)\n","5c248438":"wpf_map = {'Always':5,'Most of the time':4,\n              'Never':1,\"Don't know\":0,\n              'Sometimes':3,'Rarely':2}\ntemp_df['WorkProductionFrequency'] = temp_df['WorkProductionFrequency'].replace(wpf_map)\n","1be22f2d":"visual_map = {'51-75% of projects':4,\"76-99% of projects\":5,\n              '26-50% of projects':3,'10-25% of projects':2,\n              \"Less than 10% of projects\":1,'None':0,'100% of projects':6}\ntemp_df['WorkDataVisualizations'] = temp_df['WorkDataVisualizations'].replace(visual_map)\n","3611b0cc":"RemoteWork_map = {'Always':5,'Most of the time':4,\n              'Never':1,\"Don't know\":0,\n              'Sometimes':3,'Rarely':2}\ntemp_df['RemoteWork'] = temp_df['RemoteWork'].replace(RemoteWork_map)\n","ad870ac7":"edu_map = {'Doctoral degree':6,\"Master's degree\":5,\n              \"Bachelor's degree\":3,\"Professional degree\":4,\n              'I did not complete any formal education past high school':1,\n           'I prefer not to answer':0,\n           \"Some college\/university study without earning a bachelor's degree\":2}\ntemp_df['FormalEducation'] = temp_df['FormalEducation'].replace(edu_map)\n","0419d9a9":"def learnplatf(col_name):\n  wtfpython_map = {'Very useful':2,\n              'Somewhat useful':1,'Not Useful':0}\n  temp_df[col_name] = temp_df[col_name].replace(wtfpython_map)","750a0eac":"def workchallenge(col_name):\n  wtfpython_map = {'Often':3,'Most of the time':4,\n              'Sometimes':2,'Rarely':1}\n  temp_df[col_name] = temp_df[col_name].replace(wtfpython_map)\n","19637acd":"workchallenge(\"WorkChallengeFrequencyPolitics\")\nworkchallenge(\"WorkChallengeFrequencyUnusedResults\")\nworkchallenge(\"WorkChallengeFrequencyDirtyData\")\nworkchallenge(\"WorkChallengeFrequencyExplaining\")\nworkchallenge(\"WorkChallengeFrequencyTalent\")\nworkchallenge(\"WorkChallengeFrequencyClarity\")\nworkchallenge(\"WorkChallengeFrequencyDataAccess\")\n\nworkchallenge(\"WorkToolsFrequencySQL\")\nworkchallenge(\"WorkToolsFrequencyR\")\nworkchallenge(\"WorkToolsFrequencyPython\")\n\nlearnplatf(\"LearningPlatformUsefulnessBlogs\")\nlearnplatf(\"LearningPlatformUsefulnessKaggle\")\nlearnplatf(\"LearningPlatformUsefulnessCourses\")\nlearnplatf(\"LearningPlatformUsefulnessProjects\")\nlearnplatf(\"LearningPlatformUsefulnessSO\")\nlearnplatf(\"LearningPlatformUsefulnessTextbook\")\nlearnplatf(\"LearningPlatformUsefulnessYouTube\")\n\n","145bcc7b":"def moderforwork (column):\n    temp_df[column] = temp_df[column].fillna(0)\n\nmoderforwork(\"WorkChallengeFrequencyPolitics\")\nmoderforwork(\"WorkChallengeFrequencyUnusedResults\")\nmoderforwork(\"WorkChallengeFrequencyDirtyData\")\nmoderforwork(\"WorkChallengeFrequencyExplaining\")\nmoderforwork(\"WorkChallengeFrequencyTalent\")\nmoderforwork(\"WorkChallengeFrequencyClarity\")\nmoderforwork(\"WorkChallengeFrequencyDataAccess\")\n\nmoderforwork(\"WorkToolsFrequencySQL\")\nmoderforwork(\"WorkToolsFrequencyR\")\nmoderforwork(\"WorkToolsFrequencyPython\")\n\nmoderforwork(\"LearningPlatformUsefulnessBlogs\")\nmoderforwork(\"LearningPlatformUsefulnessKaggle\")\nmoderforwork(\"LearningPlatformUsefulnessCourses\")\nmoderforwork(\"LearningPlatformUsefulnessProjects\")\nmoderforwork(\"LearningPlatformUsefulnessSO\")\nmoderforwork(\"LearningPlatformUsefulnessTextbook\")\nmoderforwork(\"LearningPlatformUsefulnessYouTube\")\n\n","b2164b17":"bins= [13,30,50,101]\nlabels = ['Young','Adult','Old']\ntemp_df['Age'] = pd.cut(temp_df['Age'], bins=bins, labels=labels, right=False)\n\nAge_map = {'Old':2,'Young':0,'Adult':1}\ntemp_df['Age'] = temp_df['Age'].replace(Age_map)\n\n","e842692c":"def splitter(col_name):\n  emp_list = temp_df[col_name].unique()\n  emp_list = [i.split(',') for i in emp_list] \n  temp_df[col_name] = temp_df[col_name].str.split(\",\")\n  emp_list = [j for i in emp_list for j in i]\n  emp_list = list(set(emp_list))\n  return emp_list\n\ndef create_col(col):\n  temp_df[col] = 0\n\ndef change_values(list_name,col_name):\n  for i in list_name:\n    create_col(i)\n\n  for i in range(len(temp_df)):\n    t_list = []\n    t_list = temp_df.loc[i,col_name]\n    for k in range(len(t_list)):\n      temp_df.loc[i, t_list[k]] = 1\n","a83b2b8f":"\n\nlist_work = splitter(\"CurrentEmployerType\")\nchange_values(list_work,\"CurrentEmployerType\")\n\nlist_work = splitter(\"WorkAlgorithmsSelect\")\nchange_values(list_work,\"WorkAlgorithmsSelect\")\n","a3c019b8":"frequency = train_df.EmployerIndustry.value_counts().to_dict()\ntemp_df.EmployerIndustry=temp_df.EmployerIndustry.map(frequency)\nfrequency\n","919521c6":"frequency = train_df.LanguageRecommendationSelect.value_counts().to_dict()\ntemp_df.LanguageRecommendationSelect=temp_df.LanguageRecommendationSelect.map(frequency)\nfrequency","8a0789a7":"def country_to_continent(country_name):\n    country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n    country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n    country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n    return country_continent_name\n\n\ncountry = temp_df.groupby(by=[\"Country\"]).sum()   \nct = []\nfor row in country.index: \n    ct.append(row)\nct_dict = dict((l,0) for l in ct)\nfor i in range(len(ct)):\n  country_name = ct[i]\n  if country_name == \"People 's Republic of China\" or country_name == \"Republic of China\":\n    countinent_name = \"Asia\"\n    ct_dict[country_name] = countinent_name\n  elif country_name != \"Other\":\n    countinent_name = country_to_continent(country_name)\n    ct_dict[country_name] = countinent_name\n\n\ntemp_df = temp_df.replace({\"Country\": ct_dict})                \ntemp_df = pd.get_dummies(temp_df, columns=[\"Country\"])\n","0ed4943e":"temp_df = pd.get_dummies(temp_df, columns=[\"GenderSelect\",\n\"EmploymentStatus\", \n\"CurrentJobTitleSelect\",                                                                                         \n\"MajorSelect\" ,\n\"WorkMLTeamSeatSelect\"\n], drop_first=True)","2ea4fa47":"test_df = temp_df.iloc[count_row:,:]\ntemp_df = temp_df.iloc[:count_row,:]\n\nprint(test_df.shape)\n\ntest_df = test_df.drop(columns = [\"WorkAlgorithmsSelect\",\"MLTechniquesSelect\"\n                                  ,\"CurrentEmployerType\",\"WorkInternalVsExternalTools\"\n                                  ,\"PastJobTitlesSelect\",\"MLSkillsSelect\"\n                                ,\"MLMethodNextYearSelect\",\"MLToolNextYearSelect\"\n                                ])\ntest_df = test_df.reset_index(drop=True)","5bcfd07b":"temp_df = temp_df.drop(columns = [\"WorkAlgorithmsSelect\",\"MLTechniquesSelect\"\n                                  ,\"CurrentEmployerType\",\"WorkInternalVsExternalTools\"\n                                  ,\"PastJobTitlesSelect\",\"MLSkillsSelect\"\n                                ,\"MLMethodNextYearSelect\",\"MLToolNextYearSelect\"\n                                ])\ntemp_df = temp_df.reset_index(drop=True)","19cf8be2":"df_empty = test_df[[\"ID\",\"CompensationScore\"]]\ndf_empty = df_empty.rename(columns={'CompensationScore': 'linear'})\ndf_empty[\"linear\"] = df_empty[\"linear\"]*0\ndf_empty[\"Ridge\"] = df_empty[\"linear\"]\ndf_empty[\"gradient\"]= df_empty[\"linear\"]\ndf_empty[\"elastic\"]= df_empty[\"linear\"]\ndf_empty[\"lasso\"]= df_empty[\"linear\"]\ndf_empty[\"bayes\"]= df_empty[\"linear\"]\n\nx = temp_df.drop(columns=[\"ID\"])\ntest = test_df.drop(columns=[\"ID\"])\n\nx = x.reset_index(drop=True)\ntest = test.reset_index(drop=True)","be8d3fff":"\nfolds = KFold(n_splits = 5)\nscores = []\nlin_model = LinearRegression()\nfor n_fold, (train_index, valid_index) in enumerate(folds.split(x,y)):   \n    x_train, x_val = x.iloc[list(train_index)], x.iloc[list(valid_index)]\n    y_train, y_val = y.iloc[list(train_index)], y.iloc[list(valid_index)]\n    \n    \n    lin_model.fit(x_train, y_train)\n    y_pred = lin_model.predict(x_val)\n    rmse = mean_squared_error(y_val, y_pred, squared=False)\n    pred_test = lin_model.predict(test)\n    df_empty[\"linear\"] += pred_test\n    scores.append(rmse)\ndf_empty[\"linear\"] = df_empty[\"linear\"]\/5\n\nprint(df_empty[\"linear\"])\nprint(mean(scores))\n","ead508ff":"folds = KFold(n_splits = 5)\nscores = []\nridge_model = RidgeCV()\nfor n_fold, (train_index, valid_index) in enumerate(folds.split(x,y)):   \n    x_train, x_val = x.iloc[list(train_index)], x.iloc[list(valid_index)]\n    y_train, y_val = y.iloc[list(train_index)], y.iloc[list(valid_index)]\n    \n    \n    ridge_model.fit(x_train, y_train)\n    y_pred = ridge_model.predict(x_val)\n    rmse = mean_squared_error(y_val, y_pred, squared=False)\n    pred_test = ridge_model.predict(test)\n    df_empty[\"Ridge\"] += pred_test\n    scores.append(rmse)\ndf_empty[\"Ridge\"] = df_empty[\"Ridge\"]\/5\n\nprint(df_empty[\"Ridge\"])\nprint(mean(scores))\n","bac4ae20":"folds = KFold(n_splits = 5)\nscores = []\nreg = GradientBoostingRegressor(random_state=25)\nfor n_fold, (train_index, valid_index) in enumerate(folds.split(x,y)):   \n    x_train, x_val = x.iloc[list(train_index)], x.iloc[list(valid_index)]\n    y_train, y_val = y.iloc[list(train_index)], y.iloc[list(valid_index)]\n    \n    \n    reg.fit(x_train, y_train)\n    y_pred = reg.predict(x_val)\n    rmse = mean_squared_error(y_val, y_pred, squared=False)\n    pred_test = reg.predict(test)\n    df_empty[\"gradient\"] += pred_test\n    scores.append(rmse)\ndf_empty[\"gradient\"] = df_empty[\"gradient\"]\/5\n\nprint(df_empty[\"gradient\"])\nprint(mean(scores))\n","247301e4":"folds = KFold(n_splits = 5)\nscores = []\nlasso = LassoCV()\nfor n_fold, (train_index, valid_index) in enumerate(folds.split(x,y)):   \n    x_train, x_val = x.iloc[list(train_index)], x.iloc[list(valid_index)]\n    y_train, y_val = y.iloc[list(train_index)], y.iloc[list(valid_index)]\n    \n    \n    lasso.fit(x_train, y_train)\n    y_pred = lasso.predict(x_val)\n    rmse = mean_squared_error(y_val, y_pred, squared=False)\n    pred_test = lasso.predict(test)\n    df_empty[\"lasso\"] += pred_test\n    scores.append(rmse)\ndf_empty[\"lasso\"] = df_empty[\"lasso\"]\/5\n\nprint(df_empty[\"lasso\"])\nprint(mean(scores))\n","955b1231":"folds = KFold(n_splits = 5)\nscores = []\nElastic = ElasticNetCV()\nfor n_fold, (train_index, valid_index) in enumerate(folds.split(x,y)):   \n    x_train, x_val = x.iloc[list(train_index)], x.iloc[list(valid_index)]\n    y_train, y_val = y.iloc[list(train_index)], y.iloc[list(valid_index)]\n    \n    \n    Elastic.fit(x_train, y_train)\n    y_pred = Elastic.predict(x_val)\n    rmse = mean_squared_error(y_val, y_pred, squared=False)\n    pred_test = Elastic.predict(test)\n    df_empty[\"elastic\"] += pred_test\n    scores.append(rmse)\ndf_empty[\"elastic\"] = df_empty[\"elastic\"]\/5\n\nprint(df_empty[\"elastic\"])\nprint(mean(scores))\n","c2b05688":"folds = KFold(n_splits = 5)\nscores = []\nclf = linear_model.BayesianRidge()\nfor n_fold, (train_index, valid_index) in enumerate(folds.split(x,y)):   \n    x_train, x_val = x.iloc[list(train_index)], x.iloc[list(valid_index)]\n    y_train, y_val = y.iloc[list(train_index)], y.iloc[list(valid_index)]\n    \n    \n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_val)\n    rmse = mean_squared_error(y_val, y_pred, squared=False)\n    pred_test = clf.predict(test)\n    df_empty[\"bayes\"] += pred_test\n    scores.append(rmse)\ndf_empty[\"bayes\"] = df_empty[\"bayes\"]\/5\n\nprint(df_empty[\"bayes\"])\nprint(mean(scores))\n\n","b126f717":"df_empty","90955344":"output = df_empty[[\"ID\",\"bayes\"]]\noutput = output.rename(columns={'bayes': 'Prediction'})\n\noutput.describe()","71fa3c34":"output.to_csv(\"submission.csv\", index=False)","6f596570":"## Counting Map","06ddcc4d":"# Prediction Job Satisfaction of Kaggle Members","579c67da":"## Gradient Boosting\n","166e8607":"## Linear Regression","41195cbe":"**Presentation and more info: https:\/\/drive.google.com\/file\/d\/1mVdBrK_yxfsh8duJYXe5-57jyAloOnd7\/view?usp=sharing** ","06f0411d":"# Handling with Features\n\nLabel column is \"JobSatisfaction\".\n\nNumerical features are \"ID,Age,CompensationScore\"\n\nOther features are nominal or ordinal features.","1161f1a8":"# Reading Data and Merging","f5aec522":"# Output Creation","eafa51cd":"## Handle the Columns that have got more than one value","56a88256":"We merge train data and test data to do the operations on the data once.","a9c3e4d0":"This funtion change the NaN values in a given column with mode value of that column.","cf631876":"## Dropping Column with High perncentage of null values","866e8c19":"## LassoCV\n","abf57e07":"Show the percentages of NaN values in each column","92d12c1e":"We drop the datas which have got null value percentage higher than %35 percentages (which are not important)","bb97aba0":"## 2- Related Columns","7fb1c6c9":"Create a new dataframe for compare predictions of each algorithm.","5f7f4aeb":"## Seperate Test and Train Data (Drop Some Columns)","04cb2eed":"# Libraries","666c9f5d":"## One Hot Encoding (k-1 for k values)","31612243":"## ElasticCV\n","aebc007b":"## Merge Test and Train Data","01b7d5f4":"# Nominal Features\n\n\"GenderSelect\", \"Country\",\n\n\"EmploymentStatus\",\"CurrentJobTitleSelect\", \n                                             \n\"CurrentEmployerType\",\"MLToolNextYearSelect**\",\n                                             \n\"MLMethodNextYearSelect\",\"LanguageRecommendationSelect\",\n                                             \n\"MajorSelect\",\"PastJobTitlesSelect\",\n                                             \n\"MLSkillsSelect\",\"MLTechniquesSelect\",\"EmployerIndustry\",\n                                             \n\"WorkAlgorithmsSelect\",\"WorkInternalVsExternalTools\",\n\n\"WorkMLTeamSeatSelect\"","d0b6cea4":"This function is handle the nan values of two columns which are related with each other.","d7b47bc4":"## Categorize Countries by their Continent","98885ef9":"## Prediction of Algorithms\n\n","822991c9":"## Ridge CV","75956443":"## Grouping Age Column and Mapping","4100ed7c":"# Future Work","b19b3383":"We choose bayesian ridge because it gives the best rsme score.","986baa99":"## Map Ordinal Features' Columns:\n\nDataScienceIdentitySelect, CodeWriter, TitleFit, Tenure,\n\nEmployerSize, WorkProductionFrequency, WorkToolsFrequencyPython, \n\nWorkDataVisualizations, RemoteWork, FormalEducation","dac6b9d4":"# ML ALGORITHMS","aa6ea825":"## Change Null Value with Mode of Column","cff4e1c2":"Neural Network gives better result so we will implement it later.","e44c9656":"The libraries which we are using in this notebook","64dd05fe":"We pop the label data from train data. ","1817e422":"## Bayesian Ridge\n"}}