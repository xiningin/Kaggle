{"cell_type":{"760b093c":"code","8fb08556":"code","2a9da393":"code","05c6d8bf":"code","a36f1a39":"code","bbc655e6":"code","cbfb25ba":"code","f48a2369":"code","081ee7f4":"code","4a1c872c":"code","b592917e":"code","f91f696b":"code","ab1dbbf7":"code","ac5a734b":"code","82c59c20":"code","8b5540fe":"code","a86e5b08":"code","f463f1df":"code","b3fdb44f":"code","48c2b894":"code","1161e069":"code","48c89532":"code","dd3b43f8":"code","005ee1f8":"markdown","d543a073":"markdown","ca468826":"markdown","65649dae":"markdown","a2a3c152":"markdown","d417ffc7":"markdown","25ba7029":"markdown","0e5504f8":"markdown"},"source":{"760b093c":"import umap.umap_ as umap\nimport os\nimport re\nimport csv\nimport pytz\nimport json\nimport glob\nimport string\nimport datetime\nimport warnings\nimport random\nfrom datetime import datetime\n\nimport numpy as np\nimport scipy\nimport pandas as pd\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\n\nimport pyLDAvis\n# import pyLDAvis.gensim_models as gensimvis\nimport gensim\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\n\n# Print all data files recognized\nfor dirname, _, filenames in os.walk('..\/input\/data-science-for-good-careervillage'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8fb08556":"warnings.filterwarnings('ignore')\n# The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \u201d...\u201d placeholder is embedded in the output.\npd.set_option('display.max_colwidth', -1)\n\nseed = 43\nrandom.seed(seed)\nnp.random.seed(seed)\n\nDATA_PATH = '..\/input\/data-science-for-good-careervillage'\nSPLIT_DATE = '2021-07-21'\n\n# The data export was from 1. February 2019. For Production use datetime.now()\nactual_date = pytz.timezone(\"UTC\").localize(datetime(2019, 2, 1))\n","2a9da393":"# Read CSV\nanswers = pd.read_csv(os.path.join(DATA_PATH, 'answers.csv'))\nanswer_scores = pd.read_csv(os.path.join(DATA_PATH, 'answer_scores.csv'))\ncomments = pd.read_csv(os.path.join(DATA_PATH, 'comments.csv'))\nprofessionals = pd.read_csv(os.path.join(DATA_PATH, 'professionals.csv'))\nquestions = pd.read_csv(os.path.join(DATA_PATH, 'questions.csv'))\nquestion_scores = pd.read_csv(os.path.join(DATA_PATH, 'question_scores.csv'))\nstudents = pd.read_csv(os.path.join(DATA_PATH, 'students.csv'))\ntags = pd.read_csv(os.path.join(DATA_PATH, 'tags.csv'))\ntag_questions = pd.read_csv(os.path.join(DATA_PATH, 'tag_questions.csv'))\ntag_users = pd.read_csv(os.path.join(DATA_PATH, 'tag_users.csv'))","05c6d8bf":"# Change date to pandas datetime.\n\nanswers['answers_date_added'] = pd.to_datetime(\n    answers['answers_date_added'], infer_datetime_format=True)\ncomments['comments_date_added'] = pd.to_datetime(\n    comments['comments_date_added'], infer_datetime_format=True)\nprofessionals['professionals_date_joined'] = pd.to_datetime(\n    professionals['professionals_date_joined'], infer_datetime_format=True)\nquestions['questions_date_added'] = pd.to_datetime(\n    questions['questions_date_added'], infer_datetime_format=True)\nstudents['students_date_joined'] = pd.to_datetime(\n    students['students_date_joined'], infer_datetime_format=True)","a36f1a39":"# Merge Question Title and Body\nquestions['questions_full_text'] = questions['questions_title'] + ' ' + questions['questions_body']\nquestions['questions_full_text'] = questions['questions_full_text'].str.replace('#', ' ')\n\n# Questions Tags list\ntemp = pd.merge(questions, tag_questions, left_on='questions_id',\n                right_on='tag_questions_question_id', how='inner')\ntemp = pd.merge(temp, tags, left_on='tag_questions_tag_id',\n                right_on='tags_tag_id', how='inner')\ntemp = temp.groupby('questions_id')['tags_tag_name'].apply(\n    list).rename('questions_tags')\nquestions['questions_tags'] = pd.merge(questions, temp.to_frame(\n), left_on='questions_id', right_index=True, how='left')['questions_tags']\n\n\n### Answers\n# Hearts Score\ntemp = pd.merge(answers, answer_scores, left_on='answers_id',\n                right_on='id', how='left')\nanswers['answers_hearts'] = temp['score'].fillna(0).astype(int)\n\n","bbc655e6":"# Add features to professionals\n\n# Professionals Tags to List\ntemp = pd.merge(professionals, tag_users, left_on='professionals_id',\n                right_on='tag_users_user_id', how='inner')\ntemp = pd.merge(temp, tags, left_on='tag_users_tag_id',\n                right_on='tags_tag_id', how='inner')\ntemp = temp.groupby('professionals_id')['tags_tag_name'].apply(\n    list).rename('professionals_tags')\nprofessionals['professionals_tags'] = pd.merge(professionals, temp.to_frame(\n), left_on='professionals_id', right_index=True, how='left')['professionals_tags']\n\n# Total Hearts score\ntemp = answers.groupby('answers_author_id')['answers_hearts'].sum()\nprofessionals['professional_answers_hearts'] = pd.merge(professionals, pd.DataFrame(temp.rename(\n    'answers_hearts')), left_on='professionals_id', right_index=True, how='left')['answers_hearts'].fillna(0).astype(int)\n\n","cbfb25ba":"# Spacy Tokenfilter for part-of-speech tagging\ntoken_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']","f48a2369":"# Apply spaCy part-of-speech to tokenize the text. Approx 100s\n\ndata = questions['questions_full_text']\n\n# go through all questions and create docs with spaCy\ndpipe = nlp.pipe(data, disable=[\"parser\", \"ner\"])\ntokens = []\n# Tokenize all full texts and att to tokens if its a token_pos, and not stop and is a alphacharacter\nfor doc in dpipe:\n    tokens.append([t.lower_ for t in doc if (\n        t.pos_ in token_pos and not t.is_stop and t.is_alpha)])\n","081ee7f4":"#BIGRAMS AND TRIGRAMS\nbigram_phrases = gensim.models.Phrases(tokens, min_count=5, threshold=50)\ntrigram_phrases = gensim.models.Phrases(\n    bigram_phrases[tokens], threshold=50)\n\nbigram = gensim.models.phrases.Phraser(bigram_phrases)\ntrigram = gensim.models.phrases.Phraser(trigram_phrases)\n\n\ndef make_bigrams(texts):\n    return([bigram[doc] for doc in texts])\n\n\ndef make_trigrams(texts):\n    return ([trigram[bigram[doc]] for doc in texts])\n\n\ndata_bigrams = make_bigrams(tokens)\ndata_bigrams_trigrams = make_trigrams(data_bigrams)\n\ndata_bigrams_trigrams[:3]","4a1c872c":"# Create Gensim dic from the tokens\nno_below = 10\nno_above = 0.6\nkeep_n = 8000\nlda_dic = gensim.corpora.Dictionary(data_bigrams_trigrams)\nlda_dic.filter_extremes(no_below=no_below, no_above=no_above, keep_n=keep_n)\n\n# Create corpus from the dic\nlda_corpus = [lda_dic.doc2bow(doc) for doc in tokens]\n\n# Create tf-idf from corpus\ntfidf = gensim.models.TfidfModel(lda_corpus)\ntfidf_corpus = tfidf[lda_corpus]\n\n# Train model\nnum_topics = 10\npasses = 20\nchunksize = 1000\nalpha = 1\/50\nlda_model = gensim.models.ldamodel.LdaModel(tfidf_corpus, num_topics=num_topics, \\\n                                            id2word = lda_dic, passes=passes,\\\n                                            chunksize=chunksize,update_every=0,\\\n                                            alpha=alpha, random_state=seed)\n","b592917e":"# Cluster overview \n# pyLDAvis.enable_notebook()\n# vis = gensimvis.prepare(lda_model, lda_corpus, lda_dic, mds=\"mmds\", R=30)\n# vis\n","f91f696b":"# Run to get clusters to topics.txt\n# dirty_topics = lda_model.print_topics()\n# topics = []\n# for topic in dirty_topics:\n#     topics.append(re.sub(r\"0.\\d{1,5}\\*\", \"\",topic[1].replace('+', '')).replace(\"\\\"\", \"\").split())\n# \n# with open(DATA_PATH + \"topics.txt\", \"w\", encoding='utf-8') as f:\n#     for i in range(len(topics)):\n#         f.write(f\"Cluster {i}\")\n#         f.write(\"\\n\\n\")\n#         for topic in topics[i]:\n#             f.write('   ' + topic)\n#             f.write(\"\\n\")\n#         f.write(\"\\n\")","ab1dbbf7":"tf.disable_eager_execution()\nembed = hub.load(\"..\/input\/universalsentenceencoderlarge5\/\")\n","ac5a734b":"import logging\nfrom tqdm import tqdm_notebook\ntf.logging.set_verbosity(logging.WARNING)\nBATCH_SIZE = 128\n\nsentence_input = tf.placeholder(tf.string, shape=(None))\n# For evaluation we use exactly normalized rather than\n# approximately normalized.\nsentence_emb = tf.nn.l2_normalize(embed(sentence_input), axis=1)\n\nsentence_embeddings = []\nwith tf.Session() as session:\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    for i in tqdm_notebook(range(0, len(questions), BATCH_SIZE)):\n        sentence_embeddings.append(\n            session.run(\n                sentence_emb,\n                feed_dict={\n                    sentence_input: questions[\"questions_full_text\"].iloc[i:(\n                        i+BATCH_SIZE)].values\n                }\n            )\n        )\n\nsentence_embeddings = np.concatenate(sentence_embeddings, axis=0)\nsentence_embeddings.shape\n","82c59c20":"new_question = \"I want to become the best machine learning engineer in the world! How do I start? I've heard Kaggle is a good place to start ML AI \"","8b5540fe":"with tf.Session() as session:\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    sentence_embedding = session.run(embed([new_question]))\nsentence_embedding = np.concatenate(sentence_embedding, axis=0)\n\n\ndef find_similar(top_k):\n    cosine_similarities = sentence_embeddings @ sentence_embedding[:, np.newaxis]\n    return np.argsort(cosine_similarities[:, 0])[::-1][1:(top_k+1)]\n","a86e5b08":"prof_id = []\n\nsimilar_ids = find_similar(top_k=3)\n\n# Get the professionals\nfor index in similar_ids:\n    prof_id.append(answers['answers_author_id'][answers['answers_question_id']\n                   == questions[\"questions_id\"].iloc[index]].values)\n\nprof_id = np.concatenate(prof_id)\nprint(questions[\"questions_full_text\"].iloc[similar_ids], \"\\n\")\n","f463f1df":"# Scoring algoritm\nimport math\n\n\ndef answer_score_algorithm(professional, cf_df_matrix):\n    \"\"\" answer_score_algorithm will return a score for how well the professional answered the questions. 0 if no interaction.\n\n        param pId: Professional that will get score\n        param cf_df_matrix: collaborative filltering dataframe \n        return: array with all scores for each question\n    \"\"\"\n\n    def sigmoid(x):\n        \"\"\" Activation function for scoreing: real number -> range(0,1)\n            TODO: Are natural numbers better?\n\n            param x: real number\n            returns: sigmoidified number\n        \"\"\"\n        k = 0.005\n        c = 600\n        return 1\/(1 + math.exp(-k*(x-c)))\n\n    def scoreing(qId):\n        \"\"\" \n            Metod for score calculation\n            Scores given for each feature is arbitrary\n            returns score: score for a particular question\n        \"\"\"\n        score = 0\n\n        # Location calculation, if living in the same location, 10p \n        try:\n            if professional['professionals_location'] == (students.loc[students['students_id'] == \n                questions.loc[questions['questions_id'] == \n                questions['questions_id']]['questions_author_id'].item()]['students_location'].item()):\n                score += 15\n                \n        except ValueError:\n            pass\n            # print('don\u00b4t worry')\n        \n        # Professional tags\n        # if tags are not float(nan), then it is a list. Check professional tags against questions tags\n        try:\n            for tag in professional['professionals_tags']:\n                if tag in questions.loc[questions['questions_id'] == qId]['questions_tags'].item():\n                    score += 2\n        except TypeError:\n            pass\n\n        # p.professionals_industry: TODO if industry is relevent to document content give points.\n        # Idea: Lemmatize industry and check if a word matches a word in document.\n        \n        # Use this to lemmatize and tokenize: \n        # go through all questions and create docs with spaCy\n        # dpipe = nlp.pipe(testData, disable=[\"parser\", \"ner\"])\n        # tokens = []\n        # Tokenize all full texts and att to tokens if its a token_pos, and not stop and is a alphacharacter\n        # for doc in dpipe:\n        #     tokens.append([t.lower_ for t in doc if (\n        #         t.pos_ in token_pos and not t.is_stop and t.is_alpha)])\n\n        # -------------------------------------Features that will not be used in CF step---------------------------------------------\n        # professionals_headline\n        # professionals_date_joined\n        # professionals_time_since_joined\n        # professionals_answers_count\n        # professionals_comments_count\n        # date_last_answer\n        # date_first_answer\n        # date_last_comment\n        # date_first_comment\n        # date_last_activity\n        # date_first_activity\n        # -------------------------------------Features that will not be used in CF step---------------------------------------------\n\n        # professional_answers_hearts, every heart iw worth 10 points (changed to 7 due to drasticlly diverged scores)\n        score += professional.professional_answers_hearts * 7\n        # print(score)\n        score = sigmoid(score)\n        # print(score)\n        return score\n\n    temp_df = pd.DataFrame(columns=['professional_id', 'question_id', 'score'])\n    qIds = [answer for answer in answers.loc[answers['answers_author_id']\n                                             == professional['professionals_id']]['answers_question_id']]  # array of answered question_ids\n\n    if len(qIds) == 0:\n        temp_df.loc[0] = [\n            professional['professionals_id'], '', 0]\n        # print(\"no answers\")\n    else:\n        for qId in qIds:\n            temp_df.loc[qIds.index(qId)] = [\n                professional['professionals_id'], qId, scoreing(qId)]\n    \n    return cf_df_matrix.append(temp_df)\n","b3fdb44f":"# cf_matrix = pd.read_csv(os.path.join(DATA_PATH, 'cf_matrix.csv')) Load if builded\ncf_df_matrix = pd.DataFrame(\n    columns=['professional_id', 'question_id', 'score'])\nfor _, professional in professionals.iterrows():\n    cf_df_matrix = answer_score_algorithm(\n        professional, cf_df_matrix)","48c2b894":"cf_df_matrix.to_csv(\n    r'.\/cf_matrix.csv', index=False)\ncf_df_matrix.tail(100)","1161e069":"def get_recomendations(pIds, pivot_table, number_of_recomendations):\n    \"\"\" TODO: Write description\n\n        param pIds: Professionals that have answered question similar to new one\n        param pivot_table: Pivot table with calculated scores for answers made\n        param number_of_recomendations: How many recomendations\n        return: array with recomendations on form ( professional_id, score from CF )\n    \"\"\"\n\n    def pearsons_r(s0, s1):\n        s0_c = s0 - s0.mean()\n        s1_c = s1 - s1.mean()\n        return np.sum(s0_c * s1_c) \/ np.sqrt(np.sum(s0_c ** 2) * np.sum(s1_c ** 2)) \n\n    answer_scores_rec = []\n    for pId in pIds:\n        for professional in pivot_table.columns:\n            cor = pearsons_r(pivot_table[pId], pivot_table[professional])\n            if np.isnan(cor):\n                continue\n            else:\n                answer_scores_rec.append((professional, cor))\n    answer_scores_rec.sort(key=lambda x: x[1], reverse=True)\n    return answer_scores_rec[:number_of_recomendations]\n","48c89532":"# Get recomendations\ncf_df_matrix = pd.read_csv(os.path.join('.\/', 'cf_matrix.csv'))\nmatrix = cf_df_matrix.pivot_table(\n    index='question_id', columns='professional_id', values='score')\n\npros = get_recomendations(prof_id, matrix, 20)\npros\n","dd3b43f8":"# Did we get valuable output? \nfor pro in pros:\n    print(\"Professional\", pro[0])\n    answers = [answer for answer in answers.loc[answers['answers_author_id']\n                                             == pro[0]]['answers_body']]\n    print(answers[:2])","005ee1f8":"## Sentence Encoder x Collaborative Filtering\nThe tfhub Sentence Encoder encodes text into high dimensional vectors. We use this to create a \"vector space\" to make textsemantic quaries with cosine similarity. Once we have a good set of matches to a new question, we could use the professionals that have answered the matches to find other qualified professionals usind collaborative filtering. ","d543a073":"## Gensim Model for cluster overview\n","ca468826":"## Load Data","65649dae":"## Pearson correlation coefficient\nThe correlation sign is determined by the regression slope: a value of +1 implies that all data points lie on a line for which y increases as X increases, and vice versa for -1.  A value of 0 implies that there is no linear dependency between the variables.\n\n${\\displaystyle r_{s_0 s_1}={\\frac {\\sum _{i=1}^{n}(s_{0i}-{\\bar {s_0}})(s_{1i}-{\\bar {s_1}})}{{\\sqrt {\\sum _{i=1}^{n}(s_{0i}-{\\bar {s_0}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(s_{1i}-{\\bar {s_1}})^{2}}}}}} = \\frac {\\sum _{i=1}^{n}s_{0c}s_{1c}}{\\sqrt {{\\sum _{i=1}^{n}s_{0c}^{2}} {\\sum _{i=1}^{n}s_{1c}^{2}}}}$","a2a3c152":"### Data Vis","d417ffc7":"### Global Settings","25ba7029":"## Sentence Encoder Module","0e5504f8":"## Pre-processing"}}