{"cell_type":{"6f5731f7":"code","b3d2bdae":"code","a7004df2":"code","4e64400a":"code","397510e3":"code","d694f9d0":"code","c1d0e60b":"code","5b3574b4":"code","a16b5ac2":"code","556bf9ad":"code","b2d58011":"code","805fb40a":"code","9557bb8c":"code","ab473d9d":"code","8e77951d":"code","d5158521":"code","f31d87eb":"markdown","ca64992a":"markdown","28dcc041":"markdown","5e93ee38":"markdown","d12e9363":"markdown","09a189d8":"markdown"},"source":{"6f5731f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3d2bdae":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.head(3)","a7004df2":"df_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_test.head(3)","4e64400a":"# With this function we categorize the data about the titles of the pessengers\ndef encode_(value):\n    if \"Mr.\" in value:\n        titre = \"Mr.\"\n    elif \"Mrs.\" in value:\n        titre = \"Mlle.\"\n    elif \"Miss.\" in value:\n        titre = \"Mlle.\"\n    elif \"Don.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Master.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Rev.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Dr.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Ms.\" in value:\n        titre = \"Mlle.\"\n    elif \"Major.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Mlle.\" in value:\n        titre = \"Mlle.\"\n    elif \"Mme.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Lady.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Sir.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Col.\" in value:\n        titre = \"Privilegie.\"\n    elif \"Capt.\" in value:\n        titre = \"Capt.\"\n    elif \"Countess.\" in value:\n        titre = \"Privilegie.\"\n    else:\n        titre = \"other\"\n        \n    return titre","397510e3":"# this function allow us to make all changes needed for qualitative and quantitative data.\n# for example, Age is a quantitative data that we change to qualitative, by giving labels\n# all quantitative data will be dropped at the end \ndef traitement(df):\n    df2 = df\n    df2.Age = df2.Age.fillna(df.Age.mean())\n    df2[\"class_age\"] = pd.cut(df2.Age, [0,15,25,70, 81], \n                              labels=['enfant', 'ado', 'adulte','vieux'])\n    df2[\"richesse\"] = pd.cut(df2.Fare, [0,7,15,30, 513], \n                              labels=['pauvre', 'normal', 'riche','tres_riche'])\n    df2.Fare = df2.Fare.fillna(df.Fare.mean())\n    df2.Cabin = df2.Cabin.fillna(\"Unknown\")\n    df2[\"titre\"] = df2.Name.apply(lambda x : encode_(x))\n    df2[\"group_cab\"] = df2.Cabin.str[0]\n    df2=df2.drop([\"Name\",\"Ticket\",\"Cabin\",\"Age\",\"Fare\"],axis=1)\n    df2.Embarked = df2.Embarked.fillna(\"Unknown\")\n    df2.Pclass = df2.Pclass.apply(lambda x : str(x))\n    df2.SibSp = df2.SibSp.apply(lambda x : str(x))\n    df2.Parch = df2.Parch.apply(lambda x : str(x))\n    df_transf = pd.get_dummies(df2)\n    df_transf = df_transf.drop(\"Sex_male\",axis=1)\n    list_cols = list(df_transf.columns)\n    return df2, df_transf, list_cols\n    ","d694f9d0":"# here we use the previous functions to create the dataframes that can be used in \n# our models. the \"traitement\" function is used twice, for train and test dataframes\ndf2, df_transf, list_cols = traitement(df)\ndf_test2, df_test_transf, list_cols_test = traitement(df_test)","c1d0e60b":"# the dataframe that we obtain here only contains the columns we need before \n# the transformation that gives us more columns from the qualitative features \n# (by using get_dummies)\ndf2.head()","5b3574b4":"# here we can see the dataframe transformed. There's no more empty data, \n# and many new columns has been created from the previous qualitative features\ndf_transf.info()","a16b5ac2":"# let's see here how many columns we get for the train and the test dataframes\n\nprint(len(list_cols))\nprint(list_cols)\nprint(len(list_cols_test))\nprint(list_cols_test)","556bf9ad":"# so now, let's take only the columns that we have in both train and test dataframes,\n# and also concatenate that with the \"Survived\" column, so that we can use it\n# for the modelisation\ndf_transf2 = pd.concat([df_transf[[\"Survived\"]],df_transf[[x for x in list_cols_test if x in list_cols ]]],axis=1)\nprint(len(df_transf2.columns))\nprint(df_transf2.columns)\n","b2d58011":"# here we import all packages we need for modelisation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nimport warnings\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import svm\n","805fb40a":"# now let's split the data to have : data, target, X_train, X_test, y_train, y_test\ndata = df_transf2.drop(\"Survived\", axis=1)\ntarget = df_transf2.Survived\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size = 0.2,random_state=123)","9557bb8c":"# here a huge function that compares all modeling method. At the end of this \n# function, we'll be able to see and compare the performances of all models\n# and make our choice for the prediction. \n\ndef compare_modeles():\n    \n    outer_cv = StratifiedKFold(n_splits=3, shuffle=True)\n    \n    # creer un df pour historiser les scores\n    df_score = pd.DataFrame(columns=['nom_df',\n                                     'modele', \n                                     'nb_variables',\n                                     'score',\n                                     'recall',\n                                     'precision',\n                                     'f1-score',\n                                     'auc',\n                                     'nested_mean',\n                                     'nested_std'])\n\n\n    \n    # RandomForest\n    clfrf = RandomForestClassifier(n_jobs=-1, random_state=123)\n    clfrf.fit(X_train, y_train)\n    y_pred = clfrf.predict(X_test)\n    cm = pd.crosstab(y_test, y_pred, rownames=[\"realit\u00e9\"], colnames=[\"predictions\"])\n    # print(cm)\n    # clfrf.score(X_test, y_test)\n    nested_moy = \"\"\n    nested_std = \"\"\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['clfrf',\n                                         type(clfrf).__name__, \n                                         len(list(data.columns)),\n                                         clfrf.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, clfrf.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True)\n    # df_score.sort_values(\"score\", ascending = False)\n    \n    # GridSearch DecisionTree\n    clf  = DecisionTreeClassifier(random_state=123)\n    params  = {'criterion': ['entropy','gini'],\n               'max_depth': [1, 2, 3,4, 5, 6, 10, 20]}\n    gridcv = GridSearchCV(clf,\n                           param_grid=params,\n                           scoring='accuracy',\n                           cv=3)\n    nested_score = cross_val_score(gridcv, \n                                   X_train, \n                                   y_train, \n                                   cv=outer_cv)\n    gridcv.fit(X_train, y_train)\n    # dfg = pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n    # print(gridcv.best_params_)\n    # dfg[dfg[\"params\"] == gridcv.best_params_]\n    \n    \n    \n    # DecisionTree\n    dtclf = DecisionTreeClassifier(criterion=gridcv.best_params_[\"criterion\"], max_depth = gridcv.best_params_[\"max_depth\"], random_state=123)\n    dtclf.fit(X_train, y_train)\n    y_pred = dtclf.predict(X_test)\n    cm = pd.crosstab(y_test, y_pred, rownames=[\"realit\u00e9\"], colnames=[\"predictions\"])\n    nested_moy = nested_score.mean()\n    nested_std = nested_score.std()\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['dtclf',\n                                         type(dtclf).__name__, \n                                         len(list(data.columns)),\n                                         dtclf.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, dtclf.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True)\n    \n    # GridSearch for AdaBoost\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        clf  = AdaBoostClassifier(base_estimator = dtclf, random_state=123)\n        params  = {'algorithm': ['SAMME','SAMME.R'],\n                   'learning_rate': [1, 5, 10],\n                   'n_estimators' : [10, 20, 50]}\n        gridcv = GridSearchCV(clf,\n                               param_grid=params,\n                               scoring='accuracy',\n                               cv=3)\n        nested_score = cross_val_score(gridcv, \n                                   X_train, \n                                   y_train, \n                                   cv=outer_cv)\n        gridcv.fit(X_train, y_train)\n        #dfg = pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n        #print(gridcv.best_params_)\n        #dfg[dfg[\"params\"] == gridcv.best_params_]\n    \n    # AdaBoost DecisionTree\n    acdt = AdaBoostClassifier(base_estimator = dtclf, random_state=123, algorithm=gridcv.best_params_[\"algorithm\"], learning_rate=gridcv.best_params_[\"learning_rate\"], n_estimators=gridcv.best_params_[\"n_estimators\"])\n    acdt.fit(X_train, y_train)\n    y_pred = acdt.predict(X_test)\n    nested_moy = nested_score.mean()\n    nested_std = nested_score.std()\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['acdt',\n                                         type(acdt).__name__, \n                                         len(list(data.columns)),\n                                         acdt.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, acdt.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True)\n    \n    # GridSearch LogisticReg\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        clf_lr  = LogisticRegression(random_state=123)\n        params_lr  = {'solver': ['liblinear', 'lbfgs'],\n                      'C': [1,10,100],\n                      'max_iter':[10,20,30]}\n        gridcv = GridSearchCV(clf_lr,\n                               param_grid=params_lr,\n                               scoring='accuracy',\n                               cv=3)\n        nested_score = cross_val_score(gridcv, \n                                   X_train, \n                                   y_train, \n                                   cv=outer_cv)\n        gridcv.fit(X_train, y_train)\n        #dfg = pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n        #print(gridcv.best_params_)\n        #print(dfg[dfg[\"params\"] == gridcv.best_params_])\n    \n    # LogisticRegression    \n    lr = LogisticRegression(C=gridcv.best_params_['C'], \n                            solver=gridcv.best_params_['solver'], \n                            random_state=123, \n                            max_iter=gridcv.best_params_['max_iter'])\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    #cm = pd.crosstab(y_test, y_pred, rownames=[\"realit\u00e9\"], colnames=[\"predictions\"])\n    #print(cm)\n    #clf_lr.score(X_test, y_test)\n    nested_moy = nested_score.mean()\n    nested_std = nested_score.std()\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['lr',\n                                         type(lr).__name__, \n                                         len(list(data.columns)),\n                                         lr.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True)\n    \n    # Gridsearch Adaboost Lr\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        clf  = AdaBoostClassifier(base_estimator = lr, random_state=123)\n        params  = {'algorithm': ['SAMME','SAMME.R'],\n                   'learning_rate': [1, 5, 10],\n                   'n_estimators' : [10, 20, 50]}\n        gridcv = GridSearchCV(clf,\n                               param_grid=params,\n                               scoring='accuracy',\n                               cv=3)\n        nested_score = cross_val_score(gridcv, \n                                   X_train, \n                                   y_train, \n                                   cv=outer_cv)\n        gridcv.fit(X_train, y_train)\n        #dfg = pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n        #print(gridcv.best_params_)\n        #print(dfg[dfg[\"params\"] == gridcv.best_params_])\n        \n    # Adaboost avec LogReg\n    aclr = AdaBoostClassifier(base_estimator = lr, random_state=123, algorithm=gridcv.best_params_[\"algorithm\"], learning_rate=gridcv.best_params_[\"learning_rate\"], n_estimators=gridcv.best_params_[\"n_estimators\"])\n    aclr.fit(X_train, y_train)\n    y_pred = aclr.predict(X_test)\n    #cm = pd.crosstab(y_test, y_pred, rownames=[\"realit\u00e9\"], colnames=[\"predictions\"])\n    #print(cm)\n    #aclr.score(X_test, y_test)\n    nested_moy = nested_score.mean()\n    nested_std = nested_score.std()\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['aclr',\n                                         type(aclr).__name__, \n                                         len(list(data.columns)),\n                                         aclr.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, aclr.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True)\n    \n    # Bagging on DecisionTree\n    bcdt = BaggingClassifier(base_estimator = dtclf, \n                           n_estimators = 100,  \n                           random_state=123, \n                           max_features=5)\n    bcdt.fit(X_train, y_train)\n    y_pred = bcdt.predict(X_test)\n    #cm = pd.crosstab(y_test, y_pred, rownames=[\"realit\u00e9\"], colnames=[\"predictions\"])\n    #print(cm)\n    #print(\"oob score :\", bc.oob_score_)\n    #print(\"score : \", bc.score(X_test, y_test))\n    nested_moy = \"\"\n    nested_std = \"\"\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['bcdt',\n                                         type(bcdt).__name__, \n                                         len(list(data.columns)),\n                                         bcdt.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, bcdt.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True)\n    \n    # Bagging on LogisticReg\n    bclr = BaggingClassifier(base_estimator = lr, \n                           n_estimators = 100,  \n                           random_state=123, \n                           max_features=5)\n    bclr.fit(X_train, y_train)\n    y_pred = bclr.predict(X_test)\n    nested_moy = \"\"\n    nested_std = \"\"\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['bclr',\n                                         type(bclr).__name__, \n                                         len(list(data.columns)),\n                                         bclr.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, bclr.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True) \n    \n    \n    #Grid Search XGB\n    clf_xgb  = xgb.XGBClassifier(use_label_encoder=False, verbosity=0)\n    params_xgb  = {'max_depth': [5,6],\n                   'n_estimators': [40,50],\n                   'learning_rate' : [0.1, 0.3]}\n    gridcv = GridSearchCV(clf_xgb,\n                           param_grid=params_xgb,\n                           scoring='accuracy',\n                           cv=3)\n    nested_score = cross_val_score(gridcv, \n                                   X_train, \n                                   y_train, \n                                   cv=outer_cv)\n    gridcv.fit(X_train, y_train)\n    #dfg = pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n    #print(gridcv.best_params_)\n    #dfg[dfg[\"params\"] == gridcv.best_params_]\n    \n    \n    gbm = xgb.XGBClassifier(verbosity=0,\n    use_label_encoder=False,\n    max_depth=gridcv.best_params_[\"max_depth\"],\n    n_estimators=gridcv.best_params_[\"n_estimators\"],\n    learning_rate=gridcv.best_params_[\"learning_rate\"]\n    )\n    gbm.fit(X_train, y_train)\n    y_pred = gbm.predict(X_test)\n    nested_moy = nested_score.mean()\n    nested_std = nested_score.std()\n    df_score = df_score.append(dict(zip(df_score.columns,\n                                        ['gbm',\n                                         type(gbm).__name__, \n                                         len(list(data.columns)),\n                                         bclr.score(X_test, y_test),\n                                         recall_score(y_test, y_pred, average='binary'),\n                                         precision_score(y_test, y_pred, average='binary'),\n                                         f1_score(y_test, y_pred, average='binary'),\n                                         roc_auc_score(y_test, gbm.predict_proba(X_test)[:, 1]),\n                                         nested_moy,\n                                         nested_std])\n                                        ),\n                               ignore_index=True) \n    \n    \n    \n    \n    \n    \n    \n    \n    return df_score, lr, bcdt, dtclf, acdt, clfrf, aclr, bclr, gbm","ab473d9d":"# now let's use the huge function and see the performances to compare\ndf_score, lr, bcdt, dtclf, acdt, clfrf, aclr, bclr, gbm = compare_modeles()\ndf_score.sort_values(\"score\", ascending = False)","8e77951d":"# now that we see the performances , we can try using the model that we consider best\n# let's try using : acdt\ndf_test_transf2 = df_test_transf[[x for x in list_cols_test if x in list_cols]]\ny_pred = acdt.predict(df_test_transf2)","d5158521":"# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': df_test_transf2.PassengerId,\n                       'Survived': y_pred})\noutput.to_csv('submission.csv', index=False)","f31d87eb":"# 1. Explore the data (EDA)","ca64992a":"Let's build some functions to transform data for better use. ","28dcc041":"# 3. Modeling","5e93ee38":"# Overview\n\n1. Explatory Data Analysis (EDA)\n2. Feature Engineering\n3. Modeling\n4. Prediction\n","d12e9363":"# 4. Prediction","09a189d8":"# 2. Feature Engineering"}}