{"cell_type":{"c6d664df":"code","71eab795":"code","4ce05689":"code","f36f8657":"code","317c0dbb":"code","94b9ac70":"code","65022bbd":"code","5012276a":"code","b293bac5":"code","dae681d7":"markdown","1d677620":"markdown","b6b2dfd7":"markdown","cd58ca0b":"markdown","a80f24bf":"markdown","e8426049":"markdown","9e7fecb9":"markdown","a22baea6":"markdown","197c9b2e":"markdown"},"source":{"c6d664df":"import pandas as pd\nx=pd;print(x.__name__, \"version:\", x.__version__)\nimport numpy as np\nx=np;print(x.__name__, \"version:\", x.__version__)\nimport matplotlib\nimport matplotlib.pyplot as plt\nx=matplotlib;print(x.__name__, \"version:\", x.__version__)\nimport seaborn as sns\nx=sns;print(x.__name__, \"version:\", x.__version__)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model, kernel_ridge, cluster, model_selection\nimport os, sys, math, datetime, shutil, pickle, itertools, json\nfrom IPython.core.interactiveshell import InteractiveShell","71eab795":"InteractiveShell.ast_node_interactivity = \"all\" # this causes all lines of a notebook cell to show output, not just last line\n%matplotlib inline\npd.set_option('display.max_columns', 100) #default of 10 columns is just too small to see much\npd.set_option('display.max_rows', 10)\nplt.style.use('seaborn-poster') #makes the graphs bigger; important for my 4K monitor\nsns.set(palette='deep')\nsns.set(context='poster')","4ce05689":"input_prefix = '..\/input\/'\nval_prefix = '.\/val_'\noutput_prefix = '.\/'\n\nid = \"fullVisitorId\"\ndf_train = pd.read_csv(input_prefix + \"train.csv\", index_col=id, dtype={id:str})\ndf_test = pd.read_csv(input_prefix + \"test.csv\", index_col=id, dtype={id:str})","f36f8657":"json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']#from inspection of data\n\nother_cols = list(set(df_train.columns).union(set(df_test.columns)) - set(json_cols))# all columns except json columns\nfor col in other_cols:\n    if len(set(df_train[col].unique()).union(set(df_test[col].unique()))) <= 1:# if all values are the same value, the column is not needed\n        val = df_train[col].iloc[0]\n        del df_train[col]\n        del df_test[col]\n        print(\"Removing trivial column: %r with unique value: %r\" % (col, val))","317c0dbb":"#just for reproduceability\nnp.random.seed(2718281828)\n\ncolumns = json_cols #start processing with the json_columns known by inspection\nwhile len(columns) > 0: #while there are json columns left to process\n    additional_json_cols = []#if a newly-generated column is itself a json column (from nesting), we need to process it the same way\n    for col in columns:\n        print(\"Getting keys for column:\", col)\n        print(\"  Train column from string to dict:\", col)\n        d = df_train[col].apply(json.loads)#get the dicts of all entries (strings) of this column\n        print(\"  Test column from string to dict:\", col)\n        t = df_test[col].apply(json.loads)\n\n        print(\"  Union of %s train col keys:\" % len(d), col)\n        keys = {x for x in d[0]}# try to find all keys, considering that keys may be missing in some rows\n        if col == 'totals':\n            #make sure we don't miss this one!  It is the target.\n            keys.add('transactionRevenue')\n            #add keys found in previous run\n            keys = keys.union({'newVisits', 'bounces', 'transactionRevenue', 'pageviews', 'visits', 'hits'})\n        elif col == 'device':\n            #add keys found in previous run\n            keys = keys.union({'mobileInputSelector', 'mobileDeviceMarketingName', 'browserVersion', 'mobileDeviceModel', 'flashVersion', 'language', 'screenResolution', 'operatingSystem', 'mobileDeviceBranding', \n                               'browser', 'browserSize', 'isMobile', 'deviceCategory', 'operatingSystemVersion', 'mobileDeviceInfo', 'screenColors'})\n        elif col == 'geoNetwork':\n            #add keys found in previous run\n            keys = keys.union({'cityId', 'longitude', 'networkLocation', 'latitude', 'country', 'metro', 'networkDomain', 'region', 'city', 'continent', 'subContinent'})\n        elif col == 'trafficSource':\n            #add keys found in previous run\n            keys = keys.union({'isTrueDirect', 'medium', 'source', 'adwordsClickInfo', 'campaign', 'adContent', 'keyword', 'referralPath'})\n        elif col == 'trafficSource_adwordsClickInfo':\n            #add keys found in previous run\n            keys = keys.union({'slot', 'gclId', 'targetingCriteria', 'isVideoAd', 'criteriaParameters', 'page', 'adNetworkType'})\n\n        #random samples to make sure we got all the keys\n        samples = np.random.randint(1, len(d) - 1, 1000)#increase this to ensure getting all keys\n        if col == 'totals' or col == 'trafficSource' or col == 'trafficSource_adwordsClickInfo':\n            samples = np.random.randint(1, len(d) - 1, 5000)#increase this to ensure getting all keys\n        count = 0\n        for i in samples:\n            count += 1\n            if count % 20000 == 0:\n                print(\"    rows processed:\", i)\n            if len(keys) != len(d[i]):#may have missing keys to add\n                old_keys = keys\n                keys = keys.union({x for x in d[i]})\n                if len(keys - old_keys) > 0:\n                    print(\"%s new keys added\" % len(keys - old_keys))\n            else:\n                for x in d[i]:\n                    if x not in keys:#definitely have missing keys to add\n                        old_keys = keys\n                        keys = keys.union({x for x in d[i]})\n                        print(\"%s new keys added\" % len(keys - old_keys))\n                        break\n\n        print(\"  Union of %s test col keys:\" % len(t), col)\n        count = 0\n        for i in np.random.randint(1, len(t) - 1, 100):#increase this to ensure getting all keys\n            count += 1\n            if count % 20000 == 0:\n                print(\"    rows processed:\", i)\n            if len(keys) != len(t[i]):\n                old_keys = keys\n                keys = keys.union({x for x in t[i]})\n                if len(keys - old_keys) > 0:\n                    print(\"%s new keys added\" % len(keys - old_keys))\n            else:\n                for x in t[i]:\n                    if x not in keys:\n                        old_keys = keys\n                        keys = keys.union({x for x in t[i]})\n                        print(\"%s new keys added\" % len(keys - old_keys))\n                        break\n\n        print(\"Found %s keys for train\/test column %s; replacing with keyed columns; keys = %r\" % (len(keys), col, keys))\n        keylist = list(keys)\n        keylist.sort() #for reproduceability\n        \n        for key in keylist:#now, add new fields of type column_key and delete the original column.  For missing values, use 'XNA' string; this will be replaced with something better on a column-by-column basis in the next script\n            new_field = col + \"_\" + key\n            df_train[new_field] = d.apply(lambda x:x.get(key, 'XNA'))#use 'XNA' string for missing values\n            df_test[new_field] = t.apply(lambda x:x.get(key, 'XNA'))\n\n            #some json structures have extra recursive depth; make value into a string and re-process\n            try:\n                l = len(set(df_train[new_field].unique()).union(set(df_test[new_field].unique())))# l==1 means column is trivial\n            except TypeError: # most likely: found a dict as a value, which \"unique\" doesn't like, so need to recurse\n                def to_str(x):\n                    if isinstance(x, dict):\n                        return json.dumps(x)\n                    else:\n                        return json.dumps(dict())\n                print(\"Recursively processing new column:\", new_field)\n                df_train[new_field] = df_train[new_field].apply(to_str)#convert to string first\n                df_test[new_field] = df_test[new_field].apply(to_str)\n                additional_json_cols.append(new_field)#add to columns that need to be processed again\n                l = len(set(df_train[new_field].unique()).union(set(df_test[new_field].unique())))\n\n            #make sure the column isn't trivial (just one constant value)\n            if l <= 1:\n                val = df_train[new_field].iloc[0]\n                del df_train[new_field]\n                del df_test[new_field]\n                if new_field in additional_json_cols:\n                    additional_json_cols.remove(new_field)\n                print(\"Removing trivial column: %r with unique value: %r\" % (new_field, val))\n        del df_train[col]\n        del df_test[col]\n    columns = additional_json_cols","94b9ac70":"#check for equivalent columns \n\nequivalent_columns = set()\nfor i in range(len(df_train.columns)):\n    for j in range(i):\n        col_i = df_train.columns[i]\n        col_j = df_train.columns[j]\n        if (df_train[col_i].factorize()[0] == df_train[col_j].factorize()[0]).all():\n            found = False\n            for x in equivalent_columns:\n                if col_i in x or col_j in x:\n                    x.add(col_i)\n                    x.add(col_j)\n                    found = True\n                    break\n            if not found:\n                equivalent_columns.add({col_i, col_j})\n\nequivalent_columns#possibly-equivalent columns","65022bbd":"df_train.to_csv(output_prefix + \"train_clean.csv.zip\", compression='zip')\ndf_test.to_csv(output_prefix + \"test_clean.csv.zip\", compression='zip')","5012276a":"split = int(df_train[['visitStartTime']].describe().loc['75%', 'visitStartTime'])\ndf_valtrain = df_train[df_train['visitStartTime'] < split]\ndf_valtest = df_train[df_train['visitStartTime'] >= split]","b293bac5":"df_valtrain.to_csv(val_prefix + \"train_clean.csv.zip\", compression='zip')\ndf_valtest.to_csv(val_prefix + \"test_clean.csv.zip\", compression='zip')","dae681d7":"## Imports","1d677620":"## Check for columns that, looked at as categorical, have the same values with perhaps different labels.\nThis can find false positives, e.g. if you have a column \"value\" and a column \"log_value\" that is the logarithm of the first, they will be found to be equivalent.  So in real life, secondary testing is needed.\n\nBut for this data, it finds none so I skipped the secondary testing.","b6b2dfd7":"## Settings","cd58ca0b":"# Tabularize Data\n\nConvert from json to a flattened table.\n\nI wrote this before learning about:\n\n    from pandas.io.json import json_normalize\n    \nbut it works (though slow)\n\nPublished because results needed for next script to be published when I fix some bugs. If you're doing it yourself, you'll probably prefer json_normalize.","a80f24bf":"## Create a validation set for local scoring by splitting by visitStartTime","e8426049":"## Load the data","9e7fecb9":"## The cleaned and tabularized data saved here (zipped, makes it save\/load faster and use less disk space)\n'XNA' used for nulls, but that is handled in the next script, to be published when some bugs are zapped.","a22baea6":"## Split json columns into separate fields\n\nBy inspection, the columns 'device', 'geoNetwork', 'totals', 'trafficSource' of each row are json strings that we need to split into columns.\n\nIt appears not all json records of a column have the same keys, so some random sampling is done.  To do it right, use:\n    from pandas.io.json import json_normalize -- this \"seems\" to get them all (I had done multiple runs with larger samples and put keys found by them in by hand).\n\nRemove any trivial columns that were just added.\n\nAnd repeat till all json data is flattened recursively.\n","197c9b2e":"## Remove trivial (constant) columns"}}