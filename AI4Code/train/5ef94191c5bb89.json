{"cell_type":{"c7e32ab3":"code","11beec70":"code","c96c85bc":"code","f8564351":"code","8870005a":"code","9ae87a8e":"code","d38e4f22":"code","b1e63eb0":"code","d3516d7a":"code","c11865a0":"code","f59a8e67":"code","3b8ec515":"code","d7644500":"code","351d2608":"code","68dfbf98":"code","7f3cca70":"code","6c675c02":"code","08f019cc":"code","c3a5ccc9":"code","5e5a2059":"code","af358caa":"code","abd3d29e":"code","20510146":"code","1e9fe95d":"code","185871ea":"code","8990a4ce":"code","82cded6f":"code","b9c2eff2":"code","7dc60795":"code","b9da8009":"code","c421550d":"code","fe325883":"code","eefaabc1":"code","9c5a1b56":"code","0dd07f2d":"code","2268fff0":"code","8941f3e2":"code","17064752":"code","c2316aa5":"code","4f1cd27b":"code","c30456eb":"code","6b5241d8":"code","839eeb4d":"code","82bf54ee":"code","ae7daa8b":"code","95ecdf44":"code","c6fecf10":"code","278cfd81":"code","a8738771":"code","4bacf3d5":"code","af570b90":"code","b6be5f55":"code","dce819ce":"code","fcf129f6":"code","57d91ad3":"code","01192eb7":"code","b16b514a":"code","e264d772":"code","b4c6b9c3":"code","c6922e70":"code","61e6a060":"code","f2fe3828":"code","65c0277c":"code","c6f1a47e":"code","3d0e7c5e":"code","3b604d6f":"code","0350c58c":"code","55e66201":"code","10a28cd6":"code","2aaeeea7":"code","b81ed8ea":"code","df652d80":"code","0e05ad60":"code","d327e694":"code","21aaa368":"code","74de6e4f":"code","fe559bf2":"markdown","666fd986":"markdown","4170c181":"markdown","41827d99":"markdown","c3a9e020":"markdown","97e8a2ef":"markdown","7d91fa7d":"markdown","54112da1":"markdown","4c2b6464":"markdown","d09b5990":"markdown"},"source":{"c7e32ab3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11beec70":"pip install dabl","c96c85bc":"pip install xgboost","f8564351":"pip install lightgbm","8870005a":"import dabl\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport seaborn as sns\nimport  matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt","9ae87a8e":"from numpy import absolute\nfrom sklearn.datasets import make_regression","d38e4f22":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor","b1e63eb0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom yellowbrick.model_selection import ValidationCurve\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.model_selection import train_test_split as tts\nfrom yellowbrick.regressor import ResidualsPlot\nfrom yellowbrick.regressor import PredictionError","d3516d7a":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\n\n%matplotlib inline\n\npd.pandas.set_option('display.max_columns', None)\n\nimport warnings","c11865a0":"#load dataset we are going to use \n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","f59a8e67":"#see samples on train dataset\n\ntrain.head()","3b8ec515":"#see the shapes of training and testing datasets\n\nprint('Training set shape:', train.shape)\nprint('Testing set shape:', test.shape)","d7644500":"#We store Id features in a variables and will use them after \n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","351d2608":"#Automatically plots for performing EDA and detect anomalies in dataset\n\n### However, we are not going to do this for testing dataset and after because even though we detect anomalies in testing set,\n# we could not delete rows with such anomalies.\n\ndabl.plot(train, 'SalePrice')","68dfbf98":"#see the training set structure\n\ntrain.info()","7f3cca70":"#For finding \"SalePrice\" distribution\n\nfc = dabl.SimpleClassifier(random_state=0)\nX = train.drop('SalePrice', axis=1)\ny = train.SalePrice\nfc.fit(X, y)","6c675c02":"#Some of the features are seen as numericals and they may be categoricals and others must be datetypes and they are numericals. So we transform them.\n\ntrain['GarageYrBlt'].fillna(train['GarageYrBlt'].mode()[0], inplace=True)\ntrain['GarageDateBlt'] = pd.to_datetime(train['GarageYrBlt'], format='%Y') \n\ntrain.drop([\"GarageYrBlt\"], axis = 1, inplace = True) \n\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\ntrain['YrSold'] = train['YrSold'].astype(str)\ntrain['MoSold'] = train['MoSold'].astype(str)\n\ntrain['DateBuilt'] = pd.to_datetime(train['YearBuilt'], format='%Y') \n\n\ntrain['DateSold'] = pd.to_datetime(train['YrSold'], format='%Y') \n\n\ntrain['DateRemodAdd'] = pd.to_datetime(train['YearRemodAdd'], format='%Y') \n\ntrain.drop([\"YearBuilt\", \"YrSold\", \"YearRemodAdd\"], axis = 1, inplace = True) ","08f019cc":"#Some of the features are seen as numericals and they may be categoricals and others must be datetypes and they are numericals. So we transform them.\n\ntest['GarageYrBlt'].fillna(test['GarageYrBlt'].mode()[0], inplace=True)\ntest['GarageDateBlt'] = pd.to_datetime(test['GarageYrBlt'], format='%Y') \n\ntest.drop([\"GarageYrBlt\"], axis = 1, inplace = True) \n\ntest['MSSubClass'] = test['MSSubClass'].apply(str)\ntest['YrSold'] = test['YrSold'].astype(str)\ntest['MoSold'] = test['MoSold'].astype(str)\n\ntest['DateBuilt'] = pd.to_datetime(test['YearBuilt'], format='%Y') \n\n\ntest['DateSold'] = pd.to_datetime(test['YrSold'], format='%Y') \n\n\ntest['DateRemodAdd'] = pd.to_datetime(test['YearRemodAdd'], format='%Y') \n\ntest.drop([\"YearBuilt\", \"YrSold\", \"YearRemodAdd\"], axis = 1, inplace = True) ","c3a5ccc9":"#Function for finding numerical and categorical features\n\ndef characteristics(dataset):\n    print('Shape of the dataset: {}'.format(dataset.shape))\n    dataset_numerical = dataset.select_dtypes(include = [np.number])\n    print('Number of Numerical Features: {}'.format(dataset_numerical.shape[1]))\n    dataset_categorical = dataset.select_dtypes(exclude = [np.number])\n    print('Number of Categorical Features: {}'.format(dataset_categorical.shape[1]))","5e5a2059":"characteristics(train)","af358caa":"characteristics(test)","abd3d29e":"#Variables for storing numerical and categoricals features in training dataset\n\nnumerical_train = train.select_dtypes(include = [np.number])\nnumerical_features = numerical_train.columns\n\ncategorical_train = train.select_dtypes(exclude = [np.number])\ncategorical_features = categorical_train.columns","20510146":"#Variables for storing numerical and categoricals features in testing dataset\n\nnumerical_test = test.select_dtypes(include = [np.number])\nnumerical_features = numerical_test.columns\n\ncategorical_test = test.select_dtypes(exclude = [np.number])\ncategorical_features = categorical_test.columns","1e9fe95d":"# Deleting features with more than 96% of common values in both dataset because these features won't be efficient to predict \"SalePrice\".\n\noverfit_categorical_train = []\nfor i in categorical_train:\n    counts = train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(train) * 100 > 96:\n        overfit_categorical_train.append(i)\n\noverfit_categorical_train = list(overfit_categorical_train)\ntest = test.drop(overfit_categorical_train, axis=1)\ntrain = train.drop(overfit_categorical_train, axis=1)","185871ea":"overfit_numerical_train = []\nfor i in numerical_train:\n    counts = train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(train) * 100 > 96:\n        overfit_numerical_train.append(i)\n\noverfit_numerical_train = list(overfit_numerical_train)\ntest = test.drop(overfit_numerical_train, axis=1)\ntrain = train.drop(overfit_numerical_train, axis=1)","8990a4ce":"print(\"Categorical Features with >96% of the same value: \",overfit_categorical_train)\nprint(\"Numerical Features with >96% of the same value: \",overfit_numerical_train)","82cded6f":"print('Training set shape is now :', train.shape)","b9c2eff2":"print('Testing set shape is now :', test.shape)","7dc60795":"#Variables for storing numerical and categoricals features in training dataset\n\nnumerical_train = train.select_dtypes(include = [np.number])\nnumerical_features = numerical_train.columns\ncategorical_train = train.select_dtypes(exclude = [np.number])\ncategorical_features = categorical_train.columns","b9da8009":"#Variables for storing numerical and categoricals features in testing dataset\n\nnumerical_test = test.select_dtypes(include = [np.number])\nnumerical_features = numerical_test.columns\n\ncategorical_test = test.select_dtypes(exclude = [np.number])\ncategorical_features = categorical_test.columns","c421550d":"#Plots for helping determine outliers\n\ntrain.shape[1]\n#a = int(np.sqrt(train.shape[1]))\na = 4\nb = int(train.shape[1]\/4)\nr = int(train.shape[1]\/a)\nc = int(train.shape[1]\/b)\ni = 0\nfig, ax = plt.subplots(nrows=r, ncols=c, figsize=(15, 60))\nfor row in ax:\n    for col in row:\n        try:\n            col.scatter(x = train[train.columns[i]], y = np.log(train['SalePrice']))\n            col.title.set_text(train.columns[i])\n        except:\n            temp=1\n        #except Exception as e:\n        #    print(e.message, e.args)\n        finally:\n            temp=1\n        i = i + 1\n        \nplt.show()","fe325883":"fig = plt.figure(figsize=(20,15))\nfor index,col in enumerate(numerical_train):\n    plt.subplot(6,5,index+1)\n    sns.countplot(x=col, data=numerical_train.dropna())\nfig.tight_layout(pad=1.0)","eefaabc1":"train = train.drop(train[train['LotFrontage'] > 200].index)\ntrain = train.drop(train[train['LotArea'] > 100000].index)\ntrain = train.drop(train[train['BsmtFinSF1'] > 4000].index)\ntrain = train.drop(train[train['TotalBsmtSF'] > 5000].index)\ntrain = train.drop(train[train['GrLivArea'] > 4000].index)\ntrain = train.drop(train[train['EnclosedPorch'] > 401].index)\ntrain.reset_index(drop=True, inplace=True)","9c5a1b56":"trt = train.copy()\noutliers = []\n\noutliers.append(trt[trt['1stFlrSF']>2700][trt['SalePrice']<500_000].index)\noutliers.append(trt[trt['BsmtFullBath']==3.0].index)\noutliers.append(trt[trt['GrLivArea']>3300][trt['SalePrice']<300_000].index)\noutliers.append(trt[trt['FullBath']==0.0][trt['SalePrice']>300_000].index)\noutliers.append(trt[trt['GarageArea']>1200][trt['SalePrice']<200_000].index)\noutliers.append(trt[trt['OpenPorchSF']>500].index)\n\noutliers = [x[0] for x in outliers]\noutliers\ntrain.drop(outliers, axis=0, inplace=True)\ntrain.shape","0dd07f2d":"def characteristics(dataset):\n    print('Shape of the dataset: {}'.format(dataset.shape))\n    dataset_numerical = dataset.select_dtypes(include = [np.number])\n    print('Number of Numerical Features: {}'.format(dataset_numerical.shape[1]))\n    dataset_categorical = dataset.select_dtypes(exclude = [np.number])\n    print('Number of Categorical Features: {}'.format(dataset_categorical.shape[1]))","2268fff0":"characteristics(train)","8941f3e2":"characteristics(test)","17064752":"train_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)[:71]\nmissing_data = pd.DataFrame({'Missing Ratio' :train_na})\nmissing_data.head(50)","c2316aa5":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=train_na.index, y=train_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","4f1cd27b":"train[\"PoolQC\"] = train[\"PoolQC\"].fillna(\"None\")\ntrain[\"MiscFeature\"] = train[\"MiscFeature\"].fillna(\"None\")\ntrain[\"Alley\"] = train[\"Alley\"].fillna(\"None\")\ntrain[\"Fence\"] = train[\"Fence\"].fillna(\"None\")\ntrain[\"FireplaceQu\"] = train[\"FireplaceQu\"].fillna(\"None\")\ntrain[\"MasVnrType\"] = train[\"MasVnrType\"].fillna(\"None\")\n\n\ntrain[\"GarageType\"] = train[\"GarageType\"].fillna(\"None\")\ntrain[\"GarageFinish\"] = train[\"GarageFinish\"].fillna(\"None\")\ntrain[\"GarageQual\"] = train[\"GarageQual\"].fillna(\"None\")\ntrain[\"GarageCond\"] = train[\"GarageCond\"].fillna(\"None\")\ntrain[\"BsmtQual\"] = train[\"BsmtQual\"].fillna(\"None\")\ntrain[\"BsmtCond\"] = train[\"BsmtCond\"].fillna(\"None\")\ntrain[\"BsmtExposure\"] = train[\"BsmtExposure\"].fillna(\"None\")\ntrain[\"BsmtFinType1\"] = train[\"BsmtFinType1\"].fillna(\"None\")\ntrain[\"BsmtFinType2\"] = train[\"BsmtFinType2\"].fillna(\"None\")\n\ntrain[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\ntrain['GarageCars'] = train['GarageCars'].fillna(0)\n\ntrain[\"MasVnrArea\"] = train[\"MasVnrArea\"].fillna(0)\n\ntrain['Electrical'] = train['Electrical'].fillna(\"Typ\")","c30456eb":"test_na = (test.isnull().sum() \/ len(test)) * 100\ntest_na = test_na.drop(test_na[test_na == 0].index).sort_values(ascending=False)[:71]\nmissing_data = pd.DataFrame({'Missing Ratio' :test_na})\nmissing_data.head(50)","6b5241d8":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=test_na.index, y=test_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature in testing dataset', fontsize=15)","839eeb4d":"test[\"PoolQC\"] = test[\"PoolQC\"].fillna(\"None\")\ntest[\"MiscFeature\"] = test[\"MiscFeature\"].fillna(\"None\")\ntest[\"Alley\"] = test[\"Alley\"].fillna(\"None\")\ntest[\"Fence\"] = test[\"Fence\"].fillna(\"None\")\ntest[\"FireplaceQu\"] = test[\"FireplaceQu\"].fillna(\"None\")\ntest[\"MasVnrType\"] = test[\"MasVnrType\"].fillna(\"None\")\n\n\ntest[\"GarageType\"] = test[\"GarageType\"].fillna(\"None\")\ntest[\"GarageFinish\"] = test[\"GarageFinish\"].fillna(\"None\")\ntest[\"GarageQual\"] = test[\"GarageQual\"].fillna(\"None\")\ntest[\"GarageCond\"] = test[\"GarageCond\"].fillna(\"None\")\ntest[\"BsmtQual\"] = test[\"BsmtQual\"].fillna(\"None\")\ntest[\"BsmtCond\"] = test[\"BsmtCond\"].fillna(\"None\")\ntest[\"BsmtExposure\"] = test[\"BsmtExposure\"].fillna(\"None\")\ntest[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].fillna(\"None\")\ntest[\"BsmtFinType2\"] = test[\"BsmtFinType2\"].fillna(\"None\")\n\ntest[\"LotFrontage\"] = test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\ntest['GarageCars'] = test['GarageCars'].fillna(0)\n\ntest[\"MasVnrArea\"] = test[\"MasVnrArea\"].fillna(0)\n\ntest['MSZoning'].fillna(test['MSZoning'].mode()[0], inplace=True)\ntest['Functional'].fillna(test['Functional'].mode()[0], inplace=True)\n\ntest['BsmtHalfBath'].fillna(test['BsmtHalfBath'].mode()[0], inplace=True)\ntest['BsmtFullBath'].fillna(test['BsmtFullBath'].mode()[0], inplace=True)\n\ntest['SaleType'].fillna(test['SaleType'].mode()[0], inplace=True)\ntest['GarageArea'].fillna(test['GarageArea'].mode()[0], inplace=True)\ntest['KitchenQual'].fillna(test['KitchenQual'].mode()[0], inplace=True)\ntest['TotalBsmtSF'].fillna(test['TotalBsmtSF'].mode()[0], inplace=True)\ntest['BsmtUnfSF'].fillna(test['BsmtUnfSF'].mode()[0], inplace=True)\ntest['BsmtFinSF2'].fillna(test['BsmtFinSF2'].mode()[0], inplace=True)\ntest['BsmtFinSF1'].fillna(test['BsmtFinSF1'].mode()[0], inplace=True)\ntest['Exterior2nd'].fillna(test['Exterior2nd'].mode()[0], inplace=True)\ntest['Exterior1st'].fillna(test['Exterior1st'].mode()[0], inplace=True)\n","82bf54ee":"#Encoding Categorical variables\n\n\ncategorical_train = train.select_dtypes(exclude = [np.number])\ncategorical_features = categorical_train.columns\n\ncategorical_test = test.select_dtypes(exclude = [np.number])\ncategorical_features = categorical_test.columns","ae7daa8b":"for i in categorical_train:\n    fe = train.groupby(i).size()\/len(train)\n    train.loc[:, i] = train[i].map(fe)","95ecdf44":"for i in categorical_test:\n    fe = test.groupby(i).size()\/len(test)\n    test.loc[:, i] = test[i].map(fe)","c6fecf10":"# Deleting features with more than 96% of common values in training dataset because these features won't be efficient to predict \"SalePrice\".\n\noverfit_categorical_train = []\nfor i in categorical_train:\n    counts = train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(train) * 100 > 96:\n        overfit_categorical_train.append(i)\n\noverfit_categorical_train = list(overfit_categorical_train)\ntrain = train.drop(overfit_categorical_train, axis=1)\ntest = test.drop(overfit_categorical_train, axis=1)\n\noverfit_numerical_train = []\nfor i in numerical_train:\n    counts = train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(train) * 100 > 96:\n        overfit_numerical_train.append(i)\n\noverfit_numerical_train = list(overfit_numerical_train)\ntrain = train.drop(overfit_numerical_train, axis=1)\ntest = test.drop(overfit_numerical_train, axis=1)\n\nprint(\"Categorical Features with >96% of the same value: \",overfit_categorical_train)\nprint(\"Numerical Features with >96% of the same value: \",overfit_numerical_train)","278cfd81":"correlated_features = set()\ncorrelation_matrix = train.drop('SalePrice', axis=1).corr()\n\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n            \nfor i in correlated_features:\n    train.drop(i, axis=1, inplace=True)\n    test.drop(i, axis=1, inplace=True)","a8738771":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Lasso","4bacf3d5":"train_saleprice = train[\"SalePrice\"]\n\ntrain2 = train.drop(['SalePrice'],axis=1)\n\nX_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\n\nlinear = LinearRegression()\nvisualizer = PredictionError(linear)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","af570b90":"visualizer = ResidualsPlot(linear)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","b6be5f55":"X_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\nLasso = Lasso()\nvisualizer = PredictionError(Lasso)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","dce819ce":"visualizer = ResidualsPlot(Lasso)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","fcf129f6":"X_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\n\nSVR = SVR()\nvisualizer = PredictionError(SVR)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","57d91ad3":"visualizer = ResidualsPlot(SVR)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","01192eb7":"X_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\n\nDecisionTreeRegressor = DecisionTreeRegressor()\nvisualizer = PredictionError(DecisionTreeRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","b16b514a":"visualizer = ResidualsPlot(DecisionTreeRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","e264d772":"X_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\n\nRandomForestRegressor = RandomForestRegressor()\nvisualizer = PredictionError(RandomForestRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","b4c6b9c3":"visualizer = ResidualsPlot(RandomForestRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","c6922e70":"X_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\n\nGradientBoostingRegressor = GradientBoostingRegressor()\nvisualizer = PredictionError(GradientBoostingRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","61e6a060":"visualizer = ResidualsPlot(GradientBoostingRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","f2fe3828":"X_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\n\nLGBMRegressor = LGBMRegressor()\nvisualizer = PredictionError(LGBMRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","65c0277c":"visualizer = ResidualsPlot(LGBMRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","c6f1a47e":"X_train1, X_train2, y_train1, y_train2 = tts(train2, train_saleprice, test_size=0.4, shuffle=True)\n\nXGBRegressor = XGBRegressor()\nvisualizer = PredictionError(XGBRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","3d0e7c5e":"visualizer = ResidualsPlot(XGBRegressor)\nvisualizer.fit(X_train1, y_train1,)\nvisualizer.score(X_train2, y_train2)\nvisualizer.show()","3b604d6f":"X_train = train.drop(['SalePrice'],axis=1)\ny_train = train['SalePrice']\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y_train, y_pred):\n    return np.sqrt(mean_squared_error(y_train, y_pred))\n\ndef cv_rmse(model, X_train=X_train):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","0350c58c":"score1 = cv_rmse(LinearRegression())\nprint(\"LinearRegression: {:.4f} ({:.4f})\\n\".format(score1.mean(), score1.std()), )\n\nscore2 = cv_rmse(Lasso)\nprint(\"Lasso: {:.4f} ({:.4f})\\n\".format(score2.mean(), score2.std()), )\n\nscore4 = cv_rmse(DecisionTreeRegressor)\nprint(\"DecisionTreeRegressor: {:.4f} ({:.4f})\\n\".format(score4.mean(), score4.std()), )\n\nscore5 = cv_rmse(RandomForestRegressor)\nprint(\"RandomForestRegressor: {:.4f} ({:.4f})\\n\".format(score5.mean(), score5.std()), )\n\nscore6 = cv_rmse(GradientBoostingRegressor)\nprint(\"GradientBoostingRegressor: {:.4f} ({:.4f})\\n\".format(score6.mean(), score6.std()), )\n\nscore7 = cv_rmse(LGBMRegressor)\nprint(\"LGBMRegressor: {:.4f} ({:.4f})\\n\".format(score7.mean(), score7.std()), )\n\nscore8 = cv_rmse(XGBRegressor)\nprint(\"XGBRegressor: {:.4f} ({:.4f})\\n\".format(score8.mean(), score8.std()), )\n","55e66201":"X = train.drop('SalePrice', axis=1)\ntarget = train['SalePrice']\n\n# Load a regression dataset\nviz = ValidationCurve(\n    GradientBoostingRegressor, param_name=\"max_depth\",\n    param_range=np.arange(1, 11), cv=10, scoring=\"r2\"\n)\n\n# Fit and show the visualizer\nviz.fit(X, target)\nviz.show()","10a28cd6":"# Now, we gonna use RFE with linear regression but we could also use VIF Calculation for finding best features for our linear and svr models.\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n\nX = train.drop('SalePrice', axis=1)\ntarget = train['SalePrice']\n\n\nselector = RFECV(GradientBoostingRegressor, step=1, cv=5, scoring=\"neg_mean_squared_error\")\nselector = selector.fit(X, target)\nprint('Optimal number of features: {}'.format(selector.n_features_))","2aaeeea7":"plt.figure(figsize=(16, 9))\nplt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\nplt.xlabel('Number of features selected', fontsize=14, labelpad=20)\nplt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_, color='#303F9F', linewidth=3)\nplt.show()","b81ed8ea":"test.drop(X.columns[np.where(selector.support_ == False)[0]], axis=1, inplace=True)\ntrain.drop(X.columns[np.where(selector.support_ == False)[0]], axis=1, inplace=True)","df652d80":"X = train.drop('SalePrice', axis=1)\ntarget = train['SalePrice']\n\nX_train, X_val, y_train, y_val = tts(X, target, test_size=0.2, random_state=2020)","0e05ad60":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Lasso","d327e694":"#We did the same thing for GradientBoostingRegressor and detect these hyperparameters are best:\n\nGradientBoostingRegr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\nGradientBoostingRegr.fit(X_train, y_train)","21aaa368":"Predictions = GradientBoostingRegr.predict(test)","74de6e4f":"submission = pd.DataFrame({'Id': test_ID,\n                           'SalePrice': Predictions})\nsubmission.to_csv('submission_boost.csv', index=False)","fe559bf2":"### This tells us the best model for our dataset is GradientBoostingRegressor and the validation curve shoxs it's adapted to our dataset.\n\n#### Now, let use Recursive Feature Selection for having best features that we gonna use for model optimization.","666fd986":"# Outliers detection\n\n\n### The presence of outliers in a classifcation or regression dataset can result in a poor fit and lower predictive modeling performance.<br> In that way, we have to treat them effectively before modeling.<br>\n\n### The first thing to do when we want to see if there are outliers in our training dataset is to plot all variables in but we are not going <br>to perform outliers detection on our testing dataset. We will lay back after to explain why. \n\n### Here is the code to do that :","4170c181":"### As we will build differents models (Robust Regression, Support Vector Regression, XGBoost and Random Forest) and each model has it's features selection criteria, we will perform different features selection algorithms based on each models. <br>\n\n#### For example, if we wanna build a robust regression model, we should select features based on a non-collinearity criteria and looking for overfiting\/underfiting too.<br>\n\n#### Also, Linear Kernel of Support vector Regression is very similar to linear Regression, and hence the effect of multicollinearity has a very similar effect in case of Linear Kernel of SVR.<br> So we have to remove multicollinearity between features , if we want to use weight vectors directly for feature importance. \n\n#### Random Forest uses bootstrap sampling and feature sampling, i.e row sampling and column sampling. Therefore Random Forest is not affected by multicollinearity that much since it is picking different set of features for different models and of course every model sees a different set of data points. But there is a chance of multicollinear features getting picked up together, and when that happens we will see some trace of it.\n\nhttps:\/\/medium.com\/@raj5287\/effects-of-multi-collinearity-in-logistic-regression-svm-rf-af6766d91f1b","41827d99":"The first step is Exploratory Data Analysis.","c3a9e020":"# Handling missing values\n\n\n### Missing values <br> In that way, we have to treat them effectively before modeling.<br>\n\n### The first thing to do when we want to see if there are outliers in our training dataset is to plot all variables in but we are not going <br>to perform outliers detection on our testing dataset. We will lay back after to explain why. \n\n### Here is the code to do that :","97e8a2ef":"# Now we gonna start features selection for building our models.\n\nFeature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n\nHaving irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\n\nThree benefits of performing feature selection before modeling your data are:\n\n    Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n    Improves Accuracy: Less misleading data means modeling accuracy improves.\n    Reduces Training Time: Less data means that algorithms train faster.\n\n\nhttps:\/\/machinelearningmastery.com\/feature-selection-machine-learning-python\/","7d91fa7d":"https:\/\/towardsdatascience.com\/all-about-categorical-variable-encoding-305f3361fd02","54112da1":"### Now we should notice that all except SVR are good for our dataset. So, we gonna use them.","4c2b6464":"### As seen with plots, we have several outliers in our training dataset but how should we perform them ?<br>\n\n### We have two differents ways to treat outliers which are :<br>\n   \n### 1.    The first one is manually by reading plots and delete rows when certains conditions are not repected;<br>\n\n### 2.   The second one is to perform outliers detection automatically and for that, there are several methods.<br>\n      \n### You can visit [4 Automatic Outlier Detection Algorithms in Python  : site d'Estia](https:\/\/www.groupe-estia.fr\/)\n   \n   \n ### In our case, we are going to do that manually. <br>","d09b5990":"### Features selection for Linear regression and SVR"}}