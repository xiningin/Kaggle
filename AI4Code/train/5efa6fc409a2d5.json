{"cell_type":{"d8a0ad01":"code","4f513c74":"code","14e14c7a":"code","459f1a26":"code","c8b1f595":"code","25beeb35":"code","35f6e48d":"code","20e65181":"code","67e355d8":"code","faaeb95b":"code","191081f0":"code","f3121518":"code","3d93f404":"code","9329b2d3":"code","56e78ed6":"code","b03726ca":"code","76ac9b74":"code","57e21a9d":"code","8f4c79f9":"code","bf5cb677":"code","a382415d":"code","30ac5504":"code","eb2a980b":"code","bc6d1a05":"code","e3dc357f":"markdown","87f7aa87":"markdown","96fd9024":"markdown","33ffa3fc":"markdown","ed44d54d":"markdown","d2b95fb4":"markdown","036004be":"markdown"},"source":{"d8a0ad01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f513c74":"#loading the pandas library\n\nimport pandas as pd\n\n#loading the excel sheets into notebook\n\nexcel_sheets =pd.ExcelFile('\/kaggle\/input\/Question Set - FB_Page_Decoder.xlsx')\n\n\n\n#checking the names of sheets present\n\nexcel_sheets.sheet_names\n\n#creating separate dataframes for the sheets\n\ndf_post=excel_sheets.parse('Post_List')\ndf_comment=excel_sheets.parse('Comments')\ndf_react=excel_sheets.parse('Reactions')\ndf_share=excel_sheets.parse('Shares')\n","14e14c7a":"#checking if data frames are ok\n\ndf_post.head()","459f1a26":"df_comment.head()","c8b1f595":"df_react.head()","25beeb35":"df_share.head()","35f6e48d":"#extracting the required data from these dataframes and putting that data in data frame\n\nc1=pd.DataFrame(df_share.Shares_By.value_counts())\nc1.head(20)","20e65181":"#repeating the same with other dataframes \n\nc2=pd.DataFrame(df_react.Reactions_By.value_counts())\nc3=pd.DataFrame(df_comment.Name.value_counts())\n\n#merging the dataframes to single df for result\n\nfinal=pd.concat([c1,c2,c3],axis=1, join= 'outer')\nfinal['total_interactions']=final.sum(axis=1)\nfinal","67e355d8":"#getting the loactaion of the person with maximum interactions and printing his\/her name\n\nfinal.total_interactions.idxmax()\n","faaeb95b":"#repeating the process above to know which post has maximum interaction\n\n\nu2=pd.DataFrame(df_comment.Post_URL.value_counts()).rename(columns={'Post_URL':'Comment_count'})\nu3=pd.DataFrame(df_react.Post_URL.value_counts()).rename(columns={'Post_URL':'Reaction_count'})\nu4=pd.DataFrame(df_share.Post_URL.value_counts()).rename(columns={'Post_URL':'Share_count'})\nuf=pd.concat([u2,u3,u4], axis=1, join= 'outer')\nuf['Total_reactions']=uf.sum(axis=1)\n\nuf.head()\n","191081f0":"#to know the url of post with max interactions\n\nkw=uf.Total_reactions.idxmax()\nkw","f3121518":"#from post_list trying to get to know which post does url corresponds\n\ndf_post.Post_Text.loc[df_post['Post_URL']==kw]","3d93f404":"#loading required libraries\n!pip install nltk\n!pip install stop_words","9329b2d3":"import nltk\nfrom nltk import tokenize\nfrom operator import itemgetter\nimport math\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english'))","56e78ed6":"post=\"Dear Shankra Family, as we are facing an out of ordinary situation due to the pandemic of COVID-19 which affects our daily life, we felt that sharing our thoughts on the future was much needed. To reassure you, we are not planning or considering to cancel Shankra 2020. This uncertain moment has an impact on the way our society functions and Shankra Festival is no exception to that. Still, we have an unshakable hope that four months from now till our festival gate opens, the grand scale of things will be much different from now. With that faith and hope we still continue the production of the festival as planned. As event organizers we will implement and execute with responsibility all rules, regulations and recommendations issued by the Swiss Government. It always starts with a simple action\u00e2\u20ac\u00a6 now more than ever that prevails to be the truth. Individual responsibility can have an immeasurable impact in the environment surrounding us, particularly in this delicate moment that concerns the health of your beloved family and community. We request you to be constantly informed and execute as a duty the health and safety measures issued by your local government and authorities, as they may vary from country to country. For all the ticket holders, we are working on a suitable solution in case of a forced cancellation. We are evaluating the situation and will provide you with more information in due time. We hope in your patience and understanding. Your Life is Your Message Shankra Team\"","b03726ca":"total_words = post.split()\ntotal_word_length = len(total_words)\nprint(total_word_length)\n","76ac9b74":"#counting number of sentences in post\n\ntotal_sentences = tokenize.sent_tokenize(post)\ntotal_sent_len = len(total_sentences)\nprint(total_sent_len)\n","57e21a9d":"#print(total_sent_len)","8f4c79f9":"#calculating tf score\n\ntf_score = {}\nfor each_word in total_words:\n    each_word = each_word.replace('.','')\n    if each_word not in stop_words:\n        if each_word in tf_score:\n            tf_score[each_word] += 1\n        else:\n            tf_score[each_word] = 1\n\n# Dividing by total_word_length for each dictionary element\ntf_score.update((x, y\/int(total_word_length)) for x, y in tf_score.items())\n\nprint(tf_score)","bf5cb677":"#calculating idf score\n\ndef check_sent(word, sentences): \n    final = [all([w in x for w in word]) for x in sentences] \n    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n    return int(len(sent_len))\nidf_score = {}\nfor each_word in total_words:\n    each_word = each_word.replace('.','')\n    if each_word not in stop_words:\n        if each_word in idf_score:\n            idf_score[each_word] = check_sent(each_word, total_sentences)\n        else:\n            idf_score[each_word] = 1\n\n# Performing a log and divide\nidf_score.update((x, math.log(int(total_sent_len)\/y)) for x, y in idf_score.items())\n\nprint(idf_score)\n","a382415d":"tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\nprint(tf_idf_score)\n\n##to know the keyword with hisghest interaction \n\ndef get_top_n(dict_elem, n):\n    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n    return result\n\nprint(get_top_n(tf_idf_score,1))\n","30ac5504":"# In Comments, you have a column called comment - Create a wordcloud AFTER cleaning the data properly.\n\n#loading required libraries\n\nfrom wordcloud import WordCloud, STOPWORDS \n\nimport matplotlib.pyplot as plt\n\n#creating wordcloud\n\ntext = df_comment['Comment'].values \nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,min_font_size = 10).generate(str(text))\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n\nplt.show()\n\n","eb2a980b":"# \"Using the Reactions Data, Reshape the Data from Long to Wide \n#- New data will have Columns called Date, Reaction_What , Count of Reactions and Minimum Date to Maximum Dates (*Join Post_List and Reactions to get Date)\"\n\n#creating new dataframe for reshaping\n\nre1=df_react\nre1['idx']=re1.groupby('Reactions_What').cumcount()\nre1\n\n","bc6d1a05":"#counting rection and making them a separate entity an dmaking reshape\n\nre1['Count_reaction']=re1.Reactions_What.value_counts()\nfor url in re1['Post_URL']:\n    re1['Date']=df_post['post_date']\nre1.pivot(index=None, columns='Reactions_What', values= None)","e3dc357f":"**Next**","87f7aa87":"# Q-5: Counting reaction and making them a separate entity and making reshape","96fd9024":"**'Benjamin Rutschmann'** is interacting with the page the highest.(Interactions = Likes + Shares + Comments)","33ffa3fc":"# Q-4: \"Using the Reactions Data, Reshape the Data from Long to Wide \n# - New data will have Columns called Date, Reaction_What , Count of Reactions and Minimum Date to Maximum Dates (*Join Post_List and Reactions to get Date)\"\n","ed44d54d":"# Q-2 : In Post_List, you have a column called Post_Text - Identify the Keywords on which interactions are the highest. (Interactions = Likes + Shares + Comments)","d2b95fb4":"# Q-3: In Comments, you have a column called comment - Create a wordcloud AFTER cleaning the data properly.\n","036004be":"# Q-1 : Which User is interacting with the page the highest? (Interactions = Likes + Shares + Comments)"}}