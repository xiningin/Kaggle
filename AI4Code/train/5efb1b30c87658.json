{"cell_type":{"0caacd9a":"code","83bd6a45":"code","8ad90829":"code","7cb8c128":"code","8ea6d8d7":"code","36c6a8d8":"code","9d9a5553":"code","5b02f7c1":"code","80600431":"code","c20262e3":"code","61b4ef07":"markdown","0cfc7584":"markdown","4b1e0913":"markdown","57b6b97e":"markdown","1ed95599":"markdown","d0d8d288":"markdown","e9b1de31":"markdown","e2fb5a61":"markdown","87f83158":"markdown","95791d46":"markdown","e387ead1":"markdown"},"source":{"0caacd9a":"import numpy as np\n\n# Ein Multi-Layer-Perceptron mit variabler Anzahl an versteckten Ebenen (hidden layer)\nclass NeuralNetwork:\n    \n    # Lernrate\n    lr = 0.2\n    \n    # Verwaltung der Ebenen (Layer)\n    layer = []\n    \n    # Direkter Zugriff auf letzte Ebene\n    output_layer = []\n        \n    # \u00dcbergabe der Anzahl an Neuronen je Ebene.\n    # Die Anzahl an versteckten Ebenen (hidden layer) ist durch \"*args\" variabel w\u00e4hlbar\n    def __init__(self, *args):\n        # Alle Ebenen (Layer) erzeugen und deren Gewichte initialisieren\n        for i in range(len(args)):            \n            if i == 0:\n                self.layer = [Layer(args[i], [])]\n            else:\n                self.layer.append(Layer(args[i], self.layer[i-1]))\n        self.output_layer = self.layer[len(self.layer)-1]\n        \n    \n    # Wandelt die Antwort (als eine Zahl) in die entsprechende nx1 Matrix (Spaltenvektor)\n    # Wird ben\u00f6tigt, wenn es mehr als ein Neuron in der Ausgabe-Ebene gibt\n    def convertNumberToMatrix(self, number):\n        matrix = np.array([])\n        for i in range(number):\n            matrix = np.append(matrix, 0)    \n        matrix = np.append(matrix, 1)\n        for i in range(len(matrix)-1, self.output_layer.c_neurons - 1):\n            matrix = np.append(matrix, 0)\n        return matrix   \n        \n    # Wandelt die Antwort als eine nx1 Matrix in eine einzige entsprechende Zahl\n    # Wird ben\u00f6tigt, wenn es mehr als ein Neuron in der Ausgabe-Ebene gibt\n    def convertMatrixToNumber(self, matrix):\n        number = 0\n        minRange = 1\n        for i in range(len(matrix)):\n            if 1 - matrix[i] < minRange:\n                minRange = 1 - matrix[i]\n                number = i\n        return number   \n    \n    \n    # Vorw\u00e4rts (input -> output) durch das MLP gehen\n    def feedforward(self, input_data):\n        # Die Ausgabe der ersten Ebene (Eingabeebene) sind die Eingabedaten\n        self.layer[0].output = input_data\n        \n        # F\u00fcr alle verbleibenden Ebenen werden die Ausgaben berechnet\n        # Die Ausgaben werden in der jeweiligen Ebene gespeichert\n        for i in range(1, len(self.layer)):\n            self.layer[i].calculateOutput()\n        \n        # Die Ausgabe der letzen Ebene (Ausgabeebene) wird an den Aufrufer zur\u00fcckgegeben \n        return self.output_layer.output\n    \n    \n    def train(self, input_data, answer):\n        # Bestimmen der aktuellen Vorhersage des MLP\n        guess = self.feedforward(input_data)\n        \n        # Wenn mehr als ein Neuron in der Ausgabeebene existiert, entsteht als Ausgabe eine (n x 1)-Matrix\n        # Um diese mit der korrekten Ausgabe vergleichen zu k\u00f6nnen, wird auch die korrekte Ausgabe\n        # in eine Matrix umgewandelt\n        if self.output_layer.c_neurons > 1:\n            answer = self.convertNumberToMatrix(answer)\n            \n        # Fehler und Gradient der Ebenen berechnen\n        i = len(self.layer)-1\n        while i > 0:\n            if i == len(self.layer)-1:\n                self.layer[i].error = answer - guess\n            else:\n                self.layer[i].calculateError(self.layer[i+1].weights, self.layer[i+1].error)\n            self.layer[i].calculateGradient(self.lr)\n            i -= 1\n        \n        # Gewichte der Ebenen anpassen (die erste Ebene (Eingabeebene) hat keine Gewichte!)\n        for i in range(1, len(self.layer)):\n            self.layer[i].adjustWeights() \n            \n        # Werte, die nach diesem Trainingsdurchlauf nicht mehr relevant sind l\u00f6schen\n        # (Damit diese nicht im n\u00e4chsten Durchlauf zu Komplikationen f\u00fchren)\n        for layer in self.layer:\n            layer.output = []\n            layer.error = []\n            layer.gradient = []\n               \n    \n    # Eine Vorhersage treffen ausgehend von \u00fcbergebenen Daten\n    def guess(self, data):\n        guess = self.feedforward(data)\n\n        # Wenn mehr als ein Neuron in der Ausgabeebene existiert, entsteht als Ausgabe eine (n x 1)-Matrix\n        # Um beim Aufrufer eine eindeutige R\u00fcckgabe zu haben, wird diese in eine Zahl umgewandelt\n        if self.output_layer.c_neurons > 1:\n            guess = self.convertMatrixToNumber(guess)\n            \n        return guess                    ","83bd6a45":"# Die Klasse Layer    \nclass Layer:\n    \n    # Die vorherige Ebene (die Eingangsebene hat keine!)\n    prev_layer = []\n    \n    # Anzahl der Neuronen in dieser Ebene\n    c_neurons = []\n    \n    # Gewichte der Eingaben dieser Ebene\n    weights = []\n    \n    # Gewichte der Basiseingabe dieser Ebene\n    weights_bias = []\n    \n    # Ausgabe, die diese Ebene zuletzt erzeugt hat\n    output = []\n    \n    # Entstandener Fehler bei letzter Ausgabe\n    error = []\n    \n    # Gradient - ben\u00f6tigt bei backpropagation\n    gradient = []\n    \n    \n    # Initialisieren der Ebene (layer) mit der Anzahl an Neuronen auf dieser Ebene\n    # und der vorherigen Ebene\n    # Wenn eine vorherige Ebene existiert und somit diese Ebene nicht die Eingabeebene ist\n    # werden die Gewichte initialisiert\n    def __init__(self, c_neurons, prev_layer):\n        self.c_neurons = c_neurons\n        self.prev_layer = prev_layer\n        # Die Eingabeebene hat keine Gewichte!\n        if prev_layer != []:\n            self.initWeights()\n             \n    # Initialisieren der Gewichte der Eingaben dieser Ebene\n    # F\u00fcr die erste Ebene (die Eingabeebene) entf\u00e4llt dies\n    def initWeights(self):\n        # Gewichte der Eingaben der vorherigen Ebene initialisieren\n        self.weights = np.random.randn(self.c_neurons, self.prev_layer.c_neurons)\n        # Gewichte der Basis initialisieren\n        self.weights_bias = np.random.randn(self.c_neurons)\n        \n        \n    # Aktivierungsfunktion Sigmoid\n    def sigmoid(self, x):\n        return 1 \/ (1 + np.exp(-x))\n        \n    # Ableitung der Aktivierungsfunktion Sigmoid\n    # Es gillt: y = sigmoid(x) !!!\n    def dsigmoid(self, y):\n        return y * (1 - y)  \n    \n    \n    # Berechnet die gewichtete Summe\n    # Gewichtsmatrix mit Eingabematrix multiplizieren und Matrix der Basisgewichte addieren\n    def calculateWSum(self):\n        return self.weights.dot(self.prev_layer.output) + self.weights_bias\n        \n    # Berechnet die Ausgabe der Ebene\n    def calculateOutput(self):\n        self.output = self.sigmoid(self.calculateWSum()) \n        \n        \n    # Berechnet den Fehler, der bei der Ausgabe auf dieser Ebene entstanden ist\n    # Unter nxt_weights und nxt_error werden die Gewichte bzw. der Fehler der n\u00e4chsten Ebene\n    # (Richtung Ausgabeebene) verstanden!\n    def calculateError(self, nxt_weights, nxt_error):\n        self.error = np.transpose(nxt_weights).dot(nxt_error)\n        \n        \n    # Berechnet den Gradient dieser Ebene\n    def calculateGradient(self, lr):\n        self.gradient = lr * self.error * self.dsigmoid(self.output)\n        \n        \n    def adjustWeights(self):\n        # Aufzuaddierende Gewichts\u00e4nderung bestimmen\n        d_weights = np.transpose(np.array([self.gradient])).dot(np.array(self.prev_layer.output)[np.newaxis])\n        d_weights_bias = self.gradient\n        \n        # Gewichte anpassen (\u00c4nderung auf alte aufaddieren)\n        self.weights = self.weights + d_weights\n        self.weights_bias = self.weights_bias + d_weights_bias       ","8ad90829":"import pandas as pd\n\n# Die vier m\u00f6glichen Eingaben f\u00fcr XOR (als ein Pandas-DataFrame)\ninputs = pd.DataFrame([[1, 0], [0, 1], [1, 1], [0, 0]])\nprint(\"Eingaben:\\n\", inputs.values)\n# Die korrekten Ausgaben f\u00fcr die (vier) Eingaben\nanswers = pd.DataFrame([[1], [1], [0], [0]])\nprint(\"\\nSoll-Ausgaben:\\n\", answers.values)","7cb8c128":"# Neurales MLP-Netz erzeugen\nnNetwork = NeuralNetwork(len(inputs.loc[0, : ]), 16, 16, 1)","8ea6d8d7":"import random    \n\nn = 5000\n# Training n-Mal durchf\u00fchren\nfor i in range(n):\n    # Einen zuf\u00e4lligen Datensatz aus den Trainingsdaten nehmen\n    randomInt = random.randint(0, len(inputs) - 1)\n    input = inputs.loc[randomInt, : ].values\n    answer = answers.loc[randomInt].values[0]\n    # Training f\u00fcr Eingabe und korrekte Ausgabe\n    nNetwork.train(input, answer)","36c6a8d8":"# Testfall 1: Eingabe [1, 0]\ninput1 = [1,0]\nguess1 = nNetwork.guess(input1)\n\n# Testfall 2: Eingabe [0, 0]\ninput2 = [0,0]\nguess2 = nNetwork.guess(input2)\n\n# Testfall 3: Eingabe [0, 1]\ninput3 = [0,1]\nguess3 = nNetwork.guess(input3)\n\n# Testfall 4: Eingabe [1, 1]\ninput4 = [1,1]\nguess4 = nNetwork.guess(input4)\n\n\ndef printResult(input, guess):\n    print(\"Eingabe: %s -> Ausgabe: %s => %s\" \n          % (input, guess, (\"True (1)\" if guess >= 0.5 else \"False (0)\")))\n\nprint(\"Vorhersagen:\")\nprintResult(input1, guess1)\nprintResult(input2, guess2)\nprintResult(input3, guess3)\nprintResult(input4, guess4)","9d9a5553":"import pandas as pd\n\n# Einlesen der Dateien\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n# Die korrekten Ziffern separat hinterlegen\nanswers = train[\"label\"]\n\n# Die Spalte \"label\" aus der train.csv entfernen\ninputs = train.drop(labels = [\"label\"],axis = 1)\n\n# Werte verkleinern (Overflow vermeiden)\ninputs = inputs \/ 255.0\ntest = test \/ 255.0\n\n# Da nicht mehr ben\u00f6tigt l\u00f6schen um Platz zu sparen\ndel train","5b02f7c1":"# Neurales MLP-Netz erzeugen\nnNetwork = NeuralNetwork(len(inputs.loc[0, : ]), 78, 78, 10)","80600431":"import random\n\nn = 100000\n# Training n-Mal durchf\u00fchren\nmoeglicheIndizies = []\nfor i in range(n):\n    # Einen zuf\u00e4lligen Datensatz aus den Trainingsdaten nehmen\n    # Es sollen m\u00f6glichst alle Datens\u00e4tze einmal verwendet werden\n    if len(moeglicheIndizies) == 0:\n        moeglicheIndizies = list(range(0, len(inputs)))\n    randomInt = random.randint(0, len(moeglicheIndizies) - 1)\n    randomIndex = moeglicheIndizies[randomInt]\n    input = inputs.loc[randomInt, : ].values\n    answer = answers.loc[randomInt]\n    # Training f\u00fcr Eingabe und korrekte Ausgabe\n    nNetwork.train(input, answer)\n    # Genutztes Element entfernen\n    moeglicheIndizies.remove(moeglicheIndizies[randomInt])","c20262e3":"import csv\n\n# Neurales MLP-Netz testen mit Testdatens\u00e4tzen\n# Die Ergebnisse werden in eine .csv-Datei geschrieben\n\n# Eine neue .csv-Datei \u00f6ffnen und \"Header\" schreiben\nwith open('test_submission.csv', 'w') as testSubmission:\n    writer = csv.writer(testSubmission)\n    writer.writerow(['ImageId', 'Label'])\n    \n    # Jeden Datensatz in test.csv durchgehen\n    for i in range(len(test)):\n        # Vorhersage treffen\n        guess = nNetwork.guess(test.loc[i, : ].values)\n        writer.writerow([(i+1), guess])\n        \n# Datei schlie\u00dfen       \ntestSubmission.close()","61b4ef07":"# XOR als Testfall\nUm die Funktionalit\u00e4t des MLP-Netzes an sich zu testen, wird es mit dem handhabbareren XOR-Problem getestet.","0cfc7584":"# 3. MLP trainieren\nDas Neuronale Netz wird mit dem vorbereiteten Trainingsdatensatz trainiert.","4b1e0913":"# 1. Vorbereitungen\nDie Eingabedaten (train.csv und test.csv) werden mittels Pandas eingelesen und in Datasets gespeichert.\nDie Trainingsdaten aus der train.csv Datei werden in zwei Datasets aufgeteilt: Einmal nur die jeweiligen Antworten, einmal die Werte der ganzen Pixel. Damit bei sp\u00e4teren Berechnungen keine Overflows entstehen werden die Werte der Pixel auf 0 bis 1 (von vorher 0 bis 255) eingeschr\u00e4nkt bzw. umgewandelt. Das Dataset \"train\" kann dann aufgrund seiner Redundanz gel\u00f6scht werden, um Resourcen freizugeben.<br>\nDie Vorbereitung der Daten orientiert sich an der dargestellten Vorgehensweise <a href=\"https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\">dieses Kernels<\/a>.","57b6b97e":"# Das Digit Recognizer Problem\nNun da die Funktionalit\u00e4t des MLP-Netzes sichergestellt ist, widmen wir uns nun dem eigentlichen Problem des Erkennens von handgeschriebenen Ziffern.","1ed95599":"# Klasse NeuralNetwork\nHier ein Multi-Layer-Perceptron mit variabler Anzahl Schichten (Input, variabel viele Hidden Layer, Output). Die Anzahl an Neuronen je Schicht sowie die Anzahl an Schichten k\u00f6nnen beim Erzeugen einer NeuralNetwork-Instanz beliebig gew\u00e4hlt werden.<br>Der Aufbau der Klasse(n) sowie die verwendeten Algorithmen sind adaptiert von der Vorgehensweise, die in der Videoreihe \"<a href=\"https:\/\/www.youtube.com\/playlist?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb\">Session 4 - Neural Networks - Intelligence and Learning\"<\/a> von \"The Coding Train\" demonstriert wird<\/a>.","d0d8d288":"# 4. MLP Testen\nDas trainierte MLP wird nun mit den ebenfalls vorbereiteten Testdatens\u00e4tzen getestet. Die jeweilige Vorhersage wird als Zeile in eine .csv-Ausgabedatei geschrieben. Die erste Zeile der Datei enth\u00e4lt die \"Header\" der Spalten ('ImageId' und 'Label').","e9b1de31":"# Klasse Layer\nJede Ebene des MLP ist eine eigenen Instanz dieser Klasse und verwaltet u.A. ihre eigenen Gewichte und kann ihre Ausgabe als auch den produzierten Fehler bestimmen. Die gewichtete Summe als auch die Aktivierungsfunktion werden in diesen Instanzen angewandt.","e2fb5a61":"Das Ergebnis des Trainings soll nun \u00fcberpr\u00fcft werden. In diesem Fall dienen die Eingaben der Trainingsdaten auch gleichzeitig als Testf\u00e4lle (da es nun mal keine weiteren m\u00f6glichen Eingaben gibt).<br>\nErwartungsgem\u00e4\u00df sollte die Ausgabereihenfolge folgenderma\u00dfen sein: True, False, True, False","87f83158":"Das neurale MLP-Netz das wir f\u00fcr diesen Zweck nutzen soll jeweils 8 Neuronen auf den Hidden Layer haben und eine Ausgabe produzieren (True oder False).","95791d46":"# 2. Neuronales Netz (MLP) erzeugen\nDas Neurale Netz bzw. das Multi-Layer-Perceptron wird erzeugt. Es soll 784 Inputs haben (entsprechend der Anzahl an Pixeln je Bild), 12 Neuronen auf der ersten als auch der zweiten Hidden Layer und 10 Neuronen auf der Output-Schicht (entsprechend der zehn m\u00f6glichen Ziffern). Vor allem die Anzahl an Neuronen in den versteckten Ebenen (Hidden Layer) sind frei w\u00e4hlbar.","e387ead1":"Das MLP wird mit einem zuf\u00e4llig ausgew\u00e4hlten Datensatz der Trainingsdaten (hier nur eine Auswahl aus den vier m\u00f6glichen Eingaben) trainiert. Dies wird n-Mal wiederholt und geschieht aufgrund der zuf\u00e4lligen Auswahl in einer zuf\u00e4lligen Reihenfolge."}}