{"cell_type":{"3653afcf":"code","7699cee5":"code","c3ff4c01":"code","a2bb4d28":"code","1a957ccf":"code","9f60a661":"code","43343995":"code","dc153dd9":"code","a7442f65":"code","f8a1395b":"code","0a85738d":"code","bfaf39c1":"code","43407d0a":"code","d6ade028":"code","e187ebb5":"code","d3dce58e":"code","17d60ae7":"code","db3a30db":"code","ce937e73":"code","8d1c135c":"code","ce42f91c":"code","2d9de6c1":"code","d5345dca":"code","eec18586":"code","a1dc4c88":"code","f7636b0e":"code","8ee77f9f":"code","54349e06":"code","5d3cbe24":"code","4c07364f":"code","dad0eb45":"code","6b0d2745":"code","f79d8a2e":"code","1bfd4759":"code","07d1e9b9":"code","a6f8aa51":"code","c6f9337d":"code","a6942477":"code","8ebd8539":"code","788a966e":"code","79bc4664":"code","fab97061":"code","92f02151":"code","d6b86a22":"code","70310f13":"code","472ee851":"code","1ca04e13":"code","94999271":"code","f9d162bc":"code","9a4c2512":"code","23892b1a":"code","a2c62e00":"markdown","6d30162e":"markdown","c8bf27de":"markdown","9be5685f":"markdown","893a7ce8":"markdown","c9017a78":"markdown","b6e760fb":"markdown","e21d5b9a":"markdown","2a9d7b77":"markdown","6d465bcd":"markdown","f7f0a0fc":"markdown","9d7cdfd7":"markdown"},"source":{"3653afcf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\/train_amex\"))\n\n# Any results you write to the current directory are saved as output.","7699cee5":"train=pd.read_csv('..\/input\/train_amex\/train.csv',)\nhist=pd.read_csv('..\/input\/train_amex\/historical_user_logs.csv')","c3ff4c01":"test=pd.read_csv('..\/input\/test_lnmuiyp\/test.csv')","a2bb4d28":"HistJoined_train = train.join(hist, on='user_id',how='left',rsuffix='_hist')\nHistJoined_test = test.join(hist, on='user_id',how='left',rsuffix='_hist')\n#hist.head()\n#train.head()\n\ndel train\ndel test\n","1a957ccf":"HistJoined_train.head()","9f60a661":"print('train.shape: %s\\n test.shape: %s'%(HistJoined_train.shape,HistJoined_test.shape))","43343995":"#HistJoined['product_category_2'].value_counts()\nHistJoined_train.skew()\n#train['is_click'].value_counts()\n#train.info()\n#train['DateTime']\n#train.isnull().sum()\n#HistJoined_train.isnull().mean()\n","dc153dd9":"HistJoined_train.info()","a7442f65":"HistJoined_train.columns","f8a1395b":"colsToEncode=['product','gender','product_hist','action']\n#colsToEncode\nfor cols in colsToEncode:\n    HistJoined_train[cols] = pd.factorize(HistJoined_train[cols])[0]\n    HistJoined_test[cols] = pd.factorize(HistJoined_test[cols])[0]    ","0a85738d":"HistJoined_train['is_click'].skew()","bfaf39c1":"HistJoined_train.isnull().mean()","43407d0a":"NullCols=['product_category_2','user_group_id','age_level','user_depth','city_development_index']\nHistJoined_train[NullCols].head()\n","d6ade028":"#HistJoined_train.dropna(subset=['age_level','user_group_id','user_depth'],axis=0,inplace=True)\n#HistJoined_test.dropna(subset=['age_level','user_group_id','user_depth'],axis=0,inplace=True)\n\n\n#values = {'city_development_index':-1,'product_category_2':-1}\nHistJoined_train.fillna(method='ffill',inplace=True)\nHistJoined_test.fillna(method='ffill',inplace=True)\n\nvalues = {'city_development_index':-1,'product_category_2':-1}\nHistJoined_train.fillna(value=values,inplace=True)\nHistJoined_test.fillna(value=values,inplace=True)","e187ebb5":"HistJoined_train.isnull().sum()","d3dce58e":"HistJoined_test.isnull().sum()","17d60ae7":"import datetime","db3a30db":"HistJoined_train['DateTime'] = [datetime.datetime.strptime(d,'%Y-%m-%d %H:%M') for d in HistJoined_train['DateTime']]\nHistJoined_train['DateTime_hist'] = [datetime.datetime.strptime(d,'%Y-%m-%d %H:%M') for d in HistJoined_train['DateTime_hist']]\n\nHistJoined_test['DateTime'] = [datetime.datetime.strptime(d,'%Y-%m-%d %H:%M') for d in HistJoined_test['DateTime']]\nHistJoined_test['DateTime_hist'] = [datetime.datetime.strptime(d,'%Y-%m-%d %H:%M') for d in HistJoined_test['DateTime_hist']]\n","ce937e73":"HistJoined_train['hour_of_day']=[d.hour for d in HistJoined_train['DateTime']]\nHistJoined_train['hour_of_day_hist']=[d.hour for d in HistJoined_train['DateTime_hist']]\n\nHistJoined_test['hour_of_day']=[d.hour for d in HistJoined_test['DateTime']]\nHistJoined_test['hour_of_day_hist']=[d.hour for d in HistJoined_test['DateTime_hist']]","8d1c135c":"#Parts_of_day\n#Morning,afternoon,evening,night,late-night.\n\nHistJoined_train['Parts_of_day'] = pd.cut(x=HistJoined_train['hour_of_day'],bins=5,labels=[0,1,2,3,4]).astype('int64')\nHistJoined_test['Parts_of_day'] = pd.cut(x=HistJoined_test['hour_of_day'],bins=5,labels=[0,1,2,3,4]).astype('int64')\n\nHistJoined_train['Parts_of_day_hist'] = pd.cut(x=HistJoined_train['hour_of_day_hist'],bins=5,labels=[0,1,2,3,4]).astype('int64')\nHistJoined_test['Parts_of_day_hist'] = pd.cut(x=HistJoined_test['hour_of_day_hist'],bins=5,labels=[0,1,2,3,4]).astype('int64')\n\n#HistJoined_train.groupby('Parts_of_day')['hour_of_day'].value_counts()","ce42f91c":"#HistJoined_train.groupby('user_id')['webpage_id'].value_counts()\n\nHistJoined_train['No_of_Webpage_Hits'] = HistJoined_train.groupby('user_id',)['webpage_id'].transform('count')\nHistJoined_test['No_of_Webpage_Hits'] = HistJoined_test.groupby('user_id',)['webpage_id'].transform('count')\n\n#HistJoined_train['No_of_Webpage_Hits'].value_counts()","2d9de6c1":"HistJoined_train['No_of_Product_Hits_hist'] = HistJoined_train.groupby('user_id_hist',)['product_hist'].transform('count')\nHistJoined_test['No_of_Product_Hits_hist'] = HistJoined_test.groupby('user_id_hist',)['product_hist'].transform('count')\n","d5345dca":"#HistJoined_train.groupby(['user_id'])['DateTime'].diff()\nHistJoined_train.info()","eec18586":"#Function to optimize the memory by downgrading the datatype to optimal length\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b \/ 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)","a1dc4c88":"dtype_list=[]\nfor col in HistJoined_train.columns:\n    dtype_list.append(HistJoined_train[col].dtypes)\ndtype_list=list(set(dtype_list))\nprint(\"Total Datatypes present: %s \"%dtype_list)","f7636b0e":"# Analysing for Train dataset\nfor dtype in dtype_list:\n    \n    if 'int' in str(dtype):\n        print(\"Analyse %s\"%str(dtype))\n        df_int=HistJoined_train.select_dtypes(include=[str(dtype)])\n        converted_int = df_int.apply(pd.to_numeric,downcast='unsigned')\n    \n        print(mem_usage(df_int))\n        print(mem_usage(converted_int))\n        \n    elif 'float' in str(dtype):\n        print(\"Analyse %s\"%str(dtype))\n        df_float = HistJoined_train.select_dtypes(include=[str(dtype)])\n        converted_float = df_float.apply(pd.to_numeric,downcast='float')\n        \n        print(mem_usage(df_float))\n        print(mem_usage(converted_float))","8ee77f9f":"print(\"Memory Usage of Original dataset: %s\"%mem_usage(HistJoined_train))\nHistJoined_train[converted_int.columns] = converted_int\nHistJoined_train[converted_float.columns] = converted_float\nprint(\"Memory Usage of Optimized dataset: %s\"%mem_usage(HistJoined_train))","54349e06":"# Analysing for Test dataset\nfor dtype in dtype_list:\n    \n    if 'int' in str(dtype):\n        print(\"Analyse %s\"%str(dtype))\n        df_int=HistJoined_test.select_dtypes(include=[str(dtype)])\n        converted_int = df_int.apply(pd.to_numeric,downcast='unsigned')\n    \n        print(mem_usage(df_int))\n        print(mem_usage(converted_int))\n        \n    elif 'float' in str(dtype):\n        print(\"Analyse %s\"%str(dtype))\n        df_float = HistJoined_test.select_dtypes(include=[str(dtype)])\n        converted_float = df_float.apply(pd.to_numeric,downcast='float')\n        \n        print(mem_usage(df_float))\n        print(mem_usage(converted_float))","5d3cbe24":"print(\"Memory Usage of Original dataset: %s\"%mem_usage(HistJoined_test))\nHistJoined_test[converted_int.columns] = converted_int\nHistJoined_test[converted_float.columns] = converted_float\n\nprint(\"Memory Usage of Optimized dataset: %s\"%mem_usage(HistJoined_test))","4c07364f":"session_id_test = HistJoined_test.session_id\nHistJoined_train.drop(['DateTime','DateTime_hist','session_id','user_id','product','campaign_id','webpage_id'],axis=1,inplace=True)\nHistJoined_test.drop(['DateTime','DateTime_hist','session_id','user_id','product','campaign_id','webpage_id'],axis=1,inplace=True)\n\n#HistJoined_train.drop(['DateTime','DateTime_hist'],axis=1,inplace=True)\n#HistJoined_test.drop(['DateTime','DateTime_hist'],axis=1,inplace=True)\n\nHistJoined_train.head()","dad0eb45":"type(session_id_test)","6b0d2745":"fig,ax = plt.subplots(figsize=(15,15))\nax = sns.heatmap(data=HistJoined_train.corr(method='pearson'),annot=True,cmap='YlGnBu',linewidths=0.05)\n","f79d8a2e":"HistJoined_train.isnull().sum()","1bfd4759":"from sklearn.model_selection import train_test_split","07d1e9b9":"X=HistJoined_train.drop(['is_click'],axis=1)\ny=HistJoined_train['is_click']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22,shuffle=True,stratify=y)\n\nprint('X_train.shape %s, X_test.shape %s\\ny_train.shape %s, y_test.shape %s'%(X_train.shape,X_test.shape,y_train.shape,y_test.shape))","a6f8aa51":"from sklearn.preprocessing import StandardScaler","c6f9337d":"scaler = StandardScaler()\n# Fit only to the training data\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.fit_transform(X_test)","a6942477":"from sklearn import model_selection\n#performance metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n#Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n#Classifiers\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\n","8ebd8539":"from sklearn.linear_model import LogisticRegression","788a966e":"LR = LogisticRegression(class_weight='balanced',penalty='l2',solver='saga',max_iter=100,C=1)\n\nLR.fit(scaled_X_train,y_train)\ny_pred_lr = LR.predict(scaled_X_test)","79bc4664":"print('Accuracy: %.4f' %accuracy_score(y_pred=y_pred_lr,y_true=y_test))\nprint('Confusion Matrix: \\n%s'%confusion_matrix(y_pred=y_pred_lr,y_true=y_test))\nprint('Classification report: \\n %s'%classification_report(y_pred=y_pred_lr,y_true=y_test))\nprint('AUC score: %.5f'%roc_auc_score(y_test,y_pred_lr))","fab97061":"from imblearn.over_sampling import SMOTE\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(scaled_X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","92f02151":"### Oversampled data\nrfc_over = RandomForestClassifier(n_estimators=50,class_weight='balanced',criterion='entropy',max_depth=500,)\n\nrfc_over.fit(X_train_res,y_train_res)\ny_pred_rf = rfc_over.predict(scaled_X_test)","d6b86a22":"print('Accuracy: %.4f' %accuracy_score(y_pred=y_pred_rf,y_true=y_test))\nprint('Confusion Matrix: \\n%s'%confusion_matrix(y_pred=y_pred_rf,y_true=y_test))\nprint('Classification report: \\n %s'%classification_report(y_pred=y_pred_rf,y_true=y_test))\nprint('AUC score: %.5f'%roc_auc_score(y_test,y_pred_rf))","70310f13":"'''class_wt = dict({0:1.39,1:80})\nrfc = RandomForestClassifier(max_features='log2',n_estimators=10,class_weight='balanced',criterion='entropy',max_depth=500)\n\nrfc.fit(scaled_X_train,y_train)\ny_pred_rf = rfc.predict(scaled_X_test)\n\n\nprint('Accuracy: %.4f' %accuracy_score(y_pred=y_pred_rf,y_true=y_test))\nprint('Confusion Matrix: \\n%s'%confusion_matrix(y_pred=y_pred_rf,y_true=y_test))\nprint('Classification report: \\n %s'%classification_report(y_pred=y_pred_rf,y_true=y_test))\nprint('AUC score: %.5f'%roc_auc_score(y_test,y_pred_rf))\nprint(rfc.feature_importances_)\n'''","472ee851":"#Predicting the test dataset values\n#Scaling the test dataset\nscaled_test = scaler.fit_transform(HistJoined_test)\nfinal_pred_rfc = rfc_over.predict(scaled_test)","1ca04e13":"submission_rf = pd.DataFrame(data=session_id_test)\nsubmission_rf['is_click'] = pd.DataFrame({'is_click':final_pred_rfc})\nsubmission_rf.head()","94999271":"submission_rf.to_csv('submission_rfc_over.csv',index=False)","f9d162bc":"#Predicting the test dataset values\n#Scaling the test dataset\nfinal_pred_lr = LR.predict(scaled_test)","9a4c2512":"submission_lr = pd.DataFrame(data=session_id_test)\nsubmission_lr['is_click'] = pd.DataFrame({'is_click':final_pred_lr})\nsubmission_lr.head()","23892b1a":"submission_lr.to_csv('submission_lr.csv',index=False)","a2c62e00":"#### SMOTE to handle Imbalance","6d30162e":"#### Random Forest","c8bf27de":"#### Logistic Regression","9be5685f":"#### Analysing Data","893a7ce8":"#### Cleaning Data","c9017a78":"### Normalise Feature","b6e760fb":"#### Generating solution","e21d5b9a":"#### Handling Null values","2a9d7b77":"### Divide the data in test and train","6d465bcd":"#### Extracting Date feature","f7f0a0fc":"#### Reducing Dataset size","9d7cdfd7":"#### Dropping non-useful Columns"}}