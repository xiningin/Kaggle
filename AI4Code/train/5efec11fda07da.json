{"cell_type":{"4b98e319":"code","c2d93d4a":"code","c1ca90cc":"code","61daf6ee":"code","bea0a373":"code","751ff596":"code","9732e0aa":"code","dca01b03":"code","21321057":"code","1ec1ef13":"code","fceadf88":"code","54644c7f":"code","6e997852":"code","08a00995":"code","218bb6c4":"code","9951fe2f":"code","907d666d":"code","bf34b95e":"code","d3b87736":"code","cd06456b":"code","0b787445":"markdown","fd6b46ee":"markdown","a8f98c07":"markdown","9c034e77":"markdown","a06277a8":"markdown","0946aa5e":"markdown","096d1487":"markdown","7f7d2fb7":"markdown","4aea8498":"markdown","10c0ca9e":"markdown","35ca3bb7":"markdown","f882c983":"markdown","25d2fb61":"markdown","838cda23":"markdown","df864cb7":"markdown","97092eab":"markdown","45a84591":"markdown","61db69a2":"markdown","e0236a32":"markdown","de8924eb":"markdown","8277623b":"markdown","bb27c6f9":"markdown","018c94f8":"markdown","d626a2f5":"markdown","acac43db":"markdown"},"source":{"4b98e319":"\nimport numpy as np \nimport pandas as pd \nfrom xgboost import XGBRegressor\nfrom sklearn.datasets import load_boston\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","c2d93d4a":"# load dataset\nhouse_price = load_boston()\ndf_labels = pd.DataFrame(house_price.target)\ndf = pd.DataFrame(house_price.data)\nprint(df_labels.head())\nprint(df.head())","c1ca90cc":"df_labels.columns = ['PRICE']\ndf.columns = house_price.feature_names\nprint(df_labels.head())","61daf6ee":"df_total = df.merge(df_labels, left_index = True, right_index = True)\ndf_total.head()","bea0a373":"df = preprocessing.scale(df)\nX_train, X_test, y_train, y_test = train_test_split(\n    df, df_labels, test_size=0.3, random_state=10)","751ff596":"#XGBoost part here!\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, y_train, verbose=False)","9732e0aa":"#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nrmse ","dca01b03":"#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nrmse ","21321057":"#n_estimators usually varies between 100 and 1000 so let's try it:\n\nmy_model = XGBRegressor(n_estimators = 100)\nmy_model.fit(X_train, y_train, verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","1ec1ef13":"#n_estimators 2nd option, 200:\n\nmy_model = XGBRegressor(n_estimators = 200)\nmy_model.fit(X_train, y_train, verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","fceadf88":"#n_estimators 3rd option, 500:\n\nmy_model = XGBRegressor(n_estimators = 500)\nmy_model.fit(X_train, y_train, verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","54644c7f":"#n_estimators 4th option, 1000:\n\nmy_model = XGBRegressor(n_estimators = 1000)\nmy_model.fit(X_train, y_train, verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","6e997852":"# starting with a minimum of 5 early stopping rounds:\nmy_model = XGBRegressor(n_estimators = 1000)\nmy_model.fit(X_train, y_train,early_stopping_rounds=5,eval_set=[(X_test, y_test)], verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","08a00995":"# continuing with 15 early stopping rounds:\nmy_model = XGBRegressor(n_estimators = 1000)\nmy_model.fit(X_train, y_train,early_stopping_rounds=15,eval_set=[(X_test, y_test)], verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","218bb6c4":"# continuing with 15 early stopping rounds:\nmy_model = XGBRegressor(n_estimators = 1000)\nmy_model.fit(X_train, y_train,early_stopping_rounds=50,eval_set=[(X_test, y_test)], verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","9951fe2f":"# continuing with ( too much) 100 early stopping rounds:\nmy_model = XGBRegressor(n_estimators = 1000)\nmy_model.fit(X_train, y_train,early_stopping_rounds=100,eval_set=[(X_test, y_test)], verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","907d666d":"# starting with a small learning rate:\nmy_model = XGBRegressor(n_estimators = 1000,learning_rate=0.05)\nmy_model.fit(X_train, y_train,early_stopping_rounds=100,eval_set=[(X_test, y_test)], verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","bf34b95e":"# increasing the learning rate:\nmy_model = XGBRegressor(n_estimators = 1000,learning_rate=0.1)\nmy_model.fit(X_train, y_train,early_stopping_rounds=100,eval_set=[(X_test, y_test)], verbose=False)\n\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","d3b87736":"my_model = XGBRegressor(n_estimators = 1000,learning_rate=0.15)\nmy_model.fit(X_train, y_train,early_stopping_rounds=100,eval_set=[(X_test, y_test)], verbose=False)\n#early_stopping_rounds can very well be kept at a much lower value, as it doesn't make a big difference\n#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = my_model.predict(X_train)\nmse = mean_squared_error(y_train_predicted, y_train)\nrmse = np.sqrt(mse)\nprint(rmse)\n\n#on test set\ny_test_predicted = my_model.predict(X_test)\nmse = mean_squared_error(y_test_predicted, y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","cd06456b":"df_labels.describe()","0b787445":"2. early_stopping_rounds","fd6b46ee":"Now starts the XGBoost part, with the first test, non-tuned:","a8f98c07":"**Conclusion**","9c034e77":"**In this notebook I'm testing for the first time XGBoost library, by applying it on the Boston Housing dataset, on which I'll predict the house prices considering 14 features of the houses in the database.\nFirstly, I'm testing the model without tuning any parameter, and then I'll take the parameters one by one and try to improve the model's mean square error.**","a06277a8":"So, getting the same MSE tells us that in fact n_estimators is by default 100. \nLet's try with more options:","0946aa5e":"The database is splitted in two:  1. the target variable, the price - df_labels, and 2. the rest of the independent variables, df.","096d1487":"Not a significant difference, so I'll test the highest value:","7f7d2fb7":"Now giving the price column its rightful name:","4aea8498":"And putting them together to have a full database:","10c0ca9e":"MSE even lower, nice, should I try even more?","35ca3bb7":"Now let's have a look at the MSE on the train database:","f882c983":"The MSE improved, on both datasets, even if on the test one the difference is small.","25d2fb61":"The MSE is not better than when we didn't set this parameter, so let's try with another value for it:","838cda23":"I'll stop the tests on this and try to have a look at the next parameter:","df864cb7":"Smallest MSE until now, 3.42; let's increase the learning rate:","97092eab":"Got a 3.431 MSE (for comparison, using a non-tuned Linear Regression I got 5.41 so  XGBoost - even non-tuned, seems better that Linear Regression).\n\nNow, let's start tuning the first parameter:","45a84591":"I get a 0.012 MSE, ok, quite impressively and not necessarily good low value, so let's see how it performs on the test set:","61db69a2":"Now let's standardize the data and make the train\/test split:","e0236a32":"1. n_estimators","de8924eb":"Importing libraries here:","8277623b":"3.4303 MSE, slightly better.","bb27c6f9":"**For a variable that looks as follows, the best option when tuning XGBoost parameters was one that produced a MSE of 3.37 on the test set; this isn't, by no means, the perfect solution. The next step is to automatize this testing part so I could get a better match for the three parameters I worked on.**","018c94f8":"3. learning_rate","d626a2f5":"I'll stop here with the early stopping rounds parameter and move on to the next one:","acac43db":"No. It's already starting to increase again."}}