{"cell_type":{"669cb373":"code","1686b117":"code","4cad4f63":"code","df24b196":"code","bd19dd66":"code","a3916e57":"code","792e46e4":"code","4e44e22e":"code","f144b06b":"markdown","2fa4efa6":"markdown","9265f5e5":"markdown"},"source":{"669cb373":"import numpy as np\nimport pandas as pd\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\ntest_f = pd.read_csv('..\/input\/test_features.csv')\ntrain_f = pd.read_csv('..\/input\/train_features.csv')\ntrain_l = pd.read_csv('..\/input\/train_labels.csv')","1686b117":"# Grabbing the mode of a column after eliminating null values\ntrain_f_s = train_f\ntrain_f_s.head()\ntrain_f_s = train_f_s[train_f_s['construction_year'] !=0]\ntrain_f_s['construction_year'].mode()[0]","4cad4f63":"# Replacing null values with the mode of the column\ntrain_f['construction_year'].replace({0:2010}, inplace=True)\ntest_f['construction_year'].replace({0:2010}, inplace=True)","df24b196":"import category_encoders as ce\nimport numpy as np\n\n\n# Combining train and test sets\ntrain_objs_num = len(train_f)\ndataset = pd.concat(objs=[train_f, test_f], axis=0)\n\n# Binning longitude and latitiude\nstep = 20\nto_bin = lambda x: np.floor(x \/ step) * step\ndataset[\"latbin\"] = dataset.latitude.map(to_bin)\ndataset[\"lonbin\"] = dataset.longitude.map(to_bin)\n\n# Dropping the highest cardinality features\ndataset.drop(['longitude', 'latitude', 'wpt_name'],axis=1, inplace=True)\n\n# Ordinal encoding the combined dataset\nordinal = ce.OrdinalEncoder()\nordinal.fit(dataset)\ndataset = ordinal.transform(dataset)\n\n# The train and test sets are separated again\ntrain_f = dataset[:train_objs_num]\ntest_f = dataset[train_objs_num:]\n","bd19dd66":"from sklearn.model_selection import train_test_split\n# Making the train and test datasets\n\nX = train_f\ny = train_l['status_group']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","a3916e57":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n# Model being used for predictions\n\npipeline = make_pipeline(\n    StandardScaler(),\n    RandomForestClassifier(n_estimators=3000,criterion = 'entropy')\n)\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)","792e46e4":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test,y_pred)","4e44e22e":"y_pred = pipeline.predict(test_f)","f144b06b":"## The model below takes a while to run due to the 'n_estimators' paramater being so high. It is a large number because it improves the performance of the model.","2fa4efa6":"## The last step is to fit the model to the test data:","9265f5e5":"## This kernel is dedicated to a solution including data engineering, ordinal encoding, standardization, and random forest classifier use"}}