{"cell_type":{"629e3d1e":"code","5f913d87":"code","d74b72c7":"code","0a11419a":"code","148c0760":"code","f10cae9a":"code","911a4cfc":"code","82743ae9":"code","844eea4d":"code","59e2c944":"code","3bd5f147":"code","1d0b5beb":"code","8adebbb4":"code","21f9cf96":"code","e473dac1":"code","d0f6a88f":"code","e8e2894c":"code","a1d9573b":"code","4d9cba62":"code","72113d67":"code","b1912df4":"code","bb2ad727":"code","015fa243":"code","37092461":"code","0d974c36":"code","e8ac4ad3":"code","84b9ee8c":"code","f7c4366c":"code","32f2dc76":"code","85cd37e9":"markdown","f2cc97b7":"markdown","ca1a9c1b":"markdown","1f57b37b":"markdown","6126db45":"markdown","d4b6d4f8":"markdown","0f2f7e20":"markdown","b3417e99":"markdown","d64c15dc":"markdown","b0432e92":"markdown","4ad3afe4":"markdown","858428a9":"markdown","7777f42c":"markdown","0f5592e9":"markdown","39cd8c61":"markdown","d8d501b2":"markdown","93c54a1a":"markdown","2d83bf4a":"markdown","74977185":"markdown","2c23917f":"markdown","1a84b8d9":"markdown","c93ec48b":"markdown","5e113cd7":"markdown","fbadfc3d":"markdown","36049e62":"markdown","7631dc33":"markdown","9db4e040":"markdown","894b95df":"markdown","b28e4682":"markdown","8b1c6f9d":"markdown","44378c3c":"markdown"},"source":{"629e3d1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f913d87":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Handle table-like data and matrices\nimport numpy as np\nimport pandas as pd\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d74b72c7":"df = pd.read_csv(\"\/kaggle\/input\/divorce-dataset\/divorce.csv\", sep=';')\ndf.head()","0a11419a":"# function to display information on data\ndef description(df):\n    print(f'Dataset Shape:{df.shape}')\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values   \n    summary['Uniques'] = df.nunique().values\n    return summary\nprint('\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445:')\ndescription(df)","148c0760":"df.head()","f10cae9a":"sns.countplot('Class', data=df)\nplt.ylabel('\u0421lass distribution')\nplt.show()","911a4cfc":"from sklearn.decomposition import PCA","82743ae9":"features = df.drop(['Class'], axis = 1)","844eea4d":"# Create a PCA that retains 99% variance\npca = PCA(n_components=0.99, whiten=True)\nfeatures_pca = pca.fit_transform(features)","59e2c944":"# Show results\nprint(\"The initial number of signs:\", df.shape[1])\nprint(\"Reduced number of symptoms:\", features_pca.shape[1])","3bd5f147":"# convert to data frame\nfeatures_pca = pd.DataFrame(features_pca)\nfeatures_pca.head()","1d0b5beb":"from sklearn.decomposition import NMF","8adebbb4":"# Create NMF and fit it\nnmf = NMF(n_components=10, random_state=1)\nfeatures_nmf = nmf.fit_transform(features)","21f9cf96":"# Show results\nprint(\"The initial number of signs:\", features.shape[1])\nprint(\"Reduced number of symptoms:\", features_nmf.shape[1])","e473dac1":"# convert to data frame\nfeatures_nmf = pd.DataFrame(features_nmf)\nfeatures_nmf.head()","d0f6a88f":"from sklearn.decomposition import TruncatedSVD\nfrom scipy.sparse import csr_matrix","e8e2894c":"# Make a sparse matrix\nfeatures_sparse = csr_matrix(features)","a1d9573b":"# Create TSVD object\ntsvd = TruncatedSVD(n_components=10)","4d9cba62":"# Run TSVD on a sparse matrix\nfeatures_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)","72113d67":"# Show results\nprint(\"The initial number of signs:\", features.shape[1])\nprint(\"Reduced number of symptoms:\", features_sparse_tsvd.shape[1])","b1912df4":"features_sparse_tsvd = pd.DataFrame(features_sparse_tsvd)\nfeatures_sparse_tsvd.head()","bb2ad727":"# Create a correlation matrix\ncorr = features.corr().abs()","015fa243":"# Select the upper triangle of the correlation matrix\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\nupper.head(10)","37092461":"# Find the index of feature columns with a correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nto_drop","0d974c36":"hug_cor_data = features.drop(['Atr19', 'Atr20', 'Atr36'], axis=1)","e8ac4ad3":"# Show results\nprint(\"The initial number of signs:\", features.shape[1])\nprint(\"Reduced number of symptoms:\", hug_cor_data.shape[1])","84b9ee8c":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif","f7c4366c":"# Select the two traits with the highest chi-square statistic\nchi2_selector = SelectKBest(chi2, k=52)\nfeature_kbest = chi2_selector.fit_transform(features, df.Class)","32f2dc76":"# Show results\nprint(\"The initial number of signs:\", features.shape[1])\nprint(\"Reduced number of symptoms:\", feature_kbest.shape[1])","85cd37e9":"# 5. Dimension reduction through feature selection","f2cc97b7":"### Discussion","ca1a9c1b":"## 2.2 Load data","1f57b37b":"## Decreasing the number of features using matrix decomposition","6126db45":"### Discussion","d4b6d4f8":"# 1. Business Understanding\n## 1.1. Objective\nReduce data dimension","0f2f7e20":"###### 19. My wife and I have similar ideas about how roles should be in marriage  \n###### 20. My wife and I have similar values in trust  \n###### 36. I can be humiliating when we argue.","b3417e99":"### Discussion\nTSVD is similar to PCA, and indeed PCA often uses SVD at one of its steps. In the usual singular decomposition, if there are d attributes, dxd matrices are created, while truncated singular decompositions will return factors that are of size \nn\u00d7n, where n is predefined by the parameter. The practical advantage of TSVD is that, unlike PCA, this method works on sparse feature matrices.  \nOne of the problems with the TSVD method is that depending on how this method uses the random number generator, the signs of the result may change from fit to fit. A simple solution is to use the fit method in the preprocessing pipeline only once, and then use the transform method several times","d64c15dc":"## Decrease in signs using \u0420\u0421\u0410","b0432e92":"PCA (principal component analysis) is a popular method of reducing linear dimension. The PCA method projects the observations on the main components of the matrix of attributes (hopefully, a smaller number of them), which retain the greatest variance. SAR is an uncontrolled method (without a teacher), that is, it does not use information from the target vector and instead considers only a matrix of features.\nFor a mathematical description of how PCA works, refer to external resources. Principal component analysis is implemented in the scikit-leam library using the PCA class. The n_components argument has two operations, depending on the given value. If the value of this argument is greater than 1, then n_components will return the specified number of attributes. This leads to the question of choosing the optimal number of features. Fortunately for us, if the value of the n_components argument is between 0 and 1, then the object created on the basis of the PCA class returns the minimum number of features that preserve the specified variance. Typically, the values 0.95 and 0.99 are used, i.e., 95% and 99% of the variance of the original features will be preserved. The whiten = True argument converts the values of each principal component so that they have zero mean and unit variance. Another argument is svd_solver = \"randomized\", which implements a stochastic algorithm for finding the first principal components, which, as a rule, takes much less time.The result of our solution shows that the X-ray diffraction method allows one to reduce the dimension by 10 features, while retaining 99% of the information (variance) in the feature matrix.","4ad3afe4":"# 6. Conclusion ","858428a9":"## Removal of irrelevant characteristics for classification","7777f42c":"## 1.2. Description\nDivorce, also known as dissolution of marriage, is the process of terminating a marriage or marital union. Divorce usually entails the canceling or reorganizing of the legal duties and responsibilities of marriage, thus dissolving the bonds of matrimony between a married couple under the rule of law of the particular country or state.\n\nData taken from the UCI Machine Learning repository. A complete list of questions can be found below.\n\nAttribute Information:\n1. When one of our apologies apologizes when our discussions go in a bad direction, the issue does not extend. \n2. I know we can ignore our differences, even if things get hard sometimes. \n3. When we need it, we can take our discussions with my wife from the beginning and correct it. \n4. When I argue with my wife, it will eventually work for me to contact him. \n5. The time I spent with my wife is special for us. \n6. We don't have time at home as partners. \n7. We are like two strangers who share the same environment at home rather than family. \n8. I enjoy our holidays with my wife. \n9. I enjoy traveling with my wife. \n10. My wife and most of our goals are common. \n11. I think that one day in the future, when I look back, I see that my wife and I are in harmony with each other. \n12. My wife and I have similar values in terms of personal freedom. \n13. My husband and I have similar entertainment. \n14. Most of our goals for people (children, friends, etc.) are the same. \n15. Our dreams of living with my wife are similar and harmonious \n16. We're compatible with my wife about what love should be \n17. We share the same views with my wife about being happy in your life \n18. My wife and I have similar ideas about how marriage should be \n19. My wife and I have similar ideas about how roles should be in marriage \n20. My wife and I have similar values in trust \n21. I know exactly what my wife likes. \n22. I know how my wife wants to be taken care of when she's sick. \n23. I know my wife's favorite food. \n24. I can tell you what kind of stress my wife is facing in her life. \n25. I have knowledge of my wife's inner world. \n26. I know my wife's basic concerns. \n27. I know what my wife's current sources of stress are. \n28. I know my wife's hopes and wishes. \n29. I know my wife very well. \n30. I know my wife's friends and their social relationships. \n31. I feel aggressive when I argue with my wife. \n32. When discussing with my wife, I usually use expressions such as \u00e2\u20ac\u0153you always\u00e2\u20ac\u0153 or \u00e2\u20ac\u0153you never\u00e2\u20ac\u009d. \n33. I can use negative statements about my wife's personality during our discussions. \n34. I can use offensive expressions during our discussions. \n35. I can insult our discussions. \n36. I can be humiliating when we argue. \n37. My argument with my wife is not calm. \n38. I hate my wife's way of bringing it up. \n39. Fights often occur suddenly. \n40. We're just starting a fight before I know what's going on. \n41. When I talk to my wife about something, my calm suddenly breaks. \n42. When I argue with my wife, it only snaps in and I don't say a word. \n43. I'm mostly thirsty to calm the environment a little bit. \n44. Sometimes I think it's good for me to leave home for a while. \n45. I'd rather stay silent than argue with my wife. ","0f5592e9":"## Processing highly correlated traits","39cd8c61":"# 2. Data Understanding\n## 2.1 Import Libraries","d8d501b2":"## \u0421lass distribution","93c54a1a":"### Additional reading material","2d83bf4a":"# 3. Statistical summaries and visualisations","74977185":"## Reducing the number of features on sparse data\nWe use TSVD(truncated singular value decomposition)","2c23917f":"# 4. Dimension reduction with feature extraction","1a84b8d9":"### Discussion\nOne of the problems that we often encounter in the machine is the highly correlated symptoms. If two attributes are strongly correlated, then the information they contain is very similar, including both attributes will be redundant. The solution to the problem of strongly correlated signs is simple: remove one of them from the set of signs.","c93ec48b":"A set of features is given, and it is required to reduce the number of features, while maintaining the variance of the data.","5e113cd7":"People who underwent testing answered in the range from 0 to 4. Where 0 - I do not agree, 4 - I completely agree. The Class field has the value 0 - not divorced, 1 - divorced.","fbadfc3d":"There is a matrix of attributes with non-negative values, and it is required to reduce its dimension.  \nNon-negative matrix factorization (NMF) can be used to reduce the dimension of the feature matrix.","36049e62":"Use a correlation matrix to test for highly correlated features. If strongly correlated features exist, consider eliminating one of the correlated features.","7631dc33":"1. Documentation of the scikit-leam library on the analysis of the main components and the [PCA class](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) \n2. \"The choice of the number of main components\", a video tutorial on [Coursera](https:\/\/www.coursera.org\/lecture\/machine-learning\/choosing-the-number-of-principal-components-S1bq1) ","9db4e040":"The chi-square statistic tests the independence of two categorical vectors. That is, this statistical indicator is the difference between the observed number of observations in each class of the categorical attribute and what we would expect if this object were independent of the target vector.","894b95df":"I took a data set, and produced various methods to reduce data.  \nI hope someone will find it useful!","b28e4682":"# Content\n1. Business Understanding  \n    Objective  \n    Description  \n2. Data Understanding  \n    Import Libraries  \n    Load data  \n3. Statistical summaries and visualisations  \n    \u0421lass distribution  \n4. Dimension reduction with feature extraction  \n    Decrease in signs using \u0420\u0421\u0410  \n    Decreasing the number of features using matrix decomposition  \n    Reducing the number of features on sparse data  \n5. Dimension reduction through feature selection  \n    Processing highly correlated traits  \n    Removal of irrelevant characteristics for classification  \n6. Conclusion  ","8b1c6f9d":"As we can see, we have 55 columns, all have int values and there are no missing values.","44378c3c":"NMF is an uncontrolled (without a teacher) method of reducing linear dimension that factorizes (splits into several matrices, the product of which corresponds to the original matrix) a matrix of attributes into matrices representing a hidden connection between observations and their attributes. Intuitively, the NMF method can reduce the dimension, since in matrix multiplication two factors (multiplied matrices) can have significantly smaller dimensions than the product matrix. Formally, if the desired number of returned characteristics r is given, then the NMF method decomposes the matrix of attributes so that:\n\nV \u00ab WH\n\nwhere V is our matrix of signs d x n (d signs, n observations);  \nW is the matrix d x r;  \nH is the matrix r x n.  \nBy adjusting the r value, you can set the volume of the desired dimensionality reduction.  \n\nOne of the main requirements of the NMA method is that, as the name implies, the feature matrix cannot contain negative values. In addition, unlike the PCA method, the NMA method does not provide us with an explained variance of the resulting features. Thus, the best way for us to find the optimal value of the n_components components is to try to find in the range of values that which gives the best result in our final model."}}