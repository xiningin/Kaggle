{"cell_type":{"9f43f625":"code","95d21924":"code","d0515ac0":"code","61b0bde5":"code","29dcb743":"code","fa437d46":"code","e6b85690":"code","8fb1627b":"code","2dfe7e9d":"code","f8c6f7fe":"code","70875e28":"code","b3b9d9c1":"code","f0ca871f":"code","f4b3d11f":"code","c74d0fb3":"code","212b03ae":"code","274ceb95":"code","421bb91e":"code","72f4725b":"code","32588bda":"code","63f9de5c":"code","aa150800":"code","da966a92":"code","80d0926e":"code","88fe6398":"code","6643c2a4":"code","ca666d1d":"code","c8933b9b":"code","20e6aedf":"markdown","d79b5c6c":"markdown","fefa4314":"markdown","7e79c64f":"markdown","7c1a0428":"markdown","dbdc0a85":"markdown","911e5abd":"markdown","629e54cc":"markdown","188d8b70":"markdown","f7ab2d6a":"markdown","2efdaf18":"markdown","dfcdc2aa":"markdown","f1cf4467":"markdown","2dca1f86":"markdown","9f6e6351":"markdown","bb7ee39c":"markdown","00168505":"markdown","1a4ec8cd":"markdown","16d2d745":"markdown","58be16c4":"markdown","529031c9":"markdown","0922f1c4":"markdown","3edc4c8c":"markdown","2f861b3e":"markdown","bf6b7b0f":"markdown","0c0b7deb":"markdown"},"source":{"9f43f625":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import Binarizer\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport numpy as np","95d21924":"# Import the Titanic Dataset\nX = pd.read_csv('..\/input\/train.csv')\ny = X.pop(\"Survived\")","d0515ac0":"X.head()","61b0bde5":"def clean_cabin(x):\n    try:\n        return x[0]\n    except TypeError:\n        return \"None\"\n    \n# Clean Cabin\nX[\"Cabin\"] = X.Cabin.apply(clean_cabin)\n\n# Define categorical features\ncategorical_variables = [\"Sex\", \"Cabin\", \"Embarked\"]\n\n# Impute missing age with median\nX[\"Age\"].fillna(X[\"Age\"].median(), inplace=True)\n\n# Drop PassengerId, Name, Ticket\nX.drop(['PassengerId','Name','Ticket'], axis=1, inplace=True)\n\n# Impute missing categorical variables and dummify them\nfor variable in categorical_variables:\n    X[variable].fillna(\"Missing\", inplace=True)\n    dummies = pd.get_dummies(X[variable], prefix=variable)\n    X = pd.concat([X, dummies], axis=1)\n    X.drop([variable], axis=1, inplace=True)","29dcb743":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","fa437d46":"# Get a list of numerica features\nnumeric_variables = ['Pclass','Age','SibSp','Parch','Fare']\nX_numeric = X_train[numeric_variables]\nX_numeric.head()","e6b85690":"X_numeric[\"Age\"].fillna(X_numeric[\"Age\"].mean(), inplace=True)","8fb1627b":"# Create the baseline \nmodel_1 = RandomForestClassifier(oob_score=True, random_state=42)","2dfe7e9d":"# Fit and Evaluate OOB\nmodel_1 = model_1.fit(X_numeric, y_train)\n# Calculate OOB Score\nprint(\"The OOB Score is: \" + str(model_1.oob_score_))","f8c6f7fe":"rf_result = cross_val_score(model_1, X_numeric, y_train, scoring='accuracy')\nrf_result.mean()","70875e28":"pred_train = np.argmax(model_1.oob_decision_function_,axis=1)\nrf_numeric_auc = roc_auc_score(y_train, pred_train)\nrf_numeric_auc","b3b9d9c1":"# Copy the whole train set\nX_cat = X_train\nX_cat.head(3)","f0ca871f":"model_2 = RandomForestClassifier(oob_score=True, random_state=40)","f4b3d11f":"# Fit and Evaluate OOB\nmodel_2 = model_2.fit(X_cat, y_train)\n# Calculate OOB Score\nprint(\"The OOB Score is: \" + str(model_2.oob_score_))","c74d0fb3":"X_cat_scaled = StandardScaler().fit(X_cat).transform(X_cat)\nX_cat_scaled","212b03ae":"# Create the baseline \nmodel_3= RandomForestClassifier(oob_score=True, random_state=40)\n# Fit and Evaluate OOB\nmodel_3 = model_3.fit(X_cat_scaled, y_train)\n# Calculate OOB Score\nmodel_3.oob_score_","274ceb95":"# AUC Score\npred_train = np.argmax(model_2.oob_decision_function_,axis=1)\nrf_cat_auc = roc_auc_score(y_train, pred_train)\nrf_cat_auc","421bb91e":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nrf_numeric_fpr, rf_numeric_tpr, rf_numeric_thresholds = roc_curve(y_test, model_1.predict_proba(X_test[X_numeric.columns])[:,1])\nrf_cat_fpr, rf_cat_tpr, rf_cat_thresholds = roc_curve(y_test, model_2.predict_proba(X_test)[:,1])\n\n# Plot Random Forest Numeric ROC\nplt.plot(rf_numeric_fpr, rf_numeric_tpr, label='RF Numeric (area = %0.2f)' % rf_numeric_auc)\n\n# Plot Random Forest Cat+Numeric ROC\nplt.plot(rf_cat_fpr, rf_cat_tpr, label='RF Cat+Num (area = %0.2f)' % rf_cat_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1], ls=\"--\", label='Base Rate')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","72f4725b":"results  =  []\nresults2 = []\nmax_depth_size  = [1,2,3,4,5,10,20,50,100]\n\nfor depth in max_depth_size:\n    model = RandomForestClassifier(depth, oob_score=True, n_jobs=-1, random_state=44)\n    #model.fit(X, y)\n    model.fit(X_train, y_train)\n    print(depth, 'depth')\n    pred = model.predict(X_train)\n    pred2 = model.predict(X_test)\n    roc1 = roc_auc_score(y_train, pred)\n    roc2 = roc_auc_score(y_test, pred2)\n    print('AUC Train: ', roc1)\n    print('AUC Test: ', roc2)\n    results.append(roc1)\n    results2.append(roc2)\n    print (\" \")\n\n","32588bda":"plt.plot(max_depth_size, results, label='Train Set')\nplt.plot(max_depth_size, results2, label='Test Set')\nplt.xlabel('Max Depth Size')\nplt.ylabel('AUC Score')\nplt.title('Train VS Test Scores')\nplt.legend(loc=\"lower right\")\nplt.show()","63f9de5c":"results = []\nn_estimator_options = [1, 2, 3, 4, 5, 15, 20, 25, 40, 50, 70, 100]\n\nfor trees in n_estimator_options:\n    model = RandomForestClassifier(trees, oob_score=True, random_state=42)\n    #model.fit(X, y)\n    model.fit(X_train, y_train)\n    print(trees, 'trees')\n    AUC = model.oob_score_\n    print('AUC: ', AUC)\n    results.append(AUC)\n    print (\" \")    ","aa150800":"plt.plot(n_estimator_options, results, label='OOB Score')\nplt.xlabel('# of Trees')\nplt.ylabel('OOB Score')\nplt.title('OOB Score VS Trees')\nplt.legend(loc=\"lower right\")\nplt.show()","da966a92":"results = []\nmax_features_options = [\"auto\", None, \"sqrt\", \"log2\", 0.7, 0.2]\n\nfor max_features in max_features_options:\n    model = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1, random_state=42, max_features=max_features)\n    model.fit(X_train, y_train)\n    print(max_features, \"option\")\n    auc = model.oob_score_\n    print('AUC: ', auc)\n    results.append(auc)\n    print (\" \")\n    \npd.Series(results, max_features_options).plot(kind='barh')","80d0926e":"results = []\nmin_samples_leaf_options = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,20]\n\nfor min_samples in min_samples_leaf_options:\n    model = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1, random_state=42, max_features=\"auto\", min_samples_leaf=min_samples)\n    model.fit(X_train, y_train)\n    print(min_samples, \"min samples\")\n    auc = model.oob_score_\n    print('AUC: ', auc)\n    results.append(auc)\n    print (\" \")\n    \npd.Series(results, min_samples_leaf_options).plot()","88fe6398":"feature_importances = pd.Series(model_2.feature_importances_, index=X.columns)\nprint(feature_importances)\nfeature_importances.sort_values(inplace=True)\nfeature_importances.plot(kind='barh', figsize=(7,6))","6643c2a4":"# Create function to combine feature importances\ndef graph_feature_importances(model, feature_names, autoscale=True, headroom=0.1, width=10, summarized_columns=None):  \n    feature_dict=dict(zip(feature_names, model.feature_importances_))\n    \n    if summarized_columns:\n        for col_name in summarized_columns:\n            sum_value = sum(x for i, x in feature_dict.items() if col_name in i )\n            keys_to_remove = [i for i in feature_dict.keys() if col_name in i ]\n            for i in keys_to_remove:\n                feature_dict.pop(i)\n            feature_dict[col_name] = sum_value\n    results = pd.Series(feature_dict, index=feature_dict.keys())\n    results.sort_values(inplace=True)\n    print(results)\n    results.plot(kind='barh', figsize=(width, len(results)\/4), xlim=(0, .30))\n \n# Create combined feature importances\ngraph_feature_importances(model_2, X.columns, summarized_columns=categorical_variables)","ca666d1d":"from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.preprocessing import Imputer\nclf = GradientBoostingClassifier()\ntitanic_X_colns = ['Pclass','Age', 'Fare']\ntitanic_X = X_train[titanic_X_colns]\nmy_imputer = Imputer()\nimputed_titanic_X = my_imputer.fit_transform(titanic_X)\n\nclf.fit(imputed_titanic_X, y_train)\ntitanic_plots = plot_partial_dependence(clf, features=['Pclass','Age', 'Fare'], X=titanic_X, \n                                        feature_names=titanic_X_colns, grid_resolution=7)","c8933b9b":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=model_2, dataset=X_train, model_features=X_train.columns, feature='Fare')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Fare')\nplt.show()","20e6aedf":"# Max Depth\n\nThe more depth (deeper the tree) means the higher chance of overfitting","d79b5c6c":"# RF Model Evaluation","fefa4314":"**AUC Score**","7e79c64f":"# Data Cleaning and Data Imputation","7c1a0428":"# Interpreting Random Forest","dbdc0a85":"# Conclusion\n\n* It needs minimal data cleaning\n* Works with both regression and classification\n* It gives feature importance\n* Great for exploratory modeling\n* Good baseline model\n* Built in cross validation\n* Little hyper parameter tuning\n* Treats different scaling of features similarly\n* Natively detects non linear interactions","911e5abd":"**Cross Validation**","629e54cc":"**Out-of-Bag Score **","188d8b70":"![image.png](attachment:image.png)\n","f7ab2d6a":"# Feature Importance","2efdaf18":"# n_estimators\n\nGenerally the more trees the better. You'll generalize better with more trees and reduce the variance more. The only downside is computation time.","dfcdc2aa":"# Random Forest Model1","f1cf4467":"**AUC Score**","2dca1f86":"# What is Random Forest?\nRandom Forest is just a collection of random decision trees. It achieves a lower test error solely by variance reduction. Therefore increasing the number of trees in the ensemble won't have any effect on the bias of your model. Higher number of trees will only reduce the variance of your model. Moreover you can achieve a higher variance reduction by reducing the correlation between trees in the ensemble. This is the reason why we randomly select 'm' attributes at each split because it will introduce some randomness in to the ensemble and reduce the correlation between trees. Hence 'm' is the major attribute to be tuned in a random forest ensemble.\n\n* Random = Random subsets of features and observations used to build the trees\n* Forest = Number of decision trees used to ensemble\n\n**How it works**:\n* Select a random sample of features and observations(with replacement) from the entire dataset.\n* For each feature (node) you'll test different thresholds and see which gives you the best split criterion (generally entropy, gini, information gain).\n* Keep the feature and its threshold that makes the best split and repeat for each other feature\n* Stop after a you completely reach a single leaf node or a stopping criterion (max_depth size or min_samples_leaf)\n\nIndividual trees have low bias but high variance. By ensembling a lot of trees together you're going to reduce the variance, while not increasing the bias. You want each tree to be as different as possible and learn\/capture different patterns from the data.","9f6e6351":"# Random Forest Model2","bb7ee39c":"# Important Parameters\n\nParameters that will make your model better\n\n* **max_depth**: The depth size of a tree\n* **n_estimators**: The number of trees in the forest. Generally, the more trees the better accuracy, but slower computation.\n* **max_features**: The max number of features that the algorithm can assign to an individual tree. Try ['auto', 'None', 'sqrt', 'log2', 0.9 and 0.2]\n* **min_samples_leaf**: The minimum number of samples in newly created leaves. Try [1,2,3]. If 3 is best, try higher numbers.\n\nParameters that will make your model faster\n\n* **n_jobs**: Determines the amount of multiple processors should be used to train\/test the model. Always use -1 to use max cores and it'll run much faster\n* **random_state**: Set this to a number (42) for reproducibility. It's used to replicate your results and for others as well.\n* **oob_score**: Random Forest's custom validation method: out-of-bag prediction","00168505":"# Feature Scaling","1a4ec8cd":"# Train on Categorical and Numerical Features","16d2d745":"**Out-of-Bag Score**","58be16c4":"**How is target variable related with important features?**","529031c9":"# Max Features","0922f1c4":"**Fit Standardized Training Set**","3edc4c8c":"**Advantages**:\n\nBuilt in cross validation (OOB Scores)\nBuilt in Feature Selection (implicit)\nFeature importance\nDefault hyper parameters are great and Works well \"off the shelf\"\nMinimum hyper parameter tuning\nRF natively detects interactions\nIt's parametric (you don't have to make any assumptions of your data)\n\n**Disadvantages**:\n\nRF is a black box (It's literally a function of 1000 decision trees)\nIt doesn't tell you \"how\" the features are importan","2f861b3e":"**Combine the categorical features into one feature importance**","bf6b7b0f":"# Min Sample Leafs","0c0b7deb":"**Gini**\n\n* We use the Gini Index as our cost function used to evaluate splits in the dataset.\n* A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split.\n* A perfect separation results in a Gini score of 0 (ex. [0,25])\n* Whereas the worst case split that results in 50\/50 classes."}}