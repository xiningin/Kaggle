{"cell_type":{"80e79a32":"code","83ad41c4":"code","fe5f463a":"code","d2fe6456":"code","f67ed2a2":"code","61972e1a":"code","f22b6276":"code","af76699a":"code","1225de3a":"code","44a9457a":"code","ce54b906":"code","6c0bf2c1":"code","af195378":"code","48a394f6":"code","199988ad":"code","0a16b88c":"markdown","f92f1d76":"markdown","2083289b":"markdown","5aa7fee4":"markdown","d6d60fe8":"markdown","fa20670b":"markdown","f9806322":"markdown","11ed4fec":"markdown","e2763aaa":"markdown"},"source":{"80e79a32":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\npt1 = pd.read_csv('..\/input\/all-the-news\/articles1.csv', index_col=0)\npt2 = pd.read_csv('..\/input\/all-the-news\/articles2.csv', index_col=0)\npt3 = pd.read_csv('..\/input\/all-the-news\/articles3.csv', index_col=0)\n\narticles = pd.concat([pt1,pt2,pt3])\nprint(f\"Dataset contains {len(articles)} articles written between {int(articles['year'].min())} and {int(articles['year'].max())}\")\nprint(f\"\\nList of publications includes: {articles.publication.unique().tolist()}\")","83ad41c4":"articles.sample(3)","fe5f463a":"# ax = articles['publication'].value_counts().sort_index().plot(kind='bar', fontsize=14, figsize=(12,10))\n# ax.set_title('Article Count\\n', fontsize=20)\n# ax.set_xlabel('Publication', fontsize=18)\n# ax.set_ylabel('Count', fontsize=18)\n# plt.tight_layout()\n# sns.despine();\n\nax = articles['publication'].value_counts().plot(kind='bar', fontsize=14, figsize=(12,8), color=\"gray\")\nax.set_title('Article Count \\n', fontsize=20)\nax.set_xlabel('Publication', fontsize=18)\nax.set_ylabel('Count', fontsize=18)\nplt.tight_layout()\nsns.despine();","d2fe6456":"def clean_text(article):\n    # removes punctuation and converts all the text to lower case\n    clean1 = re.sub(r'['+string.punctuation + '\u2019\u2014\u201d'+']', \"\", article.lower())\n    return re.sub(r'\\W+', ' ', clean1)\n\narticles['tokenized'] = articles['content'].map(lambda x: clean_text(x))\narticles['num_wds'] = articles['tokenized'].apply(lambda x: len(x.split()))\n\nprint(f\"Average number of words in an article: {int(articles['num_wds'].mean())}\")\nprint(f\"Longest article: {int(articles['num_wds'].max())}, Shortest: {int(articles['num_wds'].min())}\")\n\nprint(f\"\\nRemoving {len(articles[articles['num_wds']==0])} articles with {int(articles['num_wds'].min())} words.\")\narticles = articles[articles['num_wds']>0]","f67ed2a2":"ax=articles['num_wds'].plot(kind='hist', bins=50, fontsize=14, figsize=(12,6), color=\"gray\")\nax.set_title('Article Length in Words\\n', fontsize=20)\nax.set_ylabel('Frequency', fontsize=18)\nax.set_xlabel('Number of Words', fontsize=18)\nax.set_yscale(\"log\")\nplt.tight_layout()\nsns.despine();","61972e1a":"# number of unique words in each article\narticles['uniq_wds'] = articles['tokenized'].str.split().apply(lambda x: len(set(x)))\n\nprint(f\"Average number of unique words in an article: {int(articles['uniq_wds'].mean())}\")\nprint(f\"Maximum unique word: {int(articles['uniq_wds'].max())}, Minimum: {int(articles['uniq_wds'].min())}\")","f22b6276":"ax=articles['uniq_wds'].plot(kind='hist', bins=50, fontsize=14, figsize=(12,6), color=\"gray\")\nax.set_title('Unique Words Per Article\\n', fontsize=20)\nax.set_ylabel('Frequency', fontsize=18)\nax.set_xlabel('Number of Unique Words', fontsize=18)\nax.set_yscale(\"log\")\nplt.tight_layout()\nsns.despine();","af76699a":"# Data\nart_grps = articles.groupby('publication')\n\npublications = art_grps['num_wds'].aggregate(np.mean).index.tolist()\navg_words = art_grps['num_wds'].aggregate(np.mean).values\nunique_words = art_grps['uniq_wds'].aggregate(np.mean).values\n\n# Sort by number of avg_words\nidx = avg_words.argsort()\npublications, avg_words, unique_words = [np.take(x, idx) for x in [publications, avg_words, unique_words]]\n\ny = np.arange(unique_words.size)\n\nfig, axes = plt.subplots(ncols=2, sharey=True, figsize=(12,6))\naxes[0].barh(y, unique_words, align='center', color='gray', zorder=10)\naxes[0].set_title('Mean Number of Unique Words', fontsize=18)\naxes[1].barh(y, avg_words, align='center', color='gray', zorder=10)\naxes[1].set_title('Mean Number of Words', fontsize=18)\n\naxes[0].invert_xaxis()\naxes[0].set_yticks(y)\naxes[0].set_yticklabels(publications, fontsize=14)\naxes[0].yaxis.tick_left()\n\naxes[1].yaxis.tick_right()\n\nfor ax in axes.flat:\n    ax.margins(0.03)\n    ax.grid(False)\n\nfig.tight_layout()\nfig.subplots_adjust(wspace=0)\nplt.tight_layout();","1225de3a":"wd_counts = Counter()\nfor i, row in articles.iterrows():\n    wd_counts.update(row['tokenized'].split())\n    \nfor sw in stopwords.words('english'):\n    del wd_counts[sw]\n    \nwd_counts.most_common(20)","44a9457a":"def find_cc_wds(content, cc_wds=['climate change','global warming', 'extreme weather', 'greenhouse gas'\n                                 'clean energy', 'clean tech', 'renewable energy']\n):\n    found = False\n    for w in cc_wds:\n        if w in content:\n            found = True\n            break\n\n    if not found:\n        disj = re.compile(r'(chang\\w+\\W+(?:\\w+\\W+){1,5}?climate) | (climate\\W+(?:\\w+\\W+){1,5}?chang)')\n        if disj.match(content):\n            found = True\n    return found\n\narticles['cc_wds'] = articles['tokenized'].apply(find_cc_wds)\n\nprint(f\"The proportion of climate coverage over all articles is {np.round(100*articles['cc_wds'].sum() \/ len(articles), 1)}%\")","ce54b906":"proportions = art_grps['cc_wds'].sum() \/ art_grps['cc_wds'].count()\nproportions.sort_values(ascending=True)\n\nax=proportions.sort_values(ascending=False).plot(kind='bar', fontsize=14, figsize=(12,8), color=\"gray\")\nax.set_title('Mean Proportion of Climate Change Related Articles per Publication\\n', fontsize=20)\nax.set_ylabel('Mean Proportion', fontsize=18)\nax.set_xlabel('Publication', fontsize=18)\nplt.tight_layout()\nsns.despine();","6c0bf2c1":"# The assignment of publications to bias slant is somewhat subjective\n#liberal, conservative, and center\nbias_assigns = {'Atlantic': 'left', 'Breitbart': 'right', 'Business Insider': 'left', 'Buzzfeed News': 'left', 'CNN': 'left', 'Fox News': 'right',\n                'Guardian': 'left', 'National Review': 'right', 'New York Post': 'right', 'New York Times': 'left',\n                'NPR': 'left', 'Reuters': 'center', 'Talking Points Memo': 'left', 'Washington Post': 'left', 'Vox': 'left'}\narticles['bias'] = articles['publication'].apply(lambda x: bias_assigns[x])\n\nbias_groups = articles.groupby('bias')\nbias_proportions = bias_groups['cc_wds'].sum() \/ bias_groups['cc_wds'].count()\n\nax=bias_proportions.plot(kind='bar', fontsize=14, figsize=(12,6), color=\"gray\")\nax.set_title('Proportion of climate change articles by Political Bias\\n', fontsize=20)\nax.set_xlabel('Bias', fontsize=18)\nax.set_ylabel('Proportion', fontsize=18)\nplt.tight_layout()\nsns.despine();","af195378":"def standard_err(p1, n1, p2, n2):\n    return np.sqrt((p1* (1-p1) \/ n1) + (p2 * (1-p2) \/ n2))\n\ndef ci_range(diff, std_err, cv=1.96):\n    return (diff - cv * std_err, diff + cv * std_err)\n\ndef calc_ci_range(p1, n1, p2, n2):\n    std_err = standard_err(p1, n1, p2, n2)\n    diff = p1-p2\n    return ci_range(diff, std_err)\n\ncenter = bias_groups.get_group('center')\nleft = bias_groups.get_group('left')\nright = bias_groups.get_group('right')\n\n# Confidence interval is \n_ = np.round(calc_ci_range(bias_proportions['left'], len(left), bias_proportions['right'], len(right)), 3)*100\nprint(f\"left vs. right: \\t {_[0]}% to {_[1]}%\")\n\n_ = np.round(calc_ci_range(bias_proportions['center'], len(center), bias_proportions['left'], len(left)), 3)*100\nprint(f\"center vs. left: \\t {_[0]}% to {_[1]}%\")\n\n_ = np.round(calc_ci_range(bias_proportions['center'], len(center), bias_proportions['right'], len(right)), 3)*100\nprint(f\"center vs. right: \\t {_[0]}% to {_[1]}%\")","48a394f6":"own_assigns = {'Atlantic': 'non-profit', 'Breitbart': 'LLC', 'Business Insider': 'corp', 'Buzzfeed News': 'private',\n               'CNN': 'corp', 'Fox News': 'corp',\n                'Guardian': 'LLC', 'National Review': 'non-profit', 'New York Post': 'corp', 'New York Times': 'corp',\n                'NPR': 'non-profit', 'Reuters': 'corp', 'Talking Points Memo': 'private', 'Washington Post': 'LLC', 'Vox': 'private'}\narticles['ownership'] = articles['publication'].apply(lambda x: own_assigns[x])\nowner_groups = articles.groupby('ownership')\nowner_proportions = owner_groups['cc_wds'].sum() \/ owner_groups['cc_wds'].count()\n\nax=owner_proportions.plot(kind='bar', fontsize=14, figsize=(12,6), color=\"gray\")\nax.set_title('Proportion of climate change articles by Ownership Group\\n', fontsize=20)\nax.set_xlabel('Ownership', fontsize=18)\nax.set_ylabel('Proportion', fontsize=18)\nplt.tight_layout()\nsns.despine();","199988ad":"llc = owner_groups.get_group('LLC')\ncorp = owner_groups.get_group('corp')\nnon_profit = owner_groups.get_group('non-profit')\nprivate = owner_groups.get_group('private')\n\n\n_ = np.round(calc_ci_range(owner_proportions['LLC'], len(llc), owner_proportions['corp'], len(corp)), 3)*100\nprint(f\"LLC vs. corp: \\t\\t\\t {_[0]}% to {_[1]}%\")\n\n_ = np.round(calc_ci_range(owner_proportions['non-profit'], len(non_profit), owner_proportions['LLC'], len(llc)), 3)*100\nprint(f\"non-profit vs. LLC \\t\\t {_[0]}% to {_[1]}%\")\n\n_ = np.round(calc_ci_range(owner_proportions['private'], len(private), owner_proportions['non-profit'], len(non_profit)), 3)*100\nprint(f\"private vs. non-profit: \\t {_[0]}% to {_[1]}%, (not significant)\")","0a16b88c":"# \ud83c\udf2a\ufe0f Which news outlets are giving climate change the most coverage?\n\nThis data set contains over 142,000 articles from 15 sources mostly from 2016 and 2017.\n\nOne of the things that might be interesting to look at is the correlation, if any, between the characteristics of these news outlets and the proportion of climate-change-related articles they publish.  We might hypothesize that right-leaning **Breitbart**, for example, would have a lower proportion of climate related articles than, say, **NPR**. [Source](https:\/\/www.dataquest.io\/blog\/tutorial-text-analysis-python-test-hypothesis\/)","f92f1d76":"# \ud83c\udf02 How many articles are talking about climate change? \n\nLet's identify keywords that might correlate with the topic, and search for them in the articles. \n\nWe want the string **`chang`** followed by the string **`climate`** within 1 to 5 words (in regular expressions, `\\w+` matches one or more word characters, and `\\W+` matches one or more nonword characters).","2083289b":"# \ud83e\uddd1\ud83c\udfff\u200d\ud83e\udd1d\u200d\ud83e\uddd1\ud83c\udfff Is a certain political group more prominent","5aa7fee4":"# \ud83d\udcac Unique words","d6d60fe8":"# \ud83d\udcf0 Articles by Publications \n\nBreitbart leads the pack","fa20670b":"# \ud83e\udd4a Vox vs. Business Insider. You decide","f9806322":"# \ud83d\uddde\ufe0f Length of articles","11ed4fec":"Next, we can look at publication ownership, using the same approach. We divide our population into four groups, LLC, corporation, non-profit, and private.","e2763aaa":"# \ud83d\udc44 Most common words"}}