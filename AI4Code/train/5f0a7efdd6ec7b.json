{"cell_type":{"79b216c7":"code","f8b30941":"code","78e98651":"code","0d2c0abe":"code","d8927e82":"code","538a8a09":"code","6327f2c7":"code","183b0c62":"code","667361bf":"code","422a0a80":"code","f936b3e7":"code","570d0c64":"code","4d6b97e7":"code","2f1d8e52":"code","aabe1b6a":"code","a2f82a0d":"code","25ad186c":"code","82ef6863":"code","f924351c":"code","c845d8a5":"code","347ca8ba":"code","e0793278":"code","d476b874":"code","db00dd1f":"code","b394e185":"code","e4091e32":"code","7bace86d":"code","c34f2990":"code","91fec115":"code","af514c49":"code","80afabfa":"markdown","616f1e80":"markdown","4dcc604f":"markdown","eca2eaa7":"markdown","629f6268":"markdown","8f8fce58":"markdown","a2641989":"markdown","e26f8cab":"markdown","f9c3fa3f":"markdown","6795949c":"markdown","6610360e":"markdown","576b17d3":"markdown"},"source":{"79b216c7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('max_rows',100,'max_columns',100)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler","f8b30941":"train = pd.read_csv('\/kaggle\/input\/sec1-tedious-data-cleaning\/ctrain.csv')\ntest = pd.read_csv('\/kaggle\/input\/sec1-tedious-data-cleaning\/ctest.csv')\n\nnulltrain = train.isnull().sum()\nprint(nulltrain[nulltrain>0])\n\nnulltest = test.isnull().sum()\nprint(nulltest[nulltest>0])\n","78e98651":"train.shape, test.shape","0d2c0abe":"train.head()","d8927e82":"catcols = train.select_dtypes(object).columns\nnumcols = train.select_dtypes(np.number).columns\n\nprint('Total categorical columns:',len(catcols),'\\nTotal numerical columns:', len(numcols))","538a8a09":"## Useful functions\n\n\ndef outlier_detector(df,feature):\n    \"\"\"\n    Detect rows which contains negative\/positive outlier in any feature columns\n    \n    \"\"\"\n    rows = (((df[feature] - df[feature].mean()) > (3*df[feature].std())) | ((df[feature] - df[feature].mean()) < (-3*df[feature].std()))).any(axis=1)\n    return rows\n\n    \n\ndef test_model(data,features,target):\n    \"\"\"\n    Evaluate RMSE for simple Linear Regression model with given features and target\n    \n    \"\"\"\n    \n    all_X = data[features].copy()\n    \n    scaler = StandardScaler()\n    all_X = scaler.fit_transform(all_X)\n    \n    all_y = data[target].copy()\n    \n    X_train,X_test,y_train,y_test = train_test_split(all_X,all_y, test_size=0.3, random_state = 0,shuffle=True)\n    \n   \n    model = LinearRegression().fit(X_train,y_train)\n    prediction = model.predict(X_test)\n    y_test = np.exp(y_test) - 1\n    prediction =np.exp(prediction) - 1\n    \n    mse = mean_squared_error(y_test,prediction)\n    rmse = np.sqrt(mse)\n    \n    print(\"Result of test model\\n\\nFeatures:\",list(features),\"\\n\\nTarget:\",target,\"\\n\\nRMSE:{:.4f}\".format(rmse))\n\n    \n    ","6327f2c7":"# SalePrice Analysis\n\nfig,(ax1,ax2,ax3) = plt.subplots(1,3, figsize=(15,5))\nlogform = np.log1p(train['SalePrice'])\nsqrtform =np.sqrt(train['SalePrice'])\n\nsns.distplot(train['SalePrice'], bins =500,kde=False,ax=ax1).set_title('Skew: {:.2f}'.format(train['SalePrice'].skew()))\nsns.distplot(logform,bins = 500,kde=False,ax=ax2).set_title('Skew: {:.2f}'.format(logform.skew()))\nsns.distplot(sqrtform, bins= 500, kde=False, ax=ax3).set_title('Skew: {:.2f}'.format(sqrtform.skew()))","183b0c62":"# We would chose log version of SalePrice as our target\ntrain['LogPrice'] = logform\n\n# Detect outliers in target column\noutliers = outlier_detector(train,['LogPrice'])\nprint('Number of outliers:',outliers.sum())\n\n# Remove Outliers\ntrain = train[~outliers]","667361bf":"# Test model with all available features and newly created target variable 'LogPrice'\nfeatures = train.select_dtypes(np.number).columns.drop(['SalePrice','LogPrice'])\ntest_model(train,features,'LogPrice')","422a0a80":"cormat = train.corr()\ncormat.style.applymap(lambda x: 'background-color : yellow' if (x>0.8) & (x!=1) else '')","f936b3e7":"# We are definitely keeping feature which has high correlation with target like OverallQual.\n# tuple list with very high correlation. \ntuplist = [('1stFlrSF','TotalBsmtSF') ,('TotRmsAbvGrd','GrLivArea'),('GarageArea','GarageCars')]\n\nfig,axs = plt.subplots(1,3,figsize=(15,5))\naxs = axs.flatten()\nfor i,tup in enumerate(tuplist):\n    sns.scatterplot(train[tup[0]],train[tup[1]], ax = axs[i])\n    plt.tight_layout()","570d0c64":"# New Feature from above analysis is 'Roomsize'\ntrain['RoomSize'] = train['GrLivArea']\/train['TotRmsAbvGrd']\nfig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5))\nsns.boxplot(train[tuplist[1][0]],train[tuplist[1][1]],ax=ax1)\nsns.boxplot(train[tuplist[1][0]],train['LogPrice'],ax=ax2)\nsns.scatterplot(train['RoomSize'],train['LogPrice'],ax=ax3).set_title('corr:{:.2f}'.format(train['RoomSize'].corr(train['LogPrice'])))\n\n","4d6b97e7":"# Visualizing outliers in scatterplot of feature vs target for first 20 numerical columns.\nfig, axs = plt.subplots(5,4,figsize=(20,20))\naxs = axs.flatten()\nfor i,col in enumerate(numcols[:20]):\n    sns.scatterplot(train[col],train['LogPrice'], ax=axs[i])\n    plt.tight_layout()","2f1d8e52":"# First set of outlier based first 20 numerical columns\n\noutlierrows1 = (train['LotFrontage']>200) | (train['LotArea']>100000) | (train['MasVnrArea']>1200) | (train['BsmtFinSF1'] > 3000) | \\\n                (train['BsmtFinSF2'] > 1200) | (train['TotalBsmtSF'] > 4200) | (train['1stFlrSF'] >3500) | (train['GrLivArea'] >4000) | \\\n                (train['BsmtFullBath'] > 2) | (train['BsmtHalfBath']> 1) ","aabe1b6a":"# Visualizing outliers in scatterplot of feature vs target for remaining numerical columns.\nfig, axs = plt.subplots(5,4,figsize=(20,20))\naxs = axs.flatten()\nfor i,col in enumerate(numcols[20:]):\n    sns.scatterplot(train[col],train['LogPrice'], ax=axs[i])\n    plt.tight_layout()","a2f82a0d":"# Second set of outliers based on remaining set of columns\n\noutlierrows2 = (train['BedroomAbvGr'] > 7 ) |(train['KitchenAbvGr'] < 1 ) | (train['KitchenAbvGr'] > 2 ) | \\\n               (train['TotRmsAbvGrd'] > 12 ) | (train['WoodDeckSF'] > 800 ) | (train['OpenPorchSF'] > 450) | (train['EnclosedPorch'] > 400) | \\\n               (train['3SsnPorch'] > 350 ) | (train['MiscVal'] > 4000 ) \n\n# Remove outliers based on features\ntrain = train[~(outlierrows1 | outlierrows2)]\n\ntrain.shape\n                                                                                                               ","25ad186c":"# Test model with data without outliers. Also remove 'Id' column as it has not much information and may affect interpretability of model.\nfeatures = train[numcols].columns.drop(['SalePrice','Id'])\ntest_model(train,features,'LogPrice')","82ef6863":"numcols","f924351c":"# Let's make set of relevant features and analyse them together\ns1 = ['LotFrontage', 'LotArea']\ns2 = ['YearBuilt', 'YearRemodAdd','MoSold', 'YrSold']\ns3 = ['BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']\ns4 = ['1stFlrSF', '2ndFlrSF','LowQualFinSF', 'GrLivArea']\ns5 = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath','HalfBath']\ns6 = ['BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd']\ns7 = ['GarageYrBlt', 'GarageCars', 'GarageArea']\ns8 = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']","c845d8a5":"# Feature Engineering using set1 ['LotFrontage', 'LotArea']\n\n# Assuming lot shape is approximately rectangle. We can develop following feature\ntrain['LotPerimeter'] = 2*((train['LotArea']\/train['LotFrontage'])+train['LotFrontage'])\ns1.extend(['LotPerimeter'])\nsns.pairplot(train[s1])","347ca8ba":"# Feature Engineering using set 2 ['YearBuilt', 'YearRemodAdd','MoSold', 'YrSold']\n\ntrain['YrMoSold'] = train['YrSold']+0.01*train['MoSold']\ntrain['Age'] = train['YrMoSold'] - train['YearBuilt']\ntrain['IsRemod'] = train['YearBuilt'] != train['YearRemodAdd'] \ntrain['IsRemod'] = train['IsRemod'].map({True:1,False:0})       # This feature would be useful if we use tree based prediction method\n\ns2.extend(['YrMoSold','Age','IsRemod'])\nsns.pairplot(train[s2])","e0793278":"# Feature Engineering using set 3 ['BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']\ntrain['TotFinBsmt'] = train['BsmtFinSF1'] + train['BsmtFinSF2']\ntrain['IsBsmt'] = train['TotalBsmtSF'] != 0        # Useful in tree based modelling\ntrain['IsBsmt'] = train['IsBsmt'].map({True:1,False:0})\n\ns3.extend(['TotFinBsmt','IsBsmt'])\nsns.pairplot(train[s3])","d476b874":"# Feature Engineering using Set 5 ['BsmtFullBath', 'FullBath','BsmtHalfBath', 'HalfBath']\ntrain['TotFullBath'] = train['BsmtFullBath']+train['FullBath']\ntrain['TotHalfBath'] = train['BsmtHalfBath'] +train['HalfBath']\n\n# Feature Engineering using set 6 ['BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd']\n# 'BedroomAbvGr'  is bedroom above grade.  We are not sure about what is grade here. Let's skip this step.\n\n# Feature Engineering using set 7 ['GarageYrBlt', 'GarageCars', 'GarageArea'] - NA\n\n# Feature Engineering using Set 8\ntrain['TotPorch'] = train['OpenPorchSF']+train['EnclosedPorch']+train['3SsnPorch']+train['ScreenPorch']","db00dd1f":"# test model with engineered features along with original\ndropcols = ['TotRmsAbvGrd','GarageCars','TotalBsmtSF','SalePrice','LogPrice','Id']\nfeatures = train.select_dtypes(np.float64).columns.drop(dropcols)\ntest_model(train,features,'LogPrice')","b394e185":"len(features)","e4091e32":"from sklearn.feature_selection import RFECV\n\nscaler = StandardScaler()\nall_X = train[features]\nall_X = scaler.fit_transform(all_X)\nall_y = train['LogPrice'].copy()\n\nselector = RFECV(LinearRegression(), step=1, cv=3,scoring = 'neg_mean_squared_error')\nselector = selector.fit(all_X,all_y)\n\nselected_features = features[selector.support_]\n\ntest_model(train,selected_features,'LogPrice')\nplt.plot(range(1, len(selector.grid_scores_) + 1), -selector.grid_scores_)\nplt.xlabel('Number of features selected')\nplt.ylabel('Cross Validation Score (MSE)')\n","7bace86d":"plt.figure(figsize=(5,10))\n\nprint('number of selected features',selector.n_features_)\nplt.barh(y=selected_features, width = selector.estimator_.coef_)","c34f2990":"def transform_feature(df):\n    df['RoomSize'] = df['GrLivArea']\/df['TotRmsAbvGrd']\n    df['LotPerimeter'] = 2*((df['LotArea']\/df['LotFrontage']) + df['LotFrontage'])\n    df['YrMoSold'] = df['YrSold']+0.01*df['MoSold']\n    df['Age'] = df['YrMoSold'] - df['YearBuilt']\n    df['IsRemod'] = df['YearBuilt'] != df['YearRemodAdd'] \n    df['IsRemod'] = df['IsRemod'].map({True:1,False:0}) \n    df['TotFinBsmt'] = df['BsmtFinSF1'] + df['BsmtFinSF2']\n    df['IsBsmt'] = df['TotalBsmtSF'] != 0                    # Useful in tree based modelling\n    df['IsBsmt'] = df['IsBsmt'].map({True:1,False:0})\n    df['TotFullBath'] = df['BsmtFullBath']+df['FullBath']\n    df['TotHalfBath'] = df['BsmtHalfBath'] +df['HalfBath']\n    df['TotPorch'] = df['OpenPorchSF']+df['EnclosedPorch']+df['3SsnPorch']+df['ScreenPorch']\n    \n    return df\n\n    ","91fec115":"holdout = transform_feature(test)\n\nscaled_df = pd.DataFrame(scaler.transform(holdout[features]),columns = features)\n\npred = selector.predict(scaled_df)\n\npred =np.exp(pred) - 1\n\nsubmission_df = pd.DataFrame({'Id':holdout['Id'].astype(int),'SalePrice':pred})\nsubmission_df.to_csv('submission.csv',index= False)\n","af514c49":"train.to_csv('engineered_train.csv',index=False)\nholdout.to_csv('engineered_test.csv',index=False)","80afabfa":"## EDA and Feature Engneering\n\nUnder this subsection, we will follow below steps \n1. Collinearity Analysis\n2. Outlier Removal\n3. Feature Engineering\n\n### Analysis for collinearity:\n\nCollinear features affect the intepretability of machine learning model as well as it increases the complexity. Therefore, it is safe to remove them as it would not affect much on final prediction accuracy. In this context, correlation> 0.8 between two features is considered as collinear features.(Refer this link for more detail: https:\/\/medium.com\/future-vision\/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168). We would analyze their role carefully before decide on their removal.","616f1e80":"## Introduction\n\nIn the previous section, we have cleaned whole dataset(train and test) while maintained size and shape of data same as original.(Refer:https:\/\/www.kaggle.com\/lajari\/sec1-tedious-data-cleaning). We are going to use this cleaned data for further analysis and feature engineering. \n\nOur main focus in this section, will be on EDA, feature engineering and feature selection for numerical features only. At every step of our data analysis we are evaluating one test model to analyse the impact of our engineered data on overall performance in terms RMSE. This test model is nothing but just simple linear regressor. \n","4dcc604f":"Our RMSE score improved significantly by outlier removal step. It has decresed from 157419.8091 to 21054.6703 which is remarkable improvement. It itself proved that unusual behavior of feature had big impact on model parameter estimation (in our case coefficient of linear model).Removing such outliers leads to actual estimate of model parameters.","eca2eaa7":"### Feature Engineering \n\nWe have created set of relevant feature and analyse them together for generating new features","629f6268":"## Analyse SalePrice:\nTarget column is right skewed and may contain some outliers. If we use parametric model for regression then it is important to normalize the data as it is underlaying assumption for most linear models. But if we use non parametric model such as tree based regressor it doesn't require.","8f8fce58":"### Load Libraries","a2641989":"### Outlier Removal:\n\nThere are many features which has significant correlation with target variable. As well as there are many data points that could possibly outlier in some pairplot in which Y-axis is SalePrice .For e.g. Unusual SalePrice for OverallCond 2 in following scatterpolt(2nd Row,2nd column). Our intutition is , It could be influence of other features which made SalePrice so unsusual. So It would be harmful if we remove such outlier. Instead, we will remove outliers which shows unusual behavior of feature. (E.g. Lot Area > 100000). ","e26f8cab":"We found some interesting insight:\n1. In some houses, basement is bigger than 1st Floor which is very less probability among all properties.(Refer:1st plot).\n2. ratio of ground living area to total room above ground is very high for some house which could be possible if rooms are bigger than usual. This important information need to be preserved. So we generate ** new feature 'RoomSize'** (Refer: 2nd plot and boxplot below).\n3. Garage capacity increases with Garage Area (Refer: 3rd Plot)\n\n\nIn short we will remove collinear features ['TotRmsAbvGrd','GarageCars','TotalBsmtSF'] but after feature engineering stage.","f9c3fa3f":"## Feature Selection:\n\nWe will apply recursive feature elimination approach. This approach select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained through a coef_ attribute in our case. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.","6795949c":"### Load Cleaned Data","6610360e":"It doesn'nt show any improvement in model performance. It could be possible that we have developed too many features. Therefore, We apply feature selection on further step to select most important features for linear regression model.","576b17d3":"RFE selected 38 features out of 41. It helps to reduce RMSE further to 20577. It means our assumption of having too many features in the model was correct. In above bar plot we have observed negative regression coefficient for positively correlated features. It's because of marginal effect in regression. \n\nWe able to grab ranking position within **top 10%** on test data just by using EDA and FE for numerical features and with simplistic model. In the next section, we will work on categorical features as well as model selection strategy. We will store all the original and newly genearated features for furhter step."}}