{"cell_type":{"fe8055c3":"code","d4422c96":"code","19b8c50b":"code","c447a609":"code","61a9524d":"code","0970a89a":"code","10970883":"code","5abbfd7d":"code","1cebb38c":"code","8997e2b5":"code","40622703":"code","3eca5d7a":"code","6e22c42a":"code","ea1662a9":"code","7195b4a3":"code","15f831ff":"code","bc31994b":"code","5d1bcb0b":"code","56b6b26f":"code","05a2f6be":"code","3614ffd4":"code","6281160b":"code","79e6dac7":"code","d537effe":"code","96fa4523":"code","8efa019f":"code","b6115cfb":"code","11c2d670":"code","be7736c2":"code","069e230e":"code","54693494":"code","958cabde":"code","f5f7bf09":"code","df2a2173":"code","0baac0f6":"code","4adda1d0":"code","6bf08bb7":"code","a8970f81":"code","0bd66da0":"code","22c64c1d":"code","e195cd1e":"code","2e952658":"code","4de55797":"code","8f26290d":"code","c2e71b6d":"code","75d6f2aa":"code","83fc335f":"markdown","c24ca21c":"markdown","643cc04f":"markdown","6f5495ac":"markdown","400733d2":"markdown","e5c0008d":"markdown","713d1ef9":"markdown","a11ae1d6":"markdown","4175b250":"markdown","fee833c2":"markdown","2d69a02f":"markdown","0534c293":"markdown","78ef912e":"markdown","be6e36e2":"markdown","f31b1c9d":"markdown","784ef612":"markdown","0cb2c753":"markdown","b710b830":"markdown","f44013d2":"markdown","1592f863":"markdown","96e8ecfc":"markdown","773d4bc0":"markdown","9dbd6df9":"markdown","2e40b063":"markdown","80460c7c":"markdown","91c5338b":"markdown","35cbaa99":"markdown","703877bb":"markdown","5da8589e":"markdown","6e69e003":"markdown","d68859f5":"markdown","d709d9a0":"markdown","ff6f2961":"markdown","fe859d47":"markdown","2c5ebb29":"markdown","1f0c2544":"markdown","a2dbeca5":"markdown","78862c80":"markdown","5e43b4e3":"markdown","81e4d60c":"markdown","c2969fd4":"markdown","7720d931":"markdown","7677af40":"markdown","ca436eb7":"markdown","8a763af6":"markdown","86e61688":"markdown","6dc53c9c":"markdown"},"source":{"fe8055c3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV ","d4422c96":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","19b8c50b":"train.info()","c447a609":"class Selector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_names):\n        self.feature_names = feature_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.feature_names].values\n    \nembarked_pipeline = Pipeline([\n    ('selector', Selector( ['Embarked'] )),\n    ('imputer', SimpleImputer( strategy='most_frequent' ))\n])","61a9524d":"first_letter = train['Cabin'].apply(lambda x: x[0] if not pd.isna(x) else 0)\nsns.countplot(x=first_letter, hue=train['Survived'])","0970a89a":"class CabinTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X['First_Cabin_Letter'] = X['Cabin'].apply(lambda x: x[0] if not pd.isna(x) else 'X')\n        return X\n\ncabin_pipeline = Pipeline([\n    ('cabin_tr', CabinTransformer()),\n    ('selector', Selector(['First_Cabin_Letter']))\n])","10970883":"sns.barplot(x=train['Pclass'], y=train['Age'], hue=train['Sex'])","5abbfd7d":"sns.barplot(x=train['Sex'], y=train['Age'])","1cebb38c":"sns.barplot(x=train['Parch'], y=train['Age'])","8997e2b5":"sns.barplot(x=train['SibSp'], y=train['Age'])","40622703":"ax = sns.barplot(x=train['SibSp'] + train['Parch'] + 1, y=train['Age'])\nax.set(xlabel='Family Size')","3eca5d7a":"sns.countplot(x=train['SibSp'] + train['Parch'] + 1)","6e22c42a":"male_mean_age = train[train['Sex'] == 'male'].groupby('Pclass').mean()['Age'].values\nfemale_mean_age = train[train['Sex'] == 'female'].groupby('Pclass').mean()['Age'].values","ea1662a9":"class AgeTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        for index, _ in X.iterrows():\n            if pd.isna(X.loc[index, 'Age']):\n\n                if X.loc[index, 'Sex'] == 'male':\n                    if X.loc[index, 'Pclass'] == 1:\n                        X.loc[index, 'Age'] = male_mean_age[0]\n                    elif X.loc[index, 'Pclass'] == 2:\n                        X.loc[index, 'Age'] = male_mean_age[1]\n                    else:\n                        X.loc[index, 'Age'] = male_mean_age[2]\n\n                else:      \n                    if X.loc[index, 'Pclass'] == 1:\n                        X.loc[index, 'Age'] = female_mean_age[0]\n                    elif X.loc[index, 'Pclass'] == 2:\n                        X.loc[index, 'Age'] = female_mean_age[1]\n                    else:\n                        X.loc[index, 'Age'] = female_mean_age[2]\n        return X\n\nage_pipeline = Pipeline([\n    ('age_tr', AgeTransformer()),\n    ('selector', Selector(['Age']))\n])","7195b4a3":"train['Name'].head(10)","15f831ff":"def getTitle(name):\n    return name.split(',')[1].split('.')[0].split()[0]\n\ntrain['Title'] = train['Name'].apply(getTitle)\ntest['Title'] = test['Name'].apply(getTitle)\n\ntrain.Title.value_counts()\ntest.Title.value_counts()","bc31994b":"class TitleAttributeAdder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        titles = X['Name'].apply(getTitle)\n        less_titles_count = titles.value_counts()[4:].keys()\n        titles = titles.apply(lambda x: 'residue' if x in less_titles_count else x)\n        return titles.values.reshape(-1, 1)\n    \nname_pipeline = TitleAttributeAdder()","5d1bcb0b":"sns.countplot(name_pipeline.fit_transform(train).reshape(-1), hue=train['Survived'])","56b6b26f":"train['Family Size'] = train['SibSp'] + train['Parch'] + 1\ntrain['Family Size'].value_counts()","05a2f6be":"sns.barplot(x=train['Family Size'], y=train['Survived'])","3614ffd4":"train['Family Size'] = train['Family Size'].replace(np.arange(5, 12), 'Large')\ntrain['Family Size'] = train['Family Size'].replace([3, 4], 'Medium')\ntrain['Family Size'] = train['Family Size'].replace(2, 'Small')\ntrain['Family Size'] = train['Family Size'].replace(1, 'Alone')","6281160b":"sns.barplot(x=train['Family Size'], y=train['Survived'])","79e6dac7":"class FamilySize_Attribute_Adder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        family_size = X['SibSp'] + X['Parch'] + 1\n        \n        family_size = family_size.replace(np.arange(5, 12), 'Large')\n        family_size = family_size.replace([3, 4], 'Medium')\n        family_size = family_size.replace(2, 'Small')\n        family_size = family_size.replace(1, 'Alone')\n        \n        return family_size.values.reshape(-1, 1)\n\nfamily_pipeline = FamilySize_Attribute_Adder()","d537effe":"sns.barplot(x='Pclass', y='Survived', data=train)","96fa4523":"sns.barplot(x='Sex', y='Survived', data=train)","8efa019f":"pcl_sex_pipeline = Selector(['Pclass', 'Sex'])","b6115cfb":"# Getting 20 random instances of the train dataset\nnp.random.seed(42)\ntrain.Ticket.loc[np.random.randint(0, len(train), size=20)]","11c2d670":"is_numerical = train['Ticket'].apply(lambda x: x.split()[0][0])\nis_numerical = is_numerical.replace('1 2 3 4 5 6 7 8 9'.split(), 1)\nis_numerical = is_numerical.apply(lambda x: 1 if type(x)==int else 0)","be7736c2":"is_numerical.value_counts()","069e230e":"sns.barplot(x=is_numerical, y=train.Survived)","54693494":"sns.distplot(train.Fare)","958cabde":"train['Fare'] = train['Fare'].apply(np.sqrt)\nsns.distplot(train.Fare)","f5f7bf09":"first_quartile = np.quantile(train.Fare, 0.25)\nthird_quartile = np.quantile(train.Fare, 0.75)\ninterquartile_amplitude = third_quartile - first_quartile\nlower_limit = first_quartile - 1.5 * interquartile_amplitude\nhigher_limit = third_quartile + 1.5 * interquartile_amplitude\ntrain.loc[(train.Fare > higher_limit), 'Fare'] = higher_limit","df2a2173":"sns.distplot(train.Fare)","0baac0f6":"class FareTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        fare_sqrt = X.Fare.apply(lambda x: np.log(x + 0.5))\n        self.mean = np.mean(fare_sqrt)\n        first_quartile = np.quantile(fare_sqrt, 0.25)\n        third_quartile = np.quantile(fare_sqrt, 0.75)\n        interquartile_amplitude = third_quartile - first_quartile\n        lower_limit = first_quartile - 1.5 * interquartile_amplitude\n        higher_limit = third_quartile + 1.5 * interquartile_amplitude\n        \n        self.higher_limit = higher_limit\n        self.lower_limit = lower_limit\n        return self\n    def transform(self, X):\n        fare_sqrt = X.Fare.apply(lambda x: np.log(x + 0.5))\n        fare_sqrt.fillna(self.mean, inplace=True)\n        fare_sqrt = fare_sqrt.where(fare_sqrt > self.lower_limit, self.lower_limit)\n        fare_sqrt = fare_sqrt.where(fare_sqrt < self.higher_limit, self.higher_limit)\n        return fare_sqrt.values.reshape(-1, 1)\n    \nfare_pipeline = FareTransformer()","4adda1d0":"numerical_pipeline = FeatureUnion([ \n    ('age_pipe', age_pipeline),\n    ('fare_pipe', fare_pipeline)\n])\n\nnumerical_pipeline = Pipeline([\n    ('num_pipe', numerical_pipeline),\n    ('scaler', StandardScaler())\n])\n\ncategorical_pipeline = FeatureUnion([\n    ('embarked_pipe', embarked_pipeline),\n    ('pcl_sex_pipe', pcl_sex_pipeline),\n    ('name_pipe', name_pipeline),\n    ('family_pipe', family_pipeline),\n    ('cabin_pipe', cabin_pipeline)\n])\n\ncategorical_pipeline = Pipeline([\n    ('cat_pipe', categorical_pipeline),\n    ('encoder', OneHotEncoder(drop='first'))\n])","6bf08bb7":"prepared_data_pipeline = FeatureUnion([\n    ('num_pipe', numerical_pipeline),\n    ('cat_pipe', categorical_pipeline)\n])","a8970f81":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\nX_train = prepared_data_pipeline.fit_transform(train)\ny_train = train.Survived\nX_test = prepared_data_pipeline.transform(test)","0bd66da0":"scores = []\nparam_grid = {'n_estimators': np.arange(10, 100, 5),\n              'max_depth': np.arange(3, 8)}\n\nrf_clf_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, verbose=1)\nrf_clf_grid.fit(X_train, y_train)\n\nscores.append(('RFC', rf_clf_grid.best_score_))\nrf_clf_best = rf_clf_grid.best_estimator_","22c64c1d":"param_grid = {'n_estimators': np.arange(10, 100, 5),\n              'max_depth': np.arange(2, 8)}\n\net_clf_grid = GridSearchCV(ExtraTreesClassifier(), param_grid, cv=3, verbose=1)\net_clf_grid.fit(X_train, y_train)\n\nscores.append(('ETC', et_clf_grid.best_score_))\net_clf_best = et_clf_grid.best_estimator_","e195cd1e":"param_grid = {'C': [0.1, 1, 2, 3, 4, 5]}\n\nlog_reg_grid = GridSearchCV(LogisticRegression(), param_grid, cv=3, verbose=1)\nlog_reg_grid.fit(X_train, y_train)\n\nscores.append((\"Logistic Regression\",log_reg_grid.best_score_))\nlog_reg_best = log_reg_grid.best_estimator_","2e952658":"param_grid = {'C': np.linspace(1, 3, 11), \n              'kernel': ['rbf', 'poly','linear'], \n              'degree': [2, 3, 4]}\n\nsvc_grid = GridSearchCV(SVC(probability=True), param_grid, cv=3, verbose=1)\nsvc_grid.fit(X_train, y_train)\n\nscores.append(('SVC', svc_grid.best_score_))\nsvc_best = svc_grid.best_estimator_","4de55797":"param_grid = {'weights': ['uniform', 'distance'], \n              'n_neighbors': np.arange(2, 20)}\n\nkn_clf_grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, verbose=1)\nkn_clf_grid.fit(X_train, y_train)\n\nscores.append(('KNC', kn_clf_grid.best_score_))\nkn_clf_best = kn_clf_grid.best_estimator_","8f26290d":"scores","c2e71b6d":"voting_clf = VotingClassifier([\n    ('rf_clf', rf_clf_best),\n    ('log_reg', log_reg_best),\n    ('svc', svc_best),\n    ('kn_clf', kn_clf_best)\n], voting='soft')\n\nvoting_clf.fit(X_train, y_train)\npredictions = voting_clf.predict(X_test)","75d6f2aa":"submission = pd.Series(predictions, index=test.PassengerId, name='Survived')\nsubmission.to_csv('titanic_submission.csv', header=True)","83fc335f":"I chose to place the titles with fewer counts in a single bucket to keep the features clean.","c24ca21c":"Extra Trees Classifier.","643cc04f":"The assumption was correct, as the survival rate is higher in the first class, and the third class has the lower survival rate of all.","6f5495ac":"It seems like every passenger has a last name followed by a comma and the passenger's title followed by his first name. So we can create a feature Title and see if it has a correlation with the target variable.","400733d2":"# Importing libs","e5c0008d":"# Preprocessing","713d1ef9":"# Ticket","a11ae1d6":"Let's take a closer look at the ticket feature.","4175b250":"Much better, even though there are a few values higher than 15 that could be considered outliers. Let's check that.","fee833c2":"There are some tickets that are only numerical, and others have a string before the numbers. Let's extract this from data.","2d69a02f":"Then we can concatenate the last two pipelines.","0534c293":"KNeighbors Classifier.","78ef912e":"There is a very high variance for family size higher than 4. Let's place those in a single bucket. Also, let's join the families with size 3 and 4 and keep the 1 and 2. ","be6e36e2":"On the train dataset, there are null values in features Age, Cabin and Embarked. An option could be get rid of the instances, but the dataset is too small, so the model would have less data to train on. Another option could be get rid of the features with null values instead, but for the same reason it wouldn't be good.","f31b1c9d":"# Submission","784ef612":"Passenger class is a strong correlated feature as the most expensive classes were in the top of the ship, having a better chance to survive. Let's see if the data matches this assumption.","0cb2c753":"# SibSp and Parch","b710b830":"Thanks if you read until here. Leave a comment below if I made some mistake or tell me where can I improve. I'll be happy to answer!","f44013d2":"Let's take a closer look to the Name feature.","1592f863":"In a first moment, I will evaluate very different kinds of model. Then I'll choose a subset of the best models and apply a Voting Classifier. Let's start by the RFC model.","96e8ecfc":"# Name","773d4bc0":"# Age","9dbd6df9":"Just like Pclass, Sex is a strong correlated feature as women and children had priority in the ship evacuation. The data corresponds to it. After analysing the remaining features, we'll one hot encode this and the other categorical features.","2e40b063":"# Fare","80460c7c":"And by loading the data again, we only have to pass it through the pipeline. At this point, we end up with 22 feature columns.","91c5338b":"We can see the distribution of the Fare feature in the graphic below.","35cbaa99":"# Embarked","703877bb":"Let's create a feature for handling the family size of each passenger (SibSp + Parch + 1).","5da8589e":"We can see that this distribution has a very high skewness, so let's apply a logarithm or root square to each point and take a look at the new plot.","6e69e003":"# Model Selection","d68859f5":"We can see that as the passenger class increases, the mean age of the class increases as well.","d709d9a0":"We can see that there's a strong correlation between this feature and the Sex feature. The buckets Mr, Mrs and Miss are as expected: men with lower survival rate than women. The Master bucket, though, has a much more higher survival rate than the Mr bucket. Maybe Master was a Title for wealthier people, so they got a higher cabin in the ship (the lower classes were below) and had a better chance to survive.","ff6f2961":"Also, the test dataset has one missing value in the Fare column. So let's take the mean and fill it.","fe859d47":"Now for the Cabin feature, let's get the first letter of the Cabin and see if there's a correlation to the Survived column: ","2c5ebb29":"Aside from the variance, if the ticket begins with numerical or non-numerical values doesn't affect the passenger survival. So I chose to drop this feature.","1f0c2544":"It's possible to see that passengers without Cabin are more likely to die. We can create a feature that will hopefully help improve our model.","a2dbeca5":"# Data Preparation","78862c80":"All the models got approximately the same score. As RFC and ETC are tree based models, I chose to keep RFC for the ensemble. Also, I kept the three remaining models.","5e43b4e3":"# Cabin","81e4d60c":"Great! Now the distribution looks more smooth.","c2969fd4":"Including Parch and SibSp features when imputing Age could be dangerous due to the high variance for family size higher than 3. Later on we can create a feature on that. So I will avoid considering them to impute the missing age values. Based on that, I chose to impute the mean age based on the sex and passenger class.","7720d931":"For the Age feature, let's visualize some plots of the other features.","7677af40":"Instead, I chose to replace the 2 null values in Embarked for the mode (most frequent value). To keep the code clean, let's build a DataFrame selector and a pipeline to handle it.","ca436eb7":"To prepare the data, I will concatenate the numerical and categorical features separately. Then, standardize the numerical ones and one hot encode the categorical ones.","8a763af6":"# Pclass and Sex","86e61688":"Logistic Regression.","6dc53c9c":"Support Vector Machine."}}