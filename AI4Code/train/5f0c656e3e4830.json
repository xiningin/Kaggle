{"cell_type":{"bff8d767":"code","74154dd0":"code","26b241f9":"code","46fca2c3":"code","e448225e":"code","9fb5b4bb":"code","4be9032c":"code","ed3c05d5":"code","44fdfd93":"code","cf9ed932":"code","a054f240":"code","5dfa95aa":"code","0b63d989":"code","8c8e927a":"code","aa8cdac0":"code","c2a0b401":"code","a522d1a3":"code","09b92338":"code","5c9550d6":"code","d93fd1da":"code","199bec6a":"code","374137cc":"code","013d6346":"code","ec9f8242":"code","1d24ffa8":"code","2c3f917c":"code","15ba207a":"code","e7bb7976":"code","cdd4bb96":"code","c6fc7c8f":"code","78a77b6b":"code","5af4b11c":"code","4078fdd2":"code","4f847212":"code","2648b073":"code","c8c47e42":"code","7ad22cdc":"code","96cb42d9":"code","87fdb443":"code","80a8b4a8":"code","b1209856":"code","d96b0643":"code","760aea9c":"code","4ca2046c":"code","b673f0a0":"code","dc33d75f":"code","916bf625":"code","e4f7ccb8":"code","aac3e92a":"code","b02c5f86":"code","6720db50":"code","9a42d5a1":"code","11ad9b3e":"code","163157f7":"code","bd71946d":"code","45429a2f":"code","9f1f5e48":"markdown","5eae22e6":"markdown","95817c92":"markdown","fee4af85":"markdown"},"source":{"bff8d767":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","74154dd0":"emp_attr = pd.read_csv('..\/input\/HR-Employee-Attrition.csv')","26b241f9":"emp_attr.shape","46fca2c3":"emp_attr.info()","e448225e":"emp_attr.head()","9fb5b4bb":"emp_attr.nunique()","4be9032c":"emp_attr.corr()","ed3c05d5":"import seaborn as sns","44fdfd93":"ax = sns.barplot(x ='Attrition', y= 'PercentSalaryHike', data=emp_attr)","cf9ed932":"# for col in emp_attr.columns:\n#     pd.crosstab(emp_attr[col],emp_attr.Attrition).plot(kind='bar',color = ('blue','red'),figsize=(5,5))","a054f240":"emp_attr[['EmployeeCount','StandardHours']].nunique()","5dfa95aa":"emp_attr.drop(columns=['EmployeeCount','StandardHours'],inplace=True)","0b63d989":"emp_attr.info()","8c8e927a":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nemp_attr.iloc[:, 1] = labelencoder.fit_transform(emp_attr.iloc[:, 1])\nemp_attr.iloc[:, 2] = labelencoder.fit_transform(emp_attr.iloc[:, 2])\nemp_attr.iloc[:, 4] = labelencoder.fit_transform(emp_attr.iloc[:, 4])\nemp_attr.iloc[:, 7] = labelencoder.fit_transform(emp_attr.iloc[:, 7])\nemp_attr.iloc[:, 10] = labelencoder.fit_transform(emp_attr.iloc[:, 10])\nemp_attr.iloc[:, 14] = labelencoder.fit_transform(emp_attr.iloc[:, 14])\nemp_attr.iloc[:, 16] = labelencoder.fit_transform(emp_attr.iloc[:, 16])\nemp_attr.iloc[:, 20] = labelencoder.fit_transform(emp_attr.iloc[:, 20])\nemp_attr.iloc[:, 21] = labelencoder.fit_transform(emp_attr.iloc[:, 21])","aa8cdac0":"emp_attr.info()","c2a0b401":"emp_attr.head()","a522d1a3":"lower_bnd = lambda x: (x.quantile(0.25) - (1.5 * ( x.quantile(0.75) - x.quantile(0.25) )))\nupper_bnd = lambda x: (x.quantile(0.75) + (1.5 * ( x.quantile(0.75) - x.quantile(0.25) )))","09b92338":"emp_attr.shape","5c9550d6":"emp_attr.plot(kind='box', figsize=(70,30))","d93fd1da":"# for col in emp_attr.columns:\n#     emp_attr = emp_attr.loc[(emp_attr[col] >= lower_bnd(emp_attr[col])) & (emp_attr[col] <= upper_bnd(emp_attr[col]))]","199bec6a":"# emp_attr = emp_attr.loc[(emp_attr['MonthlyIncome'] >= lower_bnd(emp_attr['MonthlyIncome'])) & (emp_attr['MonthlyIncome'] <= upper_bnd(emp_attr['MonthlyIncome']))]","374137cc":"emp_attr.plot(kind='box', figsize=(70,30))","013d6346":"emp_attr.shape","ec9f8242":"# for col in emp_attr.columns:\n#     pd.crosstab(emp_attr[col],emp_attr.Attrition).plot(kind='bar',color = ('blue','red'),figsize=(5,5))","1d24ffa8":"from math import ceil\ndef int_repl(df,col,n):\n    a = df[col].unique()\n    a.sort()\n    cutoff = ceil(len(a)\/n)\n    x = 0\n    y = 0\n    res = '{'\n    for i in a:\n        if x == cutoff:\n            y = y + 1\n            x = 0\n        res = res + '{0}:{1},'.format(i,y)\n        x = x + 1\n    res = res[0:len(res)-1] + '}'\n    df[col].replace(eval(res),inplace=True)","2c3f917c":"emp_attr.columns","15ba207a":"lst1 = ['Age', 'DistanceFromHome',\n        'TotalWorkingYears',\n       'YearsAtCompany', 'HourlyRate','PercentSalaryHike','YearsInCurrentRole',\n       'YearsSinceLastPromotion','YearsWithCurrManager','NumCompaniesWorked','TrainingTimesLastYear'\n       ,'JobRole','EducationField']","e7bb7976":"lst2 = ['DailyRate','MonthlyIncome', 'MonthlyRate']","cdd4bb96":"emp_attr.nunique()","c6fc7c8f":"for i in lst1:\n    int_repl(emp_attr,i,5)","78a77b6b":"for i in lst2:\n    int_repl(emp_attr,i,5)","5af4b11c":"emp_attr.drop(columns='EmployeeNumber',inplace=True)","4078fdd2":"emp_attr.nunique()","4f847212":"for col in emp_attr.columns:\n    pd.crosstab(emp_attr[col],emp_attr.Attrition).plot(kind='bar',color = ('blue','red'),figsize=(5,5))","2648b073":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nX = emp_attr.drop(columns=['Attrition','Over18'])\nX = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\ny = emp_attr['Attrition']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ntrain_Pred = logreg.predict(X_train)\n","c8c47e42":"metrics.confusion_matrix(y_train,train_Pred)","7ad22cdc":"metrics.accuracy_score(y_train,train_Pred)","96cb42d9":"test_Pred = logreg.predict(X_test)","87fdb443":"metrics.confusion_matrix(y_test,test_Pred)","80a8b4a8":"metrics.accuracy_score(y_test,test_Pred)","b1209856":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_Pred))","d96b0643":"import matplotlib.pyplot as plt \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","760aea9c":"from sklearn.preprocessing import Imputer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n","4ca2046c":"accuracy_dict = {}\naccuracy_list = []\nfor K in range(25):\n    K_value = K + 1\n    neigh = KNeighborsClassifier(n_neighbors=K_value,weights='uniform',algorithm='auto')\n    neigh.fit(X_train, y_train)\n    y_pred=neigh.predict(X_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    accuracy_dict.update({K_value:accuracy})\n    accuracy_list.append(accuracy)\n    print(\"Accuracy is\",accuracy_score(y_test,y_pred)*100,\"% for K-Value\",K_value)","b673f0a0":"key_max = max(accuracy_dict.keys(), key=(lambda k: accuracy_dict[k]))\n\nprint( \"The Accuracy value is \",accuracy_dict[key_max], \"with k= \", key_max)","dc33d75f":"elbow_curve = pd.DataFrame(accuracy_list,columns = ['accuracy'])\nelbow_curve.plot()\n\n","916bf625":"print(classification_report(y_test, test_Pred))","e4f7ccb8":"from sklearn.naive_bayes import GaussianNB\nbayes = GaussianNB()\nbayes.fit(X_train, y_train)\ny_pred=bayes.predict(X_train)","aac3e92a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\ndt = DecisionTreeClassifier(criterion = 'entropy', max_depth=18, \n                                 max_features=1,\n                               min_samples_split=4)\n# X1 = emp_attr.drop(columns=['Attrition','Over18'])\n# X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=0)\n# scaler = preprocessing.StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n# y_train = scaler.transform(y_train) \ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","b02c5f86":"for i in range(1, 20):\n    print('Accuracy score using max_depth =', i, end = ': ')\n    dt = DecisionTreeClassifier(criterion = 'entropy', max_depth=i, \n                                 max_features=1,\n                               min_samples_split=4)\n    dt.fit(X_train, y_train)\n    y_pred = dt.predict(X_test)\n    print(accuracy_score(y_test, y_pred))","6720db50":"for i in np.arange(0.1, 1.0, 0.1):\n    print('Accuracy score using max_features =', i, end = ': ')\n    dt = DecisionTreeClassifier(criterion = 'entropy', max_depth=1, \n                                 max_features=i,\n                               min_samples_split=4)\n    dt.fit(X_train, y_train)\n    y_pred = dt.predict(X_test)\n    print(accuracy_score(y_test, y_pred))","9a42d5a1":"for i in ['entropy','gini']:\n    print('Accuracy score using criterion =', i, end = ': ')\n    dt = DecisionTreeClassifier(criterion = i, max_depth=1, \n                                 max_features=0.1,\n                               min_samples_split=4)\n    dt.fit(X_train, y_train)\n    y_pred = dt.predict(X_test)\n    print(accuracy_score(y_test, y_pred))","11ad9b3e":"for i in range(2, 10):\n    print('Accuracy score using min_samples_split =', i, end = ': ')\n    dt = DecisionTreeClassifier(criterion = 'gini', max_depth=1, \n                                 max_features=0.1,\n                               min_samples_split=i)\n    dt.fit(X_train, y_train)\n    y_pred = dt.predict(X_test)\n    print(accuracy_score(y_test, y_pred))","163157f7":"from sklearn import svm\nsteps = [('scaler', preprocessing.StandardScaler()), ('SVM', svm.SVC())]\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline(steps) # define the pipeline object.","bd71946d":"from sklearn.model_selection import cross_val_score\ncvscores = cross_val_score(pipeline, X_train, y_train, n_jobs=-1)\n\nprint (\"The pipeline CV score is:\")\nprint (cvscores.mean().round(3), \"+\/-\", cvscores.std().round(3))","45429a2f":"pipeline.fit(X_train,y_train)\ny_pred = pipeline.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","9f1f5e48":"Max Depth","5eae22e6":"Criterion","95817c92":"min_samples_split","fee4af85":"Max Features"}}