{"cell_type":{"b485da23":"code","aaaa2d35":"code","9e8de925":"code","68cb285b":"code","ffc5134e":"code","a3e4003c":"code","a1e0e4ed":"code","c825114f":"code","bc213ab6":"code","91f99e63":"code","61fb98ea":"code","2914ba36":"code","da475725":"code","90957118":"code","c917a97b":"code","c8a33e3b":"code","5665c24c":"code","bc5538b6":"code","500a6f91":"code","c4a29130":"markdown","e51ab20f":"markdown","080fb2c3":"markdown","2b3f7af7":"markdown","4659810e":"markdown","0dfadb4b":"markdown","10e2e98a":"markdown","17e13a82":"markdown"},"source":{"b485da23":"%%capture\n# https:\/\/www.kaggle.com\/timesler\/facial-recognition-model-in-pytorch\n# Install facenet-pytorch\n!pip install \/kaggle\/input\/facenet-pytorch-vggface2\/facenet_pytorch-1.0.1-py3-none-any.whl\n# Copy model checkpoints to torch cache so they are loaded automatically by the package\n!mkdir -p \/tmp\/.cache\/torch\/checkpoints\/\n!cp \/kaggle\/input\/facenet-pytorch-vggface2\/20180402-114759-vggface2-logits.pth \/tmp\/.cache\/torch\/checkpoints\/vggface2_DG3kwML46X.pt\n!cp \/kaggle\/input\/facenet-pytorch-vggface2\/20180402-114759-vggface2-features.pth \/tmp\/.cache\/torch\/checkpoints\/vggface2_G5aNV2VSMn.pt","aaaa2d35":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport gc\nimport cv2\nimport glob\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import models, transforms\nfrom facenet_pytorch import MTCNN, InceptionResnetV1","9e8de925":"import sys\npackage_path = '..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master'\nsys.path.append(package_path)\n\nfrom efficientnet_pytorch import EfficientNet","68cb285b":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","ffc5134e":"seed_everything(0)","a3e4003c":"# Set Trained Weight Path\nweight_path = 'efficientnet_b0_epoch_15_loss_0.158.pth'\ntrained_weights_path = os.path.join('..\/input\/deepfake-detection-model-weight', weight_path)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark=True","a1e0e4ed":"test_dir = '..\/input\/deepfake-detection-challenge\/test_videos'\nos.listdir(test_dir)[:5]","c825114f":"def get_img_from_mov(video_file, num_img, frame_window):\n    # https:\/\/note.nkmk.me\/python-opencv-videocapture-file-camera\/\n    cap = cv2.VideoCapture(video_file)\n    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    image_list = []\n    for i in range(num_img):\n        _, image = cap.read()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_list.append(image)\n        cap.set(cv2.CAP_PROP_POS_FRAMES, (i + 1) * frame_window)\n        if cap.get(cv2.CAP_PROP_POS_FRAMES) >= frames:\n            break\n    cap.release()\n\n    return image_list","bc213ab6":"class ImageTransform:\n    def __init__(self, size, mean, std):\n        self.data_transform = transforms.Compose([\n                transforms.Resize((size, size), interpolation=Image.BILINEAR),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std)\n            ])\n\n    def __call__(self, img):\n        return self.data_transform(img)","91f99e63":"class DeepfakeDataset(Dataset):\n    def __init__(self, file_list, device, detector, transform, img_num=20, frame_window=10):\n        self.file_list = file_list\n        self.device = device\n        self.detector = detector\n        self.transform = transform\n        self.img_num = img_num\n        self.frame_window = frame_window\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n\n        mov_path = self.file_list[idx]\n        img_list = []\n\n        # Movie to Image\n        try:\n            all_image = get_img_from_mov(mov_path, self.img_num, self.frame_window)\n        except:\n            return [], mov_path.split('\/')[-1]\n        \n        # Detect Faces\n        for image in all_image:\n            \n            try:\n                _image = image[np.newaxis, :, :, :]\n                boxes, probs = self.detector.detect(_image, landmarks=False)\n                x = int(boxes[0][0][0])\n                y = int(boxes[0][0][1])\n                z = int(boxes[0][0][2])\n                w = int(boxes[0][0][3])\n                image = image[y-15:w+15, x-15:z+15]\n                \n                # Preprocessing\n                image = Image.fromarray(image)\n                image = self.transform(image)\n                \n                img_list.append(image)\n\n            except:\n                img_list.append(None)\n            \n        # Padding None\n        img_list = [c for c in img_list if c is not None]\n        \n        return img_list, mov_path.split('\/')[-1]","61fb98ea":"model = EfficientNet.from_name('efficientnet-b0')\nmodel._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)\nmodel.load_state_dict(torch.load(trained_weights_path, map_location=torch.device(device)))","2914ba36":"test_file = [os.path.join(test_dir, path) for path in os.listdir(test_dir)]","da475725":"test_file[:5]","90957118":"# Prediction\ndef predict_dfdc(dataset, model):\n    \n    torch.cuda.empty_cache()\n    pred_list = []\n    path_list = []\n    \n    model = model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        for i in tqdm(range(len(dataset))):\n            pred = 0\n            imgs, mov_path = dataset.__getitem__(i)\n            \n            # No get Image\n            if len(imgs) == 0:\n                pred_list.append(0.5)\n                path_list.append(mov_path)\n                continue\n                \n                \n            for i in range(len(imgs)):\n                img = imgs[i]\n                \n                output = model(img.unsqueeze(0).to(device))\n                pred += torch.sigmoid(output).item() \/ len(imgs)\n                \n            pred_list.append(pred)\n            path_list.append(mov_path)\n            \n    torch.cuda.empty_cache()\n            \n    return path_list, pred_list","c917a97b":"# Config\nimg_size = 120\nimg_num = 15\nframe_window = 5\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\n\ntransform = ImageTransform(img_size, mean, std)\n\ndetector = MTCNN(image_size=img_size, margin=14, keep_all=False, factor=0.5, \n                 select_largest=False, post_process=False, device=device).eval()\n\ndataset = DeepfakeDataset(test_file, device, detector, transform, img_num, frame_window)\n\npath_list, pred_list = predict_dfdc(dataset, model)","c8a33e3b":"# Submission\nres = pd.DataFrame({\n    'filename': path_list,\n    'label': pred_list,\n})\n\nres.sort_values(by='filename', ascending=True, inplace=True)","5665c24c":"plt.hist(res['label'], 20)\nplt.show()","bc5538b6":"res.head(10)","500a6f91":"res.to_csv('submission.csv', index=False)","c4a29130":"---\n## Pretrained Weights","e51ab20f":"---\n## Prediction","080fb2c3":"---\n\n## History\n\n- V3: Fixed \"ImageTransform\" Class (Resize)","2b3f7af7":"# Efficientnet Single Model\n\n---\n\n## BaseModel:\n\n- Efficientnet-b0(Pretrained)\n\n## Stats:\n\n- Optimizer: Adam\n\n- lr: 0.001\n\n- Schedular: StepLR\n\n- Epochs: 15\n\n- Face Detector: MTCNN\n\n---\n\n## What I was careful about:\n\n- Adjust imbalanced data\n\n- Train data is Only 15 images from 1 Movie","4659810e":"---\n## Submission","0dfadb4b":"---\n## Model","10e2e98a":"---\n## Helper function","17e13a82":"---\n## Library Install"}}