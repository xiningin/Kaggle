{"cell_type":{"df4668e3":"code","35356deb":"code","7a9d4d4f":"code","36fcd50f":"code","7539471e":"code","9a8b5f5c":"code","13e3258e":"code","0277ee09":"code","7eb135a5":"code","da1aeabd":"code","a4e25930":"code","58db6085":"code","09edcd51":"code","06d1685f":"code","4712aa6f":"code","6624d455":"code","590b618b":"code","6fdad06c":"code","30301476":"code","a5579a52":"code","0c90ecab":"code","fa851ce6":"code","a63b8959":"code","57bf3621":"code","94f331c8":"code","76c125b8":"code","1a03173a":"code","0f093ec0":"code","b6d21f20":"markdown","b767f1ae":"markdown","e7bc553a":"markdown","90205fbf":"markdown","a56d0114":"markdown","d6b012b8":"markdown","c0c7ba4b":"markdown","e3942058":"markdown","1c57d323":"markdown"},"source":{"df4668e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35356deb":"pip install bs4","7a9d4d4f":"\nimport random\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download(\"wordnet\")\nlemmatizer = WordNetLemmatizer()\nnltk.download(\"stopwords\") \nfrom nltk.corpus import stopwords\nnltk.download(\"words\")\n\nimport re\nfrom bs4 import BeautifulSoup          #To remove html tags while preprocessing reviews.         #To remove html tags while preprocessing reviews.","36fcd50f":"data = pd.read_csv(r\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ndata.isnull().sum()","7539471e":"tweets=data[\"tweet\"].values.tolist()\nlabels=data[\"label\"].values","9a8b5f5c":"print(tweets[:10])\nprint(len(tweets))\nprint(labels[:10])\nprint(len(data[data[\"label\"]==0]))\nprint(len(data[data[\"label\"]==1]))","13e3258e":"# PRE-PROCESSING:REMOVING STOPWORDS,NON-ENGLISH WORDS AND LEMMATIZING AND FILTERING WORDS WITH LENGTH LESS THAN 3.\n\ndef remove_underscores(sentence):\n    sentence= sentence.replace(\"_\",\" \")\n    return sentence\n\nstop_words = set(stopwords.words('english'))\nenglish_words = set(nltk.corpus.words.words())\n\ndef remove_extra_words(sentence):     #remove stop words and meaningless words\n    new_sentence=\"\"\n    for w in sentence.split():\n        w=w.lower()\n        if w in english_words and w not in stop_words and w.isalpha():\n            new_sentence=new_sentence+\" \"+w\n    return new_sentence\n\ndef lemmatize_and_filter(sentence, min_word_length):        #to lemmatize words and lose the ones with length less than equal to 3.\n    sent = \"\"\n    for word in sentence.split():\n        word=word.lower()\n        if len(lemmatizer.lemmatize(word))>3:\n            sent= sent+\" \"+lemmatizer.lemmatize(word)\n    return(sent)","0277ee09":"#re.sub(r'[^\\w\\s],\"\",string) to remove punctuations.\n#re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', string) to remove urls.\n#strip() to remove whitespaces\n#BeautifulSoup(string, \"lxml\").text) to remove html tags\n\n\n\n\n\ndef preprocessed_data(data):        #takes in list of raw data and returns list of processed string sentences and the list of their corresponding labels.\n    reviews = [lemmatize_and_filter(remove_extra_words(BeautifulSoup(re.sub(r'[^\\w\\s]|^https?:\\\/\\\/.*[\\r\\n]*|\\d+', '', remove_underscores(str(lines)).strip().lower()), \"lxml\").text),3) for lines in data]\n    return(reviews)","7eb135a5":"#  **TRAIN-VALIDATION SPLIT**\n\ndef split_train_into_tain_validate(data_,validation_ratio):\n    \n    data=data_\n    \n    train_size = int(len(data)*(1-validation_ratio))\n    \n    random.shuffle(data)\n\n    validate_samples= data[train_size:]\n\n    data = data[:train_size]\n    \n    return(data,validate_samples)","da1aeabd":"# **CLASSIFICATION [POSITIVE OR NEGATIVE REVIEW-> SENTIMENT ANALYSIS]**\n\n## TENSORFLOW (PREPROCESSING) : CREATING THE DATA TO TRAIN-TEST THE CLASSIFIER.\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","a4e25930":"training_sentences=preprocessed_data(tweets)","58db6085":"tokenizer = Tokenizer(oov_token=\"<OOV>\")           #OOV tag used for unfamiliar words encountered by the model\ntokenizer.fit_on_texts(training_sentences)\n","09edcd51":"word_index = tokenizer.word_index\nword_index #returns a dictionary with key= words, values= tokens(numbers)","06d1685f":"training_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences)\n\ndel training_sentences,training_sequences\n\n# Need this block to get it to work with TensorFlow \n\ntraining_padded = np.array(training_padded)\n#labels","4712aa6f":"vocab_size = max(list(tokenizer.word_index.values()))+1\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 10),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='relu')])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel.summary()","6624d455":"cb = [ tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=.25, patience=2, cooldown=5)]\nhistory = model.fit(training_padded, labels, epochs=50, validation_split=0.2,verbose=1,callbacks = cb)","590b618b":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","6fdad06c":"plot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","30301476":"vocab_size = max(list(tokenizer.word_index.values()))+1\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 10),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='relu')])\n\nmodel.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n\nmodel.summary()","a5579a52":"\nhistory = model.fit(training_padded, labels, epochs=50, validation_split=0.2,verbose=1)","0c90ecab":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","fa851ce6":"plot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","a63b8959":"model2 = tf.keras.Sequential()\nmodel2.add(tf.keras.layers.Embedding(vocab_size, 100))\nmodel2.add(tf.keras.layers.Dropout(0.5))\nmodel2.add(tf.keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel2.add(tf.keras.layers.Dense(1, activation='relu'))\n\nmodel2.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])\n\nmodel2.summary()","57bf3621":"callbacks = [ tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=.25, patience=2, cooldown=0)]\n\nhistory2 = model2.fit(training_padded, labels, epochs=40,\n                    batch_size=500,\n                    validation_split=0.25,\n                    verbose=1,callbacks=callbacks)  ","94f331c8":"plot_graphs(history2, \"accuracy\")\nplot_graphs(history2, \"loss\")","76c125b8":"data = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")\ndata","1a03173a":"tweets=data[\"tweet\"].values.tolist()\ntweets","0f093ec0":"test_sentences=preprocessed_data(tweets)\ntokenizer = Tokenizer(oov_token=\"<OOV>\")           #OOV tag used for unfamiliar words encountered by the model\ntokenizer.fit_on_texts(test_sentences)\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences)\n\ndel test_sentences,test_sequences\ntest_padded = np.array(test_padded)\n\nmodel2.predict_classes(test_padded)\n","b6d21f20":"Checking with test data.","b767f1ae":"Recurrent Neural Network","e7bc553a":"Conclusion: The callbacks were stoping the model from learning. Though the ideal model would have decrease in the graph plotted for loss and increase in the graph for accuracy with epochs increasing.","90205fbf":"Artificial Neural Network","a56d0114":"i.EXTRACTING THE REVIEWS IN STRING TYPE.\n\nii.REMOVING LABELS, NUMBERS, URLs,HTML TAGS AND PUNCTUATIONS.\n\niii.TRANSFORMING ALL REVIEWS TO LOWER CASE.\n\niv. REMOVING STOPWORDS.\n\nv. LEMMATIZING WORDS.","d6b012b8":"# EXTRACTING DATA ","c0c7ba4b":"ANN with different optimizer and without callbacks.","e3942058":"# IMPORTING LIBRARIES","1c57d323":"Hence, this model acts most idealy compared to the other ANN models."}}