{"cell_type":{"8c27fd41":"code","1cf04022":"code","269fa773":"code","3fc99e89":"code","50772eb3":"code","cfcae100":"code","68b66c34":"code","4a902fb6":"code","f7e58f4e":"code","d65c2c7d":"code","f693cbe2":"code","a0ff0596":"code","11a040cd":"code","4e01c3ef":"code","1f0eb2c2":"code","f329d24d":"code","8653f7f9":"code","394637f9":"code","f9b7ac65":"code","070c68f1":"code","e04ecc04":"code","01e566dd":"code","642b84d5":"code","3420043a":"code","5e758ffe":"code","2a2b5e37":"code","54017359":"code","9dc24b5a":"code","8b926ff6":"code","45b6a708":"code","238e47b7":"code","40ec5969":"code","a4f2c6ea":"code","4f24065d":"code","d0bf95da":"code","875fbeba":"code","54f3ef54":"code","a9f7fd4c":"code","781d3e49":"code","d3eebb9c":"code","a80714c6":"code","373efbb2":"code","8837fc3b":"code","1de8317f":"code","af1159e8":"code","86563915":"code","a522d1a7":"code","e3aad3c0":"code","0bfbedaf":"markdown","d2c40163":"markdown","c40696ce":"markdown","f269b5fd":"markdown","6a34ee78":"markdown","a524da57":"markdown","962466ef":"markdown","51833198":"markdown","970516ee":"markdown","bc0ae459":"markdown","68d24c13":"markdown","4a02f885":"markdown"},"source":{"8c27fd41":"!pip install --upgrade --force-reinstall --no-deps kaggle","1cf04022":"!pip install strsimpy","269fa773":"! pip install --upgrade albumentations","3fc99e89":"!python -m pip install -U scikit-image","50772eb3":"import distutils\nfrom distutils import dir_util\nfrom IPython.display import clear_output","cfcae100":"old_directory = '..\/input\/payment-detection' \nnew_directory = '\/kaggle\/working'\n\ndistutils.dir_util.copy_tree(old_directory,new_directory)\nclear_output(True)","68b66c34":"import albumentations as A\nfrom albumentations.core.composition import Compose\n\nfrom skimage.util import random_noise\nfrom skimage.filters import gaussian\n\nimport imageio\nimport imgaug as ia\nfrom imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\nfrom imgaug import augmenters as iaa ","4a902fb6":"import torch\nimport pandas as pd\n\n\nfrom PIL import Image\nimport cv2\nimport gc\nimport time\n\nfrom torch.autograd import Variable\nfrom torch.optim import Adam, Adagrad, SGD\nimport torch.nn.functional as F\n\nimport random\n\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport tqdm\nfrom torch.nn import functional as fnn\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\nfrom torch.nn.functional import ctc_loss, log_softmax\nfrom torchvision import models\n\n\nimport torchvision\nimport pickle\nimport json\n\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.transforms import *\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\nfrom itertools import chain\n\nimport torch.distributed as dist\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport matplotlib.patches as patches\n\n\nimport os\nimport tqdm\nimport json\nimport numpy as np\n\nfrom string import digits, ascii_uppercase\n\nimport math \n\n\nfrom strsimpy.levenshtein import Levenshtein\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pandas as pd\nfrom PIL import Image\nfrom PIL import Image, ImageFilter\nimport numpy as np","f7e58f4e":"torch.cuda.is_available()","d65c2c7d":"SEED = 1489\n\n\nia.random.seed(SEED)\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)","f693cbe2":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","a0ff0596":"#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\n\n#\u043f\u0443\u0442\u0438 \u043a \u0444\u0430\u0439\u043b\u0430\u043c\n\nDATASET_PATH = \"\/kaggle\/working\/\"\n\nTEST_PATH = DATASET_PATH + \"test\/test\/\" \nTRAIN_PATH = DATASET_PATH + \"train\/train\/\"\nTRAIN_INFO = DATASET_PATH + \"train.csv\"\n\nSUBMISSION_PATH = DATASET_PATH + \"submission.csv\"\n\n# \u043f\u0443\u0442\u0438 \u043a \u043c\u043e\u0434\u0435\u043b\u044f\u043c\nMODEL_PATH = DATASET_PATH + 'model.pth'\nBEST_MODEL_PATH = DATASET_PATH + 'best_model.pth'\n\n# \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u0435\n# \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043d\u0430 \u043d\u0438\u0437\u043a\u043e\u043c \u043f\u043e\u0440\u043e\u0433\u0435, \u0442\u043e\u0433\u0434\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0430 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0431\u0443\u0434\u0435\u0442 \n# \u0431\u043e\u043b\u044c\u0448\u0435, \u043d\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0431\u0443\u0434\u0443\u0442 \u0431\u043e\u043b\u0435\u0435 \u043f\u043b\u0430\u0432\u043d\u044b\u043c\u0438\nTHRESHOLD = 0.2  \n\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0434\u043b\u044f \u0440\u0435\u0441\u0430\u0439\u0437\u0438\u043d\u0433\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\nIMAGE_WIDTH = 1333\nIMAGE_HEIGHT = 1333\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nVAL_SIZE = 0.07\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\nN_ITER = 25\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nBATCH_SIZE = 10\nBATCH_SIZE_VAL = 1\n\n\nLR = 2e-4","11a040cd":"train  = pd.read_csv(TRAIN_INFO)","4e01c3ef":"def draw_box(img_name):\n    \"\"\"\n    \u0420\u0438\u0441\u0443\u0435\u0442 \u0431\u043e\u043a\u0441\u044b\n    \"\"\"\n    \n    row = train[train['image']==img_name]\n    \n    bbs =  BoundingBox(x1=row['x1'].values[0],y1=row['y1'].values[0],x2=(row['x1'].values[0] + row['w'].values[0]),y2=(row['y1'].values[0] + row['h'].values[0]))\n    sample_img = mpimg.imread(TRAIN_PATH + img_name)\n    cols = [(0, 255, 0)]\n    a = bbs.draw_on_image(sample_img,color=cols)\n    plt.imshow(a)","1f0eb2c2":"img_name = '0ea0531e10a3e282b58924b4350b8190.jpg'\nrow = train[train['image']==img_name]\nsample_img = mpimg.imread(TRAIN_PATH + img_name)\nbbs =  BoundingBoxesOnImage([BoundingBox(x1=row['x1'].values[0],y1=row['y1'].values[0],x2=(row['x1'].values[0] + row['w'].values[0]),y2=(row['y1'].values[0] + int(row['h'].values[0]\/2))),\nBoundingBox(x1=row['x1'].values[0],y1=(row['y1'].values[0] + 10 + int(row['h'].values[0]\/2)),x2=(row['x1'].values[0] + row['w'].values[0]),y2=(row['y1'].values[0] + row['h'].values[0]))]\n, shape=sample_img.shape)\ncols = [(255, 0, 0)]\na = bbs.draw_on_image(sample_img,color=cols)\nplt.imshow(a)","f329d24d":"img_name = '0ea0531e10a3e282b58924b4350b8190.jpg'\ndraw_box(img_name)","8653f7f9":"img_name = '9a0d3e2cffe184ba1a307bf3cc2dc386.jpg'\ndraw_box(img_name)","394637f9":"to_drop = ['9a0d3e2cffe184ba1a307bf3cc2dc386.jpg','4d7592139d7469e02a4dca74f10421f2.jpg',\n           '404d8bc432f4a76fcfd5a05053476f17.jpg',\n           '8c464b6a62e366072430e4d027ea470c.jpg','0ea0531e10a3e282b58924b4350b8190.jpg',\n           '8c6c65d39bcee4392a2ca2ebae0a7403.jpg','0632a70f66505dab04857a487bdffe11.jpg',\n           'ad4499513b92b3f3b1536d04bf23b768.jpg','a3e79cd280ccdc832802f713d6823a39.jpg',\n           'b9d8c46486e64e369432105b059cbba9.jpg','b7280077e2d34d541050c6a09bcceb3c.jpg',\n           '8f90588d785d85f02c4fdaeb3dc76e7a.jpg','aab853cd99995bfab75250a9178b4baa.jpg'\n           ,'4f7a79b1da7415d70320384e8a37a8df.jpg']","f9b7ac65":"for img in to_drop:\n    \n    temp = train[train['image']==img].index\n    train = train.drop(temp)","070c68f1":"train.head()","e04ecc04":"train.label.hist()","01e566dd":"valid_images = np.random.choice(train.image.unique(), size=int(VAL_SIZE * train.image.nunique()), replace=False)\n\nvalid_set = train[train.image.isin(valid_images)]\ntrain_set = train[~train.image.isin(valid_images)]","642b84d5":"def custom_rotate(data,labels,angles_1,angles_2,sigma=4.0):\n\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043b\u0435\u0439\u0431\u043b\u0430\u043c\u0438 (\u0431\u043b\u044e\u0440 \u0438 \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u044b) \n    \u0438 \u0437\u0430\u043f\u0438\u0441\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442. \n    \"\"\"\n    \n    def get_area(bbs):\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043f\u043b\u043e\u0449\u0430\u0434\u0438 \u0431\u043e\u043a\u0441\u0430 \u043f\u043e\u0441\u043b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0446\u0438\u0438. \n        \"\"\"\n\n        area = (bbs.x2 - bbs.x1)*(bbs.y2 - bbs.y1)\n\n        return area\n\n\n    def check_boxes(bbs_aug):\n        \"\"\"\n        \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0431\u043e\u043a\u0441\u043e\u0432 \u043d\u0443\u0436\u043d\u0430 \u0442.\u043a. \u043f\u0440\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0445 \u0443\u0433\u043b\u0430\u0445 \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0430\n        bbs_aug \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0431\u043e\u043a\u0441\u044b \u0441 \u0434\u0440\u0443\u0433\u0438\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u043e\u043c x1 \u0438 x2 . \n        \"\"\"\n\n        if bbs_aug.x1 > bbs_aug.x2:\n\n              bbs_aug.x1, bbs_aug.x2 = bbs_aug.x2, bbs_aug.x1\n\n        if bbs_aug.y1 > bbs_aug.y2:\n\n              bbs_aug.y1, bbs_aug.y2 = bbs_aug.y2, bbs_aug.y1\n\n        return bbs_aug \n\n    def add_blur(row,sigma):\n          \"\"\"\n          \u0411\u043b\u044e\u0440. \n          \"\"\"\n\n          blurred_img_path = '_blur_' + row.image \n          img = Image.open(TRAIN_PATH + row.image)\n          blurred = img.filter(ImageFilter.GaussianBlur(sigma))\n          sample_img = img.save(TRAIN_PATH + blurred_img_path)\n          blurred_dict = {\n          'label':row.label,\n          'x1':row.x1,\n          'y1':row.y1,\n          'w':row.w,\n          'h':row.h,\n          'width':row.width,\n          'height':row.height,\n          'image':blurred_img_path,\n          }\n      \n          return blurred_dict\n\n    def rotate(row,img,bbs,angle):\n        \"\"\"\n        \u041f\u043e\u0432\u043e\u0440\u043e\u0442. \n        \"\"\"\n        #\u043f\u043e\u0432\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438 \u0431\u043e\u043a\u0441\u044b\n        image_aug, bbs_aug = iaa.Affine(rotate=alpha)(image=img, bounding_boxes=bbs)\n        #\u043e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u0432\u044b\u0448\u0435\u0434\u0448\u0443\u044e \u0437\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u044b \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0447\u0430\u0441\u0442\u044c \u0431\u043e\u043a\u0441\u0430 (\u043f\u043e\u0447\u0435\u043c\u0443 \u0442\u043e \n        # \u044d\u0442\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043d\u0435 \u0432\u0441\u0435\u0433\u0434\u0430 \u0445\u043e\u0447\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c,\u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0438\u0436\u0435 \u0435\u0449\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043d\u0430 \u0432\u044b\u0445\u043e\u0434 \u0431\u043e\u043a\u0441\u0430 \u0437\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u044b)\n        bbs_aug = bbs_aug.clip_out_of_image(sample_img)\n        \n        aug_img_path = TRAIN_PATH + '_' + str(alpha) + '_' + row.image\n\n        image_aug = Image.fromarray(image_aug)\n        image_aug = image_aug.save(aug_img_path)\n\n\n        bbs_aug = check_boxes(bbs_aug)\n\n\n        aug_dict = {\n            'label':row.label,\n            'image':aug_img_path,\n            'x1':bbs_aug.x1,\n            'y1':bbs_aug.y1,\n            'w':(bbs_aug.x2 - bbs_aug.x1),\n            'h':(bbs_aug.y2 - bbs_aug.y1),\n            'width':row.width,\n            'height':row.height\n                }\n\n        return aug_dict,bbs_aug \n    \n    #\u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0443\u0442 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a\n    new_data = pd.DataFrame(columns=data.columns) \n\n    img_count = data.image.value_counts().to_dict() #\u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043b\u0435\u0439\u0431\u043b\u043e\u0432 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \n\n    for label in labels:\n\n\n      rows = data[data['label']==label]  \n      \n\n      for id in range(len(rows)):\n        \n          row = rows.iloc[id]\n\n          sample_img = mpimg.imread(TRAIN_PATH + row.image)\n        \n        #\u0435\u0441\u043b\u0438 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 1 \u043b\u0435\u0439\u0431\u043b\u0430, \u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0435 \u0431\u043e\u043a\u0441\u044b \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043b\u0435\u0439\u0431\u043b\u043e\u0432\n          if img_count[row.image] > 1: \n\n              imgs_rows = data[data['image']==row.image] #\u0432\u0441\u0435 \u0441\u0442\u0440\u043e\u043a\u0438 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u043c\u0430 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \n\n              for idx in range(len(imgs_rows)):\n\n                  row = imgs_rows.iloc[idx]\n\n                  bbs =  BoundingBox(row['x1'],row['y1'],(row['x1'] + row['w']),(row['y1'] + row['h']))\n\n                  img_data = pd.DataFrame(columns=data.columns)\n\n                  augs = []\n\n                  for beta in angles_1: #\u0442\u0443\u0442 \u0438\u0442\u0435\u0440\u0438\u0440\u0443\u0435\u043c\u0441\u044f \u043f\u043e \u0443\u0433\u043b\u0430\u043c \n\n                      aug_dict,bbs_aug = rotate(row,sample_img,bbs,alpha)\n\n                      augs.append(bbs_aug)\n\n                      img_data = img_data.append(aug_dict,ignore_index=True)\n\n\n                  blurred_dict = add_blur(row,sigma)#\u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0431\u043b\u044e\u0440 \n                #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0437\u0430\u0431\u043b\u044e\u0440\u0435\u043d\u043d\u0443\u044e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0432 \u043d\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\n                  new_data = new_data.append(blurred_dict,ignore_index=True)\n                  \n                #\u043d\u0438\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u0432\u0441\u0435 \u0431\u043e\u043a\u0441\u044b \u043f\u043e\u0441\u043b\u0435 \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0430 \u043d\u0435 \u0432\u044b\u0448\u043b\u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0437\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u044b \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \n                  augs = [ ((bbs.is_fully_within_image(sample_img.shape) == False) or (get_area(bbs) == 0) ) for bbs in augs]\n                 \n                #\u0435\u0441\u043b\u0438 \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u0438\u043d \u0431\u043e\u043a\u0441 \u0432\u044b\u0448\u0435\u043b \u0437\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u044b, \u0442\u043e \u043d\u0435 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0435\u0433\u043e \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \n                  if sum(augs) > 0:\n\n                        pass\n\n                  else:\n                    #\u0435\u0441\u043b\u0438 \u0432\u0441\u0435 \u0431\u043e\u043a\u0441\u044b \u0432 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0442\u043e \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u043d\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\n                        new_data = pd.concat([new_data,img_data],ignore_index=True)\n\n\n        #\u0437\u0434\u0435\u0441\u044c \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u0434\u043b\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 1 \u043b\u0435\u0439\u0431\u043b\u043e\u043c\n          else:\n\n              sample_img = mpimg.imread(TRAIN_PATH + row.image)\n              bbs =  BoundingBox(row['x1'],row['y1'],(row['x1'] + row['w']),(row['y1'] + row['h']))\n              \n\n              for alpha in angles_2:\n        \n                  aug_dict,bbs_aug = rotate(row,sample_img,bbs,alpha)\n\n                  bbs_aug = bbs_aug.clip_out_of_image(sample_img)\n                  \n                  if (bbs.is_fully_within_image(sample_img.shape) == False) or (get_area(bbs_aug) == 0):\n\n                      pass\n\n                  else:\n                  \n                      new_data = new_data.append(aug_dict,ignore_index=True)\n\n\n              \n            \n              blurred_dict = add_blur(row,sigma) #\u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0431\u043b\u044e\u0440\n\n              new_data = new_data.append(blurred_dict,ignore_index=True) #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u043d\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\n\n    data = pd.concat([data,new_data],ignore_index=True) #\u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u044b \n\n    return data","3420043a":"import matplotlib\nmatplotlib.rcParams.update({'figure.figsize': (8, 8), 'font.size': 14})\n\n\ndef display_image(img_path):\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n    \"\"\"\n    sample_img = Image.open(img_path)\n    plt.imshow(sample_img)\n    sample_img.show()","5e758ffe":"labels = [\"PC\",\"EX\",\"ST\",\"UY\"] #\u043b\u0435\u0439\u0431\u043b\u044b,\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \nangles_1 = [30,-30,60,-60] #\u0443\u0433\u043b\u044b \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0430 \u0434\u043b\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c\u0438 \u043b\u0435\u0439\u0431\u043b\u0430\u043c\u0438 \nangles_2 = [30,-30,60,-60,90,-90,120,-120] #\u0443\u0433\u043b\u044b \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0430 \u0434\u043b\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 \u043e\u0434\u043d\u0438\u043c \u043b\u0435\u0439\u0431\u043b\u043e\u043c \nsigma = 4.5  #\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0431\u043b\u044e\u0440\u0430 \n\ntrain_set = custom_rotate(train_set,labels,angles_1,angles_2,sigma)","2a2b5e37":"train_set.label.hist()","54017359":"test = pd.read_csv(SUBMISSION_PATH)","9dc24b5a":"test.head()","8b926ff6":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","45b6a708":"class ShapeDataset(Dataset):\n    \"\"\"\n    \u0418\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \n    \"\"\"\n    \n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,mode=None,\n                 transform=transforms.Compose([ToTensor(), \n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               )])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n\n        self.mode = mode \n\n        self.image_uniq = data.image.unique() #\u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u043b\u0435\u0439\u0431\u043b\u043e\u0432 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \n        self.data = data\n\n        self.transform = transform\n        \n        #\u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \n        self.augment = A.Compose([\n          A.RandomBrightnessContrast(p=0.5),\n          A.RandomRotate90(p=0.5),\n          A.RandomCrop(p=0.25,height=int(IMAGE_HEIGHT*0.50),width=int(IMAGE_WIDTH*0.50)),\n          A.RandomSizedBBoxSafeCrop(int(IMAGE_HEIGHT*0.60),int(IMAGE_WIDTH*0.60),p=0.5),\n          A.ToGray(p=0.5),\n          A.Blur(p=0.3)\n        ], bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.3,label_fields = ['class_labels']))\n\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        #\u0437\u0434\u0435\u0441\u044c \u0443\u043a\u0430\u0437\u044b\u0432\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0432 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0435\n        return len(self.image_uniq) \n    \n    def __getitem__(self, idx):\n        \n              \n        def get_boxes(obj):\n            boxes = [obj[f] for f in ['xmin', 'ymin', 'xmax', 'ymax'] ] \n            return boxes\n\n\n        def get_areas(obj):\n            areas = [(obj['xmax'] - obj['xmin']) * (obj['ymax'] - obj['ymin']) ]\n            return torch.as_tensor(areas, dtype=torch.int64)\n        \n        def get_class(obj):\n            \"\"\"\n            \u041a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0447\u0438\u0441\u043b\u043e\u043c\n            \"\"\"\n            \n            cards = {\n                'VI':1,\n                'MA':2,\n                'EX':3, \n                'PC':4, \n                'ST':5,\n                'UY':6\n                    }\n            return torch.as_tensor([cards[obj['label']]], dtype=torch.int64)\n        \n           \n            \n        img_name = self.image_uniq[idx]\n        path = os.path.join(self.IMAGE_DIR, img_name)\n\n        image = cv2.imread(path)\n        \n        shapes  = image.shape\n        \n        image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n        bboxes = []\n\n        target = {}\n\n        target['labels'] = []\n        target['area'] = []\n        \n        #\u0442\u0443\u0442 \u0438\u0442\u0435\u0440\u0438\u0440\u0443\u0435\u043c\u0441\u044f \u043f\u043e \u0432\u0441\u0435\u043c \u0441\u0442\u0440\u043e\u043a\u0430\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430 \u0434\u0430\u043d\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n        for row in self.data[self.data['image']==img_name].values:\n        \n\n            obj = {}      \n      \n            \n            cols = self.data.columns.tolist()\n            \n            obj['xmin'] = int(row[cols.index('x1')] * (IMAGE_WIDTH\/ row[cols.index('width')]))\n            obj['xmax'] = int((row[cols.index('x1')] + row[cols.index('w')])*(IMAGE_WIDTH\/ row[cols.index('width')]))\n            obj['ymin'] = int(row[cols.index('y1')] * (IMAGE_HEIGHT\/ row[cols.index('height')]))\n            obj['ymax'] = int((row[cols.index('y1')] + row[cols.index('h')])*(IMAGE_HEIGHT\/ row[cols.index('height')]))\n\n            obj['label'] = row[cols.index('label')]\n\n            bboxes.append(get_boxes(obj)) #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u0431\u043e\u043a\u0441\u044b \n\n            target['labels'].append(get_class(obj)) #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u043b\u0435\u0439\u0431\u043b\u044b\n\n            target['area'].append(get_areas(obj))  #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043b\u043e\u0449\u0430\u0434\u0438 \u0431\u043e\u043a\u0441\u043e\u0432 \n\n          \n        # \u0434\u043b\u044f \u0442\u0440\u0435\u0439\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \n        if self.mode == \"Train\":\n            \n            transformed = self.augment(image=image, bboxes=bboxes, class_labels =  target['labels'])\n            transformed_bboxes = torch.as_tensor(transformed['bboxes'],dtype=torch.int64)\n            transformed_class_labels = transformed['class_labels']\n\n\n              \n        # \u0435\u0441\u043b\u0438 \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u043e\u043a\u0441\u0430 \u043f\u043e\u0441\u043b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043c\u0435\u043d\u044c\u0448\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 min_visibility,\u0442\u043e self.augment\n        # \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0431\u043e\u043a\u0441\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0437\u0434\u0435\u0441\u044c \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430 \u043f\u0443\u0441\u0442\u043e\u0439 \u0431\u043e\u043a\u0441\n        if self.mode == \"Train\" and (transformed_bboxes.size()[0] != 0):\n\n            target['boxes'] = torch.as_tensor(transformed_bboxes,dtype=torch.int64)\n\n            if self.transform:\n\n              image = self.transform(transformed['image'])\n            \n        #\u0435\u0441\u043b\u0438 \u0431\u043e\u043a\u0441 \u043f\u0443\u0441\u0442\u043e\u0439, \u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043d\u0435\u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0443\u044e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438 \u0431\u043e\u043a\u0441\u044b\n        else :\n          \n            target['boxes'] = torch.as_tensor(bboxes, dtype=torch.float)\n\n            if self.transform:\n\n              image = self.transform(image)\n\n        target['labels'] = torch.tensor(target['labels'])\n        target['image_id'] = torch.as_tensor([idx], dtype=torch.int64)\n        target['area'] = torch.tensor(target['area'] )\n        target['iscrowd'] = torch.ones((1,), dtype=torch.int64)\n\n        if self.mode == \"Train\":\n\n          return image, target\n\n        else: #\u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0442\u0430\u043a\u0436\u0435 id \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \n\n          return image, target, img_name ","238e47b7":"class ShapeDatasetTest(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(),\n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               ) ])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        \n        img_name = self.data.iloc[idx]['image']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n        \n        \n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n\n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n        \n        \n        if self.transform:\n            image = self.transform(img)\n            \n        return image ","40ec5969":"train_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, train_set,mode=\"Train\")\nvalid_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, valid_set)","a4f2c6ea":"test_data  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, test)","4f24065d":"dataloader_train = DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\ndataloader_valid = DataLoader(\n    valid_data, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=0, collate_fn=collate_fn)","d0bf95da":"device = get_device()\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 7\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n\nmodel = model.to(device)","875fbeba":"params = [p for p in model.parameters()  if p.requires_grad]\noptimizer = optim.AdamW(params, lr=LR)\nloss_fn = fnn.mse_loss\n\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=1,\n                                               gamma=0.9)","54f3ef54":"def levenshtein(preds,true_label):\n\n    def c_sort(sub_li):\n\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n\n    def get_reversed_class(number):\n        \n        \"\"\"\n        \u0414\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430.\n        \"\"\"\n        cards = {\n                1:'VI',\n                2:'MA',\n                3:'EX', \n                4:'PC', \n                5:'ST',\n                6:'UY'\n                    }\n        return cards[number]\n\n    levenshtein = Levenshtein()     \n            \n    imgs = {}\n    \n    \n\n    for image in pd.unique(true_label.image):\n        \n\n        cur_rows = true_label[true_label['image']==image]\n        #\u0438\u0434\u0451\u043c \u043f\u043e \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0443 \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\n        \n   \n        imgs[image] = []\n\n        for index, row in cur_rows.iterrows():    \n            \n            \n            \n            imgs[row['image']].append([row['x1'], row['y1'], row['x1'] + row['w'], row['y1'] + row['h'], row['label'] ])\n            \n        imgs[row['image']] = c_sort(imgs[row['image']])\n\n       \n            \n            \n    labels = {}\n    \n    for i in imgs.keys():\n        \n        #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0442\u0440\u043e\u043a\u0443\n        \n        labels[i] = \" \".join([j[4] for j in imgs[i]])\n\n    \n    \n    lev_dist = 0\n    \n    for i in imgs.keys():\n      \n        pred_label = preds.payment[preds['image']==i].values\n        pred_label = \" \".join(pred_label)\n        true_label = labels[i]\n        lev_dist += levenshtein.distance(pred_label, true_label)\n\n    return lev_dist\/len(preds)","a9f7fd4c":"def get_prediction(dataset, df,mode, model,THRESHOLD,device=device):\n\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \n    \"\"\"\n    model = model.to(device)\n    model.eval()\n   \n    def tackle_img(img):\n        \"\"\"\n        \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043d\u0430 \u0447\u0430\u0441\u0442\u0438.\n        (\u041d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435 \u0437\u0434\u0435\u0441\u044c \u043c\u043e\u0436\u043d\u043e \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u043a\u0441\u044b \u043a\u0430\u043a \u044d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442\u0441\u044f \n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438,\u043d\u043e \u0443 \u043c\u0435\u043d\u044f \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0431\u044b\u043b\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438, \u0433\u0434\u0435 \u043e\u0434\u0438\u043d \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0439 \n        \u043b\u0435\u0439\u0431\u043b,\u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u0443\u0432\u0438\u0434\u0435\u0442\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u0435\u043b\u0430\u0442\u044c \u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0435 \u0441\u0442\u0430\u043b)\n        \"\"\"\n        \n        tr = transforms.Compose([ToTensor(),Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])])\n        \n        path = os.path.join(TEST_PATH, img)\n\n        image = cv2.imread(path)\n        \n        shapes  = image.shape\n        \n        image = cv2.resize(image, (3*IMAGE_WIDTH, 3*IMAGE_HEIGHT))  \n\n        image = tr(image).unsqueeze(0)\n\n        image = image.to(device)\n\n        # \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439 \u0434\u043b\u044f \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u043e \u0447\u0430\u0441\u0442\u044f\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \n        l = [(0,0),(0,1),(0,2),(1,0),(1,1),(1,2),(2,0),(2,1),(2,2)]   \n    \n        pred_labels = []\n\n\n        for x,y in l:\n            #\u043e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \n            img_cropped = image[:,:,x*IMAGE_WIDTH:(x+1)*IMAGE_WIDTH,y*IMAGE_WIDTH:(y+1)*IMAGE_WIDTH] \n            #\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0434\u043b\u044f \u043d\u0435\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \n            out = model(img_cropped)\n\n            for label,score in zip(out[0]['labels'],out[0]['scores']):\n\n                if score > 0.70: #\u0434\u0430\u043d\u043d\u044b\u0439 \u0442\u0440\u0435\u0448\u0445\u043e\u043b\u0434 \u0442\u043e\u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u0442\u044c \n                    #\u0435\u0441\u043b\u0438 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432\u044b\u0448\u0435 \u043f\u043e\u0440\u043e\u0433\u0430 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u043f\u0440\u0435\u0434\u0438\u043a\u0448\u0435\u043d \n                    pred_labels.append(get_reversed_class(label.item()))\n\n        pred_labels = \" \".join([j for j in pred_labels])\n\n        return pred_labels\n\n    def c_sort(sub_li):\n\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n\n        cards = {\n            1:'VI',\n            2:'MA',\n            3:'EX', \n            4:'PC', \n            5:'ST',\n            6:'UY'\n                }\n        return cards[number]\n\n\n    preds = {}\n    imgs_name = df.image.unique().tolist()\n\n\n\n    #\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n    for images,image_name in zip(dataset,df.image):\n\n          if mode == \"Val\":\n              \n              image_name = images[2][0] #\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c id \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n              images = images[0][0] #\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0441\u0430\u043c\u0443 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443\n              images = images.unsqueeze(0).to(device)\n          \n          if mode == \"Test\": \n\n              images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n            \n\n          with torch.no_grad():\n              outputs = model(images)\n\n              outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n        \n              \n          preds[image_name] = outputs\n          \n\n    labels_pred = []\n    names_pred = []\n        \n    \n    \n    # \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439, \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0438 c\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\n    for img in imgs_name:\n\n        _temp_boxes = []\n        _temp_label = []\n        _temp_confidence = []\n\n        out = preds[img]\n        \n        for box,label,score in zip(out[0]['boxes'],out[0]['labels'],out[0]['scores']):\n\n            _temp_boxes.append(box.cpu().detach().numpy().tolist())\n            _temp_label.append(label.cpu().detach().numpy().tolist())\n            _temp_confidence.append(score.cpu().detach().numpy().tolist())\n\n            for index, _ in enumerate(_temp_boxes):\n\n                _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n                _temp_boxes[index].append(_temp_confidence[index])\n            \n\n        _temp_boxes = c_sort(_temp_boxes)\n        names_pred.append(img)\n        payment = \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD])\n     \n\n        if mode == \"Test\" and len(payment) == 0:\n\n            labels_pred.append(tackle_img(img))\n\n        else:\n\n            labels_pred.append(payment)\n          \n    d = {'image': names_pred, 'payment': labels_pred}\n\n    df = pd.DataFrame(data=d)\n        \n        \n    return df ","781d3e49":"import matplotlib\nmatplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import clear_output","d3eebb9c":"def visualise(history,train_history,valid_history):\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u0442\u0440\u0435\u0439\u043d \u043b\u043e\u0441\u0441 \u043e\u0442 \u0431\u0430\u0447\u0430,\u0442\u0440\u0435\u0439\u043d \u043b\u043e\u0441\u0441 \u043e\u0442 \u044d\u043f\u043e\u0445\u0438 \n    \u0438 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u043b\u0435\u0432\u0435\u043d\u0448\u0442\u0435\u0439\u043d\u0430 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043e\u0442 \u044d\u043f\u043e\u0445\u0438). \n    \"\"\"\n      \n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 8))\n\n    clear_output(True)\n    ax[0].plot(history, label='train loss')\n    ax[0].set_xlabel('Batch')\n    ax[0].set_title('Train loss')\n\n    if len(train_history) != 0:\n        ax[1].plot(train_history, label='general train history')\n        ax[1].set_xlabel('Epoch')\n        ax[1].set_title('train loss')\n    if len(valid_history) != 0:\n        ax[2].plot(valid_history, label='general valid history')\n        ax[2].set_xlabel('Epoch')\n        ax[2].set_title('Valid lev dist')\n\n    plt.legend()\n            \n    plt.show()","a80714c6":"def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n    \"\"\"\n    \u0412\u043e\u0440\u043c\u0430\u043f \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u0430. \n    \"\"\"\n\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) \/ warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)","373efbb2":"import tqdm\nimport os\n\n\ndef train(model, optimizer, data_loader, device, epoch,train_history,valid_history,print_freq=20,warmup=True):\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f 1 \u044d\u043f\u043e\u0445\u0438. \n    \"\"\"\n    model.train()\n    train_loss = [0 for i in range(len(data_loader))]\n    cnt = 0 \n\n    warmup_scheduler = None \n    if epoch == 0 and warmup == True: #\u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u0439 \u044d\u043f\u043e\u0445\u0435 \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u043c \u0432\u043e\u0440\u043c\u0430\u043f \n\n          warmup_factor = 1. \/ 1000\n          warmup_iters = min(1000, len(data_loader) - 1)\n          warmup_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n\n    for batch in tqdm.tqdm(data_loader):\n        \n      targets = batch[1]\n      images = batch[0]\n     \n      images = list(image.to(device) for image in images)\n      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n      loss_dict = model(images, targets)\n  \n      losses = sum(loss for loss in loss_dict.values())\n      train_loss[cnt] = losses\n\n      optimizer.zero_grad()\n      losses.backward()\n      optimizer.step()\n      \n      if cnt % print_freq == 0 : # \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u0436\u0434\u044b\u0435 (print_freq) \u0431\u0430\u0442\u0447\u0435\u0439 \n\n          visualise(train_loss,train_history,valid_history)\n          print(\"\\n Training...\")\n          print(\"Epoch \",epoch+1)\n\n\n      cnt += 1\n\n      if warmup_scheduler is not None: #\u0434\u0435\u043b\u0430\u0435\u043c \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u043a\u0430\u0436\u0434\u044b\u0439 \u0431\u0430\u0442\u0447 \n\n          warmup_scheduler.step()\n\n    \n    lr_scheduler.step()\n    \n\n    return sum(train_loss)\/len(train_loss)","8837fc3b":"max_lev = np.inf \ndist_freq = 1\ntrain_losses = []\nlev_distance = []\nsave_every_epoch = False\n\n\nfor epoch in range(N_ITER):\n    \n    cur_train_loss = train(model, optimizer, dataloader_train, device, epoch,train_losses,lev_distance,print_freq=10,warmup=True)\n    train_losses.append(cur_train_loss)\n    \n    print(\"\\n Validate error: \")\n    \n    preds_val = get_prediction(dataloader_valid,valid_set,mode=\"Val\",THRESHOLD=THRESHOLD,model=model,device=device)\n    cur_lev = levenshtein(preds_val, valid_set)\n    lev_distance.append(cur_lev)\n\n    if save_every_epoch: #\u043c\u043e\u0436\u043d\u043e \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u0430\u0436\u0434\u0443\u044e \u044d\u043f\u043e\u0445\u0443,\u043d\u043e \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \n      \n         \n        torch.save(model.state_dict(), MODEL_PATH)\n    \n    \n    if (cur_lev <= max_lev): #\u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \n        \n      max_lev = cur_lev\n      torch.save(model.state_dict(), BEST_MODEL_PATH)","1de8317f":"model_best = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 7\nin_features = model_best.roi_heads.box_predictor.cls_score.in_features\n\nmodel_best.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel_best.load_state_dict(torch.load(BEST_MODEL_PATH)) # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \nmodel_best = model_best.to(device)","af1159e8":"def get_threshold(max_thresh=1.0,min_thresh=0.5,step=0.02):\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u0448\u0445\u043e\u043b\u0434\u0430\n    \"\"\"\n    \n    thresh_num = int((max_thresh - min_thresh)\/step + 1)\n    thresholds = np.linspace(min_thresh,max_thresh,thresh_num)\n    levs = []\n\n    for thresh in tqdm.tqdm(thresholds):\n    \n        THRESHOLD = thresh\n        preds_val = get_prediction(dataloader_valid,valid_set,mode=\"Val\",THRESHOLD = thresh,model=model_best,device=device)\n        cur_lev = levenshtein(preds_val, valid_set)\n        levs.append(cur_lev)\n\n    plt.grid(True)\n    plt.plot(thresholds,levs)\n    plt.xlabel('Threshold')\n    \n    ind = levs.index(min(levs))\n    best_thresh = thresholds[ind]\n    \n    return best_thresh","86563915":"THRESHOLD = get_threshold() #\u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e\u0440\u043e\u0433","a522d1a7":"dataloader_test = DataLoader(\n    test_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n\npredictions = get_prediction(dataloader_test,test,mode=\"Test\",THRESHOLD=THRESHOLD,model=model_best,device=device) ","e3aad3c0":"predictions.to_csv(SUBMISSION_PATH, index=None)","0bfbedaf":"\u041a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0432\u0438\u0434\u0435\u0442\u044c, \u043a\u043b\u0430\u0441\u0441\u044b \u043f\u043b\u043e\u0445\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u0438\u0434\u0435\u0442\u0441\u044f \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0438\u0445 \u0431\u0430\u043b\u0430\u043d\u0441 ","d2c40163":"**\u041f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u0438\u043d\u043f\u0443\u0442\u0430 \u0432 \u0432\u043e\u0440\u043a\u0438\u043d\u0433 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044e, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0442\u0443\u0434\u0430 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438**","c40696ce":"\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e, \u0435\u0441\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u0438\u0441\u0442\u0430\u043b\u044c\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438, \u0442\u043e \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0447\u0442\u043e-\u043d\u0438\u0431\u0443\u0434\u044c \u043e\u0442 \u0447\u0435\u0433\u043e \u0435\u0449\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u043d\u043e \u0432\u0440\u0443\u0447\u043d\u0443\u044e \u044d\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u0442\u044f\u0436\u0435\u043b\u043e,\u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0441\u044f \u043d\u0430 \u044d\u0442\u043e\u043c :(","f269b5fd":"\u041d\u0438\u0436\u0435 \u0435\u0441\u0442\u044c \u043a\u0440\u0430\u0442\u043a\u0438\u0435 \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u0438\u044f \u043a \u043a\u043e\u0434\u0443 ","6a34ee78":"\u042f \u0440\u0435\u0448\u0438\u043b \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u0447\u0435\u043c \u043e\u0448\u0438\u0431\u0430\u0435\u0442\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0437\u0430\u043c\u0435\u0442\u0438\u043b, \u0447\u0442\u043e \u043d\u0430 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u0445 \u0441 \u043c\u0430\u0441\u0442\u0435\u0440\u043a\u0430\u0440\u0434\u043e\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 2 \u043b\u0435\u0439\u0431\u043b\u0430 (\u0447\u0430\u0441\u0442\u044c \u0441 debit \u0438 \u043d\u0435\u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0441 \u0438\u043a\u043e\u043d\u043a\u043e\u0439 \u043c\u0430\u0441\u0442\u0435\u0440\u043a\u0430\u0440\u0434\u0430). \u041f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0432\u043e\u0442 \u0442\u0430\u043a: ","a524da57":"\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u0432\u0440\u0443\u0447\u043d\u0443\u044e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u0438\u043d \u043c\u0430\u0441\u0442\u0435\u0440\u043a\u0430\u0440\u0434 \u0438 \u0443\u0434\u0430\u043b\u0438\u043b \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438: ","962466ef":"## \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430","51833198":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","970516ee":"## \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n","bc0ae459":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n","68d24c13":"\u0415\u0441\u043b\u0438 \u043a\u043e\u0440\u043e\u0442\u043a\u043e:  \n* \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u0430\u043b \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d\u0430 \u0438 \u0434\u043e\u0431\u0430\u0432\u0438\u043b \u0441\u0432\u043e\u0438\u0445 \n* \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043b \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 (\u043f\u043e\u0432\u0435\u0440\u043d\u0443\u0442\u044b\u043c\u0438 \u0438 \u0437\u0430\u0431\u043b\u044e\u0440\u0435\u043d\u043d\u044b\u043c\u0438) \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438 \u0440\u0435\u0434\u043a\u0438\u0445 \u043b\u0435\u0439\u0431\u043b\u043e\u0432 (\u0432\u0441\u0435\u0445 \u043a\u0440\u043e\u043c\u0435 \u0432\u0438\u0437 \u0438 \u043c\u0430\u0441\u0442\u0435\u0440\u043a\u0430\u0440\u0434\u043e\u0432),\u0447\u0442\u043e\u0431\u044b \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \n* \u0442\u0430\u043a\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043b\u0435\u0439\u0431\u043b\u043e\u0432 (RandomCrop,RandomBrightnessContrast,RandomSizedBBoxSafeCrop, RandomRotate90,ToGray,Blur) \n* \u0434\u0435\u043b\u0438\u043b \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438 \u043f\u043e\u0434\u0430\u0432\u0430\u043b \u0432 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u043f\u043e \u0447\u0430\u0441\u0442\u044f\u043c, \u0435\u0441\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b\u0430 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 (\u0431\u044b\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b\u0430, \u0430 \u0431\u044b\u043b\u0438 \u0441 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c\u044e \u043d\u0438\u0436\u0435 \u043f\u043e\u0440\u043e\u0433\u0430, \u043d\u043e \u044f \u0434\u0435\u043b\u0438\u043b \u0438 \u0442\u0435 \u0438 \u0442\u0435) \n* \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u0430\u0434\u0430\u043c \u0441 \u0432\u043e\u0440\u043c\u0430\u043f\u043e\u043c \u0438 \u043b\u0440 \u0441\u043a\u0435\u043b\u0434\u0443\u0435\u0440\u043e\u043c \n* \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e fasterrcnn_resnet50_fpn \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c trainable_backbone_layers=None\n* \u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a 1333*1333 (\u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u043b\u044f faster rcnn \u0441\u0443\u0434\u044f \u043f\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043f\u0430\u0439\u0442\u043e\u0440\u0447\u0430) \n* \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043f\u043e\u0447\u0438\u0441\u0442\u0438\u043b \u0442\u0440\u0435\u0439\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442 (\u043d\u0438\u0436\u0435 \u043e\u0431\u044a\u044f\u0441\u043d\u044e \u043f\u043e\u0447\u0435\u043c\u0443) \n\n\u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0442\u0430\u043a\u0436\u0435 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0431\u0435\u043a\u0431\u043e\u0443\u043d\u044b \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438, \u043d\u043e \u0441\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0440\u043e\u0441\u0442\u0430 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u043d\u0438 \u043d\u0435 \u0434\u0430\u0432\u0430\u043b\u0438,\u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0441\u044f \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043f\u0430\u0439\u0442\u043e\u0440\u0447\u043e\u0432\u0441\u043a\u043e\u0439 fasterrcnn_resnet50_fpn. \u0411\u044b\u043b\u0430 \u0438\u0434\u0435\u044f \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c yolov4 \u0438 yolov5,\u043d\u043e \u0447\u0442\u043e-\u0442\u043e \u0440\u0443\u043a\u0438 \u0442\u0430\u043a \u0438 \u043d\u0435 \u0434\u043e\u0448\u043b\u0438 \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c. \n\n\u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0440\u0430\u0437\u043d\u044b\u0435 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u044b (sgd,adamw \u0438 \u0442\u0434),\u043d\u043e \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0441\u044f \u043d\u0430 \u0430\u0434\u0430\u043c\u0435 \u0431\u0435\u0437 \u0430\u043c\u0441\u0433\u0440\u0430\u0434\u0430.\n\n\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u0440\u0430\u0437\u043d\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438,\u043d\u043e \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0443\u0445\u0443\u0434\u0448\u0430\u043b\u0438 \u0441\u043a\u043e\u0440 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440,\u0441 HorizontalFlip \u0441\u0435\u0442\u044c \u043e\u0431\u0443\u0447\u0430\u043b\u0430\u0441\u044c \u043d\u0430 \u043e\u0442\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043a\u0430\u0440\u0442), \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u0438\u0445 \u0443\u0431\u0440\u0430\u0442\u044c.\n\n\u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0438 \u0431\u0435\u043a\u0431\u043e\u0443\u043d\u0430, \u043d\u043e \u044d\u0442\u043e \u0442\u043e\u0436\u0435 \u043d\u0435 \u0434\u0430\u043b\u043e \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0439.\n\n\u041d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043f\u043b\u044e\u0441 \u043f\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0443 \u0434\u0430\u0432\u0430\u043b \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0439 \u043f\u043e\u0440\u043e\u0433,\u043d\u043e \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u0442 \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043a \u0437\u0430\u043f\u0443\u0441\u043a\u0443 \u0435\u0433\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0434\u0431\u0438\u0440\u0430\u0442\u044c \u0437\u0430\u043d\u043e\u0433\u043e,\u0442\u043e \u043e\u043d \u0432\u044b\u0431\u0438\u0440\u0430\u043b\u0441\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043e\u0442 0.5 \u0434\u043e 1.0 \u0441 \u0448\u0430\u0433\u043e\u043c 0.02 \n\n\u0422\u0430\u043a\u0436\u0435 \u0435\u0441\u0442\u044c \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c\u044e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \u0414\u0435\u043b\u043e \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0441\u0438\u0434 \u0434\u043b\u044f \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439 albumentations, \u0442\u0435\u0440\u044f\u0435\u0442\u0441\u044f \u0441\u043c\u044b\u0441\u043b \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438\u0438, \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043e\u0434\u043d\u0443 \u0438 \u0442\u0443 \u0436\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0438 \u0442\u043e\u0439 \u0436\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043a\u0430\u0436\u0434\u0443\u044e \u044d\u043f\u043e\u0445\u0443, \u0447\u0442\u043e \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043a \u0443\u0445\u0443\u0434\u0448\u0435\u043d\u0438\u044e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0438\u0434\u0438\u043c\u043e, \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438). \u0411\u0435\u0437 \u0444\u0438\u043a\u0441\u0430\u0446\u0438\u0438 \u0441\u0438\u0434\u0430 \u0441\u043a\u043e\u0440 \u043c\u043e\u0436\u0435\u0442 \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c\u0441\u044f \u043d\u0430 +- 0.03 \u043d\u0430 \u043f\u0430\u0431\u043b\u0438\u043a\u0435 \u0438 \u043f\u0440\u0438\u0432\u0430\u0442\u0435.  \n","4a02f885":"\u041e\u0441\u043d\u043e\u0432\u0430\u043d\u043e \u043d\u0430 \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435"}}