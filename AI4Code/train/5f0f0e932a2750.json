{"cell_type":{"3a7f4821":"code","25c576c9":"code","c6b8b795":"code","15c9810d":"code","3245f69c":"code","7ae60063":"code","05e8589f":"code","3a28ba1b":"code","5ff0fd2e":"code","8462c363":"code","2b26da72":"code","84f5c48f":"code","858ed68f":"code","7d689e0a":"code","f8a696d6":"code","b6559ba4":"code","de3ef637":"code","7446ea03":"code","2fa66510":"code","04e256ee":"code","8e3e4f00":"code","0e834394":"code","451e1e81":"code","05eefca7":"code","919ecf88":"code","09c95a3f":"code","bb49e08c":"code","f4518caf":"code","5f5a34f7":"code","5971c857":"markdown","d670254c":"markdown","d54307ec":"markdown","37046ab5":"markdown","9e964e2e":"markdown","5db614ae":"markdown","4598d904":"markdown","1c7b8767":"markdown","44d02e76":"markdown","3552d354":"markdown","98d4f564":"markdown","18c1ca9f":"markdown","c86b43d4":"markdown","22971fdb":"markdown","ced57c27":"markdown","d26738fe":"markdown","d211c73a":"markdown","035c53db":"markdown","402f4ade":"markdown"},"source":{"3a7f4821":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom nltk.tokenize import word_tokenize","25c576c9":"train_df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_df.head()","c6b8b795":"labels = 'Disaster Tweets', 'Non-Disaster Tweets'\nsizes = np.array(train_df.target.value_counts())\/len(train_df)*100\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","15c9810d":"df_correlation = train_df.dropna()\ndf_correlation=df_correlation.drop(columns=['id','text'])\ndf_correlation['keyword']=df_correlation['keyword']=df_correlation['keyword'].astype('category').cat.codes\ndf_correlation['location']=df_correlation['location']=df_correlation['location'].astype('category').cat.codes\ncorr=df_correlation.corr()\nsns.heatmap(corr, vmax=0.8)\ncorr_values=corr['target'].sort_values(ascending=False)\ncorr_values=abs(corr_values).sort_values(ascending=False)\nprint(\"Correlation of keyword and location with target in ascending order\")\nprint(abs(corr_values).sort_values(ascending=False))","3245f69c":"train_df=train_df.drop(columns=['location','keyword','id'])\ntest_df=test_df.drop(columns=['location','keyword'])","7ae60063":"import re\n\ndef remove_URL(df):\n    for i in range(df.shape[0]):\n        df.text[i]=re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',df.text[i])\n        \nremove_URL(train_df)\nremove_URL(test_df)","05e8589f":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain_df['text']=train_df['text'].apply(lambda x: remove_emoji(x))\ntest_df['text']=test_df['text'].apply(lambda x: remove_emoji(x))","3a28ba1b":"VOCAB_SIZE=4500\nMAXLEN=40\ntokenizer=Tokenizer(VOCAB_SIZE,oov_token='<oov>', filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n')  # filtering special characters\n\ntokenizer.fit_on_texts(train_df.text)","5ff0fd2e":"def df_to_padded_sequences(df,tokenizer):\n    sequences=tokenizer.texts_to_sequences(df.text)                                              #text to sequence of integers\n    padded_sequences=pad_sequences(sequences,maxlen=MAXLEN, padding='post', truncating='post')  #padding\n    return padded_sequences\n\nX_train=df_to_padded_sequences(train_df,tokenizer)\nX_test=df_to_padded_sequences(test_df,tokenizer)","8462c363":"print('Original tweet: ',train_df.text[0])\nprint('Tokenized tweet: ',X_train[0])\nprint('Token to tweet: ',tokenizer.sequences_to_texts(X_train)[0])","2b26da72":"Y_train=train_df.target\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n\nprint('Training features shape: ',X_train.shape)\nprint('Validation features shape: ',X_val.shape)\n\nprint('Training labels shape: ', Y_train.shape)\nprint('Validation labels shape: ', Y_val.shape)","84f5c48f":"model1 = keras.Sequential([\n    keras.layers.Embedding(VOCAB_SIZE, 32,input_length=MAXLEN),\n    keras.layers.SpatialDropout1D(0.2),\n    keras.layers.Bidirectional(keras.layers.LSTM(16)),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])","858ed68f":"model1.summary()","7d689e0a":"EPOCHS=30\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_acc', \n    verbose=1,\n    patience=5,\n    mode='max',\n    restore_best_weights=True)\n\n\n\nmodel1.compile(loss=\"binary_crossentropy\",optimizer=keras.optimizers.RMSprop(1e-4), metrics=['acc'])\n\nhistory1 = model1.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=32, epochs=EPOCHS, callbacks = [early_stopping])","f8a696d6":"tokenizer=Tokenizer(oov_token='<oov>', filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n')  # filtering special characters\n\ntokenizer.fit_on_texts(train_df.text)\n\nvocab_length = len(tokenizer.word_index) + 1\nprint(vocab_length)","b6559ba4":"MAXLEN=40\n\ndef df_to_padded_sequences(df,tokenizer):\n    sequences=tokenizer.texts_to_sequences(df.text)                                              #text to sequence of integers\n    padded_sequences=pad_sequences(sequences,maxlen=MAXLEN, padding='post', truncating='post')  #padding\n    return padded_sequences\n\nX_train2=df_to_padded_sequences(train_df,tokenizer)\nX_test2=df_to_padded_sequences(test_df,tokenizer)","de3ef637":"Y_train2=train_df.target\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train2, Y_train2, test_size=0.2)\n\nprint('Training features shape: ',X_train2.shape)\nprint('Validation features shape: ',X_val2.shape)\n\nprint('Training labels shape: ', Y_train2.shape)\nprint('Validation labels shape: ', Y_val2.shape)","7446ea03":"embeddings_dictionary = dict()\nembedding_dim = 50\nglove_file = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\nglove_file.close()\n\n\n\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\n","2fa66510":"model2 = keras.Sequential([\n    keras.layers.Embedding(vocab_length, 50, embeddings_initializer=Constant(embedding_matrix), input_length=MAXLEN, trainable=False),\n    keras.layers.SpatialDropout1D(0.2),\n    keras.layers.Bidirectional(keras.layers.LSTM(16)),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])","04e256ee":"model2.summary()","8e3e4f00":"EPOCHS=30\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_acc', \n    verbose=1,\n    patience=5,\n    mode='max',\n    restore_best_weights=True)\n\n\n\nmodel2.compile(loss=\"binary_crossentropy\",optimizer=keras.optimizers.RMSprop(1e-4), metrics=['acc'])\n\nhistory2 = model2.fit(X_train2, Y_train2, validation_data=(X_val2, Y_val2), batch_size=32, epochs=EPOCHS, callbacks = [early_stopping])","0e834394":"loss1, acc1 = model1.evaluate(X_val,Y_val)\nloss2, acc2 = model2.evaluate(X_val2,Y_val2)\nprint(\"Val_acc model1: \",acc1)\nprint(\"Val_acc model2: \",acc2)","451e1e81":"acc = history1.history['acc']\nval_acc = history1.history['val_acc']\n\nloss = history1.history['loss']\nval_loss = history1.history['val_loss']\n\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(history1.epoch, acc, label='Training Accuracy')\nplt.plot(history1.epoch, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy ')\n\nplt.subplot(1, 2, 2)\nplt.plot(history1.epoch, loss, label='Training Loss')\nplt.plot(history1.epoch, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n","05eefca7":"def plot_cm(labels, predictions, p=0.5): \n  cm = confusion_matrix(labels, predictions > p)\n  cm_sum = np.sum(cm, axis=1, keepdims=True)\n  cm_perc = cm \/ cm_sum.astype(float) * 100\n  annot = np.empty_like(cm).astype(str)\n  nrows, ncols = cm.shape\n  for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n  plt.figure(figsize=(5,5))\n  sns.heatmap(cm, annot=annot, fmt=\"\",cmap=\"YlGnBu\")\n  plt.title('Confusion matrix')\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('Non-Disaster Tweet Detected (True Negatives): ', cm[0][0])\n  print('Disaster Tweet Incorrectly Detected (False Positives): ', cm[0][1])\n  print('Disaster Tweet Missed (False Negatives): ', cm[1][0])\n  print('Disaster Tweet Detected (True Positives): ', cm[1][1])\n  print('Total Disaster Tweet: ', np.sum(cm[1]))\n  print('Total Non-Disaster Tweet: ', np.sum(cm[0]))","919ecf88":"val_prediction=model1.predict(X_val)\nplot_cm(Y_val, val_prediction)","09c95a3f":"Y_test=model1.predict(X_test)\nprint(Y_test)","bb49e08c":"Y_test = [int(i>0.5) for i in Y_test]","f4518caf":"submission_dataframe = pd.DataFrame({\"id\" : test_df['id'], \"target\" : Y_test})\nsubmission_dataframe.to_csv(\"submission.csv\", index = False)","5f5a34f7":"submission_dataframe","5971c857":"The more occurent a word the smaller the encoded integer, 2 ('the') being the most occurent word. A word is encoded as 1 if the word is absent of the dictionnary.","d670254c":"Source of the previous cell: https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm#EDA-and-Preprocessing","d54307ec":"<font size=\"+3\" color=\"black\"><b> Visualizing training results <\/b><\/font><br> <a id=\"3\"><\/a>","37046ab5":" <font size=\"+2\" color=\"black\"><b> 1.3 Building and training the model <\/b><\/font><br> <a id=\"3\"><\/a>","9e964e2e":"<font  color=\"red\"><b> Results are very similar but they are a bit better on the first model so we will use this one for the rest of the notebook <\/b><\/font><br> <a id=\"3\"><\/a>","5db614ae":" <font size=\"+2\" color=\"black\"><b> 2.3 Building and training the model <\/b><\/font><br> <a id=\"3\"><\/a>\n We build an embedding_matrix with the Glove representation and will this matrix as our embedding layer (therefore we won't train it). We'll use 50D embedding representation.","4598d904":" <font size=\"+3\" color=\"black\"><b> Predicting on the test set with the first model and submitting the predictions <\/b><\/font><br> <a id=\"3\"><\/a>","1c7b8767":"Source of the previous cell:  https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","44d02e76":" <font size=\"+3\" color=\"darkmagenta\"><b> Disaster Tweets: very simple LSTM model with and without GloVe <\/b><\/font><br> <a id=\"3\"><\/a>\n","3552d354":"<font size=\"+3\" color=\"black\"><b> 1. Model 1: Embedding without GloVe <\/b><\/font><br> <a id=\"3\"><\/a>\n\n<font size=\"+2\" color=\"black\"><b> 1.1 Tokenization <\/b><\/font><br> <a id=\"3\"><\/a>\nWe will create a dictionnary of size 4500 using our training dataset. Each vector will have a length of 40 (using zero-padding).","98d4f564":" <font size=\"+2\" color=\"black\"><b> 1.2 Splitting the dataset into training and validation <\/b><\/font><br> <a id=\"3\"><\/a>","18c1ca9f":" <font size=\"+3\" color=\"black\"><b> Removing URLs and emojis from texts <\/b><\/font><br> <a id=\"3\"><\/a>","c86b43d4":"=> the correlation of keyword and location with target is extremely low, so we can safely remove them without losing information","22971fdb":"<font size=\"+2\" color=\"black\"><b> 2.1 Tokenization <\/b><\/font><br> <a id=\"3\"><\/a>\nWe'll use this time all the words for the vocabulary.","ced57c27":"=> Our training dataset is pretty balanced","d26738fe":"<font size=\"+3\" color=\"black\"><b> 2. Model 2: Embedding with GloVe <\/b><\/font><br> <a id=\"3\"><\/a>","d211c73a":" <font size=\"+2\" color=\"black\"><b> 2.2 Splitting the dataset into training and validation <\/b><\/font><br> <a id=\"3\"><\/a>","035c53db":" <font size=\"+3\" color=\"black\"><b> Visualising the training dataset <\/b><\/font><br> <a id=\"3\"><\/a>\n","402f4ade":" <font size=\"+3\" color=\"black\"><b> Checking the importance of 'keyword' and 'location' <\/b><\/font><br> <a id=\"3\"><\/a>\n"}}