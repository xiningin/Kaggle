{"cell_type":{"b1a02133":"code","eedbeb4e":"code","78df496e":"code","ca488a5a":"code","b565c6ec":"code","554188fd":"code","d40408a8":"code","e98d357a":"code","a3018605":"code","5da67263":"code","4afaba14":"code","6ba18da5":"code","13d66d29":"code","35914022":"code","30ad71bc":"code","71bfbb95":"code","5dea0c8a":"code","cbaa27c6":"code","776cb640":"code","fe75be8a":"code","a0c462b9":"markdown","5a0f9989":"markdown","909a085d":"markdown","3ac1afc4":"markdown","7abd493c":"markdown","24375417":"markdown"},"source":{"b1a02133":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\nfrom tensorflow import keras\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.optimizers import schedules\nimport math\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns","eedbeb4e":"# Some functions to help out with\ndef plot_predictions(test,predicted):\n    plt.plot(test, color='red',label='Actual Stock Price')\n    plt.plot(predicted, color='blue',label='Predicted Stock Price')\n    plt.title('Gamestop Stock Price Prediction')\n    plt.xlabel('Time')\n    plt.ylabel('Gamestop Stock Price')\n    plt.legend()\n    plt.show()\n\ndef return_rmse(test,predicted):\n    rmse = math.sqrt(mean_squared_error(test, predicted))\n    print(\"The root mean squared error is {}.\".format(rmse))","78df496e":"# First, we get the data\ndataset = pd.read_csv('..\/input\/gamestop-historical-stock-prices\/GME_stock.csv', index_col='date', parse_dates=['date'])\ndataset.head()","ca488a5a":"sns.heatmap(dataset.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","b565c6ec":"training_set = dataset[:'2016-12-31'].iloc[:,1:2].values\ntest_set = dataset['2017-01-01':].iloc[:,1:2].values","554188fd":"training_set","d40408a8":"test_set","e98d357a":"dataset[\"high_price\"]['2017-01-01':]","a3018605":"dataset[\"high_price\"][:'2016-12-31']","5da67263":"# We have chosen 'High' attribute for prices. Let's see what it looks like\ndataset[\"high_price\"][:'2016-12-31'].plot(figsize=(16,4),legend=True)\ndataset[\"high_price\"]['2017-01-01':].plot(figsize=(16,4),legend=True)\nplt.legend(['Training set (Before 2017)','Test set (2017 and beyond)'])\nplt.title('GameStop stock price')\nplt.show()","4afaba14":"# Scaling the training set\nsc = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = sc.fit_transform(training_set)","6ba18da5":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(60,3748):\n    X_train.append(training_set_scaled[i-60:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)","13d66d29":"# Reshaping X_train for efficient modelling\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))","35914022":"# The LSTM architecture\nregressor = Sequential()\n# First LSTM layer with Dropout regularisation\nregressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n# Second LSTM layer\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n# Third LSTM layer\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n# Fourth LSTM layer\nregressor.add(LSTM(units=50))\nregressor.add(Dropout(0.2))\n# The output layer\nregressor.add(Dense(units=1))\n\n# Compiling the RNN\nregressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n# Fitting to the training set\nregressor.fit(X_train,y_train,epochs=50,batch_size=32)","30ad71bc":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"high_price\"][:'2016-12-01'],dataset[\"high_price\"]['2017-01-01':]),axis=0)\n","71bfbb95":"inputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values\ninputs","5dea0c8a":"inputs = inputs.reshape(-1,1)\ninputs","cbaa27c6":"inputs  = sc.transform(inputs)\ninputs","776cb640":"# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(60,1025):\n    X_test.append(inputs[i-60:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)","fe75be8a":"# Visualizing the results for LSTM\nplot_predictions(test_set,predicted_stock_price)","a0c462b9":"# Time Series Prediction","5a0f9989":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","909a085d":"Recurrent Neural networks are a class of Artificial Neural Networks, used for deep learning. They work on the principle of saving the output of a layer and feeding it back. Recurrent neural networks recognize data's sequential characteristics and use patterns to predict the next likely scenario. It works on similar principles of recursive algorithms \n","3ac1afc4":"# Recurrent Neural Networks: Simple Implementations\n","7abd493c":"See more about recurrent neural networks here:\nhttps:\/\/drive.google.com\/file\/d\/1AGZ0VJZScKXAl-xqb23zIBpjT8ub8OrC\/view?usp=sharing","24375417":"Recurrent Neural Networks are often used for Machine Translation, Speech recognition, Sentiment Analysis, Stock Price Forecasting, Image captioning. They are incorporated into popular applications such as Siri, voice search, and Google Translate. They are predominantly used in areas with NLP. Recurrent Neural Networks may be used for Computer Vision too, but Convolutional Neural Networks perform better for Image Processing.\n\nRecurrent Neural Networks, are different from neural networks due to their ability to backpropagate though iterations using timesteps. During forward propagation through an unrolled RNN, weights remain constant, until a difference between Actual and Predicted values is obtained. This is preceded by backpropagation through time or BPTT, which updates th weight through each layer, with respect to the input. Recurrent neural networks do not take into consideration the weight of these input vectors. In NLP, they are used with Word embeddings, or Encoders, which transforms words into vectors."}}