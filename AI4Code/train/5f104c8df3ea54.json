{"cell_type":{"51bb9b86":"code","9b3fe0b7":"code","69cc4529":"code","8411ff10":"code","0bc076af":"code","74d16e76":"code","03aad1f2":"code","3e529421":"code","e933279d":"code","61cbaa81":"code","1fe429ea":"code","7eb7d591":"code","482f2a58":"code","d1b73c75":"code","d4cf703b":"code","81bd8eca":"code","38afae65":"code","66f30799":"code","71c25e3a":"code","d5a31b24":"code","7e677cd6":"code","333bb1fd":"code","610f72b5":"code","f2c5b78f":"code","126d0ab8":"code","209dfa42":"code","731001df":"code","db067fee":"code","98ae4db9":"code","600bc768":"code","07adf21a":"code","5ad76e51":"code","971ea029":"code","3d8324cf":"code","bbe6a7f3":"code","67bb2d1b":"code","6924cc51":"code","66a61850":"code","663bfa22":"code","41c3344e":"code","e71981f3":"code","6ac428e9":"code","d1b83618":"code","89f75ebe":"code","cccdc842":"code","45badc11":"code","12522f32":"code","8ea3003d":"code","76338b69":"code","b622bfe7":"code","d7bf2b3f":"markdown"},"source":{"51bb9b86":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set();\n\nimport datetime\nimport re\n\nimport umap","9b3fe0b7":"TRAIN = '\/kaggle\/input\/sf-crime\/train.csv.zip'\nTEST = '\/kaggle\/input\/sf-crime\/test.csv.zip'\n\ndf_train = pd.read_csv(TRAIN)\ndf_test = pd.read_csv(TEST)","69cc4529":"print('train data shape:{}'.format(df_train.shape))\nprint('test data shape:{}'.format(df_test.shape))","8411ff10":"print('train data columns:{}'.format(df_train.columns))\nprint('test data columns:{}'.format(df_test.columns))","0bc076af":"print('---train data null number---')\nprint(df_train.isnull().sum())\nprint('---test data null number---')\nprint(df_test.isnull().sum())","74d16e76":"df_train.dtypes","03aad1f2":"df_train.describe()","3e529421":"df_train.head()","e933279d":"cols_cat = df_train.columns[df_train.dtypes == 'object'].drop('Dates')","61cbaa81":"for col in cols_cat:\n    print(\"------------\" + col + \"------------\")\n    print(df_train[col].value_counts())","1fe429ea":"plt.figure(figsize=(20,10)) \n\nax = sns.countplot(x='Category', data=df_train, order = df_train['Category'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=60, ha=\"right\")\n\nplt.tight_layout()\nplt.show()","7eb7d591":"# \u65e5\u4ed8\u304b\u3089\u6708\u3001\u6642\u523b\u3001TimeGroup(\u671d\u663c\u6669\u533a\u5206)\u3092\u8ffd\u52a0\u3059\u308b\u95a2\u6570\ndef date_split(df):    \n    df[\"Dates\"] = pd.to_datetime(df[\"Dates\"])\n    df[\"Date\"] = df[\"Dates\"].dt.date\n    df[\"Year\"] = df[\"Dates\"].dt.year\n    df[\"Month\"] = df[\"Dates\"].dt.month\n    df[\"Day\"] = df[\"Dates\"].dt.day\n    df[\"Hour\"] = df[\"Dates\"].dt.hour\n    df[\"Minute\"] = df[\"Dates\"].dt.minute\n    df[\"Second\"] = df[\"Dates\"].dt.second\n\n    def func_cate(x):\n        if  x >= 3 and x < 11:  # \u671d\u306f\u30013\u6642\u304b\u308910\u664259\u5206\u307e\u3067\n            return 0\n        elif x >= 11 and x < 18: # \u663c\u306f\u300111\u6642\u304b\u308917\u664259\u5206\u307e\u3067\n            return 1\n        else:  # \u591c\u306f18\u6642\u304b\u308926\u664259\u5206\u307e\u3067\n            return 2\n\n    df['TimeGroup'] = df[\"Hour\"].apply(func_cate)\n    \n    return df","482f2a58":"df_train = date_split(df_train)\ndf_test = date_split(df_test)","d1b73c75":"df_train.dtypes","d4cf703b":"time_vs_cat = df_train.groupby(['Category', 'TimeGroup'], as_index=False).count()","81bd8eca":"time_vs_cat","38afae65":"time_vs_cat_pv = time_vs_cat.pivot(index='TimeGroup', columns='Category', values='Dates').fillna(0)","66f30799":"fig, ax = plt.subplots(figsize=(50, 5)) \nsns.heatmap(time_vs_cat_pv.apply(lambda x:x\/sum(x),axis=0), square=True, annot=True)","71c25e3a":"pt = pd.pivot_table(df_train.loc[:, ['Hour', 'Category']],index=\"Hour\",columns=\"Category\",aggfunc=len,fill_value=0)\npt.plot(figsize=(30,10))\nplt.legend(bbox_to_anchor=(1.01, 1.0), loc='upper left')","d5a31b24":"top10_cat_arr = ['LARCENY\/THEFT','OTHER OFFENSES','NON-CRIMINAL','ASSAULT','DRUG\/NARCOTIC','VEHICLE THEFT','VANDALISM','WARRANTS',\n                 'BURGLARY','SUSPICIOUS OCC']\npt = pd.pivot_table(df_train.loc[:, ['Hour', 'Category']],index=\"Hour\",columns=\"Category\",aggfunc=len,fill_value=0)\npt.loc[:, top10_cat_arr].plot(figsize=(30,10))\nplt.legend(bbox_to_anchor=(1.01, 1.0), loc='upper left')","7e677cd6":"plt.figure(figsize=(20, 10))\nsns.kdeplot(df_train.groupby('Date').count().iloc[:, 0], shade=True)\nplt.xlabel('Incidents')\nplt.ylabel('Density')","333bb1fd":"df_train.head()","610f72b5":"# Try Adversarial Validation from this cell\nprint(\"Train columns: \" + df_train.columns)\nprint(\"Test columns: \" + df_test.columns)","f2c5b78f":"# Drop Dates, Date, Descript, Resolution, Address\ndf_train.drop(['Dates', 'Date', 'Descript', 'Resolution', 'Address'], axis=1, inplace=True)\ndf_test.drop(['Dates', 'Date', 'Address'], axis=1, inplace=True)","126d0ab8":"print(\"Train columns: \" + df_train.columns)\nprint(\"Test columns: \" + df_test.columns)","209dfa42":"TARGET = 'Category'\nx_train = df_train.drop(TARGET, axis=1)\ny_train = df_train[TARGET]\n\nid_test = df_test['Id']\nx_test = df_test.drop('Id', axis=1)","731001df":"cat_cols = ['DayOfWeek', 'PdDistrict'] # Columns to be one-hot-encoded\nnum_cols = ['X', 'Y'] # Colmuns to be standardized","db067fee":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Standardized based on only training data\nscaler = StandardScaler()\nscaler.fit(x_train[num_cols])\nx_train[num_cols] = scaler.transform(x_train[num_cols])\nx_test[num_cols] = scaler.transform(x_test[num_cols])","98ae4db9":"# Add labels for adversarial validation\nx_train['IsTest'] = 0\nx_test['IsTest'] = 1","600bc768":"# Combine train data & test data, and one-hot-encoding by pandas.get_dummies\nx_all = pd.concat([x_train, x_test])\nx_all = pd.get_dummies(x_all, columns=cat_cols)","07adf21a":"x_all['IsTest'].value_counts()","5ad76e51":"x_all","971ea029":"y_all = x_all['IsTest']\nx_all = x_all.drop(['IsTest'], axis=1)","3d8324cf":"print(x_all.isnull().sum())\ny_all","bbe6a7f3":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nX_train_adv, X_valid_adv, y_train_adv, y_valid_adv = train_test_split(x_all, y_all, test_size=0.3, random_state=42, shuffle=True)\n\nmodel = lgb.LGBMClassifier(\n    n_estimators=1000,\n    random_state=42\n)\n\nmodel.fit(\n    X_train_adv,\n    y_train_adv,\n    eval_set=[(X_train_adv, y_train_adv), (X_valid_adv, y_valid_adv)],\n    eval_names=['train', 'valid'],\n    eval_metric='auc',\n    verbose=100)","67bb2d1b":"print(y_train_adv.value_counts())\nprint(y_valid_adv.value_counts())","6924cc51":"ax = lgb.plot_metric(model.evals_result_, metric='auc')\nplt.show()","66a61850":"# \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u306e\u7b97\u51fa (\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3067\u53d6\u5f97)\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,x_all.columns)), columns=['Value','Feature'])\n\nfeature_imp['Value'] = feature_imp['Value'] \/ feature_imp['Value'].sum()\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","663bfa22":"sns.countplot(x_train['Day'])","41c3344e":"sns.countplot(x_test['Day'])","e71981f3":"sns.countplot(x_train['Month'])","6ac428e9":"sns.countplot(x_test['Month'])","d1b83618":"sns.countplot(x_train['Year'])","89f75ebe":"sns.countplot(x_test['Year'])","cccdc842":"x_all_2 = x_all.drop(['Day', 'Month'], axis=1)","45badc11":"# Retry model fitting\nX_train_adv, X_valid_adv, y_train_adv, y_valid_adv = train_test_split(x_all_2, y_all, test_size=0.3, random_state=42, shuffle=True)\n\nmodel = lgb.LGBMClassifier(\n    n_estimators=1000,\n    random_state=42\n)\n\nmodel.fit(\n    X_train_adv,\n    y_train_adv,\n    eval_set=[(X_train_adv, y_train_adv), (X_valid_adv, y_valid_adv)],\n    eval_names=['train', 'valid'],\n    eval_metric='auc',\n    verbose=100)","12522f32":"ax = lgb.plot_metric(model.evals_result_, metric='auc')\nplt.show()","8ea3003d":"x_train = x_all_2[y_all==0]\nx_test = x_all_2[y_all==1]","76338b69":"## \u30e1\u30e2\u30ea\u3048\u3089\u30fc\n# um = umap.UMAP()\n# um.fit(x_train)","b622bfe7":"# # address\u524d\u51e6\u7406\u907a\u7523\n# df_train['Address'].str.contains('block', case=False)\n\n# address = df_train['Address'].value_counts()\n\n# address.index\n\n# add_arr = [re.split('of|\/', s.lower()) for s in np.array(address.index)]\n\n# add_arr[0][0]\n\n# target = 'block'\n# t_idx = add_arr[0][0].find(target)\n# print(t_idx)\n# add_arr[0][0][:t_idx]\n\n# target = 'block'\n# [[s[:s.find(target)] for s in s_list] for s_list in add_arr]\n\n# target = 'block|st|av|ln'\n# [[s[:re.search(target, s).start()] if re.search(target, s) is not None else s for s in s_list] for s_list in add_arr]\n\n# re.search(target, 'aaa') is not None\n\n# # Word2Vec Try\n# sentences = []\n# for s in df_train[\"Address\"]:\n#     sentences.append(s.split(\" \"))\n# address_model = gensim.models.Word2Vec(sentences, min_count=1)\n# encoded_address = np.zeros((df_train.shape[0], 100))\n# for i in range(len(sentences)):\n#     for j in range(len(sentences[i])):\n#         encoded_address[i] += address_model.wv[sentences[i][j]]\n#     encoded_address[i] \/= len(sentences[i])\n\n# encoded_address.shape","d7bf2b3f":"### Ref: Adversarial Validation\nhttps:\/\/www.acceluniverse.com\/blog\/developers\/2020\/01\/kaggleadversarial-validation.html\nhttps:\/\/qiita.com\/shota-imazeki\/items\/6f48c78edf0ce3b316e1"}}