{"cell_type":{"e4df9252":"code","72aadb14":"code","a2a9cdbe":"code","ce92d2e3":"code","764ad586":"code","5f47058b":"code","fdaed5b1":"code","077ce6f5":"code","74b7329d":"code","171eaa38":"code","20d106b6":"code","6e081819":"code","6f62e5e0":"code","46d3664d":"code","a45638ac":"code","6b971957":"code","a607d600":"code","d46e2359":"code","97f5377f":"code","76770ab9":"code","b907efaf":"code","052f1586":"code","55013753":"code","2a3bfd97":"code","12edb8b1":"code","a991663e":"code","1e5e6187":"code","da379338":"code","159aa7fc":"code","ad01cadd":"code","3ca8ceb9":"code","70556d6c":"code","3b00409e":"code","f714cfcf":"code","4ade8339":"code","793e4770":"code","2df8ec44":"code","d1c97833":"code","93aa7802":"code","47774b10":"code","8ddcdf08":"code","345c7ada":"code","8aa8ba7d":"code","82dc6a70":"code","afa528a6":"code","bf61167c":"code","6d40f614":"code","7cf49b38":"code","db900b06":"code","e180e660":"code","514edd5a":"code","741d3c06":"code","9f46410f":"code","65da538a":"code","763cfb09":"code","e7df9414":"code","27453e0e":"markdown","f680dec0":"markdown","b10ada1f":"markdown","a25b5567":"markdown","d2f0b365":"markdown","5c5f1721":"markdown","d2c94190":"markdown"},"source":{"e4df9252":"import numpy as np \nimport pandas as pd \nimport os\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D, LeakyReLU, PReLU\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom nltk.tokenize import TweetTokenizer\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras.callbacks import EarlyStopping\ninitializer = tf.keras.initializers.GlorotNormal()\ntf.__version__ # newest version\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nimport matplotlib.pyplot as plt\nfrom scipy.stats import spearmanr\nimport seaborn as sns\nimport transformers\nimport nltk\nimport re\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nplt.style.use('seaborn')","72aadb14":"import warnings\nwarnings.filterwarnings(\"ignore\")","a2a9cdbe":"data_path = '..\/input\/iba-ml1-final-project\/train.csv'\ndf = pd.read_csv(data_path)","ce92d2e3":"# fill the nan in features with ''(empty string)\ndf = df.fillna(' ')\n# When the text pre-processing steps are applied, it had a negative impact on the overall score. \n# That is the reason why I removed them.\n# concatenate\ndf['Reviews'] = df['Review_Title'] + ' ' + df['Review']","764ad586":"df.head(10)","5f47058b":"X_bow=df\ny_1=df[['Recommended']]\ny_2=df[['Rating']]","fdaed5b1":"# remove punctuation\nX_bow[\"Reviews\"] = X_bow['Reviews'].str.replace('[{}]'.format(string.punctuation), '')\n\n# lower-case everything\nX_bow['Reviews'] = X_bow['Reviews'].str.lower()\n\nX_bow['Reviews'] = X_bow['Reviews'].str.replace('[^\\w\\s]','')\n\nX_bow['Reviews'] = X_bow['Reviews'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\nX_bow['Reviews'] = X_bow['Reviews'].str.replace('\\d+', '')\n\nlemmatizer = WordNetLemmatizer()\nX_bow['Reviews'] = [lemmatizer.lemmatize(row) for row in X_bow['Reviews']]","077ce6f5":"X_train_1, X_test_1, y_train_1, y_test_1=train_test_split(X_bow, y_1, test_size=0.33, random_state=1234)\nX_train_2, X_test_2, y_train_2, y_test_2=train_test_split(X_bow, y_2, test_size=0.33, random_state=1234)","74b7329d":"shape_y_t=y_train_2.shape[0]","171eaa38":"y_train2 = np.zeros((shape_y_t, 5))\ny_train2[np.arange(shape_y_t), y_train_2['Rating']-1] = 1\ny_train2 = pd.DataFrame(y_train2,columns=['1','2','3','4','5'],dtype='int64')\ny_train2","20d106b6":"y_test2 = np.zeros((y_test_2.shape[0], 5))\ny_test2[np.arange(y_test_2.shape[0]), y_test_2['Rating']-1] = 1\ny_test2 = pd.DataFrame(y_test2,columns=['1','2','3','4','5'],dtype='int64')\ny_test2","6e081819":"max_features = 50000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train_1['Reviews'])\nlist_tokenized_train = tokenizer.texts_to_sequences(X_train_1['Reviews'])\n\nmaxlen = 200\nX_bow_train = pad_sequences(list_tokenized_train, maxlen=maxlen)","6f62e5e0":"max_features = 50000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_test_1['Reviews'])\nlist_tokenized_train = tokenizer.texts_to_sequences(X_test_1['Reviews'])\n\nmaxlen = 200\nX_bow_test = pad_sequences(list_tokenized_train, maxlen=maxlen)","46d3664d":"X_bow_test","a45638ac":"embed_size = 128\nmodel_bow_rating = Sequential()\nmodel_bow_rating.add(Embedding(max_features, embed_size))\nmodel_bow_rating.add(Bidirectional(LSTM(128, return_sequences = True)))\nmodel_bow_rating.add(GlobalMaxPool1D())\nmodel_bow_rating.add(Dense(20, activation=\"relu\"))\nmodel_bow_rating.add(Dense(5, activation=\"softmax\"))\nmodel_bow_rating.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhist=model_bow_rating.fit(X_bow_train,y_train2, batch_size=100, epochs=10, validation_data=(X_bow_test, y_test2))","6b971957":"loss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\nepochs = range(1,11)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","a607d600":"print(model_bow_rating.summary())\ntf.keras.utils.plot_model(model_bow_rating)","d46e2359":"embed_size = 128\nmodel_bow_rec = Sequential()\nmodel_bow_rec.add(Embedding(max_features, embed_size))\nmodel_bow_rec.add(Bidirectional(LSTM(128, return_sequences = True)))\nmodel_bow_rec.add(GlobalMaxPool1D())\nmodel_bow_rec.add(Dense(64, activation=\"relu\"))\nmodel_bow_rec.add(Dense(1, activation=\"sigmoid\"))\nmodel_bow_rec.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhist2=model_bow_rec.fit(X_bow_train,y_train_1, batch_size=100, epochs=10, validation_data=(X_bow_test, y_test_1))","97f5377f":"loss_train = hist2.history['loss']\nloss_val = hist2.history['val_loss']\nepochs = range(1,11)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","76770ab9":"print(model_bow_rec.summary())\ntf.keras.utils.plot_model(model_bow_rec)","b907efaf":"rectest=model_bow_rec.predict(X_bow_test)\nrattest=model_bow_rating.predict(X_bow_test)","052f1586":"rectest=rectest.round()\nrectest=rectest.astype('int64')\nrectest","55013753":"rattest=rattest.argmax(axis=1)+1\nrattest","2a3bfd97":"print('================Recommended================')\nprint(classification_report(y_true=y_test_1, y_pred=rectest))","12edb8b1":"print('================Rating================')\nprint(classification_report(y_true=y_test_2, y_pred=rattest, target_names=['Star 1', 'Star 2', 'Star 3', 'Star 4', 'Star 5']))","a991663e":"plt.figure(2)\nfpr1, tpr1, threshold1 = metrics.roc_curve(y_test_1, rectest)\nroc_auc1 = metrics.auc(fpr1, tpr1)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr1, tpr1, 'red', label = 'ROC AUC score = %0.2f' % roc_auc1)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","1e5e6187":"corr1, _ = spearmanr(y_test_1, rectest)\nprint('Spearmans correlation\/Recommendation: %.3f' % corr1)\ncorr2, _ = spearmanr(y_test_2, rattest)\nprint('Spearmans correlation\/Rating: %.3f' % corr2)","da379338":"voca_star_rew=(df['Review']).to_string()\nvoca_star_title=(df['Review_Title']).to_string()\nvoca_star_others=(df['Division']+' '+df['Department']+' '+df['Product_Category']).to_string()\nvoca_star=(df['Reviews']).to_string()","159aa7fc":"#voca_star_others","ad01cadd":"tknzr=TweetTokenizer()\nvoca_star=tknzr.tokenize(voca_star)\nVOCAB_SIZE=50000\nencoder_rewtit=tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder_rewtit.adapt(\n    voca_star, reset_state=True\n)\n\ntknzr=TweetTokenizer()\nvoca_star_rew=tknzr.tokenize(voca_star_rew)\nVOCAB_SIZE=30000\nencoder_rew=tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder_rew.adapt(\n    voca_star_rew, reset_state=True\n)\n\ntknzr=TweetTokenizer()\nvoca_star_title=tknzr.tokenize(voca_star_title)\nVOCAB_SIZE=10000\nencoder_tit=tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder_tit.adapt(\n    voca_star_title, reset_state=True\n)\n\ntknzr=TweetTokenizer()\nvoca_star_others=tknzr.tokenize(voca_star_others)\nVOCAB_SIZE=50000\nencoder_others=tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder_others.adapt(\n    voca_star_others, reset_state=True\n)","3ca8ceb9":"encoder_rewtit","70556d6c":"encoder_others","3b00409e":"df","f714cfcf":"X=df\ny_1=df[['Recommended']]\ny_2=df[['Rating']]","4ade8339":"X","793e4770":"X_train_1, X_test_1, y_train_1, y_test_1=train_test_split(X, y_1, test_size=0.33, random_state=1234)\nX_train_2, X_test_2, y_train_2, y_test_2=train_test_split(X, y_2, test_size=0.33, random_state=1234)","2df8ec44":"X_train_1","d1c97833":"y_train2 = np.zeros((y_train_2.shape[0], 5))\ny_train2[np.arange(y_train_2.shape[0]), y_train_2['Rating']-1] = 1\ny_train2 = pd.DataFrame(y_train2,columns=['1','2','3','4','5'],dtype='int64')","93aa7802":"y_test2 = np.zeros((y_test_2.shape[0], 5))\ny_test2[np.arange(y_test_2.shape[0]), y_test_2['Rating']-1] = 1\ny_test2 = pd.DataFrame(y_test2,columns=['1','2','3','4','5'],dtype='int64')","47774b10":"y_test2","8ddcdf08":"w_input_rewtit=keras.Input(shape=(), dtype=tf.string)\nw_input_rew=keras.Input(shape=(), dtype=tf.string)\nw_input_title=keras.Input(shape=(), dtype=tf.string)\nw_input_others=keras.Input(shape=(), dtype=tf.string)\n\nembed_rewtit=encoder_rewtit(w_input_rewtit)\nembed_rew=encoder_rew(w_input_rew)\nembed_tit=encoder_tit(w_input_title)\nembed_others=encoder_others(w_input_others)\n\nembed_rewtit=tf.keras.layers.Embedding(\n        input_dim=len(encoder_rewtit.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True)(embed_rewtit)\n\nembed_rew=tf.keras.layers.Embedding(\n        input_dim=len(encoder_rew.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True)(embed_rew)\n\nembed_tit=tf.keras.layers.Embedding(\n        input_dim=len(encoder_tit.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True)(embed_tit)\n\nembed_others=tf.keras.layers.Embedding(\n        input_dim=len(encoder_others.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True)(embed_others)\n\n\nsirius1=keras.layers.LSTM(64, kernel_initializer=initializer)(embed_rewtit)\nsirius1=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius1)\nsirius1_=keras.layers.GRU(256)(embed_rewtit)\nsirius1_=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius1_)\n\n#########\n\nsirius2=keras.layers.LSTM(64, kernel_initializer=initializer)(embed_rew)\nsirius2=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius2)  \nsirius2_=keras.layers.GRU(256)(embed_rew)\nsirius2_=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius2_)\n\n#########\n\nsirius3=keras.layers.LSTM(64, kernel_initializer=initializer)(embed_tit)\nsirius3=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius3)  \nsirius3_=keras.layers.GRU(256)(embed_tit)\nsirius3_=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius3_)\n\n#########\n\nsirius4=keras.layers.LSTM(64, kernel_initializer=initializer)(embed_others)\nsirius4=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius4)  \nsirius4_=keras.layers.GRU(256)(embed_others)\nsirius4_=keras.layers.Dense(64, activation=layers.PReLU(), kernel_initializer=initializer)(sirius4_)\n\n#########\n\nsirius_last=layers.concatenate([sirius1, sirius1_, sirius2, sirius2_, \n                            sirius3, sirius3_, sirius4, sirius4_])\n                        \nrec_pred=tf.keras.layers.Dense(1, activation='sigmoid', name='recc')(sirius_last)\nrating_pred=tf.keras.layers.Dense(5, activation='softmax', name='ratt')(sirius_last)\n","345c7ada":"modelnew = keras.Model(\n    inputs=[w_input_rew, w_input_rewtit, w_input_title, w_input_others], outputs=[rec_pred, rating_pred]\n)","8aa8ba7d":"modelnew.compile(loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True), \n                       tf.keras.losses.CategoricalCrossentropy(from_logits=True)],\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00080, beta_1=0.85, beta_2=0.8),\n              metrics=['accuracy'])","82dc6a70":"early_stop = EarlyStopping(patience=3,restore_best_weights=True,monitor='val_ratt_accuracy')","afa528a6":"X_train_rew=X_train_1['Review']\nX_train_rewtit=X_train_1['Reviews']\nX_train_title=X_train_1['Review_Title']\nX_train_others=X_train_1['Division']+' '+X_train_1['Department']+' '+X_train_1['Product_Category']","bf61167c":"X_test_rew=X_test_1['Review']\nX_test_rewtit=X_test_1['Reviews']\nX_test_title=X_test_1['Review_Title']\nX_test_others=X_test_1['Division']+' '+X_test_1['Department']+' '+X_test_1['Product_Category']","6d40f614":"history = modelnew.fit([X_train_rew, X_train_rewtit, X_train_title, X_train_others], [y_train_1, y_train2], epochs=20, validation_data=([X_test_rew, X_test_rewtit, X_test_title, X_test_others], [y_test_1, y_test2]), callbacks=early_stop)","7cf49b38":"modelnew.evaluate([X_train_rew, X_train_rewtit, X_train_title, X_train_others], [y_train_1, y_train2])","db900b06":"modelnew.evaluate([X_test_rew, X_test_rewtit, X_test_title, X_test_others], [y_test_1, y_test2])","e180e660":"[recom2, rating2]=modelnew.predict([X_test_rew, X_test_rewtit, X_test_title, X_test_others])","514edd5a":"recom2=recom2.round()\nrecom2=recom2.astype('int64')\nrating2=rating2.argmax(axis=1)+1","741d3c06":"print(modelnew.summary())\ntf.keras.utils.plot_model(modelnew)","9f46410f":"print('================Recommended================')\nprint(classification_report(y_true=y_test_1, y_pred=recom2))","65da538a":"print('================Rating================')\nprint(classification_report(y_true=y_test_2, y_pred=rating2, target_names=['Star 1', 'Star 2', 'Star 3', 'Star 4', 'Star 5']))","763cfb09":"plt.figure(3)\nfpr2, tpr2, threshold2 = metrics.roc_curve(y_test_1, recom2)\nroc_auc2 = metrics.auc(fpr2, tpr2)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr2, tpr2, 'red', label = 'ROC AUC score = %0.2f' % roc_auc2)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","e7df9414":"corr1, _ = spearmanr(y_test_1, recom2)\nprint('Spearmans correlation\/Recommendation: %.3f' % corr1)\ncorr2, _ = spearmanr(y_test_2, rating2)\nprint('Spearmans correlation\/Rating: %.3f' % corr2)","27453e0e":"# **Part 1**\n# **Bags of Words Model (Sequential)**","f680dec0":"# **Part 2**\n# **RNN (Functional API)**","b10ada1f":"### BoW Recommendation Model","a25b5567":"## Results are okay, but I will try XLNet for the final model.","d2f0b365":"## Results are not satisfactory. Now I am going to use Functional API for RNN model","5c5f1721":"### BoW Rating Model","d2c94190":"# *** The End ***"}}