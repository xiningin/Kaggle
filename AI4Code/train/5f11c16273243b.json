{"cell_type":{"b1fb0fbd":"code","d77a2dcd":"code","555cbbb9":"code","e15b9b1c":"code","c294fcd8":"code","15c82ec3":"code","82cc730c":"code","36302c17":"code","77ff58ed":"code","3c62a66c":"code","69a6d7aa":"code","44a51191":"code","6eb2b20e":"code","044e2cf2":"code","2b3103ee":"code","22cd86b9":"code","22a82ffa":"code","a09ac0f0":"code","1ad53e17":"code","cea94781":"code","27ddc5de":"code","04824a88":"code","ad76617e":"code","f99a8ec6":"code","d85eb131":"code","c86eb16d":"code","31ac86b2":"code","e00d9aa3":"code","511646bc":"code","c5f1ed84":"code","3d260614":"code","f5847bfc":"code","0bc9bd4a":"code","fecafea0":"code","59fd71d4":"code","3ad62920":"code","6099aab2":"code","41a490de":"code","0a2a013f":"code","3b16bc42":"code","30bd3466":"code","73028600":"code","bfca4da5":"code","f74d5538":"code","b4ea4d73":"code","fde6b586":"code","be4508e9":"code","1b457252":"code","cc566185":"code","80330995":"code","3c06cf81":"code","3b9b3119":"code","99ea15a5":"code","65a59159":"code","f342f71c":"code","0cff495b":"code","9f5dae42":"code","6ec5427c":"code","0822fe9a":"code","b5dec5af":"code","00e57e1e":"code","2f55e924":"code","b3d8e49b":"code","11389474":"code","2ba310cc":"code","71f702ff":"code","7e9b3258":"code","b9573531":"code","4be63143":"code","e2a1bb8f":"code","80aecfee":"code","c621e942":"code","297959ce":"code","6b9406fc":"code","3f1e53fb":"code","1a3a0caa":"code","c361a089":"code","c04a2078":"code","fd73d2d4":"code","89af95ee":"code","163bc7b9":"code","c3f9f2c7":"code","b278c88f":"code","fe8df47e":"code","8ecd1350":"code","97d313eb":"code","f9e27cc5":"code","b007fc96":"code","00dce84e":"code","c83304fe":"markdown","4758d1a8":"markdown","93fd57da":"markdown","49d38b50":"markdown","3d08ca8f":"markdown","4f57f942":"markdown","6c4b13fb":"markdown"},"source":{"b1fb0fbd":"#Now let's open it with pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport time\nimport random\n\n","d77a2dcd":"# Set up the Titanic csv file as a DataFrame\ntitanic_train = pd.read_csv('..\/input\/train.csv')\ntitanic_test = pd.read_csv('..\/input\/test.csv')\ntitanic_ytest = pd.read_csv('..\/input\/gender_submission.csv')","555cbbb9":"titanic_train.head()","e15b9b1c":"titanic_train.info()","c294fcd8":"# Actual replacement of the missing value using median value.\ntitanic_train = titanic_train.fillna((titanic_train.median()))\n","15c82ec3":"titanic_test.head()","82cc730c":"# Actual replacement of the missing value using median value.\ntitanic_test = titanic_test.fillna((titanic_test.median()))\n","36302c17":"titanic_test.info()","77ff58ed":"titanic_ytest.head()","3c62a66c":"titanic_ytest.info()","69a6d7aa":"\nresult = pd.merge(titanic_ytest,titanic_test, how='inner', on=['PassengerId'])\nresult=result.append(titanic_train)\nresult.head()\n\n","44a51191":"result.info()","6eb2b20e":"titanic=result","044e2cf2":"titanic.head()","2b3103ee":"titanic.isna().any()","22cd86b9":"# Actual replacement of the missing value using median value.\ntitanic = titanic.fillna((titanic.median()))\ntitanic.head()","22a82ffa":"titanic.isna().any()","a09ac0f0":"titanic.describe()","1ad53e17":"#Correlation with Quality with respect to attributes\ntitanic.corrwith(titanic.Survived).plot.bar(\n        figsize = (20, 10), title = \"Correlation with quality\", fontsize = 15,\n        rot = 45, grid = True)","cea94781":"## Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = titanic.corr()","27ddc5de":"corr.head()","04824a88":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","ad76617e":"titanic.columns","f99a8ec6":"#Assigning and dividing the dataset\nX = titanic.drop(columns=['Survived','Embarked','Cabin','Name','Ticket'],axis=1)\ny=titanic['Survived']","d85eb131":"X.isna().any()","c86eb16d":"y.head()","31ac86b2":"data=titanic.drop(columns=['Survived','Embarked','Cabin','Name','Ticket'],axis=1)\ndata.head()","e00d9aa3":"#Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Encoding the Dependent Variable\nlabelencoder_y = LabelEncoder()\n\nX['Sex'] = labelencoder_y.fit_transform(X['Sex'])\n\n\n","511646bc":"X.head()","c5f1ed84":"#Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Encoding the Dependent Variable\nlabelencoder_y = LabelEncoder()\n\ndata['Sex'] = labelencoder_y.fit_transform(data['Sex'])\n\ndata.head()\n","3d260614":"features_label = data.columns[1:]","f5847bfc":"features_label\n","0bc9bd4a":"#Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 0)\nclassifier.fit(X, y)\nimportances = classifier.feature_importances_\nindices = np. argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i+1, 30, features_label[i],importances[indices[i]]))","fecafea0":"plt.title('Feature Importances')\nplt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\nplt.xticks(range(X.shape[1]),features_label, rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","59fd71d4":"X_train=titanic_train.drop(columns=['Survived','Embarked','Cabin','Name','Ticket','PassengerId'],axis=1)\n","3ad62920":"#Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Encoding the Dependent Variable\nlabelencoder_y = LabelEncoder()\n\nX_train['Sex'] = labelencoder_y.fit_transform(X_train['Sex'])\n","6099aab2":"X_test=titanic_test.drop(columns=['Embarked','Cabin','Name','Ticket','PassengerId'],axis=1)\n","41a490de":"#Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Encoding the Dependent Variable\nlabelencoder_y = LabelEncoder()\n\nX_test['Sex'] = labelencoder_y.fit_transform(X_test['Sex'])\nX_test.head()","0a2a013f":"fit=titanic_train.Survived\ny_train=pd.DataFrame(fit)","3b16bc42":"fit1=titanic_ytest.Survived\ny_test=pd.DataFrame(fit1)\n","30bd3466":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train2 = pd.DataFrame(sc.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","73028600":"from sklearn.decomposition import PCA\npca = PCA(n_components = 4)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\npd.DataFrame(explained_variance)","bfca4da5":"#### Model Building ####\n\n### Comparing Models\n\n## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l1')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nprint(results)","f74d5538":"## Randomforest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","b4ea4d73":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Highest SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","fde6b586":"# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['KNN', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)\n","be4508e9":"# Fitting GradientBoosting to the Training set\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nclassifier = GradientBoostingClassifier(loss ='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0,\n                                   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                                   max_depth=3,\n                                   init=None, random_state=None, max_features=None, verbose=0)\nclassifier.fit(X_train, y_train)\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Gradient Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)\n","1b457252":"\n\n# Fitting ADABoosting to the Training set\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nclassifier = AdaBoostClassifier(base_estimator=None, n_estimators=200, learning_rate=1.0)\nclassifier.fit(X_train, y_train)\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['ADA Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)\n","cc566185":"#we took the highest accuracy model svm rbf classifier","80330995":"## K-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X= X_train, y = y_train,\n                             cv = 10)\nprint(\"SVM Classifier Accuracy: %0.2f (+\/- %0.2f)\"  % (accuracies.mean(), accuracies.std() * 2))","3c06cf81":"# Round 1: SVM tuning\nparameters = [\n              { 'C': [1,100],'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4]}]\n# Make sure classifier points to the RF model\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = classifier, \n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters","3b9b3119":"# Predicting Test Set\ny_pred = grid_search.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM tuned', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nresults","99ea15a5":"import matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n","65a59159":"# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()\n","f342f71c":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","0cff495b":"y_pred","9f5dae42":"pred =y_pred\npredict=pd.DataFrame(pred,columns=['Survived'])\npredict.info()","6ec5427c":"titanicpred=titanic_ytest.PassengerId\nID=pd.DataFrame(titanicpred)\nID.info()","0822fe9a":"RealPrediction=pd.concat ([ID, predict], sort=False,axis=1)\nRealPrediction.head()\n","b5dec5af":"RealPrediction.info()","00e57e1e":"# Save prediction_df to csv\n\n#RealPrediction.to_csv (r'D:\/test.csv', index = None, header=True)","2f55e924":"# Let's import what we'll need for the analysis and visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b3d8e49b":"# Let's first check gender\nsns.countplot('Sex',data=result)\nsns.set(rc={'figure.figsize':(6,6)})","11389474":"# Now let's seperate the genders by classes, remember we can use the 'hue' arguement here!\nsns.countplot('Pclass',data=result,hue='Sex')","2ba310cc":"# We'll treat anyone as under 16 as a child, and then use the apply technique with a function to create a new column\n\n\n# First let's make a function to sort through the sex \ndef male_female_child(passenger):\n    # Take the Age and Sex\n    age,sex = passenger\n    # Compare the age, otherwise leave the sex\n    if age < 16:\n        return 'child'\n    else:\n        return sex\n    \n\n# We'll define a new column called 'person', remember to specify axis=1 for columns and not index\nresult['person'] = result[['Age','Sex']].apply(male_female_child,axis=1)","71f702ff":"# Let's see if this worked, check out the first ten rows\nresult[0:10]","7e9b3258":"# Let's try the factorplot again!\nsns.countplot('Pclass',data=result,hue='person')","b9573531":"# Quick way to create a histogram using pandas\nresult['Age'].hist(bins=70)","4be63143":"# We could also get a quick overall comparison of male,female,child\nresult['person'].value_counts()","e2a1bb8f":"# Another way to visualize the data is to use FacetGrid to plot multiple kedplots on one plot\n\n# Set the figure equal to a facetgrid with the pandas dataframe as its data source, set the hue, and change the aspect ratio.\nfig = sns.FacetGrid(result, hue=\"Sex\",aspect=4)\n\n# Next use map to plot all the possible kdeplots for the 'Age' column by the hue choice\nfig.map(sns.kdeplot,'Age',shade= True)\n\n# Set the x max limit by the oldest passenger\noldest = result['Age'].max()\n\n#Since we know no one can be negative years old set the x lower limit at 0\nfig.set(xlim=(0,oldest))\n\n#Finally add a legend\nfig.add_legend()","80aecfee":"# We could have done the same thing for the 'person' column to include children:\n\nfig = sns.FacetGrid(result, hue=\"person\",aspect=4)\nfig.map(sns.kdeplot,'Age',shade= True)\noldest = result['Age'].max()\nfig.set(xlim=(0,oldest))\nfig.add_legend()","c621e942":"# Let's do the same for class by changing the hue argument:\nfig = sns.FacetGrid(result, hue=\"Pclass\",aspect=4)\nfig.map(sns.kdeplot,'Age',shade= True)\noldest = result['Age'].max()\nfig.set(xlim=(0,oldest))\nfig.add_legend()","297959ce":"# Let's get a quick look at our dataset again\nresult.head()","6b9406fc":"# First we'll drop the NaN values and create a new object, deck\ndeck = result['Cabin'].dropna()","3f1e53fb":"# Quick preview of the decks\ndeck.head()","1a3a0caa":"# So let's grab that letter for the deck level with a simple for loop\n\n# Set empty list\nlevels = []\n\n# Loop to grab first letter\nfor level in deck:\n    levels.append(level[0])    \n\n# Reset DataFrame and use factor plot\ncabin_df = DataFrame(levels)\ncabin_df.columns = ['Cabin']\nsns.countplot('Cabin',data=cabin_df,palette='winter_d')","c361a089":"# Redefine cabin_df as everything but where the row was equal to 'T'\ncabin_df = cabin_df[cabin_df.Cabin != 'T']\n#Replot\nsns.countplot('Cabin',data=cabin_df,palette='summer')","c04a2078":"# Now we can make a quick factorplot to check out the results, note the x_order argument, used to deal with NaN values\nsns.countplot('Embarked',data=result,hue='Pclass',order=['C','Q','S'])","fd73d2d4":"# Let's start by adding a new column to define alone\n\n# We'll add the parent\/child column with the sibsp column\nresult['Alone'] =  result.Parch + result.SibSp\nresult['Alone']","89af95ee":"# Look for >0 or ==0 to set alone status\nresult['Alone'].loc[result['Alone'] >0] = 'With Family'\nresult['Alone'].loc[result['Alone'] == 0] = 'Alone'\n\n# Note it's okay to ignore an  error that sometimes pops up here. For more info check out this link\nurl_info = 'http:\/\/stackoverflow.com\/questions\/20625582\/how-to-deal-with-this-pandas-warning'","163bc7b9":"# Let's check to make sure it worked\nresult.head()","c3f9f2c7":"# Now let's get a simple visualization!\nsns.countplot('Alone',data=result,palette='Blues')","b278c88f":"# Let's start by creating a new column for legibility purposes through mapping (Lec 36)\nresult[\"Survivor\"] = result.Survived.map({0: \"no\", 1: \"yes\"})\n\n# Let's just get a quick overall view of survied vs died. \nsns.countplot('Survivor',data=result,palette='Set1')","fe8df47e":"# Let's use a factor plot again, but now considering class\nsns.pointplot('Pclass','Survived',data=result)","8ecd1350":"# Let's use a factor plot again, but now considering class and gender\nsns.pointplot('Pclass','Survived',hue='person',data=result)","97d313eb":"# Let's use a linear plot on age versus survival\nsns.lmplot('Age','Survived',data=result)","f9e27cc5":"# Let's use a linear plot on age versus survival using hue for class seperation\nsns.lmplot('Age','Survived',hue='Pclass',data=result,palette='winter')","b007fc96":"# Let's use a linear plot on age versus survival using hue for class seperation\ngenerations=[10,20,40,60,80]\nsns.lmplot('Age','Survived',hue='Pclass',data=result,palette='winter',x_bins=generations)","00dce84e":"sns.lmplot('Age','Survived',hue='Sex',data=result,palette='winter',x_bins=generations)","c83304fe":"\n#Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 5)","4758d1a8":"# Model Training","93fd57da":"# Visualisation","49d38b50":"The data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.","3d08ca8f":"# Feature Engineering & Transformation","4f57f942":"# Model Evaluation","6c4b13fb":"# Titanic Prediction"}}