{"cell_type":{"46b0f936":"code","f30fffb8":"code","b41cea4a":"code","f474534b":"code","1d55b0c1":"code","00d3b5d2":"code","033648d4":"code","254f6943":"code","50edebc8":"code","9d5436d2":"code","1f1b7078":"code","d3a47859":"code","993ea5b4":"code","43b3cee4":"code","4a72c9d0":"code","67e48454":"code","522b5a2b":"code","fda7bb47":"code","97d5c5ce":"code","af2de16a":"code","1206af25":"code","eae8193d":"code","1191f89c":"code","5a5878e8":"code","6133e691":"code","0e3952cd":"code","b591c1a2":"code","eddc42c4":"code","d2a1c7ec":"code","49ed9aa8":"code","d48cff25":"code","54d0d5b8":"code","25045740":"code","2873b767":"code","bf9fa539":"code","a00baa09":"code","163fb65f":"code","a03ed59a":"code","8f5076c8":"code","3b281e44":"code","4c875811":"code","fc8f8692":"code","7e3e561b":"code","46db9d92":"markdown","407c20a4":"markdown","16962e8c":"markdown","f3881d41":"markdown","96e061f9":"markdown","fd7d8d36":"markdown","9a363a92":"markdown","44d5d461":"markdown","57cf8c6e":"markdown","96fad4b2":"markdown","0deb4a09":"markdown","fa1d3666":"markdown","066c3a92":"markdown","ab256526":"markdown","d0231fcf":"markdown"},"source":{"46b0f936":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f30fffb8":"X_full = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')","b41cea4a":"X_full.head()","f474534b":"X_test.head()","1d55b0c1":"SalePrice_outlier_conditional = X_full.SalePrice < 450000\n\nX_full_without_outliers = X_full[SalePrice_outlier_conditional ]","00d3b5d2":"OverallQual_Q1 = X_full.OverallQual.quantile(0.25)\nOverallQual_Q3 = X_full.OverallQual.quantile(0.75)\nOverallQual_IQR = OverallQual_Q3-OverallQual_Q1\nOverallQual_outlier_conditional =  ((X_full.OverallQual > OverallQual_Q1 - 1.5*OverallQual_IQR) & (X_full.OverallQual < OverallQual_Q3 + 1.5*OverallQual_IQR))","033648d4":"X_full_without_outliers = X_full_without_outliers[OverallQual_outlier_conditional]","254f6943":"GarageCars_Q1 = X_full.GarageCars.quantile(0.25)\nGarageCars_Q3 = X_full.GarageCars.quantile(0.75)\nGarageCars_IQR = GarageCars_Q3-GarageCars_Q1\nGarageCars_outlier_conditional =  ((X_full.GarageCars > GarageCars_Q1 - 1.5*GarageCars_IQR) & (X_full.GarageCars < GarageCars_Q3 + 1.5*GarageCars_IQR))","50edebc8":"X_full_without_outliers = X_full_without_outliers[GarageCars_outlier_conditional]","9d5436d2":"X = X_full_without_outliers.drop(['SalePrice'], axis=1)\nfeature_names = X_full_without_outliers.columns.values\ny = X_full_without_outliers.SalePrice","1f1b7078":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","d3a47859":"ordinal_features = ['ExterCond', 'BsmtQual','BsmtCond', 'HeatingQC', 'KitchenQual',\n                    'FireplaceQu','GarageQual', 'GarageCond','PoolQC']\nnominal_features= list(set(X.select_dtypes(include='object').columns.values) - set(ordinal_features))\nnumerical_features = X.select_dtypes(exclude='object').columns.values","993ea5b4":"numerical_transformer = SimpleImputer(strategy='median')\n\nnominal_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\nordinal_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n                transformers=[\n                    ('num', numerical_transformer, numerical_features),\n                    ('cat_nom', nominal_transformer, nominal_features),\n                    ('ord_nom', ordinal_transformer, ordinal_features)\n                ])\n","43b3cee4":"def depth_tests(max_depth):\n    pipe = Pipeline(steps=[\n            ('preprocessor', preprocessor),\n            ('minmaxscaler', MinMaxScaler()),\n            ('stdscaler',  StandardScaler()),\n            ('regressor', DecisionTreeRegressor(random_state=0, criterion='mse', max_depth=max_depth))\n\n    ])\n    scores = -1 * cross_val_score(pipe, X, y, cv=KFold(n_splits = 5), scoring='neg_mean_absolute_error')\n    return scores.mean()","4a72c9d0":"depth_scores = {}\ndepths = [2,3,7,9,10,12,13,14]\nfor depth in depths:\n    depth_scores[str(depth)]=depth_tests(depth)","67e48454":"depth_scores","522b5a2b":"SEED=0\ndecision_tree_regressor = DecisionTreeRegressor(random_state=SEED, criterion='mse', \n                                                max_depth=9, min_samples_leaf=32, min_samples_split=32)","fda7bb47":" pipe = Pipeline(steps=[\n            ('preprocessor', preprocessor),\n            ('minmaxscaler', MinMaxScaler()),\n            ('stdscaler',  StandardScaler()),\n            ('regressor', decision_tree_regressor)\n\n    ])","97d5c5ce":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=3)","af2de16a":"pipe.fit(X_train, y_train)","1206af25":"from sklearn import tree\nimport graphviz\n\ntree_graph = tree.export_graphviz(decision_tree_regressor, out_file=None)\ngraphviz.Source(tree_graph)","eae8193d":"val_predictions = pipe.predict(X_val)","1191f89c":"mean_squared_error(y_val, val_predictions, squared=False)","5a5878e8":"mean_absolute_error(y_val, val_predictions)","6133e691":"pd.DataFrame({'y_val': y_val,\n            'val_pred': val_predictions,\n             'abs_err': abs(val_predictions-y_val)}).describe()","0e3952cd":"from sklearn.model_selection import KFold","b591c1a2":"scores = -1 * cross_val_score(pipe, X, y, cv=KFold(n_splits = 5), scoring='neg_mean_absolute_error')","eddc42c4":"sorted(scores)","d2a1c7ec":"scores.mean()","49ed9aa8":"scores.std()","d48cff25":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold","54d0d5b8":"espaco_de_parametros = dict(\n    regressor__max_depth= [2,3,5,7,9,11,13],\n    regressor__min_samples_split= [32,64,128,256],\n    regressor__min_samples_leaf= [32,64,128,256])\n\nbusca_exaustiva = GridSearchCV(pipe,\n                              espaco_de_parametros,\n                              cv = KFold(n_splits=5))","25045740":"busca_exaustiva.fit(X, y)","2873b767":"resultados = pd.DataFrame(busca_exaustiva.cv_results_)\nresultados.head()","bf9fa539":"busca_exaustiva.best_estimator_","a00baa09":"busca_exaustiva.best_params_","163fb65f":"espaco_de_parametros = dict(\n    regressor__max_depth= [2,3,5,7,9,11,13],\n    regressor__min_samples_split= [32,64,128,256],\n    regressor__min_samples_leaf= [32,64,128,256])\n\nbusca_randomizada = RandomizedSearchCV(pipe,\n                              espaco_de_parametros,\n                              cv = KFold(n_splits=5),\n                                        n_iter=20,\n                                      random_state=11)\n\n\nbusca_randomizada.fit(X, y)","a03ed59a":"resultados = pd.DataFrame(busca_randomizada.cv_results_)\nresultados.head()","8f5076c8":"busca_randomizada.best_params_","3b281e44":"test_predictions = pipe.predict(X_test)","4c875811":"test_predictions","fc8f8692":"output = pd.DataFrame({'Id': X_test.index,\n                      'SalePrice': test_predictions})\noutput.set_index('Id', inplace=True)\noutput.to_csv('submission.csv')","7e3e561b":"output.describe()","46db9d92":"## Pipeline","407c20a4":"## Validando o modelo","16962e8c":"## Hiperpar\u00e2metros em uma \u00e1rvore de\u00a0decis\u00e3o","f3881d41":"### Pesquisa manual do *max_depth*","96e061f9":"### Randomized Search\n\nEmbora a pesquisa exaustiva em grade seja muito usada para otimiza\u00e7\u00e3o de par\u00e2metros, outros m\u00e9todos de pesquisa pode ser mais favor\u00e1veis. A busca randomizada implementa uma pesquisa aleat\u00f3ria de par\u00e2metros, em que cada configura\u00e7\u00e3o \u00e9 amostrada a partir de uma distribui\u00e7\u00e3o de poss\u00edveis valores de par\u00e2metro.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#randomized-parameter-optimization","fd7d8d36":"## Recapitulando","9a363a92":"## Generalizando para os dados de teste","44d5d461":"## Criando arquivo de submiss\u00e3o","57cf8c6e":"## T\u00e9cnicas de ajuste de hiperpar\u00e2metros\n\nDuas abordagens o ajuste de par\u00e2metros s\u00e3o fornecidas no Scikit-Learn: para determinados valores, GridSearchCVconsidera exaustivamente todas as combina\u00e7\u00f5es de par\u00e2metros, enquanto RandomizedSearchCVpode amostrar um determinado n\u00famero de candidatos em um espa\u00e7o de par\u00e2metro com uma distribui\u00e7\u00e3o especificada.","96fad4b2":"## O que s\u00e3o hiperpar\u00e2metros e para que\u00a0servem?\n\nHiperpar\u00e2metros s\u00e3o par\u00e2metros que n\u00e3o s\u00e3o aprendidos diretamente pelos modelos, portanto precisam ser imputados antes do treinamento. No Scikit-Learn, eles s\u00e3o passados como argumentos para o construtor das classes dos modelos (chamados no contexto do scikit-learn de estimadores). Os valores escolhidos para os hiperpar\u00e2metros influenciam na qualidade dos modelos, na sua capacidade de aprender com o treinamento e de generalizar para dados n\u00e3o vistos anteriormente.","0deb4a09":"## Remo\u00e7\u00e3o de Outliers","fa1d3666":"### Grid Search\n\nA pesquisa de grade gera exaustivamente candidatos a partir de uma grade de valores de par\u00e2metros previamente especificados. \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#exhaustive-grid-search","066c3a92":"## Carregando os dados","ab256526":"# Ajuste de Hiperpar\u00e2metros em Modelos de Machine\u00a0Learning\n\nEsse \u00e9 o quinto epis\u00f3dio da s\u00e9rie M\u00e3os \u00e0 Obra Cientista de Dados. Come\u00e7amos aprendendo Machine Learning do zero, passamos pelo pr\u00e9-processamento dos dados, an\u00e1lise descritiva de vari\u00e1veis, tipos de valida\u00e7\u00e3o, e chegamos \u00e0 etapa de ajuste de hiperpar\u00e2metros de um modelo.\nAgora discutiremos a import\u00e2ncia de explorar o espa\u00e7o de par\u00e2metros de um modelo e como eles podem influenciar a qualidade do nosso modelo de acordo com a m\u00e9trica de avalia\u00e7\u00e3o, no nosso caso, o erro quadr\u00e1tico m\u00e9dio e o erro absoluto m\u00e9dio. Continuaremos usando o Scikit-Learn, mais especificamente, trabalharemos com Grid Search e Random Search, utilizando tamb\u00e9m a valida\u00e7\u00e3o cruzada que aprendemos no \u00faltimo epis\u00f3dio.","d0231fcf":"O arquivo exportado ser\u00e1 submetido \u00e0 plataforma."}}