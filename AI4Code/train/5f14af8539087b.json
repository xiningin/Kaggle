{"cell_type":{"a29548f3":"code","00bea38d":"code","e551c030":"code","006e0ee3":"code","dfad1467":"code","f58556b9":"code","49556b46":"code","afafc955":"code","645c8c7c":"code","b5b40599":"code","fda20b29":"code","5f11551e":"code","ec5d42d0":"code","12c981cd":"code","9d1cc929":"code","b4b702af":"code","66e26b0a":"code","c8f6a954":"code","6ec98e87":"code","c7f376a6":"code","d3e4be34":"code","4ecb7a9a":"code","af98bc63":"code","aabff813":"code","3fd8968d":"code","2a6456a9":"code","bbf59ac6":"code","8046ab87":"code","dc79ed50":"code","2854bcd9":"code","2e8259b5":"code","373a0ccd":"code","2f8943ff":"code","8e4800d4":"code","1ed5aef7":"code","0995e2d9":"code","efb563cc":"code","28fb8433":"code","5098ba3e":"code","6d3299a6":"code","baedee86":"code","fad67231":"code","02555c50":"code","3a899a24":"code","a5b19d93":"code","521f4064":"code","f5d90257":"code","ead1575d":"code","7af7622b":"code","2b9b7a39":"code","b90d6482":"code","15dc9ada":"code","5f9188b1":"code","9552a242":"code","296cd89b":"code","f401efbe":"code","a13b73a3":"code","f73dca55":"code","43701494":"code","50014563":"code","6ffcb3b4":"markdown","d3f646c8":"markdown","27489e45":"markdown","20b6a6c1":"markdown","f4255c7d":"markdown","c8d79207":"markdown","22f14b07":"markdown","4e6cef06":"markdown","eba44cda":"markdown","361a673b":"markdown","f23f583b":"markdown","b51e4cb8":"markdown","0c7672e7":"markdown","573b3199":"markdown","f87a2c7c":"markdown","4fb0d076":"markdown","00863efd":"markdown","d7ddfc0b":"markdown","bc8b1959":"markdown","9651aa71":"markdown","1ba0ae2d":"markdown","9c6dccb5":"markdown","b84f7987":"markdown","bf3cb10c":"markdown","45c04916":"markdown","7d2441dc":"markdown","9376019c":"markdown","f04ff58f":"markdown","af991107":"markdown","6627aa40":"markdown","389e013f":"markdown","646c7d06":"markdown","4f94fab6":"markdown","7f38fd1a":"markdown","a2641c1d":"markdown","01778e97":"markdown","5a22cac1":"markdown","9aeb3597":"markdown","8f8b0ab5":"markdown","180b297a":"markdown","fe08220a":"markdown","26dc844a":"markdown","d555a9f9":"markdown","0edf6dfd":"markdown","09df31fd":"markdown","26e3a1f0":"markdown","a96268fa":"markdown","bc377a83":"markdown","2316a344":"markdown","1518d5a1":"markdown","05bf9c25":"markdown","c71e633e":"markdown","7f6f2ab5":"markdown","914d5910":"markdown","97230841":"markdown","664e6e30":"markdown","e2b96543":"markdown","919e4065":"markdown","eb560625":"markdown","9845cb61":"markdown","d28fe120":"markdown","5af6e940":"markdown","6e79fcab":"markdown","660620d0":"markdown","db7ef455":"markdown","7ccb4223":"markdown","f90a289f":"markdown"},"source":{"a29548f3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nstyle.use(\"seaborn-darkgrid\")","00bea38d":"data = pd.read_csv(r\"..\/input\/melanoma-tumor-size-prediction-machinehack\/Train.csv\") #insert file path into the \"read_csv\" function\ndata.head()","e551c030":"list(data.columns)","006e0ee3":"data.isna().sum()","dfad1467":"data.shape","f58556b9":"data.info()","49556b46":"x = data.drop(\"tumor_size\", axis=1)\ny = data[\"tumor_size\"]","afafc955":"x.head()","645c8c7c":"y.head()","b5b40599":"from sklearn.model_selection import train_test_split","fda20b29":"xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.1)","5f11551e":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()","ec5d42d0":"model.fit(xtrain, ytrain)","12c981cd":"ypred = model.predict(xtest)","9d1cc929":"from sklearn.metrics import mean_absolute_error","b4b702af":"mean_absolute_error(ytest, ypred)","66e26b0a":"ytest","c8f6a954":"from sklearn.tree import DecisionTreeRegressor","6ec98e87":"regression_tree = DecisionTreeRegressor()","c7f376a6":"regression_tree.fit(xtrain, ytrain)","d3e4be34":"ypred_train = regression_tree.predict(xtrain)\nmean_absolute_error(ytrain, ypred_train)","4ecb7a9a":"ypred_test = regression_tree.predict(xtest)\nmean_absolute_error(ytest, ypred_test)","af98bc63":"another_regression_tree = DecisionTreeRegressor(max_depth=15)","aabff813":"another_regression_tree.fit(xtrain, ytrain)","3fd8968d":"ypred_train = another_regression_tree.predict(xtrain)\nmean_absolute_error(ytrain, ypred_train)","2a6456a9":"ypred_test = another_regression_tree.predict(xtest)\nmean_absolute_error(ytest, ypred_test)","bbf59ac6":"train=pd.read_csv(\"..\/input\/airline-passenger-satisfaction\/train.csv\")\ntest=pd.read_csv(\"..\/input\/airline-passenger-satisfaction\/test.csv\")","8046ab87":"list(train.columns)","dc79ed50":"train.head()","2854bcd9":"test.head()","2e8259b5":"train.drop([\"Unnamed: 0\", \"id\"], axis=1, inplace=True)\ntest.drop([\"Unnamed: 0\", \"id\"], axis=1, inplace=True)","373a0ccd":"train.isna().sum()","2f8943ff":"test.isna().sum()","8e4800d4":"train.dropna(inplace=True)\ntest.dropna(inplace=True)","1ed5aef7":"train.info()","0995e2d9":"categoricals = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']","efb563cc":"for col in categoricals:\n    print(train[col].value_counts())\n    print(\"\\n\")","28fb8433":"sns.countplot(x=train[\"satisfaction\"])\nplt.show()","5098ba3e":"plt.figure(figsize=(20,7))\nsns.heatmap(pd.crosstab(train[\"Class\"], train[\"satisfaction\"], normalize=\"index\"),\n            annot = True, cmap=\"Blues\", fmt=\".2f\", annot_kws={\"fontsize\":20})\nplt.show()","6d3299a6":"train[\"Class\"] = train[\"Class\"].map({\"Eco\":0, \"Eco Plus\":1, \"Business\":2})\ntest[\"Class\"] = test[\"Class\"].map({\"Eco\":0, \"Eco Plus\":1, \"Business\":2})","baedee86":"train = pd.get_dummies(train, drop_first=True)\ntest = pd.get_dummies(test, drop_first=True)","fad67231":"train.head()","02555c50":"target = \"satisfaction_satisfied\"\nxtrain, ytrain = train.drop(target, axis=1), train[target]\nxtest, ytest = test.drop(target, axis=1), test[target]","3a899a24":"xtrain.head()","a5b19d93":"ytrain.head()","521f4064":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nxtrain_array = ss.fit_transform(xtrain)\nxtrain = pd.DataFrame(xtrain_array, columns=xtrain.columns)\n\nxtest_array = ss.transform(xtest)\nxtest = pd.DataFrame(xtest_array, columns=xtrain.columns)","f5d90257":"xtrain.head()","ead1575d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","7af7622b":"logreg = LogisticRegression(solver='liblinear')","2b9b7a39":"logreg.fit(xtrain, ytrain)","b90d6482":"y_predicted_testing = logreg.predict(xtest)\naccuracy_score(ytest, y_predicted_testing)","15dc9ada":"y_predicted_training = logreg.predict(xtrain)\naccuracy_score(ytrain, y_predicted_training)","5f9188b1":"from sklearn.tree import DecisionTreeClassifier\nmytree = DecisionTreeClassifier()","9552a242":"mytree.fit(xtrain, ytrain)","296cd89b":"y_predicted_training = mytree.predict(xtrain)\naccuracy_score(ytrain, y_predicted_training)","f401efbe":"y_predicted_testing = mytree.predict(xtest)\naccuracy_score(ytest, y_predicted_testing)","a13b73a3":"my_other_tree = DecisionTreeClassifier(max_depth=19)","f73dca55":"my_other_tree.fit(xtrain, ytrain)","43701494":"y_predicted_training = my_other_tree.predict(xtrain)\naccuracy_score(ytrain, y_predicted_training)","50014563":"y_predicted_testing = my_other_tree.predict(xtest)\naccuracy_score(ytest, y_predicted_testing)","6ffcb3b4":"All of these variables shown up (other than tumor_size) are the features that we will use for prediction.  \ntumor_size is the variable we will try predicting.","d3f646c8":"I'm sure that explaining this in paragraphs isn't the most useful thing, but I just couldn't make a tutorial with nothing but code in it.  \nThere are plenty of video courses\/tutorials\/explanations that explain the details of decision trees really well so make sure you check them out.","27489e45":"A regression tree basically groups points according to their y value.  \nIn other words, points that have similar y values (such as 1.2, 1.4 and 1.1) would be in the same group.  \nWhen predicting the y value of a sample\/point\/observation\/example\/row, the tree puts it in a group of points that have similar x values, then assigns to it the mean y value of that group.  ","20b6a6c1":"We can improve the testing performance by telling the tree not to grow too long\/deep.  \nGrowing too long\/deep means the tree is making more cuts in the feature space (the space defined by the features x1, x2, etc..). By making more cuts the tree adjusts better to the data that it's training on.  \nWe don't want the tree to adjust too well to the training data because then it wouldn't perform that well on data that isn't perfectly similar (the testing data).  \nThe \"max_depth\" parameter sets the maximum number of successive nodes \/ the maximum branch length \/ the maximum number of questions to be asked.","f4255c7d":"# Supervised Learning Tutorial\n### This notebook is associated with a set of workshops I presented for the Google Developper Student Club of Enet'Com, Tunisia.  \n### The workshops are recorded and can be found on the club's youtube channel:  \n### [GDSC ENET'COM Machine Learning Sessions](https:\/\/www.youtube.com\/playlist?list=PLI-fIRiuD3-CbET8Ri3Ey2HH_Kov04Ezv)\n### The workshops are in the Tunisian language\/dialect.","c8d79207":"We have to split our data into train and test sets.  \nThis means that we will not show all patients\/examples to our model.  \nWe will let the model learn from a certain number of patients\/examples, then we will test on the ones it didn't see.  \nThis is to check if the model makes good predictions for patients that it hasn't seen before.  \nWe need to give the model many examples to learn from, because the more examples you give it, the better it learns.  \nSo we show it most of our data (usually, about 70% to 90%) and keep a small number of examples\/patients for testing.","22f14b07":"# Regression","4e6cef06":"![](https:\/\/miro.medium.com\/max\/1248\/1*Ixw2RgVQ4syGyCD6ArLcZw.png)","eba44cda":"X: variables\/features that we will use to make predictions  \nY: The variable that will try to predict correctly","361a673b":"![](https:\/\/www.mathworks.com\/matlabcentral\/mlc-downloads\/downloads\/submissions\/52003\/versions\/3\/screenshot.jpg)","f23f583b":"![](https:\/\/i.stack.imgur.com\/FgdfC.jpg)","b51e4cb8":"The learning algorithm of the classification tree (which is learning what questions to ask \/ what splits to make) is almost the same as the regression tree algorithm.  \nInstead of evaluating a split\/question by comparing y values of different groups (which is basically what the regression tree does), a split\/question is evaluated by how much it sets the 2 classes apart.","0c7672e7":"In this section we will build models that aim to predict whether a client is satisfied or neutral\/dissatisfied.  \nThe following is preprocessing and exploratory analysis. It isn't very commented since it isn't the focus point of this tutorial.","573b3199":"# Classification Tree","f87a2c7c":"The following is an example of a tree that classifies samples \/ data points that belong to three classes instead of 2 ie [0, 1, 2] instead of [0, 1]","4fb0d076":"![](https:\/\/static.wixstatic.com\/media\/02b811_5df05513ffd4487d843bb401dfa5e0cb~mv2.png\/v1\/fit\/w_309%2Ch_118%2Cal_c\/file.png)","00863efd":"Of course, sometimes you can't separate the two classes (y=1 and y=0) perfectly with a straight line.  \nThe following image on the right is more realistic than the one on the left.","d7ddfc0b":"![](https:\/\/www.researchgate.net\/profile\/Hieu-Tran-17\/publication\/333457161\/figure\/fig3\/AS:763959762247682@1559153609649\/Linear-Regression-model-sample-illustration.ppm)","bc8b1959":"The MAE value is 3.99 or basically 4.  \nIs that a small error or a big one?","9651aa71":"To evaluate how good a split\/question is, we calculate the 'Gini Impurity' that corresponds to it.  \nThe Gini Impurity is calculated using the following formula:","1ba0ae2d":"![](https:\/\/www.researchgate.net\/publication\/335786324\/figure\/fig1\/AS:802479209971712@1568337361258\/Logistic-regression-and-linear-regression.jpg)","9c6dccb5":"The classification tree is more flexible than logistic regression.  \nThis is because in reality the data points (in this case clients) aren't always distributed in such a way that you can separate them with a straight line.  \nThe following image is an example where a tree is more suitable than logistic regression.","b84f7987":"## Regression Tree","bf3cb10c":"![](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/linear-regression-vs-logistic-regression.png)","45c04916":"![](https:\/\/1.bp.blogspot.com\/-kL42RjXdOEc\/XMELxXVMe3I\/AAAAAAAABRw\/mx2RoIheodwWj0CPAqg9chwXJmpOyPyJQCLcBGAs\/s1600\/Loss_Functions.PNG)","7d2441dc":"#### Notes:\n* The tree achieved better results than Linear Regression. This almost always the case.  \n* The tree has practically zero error on the training set. This is because it kept splitting until almost every point is in a group by itself.\n* The gap between training and testing scores is larger for the tree than it is for linear regression.\n* This is because the tree focused too much on the training data that it basically memorized it. Thus, when we tested it with data that it hasn't seen\/memorized it failed to produce good results.\n* This is like memorizing 10 math exercices and hoping you get something similar in the test vs understanding exercices.","9376019c":"Logistic Regression is not a regression model but rather a classification model.  \nIt constructs an S-shaped curve (in case of one variable x) that represents the probability of y=1.","f04ff58f":"![](http:\/\/miro.medium.com\/max\/1838\/1*uLHXR8LKGDucpwUYHx3VaQ.png)","af991107":"Let's take a look at the real values of tumor sizes:","6627aa40":"In this section, we will implement models that aim to predict the melanoma tumor size of a patient based on other attributes.","389e013f":"The predict function tells the model to make predictions.","646c7d06":"A decision tree predicts by asking certain questions then making judgements based on the answer.  \nTraining (fitting) consists of learning what are the questions that should be asked, and what judgement to make in each possible answer.  \nThe picture below illustrates this. It shows that the tree consists of nodes, each node being a question. Based on the answer, an appropriate y-value is decided. It also shows the number of samples\/rows that correspond to that answer.","4f94fab6":"As always the tree is very adjusted to the training data but less to the testing data.  \nHowever it still gave better results than logistic regression.  \nIf we tell the tree not to grow too long, then it should get better testing results.","7f38fd1a":"![](https:\/\/www.statology.org\/wp-content\/uploads\/2020\/11\/tree3.png)","a2641c1d":"##### Note:\nIn this tutorial, we will only focus on supervised learning.  \nExploratory Analysis and Preprocessing should take a considerable portion of any machine learning project, but they are out of the scope of this tutorial.  \nWe did cover EDA and processing though in this detailed tutorial: https:\/\/www.kaggle.com\/mahmoudlimam\/eda-processing-tutorial  \nMake sure you check it out!","01778e97":"If we represent our model the following way: Y = b1.X1 + b2.X2 + ... + bn.Xn + C,  \nthen the fit function allows the model to be find the best coefficient bi for each variable Xi, and the best constant term C.  \nIn other words, it gives them values that produce an error that is as small as possible.","5a22cac1":"![](https:\/\/camo.githubusercontent.com\/fa25e1f53a14c7839d4659edf09c1e9b7a8fcad93727a5eea63b2b4b65454164\/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313831382f302a61585578764e7556695f2d716335566b2e706e67)","9aeb3597":"Now, we need to test our model.  \nWe will evaluate the predictions it makes for the patients it hasn't seen before.  ","8f8b0ab5":"![](https:\/\/www.researchgate.net\/publication\/313816842\/figure\/fig2\/AS:962701719257088@1606537384464\/A-decision-tree-with-its-decision-boundary-Each-node-of-the-decision-tree-represents-a.gif)","180b297a":"In case of 2 variables, the S-shaped curve becomes an S-shaped surface (but kind of flat..) which we can't draw here since that would require 3 dimensions.  \nThe decision boundary becomes a straight line though which we can draw.  \nThis line tries to separate points into 2 classes.","fe08220a":"![](https:\/\/www.learnbymarketing.com\/wp-content\/uploads\/2016\/03\/linear-sep-decision-tree.png)","26dc844a":"We will be doing a multiple linear regression.  \nOur features \/ Independent variables (X1, X2, etc..) will be the columns of the dataset, other than tumor size which is Y.","d555a9f9":"*But how does the tree learn which questions to ask?*  \nWell, questions separate points, which represent clients in our case.  \nPoints that represent the \"yes\" answer will be in a group, while ones that correspond to the \"no\" answer will be in a different group.  \nIf points in a same group have very different y values then this question isn't useful to ask.  \nHowever if the question separates points into groups such that points in the same group have very similar y values then the question is good to ask.  \n**But how does the tree find the good questions?**  \nIt (kind of) asks many possible questions then picks the best ones.  \n**But what kind of questions can be asked?**  \nThe questions are usually inequalities related the values of features (the columns)  \nFor example, one question could be \"x1 > 1.2 ?\" and another could be \"x5 < 70 ?\".  \nEach question corresponds to a split in the feature space (which basically represents the x values).  \nFor example if we ask \"x2 < 23.1 ?\" then some points will be on the side of the space where x2 < 23.1 and others will be on the side where x2 >= 23.1.  \nThe following picture illustrates that.","0edf6dfd":"# Classification","09df31fd":"First, we read the data.  \n##### Note: \nIn the \"pd.read_csv\" function, you are supposed to insert the file path.  \nI inserted the file path i got from my kaggle environment.  \nIf you're working with Jupyter, you need to insert the path from your computer.  \nYou can simply find the file on your computer, go to properties, copy the Location\/Path, add the file name to it, and you're done.  \nYour file path should be something similar to: \"C:\\Users\\lenovo\\Downloads\\file_name.csv\" .  \nMake sure you add an \"r\" in the beginning of the string to avoid errors due to the \"\\\" character.  \nThus the final path you pass to the pd.read_csv function will be something like r\"C:\\Users\\lenovo\\Downloads\\file_name.csv\"","26e3a1f0":"Checking Missing Values:","a96268fa":"If we take patient number 6700, we can see that the size of the tumor is 0.88  \nOur model makes an average error of 4.2, so it will make a prediction around 0.88 + 4.2 = 5.08.  \n5.08 is different from 4.2, and thus our model is making a BIG error for this patient.  \nIf you do look at the other patients you can see that an error of 4.2 will cause predictions to be really bad and far from the truth.  \n##### What does this mean?  \nIt means that a linear regression model can not make good predictions on this dataset.  \nWe can also say that the equation Y = b1.X1 + b2.X2 + ... + bn.Xn + C is not a good approximation of reality in this case.  \n##### What do we do now?  \nWell, we'll be trying another model: the regression tree","bc377a83":"Understand why that formula is good for evaluating splits\/questions is out of the scope of this tutorial since it isn't dedicated to understanding the mathematical details of algorithms but rather the main concepts.  \nHowever, there are plenty of resources on the internet that explain these details really well.","2316a344":"### \u0627\u0644\u062d\u0645\u062f \u0644\u0644\u0647 \u0627\u0644\u0630\u064a \u0628\u0646\u0639\u0645\u062a\u0647 \u062a\u062a\u0645 \u0627\u0644\u0635\u0627\u0644\u062d\u0627\u062a","1518d5a1":"![](https:\/\/raw.githubusercontent.com\/valoxe\/image-storage-1\/master\/blog-machine-learning\/decision-tree-random-forest\/2.png)","05bf9c25":"![](https:\/\/media.springernature.com\/original\/springer-static\/image\/prt%3A978-1-4899-7687-1%2F18\/MediaObjects\/978-1-4899-7687-1_18_Part_Fig1-717_HTML.gif)","c71e633e":"The logistic curve corresponds to the logistic function, which corresponds to the formula in the following image.","7f6f2ab5":"![](https:\/\/www.saedsayad.com\/images\/LogReg_1.png)","914d5910":"Example of a tree that is too short and doesn't separate classes very well (can be improved by allowing the tree to be longer\/deeper) and a tree that is much deeper.","97230841":"![](https:\/\/sphweb.bumc.bu.edu\/otlt\/MPH-Modules\/BS\/R\/R5_Correlation-Regression\/MultipleLinearRegression-Plane.png)","664e6e30":"**But how does the tree measure the \"goodness\" of a question\/split?**  \nWell, the question divides points into two groups.  \nThe tree algorithm calculates the average error in each group.  \nMeaning it calculates the error that corresponds to every point then calculates the average.  \nThe error is the distance between the y value of that point and the average y value of the group.  \nSometimes we the square of that distance as the error, sometimes not. But both are valid.  \nAfter calculating the average error for each group, the average error of the question is calculated which is the average between the two groups.  \nThis is done by multiplying each group error by the number of points in that group, adding and dividing by the total number of points.  \nThe following image shows examples of errors that we can use. The mean absolute error is the average distance between points and their average.","e2b96543":"## Linear Regression","919e4065":"(Number of patients, number of variables):","eb560625":"The same as a regression tree but predicts classes instead of numerical values.","9845cb61":"![](https:\/\/miro.medium.com\/max\/569\/0*Yclq0kqMAwCQcIV_.jpg)","d28fe120":"I've decided to encode \"Class\" labels\/values based on their relationship with satisfaction:","5af6e940":"The x value that corresponds to y=0.5 is called the \"decision boundary\".  \nIt separates points based on their x value into two groups.  \nOne group corresponds to Y=1 and the other corresponds to y=0.","6e79fcab":"![](https:\/\/media-exp1.licdn.com\/dms\/image\/C4E22AQEUYF4-CUIZkw\/feedshare-shrink_800\/0\/1636572090596?e=1639612800&v=beta&t=V0JxLlmQL5ej6acmoJ1y3Yx9bwXLjXMoY17wvgaBAxE)","660620d0":"We can see that the training error increased because the tree isn't perfectly fit to the training data now, but the testing results are better.","db7ef455":"## Logistic Regression","7ccb4223":"### \u0628\u0633\u0645 \u0627\u0644\u0644\u0647","f90a289f":"Now we need to compare the values that the model predicted with the real values (which the model doesn't know).  \nTo do this, we can calculate the \"mean absolute error\" between the real values and the predicted values.  \nThe Mean Absolute Error simply means \"On average, how much error does the model make when predicting the tumor size of the patient?\""}}