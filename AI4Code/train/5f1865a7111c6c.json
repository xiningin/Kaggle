{"cell_type":{"584c3843":"code","72f008fa":"code","c8805f54":"code","20539897":"code","f383e466":"code","f9a09081":"code","5c43c2f2":"code","0faa66a0":"code","467b839b":"code","e9afa21f":"code","337eeb37":"code","c901a307":"code","26c5a623":"code","0d2ed5a7":"code","0f5627e8":"code","f569baff":"code","4547714c":"code","645a0843":"code","b1e10be6":"code","f6616fe7":"code","f83113a8":"code","cc36cc68":"code","7b773eb1":"code","25f3db87":"code","ee9132ff":"code","fc55c5ef":"code","1427f0b0":"code","2ce02bf8":"code","1066eed4":"code","8f18adeb":"code","fbbda90c":"code","3e7d4bc6":"code","11968d25":"code","9eda6e41":"code","50feca7b":"code","f45c699b":"code","e872ea1e":"code","e8253c24":"code","40ed5f76":"code","4740aaa6":"code","0c09714f":"code","2026385a":"code","db191a91":"code","64a6db43":"code","b9c697eb":"code","91a9e8c0":"code","f307a223":"code","769cfac0":"code","d4114a6f":"code","246f310a":"code","ed6ead2d":"code","12cd5f77":"code","d98cdec9":"code","13639221":"code","ca8fb1f0":"code","76ac61e9":"code","fa69bcba":"code","fed05cd1":"code","f0840b52":"code","75ecb7f8":"code","019eae19":"code","bd93d448":"code","5b5ea8df":"code","ae3efe1a":"code","2b462d81":"markdown","72982995":"markdown","9b49b1f7":"markdown","91dec09c":"markdown","a2b03b35":"markdown","077e6207":"markdown","70f562fa":"markdown","23bc40ae":"markdown","34b538a5":"markdown","c77fa559":"markdown","bf1746fd":"markdown","e2f54bb2":"markdown","c0be037c":"markdown","20a1d7fc":"markdown","ec1f9031":"markdown","11b40bb1":"markdown","9dbe6265":"markdown","ef94cb48":"markdown","55d50167":"markdown","37445a7b":"markdown","d9789afa":"markdown","d0766fee":"markdown","529f8491":"markdown","2269f854":"markdown","1745b204":"markdown","4ef53481":"markdown","7be2bef8":"markdown","81675356":"markdown","e5ea67ee":"markdown","a16f62da":"markdown","c1f496ae":"markdown","629b02eb":"markdown","9c322f9a":"markdown","3a54751d":"markdown","9ce5e829":"markdown","3df472cf":"markdown","b9108b6f":"markdown","5622cb99":"markdown","55423460":"markdown","9f809e69":"markdown","c2c1545c":"markdown","5e5be499":"markdown","5e22dc02":"markdown","822dabb2":"markdown","07674fa3":"markdown","8ee8f0c4":"markdown","3925388f":"markdown","78216191":"markdown","a9df20fc":"markdown","7ee523ae":"markdown","32fadca1":"markdown","c41cfb17":"markdown","b99b8793":"markdown","3f752d55":"markdown","10769ae9":"markdown","fb6af40d":"markdown","11334f14":"markdown","73a4457f":"markdown","9eadde71":"markdown","1dde7d5a":"markdown","1408d900":"markdown","d4b13a9f":"markdown","7ecdd5fb":"markdown","b4094f2d":"markdown","f50b78d2":"markdown","67ea247c":"markdown","01ad07b7":"markdown","e4de5b21":"markdown"},"source":{"584c3843":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72f008fa":"# Read data\ndata = pd.read_csv(\"\/kaggle\/input\/deep-learning-az-ann\/Churn_Modelling.csv\")","c8805f54":"# First 10 rows of data\ndata.head(10)","20539897":"# Statistical infos on data\ndata.describe()","f383e466":"# Checking types and nulls\ndata.info()","f9a09081":"data.isnull().any()","5c43c2f2":"data.drop([\"RowNumber\",\"CustomerId\",\"Surname\"], axis=1, inplace=True)","0faa66a0":"# Check data\ndata.head()","467b839b":"def bar_plot(variable):\n    \"\"\"\n    Takes a variable as an input: For example \"Gender\"\n    Outputs a bar plot and counts value\n    \"\"\"\n    \n    # creating a temporary data frame for stroring index and values\n    df = pd.DataFrame({\n        variable : data[variable].value_counts().sort_values().index, # getting index after counting values and sorting as ascending\n        \"Count\": data[variable].value_counts().sort_values().values # getting values \n                     }) \n    \n    # creating bar plot as a subplot defines as \"ax\"\n    ax = df.plot(\n            kind = \"bar\",\n            x = variable,\n            y = \"Count\",\n            edgecolor=\"black\",\n            legend = False,       \n            figsize = (3,3)\n                )\n    \n    # showing percentage on bars \n    \n    heights = []  # empty list for storing height of bar of each index item\n    \n    for patch in ax.patches:                  # loop for getting heights of each bar in ax subplot\n        heights.append(patch.get_height())    # gets height of each bar or patch and add to list of heights\n    \n    for patch in ax.patches:                  # loop for showing percentage on bars\n        ax.text(\n            patch.get_x(),                    # x position of text\n            patch.get_height()-350,           # y poisition of text\n            \"%\" + str(round(patch.get_height() \/ sum(heights)*100)),  # text to be shown\n            fontsize = 11,                     # fontsize of text\n            color = \"yellow\"                   # color of text\n         \n        )\n    # some explanation for plot\n    \n    plt.xticks(rotation = 0, fontsize = 10)\n    plt.ylabel(\"Count\", color=\"r\", fontsize=\"10\")\n    plt.xlabel(variable, color=\"r\", fontsize=\"10\")\n    plt.title(variable, fontsize=\"14\", color = \"r\")\n    plt.grid(axis=\"both\", color=\"gray\", linewidth=0.3)    # shows grid lines on plot\n    plt.show()    ","e9afa21f":"# defining a list contains the categorical variables\ncategoricals = [\"Geography\",\"Gender\",\"HasCrCard\",\"IsActiveMember\",\"Exited\",\"NumOfProducts\"]\n\nfor each in categoricals:\n    bar_plot(each)","337eeb37":"# defining function to plot histogram\ndef hist_plot(variable):\n    data[variable].plot(\n        kind = \"hist\",\n        bins = 50,\n        figsize = (4,3),\n        edgecolor = \"black\"\n                        )\n    \n    plt.xticks(rotation = 0, fontsize = 10)\n    plt.xlabel(variable, color=\"r\", fontsize=\"10\")\n    plt.title(\"Frequency of {}\".format(variable), fontsize=\"14\", color = \"r\")\n    plt.grid(axis=\"both\", color=\"gray\", linewidth=0.3)    # shows grid lines on plot\n    plt.show()","c901a307":"numericals = [\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"EstimatedSalary\"]\nfor each in numericals:\n    hist_plot(each)","26c5a623":"def count_plot(variable):\n    plt.subplots(figsize=(4,3))   # creates subplots\n    sns.countplot(\n        x = variable,\n        hue = \"Exited\",\n        data = data,\n            )\n    plt.show()  \n\n    # percentage calculation\n\n    counts = data[data.Exited == 1].groupby(variable)[\"Exited\"].count() \n    sums = data.groupby(variable)[\"Exited\"].count()\n    print(\"Percentage of Exited customers:\")\n    for i in range (counts.shape[0]):\n        print(counts.index[i],\"%\",round(counts[i]\/sums[i]*100))\n","0d2ed5a7":"count_plot(\"Geography\")","0f5627e8":"count_plot('Gender')","f569baff":"count_plot('HasCrCard')","4547714c":"count_plot('IsActiveMember')","645a0843":"data.plot(kind=\"box\", figsize=(14,3))\nplt.show()","b1e10be6":"new_data = data.copy()  # make a copy of data\n\n# definign a function that normalizes selected feature of a selected data\n\ndef normalize_feature(variable, data1):\n    data1[variable] = (data1[variable] - np.min(data1[variable]))\/(np.max(data1[variable] - np.min(data1[variable])))","f6616fe7":"# Normalizing some features in a for loop:\nfor each in ['CreditScore','Age', 'Tenure', 'Balance','EstimatedSalary']:\n    normalize_feature(each, new_data)","f83113a8":"new_data.plot(kind=\"box\",figsize=(14,4))\nplt.show()","cc36cc68":"def detect_outliers(data,feature):\n\n    # first quartile Q1\n    Q1 = np.percentile(data[feature], 25)\n    # third quartile Q3\n    Q3 = np.percentile(data[feature], 75)\n    # IQR = Q3 - Q1\n    IQR = Q3 - Q1\n    # outlier step = IQR x 1.5\n    outlier_step = IQR*1.5\n    # outliers = Q1 - outlier step or Q3 + outlier_step \n    outliers = (data[feature] < Q1 - outlier_step) |(data[feature]>Q3 + outlier_step) \n    # detect indeces of outliers in features of df\n    outlier_indexes= list(data[outliers].index)\n    return outlier_indexes","7b773eb1":"outliers_CreditScore = detect_outliers(data,\"CreditScore\")\nprint(\"There are {} rows which are outliers in CreditScore column\".format(len(outliers_CreditScore)))","25f3db87":"outliers_Age = detect_outliers(data,\"Age\")\nprint(\"There are {} rows which are outliers in Age column\".format(len(outliers_Age)))","ee9132ff":"import collections\noutliers = collections.Counter((outliers_CreditScore + outliers_Age))\n\nmultiple_outliers = list()\nfor i,v in outliers.items():\n    if v > 1:\n        multiple_outliers.append(i)\n\nif len(multiple_outliers) == 0:\n    print(\"There is no row that has multiple outliers\")\nelse:\n    print(\"There are {} rows that have multiple outliers\".format(len(multiple_outliers)))","fc55c5ef":"sns.heatmap(data.corr(), annot = True, fmt = \".2f\")\nplt.show()","1427f0b0":"plt.subplots(figsize=(17,4))\nax=sns.countplot(\n    x=\"Age\",\n    hue = \"Exited\",\n    data = data\n    )\n\nax.set_title(\"Age vs Exiting Customers\", color = \"red\")\nax.grid(axis = \"y\", color = \"yellow\", linewidth = \"0.4\")\nplt.show()","2ce02bf8":"sns.catplot(\n    data = data,\n    x = \"Age\",\n    y = \"Balance\",\n    hue = \"Exited\",\n    kind = \"bar\",\n    height = 3.5,\n    aspect = 5\n)\n\nplt.title(\"Age vs Balance mean vs Exited Customers\", color = \"red\")\nplt.grid(axis = \"y\", color = \"yellow\", linewidth = \"0.4\")\nplt.show()","1066eed4":"sns.pairplot(data[[\"CreditScore\", \"Age\",\"Balance\",\"EstimatedSalary\",\"Exited\"]])\nplt.show()","8f18adeb":"df = data.copy()","fbbda90c":"df[\"Geography\"] = df.Geography.astype(\"category\")\ndf[\"Gender\"] = df.Gender.astype(\"category\")\ndf = pd.get_dummies(df,columns=[\"Geography\",\"Gender\"])","3e7d4bc6":"df.head()","11968d25":"df[\"Tenure\"] = df.Tenure.astype(\"category\")\ndf[\"NumOfProducts\"] = df.NumOfProducts.astype(\"category\")\ndf = pd.get_dummies(df, columns=([\"Tenure\",\"NumOfProducts\"]))","9eda6e41":"for each in [\"HasCrCard\",\"IsActiveMember\",\"Exited\"]:\n    df[each] = df[each].astype(\"category\")","50feca7b":"df.info()","f45c699b":"df.head(3)","e872ea1e":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","e8253c24":"X_train = df.drop(\"Exited\", axis = 1)","40ed5f76":"for each in [\"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\"]:\n    normalize_feature(each, X_train)","4740aaa6":"X_train.head(3)","0c09714f":"y_train = df[\"Exited\"]","2026385a":"print(\"Length of X_train: \",len(X_train))\nprint(\"Shape of X_train: \", X_train.shape)\nprint(\"Length of Y_tain: \", len(y_train))\nprint(\"Shape of Y_train: \", y_train.shape)","db191a91":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train, \n    y_train, \n    test_size = 0.33, \n    random_state = 42\n)\n\nprint(\"Length of X_train: \",len(X_train))\nprint(\"Length of X_test: \",len(X_test))\nprint(\"Length of y_train: \",len(y_train))\nprint(\"Length of y_test: \",len(y_test))","64a6db43":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state = 1)\nlr.fit(X_train, y_train)\nprint(\"Accuracy with train data: \",round(lr.score(X_train,y_train)*100))\nprint(\"Accuracy with test data: \",round(lr.score(X_test, y_test)*100))\n","b9c697eb":"# Single run for 3-neighbors:\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\nprint(\"3-neighbors KNN accuracy with train data: \", round(knn.score(X_train,y_train)*100))\nprint(\"3-neighbors KNN accuracy with test data: \", round(knn.score(X_test,y_test)*100))","91a9e8c0":"accuracy_list_train = []\naccuracy_list_test = []\nfor each in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors = each)\n    knn.fit(X_train, y_train)\n    accuracy_list_train.append(round(knn.score(X_train, y_train)*100))\n    accuracy_list_test.append(round(knn.score(X_test, y_test)*100))    \n\nprint(\"Max test accuracy is % {} @ neighbor value of {}\".format(\n    max(accuracy_list_test),accuracy_list_test.index(max(accuracy_list_test))+1)\n     )","f307a223":"f,ax = plt.subplots(figsize=(10,6))\nax.plot(range(1,20), accuracy_list_train, label=\"Train accuracy\")\nax.plot(range(1,20), accuracy_list_test, label=\"Test accuracy\")\nax.legend()\nplt.xlabel(\"N-neighbor\", size = \"12\", color = \"red\")\nplt.ylabel(\"Accuracy %\", size = \"12\", color = \"red\")\nplt.title(\"KNN Classification accuracy vs n-neighbor\", size = 12, color = \"red\")\nplt.grid()\nplt.show()","769cfac0":"from sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, y_train)\n\nprint(\"SVM accuracy with train data :\", svm.score(X_train,y_train))\nprint(\"SVM accuracy with test data :\", svm.score(X_test, y_test))","d4114a6f":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\nprint(\"NB accuracy with train data :\", nb.score(X_train,y_train))\nprint(\"NB accuracy with test data :\", nb.score(X_test, y_test))","246f310a":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\n\nprint(\"DT accuracy with train data :\", dt.score(X_train,y_train))\nprint(\"DT accuracy with test data :\", dt.score(X_test, y_test))","ed6ead2d":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100, random_state = 42)\n\nrf.fit(X_train, y_train)\n\nprint(\"RF accuracy with train data :\", rf.score(X_train,y_train))\nprint(\"RF accuracy with test data :\", rf.score(X_test, y_test))","12cd5f77":"rf_train_accuracy = []\nrf_test_accuracy = []\n\nfor i in range(1,50):\n    rf = RandomForestClassifier(n_estimators = i, random_state = 1)\n    rf.fit(X_train, y_train)\n    rf.fit(X_test, y_test)\n    rf_train_accuracy.append(round(rf.score(X_train, y_train)*100))\n    rf_test_accuracy.append(round(rf.score(X_test, y_test)*100))\n\nprint(\"Max test accuracy is % {} @ n_estimator value of {}\".format(\n    max(rf_test_accuracy),rf_test_accuracy.index(max(rf_test_accuracy))+1)\n     )","d98cdec9":"f,ax = plt.subplots(figsize=(10,6))\nax.plot(range(1,50), rf_train_accuracy, label=\"Train accuracy\")\nax.plot(range(1,50), rf_test_accuracy, label=\"Test accuracy\")\nax.legend()\nplt.xlabel(\"N-estimator\", size = \"12\", color = \"red\")\nplt.ylabel(\"Accuracy %\", size = \"12\", color = \"red\")\nplt.title(\"RF Classification accuracy vs n-estimator\", size = 12, color = \"red\")\nplt.grid()\nplt.show()","13639221":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = lr, X = X_train, y = y_train, cv = 10)\nprint(\"accuracies :\",accuracies)\nprint(\"mean accuracy :\", accuracies.mean())","ca8fb1f0":"knn = KNeighborsClassifier()  # machine learning model\ngrid = {\n    \"n_neighbors\" : [18,19,30],\n    \"leaf_size\" : [1,2,3]\n}\n\nknn_gsCV = GridSearchCV(knn, grid, cv = 10, n_jobs = -1, verbose=1)  \n\nknn_gsCV.fit(X_train, y_train)\n\nprint(\"Best parameter(n_neighbor): \", knn_gsCV.best_params_)\nprint(\"Best accuracy according to best parameter: \", knn_gsCV.best_score_)","76ac61e9":"knn_gsCV.best_estimator_","fa69bcba":"votingC = VotingClassifier(\n    estimators = [(\"KNN\",knn_gsCV.best_estimator_)],\n    voting = \"soft\",\n    n_jobs = -1\n)\n\nvotingC.fit(X_train, y_train)\n\nprint(\"Accuracy score:\",votingC.score(X_test, y_test))","fed05cd1":"# Creating list of classifiers that i want to compare\nrandom_state = 42\nclassifier_list = [\n    DecisionTreeClassifier(random_state = random_state),\n    SVC(random_state = random_state),\n    RandomForestClassifier(random_state = random_state),\n    LogisticRegression(random_state = random_state),\n    KNeighborsClassifier()\n]\n\n# Creating grids for tuneable parameters\ndt_param_grid = {\n    \"min_samples_split\":range(10,500,20),\n    \"max_depth\": range(1,20,2)\n} \n\nsvc_param_grid = {\n    \"kernel\" : [\"rbf\"],\n    \"gamma\": [0.001, 0.01, 0.1, 1],\n    \"C\": [1,10,50,100,200,300,1000]\n}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\n# creating list for grids \nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","f0840b52":"cv_result = []         # cross validation results to be stacked\nbest_estimators = []   # best estimators to be stacked\n\nfor i in range(5):\n    clf = GridSearchCV(\n        classifier_list[i],\n        param_grid = classifier_param[i],\n        cv = StratifiedKFold(n_splits = 10),\n        scoring = \"accuracy\",\n        n_jobs = -1,\n        verbose = 1\n    )\n    \n    clf.fit(X_train, y_train)\n    \n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])\n","75ecb7f8":"grid_search_results = pd.DataFrame(\n    {\n        \"machine_learning_models\": [\"Decision Tree\",\"SVC\",\"Random Forest\", \"Logistic Regression\",\"KNeighbors\"],\n        \"best_accuricies\" : cv_result\n    }\n)\n\nplt.subplots(figsize=(10,8))\nplt.bar(\n    grid_search_results.machine_learning_models,\n    grid_search_results.best_accuricies    \n)\n\nplt.xlabel(\"Machine Learning Models\", color = \"red\", size = 10)\nplt.ylabel(\"Best Accuricies\", color = \"red\", size = 10)\nplt.grid(axis = \"y\",color = \"yellow\")\nplt.title(\"Grid Search Cross Validation Results\")\n\nplt.show()\n","019eae19":"best_estimators","bd93d448":"votingCls = VotingClassifier(\n    estimators = [\n        (\"dt\", best_estimators[0]),\n        (\"rf\",best_estimators[2])    \n    ],\n    voting = \"soft\",\n    n_jobs = -1\n)\n\nvotingCls.fit(X_train, y_train)","5b5ea8df":"print(\"accuracy score of voting classifier: \", votingCls.score(X_test, y_test))","ae3efe1a":"votingCls.predict(X_test)","2b462d81":"### Defining a function named as \"count_plot()\"","72982995":"* [i. Loading Libraries](#A) <br>\n* [ii. Importing data](#B) <br>\n* [1.Dropping unnecessary columns](#1) <br>\n* [2.Variable Analysis](#2) <br>\n    * [2.1. Categorical Variables](#2.1) <br>\n    * [2.2. Numerical Variables](#2.1) <br>\n* [3.Basic Data Analysis](#3) <br>\n    * [3.1. Seaborn Count Plot](#3.1) <br>\n    * [3.2. Outlier Detection](#3.2) <br>\n* [4.Visualisation](#4) <br>\n* [5.Machine Learning](#5) <br>\n    * [5.1. Modelling](#5.1) <br>\n    * [5.2. Logistic Regression](#5.2) <br>\n    * [5.3. Classification with KNN (K Neirest Neighbour)](#5.3) <br>\n    * [5.4. Classification with SVM (Support Vector Machine)](#5.4) <br>\n    * [5.5. Classification with Naive Bayes](#5.5) <br>\n    * [5.6. Classification with Decision Tree](#5.6) <br>\n    * [5.7. Classification with Random Forest](#5.7) <br>\n    * [5.8. K-Fold Cross Validation (cross_val_score)](#5.8) <br>\n    * [5.9. Grid Search Cross Validation (GridSearchCV)](#5.9) <br>\n    * [5.10. Voting Classifier](#5.10) <br>\n    * [5.11. Hyperparameter tuning -- Grid Search -- Cross Validation](#5.11) <br>\n    * [5.12. Ensemble Modelling](#5.12) <br>","9b49b1f7":"#### accuracies = cross_val_score(estimator = knn, X = x_train, y=y_train, cv=10) ","91dec09c":"* In order to analyze categorical variables, i will plot bar charts.\n* I will define a function:\n* bar_plot function takes a variable as input\n* Will output a bar chart showing variable @ x axis and counts @ y axis.\n* Also percentage of the varible count will be shown on bars.","a2b03b35":"<a id=\"2\"><\/a>\n# 2.VARIABLE ANALYSIS","077e6207":"* From above, we understood that there is no missing value in columns.\n* There are 10000 items and each columns has 10000 non-null items.\n* We can check missing value also by isnull() method as below:","70f562fa":"* Now i will detect the outliers for feature \"CreditScore\" and \"Age\"","23bc40ae":"#### Germany is the country where people mostly exited the bank with %32.","34b538a5":"* According to above plot:\n* Number of exiting customers increasing paralel with the increase of Age, until the ages of 35-37.\n* Because young people working hard, earning money and keeping it in bank accounts.\n* There are some exiters but bank is increasing customer numbers with new customers.\n* From the Age of 35 to 46-47,customer numbers decreasing while exiting customers increasing.\n* Becasue people may be investing their money for their future plans, buying home, car or for business.\n* They do not prefer to keep their savings in bank accounts.\n* After ages of 46-47, numbers of exiting customer and not exiting customers start decreasing.\n* Because people getting older and can not keep earning money, also start to spend for health problems etc.","c77fa559":"#### Female customers tend to leave the bank in a higher rate then males.","bf1746fd":"#### We can say that not having cr card does not have a big effect on leaving the bank.","e2f54bb2":"## Converting data types of columns to category","c0be037c":"* We can see that features of \"CreditScore\" and \"Age\" have outliers.\n* According to plot, features \"NumOfProdcuts\" and \"Exited\" seems to have outliers but they dont due to they are categoricals.\n* I will detect and delete the outliers from data\n* For detection, i will define a function that takes dataframe and a feature as inputs\n* Output will be a list which contains index numbers of outlier items of related feature  ","20a1d7fc":"<a id=\"2.1\"><\/a>\n## 2.1. CATEGORICAL VARIABLES","ec1f9031":"<a id=\"3.1\"><\/a>\n## 3.1. Seaborn Count Plot","11b40bb1":"* Above box plot is not clear, features that have values between 0-1 seems to be all same.\n* Due to data has small values and big values.\n* I will normalize some features of the data and plot the box again.\n* Values of normalized features to be between 0 and 1.","9dbe6265":"#### Not active members tend to leave the bank.","ef94cb48":"### Creating X_train:","55d50167":"The story:\nA bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon","37445a7b":"### Categorical Variables:\n* Geography\n* Gender\t\n* HasCrCard\t\n* IsActiveMember\t\n* Exited\n* NumOfProducts\n\n### Numerical Variables: \n* CreditScore\t\n* Age\t\n* Tenure\t\n* Balance\t\n* EstimatedSalary","d9789afa":"* Box plot of normalized data:","d0766fee":"<a id=\"5.2\"><\/a>\n## 5.2. Logistic Regression","529f8491":"### Splitting X_Train, y_train","2269f854":"<a id=\"2.2\"><\/a>\n## 2.2. NUMERICAL VARIABLES","1745b204":"### RF classification with multiple n-estimators","4ef53481":"Below pairs to be analyzed:\n* Geography - Exited\n* Gender - Exited\t\n* HasCrCard\t- Exited\n* IsActiveMember - Exited\t\n* NumOfProducts - Exited","7be2bef8":"<a id=\"3\"><\/a>\n## 3. BASIC DATA ANALYSIS","81675356":"<a id=\"1\"><\/a>\n## 1. Dropping unnecessary columns","e5ea67ee":"<a id=\"5.11\"><\/a>\n## 5.11. Hyperparameter tuning -- Grid Search -- Cross Validation","a16f62da":"* Comparing meacn accuracies of 5 ML classification algorithms with grid search and stratified cross validation\n* Decision Tree Classifier\n* Support Vector Machine\n* Random Forest\n* KNN\n* Logistic Regression","c1f496ae":"![image.png](attachment:image.png)","629b02eb":"* Features (CreditScore, Age, Balance, EstimatedSalary) to be normalized:","9c322f9a":"<a id=\"5.7\"><\/a>\n## 5.7. Classification with Random Forest","3a54751d":"<a id=\"5\"><\/a>\n# 5. MACHINE LEARNING","9ce5e829":"* So from above coding, we understood that there is no row to delete.\n* Because there is no multiple outliers.","3df472cf":"<a id=\"5.12\"><\/a>\n## 5.12. ENSEMBLE MODELLING","b9108b6f":"* Function takes variable as an input\n* Outputs countplot and percentage of varible\n* I will use groupby method to calculate percentages","5622cb99":"### Comments on bar plots:\n* %51 of the customers have only 1 product, %46 have 2 products, %3 has 3 and %1 has 4 products.\n* %20 of the customers left the bank\n* %52 of the customers are active member\n* %71 of the customers have credit cards\n* %55 of the customers are male\n* %50 of the customers from France, %25 from Germany, %25 from Spain. ","55423460":"## Defining a function named as \"hist_plot()\"","9f809e69":"### Calculation for single n-neighbor","c2c1545c":"<a id=\"4\"><\/a>\n## 4. VISUALIZATION","5e5be499":"### Creating y_train:","5e22dc02":"### Comments on histogram plots:\n* There are roughly 3500 accounts having zero balance, those are tend to be leaving.","822dabb2":"## Converting \"Tenure\" and \"NumOfProducts\" to categorical","07674fa3":"* From above coding: we saw that there are outliers in two columns.\n* I want to delete the rows that have outliers more than 1.\n* For example, we have two columns that have outliers; i want to find the rows that have outliers in both 2 columns.\n* For this purpose:\n* I will combine the two list that have outliers for 2 columns.\n* I will count the items with collections.Counter() method** if an item exist in both lists.","8ee8f0c4":"<a id=\"5.6\"><\/a>\n## 5.6. Classification with Decision Tree","3925388f":"## Train - test split","78216191":"<a id=\"5.3\"><\/a>\n## 5.3. Classification with KNN (K Neirest Neighbour)","a9df20fc":"<a id=\"5.5\"><\/a>\n## 5.5. Classification with Naive Bayes","7ee523ae":"<a id=\"B\"><\/a>\n## ii. Importing data","32fadca1":"* It is used to tune hyperparameters.\n* Tuned hyperparameter that gives the best accuracy can be detected.\n* For example, best k value can be detected for a KNN classifier usig GridSearchCV.\n* GridSearchCV(model, grid, cv)  \n* Some input parameters are: model--> machine learning model, can be KNeighborsClassifier() etc.\n* grid--> Put hyperparameters such as n_neighbrors, n_estimators in a dictionary.\n* cv --> can be set as numbers: if it is selected as 5 in a KNN model,\n* this means that, for each k value, data to be split into 5 train-validation split.\n* and mean accuracy of 5 fold to be calculated for each k value. ","c41cfb17":"### AGE vs EXITED vs BALANCE","b99b8793":"<a id=\"5.1\"><\/a>\n## 5.1. Modelling","3f752d55":"## Defining a function named as \"bar_plot()\"","10769ae9":"### AGE vs EXITED","fb6af40d":"<a id=\"5.10\"><\/a>\n## 5.10. Voting Classifier","11334f14":"* I will detect outliers : Outlier is a data point that differs significantly from other observations\n* Afterwards i will delete the outliers from data.\n* In order to see the outliers at a glance, i will use box plot first.","73a4457f":"### RF classification with single n-estimator","9eadde71":"* In order to analyze numerical variables i will plot histograms.\n* For this purpose, i will plot histograms of all num. variables at once.\n* Function takes variable as an input and outputs hist plot.","1dde7d5a":"<a id=\"5.8\"><\/a>\n## 5.8. K-Fold Cross Validation (cross_val_score)","1408d900":"<a id=\"5.9\"><\/a>\n## 5.9. Grid Search Cross Validation (GridSearchCV)","d4b13a9f":"* 30% Positive Corelation exists between Age and Exited features.\n* 30% Negative Corelation exists between Balance and NumOfProducts features.\n* 12% Positive Corelation exists between Balance and Exited features.\n* 16& Negative Corelation exists between IsActiveMember and Exited.","7ecdd5fb":"* Algorithm:\n1.     1- Split data as train and test datas.\n1.     2- Let say k of k-fold is 3, then train data to be splitted into 3.\n1.     3- Split each train data into 3 again as train-train-validation.\n1.     4- For each 3 splits, data to be trained and validated.\n1.     5- There will be 3 accuracy rate for each 3 trains.\n1.     6- Get mean of 3 accuracies.\n1.     7- Test the model with test split.","b4094f2d":"### Calculation for multiple n-neighbors","f50b78d2":"<a id=\"3.2\"><\/a>\n## 3.2. OUTLIER DETECTION","67ea247c":"<a id=\"A\"><\/a>\n## i. Loading Libraries","01ad07b7":"<a id=\"5.4\"><\/a>\n## 5.4. Classification with SVM (Support Vector Machine)","e4de5b21":"## Converting \"Geography\" and \"Gender\" to categoricals:\n* Geography --> Geography_France  \/ Geography_Germany \/ Geography_Spain\n* Gender --> Gender_Female  \/ Gender_Male\n"}}