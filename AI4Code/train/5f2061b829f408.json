{"cell_type":{"714ab4d6":"code","9dc88173":"code","343d192f":"code","b9932ef7":"code","c1472306":"code","a93cdaba":"code","bfc93a5a":"code","847a60e0":"code","598dda6d":"code","8289ea79":"code","02ea6f09":"markdown","a374e50d":"markdown","18236b31":"markdown","5a94472c":"markdown","9d4a2e50":"markdown","713b07e2":"markdown","f21133e7":"markdown","dc52f538":"markdown","a37237a8":"markdown","773f2811":"markdown","caf11394":"markdown","024b567f":"markdown","8812c38c":"markdown","ea824483":"markdown","15055dc2":"markdown","eae4ec4f":"markdown","bc5ef37c":"markdown"},"source":{"714ab4d6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport torch\nimport torchvision\nfrom glob import glob\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensor, ToTensorV2\nfrom random import randint, choice, shuffle, choices\nimport cv2\nfrom torch import nn\nimport torch.nn.functional as F\n!pip install torch-summary","9dc88173":"DATA_DIR = '..\/input\/wider-data\/WIDER'\nTRAIN_DIR = '..\/input\/wider-data\/WIDER\/WIDER_train'\nBOX_COLOR = (0, 0, 255)\nTEXT_COLOR = (255, 255, 255)\n\nS = 13 #chia \u1ea3nh th\u00e0nh grids 13x13 cells\nBOX = 5 #box number each cell = number of anchor boxes \nCLS = 2 # [face, not_face]\nH, W = 416, 416\nOUTPUT_THRESH = 0.7\n\n# pre-defined anchor-boxes\nANCHOR_BOXS = [[1.08,1.19],\n               [3.42,4.41],\n               [6.63,11.38],\n               [9.42,5.11],\n               [16.62,10.52]]\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ntorch.autograd.set_detect_anomaly(True)","343d192f":"#show 1 \u1ea3nh\ndef plot_img(img, size=(7,7)):\n    plt.figure(figsize=size)\n    plt.imshow(img[:,:,::-1])\n    plt.show()\n    \n# v\u1ebd bounding box l\u00ean \u1ea3nh\ndef visualize_bbox(img, boxes, thickness=2, color=BOX_COLOR, draw_center=True):\n    img_copy = img.cpu().permute(1,2,0).numpy() if isinstance(img, torch.Tensor) else img.copy()\n    for box in boxes:\n        x,y,w,h = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n        img_copy = cv2.rectangle(\n            img_copy,\n            (x,y),(x+w, y+h),\n            color, thickness)\n        if draw_center:\n            center = (x+w\/\/2, y+h\/\/2)\n            img_copy = cv2.circle(img_copy, center=center, radius=3, color=(0,255,0), thickness=2)\n    return img_copy","b9932ef7":"def get_xywh_from_textline(text):\n    coor = text.split(\" \")\n    result = []\n    xywh = [int(coor[i]) for i in range(4)] if(len(coor) > 4) else None\n    return xywh\n\ndef read_data(file_path, face_nb_max=10):\n    '''\n    Read data in .txt file\n    Return:\n        list of image_data,\n        each element is dict {file_path: , box_nb: , boxes:}\n    '''\n    data = []\n    with open(file_path, 'r') as f:\n        lines = f.readlines()\n        for i, cur_line in enumerate(lines):\n            if '.jpg' in cur_line:\n                img_data = {\n                    'file_path': cur_line.strip(),\n                    'box_nb': int(lines[i+1]),\n                    'boxes': [],\n                }\n                \n                face_nb = img_data['box_nb']\n                if(face_nb <= face_nb_max):\n                    for j in range(face_nb):\n                        rect = get_xywh_from_textline(lines[i+2+j].replace(\"\\n\", \"\"))\n                        if rect is not None:\n                            img_data['boxes'].append(rect)\n                    if len(img_data['boxes']) > 0:\n                        data.append(img_data)\n    return data\n\ntrain_data = read_data('..\/input\/wider-data\/WIDER\/wider_face_train_bbx_gt.txt')\nimg_data = choice(train_data)\nimg = cv2.imread(f\"{TRAIN_DIR}\/{img_data['file_path']}\").astype(np.float32)\/255.0\nvis_img = visualize_bbox(img, img_data['boxes'], thickness=3, color=BOX_COLOR)\nplot_img(vis_img)","c1472306":"class FaceDataset(Dataset):\n    def __init__(self, data, img_dir=TRAIN_DIR, transforms=None):\n        self.data = data\n        self.img_dir = img_dir\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, id):\n        \"\"\"\"\"\"\n        img_data = self.data[id]\n        img_fn = f\"{self.img_dir}\/{img_data['file_path']}\"\n        boxes = img_data[\"boxes\"]\n        box_nb = img_data[\"box_nb\"]\n        labels = torch.zeros((box_nb, 2), dtype=torch.int64)\n        labels[:, 0] = 1\n        img = cv2.imread(img_fn).astype(np.float32)\/255.0\n        \n        try:\n            if self.transforms:\n                sample = self.transforms(**{\n                    \"image\":img,\n                    \"bboxes\": boxes,\n                    \"labels\": labels,\n                })\n                img = sample['image']\n                boxes = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n        except:\n            return self.__getitem__(randint(0, len(self.data)-1))\n        target_tensor = self.boxes_to_tensor(boxes.type(torch.float32), labels)\n        return img, target_tensor\n    \n    def boxes_to_tensor(self, boxes, labels):\n        \"\"\"\n        Convert list of boxes (and labels) to tensor format\n        Output:\n            boxes_tensor: shape = (Batchsize, S, S, Box_nb, (4+1+CLS))\n        \"\"\"\n        boxes_tensor = torch.zeros((S, S, BOX, 5+CLS))\n        cell_w, cell_h = W\/S, H\/S\n        for i, box in enumerate(boxes):\n            x,y,w,h = box\n            # normalize xywh with cell_size\n            x,y,w,h = x\/cell_w, y\/cell_h, w\/cell_w, h\/cell_h\n            center_x, center_y = x+w\/2, y+h\/2\n            grid_x = int(np.floor(center_x))\n            grid_y = int(np.floor(center_y))\n            \n            if grid_x < S and grid_y < S:\n                boxes_tensor[grid_y, grid_x, :, 0:4] = torch.tensor(BOX * [[center_x-grid_x,center_y-grid_y,w,h]])\n                boxes_tensor[grid_y, grid_x, :, 4]  = torch.tensor(BOX * [1.])\n                boxes_tensor[grid_y, grid_x, :, 5:]  = torch.tensor(BOX*[labels[i].numpy()])\n        return boxes_tensor\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef target_tensor_to_boxes(boxes_tensor):\n    '''\n    Recover target tensor (tensor output of dataset) to bboxes\n    Input:\n        boxes_tensor: bboxes in tensor format - output of dataset.__getitem__\n    Output:\n        boxes: list of box, each box is [x,y,w,h]\n    '''\n    cell_w, cell_h = W\/S, H\/S\n    boxes = []\n    for i in range(S):\n        for j in range(S):\n            for b in range(BOX):\n                data = boxes_tensor[i,j,b]\n                x_center,y_center, w, h, obj_prob, cls_prob = data[0], data[1], data[2], data[3], data[4], data[5:]\n                prob = obj_prob*max(cls_prob)\n                if prob > OUTPUT_THRESH:\n                    x, y = x_center+j-w\/2, y_center+i-h\/2\n                    x,y,w,h = x*cell_w, y*cell_h, w*cell_w, h*cell_h\n                    box = [x,y,w,h]\n                    boxes.append(box)\n    return boxes","a93cdaba":"train_transforms = A.Compose([\n        A.Resize(height=416, width=416),\n        A.RandomSizedCrop(min_max_height=(350, 416), height=416, width=416, p=0.4),\n        A.HorizontalFlip(p=0.5),\n        ToTensorV2(p=1.0)\n    ],\n    bbox_params={\n        \"format\":\"coco\",\n        'label_fields': ['labels']\n})\ntrain_dataset = FaceDataset(train_data, transforms=train_transforms)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=True,\n    num_workers=2,\n    collate_fn=collate_fn, drop_last=True)\n\n# test dataset and dataloader\nimgs, targets = next(iter(train_loader))\nimg, boxes_tensor = choice(train_dataset)\nboxes = target_tensor_to_boxes(boxes_tensor)\nimg = visualize_bbox(img, boxes=boxes)\nplot_img(img)","bfc93a5a":"from torchsummary import summary\n\ndef Conv(n_input, n_output, k_size=4, stride=2, padding=0, bn=False):\n    return nn.Sequential(\n        nn.Conv2d(\n            n_input, n_output,\n            kernel_size=k_size,\n            stride=stride,\n            padding=padding, bias=False),\n        nn.BatchNorm2d(n_output),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Dropout(p=0.2, inplace=False))\n\n# This is tiny-yolo\nclass YOLO(torch.nn.Module):\n    def __init__(self, nc=32, S=13, BOX=5, CLS=2):\n        super(YOLO, self).__init__()\n        self.nc = nc\n        self.S = S\n        self.BOX = BOX\n        self.CLS = CLS\n        self.net = nn.Sequential(\n            nn.Conv2d(\n                3, nc,\n                kernel_size=4,\n                stride=2,\n                padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            Conv(nc, nc, 3,2,1),\n            Conv(nc, nc*2, 3,2,1),\n            Conv(nc*2, nc*4, 3,2,1),\n            Conv(nc*4, nc*8, 3,2,1),\n            Conv(nc*8, nc*16, 3,1,1),\n            Conv(nc*16, nc*8, 3,1,1),\n            Conv(nc*8, BOX*(4+1+CLS), 3,1,1),\n        )\n        \n    def forward(self, input):\n        output_tensor = self.net(input)\n        output_tensor = output_tensor.permute(0, 2,3,1)\n        W_grid, H_grid = self.S, self.S\n        output_tensor = output_tensor.view(-1, H_grid, W_grid, self.BOX, 4+1+self.CLS)\n        return output_tensor\n    \nmodel = YOLO(S=S, BOX=BOX, CLS=CLS)\ntest_img = torch.rand(1,3,416,416)\noutput = model(test_img)\n# summary(model, (3, 416, 416))","847a60e0":"def post_process_output(output):\n    \"\"\"Convert output of model to pred_xywh\"\"\"\n    # xy\n    xy = torch.sigmoid(output[:,:,:,:,:2]+1e-6)\n\n    # wh\n    wh = output[:,:,:,:,2:4]\n    anchors_wh = torch.Tensor(ANCHOR_BOXS).view(1,1,1,BOX,2).to(device)\n    wh = torch.exp(wh)*anchors_wh\n    \n    # objectness confidence\n    obj_prob = torch.sigmoid(output[:,:,:,:,4:5]+1e-6)\n    \n    # class distribution\n    cls_dist = torch.softmax(output[:,:,:,:,5:], dim=-1)\n    return xy, wh, obj_prob, cls_dist\n\ndef post_process_target(target_tensor):\n    \"\"\"\n    T\u00e1ch target tensor th\u00e0nh t\u1eebng th\u00e0nh ph\u1ea7n ri\u00eang bi\u1ec7t: xy, wh, object_probility, class_distribution\n    \"\"\"\n    xy = target_tensor[:,:,:,:,:2]\n    wh = target_tensor[:,:,:,:,2:4]\n    obj_prob = target_tensor[:,:,:,:,4:5]\n    cls_dist = target_tensor[:,:,:,:,5:]\n    return xy, wh, obj_prob, cls_dist\n\ndef square_error(output, target):\n    return (output-target)**2\n\ndef custom_loss(output_tensor, target_tensor):\n    \"\"\"\n    Lu\u1ed3ng x\u1eed l\u00ed:\n        1. T\u00ednh di\u1ec7n t\u00edch c\u00e1c pred_bbox\n        2. T\u00ednh di\u1ec7n t\u00edch c\u00e1c true_bbox\n        3. T\u00ednh iou gi\u1eefa t\u1eebng pred_bbox v\u1edbi true_bbox t\u01b0\u01a1ng \u1ee9ng (n\u1eb1m trong c\u00f9ng 1 cell)\n        4. Trong m\u1ed7i cell, x\u00e1c \u0111\u1ecbnh best_box - box c\u00f3 iou v\u1edbi true_bbox \u0111\u1ea1t gi\u00e1 tr\u1ecb max so v\u1edbi 4 pred_bbox c\u00f2n l\u1ea1i\n        5. T\u00ednh c\u00e1c loss th\u00e0nh ph\u1ea7n theo c\u00f4ng th\u1ee9c trong \u1ea3nh\n        6. T\u00ednh Total_loss\n    \"\"\"\n    cell_size = (W\/S, H\/S)\n    NOOB_W, CONF_W, XY_W, PROB_W, WH_W = 2.0, 10.0, 0.5, 1.0, 0.1\n\n    pred_xy, pred_wh, pred_obj_prob, pred_cls_dist = post_process_output(output_tensor)\n    true_xy, true_wh, true_obj_prob, true_cls_dist = post_process_target(target_tensor)\n    \n    # t\u00ednh di\u1ec7n t\u00edch c\u00e1c pred_bbox\n    pred_ul = pred_xy - 0.5*pred_wh\n    pred_br = pred_xy + 0.5*pred_wh\n    pred_area = pred_wh[:,:,:,:,0]*pred_wh[:,:,:,:,1]\n    \n    # T\u00ednh di\u1ec7n t\u00edch c\u00e1c true_bbox\n    true_ul = true_xy - 0.5*true_wh\n    true_br = true_xy + 0.5*true_wh\n    true_area = true_wh[:,:,:,:,0]*true_wh[:,:,:,:,1]\n\n    # T\u00ednh iou gi\u1eefa t\u1eebng pred_bbox v\u1edbi true_bbox t\u01b0\u01a1ng \u1ee9ng (n\u1eb1m trong c\u00f9ng 1 cell)\n    intersect_ul = torch.max(pred_ul, true_ul)\n    intersect_br = torch.min(pred_br, true_br)\n    intersect_wh = intersect_br - intersect_ul\n    intersect_area = intersect_wh[:,:,:,:,0]*intersect_wh[:,:,:,:,1]\n    \n    # Trong m\u1ed7i cell, x\u00e1c \u0111\u1ecbnh best_box - box c\u00f3 iou v\u1edbi true_bbox \u0111\u1ea1t gi\u00e1 tr\u1ecb max so v\u1edbi 4 pred_bbox c\u00f2n l\u1ea1i\n    iou = intersect_area\/(pred_area + true_area - intersect_area)\n    max_iou = torch.max(iou, dim=3, keepdim=True)[0]\n    best_box_index =  torch.unsqueeze(torch.eq(iou, max_iou).float(), dim=-1)\n    true_box_conf = best_box_index*true_obj_prob\n    \n    # T\u00ednh c\u00e1c loss th\u00e0nh ph\u1ea7n theo c\u00f4ng th\u1ee9c trong \u1ea3nh\n    xy_loss =  (square_error(pred_xy, true_xy)*true_box_conf*XY_W).sum()\n    wh_loss =  (square_error(pred_wh, true_wh)*true_box_conf*WH_W).sum()\n    obj_loss = (square_error(pred_obj_prob, true_obj_prob)*(CONF_W*true_box_conf + NOOB_W*(1-true_box_conf))).sum()\n    cls_loss = (square_error(pred_cls_dist, true_cls_dist)*true_box_conf*PROB_W).sum()\n\n    # Loss k\u1ebft h\u1ee3p\n    total_loss = xy_loss + wh_loss + obj_loss + cls_loss\n    return total_loss\n\n## Test loss function\ntarget_tensor = torch.stack(targets)\nimg_tensor = torch.stack(imgs)\nmodel.to(device)\noutput = model(img_tensor.to(device))\nloss = custom_loss(output_tensor=output, target_tensor=target_tensor.to(device))\nprint(loss.item())","598dda6d":"from tqdm import tqdm\n\ndef output_tensor_to_boxes(boxes_tensor):\n    cell_w, cell_h = W\/S, H\/S\n    boxes = []\n    probs = []\n    \n    for i in range(S):\n        for j in range(S):\n            for b in range(BOX):\n                anchor_wh = torch.tensor(ANCHOR_BOXS[b])\n                data = boxes_tensor[i,j,b]\n                xy = torch.sigmoid(data[:2])\n                wh = torch.exp(data[2:4])*anchor_wh\n                obj_prob = torch.sigmoid(data[4:5])\n                cls_prob = torch.softmax(data[5:], dim=-1)\n                combine_prob = obj_prob*max(cls_prob)\n                \n                if combine_prob > OUTPUT_THRESH:\n                    x_center, y_center, w, h = xy[0], xy[1], wh[0], wh[1]\n                    x, y = x_center+j-w\/2, y_center+i-h\/2\n                    x,y,w,h = x*cell_w, y*cell_h, w*cell_w, h*cell_h\n                    box = [x,y,w,h, combine_prob]\n                    boxes.append(box)\n    return boxes\n\n\ndef overlap(interval_1, interval_2):\n    x1, x2 = interval_1\n    x3, x4 = interval_2\n    if x3 < x1:\n        if x4 < x1:\n            return 0\n        else:\n            return min(x2,x4) - x1\n    else:\n        if x2 < x3:\n            return 0\n        else:\n            return min(x2,x4) - x3\n\ndef compute_iou(box1, box2):\n    \"\"\"Compute IOU between box1 and box2\"\"\"\n    x1,y1,w1,h1 = box1[0], box1[1], box1[2], box1[3]\n    x2,y2,w2,h2 = box2[0], box2[1], box2[2], box2[3]\n    \n    ## if box2 is inside box1\n    if (x1 < x2) and (y1<y2) and (w1>w2) and (h1>h2):\n        return 1\n    \n    area1, area2 = w1*h1, w2*h2\n    intersect_w = overlap((x1,x1+w1), (x2,x2+w2))\n    intersect_h = overlap((y1,y1+h1), (y2,y2+w2))\n    intersect_area = intersect_w*intersect_h\n    iou = intersect_area\/(area1 + area2 - intersect_area)\n    return iou\n\ndef nonmax_suppression(boxes, IOU_THRESH = 0.4):\n    \"\"\"remove ovelap bboxes\"\"\"\n    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n    for i, current_box in enumerate(boxes):\n        if current_box[4] <= 0:\n            continue\n        for j in range(i+1, len(boxes)):\n            iou = compute_iou(current_box, boxes[j])\n            if iou > IOU_THRESH:\n                boxes[j][4] = 0\n    boxes = [box for box in boxes if box[4] > 0]\n    return boxes","8289ea79":"model = YOLO(S=S, BOX=BOX, CLS=CLS)\nmodel.to(device)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nepochs = 10\niters = 1\n\nimgs, targets = next(iter(train_loader))\ndemo_img = imgs[0].permute(1,2,0).cpu().numpy()\nboxes = target_tensor_to_boxes(targets[0])\nplot_img(visualize_bbox(demo_img.copy(), boxes=boxes), size=(5,5))\n\nfor epoch in tqdm(range(300)):\n#     for imgs, targets in tqdm(train_loader):\n    # m\u00ecnh s\u1ebd train v\u00e0 test tr\u00ean 1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u nh\u1ecf \n    # \u0111\u1ec3 ki\u1ec3m tra model v\u00e0 code c\u00f3 ho\u1ea1t \u0111\u1ed9ng \u0111\u00fang hay kh\u00f4ng\n    model.zero_grad()\n    tensor_imgs, tensor_targets = torch.stack(imgs), torch.stack(targets)\n    output = model(tensor_imgs.to(device))\n    loss = custom_loss(output_tensor=output, target_tensor=tensor_targets.to(device))\n    loss_value = loss.item()\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    iters += 1\n\n    if iters%200 == 0:\n        ## Test nay tr\u00ean t\u1eadp train, n\u1ebfu k\u1ebft qu\u1ea3 th\u1ecfa m\u00e3n \u0111\u1ed3ng ngh\u0129a v\u1edbi code c\u1ee7a b\u1ea1n \u0111\u00e3 ch\u1ea1y \u0111\u00fang \n        boxes = output_tensor_to_boxes(output[0].detach().cpu())\n        boxes = nonmax_suppression(boxes)\n        img = imgs[0].permute(1,2,0).cpu().numpy()\n        img = visualize_bbox(img.copy(), boxes=boxes)\n        plot_img(img, size=(4,4))","02ea6f09":"Hai n\u0103m tr\u01b0\u1edbc m\u00ecnh t\u1eebng code b\u1eb1ng Tensorflow 1.x.x . H\u1ed3i \u0111\u00f3 m\u00ecnh m\u1ea5t 4 ng\u00e0y \u0111\u1ec3 code, code xong to\u00e1t c\u1ea3 m\u1ed3 h\u00f4i h\u1ed9t. Gi\u1edd th\u00ec m\u00ecnh s\u1ebd code l\u1ea1i b\u1eb1ng Torch, \u0111\u01a1n gi\u1ea3n v\u00e0 ti\u1ec7n h\u01a1n r\u1ea5t nhi\u1ec1u.\n\nN\u1ebfu ai ch\u01b0a bi\u1ebft torch c\u0169ng kh\u00f4ng sao. V\u1ec1 c\u01a1 b\u1ea3n, c\u00fa ph\u00e1p c\u1ee7a Torch c\u0169ng gi\u1ed1ng numpy, ng\u01b0\u1eddi m\u1edbi v\u1eabn \u0111\u1ecdc hi\u1ec3u \u0111\u01b0\u1ee3c","a374e50d":"## 2.1 Import libs, define CONST, define some util functions","18236b31":"## 2.3 Define Dataset Class","5a94472c":"\u0110\u1ec3 \u0111\u01a1n gi\u1ea3n v\u00e0 d\u1ec5 hi\u1ec3u, m\u00ecnh kh\u00f4ng d\u00f9ng Darknet-backbone m\u00e0 m\u00ecnh s\u1ebd x\u00e2y d\u1ee5ng TinyYOLO - phi\u00ean b\u1ea3n nh\u1ecf h\u01a1n c\u1ee7a YOLO v\u1edbi ki\u1ebfn tr\u00fac bao g\u1ed3m c\u00e1c Convolution layer x\u1ebfp ch\u1ed3ng l\u00ean nhau. Vi\u1ec7c x\u00e2y d\u1ef1ng backbone cho YOLO kh\u00f4ng h\u1ec1 kh\u00f3. Th\u1ee9 kh\u00f3 code nh\u1ea5t l\u00e0 Loss function\n\n![](https:\/\/images.viblo.asia\/full\/1109dd0a-bf3d-4e9f-91c5-a48ea3a3d3e4.gif)","9d4a2e50":"## 1.1 C\u1ea5u tr\u00fac\n\n1. Read and preprocess data from .txt file\n2. Define Dataset and Dataloader\n3. Define module and backbone of YOLO\n4. Define Loss function \n5. Train model","713b07e2":"Dataset trong Torch hay ch\u00ednh l\u00e0 generator \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 sinh data trong qu\u00e1 tr\u00ecnh train\nTorch \u0111\u00e3 cung c\u1ea5p s\u1eb5n 2 class l\u00e0 Dataset v\u00e0 Dataloader, vi\u1ec7c x\u00e2y d\u1ef1ng generator tr\u1edf l\u00ean \u0111\u01a1n gi\u1ea3n\n\nM\u00ecnh \u0111\u1ecbnh ngh\u0129a class FaceDataset, h\u00e0m __getitem__() s\u1ebd tr\u1ea3 v\u1ec1 c\u1eb7p (img, target_tensor). Trong \u0111\u00f3 target_tensor ch\u1ee9a th\u00f4ng tin v\u1ec1 bboxes, label, objectness_prob, l\u00e0 1 matrix c\u00f3 Shape = (S,S,B,(4+1+CLS)).","f21133e7":"## 2.4 Define Loss function\n","dc52f538":"# 2. Code","a37237a8":"### Kh\u1edfi t\u1ea1o transform, dataset, dataloader","773f2811":"## 2.2 Read and preprocess data in .txt file","caf11394":"\u0110\u00f3, ch\u1ec9 sau 15s, m\u00ecnh \u0111\u00e3 ki\u1ec3m ch\u1ee9ng \u0111\u01b0\u1ee3c r\u1eb1ng code, model ho\u1ea1t \u0111\u1ed9ng chu\u1ea9n. Vi\u1ec7c b\u00e2y gi\u1edd l\u00e0 ch\u1ec9nh l\u1ea1i backbone, augmentation data v\u00e0 train ti\u1ebfp cho ngon h\u01a1n","024b567f":"## 2.3 Define YOLOv2 model","8812c38c":"# 1. Introduction\n\nTrong b\u00e0i n\u00e0y, m\u00ecnh s\u1ebd t\u1eadp trung v\u00e0o ph\u1ea7n vi\u1ec7c ph\u00e2n t\u00edch c\u1ea5u tr\u00fac v\u00e0 code. All in notebook. T\u1ea5t c\u1ea3 n\u1ed9i dung n\u1eb1m tr\u1ecdn trong Notebook n\u00e0y. Ph\u1ea7n vi\u1ec7c ch\u00ednh m\u00ecnh s\u1ebd c\u1ed1 g\u1eafng vi\u1ebft d\u1ec5 hi\u1ec3u v\u00e0 r\u00fat g\u1ecdn nh\u1ea5t c\u00f3 th\u1ec3. M\u1ee5c ti\u00eau ch\u00ednh c\u1ee7a b\u00e0i l\u00e0 h\u01b0\u1edbng d\u1eabn **Build YOLO from scratch**, t\u1ee9c s\u1ebd t\u1eadp trung v\u00e0o vi\u1ec7c h\u01b0\u1edbng d\u1eabn code v\u00e0 build model + loss function. V\u1eady n\u00ean model c\u00f3 t\u1ec7 c\u0169ng kh\u00f4ng b\u1ea5t ng\u1edd, ch\u1ec9 c\u1ea7n run\/train \u0111\u00fang, k c\u1ea7n t\u1ed1t. V\u00ec th\u1eddi gian v\u00e0 resource h\u1ea1n ch\u1ebf, m\u00ecnh s\u1ebd b\u1ecf qua vi\u1ec7c test and validate model ","ea824483":"Tr\u01b0\u1edbc khi \u0111\u1ecdc code, hay nh\u00ecn l\u1ea1i c\u00e1c c\u00f4ng th\u1ee9c n\u00e0y m\u1ed9t ch\u00fat. Th\u1ee9 YOLO tr\u1ea3 v\u1ec1 kh\u00f4ng ph\u1ea3i (x,y,w,h) tr\u1ef1c ti\u1ebfp c\u1ee7a Object. Ta c\u1ea7n v\u00e0i b\u01b0\u1edbc bi\u1ebfn \u0111\u1ed5i theo c\u00f4ng th\u1ee9c\n\n![](https:\/\/images.viblo.asia\/f98ac39c-fb19-4e59-94e3-d721eed3f8fa.jpeg)\n\nNh\u01b0 m\u00ecnh \u0111\u00e3 n\u00f3i, th\u1ee9 kh\u00f3 code nh\u1ea5t c\u1ee7a YOLO l\u00e0 loss function. T\u01b0 t\u01b0\u1edfng th\u00ec \u0111\u01a1n gi\u1ea3n nh\u01b0ng vi\u1ec7c implement th\u00ec kh\u00e1 ph\u1ee9c t\u1ea1p. Loss YOLO \u0111\u01b0\u1ee3c k\u1ebft h\u1ee3p t\u1eeb 5 loss th\u00e0nh ph\u1ea7n. Trong qu\u00e1 tr\u00ecnh t\u00ednh loss, b\u1ea1n ph\u1ea3i x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c IOU c\u1ee7a t\u1eebng predicted bbox v\u1edbi groundtruth bbox n\u1eb1m c\u00f9ng trong m\u1ed9t Cell. N\u1ebfu v\u1eabn ch\u01b0a hi\u1ec3u r\u00f5, h\u00e3y \u0111\u1ecdc code :D\n\n![](https:\/\/images.viblo.asia\/full\/b5327164-efd3-425a-8e15-5440937c4151.jpeg)","15055dc2":"## 2.5 Train model","eae4ec4f":"\u0110\u1ebfn giai \u0111o\u1ea1n cu\u1ed1i c\u00f9ng: train model. Th\u1ef1c ra v\u00ec m\u00ecnh kh\u00f4ng c\u00f3 th\u1eddi gian v\u00e0 resource n\u00ean m\u00ecnh s\u1ebd test ngay tr\u00ean d\u1eef li\u1ec7u train (ch\u1ec9 v\u1edbi v\u00e0i \u1ea3nh).\n\nNghe **Test v\u1edbi d\u1eef li\u1ec7u train** th\u00ec h\u01a1i bu\u1ed3n c\u01b0\u1eddi, nh\u01b0ng \u0111\u00f3 ch\u00ednh l\u00e0 c\u00e1ch m\u00ecnh d\u00f9ng \u0111\u1ec3 ki\u1ebfm tra model c\u00f3 ho\u1ea1t \u0111\u1ed9ng \u0111\u00fang hay kh\u00f4ng. N\u1ebfu feed 1 l\u01b0\u1ee3ng nh\u1ecf data v\u00e0o train m\u00e0 model kh\u00f4ng cho output gi\u1ed1ng v\u1edbi target, \u0111i\u1ec1u \u0111\u00f3 c\u00f3 ngh\u0129a model c\u1ee7a b\u1ea1n \u0111ang sai \u1edf \u0111\u00e2u \u0111\u00f3, trong loss function, model backbone, post-preprocess ... Ng\u01b0\u1ee3c l\u1ea1i, n\u1ebfu \u0111i\u1ec1u ki\u1ec7n n\u00e0y tho\u1ea3 m\u00e3n \u0111\u1ed3ng ngh\u0129a v\u1edbi vi\u1ec7c code b\u1ea1n \u0111\u00e3 \u0111\u00fang, model \u0111\u00fang (th\u01b0\u1eddng l\u00e0 th\u1ebf). L\u00fac n\u00e0y, vi\u1ec7c c\u1ee7a b\u1ea1n ch\u1ec9 l\u00e0 tune l\u1ea1i model sao cho ngon h\u01a1n m\u00e0 th\u00f4i","bc5ef37c":"Tr\u01b0\u1edbc khi train model, m\u00ecnh s\u1ebd \u0111\u1ecbnh ngh\u0129a h\u00e0m nonmax_suppression, s\u1eed d\u1ee5ng \u0111\u1ec3 lo\u1ea1i \u0111i c\u00e1c bbox d\u01b0 th\u1eeba c\u1ee7a c\u00f9ng 1 v\u1eadt th\u1ec3\n\n![](https:\/\/images.viblo.asia\/e4da5e7c-da66-4544-9ce7-094348c58258.png)"}}