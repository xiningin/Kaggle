{"cell_type":{"022d23ed":"code","3474ae6a":"code","d40342c5":"code","5261ef41":"code","80348218":"code","87cdc469":"code","05e0a293":"code","05fd6ef4":"code","cc1ffa3b":"code","b826f4ee":"code","258052fe":"code","3b7610e9":"code","22524a0d":"code","0eaf3f40":"code","0b90571c":"code","ffc6595d":"code","56659da5":"code","293cde07":"code","af6b31ef":"code","7579fbc8":"code","cc85b612":"code","354e50b0":"code","5b5b7dde":"code","82b0fed9":"code","8a4ab452":"code","3cfbf2f7":"code","3f31f1d4":"code","ce513bd6":"code","301b160b":"code","8d24a300":"code","fb546a73":"code","433fc302":"code","1662c369":"code","8d9fd9d3":"code","10fd8817":"code","97e2ccf3":"code","6ee1c9d3":"code","75eb324b":"code","10c50478":"code","d6cc447f":"code","aa375592":"code","4e18c8bd":"code","19dfaacb":"code","257bb645":"code","8a70e9e2":"code","4d0ce99e":"code","bb3e8491":"code","ba787a2d":"code","ad61a25f":"code","856ce183":"code","83d808b0":"code","c5e4980e":"code","bba20335":"code","6987fbc2":"code","bd6072d4":"code","9e74ae64":"code","2b640cf5":"code","de6e35e4":"code","4c1bb8a9":"code","9887be66":"code","a5b8227e":"code","addbd01c":"code","0a198149":"code","e7c98c14":"code","5b53f7d1":"code","c452b9fc":"code","dfd445c0":"markdown","346454d2":"markdown","d505c792":"markdown","c99627f6":"markdown","c2a8a232":"markdown","f393955d":"markdown","76c4ac75":"markdown","368295bb":"markdown","6e461d63":"markdown","e3862f09":"markdown","176d1fc4":"markdown","510c23f0":"markdown","06847721":"markdown","c12f7f34":"markdown","9e6c9951":"markdown","e2c55c9d":"markdown","0a4d0227":"markdown","1eb4131b":"markdown","c0e016bd":"markdown","f88a43ef":"markdown","af337046":"markdown","6188daed":"markdown","7a177d3b":"markdown","46761c74":"markdown","27bca0f1":"markdown","4c70140c":"markdown","2ae2721d":"markdown","2e6530dd":"markdown","02437d1b":"markdown","a5f3b3af":"markdown","b850a66a":"markdown","f8a097e6":"markdown","e8cb4875":"markdown","6f854658":"markdown","c3a7540b":"markdown","7af0402a":"markdown"},"source":{"022d23ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3474ae6a":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","d40342c5":"# load data\ndata_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata_test  = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n#preview data\nprint (data_train.info())\nprint(\"-\"*20)\nprint('Train columns with null values:\\n', data_train.isnull().sum())\nprint(\"-\"*20)\nprint (data_test.info())\nprint('Train columns with null values:\\n', data_test.isnull().sum())\nprint(\"-\"*20)\ndata_train.sample(10)","5261ef41":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data_train,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","80348218":"#sex\nf,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=data_train,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","87cdc469":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train['Pclass'].value_counts().plot.bar(color=['#FF8C00','#FFD700','#FF4500'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data_train,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","05e0a293":"plt.figure(figsize=(18,8))\nplt.subplot(1, 2, 1)\nsns.barplot(x = \"Pclass\", \n            y = \"Survived\", \n            data=data_train, \n            linewidth=5,\n            capsize = .1)\nplt.title(\"Paclass - Survived\")\nplt.xlabel(\"Socio-Economic class\");\nplt.ylabel(\"% of Passenger Survived\");\nlabels = ['Upper', 'Middle', 'Lower']\n#val = sorted(train.Pclass.unique())\nval = [0,1,2] ## this is just a temporary trick to get the label right. \nplt.xticks(val, labels);\n# Kernel Density Plot\nplt.subplot(1, 2, 2)\nax=sns.kdeplot(data_train.Pclass[data_train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 1),'Pclass'] ,\n               color='g',\n               shade=True, \n               label='survived', )\nplt.title('Passenger Class Distribution - Survived vs Non-Survived')\nplt.ylabel(\"Frequency of Passenger Survived\")\nplt.xlabel(\"Passenger Class\")\nplt.xticks(sorted(data_train.Pclass.unique()), labels);","05fd6ef4":"sns.factorplot('Pclass','Survived',hue='Sex',data=data_train)\nplt.show()","cc1ffa3b":"print('Oldest Passenger was of:',data_train['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data_train['Age'].min(),'Years')\nprint('Average Age on the ship:',data_train['Age'].mean(),'Years')\nplt.figure(figsize=(18,8))\nplt.hist(data_train['Age'], \n        bins = np.arange(data_train['Age'].min(),data_train['Age'].max(),5),\n        normed = True, \n        color = 'steelblue',\n        edgecolor = 'k')\nplt.title('Distribution of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nax=sns.kdeplot(data_train['Age'] , \n               color='red',\n               shade=False,\n               label='not survived')\nplt.tick_params(top='off', right='off')\nplt.legend([ax],['KDE'],loc='best')\nplt.show()","b826f4ee":"data_cleaner = [data_train, data_test]\ndata_train['Title'], data_test['Title']=0,0\nfor dataset in data_cleaner:\n    dataset['Title']=dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nprint(data_train['Title'].value_counts())\nprint(\"-\"*20)\nprint(data_test['Title'].value_counts())","258052fe":"for dataset in data_cleaner:\n    dataset['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','the Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],\n                                ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Mr','Mrs'],inplace=True)\nprint(data_train.groupby('Title')['Age'].mean())\nprint(\"-\"*20)\nprint(data_test.groupby('Title')['Age'].mean())","3b7610e9":"# Assigning the NaN Values with the Ceil values of the mean ages\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Mr'),'Age']=33\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Mrs'),'Age']=36\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Master'),'Age']=5\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Miss'),'Age']=22\n\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Mr'),'Age']=32\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Mrs'),'Age']=40\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Master'),'Age']=7\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Miss'),'Age']=22\n\nfor dataset in data_cleaner:\n    print(dataset.Age.isnull().any())","22524a0d":"#now we see the distribution again\nprint('Oldest Passenger was of:',data_train['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data_train['Age'].min(),'Years')\nprint('Average Age on the ship:',data_train['Age'].mean(),'Years')\nplt.figure(figsize=(18,8))\nplt.hist(data_train['Age'], \n        bins = np.arange(data_train['Age'].min(),data_train['Age'].max(),5),\n        normed = True, \n        color = 'steelblue',\n        edgecolor = 'k')\nplt.title('Distribution of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nax=sns.kdeplot(data_train['Age'] , \n               color='red',\n               shade=False,\n               label='not survived')\nplt.tick_params(top='off', right='off')\nplt.legend([ax],['KDE'],loc='best')\nplt.show()\n","0eaf3f40":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train[data_train['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='g')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata_train[data_train['Survived']==1].Age.plot.hist(ax=ax[1],color='gray',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()\n\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 25, pad = 40)\nplt.xlabel(\"Age\", fontsize = 15, labelpad = 20)\nplt.ylabel('Frequency', fontsize = 15, labelpad= 20);","0b90571c":"# check the missing value\ndata_train[data_train.Embarked.isnull()]","ffc6595d":"sns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=data_train, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=data_test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 18)\nax2.set_title('Test Set',  fontsize = 18)","56659da5":"data_train.Embarked.fillna(\"C\", inplace=True)\ndata_train.Embarked.isnull().any()","293cde07":"f,ax=plt.subplots(1,3,figsize=(20,6))\nsns.countplot('Embarked',data=data_train,ax=ax[0])\nax[0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Survived',data=data_train,ax=ax[1])\nax[1].set_title('Embarked vs Survived')\nsns.barplot(x = 'Embarked', y = 'Survived', data=data_train,ax=ax[2])\nax[2].set_title('Embarked Survival Rate')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","af6b31ef":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.countplot('Embarked',hue='Sex',data=data_train,ax=ax[0])\nax[0].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Pclass',data=data_train,ax=ax[1])\nax[1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","7579fbc8":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data_train)\nplt.show()","cc85b612":"pd.crosstab([data_train.SibSp],data_train.Survived).style.background_gradient(cmap='summer_r')","354e50b0":"f,ax=plt.subplots(2,2,figsize=(20,12))\nsns.barplot('SibSp','Survived',data=data_train,ax=ax[0,0])\nax[0,0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp','Survived',data=data_train,ax=ax[0,1])\nax[0,1].set_title('SibSp vs Survived')\nsns.barplot('Parch','Survived',data=data_train,ax=ax[1,0])\nax[1,0].set_title('Parch vs Survived')\nsns.factorplot('Parch','Survived',data=data_train,ax=ax[1,1])\nax[1,1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","5b5b7dde":"pd.crosstab(data_train.SibSp,data_train.Pclass).style.background_gradient(cmap='summer_r')","82b0fed9":"# Fare\n# first check the missing value\ndata_test[data_test.Fare.isnull()]","8a4ab452":"missing_value = data_test[(data_test.Pclass == 3) & (data_test.Embarked == \"S\") & (data_test.Sex == \"male\")].Fare.mean()\n# replace the test.fare null values with test.fare mean\ndata_test.Fare.fillna(missing_value, inplace=True)\ndata_test.Embarked.isnull().any()","3cfbf2f7":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 0),'Fare'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Fare\", fontsize = 15, labelpad = 20);","3f31f1d4":"sns.scatterplot(data_train[\"Fare\"], data_train[\"Age\"])","ce513bd6":"data_train[data_train.Fare > 300]","301b160b":"# drop the outliers\ndata_train = data_train[data_train.Fare < 300]","8d24a300":"# check the missing value\nprint(\"Train Cabin missing: \" + str(data_train.Cabin.isnull().sum()\/len(data_train.Cabin)))\nprint(\"Test Cabin missing: \" + str(data_test.Cabin.isnull().sum()\/len(data_test.Cabin)))","fb546a73":"survivers = data_train.Survived # save the label for a while\ndata_train.drop([\"Survived\"],axis=1, inplace=True)\nall_data = pd.concat([data_train,data_test], ignore_index=False)\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)\nall_data.sort_values(\"Cabin\").head(10)","433fc302":"all_data.Cabin = [i[0] for i in all_data.Cabin]\nall_data[\"Cabin\"].value_counts(normalize=True)","1662c369":"with_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()","8d9fd9d3":"def cabin_estimator(i):\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ndata_train = all_data[:891]\n\ndata_test = all_data[891:]\n\n# adding saved target variable with train. \ndata_train['Survived'] = survivers","10fd8817":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.countplot('Cabin',data=data_train, ax=ax[0])\nsns.barplot('Cabin','Survived',data=data_train, ax=ax[1])","97e2ccf3":"# Placing 0 for female and 1 for male in the \"Sex\" column. \ndata_train['Sex'] = data_train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ndata_test['Sex'] = data_test.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\nsns.heatmap(data_train.corr(),annot=True,cmap='coolwarm',linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","6ee1c9d3":"#family size\ndata_train['family_size'] = data_train.SibSp + data_train.Parch+1\ndata_test['family_size'] = data_test.SibSp + data_test.Parch+1","75eb324b":"#is_alone\ndata_train['is_alone'] = [1 if i<2 else 0 for i in data_train.family_size]\ndata_test['is_alone'] = [1 if i<2 else 0 for i in data_test.family_size]","10c50478":"f,ax=plt.subplots(1,2,figsize=(18,6))\nsns.factorplot('family_size','Survived',data=data_train,ax=ax[0])\nax[0].set_title('family_size vs Survived')\nsns.factorplot('is_alone','Survived',data=data_train,ax=ax[1])\nax[1].set_title('is_alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","d6cc447f":"def family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n# gen fanily group by its size\ndata_train['family_group'] = data_train['family_size'].map(family_group)\ndata_test['family_group'] = data_test['family_size'].map(family_group)","aa375592":"sns.factorplot('is_alone','Survived',data=data_train,hue='Sex',col='Pclass')\nplt.show()","4e18c8bd":"#age group\nplt.subplots(figsize = (22,10),)\nsns.distplot(data_train.Age, bins = 100, kde = True, rug = False, norm_hist=False);","19dfaacb":"def age_group_fun(age):\n    a= ''\n    if age <= 10:\n        a = \"child\"\n    elif age <= 22:\n        a = \"teenager\"\n    elif age <= 33:\n        a = \"yong_Adult\"\n    elif age <= 45:\n        a = \"middle_age\"\n    else:\n        a = \"old\"\n    return a\n\ndata_train['age_group'] = data_train['Age'].map(age_group_fun)\ndata_test['age_group'] = data_test['Age'].map(age_group_fun)","257bb645":"#fare group\ndata_train['Fare_Range']=pd.qcut(data_train['Fare'],4)\ndata_train.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","8a70e9e2":"def fare_group_fun(fare):\n    a= ''\n    if fare <= 7.896:\n        a = \"low\"\n    elif fare <= 14.454:\n        a = \"normal\"\n    elif fare <= 30.696:\n        a = \"middle\"\n    else:\n        a = \"high\"\n    return a\n\ndata_train['fare_group'] = data_train['Fare'].map(fare_group_fun)\ndata_test['fare_group'] = data_test['Fare'].map(fare_group_fun)","4d0ce99e":"# prepare for onehot encoding\ndata_train['Pclass'].astype(str)\ndata_train['Sex'].astype(str)\ndata_train['is_alone'].astype(str)\ndata_test['Pclass'].astype(str)\ndata_test['Sex'].astype(str)\ndata_test['is_alone'].astype(str)","bb3e8491":"# onehot encoding & drop unused variables\ndata_train = pd.get_dummies(data_train, columns=['Title',\"Pclass\", 'Sex','is_alone','Cabin','age_group','Embarked','family_group', 'fare_group'], drop_first=False)\ndata_test = pd.get_dummies(data_test, columns=['Title',\"Pclass\", 'Sex','is_alone','Cabin','age_group','Embarked','family_group', 'fare_group'], drop_first=False)\ndata_train.drop(['family_size','Name','PassengerId','Ticket','Fare_Range'], axis=1, inplace=True)\npassengerid = data_test['PassengerId'].values\ndata_test.drop(['family_size','Name','PassengerId','Ticket'], axis=1, inplace=True)","ba787a2d":"data_train.head(10)","ad61a25f":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\nage_fare = data_train[['Age','Fare']] # get age and fare features\nage_fare = mm.fit_transform(age_fare)\nage_fare_df = pd.DataFrame(age_fare, columns=['Age','Fare']) # scaling data\ndata_train.drop(['Age','Fare'], axis=1, inplace=True)\ndata_train = data_train.reset_index(drop=True)\ndata_train = pd.concat([data_train, age_fare_df],axis=1) # merge the scaling data back to train data set\n\nage_fare = data_test[['Age','Fare']] #same for test\nage_fare = mm.fit_transform(age_fare)\nage_fare_df = pd.DataFrame(age_fare, columns=['Age','Fare']) # scaling data\ndata_test.drop(['Age','Fare'], axis=1, inplace=True)\ndata_test = data_test.reset_index(drop=True)\ndata_test = pd.concat([data_test, age_fare_df],axis=1)","856ce183":"data_train","83d808b0":"data_test","c5e4980e":"# Split the training data into test and train datasets\nX = data_train.drop(['Survived'], axis = 1)\ny = data_train[\"Survived\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)","bba20335":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) \ncolumn_names = X.columns\nX = X.values\n\n# use grid search to get the best parameters\nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18] #alpla of lasso and ridge \npenalties = ['l1','l2'] # Choosing penalties(Lasso(l1) or Ridge(l2))\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25) # Choose a cross validation strategy. \nparam = {'penalty': penalties, 'C': C_vals} # setting param for param_grid in GridSearchCV. \nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                     cv = cv\n                          )\n## Fitting the model\ngrid.fit(X, y)\n\n# get accuracy\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)","6987fbc2":"from sklearn.neighbors import KNeighborsClassifier\nk_range = range(1,31)\nweights_options=['uniform','distance']\nparam = {'n_neighbors':k_range, 'weights':weights_options}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\ngrid.fit(X,y)\nknn_grid= grid.best_estimator_\nknn_grid.score(X,y)","bd6072d4":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(X_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)","9e74ae64":"from sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\ngrid_search.fit(X,y)\nsvm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)","2b640cf5":"from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) \ndectree_grid = grid.best_estimator_\ndectree_grid.score(X,y)","de6e35e4":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \nrf_grid = grid.best_estimator_\nrf_grid.score(X,y)","4c1bb8a9":"from sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \nbagging_grid = grid.best_estimator_\nbagging_grid.score(X,y)","9887be66":"from sklearn.ensemble import AdaBoostClassifier\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                     ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \nadaBoost_grid = grid.best_estimator_\nadaBoost_grid.score(X,y)","a5b8227e":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X, y)\ny_pred = gradient_boost.predict(X_test)\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gradient_accy)","addbd01c":"from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(X_test)\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(extraTree_accy)","0a198149":"from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(X_test)\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gau_pro_accy)","e7c98c14":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lr_grid', logreg_grid),\n    ('svc', svm_grid),\n    ('random_forest', rf_grid),\n    ('gradient_boosting', gradient_boost),\n    ('decision_tree_grid',dectree_grid),\n    ('knn_classifier', knn_grid),\n    #('XGB_Classifier', XGBClassifier),\n    ('bagging_classifier', bagging_grid),\n    ('adaBoost_classifier',adaBoost_grid),\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\n    ('gaussian_classifier',gaussian),\n    ('gaussian_process_classifier', GaussianProcessClassifier)\n],voting='hard')\n\n#voting_classifier = voting_classifier.fit(train_x,train_y)\nvoting_classifier = voting_classifier.fit(X,y)\n\ny_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)","5b53f7d1":"all_models = [logreg_grid,\n              knn_grid, \n              svm_grid,\n              dectree_grid,\n              rf_grid,\n              bagging_grid,\n              adaBoost_grid,\n              voting_classifier]\n\nc = {}\nfor i in all_models:\n    a = i.predict(X_test)\n    b = accuracy_score(a, y_test)\n    c[i] = b","c452b9fc":"test_prediction = (max(c, key=c.get)).predict(data_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": test_prediction})\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic2_submission.csv\", index=False)","dfd445c0":"### **Observation:**\n\nThe Child with age smaller than 5 were saved in large numbers(The Women and Child First Policy).\n\nThe oldest Passenger was saved(80 years).\n\nMaximum number of deaths were in the age group of 30-40.","346454d2":"### AdaBoost Classifier","d505c792":"### Bagging Classifier","c99627f6":"### Extra Trees Classifier","c2a8a232":"### Support Vector Machines(SVM)","f393955d":"### Gaussian Process Classifier","76c4ac75":"# **Data Cleaning**","368295bb":"As the fare_range increases, the changes of survival increases","6e461d63":"### Creating dummy variables","e3862f09":"## Sex and Pclass","176d1fc4":"## Embarked","510c23f0":"### Gradient Boosting Classifier","06847721":"Out of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash.","c12f7f34":"From the factor plot, we can know that urvival for Women from Pclass1 is about 95-96%. It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.","9e6c9951":"### Voting Classifier","e2c55c9d":"### **Observation:**\n\nMost people stay in G and the survival rate of G is low. (Money matters)\nThe survival rates of E, D, B are relatively high","0a4d0227":"## Survived","1eb4131b":"Remember we have 177 missing values in Age feature, we can assign them with the mean age of the existing data.","c0e016bd":"\n### **Observations:**\n\n1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Embarkation.\n\n2)The survival rate for both men and women is very low in S (most passenegers are from Pclass)\n\n3)The survival rate for men is extremely low in Q, as almost all were from Pclass3","f88a43ef":"## Sex","af337046":"### K-Nearest Neighbor","6188daed":"### Scaling data","7a177d3b":"### Decision Tree","46761c74":"# Feature Engineering\n\n## Correlation Matrix and Heatmap","27bca0f1":"### Random Forest","4c70140c":"We decide to assign the N based on their fare (compare to the mean fare of each cabin)","2ae2721d":"## Cabin","2e6530dd":"## Age","02437d1b":"## Model building\n\n### Logistic Regression","a5f3b3af":"### **Observations:**\n\n1)Most passenegers boarded from S and the majority of them are from Pclass3.\n\n2)The passengers from C has a good proportion of them survived. The reason may be is that most of them are from Pclass1 and Pclass2, where the rescued rates are high.\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around 81% didn't survive.\n\n4)Port Q had almost 95% of the passengers were from Pclass3.","b850a66a":"## Pclass","f8a097e6":"### Gaussian Naive Bayes","e8cb4875":"It seems that a fare of 512 is an outlier of the fare feature. ","6f854658":"Here, in both training set and test set, the average fare closest to $80 are in the C Embarked values. ","c3a7540b":"* 63% first class passenger survived titanic tragedy, while\n* 48% second class and\n* only 24% third class passenger survived.\n\nPassenegers Of Pclass 1 were given a very high priority while rescue. Even though the the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low. So money and status matters. However, lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second.","7af0402a":"The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18-19%. (Women and Children first policy)"}}