{"cell_type":{"3073cab2":"code","499e6ecb":"code","9ce4970e":"code","4842d96b":"code","1e645aa1":"code","bbdde4af":"code","342e2176":"code","1c8297ef":"code","08a457be":"code","cc2230da":"code","ae071480":"code","9c7700f9":"code","e3649ff1":"code","0d4bb5c9":"code","28e01fc0":"code","b99c474b":"code","6ae562c7":"code","7120a29e":"markdown","cde60007":"markdown","3a58952b":"markdown","75987614":"markdown","c1d80a7b":"markdown","b4e934a1":"markdown","fc93aa63":"markdown","730c256e":"markdown","327f23b2":"markdown","0c05466a":"markdown","287470fc":"markdown","5c486ae8":"markdown","c260f7e1":"markdown","62468f02":"markdown","eaf74fef":"markdown","8769151c":"markdown","4869cae8":"markdown","f716e753":"markdown","498997c2":"markdown"},"source":{"3073cab2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","499e6ecb":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Read the data\ndata = pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')\ndata.head()","9ce4970e":"#Checking the categories in categorical variables\n\nprint(data['Geography'].unique())\nprint(data['Gender'].unique())","4842d96b":"#Find blanks in data\ndata.info()","1e645aa1":"#create a dict file to convert string variable into numerical one\n# for Gender column\ngender = {'Male':0, 'Female':1}\ndata.Gender = [gender[item] for item in data.Gender]\ndata.head()","bbdde4af":"#create a dict file to convert string variable into numerical one\n#For contries\ngeo = {'France':1, 'Spain':2, 'Germany':3}\ndata.Geography = [geo[item] for item in data.Geography]\ndata.head()","342e2176":"# delete the unnecessary features from dataset\ndata.pop('CustomerId')\ndata.pop('Surname')\ndata.pop('RowNumber')\ndata.head()","1c8297ef":"corr = data.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot = True, annot_kws={'size':12})\nheat_map=plt.gcf()\nheat_map.set_size_inches(20,15)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","08a457be":"from sklearn.model_selection import train_test_split \ntrain, test = train_test_split(data, test_size = 0.25)\n \ntrain_y = train['Exited']\ntest_y = test['Exited']\n \ntrain_x = train\ntrain_x.pop('Exited')\ntest_x = test\ntest_x.pop('Exited')","cc2230da":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n \nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(X=train_x, y=train_y)\n \ntest_y_pred = logisticRegr.predict(test_x)\nconfusion_matrix = confusion_matrix(test_y, test_y_pred)\nprint('Intercept: ' + str(logisticRegr.intercept_))\nprint('Regression: ' + str(logisticRegr.coef_))\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logisticRegr.score(test_x, test_y)))\nprint(classification_report(test_y, test_y_pred))\n \nconfusion_matrix_df = pd.DataFrame(confusion_matrix, ('No churn', 'Churn'), ('No churn', 'Churn'))\nheatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)","ae071480":"# Checking how many customers exited\ndata['Exited'].value_counts()","9c7700f9":"from sklearn.utils import resample\n \ndata_majority = data[data['Exited']==0]\ndata_minority = data[data['Exited']==1]\n \ndata_minority_upsampled = resample(data_minority,\nreplace=True,\nn_samples=7963, #same number of samples as majority classe\nrandom_state=1) #set the seed for random resampling\n# Combine resampled results\ndata_upsampled = pd.concat([data_majority, data_minority_upsampled])\n \ndata_upsampled['Exited'].value_counts()","e3649ff1":"train, test = train_test_split(data_upsampled, test_size = 0.25)\n \ntrain_y_upsampled = train['Exited']\ntest_y_upsampled = test['Exited']\n \ntrain_x_upsampled = train\ntrain_x_upsampled.pop('Exited')\ntest_x_upsampled = test\ntest_x_upsampled.pop('Exited')\n \nlogisticRegr_balanced = LogisticRegression()\nlogisticRegr_balanced.fit(X=train_x_upsampled, y=train_y_upsampled)\n \ntest_y_pred_balanced = logisticRegr_balanced.predict(test_x_upsampled)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logisticRegr_balanced.score(test_x_upsampled, test_y_upsampled)))\nprint(classification_report(test_y_upsampled, test_y_pred_balanced))\n","0d4bb5c9":"from sklearn.metrics import roc_auc_score\n \n# Get class probabilities for both models\ntest_y_prob = logisticRegr.predict_proba(test_x)\ntest_y_prob_balanced = logisticRegr_balanced.predict_proba(test_x_upsampled)\n \n# We only need the probabilities for the positive class\ntest_y_prob = [p[1] for p in test_y_prob]\ntest_y_prob_balanced = [p[1] for p in test_y_prob_balanced]\n \nprint('Unbalanced model AUROC: ' + str(roc_auc_score(test_y, test_y_prob)))\nprint('Balanced model AUROC: ' + str(roc_auc_score(test_y_upsampled, test_y_prob_balanced)))","28e01fc0":"from sklearn import tree\nfrom sklearn import tree\n \n# Create each decision tree (pruned and unpruned)\ndecisionTree_unpruned = tree.DecisionTreeClassifier()\ndecisionTree = tree.DecisionTreeClassifier(max_depth = 4)\n \n# Fit each tree to our training data\ndecisionTree_unpruned = decisionTree_unpruned.fit(X=train_x, y=train_y)\ndecisionTree = decisionTree.fit(X=train_x, y=train_y)\n \ntest_y_pred_dt = decisionTree.predict(test_x)\ntest_y_pred_dt = decisionTree_unpruned.predict(test_x)\ntest_y_pred_dt = decisionTree.predict(train_x)\ntest_y_pred_dt = decisionTree_unpruned.predict(train_x)\nprint('Accuracy of unpruned decision tree classifier on train set: {:.2f}'.format(decisionTree_unpruned.score(train_x, train_y)))\nprint('Accuracy of unpruned decision tree classifier on test set: {:.2f}'.format(decisionTree_unpruned.score(test_x, test_y)))\nprint('Accuracy of decision tree classifier on train set: {:.2f}'.format(decisionTree.score(train_x, train_y)))\nprint('Accuracy of decision tree classifier on test set: {:.2f}'.format(decisionTree.score(test_x, test_y)))","b99c474b":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","6ae562c7":"clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, test_x, test_y, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","7120a29e":"# Conclusion","cde60007":"### Up-sampling the minority class","3a58952b":"Now that we have a 1:1 ratio for our classes, let\u2019s train another logistic regression model:","75987614":"We can see that there is no high correlation between features. therefore, there is no multicollineality problem","c1d80a7b":"# KNN","b4e934a1":"To decide which features of the data to include in our predictive churn model, we\u2019ll examine the correlation between churn and each customer feature","fc93aa63":"Interestingly, the AUROC scores are very similar between the two models. Both are above 0.5 however, suggesting that both models have the ability to distiguish between observations from each class.","730c256e":"Decision tree is outperforming other models considered in this practice. Therefore, Decison tree model could be a better choice.","327f23b2":"### handling imbalanced classes\n\nIt is also important to look at the distribution of how many customers churn. If 95% of customers don\u2019t churn, we can achieve 95% accuracy by building a model that simply predicts that all customers won\u2019t churn. But this isn\u2019t a very useful model, because it will never tell us when a customer will churn, which is what we are really interested in.","0c05466a":"### Predictive modelling ###\n\n\nWe will consider several different models to predict customer churn. To ensure we are not over-fitting to our data, we will split the 10,000 customer records into a training and test set, with the test set being 25% of the total records.","287470fc":"The overall accuracy of the model has decreased, but the precision and recall scores for predicting a churn have improved.","5c486ae8":"Exactly as we suspected! The unpruned tree gets a perfect score on the training set, but a relatively lower score (81%) on the test set. Our pruned tree is less accurate on the training set, but performs better when presented with the out-of-sample test data.","c260f7e1":"We got 78% classification accuracy from our logistic regression classifier. But the precision and recall for predictions in the positive class (churn) are relatively low, which suggests our data set may be imbalanced.","62468f02":"We can notice that even KNN is doing better for this classification problem","eaf74fef":"### Correlations between customer data features and customer churn ###","8769151c":"### Logistic regression ####\n\nLogistic regression is one of the more basic classification algorithms in a data scientist\u2019s toolkit. It is used to predict a category or group based on an observation. Logistic regression is usually used for binary classification (1 or 0, win or lose, true or false). The output of logistic regression is a probability, which will always be a value between 0 and 1. While the output value does not give a classification directly, we can choose a cutoff value so that inputs with with probability greater than the cutoff belong to one class, and those with less than the cutoff belong to the other.\n\nFor example, if the classifier predicts a probability of customer attrition being 70%, and our cutoff value is 50%, then we predict that the customer will churn. Similarly, if the model outputs a 30% chance of attrition for a customer, then we predict that the customer won\u2019t churn","4869cae8":"# Cross validation (k_Fold)","f716e753":"# Decision Trees","498997c2":"### Using a different performance metric"}}