{"cell_type":{"ea0052e2":"code","90cc7c97":"code","5d74ad74":"code","a3d041b6":"code","e5f85e1e":"code","272c6551":"code","da6e74d7":"code","59e11c9a":"code","7de16e80":"code","1315593e":"code","67752d4e":"code","0517bad9":"code","91139ebc":"code","d5069766":"code","bcd5e19f":"code","bcca1256":"code","697c9102":"code","92356762":"code","5e8be98f":"code","82ffd032":"code","94dbe649":"code","240759d5":"code","d4710b04":"code","7fb9bfd8":"code","7b17f51a":"code","3383c41a":"code","58468100":"code","eb6ad0ab":"code","2234b93a":"code","5f0c4fcd":"code","13b47067":"code","43cee3c0":"code","e6838f44":"code","fbd68abe":"code","7c126dde":"code","5a349205":"code","39d22a3a":"code","80d96bcf":"code","0a9d139d":"code","9ee1463b":"code","783823d5":"code","f20f91ce":"code","cb3fbbc8":"code","26657bc5":"code","06ca94b7":"code","606a687c":"code","b26fb661":"code","fddd547a":"code","dd74e2da":"code","8e2c96af":"code","a6a6c179":"code","459c0fb8":"code","171069d7":"code","ef1c1de9":"code","c95b6b61":"code","0239c814":"code","4f58453b":"code","bd60281c":"code","5444acb3":"code","23ab5ca9":"code","ff297322":"code","f61ad40b":"code","35a921b2":"code","1cc3c6c8":"code","02c4a0b4":"code","9dba91a9":"code","2a6b1b47":"code","53ea8944":"code","45fcfa61":"code","86fb0a15":"code","0c5a6335":"code","87092c3c":"code","090cf299":"code","20454511":"code","e9e13122":"markdown","7106715f":"markdown","35b773e6":"markdown","f00e31d4":"markdown","53cfc067":"markdown","f6d32710":"markdown","4b483f14":"markdown","5b778aa9":"markdown","1cd4ff47":"markdown","9f8060ee":"markdown","633750fe":"markdown","019af786":"markdown","6b53bcf0":"markdown","211cc3aa":"markdown","dbe5a48c":"markdown"},"source":{"ea0052e2":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","90cc7c97":"pd.set_option('display.max_columns', None)","5d74ad74":"from sklearn.model_selection import train_test_split","a3d041b6":"data_train = pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv\",header=1)","e5f85e1e":"data_train.head(3)","272c6551":"data_train.info()","da6e74d7":"data_test=pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv\")","59e11c9a":"data_test.head(3)","7de16e80":"data_submission=pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv\")","1315593e":"data_submission.head(2)","67752d4e":"data_train.describe()","0517bad9":"data_train.info()","91139ebc":"data_train.isnull().sum()","d5069766":"data_train.rename(columns = {'Unnamed: 2':'DateReported', 'Unnamed: 7':'DependentsOther', 'Unnamed: 11':'DaysWorkedPerWeek'}, inplace = True)","bcd5e19f":"data_train.head(3)","bcca1256":"data_test['MaritalStatus'].isnull().sum()","697c9102":"fig = plt.figure(figsize=(10,6))\nplt.suptitle('Countplot of Marital Status: M - Married; S - Single; U - Unknown')\nplt.subplot(1, 2, 1)\ndata_train['MaritalStatus'].value_counts(dropna = False).plot(kind = 'bar', rot = 0)\n\nplt.subplot(1, 2, 2)\ndata_test['MaritalStatus'].value_counts(dropna = False).plot(kind = 'bar', rot = 0)","92356762":"data_train['MaritalStatus'].fillna('U', inplace = True)\ndata_test['MaritalStatus'].fillna('U', inplace = True)","5e8be98f":"data_train['WeeklyWages'].describe()","82ffd032":"data_train[\"WeeklyWages\"]=data_train[\"WeeklyWages\"].astype(float)","94dbe649":"data_train[\"HoursWorkedPerWeek\"]=data_train[\"HoursWorkedPerWeek\"].astype(float)","240759d5":"m=data_train['WeeklyWages'].mean()","d4710b04":"data_train['WeeklyWages'].fillna(m, inplace = True)","7fb9bfd8":"data_test['WeeklyWages'].fillna(m, inplace = True)","7b17f51a":"data_train['HoursWorkedPerWeek'].describe()","3383c41a":"data_train['HoursWorkedPerWeek']=data_train['HoursWorkedPerWeek'].astype(float)","58468100":"data_train['HoursWorkedPerWeek'].fillna(data_train['HoursWorkedPerWeek'].mean(), inplace = True)","eb6ad0ab":"data_test['HoursWorkedPerWeek'].fillna(data_train['HoursWorkedPerWeek'].mean(), inplace = True)","2234b93a":"data_train.isnull().sum()","5f0c4fcd":"data_train['Age'].describe()","13b47067":"data_test['Age'].describe()","43cee3c0":"plt.suptitle('Distribution of Age')\n\nplt.subplot(1, 2, 1)\nsns.distplot(data_train['Age'], color = '#810f7c')\nplt.title('Train')\n\nplt.subplot(1, 2, 2)\nsns.distplot(data_test['Age'], color = '#8c96c6')\nplt.title('Test');","e6838f44":"data_train['Age']=data_train['Age'].astype(int)","fbd68abe":"bins= [10,20,30,40,50,60,70,80,90]\nlabels = ['10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90']\ndata_train['AgeGroup'] = pd.cut(data_train['Age'], bins=bins, labels=labels, right=False)\ndata_train.head()","7c126dde":"bins= [10,20,30,40,50,60,70,80,90]\nlabels = ['10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90']\ndata_test['AgeGroup'] = pd.cut(data_test['Age'], bins=bins, labels=labels, right=False)\ndata_test.head()","5a349205":"sns.countplot(data_train['AgeGroup'])","39d22a3a":"sns.countplot(data_train['Gender'])","80d96bcf":"sns.countplot(data_train['MaritalStatus'])","0a9d139d":"fig = plt.figure(figsize=(10,4))\nplt.subplot(1, 2 , 1)\nsns.countplot(data_train['DependentChildren'])\nplt.subplot(1, 2 , 2)\nsns.countplot(data_train['DependentsOther']);","9ee1463b":"data_train['YearOfAccident']  = pd.DatetimeIndex(data_train['DateTimeOfAccident']).year\ndata_train['MonthOfAccident']  = pd.DatetimeIndex(data_train['DateTimeOfAccident']).month\ndata_train['DayOfAccident']  = pd.DatetimeIndex(data_train['DateTimeOfAccident']).day\ndata_train['WeekdayOfAccident']  = pd.DatetimeIndex(data_train['DateTimeOfAccident']).day_name()\ndata_train['HourOfAccident']  = pd.DatetimeIndex(data_train['DateTimeOfAccident']).hour\ndata_train['YearReported']  = pd.DatetimeIndex(data_train['DateReported']).year","783823d5":"data_test['YearOfAccident']  = pd.DatetimeIndex(data_test['DateTimeOfAccident']).year\ndata_test['MonthOfAccident']  = pd.DatetimeIndex(data_test['DateTimeOfAccident']).month\ndata_test['DayOfAccident']  = pd.DatetimeIndex(data_test['DateTimeOfAccident']).day\ndata_test['WeekdayOfAccident']  = pd.DatetimeIndex(data_test['DateTimeOfAccident']).day_name()\ndata_test['HourOfAccident']  = pd.DatetimeIndex(data_test['DateTimeOfAccident']).hour\ndata_test['YearReported']  = pd.DatetimeIndex(data_test['DateReported']).year","f20f91ce":"# Reporting delay in weeks \ndata_train['DaysReportDelay'] = pd.DatetimeIndex(data_train['DateReported']).date - pd.DatetimeIndex(data_train['DateTimeOfAccident']).date\ndata_train['DaysReportDelay'] = (data_train['DaysReportDelay']  \/ np.timedelta64(1, 'D')).astype(int)\ndata_train['WeeksReportDelay'] = np.floor(data_train['DaysReportDelay'] \/ 7.).astype(int)\ndata_train['WeeksReportDelay'] = np.clip(data_train['WeeksReportDelay'], a_max=55, a_min=None)\n#data_train['WeeksReportDelay']\n# drop unneccessary columns\n#df.drop(['ClaimNumber','DateTimeOfAccident','DaysReportDelay','DateReported'],axis=1,inplace=True)","cb3fbbc8":"data_test['DaysReportDelay'] = pd.DatetimeIndex(data_test['DateReported']).date - pd.DatetimeIndex(data_test['DateTimeOfAccident']).date\ndata_test['DaysReportDelay'] = (data_test['DaysReportDelay']  \/ np.timedelta64(1, 'D')).astype(int)\ndata_test['WeeksReportDelay'] = np.floor(data_test['DaysReportDelay'] \/ 7.).astype(int)\ndata_test['WeeksReportDelay'] = np.clip(data_test['WeeksReportDelay'], a_max=55, a_min=None)","26657bc5":"fig = plt.figure(figsize=(10,6))\nsns.countplot(data_train['WeekdayOfAccident'])","06ca94b7":"fig = plt.figure(figsize=(10,6))\nsns.countplot(data_train['MonthOfAccident'])","606a687c":"fig = plt.figure(figsize=(10,6))\nsns.countplot(data_train['YearOfAccident'])","b26fb661":"#display(data_train[data_train['TimeDiff_Hrs'] >0] )","fddd547a":"fig = plt.figure(figsize=(10,4))\ndf_temp = data_train.groupby(\"AgeGroup\")[\"WeeklyWages\"].count().reset_index()\n","dd74e2da":"sns.barplot(x=\"WeeklyWages\", y=\"AgeGroup\", orient='h', data=df_temp, color=\"green\")\nplt.xlabel('Count')\nplt.title(f\"Number of claims by age in Train set\");","8e2c96af":"fig = plt.figure(figsize=(10,4))\ndf_temp = data_train.groupby(\"PartTimeFullTime\")[\"WeeklyWages\"].count().reset_index()\nsns.barplot(x=\"WeeklyWages\", y=\"PartTimeFullTime\", orient='h', data=df_temp, color=\"blue\")\nplt.xlabel('Count')","a6a6c179":"fig = plt.figure(figsize=(15,25))\npd.pivot_table(data = data_train, index = 'Gender', \n               columns = ['MaritalStatus'], \n               values = ['InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']).plot(kind = 'bar', \n                                                                                         figsize=(10, 6),\n                                                                                         #xTitle = 'Gender', \n                                                                                         #yTitle = 'Claims Cost', \n                                                                                         title = 'Claims by MaritalStatus')\n","459c0fb8":"numerical_features = [c for c in data_train.columns if data_train[c].dtype in ['float64', 'int64'] if c not in ['Acc_Day', 'Acc_Month', 'Acc_Year']]\ncategorical_features = [c for c in data_train.columns if c not in numerical_features]\nnumerical_features, categorical_features","171069d7":"corr1 = data_train[numerical_features].corr(method = 'pearson')\n\nfig = plt.figure(figsize = (10, 8))\n#mask = np.triu(np.ones_like(corr1, dtype = bool))\nsns.heatmap(corr1, mask = None, annot = True, cmap = 'PiYG', vmin = -1, vmax = +1)\nplt.title('Pearson Correlation')\nplt.xticks(rotation = 90)\nplt.show()","ef1c1de9":"import sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\nimport sklearn.linear_model as lm","c95b6b61":"#data_train = data_train.drop(data_train[(data_train['InitialIncurredCalimsCost']>30000)])","0239c814":"data_train['InitialIncurredCalimsCost'] = np.log1p(data_train['InitialIncurredCalimsCost'])\ndata_test['InitialIncurredCalimsCost'] = np.log1p(data_test['InitialIncurredCalimsCost'])\n\ntarget = data_train['UltimateIncurredClaimCost']\n\ndata_train.drop(['ClaimNumber', 'ClaimDescription', 'UltimateIncurredClaimCost'], axis = 1, inplace = True)\ndata_test.drop(['ClaimNumber', 'ClaimDescription'], axis = 1, inplace = True)\n\nnumerical_features.remove('UltimateIncurredClaimCost')\ncategorical_features.remove('ClaimNumber')\ncategorical_features.remove('ClaimDescription')","4f58453b":"lbl = pre.LabelEncoder()\n\nfor c in data_train[categorical_features]:\n    print(f\"Label Encoding Categorical Feature - {c.upper()}\")\n    data_train[c] = lbl.fit_transform(data_train[c].astype(str))\n    data_test[c] = lbl.fit_transform(data_test[c].astype(str))\nprint('Label Encoding done...')\ndata_train[categorical_features].head(2)","bd60281c":"x_train,x_test,y_train,y_test=ms.train_test_split(data_train, target,test_size=0.4,random_state=1234456)","5444acb3":"glm=lm.LinearRegression()","23ab5ca9":"glm.fit(x_train, y_train)","ff297322":"glm.score(x_train, y_train)","f61ad40b":"glm.score(x_test, y_test)","35a921b2":"# decision tree for feature importance on a regression problem\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\n# define the model\nmodel = DecisionTreeRegressor()\n# fit the model\nmodel.fit(x_train, y_train)\n# get importance\nimportance = model.feature_importances_\n\n# summarize feature importance\nfeature_imp=pd.DataFrame({'features':data_train.columns,'importances':importance})\n# for i,v in enumerate(importance):\n# \tprint('Feature: %0d, Score: %.5f' % (i,v))\n# # plot feature importance\n# pyplot.bar([x for x in range(len(importance))], importance)\n# pyplot.show()\nfeature_imp.sort_values('importances',ascending=False).head().T\n","1cc3c6c8":"from sklearn.metrics import mean_squared_error","02c4a0b4":"from sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport time\nimport shap \n","9dba91a9":"import sklearn.metrics as mt\nmt.mean_squared_error(y_pred=glm.predict(x_test),y_true=y_test,squared=False)","2a6b1b47":"from lightgbm import LGBMRegressor","53ea8944":"lgbm = LGBMRegressor(\n               objective = 'regression', \n               num_leaves = 4,\n               learning_rate = 0.01, \n               n_estimators = 10000,\n               max_bin = 200, \n               bagging_fraction = 0.75,\n               bagging_freq = 5, \n               bagging_seed = 7,\n               feature_fraction = 0.2,\n               feature_fraction_seed = 7,\n               verbose = 1,\n            )\n\nlgbm_model = lgbm.fit(x_train, y_train)\nlg_vpreds = lgbm_model.predict(x_test)\nprint((f\"LGBM RMSE: {np.sqrt(mean_squared_error(y_test, lg_vpreds))}\"))","45fcfa61":"lg_vpred = lgbm_model.predict(data_test)","86fb0a15":"lg_vpred","0c5a6335":"# LGBMRegressor: Hyperparameter tuning with RandomizedSearchCV\nseed=123\ntic = time.time()\nparam_grid ={'learning_rate': [0.02,0.025], 'n_estimators': [500], 'num_leaves': [30,40,50],'feature_fraction': [0.7]} \nLGB_random_search = RandomizedSearchCV(LGBMRegressor(),param_grid, scoring='neg_mean_squared_error', cv=4,  n_iter=5, random_state=seed)\nLGB_random_search.fit(x_train,y_train)\nprint(\"Best parameters:\",LGB_random_search.best_params_)\nLGB = LGBMRegressor(**LGB_random_search.best_params_)    \nLGB.fit(x_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = mean_squared_error(y_test, LGB.predict(x_test))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(LGB.predict(X_val)))\nprint(\"MSE:\" + \"%6.2f\" % result)","87092c3c":"# XGBRegressor: Hyperparameter tuning with RandomizedSearchCV\ntic = time.time()\nparam_grid ={'learning_rate': [0.02,0.025], 'max_depth': [5,6,7],'n_estimators': [500],'colsample_bytree': [0.9], 'subsample': [0.7], 'tree_method': [\"hist\"] } \nXGB_random_search = RandomizedSearchCV(XGBRegressor(),param_grid, scoring='neg_mean_squared_error', cv=4,  n_iter=5, random_state=seed)\nXGB_random_search.fit(x_train,y_train)\nprint(\"Best parameters:\",XGB_random_search.best_params_)\nXGB = XGBRegressor(**XGB_random_search.best_params_)    \nXGB.fit(x_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = mean_squared_error(y_test, XGB.predict(x_test))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(XGB.predict(X_val)))\nprint(\"MSE:\" + \"%6.2f\" % result)","090cf299":"predictions = 0.5*(LGB.predict(data_test)+XGB.predict(data_test))","20454511":"\ncsv = pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv\")\ncsv[\"UltimateIncurredClaimCost\"] = predictions\ncsv.to_csv(\"Sample Submission.csv\", index = False)\n","e9e13122":"# data preprocessing","7106715f":"there is no such impact of year on claims","35b773e6":"## missing value","f00e31d4":"full time workers claim more","53cfc067":"most claims are from people with no responsibilites maybe because of freedom they meet more accidents","f6d32710":"Distribution is same for train and test","4b483f14":"we see a little bit hike in may, otherwise its mostly same","5b778aa9":"## feature engineering","1cd4ff47":"most claims are from age group 20-30","9f8060ee":"saturdays and sundays have least accidents, one reason could be that these are common holidays so no office","633750fe":"more claims are from single people rather than married.","019af786":"Let's check the claims that were reported before the accident happened!!","6b53bcf0":"#### Imputing the NaNs with 'U' is better","211cc3aa":"More claims are from males than females or others.","dbe5a48c":"# Data Loading"}}