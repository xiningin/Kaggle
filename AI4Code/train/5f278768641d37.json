{"cell_type":{"3b67238e":"code","914c318d":"code","dca81038":"code","4e6ce08d":"code","b3ac3af0":"code","04c50a54":"code","c21e6b8b":"code","3d9ed30c":"code","b37357c0":"code","38468dc8":"code","562d5706":"code","6a7aaef8":"code","4a463cc3":"code","53ea7943":"code","657ed8c1":"code","464696d6":"code","39cad355":"code","5e81475a":"code","560dbd59":"code","06465674":"markdown","8296bfad":"markdown","b65df5ca":"markdown","28b199da":"markdown","3815deab":"markdown","f0eb0965":"markdown","7b48abcf":"markdown","5d83d5af":"markdown","17f3571f":"markdown","9dd530d3":"markdown","e0806d46":"markdown","a3197ab0":"markdown","b3996166":"markdown","c5c48a7b":"markdown","2804453c":"markdown","87f7ce7d":"markdown"},"source":{"3b67238e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","914c318d":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\nimport optuna\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprimary_bgcolor = \"#f4f0ea\"\nprimary_palette = ['#ed4f37', '#40aff5']\nplt.rcParams['axes.facecolor'] = primary_bgcolor\nplt.rcParams['figure.dpi'] = 120","dca81038":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')","4e6ce08d":"train.head()","b3ac3af0":"print('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)","04c50a54":"null_count_df = train.isnull().sum().reset_index(name='count')\nnull_count_df['count'] = (null_count_df['count'] \/ train.shape[0]) * 100\nnull_count_df.sort_values(by='count', ascending=False, inplace=True)\nnull_count_df = null_count_df[null_count_df['count'] != 0]\n\nax = plt.figure(figsize=(5, 5))\nplt.title('Features with null elements')\nax = sns.barplot(x=null_count_df['index'], y=[100]*5, color='#e9f1f2')\nax = sns.barplot(data=null_count_df, x='index', y='count', palette='Set2')\n\nfor p in ax.patches:\n    val = '{:.2f}%'.format((p.get_height()))\n    ax.annotate(val, (p.get_x()+0.1, p.get_height()+1.8), va='center')","c21e6b8b":"plt.figure(figsize=(5, 5))\nsns.catplot(data=train, x='Survived', kind='count', palette=primary_palette)","3d9ed30c":"ft_cols = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived'], axis=1).columns\nnum_cols = ['Age', 'Fare']\ncat_cols = train[ft_cols].drop(num_cols, axis=1).columns","b37357c0":"def kde_plot_num_cols(cols, train):\n    L = len(cols)\n    nrow = int(np.ceil(L\/2))\n    ncol = 2\n    \n    plt.subplots(nrow, ncol, figsize=(24, 6))\n    plt.suptitle('Numerical features distribution')\n    i = 1\n    \n    for col in cols:\n        plt.subplot(nrow, ncol, i)\n        sns.kdeplot(data=train, x=col, shade=True, hue='Survived', palette=primary_palette)\n        i += 1\n        \n    plt.show()","38468dc8":"kde_plot_num_cols(num_cols, train)","562d5706":"def count_plot_cat_cols(cols, train):\n    L = len(cols)\n    nrow = int(np.ceil(L\/2))\n    ncol = 2\n    remove_last = (nrow*ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol, figsize=(15, 15))\n    plt.suptitle('Categorial features distribution')\n    ax.flat[-remove_last].set_visible(False)\n    i = 1\n    \n    for col in cols:\n        plt.subplot(nrow, ncol, i)\n        sns.countplot(data=train, x=col, hue='Survived', alpha=0.7, palette=primary_palette)\n        plt.legend()\n        i += 1\n    \n    plt.show()","6a7aaef8":"count_plot_cat_cols(cat_cols, train)","4a463cc3":"data_combined = pd.concat([train, test], axis=0)\n\ncat_cols = train.drop(['PassengerId', 'Survived'], axis=1).dtypes[train.dtypes != 'float64'].index.tolist()\n\nle = LabelEncoder()\n\nfor col in cat_cols:\n    le.fit(data_combined[col])\n    data_combined[col] = le.transform(data_combined[col])\n    \ntrain_df = data_combined[:len(train)]\ntest_df = data_combined[len(train):]","53ea7943":"print('Train size: ', train_df.shape)\nprint('Test size: ', test_df.shape)","657ed8c1":"X = train_df.drop('Survived', axis=1)\ny = train_df['Survived']","464696d6":"def objective(trial, data=X, target=y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n    \n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 300),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 20),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.01,0.02,0.05,0.005,0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'random_state': 42,\n        'boosting_type': 'gbdt',\n    }\n    \n    model = LGBMClassifier(**params)\n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=200, verbose=False)\n    pred = model.predict(X_test)\n    acc_score = accuracy_score(y_test, pred)\n    \n    return acc_score","39cad355":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nbest_params = study.best_trial.params\nprint('Best model by Optuna: ', best_params)","5e81475a":"cols = [col for col in train_df.columns if col not in ['PassengerId', 'Survived']]\n\nkfold = StratifiedKFold(5, shuffle=True, random_state=0)\nscores = []\npreds = []\n\nfor fold, (train_index, test_index) in enumerate(kfold.split(train_df[cols], train_df['Survived']), 1):\n    train_f, test_f = train_df.iloc[train_index], train_df.iloc[test_index]\n    \n    X_train = train_f[cols]\n    X_test = test_f[cols]\n    y_train = train_f['Survived']\n    y_test = test_f['Survived']\n    lgbm_clf = LGBMClassifier(**best_params)\n    \n    lgbm_clf.fit(X_train, y_train)\n    pred = lgbm_clf.predict(X_test)\n    acc_score = accuracy_score(y_test, pred)\n    pred_test = lgbm_clf.predict(test_df[cols])\n    preds.append(pred_test)\n    scores.append(acc_score)\n    \n    print(f'Fold {fold}, Accuracy score: {acc_score:.5f}')\n    \nprint(f'Average accuracy score: {np.mean(scores)}')\n\npred = np.array(preds).mean(axis=0).round()","560dbd59":"sample_submission['Survived'] = np.rint(pred)\nsample_submission['Survived'] = sample_submission['Survived'].apply(int)\nsample_submission.to_csv('lgbm_submission.csv', index=False)","06465674":"## Make submission","8296bfad":"Let's make predictions using the best parameter I've got from Optuna.\nI used LightGBM Classifier to make the prediction.","b65df5ca":"#### Categorial features","28b199da":"#### Numerical features","3815deab":"## Prediction (LGBM)","f0eb0965":"### Features with null elements\n\nWhich feature has null elements? And how much?","7b48abcf":"At last, I got my final prediction! It shows average accuracy score of 0.78431. ","5d83d5af":"# TPS April 2021 - Visualization + Optuna + LGBM ","17f3571f":"### Number of people survived\n\nHow many people survived and didn't survive? Let's find out.","9dd530d3":"After a long iteration, I finally got the best parameters! Now let's move on to the prediction part.","e0806d46":"### Numerical \/ Categorial features distribution - who survived?\n\nTo find out what kind of passengers did or didn't survive, let's take a look at the distribution.\n","a3197ab0":"## Preprocessing\n\nFor categorial features, I will do some label encoding.","b3996166":"# 2. Modeling","c5c48a7b":"\"Cabin\" has the most null elements, having a null proportion of 67.87%.","2804453c":"# 1. EDA & Visualization","87f7ce7d":"## Hyperparameter tuning (Optuna)\n\nLet's find the optimal hyperparameter using Optuna in order to get the best score. "}}