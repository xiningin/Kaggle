{"cell_type":{"65131a93":"code","79736021":"code","95c8d4d7":"code","5ca291a2":"code","b04037e3":"code","ef26a7a7":"code","561be8ab":"code","5b738c90":"code","d15d4f4b":"code","a9833c6b":"code","9bf915a5":"code","90174473":"code","44828198":"code","1c6dcc68":"code","e23cf9d0":"code","9b1fa360":"markdown","8b8b8e05":"markdown","4433dff4":"markdown","c4d07aa7":"markdown","b8b8fde9":"markdown","0179af9f":"markdown","148ee379":"markdown","536d9a24":"markdown","8a8fb5da":"markdown","fee551d0":"markdown","0eb789b9":"markdown","dd9c1de5":"markdown","6dac2c51":"markdown","7cce0c40":"markdown","d717a916":"markdown","e7fb00d1":"markdown","548bbefb":"markdown","690a6f04":"markdown","429895d5":"markdown","0ae09820":"markdown"},"source":{"65131a93":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom PIL import Image, ImageDraw\nimport tensorflow as tf\n\nimport os\nimport ast  ## Change str -> list.\nimport sys\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport greatbarrierreef","79736021":"df_train = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')\ndf_train['img_path'] = os.path.join('..\/input\/tensorflow-great-barrier-reef\/train_images')+\"\/video_\"+df_train.video_id.astype(str)+\"\/\"+df_train.video_frame.astype(str)+\".jpg\"\ndf_train.head()","95c8d4d7":"plt.figure(figsize=(8,5))\nsns.countplot(df_train['video_id'], color='#2196F3')","5ca291a2":"with_annotation = len(df_train[df_train['annotations'] != '[]'])\nwithout_annotation = len(df_train[df_train['annotations'] == '[]'])\n\nlabels = ['Without Bounding Box', 'With Bounding Box']\n\nfig = go.Figure([go.Bar(x=labels, \n                        y=[without_annotation, with_annotation], width=0.6)])\n\nfig.update_layout(autosize=False, width=700, height=400, margin=dict(l=60, r=60, b=50, t=50, pad=4))\nfig.show()","b04037e3":"# creating new column which contains the total number of bounding boxes\ndf_train['No_bbox'] = df_train['annotations'].apply(lambda x:x.count('{')) \n\n# Example\n\nn = df_train['No_bbox'][12843]\nprint(df_train['annotations'][12843])\nprint(f'Number of bounding boxes are : {n}.')","ef26a7a7":"fig = px.bar(df_train['No_bbox'].value_counts().drop(0), title='Count of Bounding Boxes')\nfig.update_layout(autosize=False, width=700, height=400, margin=dict(l=60, r=60, b=50, t=50, pad=4))\nfig.show()","561be8ab":"df_train['annotations'] = df_train['annotations'].apply(ast.literal_eval)\ndf_train.head()","5b738c90":"df2 = df_train[df_train['annotations'].astype(str) != \"[]\"]\ndf2 = df2[df2['No_bbox'] == 5]\ndf2.head()","d15d4f4b":"def img_viz(df, id):\n    image = df_train['img_path'][id]\n    img = Image.open(image)\n    \n    for box in df_train['annotations'][id]:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n    display(img)","a9833c6b":"img_viz(df_train, id=5474)","9bf915a5":"INPUT_DIR = '..\/input\/tensorflow-great-barrier-reef\/'\nsys.path.insert(0, INPUT_DIR)","90174473":"MODEL_DIR = '..\/input\/cots-detection-w-tensorflow-object-detection-api\/cots_efficientdet_d0'\nstart_time = time.time()\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(MODEL_DIR, 'output'), 'saved_model'))\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Elapsed time: ' + str(elapsed_time) + 's')","44828198":"def load_image_into_numpy_array(path):\n    \"\"\"Load an image from file into a numpy array.\n\n    Puts image into numpy array to feed into tensorflow graph.\n    Note that by convention we put it into a numpy array with shape\n    (height, width, channels), where channels=3 for RGB.\n\n    Args:\n    path: a file path (this can be local or on colossus)\n\n    Returns:\n    uint8 numpy array with shape (img_height, img_width, 3)\n    \"\"\"\n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(io.BytesIO(img_data))\n    (im_width, im_height) = image.size\n    \n    return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ndef detect(image_np):\n    \"\"\"Detect COTS from a given numpy image.\"\"\"\n\n    input_tensor = np.expand_dims(image_np, 0)\n    start_time = time.time()\n    detections = detect_fn_tf_odt(input_tensor)\n    return detections","1c6dcc68":"env = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","e23cf9d0":"DETECTION_THRESHOLD = 0.13\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n    height, width, _ = image_np.shape\n    \n    # Run object detection using the TensorFlow model.\n    detections = detect(image_np)\n    \n    # Parse the detection result and generate a prediction string.\n    num_detections = detections['num_detections'][0].numpy().astype(np.int32)\n    predictions = []\n    for index in range(num_detections):\n        score = detections['detection_scores'][0][index].numpy()\n        if score < DETECTION_THRESHOLD:\n            continue\n\n        bbox = detections['detection_boxes'][0][index].numpy()\n        y_min = int(bbox[0] * height)\n        x_min = int(bbox[1] * width)\n        y_max = int(bbox[2] * height)\n        x_max = int(bbox[3] * width)\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    # Generate the submission data.\n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","9b1fa360":"#### Clearly, very few number of images have annotations as compared to without annotations images.","8b8b8e05":"## Required Libraries","4433dff4":"## Prediction | Submission","c4d07aa7":"## Exploratory Data Analysis","b8b8fde9":"#### I tried to make the code of this Notebook as simple as possible. If you think you learned something from it. Do Upvote. \u2b06\ufe0f \ud83d\ude0a","0179af9f":"## Model\n#### Load the TensorFlow COTS detection model into memory and define some util functions for running inference.\n#### Read more about it [here](https:\/\/www.kaggle.com\/khanhlvg\/cots-detection-w-tensorflow-object-detection-api).","148ee379":"#### Creating Environment","536d9a24":"## Loading Data","8a8fb5da":"#### Let's count how many images are there from each of the three videos.","fee551d0":"#### Awesome!!!","0eb789b9":"## References\n* [COTS detection w\/ TensorFlow Object Detection API](https:\/\/www.kaggle.com\/khanhlvg\/cots-detection-w-tensorflow-object-detection-api)\n* [EDA: Let's understand the data - protect the reef](https:\/\/www.kaggle.com\/casfranco\/eda-let-s-understand-the-data-protect-the-reef)\n* [Inference using EfficientDet-D0 model](https:\/\/www.kaggle.com\/khanhlvg\/inference-using-efficientdet-d0-model-tensorflow\/notebook)","dd9c1de5":"#### Now, let's have a look at how many images with bounding boxes are available.","6dac2c51":"#### Now, we'll going to find out how many bouning boxes are there in each annotation columns.\n#### In order to do that we can simply count the number of opening curly brackets - '{' in annotation column.\n> **Example** :-  `df_train['annotations'][12843]` *where 12843 is a random number.*\n\n> Gives -> \n\n> \"[{'x': 338, 'y': 229, 'width': 45, 'height': 27},\n>\n> {'x': 357, 'y': 285, 'width': 37, 'height': 38}, \n> \n> {'x': 173, 'y': 588, 'width': 65, 'height': 56}, \n> \n> {'x': 234, 'y': 598, 'width': 31, 'height': 26}]\" \n\n> **We can see that it has 4 bounding box coordinates, the simple way to get this is to count number of opening curly brackets.**","7cce0c40":"#### Creating new DataFrame which carry images containing more than 1 Bounding Boxes (You could take any value you like) and then use that row's 'video_id' to see image with Bounding Boxes.","d717a916":"#### Most of the images has only single Bounding Box. Very few have more than 5 BBox.","e7fb00d1":"## Data Visualization","548bbefb":"#### Helper functions","690a6f04":"![](https:\/\/wallpaperaccess.com\/full\/218450.jpg)","429895d5":"## Files\n#### **train** - Folder containing training set photos of the form video_{video_id}\/{video_frame_number}.jpg.\n\n#### **[train\/test].csv** - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\n> #### **video_id** - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n> #### **video_frame** - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n> #### **sequence** - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n> #### **sequence_frame** - The frame number within a given sequence.\n> #### **image_id** - ID code for the image, in the format '{video_id}-{video_frame}'\n> #### **annotations** - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate (x_min, y_min) of its upper left corner within the image together with its width and height in pixels.\n#### **example_sample_submission.csv** - A sample submission file in the correct format. The actual sample submission will be provided by the API; this is only provided to illustrate how to properly format predictions. The submission format is further described on the Evaluation page.\n\n#### **example_test.npy** - Sample data that will be served by the example API.\n\n#### **greatbarrierreef** - The image delivery API that will serve the test set pixel arrays. You may need Python 3.7 and a Linux environment to run the example offline without errors.","0ae09820":"#### Now, lets change 'annotations' from string to list data type using ***ast***."}}