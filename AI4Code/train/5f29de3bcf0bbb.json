{"cell_type":{"02051302":"code","566b2b63":"code","a5b9bedf":"code","c2546c97":"code","be46472d":"code","7ea2568d":"code","934380d7":"code","c1f01e97":"code","64cf1f8f":"code","e79a4c1a":"code","952cbf53":"code","f01857c5":"code","d59b5fb9":"code","db45439e":"code","f75e0818":"markdown","2a29becb":"markdown","7da4def2":"markdown","ac5ad8a7":"markdown","e881701b":"markdown","9b355b16":"markdown"},"source":{"02051302":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_palette('husl')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('..\/input\/Iris.csv')","566b2b63":"data.head()","a5b9bedf":"data.info()","c2546c97":"data.describe()","be46472d":"data['Species'].value_counts()","7ea2568d":"tmp = data.drop('Id', axis=1)\ng = sns.pairplot(tmp, hue='Species', markers='+')\nplt.show()","934380d7":"g = sns.violinplot(y='Species', x='SepalLengthCm', data=data, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='SepalWidthCm', data=data, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalLengthCm', data=data, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalWidthCm', data=data, inner='quartile')\nplt.show()","c1f01e97":"X = data.drop(['Id', 'Species'], axis=1)\ny = data['Species']\n# print(X.head())\nprint(X.shape)\n# print(y.head())\nprint(y.shape)","64cf1f8f":"# experimenting with different n values\nk_range = list(range(1,26))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X, y)\n    y_pred = knn.predict(X)\n    scores.append(metrics.accuracy_score(y, y_pred))\n    \nplt.plot(k_range, scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\nplt.show()","e79a4c1a":"logreg = LogisticRegression()\nlogreg.fit(X, y)\ny_pred = logreg.predict(X)\nprint(metrics.accuracy_score(y, y_pred))","952cbf53":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=5)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","f01857c5":"# experimenting with different n values\nk_range = list(range(1,26))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores.append(metrics.accuracy_score(y_test, y_pred))\n    \nplt.plot(k_range, scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\nplt.show()","d59b5fb9":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","db45439e":"knn = KNeighborsClassifier(n_neighbors=12)\nknn.fit(X, y)\n\n# make a prediction for an example of an out-of-sample observation\nknn.predict([[6, 3, 4, 2]])","f75e0818":"# Modeling with scikit-learn","2a29becb":"## Choosing KNN to Model Iris Species Prediction with k = 12\nAfter seeing that a value of k = 12 is a pretty good number of neighbors for this model, I used it to fit the model for the entire dataset instead of just the training set.","7da4def2":"# Preview of Data\n- There are 150 observations with 4 features each (sepal length, sepal width, petal length, petal width).\n- There are no null values, so we don't have to worry about that.\n- There are 50 observations of each species (setosa, versicolor, virginica).","ac5ad8a7":"# Data Visualization\n- After graphing the features in a pair plot, it is clear that the relationship between pairs of features of a iris-setosa (in pink) is distinctly different from those of the other two species.\n- There is some overlap in the pairwise relationships of the other two species, iris-versicolor (brown) and iris-virginica (green).\n","e881701b":"## Split the dataset into a training set and a testing set\n\n### Advantages\n- By splitting the dataset pseudo-randomly into a two separate sets, we can train using one set and test using another.\n- This ensures that we won't use the same observations in both sets.\n- More flexible and faster than creating a model using all of the dataset for training.\n\n### Disadvantages\n- The accuracy scores for the testing set can vary depending on what observations are in the set. \n- This disadvantage can be countered using k-fold cross-validation.\n\n### Notes\n- The accuracy score of the models depends on the observations in the testing set, which is determined by the seed of the pseudo-random number generator (random_state parameter).\n- As a model's complexity increases, the training accuracy (accuracy you get when you train and test the model on the same data) increases.\n- If a model is too complex or not complex enough, the testing accuracy is lower.\n- For KNN models, the value of k determines the level of complexity. A lower value of k means that the model is more complex.","9b355b16":"## Train and test on the same dataset\n- This method is not suggested since the end goal is to predict iris species using a dataset the model has not seen before.\n- There is also a risk of overfitting the training data."}}