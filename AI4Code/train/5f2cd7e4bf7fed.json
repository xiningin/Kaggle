{"cell_type":{"64fab071":"code","82f16385":"code","5267bbb1":"code","2684a8b5":"code","9538a989":"code","d3228606":"code","00310ef3":"code","4ea57412":"code","b3debd58":"code","89a60a9e":"code","bc7b7641":"code","3c597851":"code","06547b34":"code","f214c9d2":"code","d47e684c":"code","f70dfb31":"code","d44d5b0b":"code","1f90aed3":"code","b4dc3a19":"code","b7bc50c9":"code","cd38174d":"code","ea8c7e17":"code","40243b0c":"code","9a2fe077":"code","9f85422f":"code","33a5ac1d":"code","2c90725f":"code","e92c40b5":"code","48823fc9":"code","dcf0024e":"code","9e826c93":"code","c98d6464":"code","0d560342":"code","bfc98c20":"code","cbf3bcfc":"code","db36fb0a":"code","70ccc31f":"code","3be92ca1":"code","cce4fd37":"code","ea9aa925":"code","7f75c6ec":"code","18e5c4c8":"code","2f7b1937":"code","f2e42f36":"code","435d109c":"code","fc17c595":"code","bf86d97c":"code","ed2e907d":"markdown","463409a8":"markdown","5faae5ca":"markdown","9084237d":"markdown","66f4ea34":"markdown","ac807565":"markdown"},"source":{"64fab071":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82f16385":"import pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport warnings\nfrom sklearn import preprocessing\nwarnings.filterwarnings(\"ignore\")","5267bbb1":"df = pd.read_csv('..\/input\/ionosphere01\/ionosphere_data.csv', error_bad_lines=False, engine ='python')\n","2684a8b5":"df.head()","9538a989":"df.shape","d3228606":"df.skew()","00310ef3":"df.isnull().sum()","4ea57412":"df.corr()","b3debd58":"#Global declartions of function names\nglobal Head\nglobal Size\nglobal Column_names\nglobal Describe\nglobal Shape\nglobal Count\nglobal Value_count\nglobal ISNULL\nglobal Tail\nglobal Ndim\nglobal Nunique\nglobal Memory_usage\nglobal Duplicated\nglobal ISNA\nglobal DTYPES\nglobal CORR\nglobal Info\nglobal operations\n        \n\n        ","89a60a9e":" def Head(value=5):\n            print('\\033[1m'+'displaying the', value, 'rows'+'\\033[0m')\n            a=df.head(value)\n            return a\n            print(\"--------------------------------------------------------------------------\")\nHead()","bc7b7641":"def Tail():\n    print('\\033[1m'+\"The last five rows of the dataframe are\"+'\\033[0m')\n    co3=df.tail()\n    return(co3)\n    print(\"--------------------------------------------------------------------------\")\nTail()","3c597851":"def Column_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    c=df.columns\n    print(c,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nColumn_names()","06547b34":"def Describe():\n    print('\\033[1m'+\"The Description of our dataset is:\"+'\\033[0m')\n    des=df.describe()\n    return(des)\n    print(\"--------------------------------------------------------------------------\")\nDescribe()","f214c9d2":"def Size():\n    print('\\033[1m'+\"The size of dataset is :\"+'\\033[0m')\n    siz=df.size\n    print(siz,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nSize()","d47e684c":"def Count():\n    print('\\033[1m'+\"The count of non null values are:\"+'\\033[0m')\n    co=df.count()\n    print(co,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nCount()","f70dfb31":"def ISNULL():\n    print('\\033[1m'+\"Detection of missing values\"+'\\033[0m')\n    co2=df.isnull().sum()\n    print(co2,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nISNULL()","d44d5b0b":"def Ndim():\n    print('\\033[1m'+\"The dimensions of data set are:\"+'\\033[0m')\n    co4=df.ndim\n    print(co4,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nNdim()","1f90aed3":"def Nunique():\n    print('\\033[1m'+\"Total number of unique values are:\"+'\\033[0m')\n    co5=df.nunique()\n    print(co5,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nNunique()","b4dc3a19":"def Memory_usage():\n    print('\\033[1m'+\"The total memory used is :\"+'\\033[0m')\n    co6=df.memory_usage()\n    print(co6,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nMemory_usage()","b7bc50c9":"def Duplicated():\n    print('\\033[1m'+\"Total number of duplicate rows\"+'\\033[0m')\n    co7=df.duplicated().count()\n    return(co7)\n    print(\"--------------------------------------------------------------------------\")\nDuplicated()","cd38174d":"def DTYPES():\n    print('\\033[1m'+\"The datatypes are :\"+'\\033[0m')\n    co9=df.dtypes\n    print(co9,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nDTYPES()","ea8c7e17":"def Info():\n    print('\\033[1m'+\"The info of data set is :\"+'\\033[0m')\n    co11=df.info()\n    print(\"--------------------------------------------------------------------------\")\nInfo()","40243b0c":"def operations(df,x):\n    if df[x].dtype==\"float64\":\n        print('\\033[1m'+'', x, 'rows'+'\\033[0m')\n        print('\\033[1m'+\"It is a quantitaive data \\n\"+'\\033[0m')\n        print(\"The mean is :\\n\",df[x].mean())\n        print(\"The median is :\\n\",df[x].median())\n        print(\"The Standard Deviation is \\n\",df[x].std())\n        q1=df[x].quantile(0.25)\n        q2=df[x].quantile(0.5)\n        q3=df[x].quantile(0.75)\n        IQR=q3-q1\n        LLP=q1-1.5*IQR\n        ULP=q3+1.5*IQR\n        print(\"The quartiles are q1 : \\n\",q1)\n        print(\"The quartiles are q2 : \\n\",q2)\n        print(\"The quartiles are q3 :\\n \",q3)\n        print(\"The Uppler limit point of the data is \\n\",ULP)\n        print(\"The lower limit point of the data is \\n \",LLP)\n        if df[x].min()>LLP and df[x].max()<ULP:\n            print(\"The outliers are not present \\n\")\n            print(\"--------------------------------------------------------------------------\")\n\n        else:\n\n            print(\"The outliers are present \\n\")\n            print(\"The outliers are :\")\n            print(df[df[x].values>ULP][x])\n            print(df[df[x].values<LLP][x])\n\n            print(\"--------------------------------------------------------------------------\")\n\n\n    elif df[x].dtype==\"int64\":\n        print('\\033[1m'+'', x, 'rows'+'\\033[0m')\n        print('\\033[1m'+\"It is a quantitaive data \\n\"+'\\033[0m')\n        print(\"The mean is : \\n\",df[x].mean())\n        print(\"The median is : \\n\",df[x].median())\n        print(\"The Standard Deviation is \\n\",df[x].std())\n        q1=df[x].quantile(0.25)\n        q2=df[x].quantile(0.5)\n        q3=df[x].quantile(0.75)\n        IQR=q3-q1\n        LLP=q1-1.5*IQR\n        ULP=q3+1.5*IQR\n        print(\"The quartiles are q1 : \\n\",q1)\n        print(\"The quartiles are q2 : \\n\",q2)\n        print(\"The quartiles are q3 : \\n\",q3)\n        print(\"The Uppler limit point of the data is \\n\",ULP)\n        print(\"The lower limit point of the data is \\n\",LLP)\n        if df[x].min()>LLP and df[x].max()<ULP:\n            print(\"The outliers are not present \\n\")\n\n            print(\"--------------------------------------------------------------------------\")\n\n        else:\n\n            print(\"The outliers are present \\n\")\n            print(\"The outliers are :\")\n            print(df[df[x].values>ULP][x])\n            print(df[df[x].values<LLP][x])\n            print(\"--------------------------------------------------------------------------\")\n\n\n\n\n\n\n\n    else:\n\n        print('\\033[1m'+\"The data is Qualitative \\n\"+'\\033[0m')\n\n\n        if df[x].nunique()==1:\n            print('\\033[1m'+\"The data is singular \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n        elif df[x].nunique()==2:\n            print('\\033[1m'+\"The data is Binary \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n        elif df[x].nunique()>2:\n            print('\\033[1m'+\"The data is Multi \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n\n        print(\"--------------------------------------------------------------------------\")\n\nc=df.columns\nfor i in c:\n    operations(df,i)\n    print(\"\\n\")\n\n\n","9a2fe077":"def Summary():\n        print('\\033[1m'+\"The Summary of data is  \\n\"+'\\033[0m')\n        print(\"The shape of the datset is :\",df.shape)\n        print(\"The sixe o the data set is :\",df.size)\n        print(\"The dimensions of the dataset are:\",df.ndim)\n        print(\"The memory usage of the data set are\",df.memory_usage())\n        print(\"The data types of the dataset are:\",df.dtypes)\n        print(\"--------------------------------------------------------------------------\")\n\nSummary()     ","9f85422f":" def Column_Summary():\n        print('\\033[1m'+\"The Column wise Summary of data is  \\n\"+'\\033[0m')\n        k=df.columns\n        for i in k:\n            print('\\033[1m'+'', i, 'rows'+'\\033[0m')\n            print(\"The Shape of the column \",i,\"is \",df[i].shape)\n            print(\"The Size of the column \",i,\"is \",df[i].size)\n            print(\"The Dimensions of the column \",i,\"is \",df[i].ndim)\n            print(\"The Memory used by the column \",i,\"is \",df[i].memory_usage())\n            print(\"The Data types  of the column \",i,\"is \",df[i].dtypes)\n            print(\"--------------------------------------------------------------------------\")\nColumn_Summary()","33a5ac1d":"df.columns","2c90725f":"#taking only numerical columns in list x for plotting distribution plot\nm=df.drop(['label'],axis=1)","e92c40b5":"for i in m.columns:\n    sns.distplot(df[i],kde=True)\n    plt.show()","48823fc9":"\nfor i in m.columns:\n    sns.boxplot(data=m,x=i)\n    plt.show()","dcf0024e":"plt.figure(figsize=(10,16))\nax = sns.heatmap(df.corr(),annot = True, cmap = 'viridis')\nplt.show()","9e826c93":"import plotly.express as px\nimport plotly.graph_objects as go\n\nfor i in m.columns:\n    x = df['label']\n    y = df[i]\n    sns.barplot(x,y)\n    plt.show()","c98d6464":"def count_outliers(data,col):\n    q1 = data[col].quantile(0.25,interpolation='nearest')\n    q2 = data[col].quantile(0.5,interpolation='nearest')\n    q3 = data[col].quantile(0.75,interpolation='nearest')\n    q4 = data[col].quantile(1,interpolation='nearest')\n    IQR = q3 -q1\n    global LLP\n    global ULP\n    LLP = q1 - 1.5*IQR\n    ULP = q3 + 1.5*IQR\n    if data[col].min() > LLP and data[col].max() < ULP:\n        print(\"No outliers in\",i)\n    else:\n        print(\"There are outliers in\",i)\n        x = data[data[col]<LLP][col].size\n        y = data[data[col]>ULP][col].size\n        a.append(i)\n        print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in m.columns:\n    count_outliers(df,i)","0d560342":"def LABEL_ENCODING(c1):\n    from sklearn import preprocessing\n    # label_encoder object knows how to understand word labels.\n    label_encoder = preprocessing.LabelEncoder()\n \n    # Encode labels in column 'species'.\n    df[c1]= label_encoder.fit_transform(df[c1])\n \n    df[c1].unique()\n    return df","bfc98c20":"df.columns","cbf3bcfc":"LABEL_ENCODING('label')","db36fb0a":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df.drop(['label'],axis = 1))","70ccc31f":"feature=df\nfeature=feature.drop('label',axis=1)","3be92ca1":"label=df['label']","cce4fd37":"X_train,X_test,y_train,y_test=train_test_split(feature,label,test_size=.3)","ea9aa925":"print(X_train.shape,y_train.shape)","7f75c6ec":"print(X_test.shape,y_test.shape)","18e5c4c8":"knn = KNeighborsClassifier()","2f7b1937":"knn.fit(X_train,y_train)","f2e42f36":"pred = knn.predict(X_test)\npred","435d109c":"from sklearn.metrics import classification_report, confusion_matrix","fc17c595":"print(classification_report(y_test,pred))","bf86d97c":"cmat = confusion_matrix(y_test,pred)\nprint('TN - True Negative {}'.format(cmat[0,0]))\nprint('FP - False Positive {}'.format(cmat[0,1]))\nprint('FN - False Negative {}'.format(cmat[1,0]))\nprint('TP - True Positive {}'.format(cmat[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))","ed2e907d":"# Data Cleaning and Preproccessing","463409a8":"# Exploratory Data Analysis","5faae5ca":"* Observe the following bar plots and see which features has more of type b or type g","9084237d":"# Data Modelling and Feature Extraction","66f4ea34":"# DATA VISUALIZATION","ac807565":"# Relation Plots"}}