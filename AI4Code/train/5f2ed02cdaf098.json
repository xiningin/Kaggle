{"cell_type":{"342d0191":"code","f37e6711":"code","844c9105":"code","ee5186a3":"code","02d02a3e":"code","221f521f":"code","45038da2":"code","86cca2ce":"code","e67d15d5":"code","3fad87a2":"code","cb3ecd6f":"code","55e5a9af":"code","20ea81b3":"code","afd3cac8":"code","c9794fae":"code","f27c2f1b":"code","9244f9c8":"code","ddbb7a38":"code","11127d8e":"code","8c22732f":"code","50a3a9eb":"code","5e4f65f8":"code","3720c4f8":"code","c9d571b4":"code","213fc384":"code","62a47270":"code","0b696b64":"code","6fbf38b0":"code","f6989116":"code","6c29194c":"code","251f9ce1":"code","131cdc02":"code","c9c6d228":"code","f41b1acb":"code","b67f14bd":"code","b5c519c0":"code","7d937de9":"code","99be1e16":"code","b1c9d7b2":"code","4714d7a3":"code","54e7cc30":"code","b901dd61":"code","77ac25b0":"code","12d7c44a":"code","6bc636a0":"markdown","840631f8":"markdown","db995eb9":"markdown","2f49071c":"markdown","5db73cf3":"markdown","c3edfa57":"markdown","315da5c6":"markdown","2ddc1195":"markdown","f6a7163d":"markdown","e43363a7":"markdown","11f18dae":"markdown","869e5abb":"markdown","e53c449b":"markdown","a7ad26a1":"markdown","48edf1db":"markdown","5b1fc551":"markdown","67a2e35e":"markdown"},"source":{"342d0191":"%%capture\n!pip install ..\/input\/pytorchimagemodels","f37e6711":"import sys\nsys.path.append('..\/input\/facebookdeit')","844c9105":"import os\nimport gc\nimport sys\n\nimport math\nimport random\n\nfrom tqdm.notebook import tqdm\n\nimport re\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset ,RandomSampler\n\nimport timm\n\nimport albumentations\n\nfrom swav_resnet import resnet50w2\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom models import deit_base_distilled_patch16_224\n\n\n\nimport cudf\nimport cuml\nimport cupy","ee5186a3":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(0)","02d02a3e":"text_models=[\n    {\n        'transformer_model': '..\/input\/shopee-paraphrase-xlm-r-multilingual\/paraphrase-xlm-r-multilingual-v1_pp_7_14',\n        'model_path' : '..\/input\/shopee-paraphrase-xlm-r-multilingual\/paraphrase-xlm-r-multilingual-v1_pp_7_14\/ckpt.pt',\n        'MAX_LEN' : 128,\n        'params' : {\n            'embed_dim' :1024,\n            'out_dim' : 11014\n        }\n        \n    },\n    {\n            'transformer_model': '..\/input\/shopee-bert-base-indonesian\/bert-base-indonesian_pp_7_8',\n            'model_path' : '..\/input\/shopee-bert-base-indonesian\/bert-base-indonesian_pp_7_8\/ckpt.pt',\n            'MAX_LEN' : 128,\n            'params' : {\n                'embed_dim' :1024,\n                'out_dim' : 11014\n            }\n\n        }\n]\n\nTRANSFORMER_EMBED_DIM = 768","221f521f":"joint_models=[\n    {\n        'transformer_model': '..\/input\/shopee-paraphrase-xlm-r-multilingual\/paraphrase-xlm-r-multilingual-v1_pp_7_14',\n        'model_path' : '..\/input\/deit-xlm-joint-weights\/finetuned_deit_xlm_joint_7_11.pt',\n        'MAX_LEN' : 128,\n        'IMAGE_SIZE' : 224,\n        'params' : {\n                'embed_dim' : 2048,\n                'out_dim' : 11014\n            }\n        \n    }\n]","45038da2":"image_models=[\n    {\n        'vision_model': '..\/input\/shopee-swav-finetuned-resnet50w2\/swav_resnet50w2_224_2048_5_epochs_loss_12_beef.pt',\n        'IMAGE_SIZE' : 224,\n        'params' : {\n            'embed_dim' :2048,\n            'out_dim' : 11014\n        }\n    },\n    {\n        'vision_model': '..\/input\/effnet-weights\/finetuned_tf_efficientnet_b3_ns_300_2048_7_9.pt',\n        'IMAGE_SIZE' : 300,\n        'params' : {\n            'backbone_name' : 'tf_efficientnet_b3_ns',\n            'embed_dim' :2048,\n            'out_dim' : 11014\n        }\n    },\n    \n    {\n        'vision_model': '..\/input\/deit-base-distilled-patch16-224\/finetuned_deit_base_distilled_patch16_224_4_18.pt',\n        'IMAGE_SIZE' : 224,\n        'params' : {\n            'embed_dim' :2048,\n            'out_dim' : 11014\n        }\n    },\n]","86cca2ce":"NUM_WORKERS = 4\nBATCH_SIZE = 128","e67d15d5":"CHECK_SUB = False\nGET_CV = True","3fad87a2":"test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nif len(test)>3: GET_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n","cb3ecd6f":"def read_dataset():\n    if GET_CV:\n        \n        df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        \n        df['filepath'] = df['image'].apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/train_images', x))\n        \n        if CHECK_SUB: \n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n                    \n    else:\n        df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n        \n        df['filepath'] = df['image'].apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/test_images', x))\n                \n    return df ","55e5a9af":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1","20ea81b3":"class DenseCrossEntropy(nn.Module):\n    def forward(self, x, target):\n        x = x.float()\n        target = target.float()\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n\n        loss = -logprobs * target\n        loss = loss.sum(-1)\n        return loss.mean()\n\n\nclass ArcMarginProduct_subcenter(nn.Module):\n    def __init__(self, in_features, out_features, k=3):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n        self.reset_parameters()\n        self.k = k\n        self.out_features = out_features\n        \n    def reset_parameters(self):\n        stdv = 1. \/ math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        \n    def forward(self, features):\n        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n        cosine, _ = torch.max(cosine_all, dim=2)\n        return cosine   \n\n\nclass ArcFaceLossAdaptiveMargin(nn.modules.Module):\n    def __init__(self, margins, s=30.0):\n        super().__init__()\n        self.crit = DenseCrossEntropy()\n        self.s = s\n        self.margins = margins\n            \n    def forward(self, logits, labels, out_dim):\n        ms = []\n        ms = self.margins[labels.cpu().numpy()]\n        cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n        sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n        th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n        mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n        labels = F.one_hot(labels, out_dim).float()\n        logits = logits.float()\n        cosine = logits\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * cos_m.view(-1,1) - sine * sin_m.view(-1,1)\n        phi = torch.where(cosine > th.view(-1,1), phi, cosine - mm.view(-1,1))\n        output = (labels * phi) + ((1.0 - labels) * cosine)\n        output *= self.s\n        loss = self.crit(output, labels)\n        return loss     \n\n","afd3cac8":"class Projection(nn.Module):\n    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:\n        super().__init__()\n        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n        self.layer_norm = nn.LayerNorm(d_out)\n        self.drop = nn.Dropout(p)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        embed1 = self.linear1(x)\n        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n        embeds = self.layer_norm(embed1 + embed2)\n        return embeds","c9794fae":"class VisionEncoderResnet(nn.Module):\n    def __init__(self, embed_dim , out_dim):\n        super().__init__()\n        base = resnet50w2() #torch.hub.load('facebookresearch\/swav', 'resnet50w2')\n\n        d_in = 4096\n        self.base = base\n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n\n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        cosine_feat= self.metric_classify(projected_vec)\n\n        return projected_vec ,cosine_feat","f27c2f1b":"class VisionEncoderEffnet(nn.Module):\n    def __init__(self,backbone_name, embed_dim , out_dim):\n        super().__init__()\n        base = timm.create_model(backbone_name, pretrained=False)\n        d_in = base.classifier.in_features\n        base.classifier = nn.Identity()\n        \n        self.base = base\n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n\n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        cosine_feat= self.metric_classify(projected_vec)\n\n        return projected_vec ,cosine_feat","9244f9c8":"class DeitWrapperVision(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.backbone = deit_base_distilled_patch16_224(pretrained=False)\n        self.backbone.head= nn.Identity()\n        self.backbone.head_dist = nn.Identity()\n\n    def forward_features(self, x):\n        # taken from https:\/\/github.com\/facebookresearch\/deit\/blob\/main\/models.py\n\n        B = x.shape[0]\n        x = self.backbone.patch_embed(x)\n\n        cls_tokens = self.backbone.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        dist_token = self.backbone.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n\n        x = x + self.backbone.pos_embed\n        x = self.backbone.pos_drop(x)\n\n        for blk in self.backbone.blocks:\n            x = blk(x)\n\n        x = self.backbone.norm(x)\n        return x\n    \n    def forward(self, x):\n        hidden_states = self.forward_features(x)\n        _cls, _cls_dist = hidden_states[:, 0], hidden_states[:, 1]\n        return (_cls + _cls_dist) \/ 2   \n    \n    \nclass VisionEncoderDeit(nn.Module):\n    def __init__(self, embed_dim , out_dim):\n        super().__init__()\n        base = DeitWrapperVision()\n\n        d_in = 768\n        self.base = base\n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n\n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        cosine_feat= self.metric_classify(projected_vec)\n\n        return projected_vec ,cosine_feat","ddbb7a38":"class ShopeeDatasetVision(Dataset):\n    def __init__(self, csv, mode, transform):\n\n        self.csv = csv.reset_index()\n        self.mode= mode\n        self.transform = transform\n\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n\n        image = cv2.imread(row.filepath)[:,:,::-1]\n\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image'].astype(np.float32)\n        else:\n            image = image.astype(np.float32)\n\n        image = image.transpose(2, 0, 1)\n\n        if self.mode == 'test':\n            return torch.tensor(image)\n        else:\n            return torch.tensor(image), torch.tensor(row.label)","11127d8e":"def get_transforms(image_size=512):\n\n    transforms_train = albumentations.Compose([\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.JpegCompression(quality_lower=99, quality_upper=100),\n        albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n        albumentations.Resize(image_size, image_size),\n        albumentations.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n        albumentations.Normalize()\n    ])\n\n    transforms_val = albumentations.Compose([\n        albumentations.Resize(image_size, image_size),\n        albumentations.Normalize()\n    ])\n\n    return transforms_train, transforms_val","8c22732f":"def generate_image_embeddings(df, config, VisionEncoder):\n    print(config)\n    \n    vision_encoder_path = config['vision_model']\n    vision_encoder_params= config['params']\n    img_size = config['IMAGE_SIZE']\n\n    ckpt= torch.load(vision_encoder_path, map_location='cpu')\n\n    model= VisionEncoder(**vision_encoder_params)\n\n    model.load_state_dict(ckpt)\n    model = model.cuda()\n\n\n    ##########################################################################################################\n\n    _, transforms_val = get_transforms(img_size)\n\n    dataset_test = ShopeeDatasetVision(df, 'test', transform=transforms_val)\n    test_loader = torch.utils.data.DataLoader(dataset_test,  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n\n    ##########################################################################################################\n\n    model.eval()\n\n    image_embeddings = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            images = batch\n\n            images = images.cuda()\n            image_embed , _= model(images)\n\n            image_embeddings.append(image_embed.detach().cpu())\n\n    image_embeddings = torch.cat(image_embeddings, dim=0)\n    #image_embeddings= F.normalize(image_embeddings)\n\n    print(f'image_embeddings shape : {image_embeddings.shape}')\n    \n    del ckpt\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return image_embeddings","50a3a9eb":"class TextEncoder(nn.Module):\n    def __init__(self, embed_dim , out_dim, transformer_model):\n        super().__init__()\n        \n        self.base = AutoModel.from_pretrained(transformer_model)\n        \n        self.projection = Projection(TRANSFORMER_EMBED_DIM, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n        \n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    @staticmethod\n    def mean_pooling(model_output, attention_mask):\n        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings \/ sum_mask\n\n    def forward(self, input_ids,attention_mask):\n        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n        out = TextEncoder.mean_pooling(out, attention_mask)\n        \n        projected_vec = self.projection(out)\n        cosine_feat= self.metric_classify(projected_vec)\n        \n        return projected_vec ,cosine_feat\n","5e4f65f8":"class Tokenizer:\n    def __init__(self,max_length, transformer_model):\n        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n        self.max_length = max_length\n\n    def __call__(self, x) :\n        return self.tokenizer(\n            x, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\"\n        )","3720c4f8":"class ShopeeDatasetText(Dataset):\n    def __init__(self, csv, mode, tokenizer):\n\n        self.csv = csv.reset_index()\n        self.mode= mode\n        self.tokenizer = tokenizer\n\n\n    def __len__(self):\n        return self.csv.shape[0]\n    \n    @staticmethod\n    def string_escape(s, encoding='utf-8'):\n        return (\n            s.encode('latin1')  # To bytes, required by 'unicode-escape'\n            .decode('unicode-escape')  # Perform the actual octal-escaping decode\n            .encode('latin1')  # 1:1 mapping back to bytes\n            .decode(encoding)\n        )  # Decode original encoding\n\n    @staticmethod\n    def preprocess_text(x):\n        x = ShopeeDatasetText.string_escape(x)\n        x = re.sub(r'[^\\w\\s]',' ', x)\n        return x\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        \n        text= ShopeeDatasetText.preprocess_text(text)\n\n        encoded_text = self.tokenizer(text)\n        \n        input_ids = encoded_text['input_ids'][0]\n        attention_mask = encoded_text['attention_mask'][0]\n\n        if self.mode == 'test':\n            return input_ids, attention_mask\n        else:\n            return input_ids, attention_mask, torch.tensor(row.label)","c9d571b4":"def generate_text_embeddings(df, config):\n    print(config)\n    \n    transformer_model= config['transformer_model']\n    max_len= config['MAX_LEN']\n\n\n    ckpt= torch.load(config['model_path'], map_location='cpu')\n\n    model= TextEncoder(**config['params'], transformer_model=transformer_model)\n\n    model.load_state_dict(ckpt)\n    model = model.cuda()\n\n\n    ##########################################################################################################\n\n    dataset_test = ShopeeDatasetText(df, 'test', Tokenizer(max_len, transformer_model))\n    test_loader = torch.utils.data.DataLoader(dataset_test,  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n\n    ##########################################################################################################\n\n    model.eval()\n\n    text_embeddings = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            \n            input_ids, attention_mask = batch\n\n            input_ids, attention_mask = input_ids.cuda(), attention_mask.cuda()\n            \n            text_embed , _= model(input_ids, attention_mask)\n\n            text_embeddings.append(text_embed.detach().cpu())\n\n    text_embeddings = torch.cat(text_embeddings, dim=0)\n    #text_embeddings= F.normalize(text_embeddings)\n\n    print(f'text_embeddings shape : {text_embeddings.shape}')\n    \n    del ckpt\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return text_embeddings","213fc384":"class ShopeeDatasetJoint(Dataset):\n    def __init__(self, csv, mode, tokenizer, transform):\n\n        self.csv = csv.reset_index()\n        self.mode= mode\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n\n    def __len__(self):\n        return self.csv.shape[0]\n    \n    @staticmethod\n    def string_escape(s, encoding='utf-8'):\n        return (\n            s.encode('latin1')  # To bytes, required by 'unicode-escape'\n            .decode('unicode-escape')  # Perform the actual octal-escaping decode\n            .encode('latin1')  # 1:1 mapping back to bytes\n            .decode(encoding)\n        )  # Decode original encoding\n\n    @staticmethod\n    def preprocess_text(x):\n        x = ShopeeDatasetJoint.string_escape(x)\n        x = re.sub(r'[^\\w\\s]',' ', x)\n        return x\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        \n        text= ShopeeDatasetJoint.preprocess_text(text)\n\n\n        ##################################################################\n        \n        encoded_text = self.tokenizer(text)\n        input_ids = encoded_text['input_ids'][0]\n        attention_mask = encoded_text['attention_mask'][0]\n        ##################################################################\n        \n        image = cv2.imread(row.filepath)[:,:,::-1]\n\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image'].astype(np.float32)\n        else:\n            image = image.astype(np.float32)\n\n        image = image.transpose(2, 0, 1)\n        ##################################################################\n        \n\n        if self.mode == 'test':\n            return image, input_ids, attention_mask\n        else:\n            return image, input_ids, attention_mask, torch.tensor(row.label)","62a47270":"class DeitWrapperJoint(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.backbone = deit_base_distilled_patch16_224(pretrained=False)\n        self.backbone.head= nn.Identity()\n        self.backbone.head_dist = nn.Identity()\n\n    def forward_features(self, x):\n        # taken from https:\/\/github.com\/facebookresearch\/deit\/blob\/main\/models.py\n\n        B = x.shape[0]\n        x = self.backbone.patch_embed(x)\n\n        cls_tokens = self.backbone.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        dist_token = self.backbone.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n\n        x = x + self.backbone.pos_embed\n        x = self.backbone.pos_drop(x)\n\n        for blk in self.backbone.blocks:\n            x = blk(x)\n\n        x = self.backbone.norm(x)\n        return x\n\n    def forward(self, x):\n        hidden_states = self.forward_features(x)\n        _cls, _cls_dist = hidden_states[:, 0], hidden_states[:, 1]\n        merge_cls= (_cls + _cls_dist) \/ 2\n        merge_cls = merge_cls.unsqueeze(1)\n        \n        new_hidden_states = torch.cat([merge_cls, hidden_states[:, 1:-1,:]], dim=1)\n        return new_hidden_states","0b696b64":"def get_text_encoder(transformer_model):\n    text_encoder = AutoModel.from_pretrained(transformer_model)\n    \n    for p in text_encoder.parameters():\n        p.requires_grad = False\n    \n    return text_encoder","6fbf38b0":"class JointTransformer(nn.Module):\n    \n    def __init__(self, embed_dim, out_dim, transformer_model):\n        super().__init__()\n        \n        \n        self.vision_encoder= DeitWrapperJoint()\n        self.text_encoder= get_text_encoder(transformer_model)\n\n        \n        d_in=768\n        encoder_layer = nn.TransformerEncoderLayer(d_model =768, nhead=8, dim_feedforward=2048, dropout=0.1, activation='gelu')\n        encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        self.transformer_encoder = encoder\n        \n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n    \n    @staticmethod \n    def merge_hidden_states(vision_output, text_output):\n        cls_img= vision_output[:,0,: ]\n        cls_text= text_output[:,0,:]\n        cls_joint= (cls_img+cls_text)\/2\n        cls_joint= cls_joint.unsqueeze(1)\n\n        merge_embeddings = torch.cat([cls_joint , vision_output[:,1: ,:] ,  text_output[:,1: ,:]], dim=1)\n        return merge_embeddings\n    \n    @staticmethod \n    def get_pad_mask(image_embed_len , text_attention_mask):\n\n        _batch_size= text_attention_mask.shape[0]\n\n        imge_attention_mask = torch.ones((_batch_size, image_embed_len), dtype=torch.float32).cuda()\n        merge_attention_mask= torch.cat([imge_attention_mask, text_attention_mask], dim=1)\n        merge_attention_mask= (1-merge_attention_mask).bool()\n        return merge_attention_mask\n    \n    def forward_features(self, image, input_ids, attention_mask):\n        \n        vision_output = self.vision_encoder(image)\n        text_output = self.text_encoder(input_ids, attention_mask)[0]\n        \n        text_attention_mask = attention_mask\n\n        image_embed_len = vision_output.shape[1]-1\n\n        joint_embeddings=  JointTransformer.merge_hidden_states(vision_output, text_output)\n        joint_embeddings= joint_embeddings.permute(1,0,2)\n\n        joint_pad_mask = JointTransformer.get_pad_mask(image_embed_len , text_attention_mask)\n        \n        out= self.transformer_encoder(joint_embeddings, src_key_padding_mask = joint_pad_mask)\n        out= out.permute(1,0,2)\n        \n        return out\n    \n    def forward(self, image, input_ids, attention_mask):\n        x= self.forward_features(image, input_ids, attention_mask)\n        x= x[:,0,:] #cls token\n        \n        projected_vec = self.projection(x)\n        cosine_feat= self.metric_classify(projected_vec)\n        return projected_vec ,cosine_feat","f6989116":"def generate_joint_embeddings(df, config):\n    \n    print(config)\n    \n    transformer_model= config['transformer_model']\n    max_len= config['MAX_LEN']\n    model_path= config['model_path']\n    img_size = config['IMAGE_SIZE']\n\n    ckpt= torch.load(model_path, map_location='cpu')\n\n    model= JointTransformer(**config['params'], transformer_model=transformer_model)\n\n    model.load_state_dict(ckpt)\n    model = model.cuda()\n\n    _, transforms_val = get_transforms(img_size)\n\n    ##########################################################################################################\n\n    dataset_test = ShopeeDatasetJoint(df, 'test', tokenizer=Tokenizer(max_len, transformer_model), transform=transforms_val)\n    test_loader = torch.utils.data.DataLoader(dataset_test,  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n\n    ##########################################################################################################\n\n    model.eval()\n\n    joint_embeddings = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            \n            images, input_ids, attention_mask = batch\n\n            images = images.cuda()\n            input_ids, attention_mask = input_ids.cuda(), attention_mask.cuda()\n            \n            _embed , _= model(images, input_ids, attention_mask)\n\n            joint_embeddings.append(_embed.detach().cpu())\n\n    joint_embeddings = torch.cat(joint_embeddings, dim=0)\n    joint_embeddings= F.normalize(joint_embeddings)\n\n    print(f'joint_embeddings shape : {joint_embeddings.shape}')\n    \n    del ckpt\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return joint_embeddings","6c29194c":"def get_neighbours_cos_sim(df,embeddings, tuned_threshold, mode='image'):\n    '''\n    When using cos_sim use normalized features else use normal features\n    \n    mode: 'image', 'text, 'joint'\n    '''\n    \n    print(f'Setting mode : {mode}')\n    \n    embeddings = cupy.array(embeddings)\n    \n    if GET_CV:\n        if mode=='image':\n            thresholds = list(np.arange(0.3,0.5,0.05))\n            \n        elif mode=='text':\n            thresholds = list(np.arange(0.3,0.5,0.05))\n            \n        elif mode=='joint':\n            thresholds = list(np.arange(0.3,0.6,0.05))\n\n        scores = []\n        for threshold in thresholds:\n            \n################################################# Code for Getting Preds #########################################\n            preds = []\n            preds2= []\n        \n            CHUNK = 1024*4\n\n            print('Finding similar titles...for threshold :',threshold)\n            CTS = len(embeddings)\/\/CHUNK\n            if len(embeddings)%CHUNK!=0: CTS += 1\n\n            for j in range( CTS ):\n                a = j*CHUNK\n                b = (j+1)*CHUNK\n                b = min(b,len(embeddings))\n\n                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n                for k in range(b-a):\n                    IDX = cupy.where(cts[k,]>threshold)[0]\n                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                    preds.append(o)\n                    \n                    o2 = ' '.join(o)\n                    preds2.append(o2)\n######################################################################################################################\n\n            df['pred_matches'] = preds2\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n            \n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n            \n    else:\n        preds = []\n        CHUNK = 1024*4\n        threshold = tuned_threshold\n\n        print('Finding similar texts...for threshold :',threshold)\n        CTS = len(embeddings)\/\/CHUNK\n        if len(embeddings)%CHUNK!=0: CTS += 1\n\n        for j in range( CTS ):\n            a = j*CHUNK\n            b = (j+1)*CHUNK\n            b = min(b,len(embeddings))\n            print('chunk',a,'to',b)\n\n            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n            for k in range(b-a):\n                IDX = cupy.where(cts[k,]>threshold)[0]\n                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                preds.append(o)\n                    \n    return df, preds","251f9ce1":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['mean_predictions'] , row['joint_predictions']])\n    return ' '.join( np.unique(x) )","131cdc02":"class SimilarityFinder:\n    def __init__(self,text_embeddings, image_embeddings, joint_embeddings):\n        self.text_embeddings = cupy.array(text_embeddings)\n        self.image_embeddings = cupy.array(image_embeddings)\n        self.joint_embeddings = cupy.array(joint_embeddings)\n        \n        \n    def get_cosine_similarity(self, idx_a, idx_b):\n        query1= self.text_embeddings[idx_a : idx_b]\n        query2= self.image_embeddings[idx_a : idx_b]\n        query3= self.joint_embeddings[idx_a : idx_b]\n        \n        \n        dot_similarity1 = cupy.matmul(query1, self.text_embeddings.T)\n        dot_similarity2 = cupy.matmul(query2, self.image_embeddings.T)\n        dot_similarity3 = cupy.matmul(query3, self.joint_embeddings.T)\n        \n        \n        dot_similarity= 0.25*dot_similarity1+ 0.25*dot_similarity2 + 0.5*dot_similarity3\n        \n        return dot_similarity\n    \n    def generate_preds(self,df, threshold):\n        preds= []\n\n        CHUNK = 1024*4\n        CTS = len(self.text_embeddings)\/\/CHUNK\n        if len(self.text_embeddings)%CHUNK!=0:\n            CTS += 1\n\n        for j in range( CTS ):\n            a = j*CHUNK\n            b = (j+1)*CHUNK\n            b = min(b,len(self.text_embeddings))\n\n            sims = self.get_cosine_similarity(a, b)\n\n            for idx in range(len(sims)):\n                indices = cupy.where(sims[idx,]>threshold)[0]         \n                _preds= df.iloc[cupy.asnumpy(indices)].posting_id.values\n                preds.append(_preds)\n\n        return preds\n    \n    def threshold_tuner(self,df):\n        thresholds = list(np.arange(0.3,0.5,0.05))\n        scores= []\n        \n        for threshold in thresholds:\n            print('Finding joint sim threshold :',threshold)\n            \n            preds= self.generate_preds(df,threshold)\n            preds2= list(map(lambda x : ' '.join(x) , preds))\n            \n            df['pred_matches'] = preds2\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            scores.append(score)\n            \n            print(f'Our f1 score for threshold {threshold} is {score}')\n\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        return df,preds\n    \n    def get_preds(self, df, tuned_threshold):\n        if GET_CV:\n            df,preds= self.threshold_tuner(df)\n            #df['matches'] = df['pred_matches']\n        else:\n            preds= self.generate_preds(df,tuned_threshold)\n            #preds= list(map(lambda x : ' '.join(x) , preds))\n            #df['matches'] = preds\n            \n        return df,preds","c9c6d228":"df = read_dataset()","f41b1acb":"image_embeddings_swav = generate_image_embeddings(df, image_models[0], VisionEncoderResnet)\nimage_embeddings_effnet = generate_image_embeddings(df, image_models[1], VisionEncoderEffnet)\nimage_embeddings_deit = generate_image_embeddings(df, image_models[2], VisionEncoderDeit)","b67f14bd":"meta_image_embedding = 0.33*image_embeddings_deit+ 0.33*image_embeddings_swav+ 0.34*image_embeddings_effnet\n\n# image_embeddings_deit = F.normalize(image_embeddings_deit)\n# image_embeddings_swav = F.normalize(image_embeddings_swav)\n# image_embeddings_effnet = F.normalize(image_embeddings_effnet)\nmeta_image_embedding  = F.normalize(meta_image_embedding)","b5c519c0":"df,image_predictions = get_neighbours_cos_sim(df,meta_image_embedding, tuned_threshold=0.75, mode='image')","7d937de9":"text_embeddings_xlm = generate_text_embeddings(df, text_models[0])\ntext_embeddings_bert = generate_text_embeddings(df, text_models[1])\n\nmeta_text_embeddings = 0.5*text_embeddings_xlm + 0.5*text_embeddings_bert\nmeta_text_embeddings= F.normalize(meta_text_embeddings)\n","99be1e16":"df,text_predictions = get_neighbours_cos_sim(df,meta_text_embeddings, tuned_threshold=0.7, mode='text')","b1c9d7b2":"joint_embeddings = generate_joint_embeddings(df, joint_models[0])\n","4714d7a3":"df,joint_predictions = get_neighbours_cos_sim(df,joint_embeddings, tuned_threshold=0.7, mode='joint')","54e7cc30":"sim_finder= SimilarityFinder(meta_text_embeddings, meta_image_embedding, joint_embeddings)","b901dd61":"df,mean_preds= sim_finder.get_preds(df,0.5)","77ac25b0":"df.head()","12d7c44a":"if GET_CV:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['joint_predictions'] = joint_predictions\n    df['mean_predictions'] = mean_preds\n    \n    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n    \n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['joint_predictions'] = joint_predictions\n    df['mean_predictions'] = mean_preds\n    \n    df['matches'] = df.apply(combine_predictions, axis = 1)\n    \n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","6bc636a0":"import matplotlib.pyplot as plt","840631f8":"# **Driver**","db995eb9":"## **Merge Predictions**","2f49071c":"sims= dot_similarity.topk(9)\n\nmatches = []\nall_dfs= [] \nfor val , each in zip(sims.values.tolist()[0],sims.indices.tolist()[0]):\n    print(each, val)\n\n    subdf=df.iloc[each:each+1]\n    all_dfs.append(subdf)\n\nmatch_df= pd.concat(all_dfs)\nmatches = list(match_df.filepath)\nmatch_title = list(match_df.title)\n\nplt.figure(figsize=(20, 20))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(cv2.imread(matches[i])[...,::-1])\n    plt.title(match_title[i])\n    plt.axis(\"off\")\n\nmatch_df","5db73cf3":"idx=12517\nquery= select_embeddings[idx].unsqueeze(0)\ndot_similarity = query@select_embeddings.T","c3edfa57":"# **Vision Model**","315da5c6":"# **Joint Predictions**","2ddc1195":"plt.figure(figsize=(20, 20))\n\ncount=0\n\nfor i, (_, row) in enumerate(list(df.loc[df['label_group'] == match_df.iloc[0].label_group].iterrows())[:9]):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(cv2.imread(row.filepath)[...,::-1])\n    plt.title(str(row.title)+'\\n')\n    plt.axis(\"off\")\n    count+=1\n    if count==8:\n        break","f6a7163d":"# **Text Predictions**","e43363a7":"# **Image Predictions**","11f18dae":"_eid = 0\n\n\nselect_embeddings = [meta_image_embedding ,meta_text_embeddings , joint_embeddings,image_embeddings_effnet][_eid]","869e5abb":"## Common Projection Util","e53c449b":"## **DEBUG**","a7ad26a1":"# **Joint Transformer**","48edf1db":"# **Smilarity Search**","5b1fc551":"# **Text Model**","67a2e35e":"## ArcFace utils"}}