{"cell_type":{"6e6f59e0":"code","81f08b1f":"code","11cd7baa":"code","46981034":"code","1229e6aa":"code","df8d0501":"code","f83d7339":"code","303e2515":"code","d4829035":"code","959a2593":"code","44c64f21":"code","4aa795bf":"code","c1c69cc7":"code","00b6e42f":"code","a25e368e":"markdown","f2e0aa37":"markdown","e050e27d":"markdown"},"source":{"6e6f59e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","81f08b1f":"df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","11cd7baa":"df = df.drop(columns = 'id')\ndf_test = df_test.drop(columns = 'id')","46981034":"x = df.drop(columns = 'claim')\ny = df.claim","1229e6aa":"# import optuna\n# from sklearn.model_selection import train_test_split\n# import lightgbm as lgb\n# from sklearn import metrics\n\n# def objective(trial):\n    \n#     train_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.2)\n \n#     params = {\n#         'objective': 'binary',\n#         'metric': 'auc',\n#         'n_estimators': trial.suggest_categorical(\"n_estimators\", [10000]),\n#         'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.3),\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n#         'bagging_fraction': trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n#         'subsample': trial.suggest_float(\"subsample\", 0.2, 0.9, step=0.1),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n#     }\n    \n#     model = lgb.LGBMClassifier(**params,device = 'gpu')\n#     model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=50)\n\n#     preds = model.predict_proba(test_x)[:,1]\n#     roc_auc_score = metrics.roc_auc_score(test_y,preds)\n    \n#     return roc_auc_score\n\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n \n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","df8d0501":"# Number of finished trials: 50\n# Best trial: {'n_estimators': 10000, 'learning_rate': 0.039373014449069724, 'lambda_l1': 1.3580250554531046, 'lambda_l2': 7.420227454832275e-05, 'num_leaves': 84, 'feature_fraction': 0.7675264126841835, 'bagging_fraction': 0.7, 'subsample': 0.4, 'min_child_samples': 24}","f83d7339":"##create folds\nfrom sklearn import model_selection\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.claim.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold","303e2515":"params ={'device':['gpu'],'n_estimators': 10000, 'learning_rate': 0.039373014449069724, \n         'lambda_l1': 1.3580250554531046, 'lambda_l2': 7.420227454832275e-05, \n         'num_leaves': 84, 'feature_fraction': 0.7675264126841835, \n         'bagging_fraction': 0.7, 'subsample': 0.4, 'min_child_samples': 24}","d4829035":"import joblib\nimport lightgbm as lgb\nimport xgboost as xg\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import tree\ndef run(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    df_train = df_train.drop(columns = 'kfold')\n    df_valid = df_valid.drop(columns = 'kfold')\n    x_train = df_train.drop('claim', axis=1).values\n    y_train = df_train.claim.values\n    x_valid = df_valid.drop('claim', axis=1).values\n    y_valid = df_valid.claim.values\n    clf = lgb.LGBMClassifierclf = lgb.LGBMClassifier(device = 'gpu',\n                            learning_rate= 0.039373014449069724,\n                            n_estimators= 1000,\n                            lambda_l1=  1.3580250554531046, lambda_l2 =  7.420227454832275e-05,subsample= 0.4,\n                            num_leaves= 84, feature_fraction= 0.7675264126841835,\n                            bagging_fraction=  0.7, min_child_samples= 24)\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict_proba(x_valid)[:,1]\n    roc_auc_score = metrics.roc_auc_score(y_valid,y_pred)\n    print(f\"Fold={fold}, roc_auc_score={roc_auc_score}\")\n    File_name = 'model_lgb' + str(fold)\n    joblib.dump(\n    clf,File_name)\nfor i in range(5):\n    run(fold = i)","959a2593":"model_0_lgb= joblib.load('.\/model_lgb0')\nmodel_1_lgb =joblib.load('.\/model_lgb1')\nmodel_2_lgb= joblib.load('.\/model_lgb2')\nmodel_3_lgb= joblib.load('.\/model_lgb3')\nmodel_4_lgb= joblib.load('.\/model_lgb4')","44c64f21":"y_final_3_lgb = model_3_lgb.predict_proba(df_test)[:,1]\ny_final_0_lgb = model_0_lgb.predict_proba(df_test)[:,1]\ny_final_1_lgb = model_1_lgb.predict_proba(df_test)[:,1]\ny_final_2_lgb = model_2_lgb.predict_proba(df_test)[:,1]\ny_final_4_lgb = model_4_lgb.predict_proba(df_test)[:,1]","4aa795bf":"y_final_avg = (y_final_0_lgb + y_final_1_lgb +y_final_2_lgb + y_final_3_lgb + y_final_4_lgb)\/5","c1c69cc7":"submission['claim'] = y_final_avg ","00b6e42f":"submission.to_csv('pred_csv_lgb_avg.csv',index = False)","a25e368e":"## scores = 0.81075\nsources\n#### https:\/\/youtu.be\/5nYqK-HaoKY\n---------------\n#### Credits to the Medium article\nhttps:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258","f2e0aa37":"## Ensemble","e050e27d":"### Train.py"}}