{"cell_type":{"0e43f3ac":"code","cafb3a25":"code","bf3201d2":"code","2a627329":"code","92346d67":"code","b0700ff4":"code","e2b5e733":"code","de0ae90f":"code","c084255d":"code","5a722bc9":"code","15005f8a":"code","39fc2d12":"code","1606bbef":"code","aaab3cd1":"code","92e54ccb":"code","52545d55":"code","d0b73446":"code","2fc26873":"code","10242730":"code","9c9332d7":"code","c1c99663":"code","b4bc6cf1":"code","b517022a":"code","d84b8ae9":"code","58503d99":"code","0eea51cb":"code","6878f670":"code","9ad1df7d":"code","13c94a64":"code","79c165df":"code","1321f42b":"code","d387f892":"markdown","ada45f55":"markdown","39ca9f7c":"markdown","b472165d":"markdown","27a11bf1":"markdown","027fa185":"markdown","d064b1ad":"markdown","9deb9485":"markdown","06733b6a":"markdown","360eb488":"markdown","9b42f06e":"markdown","af83c5e2":"markdown","c581d31b":"markdown","027f2eb1":"markdown","85ef7309":"markdown","19cd5b5d":"markdown","414fafb2":"markdown","22b01d57":"markdown","c4f5f164":"markdown","a23c25e2":"markdown"},"source":{"0e43f3ac":"#Basic liberaries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#Sklearn Packages\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n\n#get the file paths\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n#Define a function to check the RMSE       \ndef check_RMSE (y_train ,train_prediction , y_test ,  test_predicition):\n    print ('Root Mean squared error for the train data  =  ' , \n           mean_squared_error(y_train ,train_prediction , squared=False ))\n    print ('Root Mean squared error for the test data  =  ' , \n           mean_squared_error(y_test ,test_predicition , squared=False ))","cafb3a25":"df_test  = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\ndf_test.head(2)","bf3201d2":"df_items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\ndf_items.head(2)","2a627329":"df_train  = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ndf_train.head(2)","92346d67":"# add month number 34 to the test dataset\ndf_test['date_block_num'] = 34\ndf_test = df_test[['date_block_num' , 'shop_id' , 'item_id' ]]\ndf_test.head(2)","b0700ff4":"# map the latest price for the items in the train data set to the test data set\nitem_price = dict(df_train.groupby('item_id')['item_price'].last().reset_index().values)\ndf_test['item_price'] = df_test.item_id.map(item_price)\ndf_test.head(2)","e2b5e733":"df_train = df_train[df_train.item_id.isin (df_test.item_id)]\ndf_train = df_train[df_train.shop_id.isin (df_test.shop_id)]","de0ae90f":"df_train = df_train.groupby(['date_block_num' , 'shop_id' , 'item_id']).agg({'item_price': 'last', 'item_cnt_day': 'sum'}).reset_index()\ndf_train.head(2)","c084255d":"df_train['shop*item'] = df_train.shop_id *df_train.item_id\ndf_train.head(2)","5a722bc9":"df_test['shop*item'] = df_test.shop_id *df_test.item_id\ndf_test.head(2)","15005f8a":"df_items.drop('item_name' , axis  = 1 , inplace = True)\nitem_cat = dict(df_items.values)\n\ndf_train['item_cat'] = df_train.item_id.map(item_cat)\n\ndf_train.head(2)","39fc2d12":"#map the categories\ndf_test['item_cat'] = df_test.item_id.map(item_cat)\ndf_test.head(2)","1606bbef":"df = pd.concat([df_train , df_test])\n#Normalize\ndf.item_price = np.log1p(df.item_price)\n#fil l the missing\ndf.item_price = df.item_price.fillna(df.item_price.mean())\n#rremove the outlier\ndf.item_cnt_day = df.item_cnt_day.apply(lambda x : 10 if x>10 else x)","aaab3cd1":"df.head()","92e54ccb":"\n\ndef encode_the_numbers (column):\n    \"\"\"\n    function to encode the pandas column depend on thier average target from low to high\n    \"\"\"\n    helper_df = df.groupby(column)['item_cnt_day'].mean().sort_values(ascending = False).reset_index().reset_index()\n    maper = helper_df.groupby(column)[\"index\"].mean().to_dict()\n    df[f'{column}_mean'] = df[column].map(maper)\n    \n    helper_df = df.groupby(column)['item_cnt_day'].sum().sort_values(ascending = False).reset_index().reset_index()\n    maper = helper_df.groupby(column)[\"index\"].sum().to_dict()\n    df[f'{column}_sum'] = df[column].map(maper)\n    \n    helper_df = df.groupby(column)['item_cnt_day'].count().sort_values(ascending = False).reset_index().reset_index()\n    maper = helper_df.groupby(column)[\"index\"].count().to_dict()\n    df[f'{column}_count'] = df[column].map(maper)\n\n","52545d55":"columns_to_encode = ['shop_id', 'item_id','shop*item', 'item_cat']\nfor column in columns_to_encode:\n    encode_the_numbers (column)","d0b73446":"corr_df = df.select_dtypes('number').drop('item_cnt_day', axis=1).corrwith(df.item_cnt_day).sort_values().reset_index().rename(columns = {'index':'feature' ,0:'correlation'})\n\nfig , ax = plt.subplots(figsize  = (5,20))\nax.barh(y =corr_df.feature , width = corr_df.correlation )\nax.set_title('correlation between featuer and target'.title() ,\n            fontsize = 16 , fontfamily = 'serif' , fontweight = 'bold')\nplt.show();","2fc26873":"df_train = df[df.item_cnt_day.notnull()]\ndf_train.head(2)","10242730":"df_test = df[df.item_cnt_day.isnull()]\ndf_test.drop ('item_cnt_day' , axis = 1 , inplace  = True)\ndf_test.head(2)","9c9332d7":"X = df_train.drop('item_cnt_day' , axis = 1).values\ny = df_train.item_cnt_day.values","c1c99663":"SC = MinMaxScaler()\nSC.fit(X)\nX = SC.transform(X)","b4bc6cf1":"x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.30 ,  random_state=10)","b517022a":"reg = RandomForestRegressor(n_estimators=25 )\nreg.fit(x_train,y_train)\ntrain_prediction  = reg.predict(x_train)\ntest_predicition  = reg.predict(x_test)\n\ncheck_RMSE (y_train ,train_prediction , y_test ,  test_predicition)","d84b8ae9":"\"\"\"\n\nknn = KNeighborsRegressor()\nknn.fit(x_train,y_train)\ntrain_prediction  = knn.predict(x_train)\ntest_predicition  = knn.predict(x_test)\n\ncheck_RMSE (y_train ,train_prediction , y_test ,  test_predicition)\n\"\"\"","58503d99":"\"\"\"\n\nlr = LinearRegression()\nlr.fit(x_train,y_train)\ntrain_prediction  = lr.predict(x_train)\ntest_predicition  = lr.predict(x_test)\n\ncheck_RMSE (y_train ,train_prediction , y_test ,  test_predicition)\n\"\"\"\n","0eea51cb":"\"\"\"\n\n\nfrom sklearn.svm import SVR\nSVR=SVR()\nSVR.fit(x_train,y_train)\ntrain_prediction  = SVR.predict(x_train)\ntest_predicition  = SVR.predict(x_test)\n\ncheck_RMSE (y_train ,train_prediction , y_test ,  test_predicition)\n\n\"\"\"","6878f670":"\"\"\"\nfrom sklearn.linear_model import Ridge\nRidge=Ridge()\nRidge.fit(x_train,y_train)\ntrain_prediction  = Ridge.predict(x_train)\ntest_predicition  = Ridge.predict(x_test)\n\ncheck_RMSE (y_train ,train_prediction , y_test ,  test_predicition)\n\"\"\"","9ad1df7d":"\"\"\"\nfrom sklearn.linear_model import BayesianRidge\nBayesian = BayesianRidge()\nBayesian.fit(x_train,y_train)\ntrain_prediction  = Bayesian.predict(x_train)\ntest_predicition  = Bayesian.predict(x_test)\n\ncheck_RMSE (y_train ,train_prediction , y_test ,  test_predicition)\n\"\"\"","13c94a64":"X_submission =df_test.values\nX_submission = SC.transform(X_submission)","79c165df":"predection  = reg.predict(X_submission)\nsample_submission  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nsample_submission.item_cnt_month = predection\nsample_submission.head(2)","1321f42b":"sample_submission.to_csv('submission.csv' , index = False)","d387f892":"<h2> split the train and test","ada45f55":"<h2> I will concate the two train and test datasets to remove the outliers","39ca9f7c":"## SVR","b472165d":"<h2> Add feature to be unique for shop and item for the test and train dataset","27a11bf1":"## Ridge","027fa185":"<h2> Re-shape the train dataset and count the sum of sales per each month as required by the competition","d064b1ad":"<h3> now (item_cnt_day)  represent the sum of sales per month for each item in each shop ","9deb9485":"<h2> prepare the X and y","06733b6a":"<h2> prepare the test data for submission","360eb488":"## Linear reg","9b42f06e":"<h1>Read the CSV's <\/h1>","af83c5e2":"## KNN","c581d31b":"<h2> V10 : encode columns","027f2eb1":"## Random forest","85ef7309":"<h2> i will Remove the shop_id and item_id in the train dataset and not in the test dataset","19cd5b5d":"## Scale the X","414fafb2":"## Select random forest","22b01d57":"<h2> from the item dataset let's map the categories to the item_id","c4f5f164":"## BayesianRidge","a23c25e2":"<h2> First challenge , the train and test data sets were not having the dame columns"}}