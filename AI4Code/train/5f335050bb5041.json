{"cell_type":{"640ceb13":"code","dccef051":"code","15e280b3":"code","c267576e":"code","e1449193":"code","6b1a34e0":"code","793a454e":"code","a3e9a518":"code","cc7319e7":"code","e43464cf":"code","582359a5":"code","626f834b":"code","93897188":"code","cd202eb7":"code","b72ef20a":"code","744649b3":"code","2a8d42e4":"code","bfaa51bd":"code","8fc734cf":"code","b64fd23b":"code","20aaffba":"code","2eebc1a1":"code","f50d0eb5":"code","526b91d2":"code","b08efa1f":"markdown","dfa00920":"markdown","52d7f26e":"markdown","9f33255e":"markdown","fc5c3cbb":"markdown","e0784c8a":"markdown","20e37556":"markdown","947ca446":"markdown","02b2d49b":"markdown","9d24ee31":"markdown","c8b9d18b":"markdown","de3b2296":"markdown","1f28d40c":"markdown","727554aa":"markdown","edbb7609":"markdown"},"source":{"640ceb13":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npd.set_option(\"display.max_columns\", 500)","dccef051":"df = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head()","15e280b3":"df.shape","c267576e":"df.info()","e1449193":"df.describe()","6b1a34e0":"df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \", np.nan, regex=False).astype(float)","793a454e":"df.describe()","a3e9a518":"df[\"TotalCharges\"].hist()","cc7319e7":"df[\"TotalCharges\"] = df[\"TotalCharges\"].apply(lambda x: np.log1p(x))","e43464cf":"df[\"Churn\"].value_counts(normalize=True)","582359a5":"map_labels = {\"No\": 0,\n              \"Yes\": 1}\n\ndf[\"Churn\"] = df[\"Churn\"].map(map_labels)","626f834b":"df.isnull().sum()","93897188":"df = df.drop(columns=[\"customerID\"])","cd202eb7":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=23)\nfor train_index, test_index in split.split(df, df[\"Churn\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index]","b72ef20a":"X_train = strat_train_set.drop(\"Churn\", axis=1)\ny_train = strat_train_set[\"Churn\"].copy()","744649b3":"X_train.corr()","2a8d42e4":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nnum_attribs = [\"MonthlyCharges\", \"TotalCharges\", \"tenure\"]\ncat_attribs = [\"SeniorCitizen\", \"gender\", \"Partner\", \"Dependents\",\n               \"PhoneService\", \"MultipleLines\", \"InternetService\", 'OnlineSecurity', \n               'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', \n               'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n\n\nnum_pipeline = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\n\npreprocessor = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs),\n])","bfaa51bd":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)","8fc734cf":"from sklearn.linear_model import LogisticRegression\n\nclf_lr = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression(solver=\"lbfgs\", max_iter=300, class_weight=\"balanced\"))])\n\nscores = cross_val_score(clf_lr, X_train, y_train, cv=skf, scoring=\"roc_auc\", n_jobs=-1)\nscores.mean()","b64fd23b":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=clf_lr,\n                                                        X=X_train,\n                                                        y=y_train,\n                                                        train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=skf,\n                                                        scoring=\"roc_auc\")","20aaffba":"train_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(train_sizes, train_mean,\n          color='blue', marker='o',\n          markersize=5, label='Training AUC')\nplt.fill_between(train_sizes,\n                  train_mean + train_std,\n                  train_mean - train_std,\n                  alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean,\n          color='green', linestyle='--',\n          marker='s', markersize=5,\n          label='Validation AUC')\nplt.fill_between(train_sizes,\n                  test_mean + test_std,\n                  test_mean - test_std,\n                  alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training examples')\nplt.ylabel('ROC_AUC')\nplt.legend(loc='lower right')\nplt.ylim([0.8, 0.9])\nplt.show()","2eebc1a1":"final_model = clf_lr.fit(X_train, y_train)","f50d0eb5":"X_test = strat_test_set.drop(\"Churn\", axis=1)\ny_test = strat_test_set[\"Churn\"].copy()\n\n\nfinal_predictions = final_model.predict(X_test)","526b91d2":"from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n\nprint(confusion_matrix(y_test, final_predictions))\nprint(accuracy_score(y_test, final_predictions))\nprint(roc_auc_score(y_test, final_predictions))","b08efa1f":"## Looking for correlations","dfa00920":"By the learning curve, our model seems to be OK. If something it can be **underfitting**.\n\n- We could try a more complex model, but risk overfitting the training data\n- We could remove the regularization parameter that is on by default on LogisticRegression -> L2 norm\n- Looks like our model won't improve if we collect more data\n\n### Next steps\n- Explore the data better and try feature engineering\n- Tune hyperparameters\n- Stacking\n\nSince this is just a practice example, I won't bother much, let's just select the LogisticRegression and end here","52d7f26e":"### Stratified k-fold cross validation\n\nExperiments by Ron Kohavi on various real-world datasets suggest that **10-fold cross-validation** offers the best tradeoff between bias and variance ( A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection , Kohavi, Ron , International Joint Conference on Artificial Intelligence (IJCAI) , 14 (12): 1137-43, 1995 ).\n\nA slight improvement over the standard k-fold cross-validation approach is stratified k-fold cross-validation, which can yield better bias and variance estimates, especially in cases of unequal class proportions","9f33255e":"Our target are most Nos, I will make sure to stratify every split from now on[](http:\/\/)","fc5c3cbb":"## Finally testing","e0784c8a":"### Plotting the learning curve\nBy plotting the model training and validation accuracies as functions of the training dataset size, we can easily detect whether the model suffers from high variance or high bias, and whether the collection of more data could help to address this problem","20e37556":"# Framing the Problem\nSupervised Classification -> predict if a customer will churn or not","947ca446":"### Creating the preprocessing pipeline\n- It's a good practice to scale the numerical features\n- We will encode the categorical features using OneHotEncoder. Keep in mind that this introduces multicollinearity, which can be an issue for certain methods (for instance, methods that require matrix inversion). If features are highly correlated, matrices are computationally difficult to invert, which can lead to numerically unstable estimates. To reduce the correlation among variables, we can simply remove one feature column from the one-hot encoded array.\n- Let's combine both transformations with ColumnTransformer from sklearn","02b2d49b":"- Tenure seems to be highly positive correlated with TotalCharges\n- MonthlyCharges and TotalCharges also have moderate positive correlation","9d24ee31":"TotalCharges column has some empty cells, so it's treated as an object (string). Let's replace these empty cells with NaNs and cast to float","c8b9d18b":"Resembles a power law distribution (not exactly, but OK). We can apply log transform to it","de3b2296":"## Getting the data","1f28d40c":"We have 11 missing values on TotalCharges, we could drop these rows, but, since we have few data points, I will opt to impute with the median later.","727554aa":"## Investigating the data structure","edbb7609":"## Creating a test set\nStratified sampling because the target is not balanced"}}