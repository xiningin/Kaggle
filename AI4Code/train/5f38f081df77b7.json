{"cell_type":{"c988a3bc":"code","d59d0f31":"code","383a0170":"code","84c403c0":"code","0ad76b27":"code","859fc33a":"code","e306e01f":"code","594bd4af":"code","a94cd5c4":"code","55e26732":"code","03b8565e":"code","ecdfaf92":"code","6ba10652":"code","3bba5e3b":"code","bd7ef625":"code","d82318da":"markdown","6bc201f8":"markdown","5f410831":"markdown","7d6fabf1":"markdown","f14f6b50":"markdown","d9ff0ae6":"markdown","9e7edeed":"markdown","da537ac8":"markdown"},"source":{"c988a3bc":"from torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom IPython.display import display\nfrom collections import defaultdict\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport seaborn as sns\nimport tqdm\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nimport torch","d59d0f31":"train_paths = list(Path(\"\/datasets\/usg-kaggle\/train\/\").rglob(\"circle.png\"))\ntest_paths = list(Path(\"\/datasets\/usg-kaggle\/test\").rglob(\"circle.png\"))","383a0170":"print(f\"Training size: {len(train_paths)}, Test size: {len(test_paths)}\")","84c403c0":"training_classes = [path.parent.parent.name for path in train_paths]\ncounts = Counter(training_classes)\ndisplay(counts)\nplt.bar(counts.keys(), counts.values())","0ad76b27":"f, ax = plt.subplots(8, 4, figsize=(12, 24))\nax = np.asarray(ax).flatten()\nrandom_images = np.random.choice(train_paths, size=64)\nplt.axis('off')\nfor i in range(len(ax)):\n    ax[i].imshow(cv2.imread(random_images[i].as_posix()))\n    ax[i].set_title(\"class: \" + random_images[i].parent.parent.name)\n    ax[i].axis('off')","859fc33a":"f, ax = plt.subplots(8, 4, figsize=(12, 24))\nax = np.asarray(ax).flatten()\nrandom_images = np.random.choice(test_paths, size=64)\nplt.axis('off')\nfor i in range(len(ax)):\n    ax[i].imshow(cv2.imread(random_images[i].as_posix()))\n    ax[i].set_title(\"class: \" + random_images[i].parent.parent.name)\n    ax[i].axis('off')","e306e01f":"class MLP(nn.Module):\n    def __init__(self, input_size=32 * 32):\n        super().__init__()\n        self.layer_1 = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n        self.layer_2 = nn.Linear(256, 2)\n    \n    def forward(self, inputs):\n        x = self.layer_1(inputs)\n        x = self.layer_2(x)\n        \n        return x\n    \nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.layer_1 = nn.Sequential(\n            nn.Conv2d(1, 32, 5),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.MaxPool2d(2)\n        )\n        self.layer_2 = nn.Sequential(\n            nn.Conv2d(32, 64, 5),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.MaxPool2d(2)\n        )\n        \n        self.mlp = MLP(5 * 5 * 64)\n        \n    def forward(self, inputs):\n        x = self.layer_1(inputs)\n        x = self.layer_2(x)\n        x = x.view(-1, 5 * 5 * 64)\n        x = self.mlp(x)\n        return x\n        ","594bd4af":"class UsgDataset(Dataset):\n    def __init__(self, images_paths, is_training):\n        super().__init__()\n        self.images_paths = images_paths\n        self.is_training = is_training\n        \n    def __getitem__(self, index):\n        a_path = self.images_paths[index]\n        img = cv2.imread(a_path.as_posix())\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (32, 32)).astype(np.float32) \/ 255\n        img = img[np.newaxis]\n        \n        if self.is_training:\n            a_class = a_path.parent.parent.name\n        else:\n            a_class = a_path.parent.name\n        \n        expected_output = int(a_class)\n        return img, expected_output\n    \n    def __len__(self):\n        return len(self.images_paths)","a94cd5c4":"train_samples, valid_samples = train_test_split(train_paths)\ntrain_dataset = UsgDataset(train_samples, True)\nvalid_dataset = UsgDataset(valid_samples, True)\ntest_dataset = UsgDataset(test_paths, False)\n\nprint(f\"Train length: {len(train_dataset)}\")\nprint(f\"Valid length: {len(valid_dataset)}\")\nprint(f\"Test length: {len(test_dataset)}\")","55e26732":"batch_size = 32\nlearning_rate = 0.001\nnum_epochs = 100\nuse_cuda = torch.cuda.is_available()","03b8565e":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)","ecdfaf92":"model = CNN()\n\nloss_function = nn.CrossEntropyLoss()\noptimiser = optim.Adam(model.parameters())\n\n# optimiser = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n\nif use_cuda:\n    model = model.cuda()\n    loss_function = loss_function.cuda()\n    \ntrain_stats = defaultdict(list)\n    \nfor epoch in range(num_epochs):\n    print(\"Epoch: {}\/{}\".format(epoch + 1, num_epochs))\n    \n    total_correct = 0\n    total_samples = 0\n    train_losses = []\n    \n    model.train()\n    \n    with tqdm.tqdm_notebook(total=len(train_loader)) as pbar:\n        for x_train, y_train in train_loader:\n            optimiser.zero_grad()\n            if use_cuda:\n                x_train, y_train = x_train.cuda(), y_train.cuda()\n            \n            predicted_output = model(x_train)\n\n            predicted_class = predicted_output.argmax(dim=1)\n\n            total_correct += (predicted_class == y_train).float().sum()\n            total_samples += len(y_train)\n\n            loss = loss_function(predicted_output, y_train)\n            loss.backward()\n            optimiser.step()\n            \n            train_losses.append(loss.cpu().item())\n            \n            pbar.set_postfix(loss=loss.cpu().item(), acc=(total_correct \/ total_samples * 100).cpu().item())\n            pbar.update(1)\n            \n    train_stats[\"train_loss\"].append(np.mean(train_losses))\n    train_stats[\"train_acc\"].append(total_correct \/ total_samples * 100)\n            \n    validation_correct = 0\n    validation_total_samples = 0\n    validation_losses = []\n    \n    model.eval()\n    for x_train, y_train in valid_loader:\n        if use_cuda:\n            x_train, y_train = x_train.cuda(), y_train.cuda()\n\n        predicted_output = model(x_train)\n\n        predicted_class = predicted_output.argmax(dim=1)\n        loss = loss_function(predicted_output, y_train)\n\n        validation_correct += (predicted_class == y_train).float().sum().cpu().item()\n        validation_total_samples += len(y_train)\n        validation_losses.append(loss.cpu().item())\n        \n    train_stats[\"valid_loss\"].append(np.mean(validation_losses))\n    train_stats[\"valid_acc\"].append(validation_correct \/ validation_total_samples * 100)\n        \n    print(\"Validation loss: {:.4f}, validation accuracy: {:.4f}\".format(\n        np.mean(validation_losses), validation_correct \/ validation_total_samples * 100\n    ))\n    ","6ba10652":"plt.figure()\nplt.plot(list(range(num_epochs)), train_stats[\"train_loss\"], label=\"Training\")\nplt.plot(list(range(num_epochs)), train_stats[\"valid_loss\"], label=\"Validation\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss (the lower, the better)\")\nprint()","3bba5e3b":"plt.figure()\nplt.plot(list(range(num_epochs)), train_stats[\"train_acc\"], label=\"Training\")\nplt.plot(list(range(num_epochs)), train_stats[\"valid_acc\"], label=\"Validation\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss (the lower, the better)\")\nprint()","bd7ef625":"test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\nout_data = defaultdict(list)\nmodel.eval()\nfor x_train, ids in tqdm.tqdm_notebook(test_loader):\n    if use_cuda:\n        x_train = x_train.cuda()\n\n    predicted_output = model(x_train)\n\n    predicted_class = predicted_output.argmax(dim=1).cpu().numpy()\n    ids = ids.cpu().numpy()\n    \n    for an_id, a_class in zip(ids, predicted_class):\n        out_data[\"id\"].append(an_id)\n        out_data[\"label\"].append(a_class)\n\nout_data = pd.DataFrame(out_data)\nout_data = out_data.sort_values(by=\"id\")\nout_data.to_csv(\"submission_{}.csv\".format(np.mean(validation_losses)), index=False)","d82318da":"# USG Liver health classification\n\nUseful links:\n- https:\/\/pytorch.org\/tutorials\/\n- https:\/\/www.kaggle.com\/bguberfain\/elastic-transform-for-data-augmentation\n- https:\/\/software.intel.com\/en-us\/articles\/hands-on-ai-part-14-image-data-preprocessing-and-augmentation\n- https:\/\/medium.com\/ymedialabs-innovation\/data-augmentation-techniques-in-cnn-using-tensorflow-371ae43d5be9\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html\n\n**Import notes**:\nUsing a neural network here is just an example. Look at other methods, such as other classifiers (Random Forests, SVMs, all available in `scikit-learn` library) or methods based on color or other features (simple thresholding based on a color histogram).\n\nConsider also different methods of processing data:\n- using cross validation (`KFold` in `sklearn`)\n- augmentation methods available in `torchvision`","6bc201f8":"## Generating submission","5f410831":"In general validation plot should be close to the training one. This one, as above, indicates that something is wrong with our model (or data)","7d6fabf1":"## Model definition","f14f6b50":"## Dataset definition","d9ff0ae6":"## Training","9e7edeed":"## Data loading","da537ac8":"## Data analysis"}}