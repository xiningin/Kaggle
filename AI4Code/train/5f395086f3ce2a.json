{"cell_type":{"0645a82d":"code","50393a7a":"code","eb4f448a":"code","d879bff8":"code","006cc4a1":"code","3e9f6dd8":"code","ca74b6d0":"code","d24f846d":"code","fef0abe4":"code","26d07e85":"code","25d6823b":"code","92b64c42":"code","0b993a15":"code","ee0e6147":"code","719ffb4f":"code","8ca87dec":"code","7c439a73":"code","3a5bcfb5":"code","214e29a8":"code","df56c1e6":"code","879b4833":"code","8919d787":"code","38bd2d90":"markdown","934d39cc":"markdown","f0056e35":"markdown","67a2dddd":"markdown","52dc5cd8":"markdown","b9304cb7":"markdown","de1e2a1c":"markdown","1644d159":"markdown","112a7dda":"markdown","1fb5ebc9":"markdown","9403f65a":"markdown","a15977de":"markdown","05fe7751":"markdown","af3055a0":"markdown","76737a9e":"markdown","e03255e3":"markdown","540ddd2a":"markdown"},"source":{"0645a82d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.impute import SimpleImputer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50393a7a":"#Get data\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ndf = pd.read_csv('..\/input\/titanic\/train.csv')\ndf.describe()","eb4f448a":"df.isnull().sum()\n","d879bff8":"df['Embarked'].describe()\n","006cc4a1":"data = [test_data, df]\nfor d in data:\n    d['Embarked'] = d['Embarked'].fillna('S')\n    d.isnull().sum()","3e9f6dd8":"data = [test_data, df]\nfor d in data:\n    d['Age'] = d['Age'].fillna(d['Age'].mean())\n    d.isnull().sum()","ca74b6d0":"df = df.drop(['Cabin','Ticket','PassengerId'], axis = 1)\ndf","d24f846d":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,4))\nwomen = df[df['Sex'] == 'female']\nmen = df[df['Sex'] == 'male']\nax = sns.distplot(a = women[women['Survived'] == 1].Age.dropna(), kde = False, \n                 label = 'Survived', ax = axes[0], bins=20)\nax = sns.distplot(a = women[women['Survived'] == 0].Age.dropna(), kde = False, \n                 label = 'Not Survived', ax = axes[0], bins=50)\nax.legend()\nax.set_title('Women')\nax = sns.distplot(a = men[men['Survived'] == 1].Age.dropna(), kde = False, \n                 label = 'Survived', ax = axes[1], bins=20)\nax = sns.distplot(a = men[men['Survived'] == 0].Age.dropna(), kde = False, \n                 label = 'Not Survived', ax = axes[1], bins=50)\nax.legend()\nax.set_title('Men')\nplt.show()","fef0abe4":"fg = sns.FacetGrid(df, row='Embarked')\nfg.map(sns.pointplot, 'Sex', 'Survived',\n               palette=None,  order=None, hue_order=None )","26d07e85":"sns.barplot(x=df['Pclass'], y=df['Survived'])\nplt.show()","25d6823b":"fg2 = sns.FacetGrid(df, col='Survived', row='Pclass')\nfg2.map(plt.hist, 'Age', alpha=.5, bins=20)\n","92b64c42":"data = [test_data, df]\nfor d in data:\n    d['relatives'] = d['SibSp'] + d['Parch']\n    d['relatives'].value_counts()","0b993a15":"data = [test_data, df]\nfor d in data:\n    d = d.drop(['SibSp','Parch'], axis = 1)\n    d.info()","ee0e6147":"data = [test_data, df]\nfor d in data:\n    d['Age'] = d['Age'].astype(int)\n    d['Fare'] = d['Fare'].fillna(0)\n    d['Fare'] = d['Fare'].astype(int)\n    d.info()","719ffb4f":"data = [test_data, df]\nfor d in data:\n    d['Embarked'] = d['Embarked'].astype('category')\n    d['embarked_cat'] = d['Embarked'].cat.codes\n    d = d.drop(['Embarked'],axis=1)\n    d.head()","8ca87dec":"data = [test_data, df]\nfor d in data:\n    d['Sex'] = d['Sex'].astype('category')\n    d['sex_cat'] = d['Sex'].cat.codes\n    d = d.drop(['Sex','Name'],axis=1)\n    print(d)\n\n\ntest_data = test_data.drop(['Name','Cabin','Parch','Ticket','SibSp','Sex','Embarked'],axis=1)\ndf = df.drop(['Sex','Name','Embarked','SibSp','Parch'],axis=1)\ndf","7c439a73":"X_train = df.drop(['Survived'],axis=1)\ny_train = df['Survived']\nX_test = test_data.drop(['PassengerId'],axis=1)\n","3a5bcfb5":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nknn_accuracy = knn.score(X_train, y_train)\nprint(knn_accuracy)","214e29a8":"linreg = LinearRegression()\nlinreg.fit(X_train,y_train)\ny_pred = linreg.predict(X_test)\nlinreg_accuracy = linreg.score(X_train,y_train)\nprint(linreg_accuracy)","df56c1e6":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred = linreg.predict(X_test)\nlogreg_accuracy = logreg.score(X_train,y_train)\nprint(logreg_accuracy)","879b4833":"tree = DecisionTreeClassifier()\ntree.fit(X_train,y_train)\ny_pred = tree.predict(X_test)\ntree_accuracy = tree.score(X_train,y_train)\nprint(tree_accuracy)","8919d787":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\nrf_accuracy = rf.score(X_train,y_train)\nprint(rf_accuracy)","38bd2d90":"*I initially wrote this post on kaggle.com, as part of the \u201cTitanic: Machine Learning from Disaster\u201d Competition. In this challenge, we are asked to predict whether a passenger on the titanic would have been survived or not.*","934d39cc":"**Random Forest**","f0056e35":"**Logistic Regression**","67a2dddd":"Since DecisionTreeClassifier and RandomForestClassfier have highest scores we can use one of them.","52dc5cd8":"We changed Embarked values into categorigal values in embarked_cat column. 0 corresponds to C, 1 corresponds to Q and 2 corresponds to S.\n\nNow let's do the same operation to Sex column.","b9304cb7":"**Decision Tree**","de1e2a1c":"# Building Machine Learning Algortihms","1644d159":"As you can see, there is high probability that survived person is women \nand between 14 and and 40.\nOn the other hand men have low probability of surviving, especially between\n5 and 17.\nIn general babies seem have more chance to survive.\n\n\nNow let's look at relation between port of embarkation, sex and survival.\n\n\n","112a7dda":"We changed male's to 1, females's to 0.","1fb5ebc9":"We have nulls to deal with in Age, Cabin and Embarked features. Let's deal with missing values. First look at embarked feature. Since it has just 2 missing values we can fill that values with most frequent values.","9403f65a":"Now, fill missing values for age feature with mean values. ","a15977de":"Graphs shows us there is correlation between gender and embarkation. Females who have embarked on Southampton and Queenstown have more chance to survive. Inversely, females who have embarked on Cherbourg have lower chance to survive according to other ports. Males have higher probability to survive if they have embarked on Cherbourg, but low probability if they embarked on Queenstown and Southampton.\n\nNow let's look at whether or not there is relationship between\npclass and survival.","05fe7751":"Result shows us that passengers in class 1 have higher probability to survive and also passengers in class 3 have lower probability to survive.\n\nNow let's look at sibsp and parch features. It is better if we combine because it makes more sense if we look at the relatives as whole.","af3055a0":"**k - Nearest Neighbors**","76737a9e":"Drop the Cabin feature since it has lot of missing values.","e03255e3":"As you can see in graph, higher classes have higher probability of surviving.\n\nNow let's deep into Pclass and survival probabilities.","540ddd2a":"**Linear Regression**"}}