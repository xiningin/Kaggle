{"cell_type":{"145be116":"code","d0e61f01":"code","4dcea062":"code","4e9c0e2c":"code","19c156c6":"code","0aa2ca8d":"code","2c8f831e":"code","7028e4ed":"code","167fe5b6":"code","37decb23":"code","988afda6":"code","c5ed6f40":"code","fc719f1e":"code","697b1869":"code","d876acac":"code","660b3411":"code","8a9fd5f5":"code","804bec27":"code","f75de084":"code","ea7139c3":"code","b57764b3":"markdown","69c141c6":"markdown","58717afe":"markdown","7330b4c3":"markdown","186ae53a":"markdown","7e9ee7bd":"markdown","47e8f337":"markdown","738e3fdb":"markdown"},"source":{"145be116":"#Dataset for read only\nimport os\nfrom xml.etree import ElementTree as et\n#Data processing and data visualization with bounding boxes\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n#Global used to return all file paths with speicifc pattern\nimport glob\n#Linear Algebra\nimport numpy as np # linear algebra\n#Parsing HTML and XML Documents (We use the XML docs)\nfrom bs4 import BeautifulSoup\n#Model\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport torch\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2","d0e61f01":"images_dir = '..\/input\/face-mask-detection\/images\/'\nannotations_dir = '..\/input\/face-mask-detection\/annotations\/'","4dcea062":"class FaceMaskDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, images_dir, annotation_dir,width, height, transforms=None):\n        self.transforms = transforms\n        self.images_dir = images_dir\n        self.annotation_dir = annotation_dir\n        self.height = height\n        self.width = width\n        \n        # Sorting images and checking extension filename is checked to be jpg\n        self.imgs = [image for image in sorted(os.listdir(images_dir))]\n        self.annotate = [image for image in sorted(os.listdir(annotation_dir))]\n        \n        # Classes divided into 4 with background labeled 0\n        self.classes = [_, 'without_mask','with_mask','mask_weared_incorrect']\n\n    def __getitem__(self, idx):\n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.images_dir, img_name)\n\n        # Reading the images    \n        img = cv2.imread(image_path)\n        \n        # Converting size and color\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        \n        # Image Normalization\n        img_res \/= 255.0\n        \n        # Annotate files\n        annot_filename = self.annotate[idx]\n        annot_file_path = os.path.join(self.annotation_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # CV2 shaping height and width sizes\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # Bounding box coordinates for XML files\n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # bounding box\n            xmin = int(member.find('bndbox').find('xmin').text)\n            xmax = int(member.find('bndbox').find('xmax').text)\n            \n            ymin = int(member.find('bndbox').find('ymin').text)\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            # Corrected for image size given\n            xmin_corr = (xmin\/wt)*self.width\n            xmax_corr = (xmax\/wt)*self.width\n            ymin_corr = (ymin\/ht)*self.height\n            ymax_corr = (ymax\/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # Convert boxes into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # Getting the areas of the boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        # Suppose all instances are not crowd\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        \n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res, \n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)","4e9c0e2c":"# Checking dataset\ndataset = FaceMaskDataset(images_dir, annotations_dir, 224, 224)\nprint('length of dataset = ', len(dataset), '\\n')\n\n# Testing image shape\nimg, target = dataset[78]\nprint('Image shape = ', img.shape, '\\n','Target - ', target)","19c156c6":"#Visualizing bounding box\ndef plot_img_bbox(img, target):\n    # plot the image and bboxes\n    # Bounding boxes are defined as follows: x-min y-min width height\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(5,5)\n    a.imshow(img, cmap='gray')\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Drawing the bounding box on images\n        a.add_patch(rect)\n    plt.show()","0aa2ca8d":"# Plotting bounding box on the images\nimg, target = dataset[1]\nplot_img_bbox(img, target)","2c8f831e":"# Send train=True fro training transforms and False for val\/test transforms\ndef get_transform(train):\n    \n    if train:\n        return A.Compose([\n                            ToTensorV2(p=1.0) \n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","7028e4ed":"def collate_fn(batch):\n    return tuple(zip(*batch))","167fe5b6":"# use our dataset and defined transformations\ndataset = FaceMaskDataset(images_dir, annotations_dir, 480, 480, transforms= get_transform(train=True))\ndataset_test = FaceMaskDataset(images_dir, annotations_dir, 480, 480, transforms= get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\n# train test split\ntest_split = 0.2\ntsize = int(len(dataset)*test_split)\ndataset = torch.utils.data.Subset(dataset, indices[:-tsize])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=10, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)\n\nprint (len(data_loader))","37decb23":"def get_model_instance_segmentation(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","988afda6":"# to train on gpu if selected.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_classes = 4\n\n# get the model using our helper function\nmodel = get_model_instance_segmentation(num_classes)\n\nepochs = 6\n\n# move model to the right device\nmodel.to(device)\n    \n# parameters construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01,\n                                momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\nlen_dataloader = len(data_loader)\n\nfor epoch in range(epochs):\n    model.train()\n    i = 0    \n    epoch_loss = 0\n    for imgs, annotations in data_loader:\n        i += 1\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        loss_dict = model(imgs, annotations)\n        losses = sum(loss for loss in loss_dict.values())        \n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step() \n#         print(f'Iteration: {i}\/{len_dataloader}, Loss: {losses}')\n        epoch_loss += losses.item()\n    print('Epoch_loss = ',epoch_loss)\n","c5ed6f40":"print(model)","fc719f1e":"# the function takes the original prediction and the iou threshold.\n\ndef apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return transforms.ToPILImage()(img).convert('RGB')","697b1869":"# pick one image from the test set\nimg, target = dataset_test[79]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","d876acac":"print('EXPECTED OUTPUT')\nplot_img_bbox(torch_to_pil(img), target)","660b3411":"print('MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), prediction)","8a9fd5f5":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nprint (\"Predicted NMS Labels: \",len(nms_prediction['labels']))\nplot_img_bbox(torch_to_pil(img), nms_prediction)","804bec27":"def plot_image(img_tensor, annotation,predict=True):\n    \n    fig,ax = plt.subplots(1)\n    fig.set_size_inches(18.5, 10.5)\n    img = img_tensor.cpu().data\n    mask_dic = {1:'without_mask', 2:'with_mask', 3:'mask_worn_incorrectly'}\n\n    # Display the image\n    ax.imshow(img.permute(1, 2, 0), cmap='gray')\n    \n    for i,box in enumerate(annotation[\"boxes\"]):\n        xmin, ymin, xmax, ymax = box\n\n        # Create a Rectangle patch\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        label = mask_dic[int(annotation['labels'][i].data)]\n        if predict:\n            score = int((annotation['scores'][i].data) * 100)\n            ax.text(xmin, ymin, f\"{label} : {score}%\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='g')\n        else:\n            score=''\n            ax.text(xmin, ymin, f\"{label}\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='r')\n    plt.show()\n    ","f75de084":"for imgs, annotations in data_loader:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        break\n\nmodel.eval()\npreds = model(imgs)\n\nprint (len(imgs))","ea7139c3":"idx = 0;\nwhile idx < len(imgs):\n    nms_prediction = apply_nms(preds[idx], iou_thresh=0.2)\n    print(f'Prediction {idx+1}')\n    plot_image(imgs[idx], nms_prediction)\n    print(f'Target {idx+1}')\n    plot_image(imgs[idx].to('cpu'), annotations[idx],False)\n    \n    idx = idx + 1","b57764b3":"**FaceMaskDataset** are used for:\n* Sorting images and checking images version in JPG\n* Assigning classes to each labels including the background\n* Reading images\n* Converting it to specific colors which are BGR to RGB then making it into float array\n* Normalization images resolution by dividing it with 255.0\n* Annotated the files and file path\n* Creating bounding box to show where the mask\/mouth is\n* ","69c141c6":"# Train Model","58717afe":"# Function to plot image","7330b4c3":"# Face Mask Detection using FasterRCNN and PyTorch\n### Class: LA01 - Computer Vision\n### Anggota:\n1. Michael Febrianto Lu\n2. Elizabeth Ann Soelistio\n3. Gabrielle Angelica Ivandi\n4. Alif Brazali\n","186ae53a":"## Import All Needed Libraries\nEach of the libraries are used with intentions. \nThe intentions are noted on the top and what their uses are.","7e9ee7bd":"### Code Reference:\n[FasterRCnn|PyTorch|MaskDetection by Konstanter](https:\/\/www.kaggle.com\/konstanter\/fasterrcnn-pytorch-maskdetection)","47e8f337":"## Make a dataset and dataloader\nDefining datasets and data loader. We'll be using it from input and not inserting into our folder, thus this is needed to be done since input only allows read.","738e3fdb":"# Model"}}