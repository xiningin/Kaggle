{"cell_type":{"b505f835":"code","01d4d953":"code","c01dd93c":"code","f6bb0e9f":"code","2f8281ea":"code","223af0dd":"code","f4abd66b":"code","a7569898":"code","d948e61a":"code","4d985822":"code","05d011b0":"code","0ce1d89f":"code","e1628a8a":"code","4de1cd1f":"code","ae0359af":"code","f4865d4d":"code","4fcf42e6":"code","3328bcfa":"code","1e2eab85":"code","2e781cc9":"code","848dec49":"markdown","7faaef4c":"markdown","262ee8a3":"markdown","b5c3a41a":"markdown","b498d46f":"markdown","bce16d4a":"markdown","76483151":"markdown","b9c7a178":"markdown","70430628":"markdown","f3e3bd14":"markdown","51ae4703":"markdown"},"source":{"b505f835":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore  warning (from sklearn)\n\n#print(os.listdir(\"..\/input\"))","01d4d953":"house_data = pd.read_csv('..\/input\/train.csv')\nhouse_data_test = pd.read_csv('..\/input\/test.csv')","c01dd93c":"train_parent=house_data\ntest_parent=house_data_test \nhouse_data = house_data.drop('Id', axis=1)\nhouse_data_test = house_data_test.drop('Id', axis=1)","f6bb0e9f":"#We will find all the columns which have more than 40 % NaN data and drop then\nthreshold=0.4 * len(house_data)\ndf=pd.DataFrame(len(house_data) - house_data.count(),columns=['count'])\ndf.index[df['count'] > threshold]","2f8281ea":"house_data = house_data.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\nhouse_data_test = house_data_test.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)","223af0dd":"house_data['SalePrice'].describe()","f4abd66b":"house_data.select_dtypes(include=np.number).columns #will give all numeric columns ,we will remove the SalePrice column \nfor col in ('MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold'):\n    \n    house_data[col] = house_data[col].fillna(0)\n    house_data_test[col] = house_data_test[col].fillna('0')","a7569898":"house_data.select_dtypes(exclude=np.number).columns\nfor col in ('MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'SaleType', 'SaleCondition'):\n    \n    house_data[col] = house_data[col].fillna('None')\n    house_data_test[col] = house_data_test[col].fillna('None')","d948e61a":"house_data[house_data.isnull().any(axis=1)]","4d985822":"house_data_test[house_data_test.isnull().any(axis=1)]","05d011b0":"train=house_data\ntest=house_data_test\n\n#Assigning a flag to training and testing dataset for segregation after OHE .\ntrain['train']=1 \ntest['train']=0\n\n#Combining training and testing dataset\n\ncombined=pd.concat([train,test])","0ce1d89f":"#Applying One Hot Encoding to categorical data\nohe_data_frame=pd.get_dummies(combined, \n                           columns=['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'SaleType', 'SaleCondition'],\n      )","e1628a8a":"#Splitting the combined dataset after doing OHE .\ntrain_df=ohe_data_frame[ohe_data_frame['train']==1]\ntest_df=ohe_data_frame[ohe_data_frame['train']==0]\ntrain_df.drop(['train'],axis=1,inplace=True)             #Drop the Flag(train) coloumn from training dataset\ntest_df.drop(['train','SalePrice'],axis=1,inplace=True)     #Drop the Flag(train),Label(SalePrice) coloumn from test dataset","4de1cd1f":"house_data=train_df\nhouse_data_test=test_df","ae0359af":"X_train = house_data.drop('SalePrice', axis=1)\n# Taking the labels (price)\nY_train = house_data['SalePrice']\nX_test = house_data_test","f4865d4d":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\nnum_estimators = [500,1000,3000]\nlearn_rates = [0.01, 0.02, 0.05, 0.1]\nmax_depths = [1, 2, 3, 4]\nmin_samples_leaf = [5,10,15]\nmin_samples_split = [2,5,10]\n\nparam_grid = {'n_estimators': num_estimators,\n              'learning_rate': learn_rates,\n              'max_depth': max_depths,\n              'min_samples_leaf': min_samples_leaf,\n              'min_samples_split': min_samples_split}\n\ngrid_search = GridSearchCV(GradientBoostingRegressor(loss='huber'),\n                           param_grid, cv=3, return_train_score=True)\ngrid_search.fit(X_train, Y_train)\n\ngrid_search.best_params_  \n\"\"\"","4fcf42e6":"#GardientBoosting\nparams = {'n_estimators': 3000, 'max_depth': 1, 'min_samples_leaf':15, 'min_samples_split':10, \n          'learning_rate': 0.05, 'loss': 'huber','max_features':'sqrt'}\ngbr_model = GradientBoostingRegressor(**params)\ngbr_model.fit(X_train, Y_train)","3328bcfa":"gbr_model.score(X_train, Y_train)","1e2eab85":"#Predicting the SalePrice for the test data\ny_grad_predict = gbr_model.predict(X_test)\nprint(y_grad_predict)","2e781cc9":"#Submission \nmy_submission = pd.DataFrame({'Id': test_parent.Id, 'SalePrice': y_grad_predict})\nprint(my_submission)\n\nmy_submission.to_csv('submission.csv', encoding='utf-8', index=False)","848dec49":"Now we will analyze the NaN values present in the dataset and deal with them .","7faaef4c":"Will verify that the Label is a numeric data","262ee8a3":"**Data Cleaning is now complete We can now use our data to build our models**","b5c3a41a":"Find all the numeric columns and replace the NaN values with 0 ,\nand for categorical columns ,replace NaN values with 'None'.","b498d46f":"If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated .\n\nPlease drop down suggestions and comments if any, so that i can learn to build better solutions.\n\n**Thank You** :-)","bce16d4a":"Will store the Id column(information) from test dataframe ,in test_parent dataframe.\n\nThen let's drop the Id column from both test and train data. ","76483151":"Let's Load the Data","b9c7a178":"**Data Cleaning**","70430628":"Let's apply Gradient Boosting for regression and find the best parameter for GBR using GridSearchCV    ","f3e3bd14":" **Combining the two datasets and then doing One Hot Encoding on the combined dataset.**","51ae4703":"Verify that there are no null values in the data set"}}