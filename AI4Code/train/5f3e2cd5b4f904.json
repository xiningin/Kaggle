{"cell_type":{"b17bff97":"code","d4516b48":"code","03556c32":"code","f03bee43":"code","05513041":"code","b9b8c08b":"code","48bf9e3e":"code","8e52e112":"code","1e10f460":"code","ae015447":"code","788c3c2d":"code","c31baed2":"code","a21c6196":"code","2a4bd8d1":"code","33148a6c":"code","8f88df95":"code","14a9770a":"code","715e9986":"code","77ea88c2":"code","b4c602e0":"code","11e7478c":"code","01f06966":"code","fb04b715":"code","7f6fa277":"markdown","2fa8c5b1":"markdown","c7de4c63":"markdown","04bb24bb":"markdown","45e645b8":"markdown","6cd86da3":"markdown","56af8c2a":"markdown","bf4b0496":"markdown","d78a35ea":"markdown","99d945c1":"markdown","3eb5fc23":"markdown","e7573c78":"markdown","a4e97571":"markdown","87438a74":"markdown","ae1dfc0a":"markdown","7fcad47a":"markdown","6bb14ac7":"markdown","f1665c3d":"markdown","063cffae":"markdown","8852c1b5":"markdown","2652a93a":"markdown","8a8c90a2":"markdown","162194e1":"markdown","2d2e0581":"markdown","c4811c5e":"markdown"},"source":{"b17bff97":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport random as rn\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom keras.layers import Conv2D, MaxPooling2D,GlobalAvgPool2D\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten,Activation\nfrom keras.optimizers import Adam\nimport cv2\nfrom keras.preprocessing.image import ImageDataGenerator\n","d4516b48":"labels = os.listdir('..\/input\/flower\/flower_images\/training')\nprint(labels)","03556c32":"num = []\nfor label in labels:\n    path = '..\/input\/flower\/flower_images\/training\/{0}\/'.format(label)\n    folder_data = os.listdir(path)\n    k = 0\n    print('\\n', label.upper())\n    for image_path in folder_data:\n        k = k+1\n    num.append(k)\n    print('there are ', k,' images in ', label, 'class')","f03bee43":"plt.figure(figsize = (8,8))\nplt.bar(labels, num)\nplt.title('NUMBER OF IMAGES CONTAINED IN EACH CLASS')\nplt.xlabel('classes')\nplt.ylabel('count')\nplt.show()","05513041":"x_data =[]\ny_data = []\nimport cv2\nfor label in labels:\n    path = '..\/input\/flower\/flower_images\/training\/{0}\/'.format(label)\n    folder_data = os.listdir(path)\n    for image_path in folder_data:\n        image = cv2.imread(path+image_path,cv2.IMREAD_COLOR)\n        image_resized = cv2.resize(image, (150,150))\n        x_data.append(np.array(image_resized))\n        y_data.append(label)","b9b8c08b":"fig,ax=plt.subplots(5,2)\nfig.set_size_inches(15,15)\nfor i in range(5):\n    for j in range (2):\n        l=rn.randint(0,len(y_data))\n        ax[i,j].imshow(x_data[l])\n        ax[i,j].set_title('Flower: '+y_data[l])\n        \nplt.tight_layout()","48bf9e3e":"x_data = np.array(x_data)\n\ny_data = np.array(y_data)\n\nprint('the shape of X is: ', x_data.shape, 'and that of Y is: ', y_data.shape)\n","8e52e112":"le=LabelEncoder()\nY=le.fit_transform(y_data)\nY=to_categorical(Y,3)\nx_data = x_data\/255   #standarization ","1e10f460":"Y.shape","ae015447":"model = Sequential()\n\nmodel.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n\nmodel.add(Conv2D(filters =64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dense(3, activation = \"softmax\"))","788c3c2d":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['acc'])","c31baed2":"model.summary()","a21c6196":"test_label = os.listdir('..\/input\/flower\/flower_images\/validation')\nprint(test_label)","2a4bd8d1":"x_test =[]\ny_test = []\n\nfor label in test_label:\n    path = '..\/input\/flower\/flower_images\/validation\/{0}\/'.format(label)\n    folder_data = os.listdir(path)\n    for image_path in folder_data:\n        image = cv2.imread(path+image_path,cv2.IMREAD_COLOR)\n        image_resized = cv2.resize(image, (150,150))\n        x_test.append(np.array(image_resized))\n        y_test.append(label)","33148a6c":"x_test = np.array(x_test)\nx_test = x_test\/255\ny_test = np.array(y_test)\nle = LabelEncoder()\ny = le.fit_transform(y_test)\ny = to_categorical(y,3)\n","8f88df95":"x_test.shape,y.shape","14a9770a":"datagen = ImageDataGenerator(\n        rotation_range=10,\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\ndatagen.fit(x_data)\n\nval_datagen = ImageDataGenerator(\n        rotation_range=90,\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n         width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\nval_datagen.fit(x_test)\n\n","715e9986":"from keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n","77ea88c2":"History = model.fit(x_data,Y,epochs = 100, validation_data = (x_test,y),\n                    verbose = 1,callbacks=[early_stop])","b4c602e0":"History = model.fit_generator(datagen.flow(x_data,Y, batch_size=32),\n                              epochs = 100, validation_data = val_datagen.flow(x_test,y,batch_size=32),\n                              verbose = 1, steps_per_epoch=x_data.shape[0] \/\/ 32,\n                              callbacks=[early_stop])","11e7478c":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","01f06966":"plt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","fb04b715":"model.save('flowers.h5')","7f6fa277":"## Fitting model","2fa8c5b1":"### A basic machine learning workflow:\n\n1)Data walk through\n\n2)Build an input pipeline\n\n3)Build the model\n\n4)Train the model\n\n5)Test the model\n\n6)Improve the model and repeat the process ","c7de4c63":"This is a basic model,later for enhacement pooling layers,Dropout and adding strides for faster computing could be done.","04bb24bb":"# Flower - A simple dataset for flower image classification practice\nAbout this directory - \nThis is a simple yet an excellent dataset for beginner programmers to learn image classification.\nIt contains images of sunflower, lotus and rose split into training and validation set.\n\n","45e645b8":"# Creating Validation set","6cd86da3":"**Early Stopping** - Keras supports the early stopping of training via a callback called EarlyStopping. \nThis callback allows you to specify the performance measure to monitor,the trigger, and once triggered, it will \nstop the training process. For my model I've specified to stop after 7 iterations of no improvement in validation loss","56af8c2a":"## creating training set and labels","bf4b0496":"## plotting graph for classes","d78a35ea":"## Model summary\nView all the layers of the network using the model's summary method:","99d945c1":"## Data augmentation\nOverfitting generally occurs when there are a small number of training examples. Data augmentation takes the approach of generating additional training data from your existing examples by augmenting then using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n\nWe will implement data augmentation using experimental Keras Preprocessing Layers. These can be included inside your model like other layers.","3eb5fc23":"The image_batch is a tensor of the shape (272, 150, 150, 3). This is a batch of 272 images of shape 150x150x3 (the last dimension referes to color channels RGB). The label_batch is a tensor of the shape (272,), these are corresponding labels to the 272 images.\n\nYou can call .numpy() on the image_batch and labels_batch tensors to convert them to a numpy.ndarray.","e7573c78":"Same pre processing as training set\n","a4e97571":"## Create the model\nThe model consists of three convolution blocks . There's a fully connected layer with 128 units on top of it that is activated by a relu activation function. This model has not been tuned for high accuracy, the goal of this tutorial is to show a standard approach.\n![image.png](attachment:image.png)\nFor example, in the image below an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as \u201ccat\u201d.","87438a74":"## Importing important libraries","ae1dfc0a":"## Fitting model with augmentated data","7fcad47a":"## Walking through training directory ","6bb14ac7":"**Deep learning** is a vast field so we\u2019ll narrow our focus a bit and take up the challenge of solving an Image Classification project. Additionally, we\u2019ll be using a very simple deep learning architecture to achieve a pretty impressive accuracy score.\n\nYou can consider the Python code we\u2019ll see in this article as a benchmark for building **Image Classification** models. Once you get a good grasp on the concept, go ahead and play around with the code.\n\nWhat is Image Classification?\nConsider the below image:\n![image.png](attachment:image.png)\n\nYou will have instantly recognized it \u2013 it\u2019s a (swanky) car. Take a step back and analyze how you came to this conclusion \u2013 you were shown an image and you classified the class it belonged to (a car, in this instance). And that, in a nutshell, is what image classification is all about.\n\nThere are potentially n number of categories in which a given image can be classified. Manually checking and classifying images is a very tedious process. The task becomes near impossible when we\u2019re faced with a massive number of images, say 10,000 or even 100,000. How useful would it be if we could automate this entire process and quickly label images per their corresponding class?\n\nSelf-driving cars are a great example to understand where image classification is used in the real-world. To enable autonomous driving, we can build an image classification model that recognizes various objects, such as vehicles, people, moving objects, etc. on the road. We\u2019ll see a couple more use cases later in this article but there are plenty more applications around us. Use the comments section below the article to let me know what potential use cases you can come with up!\n\nA neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:\n\nconvolutional layers,pooling layers,dense layers\n\nConvolutional neural networks have had great success in certain kinds of problems, such as image recognition.","f1665c3d":"## Getting the different classes","063cffae":"### Visualize training results\nCreate plots of loss and accuracy on the training and validation sets.","8852c1b5":"### Saving model","2652a93a":"Handling categorical labels ","8a8c90a2":"## Visualize the data\n","162194e1":"Standardize the data\nThe RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small. Here, we will standardize values to be in the [0, 1] by using a Rescaling.","2d2e0581":"To complete our model, you will feed the last output tensor from the convolutional base into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. Flowers has 3 output classes, so you use a final Dense layer with 3 outputs and a softmax activation.","c4811c5e":"## Compile the model\nFor this tutorial, choose the optimizers.Adam optimizer and losses.CategoricalCrossentropy loss function. To view training and validation accuracy for each training epoch, pass the metrics argument.\n\n"}}