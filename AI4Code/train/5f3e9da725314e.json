{"cell_type":{"4d4e3e75":"code","b862ed1d":"code","21fd2745":"code","eb4a13c1":"code","b7e56030":"code","5cd3b9c6":"code","47edb8fa":"code","f4361612":"code","151e2af3":"code","b0bc84db":"code","e2f7bde2":"code","bb2ec62c":"code","60c711f8":"code","0ae92a10":"code","47d4940a":"code","4a6b9104":"code","9f1b0904":"code","8ed0ba1b":"markdown","fe83eff7":"markdown","22f5cafa":"markdown","9c476987":"markdown","0bc10e1e":"markdown","0ae6d857":"markdown"},"source":{"4d4e3e75":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Read in the files\ndf = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntraindf = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntestdf = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntraindf.head()","b862ed1d":"#combine all the both train and test data together\ntestdf[\"type\"]=\"test\"\ntraindf[\"type\"]=\"train\"\nalldf = pd.concat([testdf,traindf],axis=0)\n\n#Overview of all variables, their datatypes and mean\/variance\n#print(alldf.info(),\"\\n\")\n\n#Overview of all values\n#for i in range(alldf.shape[1]):\n   # print(alldf.iloc[:,i].describe(), '\\n')","21fd2745":"#Visualise all numeric variables\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf_num=alldf.select_dtypes(include=np.number)\n\n#Drop passenger_id as columns irrelevant to analysis\n#Drop pclass since it is a cartegorical variable\ndf_num=df_num.drop([\"PassengerId\",\"Pclass\"],axis=1)\nfig,axs= plt.subplots(2,2,figsize=(15,10))\n\ncol=0\nfor i in range(2):\n    for j in range(2):\n        sns.boxplot(x=df_num[\"Survived\"],y= df_num.iloc[:,col],ax=axs[i][j])\n        axs[i,j].set_title(df_num.columns[col])\n        col+=1","eb4a13c1":"#Visualize any pairwise relationship\n#Drop survived as it is a binary variable\ndf_num= df_num.drop([\"Survived\"],axis=1)\ng=sns.PairGrid(df_num)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)","b7e56030":"#Visualise all categorical variables\n#Drop Name,Ticket, Cabin, PassengerID due to too many levels\n#Include PClass as it is a categorical variable\nfig,axs= plt.subplots(1,5,figsize=(15,5))\n\ndf_cat= alldf.select_dtypes(exclude=np.number)\ndf_cat= df_cat.drop([\"Name\",\"Ticket\",\"Cabin\"],axis=1)\ndf_cat[\"Survived\"]= alldf[\"Survived\"]\ndf_cat[\"Pclass\"]=alldf[\"Pclass\"]\n\nname=0\nfor j in range(5):\n    col= df_cat.columns[name]\n    sns.countplot(x=col,hue=\"Survived\",data=df_cat,ax=axs[j])\n    name+=1","5cd3b9c6":"#List all Numeric variables\nnum_var= alldf.select_dtypes(include=np.number)\nprint('All numeric columns: ', num_var.columns,\"\\n\")\n\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nnum_var= pd.DataFrame(scaler.fit_transform(num_var),columns=num_var.columns)\n\n#List all Categorical variables\nalldf= alldf.drop([\"Name\", \"Ticket\",\"Cabin\",\"PassengerId\"],axis=1)\ncat_var=alldf.select_dtypes(exclude=np.number)\nprint('All non-numeric columns: ', cat_var.columns,\"\\n\")\n\nprint(\"Numerical representation of categorical variables:\")\n\n#convert categorical variables to numeric variables\nalldf1=alldf.copy()\nfor col in alldf.select_dtypes(exclude=np.number):\n    alldf1.loc[:,col]= alldf.loc[:,col].astype(\"category\").cat.codes\n    print(pd.concat([alldf1.loc[:,col],alldf.loc[:,col]],axis=1).drop_duplicates())\n    alldf.loc[:,col]=alldf1.loc[:,col].astype(\"category\").cat.codes","47edb8fa":"#NA values in traindf\nprint(\"Total number of rows in train data: \", len(traindf))\nna_train= traindf.columns[traindf.isna().any()]\nprint(\"NA values in train dataset:\")\nfor col in na_train:\n    print(col,sum(traindf.loc[:,col].isna()),\",\",round(sum(traindf.loc[:,col].isna())\/len(traindf)*100,2),\"% of data\")\n\nprint(\"\\n\")\nprint(\"Total number of rows in test data: \", len(testdf))\nprint(\"NA values in test dataset \")\nna_test= testdf.columns[testdf.isna().any()]\nfor col in na_test:\n    print(col,sum(testdf.loc[:,col].isna()),\",\", round(sum(testdf.loc[:,col].isna())\/len(testdf)*100,2),\"% of data\")","f4361612":"#Dealing with NA values:\n\n#For train dataset, remove those that have NA values in \"Age\" and \"Embarked\" column\n#For test dataset, inpute mean values for NA values in \"Age\" and \"Fare\" column\n#For both train and test dataset, remove the column \"Cabin\" due to too many missing values\n\ntrain= alldf[alldf[\"type\"]==1].drop(\"type\",axis=1)\ntest= alldf[alldf[\"type\"]==0].drop([\"Survived\",\"type\"],axis=1)\ntrain= train.dropna(subset=[\"Age\",\"Embarked\"])\ntest=test.fillna(test.mean())","151e2af3":"from sklearn.model_selection import train_test_split\n#Split train data into test and test dataset\nyvar=train.pop(\"Survived\")\nx_train, x_test, y_train, y_test = train_test_split(train,yvar, test_size=0.25)","b0bc84db":"#Define functions\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error, confusion_matrix, accuracy_score, r2_score\n\ndef evaluate(y_train,y_pred_train,y_test,y_pred_test):\n    #evaluation\n    print(\"MAE train:\", round(mean_absolute_error(y_train, y_pred_train),4))\n    print(\"RMSE train :\" ,round(mean_squared_error(y_train,y_pred_train,squared=False),4))\n    print(\"R2 train:\", round(r2_score(y_train,y_pred_train),4),\"\\n\")\n    \n    print(\"MAE test:\", round(mean_absolute_error(y_test,y_pred_test),4))\n    print(\"RMSE test :\" ,round(mean_squared_error(y_test,y_pred_test,squared=False),4))\n    print(\"R2 test:\", round(r2_score(y_test,y_pred_test),4),\"\\n\")\n    \n    #Confusion Matrix on train\n    print(\"Confusion Matrix train data\",\"\\n\",confusion_matrix(y_train,y_pred_train))\n    print(\"Accuracy on train data:\",round(accuracy_score(y_train,y_pred_train)*100,2),\"%\",\"\\n\")\n\n    #Confusion Matrix on test\n    print(\"Confusion Matrix test data\",\"\\n\",confusion_matrix(y_test,y_pred_test))\n    print(\"Accuracy on test data:\",round(accuracy_score(y_test,y_pred_test)*100,2),\"%\",\"\\n\")\n    return round(accuracy_score(y_test,y_pred_test)*100,2)\n\ndef plot_feature_im(model_im,x_train):\n    plt.plot(im)\n    locs,labels=plt.xticks()\n    plt.xticks(np.arange(len(x_train.columns)),x_train.columns)\n    plt.ylabel(\"Features Importance\")\n    plt.xlabel(\"Variables\")\n    plt.show()\n    \n    ","e2f7bde2":"#Logistic Regression\nfrom statsmodels.formula.api import ols\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogmodel= LogisticRegression(max_iter=1000, random_state=0)\nlogmodel.fit(x_train,y_train)\n\n#Prediction\ny_pred_train= logmodel.predict(x_train)\ny_pred_test= logmodel.predict(x_test)\n\n#Evaluation\ncompare_models=pd.DataFrame(columns=[\"model\",\"Accuracy\"])\nresult=evaluate(y_train,y_pred_train,y_test,y_pred_test)\ncompare_models=compare_models.append({'model': 'Logisic','Accuracy': result},ignore_index=True)","bb2ec62c":"from sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\n\n#Classification Tree\n#Parameters Tuning\nclf=tree.DecisionTreeClassifier(random_state=0)\nparams_clf={\n    \"max_depth\": list(range(1,5)),\n    \"min_samples_leaf\": list(range(10,50,5))\n}\ngridclf=GridSearchCV(estimator=clf,param_grid=params_clf,cv=5,n_jobs=-1,verbose=3)\n\n#Fit the model\ngridclf.fit(x_train,y_train)\n\n#Get the best model\nclf_best_model=gridclf.best_estimator_\n\n#Best Parameters\nprint(\"Best parameters:\", gridclf.best_params_)\n\n#Predictions\ny_pred_train = clf_best_model.predict(x_train)\ny_pred_test = clf_best_model.predict(x_test)\n\n#Evaluation\nresult= evaluate(y_train,y_pred_train,y_test,y_pred_test)\ncompare_models=compare_models.append({'model': 'DecisionTree','Accuracy': result},ignore_index=True)\n\n#Plot features importance\nim= clf_best_model.feature_importances_\nplot_feature_im(im,x_train)\n\n#Plotting of Tree\nplt.figure(figsize=(35,15))\nplot=tree.plot_tree(clf_best_model,feature_names=x_train.columns,class_names=[\"Died\",\"Survived\"],fontsize=20)\n","60c711f8":"from sklearn.ensemble import RandomForestClassifier\n\nrfmodel= RandomForestClassifier( random_state=0)\n\n#Tune parameters\nparams={\n    \"max_depth\":list(range(1,10)),\n    \"min_samples_leaf\": list(range(10,50,10))\n}\nrfmodel=GridSearchCV(estimator=rfmodel,param_grid=params,cv=5,n_jobs=-1,verbose=3)\n\n#fit the model\nrfmodel.fit(x_train,y_train)\n\n#Get the best model\nrfbest_model=rfmodel.best_estimator_\nprint(\"Best parameters:\",rfmodel.best_params_)\n\n#Predictions\ny_pred_train= rfbest_model.predict(x_train)\ny_pred_test= rfbest_model.predict(x_test)\n\n#Evaluations\nresult=evaluate(y_train,y_pred_train,y_test,y_pred_test)\ncompare_models=compare_models.append({'model': 'RandomForest','Accuracy': result},ignore_index=True)\n\n#Plot features importance\nim = rfbest_model.feature_importances_\nplot_feature_im(im,x_train)","0ae92a10":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbmodel = GradientBoostingClassifier()\n\nparams={\n    \"learning_rate\": list(np.arange(0,1,0.1)),\n    \"max_depth\": list(range(1,5)),\n    \"min_samples_leaf\": list(range(10,50,5))\n}\ngbmodel=GridSearchCV(estimator=gbmodel,param_grid=params,cv=5,n_jobs=-1,verbose=3)\n\ngbmodel.fit(x_train,y_train)\n\n#Get the best model\ngbbest_model=gbmodel.best_estimator_\nprint(\"Best parameters:\",gbmodel.best_params_)\n\ngbbest_model.fit(x_train,y_train)\n\n#Predictions\ny_pred_train= gbbest_model.predict(x_train)\ny_pred_test= gbbest_model.predict(x_test)\n\n#Evaluations\nresult=evaluate(y_train,y_pred_train,y_test,y_pred_test)\ncompare_models=compare_models.append({'model': 'GradientBoost','Accuracy': result},ignore_index=True)\n\n#Plot features importance\nim = gbbest_model.feature_importances_\nplot_feature_im(im,x_train)\n","47d4940a":"print(compare_models)","4a6b9104":"#Output\n\n#Best Model\nprint(\"Best Model:\", compare_models.loc[compare_models[\"Accuracy\"].idxmax(),\"model\"], \"with accuracy of\",max(compare_models[\"Accuracy\"]),'%')\n\n#Predict test data\npred=pd.DataFrame(gbbest_model.predict(test),columns=[\"Survived\"]).astype(\"int64\")\n\n#Output\noutputdf=pd.concat([testdf[\"PassengerId\"],pred],axis=1)\n\n#Overview of output\nprint(pred.value_counts())\noutputdf.head()","9f1b0904":"outputdf.to_csv(\"submission.csv\",index=False)","8ed0ba1b":"# Data Cleaning and Exploration","fe83eff7":"# Logistic Regression","22f5cafa":"# Decision Tree","9c476987":"# Machine Learning\n* I will be using logistic regression, decision tree , random forest and gradient boosting to predict. ","0bc10e1e":"# Gradient Boost","0ae6d857":"# Random Forest"}}