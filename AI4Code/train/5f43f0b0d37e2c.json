{"cell_type":{"734391cd":"code","7bf2a347":"code","58434a02":"code","a3323e44":"code","05ee118d":"code","c0e5ed19":"code","f443c8c4":"code","817cb956":"code","2bf04b1a":"code","2025e02a":"code","ce5ef2a9":"code","68f1a492":"code","693f1c7b":"code","70e509d8":"code","91e6fa3a":"code","dda3c564":"code","207d84d7":"code","ba421bc9":"code","9e59a5b5":"code","e8a1d875":"markdown","4d809a82":"markdown","86ed5c9a":"markdown","68c280ee":"markdown","d6490194":"markdown"},"source":{"734391cd":"#import packages\nimport numpy as np  # linear algebra\nimport pandas as pd # Data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport seaborn as sns\nfrom PIL import Image\n#from imutils import paths\nimport random\nimport pickle\nimport cv2\nimport datetime\nfrom pprint import pprint\nimport librosa\n\n\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.metrics import *\n\nimport keras\nfrom keras.utils import np_utils\nfrom keras.regularizers import *\nfrom keras.initializers import glorot_uniform\n\nimport keras.backend as K\nK.clear_session()\n\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import *\nfrom keras.callbacks import *","7bf2a347":"benign=pd.read_csv('..\/input\/nbaiot-dataset\/5.benign.csv')\ng_c=pd.read_csv('..\/input\/nbaiot-dataset\/5.gafgyt.combo.csv')\ng_j=pd.read_csv('..\/input\/nbaiot-dataset\/5.gafgyt.junk.csv')\ng_s=pd.read_csv('..\/input\/nbaiot-dataset\/5.gafgyt.scan.csv')\ng_t=pd.read_csv('..\/input\/nbaiot-dataset\/5.gafgyt.tcp.csv')\ng_u=pd.read_csv('..\/input\/nbaiot-dataset\/5.gafgyt.udp.csv')\nm_a=pd.read_csv('..\/input\/nbaiot-dataset\/5.mirai.ack.csv')\nm_sc=pd.read_csv('..\/input\/nbaiot-dataset\/5.mirai.scan.csv')\nm_sy=pd.read_csv('..\/input\/nbaiot-dataset\/5.mirai.syn.csv')\nm_u=pd.read_csv('..\/input\/nbaiot-dataset\/5.mirai.udp.csv')\nm_u_p=pd.read_csv('..\/input\/nbaiot-dataset\/5.mirai.udpplain.csv')\n\n'''\nbenign=benign.sample(frac=0.25,replace=False)\ng_c=g_c.sample(frac=0.25,replace=False)\ng_j=g_j.sample(frac=0.5,replace=False)\ng_s=g_s.sample(frac=0.5,replace=False)\ng_t=g_t.sample(frac=0.15,replace=False)\ng_u=g_u.sample(frac=0.15,replace=False)\nm_a=m_a.sample(frac=0.25,replace=False)\nm_sc=m_sc.sample(frac=0.15,replace=False)\nm_sy=m_sy.sample(frac=0.25,replace=False)\nm_u=m_u.sample(frac=0.1,replace=False)\nm_u_p=m_u_p.sample(frac=0.27,replace=False)\n'''\n\nbenign['type']='benign'\nm_u['type']='mirai_udp'\ng_c['type']='gafgyt_combo'\ng_j['type']='gafgyt_junk'\ng_s['type']='gafgyt_scan'\ng_t['type']='gafgyt_tcp'\ng_u['type']='gafgyt_udp'\nm_a['type']='mirai_ack'\nm_sc['type']='mirai_scan'\nm_sy['type']='mirai_syn'\nm_u_p['type']='mirai_udpplain'\n\ndata=pd.concat([benign,m_u,g_c,g_j,g_s,g_t,g_u,m_a,m_sc,m_sy,m_u_p],\n               axis=0, sort=False, ignore_index=True)","58434a02":"#how many instances of each class\ndata.groupby('type')['type'].count()","a3323e44":"#shuffle rows of dataframe \nsampler=np.random.permutation(len(data))\ndata=data.take(sampler)\ndata","05ee118d":"#dummy encode labels, store separately\nlabels_full=pd.get_dummies(data['type'], prefix='type')\nlabels_full.head()","c0e5ed19":"#drop labels from training dataset\ndata=data.drop(columns='type')\ndata.head()","f443c8c4":"#standardize numerical columns\ndef standardize(df,col):\n    df[col]= (df[col]-df[col].mean())\/df[col].std()\n\ndata_st=data.copy()\nfor i in (data_st.iloc[:,:-1].columns):\n    standardize (data_st,i)\n\ndata_st.head()","817cb956":"#training data for the neural net\ntrain_data_st=data_st.values\ntrain_data_st","2bf04b1a":"#labels for training\nlabels=labels_full.values\nlabels","2025e02a":"#Validation Technique\nx_train, x_test, y_train, y_test = train_test_split(train_data_st, labels, test_size=0.2)\nx_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.125)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(x_validate.shape)","ce5ef2a9":"x_train_cnn = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\nx_test_cnn = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\nx_validate_cnn = np.reshape(x_validate, (x_validate.shape[0], x_validate.shape[1],1))\nprint(x_train_cnn.shape)\nprint(x_test_cnn.shape)\nprint(x_validate_cnn.shape)","68f1a492":"#  create and fit model\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=train_data_st.shape[1], activation='relu'))\nmodel.add(Dense(40, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))\nmodel.add(Dense(labels.shape[1],activation='softmax'))\n\nmodelName = 'ANN'\nkeras.utils.plot_model(model, '.\/'+modelName+'_Archi.png',show_shapes=True)\nmodel.summary()","693f1c7b":"# Build Model CNN_LSTM\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=5, strides=1, padding='same', input_shape = (train_data_st.shape[1], 1)))\nmodel.add(Conv1D(filters=32, kernel_size=5, strides=1, padding='same'))\nmodel.add(LSTM(32, activation = 'relu', return_sequences=True))\nmodel.add(LSTM(16, return_sequences=True))  # returns a sequence of vectors of dimension 16\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Dense(labels.shape[1],activation='softmax'))\n\nmodelName = 'CNN+LSTM'\nkeras.utils.plot_model(model, '.\/'+modelName+'_Archi.png',show_shapes=True)\nmodel.summary()","70e509d8":"# Build Model\ninp = Input(shape=(train_data_st.shape[1], 1))\nC = Conv1D(filters=32, kernel_size=5, strides=1)(inp)\n\nC11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)\nA11 = Activation(\"relu\")(C11)\nC12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)\nS11 = Add()([C12, C])\nA12 = Activation(\"relu\")(S11)\nM11 = MaxPooling1D(pool_size=5, strides=2)(A12)\n\n\nC21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)\nA21 = Activation(\"relu\")(C21)\nC22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)\nS21 = Add()([C22, M11])\nA22 = Activation(\"relu\")(S11)\nM21 = MaxPooling1D(pool_size=5, strides=2)(A22)\n\n\nC31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)\nA31 = Activation(\"relu\")(C31)\nC32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)\nS31 = Add()([C32, M21])\nA32 = Activation(\"relu\")(S31)\nM31 = MaxPooling1D(pool_size=5, strides=2)(A32)\n\n\nC41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)\nA41 = Activation(\"relu\")(C41)\nC42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)\nS41 = Add()([C42, M31])\nA42 = Activation(\"relu\")(S41)\nM41 = MaxPooling1D(pool_size=5, strides=2)(A42)\n\n\nC51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)\nA51 = Activation(\"relu\")(C51)\nC52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)\nS51 = Add()([C52, M41])\nA52 = Activation(\"relu\")(S51)\nM51 = MaxPooling1D(pool_size=5, strides=2)(A52)\n\nF1 = Flatten()(M51)\n\nD1 = Dense(32)(F1)\nA6 = Activation(\"relu\")(D1)\nD2 = Dense(32)(A6)\nD3 = Dense(labels.shape[1])(D2)\nA7 = Activation(\"softmax\")(D3)\n\nmodel = Model(inputs=inp, outputs=A7)\n\n\nkeras.utils.plot_model(model, '.\/Deep_residual_CNN_model.png', show_shapes=True)\n\nmodelName='Deep residual CNN'\n\nmodel.summary()","91e6fa3a":"'''\nFor training the networks, we used Adam optimization method with the learning rate, beta-1, \nand beta-2 of 0.001, 0.9, and 0.999, respectively. \nLearning rate is decayed exponentially with the decay factor of 0.75 every 10000 iterations.\n'''\nadam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) \n\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            lr=0.00001)\nearlystop = EarlyStopping(monitor = 'val_loss',\n                          min_delta = 0,\n                          patience = 10,\n                          verbose = 1,\n                          restore_best_weights = True)\n\ncheckpoint = ModelCheckpoint('.\/'+modelName+'.h5',\n                            monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             save_weights_only=True,\n                             verbose=1)","dda3c564":"epochs = 100\nbatch_size = 512\nhistory = model.fit(x_train_cnn,y_train, batch_size=batch_size,\n    steps_per_epoch=x_train.shape[0] \/\/ batch_size,\n    epochs=epochs,\n    validation_data=(x_validate_cnn,y_validate),\n    #validation_split=0.10,\n    callbacks=[learning_rate_reduction, checkpoint] \n)\n\n#1. Function to plot model's validation loss and validation accuracy\ndef plot_model_history(model_history):\n    fig, axs = plt.subplots(1,2,figsize=(15,5))\n    # summarize history for accuracy\n    axs[0].plot(range(1,len(model_history.history['accuracy'])+1), model_history.history['accuracy'], '--*', color = (1,0,0))\n    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1), model_history.history['val_accuracy'], '-^', color = (0.7,0,0.7))\n    axs[0].set_title('Model '+modelName+' Accuracy')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].set_xlabel('Epoch')\n    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])\/10)\n    axs[0].legend(['train', 'val'], loc='best')\n    axs[0].grid('on')\n    # summarize history for loss\n    axs[1].plot(range(1,len(model_history.history['loss'])+1), model_history.history['loss'],'-x', color = (0,0.5,0))\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1), model_history.history['val_loss'],  '-.D', color = (0,0,0.5))\n    axs[1].set_title('Model '+modelName+' Loss')\n    axs[1].set_ylabel('Loss')\n    axs[1].set_xlabel('Epoch')\n    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])\/10)\n    axs[1].legend(['train', 'val'], loc='best')\n    axs[1].grid('on')\n    plt.savefig('.\/'+modelName+'.jpg',dpi=600, quality = 100, optimize = True)\n    plt.show()\n\nplot_model_history(history)\nwith open('.\/History_'+modelName, 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)\n","207d84d7":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support,  roc_curve, auc\nimport tensorflow as tf \n#model = tf.keras.models.load_model('\/'+model_name+'.h5')\n\ny_pred = model.predict(x_test_cnn)\n\ny_pred_cm  = np.argmax(y_pred, axis=1)\ny_test_cm  = np.argmax(y_test, axis=1)\n\ncm = confusion_matrix(y_test_cm, y_pred_cm) \n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()\/np.sum(cm)]\n\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts,group_percentages)]\n\nlabels = np.asarray(labels).reshape(11,11)\n\nlabel = ['benign','mirai_udp','gafgyt_combo','gafgyt_junk','gafgyt_scan','gafgyt_tcp','gafgyt_udp'\\\n        ,'mirai_ack','mirai_scan','mirai_syn','mirai_udpplain']\n\nplt.figure(figsize=(11,11))\nsns.heatmap(cm, xticklabels=label, yticklabels=label, annot=labels, fmt='', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix for'+ modelName+' model')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nplt.savefig('.\/'+modelName+'_CM.png')\nplt.show()","ba421bc9":"print(classification_report(y_test_cm, y_pred_cm, target_names= ['benign','mirai_udp','gafgyt_combo','gafgyt_junk','gafgyt_scan','gafgyt_tcp','gafgyt_udp','mirai_ack','mirai_scan','mirai_syn','mirai_udpplain']))\n\nloss, accuracy = model.evaluate(x_test_cnn, y_test, verbose=1)\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n\nwith open('.\/'+modelName+'_CR.txt','a') as f:\n    f.write(classification_report(y_test_cm, y_pred_cm, target_names= ['benign','mirai_udp','gafgyt_combo','gafgyt_junk','gafgyt_scan','gafgyt_tcp','gafgyt_udp','mirai_ack','mirai_scan','mirai_syn','mirai_udpplain']))\n    f.write(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))","9e59a5b5":"from itertools import cycle\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(labels.shape[1]):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\ncolors = cycle(['blue', 'red', 'green','aqua', 'darkorange', 'orange','fuchsia', 'lime','magenta'])\nfor i, color in zip(range(labels.shape[1]), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out (1-Specificity)')\nplt.title('Receiver Operating Characteristic (ROC) for '+modelName+' model')\nplt.legend(loc=\"lower right\")\nplt.savefig('.\/'+modelName+'_ROC.png')\n\nplt.show()","e8a1d875":"Keras model (CNN+LSTM)","4d809a82":"# IoT Intrusion Detection\n\nThe N-BaIoT Dataset contains traffic data for 9 IoT devices. The data comprise of both benign traffic and of a variety of malicious attacks. Here we run three deep neural networks to identify cyberattacks on a Provision PT-737E Security Camera.","86ed5c9a":"Keras model (Deep residual 1DCNN model)","68c280ee":"Plot Receiver Operating Characteristic (ROC)","d6490194":"Keras model (ANN)"}}