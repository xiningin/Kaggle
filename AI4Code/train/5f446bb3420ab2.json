{"cell_type":{"8761b347":"code","053c3e67":"code","5e085839":"code","6b15f203":"code","e3a6e081":"code","7e38a608":"code","a0bae253":"code","c33e24cd":"code","9e31e86b":"code","b43fd05b":"code","9e655968":"code","19d180a4":"code","999ba8fa":"code","60bcc986":"code","3a72d976":"code","947c0156":"code","386cb688":"code","2801a7d5":"code","865c38f2":"code","50af43c4":"markdown","bb9e6045":"markdown","c8ada161":"markdown","0910355f":"markdown","c5d9684c":"markdown"},"source":{"8761b347":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","053c3e67":"#importing common libraries  \nimport os , random\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, \nimport matplotlib.pyplot as plt # ploting\nimport seaborn as sns # visualisation \nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport catboost as cat\nfrom catboost import CatBoostRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style(\"ticks\")\nimport optuna\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n","5e085839":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/train.csv\", low_memory=False)\ntrain[\"date_time\"] = pd.to_datetime(train[\"date_time\"])\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/test.csv\", low_memory=False)\ntest[\"date_time\"] = pd.to_datetime(test[\"date_time\"])","6b15f203":"import seaborn as sns\n\nsns.displot(x=\"deg_C\", data=train,bins=3)\n\n","e3a6e081":"def newfeatures(df):\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n\n    df[\"SMC\"] = (df[\"absolute_humidity\"] * 100) \/ df[\"relative_humidity\"]\n    df['Dew_Point'] = 243.12*(np.log(df['relative_humidity'] * 0.01) + (17.62 *     df['deg_C'])\/(243.12+df['deg_C']))\/(17.62-(np.log(df['relative_humidity'] * 0.01)+17.62*df['deg_C']\/(243.12+df['deg_C'])))\n    df['SMC'] = (df['absolute_humidity'] * 100) \/ df['relative_humidity']\n    df['temperature_lag_3'] = df['deg_C'] - df['deg_C'].shift(periods=3, fill_value=0)\n    df['temperature_lag_6'] = df['deg_C'] - df['deg_C'].shift(periods=6, fill_value=0)\n    df['Partial_pressure'] = 243.12*(np.log(df['absolute_humidity'] * 0.01) + (17.62 * df['deg_C'])\/(243.12+df['deg_C']))\/(17.62-(np.log(df['relative_humidity'] * 0.01)+17.62*df['deg_C']\/(243.12+df['deg_C'])))\n    df['date_time'] = df['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n    df['S5-S1']=df['sensor_5']-df['sensor_1']\n    df['S1-S4']=df['sensor_1']-df['sensor_4']\n    df['S4-S2']=df['sensor_4']-df['sensor_2']\n    df['S5-S2']=df['sensor_5']-df['sensor_2']\n    df['S1-S2']=df['sensor_1']-df['sensor_3']\n    conditions = [\n    (df['deg_C'] <= 12),\n    (df['deg_C'] > 12) & (df['deg_C'] <= 22),\n    (df['deg_C'] > 22)\n    ]\n\n#create a list of the values we want to assign for each condition\n    values = [0,1,2]\n\n#create a new column and use np.select to assign values to it using our lists as arguments\n    df['temp_tier'] = np.select(conditions, values)\n    return df\n","7e38a608":"train=newfeatures(train)\ntest=newfeatures(test)\ntest.head()","a0bae253":"import seaborn as sns\nax = sns.boxplot(y=\"temp_tier\", data=train)\n","c33e24cd":"sns.displot(train, x=\"target_nitrogen_oxides\", kind=\"kde\", cut=0)   \n\nax = sns.boxplot(y=\"target_benzene\", data=train)","9e31e86b":"train = train.drop([7110], axis = 0)\n\nfor i in ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']:\n    train[i] = np.log1p(train[i])\n\n\ny1 = train['target_carbon_monoxide']\ny2 = train['target_benzene']\ny3 = train['target_nitrogen_oxides']\n\nX = train.drop(['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'], axis = 1)\nX1 = train.drop(['target_benzene', 'target_nitrogen_oxides'], axis = 1)\nX2 = train.drop(['target_nitrogen_oxides'], axis = 1)\n\n","b43fd05b":"\ny3.head()","9e655968":"# Carbon monoxide\n# Mean RMSLE on 5 Folds with Optuna - 0.0105\nparamsCB1 = {'depth': 5, \n              'learning_rate': 0.1723956529272847, \n              'iterations': 8235,\n              'max_bin': 202,\n              'min_data_in_leaf': 13, \n              'l2_leaf_reg': 0.025988660512111194, \n              'bagging_temperature': 3.126276416033602, \n              'subsample': 0.7799317755054309,\n              'grow_policy': 'Depthwise',\n              'leaf_estimation_method': 'Newton',\n             'loss_function': 'RMSE',\n             'eval_metric': 'RMSE',\n             'cat_features': ['hour', 'working_hours', 'is_weekend','temp_tier']}\n\n# Benzene\n# Mean RMSLE on 5 Folds with Optuna - 0.003\nparamsCB2 = {'depth': 5,\n             'learning_rate': 0.16922898702324848,\n             'iterations': 9624,\n             'max_bin': 187,\n             'min_data_in_leaf': 119,\n             'l2_leaf_reg': 0.15692495225226663, \n             'bagging_temperature': 0.11489860045608591,\n             'subsample': 0.518476242344081,\n             'grow_policy': 'SymmetricTree',\n             'leaf_estimation_method': 'Newton',\n             'loss_function': 'RMSE',\n             'eval_metric': 'RMSE',\n             'cat_features': ['hour', 'working_hours', 'is_weekend','temp_tier']}\n\n# Nitrogen oxides\n# Mean RMSLE on 5 Folds with Optuna - 0.021\nparamsCB3 = {'depth': 5, \n              'learning_rate': 0.1336172575126029,\n              'iterations': 9870, \n              'max_bin': 85,\n              'min_data_in_leaf': 176, \n              'l2_leaf_reg': 0.47255556409463717,\n              'bagging_temperature': 6.290468771397568,\n              'subsample': 0.7337538866725786,\n              'grow_policy': 'SymmetricTree', \n              'leaf_estimation_method': 'Gradient',\n             'loss_function': 'RMSE',\n             'eval_metric': 'RMSE',\n             'cat_features': ['hour', 'working_hours', 'is_weekend','temp_tier']}\n","19d180a4":"ss=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","999ba8fa":"s1=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')","60bcc986":"ss.head()","3a72d976":"folds = KFold(n_splits = 4, random_state = 228, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y1)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y1.iloc[trn_idx], y1.iloc[val_idx]\n\n    model = CatBoostRegressor(**paramsCB1)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 30, use_best_model = True)\n    \n    predictions += model.predict(test) \/ folds.n_splits \n    \nss['target_carbon_monoxide'] = np.expm1(predictions)\n\n\n","947c0156":"folds = KFold(n_splits = 4, random_state = 228, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y2)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y2.iloc[trn_idx], y2.iloc[val_idx]\n\n    model = CatBoostRegressor(**paramsCB2)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 30, use_best_model = True)\n    \n    predictions += model.predict(test) \/ folds.n_splits \n    \nss['target_benzene'] = np.expm1(predictions)\n\n","386cb688":"folds = KFold(n_splits = 4, random_state = 228, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y3)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y3.iloc[trn_idx], y3.iloc[val_idx]\n\n    model = CatBoostRegressor(**paramsCB3)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 30, use_best_model = True)\n    \n    predictions += model.predict(test) \/ folds.n_splits \n    \nss['target_nitrogen_oxides'] = np.expm1(predictions)","2801a7d5":"ss.to_csv('submission.csv',index=False)","865c38f2":"ss.head()","50af43c4":"\ny3\n\n {'depth': 5, 'learning_rate': 0.1336172575126029, 'iterations': 9870, 'max_bin': 85, 'min_data_in_leaf': 176, 'l2_leaf_reg': 0.47255556409463717, 'bagging_temperature': 6.290468771397568, 'subsample': 0.7337538866725786, 'grow_policy': 'SymmetricTree', 'leaf_estimation_method': 'Gradient'}\nBest value: 0.004481325552645856\n\ny2","bb9e6045":"Number of finished trials: 20\nBest trial: {'depth': 5, 'learning_rate': 0.16922898702324848, 'iterations': 9624, 'max_bin': 187, 'min_data_in_leaf': 119, 'l2_leaf_reg': 0.15692495225226663, 'bagging_temperature': 0.11489860045608591, 'subsample': 0.518476242344081, 'grow_policy': 'SymmetricTree', 'leaf_estimation_method': 'Newton'}\nBest value: 0.0025068183601230816","c8ada161":"# Optuna parameters for each target (I change here only targets)\n\ndef objective(trial, data = X, target = y3):\n\n    params = {\n        'depth': trial.suggest_int('depth', 2, 6),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n        'iterations': trial.suggest_int('iterations', 500, 10000),\n        'max_bin': trial.suggest_int('max_bin', 1, 300),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.1, 10.0),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n        'leaf_estimation_method': trial.suggest_categorical('leaf_estimation_method', ['Newton', 'Gradient']),\n        'cat_features': ['hour', 'working_hours', 'is_weekend','temp_tier'],\n        'loss_function': 'RMSE',\n        'eval_metric': 'RMSE'\n    }\n    \n    model = CatBoostRegressor(**params)\n    scores = []\n    k = KFold(n_splits = 4, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X)):\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y3.iloc[trn_idx], y3.iloc[val_idx]\n\n        model.fit(X, y3, eval_set = [(X_val, y_val)], early_stopping_rounds = 30, verbose = False, use_best_model = True)\n        \n        tr_preds = np.expm1(model.predict(X_train))\n        tr_score = np.sqrt(mean_squared_log_error(np.expm1(y_train), tr_preds))\n        \n        val_preds = np.expm1(model.predict(X_val))\n        val_score = np.sqrt(mean_squared_log_error(np.expm1(y_val), val_preds))\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i} | RMSLE: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 20)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","0910355f":"Fold 3 | RMSLE: 0.004316732916015073\nNumber of finished trials: 20\nBest trial: {'depth': 5, 'learning_rate': 0.1723956529272847, 'iterations': 8235, 'max_bin': 202, 'min_data_in_leaf': 13, 'l2_leaf_reg': 0.025988660512111194, 'bagging_temperature': 3.126276416033602, 'subsample': 0.7799317755054309, 'grow_policy': 'Depthwise', 'leaf_estimation_method': 'Newton'}\nBest value: 0.004294154550650668","c5d9684c":"### X2.columns"}}