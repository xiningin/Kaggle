{"cell_type":{"0b93bdb6":"code","421b7710":"code","44edffd5":"code","88e67bcb":"code","1524d27c":"code","606a6fdf":"code","cab12618":"code","06cb3336":"code","96c53e28":"code","94aa7797":"code","d34e8516":"code","e6096313":"code","b590c30c":"code","68264876":"code","176613f4":"code","996609af":"code","5f27294e":"code","75f26154":"code","85bd55ad":"markdown","d4c6704d":"markdown","8e765862":"markdown","98e97712":"markdown","810981b3":"markdown"},"source":{"0b93bdb6":"!pip install transformers==2.3.0","421b7710":"from tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom transformers import create_optimizer\n","44edffd5":"class_names = ['Components', 'Delivery and Customer Support','Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n               'Installation', 'Material', 'Price', 'Quality', 'Usability', 'Polarity']","88e67bcb":"train = pd.read_csv('..\/input\/uhack-sentiments-20-decode-code-words\/Participants_Data_DCW\/train.csv')\ntest = pd.read_csv('..\/input\/uhack-sentiments-20-decode-code-words\/Participants_Data_DCW\/test.csv')\nsub = pd.read_csv('..\/input\/uhack-sentiments-20-decode-code-words\/Participants_Data_DCW\/submission.csv')","1524d27c":"train.head(2)","606a6fdf":"bert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 128","cab12618":"def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)","06cb3336":"input_ids = tokenize_sentences(train['Review'], tokenizer, MAX_LEN)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","96c53e28":"labels =  train[class_names].values\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n\ntrain_size = len(train_inputs)\nvalidation_size = len(validation_inputs)","94aa7797":"BATCH_SIZE = 32\nNR_EPOCHS = 1\n\ndef create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    if train:\n        dataset = dataset.shuffle(buffer_size=buffer_size)\n    dataset = dataset.repeat(epochs)\n    dataset = dataset.batch(batch_size)\n    if train:\n        dataset = dataset.prefetch(1)\n    \n    return dataset","d34e8516":"train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\nvalidation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)","e6096313":"from transformers import TFBertModel\nfrom tensorflow.keras.layers import Dense, Flatten\n\nclass BertClassifier(tf.keras.Model):    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        super().__init__()\n        self.bert = bert\n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    @tf.function\n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, \n                            position_ids=position_ids, head_mask=head_mask)\n        cls_output = outputs[1]\n        cls_output = self.classifier(cls_output)\n                \n        return cls_output","b590c30c":"model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(class_names))","68264876":"steps_per_epoch = train_size \/\/ BATCH_SIZE\nvalidation_steps = validation_size \/\/ BATCH_SIZE\n\n# | Loss Function\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nvalidation_loss = tf.keras.metrics.Mean(name='test_loss')\n\n# | Optimizer (with 1-cycle-policy)\nwarmup_steps = steps_per_epoch \/\/ 3\ntotal_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\noptimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n# | Metrics\ntrain_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(class_names))]\nvalidation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(class_names))]","176613f4":"@tf.function\ndef train_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    with tf.GradientTape() as tape:\n        predictions = model(token_ids, attention_mask=masks)\n        loss = loss_object(labels, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n\n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n        \n@tf.function\ndef validation_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    v_loss = loss_object(labels, predictions)\n\n    validation_loss(v_loss)\n    for i, auc in enumerate(validation_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n                                              \ndef train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n    for epoch in range(epochs):\n        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n\n        start = time.time()\n\n        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n            train_step(model, token_ids, masks, labels)\n            if i % 1000 == 0:\n                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n                for i, label_name in enumerate(class_names):\n                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n                    train_auc_metrics[i].reset_states()\n        \n        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n            validation_step(model, token_ids, masks, labels)\n\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n\n        for i, label_name in enumerate(class_names):\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n            validation_auc_metrics[i].reset_states()\n\n        print('\\n')\n\n        \ntrain(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)","996609af":"test_input_ids = tokenize_sentences(test['Review'], tokenizer, MAX_LEN)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","5f27294e":"TEST_BATCH_SIZE = 32\ntest_steps = len(test) \/\/ TEST_BATCH_SIZE\n\ntest_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n\n# df_submission = pd.read_csv(subm_path, index_col='id')\n\nfor i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n    sample_ids = test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['Id']\n    predictions = model(token_ids, attention_mask=masks).numpy()\n\n    sub.loc[sample_ids, class_names] = predictions","75f26154":"sub.to_csv(\"bert.csv\", index=False)","85bd55ad":"**Run predictions on test-set & save submission**","d4c6704d":"**BERT Model**\n\n- Load the pretrained BERT base-model from Transformers library\n- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 12 neurons and sigmoid activation (Classifier). The outputs of this layer can be interpreted as probabilities for each of the 12 classes.","8e765862":"## Import Libraries","98e97712":"Thank you","810981b3":"**Training Loop**\n\n- Use BinaryCrossentropy as loss function (is calculated for each of the output 6 output neurons ...that's like training 6 binary classification tasks at the same time)\n- Use the AdamW optimizer with 1-cycle-policy from the Transformers library\n- AUC evaluation metrics"}}