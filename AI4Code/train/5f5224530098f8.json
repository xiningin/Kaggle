{"cell_type":{"9620bdb6":"code","7dd85314":"code","5aeb0509":"code","84d8d8f4":"code","e4b3c0ba":"code","ce9b0091":"code","a3243fb9":"markdown","0c6c9c05":"markdown","2b5c59b2":"markdown","f0a978d5":"markdown","8704329b":"markdown","90677503":"markdown","74f173ff":"markdown","e33a4f6d":"markdown","7c90432c":"markdown"},"source":{"9620bdb6":"import os\nimport tensorflow\nos.environ['KERAS_BACKEND'] = 'tensorflow'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 3 = INFO, WARNING, and ERROR messages are not printed\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator","7dd85314":"dataset = pd.read_csv('..\/input\/driver_imgs_list.csv')\ndataset.head(5)","5aeb0509":"import os\nfrom IPython.display import display, Image\nimport matplotlib.image as mpimg\n\nactivity_map = {'c0': 'Safe driving', \n                'c1': 'Texting - right', \n                'c2': 'Talking on the phone - right', \n                'c3': 'Texting - left', \n                'c4': 'Talking on the phone - left', \n                'c5': 'Operating the radio', \n                'c6': 'Drinking', \n                'c7': 'Reaching behind', \n                'c8': 'Hair and makeup', \n                'c9': 'Talking to passenger'}\n\nplt.figure(figsize = (12, 20))\nimage_count = 1\nBASE_URL = '..\/input\/train\/'\nfor directory in os.listdir(BASE_URL):\n    if directory[0] != '.':\n        for i, file in enumerate(os.listdir(BASE_URL + directory)):\n            if i == 1:\n                break\n            else:\n                fig = plt.subplot(5, 2, image_count)\n                image_count += 1\n                image = mpimg.imread(BASE_URL + directory + '\/' + file)\n                plt.imshow(image)\n                plt.title(activity_map[directory])","84d8d8f4":"classifier = Sequential()\nclassifier.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', input_shape = (240, 240, 3), data_format = 'channels_last'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\nclassifier.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\nclassifier.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\nclassifier.add(Flatten())\nclassifier.add(Dense(units = 1024, activation = 'relu'))\nclassifier.add(Dense(units = 256, activation = 'relu'))\nclassifier.add(Dense(units = 10, activation = 'sigmoid'))\nclassifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nclassifier.summary()","e4b3c0ba":"train_datagen = ImageDataGenerator(rescale = 1.0\/255, \n                                   shear_range = 0.2, \n                                   zoom_range = 0.2, \n                                   horizontal_flip = True, \n                                   validation_split = 0.2)\n\ntraining_set = train_datagen.flow_from_directory('..\/input\/train', \n                                                 target_size = (240, 240), \n                                                 batch_size = 32,\n                                                 subset = 'training')\n\nvalidation_set = train_datagen.flow_from_directory('..\/input\/train', \n                                                   target_size = (240, 240), \n                                                   batch_size = 32,\n                                                   subset = 'validation')","ce9b0091":"classifier.fit_generator(training_set,\n                         steps_per_epoch = 17943\/32,\n                         epochs = 10,\n                         validation_data = validation_set,\n                         validation_steps = 4481\/32)","a3243fb9":"# Creating training data\n\nOnce the model is ready, I'll use the data on which I want to train the model. The folder `train` includes the images I need. I'll generate more images using **ImageDataGenerator** and split the training data into 80% train and 20% validation split.","0c6c9c05":"# Import libraries\n\nI'll use Keras and Tensorflow libraries to create a **Convolutional Neural Network**. So, I'll import the necessary libraries to do the same.","2b5c59b2":"# Building the model\n\nI'll develop the model with a total of 3 Convolutional layers, then a Flatten layer and then 3 Dense layers. I'll use the optimizer as `adam`, and loss as `categorical_crossentropy`.","f0a978d5":"The trained model achieved a validation accuracy of over 97%.","8704329b":"# CNN to detect driver actions\n\nIn this notebook, I'll use the dataset which includes images of drivers while performing a number of tasks including drinking, texting etc. The aim is to correctly identify if the driver is distracted from driving. We might also like to check what activity the person is performing.","90677503":"# Images overview\n\nLet's take a look at the various images in the dataset. I'll plot an image for each of the 10 classes. As the directory names are not descriptive, I'll use a map to define the title for each image that is more descriptive.","74f173ff":"From the csv file, I'll use the `classname` as the labels for the images and use the image names to match the labels with the correct images.","e33a4f6d":"# Import the dataset\n\nI'll import the .csv file to read the labels.","7c90432c":"# Train the model\n\nUsing `fit_generator`, I'll train the model."}}