{"cell_type":{"c67715fb":"code","23076103":"code","e90e3c78":"code","fe7abf2e":"code","600782a9":"code","ba0c7ac6":"code","1b5e6085":"code","e65f04d6":"markdown"},"source":{"c67715fb":"# import packages\nimport os\nimport torch \nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n \nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nfrom tqdm.notebook import tqdm","23076103":"# leanring parameters\nepochs = 10\nbatch_size = 64\nlr = 0.0001\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# image transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])","e90e3c78":"train_data = datasets.FashionMNIST(\n    root='.\/data',\n    train=True, \n    download=True,\n    transform=transform\n)\nval_data = datasets.FashionMNIST(\n    root='.\/data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrain_loader = DataLoader(\n    train_data, \n    batch_size=batch_size,\n)\nval_loader = DataLoader(\n    val_data, \n    batch_size=batch_size, \n)","fe7abf2e":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n\n        # encoder\n        self.enc1 = nn.Linear(in_features=784, out_features=512)\n        self.enc2 = nn.Linear(in_features=512, out_features=32)\n\n        # decoder \n        self.dec1 = nn.Linear(in_features=32, out_features=512)\n        self.dec2 = nn.Linear(in_features=512, out_features=784)\n\n    def forward(self, x):\n        # encoding\n        x = F.relu(self.enc1(x))\n        x = F.relu(self.enc2(x))\n        \n        # decoding\n        x = F.relu(self.dec1(x))\n        x = torch.sigmoid(self.dec2(x))\n        return x\n\nmodel = Autoencoder().to(device)\nprint(model)","600782a9":"criterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","ba0c7ac6":"def fit(model, dataloader):\n    model.train()\n    running_loss = 0.0\n    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)\/dataloader.batch_size)):\n        data, _ = data\n        data = data.to(device)\n        data = data.view(data.size(0), -1)\n        optimizer.zero_grad()\n        reconstruction= model(data)\n        loss = criterion(reconstruction, data)\n        loss.backward()\n        running_loss += loss.item()\n        optimizer.step()\n\n    train_loss = running_loss\/len(dataloader.dataset)\n    return train_loss\n\n\ndef validate(model, dataloader):\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)\/dataloader.batch_size)):\n            data, _ = data\n            data = data.to(device)\n            data = data.view(data.size(0), -1)\n            reconstruction = model(data)\n            loss = criterion(reconstruction, data)\n            running_loss += loss.item()\n        \n            # save the last batch input and output of every epoch\n            if i == int(len(val_data)\/dataloader.batch_size) - 1:\n                num_rows = 8\n                both = torch.cat((data.view(batch_size, 1, 28, 28)[:8], \n                                  reconstruction.view(batch_size, 1, 28, 28)[:8]))\n                save_image(both.cpu(), f\"output{epoch}.png\", nrow=num_rows)\n                output = plt.imread(f\"output{epoch}.png\")\n                plt.imshow(output)\n                plt.show()\n\n    val_loss = running_loss\/len(dataloader.dataset)\n    return val_loss","1b5e6085":"train_loss = []\nval_loss = []\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss = fit(model, train_loader)\n    val_epoch_loss = validate(model, val_loader)\n    train_loss.append(train_epoch_loss)\n    val_loss.append(val_epoch_loss)\n    print(f\"Train Loss: {train_epoch_loss:.6f}\")\n    print(f\"Val Loss: {val_epoch_loss:.6f}\")","e65f04d6":"## <u>Introduction<\/u>\nIn this notebook, we take a hands-on approach to building deep learning autoencoders. We will implement deep autoencoders using linear layers with PyTorch.\n### <u>The Dataset<\/u>\nWe will use the very popular Fashion MNIST dataset. I hope that this will help newcomers in the field of deep learning who are trying to learn about autoencoders."}}