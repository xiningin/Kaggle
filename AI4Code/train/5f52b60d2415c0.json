{"cell_type":{"1a471e42":"code","a33ee1fc":"code","09c599c8":"code","40328354":"code","2ebdb0ac":"code","5a2b40a2":"code","aad23552":"code","6eedb615":"code","36ab622d":"code","7b28b9f4":"code","870e105f":"code","e6c4c227":"code","98f49737":"code","955d11e5":"code","43cf16b0":"code","5d7bcaf6":"code","76e9d2d6":"code","5449c37a":"code","1637eabf":"code","4a27207c":"code","2670cd89":"code","efde6149":"code","cd3a7fe8":"code","de4d4d17":"code","fe6283f1":"code","f831d269":"code","df97e092":"code","292b4c8e":"code","8cfe7601":"code","a99bcef4":"code","fff23a18":"code","ccad65e8":"code","355d025e":"code","61728470":"code","b13fa3ab":"code","cb06bef9":"code","d1863554":"code","448db512":"code","825c8345":"markdown","595ba658":"markdown","b3476607":"markdown","5a5e1df5":"markdown","67bf06bd":"markdown","10c7e5c6":"markdown","d6a5a49a":"markdown","6fed0638":"markdown","7fc05804":"markdown","01328e34":"markdown","08ac00b2":"markdown","1117a6e0":"markdown","7628a938":"markdown","2e9dec85":"markdown","053a21b2":"markdown","c2781169":"markdown","b448477b":"markdown","c4f5915b":"markdown","0d8b8ceb":"markdown","1d435aa7":"markdown","b356db04":"markdown","53a29ac3":"markdown","9646ffcb":"markdown","61de48d8":"markdown","08828969":"markdown","da3ef9c9":"markdown","367a8162":"markdown","cfff31a3":"markdown","49dbf811":"markdown","1470ec40":"markdown","eb156e63":"markdown","d39c5886":"markdown","be7fc883":"markdown","4f5e1821":"markdown","876588f6":"markdown"},"source":{"1a471e42":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport math \nnp.random.seed(2019)\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost.sklearn import XGBRegressor\n\nimport statsmodels\n\n#!pip install ml_metrics\nfrom ml_metrics import rmsle\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint(\"done\")","a33ee1fc":"def read_and_concat_dataset(training_path, test_path):\n    train = pd.read_csv(training_path)\n    train['train'] = 1\n    test = pd.read_csv(test_path)\n    test['train'] = 0\n    data = train.append(test, ignore_index=True)\n    return train, test, data\n\ntrain, test, data = read_and_concat_dataset('..\/input\/train.csv', '..\/input\/test.csv')\ndata = data.set_index('Id')","09c599c8":"data.columns[data.isnull().sum()>0]","40328354":"def filling_missing_values(data,variable, new_value):\n    data[variable] = data[variable].fillna(new_value)","2ebdb0ac":"filling_missing_values(data,'GarageCond','None')\nfilling_missing_values(data,'GarageQual','None')\nfilling_missing_values(data,'FireplaceQu','None')\nfilling_missing_values(data,'BsmtCond','None')\nfilling_missing_values(data,'BsmtQual','None')\nfilling_missing_values(data,'PoolQC','None')\nfilling_missing_values(data,'MiscFeature','None')","5a2b40a2":"data['MSSubClass'][data['MSSubClass'] == 20] = '1-STORY 1946 & NEWER ALL STYLES'\ndata['MSSubClass'][data['MSSubClass'] == 30] = '1-STORY 1945 & OLDER'\ndata['MSSubClass'][data['MSSubClass'] == 40] = '1-STORY W\/FINISHED ATTIC ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 45] = '1-1\/2 STORY - UNFINISHED ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 50] = '1-1\/2 STORY FINISHED ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 60] = '2-STORY 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 70] = '2-STORY 1945 & OLDER'\ndata['MSSubClass'][data['MSSubClass'] == 75] = '2-1\/2 STORY ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 80] = 'SPLIT OR MULTI-LEVEL'\ndata['MSSubClass'][data['MSSubClass'] == 85] = 'SPLIT FOYER'\ndata['MSSubClass'][data['MSSubClass'] == 90] = 'DUPLEX - ALL STYLES AND AGES'\ndata['MSSubClass'][data['MSSubClass'] == 120] = '1-STORY PUD (Planned Unit Development) - 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 150] = '1-1\/2 STORY PUD - ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 160] = '2-STORY PUD - 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 180] = 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER'\ndata['MSSubClass'][data['MSSubClass'] == 190] = '2 FAMILY CONVERSION - ALL STYLES AND AGES'","aad23552":"def fixing_ordinal_variables(data, variable):\n    data[variable][data[variable] == 'Ex'] = 5\n    data[variable][data[variable] == 'Gd'] = 4\n    data[variable][data[variable] == 'TA'] = 3\n    data[variable][data[variable] == 'Fa'] = 2\n    data[variable][data[variable] == 'Po'] = 1\n    data[variable][data[variable] == 'None'] = 0","6eedb615":"fixing_ordinal_variables(data,'ExterQual')\nfixing_ordinal_variables(data,'ExterCond')\nfixing_ordinal_variables(data,'BsmtCond')\nfixing_ordinal_variables(data,'BsmtQual')\nfixing_ordinal_variables(data,'HeatingQC')\nfixing_ordinal_variables(data,'KitchenQual')\nfixing_ordinal_variables(data,'FireplaceQu')\nfixing_ordinal_variables(data,'GarageQual')\nfixing_ordinal_variables(data,'GarageCond')\nfixing_ordinal_variables(data,'PoolQC')","36ab622d":"data['PavedDrive'][data['PavedDrive'] == 'Y'] = 3\ndata['PavedDrive'][data['PavedDrive'] == 'P'] = 2\ndata['PavedDrive'][data['PavedDrive'] == 'N'] = 1","7b28b9f4":"colu = data.columns[(data.isnull().sum()<50) & (data.isnull().sum()>0)]\nfor i in colu:\n    print(data[colu].isnull().sum())","870e105f":"colu = data.columns[data.isnull().sum()>=50]\nfor i in colu:\n    print(data[colu].isnull().sum())","e6c4c227":"filling_missing_values(data, 'GarageArea',0)\nfilling_missing_values(data, 'GarageCars',0)\ndata['GarageFinish'][(data.GarageFinish.isnull()==True) & (data.GarageCond==0)] =0\ndata['GarageType'][(data.GarageType.isnull()==True) & (data.GarageCond==0)] =0\ndata['GarageYrBlt'][(data.GarageYrBlt.isnull()==True) & (data.GarageCond==0)] =0","98f49737":"print(data[['MiscFeature','MiscVal']][(data.MiscFeature=='None') & (data.MiscVal>0)])\ndata.MiscVal.loc[2550] = 0\n\nprint(data[['MiscFeature','MiscVal']][(data.MiscVal==0) & (data.MiscFeature!='None')])\nc=data[['MiscFeature','MiscVal']][(data.MiscVal==0) & (data.MiscFeature!='None')].index\ndata.MiscFeature.loc[c] = 'None'","955d11e5":"def inputing(variab):\n    y = data[variab]\n    data2 = data.drop([variab],axis=1)\n    col = data2.columns[data2.isnull().sum()==0]\n    data2 = data2[col]\n    data2 = pd.get_dummies(data2)\n    c_train = y[y.notnull()==True].index\n    y_train = y[c_train]\n    columny = data2.columns\n    X_train = data2[columny].loc[c_train]\n    c_test = y[y.notnull()!=True].index\n    y_test = y[c_test]\n    X_test = data2[columny].loc[c_test]\n    #Model\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    #Filling missing data\n    y_pred = pd.Series(y_pred, index=c_test)\n    data[variab].loc[c_test] = y_pred.loc[c_test]\n    \ndef inputingnum(variab):\n    y = data[variab]\n    data2 = data.drop([variab],axis=1)\n    col = data2.columns[data2.isnull().sum()==0]\n    data2 = data2[col]\n    data2 = pd.get_dummies(data2)\n    c_train = y[y.notnull()==True].index\n    y_train = y[c_train]\n    columny = data2.columns\n    X_train = data2[columny].loc[c_train]\n    c_test = y[y.notnull()!=True].index\n    y_test = y[c_test]\n    X_test = data2[columny].loc[c_test]\n    #Model\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    #Filling missing data\n    y_pred = pd.Series(y_pred, index=c_test)\n    data[variab].loc[c_test] = y_pred.loc[c_test]","43cf16b0":"inputing(variab='Electrical')\ninputing(variab='Exterior2nd')\ninputing(variab='Exterior1st')\ninputing(variab='MasVnrType')\ninputing(variab='Functional')\ninputing(variab='MSZoning')\ninputing(variab='SaleType')\ninputing(variab='Alley')\ninputing(variab='BsmtExposure')\ninputing(variab='BsmtFinType1')\ninputing(variab='BsmtFinType2')\ninputing(variab='Fence')\n\ninputingnum(variab='KitchenQual')\ndata['KitchenQual'] = data.KitchenQual.astype(int)\ninputingnum(variab='BsmtFullBath')\ndata['BsmtFullBath'] = data.BsmtFullBath.astype(int)\ninputingnum(variab='BsmtHalfBath')\ndata['BsmtHalfBath'] = data.BsmtHalfBath.astype(int)\n\ninputingnum(variab='TotalBsmtSF')\ninputingnum(variab='BsmtFinSF1')\ninputingnum(variab='BsmtFinSF2')\ninputingnum(variab='MasVnrArea')\ninputingnum(variab='BsmtUnfSF')\ninputingnum(variab='LotFrontage')","5d7bcaf6":"print(data['Utilities'].value_counts())\ndata  = data.drop(['Utilities'],axis=1)","76e9d2d6":"data.columns[data.isnull().sum()>0]","5449c37a":"data.describe()","1637eabf":"from scipy.stats import norm\nplt.figure(figsize=(15,8))\nsns.distplot(data['SalePrice'][data.SalePrice.isnull()==False], fit= norm,kde=True)\nplt.show()","4a27207c":"print(data.plot.scatter(x='LotFrontage',y='SalePrice'))","2670cd89":"def dropping_outliers(data, condition):\n    #put condition with with reference to the data table, use brackets and (& |) operators, remember about you can drop observation only from train dataset\n    condition_to_drop = data[condition].index\n    data = data.drop(condition_to_drop)","efde6149":"dropping_outliers(data, (data.SalePrice<100000) & (data.train==1) & (data.LotFrontage>150))\ndropping_outliers(data, (data.LotFrontage>200) & (data.train==1))\ndropping_outliers(data, (data.SalePrice>700000) & (data.train==1))\ndropping_outliers(data, (data.SalePrice>700000) & (data.train==1))\ndropping_outliers(data, (data.LotArea>60000) & (data.train==1))\ndropping_outliers(data, (data.MasVnrArea>1450) & (data.train==1))\ndropping_outliers(data, (data.BedroomAbvGr==8) & (data.train==1))\ndropping_outliers(data, (data.KitchenAbvGr==3) & (data.train==1))\ndropping_outliers(data, (data['3SsnPorch']>400) & (data.train==1))\ndropping_outliers(data, (data.LotArea>100000) & (data.train==1))\ndropping_outliers(data, (data.MasVnrArea>1300) & (data.train==1))\ndropping_outliers(data, (data.BsmtFinSF1>2000) & (data.train==1) & (data.SalePrice<300000))\ndropping_outliers(data, (data.BsmtFinSF2>200) & (data.SalePrice>350000)  & (data.train==1))\ndropping_outliers(data, (data.BedroomAbvGr==8) & (data.train==1))\ndropping_outliers(data, (data.KitchenAbvGr==3) & (data.train==1))\ndropping_outliers(data, (data.TotRmsAbvGrd==2) & (data.train==1))","cd3a7fe8":"#CentralAir\nprint(data['CentralAir'].value_counts())\ndata['CentralAir'] = pd.Series(np.where(data['CentralAir'].values == 'Y', 1, 0),\n          data.index)","de4d4d17":"data['2ndFloor'] = pd.Series(np.where(data['2ndFlrSF'].values == 0, 0, 1),data.index)\ndata['Floors'] = data['1stFlrSF'] + data['2ndFlrSF']\ndata = data.drop(['1stFlrSF'],axis=1)\ndata = data.drop(['2ndFlrSF'],axis=1)\ndata['TotBath'] = data['FullBath'] + (0.5 * data['HalfBath']) + data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\ndata['Porch'] = data['OpenPorchSF'] + data['3SsnPorch'] + data['EnclosedPorch'] + data['ScreenPorch']\ndata['TotalSF'] = data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['Floors'] \ndata['Pool'] = pd.Series(np.where(data['PoolArea'].values == 0, 0, 1),data.index)\ndata['Bsmt'] = pd.Series(np.where(data['TotalBsmtSF'].values == 0, 0, 1),data.index)\ndata['Garage'] = pd.Series(np.where(data['GarageArea'].values == 0, 0, 1),data.index)\ndata['Fireplace'] = pd.Series(np.where(data['Fireplaces'].values == 0, 0, 1),data.index)\ndata['Remod'] = pd.Series(np.where(data['YearBuilt'].values == data['YearRemodAdd'].values, 0, 1),data.index)\ndata['NewHouse'] = pd.Series(np.where(data['YearBuilt'].values == data['YrSold'].values, 1, 0),data.index)\ndata['Age'] = data['YrSold'] - data['YearRemodAdd']","fe6283f1":"c = data[(data['Floors']>4000) & (data.train==1)].index\ndata = data.drop(c)\nc = data[(data['SalePrice']>500000) & (data['TotalSF']<3500) & (data.train==1)].index\ndata = data.drop(c)","f831d269":"data = data.drop(['PoolQC'],axis=1)\ndata = data.drop(['GrLivArea'],axis=1)\ndata = data.drop(['Street'],axis=1)\ndata = data.drop(['GarageYrBlt'],axis=1)\ndata = data.drop(['PoolArea'],axis=1)\ndata = data.drop(['MiscFeature'],axis=1)","df97e092":"Results = pd.DataFrame({'Model': [],'RMSLE': []})","292b4c8e":"data = pd.get_dummies(data)","8cfe7601":"from sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)","a99bcef4":"from xgboost.sklearn import XGBRegressor\n\nmodel = XGBRegressor(learning_rate=0.001,n_estimators=4600,\n                                max_depth=7, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27,\n                                reg_alpha=0.00006)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\n\nres = pd.DataFrame({\"Model\":['XGBoost'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","fff23a18":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(max_depth=6)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\n\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Decision Tree'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","ccad65e8":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=1500,\n                                max_depth=6)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Random Forest'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","355d025e":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.0005)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['LASSO'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","61728470":"import statsmodels.api as sm\n\nX2 = sm.add_constant(trainX)\no=0\nfor i in X2.columns:\n    o+=1\n    print(o)\n    model = sm.OLS(trainY, X2.astype(float))\n    model = model.fit()\n    p_values = pd.DataFrame(model.pvalues)\n    p_values = p_values.sort_values(by=0, ascending=False)\n    if float(p_values.loc[p_values.index[0]])>=0.05:\n        X2=X2.drop(p_values.index[0],axis=1)\n    else:\n        break\n\nkolumny = X2.columns\ntestX2 = sm.add_constant(testX)\ntestX2 = testX2[kolumny]\n\ny_pred = model.predict(testX2)\ny_pred = np.exp(y_pred)\n\n\nres = pd.DataFrame({\"Model\":['Stepwise Regression'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","b13fa3ab":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=0.0005)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Ridge'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","cb06bef9":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Linear Regression'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","d1863554":"Results","448db512":"trainX = data[data.SalePrice.isnull()==False].drop(['SalePrice','train'],axis=1)\ntrainY = data.SalePrice[data.SalePrice.isnull()==False]\ntestX = data[data.SalePrice.isnull()==True].drop(['SalePrice','train'],axis=1)\ntrainY = np.log(trainY)\nmodel = Lasso(alpha=0.0005)\nmodel.fit(trainX, trainY)\ntest = data[data.train==0]\ntest['SalePrice'] = model.predict(testX)\ntest['SalePrice'] = np.exp(test['SalePrice'] )\ntest = test.reset_index()\ntest[['Id','SalePrice']].to_csv(\"submissionLASSO.csv\",index=False)\nprint(\"done1\")","825c8345":"##**Results**","595ba658":"LASSO Regression model gives the best results. This model helps me to get 0.12903 (RMSLE) on competition test dataset and it gives me place in 20% best results on Leaderboard.","b3476607":"**Preparing to modeling:**\n- dummies variables\n- two data frames with independent variables for train and test set\n- vector y with Sale Price variable for train set","5a5e1df5":"Let's understand a data set variable after variable, check basic statistics and drop a few outliers. I'll also drop variables with little differentiation. ","67bf06bd":"..and one more but in different way.","10c7e5c6":"##**Missing values**","d6a5a49a":"For example, let's look at scatter plot of SalePrice and Lot Frontage.","6fed0638":"**Stepwise Regression**","7fc05804":"MSSubClass is not a numerical variables, so let's transform it to caterogical variable.","01328e34":"Now I'm gonna write two functions to help me in imputing missing values in variables. I'm using here Random Forest Regressor and Classifier. ","08ac00b2":"##**Feature engineering**","1117a6e0":"**Import the Libraries**","7628a938":"**Droping a few variables**","2e9dec85":"On the scatter charts, I checked which observations could be considered outliers and I decided to delete them.\nI must be very careful because I don't want to remove observations from the test set.","053a21b2":"##**Modeling:**\n\n- XGB Regressor\n- Decision Tree Regressor\n- Random Forest Regressor\n- LASSO Regression\n- Ridge Regression\n- Linear Regression\n\n\nFor each model I tuned the parameters using loops and each model contains SalePrice variable tranformed to logarithm.","c2781169":"I'm gonna put 0 in MiscVal for house which don't have any MiscFeature and 'None' value for house with 0 in MiscValue and some value in MiscFeature.","b448477b":"**Import Data**","c4f5915b":"First of all I'm gonna look how many variables have less than 50 missing values and fix it. Then I'll look how about variables with more than 50 missing values.","0d8b8ceb":"I'm gonna drop more observations.","1d435aa7":"A few categorical variables are ordinal variables, so let's fix them. ","b356db04":"There are a few variables with NaN value but in these cases 'NaN' means something else than missing value. For example 'NaN' in 'GarageCond' means that this house hasn't a garage. I'm gonna change 'NaN' values to 'None' string. ","53a29ac3":"Let's imput missing values using two functions which I wrote. In KitchenQual, BsmtFullBath and BsmtHalfBath cases I'm gonna use Regressor model and convert them to integer.","9646ffcb":"It's everything about imputing missing values.","61de48d8":"I'm putting 0 in GarageArea, GarageFinish, GarageType, GarageYrBlt and GarageCars where houses don't have garage. ","08828969":"**LASSO Regression**","da3ef9c9":"##**Fixing variables**","367a8162":"**Linear Regression**\n\nWhen you change alpha to 0 value in LASSO, you have simple Linear Regression model.","cfff31a3":"* 2ndFloor - if the house has a second floor\n* Floors - total area of the first and second floor\n* TotBath - how many bathrooms house has\n* Porch - total area of the porch\n* TotalSF - total area of the house\n* Pool - if the house has a swimming pool\n* Bsmt - if the house has a basement\n* Garage - if the house has a garage\n* Fireplace - if the house has a fireplace\n* Remod - if the house was renovated\n* NewHouse - if the house is new\n* Age - ages of house\n","49dbf811":"**Decision Tree Regressor**","1470ec40":"I'm adding here 'train' variable in order to check in the easiest way which observations are from train and test dataset because I'm gonna join train and test datasets.","eb156e63":"**Ridge Regression**","d39c5886":"CentalAir variable needs transformation to binary variable.","be7fc883":"##**Dropping outliers**","4f5e1821":"**XGBoost Regressor**","876588f6":"House Price competition is a very good way to introduce feature engineering and regression models. I'm gonna explore the data and make something with them and also imput missing values. Feature engineering is an important part of machine learning process so I want to spend more time for this part. I'm gonna try I few models and tell you which work the best with train dataset from this competition. Please consider upvoting if this is useful to you :)"}}