{"cell_type":{"49296ed9":"code","7da7d0a5":"code","6c113ffb":"code","9f7b7489":"code","cae4cac0":"code","aa029a7a":"code","b3eae421":"code","a703f7f6":"code","23cec7f0":"code","bd72445b":"code","7e01ef4f":"code","249dd332":"code","980cc9e2":"code","8b5620fb":"code","98d8a382":"code","64b89f62":"code","c0682be0":"code","d5d8d75d":"code","11268b1c":"code","188b2e45":"code","93aa12e7":"code","e1bd4948":"code","5b191a85":"code","75abc019":"code","f2038ac6":"code","90ffdbe2":"code","2d68352d":"code","459c8675":"code","ad328b07":"code","8bfb95d7":"code","b997d3b9":"code","df083df1":"code","ad9dd95e":"code","77cf8045":"code","9e53c758":"code","8e19c08f":"code","5e6737fc":"code","ab0fbe78":"code","fa848645":"code","27e287f9":"code","9eed53e8":"code","ccdcc4fe":"code","0ca26ae3":"code","f9dfd1f6":"code","7cb3f2e5":"code","909ad244":"code","9ec10e67":"code","795f8bdb":"code","b6d3ed20":"code","15b20b8c":"code","10f7c653":"code","65c55c05":"code","77d6af81":"code","25920a2e":"code","5044d559":"code","33a50886":"code","d658e65e":"code","47bc7e87":"code","dc08ea76":"code","893ed79c":"code","c82c22cd":"code","8a9ad0d0":"code","54ca6f1c":"code","ed9b1ef5":"code","f896e8a1":"code","63303fe0":"code","906dd7a6":"code","531634ba":"code","2473a925":"code","2a0f5c08":"code","b5cdd890":"code","d6944dac":"markdown","46d95e4f":"markdown","884bd8b7":"markdown","7c3249e1":"markdown","4362452b":"markdown","5cb019ef":"markdown","35bd5744":"markdown","efc4a4f2":"markdown","9485f8d7":"markdown","3a241237":"markdown","3534dd05":"markdown","e5033a49":"markdown","111f8494":"markdown","5496082f":"markdown","85723286":"markdown","d1a5b549":"markdown","b7bc2088":"markdown","33b722d1":"markdown","96d469fe":"markdown","7b757322":"markdown","4bc20735":"markdown","1b0a41b0":"markdown","8fce93ad":"markdown","dd7123c1":"markdown","7cdebcfb":"markdown","fb471b4c":"markdown","316395d7":"markdown","e8cc4702":"markdown","e007b703":"markdown","149299fe":"markdown","8447cef5":"markdown","cd9f0350":"markdown","2046b0eb":"markdown","fb10c7db":"markdown","eaae8785":"markdown","79e0451b":"markdown","8dff2ca4":"markdown","b9f2d3a8":"markdown","88180a5f":"markdown","f7fc3772":"markdown","f703956f":"markdown","c20cdfe7":"markdown"},"source":{"49296ed9":"import numpy as np\nimport pandas as pd\n\nimport cv2\nimport hashlib\nimport matplotlib.pyplot as plt\n\nimport PIL\nfrom PIL import Image\nfrom PIL import ImageStat\n\nfrom tqdm.notebook import tqdm\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff","7da7d0a5":"!pip install -U albumentations\nimport albumentations\nalbumentations.__version__","6c113ffb":"CFG = {\n    'rootdir': '..\/input\/classify-leaves\/'\n}","9f7b7489":"train = pd.read_csv(CFG['rootdir'] + 'train.csv')\ntest = pd.read_csv(CFG['rootdir'] + 'test.csv')\nsubmission_sample = pd.read_csv(CFG['rootdir'] + 'sample_submission.csv')","cae4cac0":"train.sample(10)","aa029a7a":"test.sample(10)","b3eae421":"print(train.shape)\nprint(test.shape)","a703f7f6":"train.label.value_counts()","23cec7f0":"submission_sample.head()","bd72445b":"fig = px.imshow(cv2.imread(CFG['rootdir']+'{}'.format(train.sample(1)['image'].values[0])))\nfig.show()","7e01ef4f":"def get_hash(image):\n    md5 = hashlib.md5()\n    md5.update(np.array(image).tobytes())\n    return md5.hexdigest()\n\ndef get_image_meta(image_id, image_src, dataset = 'train'):\n    img = Image.open(image_src)\n    extrema = img.getextrema()\n    stat = ImageStat.Stat(img)\n    \n    meta = {\n        'image': image_id,\n        'dataset': dataset,\n        'hash': get_hash(img),\n        'R_min': extrema[0][0],\n        'R_max': extrema[0][1],\n        'G_min': extrema[1][0],\n        'G_max': extrema[1][1],\n        'B_min': extrema[2][0],\n        'B_max': extrema[2][1],\n        'R_avg': stat.mean[0],\n        'G_avg': stat.mean[1],\n        'B_avg': stat.mean[2],\n        'height': img.height,\n        'width': img.width,\n        'format': img.format,\n        'mode': img.mode\n    }\n    return meta","249dd332":"def image_metadata(df, rootdir, mark='train'):\n    img_data = []\n    for i, image_id in enumerate(tqdm(df['image'], total=df.shape[0])):\n        img_data.append(get_image_meta(image_id, (CFG['rootdir'] + '{}'.format(image_id)), mark))\n    \n    meta_pd = pd.DataFrame(img_data)\n    return meta_pd","980cc9e2":"meta_train = image_metadata(df=train, rootdir = CFG['rootdir'], mark='train')\nmeta_train.head()","8b5620fb":"meta_test = image_metadata(test, rootdir = CFG['rootdir'], mark='test')\nmeta_test.head()","98d8a382":"fig = px.histogram(train, 'label', marginal='violin', hover_data=train.columns)\nfig.update_layout(title_text=\"Distribution of Classes\")\nfig.show()","64b89f62":"fig = ff.create_distplot([meta_train['R_avg']], group_labels=[\"ChannelsR\"], colors=[\"RED\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"Distribution of Channel Values\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig","c0682be0":"fig = ff.create_distplot([meta_train['R_avg'], meta_train['G_avg'], meta_train['B_avg']], group_labels=['R','G','B'], colors=[\"RED\",'GREEN','BLUE'])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"Distribution of Channel Values\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.data[1].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[1].marker.line.width = 0.5\nfig.data[2].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[2].marker.line.width = 0.5\nfig","d5d8d75d":"fig = go.Figure()\n\nfor idx, values in enumerate([meta_train['R_avg'], meta_train['G_avg'], meta_train['B_avg']]):\n    if idx == 0:\n        color = \"RED\"\n    if idx == 1:\n        color = \"GREEN\"\n    if idx == 2:\n        color = \"BLUE\"\n    fig.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n    \nfig.update_layout(yaxis_title=\"Mean Value\", xaxis_title=\"Color Channel\",\n                  title=\"Mean Value vs. Color Channel\", template=\"plotly_white\")","11268b1c":"def get_duplicate(dt, mark = 'train'):\n    dup = dt.groupby(by = 'hash')[['dataset']].count().reset_index()\n    dup = dup[dup['dataset'] > 1]\n    dup.reset_index(drop = True, inplace = True)\n    dup = dup.merge(dt[['image', 'hash']], on = 'hash')\n    dup['dup_number'] = dup['dataset']\n    dup.loc[:,'dataset'] = mark\n    \n    return dup","188b2e45":"dup_train = get_duplicate(meta_train, 'train')\ndup_train","93aa12e7":"duptest = get_duplicate(meta_test, 'test')\nduptest","e1bd4948":"duptest['dup_number'].value_counts()","5b191a85":"dup_train = dup_train.merge(train[['image','label']], on = 'image')\ndup_train","75abc019":"tester = dup_train.copy()\n# we will drop those rows both have the same hash and label which just duplicated data, not noisy confusion data.\n#\u628a hash\u548c label\u90fd\u76f8\u540c\u7684 drop\u6389\ntester = tester.drop_duplicates(subset=['hash','label'], keep=False)","f2038ac6":"tester['dup_number'].value_counts()","90ffdbe2":"tester.head(10)","2d68352d":"def plot_dup(df, number):\n    fig, ax = plt.subplots(int(number\/2), 2, figsize=(10, 30))\n    ax = ax.flatten()\n\n    for i in range(0, min(tester.shape[0], number), 2):\n        image_i = cv2.imread(CFG['rootdir'] + '{}'.format(df.iloc[i, 2]), cv2.IMREAD_COLOR)\n        image_i = cv2.cvtColor(image_i, cv2.COLOR_BGR2RGB)\n        ax[i].set_axis_off()\n        ax[i].imshow(image_i)\n        ax[i].set_title(df.iloc[i, 2] + '\\n' + df.iloc[i, 4])\n\n        image_i_1 = cv2.imread(CFG['rootdir'] + '{}'.format(df.iloc[i + 1, 2]), cv2.IMREAD_COLOR)\n        image_i_1 = cv2.cvtColor(image_i_1, cv2.COLOR_BGR2RGB)\n        ax[i + 1].set_axis_off()\n        ax[i + 1].imshow(image_i_1)\n        ax[i + 1].set_title(df.iloc[i + 1, 2] + '\\n' + df.iloc[i + 1,4])","459c8675":"#plot_dup(dup_train, 12)","ad328b07":"plot_dup(tester, 12)","8bfb95d7":"train_clean = train[~train['image'].isin(tester['image'])].reset_index(drop=True)","b997d3b9":"train_clean.nunique()","df083df1":"#save cleaned data\ntrain_clean.to_csv('train_clean.csv', index=False)","ad9dd95e":"def edge_and_cut(img_src):\n    img = cv2.imread(img_src)\n    emb_img = img.copy()\n    edges = cv2.Canny(img, 100, 200)\n    edge_coors = []\n    for i in range(edges.shape[0]):\n        for j in range(edges.shape[1]):\n            if edges[i][j] != 0:\n                edge_coors.append((i, j))\n    \n    row_min = edge_coors[np.argsort([coor[0] for coor in edge_coors])[0]][0]\n    row_max = edge_coors[np.argsort([coor[0] for coor in edge_coors])[-1]][0]\n    col_min = edge_coors[np.argsort([coor[1] for coor in edge_coors])[0]][1]\n    col_max = edge_coors[np.argsort([coor[1] for coor in edge_coors])[-1]][1]\n    new_img = img[row_min:row_max, col_min:col_max]\n    \n    emb_img[row_min-5:row_min+5, col_min:col_max] = [255, 0, 0]\n    emb_img[row_max-5:row_max+5, col_min:col_max] = [255, 0, 0]\n    emb_img[row_min:row_max, col_min-5:col_min+5] = [255, 0, 0]\n    emb_img[row_min:row_max, col_max-5:col_max+5] = [255, 0, 0]\n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(edges, cmap='gray')\n    ax[1].set_title('Canny Edges', fontsize=24)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('Bounding Box', fontsize=24)\n    plt.show()","77cf8045":"edge_and_cut('..\/input\/classify-leaves\/images\/233.jpg')\nedge_and_cut('..\/input\/classify-leaves\/images\/2333.jpg')\nedge_and_cut('..\/input\/classify-leaves\/images\/25261.jpg')\n# edge_and_cut('..\/input\/classify-leaves\/images\/25909.jpg')\n# edge_and_cut('..\/input\/classify-leaves\/images\/26634.jpg')\n# edge_and_cut('..\/input\/classify-leaves\/images\/26723.jpg')","9e53c758":"from albumentations import (\n    HorizontalFlip, VerticalFlip, Rotate, ShiftScaleRotate, RandomBrightnessContrast, Perspective, CLAHE, \n    Transpose, Blur, OpticalDistortion, GridDistortion, HueSaturationValue, ColorJitter, GaussNoise, MotionBlur, MedianBlur,\n    Emboss, Sharpen, Flip, OneOf, SomeOf, Compose, Normalize, CoarseDropout, CenterCrop, GridDropout, Resize\n)","8e19c08f":"# helper function\ndef get_img(imgsrc):\n    im_bgr = cv2.imread(imgsrc)\n    im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)\n    return im_rgb\n\ndef visualize(image):\n    plt.figure(figsize=(5, 5))\n    plt.axis('off')\n    plt.imshow(image)","5e6737fc":" def plot_aug(img_src, method):\n    img = get_img(img_src)\n    augmented_img = method(p=1)(image=img)['image']\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(augmented_img)\n    ax[1].set_title(method.__name__, fontsize=24)\n    plt.show()","ab0fbe78":"plot_aug('..\/input\/classify-leaves\/images\/233.jpg', HorizontalFlip)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', VerticalFlip)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', Transpose)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', ShiftScaleRotate)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', Blur)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', GaussNoise)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', HueSaturationValue)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', CoarseDropout)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', GridDistortion)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', Sharpen)\nplot_aug('..\/input\/classify-leaves\/images\/233.jpg', CLAHE)","fa848645":"transform = Compose([\n        SomeOf([\n            Flip(),\n            GridDistortion(),\n            Transpose(),\n            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n        ], n=2, p=1),\n        OneOf([\n            MotionBlur(p=.2),\n            MedianBlur(blur_limit=3, p=0.1),\n            Blur(blur_limit=3, p=0.1),\n            GaussNoise(p=0.2),\n        ], p=0.2),\n        OneOf([\n            CLAHE(clip_limit=2),\n            Sharpen(),\n            Emboss(),\n            RandomBrightnessContrast(),            \n        ], p=0.3),\n        OneOf([\n            CoarseDropout(),\n            GridDropout(),\n        ], p=0.2),\n        HueSaturationValue(p=0.3),\n    ])\n\nimg = get_img('..\/input\/classify-leaves\/images\/233.jpg')\n\nvisualize(transform(image=img)['image'])\nvisualize(transform(image=img)['image'])\nvisualize(transform(image=img)['image'])","27e287f9":"#!pip install git+https:\/\/github.com\/ildoonet\/pytorch-randaugment","9eed53e8":"#from RandAugment import RandAugment","ccdcc4fe":"# from RandAugment import RandAugment\n\n# N = 5\n# M = 5\n\n# train_transforms = Compose([\n#             RandAugment(N, M),\n#             Transpose(p=0.5),\n#             VerticalFlip(p=0.5),\n#             HorizontalFlip(p=0.5),\n#             ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n#             Resize(img_size[0], img_size[1], p=1.0),\n#             #A.Normalize(),\n#             #ToTensorV2(p=1.0),\n#         ], p=1.0)","0ca26ae3":"import os\nimport time\nfrom glob import glob\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\nimport cv2\n\nfrom tqdm import tqdm\n\n#import timm","f9dfd1f6":"!pip install timm\n!pip install torchinfo","7cb3f2e5":"import timm\nfrom torchinfo import summary","909ad244":"model_batch_size = 32","9ec10e67":"eff_model = timm.create_model('tf_efficientnet_b4', pretrained=True)\nsummary(eff_model, input_size = (model_batch_size, 3, 224, 224))","795f8bdb":"eff_model_2 = timm.create_model('tf_efficientnet_b5', pretrained=True)\nsummary(eff_model, input_size = (model_batch_size, 3, 224, 224))","b6d3ed20":"res_model = timm.create_model('resnest50d', pretrained=True)\nres_model.fc = nn.Linear(2048, 176)\nsummary(res_model, input_size = (model_batch_size, 3, 224, 224))","15b20b8c":"del eff_model\ndel eff_model_2\ndel res_model","10f7c653":"CFG = {\n    'rootdir': '..\/input\/classify-leaves\/',\n    'fold_num': 5,\n    'seed': 123,\n    'model_arch': 'tf_efficientnet_b5',\n    'img_size': 224,\n    'epochs': 10,\n    'train_bs': 32,\n    'valid_bs': 32,\n    'T_0': 10, # Number of iterations for the first restart\n    'lr': 3e-4,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 8,\n    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1, # 0 (silent),1 (progress bar), 2 (one line per epoch)\n    'device': 'cuda:0'\n}","65c55c05":"# labelencoder for dataset\nt_labelencoder = preprocessing.LabelEncoder()\nt_labelencoder.fit(train['label'])","77d6af81":"train_clean['label'] = t_labelencoder.transform(train_clean['label'])","25920a2e":"train_clean","5044d559":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    #print(im_rgb)\n    return im_rgb","33a50886":"class MyDataset(Dataset):\n    def __init__(self, df, data_root, transforms=None, output_label=True):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        \n        self.output_label = output_label\n        \n        if output_label == True:\n            self.labels = self.df['label'].values\n            \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        # get labels\n        if self.output_label:\n            target = self.labels[index]\n          \n        img  = get_img(\"{}\/{}\".format(self.data_root, self.df.loc[index]['image']))\n\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n        \n        if self.output_label == True:\n            return img, target\n        else:\n            return img","d658e65e":"from albumentations import (\n    HorizontalFlip, VerticalFlip, Rotate, ShiftScaleRotate, RandomBrightnessContrast, Perspective, CLAHE, \n    Transpose, Blur, OpticalDistortion, GridDistortion, HueSaturationValue, ColorJitter, GaussNoise, MotionBlur, MedianBlur,\n    Emboss, Sharpen, Flip, OneOf, SomeOf, Compose, Normalize, CoarseDropout, CenterCrop, GridDropout, Resize\n)\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            OneOf([\n            CoarseDropout(p=0.5),\n            GaussNoise(),\n            ], p=0.5),\n            SomeOf([\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            ], n=3, p=0.6),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","47bc7e87":"def prepare_dataloader(df, trn_idx, val_idx, data_root='..\/classify-leaves\/'):\n\n    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n        \n    train_ds = MyDataset(train_, data_root, transforms=get_train_transforms(), output_label=True)\n    valid_ds = MyDataset(valid_, data_root, transforms=get_valid_transforms(), output_label=True)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_ds,\n        batch_size=CFG['train_bs'],\n        pin_memory=True,\n        drop_last=False,\n        shuffle=True,        \n        num_workers=CFG['num_workers'],\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_ds, \n        batch_size=CFG['valid_bs'],\n        num_workers=CFG['num_workers'],\n        shuffle=False,\n        pin_memory=True,\n    )\n    return train_loader, val_loader","dc08ea76":"class EfficientModel(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, n_class)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nclass ResnetModel(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, n_class)\n        '''\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            #nn.Linear(n_features, hidden_size,bias=True), nn.ELU(),\n            nn.Linear(n_features, n_class, bias=True)\n        )\n        '''\n    def forward(self, x):\n        x = self.model(x)\n        return x","893ed79c":"def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None):\n    model.train()\n\n    running_loss = None\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n\n        with autocast():\n            image_preds = model(imgs)\n\n            loss = loss_fn(image_preds, image_labels)\n            scaler.scale(loss).backward()\n\n            if running_loss is None:\n                running_loss = loss.item()\n            else:\n                running_loss = running_loss * .99 + loss.item() * .01\n\n            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad() \n\n            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n                description = f'epoch {epoch} loss: {running_loss:.4f}'\n                \n                pbar.set_description(description)\n                \n    if scheduler is not None:\n        scheduler.step()\n        \n    return running_loss\n        \ndef valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None):\n    model.eval()\n\n    loss_sum = 0\n    sample_num = 0\n    image_preds_all = []\n    image_targets_all = []\n    val_loss = None\n    \n    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n        \n        image_preds = model(imgs)\n        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n        image_targets_all += [image_labels.detach().cpu().numpy()]\n        \n        loss = loss_fn(image_preds, image_labels)\n        \n        loss_sum += loss.item()*image_labels.shape[0]\n        sample_num += image_labels.shape[0]\n\n        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n            val_loss = loss_sum\/sample_num\n            description = f'epoch {epoch} loss: {val_loss:.4f}'\n            pbar.set_description(description)\n    \n    image_preds_all = np.concatenate(image_preds_all)\n    image_targets_all = np.concatenate(image_targets_all)\n    \n    val_acc = (image_preds_all==image_targets_all).mean()\n    print('validation multi-class accuracy = {:.4f}'.format(val_acc))\n    \n    if scheduler is not None:\n        scheduler.step()\n        \n    return val_acc, val_loss","c82c22cd":"if __name__ == '__main__':\n\n    seed_everything(CFG['seed'])\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train_clean.shape[0]), train_clean.label.values)\n    train_loss_all = []\n    val_loss_all = []\n    val_acc_all = []\n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        start_time = time.time()\n        # \u6d4b\u8bd5\u4e24\u4e2afold\n        if fold > 1:\n            break \n            \n        print('Training with {} started'.format(fold))\n\n        print(len(trn_idx), len(val_idx))\n        train_loader, val_loader = prepare_dataloader(train_clean, trn_idx, val_idx, data_root=CFG['rootdir'])\n\n        device = torch.device(CFG['device'])\n        \n        model = EfficientModel(CFG['model_arch'], train_clean.label.nunique(), pretrained=True).to(device)\n        scaler = GradScaler()   \n        optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=CFG['epochs']-1)\n        \n        loss_tr = nn.CrossEntropyLoss().to(device)\n        loss_fn = nn.CrossEntropyLoss().to(device)\n        train_loss_temp = []\n        val_loss_temp = [] \n        val_acc_temp = []\n        \n        for epoch in range(CFG['epochs']):\n            train_loss_temp = train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler)\n            train_loss_all.append([fold, epoch, train_loss_temp])\n            with torch.no_grad():\n                val_acc_temp, val_loss_temp = valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None)\n                \n                val_loss_all.append([fold, epoch, val_loss_temp])\n                val_acc_all.append([fold, epoch, val_acc_temp])\n            if epoch > CFG['epochs'] - 4:  #save last three models\n                torch.save(model.state_dict(),'{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))\n                \n        del model, optimizer, train_loader, val_loader, scaler, scheduler\n        torch.cuda.empty_cache()","8a9ad0d0":"train_loss_all_df = pd.DataFrame(np.array(train_loss_all), columns = ['fold','epoch','train_loss'])\nval_loss_all_df = pd.DataFrame(np.array(val_loss_all), columns = ['fold','epoch','val_loss'])\nval_acc_all_df = pd.DataFrame(np.array(val_acc_all), columns = ['fold','epoch','val_acc'])","54ca6f1c":"fig1 = px.line(train_loss_all_df, x=\"epoch\", y=\"train_loss\", color='fold')\nfig2 = px.line(val_loss_all_df, x=\"epoch\", y=\"val_loss\", color='fold')\nfig3 = px.line(val_acc_all_df, x=\"epoch\", y=\"val_acc\", color='fold')\nfig4 = go.Figure(data=fig1.data + fig2.data + fig3.data)\nfig4.show()","ed9b1ef5":"CFG['weights'] = [0.8,1,0.9] # weight for out model\nCFG['tta'] = 3 # set TTA times\nCFG['used_epochs'] = [7,8,9] # choose the model\n\nCFG","f896e8a1":"def get_inference_transforms():\n    return Compose([\n            Resize(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            #HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            #RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","63303fe0":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","906dd7a6":"if __name__ == '__main__':\n    \n    seed_everything(CFG['seed'])\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train_clean.shape[0]), train_clean.label.values)\n    tst_preds_all = []\n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        \n        if fold > 1:\n            break\n\n        print('Inference fold {} started'.format(fold))\n        \n        valid_ = train_clean.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = MyDataset(valid_, CFG['rootdir'], transforms=get_inference_transforms(), output_label=False)\n\n        test = pd.read_csv( CFG['rootdir'] + 'test.csv')\n        test_ds = MyDataset(test, CFG['rootdir'], transforms=get_inference_transforms(), output_label=False)\n\n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=True,\n        )\n\n        tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=True,\n        )\n        \n        device = torch.device(CFG['device'])\n        model = EfficientModel(CFG['model_arch'], train.label.nunique()).to(device)\n        \n        val_preds = []\n        tst_preds = []\n        \n        for i, epoch in enumerate(CFG['used_epochs']):    \n            model.load_state_dict(torch.load('{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch)))\n            \n            with torch.no_grad():\n                for _ in range(CFG['tta']):\n                    val_preds += [CFG['weights'][i]\/sum(CFG['weights'])\/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n                    tst_preds += [CFG['weights'][i]\/sum(CFG['weights'])\/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n\n                    \n        val_preds = np.mean(val_preds, axis=0) \n        tst_preds = np.mean(tst_preds, axis=0) \n        tst_preds_all.append(tst_preds)\n        \n        print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n        print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n        del model\n        torch.cuda.empty_cache()","531634ba":"# calculate the average result from all folds\navg_tst = np.mean(tst_preds_all, axis=0)","2473a925":"test['label'] = np.argmax(avg_tst, axis=1)\ntest.head()","2a0f5c08":"# inverse our label\ntest['label'] = t_labelencoder.inverse_transform(test['label'])","b5cdd890":"test.to_csv('submission.csv', index=False)","d6944dac":"## Model","46d95e4f":"## Prepare Data","884bd8b7":"Augmentation","7c3249e1":"<a id=\"subsection-meta\"><\/a>\n## Image Meta Data","4362452b":"<a id=\"subsection-canny\"><\/a>\n## Canny edge detection","5cb019ef":"# something might help\n\n0. Seed everything ---[reproducible](https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html)\n1. Get know your data, Do data laundry, Take care of the imbalance dataset ---[dealing with imbalanced Classification ](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)\n2. \u5408\u9002\u7684 augmentation method ---[Fast AutoAugment](https:\/\/github.com\/kakaobrain\/fast-autoaugment) [RandAugment](https:\/\/arxiv.org\/abs\/1909.13719)\n3. \u5408\u9002\u7684 lr schedule \u65b9\u6cd5 ---[LR_schedule](https:\/\/www.kaggle.com\/isbhargav\/guide-to-pytorch-learning-rate-scheduling)\n4. APEX ---[Automatic Mixed Precision](https:\/\/pytorch.org\/blog\/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision\/)\n4. Model Selection ---[TIMM](https:\/\/paperswithcode.com\/lib\/timm)\n5. Cross-validation, TTA ---[Stratified k-fold](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#:~:text=3.1.2.2.1.-,Stratified%20k-fold,-%C2%B6) [When and Why TTA](https:\/\/arxiv.org\/abs\/2011.11156)\n5. \u5408\u9002\u7684 Metric method ---[quantifying the quality of predictions](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics)","35bd5744":"Add label to our dup dataset","efc4a4f2":"# Introduction","9485f8d7":"We can see that RGB channel plot are mostly over lapped, since there are a lot of white space in out image data. The white channel [255,255,255] raised a lot of our average level. A boundbox might can help us reduce those useless pixels.(TBD)\n\n\u901a\u8fc7\u67e5\u770bRGB\u7684channel\u56fe, \u53ef\u4ee5\u770b\u5230\u5230RGB\u901a\u9053\u5927\u81f4\u4e0a\u90fd\u975e\u5e38\u63a5\u8fd1. \u4e3b\u8981\u539f\u56e0\u662f\u56fe\u7247\u96c6\u4e2d\u6709\u592a\u591a\u7684\u7a7a\u767d, \u6240\u4ee5[255,255,255]\u7684\u6570\u503c\u8fc7\u591a,\u5bfc\u81f4\u5e73\u5747\u6570\u503c\u4e5f\u5f88\u9ad8.\n\u4e5f\u8bb8\u53ef\u4ee5\u8003\u8651\u901a\u8fc7\u52a0\u4e00\u4e2aboundbox\u6765\u7f29\u5c0f\u641c\u7d22\u8303\u56f4.","3a241237":"[RandAugment](https:\/\/arxiv.org\/abs\/1909.13719)","3534dd05":"## Helper Function","e5033a49":"Fast AutoAugment \nhttps:\/\/github.com\/kakaobrain\/fast-autoaugment\n\npip install -U autoalbument\n\nhttps:\/\/github.com\/albumentations-team\/autoalbument","111f8494":"<a id=\"section-ONE\"><\/a>\n# EDA","5496082f":"## Content\n* [EDA](#section-ONE)\n    - [Import Dataset](#subsection-import)\n    - [Image Meta Data](#subsection-meta)\n    - [Visualize The Data](#subsection-visualize)\n    - [Noisy in Data](#subsection-noisy)\n    - [Canny edge detection](#subsection-canny)\n* [Image Augmentation](#section-TWO)\n    - [Albumentations](#subsection-albumentations)\n    - [AutoAlbument](#subsection-autoaug)\n    - [RangomAug](#subsection-randomaug)\n* [Model](#section-THREE)\n    - [Model Collection](#subsection-collection)\n* [Training Process](#section-FOUR)\n    - [Config](#subsection-Config)\n    - [Prepare Data](#section-PData)\n    - [Model](#section-Model)\n    - [Train Loop](#section-Loop)\n* [Inference Process](#section-FIVE)\n    \n GL&HF Kaggling :D\n    ","85723286":"Here, we will drop the duplicated row. \n\n\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u8003\u8651\u628adup\u7684\u6570\u636e\u5168\u90fddrop\u6389, \u6216\u8005\u4e5f\u53ef\u4ee5\u52a0\u4e0asoft label.","d1a5b549":"\u901a\u5e38\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528[torchvision.models](https:\/\/pytorch.org\/vision\/stable\/models.html)\u6765\u9009\u62e9\u4e3b\u6d41\u7684\u6a21\u578b. \n\n\u8fd9\u91cc\u63a8\u8350\u4f7f\u7528timm\u5e93, \u76f8\u6bd4pytorch\u7684\u5b98\u65b9\u5e93, \u6a21\u578b\u9009\u62e9\u66f4\u591a,\u4e5f\u66f4\u65b0\uff0c\u53ef\u4ee5\u67e5\u770b\u5f88\u591a\u6a21\u578b\u53c2\u6570\u7ec6\u8282. \n\n\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u4e0b\u9762\u7684\u94fe\u63a5. \u53ef\u4ee5\u901a\u8fc7torchinfo\uff0c\u8f85\u52a9\u4e86\u89e3\u9009\u62e9\u7684\u6a21\u578b.\n\nhttps:\/\/paperswithcode.com\/lib\/timm\n","b7bc2088":"176 labes in dataset","33b722d1":"<a id=\"subsection-autoaug\"><\/a>\n## AutoAlbument\n","96d469fe":"### Common pipeline for augmentation\n\nhttps:\/\/albumentations.readthedocs.io\/en\/latest\/probabilities.html\n\nhttps:\/\/albumentations.ai\/docs\/getting_started\/setting_probabilities\/\n\n\u5982\u679c\u662f\u4f7f\u7528 albumentations\u7684\u8bdd, \u53ef\u4ee5\u8003\u8651\u5408\u7406\u4f7f\u7528 someof \u548c oneof \u548c\u4e0d\u540c\u7684 probability rate \u6765\u7ec4\u5408\u6570\u636e\u589e\u5f3a\u65b9\u6cd5, \u53ef\u4ee5\u9002\u5f53\u7684\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4.","7b757322":"## Training","4bc20735":"Lets disply a single image","1b0a41b0":"https:\/\/docs.opencv.org\/3.4\/da\/d22\/tutorial_py_canny.html\n\nHere we use canny algorithm to search the edge, then we use the bound of edge to estimate our bounding.\n\n\u8fd9\u91cc\u4f7f\u7528\u7684\u65b9\u6cd5\u662f\u7528canny\u627e\u51fa\u8f6e\u5ed3\u540e\uff0c\u7136\u540e\u6839\u636e\u8f6e\u5ed3\u8fb9\u754c\u753b\u51fabounding box","8fce93ad":"We can use summary to estimate our GPU RAM consumption with different settings\n","dd7123c1":"## Config","7cdebcfb":"<a id=\"subsection-import\"><\/a>\n## Import Dataset","fb471b4c":"<a id=\"subsection-randomaug\"><\/a>\n## RandomAug","316395d7":"<a id=\"section-FIVE\"><\/a>\n# Inference Loop","e8cc4702":"<a id=\"section-TWO\"><\/a>\n# Image Augmentation","e007b703":"<a id=\"section-FOUR\"><\/a>\n# Train Loop","149299fe":"<a id=\"subsection-visualize\"><\/a>\n## Visualize The Data","8447cef5":"\u6709\u5173Augmentation, \u6211\u4eec\u53ef\u4ee5\u7528pytorch\u81ea\u5e26\u7684\uff0c\u4e5f\u53ef\u4ee5\u8003\u8651\u4f7f\u7528albumentations lib.","cd9f0350":"## Plot the result","2046b0eb":"\u8fd9\u91cc\u7b80\u5355\u63d0\u4e00\u4e0b\u4e24\u79cd\u81ea\u52a8\u9009\u62e9augmentation\u65b9\u6cd5.","fb10c7db":"## Main Loop","eaae8785":"The augmentation method can be divided as follows:\n1. geometric transformations,  \n2. color space transformations, \n3. kernel filters, \n4. mixing images,\n5. random erasing\n\nWe should wisely choose and combine our augmentation method. Since some methods will increase your search space, \nand even dmage your performance.\n\n\n\n\u901a\u5e38\u56fe\u7247\u7684\u6570\u636e\u589e\u5f3a\u4e3b\u8981\u5206\u4e3a\n1. \u51e0\u4f55\u53d8\u6362 2.\u989c\u8272\u53d8\u6362 3.\u6ee4\u955c\u5316 4. \u56fe\u7247\u7ec4\u5408 5. \u968f\u673a\u64e6\u9664\n\n\u6211\u4eec\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u76ee\u6807\u6570\u636e\u96c6\uff0c\u5408\u7406\u7684\u9009\u62e9\u9002\u5408\u6211\u4eec\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u6709\u4e9b\u65f6\u5019\u4e00\u4e9b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u751a\u81f3\u4f1a\u635f\u5bb3\u7ed3\u679c.\n\n\u63a8\u8350\u5927\u5bb6\u770b\u770b\u8fd9\u4e00\u7bc7 [A survey on Image Data Augmentation for Deep Learning](https:\/\/journalofbigdata.springeropen.com\/articles\/10.1186\/s40537-019-0197-0#Fun)\n\nAbout mixup and cutmix https:\/\/www.kaggle.com\/cdeotte\/cutmix-and-mixup-on-gpu-tpu","79e0451b":"<a id=\"subsection-albumentations\"><\/a>\n## Albumentations","8dff2ca4":"<a id=\"section-THREE\"><\/a>\n# Model","b9f2d3a8":"## Reference\nhttps:\/\/www.kaggle.com\/tarunpaparaju\/plant-pathology-2020-eda-models\/notebook\n\nhttps:\/\/www.kaggle.com\/pestipeti\/eda-plant-pathology-2020","88180a5f":"<a id=\"subsection-collection\"><\/a>\n## Model Collection","f7fc3772":"<a id=\"subsection-noisy\"><\/a>\n## Noisy in Data","f703956f":"Most of the time, we use the normalization from ImageNet's [mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0]\nbut we can also make our own normalization data from our dataset.\n\n\n\u5927\u591a\u6570\u65f6\u5019\u7684normalization\u90fd\u4f7f\u7528Imagenet\u7684\u5c31\u884c\u4e86\uff0c\u4f46\u662f\u6211\u4eec\u4e5f\u53ef\u4ee5\u8003\u8651\u81ea\u5df1\u8ba1\u7b97 mean\u548c std. \u6216\u8005\u4f7f\u7528\u5176\u4ed6\u7684\u65b9\u6cd5\u6765\u505anormalization \n\n\u6bd4\u5982\uff1aalbumentations.augmentations.domain_adaptation.HistogramMatching","c20cdfe7":"Here we can see that the distribution seems like a long-tail dataset, and our testing data set might based on 176 x 50 = 88800 (with an even distribution). \n\n\u8fd9\u91cc\u53ef\u4ee5\u770b\u5230\uff0c\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\u770b\u8d77\u6765\u6709\u90a3\u4e48\u4e00\u70b9\u70b9\u50cflong-tail\u7684\u6570\u636e\u96c6, \u5e76\u4e14\u56e0\u4e3a\u76ee\u6807\u6d4b\u8bd5\u96c6\u5e94\u8be5\u662f\u6309\u7167 176 X 50 = 8800(\u6309\u7167\u5747\u5300\u5206\u5e03).\n\u6240\u4ee5\u6211\u4eec\u53d6sampler\u7684\u65f6\u5019\u9700\u8981\u6ce8\u610f\uff0c\u8fd8\u6709cross validation, \u4e5f\u8981\u6ce8\u610f\u6bcf\u4e2afold\u7684\u5206\u6cd5."}}