{"cell_type":{"ee4d4820":"code","00f5a0bb":"code","ff51f168":"code","752a7e9c":"code","058f7ae2":"code","51f75f46":"code","9d124cce":"code","7c76b10e":"code","86fd698a":"code","80cffb5c":"code","d496854f":"code","e5c843a8":"code","cc2146f7":"code","0d48b4d8":"code","5cd4c0d6":"code","59248dfc":"code","2e1f0581":"code","e2125567":"code","e07c2068":"code","dc2531ce":"code","c9828168":"code","a2e4962b":"code","c155c11e":"code","e37e122e":"code","2c49357d":"code","54c79490":"code","6693a4f9":"code","94e4fadc":"code","3a81c44d":"code","fe32adf8":"code","6e574bca":"code","feac6713":"markdown","e5efaa99":"markdown","bce67a24":"markdown","efeebafc":"markdown","d18428a7":"markdown","6042372c":"markdown","d5fde507":"markdown","4a4a923b":"markdown","ce06b9f9":"markdown","a6cb9c94":"markdown","08572f23":"markdown","a19af48e":"markdown","0578de43":"markdown","0ab40929":"markdown","4796b632":"markdown","5d9bfce8":"markdown","83357ecd":"markdown","8df0b3e2":"markdown","62c7d075":"markdown","19f686c4":"markdown"},"source":{"ee4d4820":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","00f5a0bb":"train = pd.read_csv(\"..\/input\/jobathon-may-2021-credit-card-lead-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/jobathon-may-2021-credit-card-lead-prediction\/test.csv\")\ntrain","ff51f168":"train.info()","752a7e9c":"train.describe(include='all')","058f7ae2":"train.isna().sum()","51f75f46":"## for this case, i will delete the null value\n\ntrain = train[~train['Credit_Product'].isna()]\ntrain.isna().sum()","9d124cce":"## Drop Feature which are not used\ntrain = train.drop(['ID'],axis=1)\ntrain = pd.DataFrame(train.reset_index(drop=True))\ntrain.head()","7c76b10e":"# Visualizing Categorical features\n# Note we have filled the missng 'Credit_Product' with 'Missing' for sake of visualization\ncat_features = ['Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active']\n\nplt.figure(figsize=(16, 14))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(3, 2, i+1)\n    sns.countplot(data=train, x=feature, hue='Is_Lead', palette='rainbow')\n    if feature == 'Region_Code':\n        plt.xticks(rotation=90)\n    \nsns.despine()","86fd698a":"# It was found that the age could be dividen into age groups\nplt.figure(figsize=(16, 7))\ntemp = train.copy()\n\nsns.countplot(data=temp, x='Age', hue='Is_Lead', palette='autumn')\n\nplt.show()","80cffb5c":"#We shall now plot the numberical variables to look at the distribution\nnumerical = ['Age','Vintage','Avg_Account_Balance']\nsns.pairplot(data=train,x_vars=numerical, hue = 'Is_Lead', palette='Set2')","d496854f":"# We shall log trasform the variables and plot again\ntemp = train.copy()\ntemp[numerical] = np.log(train[numerical])\nsns.pairplot(data=temp,x_vars=numerical, hue = 'Is_Lead', palette='Set2')","e5c843a8":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nX = np.zeros((len(train['Gender']),1))\nfor i, name in enumerate(cat_features):\n    x = label_encoder.fit_transform(train[name]).reshape(-1,1)\n    X = np.hstack((X,x))\n\nX = pd.DataFrame(X).drop([0],axis=1)\nX.columns = cat_features\nfor i, name in enumerate(numerical):\n    if name == 'Avg_Account_Balance':\n        X = pd.concat([X,np.log(train[name])],axis=1)\n    else:\n        X = pd.concat([X,train[name]],axis=1)\ndata = pd.concat([X,train['Is_Lead']],axis=1)\nY = data.iloc[:,-1:]","cc2146f7":"sns.countplot(x = \"Is_Lead\",data = data)","0d48b4d8":"X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.25, stratify=Y,\n                                                    random_state=123)","5cd4c0d6":"scaler = StandardScaler()\nscaled_numfeats_train = pd.DataFrame(scaler.fit_transform(X_train[numerical]), \n                                     columns=numerical, index= X_train.index)\nfor col in numerical:\n    X_train[col] = scaled_numfeats_train[col]\n    \nscaled_numfeats_test = pd.DataFrame(scaler.transform(X_test[numerical]),\n                                    columns=numerical, index= X_test.index)\n\nfor col in numerical:\n    X_test[col] = scaled_numfeats_test[col]","59248dfc":"from imblearn.over_sampling import SMOTENC\nsmote_nc = SMOTENC(categorical_features=list(range(0,(len(X_train.columns)-len(numerical)))), random_state=0)\nX_train, y_train = smote_nc.fit_resample(X_train, y_train)","2e1f0581":"y_train.value_counts()","e2125567":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV","e07c2068":"key = ['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier',\n       'GradientBoostingClassifier','AdaBoostClassifier','XGBClassifier']\nvalue = [LogisticRegression(),KNeighborsClassifier(algorithm = 'kd_tree', n_jobs = 1, n_neighbors = 1, weights = 'uniform'),\n         SVC(C=.5, gamma = 0.1,kernel = 'rbf'),\n         DecisionTreeClassifier(),RandomForestClassifier(n_estimators = 1000),GradientBoostingClassifier(),AdaBoostClassifier(),xgb.XGBClassifier()]\nmodels = dict(zip(key,value))\nprint(models)","dc2531ce":"predicted =[]\nfor name,algo in models.items():\n    model=algo\n    model.fit(X_train,y_train)\n    predict = model.predict(X_test)\n    acc = accuracy_score(y_test, predict)\n    predicted.append(acc)\n    print(name,acc)","c9828168":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout","a2e4962b":" #creating model\nmodel = Sequential()\n\nneuron_hidden = [128,64,64,1]\nact_func = ['relu','relu','relu','sigmoid']\n\nfor i in range(len(neuron_hidden)):\n    if i == 0:\n        model.add(Dense(neuron_hidden[0], input_dim=X_train.shape[1], activation=act_func[0]))\n    else:\n        model.add(Dense(neuron_hidden[i], activation=act_func[i])),\n        model.add(Dropout(0.4))\n\n######### compile the keras model #########\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#model.fit(X, y, epochs=150, batch_size=10)\nhistory = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=100, batch_size=2000, verbose=0, shuffle = False)\n_,acc_test = model.evaluate(X_test,y_test, verbose = 0)\nprint('acc: ',acc_test)","c155c11e":"key.append('ANN')\npredicted.append(acc_test)","e37e122e":"plt.figure(figsize = (10,5))\nsns.barplot(x = predicted, y = key)","2c49357d":"model = xgb.XGBClassifier()\nmodel.fit(X_train,y_train)\npredict = model.predict(X_test)\nacc = accuracy_score(y_test, predict)\nprint('XGB Accuracy: ',acc)","54c79490":"test.isna().sum()","6693a4f9":"## for this case, i will delete the null value\n\ntest = test[~test['Credit_Product'].isna()]\ntest.isna().sum()","94e4fadc":"## Drop Feature which are not used\ntest = test.drop(['ID'],axis=1)\ntest = pd.DataFrame(test.reset_index(drop=True))\ntest.head()","3a81c44d":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nX_unseen = np.zeros((len(test['Gender']),1))\nfor i, name in enumerate(cat_features):\n    x_unseen = label_encoder.fit_transform(test[name]).reshape(-1,1)\n    X_unseen = np.hstack((X_unseen,x_unseen))\n\nX_unseen = pd.DataFrame(X_unseen).drop([0],axis=1)\nX_unseen.columns = cat_features\nfor i, name in enumerate(numerical):\n    if name == 'Avg_Account_Balance':\n        X_unseen = pd.concat([X_unseen,np.log(test[name])],axis=1)\n    else:\n        X_unseen = pd.concat([X_unseen,test[name]],axis=1)\nX_unseen.head()","fe32adf8":"scaled_numfeats_unseen = pd.DataFrame(scaler.fit_transform(X_unseen[numerical]), \n                                      columns=numerical, index= X_unseen.index)\nfor col in numerical:\n    X_unseen[col] = scaled_numfeats_unseen[col]\nX_unseen.head()","6e574bca":"predicted = model.predict(X_unseen).reshape(-1,1)\nfinal = pd.concat([test,pd.DataFrame(predicted,columns = ['Is_Lead'])],axis=1)\nfinal","feac6713":"## Standarize Before Predict the Unseen Data","e5efaa99":"### Train - Test Split","bce67a24":"## Machine Learning Model","efeebafc":"### Balancing data","d18428a7":"## Check Null Value and Handling","6042372c":"## Deep Neural Network","d5fde507":"### Check Distribution of Data","4a4a923b":"Firstly, we will convert categorical to numerical data","ce06b9f9":"## Prediction 'Is_Lead' of Unseen Data","a6cb9c94":"## Dataset Information","08572f23":"## Data Preprocessing","a19af48e":"## Results","0578de43":"### Standarize Data","0ab40929":"We must balancing the data firstly","4796b632":"We should apply log transformation in \"Avg_Account_Balance\"","5d9bfce8":"## Load Dataset","83357ecd":"## Preprocessing Data Before Predict the Unseen Data","8df0b3e2":"### Check and Handling the Missing Value","62c7d075":"## Prediction Using XGB","19f686c4":"## Visuzalization"}}