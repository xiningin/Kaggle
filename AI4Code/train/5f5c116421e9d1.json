{"cell_type":{"e8237449":"code","53803a9d":"code","70881d1a":"code","0bd93aa8":"code","4d12e9eb":"code","5ca5796d":"code","cd77c3a1":"code","dd2e5616":"code","f738d6e8":"code","2d4c0743":"code","9d120c65":"code","444b343e":"code","b838da44":"code","503e57dd":"code","5452b8b0":"code","a1025118":"code","d741cc88":"code","ec03e814":"code","470e3172":"code","e1ff2ac1":"code","72565108":"code","165e8f08":"code","27e35659":"code","b5131822":"code","60c60c3e":"code","535a82c0":"code","167080d7":"code","17ba951c":"code","f5089d75":"code","9c9e1028":"code","91276951":"code","2f339c94":"code","0aa89cba":"code","7454fde6":"code","b1ac7aa3":"code","1a675535":"code","e19c7aa3":"code","fc3168fe":"code","8ace38f0":"code","1cbb8cfe":"code","bf42f4bf":"code","640c0ebd":"code","34a7f76f":"markdown","aa76b51f":"markdown","e2b32b9b":"markdown","7ab48ca6":"markdown","e750a473":"markdown","3b4876c6":"markdown","d403e704":"markdown","22fb418d":"markdown","71f3eb23":"markdown","7b2d1b7b":"markdown","54a45da3":"markdown","d7bd86b3":"markdown","2a73a3fd":"markdown","e006c753":"markdown","81a0007e":"markdown","2181e4d0":"markdown","c77f5adb":"markdown"},"source":{"e8237449":"import os\nimport math\nimport random\nimport time\nimport glob\nimport re\nimport gc; gc.enable()\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nfrom tqdm import tqdm\nimport gc\n\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Lasso\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\n# from torch.utils.data import Dataset\n# from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset, SequentialSampler, DataLoader\n\n\nimport transformers\nfrom transformers import get_cosine_schedule_with_warmup\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n\nimport gc\ngc.enable()\n\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D , GlobalMaxPooling1D , concatenate , Flatten\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.models import Model,load_model,save_model , model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nfrom transformers import TFBertModel, BertTokenizerFast , BertTokenizer , RobertaTokenizerFast , TFRobertaModel , RobertaConfig , TFAutoModel , AutoTokenizer\n\n","53803a9d":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")","70881d1a":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\/\"\nTOKENIZER_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\/\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n\ntrain_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\nconfig = {\n    'batch_size': BATCH_SIZE,\n    'max_len': MAX_LEN,\n    'nfolds':10,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n    \n\ndef predict(model, data_loader):\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result\n\ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    #tokenizer = AutoTokenizer.from_pretrained('..\/input\/clrp-roberta-base\/clrp_roberta_base')\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n    \n    ds = CLRPDataset(df, tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n    \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)\n\n\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=10,C=10,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    preds_ridge = np.zeros((X_test.shape[0]))\n    preds_lasso = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR(C=5,kernel=kernel,gamma='auto')\n        model_ridge = Ridge(alpha=20)\n        model_lasso = Lasso(alpha=0.05)\n\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        model_ridge.fit(X_train, y_train)\n        model_lasso.fit(X_train,y_train)\n        \n        prediction = model.predict(X_valid)\n        pred_ridge = model_ridge.predict(X_valid)\n        pred_lasso = model_lasso.predict(X_valid)\n        \n        pred_mean = (prediction + pred_ridge + pred_lasso)\/3\n        \n        #score = rmse_score(prediction,y_valid)\n        score = rmse_score(y_valid, pred_mean)\n        print(f'Fold {k} , rmse score: {score}')\n        \n        scores.append(score)\n        preds += model.predict(X_test)\n        preds_ridge += model_ridge.predict(X_test)\n        preds_lasso += model_lasso.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return (np.array(preds)\/nfolds + np.array(preds_ridge)\/nfolds + np.array(preds_lasso)\/nfolds)\/3","0bd93aa8":"train_data = pd.read_csv('..\/input\/commonlit-train-dataset\/train_stratiKfold.csv')\n\ntrain_data.drop(train_data[(train_data.target == 0) & (train_data.standard_error == 0)].index,\n              inplace=True)\ntrain_data.reset_index(drop=True, inplace=True)\n\nNUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nSEED = 1000\n\n\n# kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n\n# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_data)): \n    \n#     print(\"********\",fold,\"********\")\n#     train_data.loc[val_indices, 'fold'] = fold\n#     # traindf1,val_df1 = train_df.iloc[train_indices],train_df.iloc[val_indices]\n\ntrain_data.head()","4d12e9eb":"from sklearn.linear_model import Lasso\n\ndef run():\n    preds = []\n    scores = []\n\n    svmpreds_list = []\n    ridgepreds_list = []\n    lassopreds_list = []\n\n    for fold in range(5):\n\n        predssvm = np.zeros((test_df.shape[0]))\n        predsridge = np.zeros((test_df.shape[0]))\n        predslasso= np.zeros((test_df.shape[0]))\n\n        print('fold  :  ',fold)\n        X_train = train_data[train_data[\"kfold\"] != fold]\n        y_train = train_data[train_data[\"kfold\"] != fold]['target']\n        X_valid = train_data[train_data[\"kfold\"] == fold]\n        y_valid = train_data[train_data[\"kfold\"] == fold]['target']\n\n        train_embeddings = get_embeddings(X_train,f'..\/input\/roberta-base-20210730175534-stk\/model_{fold + 1}.pth')\n        valid_embeddings = get_embeddings(X_valid,f'..\/input\/roberta-base-20210730175534-stk\/model_{fold + 1}.pth')\n        test_embeddings = get_embeddings(test_data,f'..\/input\/roberta-base-20210730175534-stk\/model_{fold + 1}.pth')\n\n\n#         model = SVR(C=5,kernel='rbf',gamma='auto')\n        model_ridge = Ridge(alpha=20)\n        model_lasso = Lasso(alpha=0.05)\n#         model_xgb = XGBRegressor(booster = 'gblinear',lamdba = 2)#min_child_weight=0.5\n\n#         model.fit(train_embeddings,y_train)\n        model_ridge.fit(train_embeddings,y_train)\n        model_lasso.fit(train_embeddings,y_train)\n\n#         prediction_svm = model.predict(valid_embeddings)\n        prediction_ridge = model_ridge.predict(valid_embeddings)\n        prediction_lasso = model_lasso.predict(valid_embeddings)\n\n    #     preds += model.predict(X_test)\n    #     preds_ridge += model_ridge.predict(X_test)\n\n#         pred_mean = (prediction_svm + prediction_ridge)\/2\n        pred_mean = (prediction_ridge + prediction_lasso)\/2\n        score = rmse_score(y_valid, pred_mean)\n\n        preds.append(pred_mean)\n\n        score = rmse_score(y_valid, pred_mean)\n        scores.append(score)\n        print(f'fold {fold} score is  : ',score)\n        print(scores)\n\n#         predssvm += model.predict(test_embeddings)\n        predsridge += model_ridge.predict(test_embeddings)\n        predslasso += model_lasso.predict(test_embeddings)\n\n#         svmpreds_list.append(predssvm)\n        ridgepreds_list.append(predsridge)\n        lassopreds_list.append(predslasso)\n        \n    print('mean  :  ',np.array(scores).mean())\n    \n    return (np.array(ridgepreds_list).mean(axis=0) + np.array(lassopreds_list).mean(axis=0))\/2\n#     return (np.array(svmpreds_list).mean(axis=0) + np.array(ridgepreds_list).mean(axis=0) + np.array(lassopreds_list).mean(axis=0))\/3","5ca5796d":"roberta_lassoridge_pred = run()\nroberta_lassoridge_pred\n","cd77c3a1":"import yaml, gc\nfrom pathlib import Path\nfrom transformers import T5EncoderModel\n\nclass CommonLitModel(nn.Module):\n    def __init__(self):\n        super(CommonLitModel, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = T5EncoderModel.from_pretrained(cfg.MODEL_PATH, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n\n\nBASE_PATH = Path('\/kaggle\/input\/commonlit-t5-large')\nDATA_PATH = Path('\/kaggle\/input\/commonlitreadabilityprize\/')\nassert DATA_PATH.exists()\nMODELS_PATH = Path(BASE_PATH\/'best_models')\nassert MODELS_PATH.exists()\n\nclass Config(): \n    NUM_FOLDS = 6\n    NUM_EPOCHS = 3\n    BATCH_SIZE = 16\n    MAX_LEN = 248\n    MODEL_PATH = BASE_PATH\/'lm'\n#     TOKENIZER_PATH = str(MODELS_PATH\/'roberta-base-0')\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    SEED = 1000\n    NUM_WORKERS = 2\n    MODEL_FOLDER = MODELS_PATH\n    model_name = 't5-large'\n    svm_kernels = ['rbf']\n    svm_c = 5\n\ncfg = Config()\n\ntrain_df['normalized_target'] = (train_df['target'] - train_df['target'].mean()) \/ train_df['target'].std()\n\nmodel_path = MODELS_PATH\nassert model_path.exists()","dd2e5616":"!ls {MODELS_PATH}","f738d6e8":"from transformers import T5EncoderModel\n\nclass AttentionHead(nn.Module):\n    \n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        \n        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n        self.final_layer = nn.Linear(hidden_dim, num_targets)\n        self.out_features = hidden_dim\n        \n    def forward(self, features):\n        att = torch.tanh(self.hidden_layer(features))\n        score = self.final_layer(att)\n        attention_weights = torch.softmax(score, dim=1)\n        return attention_weights\n\n\n\nclass CommonLitModel(nn.Module):\n    def __init__(self):\n        super(CommonLitModel, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = T5EncoderModel.from_pretrained(cfg.MODEL_PATH, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n    \ndef load_model(i):\n    inference_model = CommonLitModel()\n    inference_model = inference_model.cuda()\n    inference_model.load_state_dict(torch.load(str(model_path\/f'{i + 1}_pytorch_model.bin')))\n    inference_model.eval();\n    return inference_model\n\ndef convert_to_list(t):\n    return t.flatten().long()\n\nclass CommonLitDataset(nn.Module):\n    def __init__(self, text, test_id, tokenizer, max_len=128):\n        self.excerpt = text\n        self.test_id = test_id\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return {'input_ids': convert_to_list(encode['input_ids']),\n                'attention_mask': convert_to_list(encode['attention_mask']),\n                'id': self.test_id[idx]}\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \ndef create_dl(df, tokenizer):\n    text = df['excerpt'].values\n    ids = df['id'].values\n    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n    return DataLoader(ds, \n                      batch_size = cfg.BATCH_SIZE,\n                      shuffle=False,\n                      num_workers = 1,\n                      pin_memory=True,\n                      drop_last=False\n                     )\n\n\ndef get_cls_embeddings(dl, transformer_model):\n    cls_embeddings = []\n    with torch.no_grad():\n        for input_features in tqdm(dl, total=len(dl)):\n            _, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n            embedding_out = context_vector.detach().cpu().numpy()\n            cls_embeddings.extend(embedding_out)\n    return np.array(cls_embeddings)\n\n\ndef rmse_score(X, y):\n    return np.sqrt(mean_squared_error(X, y))\n\ndef calc_mean(scores):\n    return np.mean(np.array(scores), axis=0)","2d4c0743":"!ls {MODELS_PATH}\/tokenizer-1","9d120c65":"from transformers import T5Tokenizer\n\ntokenizers = []\nfor i in range(1, cfg.NUM_FOLDS):\n    tokenizer_path = MODELS_PATH\/f\"tokenizer-{i}\"\n    print(tokenizer_path)\n    assert(Path(tokenizer_path).exists())\n    tokenizer = T5Tokenizer.from_pretrained(str(tokenizer_path))\n    tokenizers.append(tokenizer)\n    \nnum_bins = int(np.ceil(np.log2(len(train_df))))\ntrain_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\nbins = train_df['bins'].values","444b343e":"%%time\n\ntrain_target = train_df['normalized_target'].values\n\neach_mean_score = []\neach_fold_score = []\n\n\n\n# for c in C:\n\nfinal_scores_svm = []\nfinal_scores_ridge = []\nfinal_scores_lasso = []\nfinal_rmse = []\n\nfor j, tokenizer in enumerate(tokenizers):\n    print('Model', j)\n    test_dl = create_dl(test_df, tokenizer)\n    train_dl = create_dl(train_df, tokenizer)\n    transformer_model = load_model(j)\n    transformer_model.cuda()\n    X = get_cls_embeddings(train_dl, transformer_model)\n    y = train_target\n    X_test = get_cls_embeddings(test_dl, transformer_model)\n    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n    \n    scores = []\n    scores_svm = []\n    scores_ridge = []\n    scores_lasso = []\n    \n    rmse_scores_svm = []\n    rmse_scores_ridge = []\n    rmse_scores_lasso = []\n    rmse_scores = []\n    \n    for kernel in cfg.svm_kernels:\n        print('Kernel', kernel)\n        kernel_scores_svm = []\n        kernel_scores_ridge = []\n        kernel_scores_lasso = []\n        kernel_scores = []\n        kernel_rmse_scores = []\n        \n        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n\n            print('Fold', k, train_idx.shape, valid_idx.shape)\n            model_svm = SVR(C=5, kernel=kernel, gamma='auto')\n            model_ridge = Ridge(alpha=20)\n            model_lasso = Lasso(alpha=0.05)\n#                 model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_valid, y_valid = X[valid_idx], y[valid_idx]\n        \n            model_svm.fit(X_train, y_train)\n            model_ridge.fit(X_train, y_train)\n#             model_lasso.fit(X_train,y_train)\n\n            pred_svm = model_svm.predict(X_valid)\n            pred_ridge = model_ridge.predict(X_valid)\n#             pred_lasso = model_lasso.predict(X_valid)\n\n            prediction = (pred_svm + pred_ridge)\/2\n#             prediction = (pred_svm + pred_ridge + pred_lasso)\/3\n\n            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n            print('rmse_score', kernel_rmse_scores[k])\n            \n            kernel_scores_svm.append(model_svm.predict(X_test))\n            kernel_scores_ridge.append(model_ridge.predict(X_test))\n#             kernel_scores_lasso.append(model_lasso.predict(X_test))\n            \n#             score_mean = (np.array(kernel_scores_svm) + np.array(kernel_scores_ridge) + np.array(kernel_scores_lasso))\/3\n            score_mean = (np.array(kernel_scores_svm) + np.array(kernel_scores_ridge))\/2\n#             score_mean = (np.array(kernel_scores_ridge) + np.array(kernel_scores_lasso))\/2\n            kernel_scores.append(score_mean)\n            \n#         scores.append(calc_mean(kernel_scores))\n        scores_svm.append(calc_mean(kernel_scores_svm))\n        scores_ridge.append(calc_mean(kernel_scores_ridge))\n#         scores_lasso.append(calc_mean(kernel_scores_ridge))\n        rmse_scores.append(calc_mean(kernel_rmse_scores))\n        \n    final_scores_svm.append(calc_mean(scores_svm))\n    final_scores_ridge.append(calc_mean(scores_ridge))\n#     final_scores_lasso.append(calc_mean(scores_lasso))\n    \n    final_rmse.append(calc_mean(rmse_scores))\n    del transformer_model\n    torch.cuda.empty_cache()\n    del tokenizer\n    gc.collect()\nprint('FINAL RMSE score', np.mean(np.array(final_rmse)))\n\neach_mean_score.append(np.mean(np.array(final_rmse)))\neach_fold_score.append(final_rmse)\n    \nprint('BEST SCORE : ',each_mean_score)\nprint('FOLD SCORE : ',each_fold_score)","b838da44":"final_rmse","503e57dd":"# final_scores = (np.array(final_scores_svm)+np.array(final_scores_ridge) + np.array(final_scores_lasso))\/3\nfinal_scores = (np.array(final_scores_svm)+np.array(final_scores_ridge))\/2\nfinal_scores2 = final_scores * train_df['target'].std() + train_df['target'].mean()\n\ntarget_mean = train_df['target'].mean()\nt5_embedding_pred = calc_mean(final_scores2).flatten()","5452b8b0":"t5_embedding_pred","a1025118":"calc_mean(final_scores2)","d741cc88":"del final_scores,train_target,tokenizers,model_svm,model_ridge,X_train,X_valid,y_valid\ngc.collect","ec03e814":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n\n### nishipy\u306b\u5408\u308f\u305b\u3066target and std = 0\u3092\u6d88\u3059\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\nroberta_df = pd.read_csv('..\/input\/nishipyroberta-dataset\/nishipy_roberta_stacking.csv')\nrobertalarge_df = pd.read_csv('..\/input\/nishipy-robertalarge-dataset\/nishipy_robertalarge_stacking.csv')\nmeanpooling_df = pd.read_csv('..\/input\/meanpoolingdataset\/meanpooling_stackingdata.csv')\n\n# robertamean_df = pd.read_csv('..\/input\/meanpoolingrobertabasedataset\/meanpooling_roberta_stacking.csv')\n# ## \u5916\u308c\u5024\u524a\u9664\n# robertamean_df = robertamean_df[robertamean_df['id'] != '436ce79fe'].reset_index()\n\n\nroberta_pred = roberta_df['meanpooling pred']\nrobertalarge_pred = robertalarge_df['meanpooling pred']\nmeanpooling_pred = meanpooling_df['meanpooling pred']\n# robertamean_pred = robertamean_df['meanpooling roberta pred']","470e3172":"stacking_df = train_df.drop(['url_legal','license','excerpt','standard_error'],axis = 1)\n# stacking_df[['roberta_pred','robertalarge_pred','meanpooling_pred']] = \n\nstacking_df['roberta_pred'] = roberta_pred\nstacking_df['robertalarge_pred'] = robertalarge_pred\nstacking_df['meanpooling_pred'] = meanpooling_pred\n# stacking_df['robertamean_pred'] = robertamean_pred\n","e1ff2ac1":"import seaborn as sns\n\nsns.distplot(stacking_df['roberta_pred'], bins=20, color='red')\nsns.distplot(stacking_df['robertalarge_pred'], bins=20, color='green')\nsns.distplot(stacking_df['meanpooling_pred'], bins=20, color='yellow')\n# sns.distplot(stacking_df['robertamean_pred'], bins=20, color='blue')\n# sns.displot()","72565108":"import numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nprint('nishipy roberta base RMSE is : ',np.sqrt(mean_squared_error(stacking_df['target'], stacking_df['roberta_pred'])))\nprint('nishipy roberta large RMSE is : ',np.sqrt(mean_squared_error(stacking_df['target'], stacking_df['robertalarge_pred'])))\nprint('cpptake roberta large RMSE is : ',np.sqrt(mean_squared_error(stacking_df['target'], stacking_df['meanpooling_pred'])))\n# print('cpptake roberta base RMSE is : ',np.sqrt(mean_squared_error(stacking_df['target'], stacking_df['robertamean_pred'])))","165e8f08":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\/\"\nTOKENIZER_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\/\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n\nclass LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)\n        \nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \n\ndef predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","27e35659":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(5):\n    #CHANGEME\n    model_path = f\"..\/input\/roberta-base-20210711202147-sche\/model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()\n    \n### nishipy_roberta \u306bNumpy\u5f62\u5f0f\u3067\u7d50\u679c\u3092\u4fdd\u5b58\nnishipy_roberta = all_predictions.mean(axis=0)\n# nishipy_roberta.target = predictions","b5131822":"del all_predictions,test_dataset,test_loader,tokenizer\ngc.collect()","60c60c3e":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/roberta-large-20210712191259-mlm\/clrp_roberta_large\"\nTOKENIZER_PATH = \"..\/input\/roberta-large-20210712191259-mlm\/clrp_roberta_large\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n\n# nishipy_robertalarge = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")","535a82c0":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)\n        \nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n        #https:\/\/towardsdatascience.com\/attention-based-deep-multiple-instance-learning-1bb3df857e24\n        # 768: node fully connected layer\n        # 512: node attention layer\n        # self.attention = nn.Sequential(            \n        #     nn.Linear(768, 512),            \n        #     nn.Tanh(),                       \n        #     nn.Linear(512, 1),\n        #     nn.Softmax(dim=1)\n        # )        \n\n        # self.regressor = nn.Sequential(                        \n        #     nn.Linear(768, 1)                        \n        # )\n\n        #768 -> 1024\n        #512 -> 768\n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 768),            \n            nn.Tanh(),                       \n            nn.Linear(768, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n\ndef predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","167080d7":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(5):\n    #CHANGEME\n    model_path = f\"..\/input\/roberta-large-20210720020531-sch\/model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()\n    \n    \n## nishipy_robertalarge\u306bNumpy\u5f62\u5f0f\u3067\u4fdd\u5b58\nnishipy_robertalarge = all_predictions.mean(axis=0)\n\n# predictions = all_predictions.mean(axis=0)\n# nishipy_robertalarge.target = predictions\n# # nishipy_robertalarge","17ba951c":"# nishipy_robertalarge","f5089d75":"\ndel all_predictions,test_dataset,test_loader,tokenizer\ngc.collect()","9c9e1028":"\nSEED = 42\n\nHIDDEN_SIZE = 1024\nMAX_LEN = 300\n\nINPUT_DIR = '..\/input\/commonlitreadabilityprize'\n# BASELINE_DIR = '..\/input\/commonlit-readability-models\/'\nBASELINE_DIR = '..\/input\/robertalargemeanpooling'\nMODEL_DIR = '..\/input\/roberta-transformers-pytorch\/roberta-large'\n\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_DIR)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 8\n\n\n# robertalarge_meanpool = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n# test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n# test.head()\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    \nseed_everything(SEED)\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        encode = self.tokenizer(\n            self.texts[idx],\n            padding='max_length',\n            max_length=MAX_LEN,\n            truncation=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ) \n        return encode\n\n\nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \ndef predict(df, model):\n    \n    ds = CLRPDataset(df.excerpt.tolist(), TOKENIZER)\n    dl = DataLoader(\n        ds,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        pin_memory=False\n    )\n    \n    model.to(DEVICE)\n    model.eval()\n    model.zero_grad()\n    \n    predictions = []\n    for batch in tqdm(dl):\n        inputs = {key:val.reshape(val.shape[0], -1).to(DEVICE) for key,val in batch.items()}\n        outputs = model(**inputs)\n        predictions.extend(outputs.detach().cpu().numpy().ravel())\n        \n    return predictions","91276951":"\nfold_predictions = []\n# modelpath = glob.glob(BASELINE_DIR)\nmodelpath = glob.glob(BASELINE_DIR+\"\/*\")\n#\u4f59\u8a08\u306a\u30c7\u30fc\u30bf\u3092\u524a\u9664\nmodelpath.remove('..\/input\/robertalargemeanpooling\/experiment-1.csv')\n\n# for path in glob.glob(BASELINE_DIR + '\/*.ckpt'):\nfor path in modelpath:\n    print(path)\n    model = MeanPoolingModel(MODEL_DIR)\n    model.load_state_dict(torch.load(path))\n#     fold = int(re.match(r'.*_f(\\d)_.*', path).group(1))\n#     print(f'*** fold {fold} ***')\n#     y_pred = predict(test, model)\n    y_pred = predict(test_df, model)\n    fold_predictions.append(y_pred)\n    \n    # Free memory\n    del model\n    gc.collect()\n\n\nrobertalarge_meanpool = np.mean(fold_predictions, axis=0)\n\n\ndel fold_predictions,modelpath,y_pred,TOKENIZER\ngc.collect()","2f339c94":"# # test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n# # submission_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n\n# config = {\n#     'lr': 2e-5,\n#     'wd':0.01,\n#     'batch_size':16,\n#     'valid_step':10,\n#     'max_len':256,\n#     'epochs':3,\n#     'nfolds':5,\n#     'seed':42,\n#     'model_path':'..\/input\/clrp-roberta-base\/clrp_roberta_base',\n# }\n\n# # for i in range(config['nfolds']):\n# #     os.makedirs(f'model{i}',exist_ok=True)\n\n# def seed_everything(seed=42):\n#     random.seed(seed)\n#     os.environ['PYTHONASSEED'] = str(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = True\n\n# seed_everything(seed=config['seed'])","0aa89cba":"# class CLRPDataset(Dataset):\n#     def __init__(self,df,tokenizer):\n#         self.excerpt = df['excerpt'].to_numpy()\n#         self.tokenizer = tokenizer\n    \n#     def __getitem__(self,idx):\n#         encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n#                                 max_length=config['max_len'],\n#                                 padding='max_length',truncation=True)\n#         return encode\n    \n#     def __len__(self):\n#         return len(self.excerpt)\n    \n    \n    \n# ### mean pooling \n# class Model(nn.Module):\n#     def __init__(self,path):\n#         super(Model,self).__init__()\n#         self.config = AutoConfig.from_pretrained(path)\n#         self.config.update({'output_hidden_states':True,\"hidden_dropout_prob\": 0.0})\n#         self.roberta = AutoModel.from_pretrained(path,config=self.config)  \n# #         self.linear = nn.Linear(self.config.hidden_size*4, 1, 1)\n#         self.linear = nn.Linear(self.config.hidden_size, 1)\n#         self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n\n#     def forward(self,**xb):\n#         attention_mask = xb['attention_mask']\n#         outputs = self.roberta(**xb)\n#         last_hidden_state = outputs[0]\n#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n#         sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n#         sum_mask = input_mask_expanded.sum(1)\n#         sum_mask = torch.clamp(sum_mask, min=1e-9)\n#         mean_embeddings = sum_embeddings \/ sum_mask\n#         norm_mean_embeddings = self.layer_norm(mean_embeddings)\n#         logits = self.linear(norm_mean_embeddings)\n        \n#         preds = logits.squeeze(-1).squeeze(-1)\n#         return preds.view(-1).float()\n    \n    \n# def get_prediction(df,path,model_path,device='cuda'):        \n#     model = Model(model_path)\n#     model.load_state_dict(torch.load(path,map_location=device))\n#     model.to(device)\n#     model.eval()\n    \n#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n#     test_ds = CLRPDataset(df,tokenizer)\n#     test_dl = DataLoader(test_ds,\n#                         batch_size = config[\"batch_size\"],\n#                         shuffle=False,\n#                         num_workers = 4,\n#                         pin_memory=True)\n    \n#     predictions = list()\n#     for i, (inputs) in tqdm(enumerate(test_dl)):\n#         inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n#         outputs = model(**inputs)\n#         outputs = outputs.cpu().detach().numpy().ravel().tolist()\n#         predictions.extend(outputs)\n        \n#     torch.cuda.empty_cache()\n    \n#     del model,tokenizer\n#     gc.collect\n    \n#     return np.array(predictions)","7454fde6":"# all_predictions = np.zeros((5, len(test_df)))\n\n# for fold in range(5):\n#     #CHANGEME\n#     path = f\"..\/input\/robertabasemeanpooling\/model{fold}\/model{fold}.bin\"\n#     model_path = '..\/input\/clrp-roberta-base\/clrp_roberta_base'\n#     print(f\"\\nUsing  {path} \")\n                        \n# #     model = Model('..\/input\/clrp-roberta-base\/clrp_roberta_base')\n# #     model.load_state_dict(torch.load(model_path))\n# #     model.to(DEVICE)\n    \n#     pred = get_prediction(test_df,path,model_path)\n#     all_predictions[fold] = pred\n#     gc.collect()\n    \n# robertabase_meanpool = all_predictions.mean(axis=0)","b1ac7aa3":"# robertabase_meanpool","1a675535":"# del all_predictions,pred","e19c7aa3":"submission_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsubmission_df = submission_df.drop('target',axis = 1)\n\nsubmission_df['roberta_pred'] = nishipy_roberta\nsubmission_df['robertalarge_pred'] = nishipy_robertalarge\nsubmission_df['large_meanpooling_pred'] = robertalarge_meanpool\n# submission_df['roberta_meanpooling_pred'] = robertabase_meanpool\n\nsubmission_df","fc3168fe":"y_train = stacking_df.target\nX_train = stacking_df.drop(['id','target'],axis = 1)\nX_test = submission_df.drop('id',axis = 1)\n# y_test = submission_df.shape[0]","8ace38f0":"y_train.shape","1cbb8cfe":"X_train.shape","bf42f4bf":"from sklearn.linear_model import LinearRegression\n\nstacking_model = SVR(kernel='linear', gamma='auto')#'linear'\n# stacking_model = LinearRegression()\n\nstacking_model.fit(X_train, y_train)\n\nstacking_pred = stacking_model.predict(X_test)\nstacking_pred","640c0ebd":"submission_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\npredictions = pd.DataFrame()\n# predictions = y_test * 0.6 + svm_ridge_preds * 0.2 + roberta_svm_ridge_preds * 0.2\n\n## ver10 t5_large_svm\u5229\u7528\n# predictions = y_test * 0.6 + svm_ridge_pred * 0.2 + roberta_svm_ridge_preds * 0.2 \n\n# ## ver12 roberta meanpooling , stacking + blending + t5_large_svm + nishipy roberta svm\u5229\u7528\n# blending_pred =  (nishipy_roberta + nishipy_robertalarge + robertalarge_meanpool + robertabase_meanpool)\/3\n# predictions = stacking_pred * 0.3 + blending_pred * 0.3 + svm_ridge_pred * 0.2 + roberta_svm_ridge_preds * 0.2 \n\n## ver14 \u30a8\u30e9\u30fc\u306e\u539f\u56e0\u8abf\u67fb\u3000SVM\u30e2\u30c7\u30eb\u3092\u524a\u9664\u3057\u3066\u3000Roberta mean\u304c\u60aa\u3044\u306e\u304b\u5207\u308a\u5206\u3051\n# blending_pred =  (nishipy_roberta + nishipy_robertalarge + robertalarge_meanpool + robertabase_meanpool)\/4\n# predictions = stacking_pred * 0.3 + blending_pred * 0.3 + large_svmridge_pred * 0.2 + roberta_svmridge_pred * 0.2\n\n# ## ver17\n# blending_pred =  (nishipy_roberta + nishipy_robertalarge + robertalarge_meanpool)\/3\n# predictions = stacking_pred * 0.3 + blending_pred * 0.3 + large_svmridge_pred * 0.2 + roberta_svmridge_pred * 0.2\n\n# ## ver17\n# blending_pred =  (nishipy_roberta + nishipy_robertalarge + robertalarge_meanpool)\/3\n# predictions = stacking_pred * 0.3 + blending_pred * 0.3 + ((large_svmridge_pred + roberta_svmridge_pred + t5_embedding_pred)\/3) * 0.4\n\n## ver22\n# blending_pred =  (nishipy_roberta + nishipy_robertalarge + robertalarge_meanpool)\/3\n# predictions = stacking_pred * 0.2 + blending_pred * 0.25 + roberta_svmridge_pred * 0.2 + t5_embedding_pred * 0.35\n\n## ver22\n# blending_pred =  (nishipy_roberta + nishipy_robertalarge + robertalarge_meanpool + robertabase_meanpool)\/4\n# predictions = stacking_pred * 0.25 + blending_pred * 0.25 + ((roberta_svmridge_pred + large_svmridge_pred + t5_embedding_pred  + svm_ridge_preds)\/4) * 0.5\n\nblending_pred =  (nishipy_roberta + nishipy_robertalarge + robertalarge_meanpool)\/3\npredictions = stacking_pred * 0.25 + blending_pred * 0.25 + ((roberta_lassoridge_pred + t5_embedding_pred)\/2) * 0.5\n\nsubmission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","34a7f76f":"# Roberta base mean pooling LB:0.476","aa76b51f":"# pytorch_t5_large_svm LB:0.476\nhttps:\/\/www.kaggle.com\/gilfernandes\/commonlit-pytorch-t5-large-svm","e2b32b9b":"## release memory","7ab48ca6":"# First of all\n<br> This is our ([nishipy](https:\/\/www.kaggle.com\/iamnishipy) & [cpptake](https:\/\/www.kaggle.com\/takeshikobayashi)) first NLP competition's solution\n<br> This kernel was in the silver medal range in the public LB, but in the private LB, the shake down happened and we fall into the bronze medal range\n<br> It's frustrating, but we learned a lot, so we'll leave you with the solution\n### We'll get a better ranking in next competition!\n\n\n# Pipe line\n\n![clrp-soltion2.png](attachment:f6e0969c-bc4b-4b09-a3c0-eb13a75c2133.png)\n\n\n# Our idea\n* Using Lasso,Ridge,SVM Blending in the embedding regression\n* Using get_liner_schedule_with_warmup at the training phase\n* Blending stacking prediction\n* Blending pretrain model and unpretrain model\n<br> and so on..\n\n# Further improvement points\n* Poor diversity of the models\n* Poor number of meta feature in the stacking feateure values\n* Weak CV in Roberta large\n\n\n# reference\n* https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3\n* https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-itpt\n* https:\/\/www.kaggle.com\/andretugan\/pre-trained-roberta-solution-in-pytorch\n* https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm\n* https:\/\/www.kaggle.com\/jcesquiveld\/best-transformer-representations\n* https:\/\/www.kaggle.com\/gilfernandes\/commonlit-pytorch-t5-large-svm\n<br>Thank you [@torch](https:\/\/www.kaggle.com\/rhtsingh) [@Andrey Tuganov](https:\/\/www.kaggle.com\/andretugan) [@Maunish dave](https:\/\/www.kaggle.com\/maunish) [@jcesquivel](https:\/\/www.kaggle.com\/jcesquiveld) [@gil fernandes](https:\/\/www.kaggle.com\/gilfernandes) and the comunity for sharing great notebooks!\n\n\n### If you have any pointers, please comment here!","e750a473":"## inference","3b4876c6":"## import","d403e704":"# Stacking ","22fb418d":"# Robertalarge meanpooling CV:0.479 LB:0.468\n\n\u53c2\u7167\uff1ahttps:\/\/www.kaggle.com\/jcesquiveld\/roberta-large-5-fold-single-model-meanpooling","71f3eb23":"## train","7b2d1b7b":"## release memory","54a45da3":"## Inference\n","d7bd86b3":"# prepare stacking\n\nThis kernel is not using roberta mean df","2a73a3fd":"# RoBERTa Large CV:0.480 LB:0.468 ","e006c753":"# Emsemble","81a0007e":"# Define","2181e4d0":"# RoBERTa Base CV:0.475 LB:0.466","c77f5adb":"# roberta base embedding Lasso & Ridge CV: 0.4753 LB:0.471"}}