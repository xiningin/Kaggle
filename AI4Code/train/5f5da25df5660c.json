{"cell_type":{"23cd4d1d":"code","40224a8b":"code","891ebacd":"code","108dc9bd":"code","5f38c734":"code","1adb2889":"code","1e30742e":"code","0007d9e0":"code","f59b4dc3":"code","65453e2b":"code","1b19ccd2":"code","17cfeb3c":"code","310f0db5":"markdown","4c18cb5c":"markdown","1da00a04":"markdown","c8244525":"markdown","dd71d3c3":"markdown","78ed0f98":"markdown","1975580c":"markdown","a2757349":"markdown","e5d0126a":"markdown","098d8294":"markdown"},"source":{"23cd4d1d":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\ndata = pd.read_csv(\"\/kaggle\/input\/mobile-price-classification\/train.csv\")\nX = data.iloc[:,0:20]  #independent columns\ny = data.iloc[:,-1]    #target column i.e price range\n","40224a8b":"data.head()","891ebacd":"data.shape","108dc9bd":"# apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2 , k=10)\nfit = bestfeatures.fit(X, y )","5f38c734":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)","1adb2889":"# concatenate two dataframes for better visualizaton\nfeatureScores = pd.concat([dfcolumns , dfscores] ,axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns","1e30742e":"featureScores","0007d9e0":"featureScores.nlargest(10 , 'Score')","f59b4dc3":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)","65453e2b":"print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers","1b19ccd2":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","17cfeb3c":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","310f0db5":"Feature Selection Methods:\n\nI will share 3 Feature selection techniques that are easy to use and also gives good results.\n1. Univariate Selection\n2. Feature Importance \n3. Correlation Matrix with Heatmap","4c18cb5c":"We have discovered how to select relevant features from data using Univariate Selection technique, feature importance and correlation matrix.\n\nIf you found this kernel useful please upvote and share it with others. Thank you. ","1da00a04":"### **1. Univariate Selection**\n\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\n\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\nThe example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select 10 of the best features from the Mobile Price Range Prediction Dataset.","c8244525":"### 2. Feature Importance\nYou can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n\nFeature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n\nFeature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.","dd71d3c3":"We all may have faced this problem of identifying the related features from a set of data and removing the irrelevant or less important features with do not contribute much to our target variable in order to achieve better accuracy for our model.\n\n**Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model**. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\n\n**Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in****","78ed0f98":"Refrence :  https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e","1975580c":"Have a look at the last row i.e price range, see how the price range is correlated with other features, ram is the highly correlated with price range followed by battery power, pixel height and width while m_dep, clock_speed and n_cores seems to be least correlated with price_range.\n","a2757349":"## Feature Selection","e5d0126a":"### **3.Correlation Matrix with Heatmap**\nCorrelation states how the features are related to each other or the target variable.\n\nCorrelation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\nHeatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library.","098d8294":"![image.png](attachment:image.png)"}}