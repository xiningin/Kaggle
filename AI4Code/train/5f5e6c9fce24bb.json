{"cell_type":{"024846f9":"code","4b608ac7":"code","ad330baa":"code","5ed2df77":"code","377a58b1":"code","92689962":"code","7edcb2c8":"code","cb4ac6c0":"code","3c8e550c":"code","bdb932b1":"code","a5bac7b8":"code","ff372295":"code","cae3a40b":"code","60c50f29":"code","77ed1840":"code","33ceb01f":"code","430ab353":"code","fb8549a4":"code","ab669b28":"code","80c9dae7":"code","1f581e17":"code","1250a1c8":"code","8dc0e693":"code","d82c2f47":"code","536113fa":"code","098f9947":"code","09120598":"markdown"},"source":{"024846f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b608ac7":"! unzip \/kaggle\/input\/platesv2\/plates.zip","ad330baa":"! ls ","5ed2df77":"! pip install tf-nightly ","377a58b1":"import tensorflow as tf \ntf.__version__","92689962":"image_size = (200, 200)\nbatch_size = 10","7edcb2c8":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"plates\/train\",\n    validation_split=0.3,\n    subset=\"training\",\n    seed=1307,\n    image_size=image_size,\n    batch_size=batch_size,\n)\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"plates\/train\",\n    validation_split=0.3,\n    subset=\"validation\",\n    seed=1307,\n    image_size=image_size,\n    batch_size=batch_size,\n)","cb4ac6c0":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","3c8e550c":"from tensorflow import keras \nfrom tensorflow.keras import layers \n\ndata_augmentation = keras.Sequential(\n[\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    layers.experimental.preprocessing.RandomRotation(0.45),\n    layers.experimental.preprocessing.RandomZoom(0.1), \n    layers.experimental.preprocessing.RandomContrast(0.4)\n]\n)","bdb932b1":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")","a5bac7b8":"augmented_train_ds = train_ds.map(\n  lambda x, y: (data_augmentation(x, training=True), y)).repeat(300).shuffle(32)","ff372295":"augmented_train_ds","cae3a40b":"train_ds","60c50f29":"train_ds = train_ds.prefetch(buffer_size=32)\nval_ds = val_ds.prefetch(buffer_size=32)","77ed1840":"\ndef make_model(input_shape, num_classes):\n    inputs = keras.Input(shape=input_shape)\n    # Image augmentation block\n    x = data_augmentation(inputs)\n\n    # Entry block\n    x = layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(x)\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    for size in [128, 256, 512, 728]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    if num_classes == 2:\n        activation = \"sigmoid\"\n        units = 1\n    else:\n        activation = \"softmax\"\n        units = num_classes\n\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(units, activation=activation)(x)\n    return keras.Model(inputs, outputs)\n\n\nmodel = make_model(input_shape=image_size + (3,), num_classes=2)\nkeras.utils.plot_model(model, show_shapes=True)\n","33ceb01f":"epochs = 50\n\nmodel_cp = keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\")\nearlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-3),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory = model.fit(\n    augmented_train_ds, epochs=epochs, callbacks=[model_cp, earlystop], validation_data=val_ds\n)\n","430ab353":"def plot_history(history, epochs):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs_range = range(epochs+1)\n    plt.figure(figsize=(16, 8))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()","fb8549a4":"plot_history(history, earlystop.stopped_epoch)","ab669b28":"from keras.preprocessing.image import ImageDataGenerator\ntest_datagen = ImageDataGenerator()\ntest_generator = test_datagen.flow_from_directory(  \n        'plates',\n        classes=['test'],\n        target_size = (200, 200),\n        batch_size = 1,\n        shuffle = False,        \n        class_mode = None)  ","80c9dae7":"test_generator.reset()\npredict = model.predict_generator(test_generator, steps = len(test_generator.filenames))\nlen(predict)","1f581e17":"predict[:5]","1250a1c8":"import pandas as pd\nsub_df = pd.read_csv('..\/input\/platesv2\/sample_submission.csv')\nsub_df.head()","8dc0e693":"sub_df.label.value_counts()","d82c2f47":"sub_df['label'] = predict\nsub_df['label'] = sub_df['label'].apply(lambda x: 'dirty' if x > 0.5 else 'cleaned')\nsub_df.head()","536113fa":"sub_df.label.value_counts()","098f9947":"sub_df.to_csv('sub.csv', index=False)","09120598":"# install TF nightly "}}