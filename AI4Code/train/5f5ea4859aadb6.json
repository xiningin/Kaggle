{"cell_type":{"f10fece9":"code","e67ec741":"code","6de1bc6f":"code","189acc63":"code","49d28d80":"code","0c59edf8":"code","742dd923":"code","4f83aedc":"code","ed32c8a9":"code","371ca5d8":"code","25bb009b":"code","a8349ed4":"code","cf2980dc":"code","9a45203a":"markdown","0a2f1311":"markdown","aa39e941":"markdown","27e6d1a4":"markdown","ec466548":"markdown","1659b98e":"markdown","db40eda9":"markdown","a197f289":"markdown","978a5e30":"markdown","199e34c7":"markdown","9c5458ad":"markdown","3cee2287":"markdown","76cee6af":"markdown","ca003b38":"markdown","46a41863":"markdown","2f667716":"markdown","656375e9":"markdown","d104c837":"markdown","222fcb58":"markdown","55059c00":"markdown","b6023012":"markdown","94be7eb4":"markdown"},"source":{"f10fece9":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","e67ec741":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","6de1bc6f":"bg_clf = BaggingClassifier(verbose=VERBOSE,\n                           random_state=RANDOM_STATE,\n                           n_jobs=N_JOBS)","189acc63":"parameters = {\n    'n_estimators': [20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","49d28d80":"parameters = {\n    'max_samples': [x \/ 10 for x in range(1, 11)]\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","0c59edf8":"parameters = {\n    'max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.90, 0.92, 0.95, 1.0]\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","742dd923":"parameters = {\n    'bootstrap': [True, False]\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","4f83aedc":"parameters = {\n    'bootstrap_features': [True, False]\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","ed32c8a9":"parameters = {\n    'oob_score': [True, False]\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","371ca5d8":"parameters = {\n    'warm_start': [True, False]\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","25bb009b":"parameters = {\n    'n_estimators': [300, 400, 500, 600, 700, 800],\n    'max_features': [0.90, 0.92, 0.95, 1.0],\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False],\n}\nclf = GridSearchCV(bg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","a8349ed4":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","cf2980dc":"clf.best_estimator_","9a45203a":"The score increases as the n_estimator does too.\nFrom 200 estimators, the score estabilizes, with some noise.\nThe fit and score times incresase as well.","0a2f1311":"**Note**: Not evaluated","aa39e941":"# max_features\n##### : int or float, optional (default=1.0)\n\nThe number of features to draw from X to train each base estimator.\n- If int, then draw `max_features` features.\n- If float, then draw `max_features * X.shape[1]` features.","27e6d1a4":"There is no difference in score when using out-of-bag samples.\nInstead, the fit and score times are higher.","ec466548":"When samples are drawn with replacement, the score is higher.","1659b98e":"When features are drawn with replacement, the score is higher.","db40eda9":"# Search over parameters","a197f289":"# Prepare data","978a5e30":"# max_samples\n##### : int or float, optional (default=1.0)\n\nThe number of samples to draw from X to train each base estimator.\n- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples.","199e34c7":"## Export grid search results","9c5458ad":"# oob_score\n##### : bool, optional (default=False)\n\nWhether to use out-of-bag samples to estimate\nthe generalization error.","3cee2287":"# bootstrap_features\n##### : boolean, optional (default=False)\n\nWhether features are drawn with replacement.","76cee6af":"# Introduction\n\nThe aim of this notebook is to optimize the Logistic Regression model.\n\nFirst, all [Bagging classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","ca003b38":"More features, more score.\nBut it decays near the end.\nThe optimal point is with the 92 % of the features.","46a41863":"While with half of samples it is already a good approximation,\nthe maximum score is achieved then all samples are taken into account.","2f667716":"# Exhaustive search","656375e9":"# warm_start\n##### : bool, optional (default=False)\n\nWhen set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit\na whole new ensemble.","d104c837":"The best results are achieved with `bootstrap` `False` and `bootstrap_features` `True`.\n\nThe score is better with `max_features` 0.92 and `n_estimators` 500.","222fcb58":"# n_estimators\n##### : int, optional (default=10)\n\nThe number of base estimators in the ensemble.","55059c00":"There is no difference in score when using `warm_start`.\nInstead, the fit and score times are higher.","b6023012":"# bootstrap\n##### : boolean, optional (default=True)\n\nWhether samples are drawn with replacement. If False, sampling\nwithout replacement is performed.","94be7eb4":"# base_estimator\n##### : object or None, optional (default=None)\n\nThe base estimator to fit on random subsets of the dataset.\nIf None, then the base estimator is a decision tree."}}