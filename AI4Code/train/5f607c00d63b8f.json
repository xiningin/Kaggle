{"cell_type":{"80b71068":"code","7717c668":"code","5ce92022":"code","45720a57":"code","ec125d8e":"code","82b14949":"code","7ed80558":"code","6499075f":"code","1f2d5c9f":"code","3f9a4ce5":"code","2225aec4":"code","b9f169a5":"code","38da030c":"code","cfc102ac":"code","bcc214d0":"code","9c7f65b2":"code","77335806":"code","4d20e30b":"code","b6687ec7":"code","7744330b":"code","06e1dd9c":"code","4190e8f6":"code","659f3196":"code","eb804657":"code","bf4ece4c":"code","603f15f0":"code","57c15eac":"code","0f40c4cb":"code","65dafa95":"code","f4b47840":"code","e8465593":"code","fedb0a8a":"code","87e56df5":"code","6627c0d1":"code","fbe447e8":"code","9b6a9c69":"code","997996d4":"code","45c3d150":"code","c375a485":"code","dc65fee7":"code","0370585e":"code","8b4034d6":"code","13ed201c":"code","cc40bf7f":"code","80752b46":"code","705af6ee":"code","8b8f27be":"code","a84cda12":"code","56b1c176":"code","2c112e5c":"code","d4f38c52":"code","d93ee33d":"code","4b01bb4a":"code","3060ea72":"code","65abde39":"code","cbf08750":"code","d4639ef2":"code","c7a0f5db":"code","75614359":"code","9b3280c4":"code","6d18e6a4":"code","6cb0ec86":"code","202f06d3":"code","9e1bad9d":"markdown","caccb2da":"markdown","3c8f0204":"markdown","62c93b05":"markdown","9b0d2bcc":"markdown","c8729227":"markdown","8b91e5cc":"markdown","d7004ebc":"markdown","e1706dbe":"markdown","60c48f61":"markdown","46af2a03":"markdown","674987c6":"markdown","daf32375":"markdown","e29a4960":"markdown","df3fda47":"markdown","3e003157":"markdown","67c0694b":"markdown","ef69e7b1":"markdown","3e0f7468":"markdown","7574ed98":"markdown","abb6a070":"markdown","63d78c85":"markdown","a4313505":"markdown","729419c5":"markdown","8b64aea8":"markdown","628f2aab":"markdown","2d186620":"markdown","28ee3cee":"markdown","35d43576":"markdown","c4d3cf57":"markdown","3028c4d4":"markdown","9e071084":"markdown","f2fb1c52":"markdown","610046d1":"markdown","3a096238":"markdown","a5e196be":"markdown","f01a9769":"markdown","ad919daa":"markdown","5da537ce":"markdown"},"source":{"80b71068":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error\n\nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","7717c668":"tourney_results = pd.read_csv('..\/input\/march-madness-analytics-2020\/2020DataFiles\/2020DataFiles\/2020-Mens-Data\/MDataFiles_Stage1\/MNCAATourneyDetailedResults.csv')\ntourney_results = tourney_results[['Season','WTeamID','WScore','LTeamID','LScore']]\ntourney_results","5ce92022":"regular_results = pd.read_csv('..\/input\/march-madness-analytics-2020\/2020DataFiles\/2020DataFiles\/2020-Mens-Data\/MDataFiles_Stage1\/MRegularSeasonDetailedResults.csv')\nregular_results = regular_results[['Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore']].copy()\nregular_results_swap = regular_results\nregular_results","45720a57":"regular_results.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(regular_results.columns)]\nregular_results_swap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(regular_results.columns)]\nregular_data = pd.concat([regular_results, regular_results_swap]).sort_index().reset_index(drop = True)","ec125d8e":"# data are duplicated, so only need to take T1_TeamID average, T2 will be average points allowed\nseason_statistics = regular_data.groupby(['Season','T1_TeamID'])['T1_Score','T2_Score'].agg(np.mean)\nseason_statistics.columns = [''.join(col).strip() for col in season_statistics.columns.values]\nseason_statistics = season_statistics.reset_index()\nseason_statistics.rename(columns={'T1_TeamID':'TeamID','T1_Score':'AvgPtsScored','T2_Score':'AvgPtsAllowed'}, inplace=True)\nseason_statistics","82b14949":"kp_df=[]\nyears_list = [2019,2018,2017,2016,2015,2014,2013,2012,2011,2010,2009,2008,2007,2006,2005,2004,2003,2002]\nfor year in years_list:\n    temp_kp_df = pd.read_csv(\"..\/input\/kenpomeffiencydata\/KP\" + str(year) + \".csv\") \n    temp_kp_df = temp_kp_df[['team','conf','adjem','adjo','adjd','luck']]\n    temp_kp_df['Season'] = year\n    year_last = year   \n    if year==2019:\n        kp_df = temp_kp_df\n    else:\n        kp_df = kp_df.append(temp_kp_df)","7ed80558":"kp_df['team'] = kp_df['team'].str.lower()","6499075f":"# these teams are in the tournament historically and don't match to the team names file without these fixes\nkp_df.loc[kp_df['team']==\"st. louis\",\"team\"] = \"saint louis\"\nkp_df.loc[kp_df['team']==\"cal st. bakersfield\",\"team\"] = \"csu bakersfield\"\nkp_df.loc[kp_df['team']==\"illinois chicago\",\"team\"] = \"illinois-chicago\"\nkp_df.loc[kp_df['team']==\"texas a&m corpus chris\",\"team\"] = \"a&m-corpus chris\"\nkp_df.loc[kp_df['team']==\"texas a&m corpus chri\",\"team\"] = \"a&m-corpus chris\"\nkp_df.loc[kp_df['team']==\"nevada las vegas\",\"team\"] = \"unlv\"\nkp_df.loc[kp_df['team']==\"arkansas pine bluff\",\"team\"] = \"ark pine bluff\"\nkp_df.loc[kp_df['team']==\"mississippi valley st.\",\"team\"] = \"miss valley st.\"\nkp_df.loc[kp_df['team']==\"arkansas little rock\",\"team\"] = \"ark little rock\"\nkp_df.loc[kp_df['team']==\"texas el paso\",\"team\"] = \"texas-el paso\"\nkp_df.loc[kp_df['team']==\"wisconsin green bay\",\"team\"] = \"wisconsin-green bay\"\nkp_df.loc[kp_df['team']==\"wisconsin milwaukee\",\"team\"] = \"wisconsin-milwaukee\"\nkp_df.loc[kp_df['team']==\"md baltimore county\",\"team\"] = \"maryland-baltimore county\"\nkp_df.loc[kp_df['team']==\"winston salem st.\",\"team\"] = \"winston-salem-state\"\nkp_df.loc[kp_df['team']==\"southwest missouri st.\",\"team\"] = \"southwest missouri state\"","1f2d5c9f":"teams_df = pd.read_csv('..\/input\/march-madness-analytics-2020\/2020DataFiles\/2020DataFiles\/2020-Mens-Data\/MDataFiles_Stage1\/MTeamSpellings.csv', sep='\\,', engine='python')","3f9a4ce5":"kp_df = pd.merge(kp_df, teams_df, left_on=['team'], right_on = ['TeamNameSpelling'], how='left')\nkp_df = kp_df.drop(['TeamNameSpelling'], axis=1)","2225aec4":"# these teams haven't played in MM, fine that they don't match\ntemp = kp_df[kp_df['TeamID'].isna()]\ntemp['team'].value_counts(dropna=False)","b9f169a5":"kp_df['Season'].value_counts(dropna=False)","38da030c":"kp_df.head()","cfc102ac":"spread_df=[]\nyears_list = [18,17,16,15,14,13,12,11,10,'09','08','07','06','05','04','03']\nyear_int = 2019\nfor year in years_list:\n    #Note: 18 refers to 2019 and so on\n    temp_spread_df = pd.read_csv(\"..\/input\/spread-data-03-to-18\/ncaabb\" + str(year) + \".csv\")\n    temp_spread_df['Season'] = year_int\n    year_last = year\n    year_int = year_int-1    \n    if year==18:\n        spread_df = temp_spread_df\n    else:\n        spread_df = spread_df.append(temp_spread_df)\n    \nspread_df = spread_df[['Season','line','home','hscore','road','rscore']]\n\nspread_df = spread_df[(spread_df['hscore'] != \".\") & (spread_df['rscore'] != \".\") & (spread_df['line'] != \".\")]\nspread_df = spread_df.dropna()\n\nspread_df['rscore'] = spread_df['rscore'].astype(float)\nspread_df['hscore'] = spread_df['hscore'].astype(float)\nspread_df['line'] = spread_df['line'].astype(float)\n\nspread_df['rscore'] = spread_df['rscore'].astype(int)\nspread_df['hscore'] = spread_df['hscore'].astype(int)\nspread_df","bcc214d0":"# make teams W and L consistent with tourney data\nhome_team_wins = spread_df[spread_df['hscore']>spread_df['rscore']]\nhome_team_wins.rename(columns={'home':'WTeam','hscore':'WScore','road':'LTeam','rscore':'LScore'}, inplace=True)                          \n\naway_team_wins = spread_df[spread_df['hscore']<spread_df['rscore']]\naway_team_wins.rename(columns={'home':'LTeam','hscore':'LScore','road':'WTeam','rscore':'WScore'}, inplace=True)  \n#line (aka spread) is in terms of home team, multiply by -1 if the away team wins\naway_team_wins['line'] = away_team_wins['line']*-1\n\nspread_df = home_team_wins.append(away_team_wins)\nspread_df.rename(columns={'line':'WSpread'}, inplace=True)   \nspread_df","9c7f65b2":"spread_df['WTeam'] = spread_df['WTeam'].str.lower()\nspread_df['LTeam'] = spread_df['LTeam'].str.lower()","77335806":"team_list = ['WTeam','LTeam']\nfor team in team_list:\n    spread_df = pd.merge(spread_df, teams_df, left_on=[team], right_on = ['TeamNameSpelling'], how='left')\n    spread_df.rename(columns={'TeamID': team+'ID'}, inplace=True)\n    spread_df = spread_df.drop(['TeamNameSpelling'], axis=1)","4d20e30b":"spread_df.head()","b6687ec7":"spread_df['Season'].value_counts(dropna=False)","7744330b":"rank_df = pd.read_csv('..\/input\/march-madness-analytics-2020\/2020DataFiles\/2020DataFiles\/2020-Mens-Data\/MDataFiles_Stage1\/MMasseyOrdinals.csv')\n# 133 is last day of regular season\nrank_df = rank_df.loc[rank_df['RankingDayNum'] == 133]\nrank_end_df = rank_df.groupby(['Season','TeamID'])['OrdinalRank'].agg(pd.np.mean).reset_index()\nrank_end_df.rename(columns={'OrdinalRank': 'Rank'}, inplace=True)\nrank_end_df","06e1dd9c":"tourney_results.head()","4190e8f6":"spread_df.head()","659f3196":"season_statistics.head()","eb804657":"kp_df.head()","bf4ece4c":"rank_end_df.head()","603f15f0":"combined_data = pd.merge(tourney_results, spread_df, on = ['Season','WTeamID','WScore','LTeamID','LScore'], how = 'left')\ncombined_data.tail(5)","57c15eac":"reg_W = season_statistics[['Season','TeamID','AvgPtsScored','AvgPtsAllowed']].copy()\nreg_L = season_statistics[['Season','TeamID','AvgPtsScored','AvgPtsAllowed']].copy()\nreg_W.columns = ['Season','WTeamID','WAvgPtsScored','WAvgPtsAllowed']\nreg_L.columns = ['Season','LTeamID','LAvgPtsScored','LAvgPtsAllowed']\n\ncombined_data = pd.merge(combined_data, reg_W, on = ['Season', 'WTeamID'], how = 'left')\ncombined_data = pd.merge(combined_data, reg_L, on = ['Season', 'LTeamID'], how = 'left')\ncombined_data","0f40c4cb":"kp_W = kp_df[['Season','TeamID','adjem']].copy()\nkp_L = kp_df[['Season','TeamID','adjem']].copy()\nkp_W.columns = ['Season','WTeamID','Wadjem']\nkp_L.columns = ['Season','LTeamID','Ladjem']\n\ncombined_data = pd.merge(combined_data, kp_W, on = ['Season', 'WTeamID'], how = 'left')\ncombined_data = pd.merge(combined_data, kp_L, on = ['Season', 'LTeamID'], how = 'left')\ncombined_data","65dafa95":"rank_W = rank_end_df[['Season','TeamID','Rank']].copy()\nrank_L = rank_end_df[['Season','TeamID','Rank']].copy()\nrank_W.columns = ['Season','WTeamID','WRank']\nrank_L.columns = ['Season','LTeamID','LRank']\n\ncombined_data = pd.merge(combined_data, rank_W, on = ['Season', 'WTeamID'], how = 'left')\ncombined_data = pd.merge(combined_data, rank_L, on = ['Season', 'LTeamID'], how = 'left')\ncombined_data","f4b47840":"#we'll also include 0 in this one:\nfavorite_won = combined_data[combined_data['WSpread']>-0.01]\nfavorite_won","e8465593":"favorite_won.columns = [x.replace('W','F').replace('L','U') for x in list(favorite_won.columns)]\nfavorite_won","fedb0a8a":"underdog_won = combined_data[combined_data['WSpread']<0]\nunderdog_won.columns = [x.replace('W','U').replace('L','F') for x in list(underdog_won.columns)]\n#The winning spread is related to the underdog, swap by -1 to make relative to favorite\nunderdog_won['FSpread'] = underdog_won['USpread']*-1\nunderdog_won = underdog_won.drop(['USpread'], axis=1)\nunderdog_won","87e56df5":"combined_data = favorite_won.append(underdog_won)\ncombined_data","6627c0d1":"combined_data['Season'].value_counts(dropna=False).sort_index()","fbe447e8":"combined_data['FAvgPointMargin'] = combined_data['FAvgPtsScored']-combined_data['FAvgPtsAllowed']\ncombined_data['UAvgPointMargin'] = combined_data['UAvgPtsScored']-combined_data['UAvgPtsAllowed']","9b6a9c69":"# make variables we will use as features differences (cuts down the number of features to potentially generalize better)\nvarlist = [\n'AvgPointMargin',\n'adjem',\n'Rank'\n]\nfor var in varlist:\n    combined_data[var+'Diff'] = combined_data['F'+var]-combined_data['U'+var]","997996d4":"# add target variables:\ncombined_data['ScoreDiff'] = combined_data['FScore']-combined_data['UScore']\n\ncombined_data['FavoriteCovered'] = 0\ncombined_data.loc[combined_data['FSpread']<combined_data['ScoreDiff'], 'FavoriteCovered'] =  1\n\ncombined_data['FWon'] = 0\ncombined_data.loc[combined_data['FScore']>combined_data['UScore'], 'FWon'] = 1","45c3d150":"features = [\n'AvgPointMarginDiff',\n'adjemDiff',\n'RankDiff',\n'FSpread'\n]","c375a485":"def predict(df,season,features_array,dep_var):\n    # train on prior seasons, validate on current season\n    train_df = df[df['Season']<season]\n    valid_df = df[df['Season']==season]\n\n    X_train = train_df[features_array].reset_index(drop=True)\n    y_train = train_df[dep_var].reset_index(drop=True)\n\n    X_valid = valid_df[features_array].reset_index(drop=True)\n    y_valid = valid_df[dep_var].reset_index(drop=True)\n    \n    X_train_xgb = xgb.DMatrix(X_train, label = y_train)\n    X_valid_xgb = xgb.DMatrix(X_valid)\n\n    param = {'max_depth':3,'eta':.001,'seed':201,'objective':'binary:logistic',\n             'eval_metric':'mae', 'gamma':0, 'nthread':-1}\n\n    num_round=2000\n    \n    xgb_train = xgb.train(param, X_train_xgb, num_round)   \n    pred = xgb_train.predict(X_valid_xgb)\n        \n    return pred, y_valid, valid_df, xgb, xgb_train","dc65fee7":"#this is the scoring metric for Kaggle competitions\ndef LogLoss(predictions, realizations):\n    predictions_use = predictions.clip(0)\n    realizations_use = realizations.clip(0)\n    LogLoss = -np.mean( (realizations_use * np.log(predictions_use)) + \n                        (1 - realizations_use) * np.log(1 - predictions_use) )\n    return LogLoss","0370585e":"#evaluate log loss during the last 4 seasons\nlosses = 0\nseasons = [2016,2017,2018,2019]\nfor season in seasons:\n    pred, y_valid, valid_df, xgb, xgb_train = predict(combined_data,season,features,'FWon')    \n    loss = LogLoss(pred, y_valid)\n        \n    print(\"Season\",season,\"valid:\",loss)\n    losses = losses+loss\n    \nlosses_avg = losses\/4\nprint(\"All seasons :\",losses_avg)","8b4034d6":"combined_data","13ed201c":"def predict(df,season,features_array,dep_var):\n    train_df = df[df['Season']<season]\n    valid_df = df[df['Season']==season]\n\n    X_train = train_df[features_array].reset_index(drop=True)\n    y_train = train_df[dep_var].reset_index(drop=True)\n\n    X_valid = valid_df[features_array].reset_index(drop=True)\n    y_valid = valid_df[dep_var].reset_index(drop=True)\n    \n    X_train_xgb = xgb.DMatrix(X_train, label = y_train)\n    X_valid_xgb = xgb.DMatrix(X_valid)\n\n    param = {'max_depth':3,'eta':.01,'seed':201,'objective':'reg:squarederror',\n             'eval_metric':'mae', 'gamma':1, 'nthread':-1}\n\n    num_round=200\n            \n    xgb_train = xgb.train(param, X_train_xgb, num_round)   \n    pred = xgb_train.predict(X_valid_xgb)\n        \n    return pred, y_valid, valid_df, xgb, xgb_train","cc40bf7f":"#2009 is the first season we have without a data leak in Ken Pom. \n#We still include the training data prior to 2009 that has a leak as the validation performance is better with more data, even if some of the years of training data have Ken Pom leaks\n#We use all prior years and validate on each season since 2009\n\nseasons = [2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019]\nfor season in seasons:\n    # train model using all seasons in the past, validate on current season  \n    pred, y_valid, valid_df, xgb, xgb_train = predict(combined_data,season,features,'ScoreDiff')  \n    # mean average error is average difference between predicted and actual score difference\n    mae = mean_absolute_error(pred, y_valid)\n    print(season,'MAE',mae)\n    \n    valid_df['PredSpread'] = pd.array(pred)\n\n    valid_df['PredFavoriteCovered'] = 0\n    valid_df.loc[valid_df['PredSpread']>valid_df['FSpread'],'PredFavoriteCovered'] = 1     \n    \n    valid_df['DollarsBet'] = 110\n    #wrong: lose $110\n    valid_df['DollarsWonLost'] = -110\n    #right: gain $100\n    valid_df.loc[(valid_df['FavoriteCovered']==1) & (valid_df['PredFavoriteCovered']==1),'DollarsWonLost'] = 100   \n    valid_df.loc[(valid_df['FavoriteCovered']==0) & (valid_df['PredFavoriteCovered']==0),'DollarsWonLost'] = 100 \n\n    # We exclude 2009 from the simulation because the MAE is high. We don't have enough training data.\n    if season==2010:\n        all_validation_bets = valid_df\n    if season>2010:\n        all_validation_bets = all_validation_bets.append(valid_df)","80752b46":"# this is the feature importance from last iteration above\nxgb.plot_importance(xgb_train)\nplt.rcParams['figure.figsize'] = [20, 20]\nplt.show()","705af6ee":"all_validation_bets.shape","8b8f27be":"# What is optimal threshold to cutoff in difference? \n# Use 2010-2013 to determine threshold and then test strategy in 2014-2019\ntrain = all_validation_bets[all_validation_bets['Season']<2014]\ntest = all_validation_bets[all_validation_bets['Season']>2013]","a84cda12":"train['Difference'] = abs(train['PredSpread']-train['FSpread'])\ntrain[['Season','FTeam','FScore','UTeam','UScore','ScoreDiff','FSpread','PredSpread','Difference']].tail(10)","56b1c176":"# keep games where we differ from spread by at least 3\ntrain0 = train[abs(train['PredSpread']-train['FSpread'])>0]\ntrain1 = train[abs(train['PredSpread']-train['FSpread'])>1]\ntrain2 = train[abs(train['PredSpread']-train['FSpread'])>2]\ntrain3 = train[abs(train['PredSpread']-train['FSpread'])>3]\ntrain4 = train[abs(train['PredSpread']-train['FSpread'])>4]\ntrain5 = train[abs(train['PredSpread']-train['FSpread'])>5]","2c112e5c":"for i in range(0,6):\n    train_temp = train[abs(train['PredSpread']-train['FSpread'])>i]\n    train_temp['DifferenceVspread'] = i\n    train_temp['GamesBetOn'] = 1\n    if i==0:\n        appended = train_temp\n    else:\n        appended = appended.append(train_temp)\n        \nsummary_df = appended.groupby(['DifferenceVspread'])['DollarsWonLost','GamesBetOn'].agg(np.sum).reset_index()\nsummary_df = summary_df.style.format({\n    'DollarsWonLost': '$ {:,.0f}'.format\n})\nsummary_df","d4f38c52":"for i in range(0,6):\n    test_temp = test[abs(test['PredSpread']-test['FSpread'])>i]\n    test_temp['DifferenceVspread'] = i\n    test_temp['GamesBetOn'] = 1\n    test_temp['ModelCorrect'] = test_temp['DollarsWonLost']==100\n    if i==0:\n        appended = test_temp\n    else:\n        appended = appended.append(test_temp)\n        \nsummary_df = appended.groupby(['DifferenceVspread'])['DollarsWonLost','GamesBetOn','ModelCorrect'].agg(np.sum).reset_index()\nsummary_df['ModelCorrect'] = summary_df['ModelCorrect'] \/ summary_df['GamesBetOn']\nsummary_df = summary_df.style.format({\n    'DollarsWonLost': '$ {:,.0f}'.format,\n    'ModelCorrect': '{:,.0%}'.format\n})\nsummary_df","d93ee33d":"test.shape","4b01bb4a":"test = test[abs(test['PredSpread']-test['FSpread'])>3]","3060ea72":"# we bet on 62 out of 391 games\ntest.shape","65abde39":"62\/391","cbf08750":"test['ModelCorrect'] = 0\ntest.loc[test['DollarsWonLost']>0,'ModelCorrect'] = 1 \ntest['Seasons'] = \"2014-2019\"\nsummary_df = test.groupby(['Seasons'])['ModelCorrect'].agg(np.mean).reset_index()\nsummary_df = summary_df.style.format({\n    'ModelCorrect': '{:,.0%}'.format\n})\nsummary_df","d4639ef2":"test['ModelCorrect'] = 0\ntest.loc[test['DollarsWonLost']>0,'ModelCorrect'] = 1 \n\nsummary_df = test.groupby(['Season'])['ModelCorrect'].agg(np.mean).reset_index()\nsummary_df = summary_df.style.format({\n    'ModelCorrect': '{:,.0%}'.format\n})\nsummary_df","c7a0f5db":"test[['Season','FTeam','FScore','UTeam','UScore','ScoreDiff','FavoriteCovered','FSpread','PredFavoriteCovered','PredSpread','DollarsBet','DollarsWonLost']].head(10)","75614359":"test['GamesBetOn'] = 1\n\ntest['NotCoverCorrect'] = np.where((test['PredFavoriteCovered']==0) & (test['FavoriteCovered']==0), 1, 0)\ntest['CoverCorrect'] = np.where((test['PredFavoriteCovered']==1) & (test['FavoriteCovered']==1), 1, 0)\n\ntest_all = test.copy()\ntest_all['Season'] = 'Total'\ntest_appended = test.append(test_all)","9b3280c4":"summary_df = test_appended.groupby(['Season'])['GamesBetOn','ModelCorrect','DollarsBet','DollarsWonLost','PredFavoriteCovered','NotCoverCorrect','CoverCorrect'].agg(np.sum).reset_index()\n\nsummary_df['ModelCorrect'] = summary_df['ModelCorrect']\/summary_df['GamesBetOn']\nsummary_df['TookCover'] = summary_df['PredFavoriteCovered']\/summary_df['GamesBetOn']\nsummary_df['TookNotCover'] = 1-summary_df['TookCover'] \n\nsummary_df['CoverCorrect'] = summary_df['CoverCorrect']\/summary_df['PredFavoriteCovered']\nsummary_df['NotCoverCorrect'] = summary_df['NotCoverCorrect']\/(summary_df['GamesBetOn']-summary_df['PredFavoriteCovered'])\n\nsummary_df = summary_df[['Season','GamesBetOn','ModelCorrect','DollarsBet','DollarsWonLost','TookNotCover','TookCover','NotCoverCorrect','CoverCorrect']]\n\nsummary_df = summary_df.style.format({\n    'ModelCorrect': '{:,.0%}'.format,\n    'TookCover': '{:,.0%}'.format,\n    'TookNotCover': '{:,.0%}'.format,\n    'CoverCorrect': '{:,.0%}'.format,\n    'NotCoverCorrect': '{:,.0%}'.format,\n    'DollarsBet': '$ {:,.0f}'.format,\n    'DollarsWonLost': '$ {:,.0f}'.format\n})\n\nsummary_df","6d18e6a4":"summary_df = test_appended.groupby(['Season'])['GamesBetOn','ModelCorrect','DollarsBet','DollarsWonLost','PredFavoriteCovered','NotCoverCorrect','CoverCorrect'].agg(np.sum).reset_index()\n\nsummary_df['ModelCorrect'] = summary_df['ModelCorrect']\nsummary_df['TookCover'] = summary_df['PredFavoriteCovered']\nsummary_df['TookNotCover'] = summary_df['GamesBetOn']-summary_df['TookCover'] \n\nsummary_df = summary_df[['Season','GamesBetOn','ModelCorrect','TookNotCover','TookCover','NotCoverCorrect','CoverCorrect']]\n\nsummary_df = summary_df.style.format({\n    'DollarsBet': '$ {:,.0f}'.format,\n    'DollarsWonLost': '$ {:,.0f}'.format\n})\n\nsummary_df","6cb0ec86":"pd.options.display.max_columns = None\npd.options.display.max_rows = None","202f06d3":"test[['Season','FTeam','FScore','UTeam','UScore','ScoreDiff','FSpread','PredSpread','PredFavoriteCovered','DollarsWonLost']]","9e1bad9d":"I determined the optimal betting strategy using games during 2010-2013 and applied this strategy to \"unseen\" games from 2014-2019. I then evaluated the hypothetical winnings had various strategies been applied. I tested different thresholds of the difference between the spread and model predictions in games from 2010-2013 for six different thresholds, ranging from 0 to 5. The winnings from 2010-2013 for each threshold are summarized in the table below.","caccb2da":"(in addition to Kaggle data)\n\nKen Pom:\n    \n2019\thttps:\/\/web.archive.org\/web\/20190317211809\/https:\/\/kenpom.com\/\n\n2018\thttps:\/\/web.archive.org\/web\/20180311122559\/https:\/\/kenpom.com\/\n\n2017\thttps:\/\/web.archive.org\/web\/20170312131016\/http:\/\/kenpom.com\/\n\n2016\thttps:\/\/web.archive.org\/web\/20160314134726\/http:\/\/kenpom.com\/\n\n2015\thttps:\/\/web.archive.org\/web\/20150316212936\/http:\/\/kenpom.com\/\n\n2014\thttps:\/\/web.archive.org\/web\/20140318100454\/http:\/\/kenpom.com\/\n\n2013\thttps:\/\/web.archive.org\/web\/20130318221134\/http:\/\/kenpom.com\/\n\n2012\thttps:\/\/web.archive.org\/web\/20120311165019\/http:\/\/kenpom.com\/\n\n2011\thttps:\/\/web.archive.org\/web\/20110311233233\/http:\/\/www.kenpom.com\/\n\n2010\thttps:\/\/web.archive.org\/web\/20100304023540\/http:\/\/kenpom.com\/rate.php\n\n2009\thttps:\/\/web.archive.org\/web\/20090315085050\/http:\/\/kenpom.com\/rate.php\n\nPrior to 2009 (only used for training, not validation): Downloaded directly from Ken Pom website (https:\/\/kenpom.com\/index.php) as Wayback data are thin.\n\nSpread data:\n\nhttp:\/\/www.thepredictiontracker.com\/ncaaresults.php \n\n(e.g., http:\/\/www.thepredictiontracker.com\/ncaabb18.csv)","3c8f0204":"**Introduction to Sports Betting**\n\nIn sports betting, the spread represents the difference in score between two opponents. For example, a spread of 2.5 for UVA favored against Duke means that UVA is expected to win by 2.5 points. A bettor has two options:\n\n1. **Bet favorite covers:** Bet that UVA will win by more than 2.5 points (i.e., \u201ccover the spread\u201d)\n2. **Bet favorite doesn't cover:** Bet that UVA will either (a) win by less than 2.5 points or (b) lose\n\nSports bettors typically must bet \\\\$110 to win \\$100. A bettor can place a bet of \\\\$110 for UVA to cover. If UVA covers, the bettor gets \\$210 back, winning \\\\$100. If UVA doesn\u2019t cover, the bettor gets nothing back and loses the full $110. \n\nTherefore, the profit function is p x \\\\$100 + (1-p) x -$110, where p is the percentage of games picked correctly. I calculate the breakeven percentage as 52.4% by setting profit equal to \\$0 and solving for p.\n\nThe spread is set by the bookie (e.g., Vegas) and adjusts as bettors place bets in the days leading up to the start of the game. To maximize profit, the bookie adjusts the spread to entice 50% of the bettors to bet on the favorite covering and 50% to bet on the favorite not covering. If two bettors select a favorite to cover and two bettors select a favorite won't cover, the bookie collects \\\\$40 (\\$10 per \\\\$100 bet) no matter what the outcome is. Because the bookie adjusts the spread as bets are placed, the wisdom of the crowd results in the spread being an accurate predictor that incorporates all information available (team strength, injuries, etc.).\n\n**The Model**\n\nI gathered historical spread data from www.thepredictiontracker.com to utilize for this analysis. I trained a tree-based ensemble model using XGBoost that includes four features to predict the difference in score between two opponents. These four features are explained below.\n\n* Difference between Favorite and Underdog in Ken Pom Efficiency Margin prior to start of March Madness:\n\n    The Ken Pom Efficiency Margin is the offensive efficiency less the defensive efficiency, where offensive efficiency is the estimated points per 100 possessions and defensive efficiency is the points allowed per 100 possessions. Both calculations are strength of schedule adjusted to reflect efficiency against an average D-1 team (see: https:\/\/kenpom.com\/blog\/ratings-glossary\/). If the favorite has an efficiency margin of 30 and Team 2 has an efficiency margin of 10, the difference would be 20.\n\n    I collected Ken Pom efficiency margin data from before the tournaments began using the wayback machine, which captures a historical screen shot of web pages. By relying on historical data captured prior to the tournament beginning, I prevented data leakage.\n\n\n* Difference between Favorite and Underdog in Rank prior to start of March Madness:\n\n    I took an average of all ranking systems (e.g., ESPN, AP, Massey) before the tournaments began. If the favorite has a rank of 2 and underdog has a rank of 10, the difference is 8. Kaggle provided ranking data.\n    \n\n* Difference between Favorite and Underdog in average point margin during regular season:\n\n    I took an average of the points scored less points allowed by each team during the regular season. If the favorite won by 5 points, on average, and the underdog won by 1 point, on average, the difference would be 4. Kaggle provided regular season statistics.\n    \n\n* Spread relative to favorite team:\n    \n    I used the spread as a feature in the model and transformed this feature such that the spread is always relative to the favorite (i.e., it is always positive). I also used the spread to determine who is the favorite and underdog. In the UVA example provided earlier, UVA is the favorite and the spread is 2.5.\n    \n\nAs shown by feature importance chart below, the most important feature was rank, followed by the efficiency margin. Surprisingly, the spread was the third most important feature.","62c93b05":"# How to Make Money Betting on March Madness using Machine Learning","9b0d2bcc":"Even above average sports bettors lose money betting on March Madness. A bettor must pick 52.4% of games correctly just to break even, because bookies collect a portion of bets placed. Can an above average machine learning model beat the spread?\n\nI trained a machine learning model to predict the score difference between two opponents. The predictions from this model correctly picked a betting position in 65% of the games where the model identified an opportunity to bet on the spread. \n\nUsing tournament games from 2010-2013, I determined the optimal betting strategy was to bet on games where the model's prediction varied by more than 3 points from the spread. I then simulated this betting strategy on tournaments from 2014-2019. The model placed bets on 16% of tournament games from 2014-2019 where the model's prediction was 3 points more or less than the spread.\n\nThe hypothetical bets placed are summarized in the figure below. Had I bet on these 62 games, I would have made \\\\$1,580 off \\$6,820 in bets placed, a return of 23%.","c8729227":"| Season | Favorite | Score | Underdog | Score | Actual Score Difference | Favorite Covered | FSpread | Predicted Favorite Covered | Predicted Score Difference | Amount Bet | Amount Won\/Lost\n--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n**721**\t| **2014**\t| **louisville** | **71** | **manhattan** | **64** | **7** | **0** | **17.0** | **0** | **11.75** | **\\\\$110** | **\\\\$100**\n723\t| 2014 | michigan st. | 93 | delaware | 78 | 15 | 0 | 15.0\t | 0 | 11.69 | \\\\$110 | \\\\$100\n**732** | **2014** | **wisconsin** | **75** | **american** | **35** | **40** | **1** | **14.5** | **0** | **11.45** | **\\\\$110** | **-\\\\$110**\n733 | 2014 | arizona | 68 | weber st. | 59 | 9 | 0 | 20.5 | 0 | 15.44 | \\\\$110 | \\\\$100\n740 | 2014 | memphis | 71 | george washington | 66 | 5 | 1 | 4.0 | 0 | 0.63 | \\\\$110 | -\\\\$110\n748 | 2014 | wichita st. | 64 | cal poly slo | 37 | 27 | 1 | 17.0 | 1 | 22.20 | \\\\$110 | \\\\$100\n754 | 2014 | michigan st. | 80 | harvard | 73 | 7 | 0 | 8.5 | 0 | 3.52 | \\\\$110 | \\\\$100\n772 | 2014 | michigan st. | 61 | virginia | 59 | 2 | 0 | 2.5 | 0 | -2.47 | \\\\$110 | \\\\$100\n776 | 2014 | kentucky | 75 | michigan | 72 | 3 | 1 | 2.0 | 0 | -2.47 | \\\\$110 | -\\\\$110\n778 | 2014 | kentucky | 74 | wisconsin | 73 | 1 | 0 | 1.5 | 0 | -2.24 | \\\\$110 | \\\\$100","8b91e5cc":"Does this same strategy hold into the future? Let's see the optimal threshold if we include all seasons. This same strategy does hold into the future.","d7004ebc":"Model Difference from Spread | Amount Won\/Lost | Games Bet On\n--- | --- | --- \n0 | -\\\\$3,330 | 246\n1 | -\\\\$2,430 | 171\n2 | -\\\\$1,410 | 114\n**3** | **\\\\$690** | **72**\n4 | -\\\\$20 | 46\n5 | -\\\\$260 | 31","e1706dbe":"The only threshold with positive returns was the optimal threshold of 3 points, which returns \\\\$690 from 2010-2013. Therefore, I applied this betting strategy to an \"unseen\" set of games from 2014-2019. When the model returned a predicted point difference that was more than 3 points different than the spread, the game was selected to bet upon and the model picked whether the favorite would cover. Consider the examples below.\n\n(721) Louisville was the favorite against Manhattan and won 71-64, a difference of 7 points. Louisville was favored to win by 17 points, as represented by the spread. The model predicted Louisville would win by 11.7. Therefore, the model predicted Louisville would not cover. Because Louisville did not cover the spread, the model won \\\\$100 off \\$110 bet (i.e., bet \\\\$110, returned \\$210).\n\n(732) Wisconsin was the favorite against American and won 75-35, a difference of 40 points. Wisconsin was favored to win by 14.5 points, as represented by the spread. The model predicted Wisconsin would win by 11.4 points. Therefore, the model predicted Wisconsin would not cover. Wisconsin won by far more than 14.5. Therefore, the model lost the full \\\\$110 bet (i.e., bet \\$110, returned \\\\$0).","60c48f61":"# Evaluate bets","46af2a03":"Season | Games Bet On | Model Correct | Amount Bet | Amount Won\/Lost | Model Predicted Favorite Not Cover | Model Predicted Favorite Cover | Not Cover Correct | Cover Correct\n--- | --- | --- | --- | --- | --- | --- | --- | --- \n2014 | 13 | 77% | \\\\$1,430 | \\\\$670 | 92% | 8% | 75% | 100%\n2015 | 14 | 64% | \\\\$1,540 | \\\\$350 | 86% | 14% | 58% | 100%\n2016 | 8 | 62% | \\\\$880 | \\\\$170 | 88% | 12% | 57% | 100%\n2017 | 17 | 53% | \\\\$1,870 | \\\\$20 | 76% | 24% | 38% | 100%\n2018 | 4 | 75% | \\\\$440 | \\\\$190 | 50% | 50% | 50% | 100%\n2019 | 6 | 67% | \\\\$660 | \\\\$180 | 83% | 17% | 80% | 0%\n**Total** | **62** | **65%** | **\\\\$6,820** | **\\\\$1,580** | **82%** | **18%** | **59%** | **91%**","674987c6":"**Additional Considerations**\n\nUsing tournaments from 2010-2013, I determined the optimal betting strategy was to bet on games where the model's prediction varied by more than 3 points from the spread. But does this strategy hold moving forward? Using tournaments from 2014-2019, the model generated hypothetical winnings at the thresholds previously tested from 2010-2013.\n\nAs shown below, the optimal threshold was still 3. Even at a threshold of 1, the model had positive returns. This suggests the model generates more accurate predictions with additional training data. As the threshold increases 0 to 5, the model became more and more accurate. The model picked 50% of games correctly at a threshold of 0 and 78% correctly at a threshold of 5. However, the number of games selected for betting also decreases as the threshold increases. When the threshold increases from 3 to 4, the decrease in the number of games bet on outweighs the incremental increase in the percentage of games picks correctly. Therefore, the optimal threshold remains 3.\n\nMy analysis suggests an above average machine learning can beat the spread. The countdown to 2021 March Madness begins now. ","daf32375":"**Tournament games**","e29a4960":"**Ken Pom data**","df3fda47":"| Season | Favorite | Score | Underdog | Score | Actual score Diff | FSpread | Predicted Score Difference | Predicted - Spread\n--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n681 | 2013 | north carolina st. | 72 | temple | 76 | -4 | 4.0 | 4.12 | 0.12\n687 | 2013 | saint louis | 57 | oregon | 74 | -17 | 4.0 | 2.19 | 1.81\n689 | 2013 | gonzaga | 70 | wichita st. | 76 | -6 | 6.5 | 7.29 | 0.79\n**691** | **2013** | **san diego st.** | **71** | **florida gulf coast** | **81** | **-10** | **8.0** | **11.23** | **3.23**\n695 | 2013 | mississippi | 74 | la salle | 76 | -2 | 4.0 | 2.23 | \t1.77\n**698** | **2013** | **miami-florida** | **61** | **marquette** | **71** | **-10** | **5.5** | **2.26** | **3.24**\n700 | 2013 | indiana | 50 | syracuse | 61 | -11 | 5.0 | 5.73 | 0.73\n705 | 2013 | kansas | 85 | michigan | 87 | -2 | 2.0 | 1.07 | 0.93\n707 | 2013 | ohio st. | 66 | wichita st. | 70 | -4 | 4.5 | 5.73 | 1.23\n709 | 2013 | florida | 59 | michigan | 79 | -20 | 3.0 | 5.79 | 2.80","3e003157":"**Evaluating Betting Strategy**\n\nWhen I applied this strategy to games from 2014-2019, the model identified 62 games where the predicted score was more than 3 points different than the spread. I simulated placing \\\\$110 bets to win \\$100 if the model picked correctly. If the model picked incorrectly, the full \\\\$110 was lost. The model placed the proper bet in 65% of games, winning \\$1,580 on \\\\$6,820 bet. The winnings were consistently positive in all tournament years where the strategy was tested.\n\nThe model more often selected the favorite not to cover, but it was more often correct when it selected the favorite to cover. During the simulation applied to 2014-2019, the model predicted the favorite would not cover in 82% of games and would cover in 18% of games. The model was correct in 59% of the 51 games where it picked the favorite would not cover. It was correct in 91% of the 11 games where it picked the favorite would cover. The suggests the model more often identified games where the favorite is overvalued (the spread is greater than the prediction), but is more accurate in games where the model determined the favorite is undervalued (the spread is less than the prediction).","67c0694b":"Let's evaluate our model accuracy:","ef69e7b1":"Season | Games Bet On | Model Correct | Amount Bet | Amount Won\/Lost \n--- | --- | --- | --- | --- | --- \n2014 | 13 | 77% | \\\\$1,430 | \\\\$670 \n2015 | 14 | 64% | \\\\$1,540 | \\\\$350 \n2016 | 8 | 62% | \\\\$880 | \\\\$170 \n2017 | 17 | 53% | \\\\$1,870 | \\\\$20 \n2018 | 4 | 75% | \\\\$440 | \\\\$190 \n2019 | 6 | 67% | \\\\$660 | \\\\$180 \n**Total** | **62** | **65%** | **\\\\$6,820** | **\\\\$1,580** ","3e0f7468":"**Spread data**","7574ed98":"# Sources","abb6a070":"# Combine data","63d78c85":"# Supporting code and details","a4313505":"Average (2016-2019): 19th percentile\n\n2016: 1st percentile\n\n2017: 29th percentile\n\n2018: 31st percentile\n\n2019: 14th percentile\n\n\nSource: Kaggle leaderboards\n\nBackup:\n\n2016: 9\/597\n\n2017: 126\/441\n\n2018: 292\/933\n\n2019: 122\/866","729419c5":"From: https:\/\/www.thelines.com\/betting\/point-spread\/\n\n\"Point spreads are usually set with -110 odds, but pricing often fluctuates at online sportsbooks. This is the sportsbook operators\u2019 house edge. The odds guarantee the sportsbook operator will see a little money over time. When the odds are set at -110, the bettor must wager 110 to win 100 (or 11 to win 10).\"\n\nHere, we will always place 110 bet and  win 100","8b64aea8":"**Having features relative to winning team would cause a data leak since the features incorporate the outcome**\n\nWe instead make the variables relevant the favorite team. If WSpread is positive, it means the favorite won. If WSpread is negative, the underdog won.","628f2aab":"# Train model to predict wins and compare to historical Kaggle leaderboard","2d186620":"I compared this model against prior Kaggle March Madness competitions to evaluate the predictive performance. Using the four features above, I instead predicted the probability of the favorite winning. This allowed me to compare the model\u2019s performance to the historical Kaggle leaderboard, which ranked entries based on log loss of predicted win probabilities. This model would have performed in the top 19% of all March Madness models from 2016-2019. It would have placed in the top 1% once. This confirms the model is above average.\n\n**Betting Strategy**\n\nAfter the model generated predictions, I determined the optimal betting strategy. The model is more confident if the predicted score difference is further from the spread. For example, the model could place bets on games where the prediction differs by more than 5 points compared to the spread. In the UVA example above, the model would bet when the predicted score difference is greater than 7.5 (2.5 + 5) or less than -3.5 (2.5 \u2013 5). Although a higher threshold suggests greater confidence, the model may place bets on fewer games, resulting in lower returns.\n\nIn the 10 games from the table below, two games differ by more than 3 points. If the threshold was set to 3, the model would have bet on these two games only. ","28ee3cee":"# Game-level detail for bets placed","35d43576":"In the 10 games below, we can see 2 differ by more than 3 points. In a strategy where we set the threshold at 3, we would bet on these two games and not bet on the other 8. More specifically, we would bet on San Diego State v Florida Gulf Coast and UMiami v Marquette. If we set the threshold at 1, we would bet on 6 out of 10 of the games.","c4d3cf57":"**Regular season stats**","3028c4d4":"We will train the model. We will then determine an optimal betting strategy during 2010-2013. We will then simulate this optimal betting strategy during 2014-2019. ","9e071084":"![image.png](attachment:image.png)","f2fb1c52":"**Rankings data**","610046d1":"The optimal threshold is 3","3a096238":"Let's look at a few examples of how the above works to unpack this a bit. \n\n(721) Louisville was the favorite against Manhattan and won 71-64, a difference of 7 points. Louisville was favored to win by 17 points, as represented by the spread. The model predicted Louisville would win by 11.7. Therefore, the model predicted Louisville would not cover. Because Louisville did not cover the spread, the model won \\\\$100 off \\$110 bet (i.e., bet \\\\$110, returned \\$210).\n\n(732) Wisconsin was the favorite against American and won 75-35, a difference of 40 points. Wisconsin was favored to win by 14.5 points, as represented by the spread. The model predicted Wisconsin would win by 11.4 points. Therefore, the model predicted Wisconsin would not cover. Wisconsin won by far more than 14.5. Therefore, the model lost the full \\\\$110 bet (i.e., bet \\$110, returned \\\\$0).","a5e196be":"Model Difference from Spread | Amount Won\/Lost | Games Bet On | Model Correct\n--- | --- | --- | --- \n0 | -\\\\$1,850 | 391 | 50%\n1 | \\\\$310 | 232 | 53%\n2 | \\\\$670 | 118 | 55%\n**3** | **\\\\$1,580** | **62** | **65%**\n4 | \\\\$1,420 | 31 | 74%\n5 | \\\\$960 | 18 | 78%\n","f01a9769":"How many games do we bet on in the test set (2014-2019)?","ad919daa":"# Train model to predict point difference","5da537ce":"The supporting code and sources are below.\n\nThis analysis is hypthetical and for research purposes. The Kaggle rules for this competition state \"You will not: (a) use or access the NCAA\u00ae Data for any commercial, gambling, or illegal purpose.\""}}