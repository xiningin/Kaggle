{"cell_type":{"e3e29ac4":"code","7bbf5c8e":"code","4c9bc31c":"code","1bf48619":"code","be4dd35f":"code","1131b0b5":"code","c6fb78ef":"code","dfce62d1":"code","8aa4cf68":"code","f19dc504":"code","af85201d":"code","8989430b":"code","abe46810":"code","1f1cedd6":"code","5ad69ff1":"code","a294350f":"code","16d1ce47":"code","d1497624":"code","c1d8d367":"code","7bae724b":"code","bf7108d7":"code","c01ce87a":"code","14a7c619":"code","55e626bb":"code","6d482a40":"code","f751cea6":"code","a1f93f7e":"code","9d0d8ca4":"code","130e4a51":"code","61ec182c":"code","7b96550e":"code","4accd63e":"code","b3a67927":"code","1255a3b8":"code","09534e57":"code","9cdf3907":"code","c0baf503":"code","000d7206":"code","993c81b0":"code","6928bd78":"code","b363734a":"code","4ab28323":"code","99e6858f":"code","f27e6a54":"markdown"},"source":{"e3e29ac4":"!pip install dataprep by","7bbf5c8e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\nfrom dataprep.eda import *\nfrom dataprep.eda import plot\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot_missing\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport warnings\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings(\"ignore\")","4c9bc31c":"df=pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndf","1bf48619":"df.info()","be4dd35f":"df.isnull().sum()","1131b0b5":"df.describe().T","c6fb78ef":"df.isna()","dfce62d1":"df.shape","8aa4cf68":"# Chack if we have miss data\nplot_missing(df)","f19dc504":"import pandas_profiling as pp\npp.ProfileReport(df)","af85201d":"plot(df)","8989430b":"# plots the distribution of column x in various ways and calculates column statistics\nplot(df, 'output')","abe46810":"plot(df, 'age')","1f1cedd6":"plot(df, 'oldpeak')","5ad69ff1":"plot_correlation(df)","a294350f":"create_report(df)","16d1ce47":"sns.distplot(df)","d1497624":"sns.pairplot(df, height=3.5, aspect=1.3)","c1d8d367":"plt.subplots(figsize=(6, 8))\nsns.countplot(x=\"output\", data=df)\nplt.show()","7bae724b":"sns.boxplot(x=\"sex\", y=\"output\", data=df)","bf7108d7":"sns.barplot(x='sex', y='output', data=df)","c01ce87a":"sns.stripplot(x='age', data=df, color=\"#474646\")","14a7c619":"sns.catplot(data=df, x=\"exng\", y=\"age\", hue='output',aspect=1.7,kind=\"swarm\",palette=\"Set2\")","55e626bb":"plt.figure(figsize=(18,10))\nplt.style.use(\"ggplot\")\nsns.countplot(x=df[\"age\"])    #using countplot\nplt.title(\"COUNT OF PATIENTS AGE\",fontsize=20)\nplt.xlabel(\"AGE\",fontsize=20)\nplt.ylabel(\"COUNT\",fontsize=20)\nplt.show()","6d482a40":"s=df[\"sex\"].value_counts().reset_index()\npx.pie(s,names=\"index\",values=\"sex\")  #using pie here","f751cea6":"c=df[\"cp\"].value_counts().reset_index()\nc","a1f93f7e":"plt.figure(figsize=(20,10))\nplt.style.use(\"ggplot\")\nsns.barplot(x=c[\"index\"],y=c[\"cp\"])   #using bar here for visualization\nplt.title(\"TYPE OF CHEST PAIN WITH NUMBER OF PATIENTS\",fontsize=20)\nplt.xlabel(\"TYPE\",fontsize=20)\nplt.ylabel(\"COUNT\",fontsize=20)\nplt.show()","9d0d8ca4":"plt.figure(figsize=(20,10))\nplt.style.use(\"ggplot\")\nsns.lineplot(y=\"trtbps\",x=\"age\",data=df)\nplt.title(\"BLOOD PRESSURE WITH AGE\",fontsize=20)\nplt.xlabel(\"AGE\",fontsize=20)\nplt.ylabel(\"BLOOD PRESSURE\",fontsize=20)\nplt.show()","130e4a51":"plt.figure(figsize=(20,10))\nplt.style.use(\"ggplot\")\nsns.lineplot(y=\"chol\",x=\"age\",data=df)\nplt.title(\"CHOLESTROL LEVEL  WITH AGE\",fontsize=20)\nplt.xlabel(\"AGE\",fontsize=20)\nplt.ylabel(\"CHOLESTROL LEVEL\",fontsize=20)\nplt.show()","61ec182c":"plt.figure(figsize=(20,10))\nplt.style.use(\"ggplot\")\nsns.lineplot(y=\"thalachh\",x=\"age\",data=df)\nplt.title(\"HEART RATE  WITH AGE\",fontsize=20)\nplt.xlabel(\"AGE\",fontsize=20)\nplt.ylabel(\"HEART RATE\",fontsize=20)\nplt.show()","7b96550e":"plt.figure(figsize=(10,6))\nplt.style.use(\"ggplot\")\nsns.lineplot(x=\"age\",y=\"thalachh\",hue=\"output\",data=df)\nplt.title(\"EFFECT OF HEART ATTACK WITH INCREASE IN AGE AND MAXIMUM HEART RATE\")\nplt.show()","4accd63e":"plt.figure(figsize=(12,8))\nplt.style.use(\"ggplot\")\nsns.histplot(data = df, x = 'age', hue = 'output')\nplt.title(\"DOES AGE EFFECT THE HEART-ATTACK\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.show()","b3a67927":"plt.figure(figsize=(15,8))\nsns.heatmap(df.corr(), annot=True)","1255a3b8":"x = df.drop(['output'], axis=1)\n\ny = df['output']","09534e57":"x","9cdf3907":"y","c0baf503":"X_train, X_test, Y_train, Y_test  = train_test_split(x, y, test_size = 0.20, random_state = 0)","000d7206":"X_train","993c81b0":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6928bd78":"X_train","b363734a":"def models(X_train,Y_train):\n  \n  #Using Logistic Regression Algorithm to the Training Set\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train, Y_train)\n  \n  #Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n  from sklearn.neighbors import KNeighborsClassifier\n  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n  knn.fit(X_train, Y_train)\n    \n  #Using SVC method of svm class to use Support Vector Machine Algorithm\n  from sklearn.svm import SVC\n  svc_lin = SVC(kernel = 'linear', random_state =0)\n  svc_lin.fit(X_train, Y_train)\n\n  #Using SVC method of svm class to use Kernel SVM Algorithm\n  from sklearn.svm import SVC\n  svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n  svc_rbf.fit(X_train, Y_train)\n\n  #Using GaussianNB method of na\u00efve_bayes class to use Na\u00efve Bayes Algorithm\n  from sklearn.naive_bayes import GaussianNB\n  gauss = GaussianNB()\n  gauss.fit(X_train, Y_train)\n\n \n\n  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n  from sklearn.ensemble import RandomForestClassifier\n  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 10)\n  forest.fit(X_train, Y_train)\n  \n    \n  #Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n  from sklearn.tree import DecisionTreeClassifier\n  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n  tree.fit(X_train, Y_train)\n    \n    \n    \n    \n #Using xgboostClassifier of tree class to use Decision Tree Algorithm\n  from xgboost import XGBClassifier \n  xgboost = XGBClassifier(max_depth=5, learning_rate=0.01, n_estimators=100, gamma=0, \n                        min_child_weight=1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005)\n  xgboost.fit(X_train, Y_train)\n    \n    \n    \n  \n  #print model accuracy on the training data.\n  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train)*100)\n  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train)*100)\n  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train)*100)\n  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train)*100)\n  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train)*100)\n  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train)*100)\n  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train)*100)\n  print('[7]xgboost Classifier Training Accuracy:', xgboost.score(X_train, Y_train)*100)\n\n  \n  return log, knn, svc_lin, svc_rbf, gauss,tree,forest,xgboost\n\nmodel = models(X_train,Y_train)","4ab28323":"#Show the confusion matrix and accuracy for all of the models on the test data\n#Classification accuracy is the ratio of correct predictions to total predictions made.\nfrom sklearn.metrics import confusion_matrix\nfor i in range(len(model)):\n  cm = confusion_matrix(Y_test, model[i].predict(X_test))\n  TN = cm[0][0]\n  TP = cm[1][1]\n  FN = cm[1][0]\n  FP = cm[0][1]\n  print(cm)\n  print('Model[{}] Testing Accuracy = \"{}!\"'.format(i,  (TP + TN) \/ (TP + TN + FN + FP)))\n  print()# Print a new line\n\n#Show other ways to get the classification accuracy & other metrics \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(len(model)):\n  print('Model ',i)\n  #Check precision, recall, f1-score\n  print( classification_report(Y_test, model[i].predict(X_test)) )\n  #Another way to get the models accuracy on the test data\n  print( accuracy_score(Y_test, model[i].predict(X_test)))\n  print()#Print a new line","99e6858f":"#Print Prediction of Random Forest Classifier model\npred = model[6].predict(X_test)\nprint(pred)\n#Print a space\nprint()\n#Print the actual values\nprint(Y_test)","f27e6a54":"\n\n<h1 style='background-color:Red ; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;' > Heart Attack <\/h1>\n\n A heart attack occurs when an artery supplying your heart with blood and oxygen becomes blocked. Fatty deposits build up over time, forming plaques in your heart's arteries. If a plaque ruptures, a blood clot can form and block your arteries, causing a heart attack\n\n\n\n\n<img src=\"https:\/\/www.columbiaasia.com\/malaysia\/sites\/default\/files\/health-article\/health-article-what-is-heart-attack-banner-1.jpg\" width=\"700px\">\n\n\n\n\n<h1 style='background-color:Red ; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;' > Exploratory Data Analysis <\/h1>\n\nExploratory Data Analysis (EDA) is the method of examining a dataset to learn about its main characteristics. The dataprep.eda package makes this step easier by allowing users to explore key characteristics using simple APIs. Each API allows the user to examine the dataset at various levels, from high to low, and from various angles. Especially.\n\n\n<img src=\"https:\/\/latize.com\/wp-content\/uploads\/2017\/11\/business_advantages_of_data_analysis.jpg\" width=\"700px\">\n\n\n\n\n<h1 style='background-color:Red ; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;' > Heart attack classification with 8 Algorithms <\/h1>\n\n\n* 1- Logistic Regression \n* 2- K Nearest Neighbor \n* 3- Support Vector Machine with linear\n* 4- Support Vector Machine with rbf\n* 5- Gaussian Naive Bayes \n* 6- Decision Tree\n* 7- Random Forest Classifier\n* 8- xgboost Classifier\n\n\n\n## Description \n\n1-Age : Age of the patient.\n\n2-Sex : Sex of the patient.\n\n3-exang: exercise induced angina (1 = yes; 0 = no).\n\n4-ca: number of major vessels (0-3).\n\n5-cp : Chest Pain type chest pain type.\n\n* Value 1: typical angina\n* Value 2: atypical angina\n* Value 3: non-anginal pain\n* Value 4: asymptomatic\n\n6-trtbps : resting blood pressure (in mm Hg)\n\n7-chol : cholestoral in mg\/dl fetched via BMI sensor\n\n8-fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n9-rest_ecg : resting electrocardiographic results\n\n* Value 0: normal\n* Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n* Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n10-thalach : maximum heart rate achieved\n\n11-target : 0= less chance of heart attack 1= more chance of heart attack\n\n### Dataset:\n\n[Link](https:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset)"}}