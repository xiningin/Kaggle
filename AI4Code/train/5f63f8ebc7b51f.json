{"cell_type":{"b1beeea8":"code","3721b769":"code","5dc3ae16":"code","1052f1f2":"code","16774c77":"code","71cff2aa":"code","de9951e9":"code","4a784fb1":"code","e929bd42":"code","52936c1d":"code","87a6ecc2":"code","edb20738":"code","57885b55":"code","f1865618":"code","4d0355f7":"code","a941e04d":"code","47e457a0":"code","f5ddad6a":"code","6d64d7c0":"code","0138eca6":"code","e69ec587":"code","b9d47996":"code","8531e729":"markdown","6d132e19":"markdown","06e7ba97":"markdown","8d457095":"markdown","7ca084fd":"markdown","9918b9dd":"markdown","c54bbb77":"markdown","5aa55b4c":"markdown"},"source":{"b1beeea8":"import numpy as np \nimport pandas as pd \n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport zipfile\nwith zipfile.ZipFile('..\/input\/platesv2\/plates.zip', 'r') as zip_obj:\n   zip_obj.extractall('\/kaggle\/working\/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"\/kaggle\/working\/\"))","3721b769":"import shutil\nfrom tqdm import tqdm\nfrom IPython.display import FileLink\n\nimport torch\nimport torchvision \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport copy\n\nfrom torchvision import transforms,models\n\n!pip install git+https:\/\/github.com\/aleju\/imgaug\nfrom imgaug import augmenters as iaa\nimport imgaug as ia\n\nimport PIL\nimport matplotlib as mpl\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5dc3ae16":"data_root = '\/kaggle\/working\/plates\/'\nprint(os.listdir(data_root))","1052f1f2":"train_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","16774c77":"mpl.rcParams['axes.grid'] = False\nmpl.rcParams['image.interpolation'] = 'nearest'\nmpl.rcParams['figure.figsize'] = 45, 75\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\ndef show_dataset(dataset, n=6):\n    img = np.vstack((np.hstack((np.asarray(dataset[i][0].permute(1, 2, 0).numpy() * std + mean )for _ in range(n)))\n                   for i in range(len(dataset))))\n    \n    plt.imshow(img)\n    plt.axis('off')","71cff2aa":"class ImgAugTransform: \n  def __init__(self):\n    self.aug = iaa.Sequential([\n        #iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n        #iaa.Fliplr(0.5),\n        #iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n        iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True),\n        iaa.MultiplyHue(mul = (-1,1)),\n        #iaa.ChannelShuffle(p = 0.5),\n    ])\n      \n  def __call__(self, img):\n    img = np.array(img)\n    return self.aug.augment_image(img)\n\nimg_transforms = ImgAugTransform()","de9951e9":"train_transforms = torchvision.transforms.Compose([\n    transforms.RandomChoice(transforms = [transforms.RandomRotation(degrees = 60),transforms.RandomRotation(degrees = 90)]),\n    transforms.RandomChoice(transforms = [transforms.CenterCrop((224,224)),transforms.RandomCrop((224,224))]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_aug = torchvision.transforms.Compose([\n    ImgAugTransform(),\n    lambda x: PIL.Image.fromarray(x),\n    transforms.RandomChoice(transforms = [transforms.RandomRotation(degrees = 60),transforms.RandomRotation(degrees = 90)]),\n    transforms.RandomChoice(transforms = [transforms.CenterCrop((224,224)),transforms.RandomCrop((224,224))]),\n    transforms.RandomChoice(transforms = [transforms.RandomHorizontalFlip()]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n\nval_transforms = torchvision.transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","4a784fb1":"train_dataset = torchvision.datasets.ImageFolder(train_dir,train_transforms)\nshow_dataset(train_dataset)","e929bd42":"train_dataset = torchvision.datasets.ImageFolder(train_dir,train_transforms_aug)\n\nval_dataset = torchvision.datasets.ImageFolder(val_dir,val_transforms)\n\nshow_dataset(train_dataset)","52936c1d":"batch_size = 8\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size,\n                                               num_workers = 0,shuffle = True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size = batch_size,\n                                             num_workers = 0,shuffle = False)","87a6ecc2":"def train_model(model, loss, optimizer, scheduler, num_epochs,early_stop):\n    \n    loss_history = []\n    acc_history = []\n    \n    best_acc = 0.\n    best_loss = 1000000\n    \n    best_acc_val = 0.\n    best_loss_val = 1000000\n    \n    improve_count = 0\n    for epoch in range(num_epochs):\n        print('\\nEpoch {}\/{}:'.format(epoch, num_epochs - 1), flush=True)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train()  \n            else:\n                dataloader = val_dataloader\n                model.eval()\n\n            running_loss = 0.\n            running_acc = 0.\n            running_clean = 0.\n            running_recall = 0.\n\n            for inputs, labels in dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                \n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean()\n                if sum(labels.data) > 0:\n                    running_recall += (preds_class[labels.data == 1] == 1).float().mean()\n\n            epoch_loss = running_loss \/ len(dataloader)\n            epoch_acc = running_acc \/ len(dataloader)\n            epoch_recall = running_recall \/ len(dataloader)\n            \n            if phase == 'train':\n                \n                improve_count+=1 \n                loss_history.append(epoch_loss)\n                acc_history.append(epoch_acc)\n            print('{} Loss: {:.4f} Acc: {:.4f} Recall: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_recall), flush=True,end = ' ')\n            \n            if(phase == 'train' and best_loss>epoch_loss and epoch_recall <0.96):\n                \n                improve_count = 0\n                \n                best_loss = epoch_loss    \n                best_acc = epoch_acc\n                \n                best_model_train = copy.deepcopy(model)\n                save_epoch_train = epoch\n                \n                print('| save')\n                \n            elif(phase == 'val' and best_loss_val>epoch_loss):\n                \n                best_loss_val = epoch_loss    \n                best_acc_val = epoch_acc\n                \n                best_model_val = copy.deepcopy(model)\n                save_epoch_val = epoch\n                \n                print('| save')\n                \n            elif (phase == 'val' and best_acc_val == epoch_acc and best_loss_val > epoch_loss):\n                \n                best_loss_val = epoch_loss    \n                best_acc_val = epoch_acc\n                \n                best_model_val = copy.deepcopy(model)\n                save_epoch_val = epoch\n                print('| save')\n            \n            else: print('')\n            \n            if(phase == 'val' and improve_count == early_stop):\n                print('\\nLoss does not decrease {} epochs, learning is stopped'.format(early_stop))\n                \n                torch.save(best_model_train.state_dict(), \"best_model_train.pth\")\n                torch.save(best_model_val.state_dict(), \"best_model_val.pth\")\n                \n                print('\\n Saved model from the {}th epoch with the best loss on train'.format(save_epoch_train))\n                print('\\n Saved model from the {}th epoch with the best loss on val'.format(save_epoch_val))\n                \n                return loss_history,acc_history\n            \n    print('\\nSaved model from the {}th epoch with the best loss on train'.format(save_epoch_train))\n    print('\\nSaved model from the {}th epoch with the best loss on val'.format(save_epoch_val))\n    \n    return loss_history,acc_history","edb20738":"vgg16 = models.vgg16(pretrained=True)\n\nfor param in vgg16.features.parameters(): # we train the weights only of the last layers\n    param.requires_grad = False\n\nvgg16.classifier[6] = torch.nn.Linear(vgg16.classifier[6].in_features, 2)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nvgg16 = vgg16.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(vgg16.parameters(), lr=1.0e-2)\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)","57885b55":"train_model(vgg16, loss, optimizer, scheduler, num_epochs=40,early_stop = 10);","f1865618":"model = models.vgg16(pretrained=False)\n\nmodel.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)\n\n#model_keys = torch.load('best_model_train.pth')\nmodel_keys = torch.load('\/kaggle\/input\/vgg-models\/vgg_91.pth') # i load the model that gave the best result in public leaderboard\n\nmodel.load_state_dict(model_keys)\n\nmodel = model.to(device)","4d0355f7":"# FileLink(r'best_model_train.pth')\n# FileLink(r'best_model_val.pth')","a941e04d":"test_dir = 'test'\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","47e457a0":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \ntest_dataset = ImageFolderWithPaths('\/kaggle\/working\/test', val_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","f5ddad6a":"model.eval()\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs, labels = inputs.to(device), labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths)\n    \ntest_predictions = np.concatenate(test_predictions)","6d64d7c0":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})\nsubmission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\nsubmission_df['id'] = submission_df['id'].str.replace('\/kaggle\/working\/test\/unknown\/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(n=6)","0138eca6":"submission_df.label.map({'dirty':1,'cleaned':0}).mean()","e69ec587":"submission_df.to_csv('submission.csv')","b9d47996":"!rm -rf train val test ","8531e729":"Train model","6d132e19":"We can download the models that we have saved","06e7ba97":"I used pretrained model VGG16 and started training several times for the best result","8d457095":"Additional augmentation \n\nimgaug library documantation for experiments:\nhttps:\/\/github.com\/aleju\/imgaug\n\n","7ca084fd":"we will use this augmentation","9918b9dd":"augmentation without color correction","c54bbb77":"Predict","5aa55b4c":"in the test sample 65% of the positive classes, let's check how much we have"}}