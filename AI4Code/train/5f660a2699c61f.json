{"cell_type":{"735c44a5":"code","4ea9940e":"code","9d5c5057":"code","381123fd":"code","dc653b9e":"code","bf2db012":"code","5f31e017":"code","e37a6842":"code","2aa1b6a7":"code","26dc8c5a":"code","564d1d42":"code","d632cd88":"code","adcd775d":"code","a947f121":"code","f1d4db86":"code","931a4f69":"code","111da437":"code","1d523d10":"code","dcc2b5eb":"code","d9b11361":"code","96910352":"code","2027f4e1":"code","1b84e0eb":"code","1165415d":"code","2aa299c0":"code","1daf7776":"code","b26f7d7e":"code","d1111c52":"code","904a2307":"code","89dd2039":"code","227e9967":"code","3b0a3377":"code","eb92d7f8":"code","d7898d79":"code","21fc1078":"code","4d3619ec":"code","04b87093":"code","9079cd91":"code","e6451d92":"code","fbee94e7":"code","d64d39c4":"code","4c312524":"code","b017c59f":"code","6683151a":"code","219769fd":"code","26df7d46":"code","e14224ec":"markdown","531711e2":"markdown","fd132078":"markdown","d7215205":"markdown","c1a4f7bb":"markdown","0959fc09":"markdown","cd3e7dc0":"markdown","6087f949":"markdown","57e7be52":"markdown","b4ac16eb":"markdown","df37c2a2":"markdown","1b26e4ab":"markdown","3fbb0884":"markdown","2edb3922":"markdown","b299c70f":"markdown"},"source":{"735c44a5":"# Libraries for working with data\nimport numpy as np \nimport pandas as pd\n\n# libraries for visualizing data\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Libraries for pre-processing data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# Libraries for machine learning\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier,RidgeClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Libraries for eveluating model performance\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n\n#########################################################\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_columns', None)","4ea9940e":"# load data\ntrain = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')","9d5c5057":"display(train.info())\ntrain.head(3)","381123fd":"display(test.info())\ntest.head(3)","dc653b9e":"# Let's dig deeper \n# Exploring null values\n\nfrom statistics import mean\ncols = train.drop(['id', 'song_popularity'], axis = 1).columns.tolist()\ndef info(data):\n    \n    print(f'Length of data: {len(data)}')\n    \n    print('')\n    \n    x = pd.Series([])\n    for i in data.columns.tolist():\n        x = x.append(pd.Series([data[i].dtypes]))\n#     print(x)\n    print(x.value_counts().to_frame().reset_index().rename(columns={0: 'count', 'index': 'type'}))\n    \n    print('')\n    \n    flag = True\n    for i in cols:\n        if data[i].isna().sum() == 0:\n            flag = False\n            break\n            \n    print(f'All features have missing values: {flag}\\n')\n    \n    list_na = []\n    for i in cols:\n        list_na.append(data[i].isna().sum())\n    print(f'Mean of missing values is {mean(list_na)} ({round((mean(list_na)\/len(data)) * 100,2)}%)\\n')\n    print(f'Max of missing values has {cols[list_na.index(max(list_na))]}: {max(list_na)} ({round((max(list_na)\/len(data)) * 100,2)}%)\\n')\n    print(f'Min of missing values has {cols[list_na.index(min(list_na))]}: {min(list_na)} ({round((min(list_na)\/len(data)) * 100,2)}%)\\n')\n\nprint('TRAINING DATASET INFORMATION')\nprint('')\ninfo(train)\nprint('---------------------------------------------')\nprint('TEST DATASET INFORMATION')\nprint('')\ninfo(test)","bf2db012":"# Okay, this is boring and technical \n#Let's do the same thing with the help of a library\n# !pip install dataprep\n# from dataprep.eda import plot_missing\nimport missingno as msno\ndisplay(msno.matrix(train))\n# plot_missing(train)\n","5f31e017":"display(msno.matrix(test))\n# plot_missing(test)","e37a6842":"train['isTrain'] = True\ntest['isTrain'] =  False\ntt = pd.concat([train,test]).reset_index(drop=True).copy()","2aa1b6a7":"tt.info()","26dc8c5a":"train.isna().sum()","564d1d42":"# percentage of missing values\nncounts = pd.DataFrame([train.isna().mean(), test.isna().mean()]).T\nncounts = ncounts.rename(columns={0: 'train_missing',1: 'test_missing'})\n# Filter out the values which are not-null and then plot it\nncounts.query('train_missing > 0').plot(\n    kind='barh', figsize=(12,5), title=' % of missing values')\n","d632cd88":"# List of columns that contain na values\nnacols = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n       'instrumentalness', 'key', 'liveness', 'loudness']","adcd775d":"tt['n_missing'] = tt[nacols].isna().sum(axis= 1)\ntrain['n_missing'] = train[nacols].isna().sum(axis= 1)\ntest['n_missing'] = test[nacols].isna().sum(axis= 1)","a947f121":"tt['n_missing'].value_counts()","f1d4db86":"tt['n_missing'].value_counts().plot(\n    kind= 'bar', title= 'Number of missing values per sample'\n)","931a4f69":"tt.query('n_missing == 6')","111da437":"tt['audio_mode'].value_counts()","1d523d10":"cat_features = ['key', 'audio_mode']\ntt.groupby('audio_mode')['n_missing'].mean()","dcc2b5eb":"tt.groupby(tt.time_signature)['n_missing'].mean()","d9b11361":"tt.groupby(tt.time_signature)['n_missing'].agg(['mean','count'])","96910352":"tt.groupby('song_popularity')['n_missing'].mean()","2027f4e1":"# getting rid of the null values\ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)","1b84e0eb":"groups = {1 : 'Song is popular', 0 : 'Song is Not-popular'}\ntrain['song_popularity'] = train['song_popularity'].map(groups)\n","1165415d":"group_by_target = train.groupby('song_popularity').agg({'song_popularity': 'count'}).rename(columns = {'song_popularity': 'count'}).reset_index()\ngroup_by_target","2aa299c0":"fig = px.pie(group_by_target, values = group_by_target['count'], names = group_by_target['song_popularity'])\nfig.update_traces(textposition = 'inside', \n                  textinfo = 'percent + label', \n                  hole = 0.80, \n                  marker = dict(colors = ['#2A3132','#336B87'], line = dict(color = 'white', width = 2)))\n\nfig.update_layout(title_text = '\ud83c\udfb6 distribution', title_x = 0.5, title_y = 0.53, title_font_size = 32, title_font_family = 'Calibri', title_font_color = 'black',\n                  showlegend = False)\n                  \nfig.show()","1daf7776":"groups = {'Song is popular': 1,'Song is Not-popular': 0}\ntrain['song_popularity'] = train['song_popularity'].map(groups)","b26f7d7e":"train_count = train.drop('id',axis= 1)\nfor c in train_count.columns:\n        fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n        a=sns.boxplot(y=c, data=train_count, ax=axs[0])\n\n        b=sns.violinplot(y=c, data=train_count, ax=axs[1]) \n\n        c=sns.stripplot(y=c, data=train_count, size=4, color=\".3\", linewidth=0, ax=axs[2]) \n        \n        for i in [a,b,c]:\n            for j in ['right', 'left', 'top']:\n                i.spines[j].set_visible(False)\n                i.spines['bottom'].set_linewidth(1.2)\n\n\n        fig.suptitle(c, fontsize=15, y=1.1)\n        axs[0].set_title('Box Plot')\n        axs[1].set_title('Violin Plot')\n        axs[2].set_title('Strip Plot')\n\n        plt.tight_layout()\n        plt.show()","d1111c52":"# comparing test and training data\ntrain_count = train_count.drop('song_popularity', axis= 1)\ntest_count = test.drop('id',axis= 1)\n\nfor c in train_count.columns:\n    fig = plt.subplots(1, 1, figsize=(18, 5))\n    plt.title(c, size = 12, fontname = 'monospace')\n    a = sns.kdeplot(x= c, data=train_count , color = '#195CB8', linewidth = 1.3,shade=True, alpha=.85)\n    sns.kdeplot(x= c, data=test_count ,color = '#fc2626', linewidth = 1.3,shade=True, alpha=0.35)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n    plt.tight_layout()\n    plt.figtext(0.365, 1.02, 'Distribution of features', color = '#2f3131', fontname = 'monospace', size = 25)\n    plt.figtext(0.2, 1.01, 'Train', color = '#195CB8', fontname = 'monospace', size = 18)\n    plt.figtext(0.8, 1.01, 'Test', color = '#953553', fontname = 'monospace', size = 18)\n    plt.show()","904a2307":"#Heatmap\nmatrix = np.triu(train.drop('id', axis = 1).corr())\nplt.figure(figsize = (18, 12))\nsns.heatmap(train.drop('id', axis = 1).corr(), annot = False, cmap = 'Greens', mask = matrix, linewidths = 0.1, linecolor = 'white', cbar = True)\nplt.xticks(size = 12, fontname = 'monospace')\nplt.yticks(size = 12, fontname = 'monospace')\nplt.figtext(0.77, 0.8, '''There seems to be some correlation ''', fontsize = 20, fontname = 'monospace', ha = 'right', color = '#f9ba32')\nplt.show()","89dd2039":"#Since there is a high corellation between energy and loudness\ng = sns.jointplot(x=\"loudness\", y=\"energy\", data=train, kind=\"kde\", color=\"b\")\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"Loudness\", \"Energy\");","227e9967":"# Energy and acousticness have a negative correlation, let's explore that \ng = sns.jointplot(x=\"acousticness\", y=\"energy\", data=train, kind=\"kde\", color=\"b\")\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"Acousticness\", \"Energy\");","3b0a3377":"train.instrumentalness.hist(bins=200,grid=False)","eb92d7f8":"a = np.log10(train.instrumentalness)\na.hist(bins=200,grid=False)","d7898d79":"# from pandas_profiling import ProfileReport\n# ProfileReport(train)","21fc1078":"# I will impute the missing values later. Let's apply ML after removing all the null values\n#Splitting the test data into training and test set \nX_train = train.drop('song_popularity',axis=1)\ny_train = train.song_popularity\ntest.drop('id',axis= 1, inplace = True)\n#Cross validation\ncv = KFold(n_splits=5, shuffle=True, random_state=42)","4d3619ec":"#Freeing up some memory\n#The function is taken from https:\/\/www.kaggle.com\/rinnqd\/reduce-memory-usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","04b87093":"reduce_mem_usage(X_train)\nreduce_mem_usage(pd.DataFrame(y_train))\nreduce_mem_usage(test)","9079cd91":"%%time\nfrom sklearn.linear_model import  LogisticRegression\nLogReg = LogisticRegression()\nscores = cross_val_score(LogReg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nLogRegScore = np.sqrt(-scores)\nprint('Logistic Regression Performance', LogRegScore)\nprint(f'Mean score: {LogRegScore.mean():.5f}')","e6451d92":"%%time\nfrom sklearn.linear_model import RidgeClassifier\nRegCla = RidgeClassifier()\nscores = cross_val_score(RegCla, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nmodelScore = np.sqrt(-scores)\nprint('Model Performance', modelScore)\nprint(f'Mean score: {modelScore.mean():.5f}')","fbee94e7":"results = pd.DataFrame(columns = ['LR', 'RC', 'DTC'], index = range(4))\nresults","d64d39c4":"\nnum_cols = ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness','liveness', 'loudness','speechiness', 'tempo', 'audio_valence']\ncat_cols = ['key','audio_mode','time_signature']","4c312524":"def label_encoder(df):\n    for i in cat_cols:\n        le = LabelEncoder()\n        df[i] = le.fit_transform(df[i])\n    return df","b017c59f":"from sklearn.preprocessing import LabelEncoder\nsc = StandardScaler()\nX_train[num_cols] = sc.fit_transform(X_train[num_cols])\n\n# Label encoding\nX_train = label_encoder(X_train)\n\nX_train.head()","6683151a":"X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 228)","219769fd":"lg = LogisticRegression(random_state = 228)\nlg.fit(X_train, y_train)\ny_pred = lg.predict(X_test)\ny_prob = lg.predict_proba(X_test)[:,1]\n\n# Metrics\nresults.iloc[0, 0] = round(precision_score(y_test, y_pred), 2)\nresults.iloc[1, 0] = round(recall_score(y_test, y_pred), 2)\nresults.iloc[2, 0] = round(f1_score(y_test, y_pred), 2)\nresults.iloc[3, 0] = round(roc_auc_score(y_test, y_prob), 3)\nlg_cm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {round(roc_auc_score(y_test, y_prob), 3)}')\nprint('')\nprint('-----------------------------------------------------')\nprint('')\nprint('Cross-validation scores with 5 folds:')\nprint('')\nprint(f\"ROC AUC: {round(cross_val_score(lg, X_train, y_train, cv = 5, scoring = 'roc_auc').mean(), 3)}\")\nprint(f\"precision: {round(cross_val_score(lg, X_train, y_train, cv = 5, scoring = 'precision').mean(), 2)}\")\nprint(f\"recall: {round(cross_val_score(lg, X_train, y_train, cv = 5, scoring = 'recall').mean(), 2)}\")\nprint(f\"f1: {round(cross_val_score(lg, X_train, y_train, cv = 5, scoring = 'f1').mean(), 2)}\")\n\n# Visualize confusion matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(lg_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Song is not-popular', 'Song is popular'], xticklabels = ['Predicted song is popular', 'Predicted song is not popular'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# Feature importance\nf_imp = pd.DataFrame(columns = ['feature', 'importance (abs coef)'], index = range(14))\nfor i in range(len(f_imp.index)):\n    f_imp.iloc[i, 0] = X_train.columns.to_list()[i]\nf_imp['importance (abs coef)'] = abs(lg.coef_)[0]\nf_imp = f_imp.sort_values('importance (abs coef)', ascending = False)\nf_imp[0:12].style.background_gradient(cmap = 'Blues')","26df7d46":"rf = RandomForestClassifier(random_state = 228, max_depth = 5)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\ny_prob = rf.predict_proba(X_test)[:,1]\n\n# Metrics\nresults.iloc[0, 1] = round(precision_score(y_test, y_pred), 2)\nresults.iloc[1, 1] = round(recall_score(y_test, y_pred), 2)\nresults.iloc[2, 1] = round(f1_score(y_test, y_pred), 2)\nresults.iloc[3, 1] = round(roc_auc_score(y_test, y_prob), 3)\nrf_cm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {round(roc_auc_score(y_test, y_prob), 3)}')\nprint('')\nprint('-----------------------------------------------------')\nprint('')\nprint('Cross-validation scores with 5 folds:')\nprint('')\nprint(f\"ROC AUC: {round(cross_val_score(rf, X_train, y_train, cv = 5, scoring = 'roc_auc').mean(), 3)}\")\nprint(f\"precision: {round(cross_val_score(rf, X_train, y_train, cv = 5, scoring = 'precision').mean(), 2)}\")\nprint(f\"recall: {round(cross_val_score(rf, X_train, y_train, cv = 5, scoring = 'recall').mean(), 2)}\")\nprint(f\"f1: {round(cross_val_score(rf, X_train, y_train, cv = 5, scoring = 'f1').mean(), 2)}\")\n\n# Visualize confusion matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(lg_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Song is not-popular', 'Song is popular'], xticklabels = ['Predicted song is popular', 'Predicted song is not popular'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# Feature importance\nf_imp2 = pd.DataFrame(columns = ['feature', 'importance'], index = range(14))\nfor i in range(len(f_imp2.index)):\n    f_imp2.iloc[i, 0] = X_train.columns.to_list()[i]\nf_imp2['importance'] = rf.feature_importances_\nf_imp2 = f_imp2.sort_values('importance', ascending = False)\nf_imp2[0:12].style.background_gradient(cmap = 'Blues')","e14224ec":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# 3. EDA\n\n<a id='general'><\/a>\n## 3.1 General\n\n**Observations:**\n1. The Training data consists of 40,000 rows and 15 columns\n2. The Testing  data consists of 10,000 rows and 14 columns\n3. There are missing values in both the training and testing dataset\n4. Target variable is binary ","531711e2":"# Table of Contents\n\n<a id=\"table-of-contents\"><\/a>\n1. [Introduction](#introduction)\n2. [Preparation](#preparation)\n3. [EDA](#eda)\n    * 3.1 [General](#general)\n    * 3.2 [Null values](#null_values)\n    * 3.3 [Univariate analysis](#univariate_analysis)\n    * 3.4 [Pandas profiling](#pandas_profiline)    \n4. [Data Preprossing](#data_preprossing)\n    * 4.1 [Machine Learning](#machine_learning)\n    * 4.2 [Logestic Regression in depth](#logestic_regression)","fd132078":"[back to top](#table-of-contents)\n<a id='machine_learning'><\/a>\n## 4.1 Machine Learning \ud83e\udde0\n\nWe will start with some simple models\n\n### 1. Logistic Regression","d7215205":"[back to top](#table-of-contents)\n<a id='pandas_profiline'><\/a>\n## 3.3 Pandas Profiling\n","c1a4f7bb":"### Is there any imbalance in the missing values when splitting by other features","0959fc09":"### 2. Ridge Classifier","cd3e7dc0":"[back to top](#table-of-contents)\n<a id='data_preprossing'><\/a>\n## 4 Data Prepossing","6087f949":"### Counts of missing values in train vs test ","57e7be52":"[back to top](#table-of-contents)\n<a id=\"introduction\"><\/a>\n# 1. Introduction\nThe goal of this notebook is to provide a fun, and intresting way of exporing data and make a machine learning model for this  tabular dataset. \n\n[MLSpace](https:\/\/github.com\/abhishekkrthakur\/mlspace) is a no-hassle tool for data science, machine learning and deep learning\n\n","b4ac16eb":"[back to top](#table-of-contents)\n<a id='logestic_regression'><\/a>\n## 4.2 Logestic Regression in depth\n","df37c2a2":"[back to top](#table-of-contents)\n<a id='null_values'><\/a>\n## 3.2 Null Values","1b26e4ab":"[back to top](#table-of-contents)\n<a id=\"preparation\"><\/a>\n# 2. Prepration","3fbb0884":"[back to top](#table-of-contents)\n<a id='univariate_analysis'><\/a>\n## 3.2 univariate Analysis\n","2edb3922":"### missing values per observation\n","b299c70f":"### Prep- create tag columns with missing indicators"}}