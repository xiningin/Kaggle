{"cell_type":{"e0603e93":"code","f0120b4f":"code","b44e053f":"code","a6e41b37":"code","3e0a7f9c":"code","d520bcac":"code","426d59a5":"code","d6f49abb":"code","8b898514":"code","3b0235df":"code","3055dbb9":"code","71abc07e":"code","72005ffa":"code","f94c2a7d":"code","1245b7b4":"code","361bbe24":"code","05480545":"code","e1bcf91d":"code","fe73537a":"code","b6fa9dcc":"code","309169ed":"code","1f87fa67":"code","10f6a02d":"code","5fc07d80":"code","a18c1915":"code","d06c73a9":"code","c47f16b4":"code","eb1a6c19":"code","3c7cf06d":"code","abb7e544":"code","ca4a99ed":"code","78b632fd":"code","6f6b575f":"code","12483fee":"code","f618ad52":"code","be9eea33":"code","0ce5e5d8":"code","08572e30":"code","769ab344":"code","cc59e21d":"code","9f7b293f":"code","950fff2e":"code","3a760072":"markdown","6b2691b7":"markdown","4d9dc361":"markdown","1223278a":"markdown","885b96a1":"markdown","3a667087":"markdown","773d70d2":"markdown","74b7d605":"markdown","7c9cef64":"markdown","fe16a1b6":"markdown","0db7fcfb":"markdown","7110ed07":"markdown","65445af1":"markdown","37b702d6":"markdown","ebe8e7bc":"markdown","03e46c27":"markdown","cc810f03":"markdown","89909ac7":"markdown","75963703":"markdown","38f152b3":"markdown","68f4c105":"markdown","18d19c0d":"markdown","d5be9321":"markdown","71a92eae":"markdown","de1c451f":"markdown","81e619f5":"markdown","69fc4cc6":"markdown","2cb3232c":"markdown","136924f4":"markdown"},"source":{"e0603e93":"# Data manipulation, visualization\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n!pip install openpyxl\n\n# Preprocessing, modeling, and evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter","f0120b4f":"path = '\/kaggle\/input\/telco-customer-churn-1113\/'  # Read in datasets\nstat = pd.read_excel(path+'Telco_customer_churn_status.xlsx')\ndemo = pd.read_excel(path+'Telco_customer_churn_demographics.xlsx')\nserv = pd.read_excel(path+'Telco_customer_churn_services.xlsx')\n\nkey = ['Customer ID']\ndf = stat.merge(  # Merge into a single dataframe\n    demo, left_on=key, right_on=key).merge(\n    serv, left_on=key, right_on=key)\ndf.head()","b44e053f":"to_drop = [  # Drop columns not used in the analyses\n    'Customer ID',\n    'Count_x',\n    'Quarter_x',\n    'Customer Status',\n    'Churn Value',\n    'Churn Score',\n    'Churn Category',\n    'Churn Reason',\n    'Count_y',\n    'Age',\n    'Number of Dependents',\n    'Quarter_y',\n    'Referred a Friend',\n    'Number of Referrals',\n    'Phone Service',\n    'Internet Service',\n    'Streaming Music',\n    'Total Charges',\n    'Total Refunds',\n    'Total Extra Data Charges',\n    'Total Long Distance Charges',\n    'Total Revenue']\ndf.drop(to_drop, axis=1, inplace=True)","a6e41b37":"df.info()","3e0a7f9c":"df.isna().sum()  # Check for missing values","d520bcac":"df.duplicated().sum()  # Check for duplicated data","426d59a5":"df.describe()  # Obtain statistical summary: numeric data","d6f49abb":"df.describe(include='O')  # Obtain statistical summary: categorical data","8b898514":"cpal = {'No':'#76528BFF', 'Yes':'#DF6589FF'}\n\ndef visualize_labels(img_title, df, col):\n    fig, ax = plt.subplots(figsize=(6,5))\n    sns.countplot(data=df, x=col, palette=cpal)\n    for bar in ax.patches:\n        height = bar.get_height()\n        width = bar.get_width()\n        col_total = df[col].count()\n        pct = 100*height\/(col_total)\n        ax.text(bar.get_x() + width\/2,\n                bar.get_y() + height\/2,\n                f'{int(height)}',\n                ha='center', va='center',\n                color='white', weight='bold')\n        ax.text(bar.get_x() + width\/2,\n                bar.get_y() + height\/2 - 300,\n                f'({pct:.1f}%)',\n                ha='center', va='center',\n                color='white', weight='bold')\n    for pos in ['right', 'top', 'left']:\n        ax.spines[pos].set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax.set_xlabel(None)\n    ax.set_title(f'{col}', loc='left', weight='bold')\n    fig.savefig(f'{img_title}.png', dpi=300, bbox_inches='tight')\n    \ndef stack_bar(img_title, df, cols):\n    fig, axes = plt.subplots(1, 3, figsize=(16,5), sharey=True)\n    for i, col in enumerate(cols):\n        # Calculate totals and percentages\n        total = df.groupby(col)['Count'].sum().reset_index()\n        churn = df[df['Churn Label']=='Yes'].groupby(col)['Count'].sum().reset_index()\n        churn['Count'] = [100*i\/j for i,j in zip(churn['Count'], total['Count'])]\n        total['Count'] = [100*i\/j for i,j in zip(total['Count'], total['Count'])]\n        bar1 = sns.barplot(  # top bars (group of 'Churn = No')\n            x=col, y='Count', data=total, color=cpal['No'], ax=axes[i])\n        bar2 = sns.barplot(  # bottom bars (group of 'Churn = Yes')\n            x=col, y='Count', data=churn, color=cpal['Yes'], ax=axes[i])\n        top_bar = mpatches.Patch(color=cpal['No'], label='Not Churn')\n        bot_bar = mpatches.Patch(color=cpal['Yes'], label='Churn')  \n        axes[i].legend(loc='lower center', ncol=2,\n                       bbox_to_anchor=(0.5, -0.21),\n                       handles=[top_bar, bot_bar])\n        axes[i].spines['top'].set_visible(False)\n        axes[i].spines['right'].set_visible(False)\n        axes[i].set_xlabel(None)\n        axes[i].set_ylabel('Percentage')\n        axes[i].set_title(f'{col}', loc='left', weight='bold')  \n        if i != 0:\n            axes[i].legend().set_visible(False)\n            axes[i].set_ylabel(None)\n        else:\n            pass\n        for bar in axes[i].patches:\n            if bar.get_height() != 100:\n                axes[i].text(bar.get_x() + bar.get_width()\/2,\n                             bar.get_y() + bar.get_height()\/2,\n                             f'{int(bar.get_height())}%',\n                             ha='center', va='center',\n                             color='white', weight='bold')\n    fig.savefig(f'{img_title}.png', dpi=300, bbox_inches='tight')\n\ndef unstack_bar(img_title, df, col):\n    fig, ax = plt.subplots(figsize=(6,5))\n    ax = sns.countplot(data=df, x=col, hue='Churn Label', palette=cpal)\n    ax.legend(loc='lower center', ncol=2,\n              bbox_to_anchor=(0.5, -0.21),\n              labels=['Churn = Yes', 'Churn = No'])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xlabel(None)\n    ax.set_ylabel('Count')\n    ax.set_title(f'{col}', loc='left', weight='bold')\n    fig.savefig(f'{img_title}.png', dpi=300, bbox_inches='tight')\n\ndef make_boxplot(img_title, df, cols):\n    fig, axes = plt.subplots(1, 4, figsize=(18,5))\n    for i, col in enumerate(cols):\n        ax = sns.boxplot(data=df, x='Churn Label', y=col, palette=cpal, ax=axes[i])\n        ax.set_xlabel('Churn')\n        ax.set_ylabel(None)\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.set_title(f'{col}', loc='left', weight='bold')\n    fig.savefig(f'{img_title}.png', dpi=300, bbox_inches='tight')\n\ndef make_histogram(img_title, df, col):\n    fig, axes = plt.subplots(figsize=(6,5))\n    ax = sns.histplot(x=col, data=df, hue='Churn Label', palette=cpal,\n                      bins=6, multiple='stack')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xlabel(None)\n    ax.set_ylabel('Count')\n    ax.set_title(f'{col}', loc='left', weight='bold')\n    fig.savefig(f'{img_title}.png', dpi=300, bbox_inches='tight')","3b0235df":"visualize_labels('img-01', df, 'Churn Label')","3055dbb9":"categorical_cols = [  # List columns of categorical data type\n    'Gender',\n    'Under 30',\n    'Senior Citizen',\n    'Married',\n    'Dependents',\n    'Offer',\n    'Multiple Lines',\n    'Internet Type',\n    'Online Security',\n    'Online Backup',\n    'Device Protection Plan',\n    'Premium Tech Support',\n    'Streaming TV',\n    'Streaming Movies',\n    'Unlimited Data',\n    'Contract',\n    'Paperless Billing',\n    'Payment Method']\nprint('PROPORTION OF CATEGORIES ACROSS VARIABLES')\nfor col in categorical_cols:\n    freq = df[col].value_counts(normalize=True).reset_index()\n    freq.columns = [f'{col}', 'Proportion']\n    print('-'*40+'\\n', freq)","71abc07e":"stack_bar('img-02', df, ['Gender', 'Under 30', 'Senior Citizen'])","72005ffa":"stack_bar('img-03', df, ['Married', 'Dependents', 'Offer'])","f94c2a7d":"stack_bar('img-04', df, ['Multiple Lines', 'Internet Type', 'Online Security'])","1245b7b4":"stack_bar('img-05', df, ['Online Backup', 'Device Protection Plan', 'Premium Tech Support'])","361bbe24":"stack_bar('img-06', df, ['Streaming TV', 'Streaming Movies', 'Unlimited Data'])","05480545":"stack_bar('img-07', df, ['Contract', 'Paperless Billing', 'Payment Method'])","e1bcf91d":"unstack_bar('img-08', df, 'Satisfaction Score')","fe73537a":"numerical_cols = [  # List columns of numerical data type\n    'CLTV',\n    'Tenure in Months',\n    'Avg Monthly Long Distance Charges',\n    'Avg Monthly GB Download',\n    'Monthly Charge']\n\nmed_churn_y = []\nmed_churn_n = []\nfor col in numerical_cols:\n    med_churn_y.append(df[df['Churn Label']=='Yes'][col].median())\n    med_churn_n.append(df[df['Churn Label']=='No'][col].median())\n\nmedians = pd.DataFrame(\n    index=numerical_cols,\n    data={\n        'Median_Churn_Yes': med_churn_y,\n        'Median_Churn_No': med_churn_n\n    })\nmedians","b6fa9dcc":"make_boxplot('img-09', df, ['CLTV', 'Avg Monthly Long Distance Charges', 'Avg Monthly GB Download', 'Monthly Charge'])","309169ed":"make_histogram('img-10', df, 'Tenure in Months')","1f87fa67":"df.head()","10f6a02d":"binary_cols = [col for col in df.columns if df[col].nunique() == 2]\nencode_dict = {  # Encoding dictionary\n    'Female':0, 'Male':1, 'No':0, 'Yes':1}\nfor col in binary_cols:\n    df[col] = df[col].map(encode_dict)\n\ndummy_cols = [  # Columns to one-hot encode\n    'Satisfaction Score',\n    'Offer',\n    'Internet Type',\n    'Contract',\n    'Payment Method']\ndf = pd.get_dummies(df, columns=dummy_cols)\n\n# Remove unnecessary column\ndf.drop('Count', axis=1, inplace=True)\n\nX = df.drop('Churn Label', axis=1)  # Select features\ny = df['Churn Label']  # Target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)  # Split data 80\/20\n\nscaler = MinMaxScaler()  # Normalize train & test features\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nover = RandomOverSampler(random_state=1)  # Oversample training data\nX_train_res, y_train_res = over.fit_resample(X_train, y_train.ravel())","5fc07d80":"print('-'*45+'\\nCLASS PROPORTION'+'\\n'+'-'*45,\n      f'\\nBefore resampling: {Counter(y_train)}',\n      f'\\nAfter resampling : {Counter(y_train_res)}')","a18c1915":"print(f'SAMPLE FEATURES:\\n{X_train_res[0]}',\n      f'\\n\\nSAMPLE TARGETS:\\n{y_train_res[:30]}')","d06c73a9":"# Define instances of classifier\nlogit = LogisticRegression(random_state=1)\ndtree = DecisionTreeClassifier(random_state=1)\nneigh = KNeighborsClassifier()\n\nfor clf in [logit, dtree, neigh]:  # Display current parameters\n    print('-'*40+f'\\n{clf.__class__.__name__} parameters\\n'+'-'*40)\n    display(clf.get_params())","c47f16b4":"# Learning: Fit models on training data\nlogit.fit(X_train_res, y_train_res)\ndtree.fit(X_train_res, y_train_res)\nneigh.fit(X_train_res, y_train_res)\n\n# Make predictions on testing data\nlogit_pred = logit.predict(X_test)\ndtree_pred = dtree.predict(X_test)\nneigh_pred = neigh.predict(X_test)","eb1a6c19":"def print_reports(classifiers, predictions, y_test):\n    reports = []\n    for clf, pred in zip(classifiers, predictions):\n        print('-'*55, f'\\n{clf.__class__.__name__}', '\\n'+'-'*55)\n        print(classification_report(y_test, pred, digits=4))\n        reports.append(\n            classification_report(\n                y_test, pred, output_dict=True))\n    return reports","3c7cf06d":"reports = print_reports(  # Display classification metrics\n    [logit, dtree, neigh],\n    [logit_pred, dtree_pred, neigh_pred],\n    y_test)","abb7e544":"# Specify grids of hyperparameters to try\nlogit_grid = {'C': [.1, 1, 10],\n              'penalty': ['l1', 'l2'],\n              'solver': ['liblinear']}\ndtree_grid = {'criterion': ['gini', 'entropy'],\n              'max_depth': [5, 7, 8, 9, 10]}\nneigh_grid = {'n_neighbors': [3, 4, 5, 6, 7],\n              'p': [1, 2]}\n\n# Create instances of GridSearchCV object\ngsc_logit = GridSearchCV(logit, logit_grid, scoring='recall', cv=5)\ngsc_dtree = GridSearchCV(dtree, dtree_grid, scoring='recall', cv=5)\ngsc_neigh = GridSearchCV(neigh, neigh_grid, scoring='recall', cv=5)","ca4a99ed":"gsc_logit.fit(X_train_res, y_train_res)","78b632fd":"gsc_dtree.fit(X_train_res, y_train_res)","6f6b575f":"gsc_neigh.fit(X_train_res, y_train_res)","12483fee":"for clf, gsc in zip([logit, dtree, neigh], [gsc_logit, gsc_dtree, gsc_neigh]):\n    print('-'*70, f'\\n{clf.__class__.__name__}', '\\n'+'-'*70)\n    print(f'Best parameters: {gsc.best_params_}')\n    print(f'Best recall    : {gsc.best_score_*100:.2f}%\\n')","f618ad52":"# Define instances of classifier with tuned parameters\nlogit2 = LogisticRegression(random_state=1, C=10, penalty='l1', solver='liblinear')\ndtree2 = DecisionTreeClassifier(random_state=1, criterion='entropy', max_depth=10)\nneigh2 = KNeighborsClassifier(n_neighbors=4, p=1)\n\n# Learning: Fit models on training data\nlogit2.fit(X_train_res, y_train_res)\ndtree2.fit(X_train_res, y_train_res)\nneigh2.fit(X_train_res, y_train_res)\n\n# Make predictions on testing data\nlogit2_pred = logit2.predict(X_test)\ndtree2_pred = dtree2.predict(X_test)\nneigh2_pred = neigh2.predict(X_test)","be9eea33":"reports2 = print_reports(  # Classification report\n    [logit2, dtree2, neigh2],  # After tuning\n    [logit2_pred, dtree2_pred, neigh2_pred],\n    y_test)","0ce5e5d8":"def compare_metrics(reports):\n    metrics_logit = []\n    metrics_dtree = []\n    metrics_neigh = []\n    metrics_data = [metrics_logit, metrics_dtree, metrics_neigh]\n    for i, metric in enumerate(metrics_data):\n        metric.append(round(reports[i]['accuracy']*100, 2))\n        metric.append(round(reports[i]['1']['precision']*100, 2))\n        metric.append(round(reports[i]['1']['recall']*100, 2))\n        metric.append(round(reports[i]['1']['f1-score']*100, 2))\n    metrics_cols = ['%Accuracy', '%Precision', '%Recall', '%F1-Score']\n    metrics_idx = ['Logistic Regression', 'Decision Tree', 'KNN']\n    metrics_df = pd.DataFrame(metrics_data, index=metrics_idx, columns=metrics_cols)\n    return metrics_df","08572e30":"# Show metrics of Class 1 (Churn = Yes) prediction\nmetrics_df = pd.concat([compare_metrics(reports),\n    compare_metrics(reports2).rename(index= lambda s: s+' Tuned')])\nmetrics_df.sort_values(by=['%Accuracy'], ascending=False, inplace=True)\nmetrics_df.style.background_gradient(cmap='Blues').format(\"{:.1f}\")","769ab344":"def plot_confusion(img_title, classifiers, X_test, y_test):\n    fig, axes = plt.subplots(2, 3, figsize=(16,12))\n    for i, ax, clf in zip(range(7), axes.flatten(), classifiers):\n        plot_confusion_matrix(\n            clf, X_test, y_test, ax=ax,\n            display_labels=['Stay', 'Churn'],\n            cmap='Purples', colorbar=False)\n        if i in [0, 1, 2]:\n            ax.set_title(clf.__class__.__name__)\n        else:\n            ax.set_title(f'{clf.__class__.__name__}_Tuned')\n        if i not in [0, 3]:\n            ax.set_ylabel(None)\n    fig.savefig(f'{img_title}.png', dpi=300, bbox_inches='tight')\n\ndef plot_rocs(img_title, classifiers, classifiers2, X_test, y_test):\n    fig, axes = plt.subplots(1, 2, figsize=(16,5))\n    for i, ax in zip(range(3), axes.flatten()):\n        if i == 0:\n            for clf in classifiers:\n                plot_roc_curve(clf, X_test, y_test, ax=ax, lw=2)\n            ax.set_title('ROC curve', loc='left', weight='bold')\n        else:\n            for clf in classifiers2:\n                plot_roc_curve(clf, X_test, y_test, ax=ax, lw=2)\n            ax.set_title('ROC curve - Tuned hyperparameters',\n                         loc='left', weight='bold')\n        ax.plot([0, 1], [0, 1], linestyle='--', lw=2)\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n    fig.savefig(f'{img_title}.png', dpi=300, bbox_inches='tight')","cc59e21d":"classifiers = [logit, dtree, neigh]  # Before tuning\nclassifiers2 = [logit2, dtree2, neigh2]  # After tuning\n\nplot_confusion(  # Show confusion matrices\n    'img-11', classifiers+classifiers2, X_test, y_test)","9f7b293f":"plot_rocs(  # Show ROC curves and AUC scores\n    'img-12', classifiers, classifiers2, X_test, y_test)","950fff2e":"logit2_coef = pd.DataFrame({\n    'feature': list(X.columns),\n    'coefficient': [i for i in logit2.coef_[0]]\n}).sort_values('coefficient', ascending=False)\nlogit2_coef","3a760072":"# Data","6b2691b7":"We see that new customers (i.e., tenure under 12 months) have the highest cases of churn. On the other hand, it is observed that longer tenures translate into lower proportions of churn, especially in the case of very loyal customers (i.e., tenure over 60 months)","4d9dc361":"- About 52% customers are not married. These customers have 13% higher churn rate than married ones.\n- Having dependents (e.g., children) reduce the likelihood of churn. There is only 6% churn among customers that have dependents, about 5 times lower than customers that do not.\n- More than 50% of customers who subscribed to *Offer E* have churned in the past quarter, almost twice as high as customers who do not subscribe to any offer.","1223278a":"Our target in this prediction is whether or not a customer will churn, represented by the column `Churn Label`. Looking at the proportion of class labels in said column, however, we see an imbalance. There is an unequal distribution between `Churn Label = Yes` and `Churn Label = No`. A majority of the churn data are labeled `No` while this case places more importance on predicting the `Yes`.\n\nAccording to Jason Brownlee on [Machine Learning Mastery](https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/):\n> Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.\n\nIn this notebook, the approach used to address class imbalance is to resample the training dataset. [Random oversampling](https:\/\/machinelearningmastery.com\/random-oversampling-and-undersampling-for-imbalanced-classification\/) method is chosen, in which examples in the minority class are randomly duplicated.","885b96a1":"- There is an almost equal proportion of female (49.5%) and male (50.5%) customers. Both genders have similar churn rate at 26%, signifying that gender has little to no effect on leaving the telco service.\n- A large portion of customers (80%) are represented by those under the age of 30. These young customers have churn rate that is 6% lower than customers over 30.\n- Senior citizens represent only 16% of total customers but have a relatively high churn rate at 41%, which is almost twice as high as non-senior citizens.","3a667087":"- Although showing no improvement after tuning, the logistic regression model is still the most well-performing in terms of overall accuracy and recall for predicting class 1 (Churn = Yes).\n- While tuning improves recall score for decision tree, it results in poorer precision. This is reflected in the confusion matrices below: false negative (FN) cases decrease, but false positives (FP) increase. The area under ROC curve is also consequently affected: it yields bigger AUC score.\n  - In the case of K-nearest neighbors model, the reverse is true: better precision (less FP), lower recall (more FN) and smaller AUC.","773d70d2":"Earlier, the models are trained with default hyperparameters. Better model performances may be achieved by [optimizing](https:\/\/machinelearningmastery.com\/hyperparameter-optimization-with-random-search-and-grid-search\/) these hyperparameters. One approach that can be applied is using the [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html). Given a set of values for hyperparameters, GridSearchCV fits a model using every single combination of these hyperparameters and evaluates it using cross-validation (hence the 'CV'). The set of hyperparameters that resulted in the best score can be accessed from the search result.","74b7d605":"The models have performed fairly well in accurately predicting customer churn. However, it is equally important that one also understands which features are most relevant in that prediction. [Feature importance](https:\/\/machinelearningmastery.com\/calculate-feature-importance-with-python\/) is a technique in which input features are assigned a score based on how useful they are at predicting a target variable. It helps us better understand the dataset and can be used to improve the predictive model.\n\nFor logistic regression model, feature importance scores can be retrieved from the *coef_* attribute. Positive scores indicate a feature that predicts class 1 (Churn = Yes), whereas the negative scores indicate a feature that predicts class 0 (Churn = No).","7c9cef64":"# Modeling","fe16a1b6":"- Customers with low CLTV (customer lifetime value) are more likely to churn than ones with high CLTV.\n- Looking at the distribution of data, long distance charges have negligible effect on churn.\n- Customers who have churned have a slightly higher average of monthly download and are paying higher monthly charges.","0db7fcfb":"- Thirty-nine percent of customers have streaming services, either TV or Movies. They have a 5-6% higher churn rate when compared to customers who do not subscribe to any streaming services.\n- Thirty-one percent of customers who have unlimited data included in their subscription plan have churned, twice the number of churn in customers with limited data plan.","7110ed07":"# Preprocessing","65445af1":"A number of variables have driven customers to be more likely to churn in the past quarter: \n- expressing low satisfaction (esp. score 1 and 2),\n- opting for monthly contract,\n- paying higher monthly charge,\n- purchasing offer A and E,\n- being a senior citizen,\n- not subscribing to online security service,\n- not having dependents, and\n- having recently joined (low tenure length).","37b702d6":"After the tables are loaded, they are merged into a single data frame. Each table contains one identical column, `Customer ID`, on which the merging is performed. A total of 48 columns are output, but some are dropped because they contain redundant information, are already directly related to churn, or are highly related with other columns (which can result in multicollinearity).","ebe8e7bc":"- Customers with either online backup or device protection subscription have 6-7% lower churn rate than customers with none of these services.\n- About 31% of customers who do not subscribe to premium tech support have churned, the rate is twice as high as customers with support.","03e46c27":"Predicting whether or not a customer will churn is a binary classification problem, since there are only two possible outcomes: `Yes (1)` or `No (0)`. Three classification algorithms are implemented in the prediction, namely [Logistic Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html), [Decision Tree](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html), and [K-Nearest Neighbors](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html). The models are fit on the train set and subsequently used to make prediction on unseen data. Model performances are evaluated with [metrics](https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9) such as accuracy, precision, recall, F1-score, as well as using confusion matrix and [ROC curve](https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5).","cc810f03":"A newer version of telco customer churn data is used in this notebook, obtained from a data module in [IBM Accelerator Catalog](https:\/\/community.ibm.com\/accelerators\/?context=analytics&type=Data&industry=Telecommunications). The original module contains five data tables, but only three will be considered for analysis: `Demographics`, `Services`, and `Status`. A detailed description of columns in each table is available on [IBM Business Analytics Community](https:\/\/community.ibm.com\/community\/user\/businessanalytics\/blogs\/steven-macko\/2019\/07\/11\/telco-customer-churn-1113).","89909ac7":"# Feature importance","75963703":"The chart above shows churn distribution by customer's satisfaction, from a score of 1 (Very Unsatisfied) to 5 (Very Satisfied). We see that customers who give a score of 1 and 2 have all churned. *Unhappy customers stop having business with you*.","38f152b3":"There are 7043 observations in the data frame, each representing a unique customer. A majority of the columns contain categorical data of the nominal type, e.g., `Yes` or `No`. Six columns are of the numerical type, namely: `Satisfaction Score`, `CLTV`, `Tenure in Months`, `Avg Monthly Long Distance Charges`, `Avg Monthly GB Download`, and `Monthly Charge`. The column `Count` is only kept for the purpose of aggregating data and creating visualizations; it will be removed later. Upon checking, the data frame contains neither missing values nor duplicated rows.","68f4c105":"# Background","18d19c0d":"# Data Exploration","d5be9321":"# Hyperparameter tuning","71a92eae":"A majority of columns in the data frame contains categorical data. However, many machine learning models require numeric inputs. [Encoding and One-Hot Encoding](https:\/\/machinelearningmastery.com\/one-hot-encoding-for-categorical-data\/) are implemented to convert these categorical data into numerical form (0 or 1).\n\nThe churn data contains numeric features with a mixture of scales (e.g., dollars for `Monthly Charge`, Gigabytes for `Avg Monthly GB Download`), so another important step in the preprocessing stage is rescaling. The normalization method is applied to rescale numeric features into the range of 0 and 1.\n\nA train\/test split is defined where 20% of the data will be reserved for testing. Lastly, oversampling is performed on the training data to address class imbalance.","de1c451f":"Customer churn is defined as the number of customers who have stopped doing business with a company during a given time period. Churn poses a problem for a business as it lowers revenues and profits. Moreover, attracting new customers [costs 5 to 25 times more expensive](https:\/\/hbr.org\/2014\/10\/the-value-of-keeping-the-right-customers) than retaining existing ones. [According to Bain & Co.](https:\/\/media.bain.com\/Images\/BB_Prescription_cutting_costs.pdf), increasing customer retention by 5% will increase profits to more than 25%.\n\nAccurately predicting churn and identifying the relevant factors can help a company develop effective customer retention strategies which, in turn, reduce churn.","81e619f5":"# Model evaluation","69fc4cc6":"- Customers with multiple phone lines have a slightly higher churn rate than customers without ones; about 3% difference.\n- Fiber optic is the most preferred of the three internet service types available, 43% customers have it installed. However, the highest churn (40% rate) is also observed on customers having fiber optic.\n- Of customers who subscribe to the online security service (29% of all customers), 14% have churned. Their churn rate is twice as low as customers who do not subscribe.","2cb3232c":"# Evaluating tuned models","136924f4":"- Half of the telco customers (51%) opt for the Month-to-Month subscription plan and they are the most likely to churn when compared to others who opt for a One-Year or Two-Year plan. Customers with longer subscription plans have more than 30% lower churn rate.\n- The paperless billing option, preferred by 60% of customers, relates to twice as high churn rate as the other billing option.\n- Customers who pay their services with mailed checks, which is the least popular option (only 5% of all customers), have higher churn rate than customers who chose other methods. The most preferred payment method is bank withdrawal (55% customers)."}}