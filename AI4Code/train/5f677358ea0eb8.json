{"cell_type":{"b40b71b3":"code","9c165814":"code","3ffda53f":"code","b8c9541a":"code","25201065":"code","30bd6ed2":"code","c11d21f1":"code","523e819b":"code","80b3f848":"code","7aa3e571":"code","a5ce44d4":"code","21203532":"code","e2dfb915":"code","4590825f":"code","29c30632":"code","8141e5dd":"code","f37389fb":"code","2fcdcb81":"code","c084aef5":"code","1a2bc193":"code","0e5446c6":"code","d818459c":"markdown","c706eeb7":"markdown","68b21c8c":"markdown","0a411780":"markdown","d78f1106":"markdown","bb7766e7":"markdown","8e3f2fae":"markdown","12a67513":"markdown","1c975431":"markdown","3e5e5c6b":"markdown","9c5e1dd9":"markdown","b7c9b2bf":"markdown","3dcb01a2":"markdown","90014bc4":"markdown","7217babd":"markdown","bf82f6ee":"markdown","8b5f5cf6":"markdown","9aea5442":"markdown","347d198a":"markdown","f7b11db0":"markdown","dd463d12":"markdown"},"source":{"b40b71b3":"#import libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.pipeline import Pipeline\n\n#import visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#import preprocessing libraries\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n\n#import dimensionality reduction libraries\nfrom sklearn.decomposition import PCA\n\n#import algorithm libraries\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#import error metrics\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve, accuracy_score\n","9c165814":"#get the path of the dataset\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3ffda53f":"data = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nprint(data.shape)\ndata.head()","b8c9541a":"data.describe()","25201065":"data.info()","30bd6ed2":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(data, title='Pandas Profiling Report')","c11d21f1":"profile.to_widgets()","523e819b":"# Create a function to detect outliers\ndef detect_outliers(df,n,features):\n    # create a array to store the values of outliers\n    outliers_index = []\n    \n    # calculate the Q1 and Q3. Then we can calculate IQR\n    for col in features:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        \n        #outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine the indices of outliers in a particular column\n        outlier_list_col = df[(df[col] < (Q1 - outlier_step))|(df[col] > (Q3 + outlier_step))].index\n        \n        # append the found outlier indices for col to the list of outlier indices\n        outliers_index.extend(outlier_list_col)\n        \n        # select observations containing more than 2 outliers\n        outliers_index = Counter(outliers_index)\n        multiple_outliers = list( k for k, v in outliers_index.items() if v > n )\n        \n        return multiple_outliers","80b3f848":"detect_outliers(data,2,['DailyRate', 'DistanceFromHome','YearsAtCompany','YearsWithCurrManager',\n                        'EmployeeNumber','JobInvolvement','PercentSalaryHike'])","7aa3e571":"# since our output is categorical, we will try with chi-squared (chi\u00b2) statistical test first\n\nfrom sklearn.feature_selection import SelectKBest, chi2,f_classif\n\n# Get list of categorical variables\ns = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\n# apply label encoder to all the categorical columns\ndata_copy = data.copy()\nle = LabelEncoder()\nfor col in object_cols:\n    data_copy[col] = le.fit_transform(data_copy[col].astype(str))\n   \n # get the x and y value\nxl,yl = data_copy.drop(columns = 'Attrition'), data_copy['Attrition']\n    \n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=10)\ndfit = bestfeatures.fit(xl,yl)\ndf_scores = pd.DataFrame(dfit.scores_)\ndf_cols = pd.DataFrame(xl.columns)\n# now we will concatenate the 2 dataframes\nfeature_importance = pd.concat([df_cols, df_scores],axis=1)\nfeature_importance.columns = ['Feature','Score']\nprint(feature_importance.nlargest(20,'Score'))","a5ce44d4":"# now we will check with ANOVA test. If the features are quantitative, compute the ANOVA F-value \n# between each feature and the target vector.\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=f_classif, k=10)\ndfit = bestfeatures.fit(xl,yl)\ndf_scores = pd.DataFrame(dfit.scores_)\ndf_cols = pd.DataFrame(xl.columns)\n# now we will concatenate the 2 dataframes\nfeature_importance = pd.concat([df_cols, df_scores],axis=1)\nfeature_importance.columns = ['Feature','Score']\nprint(feature_importance.nlargest(10,'Score'))","21203532":"# Get list of categorical columns\ns = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\n# apply label encoder to all the categorical columns\ndata_copy = data.copy()\nle = LabelEncoder()\nfor col in object_cols:\n    data_copy[col] = le.fit_transform(data_copy[col].astype(str))\n    \n # get the x and y value\nx,y = data_copy.drop(columns = 'Attrition'), data_copy['Attrition']    ","e2dfb915":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state= 142)\nfor train_index, val_index in skf.split(x,y):\n    train_x, val_x = x.iloc[train_index], x.iloc[val_index]\n    train_y, val_y = y.iloc[train_index], y.iloc[val_index]\n    \ntrain_x.shape, val_x.shape    ","4590825f":"dtc_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=67)\n\n# Train Decision Tree Classifer\ndtc_model.fit(train_x,train_y)\n\n#Predict the response for validation dataset\ny_pred = dtc_model.predict(val_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(val_y, y_pred))","29c30632":"#create a function to get the importance of features\n\ndef feat_imp(model, data, text):\n    importances = model.feature_importances_\n    \n    #sort features in decreasing order of their importance\n    indices = np.argsort(importances)[::-1]\n    \n    # Rearrange feature names so they match the sorted feature importances\n    names = [data.columns[i] for i in indices]\n    \n    # Create plot\n    plt.figure(figsize=(14,6))\n\n    # Create plot title\n    plt.title(text)\n\n    # Add bars\n    plt.bar(range(train_x.shape[1]), importances[indices])\n\n    # Add feature names as x-axis labels\n    plt.xticks(range(train_x.shape[1]), names, rotation=90)\n\n    # Show plot\n    plt.show()","8141e5dd":"feat_imp(dtc_model,data, \"Feature Importance for Decision Tree\")","f37389fb":"rfc_model = RandomForestClassifier()\n\nrfc_model.fit(train_x,train_y)\n\n#Predict the response for validation dataset\ny_pred = rfc_model.predict(val_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(val_y, y_pred))","2fcdcb81":"feat_imp(rfc_model,data, \"Feature Importance for Random Forest Classifier\")","c084aef5":"svc_model = SVC(kernel='linear')\n\nsvc_model.fit(train_x,train_y)\n\n#Predict the response for validation dataset\ny_pred = svc_model.predict(val_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(val_y, y_pred))","1a2bc193":"knn_model = KNeighborsClassifier()\n\nknn_model.fit(train_x,train_y)\n\n#Predict the response for validation dataset\ny_pred = knn_model.predict(val_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(val_y, y_pred))","0e5446c6":"nb_model = GaussianNB()\n\nnb_model.fit(train_x,train_y)\n\n#Predict the response for validation dataset\ny_pred = nb_model.predict(val_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(val_y, y_pred))","d818459c":"Similar to SVM, feature importance is not defined for the KNN Classification algorithm.","c706eeb7":"# Support Vector Machines Classifier: \n\nThe core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes. \n\n![image.png](attachment:image.png)\n\n**Hyperparameters: **\n\n* Kernel: The main function of the kernel is to transform the given dataset input data into the required form.\n* Regularization: Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term.\n* Gamma: A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting.","68b21c8c":"# From Pandas Profiler, we got few interesting points:\n\n1. MonthlyIncome is highly correlated with JobLevel.\n2. JobRole is highly correlated with Department\n3. Attrition property has 1233 as No value and 237 as Yes. Which means that 237 people left the company. ","0a411780":"# Lets discuss Random Forest:\n\nThe random forest approach is a bagging method where deep trees, fitted on bootstrap samples, are combined to produce an output with lower variance.\n\n* n_estimators = number of trees in the foreset\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min number of data points placed in a node before the node is split\n* min_samples_leaf = min number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)","d78f1106":"**So till now we have checked that there is no null value or no outliers.**","bb7766e7":"# Lets do some EDA","8e3f2fae":"# Now we will check the feature selection\n\n![Image Source: machinelearningmastery](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/11\/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)","12a67513":"Since Naive Bayes assumes independence and outputs class probabilities most feature importance criteria are not a direct fit. ","1c975431":"**Lets split the data using StratifiedKFold**","3e5e5c6b":"Since we know that the Chi2 test is for finding correlation between 2 categorical values. So we will check only for categorical features. ","9c5e1dd9":"**Attrition refers to the number or percentage of employees who are leaving a company to work for other companies or who have decided to pursue other opportunities. The attrition rate is the measurement you use to determine the percentage of employees who have left a company in a given period.**","b7c9b2bf":"**We will now check the outliers.** ","3dcb01a2":"# Let's discuss KNN\n\nKNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).","90014bc4":"# Here we will try to apply various machine learning algorithms and getting the important features which are impacting attrition the most. ","7217babd":"# Now we will check with Naive Bayes\n\n![image.png](attachment:image.png)","bf82f6ee":"![](https:\/\/i2.wp.com\/thecontextofthings.com\/wp-content\/uploads\/2017\/01\/employee-attrition.jpg?resize=300%2C300)","8b5f5cf6":"# Conclusion: \n\nWe can see that the Random Forest is performing better than other classification algorithms. \n\n> So we will check the top 5 features impacting the Attrition Rate:\n\n1. MaritalStatus\n2. Age\n3. BusinessTravel\n4. Over18\n5. EmployeeCount","9aea5442":"# Now we will Encode our object columns and then will apply different models. ","347d198a":"The feature importance of linear SVMs could be found out but not for a nonlinear SVMs, the reason being that, when the SVM is non-linear the dataset is mapped into a space of higher dimension, which is quite different from the parent dataset and the hyperplane is obtained and this high dimensional data and hence the property is changed from that of the parent dataset and hence it is not possible to find the feature importance of this SVM in relation to the parent dataset features.\n\nReference: https:\/\/stackoverflow.com\/questions\/41628264\/how-to-determine-feature-importance-of-non-linear-kernals-in-svm\n\nAlso we don't have direct method like feature_importances_ just like other Tree Algorithms. ","f7b11db0":"For this purpose, I will be using 5 algorithms and then will check the accuracy score and most important features. \n\n\n| Algorithm Name |     Handles missing data ? |    Feature Scaling Required?     | Outliers Sensitive ? | \n| --- | --- | --- | --- |\n| Decision Tree | Yes | No | No | \n| Random Forest | Yes | No | No |\n| SVM    | No | Yes | Yes | \n| KNN |  No | Yes | Yes | \n| Naive Bayes | Yes(by ignoring them) | No | Yes | \n\n\n\n> I tried to consolidate some facts about these algorithms. Feel free to provide any suggestion regarding the above points. ","dd463d12":"# Lets start with DesisionTreeClassifier\n\nImportant Terminologies related to Decision Tree:\n\n1. criterion: optional (default=\u201dgini\u201d) or Choose attribute selection measure: This parameter allows us to use the different-different attribute selection measure. Supported criteria are \u201cgini\u201d for the Gini index and \u201centropy\u201d for the information gain.\n\n2. splitter: string, optional (default=\u201dbest\u201d) or Split Strategy: This parameter allows us to choose the split strategy. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.\n\n3. max_depth: int or None, optional (default=None) or Maximum Depth of a Tree: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting. \n\nIn Scikit-learn, optimization of decision tree classifier performed by only pre-pruning. The maximum depth of the tree can be used as a control variable for pre-pruning."}}