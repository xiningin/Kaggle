{"cell_type":{"eda2223d":"code","5075b8d3":"code","54a4cc3a":"code","1e2d0a09":"code","484736df":"code","f2667569":"code","ae0c427f":"code","83688a61":"code","38db70e5":"code","d5843c3e":"code","543c6c3d":"code","7ff22110":"markdown","13da876e":"markdown","091f3cfb":"markdown","08883aef":"markdown","4fa07804":"markdown","e684c9cc":"markdown"},"source":{"eda2223d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","5075b8d3":"df = pd.read_csv(\"..\/input\/IRIS.csv\")\ndf.shape\n","54a4cc3a":"df.head()","1e2d0a09":"df.info()","484736df":"label_encoder = LabelEncoder()\ndf['species'] = label_encoder.fit_transform(df['species'])\ndf.tail()","f2667569":"df.corr()","ae0c427f":"y = df.species\nfeatures = ['petal_length','petal_width','sepal_length','sepal_width']\nX = df[features].copy()\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size=0.8, test_size=0.2, random_state=0)","83688a61":"model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmodel.fit(X_train,y_train)\npreds = model.predict(X_valid)\nprint(\"MAE: \" + str(mean_absolute_error(preds, y_valid)))","38db70e5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\n\nX_train, X_valid, y_train, y_valid = train_test_split(rescaledX,y, train_size=0.8, test_size=0.2, random_state=0)\n\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmodel.fit(X_train,y_train)\npreds = model.predict(X_valid)\nprint(\"MAE: \" + str(mean_absolute_error(preds, y_valid)))","d5843c3e":"model = XGBClassifier(n_estimators=1000, learning_rate=0.05)\nmodel.fit(X_train,y_train)\npreds = model.predict(X_valid)\nprint(\"Accuracy Score: \" + str(accuracy_score(preds, y_valid)))","543c6c3d":"print(preds)","7ff22110":"Now let's check the MAE after scaling.","13da876e":"Using XGBRegressor to check the MAE for different features combinations.","091f3cfb":"Cool. :)","08883aef":"MAE decreased by 0.002. Meh. Maybe the difference would be bigger on a bigger dataset.","4fa07804":"Removing sepal_width (least corr() value) - MAE increased by 0.01\n\nRemoving petal_width (highest corr() value) - MAE increased by 0.07!","e684c9cc":"No missing values. One categorical variable."}}