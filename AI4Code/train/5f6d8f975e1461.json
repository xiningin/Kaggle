{"cell_type":{"3b4a4be9":"code","b323d3a9":"code","62772c73":"code","74885645":"code","32e2dd9a":"code","19209e5a":"code","04e4c9e5":"code","9b10ddc0":"code","fb0c9b70":"code","aba43cdd":"code","0f71a3a1":"code","9000a6c3":"code","f5e4c897":"code","30fa891c":"code","bba9fd0c":"code","afe1ff34":"code","a9794301":"code","8d981df9":"code","0df1288c":"code","0b5f3928":"code","fac1db96":"code","fc960923":"code","4375448f":"markdown","e52ea269":"markdown","e3671794":"markdown","4001fc28":"markdown","07bb6a5a":"markdown","c6bdf404":"markdown","252e229f":"markdown","c2ab3aaf":"markdown","8c74a8e9":"markdown","303d96da":"markdown","6439da83":"markdown","3b696d90":"markdown","e6700d36":"markdown","d1eddba0":"markdown","1b2f0266":"markdown","bee86eea":"markdown","e5f2e682":"markdown","170a7301":"markdown"},"source":{"3b4a4be9":"!pip install kaggle-environments --upgrade","b323d3a9":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport PIL.Image\n\nimport tensorflow as tf\nimport logging\n\nfrom sklearn import preprocessing\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import evaluate, make\nfrom kaggle_environments.envs.halite.helpers import *\n","62772c73":"seed=123\ntf.compat.v1.set_random_seed(seed)\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)\nlogging.disable(sys.maxsize)\nglobal ship_","74885645":"env = make(\"halite\", debug=True)\nenv.run([\"random\"])\nenv.render(mode=\"ipython\",width=800, height=600)","32e2dd9a":"env.configuration","19209e5a":"env.specification","04e4c9e5":"env.specification.reward","9b10ddc0":"env.specification.action","fb0c9b70":"env.specification.observation","aba43cdd":"def getDirTo(fromPos, toPos, size):\n    fromX, fromY = divmod(fromPos[0],size), divmod(fromPos[1],size)\n    toX, toY = divmod(toPos[0],size), divmod(toPos[1],size)\n    if fromY < toY: return ShipAction.NORTH\n    if fromY > toY: return ShipAction.SOUTH\n    if fromX < toX: return ShipAction.EAST\n    if fromX > toX: return ShipAction.WEST\n\n# Directions a ship can move\ndirections = [ShipAction.NORTH, ShipAction.EAST, ShipAction.SOUTH, ShipAction.WEST]\n\n# Will keep track of whether a ship is collecting halite or carrying cargo to a shipyard\nship_states = {}\n\n# Returns the commands we send to our ships and shipyards\ndef simple_agent(obs, config):\n    size = config.size\n    board = Board(obs, config)\n    me = board.current_player\n    # If there are no ships, use first shipyard to spawn a ship.\n    if len(me.ships) == 0 and len(me.shipyards) > 0:\n        me.shipyards[0].next_action = ShipyardAction.SPAWN\n\n    # If there are no shipyards, convert first ship into shipyard.\n    if len(me.shipyards) == 0 and len(me.ships) > 0:\n        me.ships[0].next_action = ShipAction.CONVERT\n    \n    for ship in me.ships:\n        if ship.next_action == None:\n            \n            ### Part 1: Set the ship's state \n            if ship.halite < 200: # If cargo is too low, collect halite\n                ship_states[ship.id] = \"COLLECT\"\n            if ship.halite > 500: # If cargo gets very big, deposit halite\n                ship_states[ship.id] = \"DEPOSIT\"\n                \n            ### Part 2: Use the ship's state to select an action\n            if ship_states[ship.id] == \"COLLECT\":\n                # If halite at current location running low, \n                # move to the adjacent square containing the most halite\n                if ship.cell.halite < 100:\n                    neighbors = [ship.cell.north.halite, ship.cell.east.halite, \n                                 ship.cell.south.halite, ship.cell.west.halite]\n                    best = max(range(len(neighbors)), key=neighbors.__getitem__)\n                    ship.next_action = directions[best]\n            if ship_states[ship.id] == \"DEPOSIT\":\n                # Move towards shipyard to deposit cargo\n                direction = getDirTo(ship.position, me.shipyards[0].position, size)\n                if direction: ship.next_action = direction\n                \n    return me.next_actions","0f71a3a1":"trainer = env.train([None, \"random\"])\nobservation = trainer.reset()\nwhile not env.done:\n    my_action = simple_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation = trainer.step(my_action)[0]\n    print(\"Reward gained\",observation.players[0][0])","9000a6c3":"env.render(mode=\"ipython\",width=800, height=600)","f5e4c897":"def ActorModel(num_actions,in_):\n    common = tf.keras.layers.Dense(128, activation='tanh')(in_)\n    common = tf.keras.layers.Dense(32, activation='tanh')(common)\n    common = tf.keras.layers.Dense(num_actions, activation='softmax')(common)\n    \n    return common","30fa891c":"def CriticModel(in_):\n    common = tf.keras.layers.Dense(128)(in_)\n    common = tf.keras.layers.ReLU()(common)\n    common = tf.keras.layers.Dense(32)(common)\n    common = tf.keras.layers.ReLU()(common)\n    common = tf.keras.layers.Dense(1)(common)\n    \n    return common","bba9fd0c":"input_ = tf.keras.layers.Input(shape=[441,])\nmodel = tf.keras.Model(inputs=input_, outputs=[ActorModel(5,input_),CriticModel(input_)])\nmodel.summary()","afe1ff34":"optimizer = tf.keras.optimizers.Adam(lr=7e-4)","a9794301":"huber_loss = tf.keras.losses.Huber()\naction_probs_history = []\ncritic_value_history = []\nrewards_history = []\nrunning_reward = 0\nepisode_count = 0\nnum_actions = 5\neps = np.finfo(np.float32).eps.item()\ngamma = 0.99  # Discount factor for past rewards\nenv = make(\"halite\", debug=True)\ntrainer = env.train([None,\"random\"])","8d981df9":"le = preprocessing.LabelEncoder()\nlabel_encoded = le.fit_transform(['NORTH', 'SOUTH', 'EAST', 'WEST', 'CONVERT'])\nlabel_encoded","0df1288c":"def getDirTo(fromPos, toPos, size):\n    fromX, fromY = divmod(fromPos[0],size), divmod(fromPos[1],size)\n    toX, toY = divmod(toPos[0],size), divmod(toPos[1],size)\n    if fromY < toY: return ShipAction.NORTH\n    if fromY > toY: return ShipAction.SOUTH\n    if fromX < toX: return ShipAction.EAST\n    if fromX > toX: return ShipAction.WEST\n\n# Directions a ship can move\ndirections = [ShipAction.NORTH, ShipAction.EAST, ShipAction.SOUTH, ShipAction.WEST]\n   \ndef decodeDir(act_):\n    if act_ == 'NORTH':return directions[0]\n    if act_ == 'EAST':return directions[1]\n    if act_ == 'SOUTH':return directions[2]\n    if act_ == 'WEST':return directions[3]\n    \n# Will keep track of whether a ship is collecting halite or carrying cargo to a shipyard\nship_states = {}\nship_ = 0\ndef update_L1():\n    ship_+=1\n# Returns the commands we send to our ships and shipyards\ndef advanced_agent(obs, config, action):\n    size = config.size\n    board = Board(obs, config)\n    me = board.current_player \n    act = le.inverse_transform([action])[0]\n    global ship_\n    \n   # If there are no ships, use first shipyard to spawn a ship.\n    if len(me.ships) == 0 and len(me.shipyards) > 0:\n        me.shipyards[ship_-1].next_action = ShipyardAction.SPAWN\n\n    # If there are no shipyards, convert first ship into shipyard.\n    if len(me.shipyards) == 0 and len(me.ships) > 0 and ship_==0:\n        me.ships[0].next_action = ShipAction.CONVERT   \n    try: \n        if act=='CONVERT':\n            me.ships[0].next_action = ShipAction.CONVERT\n            update_L1()\n            if len(me.ships)==0 and len(me.shipyards) > 0:\n                me.shipyards[ship_-1].next_action = ShipyardAction.SPAWN\n        if me.ships[0].halite < 200:\n            ship_states[me.ships[0].id] = 'COLLECT'\n        if me.ships[0].halite > 800:\n            ship_states[me.ships[0].id] = 'DEPOSIT' \n\n        if ship_states[me.ships[0].id] == 'COLLECT': \n            if me.ships[0].cell.halite < 100:\n                me.ships[0].next_action = decodeDir(act)\n        if ship_states[me.ships[0].id] == 'DEPOSIT':\n            # Move towards shipyard to deposit cargo\n            direction = getDirTo(me.ships[0].position, me.shipyards[ship_-1].position, size)\n            if direction: me.ships[0].next_action = direction\n    except:\n        pass\n                \n    return me.next_actions","0b5f3928":"while not env.done:    \n    state = trainer.reset()\n    episode_reward = 0\n    with tf.GradientTape() as tape:\n        for timestep in range(1,env.configuration.episodeSteps+200):\n            # of the agent in a pop up window.\n            state_ = tf.convert_to_tensor(state.halite)\n            state_ = tf.expand_dims(state_, 0)\n            # Predict action probabilities and estimated future rewards\n            # from environment state\n            action_probs, critic_value = model(state_)\n            critic_value_history.append(critic_value[0, 0])\n            \n            # Sample action from action probability distribution\n            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n            action_probs_history.append(tf.math.log(action_probs[0, action]))\n            \n            # Apply the sampled action in our environment\n            action = advanced_agent(state, env.configuration, action)\n            state = trainer.step(action)[0]\n            gain=state.players[0][0]\/5000\n            rewards_history.append(gain)\n            episode_reward += gain\n            \n            if env.done:\n                state = trainer.reset() \n        # Update running reward to check condition for solving\n        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n\n        # Calculate expected value from rewards\n        # - At each timestep what was the total reward received after that timestep\n        # - Rewards in the past are discounted by multiplying them with gamma\n        # - These are the labels for our critic\n        returns = []\n        discounted_sum = 0\n        for r in rewards_history[::-1]:\n            discounted_sum = r + gamma * discounted_sum\n            returns.insert(0, discounted_sum)\n        # Normalize\n        returns = np.array(returns)\n        returns = (returns - np.mean(returns)) \/ (np.std(returns) + eps)\n        returns = returns.tolist()\n        # Calculating loss values to update our network\n        history = zip(action_probs_history, critic_value_history, returns)\n        actor_losses = []\n        critic_losses = []\n        for log_prob, value, ret in history:\n            # At this point in history, the critic estimated that we would get a\n            # total reward = `value` in the future. We took an action with log probability\n            # of `log_prob` and ended up recieving a total reward = `ret`.\n            # The actor must be updated so that it predicts an action that leads to\n            # high rewards (compared to critic's estimate) with high probability.\n            diff = ret - value\n            actor_losses.append(-log_prob * diff)  # actor loss\n\n            # The critic must be updated so that it predicts a better estimate of\n            # the future rewards.\n            critic_losses.append(\n                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n            )\n        # Backpropagation\n        loss_value = sum(actor_losses) + sum(critic_losses)\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        \n        # Clear the loss and reward history\n        action_probs_history.clear()\n        critic_value_history.clear()\n        rewards_history.clear()\n        \n    # Log details\n    episode_count += 1\n    if episode_count % 10 == 0:\n        template = \"running reward: {:.2f} at episode {}\"\n        print(template.format(running_reward, episode_count))\n\n    if running_reward > 550:  # Condition to consider the task solved\n        print(\"Solved at episode {}!\".format(episode_count))\n        break","fac1db96":"while not env.done:\n    state_ = tf.convert_to_tensor(state.halite)\n    state_ = tf.expand_dims(state_, 0)\n    action_probs, critic_value = model(state_)\n    critic_value_history.append(critic_value[0, 0])\n    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n    action_probs_history.append(tf.math.log(action_probs[0, action]))\n    action = advanced_agent(state, env.configuration, action)\n    state = trainer.step(action)[0]","fc960923":"env.render(mode=\"ipython\",width=800, height=600)","4375448f":"## Analyzing the environment\nLets take a tour of our environment and its settings first.","e52ea269":"## What is Actor-Critic agent?\nNow, before jumping into the concept of Actor-Critic agent, I would recommend you to have some basic knowledge about Q-Learning, followed by deep Q-Learning because without these two you won't understand the significance and necessity Actor-Critic agent.\n<br>\nIn sort,<br>\nAs an agent takes actions and moves through an environment, it learns to map the observed state of the environment to two possible outputs:\n* Recommended action: A probabiltiy value for each action in the action space. The part of the agent responsible for this output is called the **actor**.\n* Estimated rewards in the future: Sum of all rewards it expects to receive in the future. The part of the agent responsible for this output is the **critic**.\n\nAgent and Critic learn to perform their tasks, such that the recommended actions from the actor maximize the rewards.<br>\nSource - [Keras.io](https:\/\/keras.io\/examples\/rl\/actor_critic_cartpole\/)","e3671794":"## Our objective\nAs you could see from the results that the yellow ships on the left-hand side show almost no movement whereas the red ships on the right-hand side show some smart movements to collect halite, deposit them in the shipyard and spawn accordingly. Our objective would be to train the yellow ships through reinforcement learning and program an AI model which could perform the given task in the most efficient path possible.","4001fc28":"## Conclusion\nHey!! our ship is pretty smartly performing you see....\n<br><br>\nour ship is collecting halites, transforming into shipyards and also spawning if now ship is available in the most efficient way possible. In simple words, we have successfully trained our agent to direct the ship to collect halites in the most efficient way possible. I wish if I could have found a way to track the nearest shipyard and deposits collected halites or find a way to control multiple agents through reinforcement learning. Although there are research papers which I found explaining Multi-Reinforcement learning and Multi-goal Reinforcement learning, which I think could be more useful to solve this problem. Anyways my knowledge is currently limited to Actor-Critic agent, and I would study more to find a better solution than this.","07bb6a5a":"![game](http:\/\/www.andreykurenkov.com\/writing\/images\/2016-4-15-a-brief-history-of-game-ai\/5-samuel.jpg)\n> On February 24, 1956, Arthur Samuel\u2019s Checkers program, which was developed for play on the IBM 701, was demonstrated to the public on television","c6bdf404":"## Introduction\nIn this notebook, we are going to design a neural network to simulate a game through A.I. The game is **Halite** by **Two Sigma**. It is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.<br>\nWe would be using Actor-Critic agent, as our base reinforcement learning model. The purpose of the agent would be to predict moves to control the direction of the ship to collect halite and deposit them in the shipyard.","252e229f":"## Thank you.\n![quote](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fquotefancy.com%2Fmedia%2Fwallpaper%2F3840x2160%2F1741586-Magnus-Carlsen-Quote-Some-people-think-that-if-their-opponent.jpg&f=1&nofb=1)\n<br><br>\n### Please <span style=\"color:red\">Up-Vote<\/span> and <span style=\"color:red\">Share<\/span> this notebook if you like it or find the content informative. Also, let me know your opinions and suggestions in the comment section below.","c2ab3aaf":"## Rules of the Game\nTo go through details about the rules of the game I would recommend the notebook [Getting started with Halite](https:\/\/www.kaggle.com\/alexisbcook\/getting-started-with-halite) by [Alexis Cook](https:\/\/www.kaggle.com\/alexisbcook). She has elabored and explained the rules of the game very well in her notebook.","8c74a8e9":"Now, lets talk about some of the most important terms like agent, policy, states.\n<br>\nIn reinforcement learning an **Agent** is a self-learning model that learns some type of interaction between it and the environment. The agent wants to achieve some kind of **goal** within mentioned environment while it interacts with it. This interaction is divided into time steps. In each time step, **action** is performed by agent. This action changes the **state** of the environment and based on the success of it agent gets a certain **reward**. This way the agent learns what actions should be performed an which shouldn\u2019t in a defined environment state. ","303d96da":"## Encoding our moves","6439da83":"## Implementation","3b696d90":"## The Obstacles\nI am not a mastermind with reinforcement learning. So, I faced some problems which I would discuss now and also how I tried to solve some of them.\n* Controlling only one ship - I have made this program in a way that the agent could only control one ship at a time. Which indeed means I have disabled the respawning of multiple ships.\n* 5 Moves Game - Due to the first problem I have made this game limited up to the prediction of 4 direction(East, West, North, South) and predicting when to transform from ship to ship-yard. I have removed the SPAWN feature from prediction because I still had not discovered any way to control multiple ships through Actor-Critic agent.\n* Deposit to the last shipyard - If there is no ship, the ship would spawn from the most recent shipyard developed and would deposit halites to the most recent shipyard developed. It would have been better if I would have discovered a way to calculate the nearest shipyard for deposition of collected halites. \n![chess](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.GTWpPAXsc0-kjWXyEqpGywHaEt%26pid%3DApi&f=1)","e6700d36":"## Results\nThe Yellow ships and shipyards are controlled by our trained actor-critic model and the red ship and shipyards are trained against the random predicting agent.","d1eddba0":"## Designing game AI with Reinforcement learning","1b2f0266":"## What is Reinforcement Learning?\nReinforcement Learning is the science of making optimal decisions using experience. It is categorized with supervised learning and unsupervised learning, rather than categorizing it with Machine Learning and Deep Learning. Reinforcement learning is the methodology that deals with the interaction between agent and environment  through actions and has got nothing to do with labeled and unlabeled data,  although there is another category called **Semi-supervised learning**, which is indeed a hybrid of supervised and unsupervised learning.<br><br>\nThe word \"reinforce\" means strengthen or support (an object or substance), especially with additional material.\n<br>\nBut what we are strengthening here, and what is our support which strengthen?<br>\nWe are trying to strengthen the learning ability of an **agent** to understand the environment. But that also happens in machine learning and deep learning, where the model is trained and the model learns a pattern from the trained data while minimizing the loss and improving the accuracy. The factor that strengthens the learning ability in reinforcement learning is **Reward**. A high positive reward is awarded to the agent for making a correct decision, and the agent should be penalized for making a wrong decision. The agent should get a slight negative reward for not making a correct decision after every time-step. \"Slight\" negative because we would prefer our agent to take more time in taking a decision rather than making the wrong decision.","bee86eea":"At each time step, the agent takes an action on the environment based on its policy $\\pi(a_t|s_t)$, where $s_t$ is the current observation from the environment, and receives a reward $r_{t+1}$ and the next observation $s_{t+1}$ from the environment. The goal is to improve the policy so as to maximize the sum of rewards (return). \n> a policy is an agent's strategy.","e5f2e682":"## The Actor-Critic model","170a7301":"## The game begins\nSo lets train our model with respect to random actions and see what happens..."}}