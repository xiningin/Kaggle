{"cell_type":{"1e6d1237":"code","ead9c557":"code","3f7b6ca1":"code","eb00266c":"code","b7457090":"code","b0422cb8":"code","c71650d5":"code","987ded88":"code","2fbcc14b":"code","0c130db7":"code","76b67e59":"code","b46223b0":"code","2a21ecbb":"code","f94f3f26":"code","c9ff9243":"code","9419428e":"code","970dfca5":"code","e166a508":"code","f38f1a79":"code","4343f731":"code","7c1ca1c8":"code","7b0ccf4a":"code","435efcd7":"code","8e3e0131":"code","9605a6ec":"code","c8de776f":"code","2febefbf":"code","2e5aa0a3":"code","9931b07e":"code","c81c1da2":"code","32272c45":"code","393d626e":"code","54026d55":"markdown","5fe6cd2e":"markdown","b00c3a7d":"markdown","1b15a15c":"markdown","df348f1a":"markdown","c1944121":"markdown","4500a8cf":"markdown","e090134a":"markdown","29ad4932":"markdown","c0eb1e36":"markdown","9581075b":"markdown","2973e58f":"markdown","6f7bffa0":"markdown","b037c010":"markdown","08ad1da5":"markdown","b07c81f3":"markdown","5d132642":"markdown","147eabe2":"markdown","698b1317":"markdown","8e04da3f":"markdown","9f0ff264":"markdown","fa5ae8d4":"markdown","70944a2a":"markdown","5fd8b4bd":"markdown","98f22f36":"markdown","3e435e6c":"markdown"},"source":{"1e6d1237":"import pandas as pd \nimport numpy as np\nimport re\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\n\nfrom sklearn import model_selection, ensemble, metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nimport nltk\nnltk.download('stopwords', 'wordnet', 'punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","ead9c557":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3f7b6ca1":"train_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\ntest_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/unlabeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\nsubmit_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)","eb00266c":"print('Shape of train data: ', train_data.shape)\nprint('Shape of test data: ', test_data.shape)\nprint('Shape of submit data: ', submit_data.shape)","b7457090":"train_data.head()","b0422cb8":"print('Column names:', list(train_data.columns))","c71650d5":"train_data.review[0]","987ded88":"def my_tokenizer(sample):\n    # Split into words\n    words = nltk.word_tokenize(sample)\n#     print('3',words)\n    \n    # Leave alphabetical tokens\n    tokens = [word for word in words if word.isalnum()]\n    tokens = [word for word in tokens if not word.isdigit()]\n#     print('4',tokens)\n    \n    # Remove stopwords\n    meaningful_words = [w for w in tokens if not w in stops]\n#     print('5',meaningful_words)\n    \n    # Lemmatization \n    word_list = [lemmatizer.lemmatize(w) for w in meaningful_words]\n#     print('6',word_list)\n    \n    return word_list\n\ndef my_preprocessor(sample):\n    # Remove HTML tags\n    no_tags_text = BeautifulSoup(sample).get_text()  \n#     print('1',no_tags_text)\n    \n    # To lowercase\n    review_text = no_tags_text.lower()\n#     print('2',review_text)\n    \n    return review_text","2fbcc14b":"stops = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\nvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = my_tokenizer, preprocessor = my_preprocessor, \\\n                             stop_words = None, max_features = 5000) ","0c130db7":"sample = train_data.review[0]\ntrain_data_features = vectorizer.fit_transform([sample])\ntrain_data_features = train_data_features.toarray()\n\ntrain_data_features","76b67e59":"train_data_features = vectorizer.fit_transform(train_data.review)\ntrain_data_features = train_data_features.toarray()","b46223b0":"# test_data_features = vectorizer.transform(test_data.review)\n# test_data_features = test_data_features.toarray()","2a21ecbb":"submit_data_features = vectorizer.transform(submit_data.review)\nsubmit_data_features = submit_data_features.toarray()","f94f3f26":"train_data_features.shape","c9ff9243":"vocab = vectorizer.get_feature_names()","9419428e":"roc_auc_scorer = metrics.make_scorer(metrics.roc_auc_score)\nX_train = train_data_features[:500]\nY_train = train_data.sentiment[:500]","970dfca5":"forest = RandomForestClassifier(n_estimators = 100) \nforest = forest.fit( train_data_features, train_data.sentiment )","e166a508":"result = forest.predict(submit_data_features)","f38f1a79":"params = {'kernel':['linear', 'rbf'], 'C':[0.1, 1, 5, 10]}\nsvc = SVC(probability = True, random_state = 0)\nclf = GridSearchCV(svc, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","4343f731":"svc_best = SVC(C = clf.best_params_['C'], kernel = clf.best_params_['kernel'], probability = True, random_state = 0)","7c1ca1c8":"params = {'n_estimators':[10, 50, 100, 150], 'criterion':['gini', 'entropy'], 'max_depth':[None, 5, 10, 50]}\nrf = RandomForestClassifier(random_state = 0)\nclf = GridSearchCV(rf, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","7b0ccf4a":"rf_best = RandomForestClassifier(n_estimators = clf.best_params_['n_estimators'], criterion = clf.best_params_['criterion'], \\\n                                 max_depth = clf.best_params_['max_depth'], random_state = 0)","435efcd7":"params = {'penalty':['l1', 'l2'], 'C':[1, 2, 3, 5, 10]}\nlr = LogisticRegression(random_state = 0)\nclf = GridSearchCV(lr, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","8e3e0131":"lr_best = LogisticRegression(penalty = clf.best_params_['penalty'], C = clf.best_params_['C'], random_state = 0)\n# lr_best = LogisticRegression(penalty = 'l2', C = 10, random_state = 0)","9605a6ec":"params = {\"var_smoothing\" : [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]}\nnb = GaussianNB()\nclf = GridSearchCV(nb, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","c8de776f":"nb_best = GaussianNB(var_smoothing = clf.best_params_['var_smoothing'])","2febefbf":"params = {'n_neighbors':[3, 5, 10, 20], 'p':[1, 2, 5], 'weights':['uniform', 'distance']}\nknc = KNeighborsClassifier()\nclf = GridSearchCV(knc, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","2e5aa0a3":"knc_best = KNeighborsClassifier(n_neighbors = clf.best_params_['n_neighbors'], p=clf.best_params_['p'],\\\n                               weights = clf.best_params_['weights'])","9931b07e":"# voting_clf = VotingClassifier(estimators=[('svc', svc_best), ('rf', rf_best), ('lr', lr_best), ('nb', nb_best),\\\n#                                           ('knc', knc_best)], voting='hard')\n# voting_clf.fit(train_data_features, train_data.sentiment)\n","c81c1da2":"lr_best.fit(train_data_features, train_data.sentiment)\ny_pred = lr_best.predict(submit_data_features)","32272c45":"submission = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/sampleSubmission.csv\", header=0, delimiter=\",\", quoting=3)\ncol = submission.columns[1]\nsubmission[col] = y_pred\nsubmission.to_csv('submission.csv', index=False)","393d626e":"f = open(\"submission.csv\", \"r\")\nf.readline()\ns = open(\"valid_submission.csv\",\"w+\")\ns.write('\\\"id\\\",\\\"sentiment\\\"\\n')\nfor x in f:\n    x = x.split(',')\n    x[0] = x[0][2:-2]\n    s.write(','.join(x))","54026d55":"Let's see review samples.","5fe6cd2e":"## 3 - Data Cleaning and Text Preprocessing","b00c3a7d":"Somewhy this file saved symbols \"\"\" instead of \". And I rewrited it in this way:","1b15a15c":"## 2 - Overview of the Dataset","df348f1a":"Loading data:","c1944121":"Then we will instantiate lemmatizer and load useful set of stopwords. Now we are ready to instantiate `vectorizer` to perform whole text preprocessing and create bag-of-words features.","4500a8cf":"Check column names:","e090134a":"Next functions are created for data cleaning. Preprocessing function prepares a review for following splitting into tokens. Tokenizing function works with every word in review to get the most accurate tokens for analyzing.","29ad4932":"Now that the Bag of Words model is trained, let's look at the vocabulary:","c0eb1e36":"### Naive Bayes","9581075b":"# Bag of Words Meets Bags of Popcorn","2973e58f":"Text has HTML tags, punctuation, stop words(such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) and numbers. So, we need to clean our data and then tokenize it.","6f7bffa0":"### LogisticRegression","b037c010":"### RandomForestClassifier","08ad1da5":"### KNN","b07c81f3":"## 1 - Packages","5d132642":"Downloading results to file. ","147eabe2":"### SVM","698b1317":"Let's check work of `vectorizer` on the first sample in out train dataset.","8e04da3f":"Shape of datasets:","9f0ff264":"## 4 - Model","fa5ae8d4":"The training data array now looks like:","70944a2a":"Below we will consider different models and choose that one, which gives the best metric rate. By the terms, submissions are judged on area under the ROC curve. \n\nLet's configure:\n1. SVM\n2. Naive Bayes\n3. RandomForestClassifier\n4. KNN\n5. Logistic Regression","5fd8b4bd":"This is my second Kaggle competition. This is tutorial competition to learn about Word2vec neural network implementation but I realized project depending on tips for applying a simple Bag of Words model, mentioned in the tutorial. \n\nLink: https:\/\/www.kaggle.com\/c\/word2vec-nlp-tutorial\n\nProblem description: need to create model for sentiment analysis of movie reviews. The model must distinguish negative and positive movie reviews and mark them as 0 and 1 accordingly. (NLP problem)","98f22f36":"First 5 rows of train data dataset:","3e435e6c":"So, at least it works. I don't really know how to check if it's valid. Now fit our model and transform all train dataset."}}