{"cell_type":{"71d841ed":"code","749376fa":"code","d9279ece":"code","a08cf920":"code","78dd0595":"code","08b5e14c":"code","b7d0e389":"code","0ed0c12e":"code","79ac80b4":"code","0998a147":"code","5c900412":"code","d931bae7":"code","9efcc38e":"code","a0886b3b":"code","8b5acbe2":"code","6f7a0550":"code","77f5c396":"code","63ca4b36":"code","1c6dda33":"code","7aec00e2":"code","3ee5981a":"code","b80e6656":"code","aa9cd039":"code","e7461990":"code","ec72f034":"code","dfb4d9a4":"code","5d85d61d":"code","ec918673":"code","70a5dd3c":"code","5401cb7c":"code","1d9b490c":"code","12aa3416":"code","62b85367":"code","df19525d":"code","09f1629a":"code","c9af8ab8":"code","9dfdf37a":"code","bcb43da5":"code","806c431f":"code","69ddecb3":"code","d83af2a8":"code","ad75a2b3":"markdown","87791d44":"markdown","45d88bb8":"markdown","430164e2":"markdown","e9cd3a69":"markdown","08f6c3e5":"markdown","33f7b9ea":"markdown","ed3fe166":"markdown","1bcd88e1":"markdown","fdddcbc7":"markdown","cecc4f60":"markdown","746b8589":"markdown","e390d8de":"markdown","bd9abf4e":"markdown","92b94aa9":"markdown","3b0737db":"markdown","8912a8d4":"markdown","c21fa8f6":"markdown","b5dd9a3f":"markdown","351be26f":"markdown","26582713":"markdown","95b3b6f9":"markdown","fa801ae2":"markdown","2f5d5f3c":"markdown","d228925b":"markdown","1cb9dc7e":"markdown","a10e93ae":"markdown"},"source":{"71d841ed":"# Helper libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\n# Sci-kit learn libraries\nimport sklearn as sk\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# Imbalanced-learn libraries\nfrom imblearn import under_sampling, over_sampling\nfrom imblearn.over_sampling import SMOTE","749376fa":"# Helper functions, used much later on\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    # Retrieved from https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\ndef quaternion_to_euler(x, y, z, w):\n    # Retrieved from https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\n    # Returns a radian measurement for roll, pitch, and yaw\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef normalize(data):\n    # Normalizes the direction-dependent parameters for an input dataset 'data'\n    # Specifically, creates unit vectors for orientation, velocity, and acceleration\n    data['mod_quat'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2 + data['orientation_W']**2)**.5\n    data['norm_orientation_X'] = data['orientation_X']\/data['mod_quat']\n    data['norm_orientation_Y'] = data['orientation_Y']\/data['mod_quat']\n    data['norm_orientation_Z'] = data['orientation_Z']\/data['mod_quat']\n    data['norm_orientation_W'] = data['orientation_W']\/data['mod_quat']\n    \n    data['mod_angular_velocity'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)**.5\n    data['norm_velocity_X'] = data['angular_velocity_X']\/data['mod_angular_velocity']\n    data['norm_velocity_Y'] = data['angular_velocity_Y']\/data['mod_angular_velocity']\n    data['norm_velocity_Z'] = data['angular_velocity_Z']\/data['mod_angular_velocity']\n    \n    data['mod_linear_acceleration'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**.5\n    data['norm_acceleration_X'] = data['linear_acceleration_X']\/data['mod_linear_acceleration']\n    data['norm_acceleration_Y'] = data['linear_acceleration_Y']\/data['mod_linear_acceleration']\n    data['norm_acceleration_Z'] = data['linear_acceleration_Z']\/data['mod_linear_acceleration']\n    return data\n\ndef add_euler_angles(data):\n    # Derives Euler angles from the quaternion for an input dataset 'data'\n    # *Requires normalized quaternion orientations first*\n    x = data['norm_orientation_X'].tolist()\n    y = data['norm_orientation_Y'].tolist()\n    z = data['norm_orientation_Z'].tolist()\n    w = data['norm_orientation_W'].tolist()\n    eX, eY, eZ = [],[],[]\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        eX.append(xx)\n        eY.append(yy)\n        eZ.append(zz)\n    data['euler_X'] = eX\n    data['euler_Y'] = eY\n    data['euler_Z'] = eZ\n    return data\n\ndef add_direction_vectors(data):\n    # Derives unit direction vectors from Euler angles in dataset 'data'\n    roll = data['euler_X'].tolist()\n    pitch = data['euler_Y'].tolist()\n    yaw = data['euler_Z'].tolist()\n    uX, uY, uZ = [],[],[]\n    for i in range(len(roll)):\n        xx = math.cos(yaw[i])*math.cos(pitch[i])\n        yy = math.sin(yaw[i])+math.cos(pitch[i])\n        zz = math.sin(pitch[i])\n        uX.append(xx)\n        uY.append(yy)\n        uZ.append(zz)\n    data['orientation_vector_X'] = uX\n    data['orientation_vector_Y'] = uY\n    data['orientation_vector_Z'] = uZ\n    return data\n\ndef eng_data(data):\n    # Creates engineered features within dataset 'data'\n    # Intended for use on the raw X data\n    \n    # Idea 1: Ratios\n    data['ratio_velocity-acceleration'] = data['mod_angular_velocity'] \/ data['mod_linear_acceleration']\n    \n    # Idea 2: \n    \n    return data\n\ndef descriptive_features(features, data, stats):\n    # Creates descriptive statistics such as max, min, std. dev, mean, median, etc. from\n    # features 'stats' in dataset 'data' and stores these in 'features'\n    for col in data.columns:\n        if col not in stats:\n            continue\n        # Base statistics\n        colData = data.groupby(['series_id'])[col]\n        features[col + '_min'] = colData.min()\n        features[col + '_max'] = colData.max()\n        features[col + '_std'] = colData.std()\n        features[col + '_mean'] = colData.mean()\n        features[col + '_median'] = colData.median()\n        \n        # Derivative statistics\n        features[col + '_range'] = features[col + '_max']-features[col + '_min']\n        features[col + '_maxOverMin'] = features[col + '_max']\/features[col + '_min']\n        features[col + '_mean_abs_chg'] = colData.apply(lambda x: \n                                                        np.mean(np.abs(np.diff(x))))\n        features[col + '_abs_max'] = colData.apply(lambda x: \n                                                   np.max(np.abs(x)))\n        features[col + '_abs_min'] = colData.apply(lambda x: \n                                                   np.min(np.abs(x)))\n        features[col + '_abs_avg'] = (features[col + '_abs_min'] \n                                      + features[col + '_abs_max'])\/2\n    return features\n\ndef eng_features(features):\n    # Creates engineered features within dataset 'features'\n    # Intended for use on the modified X data\n    \n    # Idea 1: Dot and cross products of unit direction vectors\n    # Note: np.dot and np.cross are very slow to perform on large sets of data,\n    # minimize iterations used of them\n    stat = '_mean'\n    Ox = features['orientation_vector_X' + stat]\n    Oy = features['orientation_vector_Y' + stat]\n    Oz = features['orientation_vector_Z' + stat]\n    Vx = features['norm_velocity_X' + stat]\n    Vy = features['norm_velocity_Y' + stat]\n    Vz = features['norm_velocity_Z' + stat]\n    Ax = features['norm_acceleration_X' + stat]\n    Ay = features['norm_acceleration_Y' + stat]\n    Az = features['norm_acceleration_Z' + stat]\n    \n    oDv,oDa,vDa = [],[],[]\n    oCv_x,oCv_y,oCv_z = [],[],[]\n    oCa_x,oCa_y,oCa_z = [],[],[]\n    vCa_x,vCa_y,vCa_z = [],[],[]\n    for i in range(len(Ox)):\n        oDv.append(np.dot([Ox[i],Oy[i],Oz[i]],[Vx[i],Vy[i],Vz[i]]))\n        oCv = np.cross([Ox[i],Oy[i],Oz[i]],[Vx[i],Vy[i],Vz[i]])\n        oCv_x.append(oCv[0])\n        oCv_y.append(oCv[1])\n        oCv_z.append(oCv[2])\n        oDa.append(np.dot([Ox[i],Oy[i],Oz[i]],[Ax[i],Ay[i],Az[i]]))\n        oCa = np.cross([Ox[i],Oy[i],Oz[i]],[Vx[i],Vy[i],Vz[i]])\n        oCa_x.append(oCa[0])\n        oCa_y.append(oCa[1])\n        oCa_z.append(oCa[2])\n        vDa.append(np.dot([Vx[i],Vy[i],Vz[i]],[Ax[i],Ay[i],Az[i]]))\n        vCa = np.cross([Ox[i],Oy[i],Oz[i]],[Vx[i],Vy[i],Vz[i]])\n        vCa_x.append(vCa[0])\n        vCa_y.append(vCa[1])\n        vCa_z.append(vCa[2])\n        \n    features['orientation_dot_velocity'] = oDv\n    features['orientation_cross_velocity_X'] = oCv_x\n    features['orientation_cross_velocity_Y'] = oCv_y\n    features['orientation_cross_velocity_Z'] = oCv_z\n    features['orientation_dot_acceleration'] = oDa\n    features['orientation_cross_acceleration_X'] = oCa_x\n    features['orientation_cross_acceleration_Y'] = oCa_y\n    features['orientation_cross_acceleration_Z'] = oCa_z\n    features['velocity_dot_acceleration'] = vDa\n    features['velocity_cross_acceleration_X'] = vCa_x\n    features['velocity_cross_acceleration_Y'] = vCa_y\n    features['velocity_cross_acceleration_Z'] = vCa_y\n    \n    return features","d9279ece":"x_test = pd.read_csv('..\/input\/X_test.csv')\nx_train = pd.read_csv('..\/input\/X_train.csv')\ny_train = pd.read_csv('..\/input\/y_train.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nprint('Train X: {}\\nTrain Y: {}\\nTest X: {}\\nSubmission: {}'.format(x_train.shape,y_train.shape,x_test.shape,submission.shape))","a08cf920":"x_train.head()","78dd0595":"x_train.describe()","08b5e14c":"y_train.head()","b7d0e389":"y_train.describe()","0ed0c12e":"x_test.head()","79ac80b4":"x_test.describe()","0998a147":"submission.head()","5c900412":"submission.describe()","d931bae7":"sns.set(style='darkgrid')\nsns.countplot(y = 'surface',\n              data = y_train,\n              order = y_train['surface'].value_counts().index)\nplt.show()","9efcc38e":"plt.figure(figsize=(30,10)) \nsns.set(style=\"darkgrid\",font_scale=1.5)\nsns.countplot(x=\"group_id\", data=y_train, order = y_train['group_id'].value_counts().index)\nplt.show()","a0886b3b":"f,ax = plt.subplots(figsize=(8, 8))\nplt.title(\"X Train\")\nsns.heatmap(x_train.iloc[:,3:].corr(), annot=True, linewidths=1.5, fmt= '.2f', annot_kws={\"size\": 10}, ax=ax)","8b5acbe2":"f,ax = plt.subplots(figsize=(8, 8))\nplt.title(\"X Test\")\nsns.heatmap(x_test.iloc[:,3:].corr(), annot=True, linewidths=1.5, fmt= '.2f', annot_kws={\"size\": 10}, ax=ax)","6f7a0550":"x_train = normalize(x_train)\nx_train = add_euler_angles(x_train)\nx_test = normalize(x_test)\nx_test = add_euler_angles(x_test)","77f5c396":"fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize=(15,5))\n\nax1.set_title('Roll')\nsns.kdeplot(x_train['euler_X'], ax=ax1, label='train')\nsns.kdeplot(x_test['euler_X'], ax=ax1, label='test')\n\nax2.set_title('Pitch')\nsns.kdeplot(x_train['euler_Y'], ax=ax2, label='train')\nsns.kdeplot(x_test['euler_Y'], ax=ax2, label='test')\n\nax3.set_title('Yaw')\nsns.kdeplot(x_train['euler_Z'], ax=ax3, label='train')\nsns.kdeplot(x_test['euler_Z'], ax=ax3, label='test')\n\nplt.show()","63ca4b36":"train_features = pd.DataFrame()\ntest_features = pd.DataFrame()","1c6dda33":"x_train = add_direction_vectors(x_train)\nx_test = add_direction_vectors(x_test)\nx_train.head()","7aec00e2":"stats = ['norm_orientation_X', 'norm_orientation_Y', 'norm_orientation_Z', \n         'norm_orientation_W',\n         'norm_velocity_X', 'norm_velocity_Y', 'norm_velocity_Z', \n         'norm_acceleration_X', 'norm_acceleration_Y', 'norm_acceleration_Z',\n         'mod_linear_acceleration',\n         'ratio_velocity-acceleration', \n         'orientation_vector_X', 'orientation_vector_Y', 'orientation_vector_Z']\ntrain_features = descriptive_features(train_features, x_train, stats)\ntest_features = descriptive_features(test_features, x_test, stats)","3ee5981a":"train_features = eng_features(train_features)\ntest_features = eng_features(test_features)\nprint(\"Train features: \", train_features.shape)\nprint(\"Test features: \", test_features.shape)","b80e6656":"train_features.head()","aa9cd039":"test_features.head()","e7461990":"#https:\/\/stackoverflow.com\/questions\/17778394\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ncorr_matrix = train_features.corr().abs()\nraw_corr = train_features.corr()\n\nsol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\ntop_corr = pd.DataFrame(sol).reset_index()\ntop_corr.columns = [\"var1\", \"var2\", \"abs corr\"]\n# with .abs() we lost the sign, and it's very important.\nfor x in range(len(top_corr)):\n    var1 = top_corr.iloc[x][\"var1\"]\n    var2 = top_corr.iloc[x][\"var2\"]\n    corr = raw_corr[var1][var2]\n    top_corr.at[x, \"raw corr\"] = corr","ec72f034":"top_corr.head(15)","dfb4d9a4":"top_corr.tail(15)","5d85d61d":"drops = []\n# Takes the most correlated features out above a given threshold\nthreshold = .95\nfor i in range(len(top_corr)):\n    if top_corr.iloc[i][\"raw corr\"] > threshold:\n        if top_corr.iloc[i][\"var1\"] not in drops and top_corr.iloc[i][\"var2\"] not in drops:\n            drops.append(top_corr.iloc[i][\"var2\"])\ntrain_features = train_features.drop(columns=drops)\ntest_features = test_features.drop(columns=drops)","ec918673":"seed = 1920348\nsm = SMOTE(sampling_strategy='not majority', random_state=seed, k_neighbors=10)\ny_train_rs = pd.DataFrame()\ntrain_features_rs, y_train_rs['surface'] = sm.fit_resample(train_features, y_train['surface'])","70a5dd3c":"sns.set(style='darkgrid')\nsns.countplot(y = 'surface',\n              data = y_train_rs,\n              order = y_train_rs['surface'].value_counts().index)\nplt.show()","5401cb7c":"train_features.head()","1d9b490c":"y_train.head()","12aa3416":"test_features.head()","62b85367":"# Choose ensemble method used\n#model1 = ensemble.RandomForestClassifier(n_estimators=75)\n#model2 = ensemble.BaggingClassifier(n_estimators=25)\n#model3 = ensemble.ExtraTreesClassifier(n_estimators=25)\nmodel = ensemble.RandomForestClassifier(n_estimators=75)\n\n# create the ensemble model\nseed = 89175915\nkfold = model_selection.StratifiedKFold(n_splits=10, random_state=seed)\nresults = model_selection.cross_val_score(model, train_features, \n                    y_train['surface'], cv=kfold)\nfor i in range(len(results)):\n    print(\"Fold\", i+1, \"score: \", results[i])\nprint(\"Cross-validation score average on original data: \", results.mean())","df19525d":"model.fit(train_features, y_train['surface'])\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\nfeature_importances = pd.DataFrame(importances, index = train_features.columns, columns = ['importance'])\nfeature_importances.sort_values('importance', ascending = False).plot(kind = 'bar',\n                        figsize = (35,8), color = 'r', yerr=std[indices], align = 'center')\nplt.xticks(rotation=90)\nplt.show()","09f1629a":"# create the ensemble model\nseed = 666839274\nkfold = model_selection.StratifiedKFold(n_splits=10, random_state=seed)\nmodel_rs = ensemble.RandomForestClassifier(n_estimators=75)\nresults = model_selection.cross_val_score(model_rs, train_features, \n                    y_train['surface'], cv=kfold)\nfor i in range(len(results)):\n    print(\"Fold \", i+1, \"score: \", results[i])\nprint(\"Cross-validation score average on resampled data: \", results.mean())","c9af8ab8":"model_rs.fit(train_features, y_train['surface'])\nimportances = model_rs.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\nfeature_importances = pd.DataFrame(importances, index = train_features.columns, columns = ['importance'])\nfeature_importances.sort_values('importance', ascending = False).plot(kind = 'bar',\n                        figsize = (35,8), color = 'r', yerr=std[indices], align = 'center')\nplt.xticks(rotation=90)\nplt.show()","9dfdf37a":"resample = True\nif resample:\n    x, y = train_features_rs, y_train_rs['surface']\n    final_model = model_rs\nelse:\n    x, y = train_features, y_train['surface']\n    final_model = model","bcb43da5":"sk.metrics.confusion_matrix(y,final_model.predict(x), y.unique())","806c431f":"submission['surface'] = final_model.predict(test_features)\nsubmission.head()","69ddecb3":"print(\"Final submission file has dimensions: \", submission.shape)","d83af2a8":"submission.to_csv('final_submission_smote.csv', index = False)","ad75a2b3":"After generating these descriptive features, we want to begin engineering custom parameters. So far, I've added dot and cross products of the mean direction vector values. While using the mean vector values will run into all sorts of arithmetic issues (namely not generating a unit vector), it would the most representative of the simple statistical measures I've used.","87791d44":"#### Feature importances for original data","45d88bb8":"Chart of the data to confirm that the resampling worked as intended:","430164e2":"#### Cross-validation results for original data:","e9cd3a69":"First, we want to convert the quarternion into Euler angles and normalize the parameters (Refer to helper functions file)","08f6c3e5":"#### Cross-validation results for resampled data:","33f7b9ea":"## Classification\n\nHere I run the ensemble on both versions of training data, with a bagging classifier currently in use due to overfitting issues","ed3fe166":"Let's visualize the Euler angles for each set of X data:","1bcd88e1":"The following code allows you to toggle between the original and resampled versions of the data, simply change \"resample\" to True or False as desired. In hindsight, resampling doesn't appear to do much good as currently set up.","fdddcbc7":"First, we want to determine the distribution of the surfaces and group_ids in order to alter our cross-validation methodology accordingly","cecc4f60":"Now we want to create derivative features based on these existing variables and store them in the feature sets (in order to convert from the number of instances in the raw X datasets to the number of Y instances)","746b8589":"### Feature Selection\n\nConduct feature selection on x_train and x_test data for use in model and predictions","e390d8de":"## Data Exploration\n\nLoad in data and analyze its composition","bd9abf4e":"# Robot Ensemble Prediction: Kaggle CareerCon 2019\n\nCredits to jesucristo on Kaggle for their wonderful public kernel (https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73) that was instrumental in me learning the ropes; various bits of exploration graphing and the general format of the notebook are greatly inspired by the work there.\n\nIn this notebook, I will be using an Ensemble method prediction after creating derivative features such as normalized parameters, Euler angles, and dot and cross products then balancing the surface classes using SMOTE.\n\nNOTE: Through iteration, I've found the model to be slightly improved when I don't include dot and cross products, and only use orientation-based parameters. SMOTE resampling seems to (very slightly) improve outcomes, though the difference seems insignificant.\n\n## Setup\n\n#### Initialize libraries and outside packages","92b94aa9":"The results suggest that the new features were constructed as intended, we now want to narrow down the most highly correlated features to simplify the model. We will examine the correlations on the training feature set and apply any changes to both sets of features","3b0737db":"It is important to set \"index = False\" when creating the file, else you will wind up with an unwanted header column.","8912a8d4":"### Visualize data composition","c21fa8f6":"#### Feature importances for resampled data","b5dd9a3f":"My confusion matrix currently suggests overfitting, likely due to too many features.","351be26f":"From these correlation matrices, we can see that the Y and Z orientations and X and W orientations are perfectly correlated. The angular velocity and linear accelerations in directions Y and Z are each also highly correlated, dropping the features associated with one of these directions may prove beneficial later on.","26582713":"#### Load in helper functions, these are quite lengthy!","95b3b6f9":"We saw that the surface classes were very heavily imbalanced, and so to solve this I will use SMOTE (Synthetic Minority Over-sampling Technique) to balance them by creating resampled training sets, to be used in the final fitting. I still will retain the original data to run a separate cross-validation on and to easily switch to if desired.\n\nThe sampling strategy I intended is to set all classes to be of the same quantity as the majority class, alter \"sampling_strategy\" parameter if you wish to change this","fa801ae2":"#### The end! Let me know if you have any comments or suggestions about the solution","2f5d5f3c":"Possible drops to reduce model complexity will be made here, although I experienced better performance :","d228925b":"Now we will create the final submission on the test data, first ensuring that the file is formatted as expected","1cb9dc7e":"Next, we want to determine feature correlation in the X train and test sets in order to drop heavily correlated variables, as these will likely overfit the model.","a10e93ae":"### Class Balancing"}}