{"cell_type":{"b3236141":"code","e8c284f3":"code","78664272":"code","3f3ead30":"code","b32a63c8":"code","4ea4e7be":"code","082fc5ba":"code","52c09f7b":"code","d49e7f5c":"code","6701c1b0":"code","26ebf44f":"code","3d832542":"code","3cec32a9":"code","6bac51ca":"code","f037ffb1":"code","16b9fa47":"markdown","2cfec36d":"markdown","745d1a02":"markdown","5f163d94":"markdown","70eff3e0":"markdown","e94d6a9d":"markdown","f822441a":"markdown","a1f28a1c":"markdown","992e3ff3":"markdown"},"source":{"b3236141":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport string \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext import data\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e8c284f3":"def load_file(filepath, device, MAX_VOCAB_SIZE = 25_000):\n    # our tokenizer: removing the punctuation & spliting the sentence.\n    tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n    \n    # Step one defination of our fields. \n    TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer, fix_length=100)\n    LABEL = data.Field(sequential=False, use_vocab=False)\n    \n    print(\"loading from csv ...\")\n    tv_datafields = [(\"text\", TEXT), (\"label\", LABEL)]\n    \n    # Step two construction our dataset.\n    train, valid, test = data.TabularDataset.splits(path=filepath,\n                                                    train=\"Train.csv\", validation=\"Valid.csv\",\n                                                    test=\"Test.csv\", format=\"csv\",\n                                                    skip_header=True, fields=tv_datafields)\n    print(train[0].__dict__.keys())\n    \n    \n    # Step three We should build_vocab for the field with use_vocab=True. \n    # If not we will get an error during the loop section.\n    TEXT.build_vocab(train, max_size = MAX_VOCAB_SIZE)\n    \n    print(\"build vocab success...\")\n    \n    # Step four construct our iterator to our dataset. \n    train_iter = data.BucketIterator(train, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n                                     sort_within_batch=False, repeat=False)\n    valid_iter = data.BucketIterator(valid, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n                                     sort_within_batch=False, repeat=False)\n    test_iter = data.BucketIterator(test, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n                                     sort_within_batch=False, repeat=False)\n    print(\"construct iterator success...\")\n    return TEXT, LABEL, train, valid, test, train_iter, valid_iter, test_iter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTEXT, LABEL, train, valid, test, train_iter, valid_iter, test_iter = load_file('\/kaggle\/input\/imdb-dataset-sentiment-analysis-in-csv-format', device)","78664272":"# most common words and their frequencies.\nprint(TEXT.vocab.freqs.most_common(20))\n\n# top ten index to words transform.\nprint(TEXT.vocab.itos[:10])","3f3ead30":"class SentimentModel(nn.Module):\n    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, text):\n        # text [sentence length, batch_size]\n\n        embedded = self.embedding(text)\n        \n        # embedded = [sentence length, batch_size, emb dim]\n        output, hidden = self.rnn(embedded)\n        \n        # output = [sent len, batch_size, hid dim]\n        # hidden = [1, batch_size, hid dim]\n        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n        \n        return self.fc(hidden.squeeze(0))   ","b32a63c8":"INPUT_DIM = len(TEXT.vocab)\nprint(INPUT_DIM)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\n\nmodel = SentimentModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)","4ea4e7be":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","082fc5ba":"optimizer = optim.SGD(model.parameters(), lr=1e-3)","52c09f7b":"criterion = nn.BCEWithLogitsLoss()","d49e7f5c":"model = model.to(device)\ncriterion = criterion.to(device)","6701c1b0":"def binary_accuracy(preds, y):\n    '''\n    Return accuracy per batch ..\n    '''\n    \n    # round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float()\n    acc = correct.sum() \/ len(correct)\n    \n    return acc","26ebf44f":"def train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for i, batch in enumerate(iterator):\n        optimizer.zero_grad()\n\n        predictions = model(batch.text).squeeze(1)\n        \n        # note we must transform the batch.label into float or we will get an error later.\n        loss = criterion(predictions, batch.label.float())\n        acc = binary_accuracy(predictions, batch.label)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        if i % 200 == 199:\n            print(f\"[{i}\/{len(iterator)}] : epoch_acc: {epoch_acc \/ len(iterator):.2f}\")\n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","3d832542":"def evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            # prediction [batch_size]\n            predictions = model(batch.text).squeeze(1)\n            \n            loss = criterion(predictions, batch.label.float())\n            \n            acc = binary_accuracy(predictions, batch.label)\n        \n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            \n    return epoch_loss \/ len(iterator),  epoch_acc \/ len(iterator)","3cec32a9":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  \/ 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs","6bac51ca":"N_epoches = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_epoches):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'Sentiment-model.pt')\n        \n    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain  Loss: {train_loss: .3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\tValid  Loss: {valid_loss: .3f} | Valid Acc: {valid_acc*100:.2f}%')","f037ffb1":"model.load_state_dict(torch.load('Sentiment-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iter, criterion)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","16b9fa47":"This is the trival model of using pytorch & torchtext. You may find the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which can be improve in your own notebook. \n\nAnd I will also commit some notebook later.\n\nFinally the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss.","2cfec36d":"## Train the Model\n\nNow we will set up the training and then train the model.\n\nFirst we will create an optimizer. This is the algorithm we use to update the parameters of the module. Here we used the SGD. The first argument is the parameters will be update by the optimizer, the second is the learning rate.","745d1a02":"Next we will define our loss function. In Pytorch this is commonly called a criterion.\n\nThe loss function here is binary cross entropy with logits.\n\nOur model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the sigmoid \/ logit function.\n\nWe then use this bound scalar  to  calculate the loss using binary cross entropy.\n\nThe  BCEWithLogitsLoss criterion carries out both the sigmoid and the binary cross entropy steps.","5f163d94":"Using .to we can place the model and the criterion on the GPU (if we have one).","70eff3e0":"We can also view the most common words in the vocabulary and their frequencies.\n\nAnd also we can use TEXT.vocab.itos & TEXT.vocab.stoi to get the transform between word and index.","e94d6a9d":"## Next Steps\n\nSome improvements we can make are as follows:\n\n-  packed padded sequences\n-  pre-trained word embeddings\n-  different RNN architecture\n-  bidirectional RNN\n-  multi-layer RNN\n- regularization\n- a different optimizer","f822441a":"Our criterion function calculates the loss, however we have to write our function to calculate the accuracy.\n\nThis function first feeds the predictions through a sigmoid layer, squashing the values between 0 and 1, We then round them to the nearest integer.\n\n\nWe then calculate how many rounded predictions equal the actual labels and average it across the batch.","a1f28a1c":"## Load dataset\n\n- First we write a function helping us to load data using torchtext.","992e3ff3":"## Our Model \n\nbuild our RNN(LSTM) model using pytorch !\n\nEach Batch, text, is a tensor of size **\\[sentence length, batch_size\\]**. \n\nThe input batch is then passed through the embedding layer to get embedded, which gives us a dense vector representation of our sentences. embedded is a tensor of size \\[sentence length, batch_size, embedding dim\\]\n\nembedded is the fed into the RNN. In some frameworks you must feed the initial hidden state, $h_0$, into RNN, however in Pytorch, if nno initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n\nThe RNN returns 2 tensors, output of size \\[sentence length, batch size, hidden dim\\] and hidden of size \\[1, batch_size, hidden_dim \\]. output is the concatenation of the hidden state from every time step, whereas hidden is simply the final hidden state. \n\n**squeeze method** : used to remove a dimension of size 1.\n\nFinally, we feed the last hidden state, hidden, through the linear layer, fc to produce a prediction."}}