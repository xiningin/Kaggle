{"cell_type":{"f0b7a1a9":"code","dac98a04":"code","d087cfd2":"code","362f21aa":"code","45587d95":"code","455d1be8":"code","76df17b8":"code","51b9a229":"code","b2d4eb49":"code","7618832c":"code","d46d71ce":"code","7ff378a0":"code","d69d454e":"code","a8a4000c":"code","bd2abb01":"code","2a01ce4c":"code","dc45242a":"code","301709aa":"code","a6acb611":"code","9766c702":"code","66462797":"code","6b5ace95":"code","5cf23da1":"code","7e94c2ba":"code","de625fc0":"code","10c83838":"code","2a9e50fb":"code","2379acac":"code","8a38446f":"code","9db3f091":"code","89e27542":"code","ccca641c":"code","e783d4d6":"code","c7bf0db5":"code","018dc262":"code","cda0ebe6":"code","85a34086":"code","e514364c":"code","6e9f0cb7":"code","dfb9119d":"code","f038391b":"code","b82957af":"code","9ad2c740":"code","165ac188":"code","d6325e37":"code","6e0d2396":"code","6e4b8c9a":"code","eb7e9288":"code","d47a3c49":"code","41710461":"code","915914be":"code","531e7e40":"code","38b07167":"code","5b77aee5":"code","93d41da5":"code","6d5be36a":"code","a5e18b84":"code","db4eecf3":"code","597e7f89":"code","9fe6cf05":"code","b39e4e44":"code","d987ec6a":"code","921daf24":"code","e14d9d92":"code","2cede5a4":"code","182a79e1":"code","6f330463":"code","4bd88a1d":"code","55951e68":"code","5355569b":"code","5906cae9":"code","519afafb":"code","71f5fc85":"code","71d091ef":"code","7e42361d":"code","3af503d2":"code","f1676d46":"code","786992d2":"code","a51725a8":"code","d5abc06e":"code","d5efac9f":"code","f37be174":"code","bbcb92db":"code","fb0bbe74":"code","efb6e0cf":"code","37725646":"code","17640e1c":"code","5f07c51a":"code","a7224db0":"code","003284ed":"code","9e15f266":"code","8ccd9adc":"code","8bede58b":"code","1c4bf2fc":"code","7f0f7a29":"code","cd88b2d8":"code","e726db72":"code","7fc3fb38":"code","5cd8ab39":"code","01fb102e":"code","0f23e443":"code","81d4c53e":"code","fa175534":"code","89316f00":"code","53bed095":"code","cb0c700e":"code","5aaa3a1f":"code","e31852f8":"code","66c76844":"code","2938b7d0":"code","c5f222a2":"code","d85f5692":"code","a1f97384":"code","f2ce4eac":"code","2a6e6fd3":"code","ebd5c3ee":"code","a2938704":"code","daf14317":"code","1bff2f82":"code","251a8608":"code","f57c5bd7":"code","ce41823c":"code","c9c426e5":"code","16416622":"code","f7396756":"code","e26ce169":"code","395d7d85":"code","6a57da8b":"code","e48be77a":"code","40eb8564":"code","2088ab94":"code","121934b7":"code","f28e760b":"code","602bd586":"code","0f3b6752":"code","e2e76ecc":"code","864e7cc1":"code","78ee6ae2":"code","5377c588":"code","13e95066":"code","01c1e4dc":"code","7bba7e10":"code","7e7d37d5":"code","597d12e4":"code","f0754bdc":"code","58c4d960":"markdown","563a28d5":"markdown","8b531ee8":"markdown","02edb711":"markdown","e716baa1":"markdown","f9efa181":"markdown","ab07d0be":"markdown","2ac5f809":"markdown","0af098b3":"markdown","7c93db6d":"markdown","ec13271b":"markdown","9abe36ec":"markdown","37b3a0a4":"markdown","ff7f148c":"markdown","e4d2dd0a":"markdown","717e2342":"markdown","e30a1f65":"markdown","37dc50e0":"markdown","9212dbac":"markdown","e2a0ff05":"markdown","cc6ca74f":"markdown","3f979f82":"markdown","38fd0a8e":"markdown","86b2665e":"markdown","6ec00971":"markdown","69068d95":"markdown","3e6e43fe":"markdown","5ec83efc":"markdown","cef1de24":"markdown","087bfe02":"markdown","5b585781":"markdown","e2c690e9":"markdown","27ef05b8":"markdown","700acfaa":"markdown","ca660468":"markdown","3a360a92":"markdown","80c6a0c5":"markdown","e4b4ef11":"markdown","137339fd":"markdown","12a40b98":"markdown","edea7407":"markdown","be216b09":"markdown","a2a12c53":"markdown","dd9040f0":"markdown","ba9d8eaf":"markdown","46c8441e":"markdown","275ab427":"markdown","ecf448b1":"markdown","07104bf1":"markdown","f4d343b4":"markdown","c3a72740":"markdown","2c4acdfe":"markdown","60d6a2c3":"markdown","d915deec":"markdown","5030eaa5":"markdown","ea6af829":"markdown"},"source":{"f0b7a1a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dac98a04":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials , space_eval\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')","d087cfd2":"#Read data\ndata = pd.read_csv('..\/input\/titanic\/train.csv')\n\n#Printing first 5 rows\ndata[:5]","362f21aa":"data.info()","45587d95":"data['Survived'].value_counts()","455d1be8":"sns.set()\nsns.set(rc = {'figure.figsize':(9,5)})\nsns.countplot(x=\"Survived\",data=data)","76df17b8":"data.corrwith(data[\"Survived\"])","51b9a229":"plt.bar(dict(data.corrwith(data[\"Survived\"])).keys(), dict(data.corrwith(data[\"Survived\"])).values())","b2d4eb49":"#Checking for Nan values in features excluding PassengerId, Name\nprint('Nan values exist in')\nfor col in [data.columns[i] for i in range(0, len(data.columns))]: #[1,2,4,5,6,7,8,9,10,11]]:\n    print(col,':', any(data[col].isna()))","7618832c":"data['Age'] = data['Age'].fillna('nan')\n\nprint(data[data['Age'] == 'nan']['Age'])","d46d71ce":"def impute_missing_data(test, col, median, mode):\n    if col in test :\n        test[col] = test[col].fillna('nan')\n\n        test_feature = test.drop(test[test[col]== 'nan'].index, axis= 0)[col]\n\n        nan_index = test[test[col]== 'nan'].index\n\n        for i in nan_index:\n            if median == True:\n                test[col].iloc[i] = test_feature.median()\n            elif mode == True:\n                test[col].iloc[i] = test_feature.value_counts().index[0]\n        \n    return test","7ff378a0":"#Imputing mean values for missing data\ndata = impute_missing_data(data, 'Age', median =True, mode=False)","d69d454e":"any(data['Age'].isna())","a8a4000c":"data['Age'] = data['Age'].map(lambda x: int(x))","bd2abb01":"'''data = data.drop(data[data['Age']== 'nan'].index, axis= 0)\ndata.info()'''","2a01ce4c":"data.corrwith(data[\"Survived\"])","dc45242a":"data['Cabin'] = data['Cabin'].fillna('nan')\n\nprint(data[data['Cabin'] == 'nan']['Cabin'])","301709aa":"data['cabin_nan'] = [1 if data['Cabin'].iloc[i] =='nan' else 0 for i in range(len(data))]\n#data['cabin_val'] = [0 if data['Cabin'].iloc[i] =='nan' else 1 for i in range(len(data))]\n","a6acb611":"data['Embarked'] = data['Embarked'].fillna('nan')\n\nprint(data[data['Embarked'] == 'nan']['Embarked'])","9766c702":"#Imputing the mode values for missing data\ndata = impute_missing_data(data, 'Embarked', median = False, mode = True)","66462797":"any(data['Embarked'].isna())","6b5ace95":"data","5cf23da1":"data.corrwith(data[\"Survived\"])","7e94c2ba":"plt.bar(dict(data.corrwith(data[\"Survived\"])).keys(), dict(data.corrwith(data[\"Survived\"])).values())","de625fc0":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"Age\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'Age', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","10c83838":"def plot_stacked_plot_percentages(class_list, df_p, add_x_val):\n    \n    '''This function adds the percentage in each stacked plot\n    class_list : contains three params, count of each rect\/class, total for that rect\/class, ax object of that rect\/class\n    df_p : df.axes.patches object for each rectange\n    add_x_val : x axis value to center the text\n    '''\n    \n    for class_val in class_list:\n        \n        percentage = (class_val[0]\/class_val[1])*100\n        height = class_val[2].get_height() \n        df_p.axes.text(class_val[2].get_x()+add_x_val, height,\"{:1.1f}%\".format(percentage))","2a9e50fb":"def calculate_stacked_plot_percentages(df_plot, df_p):\n    #Find the counts for each age bin for calcualting percentage\n    df_plot_sum = df_plot.sum(axis=1)\n    class_sum = np.array([df_plot_sum.iloc[int(i\/2)] if i%2==0 else df_plot_sum.iloc[int((i-1)\/2)] for i in range(2*len(df_plot_sum)) ]).reshape((len(df_plot_sum),2)).T.flatten()\n    class_list = zip(np.roll(df_plot.to_numpy().T.flatten(), len(df_plot)), class_sum, list(df_p.axes.patches))\n\n    return class_list","2379acac":"#Stacked countplot : https:\/\/stackoverflow.com\/questions\/50319614\/count-plot-with-stacked-bars-per-hue #Add percentages on stacked countplots : https:\/\/stackoverflow.com\/questions\/31749448\/how-to-add-percentages-on-top-of-bars-in-seaborn\nsns.set(rc = {'figure.figsize':(15,8)})\n\n#Find the bins\ndata['age_bin'] = pd.cut(data['Age'], 3, precision = 0)\n\n#Calculate the counts of categorical variable Survived for different age bins\ndf_plot = data.groupby([ 'Survived', 'age_bin']).size().reset_index().pivot(columns='Survived', index='age_bin', values=0)\nprint(df_plot)\n\n#Plot the stacked bar plot\ndf_p = df_plot.plot(kind='bar', stacked=True)\n\n#Set labels\ndf_p.set_xlabel(\"Age groups\", fontsize = 20)\ndf_p.set_ylabel(\"Count\", fontsize = 20)\n\nclass_list = calculate_stacked_plot_percentages(df_plot, df_p)\n\nplot_stacked_plot_percentages(class_list, df_p, 0.12)","8a38446f":"#how to drop outliers from data after analysing with boxplot : https:\/\/datascience.stackexchange.com\/questions\/54808\/how-to-remove-outliers-using-box-plot\ndef drop_outliers(data , col):\n    Q1 = data['Age'].quantile(0.25)\n    Q3 = data['Age'].quantile(0.75)\n    IQR = Q3 - Q1    #IQR is interquartile range. \n\n    filter = (data['Age'] >= Q1 - 1.5 * IQR) & (data['Age'] <= Q3 + 1.5 *IQR) #filter filters out data within Q1 - 1.5*IQR to Q3 + 1.5*IQR\n    data = data.loc[filter]\n    return data","9db3f091":"#data = drop_outliers(data , 'Age')\n#data","89e27542":"#Stacked countplot : https:\/\/stackoverflow.com\/questions\/50319614\/count-plot-with-stacked-bars-per-hue #Add percentages on stacked countplots : https:\/\/stackoverflow.com\/questions\/31749448\/how-to-add-percentages-on-top-of-bars-in-seaborn\nsns.set(rc = {'figure.figsize':(15,8)})\n\n#Calculate the counts of categorical variable Survived for different age bins\ndf_plot = data.groupby([ 'Survived', 'Sex']).size().reset_index().pivot(columns='Survived', index='Sex', values=0)\nprint(df_plot)\n\n#Plot the stacked bar plot\ndf_p = df_plot.plot(kind='bar', stacked=True)\n\n#Set labels\ndf_p.set_xlabel(\"Sex\", fontsize = 20)\ndf_p.set_ylabel(\"Count\", fontsize = 20)\n\nclass_list = calculate_stacked_plot_percentages(df_plot, df_p)\n\nplot_stacked_plot_percentages(class_list, df_p, 0.2)","ccca641c":"'''sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"Sex\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'Sex', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)'''","e783d4d6":"#data = drop_outliers(data , 'Sex')\n#data","c7bf0db5":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"Fare\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'Fare', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","018dc262":"#data = drop_outliers(data , 'Fare')\n#data","cda0ebe6":"def fares_categorical(data):\n    data['fare_bin'] = pd.cut(data['Fare'], 3, precision = 0)\n    return data\n\ndata = fares_categorical(data)\nmap_obj = {(-1.0, 171.0) : 0, (171.0, 342.0): 1,(342.0, 512.0):2}\ndata['fare_bin'] = data['fare_bin'].map(lambda x: map_obj[(x.left, x.right)])","85a34086":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"fare_bin\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\n#plt.subplot(1,2,2)\n#dp_ax = sns.kdeplot(x = 'fare_bin', hue = 'Survived', data = data)\n#dp_ax.set_title('Density plot', fontsize = 20)","e514364c":"sns.set(rc = {'figure.figsize':(15,8)})\n\n#Calculate the counts of categorical variable Survived for different age bins\ndf_plot = data.groupby([ 'Survived', 'Embarked']).size().reset_index().pivot(columns='Survived', index='Embarked', values=0)\nprint(df_plot)\n\n#Plot the stacked bar plot\ndf_p = df_plot.plot(kind='bar', stacked=True)\n\n#Set labels\ndf_p.set_xlabel(\"Embarked\", fontsize = 20)\ndf_p.set_ylabel(\"Count\", fontsize = 20)\n\nclass_list = calculate_stacked_plot_percentages(df_plot, df_p)\n\nplot_stacked_plot_percentages(class_list, df_p, 0.2)","6e9f0cb7":"'''sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"Embarked\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'Embarked', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)'''","dfb9119d":"data['Name'].value_counts()","f038391b":"data_name_title_1 = data[data['Name'].str.contains('Miss')]['Name']\ndata_name_title_2 = data[data['Name'].str.contains('Mrs')]['Name']\ndata_name_title_3 = data[data['Name'].str.contains('Master')]['Name']\ndata_name_title_4 = data[data['Name'].str.contains('Mr')]['Name']\ndata_name_title_5 = data[~ (data['Name'].str.contains('Miss') | data['Name'].str.contains('Mrs') | data['Name'].str.contains('Mr') | data['Name'].str.contains('Master'))]['Name']","b82957af":"data_name_title_5.value_counts()","9ad2c740":"def name_encoding(data):\n    data['name_title'] = [np.nan for i in range(len(data))]\n\n    data_name_title_1 = data[data['Name'].str.contains('Miss')]['Name']\n    data_name_title_2 = data[data['Name'].str.contains('Mrs')]['Name']\n    data_name_title_3 = data[data['Name'].str.contains('Master')]['Name']\n    data_name_title_4 = data[data['Name'].str.contains('Mr')]['Name']\n    data_name_title_5 = data[~ (data['Name'].str.contains('Miss') | data['Name'].str.contains('Mrs') | data['Name'].str.contains('Mr') | data['Name'].str.contains('Master'))]['Name']\n    \n    for i in range(len(data)):\n        if data['Name'].iloc[i] in list(data_name_title_1):\n            data['name_title'].iloc[i] = 0\n        elif data['Name'].iloc[i] in list(data_name_title_2):\n            data['name_title'].iloc[i] = 1\n        elif data['Name'].iloc[i] in list(data_name_title_3):\n            data['name_title'].iloc[i] = 2\n        elif data['Name'].iloc[i] in list(data_name_title_4):\n            data['name_title'].iloc[i] = 3\n        elif data['Name'].iloc[i] in list(data_name_title_5):\n            data['name_title'].iloc[i] = 4\n        \n    return data","165ac188":"data = name_encoding(data)","d6325e37":"data['name_title'].value_counts()","6e0d2396":"sns.set(rc = {'figure.figsize':(15,8)})\n\n#Calculate the counts of categorical variable Survived for different age bins\ndf_plot = data.groupby([ 'Survived', 'name_title']).size().reset_index().pivot(columns='Survived', index='name_title', values=0)\nprint(df_plot)\n\n#Plot the stacked bar plot\ndf_p = df_plot.plot(kind='bar', stacked=True)\n\n#Set labels\ndf_p.set_xlabel(\"name_title\", fontsize = 20)\ndf_p.set_ylabel(\"Count\", fontsize = 20)\n\nclass_list = calculate_stacked_plot_percentages(df_plot, df_p)\n\nplot_stacked_plot_percentages(class_list, df_p, 0.2)","6e4b8c9a":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"name_title\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'name_title', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","eb7e9288":"data_name_title_1 = data[data['Name'].str.contains('Miss') | data['Name'].str.contains('Mrs') | data['Name'].str.contains('Mr') | data['Name'].str.contains('Master')]['Name']\ndata_name_title_2 = data[~ (data['Name'].str.contains('Miss') | data['Name'].str.contains('Mrs') | data['Name'].str.contains('Mr') | data['Name'].str.contains('Master'))]['Name']","d47a3c49":"data_name_title_2.value_counts()","41710461":"def name_encoding_1(data):\n    data['name_title_1'] = [np.nan for i in range(len(data))]\n    data_name_title_1 = data[data['Name'].str.contains('Miss') | data['Name'].str.contains('Mrs') | data['Name'].str.contains('Mr') | data['Name'].str.contains('Master')]['Name']\n    data_name_title_2 = data[~ (data['Name'].str.contains('Miss') | data['Name'].str.contains('Mrs') | data['Name'].str.contains('Mr') | data['Name'].str.contains('Master'))]['Name']    \n    for i in range(len(data)):\n        if data['Name'].iloc[i] in list(data_name_title_1):\n            data['name_title_1'].iloc[i] = 0\n        elif data['Name'].iloc[i] in list(data_name_title_2):\n            data['name_title_1'].iloc[i] = 1\n        \n    return data","915914be":"data = name_encoding_1(data)","531e7e40":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"name_title_1\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'name_title_1', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","38b07167":"data['name_length'] = data['Name'].map(lambda x: len(str(x)))","5b77aee5":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"name_length\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'name_length', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","93d41da5":"def name_length_categorical(data):\n    data['name_length_bin'] = pd.cut(data['name_length'], 3, precision = 0)\n    return data","6d5be36a":"data = name_length_categorical(data)\ndata['name_length_bin'].value_counts().keys()[1]","a5e18b84":"map_obj = {(data['name_length_bin'].value_counts().keys()[0].left, data['name_length_bin'].value_counts().keys()[0].right) : 0, (data['name_length_bin'].value_counts().keys()[1].left, data['name_length_bin'].value_counts().keys()[1].right): 1,(data['name_length_bin'].value_counts().keys()[2].left, data['name_length_bin'].value_counts().keys()[2].right):2}\ndata['name_length_bin'] = data['name_length_bin'].map(lambda x: map_obj[(x.left, x.right)])","db4eecf3":"from sklearn.feature_extraction.text import CountVectorizer\n'''\nvectorizer = CountVectorizer()\nX1 = vectorizer.fit_transform(data['Name'])\ndata_name_vectorized = pd.DataFrame(data['Name'].map(lambda x : X1.toarray()[list(data['Name']).index(x)]))'''","597e7f89":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_name = test['Name']\nname_list = pd.concat([test_name, data['Name']], axis = 0)","9fe6cf05":"#Reference : https:\/\/www.kaggle.com\/slythe\/infamous-titanic-80-accuracy\nvectorizer = CountVectorizer()\nname_v = vectorizer.fit(name_list)","b39e4e44":"def name_vectorizer(data):\n    if 'Name' in data.columns:\n        #data['name_vectorized'] = data['Name'].map(lambda x : X1.toarray()[list(data['Name']).index(x)].sum())\n        name_v = vectorizer.transform(data[\"Name\"])\n        name_vector_df = pd.DataFrame(data = name_v.todense(), columns = vectorizer.get_feature_names())\n        data = pd.concat([data,name_vector_df], axis =1 )\n    return data","d987ec6a":"#data = name_vectorizer(data)","921daf24":"data","e14d9d92":"'''sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"name_vectorized\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'name_vectorized', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)'''","2cede5a4":"data['Name'].value_counts()","182a79e1":"highest = {}\nfor letter in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]:\n    data_name_1 = data['Name'][data['Name'].str.startswith(letter)]\n    highest[letter] = len(data_name_1)","6f330463":"highest = sorted(highest.items(), key = lambda x:x[1], reverse = True)\nprint('Maximum occurence of letters in surname beginning :',highest[:5])","4bd88a1d":"def surname_encoding(data):\n    data['surname'] = [np.nan for i in range(len(data))]\n    \n    data_surname_1 = data['Name'][data['Name'].str.startswith(\"S\")]\n    data_surname_2 = data['Name'][data['Name'].str.startswith(\"M\")]\n    data_surname_3 = data['Name'][data['Name'].str.startswith(\"B\")]\n    data_surname_4 = data['Name'][data['Name'].str.startswith(\"C\")]\n    data_surname_5 = data['Name'][data['Name'].str.startswith(\"H\")]\n    data_surname_6 = data['Name'][~ (data['Name'].str.startswith(\"S\") | data['Name'].str.startswith(\"M\") | data['Name'].str.startswith(\"B\")| data['Name'].str.startswith(\"C\")| data['Name'].str.startswith(\"H\"))]\n    \n    for i in range(len(data)):\n        if data['Name'].iloc[i] in list(data_surname_1):\n            data['surname'].iloc[i] = 0\n        elif data['Name'].iloc[i] in list(data_surname_2):\n            data['surname'].iloc[i] = 1\n        elif data['Name'].iloc[i] in list(data_surname_3):\n            data['surname'].iloc[i] = 2\n        elif data['Name'].iloc[i] in list(data_surname_4):\n            data['surname'].iloc[i] = 3\n        elif data['Name'].iloc[i] in list(data_surname_5):\n            data['surname'].iloc[i] = 4\n        elif data['Name'].iloc[i] in list(data_surname_6):\n            data['surname'].iloc[i] = 5\n\n        \n    return data","55951e68":"data = surname_encoding(data)","5355569b":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"surname\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'surname', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","5906cae9":"data['Ticket'].value_counts()","519afafb":"data_ticket_1 = data[data['Ticket'].str.isdigit() == True]['Ticket']\nprint(data_ticket_1)","71f5fc85":"data_ticket_2 = data[data['Ticket'].str.isdigit() == False]['Ticket']\nprint(data_ticket_2.value_counts())","71d091ef":"data_ticket_2_1 = data_ticket_2[data_ticket_2.str.startswith(\"C\")]\nprint(data_ticket_2_1, len(data_ticket_2_1))","7e42361d":"data_ticket_2_2 = data_ticket_2[data_ticket_2.str.startswith(\"S\")]\nprint(data_ticket_2_2, len(data_ticket_2_2))","3af503d2":"data_ticket_2_3 = data_ticket_2[data_ticket_2.str.startswith(\"P\")]\nprint(data_ticket_2_3, len(data_ticket_2_3))","f1676d46":"data_ticket_2_4 = data_ticket_2[~ (data_ticket_2.str.startswith(\"C\") | data_ticket_2.str.startswith(\"S\") | data_ticket_2.str.startswith(\"P\"))]\nprint(data_ticket_2_4, len(data_ticket_2_4))","786992d2":"def ticket_bin_encoding(data):\n    data['ticket_bin'] = [np.nan for i in range(len(data))]\n    \n    data_ticket_1 = data[data['Ticket'].str.isdigit() == True]['Ticket']\n    data_ticket_2 = data[data['Ticket'].str.isdigit() == False]['Ticket']\n    #data_ticket_2_1 = data_ticket_2[data_ticket_2.str.startswith(\"C\")]\n    #data_ticket_2_2 = data_ticket_2[data_ticket_2.str.startswith(\"P\")]\n    #data_ticket_2_3 = data_ticket_2[data_ticket_2.str.startswith(\"S\")]\n    #data_ticket_2_4 = data_ticket_2[~ (data_ticket_2.str.startswith(\"C\") | data_ticket_2.str.startswith(\"S\") | data_ticket_2.str.startswith(\"P\"))]\n    \n    for i in range(len(data)):\n        if data['Ticket'].iloc[i] in list(data_ticket_1):\n            data['ticket_bin'].iloc[i] = 0\n        elif data['Ticket'].iloc[i] in list(data_ticket_2):\n            data['ticket_bin'].iloc[i] = 1\n        #elif data['Ticket'].iloc[i] in list(data_ticket_2_1):\n        #    data['ticket_bin'].iloc[i] = 1\n        #elif data['Ticket'].iloc[i] in list(data_ticket_2_2):\n        #    data['ticket_bin'].iloc[i] = 2\n        #elif data['Ticket'].iloc[i] in list(data_ticket_2_3):\n        #    data['ticket_bin'].iloc[i] = 3\n        #elif data['Ticket'].iloc[i] in list(data_ticket_2_4):\n        #    data['ticket_bin'].iloc[i] = 4\n        \n        \n    return data","a51725a8":"data = ticket_bin_encoding(data)","d5abc06e":"data['ticket_bin'].value_counts()","d5efac9f":"sns.set(rc = {'figure.figsize':(15,8)})\n\n#Calculate the counts of categorical variable Survived for different age bins\ndf_plot = data.groupby([ 'Survived', 'ticket_bin']).size().reset_index().pivot(columns='Survived', index='ticket_bin', values=0)\nprint(df_plot)\n\n#Plot the stacked bar plot\ndf_p = df_plot.plot(kind='bar', stacked=True)\n\n#Set labels\ndf_p.set_xlabel(\"ticket_bin\", fontsize = 20)\ndf_p.set_ylabel(\"Count\", fontsize = 20)\n\nclass_list = calculate_stacked_plot_percentages(df_plot, df_p)\n\nplot_stacked_plot_percentages(class_list, df_p, 0.2)","f37be174":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"ticket_bin\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'ticket_bin', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","bbcb92db":"data['Cabin'].value_counts()","fb0bbe74":"def cabin_encoding(data):\n    data['cabin_bin'] = [np.nan for i in range(len(data))]\n    \n    data_cabin_1 = data['Cabin'][data['Cabin'].str.startswith(\"A\")]\n    data_cabin_2 = data['Cabin'][data['Cabin'].str.startswith(\"B\")]\n    data_cabin_3 = data['Cabin'][data['Cabin'].str.startswith(\"C\")]\n    data_cabin_4 = data['Cabin'][data['Cabin'].str.startswith(\"D\")]\n    data_cabin_5 = data['Cabin'][data['Cabin'].str.startswith(\"E\")]\n    data_cabin_6 = data['Cabin'][~ (data['Cabin'].str.startswith(\"A\") | data['Cabin'].str.startswith(\"B\") | data['Cabin'].str.startswith(\"C\")| data['Cabin'].str.startswith(\"D\")| data['Cabin'].str.startswith(\"E\"))]\n    \n    for i in range(len(data)):\n        if data['Cabin'].iloc[i] in list(data_cabin_1):\n            data['cabin_bin'].iloc[i] = 0\n        elif data['Cabin'].iloc[i] in list(data_cabin_2):\n            data['cabin_bin'].iloc[i] = 1\n        elif data['Cabin'].iloc[i] in list(data_cabin_3):\n            data['cabin_bin'].iloc[i] = 2\n        elif data['Cabin'].iloc[i] in list(data_cabin_4):\n            data['cabin_bin'].iloc[i] = 3\n        elif data['Cabin'].iloc[i] in list(data_cabin_5):\n            data['cabin_bin'].iloc[i] = 4\n        elif data['Cabin'].iloc[i] in list(data_cabin_6):\n            data['cabin_bin'].iloc[i] = 5\n        elif data['Cabin'].iloc[i] == 'nan':\n            data['cabin_bin'].iloc[i] = 6\n\n    return data","efb6e0cf":"data = cabin_encoding(data)","37725646":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"cabin_bin\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'cabin_bin', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","17640e1c":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"cabin_nan\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'cabin_nan', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","5f07c51a":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"SibSp\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'SibSp', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","a7224db0":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"Parch\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'Parch', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","003284ed":"data['family'] = data['SibSp'] + data['Parch']","9e15f266":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"family\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'family', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","8ccd9adc":"sns.set(rc = {'figure.figsize':(15,8)})\n\n#Calculate the counts of categorical variable Survived for different age bins\ndf_plot = data.groupby([ 'Survived', 'Pclass']).size().reset_index().pivot(columns='Survived', index='Pclass', values=0)\nprint(df_plot)\n\n#Plot the stacked bar plot\ndf_p = df_plot.plot(kind='bar', stacked=True)\n\n#Set labels\ndf_p.set_xlabel(\"Pclass\", fontsize = 20)\ndf_p.set_ylabel(\"Count\", fontsize = 20)\n\nclass_list = calculate_stacked_plot_percentages(df_plot, df_p)\n\nplot_stacked_plot_percentages(class_list, df_p, 0.2)","8bede58b":"sns.set(rc = {'figure.figsize':(16,6)})\nplt.plot(figure_size = (1,2))\n\n#Boxplots\nplt.subplot(1,2,1)\nbp_ax = sns.boxplot(x = \"Survived\",y=\"Pclass\", data = data)\nbp_ax.set_title('Box plot', fontsize = 20)\n\n#Density plots\nplt.subplot(1,2,2)\ndp_ax = sns.kdeplot(x = 'Pclass', hue = 'Survived', data = data)\ndp_ax.set_title('Density plot', fontsize = 20)","1c4bf2fc":"data.columns","7f0f7a29":"def combine_categorical_vars(data, col1,col2):\n    #Get only the required columns from the dataframe\n    cols = [col1, col2]\n    one_hot_df = data.drop(columns = [col for col in data.columns if col not in cols])\n    \n    #Get the one hot encodings\n    one_hot_df = pd.get_dummies(one_hot_df, columns = one_hot_df.columns)\n    \n    #Get the unique values mapped to integers\n    arr = [int(\"\".join(str(i) for i in List),2) for List in np.array(one_hot_df)] #Convert binary values to int    \n    map_dict = {}\n    for index, j in enumerate(set(arr)): #Map each int value to index (0,1,..)\n        map_dict[j] = index\n    \n    arr = list(map(lambda x: map_dict[x], arr))\n    \n    #Get the final labels\n    data[col1+'_'+col2] = arr# np.argmax(np.array(one_hot_df), axis =1)\n    return data","cd88b2d8":"data = combine_categorical_vars(data, 'Pclass','fare_bin')\ndata = combine_categorical_vars(data, 'Pclass_fare_bin', 'cabin_bin')\ndata = combine_categorical_vars(data, 'Sex','name_title')\n\ndata","e726db72":"drop_list = ['Survived','PassengerId', 'Name', 'Ticket', 'Cabin', 'age_bin',  'SibSp', 'Parch','name_title','name_title_1', 'fare_bin', 'Pclass_fare_bin','name_length']\n#drop_list = ['Survived','PassengerId', 'Name', 'Ticket', 'Cabin', 'age_bin',  'SibSp', 'Parch']\n\nX = data.drop(columns =drop_list)\ny = data['Survived']\n\nprint(X.columns)","7fc3fb38":"cols_dummies = ['ticket_bin', 'surname', 'name_length_bin', 'Embarked','cabin_nan','Pclass']\n#cols_dummies = ['ticket_bin', 'surname','Pclass', 'Embarked', 'Pclass_fare_bin_cabin_bin', 'Sex_name_title']\nX = pd.get_dummies(X, columns = cols_dummies)","5cd8ab39":"categorical_features = ['Sex', 'pClass', 'Embarked' ]\nnumerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'family','name_length']\nfor col in categorical_features:\n    if col in X.columns:\n        try:\n            le = LabelEncoder()\n            X[col]= le.fit_transform(X[col])\n        except:\n            pass\n\nfor col in numerical_features:\n    le = MinMaxScaler()\n    if col in X.columns:\n        X[col]= le.fit_transform(np.array(X[col]).reshape(-1,1))","01fb102e":"from sklearn.ensemble import ExtraTreesClassifier\n\n# Building the model\nextra_tree_forest = ExtraTreesClassifier(n_estimators = 100)\n  \n# Training the model\nextra_tree_forest.fit(X,y)\n  \n# Computing the importance of each feature\nfeature_importance = extra_tree_forest.feature_importances_\n  \n# Normalizing the individual importances\nfeature_importance_normalized = np.std([tree.feature_importances_ for tree in \n                                        extra_tree_forest.estimators_],\n                                        axis = 0)","0f23e443":"ind = np.argsort(feature_importance_normalized)[::-1]   #indices sorted in descending order\nX_val = [X.columns[ind[i]] for i in range(0,7)]\ny_val = [feature_importance_normalized[ind[i]] for i in range(0,7)]","81d4c53e":"plt.figure(figsize = (10,10))\nplt.barh(X_val, y_val, color = ['b', 'g', 'r', 'c', 'm', 'y', 'k'])\nplt.xlabel('Feature Labels')\nplt.ylabel('Normalized Feature Importances')\nplt.title('Comparison of different Feature Importances')\nplt.show()","fa175534":"#X = X.drop(columns = [cols for cols in X.columns if cols not in X_val])","89316f00":"X.columns","53bed095":"#Train test stratified split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1)","cb0c700e":"sns.set(rc = {'figure.figsize':(15,15)})\nsns.heatmap(X.corr(),\n            #vmin=-1, vmax=1, center=0,\n            cmap=sns.diverging_palette(50, 500, n=500),\n            square=True,\n            annot=True)","5aaa3a1f":"X_train[:5]","e31852f8":"'''def categorical_encoding(X_train, X_test, col):\n    if col in X_train.columns:\n        le = LabelEncoder()\n        X_train[col]= le.fit_transform(X_train[col])\n        X_test[col]= le.transform(X_test[col])\n    \n    return X_train, X_test\n\n\ndef normalize(X_train, X_test, col):\n    if col in X_train.columns:\n        le = MinMaxScaler()\n        X_train[col]= le.fit_transform(np.array(X_train[col]).reshape(-1,1))\n        X_test[col]= le.transform(np.array(X_test[col]).reshape(-1,1))\n    \n    return X_train, X_test'''","66c76844":"'''categorical_features = ['Sex', 'pClass', 'Embarked' ]\nnumerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'family']\n\nfor col in categorical_features:\n    X_train, Xtest = categorical_encoding(X_train, X_test, col)\n\nfor col in numerical_features:\n    X_train, Xtest = normalize(X_train, X_test, col)\nprint(X_train[:5], X_test[:5])'''","2938b7d0":"X = data\ncategorical_features_ = ['Sex', 'pClass', 'Embarked' ]\nnumerical_features_ = ['Age', 'SibSp', 'Parch', 'Fare']\n\nfor col in categorical_features_:\n    if col in X.columns:\n        le = LabelEncoder()\n        X[col]= le.fit_transform(X[col])    \n\nfor col in numerical_features_:\n    if col in X.columns:\n        le = MinMaxScaler()\n        X[col]= le.fit_transform(np.array(X[col]).reshape(-1,1))","c5f222a2":"sns.set(rc = {'figure.figsize':(14,14)})\nsns.heatmap(X.corr(),\n            #vmin=-1, vmax=1, center=0,\n            cmap=sns.diverging_palette(50, 500, n=500),\n            square=True,\n            annot=True)","d85f5692":"import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\ndata_1000 = X_train\nlabels_1000 = y_train\n\nmodel = TSNE(n_components = 2, random_state = 0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations\n# for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n\n# creating a new data frame which\n# help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data = tsne_data,columns =(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsns.FacetGrid(tsne_df, hue =\"label\", size = 6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n\nplt.show()","a1f97384":"#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\nskf = KFold(n_splits=15, shuffle=True)\n\nfor train_idx, test_idx in skf.split(X_train, y_train):\n    #print(train_idx, test_idx)\n    #print(data.iloc[train_idx], y.iloc[train_idx])\n    rf = RandomForestClassifier(n_estimators = 500)\n    rf.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n    print('Accuracy for th fold CV is', accuracy_score(rf.predict(X_train.iloc[test_idx]), y_train.iloc[test_idx]))","f2ce4eac":"rf = RandomForestClassifier(n_estimators = 500)\nrf.fit(X_train, y_train)\nprint('Final accuracy score :',  accuracy_score(rf.predict(X_test), y_test))","2a6e6fd3":"#Reference : https:\/\/www.kaggle.com\/sivasaiyadav8143\/10-hyperparameter-optimization-frameworks#3.-Hyperopt ; https:\/\/www.kaggle.com\/maxdiazbattan\/titanic-top-3-eda-f-eng-avg-6-models-optuna\ncriterion_list = [\"gini\", \"entropy\"]\n\ndef hyperopt_train_test(params):\n    clf = RandomForestClassifier(**params)\n    return cross_val_score(clf, X_train, y_train).mean()\n\nspace = {\n    'max_depth': hp.choice('max_depth', range(1,20)),\n    'max_features': hp.choice('max_features', range(1,5)),\n    'n_estimators': hp.choice('n_estimators', range(1,1000)),\n    'criterion': hp.choice('criterion', criterion_list)\n            }\nbest = 0\ndef f(params):\n    global best\n    acc = hyperopt_train_test(params)\n    if acc > best:\n      best = acc\n      print( 'new best:', best, params)\n    return {'loss': -acc, 'status': STATUS_OK}\ntrials = Trials()\nbest = fmin(f, space, algo=tpe.suggest, max_evals=10, trials=trials)\nprint(best)","ebd5c3ee":"best","a2938704":"#rf = RandomForestClassifier(criterion = 'gini', max_depth= 10, n_estimators= 500)\nrf = RandomForestClassifier(criterion = criterion_list[best['criterion']], max_depth= best['max_depth'], n_estimators= best['n_estimators'])\nrf.fit(X_train, y_train)\nprint('Final accuracy score :',  accuracy_score(rf.predict(X_test), y_test))","daf14317":"from sklearn.linear_model import LogisticRegression","1bff2f82":"'''penalty_list = ['l1', 'l2', 'elasticnet', 'none']\n\ndef hyperopt_train_test(params):\n    clf = LogisticRegression(**params)\n    return cross_val_score(clf, X_train, y_train).mean()\n\nspace = {\n    'penalty': hp.choice('penalty', penalty_list),\n    'C': hp.choice('C', range(0,100))\n            }\nbest = 0\ndef f(params):\n    global best\n    acc = hyperopt_train_test(params)\n    if acc > best:\n      best = acc\n      print( 'new best:', best, params)\n    return {'loss': -acc ,'status': STATUS_OK}\ntrials = Trials()\nbest = fmin(f, space, algo=tpe.suggest, max_evals=300, trials=trials)\nprint(best)'''","251a8608":"'''best'''","f57c5bd7":"'''lr = LogisticRegression(C = best['C'], penalty = penalty_list[best['penalty']])\nlr.fit(X_train, y_train)\nprint('Final accuracy score :',  accuracy_score(lr.predict(X_test), y_test))'''","ce41823c":"!pip install pygam\nfrom pygam import LogisticGAM\nimport pygam","c9c426e5":"gam = LogisticGAM().fit(X_train, y_train)\ngam.summary()","16416622":"gam.accuracy(X_test, y_test)","f7396756":"fig, axs = plt.subplots(1, len(X_train.columns))\ntitles = X_train.columns\n\nfor i, ax in enumerate(axs):\n    XX = gam.generate_X_grid(term=i)\n    pdep, confi = gam.partial_dependence(term=i, width=.95)\n\n    ax.plot(XX[:, i], pdep)\n    ax.plot(XX[:, i], confi, c='r', ls='--')\n    ax.set_title(titles[i]);","e26ce169":"import lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","395d7d85":"criterion_list = [\"gini\", \"entropy\"]\n\ndef hyperopt_train_test(params):\n    clf = XGBClassifier(**params)\n    return cross_val_score(clf, X_train, y_train).mean()\n\nspace = {\n    'learning_rate' : hp.choice('learning_rate', [0.001,0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 0.6,1]),\n    'max_depth': hp.choice('max_depth', range(1,20)),\n    'max_features': hp.choice('max_features', range(1,5)),\n    'n_estimators': hp.choice('n_estimators', range(1,1000)),\n    'subsample' : hp.choice('subsample', [0,0.5,1])\n            }\nbest = 0\ndef f(params):\n    global best\n    acc = hyperopt_train_test(params)\n    if acc > best:\n      best = acc\n      print( 'new best:', best, params)\n    return {'loss': -acc, 'status': STATUS_OK}\ntrials = Trials()\nbest = fmin(f, space, algo=tpe.suggest, max_evals=10, trials=trials)\nprint(best)","6a57da8b":"best","e48be77a":"if best['subsample'] >=1:\n    best['subsample'] = 1\nxgb = XGBClassifier(subsample = best['subsample'], learning_rate = best['learning_rate'], max_depth= best['max_depth'], n_estimators= best['n_estimators'], objective= 'binary:logistic')\nxgb.fit(X_train, y_train)\nprint('Final accuracy score :',  accuracy_score(xgb.predict(X_test), y_test))","40eb8564":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor, VotingClassifier, VotingRegressor","2088ab94":"'''vc = VotingClassifier([(\"xgb_m\",xgb),\n                       (\"log\",lr),\n                       (\"rf_m\",rf)],\n                      voting = \"hard\")\n\nvc.fit(X_train,y_train)'''","121934b7":"'''print('Final accuracy score :',  accuracy_score(vc.predict(X_test), y_test))'''","f28e760b":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest","602bd586":"print(test.columns)","0f3b6752":"#categorical_features = ['Sex', 'pClass', 'Embarked' ]\n#numerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'family', 'name_length']\n\ndef clean_data(test):\n    for col in [test.columns[i] for i in [1,3,4,5,6,7,8,10]]:\n        if any(test[col].isna()) == True and col in ['Age', 'Fare']:\n            test = impute_missing_data(test, col, median =True, mode=False)\n\n                \n    return test\n\n\ndef preprocess(test):\n    test['Cabin'] = test['Cabin'].fillna('nan')\n    test['cabin_nan'] = [1 if test['Cabin'].iloc[i] =='nan' else 0 for i in range(len(test))]\n\n    test = ticket_bin_encoding(test)\n    test = name_encoding(test)\n    test['family'] = test['SibSp'] + test['Parch']\n    test['name_length'] = test['Name'].map(lambda x: len(str(x)))\n    #test = name_vectorizer(test)\n    test = cabin_encoding(test)\n    test = name_encoding_1(test)\n    test = surname_encoding(test)\n    test = fares_categorical(test)\n    map_obj = {(-1.0, 171.0) : 0, (171.0, 342.0): 1,(342.0, 512.0):2}\n    test['fare_bin'] = test['fare_bin'].map(lambda x: map_obj[(x.left, x.right)])\n    test = combine_categorical_vars(test, 'Pclass','fare_bin')\n    test = combine_categorical_vars(test, 'Pclass_fare_bin', 'cabin_bin')\n    test = combine_categorical_vars(test, 'Sex','name_title')\n    test = name_length_categorical(test)\n    map_obj = {(test['name_length_bin'].value_counts().keys()[0].left, test['name_length_bin'].value_counts().keys()[0].right) : 0, (test['name_length_bin'].value_counts().keys()[1].left, test['name_length_bin'].value_counts().keys()[1].right): 1,(test['name_length_bin'].value_counts().keys()[2].left, test['name_length_bin'].value_counts().keys()[2].right):2}\n    test['name_length_bin'] = test['name_length_bin'].map(lambda x: map_obj[(x.left, x.right)])\n\n    drop_list = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'ticket_bin', 'cabin_nan', 'Fare']\n    drop_list = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch','name_title','name_title_1', 'fare_bin', 'Pclass_fare_bin','name_length']\n    \n    test = pd.get_dummies(test, columns = cols_dummies)\n\n    test = test.drop(columns = drop_list)##Incl cabin\n    \n    for col in categorical_features:\n        if col in test.columns:\n            try:\n                le = LabelEncoder()\n                test[col]= le.fit_transform(test[col])\n            except:\n                pass\n        \n    for col in numerical_features:\n        le = MinMaxScaler()\n        if col in test.columns:\n            test[col]= le.fit_transform(np.array(test[col]).reshape(-1,1))\n        \n                \n    return test","e2e76ecc":"test = clean_data(test)\n\nfor col in [test.columns[i] for i in [1,3,4,5,6,7,8,10]]:\n    print(col,':', any(test[col].isna()))\n    \ntest = preprocess(test)","864e7cc1":"test","78ee6ae2":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission","5377c588":"pred = gam.predict(test)\ngam_pred = [1 if i == True else 0 for i in pred ]\n\n'''rf_pred = rf.predict(test)\n\nlr_pred = lr.predict(test)\n\nxgb_pred =xgb.predict(test)\n\nvc_pred = vc.predict(test)'''","13e95066":"'''final_pred = [1 if (i + 2*j+k)\/4 >=0.5 else 0 for i,j,k in zip(gam_pred, rf_pred, lr_pred)]'''","01c1e4dc":"submission['Survived'] = rf.predict(test)#final_pred#vc_pred#final_pred #gam_pred #rf.predict(test) ","7bba7e10":"submission","7e7d37d5":"submission.to_csv('.\/submission.csv', index = False)","597d12e4":"sns.heatmap(test.corr(),\n            #vmin=-1, vmax=1, center=0,\n            cmap=sns.diverging_palette(50, 500, n=500),\n            square=True)","f0754bdc":"plt.bar(dict(test.corrwith(submission[\"Survived\"])).keys(), dict(test.corrwith(submission[\"Survived\"])).values())","58c4d960":"- There are a total of 681 different ticket types.\n- For starters, lets try to form categories based on the the presence of a digit in the string","563a28d5":"- From the above stacked plots, it is clear **more percentage of female passengers survived** i.e. 74.2 % while only 18.9 % of male passengers survived, even though more male passesngers were aboard","8b531ee8":"## Age ","02edb711":"- Clearly, people who had more family aboard had a higher chance of survival","e716baa1":"# Data preprocessing","f9efa181":"About the features : \n- **PassengerId** : Id of each passsenger (in int)\n- **Survived** : If survived or no (0 = No, 1 = Yes)\n- **Pclass** : Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n- **Name** : name (string)\n- **Sex** : sex (string : 'male' 'female')\n- **Age** : age in years\n- **SibSp** : # of siblings \/ spouses aboard the Titanic\n- **Parch** : # of parents \/ children aboard the Titanic\n- **Ticket** : Ticket number (string)\n- **Fare** : Passenger fare\n- **Cabin** : Cabin number (string)\n- **Embarked** : Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) ","ab07d0be":"### Name title (without m\/f)","2ac5f809":"## Embarked","0af098b3":"- Around 500 passengers belonged to Class 3 which is the majority of the passengers and 75 % of them did not survive\n- Class 1 and Class 2 had around half of the total Class 1 count but higher percentage of survivors\n- Clearly, **percentage of Class 1 and Class 2 survivors is higher (63 % and 47 %) than the percentage of Class 3 survivors (24 %)**","7c93db6d":"- name_title and Sex\n- combining fare, cabin_bin and Pclass","ec13271b":"## Hyperparameter optimization using hyperopt","9abe36ec":"# Missing values?","37b3a0a4":"For baseline model to compare further models with, i used a RandomForest model since it is a simple, robust and high performing classification algorithm ","ff7f148c":"- Distribution of people who didn't survive is slightly right skewed, or the mean of the people who did not survive > median of the distribution which is around ~27, i.e. **mean age of the people who did not survive is higher than 27**\n- People of ages > ~67 are considered outliers in the disb of people who did not survive. owing to the fact **people above the age of ~67 are very less** and people of age ~60 above are outliers in the disb of people who did survive, i.e. **very few people above 60 actually did survive**","e4d2dd0a":"# Baseline Model","717e2342":"# Test predictions","e30a1f65":"## Correlation with target variables","37dc50e0":"- There are 177 Nan values in the Age column\n\n- Now, we impute the missing values with a central tendency measure (median performed the best in this case)","9212dbac":"# Results:\n\n- Submission #1 : Scored 0.57: Used only features Pclass  Sex       Age     SibSp  Parch      Fare  Embarked, and basic normalization + categorical encoding + RF with 500 estimators\n- Submission #2: Scored 0.67 after dropping nan valued features while training + imputing test data with mean values\n- Submission #3: Scored 0.75 after imputing nan values with the median of the feature\n- Submission #4: Logistic GAM instead of RF improved score slightly (0.76)\n- Submission #5: Scored 0.77 with RF(with optimized hyperparams) (Score improved to 0.772 w results of RF + twice weighted results of GAM)\n- Submission #6: Scored 0.775 with feature ticket bin encoding and name title together inc accuracy to 0.775 +weighted GAM and RF prediction ~top 75% in Kaggle. Hmmm, lets see what else we can try.\n- Added cabin_nan, accuracy dec\n- => Features : [ Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, name_title] +RF (optimized) =>0.7799\n- => Features : [Pclass  Sex     Age  Embarked  name_title  family] +RF (optimized) => 0.78468\n- => Features : [Pclass  Sex     Age  Embarked  family] +RF (optimized) => 0.78708","e2a0ff05":"# EDA","cc6ca74f":"Three visualizations :\n1. Boxplots : To tell us about the distribution of the feature for each output class and give us an idea about the outliers in the data\n2. Density plots : To tell us about the distribution and more about the shape of the distribution (resembling normal or any other disb) of the feature\n3. Stacked countplots : Tells us more about count of each category of the feature with information about the percentage belonging to which output class","3f979f82":"## Age of the people who survived?","38fd0a8e":"- 'Age', 'Cabin' and 'Embarked' have Nan values. Lets check them further..","86b2665e":"- We have 825 rows now that 66 rows containing age outliers were removed","6ec00971":"## Correlation between features and target variable","69068d95":"### Name vectorizer","3e6e43fe":"- There are 661 tickets that contain only numbers","5ec83efc":"## Ticket class of the surivors ? (Pclass)","cef1de24":"## Sex of the surviors ?","087bfe02":"- In the above stacked plots, the age has been binned into 3 age bins, Bin1 or the younger people in age group 0-27 years, Bin 2 or the iddle aged in age group 27-53 years, and Bin3 or the older people in age group 53 -80 years\n- Majority of people fall in age bin Bin 1 [0-27], and Bin 2 [27 - 53] while a minority fall in Bin3 [53-80]\n- The percentage of people who survived is **slightly lower in Bin 1 and Bin 2** (40.8 and 37.5 % respectively), while the **percentage of people who survived is much lower in Bin 3 (32%)**","5b585781":"## Port embarked of survivors?","e2c690e9":"- name_title is highly negatively correlated with the output class\n- Fare and Pclass are also highly negatively correlated\n- Pclass and the output class are moderately negatively correlated","27ef05b8":"- Distribution of fares of survivors clearly has a higher median value, hence **passengers who survived had higher average fares**\n- Lots of outliers in the distribution of both\n- A huge spike in the pdf of the people who didn't survive, or the probability that the person who didn't survive had a fare of ~10 units of currency is very high","700acfaa":"# Model 3: Ensembles","ca660468":"- There are 167 tickets that contain a string plus numbers. For these, we categorise them based on the letter the ticket name starts with, namely 'C', 'P' and 'S' since these are the most common","3a360a92":"- Since 'Cabin' has around 529 nan values which is more than 50 percent of the total data, we drop this feature","80c6a0c5":"Note : Countplots vs box plots","e4b4ef11":"## Letter of surname","137339fd":"## Name of the survivors","12a40b98":"- **Data is clearly imbalanced**, the count of people who survived **is almost close to half** the count of people who didn't survive ","edea7407":"## Cabin","be216b09":"## Tickets of survivors?","a2a12c53":"## Correlation w target variable","dd9040f0":"### Name title","ba9d8eaf":"## Fares of the survivors?","46c8441e":"# About the dataset :","275ab427":"## Is the data imbalanced?","ecf448b1":"# Model 1: Logistic Regression, GLM","07104bf1":"# Voting Classifier","f4d343b4":"## Cabin of the survivors","c3a72740":"- Only two nan values, we impute them with the maximum occuring category for the feature or the mode","2c4acdfe":"### Name length","60d6a2c3":"# Model 2 : Generative Additive Models","d915deec":"### Converting fares into categorical variables","5030eaa5":"### Visualizing","ea6af829":"## Number of family members of the survivors ? (SibSp, Parch)"}}