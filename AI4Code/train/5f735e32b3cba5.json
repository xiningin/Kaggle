{"cell_type":{"ba0e3ced":"code","33c9eb62":"code","302664c8":"code","6576fd87":"code","2da64110":"code","bba557dd":"code","e2be4e14":"code","44798787":"code","09a81ae5":"code","842741a6":"code","1a46e77a":"code","da75a2ca":"code","adf52ea7":"code","bed247c4":"code","aa7c707e":"code","12d0b943":"code","e2e0efa1":"code","f294cf89":"code","2848f9eb":"code","d61435ad":"code","aaeb2a1c":"code","d1d95872":"code","1f9c232f":"code","d43befe8":"code","e24e0de5":"code","48fbbacb":"code","40b89c11":"code","7ea82ff1":"markdown","fbdcb8ea":"markdown","dcf0273f":"markdown","df5d3de6":"markdown","784dd511":"markdown","98ebd779":"markdown","3d798cec":"markdown","89c032c8":"markdown","cbc795c1":"markdown","7e0bdffa":"markdown","ee3ecbb2":"markdown","53f42518":"markdown","d31acccf":"markdown","307cf9c5":"markdown","e3bea7b7":"markdown","f85156a4":"markdown","4c77ef10":"markdown","7a4e662a":"markdown","f095ce4a":"markdown","8018c2c1":"markdown"},"source":{"ba0e3ced":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nsns.set_color_codes(\"pastel\")\n%matplotlib inline","33c9eb62":"import os\nos.listdir('..\/input')","302664c8":"# I'm use only student-mat.csv\ndata = pd.read_csv('..\/input\/student-mat.csv')","6576fd87":"print(\"G3 range: Min={}, Max={}\".format(data[\"G3\"].min(), data[\"G3\"].max()))\ndata.head(5)\n","2da64110":"def create_g3_class(data):\n    return [\"Fail\", \"Medium\", \"Good\"][0 if data[\"G3\"] <= 5 else 1 if data[\"G3\"] <= 15 else 2]\n\ndata[\"G3_class\"] = data.apply(lambda row: create_g3_class(row), axis=1)\ndata.head()\ndata.info()","bba557dd":"from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\n\nfrom sklearn.model_selection import cross_val_score","e2be4e14":"y = data['G3_class']\nX = data.drop(['G3', 'G3_class'], axis=1)","44798787":"X = pd.get_dummies(X)","09a81ae5":"names = ['RandomForestClassifier', 'NaiveBayes' , 'DecisionTreeClassifier', 'XGBClassifier']\n\nclf_list = [RandomForestClassifier(),\n            MultinomialNB(),\n            DecisionTreeClassifier(),\n           xgb.XGBClassifier()]","842741a6":"clf_scores = {}\nfor name, clf in zip(names, clf_list):\n    clf_scores[name]= cross_val_score(clf, X, y, cv=5).mean()\n    print(name, end=': ')\n    print(clf_scores[name])","1a46e77a":"best_classifier = sorted(clf_scores, key=clf_scores.get, reverse=True)[0]\nbest_classifier","da75a2ca":"clf = clf_list[names.index(best_classifier)]\nclf.fit(X, y)","adf52ea7":"importances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X.shape[1]):\n    if(importances[indices[f]] >= 0.01):\n        print(\"%d. Feature %s (%f)\" % (f + 1, X.columns.values[indices[f]], importances[indices[f]]))","bed247c4":"X = data.drop(['G3', 'G2', 'G1', 'G3_class'], axis=1)","aa7c707e":"X = pd.get_dummies(X)","12d0b943":"clf_scores = {}\nfor name, clf in zip(names, clf_list):\n    clf_scores[name]= cross_val_score(clf, X, y, cv=5).mean()\n    print(name, end=': ')\n    print(clf_scores[name])","e2e0efa1":"best_classifier = sorted(clf_scores, key=clf_scores.get, reverse=True)[0]\nbest_classifier","f294cf89":"clf = clf_list[names.index(best_classifier)]\nclf.fit(X, y)\n\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X.shape[1]):\n    if(importances[indices[f]] >= 0.01):\n        print(\"%d. Feature %s (%f)\" % (f + 1, X.columns.values[indices[f]], importances[indices[f]]))","2848f9eb":"from sklearn.model_selection import train_test_split\nX = data.drop(['G3_class'], axis=1)\nX = pd.get_dummies(X)     #Convert to categorical\ny = data['G3_class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, stratify=y)\nX_train= X_train.drop(['G3'], axis=1)\nimport copy\nX_test_withG3 = copy.deepcopy(X_test)    #will be used in end to display actual G3 score\nX_test= X_test.drop(['G3'], axis=1)","d61435ad":"import itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","aaeb2a1c":"clf = clf_list[names.index(best_classifier)]\nprint(\"using classifer: %s\"%best_classifier)\n# clf = xgb.XGBClassifier()\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nnp.set_printoptions(precision=2)\n# print(clf.classes_)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","d1d95872":"plt.figure()\nplt.boxplot(data['G3'], notch=True, sym='gD', vert=False)\nplt.title('G3 (final grade) score distribution in dataset')\nplt.show()","1f9c232f":"p = sns.countplot(data['G3_class']).set_title('G3 class distribution')","d43befe8":"from imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom imblearn.under_sampling import CondensedNearestNeighbour, AllKNN, OneSidedSelection, RandomUnderSampler\nfrom imblearn.ensemble import BalanceCascade, EasyEnsemble\n\nfrom collections import Counter\n# X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)\nX_resampled, y_resampled = AllKNN(sampling_strategy=['Medium']).fit_resample(X_train, y_train)\n# X_resampled, y_resampled = SMOTETomek().fit_resample(X_train, y_train)  #sampling_strategy='minority'\n# X_resampled, y_resampled = EasyEnsemble().fit_resample(X_train, y_train)\n# X_resampled = X_resampled[0] ; y_resampled = y_resampled[0]\nprint(sorted(Counter(y_resampled).items()))","e24e0de5":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(X_resampled) #pd.get_dummies(data)\nprincipalDf_train = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\nprincipalComponentstest = pca.fit_transform(X_test) #pd.get_dummies(data)\nprincipalDf_test = pd.DataFrame(data = principalComponentstest\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n","48fbbacb":"clf = clf_list[names.index(best_classifier)]\nprint(\"Using classifier: %s\"%best_classifier)\n# clf = xgb.XGBClassifier()\n# clf = DecisionTreeClassifier()  #Using for demo and consistency\nclf.fit(principalDf_train, y_resampled)\n\ny_pred = clf.predict(principalDf_test)\ny_pred_prob = clf.predict_proba(principalDf_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nnp.set_printoptions(precision=2)\n# print(clf.classes_)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","40b89c11":"y_test_list = list(y_test)\nX_test_withG3_list = list(X_test_withG3['G3'])\nfor idx, item in enumerate(y_pred):\n    if(item == 'Fail'):\n        print(\"Student {} \\t [Actual Failed?: {}  \\tG3: {}]\".format(idx, y_test_list[idx], X_test_withG3_list[idx]))","7ea82ff1":"If test resuts are ignored then students should also be told to focus on their - \n\n1. Attendance\n2. Time spent on Outing \n3. Student with less and more age should focus more on studies\n4. number of past class failures \n5. Freetime after school\n6. Alcohol consumption\n7. studytime\n8. Maintenance of health\n9. Student with less educated mothers shoud focus more on studies","fbdcb8ea":"We can see the top selection criteria is results achieved in last tests done by students.  \nOther than test resuts, students should also be told to focus on their - \n* Attendance\n* number of past class failures ","dcf0273f":"## With G1 and G2 test results features","df5d3de6":"To counter this issue we need to oversample the data in classes which are having less samples.   \nimblearn package is used to do this.","784dd511":"**Creating a new column G3_class with values - Fail, Medium and Good**  \nIf student final grade is below 5 then fail, above 15 then Good else in Medium category\n\n*In last print the new column - Scroll horizonatally in last to see the new values*","98ebd779":"# Data Overview","3d798cec":"Apply PCA to find out most relevant features","89c032c8":"## Feature Importances","cbc795c1":"# Final Grade Prediction","7e0bdffa":"# Few points to note and justification\n## * Undersampling is used on Fail class students as these are the students which should be focused\n## * The data is not accurate thereby resulting some incorrect prediction in above results\n## * **The results above contain failed students and a lot of students with Medium G3 scores also. The choice was to reduce the Medium G3 scorers and miss some Failed students too.  Important point is no\/minimum Failed students are missed (As seen in last confusion matrix), So I resampled focusing on Failed classes.     It is important to focus on NOT MISSING ANY FAILING STUDENT, and focusing on Medium students will make them only better.**","ee3ecbb2":"**Stritified Split the data in training, test **","53f42518":"Much better prediction results after over and under sampling   \n# Let us show the students which are in danger zone","d31acccf":"Output target of this dataset is **Final Grade**. Let's use some regression model to predict it. I'll limit myself to 4 simple regression models (without searching the best parameters): decision tree regression, linear regression, lasso and ridge regression.","307cf9c5":"Print main features contributing to top 1% for selection","e3bea7b7":"# In this kernel I am going to classify the data in 3 categories **Fail, Medium and Good**  \nIf student final grade (0-20) is below 5 then fail, above 15 then Good else in Medium category\n\n## And then will predict which students are lying in which category and should be more focused. Will find out students which may fail so that attention can be paid to those failing students","f85156a4":"G3 output class is unevenly distributed. Above box plot shows that most values are in range approx 8-13.    \nBelow count plot also shows uneven distribution.  ","4c77ef10":"## Without G1 and G2 features","7a4e662a":"Let's look at scores of models, trained without G1 and G2 features (Test results).","f095ce4a":"let's look at feature importances.","8018c2c1":"**As we can see in above confusion matrix there are a lot of wrong predictions (above and below diagnal).**  \n*Most probable reason of this is bias-ness in input data.* Let us verify this below"}}