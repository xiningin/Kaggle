{"cell_type":{"14ecaa8b":"code","c0ee9acc":"code","6594a32c":"code","1078b6df":"code","7907a2a3":"code","1f4f2b6b":"code","1a1cce64":"code","d8602399":"code","de7917da":"code","37315868":"markdown","5b891615":"markdown","666b8d9c":"markdown","93fe7e7a":"markdown","d29b8f15":"markdown","ef39ab6f":"markdown","fe569c8f":"markdown"},"source":{"14ecaa8b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#To develop the KNN classifier from scratch\nfrom collections import Counter\n\n#To evaluate the performance of the model\nimport numpy as np\nfrom numpy import genfromtxt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier \nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline","c0ee9acc":"# Minkowski distance formula for classifier\ndef minkowski(x,y,p):\n    inner=[]\n    for i in range(len(x)):\n        inner.append(pow((abs(x[i]-y[i])),p))\n    dist=pow(sum(inner),1\/p)\n    return dist","6594a32c":"class KNN_classifier: \n    def __init__(self,k=5,p=2):\n        self.k = k\n        self.p = p\n        \n    #List to hold x_train and y_train together   \n    def labeled_data(self, x_train, y_train):\n        self.labeled_data=list(zip(x_train,y_train))        \n        \n    #Prediction method    \n    def predictor(self, x_test):        \n        predictions=[]\n        for point in x_test:\n            distances=[]\n            for line in self.labeled_data:                \n                distances.append([(minkowski(point,line[0],self.p)),line[-1]])\n                \n            distances.sort(key=lambda x: x[0])\n            classes=list(map(lambda x: x[1], distances[:self.k]))\n            predictions.append(Counter(classes).most_common()[0][0])\n               \n        return predictions ","1078b6df":"data_array = genfromtxt('\/kaggle\/input\/seed-from-uci\/Seed_Data.csv', delimiter=',',skip_header=1)","7907a2a3":"features=data_array[:,:7]\nlabel=data_array[:,7:8].reshape(-1)\nX_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=0, stratify=label)","1f4f2b6b":"#My KNN classifier\nstart = time.time()\nmy_accuracies=[]\nmy_confusion_matrices=[]\nmy_times=[]\n#k parameter tuning\nfor i in range(1,20):\n    inner_start = time.time()\n    \n    #model\n    knn = KNN_classifier(k=i)\n    knn.labeled_data(X_train,y_train)\n    predicted=knn.predictor(X_test)\n    \n    #indicators\n    my_accuracies.append([(accuracy_score(y_test, predicted)),i])\n    my_confusion_matrices.append([(confusion_matrix(y_test, predicted)),i])\n    inner_end = time.time()\n    my_times.append([(inner_end-inner_start),i])\n    \nend = time.time()\nprint(\"Total Time:\",end - start)\n\nsorted_my_accuracies=sorted(my_accuracies,key=lambda x: x[0], reverse=True)  \nprint(\"Top 5 Accuracies:\",my_accuracies[:5])","1a1cce64":"#sklearn's KNN classifier\nstart = time.time()\nsklearn_accuracies=[]\nsklearn_confusion_matrices=[]\nsklearn_times=[]\n#k parameter tuning\nfor i in range(1,20):\n    inner_start = time.time()\n    \n    #model\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    \n    #indicators\n    sklearn_accuracies.append([(accuracy_score(y_test, y_pred)),i])\n    sklearn_confusion_matrices.append([(confusion_matrix(y_test, y_pred)),i])\n    inner_end = time.time()\n    sklearn_times.append([(inner_end-inner_start),i])\nend = time.time()\nprint(\"Total Time:\",end - start)    \n\nsorted_sklearn_accuracies=sorted(sklearn_accuracies,key=lambda x: x[0], reverse=True)  \nprint(\"Top 5 Accuracies:\",sorted_sklearn_accuracies[:5])","d8602399":"x=range(1,20)\nplt.plot(x, list(map(lambda x: x[0], my_accuracies)),marker='o', label = \"My KNN\")\nplt.plot(x, list(map(lambda x: x[0], sklearn_accuracies)), marker='o',label = \"Sklearn KNN\")\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracies\")\nplt.title('Accuracy Comparison of KNN Classifiers for Different K Values')\nplt.legend()\nplt.show()","de7917da":"print(\"My results for k=2\")\nprint(\"------------------\")\nprint(\"Accuracy:\", my_accuracies[1][0] )\nprint(\"Time:\",my_times[1][0])\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, predicted))\nprint(\"****************************************\")\nprint(\"Sklearn results for k=2\")\nprint(\"------------------\")\nprint(\"Accuracy:\", sklearn_accuracies[1][0] )\nprint(\"Time:\",sklearn_times[1][0])\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))","37315868":"# Developing the KNN classifier from scratch","5b891615":"# Testing the performance of the model","666b8d9c":"When we compare the models for k = 2, which is one of the small k values that gives the highest accuracy in both classifiers, it is seen that the confusion matrices are the same. The most significant difference appears to be time for the k = 2 value. Sklearn's KNN arranges the data for finding the closest neighbors efficiently during the prediction phase. Since my model does not do such a process, the time difference is an expected result and this part can be improved.","93fe7e7a":"In the seed data set, the first 7 columns hold the features and the last column holds the labels. We have a total of 210 data belonging to 3 classes, without any null-values.","d29b8f15":"My model appears to give lower accuracy for k = 10 and k = 15 compared to the Sklearn model. However, the same accuracy result was obtained for k values other than that.","ef39ab6f":"Let's evaluate the performance of KNN classifiers, both developed by me and available on Sklearn for different k values.","fe569c8f":"In this notebook, I attempted to develop a KNN classifier from scratch. It is an open-to-develop model, so I would be glad to hear your suggestions."}}