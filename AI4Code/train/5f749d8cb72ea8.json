{"cell_type":{"4e743d52":"code","6c82cfda":"code","97936cf3":"code","c48d3130":"code","f436713a":"code","c15c00aa":"code","56063472":"code","a0a0ebc1":"code","8d677bc8":"code","fd0efb05":"code","4b71f789":"code","8b4c47a6":"code","fbf824f6":"code","267fee40":"markdown","1454a90d":"markdown","abfd904d":"markdown","b6ffe302":"markdown","29c729c0":"markdown","27a75fd0":"markdown","2fa98062":"markdown","54c40ede":"markdown","d2b7ca64":"markdown","00665700":"markdown","bbbefe46":"markdown","83a794ed":"markdown","75d39e14":"markdown","1036a731":"markdown","41e361fc":"markdown","908664e1":"markdown","70f4110c":"markdown","be6f8cbb":"markdown","833dece9":"markdown"},"source":{"4e743d52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c82cfda":"import cv2\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import transforms\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image","97936cf3":"imdir = '\/kaggle\/input\/fruit-images-for-object-detection\/train_zip\/train\/'\nfilenames = os.listdir(imdir)\nfilenames = list(s for s in filenames if 'jpg' in s)\nlen(filenames)","c48d3130":"image_size = 506\nnum_classes = 3\n\nclass CatsDogsDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, filenames, transform = None):\n        self.filenames = filenames\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.filenames)\n    \n    def __getitem__(self, idx):\n        filename = imdir + self.filenames[idx]\n        image = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            image = self.transform(image)\n        label = 1 if 'apple' in filename else 2 if 'banana' in filename else 0\n        return image, np.array(label)\n    \ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n      mean=[0.485, 0.456, 0.406],\n      std=[0.229, 0.224, 0.225]\n    )\n])    \ndataset = CatsDogsDataset(filenames, transform)\ndataloader = torch.utils.data.DataLoader(dataset,\n                                        batch_size = 32,\n                                        shuffle = True)","f436713a":"class BaseModel(torch.nn.Module):\n    \n    def get_cams(self, input):\n        features = self.get_feature_map(input)\n        cams = torch.einsum('nmij, km -> nkij', features, self.fc.weight)\n        return cams\n    \n    def forward(self, input):\n        features = self.get_feature_map(input)\n        out = self.avg(features)\n        shape = features.shape\n        out = out.reshape(shape[0], shape[1])\n        return self.fc(out)","c15c00aa":"class VanillaCNN(BaseModel):\n    \n    def __init__(self, in_channels, num_classes):\n        super(VanillaCNN, self).__init__()\n        self.conv11 = torch.nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3)\n        self.conv12 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3)\n        self.pool1 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n\n        self.conv21 = torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n        self.conv22 = torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n        self.pool2 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        self.conv31 = torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)\n        self.pool3 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        self.conv41 = torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3)\n        \n        \n        self.avg = torch.nn.AvgPool2d(kernel_size = 55)\n        self.fc = torch.nn.Linear(256, num_classes)\n        \n    def get_feature_map(self, input):\n        out = self.conv11(input)\n        out = self.conv12(out)\n        out = self.pool1(out)\n        out = torch.nn.ReLU()(out)\n        \n        out = self.conv21(out)\n        out = self.conv22(out)\n        out = self.pool2(out)\n        out = torch.nn.ReLU()(out)\n        \n        out = self.conv31(out)\n        #out = self.conv32(out)\n        out = self.pool3(out)\n        out = torch.nn.ReLU()(out)\n        \n        out = self.conv41(out)\n        #out = self.conv42(out)\n        out = torch.nn.ReLU()(out)\n        \n        return out","56063472":"class DilatedCNN(BaseModel):\n    \n    def __init__(self, in_channels, num_classes):\n        super(DilatedCNN, self).__init__()\n        self.conv11 = torch.nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3)\n        self.conv12 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3)\n        self.pool1 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n\n        self.conv21 = torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n        self.conv22 = torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n        \n        self.conv31 = torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, dilation = 2, padding = 2)\n        \n        self.conv41 = torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, dilation = 2, padding = 2)\n        \n        \n        self.avg = torch.nn.AvgPool2d(kernel_size = 247)\n        self.fc = torch.nn.Linear(256, num_classes)\n        \n    def get_feature_map(self, input):\n        out = self.conv11(input)\n        out = self.conv12(out)\n        out = self.pool1(out)\n        out = torch.nn.ReLU()(out)\n        \n        out = self.conv21(out)\n        out = self.conv22(out)\n        out = torch.nn.ReLU()(out)\n        \n        out = self.conv31(out)\n        out = torch.nn.ReLU()(out)\n        \n        out = self.conv41(out)\n        out = torch.nn.ReLU()(out)\n        \n        return out","a0a0ebc1":"cnn = VanillaCNN(in_channels = 3, num_classes = num_classes)\ndilatedCnn = DilatedCNN(in_channels = 3, num_classes = num_classes)","8d677bc8":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nepochs = 20\n\ncnn.to(device)\ndilatedCnn.to(device)","fd0efb05":"def train_model(model, epochs, lr = 0.001):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in range(epochs):\n        for idx, (data, label) in enumerate(dataloader):\n            optimizer.zero_grad()\n            data = data.to(device).float()\n            label = label.to(device).float()\n            pred = model(data)\n            loss = criterion(pred, label.long())\n            loss.backward()\n            optimizer.step()\n            if idx % 10 == 0:\n                y_hat = torch.argmax(pred, dim = 1)\n                correct = (y_hat == label).sum()\n                print(f\"Epoch {epoch} Loss = {loss.data:.03f}, acc = {correct \/ label.shape[0]:.02f}\")","4b71f789":"train_model(cnn, epochs)","8b4c47a6":"train_model(dilatedCnn, epochs)","fbf824f6":"cnn.cpu()\ndilatedCnn.cpu()\n\nn = 7\nfig, axs = plt.subplots(n, 3, figsize=(15,25))\n\nfor i in range(n):\n    im = cv2.cvtColor(cv2.imread(imdir + filenames[i]), cv2.COLOR_BGR2RGB)\n    axs[i, 0].imshow(im)\n    axs[i, 0].set_title('Original image')\n    \n    classIdx = 1 if 'apple' in filename else 2 if 'banana' in filename else 0\n    im = transform(im).reshape(1,3, 506, 506)\n    cnnCams = cnn.get_cams(im).detach().numpy()\n    dilatedCnnCams = dilatedCnn.get_cams(im).detach().numpy()\n    \n    axs[i, 1].imshow(cnnCams[0][classIdx])\n    axs[i, 1].set_title('CNN CAM')\n    axs[i, 2].imshow(dilatedCnnCams[0][classIdx])\n    axs[i, 2].set_title('Dilated CNN CAM')\n\nplt.show()","267fee40":"In this kernels we will be learning about a **CAM - class activation map**, a concept, which was introduced in paper [Learning Deep Features for Discriminative Localization\n](http:\/\/https:\/\/arxiv.org\/abs\/1512.04150) CAMs are used to visualize features which CNN extracts from images. It is also widely used different segmentation tasks, when we don't have ground-truth pixelwise labels and we need to detect some important regions from objects of different classes.","1454a90d":"The main problem of using vanilla CNNs for calculating CAM is that the sequence of convolution and max-pooling layers reduces the resolution of the received CAM. <\/br>\nOne solution to this problem has been suggested in paper [DeepLab: Semantic Image Segmentation with\nDeep Convolutional Nets, Atrous Convolution,\nand Fully Connected CRFs\n](https:\/\/arxiv.org\/pdf\/1606.00915.pdf)","abfd904d":"![CAM-min.PNG](attachment:144cb48d-d870-4d97-8c5b-2a7b86b4a5eb.PNG)","b6ffe302":"<h2>Class activation map<\/h2>","29c729c0":"![atrous.PNG](attachment:f31bf8c9-ba1e-4f85-a7db-d1aa50ee7c8b.PNG)","27a75fd0":"<h2>Atrous convolution<\/h2>","2fa98062":"<h1>Introduction<\/h1>","54c40ede":"After we learned the model for predicting a class, producing of a CAM for an image consists of two steps:\n1. Calculate outputs of the last convolution layer. Thus we get $n$ feature maps where $n$ - the number of filters in the convolution layer.\n2. Each feature map we need to multiply by the weight of the corresponding connection between the GAP and the last fully connected layers and finally sum up all the received feature maps for the particular class.","d2b7ca64":"Dilated convolution differs from the usual one in that it's kernel's elements are located at some distance from each other. This distance is determined by the dilation factor. Dilation = 1 means the usual convolution. Convolution with kernel size = 3 and dilation = 2 is shown on the picture below.","00665700":"Lets try to produce CAMs using different approaches and compare results.","bbbefe46":"As you can see, CAMs generated by the dilated CNN have much more higher resolution and precision ","83a794ed":"As was sad, CAM shows discriminative regions for the particular class, which are most important for the classification task.<\/br>\nTo produce CAM, we need to use any CNN feature extractor with a GAP (Global average pooling) and fully connected layers on the top, as shown on the picture below.","75d39e14":"**If my kernel was helpful, please consider upvoting**","1036a731":"This solution is to use 'atrous convolution' (or dilated convolution). ","41e361fc":"<h1>Experiment<h1>","908664e1":"![dilated conv.PNG](attachment:43dd68ba-be61-478c-93b3-532ff6c947ca.PNG)","70f4110c":"So, if we have an image of a certain size, we can firstly apply max-pooling to it for downsampling, next apply convolution filter with dilateion = 1 and then upsample it to the original size. In this case an output feature map will have responses from only 1\/4 of the original image pixels. Instead, we can apply only one dilated convolution, which allows us to calculate responses from all images pixels. Dilated convolution will have the same number of parameters as the first one, so the number of parameters remain the same. Thus, using dilated convolution we can preserve an image resolution, which is especially important in the image segmentation task.","be6f8cbb":"![sdfsd-min.PNG](attachment:16813361-efe6-49a0-a5d0-5e106c244eb1.PNG)","833dece9":"Another feature of using the dilated convolution is receptive field. Receptive field in our example is a set of the input pixels which are taken into account in the calculation of a certain output pixel. <\/br>\nOn the picture below you can see example of 3 sequential convolution layers with kernel size 3 and dilation rates 1, 2 and 4 respectively. An output pixel from the first layer depends on the 3x3 region of an input image. An output pixel from the second layer depends on the 9 pixels from the previous layer and by the fact that these pixels are 1 pixel apart from each other, an output depends on the 7x7 region of the input image. On the same principle, an output pixel from the last third layer depends on the 15x15 region of an input image. Thus, the receptive field in that case is much more bigger if we just used 3 convolutional layers of size 3 and dilation = 3 (in that case receptive fiels was 7x7). "}}