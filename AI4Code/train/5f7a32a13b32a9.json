{"cell_type":{"b4b425fe":"code","691fc9b6":"code","8e336c8d":"code","cf0c4f9c":"code","6dbe8e66":"code","4f69f1f4":"code","c48ca33d":"code","7fbb0ac7":"code","6b846f0a":"code","d7dbe413":"code","d9ad36ed":"code","2ceb8a82":"code","f4be32e4":"code","f0fd2d87":"code","f328837d":"code","e392a7fd":"code","f6c06704":"code","137735dc":"code","c0e87fc4":"code","53b70568":"code","0787ead6":"code","b6582b7c":"code","9da53f9c":"code","01b3784c":"code","0aa97d36":"code","5672641b":"code","7f256af4":"code","c7c39240":"code","dd975a98":"code","5f5c2eeb":"code","fd20f8ab":"code","b7ae8181":"code","54df6d62":"code","9cc8fde5":"code","6ca4740c":"code","ecc1ccc3":"code","20163254":"code","8fe17692":"code","26cb8753":"code","afbbb85f":"code","71e2427b":"code","9add53b7":"code","c5808baf":"code","545da25b":"code","9dd9fb4d":"code","252daca8":"code","feeef51c":"code","b1b665ec":"code","125084cb":"code","d2c93877":"code","f96b79aa":"code","c69a054b":"code","1e9de4d1":"code","4e92f19d":"code","56380eba":"code","db25de81":"code","587a9b25":"code","5aab1b43":"code","394bbf3c":"code","fc2c5239":"code","46919a93":"code","3d3e1b76":"code","88f854b0":"code","84d465da":"code","ef0babb1":"code","7b51bc0f":"code","2037143f":"code","83f020fd":"code","ba6da75a":"code","1e8ad54f":"code","7b98823e":"code","6ed1b088":"code","bff71b1b":"code","5fdd2615":"code","1b48a973":"code","caccf24b":"code","6e47301d":"code","6c779509":"code","3e3f5882":"code","4da1e798":"code","0105f6a9":"markdown","9403360e":"markdown","5a990d18":"markdown","e53b1240":"markdown","baeacf05":"markdown","815641d5":"markdown","83b4e247":"markdown","163a7d89":"markdown","555c768e":"markdown","d474bf0c":"markdown","2c3a3d10":"markdown","345a10f6":"markdown","f7965e12":"markdown","33149d9a":"markdown","ed0c1f9c":"markdown","c93302ec":"markdown","6fad3a2e":"markdown","a4342bf0":"markdown","9818de66":"markdown","535c2e32":"markdown","2d5ea043":"markdown","e334ffe5":"markdown","a168e33e":"markdown","92e7cc07":"markdown","92d10b49":"markdown","83f23c28":"markdown","053a17ef":"markdown","d3e1deb8":"markdown","06891a52":"markdown","8ed4e83a":"markdown","1f73630b":"markdown","a004408f":"markdown","83b35e3d":"markdown","04013bc2":"markdown","02e7ec18":"markdown","75e13615":"markdown","18b4eb79":"markdown","acbda3b5":"markdown","cfa1a060":"markdown","6f7b924c":"markdown","76089030":"markdown","60fa531f":"markdown","31a7f454":"markdown","87f1c6a9":"markdown","8ad41904":"markdown","67c1f3ff":"markdown","a53d5491":"markdown","4f22966f":"markdown","7c5ccc8b":"markdown","938528c0":"markdown","c5ea369f":"markdown","a16d12dd":"markdown","a92c4b24":"markdown","feeaee94":"markdown"},"source":{"b4b425fe":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/2020-cost-of-living\/alerts.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()","691fc9b6":"!pip install pandarallel","8e336c8d":"# BASIC PACKAGES LOAD\nimport numpy as np\nimport pandas as pd\nimport random as rd\nimport datetime\nimport calendar\nimport os\nimport gc\nfrom pandarallel import pandarallel\n\n# VISUALIZATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport holoviews as hv\nfrom holoviews import opts\npandarallel.initialize()\nhv.extension('bokeh')\n\n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic, kpss\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n\n# PREPROCESSING\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom itertools import product\n\n# Modelling\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('seaborn-whitegrid')","cf0c4f9c":"sns.color_palette(\"YlOrRd\",  as_cmap=True)","6dbe8e66":"YlOrRd_palette_5 = sns.color_palette(\"YlOrRd\", 50)\nsns.palplot(YlOrRd_palette_5)","4f69f1f4":"# DATA LOAD\n\nsales_train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\", parse_dates = ['date'])\nitem_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nsubmission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","c48ca33d":"print(f'items.csv : {items.shape}')\nitems.head(3)","7fbb0ac7":"print(f'item_categories.csv : {item_categories.shape}')\nitem_categories.head(3)","6b846f0a":"print(f'shops.csv : {shops.shape}')\nshops.head(3)","d7dbe413":"print(f'sales_train.csv : {sales_train.shape}')\nsales_train.head(3)","d9ad36ed":"print(f'sample_submission.csv : {submission.shape}')\nsubmission.head(3)","2ceb8a82":"print(f'test.csv : {test.shape}')\ntest.head(3)","f4be32e4":"shops['city_name'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops['city_name'].unique()","f0fd2d87":"shops.loc[shops['city_name']=='!\u042f\u043a\u0443\u0442\u0441\u043a', 'city_name'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city_name']).astype(np.int8)\nshops.head(3)","f328837d":"item_categories['item_maincategory_name'] = item_categories['item_category_name'].str.split(' - ').map(lambda x: x[0])\nitem_categories['item_maincategory_name'].unique()","e392a7fd":"item_categories['item_subcategory_name'] = item_categories['item_category_name'].str.split('-').map(lambda x: '-'.join(x[1:]).strip() if len(x) > 1 else x[0].strip())\nitem_categories['item_subcategory_name'].unique()","f6c06704":"item_categories.loc[item_categories['item_maincategory_name']=='\u0418\u0433\u0440\u044b Android', 'item_maincategory_name'] = '\u0418\u0433\u0440\u044b'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0418\u0433\u0440\u044b MAC', 'item_maincategory_name'] = '\u0418\u0433\u0440\u044b'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0418\u0433\u0440\u044b PC', 'item_maincategory_name'] = '\u0418\u0433\u0440\u044b'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b (\u041a\u0438\u043d\u043e, \u041c\u0443\u0437\u044b\u043a\u0430, \u0418\u0433\u0440\u044b)', 'item_maincategory_name'] = '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)', 'item_maincategory_name'] = '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438'\nitem_categories.loc[item_categories['item_maincategory_name']=='\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)', 'item_maincategory_name'] = '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438'\nitem_categories['item_maincategory_id'] = LabelEncoder().fit_transform(item_categories['item_maincategory_name']).astype(np.int8)\nitem_categories['item_subcategory_id'] = LabelEncoder().fit_transform(item_categories['item_subcategory_name']).astype(np.int8)\nitem_categories.head(3)","137735dc":"item_info = pd.merge(items, item_categories, on='item_category_id', how='inner')\ntrain_tmp = pd.merge(sales_train,item_info, on='item_id', how='inner')\ntrain = pd.merge(train_tmp, shops, on='shop_id', how='inner')\ntrain.head(3)","c0e87fc4":"test_tmp = pd.merge(test,item_info, on='item_id', how='inner')\ntest = pd.merge(test_tmp, shops, on='shop_id', how='inner')\ntest.head(3)","53b70568":"train['total_sales'] = train['item_price'] * train['item_cnt_day']\ntrain.head(3)","0787ead6":"# Train\ntrain['date_block_num'] =train['date_block_num'].astype(np.int8)\ntrain['shop_id'] = train['shop_id'].astype(np.int8)\ntrain['item_id'] = train['item_id'].astype(np.int16)\ntrain['item_category_id'] = train['item_category_id'].astype(np.int16)\n\n# Test\ntest['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\ntest['item_category_id'] = test['item_category_id'].astype(np.int16)","b6582b7c":"shop_rank_df = train.shop_name.value_counts().sort_values(ascending=False)\nhv.Bars(shop_rank_df[0:20]).opts(title=\"Shop Count top20\", color=\"orangered\", xlabel=\"Shop Name\", ylabel=\"Count\")\\\n                            .opts(opts.Bars(width=700, height=500,tools=['hover'],xrotation=45,show_grid=True))","9da53f9c":"shop_rank_df = train.shop_name.value_counts().sort_values(ascending=False)\nhv.Bars(shop_rank_df[-20:]).opts(title=\"Worst Sales Shop20\", color=\"orangered\", xlabel=\"Shop Name\", ylabel=\"Count\")\\\n                            .opts(opts.Bars(width=700, height=500,tools=['hover'],xrotation=45,show_grid=True))","01b3784c":"hv.Bars(train['city_name'].value_counts()).opts(title=\"City Count\", color=\"orangered\", xlabel=\"City Name\", ylabel=\"Count\")\\\n                                            .opts(opts.Bars(width=700, height=500,tools=['hover'],xrotation=45,show_grid=True))","0aa97d36":"item_rank_df = train.item_name.value_counts().sort_values(ascending=False)\nhv.Bars(item_rank_df[0:20]).opts(title=\"Item Count top20\", color=\"orangered\", xlabel=\"Item Name\", ylabel=\"Count\")\\\n                            .opts(opts.Bars(width=700, height=500,tools=['hover'],xrotation=45,show_grid=True))","5672641b":"item_rank_df = train.item_name.value_counts().sort_values(ascending=False)\nhv.Bars(item_rank_df[-10:]).opts(title=\"Worst Sales Item20\", color=\"orangered\", xlabel=\"Item Name\", ylabel=\"Count\")\\\n                            .opts(opts.Bars(width=700, height=500,tools=['hover'],xrotation=45,show_grid=True))","7f256af4":"item_cat_rank_df = train.item_category_name.value_counts().sort_values(ascending=False)\nhv.Bars(item_cat_rank_df[0:20]).opts(title=\"Item Category Count top20\", color=\"orangered\" ,xlabel=\"Item categories\", ylabel=\"Count\")\\\n                                .opts(opts.Bars(width=700, height=500,tools=['hover'],xrotation=45,show_grid=True))","c7c39240":"item_cat_rank_df = train.item_category_name.value_counts().sort_values(ascending=False)\nhv.Bars(item_cat_rank_df[-20:]).opts(title=\"Worst Sales Item Category20\", color=\"orangered\" ,xlabel=\"Item categories\", ylabel=\"Count\")\\\n                                .opts(opts.Bars(width=700, height=500,tools=['hover'],xrotation=45,show_grid=True))","dd975a98":"train[[\"date_block_num\",\"shop_id\",\"item_id\",\"date\",\"item_price\",\"item_cnt_day\",\"total_sales\"]].groupby([\"date_block_num\",\"shop_id\",\"item_id\"])\\\n            .agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\",\"total_sales\":\"sum\"}).head(10)","5f5c2eeb":"monthly_ts = train.groupby([\"date_block_num\"])[\"total_sales\",\"item_cnt_day\"].sum()\nmonth_ts_sales = hv.Curve(monthly_ts[\"total_sales\"]).opts(title=\"Monthly Sales Time Series\", xlabel=\"Month\", ylabel=\"Total Sales\")\nmonth_ts_cnt = hv.Curve(monthly_ts[\"item_cnt_day\"]).opts(title=\"Monthly Item Count Time Series\", xlabel=\"Month\", ylabel=\"Item Count\")\n(month_ts_sales + month_ts_cnt).opts(opts.Curve(width=400, height=300,color=\"orangered\",tools=['hover'],show_grid=True,line_width=5,line_dash='dotted'))","fd20f8ab":"TS = train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nRolling_Mean = hv.Curve(TS.rolling(window=12,center=False).mean(), label = 'Mean').opts(color=\"orange\")\nRolling_std = hv.Curve(TS.rolling(window=12,center=False).std(), label = 'std').opts(color=\"orangered\")\n(Rolling_Mean * Rolling_std).opts(title=\"Moving Mean and Std\", xlabel=\"date_block_num\", ylabel=\"item_cnt_day\").opts(opts.Curve(width=600, height=300,tools=['hover'],show_grid=True,line_width=5)).opts(legend_position='top_left')","b7ae8181":"sales_dec = sm.tsa.seasonal_decompose(monthly_ts[\"total_sales\"].values,period=12,model=\"multiplicative\").plot()","54df6d62":"item_cnt_dec = sm.tsa.seasonal_decompose(monthly_ts[\"item_cnt_day\"].values,period=12,model=\"multiplicative\").plot()","9cc8fde5":"# ADF Test\ndftest = adfuller(monthly_ts[\"total_sales\"].values, autolag='AIC')\ndf_output = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lag Used', 'Number of Observation Used'])\nfor key, value in dftest[4].items():\n    df_output['Critical value (%s)'%key] = value\nprint(\"Sales\")    \nprint(df_output)\nprint()\n\ndftest = adfuller(monthly_ts[\"item_cnt_day\"].values, autolag='AIC')\ndf_output = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lag Used', 'Number of Observation Used'])\nfor key, value in dftest[4].items():\n    df_output['Critical value (%s)'%key] = value\nprint(\"Item\")    \nprint(df_output)","6ca4740c":"from pandas import Series as Series\n\n# to remove trend\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","ecc1ccc3":"ts = train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts, color = \"orangered\")\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts, color = \"orangered\")\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts, color = \"orangered\")\nplt.plot()","20163254":"dftest = adfuller(new_ts, autolag='AIC')\ndf_output = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lag Used', 'Number of Observation Used'])\nfor key, value in dftest[4].items():\n    df_output['Critical value (%s)'%key] = value\nprint(\"Item\")    \nprint(df_output)","8fe17692":"from matplotlib.collections import PolyCollection, LineCollection\n\nfig, curr_ax = plt.subplots(1, 2, figsize=(16, 3))\nsales_acf = sm.graphics.tsa.plot_acf(monthly_ts[\"item_cnt_day\"].values, lags=24,color=\"#E56717\",vlines_kwargs={\"colors\": 'orange'}, ax = curr_ax[0])\nsales_pacf = sm.graphics.tsa.plot_pacf(monthly_ts[\"item_cnt_day\"].values, lags=16, color=\"#E56717\",vlines_kwargs={\"colors\": 'orange'},ax = curr_ax[1])\n\nfor i in range(2):\n    for item in curr_ax[i].collections:\n        if type(item)==PolyCollection:\n            item.set_facecolor(\"orangered\")\nfig.suptitle('ACF & PCAF of Items', fontsize = 20, y=1.2)\nplt.show()","26cb8753":"fig, curr_ax = plt.subplots(1, 2, figsize=(16, 3))\nsales_acf = sm.graphics.tsa.plot_acf(monthly_ts[\"total_sales\"].values, lags=24,color=\"#E56717\",vlines_kwargs={\"colors\": 'orange'}, ax = curr_ax[0])\nsales_pacf = sm.graphics.tsa.plot_pacf(monthly_ts[\"total_sales\"].values, lags=16, color=\"#E56717\",vlines_kwargs={\"colors\": 'orange'},ax = curr_ax[1])\n\nfor i in range(2):\n    for item in curr_ax[i].collections:\n        if type(item)==PolyCollection:\n            item.set_facecolor(\"orangered\")\nfig.suptitle('ACF & PCAF of Sales', fontsize = 20, y=1.2)\nplt.show()","afbbb85f":"from pandas.plotting import lag_plot\n\nax_idcs = [(x, y) for x in range(8) for y in range(4)]\nplt.rcParams.update({'ytick.left' : False, 'axes.titlepad' : 10})\n\nfig, axes = plt.subplots(8, 4, figsize=(25, 20),sharex=True, sharey=True, dpi=100)\n# for lag, ax_coords in enumerate(ax_idcs):\n#     ax_row, ax_col = ax_coords\n#     axis = axes[ax_row][ax_col]\n#     lag_plot(monthly_ts[\"item_cnt_day\"], lag=lag+1, ax=axis, c=\"orangered\")\n#     ax.set_title('Lag' + str(i+1))\nfor i, ax in enumerate(axes.flatten()[:32]):\n    lag_plot(monthly_ts[\"item_cnt_day\"], lag=i+1, ax=ax, c=\"orangered\")\n    \n    ax.set_title('Lag' + str(i+1))\nfig.suptitle('Lag Plots of item', y=1)\nplt.tight_layout()\nplt.show()","71e2427b":"fig, axes = plt.subplots(8, 4, figsize=(25, 22), sharex=True, sharey=True, dpi=100)\nplt.rcParams.update({'ytick.left' : False, 'axes.titlepad' : 10})\nfor i, ax in enumerate(axes.flatten()[:32]):\n    lag_plot(monthly_ts[\"total_sales\"], lag=i+1, ax=ax, c=\"orangered\")\n    ax.set_title('Lag' + str(i+1))\nfig.suptitle('Lag Plots of Sales', y=1)\nplt.tight_layout()\nplt.show()","9add53b7":"price_bx = hv.BoxWhisker(train[['item_price']].sort_values('item_price',ascending=False)[0:500].values,label='Item Price BoxPlot',vdims='Price').opts(box_fill_color=\"orangered\")\ncnt_bx = hv.BoxWhisker(train[['item_cnt_day']].sort_values('item_cnt_day',ascending=False)[0:500].values,label='Item Count Day BoxPlot',vdims='Count').opts(box_fill_color=\"orangered\")\n(price_bx + cnt_bx).opts(opts.BoxWhisker(width=400, height=400,show_grid=True,tools=['hover']))","c5808baf":"# DATA LOAD\n\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')","545da25b":"train = sales[(sales.item_price < 100000) & (sales.item_price > 0)]\ntrain = train[sales.item_cnt_day < 1001]","9dd9fb4d":"print(shops[shops.shop_id == 0]['shop_name'].unique(), shops[shops.shop_id == 57]['shop_name'].unique())\nprint(shops[shops.shop_id == 1]['shop_name'].unique(), shops[shops.shop_id == 58]['shop_name'].unique())\nprint(shops[shops.shop_id == 40]['shop_name'].unique(), shops[shops.shop_id == 39]['shop_name'].unique())\nprint(shops[shops.shop_id == 10]['shop_name'].unique(), shops[shops.shop_id == 11]['shop_name'].unique())","252daca8":"# Deduplication\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\n# P\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443 \u0422\u0420\u041a \"\u041c\u0435\u0433\u0430\u0446\u0435\u043d\u0442\u0440 \u0413\u043e\u0440\u0438\u0437\u043e\u043d\u0442\" \u041e\u0441\u0442\u0440\u043e\u0432\u043d\u043e\u0439\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39","feeef51c":"# Add shop_id, item_id, date_block_num\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\ndf = [] \nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    df.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\ndf = pd.DataFrame(np.vstack(df), columns = index_cols,dtype=np.int32)\n\n#Add month sales\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\ndf = pd.merge(df, group, on=index_cols, how='left')\ndf['item_cnt_month'] = (df['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20)\n                                .astype(np.float16))\ndf.head(5)","b1b665ec":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\ndf = pd.concat([df, test], ignore_index=True, sort=False, keys=index_cols)\ndf.fillna(0, inplace=True)\ndf.info()","125084cb":"shops['city'] = shops['shop_name'].apply(lambda x: x.split()[0].lower())\nshops.loc[shops.city == '!\u044f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u044f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n\ncoords = dict()\ncoords['\u044f\u043a\u0443\u0442\u0441\u043a'] = (62.028098, 129.732555, 4)\ncoords['\u0430\u0434\u044b\u0433\u0435\u044f'] = (44.609764, 40.100516, 3)\ncoords['\u0431\u0430\u043b\u0430\u0448\u0438\u0445\u0430'] = (55.8094500, 37.9580600, 1)\ncoords['\u0432\u043e\u043b\u0436\u0441\u043a\u0438\u0439'] = (53.4305800, 50.1190000, 3)\ncoords['\u0432\u043e\u043b\u043e\u0433\u0434\u0430'] = (59.2239000, 39.8839800, 2)\ncoords['\u0432\u043e\u0440\u043e\u043d\u0435\u0436'] = (51.6720400, 39.1843000, 3)\ncoords['\u0432\u044b\u0435\u0437\u0434\u043d\u0430\u044f'] = (0, 0, 0)\ncoords['\u0436\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439'] = (55.5952800, 38.1202800, 1)\ncoords['\u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d'] = (0, 0, 0)\ncoords['\u043a\u0430\u0437\u0430\u043d\u044c'] = (55.7887400, 49.1221400, 4)\ncoords['\u043a\u0430\u043b\u0443\u0433\u0430'] = (54.5293000, 36.2754200, 4)\ncoords['\u043a\u043e\u043b\u043e\u043c\u043d\u0430'] = (55.0794400, 38.7783300, 4)\ncoords['\u043a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a'] = (56.0183900, 92.8671700, 4)\ncoords['\u043a\u0443\u0440\u0441\u043a'] = (51.7373300, 36.1873500, 3)\ncoords['\u043c\u043e\u0441\u043a\u0432\u0430'] = (55.7522200, 37.6155600, 1)\ncoords['\u043c\u044b\u0442\u0438\u0449\u0438'] = (55.9116300, 37.7307600, 1)\ncoords['\u043d.\u043d\u043e\u0432\u0433\u043e\u0440\u043e\u0434'] = (56.3286700, 44.0020500, 4)\ncoords['\u043d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a'] = (55.0415000, 82.9346000, 4)\ncoords['\u043e\u043c\u0441\u043a'] = (54.9924400, 73.3685900, 4)\ncoords['\u0440\u043e\u0441\u0442\u043e\u0432\u043d\u0430\u0434\u043e\u043d\u0443'] = (47.2313500, 39.7232800, 3)\ncoords['\u0441\u043f\u0431'] = (59.9386300, 30.3141300, 2)\ncoords['\u0441\u0430\u043c\u0430\u0440\u0430'] = (53.2000700, 50.1500000, 4)\ncoords['\u0441\u0435\u0440\u0433\u0438\u0435\u0432'] = (56.3000000, 38.1333300, 4)\ncoords['\u0441\u0443\u0440\u0433\u0443\u0442'] = (61.2500000, 73.4166700, 4)\ncoords['\u0442\u043e\u043c\u0441\u043a'] = (56.4977100, 84.9743700, 4)\ncoords['\u0442\u044e\u043c\u0435\u043d\u044c'] = (57.1522200, 65.5272200, 4)\ncoords['\u0443\u0444\u0430'] = (54.7430600, 55.9677900, 4)\ncoords['\u0445\u0438\u043c\u043a\u0438'] = (55.8970400, 37.4296900, 1)\ncoords['\u0446\u0438\u0444\u0440\u043e\u0432\u043e\u0439'] = (0, 0, 0)\ncoords['\u0447\u0435\u0445\u043e\u0432'] = (55.1477000, 37.4772800, 4)\ncoords['\u044f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c'] = (57.6298700, 39.8736800, 2) \n\nshops['city_coord_1'] = shops['city'].apply(lambda x: coords[x][0])\nshops['city_coord_2'] = shops['city'].apply(lambda x: coords[x][1])\nshops['country_part'] = shops['city'].apply(lambda x: coords[x][2])\n\nshops = shops[['shop_id', 'city_code', 'city_coord_1', 'city_coord_2', 'country_part']]\n\n# Merge\ndf = pd.merge(df, shops, on=['shop_id'], how='left')","d2c93877":"df.head(5)","f96b79aa":"map_dict = {\n            '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)': '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438',\n            '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)' : '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438',\n            'PC ': '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b',\n            '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435': '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 '\n            }\n\nitems = pd.merge(items, item_cats, on='item_category_id')\n\nitems['item_category'] = items['item_category_name'].apply(lambda x: x.split('-')[0])\nitems['item_category'] = items['item_category'].apply(lambda x: map_dict[x] if x in map_dict.keys() else x)\nitems['item_category_common'] = LabelEncoder().fit_transform(items['item_category'])\n\nitems['item_category_code'] = LabelEncoder().fit_transform(items['item_category_name'])\nitems = items[['item_id', 'item_category_common', 'item_category_code']]\n\n# Merge\ndf = pd.merge(df, items, on=['item_id'], how='left')","c69a054b":"df.head(5)","1e9de4d1":"def count_days(date_block_num):\n    year = 2013 + date_block_num \/\/ 12\n    month = 1 + date_block_num % 12\n    weeknd_count = len([1 for i in calendar.monthcalendar(year, month) if i[6] != 0])\n    days_in_month = calendar.monthrange(year, month)[1]\n    return weeknd_count, days_in_month, month\n\nmap_dict = {i: count_days(i) for i in range(35)}\n\ndf['weeknd_count'] = df['date_block_num'].apply(lambda x: map_dict[x][0])\ndf['days_in_month'] = df['date_block_num'].apply(lambda x: map_dict[x][1])\ndf['month'] = df['date_block_num'].apply(lambda x: map_dict[x][2])\ndf['christmas'] = df['date_block_num'].apply(lambda x: 1 if map_dict[x][2] == 12 else 0)","4e92f19d":"df.head(5)","56380eba":"first_item_block = df.groupby(['item_id'])['date_block_num'].min().reset_index()\nfirst_item_block['item_first_interaction'] = 1\n\nfirst_shop_item_buy_block = df[df['date_block_num'] > 0].groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\nfirst_shop_item_buy_block['first_date_block_num'] = first_shop_item_buy_block['date_block_num']","db25de81":"df = pd.merge(df, first_item_block[['item_id', 'date_block_num', 'item_first_interaction']], on=['item_id', 'date_block_num'], how='left')\ndf = pd.merge(df, first_shop_item_buy_block[['item_id', 'shop_id', 'first_date_block_num']], on=['item_id', 'shop_id'], how='left')\n\ndf['first_date_block_num'].fillna(100, inplace=True)\ndf['shop_item_sold_before'] = (df['first_date_block_num'] < df['date_block_num']).astype('int8')\ndf.drop(['first_date_block_num'], axis=1, inplace=True)\n\ndf['item_first_interaction'].fillna(0, inplace=True)\ndf['shop_item_sold_before'].fillna(0, inplace=True)\n \ndf['item_first_interaction'] = df['item_first_interaction'].astype('int8')  \ndf['shop_item_sold_before'] = df['shop_item_sold_before'].astype('int8') ","587a9b25":"df.head(5)","5aab1b43":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        df[col+'_lag_'+str(i)] = df[col+'_lag_'+str(i)].astype('float16')\n    return df","394bbf3c":"#Add sales lags for last 3 months\ndf = lag_feature(df, [1, 2, 3], 'item_cnt_month')\n\n#Critical point: True or False (Affects qmean calculation)\ndf['qmean'] = df[['item_cnt_month_lag_1', \n                    'item_cnt_month_lag_2', \n                    'item_cnt_month_lag_3']].mean(skipna=True, axis=1)","fc2c5239":"#Add avg shop\/item price\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngroup = train.groupby(index_cols)['item_price'].mean().reset_index().rename(columns={\"item_price\": \"avg_shop_price\"}, errors=\"raise\")\ndf = pd.merge(df, group, on=index_cols, how='left')\n\ndf['avg_shop_price'] = (df['avg_shop_price']\n                                .fillna(0)\n                                .astype(np.float16))\n\nindex_cols = ['item_id', 'date_block_num']\ngroup = train.groupby(['date_block_num','item_id'])['item_price'].mean().reset_index().rename(columns={\"item_price\": \"avg_item_price\"}, errors=\"raise\")\n\n\ndf = pd.merge(df, group, on=index_cols, how='left')\ndf['avg_item_price'] = (df['avg_item_price']\n                                .fillna(0)\n                                .astype(np.float16))\n\ndf['item_shop_price_avg'] = (df['avg_shop_price'] - df['avg_item_price']) \/ df['avg_item_price']\ndf['item_shop_price_avg'].fillna(0, inplace=True)\n\ndf = lag_feature(df, [1, 2, 3], 'item_shop_price_avg')\ndf.drop(['avg_shop_price', 'avg_item_price', 'item_shop_price_avg'], axis=1, inplace=True)","46919a93":"#Add target encoding for items for last 3 months \nitem_id_target_mean = df.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_enc\"}, errors=\"raise\")\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_id'], how='left')\n\ndf['item_target_enc'] = (df['item_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ndf = lag_feature(df, [1, 2, 3], 'item_target_enc')\ndf.drop(['item_target_enc'], axis=1, inplace=True)","3d3e1b76":"#Add target encoding for item\/city for last 3 months \nitem_id_target_mean = df.groupby(['date_block_num','item_id', 'city_code'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_loc_target_enc\"}, errors=\"raise\")\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_id', 'city_code'], how='left')\n\ndf['item_loc_target_enc'] = (df['item_loc_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ndf = lag_feature(df, [1, 2, 3], 'item_loc_target_enc')\ndf.drop(['item_loc_target_enc'], axis=1, inplace=True)","88f854b0":"#Add target encoding for item\/shop for last 3 months \nitem_id_target_mean = df.groupby(['date_block_num','item_id', 'shop_id'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_shop_target_enc\"}, errors=\"raise\")\n\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_id', 'shop_id'], how='left')\n\ndf['item_shop_target_enc'] = (df['item_shop_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ndf = lag_feature(df, [1, 2, 3], 'item_shop_target_enc')\ndf.drop(['item_shop_target_enc'], axis=1, inplace=True)","84d465da":"#For new items add avg category sales for last 3 months\nitem_id_target_mean = df[df['item_first_interaction'] == 1].groupby(['date_block_num','item_category_code'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"new_item_cat_avg\"}, errors=\"raise\")\n\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_category_code'], how='left')\n\ndf['new_item_cat_avg'] = (df['new_item_cat_avg']\n                                .fillna(0)\n                                .astype(np.float16))\n\ndf = lag_feature(df, [1, 2, 3], 'new_item_cat_avg')\ndf.drop(['new_item_cat_avg'], axis=1, inplace=True)","ef0babb1":"#For new items add avg category sales in a separate store for last 3 months\nitem_id_target_mean = df[df['item_first_interaction'] == 1].groupby(['date_block_num','item_category_code', 'shop_id'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"new_item_shop_cat_avg\"}, errors=\"raise\")\n\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_category_code', 'shop_id'], how='left')\n\ndf['new_item_shop_cat_avg'] = (df['new_item_shop_cat_avg']\n                                .fillna(0)\n                                .astype(np.float16))\n\ndf = lag_feature(df, [1, 2, 3], 'new_item_shop_cat_avg')\ndf.drop(['new_item_shop_cat_avg'], axis=1, inplace=True)","7b51bc0f":"# Add sales for the last three months for similar item \n# item with id = item_id - 1; kinda tricky feature, but increased the metric significantly\ndef lag_feature_adv(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)+'_adv']\n        shifted['date_block_num'] += i\n        shifted['item_id'] -= 1\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        df[col+'_lag_'+str(i)+'_adv'] = df[col+'_lag_'+str(i)+'_adv'].astype('float16')\n    return df\n\ndf = lag_feature_adv(df, [1, 2, 3], 'item_cnt_month')","2037143f":"df.fillna(0, inplace=True)\ndf = df[(df['date_block_num'] > 2)]\ndf.head(5)","83f020fd":"#Save dataset\ndf.drop(['ID'], axis=1, inplace=True, errors='ignore')\ndf.to_pickle('df.pkl')","ba6da75a":"ts = train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()","1e8ad54f":"tslogdiffshifting = ts - ts.shift()","7b98823e":"!pip install pmdarima","6ed1b088":"from pmdarima import auto_arima\n\nstepwise_fit = auto_arima(ts, trace=True,\nsuppress_warnings=True)","bff71b1b":"model = ARIMA(ts, order=(3,1,1))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(tslogdiffshifting, color = 'orange')\nplt.plot(results_ARIMA.fittedvalues, color='orangered')\nprint('Plotting ARIMA model')","5fdd2615":"df = pd.read_pickle('df.pkl')\ndf.info()","1b48a973":"X_train = df[df.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = df[df.date_block_num < 33]['item_cnt_month']\nX_valid = df[df.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = df[df.date_block_num == 33]['item_cnt_month']\nX_test = df[df.date_block_num == 34].drop(['item_cnt_month'], axis=1)\ndel df","caccf24b":"# !nvidia-smi","6e47301d":"feature_name = X_train.columns.tolist()\n\nparams = {\n    'objective': 'mse',\n    'metric': 'rmse',\n    'num_leaves': 15,\n    'learning_rate': 0.005,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 5,\n    'seed': 1,\n    'verbose': 1,\n    'device': 'gpu',\n#     'gpu_platform_id': 0,\n#     'gpu_device_id': 0,\n    'force_row_wise' : True\n}\n\nfeature_name_indexes = [ \n                        'country_part',\n                        'month',\n                        'item_category_common',\n                        'item_category_code', \n                        'city_code',\n]\n\nlgb_train = lgb.Dataset(X_train[feature_name], Y_train)\nlgb_eval = lgb.Dataset(X_valid[feature_name], Y_valid, reference=lgb_train)\n\nevals_result = {}\ngbm = lgb.train(\n        params, \n        lgb_train,\n        num_boost_round= 3000,\n        valid_sets=(lgb_train, lgb_eval), \n        feature_name = feature_name,\n        categorical_feature = feature_name_indexes,\n        verbose_eval=50, \n        evals_result = evals_result,\n        early_stopping_rounds = 100)","6c779509":"YlOrRd_palette_5 = sns.color_palette(\"YlOrRd\", 30 )\nsns.palplot(YlOrRd_palette_5)","3e3f5882":"fig, ax = plt.subplots(1,1, figsize=(8,6))\nlgb.plot_importance(gbm, max_num_features=50,color=YlOrRd_palette_5, importance_type='gain', ax=ax)\nax.set_title(\"Result ( type : gain )   \", fontweight=\"bold\", fontsize=15)\nax.patch.set_alpha(0) \nplt.show()","4da1e798":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nY_test = gbm.predict(X_test[feature_name]).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('gbm_submission.csv', index=False)\nsubmission.head(5)","0105f6a9":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Calculating the amount of sales per a day<\/b><\/p>","9403360e":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83d\udca5 <b>References<\/b> \ud83d\udca5\n    \n <div class=\"alert warning-alert\">   \n\ud83d\udccc <b>Thank you for always sharing good data\ud83d\ude4f\ud83c\udffb <\/b><br>\n    \n<br>&nbsp; <b>[Time series Basics : Exploring traditional TS]<br> \ud83d\udc49 https:\/\/www.kaggle.com\/jagangupta\/time-series-basics-exploring-traditional-ts<\/b><br><br>\n&nbsp; <b>[Prophet\/LightGBM - EDA&Feature Engineering&Tuning]<br>\ud83d\udc49 https:\/\/www.kaggle.com\/koheimuramatsu\/prophet-lightgbm-eda-feature-engineering-tuning<\/b><br><br>\n&nbsp; <b>[lsmmay322.log]<br>\n    \ud83d\udc49 https:\/\/velog.io\/@lsmmay322\/Kaggle-AirPassnegerst<\/b><br><br>\n&nbsp; <b>[Modelling]<br>\n    \ud83d\udc49 https:\/\/www.kaggle.com\/uladzimirkapeika\/feature-engineering-lightgbm-top-1?select=gbm_submission.csv<br><br>\n    \ud83d\udc49 https:\/\/www.kaggle.com\/uladzimirkapeika\/feature-engineering-lightgbm-top-1?select=gbm_submission.csv<\/b><br>\n     <\/div>","5a990d18":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Traget lags feature<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We will extract the lags by last 3.<\/b><\/div>","e53b1240":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Remove data for the first three months<\/b><\/p>","baeacf05":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Why can't Non-Stationary data be used?<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>1. Most statistical prediction methods are designed for normality time series data.<br>\n2. Predicting Stationary data is relatively easy and stable.<br>\n3. AR models are essentially linear regression models. It uses its own lags as predictors.<br>\n4. Linear regression performs well when explanatory variables are uncorrelated.<br>\n5. Stationarization eliminates autocorrelation, creating explanatory variables in the prediction model independently.<br><br>\n    \ud83d\udc49 Therefore, the first step in predicting time series data can be said to be converting nonnormality data into normality data.<\/b><\/div>","815641d5":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83d\udca5 <b>Feature engineering for Modelling<\/b> \ud83d\udca5","83b4e247":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Extra interaction features<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We will target encodings using category sales.<\/b><\/div>","163a7d89":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Create derived features in date<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We will extract weekend, day and year-end data from date data. Weekends are set at 4 or 5.<\/b><\/div>","555c768e":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Converting class type to reduce memory load<\/b><\/p>","d474bf0c":"    \n<p style=\"font-family:Comic Sans MS; font-size:200%; color: #ff7f00; text-align:center;\"><b><br>Pls, \"UPVOTE\" if this code helped ! \ud83d\udc40<\/b><\/p>","2c3a3d10":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Data preprocessing for EDA<\/b><\/p>","345a10f6":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Monthly Aggregation<\/b><\/p>","f7965e12":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Which item sold the most?<\/b><\/p>","33149d9a":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83d\udca5 <b>Exploratory Data Analysis<\/b> \ud83d\udca5","ed0c1f9c":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 ARIMA Model<\/b><\/p>","c93302ec":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Load Data for LGBM<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Let's bring the data back to create a clean modeling dataset.<\/b><\/div>","6fad3a2e":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Which store sold the most?<\/b><\/p>","a4342bf0":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Load data for LGBM<\/b><\/p>","9818de66":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Separate Train, Valid and TEST Set\n<\/b><\/p>","535c2e32":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 What is a Stationary?<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Stationary is a time series with constant statistical characteristics even when time changes. Therefore, regardless of the trend of time, the mean, variance, etc. are invariant.<\/b><\/div>","2d5ea043":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Create derived features in items<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We will extract the category of the item and the category code here.<\/b><\/div>","e334ffe5":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\">\ud83d\udc49 <b>Movie DVDs, game-related items, and music devices are among the most popular.<\/b><\/div>","a168e33e":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 ACF & PCAF<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 ACF : The ACF is self-correlated with its own time difference. If the autocorrelation is large, the time series data will help predict the future with previous values. The big correlation is because there is a certain pattern.<br><br> \ud83d\udc49 PACF : PACF also delivers similar information, but provides autocorrelation for pure time series and lags, except for autocorrelation contributed from intermediate lags.\nIn other words, PACF is used when looking at correlations, except for the factors that affect them.<\/b><\/div>","92e7cc07":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We can see that the ACF and PACF of both data converge to zero relatively quickly.<\/b><\/div>","92d10b49":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Now after the transformations, our P-value for the ADF test is well within 5 %. Hence we can assume Stationarity of the series.<\/b><\/div>","83f23c28":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Lag Plots<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Lag Plots is a scatterplot of the time series for its own Lag. It is commonly used to check autocorrelation. If any pattern exists, the time series means that autocorrelation exists. If there is no pattern, the time series is likely to be random white noise.<\/b><\/div>","053a17ef":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Monthly sales<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Monthly sales are needed for next month's forecast. Let's change it to a simple train dataset for future predictions.<\/b><\/div>","d3e1deb8":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Add coordinates in Shop<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Store the longitude and latitude by taking the city coordinates of the shop. It also maps values between 0 and 4 to a country part.<\/b><\/div>","06891a52":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\">\ud83d\udc49 <b>The item has a long name, so I can't see the best-selling item. The best-selling item is \"\u0438\u043e\u043d\u0430\u043d\u0430 \u043d\u0430\u043d\u0430 1 \u043d\u0430\u043d\u0430 (34*42) 45\".<\/b><\/div>","8ed4e83a":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 About the seasonality, trends, and residuals in Sales and Items<\/b><\/p>","1f73630b":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83d\udca5 <b>Single time-series<\/b> \ud83d\udca5","a004408f":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS ; font-size:150%; text-align:center\"> \ud83d\udca5 <b>Data and Packages Imports<\/b> \ud83d\udca5","83b35e3d":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Remove Trend and Seasonality<\/b><\/p>","04013bc2":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Additional Sales Data<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Let's get data on whether the customer first purchased it or not and whether they had previously purchased it.<\/b><\/div>","02e7ec18":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Outliers are found in prices and items. It needs to be removed through a detailed search.<\/b><\/div>","75e13615":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 The Arima model seemed to have a very low prediction rate, with little profit...<\/b><\/div>","18b4eb79":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Merge train and test data<\/b><\/p>","acbda3b5":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83d\udca5 <b>Introduction<\/b> \ud83d\udca5\n    \n<p style=\"font-family:Comic Sans MS; font-size:200%; color: #ff7f00; text-align:center;\"><b>In this notebook, we will explore variable time-series concepts and build a model that can predict sales.<\/b><\/p>\n\n<br>\n<br>\n\n<div class=\"alert warning-alert\">\n\ud83d\udccc <b>Competition Goal : <\/b>\n\n<br>&nbsp; This challenge serves as final project for the \"How to win a data science competition\" Coursera course.\n\n<br>&nbsp; In this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.\n\n<br>&nbsp; We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n<\/div>\n\n\n<div class=\"alert warning-alert\">\n\ud83d\udccc <b>Data fields : <\/b>\n    \n<br>&nbsp; <b>ID<\/b> - an Id that represents a (Shop, Item) tuple within the test set<br>\n&nbsp; <b>shop_id<\/b> - unique identifier of a shop<br>\n&nbsp; <b>item_id<\/b> - unique identifier of a product<br>\n&nbsp; <b>item_category_id<\/b> - unique identifier of item category<br>\n&nbsp; <b>item_cnt_day<\/b> - number of products sold. You are predicting a monthly amount of this measure<br>\n&nbsp; <b>item_price<\/b> - current price of an item<br>\n&nbsp; <b>date<\/b> - date in format dd\/mm\/yyyy<br>\n&nbsp; <b>date_block_num<\/b> - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33<br>\n&nbsp; <b>item_name<\/b> - name of item<br>\n&nbsp; <b>shop_name<\/b> - name of shop<br>\n&nbsp; <b>item_category_name<\/b> - name of item category   \n<\/div>","cfa1a060":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Concat train and test<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Train and test data set are combined into one for future feature engineering.<\/b><\/div>","6f7b924c":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We can see that the tightest linear graph appears when lag is 1.<\/b><\/div>","76089030":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Detect same shops<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We can find several shop_name duplicates. Therefore, duplication is needed.<\/b><\/div>","60fa531f":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Remove Outliers<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 On the boxplot, we found an outlier. With reference to other kernels, the company decided to remove the price of 100,000 or more, and sales of 1001 or more.<\/b><\/div>","31a7f454":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83d\udca5 <b>Modelling<\/b> \ud83d\udca5","87f1c6a9":"<a id='1'><\/a>\n# <p style=\"background-color:orange; font-family:Comic Sans MS; font-size:150%; text-align:center\"> \ud83d\udca5 <b>Submission<\/b> \ud83d\udca5","8ad41904":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Checking for outliers<\/b><\/p>","67c1f3ff":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 What are the Stationarization Test methods?<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>1. ADF(Augmented Dicky-Fuller)<br>\n    2. KPSS(Kwiatkowski\u2013Phillips\u2013Schmidt\u2013Shin)<br>3. PP (Phillips-Perron)<br><br>\n    \ud83d\udc49 All right, so let's start testing.<\/b><\/div>","a53d5491":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\">\ud83d\udc49 <b>Movie DVDs, game-related items, and music devices are among the most popular.<\/b><\/div>","4f22966f":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Looking at the number of shops,  the shops in Moscow stand out.<\/b><\/div>","7c5ccc8b":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Sales has a P-value of less than 5%, but Item is above. Therefore, trends and seasonality need to be eliminated.<\/b><\/div>","938528c0":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Both data have seasonality. Prices tend to rise and fall in the middle, but the number of items continues to decline.<\/b><\/div>","c5ea369f":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Traget encoding<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We will target encodings using items, cities and shops.<\/b><\/div>","a16d12dd":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Which item category sold the most?<\/b><\/p>","a92c4b24":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>Insights<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 We can confirm that \"Seasonality\" and \"Trend\" are evident.<\/b><\/div>","feeaee94":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #ff7f00\"><b>\ud83d\udce3 Moving Averages<\/b><\/p>\n\n><div class=\"alert warning-alert\" role=\"alert\"><b>\ud83d\udc49 Moving averages are techniques for averaging by moving to a window of defined width. You have to choose this area wisely. If you average the movement with a very wide window, it will be an excessively smooth time series. This is because it nullifies the seasonal effect.<\/b><\/div>"}}