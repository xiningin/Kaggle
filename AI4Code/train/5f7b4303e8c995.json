{"cell_type":{"62e2803d":"code","03f1c1e1":"code","ef0b01ef":"code","8f1cb0be":"code","1348d7f1":"code","49520e43":"code","d0180e45":"code","2b032bb5":"code","191da9fb":"code","3dcf9585":"code","db79c2ba":"code","3ffd9f50":"code","21069843":"code","a24a5300":"code","73d76096":"code","ad5c4da1":"code","83b3e68e":"code","dbc4c122":"code","00f2591a":"code","b5a6a919":"code","aed95c56":"code","f3484509":"code","c2dad5fd":"code","80ee5bc5":"code","64bacd16":"code","c19de1fa":"markdown","437ca0ac":"markdown","35fbf798":"markdown","b41d190a":"markdown","94ee7046":"markdown","11d66548":"markdown","d4f7e97b":"markdown","d645910a":"markdown","087dd62d":"markdown","6f861ca7":"markdown","8488c110":"markdown","8d492326":"markdown","b889449b":"markdown","db8f5cc7":"markdown","9c6b5d91":"markdown","92ebdade":"markdown","bb7992ad":"markdown","58cb3f69":"markdown","fa27f445":"markdown","cf6c3c69":"markdown","f7480011":"markdown","0bb34d44":"markdown","8b8802e6":"markdown","edf68d1e":"markdown","c9d152a5":"markdown","4ac45feb":"markdown","f82190ed":"markdown","48601680":"markdown","99ed8057":"markdown","0c64f287":"markdown","68e2f55a":"markdown","fe2ed62e":"markdown"},"source":{"62e2803d":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport spacy\nfrom spacy import displacy\nimport nltk\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nfrom sklearn.metrics import plot_precision_recall_curve,plot_roc_curve","03f1c1e1":"text = pd.read_csv('..\/input\/positiveornegative\/text.csv')\ntext.drop('Unnamed: 0',axis=1,inplace=True)\ntext.head()","ef0b01ef":"text.dtypes","8f1cb0be":"nlp = spacy.load('en_core_web_sm')\nstopword = nltk.corpus.stopwords.words('english')\ndef text_cleaning(text):\n    text = re.sub(r'[^\\w\\s]', '',str(text))             #Punctuations\n    text=re.split(\"\\W+\",text)                           #Tokenizing\n    text=[word for word in text if word not in stopword]#Stop words\n    text = ' '.join(text)                              \n    return text\n","1348d7f1":"text.review = text.review.apply(lambda x :text_cleaning(x))\ntext","49520e43":"def missing_values(dataframe): \n    drop_list = []  \n    for i,j,k in dataframe.itertuples(): \n        if type(k)==str:            \n            if k.isspace():         \n                drop_list.append(i)\n\n    dataframe.drop(drop_list,axis=0,inplace=True)\n    return dataframe","d0180e45":"text = missing_values(text)","2b032bb5":"#length of our data set \nlen(text)","191da9fb":"text_instance = nlp(u'Mark Zuckerberg is one of the founders of Facebook, a company from the United States\u201d')\nfor sentence in text_instance.sents:\n    docx = nlp(sentence.text)\n    if docx.ents:\n        displacy.render(docx, style='ent', jupyter=True)\n    else:\n        print(docx.text)","3dcf9585":"text_instance = nlp(u\"Apple's first company logo featured a drawing of the father of physics, Sir Isaac Newton. To raise capital for Apple, co-founder Steve Wozniak had to sell his scientific calculator.\")\nfor sentence in text_instance.sents:\n    docx = nlp(sentence.text)\n    if docx.ents:\n        displacy.render(docx, style='ent', jupyter=True)\n    else:\n        print(docx.text)","db79c2ba":"text_instance = nlp(u\" There are 32 teams in the NFL, each vying for the Super Bowl win at the end of the season. The first game to be televised was between the Philadelphia Eagles and the Brooklyn Dodgers in 1939. There were approximately 500 television sets in new York able to play the game.\")\nfor sentence in text_instance.sents:\n    docx = nlp(sentence.text)\n    if docx.ents:\n        displacy.render(docx, style='ent', jupyter=True)\n    else:\n        print(docx.text)","3ffd9f50":"sns.countplot(x=\"label\",data=text)","21069843":"cloud_data = [\"NLP\",\"CNN\",\"ANN\",\"RNN\",\"Deep Learning\",\"Machine Learning\",\"OpenCV\",\"Tokenizing\",\"StopWords\",\"Punctuations\",\"TFIDF\",\"CountVector\",\"NLPTK\",\"Pipeline\",\"Performance Metrics\"]\nwordcloud = WordCloud(width = 400, height = 400,\n                background_color ='white',\n                stopwords = stopword,\n                min_font_size = 10).generate(' '.join(cloud_data))\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","a24a5300":"input_data = text['review']\noutput_data = text['label']\n\ntrain_data, test_data, train_output, test_output = train_test_split(input_data, output_data, test_size=0.2, random_state=101)","73d76096":"tfidf = TfidfVectorizer()\ntfidf_text_train = tfidf.fit_transform(train_data.values.astype('U'))\ntfidf_text_test = tfidf.transform(test_data.values.astype('U'))","ad5c4da1":"text_example = 'This is very good example'\nprint(tfidf.transform([text_example]))","83b3e68e":"linear_svc = LinearSVC()\nlinear_svc.fit(tfidf_text_train,train_output)\npredictions = linear_svc.predict(tfidf_text_test)","dbc4c122":"positive_or_negative = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('linear_svc', LinearSVC()),])\n\npositive_or_negative.fit(train_data,train_output)","00f2591a":"pred = positive_or_negative.predict(test_data)","b5a6a919":"plot_confusion_matrix(linear_svc,tfidf_text_test,test_output)","aed95c56":"print(classification_report(test_output,pred))","f3484509":"plot_precision_recall_curve(linear_svc,tfidf_text_test,test_output)","c2dad5fd":"plot_roc_curve(linear_svc,tfidf_text_test,test_output)","80ee5bc5":"single_test_text = \"very bad movie. Don't watch it\"\npositive_or_negative.predict([single_test_text])","64bacd16":"single_test_text = \"It is very delicious. I strongly recommend that\"\npositive_or_negative.predict([single_test_text])","c19de1fa":"<img src=\"https:\/\/pa1.narvii.com\/6292\/ce507693b01eb658851d77a7b601902509c63b03_hq.gif\">","437ca0ac":">  APPLYING FUNCTION TO OUR TEXT DATA","35fbf798":"<img src=\"https:\/\/miro.medium.com\/max\/714\/1*wgKxsWlT3ifsZmNy-DihFQ.png\">","b41d190a":"# Named Entity Recognition \n\nNamed entity recognition (NER) helps you easily identify the key elements in a text, like names of people, places, brands, monetary values, and more. Extracting the main entities in a text helps sort unstructured data and detect important information, which is crucial if you have to deal with large datasets","94ee7046":"> Lets see how it works","11d66548":"> Now we have ready dataset for model but before this lets explore some awesome visualization. What can we do with words?\n\n<img src=\"https:\/\/thumbs.gfycat.com\/CrazyIllinformedAstrangiacoral-size_restricted.gif\">","d4f7e97b":"### LETS GO! ","d645910a":"### TEST AND TRAIN","087dd62d":"#### Instead of using both TF IDF and SVM apart from each other we can use pipeline for doing both at the same time. ","6f861ca7":"# MODEL","8488c110":"> It seems like model works well","8d492326":"# VISUALIZATION","b889449b":"Classes : \n* negative(neg)\n* positive(pos)","db8f5cc7":"# NLP WITH TF IDF","9c6b5d91":"### Our dataset consists of reviews and their types. We are trying to find the sentence is negative or positive\n\nColumns: \n> Review : how do films like mouse hunt get into theatres ? isn't there a law or something ? \n\n> Label  :  negative(neg)","92ebdade":"# SINGLE TEST","bb7992ad":"#### CHECKING TYPES","58cb3f69":"### I hope you enjoy \ud83d\ude0a","fa27f445":"# PIPELINE","cf6c3c69":"# TEXT CLEANING\n#### 1.Punctuations are often unnecessary as it doesn\u2019t add value or meaning to the NLP model.\n> Example of punctuations : !\"#,,%,&,',(),*,+, -.\/,:;<,=,>,? etc.\n#### 2.Tokenizing is the process of splitting strings into a list of words. We will make use of Regular Expressions or regex to do the splitting.Regex can be used to describe a search pattern.\n> Example for tokenizing : [ 'I love Programming' ]->[ 'I' , 'love' , 'Programming' ] \n#### 3.Stop words are irrelevant words that won\u2019t help in identifying a text as NEGATIVE or POSITIVE. \n> Example for stopword : 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"","f7480011":"<img src=\"https:\/\/ichi.pro\/assets\/images\/max\/724\/0*C-cPP9D2MIyeexAT.gif\">","0bb34d44":"In this research,We are going to use TfidfVectorizer which assigns a vector to each word. Also, we'll look up how to use named entity recognizer, worldcloud.","8b8802e6":"### Word Cloud Usage","edf68d1e":"# PERFORMANCE METRICS","c9d152a5":"Just change cloud_data with your own words and make your own wordcloud \ud83d\ude0a\u270c\ud83c\udffb","4ac45feb":"> LETS DEFINE FUNCTION FOR MISSING VALUES","f82190ed":"### We are good so far. \ud83d\udc4c\ud83c\udffb","48601680":"# LIBRARIES","99ed8057":"> We have same amount of input for both label","0c64f287":"# DATA SET","68e2f55a":"### IMPORT DATA","fe2ed62e":"#### TfidfVectorizer TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents. It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).\n\n\n\n<img src= \"https:\/\/miro.medium.com\/max\/3604\/1*qQgnyPLDIkUmeZKN2_ZWbQ.png\">"}}