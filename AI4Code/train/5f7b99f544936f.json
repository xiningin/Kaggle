{"cell_type":{"1bcbec9e":"code","e7781001":"code","dcfb073d":"code","75e77973":"code","1ec6f4e4":"code","da2a7da3":"code","247d52ab":"code","e8ca67b8":"code","bd96779e":"code","ef52c3c6":"code","837c070d":"code","378c501b":"code","aac56216":"code","2f95e190":"markdown","7e61a715":"markdown"},"source":{"1bcbec9e":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\nimport torch.optim as optim\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","e7781001":"news = pd.read_csv(\"..\/input\/real-and-fake-news-dataset\/news.csv\")\nnews.drop('Unnamed: 0', axis=1, inplace=True)\nnews['titletext'] = news['title'] + \" \" + news['text']\nnews.head()","dcfb073d":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(news[['title','text','titletext']],news['label'],stratify=news['label'],test_size=0.3)\nX_test,X_valid,y_test,y_valid = train_test_split(X_test,y_test,stratify=y_test,test_size=0.5)","75e77973":"print(f\"Train Size: {X_train.shape} {y_train.shape}\")\nprint(f\"Test Size: {X_test.shape} {y_test.shape}\")\nprint(f\"Valid Size: {X_valid.shape} {y_valid.shape}\")","1ec6f4e4":"X_train['label'] = y_train.values\nX_test['label'] = y_test.values\nX_valid['label'] = y_valid.values","da2a7da3":"X_train.to_csv(\"train.csv\")\nX_test.to_csv(\"test.csv\")\nX_valid.to_csv(\"valid.csv\")","247d52ab":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# Model parameter\nMAX_SEQ_LEN = 128\nPAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nUNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n\nlabel_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\ntext_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\nfields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]","e8ca67b8":"%%timeit\n\n# TabularDataset\n\ntrain, valid, test = TabularDataset.splits(path=\"..\/input\/realfakenews-splitted\", train='train.csv', validation='valid.csv',\n                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n\n# Iterators\n\ntrain_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n                            device=device, train=True, sort=True, sort_within_batch=True)\nvalid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n                            device=device, train=True, sort=True, sort_within_batch=True)\ntest_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)","bd96779e":"class BERT(nn.Module):\n    def __init__(self):\n        super(BERT,self).__init__()\n        options_name = 'bert-base-uncased'\n        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n        \n        \n    def forward(self, text, label):\n        loss, text_fea = self.encoder(text, labels=label)[:2]\n\n        return loss, text_fea","ef52c3c6":"# Save and Load Functions\n\ndef save_checkpoint(save_path, model, valid_loss):\n    if save_path == None:\n        return\n    state_dict = {'model_state_dict': model.state_dict(),\n                  'valid_loss': valid_loss}\n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\ndef load_checkpoint(load_path, model):\n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    model.load_state_dict(state_dict['model_state_dict'])\n    return state_dict['valid_loss']","837c070d":"def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n    if save_path == None:\n        return\n    state_dict = {'train_loss_list': train_loss_list,\n                  'valid_loss_list': valid_loss_list,\n                  'global_steps_list': global_steps_list}\n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\n\ndef load_metrics(load_path):\n    if load_path==None:\n        return\n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n","378c501b":"def train(model, optimizer, critertion=nn.BCELoss(),train_loader=train_iter,valid_loader=valid_iter,num_epochs=5\n                   ,eval_every = len(train_iter) \/\/ 2,file_path = \"\",best_valid_loss = float(\"Inf\")):\n    # initialize running values\n    running_loss = 0.0\n    valid_running_loss = 0.0\n    global_step = 0\n    train_loss_list = []\n    valid_loss_list = []\n    global_steps_list = []\n    \n    model.train()\n    for epoch in range(num_epochs):\n        for (labels, title, text, titletext), _ in train_loader:\n            labels = labels.type(torch.LongTensor) \n            labels = labels.to(device)\n            \n            titletext = titletext.type(torch.LongTensor)  \n            titletext = titletext.to(device)\n            print(labels.shape)\n            print(titletext.shape)\n            \n            output = model(titletext, labels)\n            loss, _ = output\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            global_step += 1\n            \n            if global_step % eval_every == 0:\n                model.eval()\n                with torch.no_grad():                    \n                    # validation loop\n                    for (labels, title, text, titletext), _ in valid_loader:\n                        labels = labels.type(torch.LongTensor)           \n                        labels = labels.to(device)\n                        titletext = titletext.type(torch.LongTensor)  \n                        titletext = titletext.to(device)\n                        output = model(titletext, labels)\n                        loss, _ = output\n                        \n                        valid_running_loss += loss.item()\n                        \n                # evaluation\n                average_train_loss = running_loss \/ eval_every\n                average_valid_loss = valid_running_loss \/ len(valid_loader)\n                train_loss_list.append(average_train_loss)\n                valid_loss_list.append(average_valid_loss)\n                global_steps_list.append(global_step)\n                \n                # resetting running values\n                running_loss = 0.0                \n                valid_running_loss = 0.0\n                model.train()\n\n                # print progress\n                print('Epoch [{}\/{}], Step [{}\/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n                              average_train_loss, average_valid_loss))\n                \n                # checkpoint\n                if best_valid_loss > average_valid_loss:\n                    best_valid_loss = average_valid_loss\n                    save_checkpoint(file_path + model.pt, model, best_valid_loss)\n                    save_metrics(file_path + metrics.pt, train_loss_list, valid_loss_list, global_steps_list)\n    \n    save_metrics(file_path + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n    print('Finished Training!')","aac56216":"model = BERT().to(device)\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\n\ntrain(model=model, optimizer=optimizer)","2f95e190":"## Functions to Save, Load Checkpoint and Metrics","7e61a715":"Note:\n<span style='color:Red'>In order to use BERT tokenizer with TorchText, we have to set use_vocab=False and tokenize=tokenizer.encode. This will let TorchText know that we will not be building our own vocabulary using our dataset from scratch, but instead, use the pre-trained BERT tokenizer and its corresponding word-to-index mapping."}}