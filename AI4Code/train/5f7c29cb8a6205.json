{"cell_type":{"11005f77":"code","d7bc422c":"code","9775e480":"code","9000aab5":"code","494a44a5":"code","79a7053a":"code","295a2cd4":"code","94da7f90":"code","2aa3d5a1":"code","a4cfa70c":"code","8cf77d78":"code","3d3bff4c":"code","4d2751f7":"code","e740324e":"code","16a9ed47":"code","d2637eb7":"code","4f21a8f4":"code","9cb3c538":"code","b9ec0afe":"code","c0762679":"code","831cfad1":"code","3ea53e78":"code","c3fce1ec":"code","ef20d51b":"code","c0214467":"code","6ae71838":"code","2214f11f":"code","3f7fe37c":"code","f33b92eb":"code","afb67388":"code","48dd72ec":"code","0450e911":"code","19ab1fd4":"code","e536b869":"markdown"},"source":{"11005f77":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom bs4 import BeautifulSoup\nfrom nltk.stem import WordNetLemmatizer","d7bc422c":"pd.pandas.set_option('display.max_rows',None)\ndata = pd.read_csv('..\/input\/analyticvidhyadatasetsentiment\/train_F3WbcTw.csv')\ndata.head()","9775e480":"print('The size of the dataset is {}'.format(data.shape))\nprint('Total unique values in unique_hash is {}'.format(len(np.unique(data['unique_hash']))))","9000aab5":"data = data.iloc[:,1:4]\ndata.head()","494a44a5":"data['drug'].value_counts()","79a7053a":"data = data[(data['drug']=='ocrevus')|(data['drug']=='gilenya')|(data['drug']=='ocrelizumab')|(data['drug']=='entyvio')|(data['drug']=='humira')|(data['drug']=='fingolimod')|(data['drug']=='remicade')|(data['drug']=='opdivo')|(data['drug']=='tarceva')|(data['drug']=='cladribine')]\n#data = data[(data['drug']=='fingolimod')|(data['drug']=='remicade')|(data['drug']=='opdivo')|(data['drug']=='tarceva')|(data['drug']=='cladribine')]","295a2cd4":"data['drug'].value_counts()","94da7f90":"# segregating dataframe for analyzing individual condition\ndata_ocrevus = data[(data['drug']=='ocrevus')]\ndata_gilenya = data[(data['drug']=='gilenya')]\ndata_ocrelizumab = data[(data['drug']=='ocrelizumab')]\ndata_entyvio = data[(data['drug']=='entyvio')]\ndata_humira = data[(data['drug']=='humira')]\ndata_fingolimod = data[(data['drug']=='fingolimod')]\ndata_remicade = data[(data['drug']=='remicade')]\ndata_opdivo = data[(data['drug']=='opdivo')]\ndata_tarceva = data[(data['drug']=='tarceva')]\ndata_cladribine = data[(data['drug']=='cladribine')]","2aa3d5a1":"from wordcloud import WordCloud\nplt.figure(figsize = (20,20)) # Text that is Fake News Headlines\nwc = WordCloud(max_words = 500 , width = 1600 , height = 800).generate(\" \".join(data_ocrevus.text))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.title('Word cloud for )crevus',fontsize=14)","a4cfa70c":"from nltk.corpus import stopwords\n\n\nstop = stopwords.words('english')\nprint(stop)","8cf77d78":"def clean_data(raw_data):\n    # 1. Delete HTML \n    text = BeautifulSoup(raw_data, 'html.parser').get_text()\n    # 2. Make a space\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    # 3. lower letters\n    text = text.lower().split()\n    # 5. Stopwords \n    meaningful_text = [w for w in text if not w in stop]\n    # 6. lemmitization\n    lemmatizer = WordNetLemmatizer()\n    lemmitize_words = [lemmatizer.lemmatize(w) for w in meaningful_text]\n    # 7. space join words\n    return( ' '.join(lemmitize_words))","3d3bff4c":"data['updated_text'] = data['text'].apply(clean_data)","4d2751f7":"data.head()","e740324e":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nle.fit(['ocrevus','gilenya','ocrelizumab','entyvio','humira','fingolimod','remicade','opdivo','tarceva','cladribine'])\ndata['drug']=le.transform(data['drug'])","16a9ed47":"x = data['updated_text']\ny = data['drug']\n#y=pd.get_dummies(data['drug'])","d2637eb7":"#Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 42)\nprint(f'Rows in train set: {len(x_train)}\\nRows in test set: {len(x_test)}')","4f21a8f4":"#Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.15, random_state = 42)\nprint(f'Rows in train set: {len(x_train)}\\nRows in valid set: {len(x_valid)}')","9cb3c538":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=2500)\nx_train = cv.fit_transform(x_train).toarray()\nx_valid = cv.fit_transform(x_valid).toarray()\nx_test = cv.fit_transform(x_test).toarray()","b9ec0afe":"import xgboost as xgb\nxgb_classifier = xgb.XGBClassifier(learning_rate=0.01, min_child_weight=3, n_estimators=300, reg_alpha = 0.3, \n                                 subsample=0.8, random_state=42, gamma=5, max_depth=2)\nxgb_classifier.fit(x_train,y_train,early_stopping_rounds=40, eval_set=[(x_valid, y_valid)])","c0762679":"#Visualizing the performace of the model\ny_pred_train = xgb_classifier.predict(x_train)\n\n#Printing the classification_report\nprint(classification_report(y_train, y_pred_train))\n\n#Printing the confusion_matrix\ncm = confusion_matrix(y_train , y_pred_train)\nprint(cm)\n\n#Printing the accuracy score\nacc_score = accuracy_score(y_pred_train , y_train)\nacc_score = acc_score*100\nprint('Accuracy of training set {}%'.format(acc_score))","831cfad1":"#Visualizing the performace of the model\ny_pred_test = xgb_classifier.predict(x_test)\n\n#Printing the classification_report\nprint(classification_report(y_test, y_pred_test))\n\n#Printing the confusion_matrix\ncm = confusion_matrix(y_test , y_pred_test)\nprint(cm)\n\n#Printing the accuracy score\nacc_score = accuracy_score(y_pred_test , y_test)\nacc_score = acc_score*100\nprint('Accuracy of training set {}%'.format(acc_score))","3ea53e78":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier_knn.fit(x_train, y_train)","c3fce1ec":"#Visualizing the performace of the model\ny_pred_train = classifier_knn.predict(x_train)\n\n#Printing the classification_report\nprint(classification_report(y_train, y_pred_train))\n\n#Printing the confusion_matrix\ncm = confusion_matrix(y_train , y_pred_train)\nprint(cm)\n\n#Printing the accuracy score\nacc_score = accuracy_score(y_pred_train , y_train)\nacc_score = acc_score*100\nprint('Accuracy of training set {}%'.format(acc_score))","ef20d51b":"#Visualizing the performace of the model\ny_pred_test = classifier_knn.predict(x_test)\n\n#Printing the classification_report\nprint(classification_report(y_test, y_pred_test))\n\n#Printing the confusion_matrix\ncm = confusion_matrix(y_test , y_pred_test)\nprint(cm)\n\n#Printing the accuracy score\nacc_score = accuracy_score(y_pred_test , y_test)\nacc_score = acc_score*100\nprint('Accuracy of training set {}%'.format(acc_score))","c0214467":"text = 'Autoimmune diseases tend to come in clusters. As for Gilenya \u2013 if you feel good, don\u2019t think about'","6ae71838":"from textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\ntb = TextBlob(text, analyzer = NaiveBayesAnalyzer())\ntb.sentiment","2214f11f":"data.head()","3f7fe37c":"data.iloc[0][0]","f33b92eb":"data_new = data.head(20)\ndata_new.iloc[:,3]","afb67388":"sentiment_score = []\n\nfor i in range(0,data_new.shape[0]):\n    score = TextBlob(data_new.iloc[i][3], analyzer=NaiveBayesAnalyzer())\n    #score = score[0]\n    sentiment_score.append(score)","48dd72ec":"data_text = data_new['updated_text']","0450e911":"data_final = pd.concat([data_text, pd.Series(sentiment_score)],axis=1)","19ab1fd4":"data_final","e536b869":"### Seperating the dependent and independet features"}}