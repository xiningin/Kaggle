{"cell_type":{"e85d8eb6":"code","1e48aeb8":"code","bcd9eb96":"code","c0707150":"code","7138c284":"code","a5e3874f":"code","a5d772c4":"code","0d21a54f":"code","3d7b8933":"code","5100ed3b":"code","05bdfc32":"code","2e1c77d9":"code","96df4a79":"code","c5708f5e":"code","720f74f0":"code","ca8b3b56":"code","8589eb34":"code","d8a82028":"code","c47e6adb":"code","b370faaf":"code","bcb8329c":"code","2c2a591f":"code","d5a4980d":"code","ab74b8ad":"markdown","f9f2998e":"markdown","92922c1f":"markdown","d545fcf4":"markdown","71d96f3f":"markdown","2cef7270":"markdown","d34ad742":"markdown","3768838b":"markdown","9481c17f":"markdown","8c62b743":"markdown","44847b98":"markdown","a21bf468":"markdown","ac39d91c":"markdown","9f00deb2":"markdown","d047bdb2":"markdown","24a9e8cf":"markdown","43be714b":"markdown","373316d5":"markdown","f9b137ab":"markdown"},"source":{"e85d8eb6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport gc; import os\nimport torch\nfrom torch.nn import *\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom tqdm.notebook import tqdm","1e48aeb8":"import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence","bcd9eb96":"def tile(a, dim, n_tile):\n    init_dim = a.size(dim)\n    repeat_idx = [1] * a.dim()\n    repeat_idx[dim] = n_tile\n    a = a.repeat(*(repeat_idx))\n    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n    return torch.index_select(a, dim, order_index)","c0707150":"class RelMultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False):\n        super(RelMultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.scale = 1 \/ (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n    def _parallelogram_mask(self, h, w, left=False):\n        mask = torch.ones((h, w)).byte()\n        m = min(h, w)\n        mask[:m,:m] = torch.triu(mask[:m,:m])\n        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n\n        if left:\n            return mask\n        else:\n            return mask.flip(0)\n\n    def _shift(self, x, qlen, klen, mask, left=False):\n        if qlen > 1:\n            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n                                    device=x.device, dtype=x.dtype)\n        else:\n            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n\n        if left:\n            mask = mask.flip(1)\n            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n        else:\n            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n\n        x = x_padded.masked_select(mask[:,:,None,None]) \\\n                    .view(qlen, klen, x.size(2), x.size(3))\n\n        return x\n\n    def _rel_shift(self, x, zero_triu=False):\n        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n                               device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n\n        x = x_padded[1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(0), x.size(1)))\n            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        raise NotImplementedError\n\nclass RelPartialLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelPartialLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n\n    def forward(self, w, r, r_w_bias, r_r_bias, attn_mask=None, mems=None):\n        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n\n        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + r_r_bias\n        BD = torch.einsum('ibnd,jnd->ijbn', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[None,:,:,None], -float('inf')).type_as(attn_score)\n            elif attn_mask.dim() == 3:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass RelLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n    def forward(self, w, r_emb, r_w_bias, r_bias, attn_mask=None, mems=None):\n        # r_emb: [klen, n_head, d_head], used for term B\n        # r_w_bias: [n_head, d_head], used for term C\n        # r_bias: [klen, n_head], used for term D\n\n        qlen, bsz = w.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)\n\n        if klen > r_emb.size(0):\n            r_emb_pad = r_emb[0:1].expand(klen-r_emb.size(0), -1, -1)\n            r_emb = torch.cat([r_emb_pad, r_emb], 0)\n            r_bias_pad = r_bias[0:1].expand(klen-r_bias.size(0), -1)\n            r_bias = torch.cat([r_bias_pad, r_bias], 0)\n        else:\n            r_emb = r_emb[-klen:]\n            r_bias = r_bias[-klen:]\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias[None]                                   # qlen x bsz x n_head x d_head\n\n        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n        B_ = torch.einsum('ibnd,jnd->ijbn', (w_head_q, r_emb))                  # qlen x klen x bsz x n_head\n        D_ = r_bias[None, :, None]                                              # 1    x klen x 1   x n_head\n        BD = self._rel_shift(B_ + D_)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float('inf'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float('inf'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output","7138c284":"class PositionwiseLSTMFF(torch.nn.Module):\n    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False):\n        super(PositionwiseLSTMFF, self).__init__()\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.CoreNet = nn.Sequential(\n            nn.Linear(d_model, d_inner), \n            nn.LSTMCell(d_model, d_inner),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(d_inner, d_model),\n            nn.Dropout(dropout),\n        )\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, inp):\n        if self.pre_lnorm:\n            # layer normalization + positionwise feed-forward\n            core_out = self.CoreNet(self.layer_norm(inp))\n\n            # residual connection\n            output = core_out + inp\n        else:\n            # positionwise feed-forward\n            core_out = self.CoreNet(inp)\n\n            # residual connection + layer normalization\n            output = self.layer_norm(inp + core_out)\n\n        return output","a5e3874f":"class PositionwiseGRUFF(torch.nn.Module):\n    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False):\n        super(PositionwiseGRUFF, self).__init__()\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.CoreNet = nn.Sequential(\n            nn.Linear(d_model, d_inner), \n            nn.GRUCell(d_model, d_inner),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(d_inner, d_model),\n            nn.Dropout(dropout),\n        )\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, inp):\n        if self.pre_lnorm:\n            # layer normalization + positionwise feed-forward\n            core_out = self.CoreNet(self.layer_norm(inp))\n\n            # residual connection\n            output = core_out + inp\n        else:\n            # positionwise feed-forward\n            core_out = self.CoreNet(inp)\n\n            # residual connection + layer normalization\n            output = self.layer_norm(inp + core_out)\n\n        return output","a5d772c4":"# Create Attention mask\ndef create_mask(qlen, mlen, dtype=torch.float32, same_length=False):\n      \"\"\"Creates attention mask when single-side context allowed only.\"\"\"\n      attn_mask = torch.ones([qlen, qlen], dtype=dtype)\n      mask_u = torch.triu(attn_mask, 0, -1)\n      mask_dia = torch.triu(attn_mask, 0, 0)\n      attn_mask_pad = torch.zeros([qlen, mlen], dtype=dtype)\n      ret = torch.concat([attn_mask_pad, mask_u - mask_dia], 1)\n      if same_length:\n        mask_l = torch.triu(attn_mask, -1, 0)\n        ret = torch.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n\n      return ret","0d21a54f":"class PositionalEmbedding(Module):\n  def __init__(self, dim, **kwargs):\n    super(PositionalEmbedding, self).__init__(**kwargs)\n    self.dim = dim\n\n    \"\"\"Constructs inversed frequency vector for positional embedding layer.\"\"\"\n    self.inv_freq = 1.0 \/ (10000.0**(torch.range(0, 19380, 10.0) \/ self.dim))\n\n  def forward(self, pos_seq, batch_size):\n    \"\"\"Implements call() for the layer.\"\"\"\n    sinusoid_inp = torch.einsum('i,d->id', pos_seq, self.inv_freq)\n    pos_emb = torch.cat([torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], -1)\n    pos_emb = pos_emb[:, None, :]\n\n    if batch_size is not None:\n      pos_emb = tile(pos_emb, 2, self.dim)\n\n    return pos_emb","3d7b8933":"## Took this from the TF GitHub repo and translated into Pytorch ##\nclass RelativeAttention(Module):\n  \"\"\"Core calculations for relative attention.\"\"\"\n\n  def __init__(self, dropout_att, scale):\n    super(RelativeAttention, self).__init__()\n    self.scale = scale\n    self.dropout_att = dropout_att\n\n  def build(self, unused_input_shapes):\n    self.attention_probs_dropout = torch.nn.Dropout(\n        p=self.dropout_att)\n\n    super(RelativeAttention, self).build(unused_input_shapes)\n\n  def call(self, q_head, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat,\n           r_w_bias, r_r_bias, r_s_bias, attn_mask):\n    # content based attention score\n    ac = torch.einsum('ibnd,jbnd->ijbn', q_head + r_w_bias, k_head_h)\n\n    # position based attention score\n    bd = torch.einsum('ibnd,jbnd->ijbn', q_head + r_r_bias, k_head_r)\n    bd = rel_shift(bd, klen=tf.shape(ac)[1])\n\n    # segment-based attention score\n    if seg_mat is None:\n      ef = 0\n    else:\n      ef = torch.einsum('ibnd,snd->isbn', q_head + r_s_bias, seg_embed)\n      tgt_shape = torch.shape(bd)\n      ef = torch.where(\n          torch.Tensor(np.broadcast_to(torch.expand_dims(seg_mat, 3), tgt_shape)),\n          torch.Tensor(np.broadcast_to(ef[:, 1:, :, :], tgt_shape)),\n          torch.Tensor(np.broadcast_to(ef[:, :1, :, :], tgt_shape)))\n\n    # merges attention scores and performs masking\n    attn_score = (ac + bd + ef) * self.scale\n    if attn_mask is not None:\n      attn_score = attn_score - 1e30 * attn_mask\n\n    # attention probability\n    attn_prob = functional.softmax(attn_score, 1)\n    attn_prob = self.attention_probs_dropout(attn_prob)\n\n    # attention output\n    attn_vec = torch.einsum('ijbn,jbnd->ibnd', attn_prob, v_head_h)","5100ed3b":"class MultiHeadAttn(torch.nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0, \n                 pre_lnorm=False):\n        super(MultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)\n        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.scale = 1 \/ (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, h, attn_mask=None, mems=None):\n        ##### multihead attention\n        # [hlen x bsz x n_head x d_head]\n\n        if mems is not None:\n            c = torch.cat([mems, h], 0)\n        else:\n            c = h\n\n        if self.pre_lnorm:\n            ##### layer normalization\n            c = self.layer_norm(c)\n\n        head_q = self.q_net(h)\n        head_k, head_v = torch.chunk(self.kv_net(c), 2, -1)\n\n        head_q = head_q.view(h.size(0), h.size(1), self.n_head, self.d_head)\n        head_k = head_k.view(c.size(0), c.size(1), self.n_head, self.d_head)\n        head_v = head_v.view(c.size(0), c.size(1), self.n_head, self.d_head)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = torch.einsum('ibnd,jbnd->ijbn', (head_q, head_k))\n        attn_score.mul_(self.scale)\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float('inf'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float('inf'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        # [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -> [qlen x bsz x n_head x d_head]\n        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, head_v))\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = h + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(h + attn_out)\n\n        return output\n\nclass RelMultiHeadAttn(torch.nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False):\n        super(RelMultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.scale = 1 \/ (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n    def _parallelogram_mask(self, h, w, left=False):\n        mask = torch.ones((h, w)).byte()\n        m = min(h, w)\n        mask[:m,:m] = torch.triu(mask[:m,:m])\n        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n\n        if left:\n            return mask\n        else:\n            return mask.flip(0)\n\n    def _shift(self, x, qlen, klen, mask, left=False):\n        if qlen > 1:\n            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n                                    device=x.device, dtype=x.dtype)\n        else:\n            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n\n        if left:\n            mask = mask.flip(1)\n            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n        else:\n            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n\n        x = x_padded.masked_select(mask[:,:,None,None]) \\\n                    .view(qlen, klen, x.size(2), x.size(3))\n\n        return x\n\n    def _rel_shift(self, x, zero_triu=False):\n        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n                               device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n\n        x = x_padded[1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(0), x.size(1)))\n            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        raise NotImplementedError","05bdfc32":"def CBR(x, out_layer, kernel, stride, dilation):\n    x = torch.nn.Conv1d(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n    x = torch.nn.BatchNormalization()(x)\n    x = torch.nn.functional.Activation(\"relu\")(x)\n    return x\n\ndef se_block(x_in, layer_n):\n    x = torch.nn.GlobalAveragePooling1D()(x_in)\n    x = torch.nn.Dense(layer_n\/\/8, activation=\"relu\")(x)\n    x = torch.nn.Dense(layer_n, activation=\"sigmoid\")(x)\n    x_out=torch.nn.Multiply()([x_in, x])\n    return x_out\n\ndef resblock(x_in, layer_n, kernel, dilation, use_se=True):\n    x = CBR(x_in, layer_n, kernel, 1, dilation)\n    x = CBR(x, layer_n, kernel, 1, dilation)\n    if use_se:\n        x = se_block(x, layer_n)\n    x = torch.nn.Add()([x_in, x])\n    return x","2e1c77d9":"class TimeDistributed(torch.nn.Module):\n    def __init__(self, layer, time_steps, *args):        \n        super(TimeDistributed, self).__init__()\n        \n        self.layers = nn.ModuleList([layer(*args) for i in range(time_steps)])\n\n    def forward(self, x):\n\n        batch_size, time_steps, C, H, W = x.size()\n        output = torch.tensor([])\n        for i in range(time_steps):\n          output_t = self.layers[i](x[:, i, :, :, :])\n          output_t  = y.unsqueeze(1)\n          output = torch.cat((output, output_t ), 1)\n        return output","96df4a79":"class TransformerXLHybridEncoder(torch.nn.Module):\n  def __init__(self,\n               n_token,\n               n_layer,\n               d_model,\n               n_head,\n               d_head,\n               d_inner,\n               dropout,\n               dropout_att,\n               attn_type,\n               bi_data,\n               is_training,\n               initializer,\n               mem_len=None,\n               same_length=False,\n               clamp_len=-1,\n               untie_r=False,\n               use_tpu=True,\n               reuse_len=None,\n               ff_activation='relu',\n               use_cls_mask=False,\n               **kwargs):\n\n    super(TransformerXLHybridEncoder, self).__init__(**kwargs)\n\n    self.n_token = n_token\n    self.initializer = initializer\n    self.attn_type = attn_type\n    self.n_layer = n_layer\n    self.d_model = d_model\n    self.n_head = n_head\n    self.d_head = d_head\n    self.d_inner = d_inner\n    self.ff_activation = ff_activation\n    self.untie_r = untie_r\n    self.use_tpu = use_tpu\n    self.dropout = dropout\n    self.dropout_att = dropout_att\n\n    self.mem_len = mem_len\n    self.reuse_len = reuse_len\n    self.bi_data = bi_data\n    self.clamp_len = clamp_len\n    self.same_length = same_length\n    self.use_cls_mask = use_cls_mask\n\n  def build(self, unused_input_shapes):\n    \n    torch_float = torch.float32\n    tf_float = torch_float\n    self.inp = torch.nn.Input((64, 1))\n    self.cbr = CBR(self.inp, 64, 7, 1, 1)\n    self.embedding_lookup = EmbeddingLookup(\n        n_token=self.n_token,\n        d_embed=self.d_model,\n        initializer=self.initializer,\n        dtype=self.tf_float,\n        name='embedding1')\n\n    self.h_dropout = torch.nn.Dropout(p=self.dropout)\n    self.g_dropout = torch.nn.Dropout(p=self.dropout)\n\n    if self.untie_r:\n      self.r_w_bias = (\n          self.add_weight(\n              'r_w_bias',\n              shape=[self.n_layer, self.n_head, self.d_head],\n              dtype=self.torch_float,\n              initializer=self.initializer))\n      self.r_r_bias = (\n          self.add_weight(\n              'r_r_bias',\n              shape=[self.n_layer, self.n_head, self.d_head],\n              dtype=self.torch_float,\n              initializer=self.initializer))\n      self.r_s_bias = (\n          self.add_weight(\n              'r_s_bias',\n              shape=[self.n_layer, self.n_head, self.d_head],\n              dtype=self.torch_float,\n              initializer=self.initializer))\n    else:\n      self.r_w_bias = (\n          self.add_weight(\n              'r_w_bias',\n              shape=[self.n_head, self.d_head],\n              dtype=self.torch_float,\n              initializer=self.initializer))\n      self.r_r_bias = (\n          self.add_weight(\n              'r_r_bias',\n              shape=[self.n_head, self.d_head],\n              dtype=self.torch_float,\n              initializer=self.initializer))\n      self.r_s_bias = (\n          self.add_weight(\n              'r_s_bias', [self.n_head, self.d_head],\n              dtype=self.torch_float,\n              initializer=self.initializer))\n\n    self.seg_embed = self.add_weight(\n        'seg_embed', [self.n_layer, 2, self.n_head, self.d_head],\n        dtype=self.torch_float,\n        initializer=self.initializer)\n\n    self.mask_emb = self.add_weight(\n        'mask_emb\/mask_emb', shape=[1, 1, self.d_model], dtype=self.torch_float)\n\n    self.emb_dropout = torch.nn.Dropout(p=self.dropout)\n    self.fwd_position_embedding = PositionalEmbedding(self.d_model)\n    self.fwd_td                 = TimeDistributed(self.fwd_position_embedding, time_steps=20)\n    self.fwd_lstm               = torch.nn.LSTM(128)(self.fwd_td)\n    self.hidden_vect_1 = (\n        Variable(torch.zeros(1, 1, hidden_size)),\n        Variable(torch.zeros(1, 1, hidden_size)))\n    self.output1, self.hidden1 = self.fwd_lstm(Variable(torch.rand(1, 5, 10)), hidden_vect_1)\n    self.bwd_position_embedding = PositionalEmbedding(self.d_model)\n    self.bwd_td                 = TimeDistributed(self.bwd_position_embedding, time_steps=20)\n    self.bwd_lstm               = torch.nn.LSTM(128)(self.bwd_td)\n    self.hidden_vect_1 = (\n        Variable(torch.zeros(1, 1, hidden_size)),\n        Variable(torch.zeros(1, 1, hidden_size)))\n    self.output2, self.hidden2 = self.bwd_lstm(Variable(torch.rand(1, 5, 10)), hidden_vect_1)\n\n    self.rel_multihead_layers = []\n    self.h_positionwise_ffn_layers = []\n    self.layer_norm_layers = []\n    for i in range(self.n_layer):\n      self.rel_multihead_layers.append(\n          RelMultiHeadAttn(\n              d_model=self.d_model,\n              dropout=self.dropout,\n              n_head=self.n_head,\n              d_head=self.d_head,\n              name='layer_%d\/rel_attn' % (i)))\n      self.h_positionwise_ffn_layers.append(\n          PositionwiseFF(\n              d_model=self.d_model,\n              d_inner=self.d_inner,\n              dropout=self.dropout,\n              kernel_initializer=self.initializer,\n              activation_type=self.ff_activation,\n              name='layer_%d\/ff' % (i)))\n\n    self.output_dropout = torch.nn.Dropout(p=self.dropout)\n    \n    def __call__(self,\n               inp_k,\n               seg_id=None,\n               input_mask=None,\n               mems=None,\n               perm_mask=None,\n               target_mapping=None,\n               inp_q=None,\n               **kwargs):\n    # Uses dict to feed inputs into call() in order to keep mems as a python\n    # list.\n        inputs = {\n        'inp_k': inp_k,\n        'seg_id': seg_id,\n        'input_mask': input_mask,\n        'mems': mems,\n        'perm_mask': perm_mask,\n        'target_mapping': target_mapping,\n        'inp_q': inp_q\n    }\n    return super(TransformerXLModel, self).__call__(inputs, **kwargs)\n\n  def call(self, inputs):\n    \"\"\"Implements call() for the layer.\"\"\"\n    inp_k = inputs['inp_k']\n    seg_id = inputs['seg_id']\n    input_mask = inputs['input_mask']\n    mems = inputs['mems']\n    perm_mask = inputs['perm_mask']\n    target_mapping = inputs['target_mapping']\n    inp_q = inputs['inp_q']\n\n    new_mems = []\n\n    bsz = torch.shape(inp_k)[1]\n\n    qlen = inp_k.shape.as_list()[0]\n\n    mlen = mems[0].shape.as_list()[0] if mems is not None else 0\n    klen = mlen + qlen\n\n    ##### Attention mask\n    # causal attention mask\n    if self.attn_type == 'uni':\n      attn_mask = _create_mask(qlen, mlen, self.tf_float, self.same_length)\n      # pylint: enable=protected-access\n      attn_mask = attn_mask[:, :, None, None]\n    elif self.attn_type == 'bi':\n      attn_mask = None\n    else:\n      raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n\n    # data mask: input mask & perm mask\n    if input_mask is not None and perm_mask is not None:\n      data_mask = input_mask[None] + perm_mask\n\n    elif input_mask is not None and perm_mask is None:\n      data_mask = input_mask[None]\n    elif input_mask is None and perm_mask is not None:\n      data_mask = perm_mask\n    else:\n      data_mask = None\n\n    if data_mask is not None:\n      # all mems can be attended to\n      mems_mask = torch.zeros([tf.shape(data_mask)[0], mlen, bsz],\n                           dtype=self.tf_float)\n      data_mask = torch.cat([mems_mask, data_mask], 1)\n      if attn_mask is None:\n        attn_mask = data_mask[:, :, :, None]\n      else:\n        attn_mask += data_mask[:, :, :, None]\n\n    if attn_mask is not None:\n      attn_mask = torch.cast(attn_mask > 0, dtype=self.tf_float)\n\n    if attn_mask is not None:\n      non_tgt_mask = -torch.eye(qlen, dtype=self.tf_float)\n      non_tgt_mask = torch.cat(\n          [tf.zeros([qlen, mlen], dtype=self.tf_float), non_tgt_mask], axis=-1)\n      non_tgt_mask = torch.cast(\n          (attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=self.tf_float)\n    else:\n      non_tgt_mask = None\n\n    word_emb_k = self.embedding_lookup(inp_k)\n\n    if inp_q is not None:\n      if target_mapping is not None:\n        word_emb_q = torch.tile(self.mask_emb,\n                             [tf.shape(target_mapping)[0], bsz, 1])\n      else:\n        inp_q_ext = inp_q[:, :, None]\n        word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n\n    output_h = self.h_dropout(word_emb_k)\n    output_g = None\n    if inp_q is not None:\n      output_g = self.g_dropout(word_emb_q)\n\n    ##### Segment embedding\n    if seg_id is not None:\n\n      # Convert `seg_id` to one-hot `seg_mat`\n\n      mem_pad = torch.zeros([mlen, bsz], dtype=tf.int32)\n\n      cat_id = torch.concat([mem_pad, seg_id], 0)\n\n      if self.use_cls_mask:\n        # `1` indicates not in the same segment [qlen x klen x bsz]\n        # seg_id: [qlen x bsz] & cat_id: [klen x bsz]\n        cls_mat = torch.logical_or(\n            torch.equal(seg_id, tf.constant([data_utils.SEG_ID_CLS]))[:, None],\n            torch.equal(cat_id, tf.constant([data_utils.SEG_ID_CLS]))[None, :])\n        seg_mat = torch.equal(seg_id[:, None], cat_id[None, :])\n        seg_mat = torch.logical_or(cls_mat, seg_mat)\n      else:\n        seg_mat = tf.logical_not(tf.equal(seg_id[:, None], cat_id[None, :]))\n    else:\n      seg_mat = None\n\n    dtype = self.tf_float\n    freq_seq = tf.range(0, self.d_model, 2.0)\n    if dtype is not None and dtype != tf.float32:\n      freq_seq = tf.cast(freq_seq, dtype=self.dtype)\n\n    if self.attn_type == 'bi':\n      beg, end = klen, -qlen\n    elif self.attn_type == 'uni':\n      beg, end = klen, -1\n    else:\n      raise ValueError('Unknown `attn_type` {}.'.format(self.attn_type))\n\n    if self.bi_data:\n      fwd_pos_seq = torch.range(beg, end, -1.0)\n      bwd_pos_seq = torch.range(-beg, -end, 1.0)\n\n      if dtype is not None and dtype != tf.float32:\n        fwd_pos_seq = torch.cast(fwd_pos_seq, dtype=dtype)\n        bwd_pos_seq = torxh.cast(bwd_pos_seq, dtype=dtype)\n\n      if self.clamp_len > 0:\n        fwd_pos_seq = torch.clip_by_value(fwd_pos_seq, -self.clamp_len,\n                                       self.clamp_len)\n        bwd_pos_seq = torch.clip_by_value(bwd_pos_seq, -self.clamp_len,\n                                       self.clamp_len)\n\n      if bsz is not None:\n        fwd_pos_emb = self.fwd_position_embedding(fwd_pos_seq, bsz \/\/ 2)\n        bwd_pos_emb = self.bwd_position_embedding(bwd_pos_seq, bsz \/\/ 2)\n      else:\n        fwd_pos_emb = self.fwd_position_embedding(fwd_pos_seq, None)\n        bwd_pos_emb = self.bwd_position_embedding(bwd_pos_seq, None)\n\n      pos_emb = tf.concat([fwd_pos_emb, bwd_pos_emb], axis=1)\n    else:\n      fwd_pos_seq = tf.range(beg, end, -1.0)\n      if dtype is not None and dtype != tf.float32:\n        fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)\n      if self.clamp_len > 0:\n        fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len,\n                                       self.lamp_len)\n\n      pos_emb = self.fwd_position_embedding(fwd_pos_seq, bsz)\n\n    pos_emb = self.emb_dropout(pos_emb)\n\n    if mems is None:\n      mems = [None] * self.n_layer\n    for i in range(self.n_layer):\n      # cache new mems\n      new_mems.append(\n          _cache_mem(output_h, mems[i], self.mem_len, self.reuse_len))\n      # pylint: enable=protected-access\n\n      # segment bias\n      if seg_id is None:\n        r_s_bias_i = None\n        seg_embed_i = None\n      else:\n        r_s_bias_i = self.r_s_bias if not self.untie_r else self.r_s_bias[i]\n        seg_embed_i = self.seg_embed[i]\n\n      ffn_layer = self.h_positionwise_ffn_layers[i]\n      attention_layer = self.rel_multihead_layers[i]\n      output_h, output_g = attention_layer(\n          h=output_h,\n          g=output_g,\n          r=pos_emb,\n          r_w_bias=self.r_w_bias if not self.untie_r else self.r_w_bias[i],\n          r_r_bias=self.r_r_bias if not self.untie_r else self.r_r_bias[i],\n          seg_mat=seg_mat,\n          r_s_bias=r_s_bias_i,\n          seg_embed=seg_embed_i,\n          attn_mask_h=non_tgt_mask,\n          attn_mask_g=attn_mask,\n          mems=mems[i],\n          target_mapping=target_mapping)\n      output_h = ffn_layer(output_h)\n      if output_g is not None:\n        output_g = ffn_layer(output_g)\n\n    if inp_q is not None:\n      output = output_g\n    else:\n      output = output_h\n\n    return output, new_mems, None","c5708f5e":"class AdaptiveEmbedding(torch.nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,\n                 sample_softmax=False):\n        super(AdaptiveEmbedding, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n\n        self.cutoffs = [cutoffs] + [n_token]\n        self.div_val = div_val\n        self.d_proj = d_proj\n\n        self.emb_scale = d_proj ** 0.5\n\n        self.cutoff_ends = [0] + [self.cutoffs]\n\n        self.emb_layers = nn.ModuleList()\n        self.emb_projs = nn.ParameterList()\n        if div_val == 1:\n            for i in range(1,self.n_token):\n                self.emb_layers.append(\n                    nn.Embedding(n_token, d_embed, sparse=(sample_softmax > 0))\n                )\n            if d_proj != d_embed:\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_embed)))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed \/\/ (div_val ** i)\n                self.emb_layers.append(nn.Embedding(r_idx-l_idx, d_emb_i))\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_emb_i)))\n\n    def forward(self, inp):\n        if self.div_val == 1:\n            for i in range(1,self.n_token):\n                embed = []\n                embed.append(self.emb_layers[i](inp))\n                if self.d_proj != self.d_embed:\n                    embed = F.linear(embed, self.emb_projs[0])\n        else:\n            param = next(self.parameters())\n            inp_flat = inp.view(-1)\n            emb_flat = torch.zeros([inp_flat.size(0), self.d_proj],\n                                   dtype=param.dtype, device=param.device)\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n\n                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n\n                if indices_i.numel() == 0:\n                    continue\n\n                inp_i = inp_flat.index_select(0, indices_i) - l_idx\n                emb_i = self.emb_layers[i](inp_i)\n                emb_i = F.linear(emb_i, self.emb_projs[i])\n\n                emb_flat.index_copy_(0, indices_i, emb_i)\n\n            embed = emb_flat.view(*inp.size(), self.d_proj)\n\n        embed.mul_(self.emb_scale)\n\n        return embed","720f74f0":"class GPT2OptimizedDecoderLayer(torch.nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout, hidden_size, **kwargs):\n        super(GPT2OptimizedDecoderLayer, self).__init__()\n\n        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseLSTMFF(d_model, d_inner, dropout,\n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n        self.dec_attn2 = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n        self.pos_gru_ff = PositionwiseGRUFF(d_model, d_inner, dropout,\n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n        \n    \n        ###### ATTENTION #####################################\n        # self.dec_attn3 = GPT2ParallelSelfAttn(hidden_size, )\n        ###MUST WORK ON GPT2 ATTENTION########################\n\n    def forward(self, dec_inp, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_gru_ff(output)\n        output = self.dec_attn2(dec_inp, attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\n\nclass RelLearnableDecoderLayer(torch.nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelLearnableMultiHeadAttn(n_head, d_model, d_head,\n                                                  dropout, **kwargs)\n        self.pos_gru_ff = PositionwiseGRUFF(d_model, d_inner, dropout,\n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n        self.dec_attn2 = RelLearnableMultiHeadAttn(n_head, d_model, d_head,\n                                                  dropout, **kwargs)\n        self.pos_ff = PositionwiseLSTMFF(d_model, d_inner, dropout,\n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n\n    def forward(self, dec_inp, r_emb, r_w_bias, r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r_emb, r_w_bias, r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_gru_ff(output)\n        output = self.dec_attn2(dec_inp, r_emb, r_w_bias, r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\n\nclass RelPartialLearnableDecoderLayer(torch.nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelPartialLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n                                                         d_head, dropout,\n                                                         **kwargs)\n        self.pos_ff = PositionwiseLSTMFF(d_model, d_inner, dropout,\n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n\n    def forward(self, dec_inp, r, r_w_bias, r_r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output","ca8b3b56":"class Transformer(torch.nn.Module):\n    def __init__(self, n_token, n_layer, n_head, d_model, d_head, d_inner,\n                 dropout, dropatt, dtype, attention_dropout_prob, output_dropout_prob, \n                 init_method, bi_data, tie_weight=True, d_embed=None,\n                 div_val=1, tie_projs=[False], pre_lnorm=False,\n                 tgt_len=10, ext_len=10, mem_len=10,\n                 cutoffs=[], adapt_inp=False,\n                 same_length=False, attn_type=2, clamp_len=-1,\n                 sample_softmax=-1):\n        super(Transformer, self).__init__()\n        self.n_token = n_token\n\n        d_embed = d_model if d_embed is None else d_embed\n        self.d_embed = d_embed\n        self.d_model = d_model\n        self.n_head = n_head\n        self.d_head = d_head\n        self.drop = nn.Dropout(dropout)\n\n        self.tie_weight = tie_weight\n        self.tie_projs = tie_projs\n        self.div_val = div_val\n\n        self.n_layer = n_layer\n\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n        self.max_klen = tgt_len + ext_len + mem_len\n\n        self.attn_type = attn_type\n        \n        self.word_emb = PositionalEmbedding(d_model)\n\n        self.layers = nn.ModuleList()\n        self.layers.append(\n            TransformerXLHybridEncoder(\n               n_token,\n               n_layer,\n               d_model,\n               n_head,\n               d_head,\n               d_inner,\n               dropout,\n               dropatt,\n               bi_data,\n               attn_type,\n               is_training=True,\n               initializer=torch.optim.SGD,\n            )\n        )\n        self.layers.append(\n            TransformerXLHybridEncoder(\n               n_token,\n               n_layer,\n               d_model,\n               n_head,\n               d_head,\n               d_inner,\n               dropout,\n               dropatt,\n               bi_data,               \n               attn_type,\n               is_training=True,\n               initializer=torch.optim.SGD,\n            )\n        )\n        self.layers.append(\n            TransformerXLHybridEncoder(\n               n_token,\n               n_layer,\n               d_model,\n               n_head,\n               d_head,\n               d_inner,\n               dropout,\n               dropatt,\n               bi_data,\n               attn_type,\n               is_training=True,\n               initializer=torch.optim.SGD,\n            )\n        )\n        # the default attention\n        if attn_type == 0:\n            for i in range(n_layer):\n                self.layers.append(\n                    RelPartialLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n        # learnable embeddings\n        elif attn_type == 1:\n            for i in range(n_layer):\n                self.layers.append(\n                    RelLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n        # absolute embeddings\n        elif attn_type in [2, 3]:\n            for i in range(n_layer):\n                self.layers.append(\n                    GPT2OptimizedDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm, hidden_size=16)\n                )\n        \n        self.sample_softmax = sample_softmax\n        # use sampled softmax\n        if sample_softmax > 0:\n            self.out_layer = nn.Linear(d_model, n_token)\n            self.tie_weight = tie_weight\n            self.sampler = LogUniformSampler(n_token, sample_softmax)\n        \n        \n        \n        # use adaptive softmax (including standard softmax)\n        else:\n            if tie_weight:\n                emb_layers = [i.weight for i in AdaptiveEmbedding(d_model, d_head, d_inner, n_head).emb_layers]\n            else:\n                emb_layers = None\n\n            emb_projs = AdaptiveEmbedding(d_model, d_head, d_inner, n_head).emb_projs\n\n        self.same_length = same_length\n        self.clamp_len = clamp_len\n\n        self._create_params()\n\n    def backward_compatible(self):\n        self.sample_softmax = -1\n    cutoffs=[]\n    \n    def _create_params(self):\n        # default attention\n        cutoffs=[]\n        if self.attn_type == 0:\n            self.pos_emb = AdaptiveEmbedding(self.n_token, self.d_embed, self.d_model, cutoffs,\n                                          div_val=self.div_val)\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        # learnable\n        elif self.attn_type == 1:\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.n_head, self.d_head))\n            self.r_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head))\n        # absolute standard\n        elif self.attn_type == 2:\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        # absolute deeper SA\n        elif self.attn_type == 3:\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.d_model))\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n\n    def init_mems(self):\n        if self.mem_len > 0:\n            mems = []\n            param = next(self.parameters())\n            for i in range(self.n_layer+1):\n                empty = torch.empty(0, dtype=param.dtype, device=param.device)\n                mems.append(empty)\n\n            return mems\n        else:\n            return None\n\n    def _update_mems(self, hids, mems, qlen, mlen):\n        # does not deal with None\n        if mems is None:\n            return None\n\n        # mems is not None\n        assert len(hids) == len(mems), 'len(hids) != len(mems)'\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n            beg_idx = max(0, end_idx - self.mem_len)\n            for i in range(len(hids)):\n\n                cat = torch.cat([mems[i], hids[i]], dim=0)\n                new_mems.append(cat[beg_idx:end_idx].detach())\n\n        return new_mems\n\n    def _forward(self, dec_inp, mems=None):\n        qlen, bsz = dec_inp.size()\n        true_size = 7\n\n        word_emb = PositionalEmbedding(dec_inp)\n\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n\n        # absolute\n        if self.attn_type == 2:\n            pos_seq = torch.LongTensor(torch.arange(klen - 1, -1, -1.0, dtype=torch.long))\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq, 64)\n\n            core_out = self.drop(pos_emb[-qlen:])\n            hids = []\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and len(mems_i) and i == 0:\n                    mems_i += pos_emb[:mlen]\n                core_out = core_out\n                hids.append(core_out)\n        elif self.attn_type == 3:\n            core_out = self.drop(word_emb)\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and len(mems_i) and mlen > 0:\n                    cur_emb = self.r_emb[i][:-qlen]\n                    cur_size = cur_emb.size(0)\n                    if cur_size < mlen:\n                        cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n                        cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n                    else:\n                        cur_emb = cur_emb[-mlen:]\n                    mems_i += cur_emb.view(mlen, 1, -1)\n                core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n                hids.append(core_out)\n\n        core_out = self.drop(core_out)\n\n        new_mems = self._update_mems(hids, mems, qlen, mlen)\n\n        return core_out, new_mems\n\n    def forward(self, data, target, mems):\n        # nn.DataParallel does not allow size(0) tensors to be broadcasted.\n        # So, have to initialize size(0) mems inside the model forward.\n        # Moreover, have to return new_mems to allow nn.DataParallel to piece\n        # them together.\n        if mems is None:\n            mems = self.init_mems()\n\n        tgt_len = target.size(0)\n        hidden, new_mems = self._forward(data, mems=None)\n\n        pred_hid = hidden[-tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            assert self.tie_weight\n            logit = sample_logits(self.word_emb, self.out_layer.bias, target,\n                                  pred_hid, self.sampler)\n            loss = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            loss = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target.view(-1))\n            loss = loss.view(tgt_len, -1)","8589eb34":"train = pd.read_csv('..\/input\/preprocessing-for-m5\/training.csv')\ntest = pd.read_csv('..\/input\/preprocessing-for-m5\/test.csv')","d8a82028":"from sklearn.model_selection import train_test_split as T\nfrom sklearn.preprocessing import OneHotEncoder as OHE\nX_train, y_train = T(train,test_size=0.1)\nX_test, y_test = T(test,test_size=0.1)","c47e6adb":"X = Transformer(\n    n_token=500,\n    n_layer=18,\n    n_head=8,\n    d_model=10,\n    d_head=8,\n    d_inner=12,\n    dropout=0.2,\n    dropatt=0.2,\n    dtype=torch.float32,\n    attention_dropout_prob=0.15,\n    output_dropout_prob=0.175,\n    init_method=torch.optim.SGD,\n    bi_data=10\n)","b370faaf":"X","bcb8329c":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(X.parameters(), lr=0.001, momentum=0.9)\ntrain = train.drop(['id'], axis=1)\ntrain = train.drop(['item_id'], axis=1)\ntrain = train.drop(['dept_id'], axis=1)\ntrain = train.drop(['cat_id'], axis=1)\ntrain = train.drop(['store_id'], axis=1)\ntrain = train.drop(['state_id'], axis=1)","2c2a591f":"X_train = X_train.drop(['id'], axis=1)\nX_train = X_train.drop(['item_id'], axis=1)\nX_train = X_train.drop(['dept_id'], axis=1)\nX_train = X_train.drop(['cat_id'], axis=1)\nX_train = X_train.drop(['store_id'], axis=1)\nX_train = X_train.drop(['state_id'], axis=1)\ny_train = y_train.drop(['id'], axis=1)\ny_train = y_train.drop(['item_id'], axis=1)\ny_train = y_train.drop(['dept_id'], axis=1)\ny_train = y_train.drop(['cat_id'], axis=1)\ny_train = y_train.drop(['store_id'], axis=1)\ny_train = y_train.drop(['state_id'], axis=1)\nX_train.to_csv('TRAIN_X.csv', index=False)\ndel X_train\ny_train.to_csv('TRAIN_Y.csv', index=False)\ndel y_train","d5a4980d":"class Transformer(torch.nn.Module):\n    def __init__(self, n_token, n_layer, n_head, d_model, d_head, d_inner,\n                 dropout, dropatt, dtype, attention_dropout_prob, output_dropout_prob, \n                 init_method, bi_data, tie_weight=True, d_embed=None,\n                 div_val=1, tie_projs=[False], pre_lnorm=False,\n                 tgt_len=10, ext_len=10, mem_len=10,\n                 cutoffs=[], adapt_inp=False,\n                 same_length=False, attn_type=2, clamp_len=-1,\n                 sample_softmax=-1):\n        super(Transformer, self).__init__()\n        self.n_token = n_token\n\n        d_embed = d_model if d_embed is None else d_embed\n        self.d_embed = d_embed\n        self.d_model = d_model\n        self.n_head = n_head\n        self.d_head = d_head\n        self.drop = nn.Dropout(dropout)\n\n        self.tie_weight = tie_weight\n        self.tie_projs = tie_projs\n        self.div_val = div_val\n\n        self.n_layer = n_layer\n\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n        self.max_klen = tgt_len + ext_len + mem_len\n\n        self.attn_type = attn_type\n        \n        self.word_emb = PositionalEmbedding(d_model)\n\n        self.layers = nn.ModuleList()\n        self.layers.append(\n            TransformerXLHybridEncoder(\n               n_token,\n               n_layer,\n               d_model,\n               n_head,\n               d_head,\n               d_inner,\n               dropout,\n               dropatt,\n               bi_data,\n               attn_type,\n               is_training=True,\n               initializer=torch.optim.SGD,\n            )\n        )\n        # the default attention\n        if attn_type == 0:\n            for i in range(n_layer):\n                self.layers.append(\n                    RelPartialLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n        # learnable embeddings\n        elif attn_type == 1:\n            for i in range(n_layer):\n                self.layers.append(\n                    RelLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n        # absolute embeddings\n        elif attn_type in [2, 3]:\n            for i in range(n_layer):\n                self.layers.append(\n                    GPT2OptimizedDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm, hidden_size=16)\n                )\n        \n        self.sample_softmax = sample_softmax\n        # use sampled softmax\n        if sample_softmax > 0:\n            self.out_layer = nn.Linear(d_model, n_token)\n            self.tie_weight = tie_weight\n            self.sampler = LogUniformSampler(n_token, sample_softmax)\n        \n        \n        \n        # use adaptive softmax (including standard softmax)\n        else:\n            if tie_weight:\n                emb_layers = [i.weight for i in AdaptiveEmbedding(d_model, d_head, d_inner, n_head).emb_layers]\n            else:\n                emb_layers = None\n\n            emb_projs = AdaptiveEmbedding(d_model, d_head, d_inner, n_head).emb_projs\n\n        self.same_length = same_length\n        self.clamp_len = clamp_len\n\n        self._create_params()\n\n    def backward_compatible(self):\n        self.sample_softmax = -1\n    cutoffs=[]\n    \n    def _create_params(self):\n        # default attention\n        cutoffs=[]\n        if self.attn_type == 0:\n            self.pos_emb = AdaptiveEmbedding(self.n_token, self.d_embed, self.d_model, cutoffs,\n                                          div_val=self.div_val)\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        # learnable BUT slower\n        \"\"\"\n        elif self.attn_type == 1:\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.n_head, self.d_head))\n            self.r_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head))\"\"\"\n        # absolute standard and faster\n        if self.attn_type == 2 or self.attn_type == 1:\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        # absolute deeper SA\n        elif self.attn_type == 3:\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.d_model))\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n\n    def init_mems(self):\n        if self.mem_len > 0:\n            mems = []\n            param = next(self.parameters())\n            for i in range(self.n_layer+1):\n                empty = torch.empty(0, dtype=param.dtype, device=param.device)\n                mems.append(empty)\n\n            return mems\n        else:\n            return None\n\n    def _update_mems(self, hids, mems, qlen, mlen):\n        # does not deal with None\n        if mems is None:\n            return None\n\n        # mems is not None\n        assert len(hids) == len(mems), 'len(hids) != len(mems)'\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n            beg_idx = max(0, end_idx - self.mem_len)\n            for i in range(len(hids)):\n\n                cat = torch.cat([mems[i], hids[i]], dim=0)\n                new_mems.append(cat[beg_idx:end_idx].detach())\n\n        return new_mems\n\n    def _forward(self, dec_inp, mems=None):\n        qlen, bsz = dec_inp.size()\n        true_size = 7\n\n        word_emb = PositionalEmbedding(dec_inp)\n\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n\n        # absolute\n        if self.attn_type == 2:\n            pos_seq = torch.LongTensor(torch.arange(klen - 1, -1, -1.0, dtype=torch.long))\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq, 64)\n\n            core_out = self.drop(pos_emb[-qlen:])\n            hids = []\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and len(mems_i) and i == 0:\n                    mems_i += pos_emb[:mlen]\n                core_out = core_out\n                hids.append(core_out)\n        elif self.attn_type == 3:\n            core_out = self.drop(word_emb)\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and len(mems_i) and mlen > 0:\n                    cur_emb = self.r_emb[i][:-qlen]\n                    cur_size = cur_emb.size(0)\n                    if cur_size < mlen:\n                        cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n                        cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n                    else:\n                        cur_emb = cur_emb[-mlen:]\n                    mems_i += cur_emb.view(mlen, 1, -1)\n                core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n                hids.append(core_out)\n\n        core_out = self.drop(core_out)\n\n        new_mems = self._update_mems(hids, mems, qlen, mlen)\n\n        return core_out, new_mems\n\n    def forward(self, data, target, mems):\n        # nn.DataParallel does not allow size(0) tensors to be broadcasted.\n        # So, have to initialize size(0) mems inside the model forward.\n        # Moreover, have to return new_mems to allow nn.DataParallel to piece\n        # them together.\n        if mems is None:\n            mems = self.init_mems()\n\n        tgt_len = target.size(0)\n        hidden, new_mems = self._forward(data, mems=None)\n\n        pred_hid = hidden[-tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            assert self.tie_weight\n            logit = sample_logits(self.word_emb, self.out_layer.bias, target,\n                                  pred_hid, self.sampler)\n            loss = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            loss = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target.view(-1))\n            loss = loss.view(tgt_len, -1)","ab74b8ad":"This is a huge and state of the art Attention module. We will be using many besides this as well.","f9f2998e":"### Embeddings and attention","92922c1f":"# Imports","d545fcf4":"# Transformer Hybrid\n\n---\n\nNotes pending\n\n---","71d96f3f":"My CSV files are in [this dataset](https:\/\/www.kaggle.com\/nxrprime\/preprocessing-for-m5) as well as the official M5 dataset.","2cef7270":" # Have to give credit to Nvidia and their wonderful GitHub repo.\n \n This is a hybridized Transformer-XL which also uses:\n * CBR Blocks\n * LSTM-base FFNs\n * Adaptive Embeddings\n\n \n This has adapted their work into PyTorch. I prefer TF, but PyTorch has the benefit of Open.Ai and their huge ecosystem (Pyro.ai etc).\n The TFXL was previously written in TF2.1, but I converted it into PyTorch. Why?\n \n ### The main reason?\n \n **MEGATRON-LM**\n \n The monstrous state-of-the-art transformer is the main reason why I am working with PyTorch. I myself only used the TFXL until now.","d34ad742":"---\n\n# My other works for this competition\n\nPlease check out these other works by me for this competition:\n+ https:\/\/www.kaggle.com\/nxrprime\/thoughts-on-creating-a-good-nn-model\n+ https:\/\/www.kaggle.com\/nxrprime\/preprocessing-fe\n+ https:\/\/www.kaggle.com\/nxrprime\/loss-functions\n+ https:\/\/www.kaggle.com\/nxrprime\/m5-feather-files-for-fast-loading-48x-faster","3768838b":"Unfortunately, the Kaggle kernels environment has very less memory. I cannot run the full NN on Kaggle kernels, so  it has been quicksaved.","9481c17f":"### Feed-forwards\n\nWe will implement these in our decoders.","8c62b743":"## Definition of functions","44847b98":"# A Full Training Pipeline\n\n### Link: https:\/\/github.com\/Trigram19\/m5-python-starter\n\nThis pipeline by no means is my complete version: it is a *distilled version.*","a21bf468":"### Megatron-LM GPT2 Layer (for later)","ac39d91c":"Just importing the necessary libraries.","9f00deb2":"Most ideas are taken from the NVIDIA repo (https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/blob\/master\/PyTorch\/LanguageModeling\/Transformer-XL\/pytorch\/mem_transformer.py). Credit to them.","d047bdb2":"This model is **TITANIC.** It is very, very big and has so many components like RNNs, positional encodings etc. But why? Why should I create such a monstrosity of titanic proportions and let it tango with the M5 dataset (which is by no means small)? The answer is simple: **this is to beat N-BEATS.** (despite being a long, long way off from that goal of triumph over Yoshua Bengio).\n\nMammoth models often tend to have **ONE bad egg in them**, so over here I'm guessing I'll need to fix the positional encoding. It is literally impossible to perform any sort of increase in the parameters beyond the optimum without a titanic amount of RAM. ","24a9e8cf":"# Why such a big model?","43be714b":"# Alternate (simpler) structure\n\nBasically take out all the hijinks from the model.","373316d5":"## Data wrangling ","f9b137ab":"A lot of people (i.e kkiller, KeepLearning etc.) have wanted this kernel so here it is."}}