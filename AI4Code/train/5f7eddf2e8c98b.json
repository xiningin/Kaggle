{"cell_type":{"3e7ae75b":"code","0c2fbbe8":"code","fcc2b2c8":"code","9e37748f":"code","91359b2e":"code","9bca3402":"code","6183b5d8":"code","034db39f":"code","ae4b2368":"code","61218ce0":"code","1d446091":"code","7eea591b":"code","ab161da5":"code","4ba51fa8":"code","ff4c44fc":"code","971d902b":"code","0b25c262":"code","a0de82a6":"code","f296b70d":"code","150664b0":"code","e7561d60":"code","66cf8941":"code","df94bfad":"code","8243762c":"code","923e18d7":"code","ec2d326e":"markdown","c3f21471":"markdown","7af0118a":"markdown","b4aa4582":"markdown","0d24e068":"markdown","3d8349e1":"markdown","808660b6":"markdown","a3843631":"markdown","3ef4ab72":"markdown","3b8c20a3":"markdown","83bbc065":"markdown","0edcd3a3":"markdown","33f51b44":"markdown","5b1bfa21":"markdown","8e2d0a31":"markdown","ee94c6cd":"markdown","1ebfe22f":"markdown","7d786ee6":"markdown","57af8596":"markdown","9e813026":"markdown","cbf220b3":"markdown","cc57fa1a":"markdown","8717582a":"markdown","21f0c3df":"markdown","652dba7b":"markdown"},"source":{"3e7ae75b":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport math\nfrom scipy import special #comb, factorial\nfrom keras import backend as K\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c2fbbe8":"sns.set_style('darkgrid')\ndf = pd.read_csv('\/kaggle\/input\/chess\/games.csv')\ndf.head()","fcc2b2c8":"dur = df[df['last_move_at']-df['created_at'] == 0].shape[0]\nprint(f'Number of games that had zero duration: {dur}, which makes \\\nup {round(dur\/df.shape[0],2)*100}% of all games')","9e37748f":"cols_to_drop = ['id','white_id','black_id','last_move_at','created_at','moves']\ndf.drop(cols_to_drop,axis=1,inplace=True)","91359b2e":"df['winner'].describe()","9bca3402":"df['winner'].value_counts()","6183b5d8":"df = df[df['winner'] != 'draw']","034db39f":"cat_features = np.array(['rated','victory_status','increment_code','opening_eco','opening_name'])\ncount = np.array([df[feature].unique().size for feature in cat_features])\n\nto_sort = np.argsort(count)[::-1]\ncat_features = cat_features[to_sort]\ncount = count[to_sort]\n\nplt.figure(figsize=(11,6))\nsns.barplot(cat_features,count)\nplt.title(\"Number of unique values per each feature\")\nplt.ylabel('Count')\nplt.xlabel('Feature')\nplt.show()","ae4b2368":"cat_features = np.array(['rated','victory_status','increment_code','opening_eco','opening_name'])\ncount = []\n\nfor feature in cat_features:\n    freq = 0\n    for value in df[feature].unique():\n        if df[df[feature] == value].shape[0] == 1:\n            freq+=1\n    count.append(freq)\n    \npd.DataFrame({'Feature': cat_features, 'Count of rare values': count})","61218ce0":"cont_features = ['white_rating','black_rating','opening_ply']\ndf[cont_features].describe().round(2).T","1d446091":"cont_features = ['white_rating','black_rating','opening_ply']\nWIDTH = 16\nLENGTH = 7\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(rows,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    ax[i].hist(df[feature],alpha=0.6)\n    ax[i].set_title(f'Distribution of a feature `{feature}`')","7eea591b":"cont_features = ['white_rating','black_rating','opening_ply']\ncat_variable = 'winner'\nWIDTH = 16\nLENGTH = 7\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(rows,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i])\n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","ab161da5":"from scipy.stats import f_oneway\n\ncont_features = ['white_rating','black_rating','opening_ply']\nlabel = 'winner'\n\ndic = {'Categorical': [],\n    'Numerical': [],\n    'p-value': [],\n    'p < 0.05': [],\n    'statistic': []}\n\n\nfor feature in cont_features:\n    values = []\n    for value in df[label].unique():\n        values.append(df[df[label] == value][feature].values)\n    \n    statistic, pval = f_oneway(*values)\n    \n    dic['Categorical'].append(label)\n    dic['Numerical'].append(feature)\n    dic['p-value'].append(pval)\n    dic['p < 0.05'].append(pval<0.05)\n    dic['statistic'].append(statistic)\n\n\npd.DataFrame(dic)","4ba51fa8":"df['rating_diff'] = df['white_rating']-df['black_rating']","ff4c44fc":"df['rating_diff'].describe()","971d902b":"dataframe = df\nfeature_1 = 'blueWins'\nfeature_2 = 'rating_diff'\nplt.figure(figsize=(7,7))\nsns.boxplot(y=feature_2, data=dataframe)\nplt.show()","0b25c262":"plt.figure(figsize=(10,7))\nplt.hist(df['rating_diff'],alpha=0.6)\nplt.title(\"Difference between white rating and black rating\")\nplt.xlabel('difference')\nplt.ylabel(\"count\")\nplt.show()","a0de82a6":"dataframe = df\nfeature_1 = 'winner'\nfeature_2 = 'rating_diff'\nplt.figure(figsize=(7,7))\nsns.boxplot(x=feature_1, y=feature_2, data=dataframe)\nplt.show()","f296b70d":"from scipy.stats import f_oneway\n\ncont_features = ['rating_diff']\nlabel = 'winner'\n\ndic = {'Categorical': [],\n    'Numerical': [],\n    'p-value': [],\n    'p < 0.05': [],\n    'statistic': []}\n\n\nfor feature in cont_features:\n    values = []\n    for value in df[label].unique():\n        values.append(df[df[label] == value][feature].values)\n    \n    statistic, pval = f_oneway(*values)\n    \n    dic['Categorical'].append(label)\n    dic['Numerical'].append(feature)\n    dic['p-value'].append(pval)\n    dic['p < 0.05'].append(pval<0.05)\n    dic['statistic'].append(statistic)\n\n\npd.DataFrame(dic)","150664b0":"highcardf = df[['increment_code','opening_eco','opening_name']].copy()\nsparse_high = OneHotEncoder().fit_transform(highcardf)\n","e7561d60":"from scipy import sparse\n\n\nnohighcar_df = df[['rated','victory_status',\n                   'turns','white_rating', \n                   'black_rating', 'opening_ply', \n                   'rating_diff']].copy()\n\n#Process categorical features\nnohighcar_df['rated'] = nohighcar_df['rated'].map({False: 0, True:1})\nnohighcar_df = pd.get_dummies(nohighcar_df)\n\n\n#Move all numerical features to the right of the dataframe\nnum_feat = df[['turns', 'white_rating', 'black_rating', 'opening_ply',\n       'rating_diff']]\nnohighcar_df.drop(['turns', 'white_rating', 'black_rating', 'opening_ply',\n       'rating_diff'],axis=1,inplace=True)\nnohighcar_df = pd.concat([nohighcar_df,num_feat],axis=1)\n\nnohigh_sparse = sparse.csr_matrix(nohighcar_df.values)","66cf8941":"X,y = sparse.hstack((sparse_high,nohigh_sparse)), df['winner']","df94bfad":"X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=11)\n\n\nsc = StandardScaler()\n\n\n\nleft = X_train[:,:-5]\nright = sparse.csr_matrix(sc.fit_transform(X_train[:,-5:].todense()))\nX_train = sparse.hstack((left,right)).tocsr()\n\n\nleft = X_test[:,:-5]\nright = sparse.csr_matrix(sc.transform(X_test[:,-5:].todense()))\nX_test = sparse.hstack((left,right)).tocsr()\n","8243762c":"log_random_state = None\nlog_clf = LogisticRegression(random_state=log_random_state,max_iter=500).fit(X_train, y_train)\nprint(classification_report(y_true=y_test, y_pred=log_clf.predict(X_test)))\nplot_confusion_matrix(log_clf, X_test, y_test)","923e18d7":"tree_clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\nprint(classification_report(y_true=y_test, y_pred=tree_clf.predict(X_test)))\nplot_confusion_matrix(tree_clf, X_test, y_test)","ec2d326e":"We see that most of the features have very high cardinality.","c3f21471":"One hot encoding features with high cardinality","7af0118a":"One can see that, the larger the value in `rating_diff`, the more advantage (rating-wise) white side has over black side. Let's see the summary of our new variable and the distribution.","b4aa4582":"As expected: The smaller the value (i.e., black side has higher rating), the less likely it is that white side will win (and vice versa). Let's use ANOVA to check whether there is a statistical signifance of the difference between the conditional distributions. ","0d24e068":"Now we will have a look at continuous features","3d8349e1":"As wee see, in most cases, games are relatively fair (i.e., both players have similar rating; more concretely $|\\text{white_rating} - \\text{black_rating}| \u2264 500$). But there is decent number of games where the discrepancy is relatively large.\n\nNow, let's have a look at how our new feature discerns the winner of the game.","808660b6":"In this analysis, we will restrict our attention only to those games where there is a winner (i.e., we will not conisder draws).","a3843631":"For each feature, we will calculate how many values occur only once.","3ef4ab72":"We see that even relatively simple model (with no hyperparameters tuning) gives us reasonable results.","3b8c20a3":"# Logistic regression","83bbc065":"The upshot is: We should remove the columns `last_move_at` and `created_at`.","0edcd3a3":"Now we will try to classify. The features we will be using are the folliwng:\n\n'increment_code', 'opening_eco', 'opening_name', 'rated', 'victory_status',\n                    'turns', 'white_rating', \n                   'black_rating', 'opening_ply', \n                   'rating_diff'","33f51b44":"Preprocessing continuous\/non-high cardinality categorical features","5b1bfa21":"There is one more thing: column `moves` should be removed. If particular game was ended by a check mate, then by using data from the column `moves` we can say with 100% certainty who won (esentially with data in `moves` we can reconstruct the whole game). Our main goal is to see whether we can accurately predict a winner based on LIMITED information.","8e2d0a31":"As expected, features with high cardinality have a lot of extremely rare values (i.e., values that occur only once).","ee94c6cd":"It seems that the difference between ratings is indeed a good predictor of a winner.","1ebfe22f":"Let's have a look at categorical features","7d786ee6":"Having prepared our data, let's try to classify. We will only use two models here: Logistic regression and decision trees.","57af8596":"Let's try to define new feature: difference between white_rating and black_rating, i.e., `white_rating`-`black_rating`.","9e813026":"The features alone doesn't seem to be doing good job at separating the winner. Let's use ANOVA test to verify independence.","cbf220b3":"Rather unexpectedly, the ANOVA test suggests that each feature and target variable are in fact dependent.","cc57fa1a":"Importing relevant modules","8717582a":"# Decision trees","21f0c3df":"Let's look at the label distribution (i.e., `winner`)","652dba7b":"Couple observations can be made:\n\n1. Columns `id`, `white_id`, `black_id` are not important.\n\n2. Either column `created_at` or `last_move_at` must contain some errors. Per [description](https:\/\/www.kaggle.com\/datasnaek\/chess) provided by the uploader, `created_at` stands for the time when the game began, and `last_move_at` stands for the time when the game ended. Hence $($ `last_move_at` $-$ `created_at` $)$ must represent a duration of the game. But according to the dataset, for some games, the duration is $0$ (which, given the cirumstances, is not possible). For example, consider the game with `id` TZJHLljE (first row in the dataset). The value in the column `created_at` is 1.504210e+12. And the value in `last_move_at` is also 1.504210e+12. But then it follows that (1.504210e+12)-(1.504210e+12)=0, implying that the duration of the game is zero. But also note that according to the dataset, the game ended because either of two players ran out of time (the value in the column `victory_status` equals to \"outoftime\"), which clearly contradicts the fact that the duration of the game was 0 seconds (or whatever metric was used to calculate time). Hence it is likely that either of two columns contains false number. In fact, we can see that there are numerous games that purportedly had zero duration:"}}