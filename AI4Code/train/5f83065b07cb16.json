{"cell_type":{"9ce3f890":"code","a8a17937":"code","fb15995c":"code","2e7a7efa":"code","8a609388":"code","b184465c":"code","668c1381":"code","3b4247d6":"code","3b9c5178":"code","fa9ab346":"code","7a1d5527":"code","b7cd8fbf":"code","7b8aa075":"code","2c968acb":"code","2fcebd5f":"code","d7f6719e":"code","85e23aa8":"code","67eed2a9":"code","1f13e154":"code","2fde2a0a":"code","095862ee":"code","0ade6770":"code","6853853c":"markdown","c602e87b":"markdown","8cb2f3ac":"markdown","6fad8df5":"markdown","8af35fb5":"markdown","a39de698":"markdown","410f7f38":"markdown"},"source":{"9ce3f890":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom time import perf_counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom IPython.display import Markdown, display\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","a8a17937":"# Create a list with the filepaths for training and testing\ndir_ = Path('..\/input\/natural-images\/')\nfilepaths = list(dir_.glob(r'**\/*.jpg'))","fb15995c":"def proc_img(filepath):\n    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"\/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1,random_state=0).reset_index(drop = True)\n    \n    return df\n\ndf = proc_img(filepaths)\n\nprint(f'Number of pictures: {df.shape[0]}\\n')\nprint(f'Number of different labels: {len(df.Label.unique())}\\n')\nprint(f'Labels: {df.Label.unique()}')\n\n# The DataFrame with the filepaths in one column and the labels in the other one\ndf.head(5)","2e7a7efa":"# Display the number of pictures of each category\nvc = df['Label'].value_counts()\nplt.figure(figsize=(9,5))\nsns.barplot(x = vc.index, y = vc, palette = \"rocket\")\nplt.title(\"Number of pictures of each category\", fontsize = 15)\nplt.show()","8a609388":"# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=4, ncols=10, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df.Filepath[i]))\n    ax.set_title(df.Label[i], fontsize = 12)\nplt.tight_layout(pad=0.5)\nplt.show()","b184465c":"# Training\/test split\n# Use only 20% on the pictures to speed up the training\ntrain_df,test_df = train_test_split(df.sample(frac=0.2), test_size=0.1,random_state=0)\n# train_df,test_df = train_test_split(df, test_size=0.1,random_state=0)","668c1381":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        rotation_range=30,\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30,\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","3b4247d6":"def get_model(model):\n# Load the pretained model\n    kwargs =    {'input_shape':(224, 224, 3),\n                'include_top':False,\n                'weights':'imagenet',\n                'pooling':'avg'}\n    \n    pretrained_model = model(**kwargs)\n    pretrained_model.trainable = False\n    \n    inputs = pretrained_model.input\n\n    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n\n    outputs = tf.keras.layers.Dense(8, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model","3b9c5178":"# Dictionary with the models\nmodels = {\n    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"DenseNet169\": {\"model\":tf.keras.applications.DenseNet169, \"perf\":0},\n    \"DenseNet201\": {\"model\":tf.keras.applications.DenseNet201, \"perf\":0},\n    \"EfficientNetB0\": {\"model\":tf.keras.applications.EfficientNetB0, \"perf\":0},\n    \"EfficientNetB1\": {\"model\":tf.keras.applications.EfficientNetB1, \"perf\":0},\n    \"EfficientNetB2\": {\"model\":tf.keras.applications.EfficientNetB2, \"perf\":0},\n    \"EfficientNetB3\": {\"model\":tf.keras.applications.EfficientNetB3, \"perf\":0},\n    \"EfficientNetB4\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB5\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB6\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB7\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"MobileNetV3Large\": {\"model\":tf.keras.applications.MobileNetV3Large, \"perf\":0},\n    \"MobileNetV3Small\": {\"model\":tf.keras.applications.MobileNetV3Small, \"perf\":0},\n#     \"NASNetLarge\": {\"model\":tf.keras.applications.NASNetLarge, \"perf\":0}, Deleted because the input shape has to be another one\n    \"NASNetMobile\": {\"model\":tf.keras.applications.NASNetMobile, \"perf\":0},\n    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n    \"ResNet101V2\": {\"model\":tf.keras.applications.ResNet101V2, \"perf\":0},\n    \"ResNet152\": {\"model\":tf.keras.applications.ResNet152, \"perf\":0},\n    \"ResNet152V2\": {\"model\":tf.keras.applications.ResNet152V2, \"perf\":0},\n    \"ResNet50\": {\"model\":tf.keras.applications.ResNet50, \"perf\":0},\n    \"ResNet50V2\": {\"model\":tf.keras.applications.ResNet50V2, \"perf\":0},\n    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n    \"VGG19\": {\"model\":tf.keras.applications.VGG19, \"perf\":0},\n    \"Xception\": {\"model\":tf.keras.applications.Xception, \"perf\":0}\n}\n\n# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()\nprint('\\n')\n\n# Fit the models\nfor name, model in models.items():\n    \n    # Get the model\n    m = get_model(model['model'])\n    models[name]['model'] = m\n    \n    start = perf_counter()\n    \n    # Fit the model\n    history = m.fit(train_images,validation_data=val_images,epochs=1,verbose=0)\n    \n    # Sav the duration and the val_accuracy\n    duration = perf_counter() - start\n    duration = round(duration,2)\n    models[name]['perf'] = duration\n    print(f\"{name:20} trained in {duration} sec\")\n    \n    val_acc = history.history['val_accuracy']\n    models[name]['val_acc'] = [round(v,4) for v in val_acc]","fa9ab346":"for name, model in models.items():\n    \n    # Predict the label of the test_images\n    pred = models[name]['model'].predict(test_images)\n    pred = np.argmax(pred,axis=1)\n\n    # Map the label\n    labels = (train_images.class_indices)\n    labels = dict((v,k) for k,v in labels.items())\n    pred = [labels[k] for k in pred]\n\n    y_test = list(test_df.Label)\n    acc = accuracy_score(y_test,pred)\n    models[name]['acc'] = round(acc,4)\n#     printmd(f'**{name} has a {acc * 100:.2f}% accuracy on the test set**')","7a1d5527":"# Create a DataFrame with the results\nmodels_result = []\n\nfor name, v in models.items():\n    models_result.append([ name, models[name]['val_acc'][-1], \n                          models[name]['acc'],\n                          models[name]['perf']])\n    \ndf_results = pd.DataFrame(models_result, \n                          columns = ['model','val_accuracy','accuracy','Training time (sec)'])\ndf_results.sort_values(by='accuracy', ascending=False, inplace=True)\ndf_results.reset_index(inplace=True,drop=True)\ndf_results","b7cd8fbf":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'accuracy', data = df_results)\nplt.title('Accuracy on the test set (after 1 epoch))', fontsize = 15)\nplt.ylim(0,1)\nplt.xticks(rotation=90)\nplt.show()","7b8aa075":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'Training time (sec)', data = df_results)\nplt.title('Training time for each model in sec', fontsize = 15)\n# plt.ylim(0,20)\nplt.xticks(rotation=90)\nplt.show()","2c968acb":"train_df,test_df = train_test_split(df, test_size=0.1, random_state=0)\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()","2fcebd5f":"model = get_model(tf.keras.applications.ResNet152V2)\nhistory = model.fit(train_images,validation_data=val_images,epochs=3)","d7f6719e":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","85e23aa8":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","67eed2a9":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]","1f13e154":"y_test = list(test_df.Label)\nacc = accuracy_score(y_test,pred)\nprintmd(f'# Accuracy on the test set: {acc * 100:.2f}%')","2fde2a0a":"class_report = classification_report(y_test, pred, zero_division=1)\nprint(class_report)","095862ee":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,7))\nsns.heatmap(cf_matrix, annot=False, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)),cbar=False)\nplt.title('Normalized Confusion Matrix', fontsize = 23)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","0ade6770":"# Display picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(20, 12),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i].split('_')[0]}\\nPredicted: {pred[i].split('_')[0]}\", fontsize = 15)\nplt.tight_layout()\nplt.show()","6853853c":"# 3. Test 27 canned architectures with pre-trained weights<a class=\"anchor\" id=\"3\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n\nMore info about the architectures under: [Module: tf.keras.applications](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=enhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=en)","c602e87b":"# 4. Train the architecture with the best result (ResNet152V2)<a class=\"anchor\" id=\"4\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n*Use all of the training data*","8cb2f3ac":"# Table of contents\n\n[<h3>1. Data preprocessing and visualization<\/h3>](#1)\n\n[<h3>2. Load the Images with a generator and Data Augmentation<\/h3>](#2)\n\n[<h3>3. Test 27 canned architectures with pre-trained weights<\/h3>](#3)\n\n[<h3>4. Train the best architecture<\/h3>](#4)\n\n[<h3>5. Examples of prediction<\/h3>](#5)\n\n# Description\nThis dataset contains 6,899 images from 8 distinct classes compiled from various sources (see Acknowledgements). The classes include airplane, car, cat, dog, flower, fruit, motorbike and person.\n\n# Acknowledgements\n- Airplane images obtained from http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\n- Car images obtained from https:\/\/ai.stanford.edu\/~jkrause\/cars\/car_dataset.html\n- Cat images obtained from https:\/\/www.kaggle.com\/c\/dogs-vs-cats\n- Dog images obtained from https:\/\/www.kaggle.com\/c\/dogs-vs-cats\n- Flower images obtained from http:\/\/www.image-net.org\n- Fruit images obtained from https:\/\/www.kaggle.com\/moltean\/fruits\n- Motorbike images obtained from http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\n- Person images obtained from http:\/\/www.briancbecker.com\/blog\/research\/pubfig83-lfw-dataset\n\nPrasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal (2018); *Effects of Degradations on Deep Neural Network Architectures*. Published in arXiv preprint arXiv:1807.10108.","6fad8df5":"# 1. Data preprocessing and visualization<a class=\"anchor\" id=\"1\"><\/a><a class=\"anchor\" id=\"1\"><\/a>","8af35fb5":"# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"><\/a><a class=\"anchor\" id=\"1\"><\/a>","a39de698":"# 5. Examples of prediction<a class=\"anchor\" id=\"5\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n","410f7f38":"# Competition between 27 architectures to classify images\n## *Prediction and speed of 27 neural network architectures*\n\n![Competition architecture](https:\/\/i.imgur.com\/hk880fz.png)"}}