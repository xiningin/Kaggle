{"cell_type":{"b2332fc1":"code","53a1ea7d":"code","deafa041":"code","479b3ae7":"code","73a5bd0f":"code","b934b3b9":"code","a793ead4":"code","c5551646":"code","021a8f95":"code","5f692a4f":"code","01ed722a":"code","366611e2":"code","d5dddfd7":"code","d9c60ef2":"code","21066ad2":"code","de3e4e2b":"code","cfadf8ff":"code","68c0828b":"code","8149b99e":"code","4a8ed837":"markdown","d5e8f29f":"markdown","c8c46322":"markdown","a3ac1748":"markdown","2a860950":"markdown","3ac67bda":"markdown","aa4e0910":"markdown","ff1132cd":"markdown","9ecb356a":"markdown","94750121":"markdown","48e287ec":"markdown","28b01a09":"markdown","ecb67318":"markdown","46d3592c":"markdown","9a2913a9":"markdown","0f1e5635":"markdown","3cffe0f2":"markdown","968b76a9":"markdown","5c6e0079":"markdown","49dd58cd":"markdown","31b8b5f7":"markdown","fd7e1c71":"markdown","3db43742":"markdown"},"source":{"b2332fc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","53a1ea7d":"telecom_data = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ntelecom_data.info()","deafa041":"telecom_data.head()","479b3ae7":"telecom_data = telecom_data.replace( { 'SeniorCitizen': { 0: 'No', 1:'Yes' } } )","73a5bd0f":"def categorical_segment(column_name:str) -> 'grouped_dataframe':\n    segmented_df = telecom_data[[column_name, 'Churn']]\n    segmented_churn_df = segmented_df[segmented_df['Churn'] == 'Yes']\n    grouped_df = segmented_churn_df.groupby(column_name).count().reset_index().rename(columns = {'Churn':'Churned'})\n    total_count_df = segmented_df.groupby(column_name).count().reset_index().rename(columns = {'Churn':'Total'})\n    merged_df = pd.merge(grouped_df, total_count_df, how = 'inner', on = column_name)\n    merged_df['Percent_Churned'] = merged_df[['Churned','Total']].apply(lambda x: (x[0] \/ x[1]) * 100, axis=1) \n    return merged_df\n\ncategorical_columns_list = list(telecom_data.columns)[1:5] + list(telecom_data.columns)[6:18]\n\ngrouped_df_list = []\n\nfor column in categorical_columns_list:\n    grouped_df_list.append( categorical_segment( column ) )\n    \ngrouped_df_list[0]","b934b3b9":"import matplotlib.pyplot as plt \nfor i , column in enumerate(categorical_columns_list):\n    fig, ax = plt.subplots(figsize=(13,5))\n    plt.bar(grouped_df_list[i][column] , [ 100 - i for i in grouped_df_list[i]['Percent_Churned'] ],width = 0.1, color = 'g')\n    plt.bar(grouped_df_list[i][column],grouped_df_list[i]['Percent_Churned'], bottom =  [ 100 - i for i in grouped_df_list[i]['Percent_Churned'] ],\n            width = 0.1, color = 'r')\n    plt.title('Percent Churn by ' + column)\n    plt.xlabel(column)\n    plt.ylabel('Percent Churned')\n    plt.legend( ('Retained', 'Churned') )\n    plt.show()\n","a793ead4":"def continous_var_segment(column_name:str) -> 'segmented_df':\n    segmented_df = telecom_data[[column_name, 'Churn']]\n    segmented_df = segmented_df.replace( {'Churn': {'No':'Retained','Yes':'Churned'} } )\n    segmented_df['Customer'] = ''\n    return segmented_df\n\ncontinous_columns_list = [list(telecom_data.columns)[18]] + [list(telecom_data.columns)[5]]\n\n\ncontinous_segment_list = []\n\nfor var in continous_columns_list:\n    continous_segment_list.append( continous_var_segment(var) )\n    \nimport seaborn as sns\nsns.set('talk')\n\nfor i, column in enumerate( continous_columns_list ):\n    fig, ax = plt.subplots(figsize=(8,11))\n    sns.violinplot(x = 'Customer', y = column, data = continous_segment_list[i], hue = 'Churn', split = True)\n    plt.title('Churn by ' + column)\n    plt.show()\n","c5551646":"from sklearn.cluster import KMeans \nfrom sklearn.preprocessing import MinMaxScaler\n\nmonthlyp_and_tenure = telecom_data[['MonthlyCharges','tenure']][telecom_data.Churn == 'Yes']\n\nscaler = MinMaxScaler()\nmonthly_and_tenure_standardized = pd.DataFrame( scaler.fit_transform(monthlyp_and_tenure) )\nmonthly_and_tenure_standardized.columns = ['MonthlyCharges','tenure']\n\nkmeans = KMeans(n_clusters = 3, random_state = 42).fit(monthly_and_tenure_standardized)\n\nmonthly_and_tenure_standardized['cluster'] = kmeans.labels_\n\nfig, ax = plt.subplots(figsize=(13,8))\nplt.scatter( monthly_and_tenure_standardized['MonthlyCharges'], monthly_and_tenure_standardized['tenure'],\n           c = monthly_and_tenure_standardized['cluster'], cmap = 'Spectral')\n\nplt.title('Clustering churned users by monthly Charges and tenure')\nplt.xlabel('Monthly Charges')\nplt.ylabel('Tenure')\n\n\nplt.show()\n\n","021a8f95":"telecom_data_filtered = telecom_data.drop(['TotalCharges','customerID'], axis = 1)\n\ndef encode_binary(column_name:str):\n    global telecom_data_filtered\n    telecom_data_filtered = telecom_data_filtered.replace( { column_name: { 'Yes': 1 , 'No': 0 } }  )\n    \n\nbinary_feature_list = list(telecom_data_filtered.columns)[1:4] + [list(telecom_data_filtered.columns)[5]] \\\n+ [list(telecom_data_filtered.columns)[15]]  \\\n+ [list(telecom_data_filtered.columns)[18]]\n    \nfor binary_feature in binary_feature_list:\n    encode_binary(binary_feature)\n    \n\ntelecom_data_processed = pd.get_dummies( telecom_data_filtered, drop_first = True )\n\ntelecom_data_processed.head(10)\n\n","5f692a4f":"telecom_data.Churn.value_counts()","01ed722a":"import numpy as np \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nimport sklearn.metrics as metrics\n%matplotlib inline \n\nX = np.array( telecom_data_processed.drop( ['Churn'] , axis = 1 ) )\ny = np.array( telecom_data_processed['Churn'] )\n\nX_train,X_test,y_train,y_test = train_test_split( X, y , test_size = 0.2 , random_state = 42 )\n\ndef get_metrics( model ):\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)\n    y_actual = y_test \n    print()\n    print('-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n    print()\n    print('Accuracy on unseen hold out set:' , metrics.accuracy_score(y_actual,y_pred) * 100 , '%' )\n    print()\n    f1_score = metrics.f1_score(y_actual,y_pred)\n    precision = metrics.precision_score(y_actual,y_pred)\n    recall = metrics.recall_score(y_actual,y_pred)\n    score_dict = { 'f1_score':[f1_score], 'precision':[precision], 'recall':[recall]}\n    score_frame = pd.DataFrame(score_dict)\n    print(score_frame)\n    print()\n    fpr, tpr, thresholds = metrics.roc_curve( y_actual, y_prob[:,1] ) \n    fig, ax = plt.subplots(figsize=(8,6))\n    plt.plot( fpr, tpr, 'b-', alpha = 0.5, label = '(AUC = %.2f)' % metrics.auc(fpr,tpr) )\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend( loc = 'lower right')\n    plt.show()","366611e2":"rf = RandomForestClassifier( n_estimators = 20, n_jobs=-1, max_features = 'sqrt', random_state = 42 )\nparam_grid1 = {\"min_samples_split\": np.arange(2,11), \n              \"min_samples_leaf\": np.arange(1,11)}\nrf_cv = GridSearchCV( rf, param_grid1, cv=5, iid = False )\nrf_cv.fit(X_train,y_train)\nprint( rf_cv.best_params_ )\nprint( rf_cv.best_score_ )","d5dddfd7":"rf = RandomForestClassifier( n_estimators = 20, n_jobs = -1, min_samples_split = 2, min_samples_leaf = 8, random_state = 2 )\nparam_grid2 = {'max_depth': np.arange(9,21),\n              'max_features':['sqrt','log2']}\nrf_cv = GridSearchCV(rf, param_grid2, cv=5, iid = False)\nrf_cv.fit(X_train,y_train)\nprint( rf_cv.best_params_ )\nprint( rf_cv.best_score_ )","d9c60ef2":"rf = RandomForestClassifier( n_estimators = 1000, max_features = 'log2', max_depth = 11, min_samples_split = 2, \n                          min_samples_leaf = 8, n_jobs = -1 , random_state = 42, class_weight = {0:0.95, 1:2})\nrf.fit(X_train,y_train)\nprint('Training Accuracy:',rf.score(X_train,y_train)*100,'%')\nget_metrics(rf)","21066ad2":"model_pipeline = Pipeline( steps = [( 'normalizer', MinMaxScaler() ), \n                                   ( 'log_reg', LogisticRegression( penalty = 'l2', random_state = 42 ) ) ] )\nparam_dict = dict( log_reg__C = [0.001, 0.01, 0.1, 1, 10, 100])\nestimator = GridSearchCV( model_pipeline, param_grid = param_dict, cv = 5, n_jobs = -1, iid = False )\nestimator.fit(X_train,y_train)\nprint(estimator.best_params_)\nprint(estimator.best_score_)","de3e4e2b":"model_pipeline = Pipeline( steps = [( 'normalizer', MinMaxScaler() ), \n                                   ( 'log_reg', LogisticRegression( penalty = 'l2', C = 100, random_state = 42, class_weight = {0:.95 , 1:2} ) ) ] )\nmodel_pipeline.fit(X_train,y_train)\nprint('Training Accuracy:',model_pipeline.score(X_train,y_train)*100,'%')\nget_metrics(model_pipeline)","cfadf8ff":"svc_pipeline = Pipeline( steps = [( 'normalizer', MinMaxScaler() ), \n                                   ( 'svc', SVC(random_state = 42, probability = True) ) ] )\nparams = [0.001, 0.01, 0.1, 1, 10]\nparam_dict = dict( svc__C = params, svc__gamma = params)\nestimator = GridSearchCV( svc_pipeline, param_grid = param_dict, cv = 5, n_jobs = -1, iid = False )\nestimator.fit(X_train,y_train)\nprint(estimator.best_params_)\nprint(estimator.best_score_)","68c0828b":"svc = SVC(C = 1, gamma = 0.01, class_weight = {0:1, 1:2}, random_state = 42, probability = True)\nsvc.fit(X_train,y_train) \nprint('Training Accuracy:',svc.score(X_train,y_train)*100,'%')\nget_metrics(svc)","8149b99e":"coeff = model_pipeline.named_steps['log_reg'].coef_.flatten()\ncoeff_updated = np.append( coeff[:8], [sum(coeff[8:10]), sum(coeff[10:12]), sum(coeff[12:14]),sum(coeff[14:16]), \n                         sum(coeff[16:18]), sum(coeff[18:20]), sum(coeff[20:22]), sum(coeff[22:24]), sum(coeff[24:26]), sum(coeff[26:]) ] )\ncolumns = ['SeniorCitizen', 'Partner', 'Dependents', 'Tenure', 'PhoneService', 'PaperlessBilling', 'MonthlyCharges', 'Gender', 'MultipleLines',\n          'InternetService', 'OnlineSecurity','OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV','StreamingMovies', 'Contract', 'PaymentMethod']\nfig, ax = plt.subplots(figsize=(50,20))\nplt.plot(columns, coeff_updated, c = 'yellow', marker='o', linewidth = 6, linestyle='dashed', markersize=20, mfc = 'red')\nplt.title('Coefficients Learned by the Logistic Regression Model')\nplt.ylabel('Coeff')\nplt.xlabel('Features')\nplt.show()","4a8ed837":"I see that unlike all the other binary categorical features , senior citizen has alreayd been encoded. I'm going to reverse that for the visualization part of this notebook as It'll be taken care of during the pre-processing stage with all the other categorical columns.","d5e8f29f":"Model 2: Logistic Regression ","c8c46322":"Pre-processing the data using label encoding and one hot encoding to get it ready for machine learning algorithms.","a3ac1748":"**Churn by categorical features**\n\nWrting a loop that goes through each categorical feature and calls the above function to get a grouped dataframe for that feature and visualizes it using a stacked bargraph. ","2a860950":"Normalizing  tenure and monthly charges and using K-means clustering to cluster churned customers based on them.","3ac67bda":"First things first. Let's take a look at the columns to check for missing or weird values in the dataset.","aa4e0910":"Not an very imbalanced dataset in terms of the positive and negative target variable.","ff1132cd":"**Note:** \n* If you think about it from the perspective of a big telecom company , it would be more important to catch most of the postive cases, that is the users who might churn and provide incentives to them to make them stay than it is to catch the customers who will not be churning for now. This means there is a higher cost associated with misclassifying a churned user as retained than there is for missclassifying a retained user as churned. As a result for all models from here on I will focus on improving the recall to maximize the amount of positive cases our model catches. For this I would specify a higher class weight for the positive class '1' and lower very slightly the weight associated with the negative class '0'. This would most likely lower the overall accuracy of the model but as any good data scientist knows, accuracy is not everything. \n* I will plot the ROC curve and specify the AUC for each model to see how well they were able to separate the two classes.","9ecb356a":"Since our logistic regression and random forest model perfomed more or less the same, It's a good idea to go with the simpler and more intepretable model that is the logistic regression for the choice of model. \n\nNow let's take a look under the hood of our logistic regression model to see what it has learned by plotting the various coefficients it assigned to all the features. Taking a look at what our model has learned is also a very good way to assess a model before you deploy it for any kind of production.","94750121":"Some Interesting observations:\n* Not suprisingly most retained customers have monthly charges concentrated around 20 Dollars  while most churned customers had monthly charges between 80 - 100 Dollars.\n* Most of that customers that churned, churned within approximately the first 5-7 months of joining the service while a lot of the retained customers have been around for upwards of 60-65 months ( 5 years and up).","48e287ec":"In the unseen\/validation set our random forest model was able to capture 75.6% of all positive cases and has an AUC of 0.87","28b01a09":"I'm gonna write a function that accepts the name of a column (feature) as it's argument and groups the dataframe on that feature giving us the total amount of churned users with respect to each value in that feature and what percent of churned users does each value constitue for.","ecb67318":"Our SVC performed worse than the logistic regression and random forest model by catching 71.8% of the positive cases in the unseen\/validation set and had an AUC of 0.84. ","46d3592c":"Our model assigned very high coefficients to Monthly Charges, Tenure and Internet service which was something I was expecting and surprisingly a high coefficient for contract as well, while all the other coeffcients were pushed close to 0 by our L2 regularization parameter. Although I used C=100 which is considered a pretty large value which means less regularization we can clearly see what our logistic regression model learned.","9a2913a9":"Model 3: SVC ","0f1e5635":"No missing values , a very very clean dataset. Not the most real world conditions but it makes our job easier. \n\nLet's take a look at the first 5 rows.\n","3cffe0f2":"Seems our algorithm found one tight and 2 semi-loose clusters to group churned users by:\n* *Customers with low monthly charges and low tenure*:  Could have been a temporary connection  for them or people looking for very minimal service who found a service provider offering even lower charges for basic services and churned quickly despite low monthly charges.\n* *Customers with high monthly charges and low tenure:* The heaviest concentration of churned users. The most common churned users who were possibly unhappy with the prices and stayed for a little while before quickly leaving the service provider for better , cheaper options.\n* *Customers with high monthly charges and high tenure:* The most interesting group of churned users. They might have stayed initially despite high prices becuase they either thought the service was worth the price or simply due to lack of better alternatives and churned after a while in contrast with most other churned users who churned pretty quickly in their tenure. \n\nThe interesting observation here is that most churned users with low monthly charges churned pretty quickly. There is a very small concentration of churned users who had low prices in the high tenure zone. This usually points to very dissatisfied customers or customers who were looking for temporary service providers at the time. \n","968b76a9":"Model 1: Random Forest \n* Tuning min_samples_split, min_samples_leaf, max_features and max_depth using Grid Search with 5 fold cross validation to get a good bias vs variance tradeoff from our model\n\n    ","5c6e0079":"Importing necessary libraries and writing a function that makes it easily to assess and visualize model perfomance using sklearn metrics and matplotlib ","49dd58cd":"**Churn by numerical features**\n\nWrting a function like above that groups by numerical features and using a similar for loop to plot one of my favorite plots namely violin plots to visualie churn by  monthly charges and tenure.","31b8b5f7":"Our logistic regression preformed slighly better than our ensemble method by catching 76.6% of all postive cases in our unseen\/validation set but had a very slighly lower AUC of 0.86","fd7e1c71":"Thank you very much if you took a look at my Kernel. I love feedback and I thrive on criticism. If I did something wrong or stupid or if you simply have nay suggestions for stuff I can improve on, no one would be more happy than me to hear about it. \nCheers :) ","3db43742":"Some interesting observations:\n* A higher percentage of senior citizens churned over people who were not senior citizens \n* More people with partners churned over people who did not have partners. Same for people with dependents \n* The highest churn by internet service is accounted for by Fiber Optic connection users over people who use the good ol DSL and people not using the internet service. Possibly something to do with price, I would assume.\n\nFeel free to take a gander at all the other plots to see what you find interesting. "}}