{"cell_type":{"474d430d":"code","3755be89":"code","9760350b":"code","4b4f1012":"code","f36a80b1":"code","fdb4dc75":"code","8d754f61":"code","34b53d30":"code","d0cf2d0c":"markdown","7caef928":"markdown","f3221236":"markdown","08d90218":"markdown","a46c1024":"markdown","379e5977":"markdown","298e3479":"markdown"},"source":{"474d430d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3755be89":"from sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# Instantiate a model and fit it to the training set\nlogreg = LogisticRegression(multi_class='multinomial',solver='lbfgs').fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Logistic Regressin Accuracy Score : \",logreg.score(X_test, y_test))\n# we predicted the correct class on 88% of the samples in X_test\ny_pred = logreg.predict(X_test)\n#Print Classification Report\nprint(classification_report(y_test, y_pred))","9760350b":"from sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\niris = load_iris()\n# split data into training and test data.\n#train_X, test_X, train_y, test_y = train_test_split(iris.data, iris.target, train_size=0.8, test_size=0.5,random_state=123)\n\nlogreg = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=500)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=3)\nprint(\"cross-validation scores: \", scores)\n\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\nprint(\"cross-validation with 5 folds, scores: \", scores)\n","4b4f1012":"from sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\n\n#Loading Iris dataset\niris = load_iris()\nlogreg = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=500)\n\n#Setting up kfolds splitter\nkfolds= KFold(n_splits=5)\n\n#Using splitter with cross validation\nscores = cross_val_score(logreg, iris.data, iris.target, cv=kfolds)\nprint(\"cross-validation with 5 folds, scores: \", scores)\nprint(\"cross-validation with 5 folds, mean scores: \", scores.mean())\n# Using shuffling to resolve possible issues due to data imbalance\nkfold = KFold(n_splits=5, shuffle=True, random_state=0)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=kfold)\nprint(\"cross-validation with 5 folds after adding shuffle step, scores, mean score: \", scores)\nprint(\"cross-validation with 5 folds after adding shuffle step, mean score: \", scores.mean())\n\n","f36a80b1":"from sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\n#Loading Iris dataset\niris = load_iris()\n\nlogreg = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=500)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"number of cv iterations: \", len(scores))\nprint(\"mean accuracy: \", scores.mean())","fdb4dc75":"from sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n#Loading Iris dataset\niris = load_iris()\nclass_names = list(iris.target_names)\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,test_size=0.30, random_state=0)\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C)\n\nlogreg = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=1000)\n\n\nCs = np.logspace(-6, -1, 10)\nclf = GridSearchCV(estimator=logreg, param_grid = hyperparameters, cv=5, verbose=0)\n\n# Fit grid search\nbest_model = clf.fit(X_train, y_train)\n\n#Prediction using best model\n\ny_pred = best_model.predict(X_test)\n\n# Classification Report\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\n\n# Confusion Matrix\nprint(\"Confusion Matrix : \\n\", confusion_matrix(y_test, y_pred))\n\n","8d754f61":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\n\n# Import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binarize the output\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n# Add noisy features to make the problem harder\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n\n# Learn to predict each class against the other\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plotting ROC for Specific Class\nplt.figure()\nlw = 2\nplt.plot(fpr[2], tpr[2], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n","34b53d30":"\n# Compute macro-average ROC curve and ROC area\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","d0cf2d0c":"**Cross-validation in scikit-learn**\n\nCross-validation is implemented in scikit-learn using the cross_val_score function from the model_selection module.\nThe parameters of the cross_val_score function are the model we want to evaluate, the training data and the ground-truth labels. Let\u2019s evaluate LogisticRegression on the iris dataset:","7caef928":"**Evaluating Models Based on Accuracy **\n\nClassification results can be quite misleading, if you don\u2019t pay enough attention !\nA first step to improve the evaluation process in machine learning is to be more aware of the tradeoffs involved in choosing one evaluation method over another. \nIn this tutorial, we have covered a number of evaluation measures, data selection techniques for getting optimal classifier performance. Choice of the evaluation criteria\/process depends on the classifier chosen, dataset and the goals of the classification.\n\nWe will focus on the supervised methods, regression and classification, as evaluating and selecting models in unsupervised learning is often a very qualitative process.\n\nTo evaluate our supervised models, so far we have split our data set in to a training set and a test set using the train_test_split function, built a model on the training set calling the fit method, and evaluated it on the test set using the score method,\nwhich, for classification, computes the fraction of correctly classified samples:","f3221236":"**Plot ROC curves for the multiclass problem**","08d90218":"**Grid Search for Parameter Optimization**\n","a46c1024":"**Receiver Operating Characteristic (ROC)**\n\nExample of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality.\n\nROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the \u201cideal\u201d point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\n\nThe \u201csteepness\u201d of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\n\n**Multiclass settings**\n\nROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output. One ROC curve can be drawn per label, but one can also draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).\n\nAnother evaluation measure for multi-class classification is macro-averaging, which gives equal weight to the classification of each label.","379e5977":"**More Control over Cross-Validation**\n\nScikit-learn allows for much finer control over what happens during the splitting of the data, by providing a crossvalidation splitter as the cv parameter. We first have to import\nthe KFold splitter class from the model_selection module, and instantiate it with the number of folds you want to use:\n\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_folds=5)\n\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\n\ncross_val_score(logreg, iris.data, iris.target, cv=kfold)\n\nFrom the first set of results, we can verify that it is indeed a really bad idea to use 3-fold (non-stratified) cross-validation on the iris database. Another way to resolve this problem instead of stratifying the folds is to shuffle the data, to remove the ordering of the samples by label. We can do that setting the shuffle parameter of KFold to True. If we shuffle the data, we also need to fix the random_state to get a reproducible shuffling. Otherwise, each run of cross_val_score would yield a different result, as each time a different split would be used (this might not be a problem, but can be surprising).\n\nHere's the complete program :-","298e3479":"**Leave-One-Out cross-validation**\n\nAnother frequently used cross-validation method is leave-one-out. You can think of leave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very time-consuming, in particular for large datasets, but sometimes provides better esti\u2010\nmates on small datasets:"}}