{"cell_type":{"7295425f":"code","cb585547":"code","1963cf62":"code","2902ab73":"code","848fd19d":"code","24a05d63":"code","84c7f1da":"code","a15d93be":"code","544f2c64":"code","46e230c4":"code","c3079041":"code","f7b447af":"code","3364f6cf":"code","ae0bf352":"code","e70d9dd6":"code","e3113256":"code","ea03eb3e":"code","360b61a2":"code","32e29bca":"markdown","e2f2f22a":"markdown","edd11041":"markdown","16f1fb94":"markdown","70f7868a":"markdown","e4b7c5d9":"markdown","6a7ac914":"markdown","ed5ecd83":"markdown","18807225":"markdown","4850d38f":"markdown","3a9a7f9a":"markdown","d5204a36":"markdown","8c1b92f0":"markdown","f72006f9":"markdown","796dcae6":"markdown","655c933c":"markdown"},"source":{"7295425f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport umap\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras import Model, Sequential\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\n\n\n\n","cb585547":"data = pd.read_csv(\"..\/input\/kinematics-motion-data\/Kinematics_Data.csv\")\ndata.head()","1963cf62":"data.info()","2902ab73":"X  = data.drop(['time','date',\"username\",'activity'], axis = 1)\ny = data[\"activity\"]\n\nX.head()","848fd19d":"sns.countplot(x = y)\nplt.show()","24a05d63":"fit = umap.UMAP(\n    n_neighbors = 20,\n    min_dist = 0.5,\n    n_components=2,\n    metric ='euclidean'\n)\n\nax = plt.gca()\nu = fit.fit_transform(X)\nax.scatter(u[:,0], u[:,1],c = y,s=0.1)\n","84c7f1da":"ax=plt.gca()\nax.scatter(u[:,0], u[:,1],c = X.wrist,s = 0.1)\nplt.show() ","a15d93be":"#Scaling of input and splitting into a training and validation sample\nMMS = MinMaxScaler()\nX = MMS.fit_transform(X)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .3, \n                                                  shuffle = True, random_state = 0)\n\nprint(X_train.shape,X_val.shape)","544f2c64":"neighbors=np.array([3,5,7,11,15,23])\naccuracy_knn = np.zeros_like(neighbors,dtype=\"float\")\nknn_time = np.zeros_like(neighbors,dtype=\"float\")\n\nfor i,neighbors_val in enumerate(neighbors):\n    neigh = KNeighborsClassifier(n_neighbors = neighbors_val)\n    neigh.fit(X_train, y_train)\n    start = time.perf_counter()\n    y_pred = neigh.predict(X_val)\n    knn_time[i] = time.perf_counter() - start\n    accuracy = (y_val == y_pred).sum()\/len(y_val)\n    accuracy_knn[i] = accuracy*100","46e230c4":"plt.plot(neighbors,accuracy_knn)\nplt.xlabel(\"k\")\nplt.ylabel(\"Accuracy of K-nn\")\nplt.xticks(np.arange(3,24,2))\nplt.show()\nprint(\"k-nn achieved a maximal accuracy of %.1f %% with k = %d\" %  (np.max(accuracy_knn),neighbors[np.argmax(accuracy_knn)]))","c3079041":"decision_tree = tree.DecisionTreeClassifier().fit(X_train, y_train)\n\nstart = time.perf_counter()\ny_pred = decision_tree.predict(X_val)\nDT_time = time.perf_counter() - start\n\naccuracy_DT = (y_val == y_pred).sum()\/len(y_val)*100\nprint(\"Accuracy of Decision Tree : %.1f %%\" %  accuracy_DT)\n","f7b447af":"gnb = GaussianNB().fit(X_train, y_train)\nstart = time.perf_counter()\ny_pred = gnb.predict(X_val)\nGNB_time = time.perf_counter() - start\n\naccuracy_GNB =  ((y_val == y_pred).sum()\/len(y_val)*100)\nprint(\"Accuracy of Gaussian Na\u00efve Bayes : %.1f %%\" % accuracy_GNB)","3364f6cf":"DNN = Sequential([\n    layers.Dense(16, input_shape=(7,),  activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(32, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])","ae0bf352":"DNN.summary()","e70d9dd6":"DNN.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              metrics=['accuracy'])","e3113256":"import time\nstart = time.perf_counter()\n\nearly_stopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                          patience = 20)\n\nhistory = DNN.fit(X_train, y_train,\n                    epochs = 300, \n                    validation_data = (X_val, y_val), \n                    shuffle = True,\n                    callbacks = [early_stopping])\n\nelapsed = time.perf_counter() - start\nprint('Elapsed %.3f seconds.' % elapsed)","ea03eb3e":"start = time.perf_counter()\ny_pred = DNN.predict(X_val)[:,0]\ny_pred[y_pred <= 0.5] = 0.\ny_pred[y_pred > 0.5] = 1.\nDNN_time = time.perf_counter() - start\naccuracy_DNN = (y_val == y_pred).sum()\/len(y_val)*100\n\nprint(\"Accuracy of DNN : %.1f %%\" %  (accuracy_DNN))","360b61a2":"plt.scatter(knn_time[np.argmax(accuracy_knn)],accuracy_knn[np.argmax(accuracy_knn)],s=100,label = \"k-nn\")\nplt.scatter(DNN_time,accuracy_DNN, s=100,label = \"Deep Neural Network\")\nplt.scatter(DT_time,accuracy_DT,s=100,label = \"Decision Tree\")\nplt.scatter(GNB_time,accuracy_GNB,s=100,label = \"Gaussian Naive Bayes\")\nplt.legend()\nplt.ylabel('Accuracy (%)')\nplt.xlabel('Prediction time (s)')\n\nplt.show()","32e29bca":"We drop informations irrelevant for a classification : Time, date and username.","e2f2f22a":"Hence we could potentially remove the wrist parameter by applying a symmetry. However it's not possible without more information about the sensor's axes. I choose to keep this parameter for now.","edd11041":"The Neural Network clearly take the most time to train (10mn). But I'm more interested about the computation time of prediction.","16f1fb94":"Any comments or suggestion would be greatly appreciated :D","70f7868a":"It seems that the best accuracy is achieved with k = 7. We keep the accuracy and computation time for later","e4b7c5d9":"# **1st model : K-nn**","6a7ac914":"Activity seems to be equaly split. We can take a look at the data distribution. I use UMAP (see [here](https:\/\/umap-learn.readthedocs.io\/en\/latest\/)) and reduce the data to 2 dimensions :","ed5ecd83":"I apply a k-nn to this data with multiple k values and save the accuracy","18807225":"# **DATA EXTRACTION**","4850d38f":"In this notebook I use this simple data and try multiple classification algorithm, by order of complexity :\n\n* K-nn\n* Decision Tree \n* Naive Bayes\n* Neural Networks\n\nI train each algorithm and compare their accuracy. I also compare the computation time to classify the validation sample.","3a9a7f9a":"# **4th model : Deep Neural Network**","d5204a36":"Once we have all the information about all the different methods, we can plot the accuracy and the prediction time for each.","8c1b92f0":"# **3rd model : Gaussian Naive Bayes**","f72006f9":"The K-nn method is clearly the most efficient in terms of accuracy, this is due to the fact that the two different classes are well separated in the parameter space (as seen with UMAP).\n\nHowever, K-nn take a lot of time to compute a prediction. This is not a problem for our small sample (26577 predictions), but could be an important limitation. DNN seems to be the most efficient method here in terms of accuracy and prediction time.\n\n","796dcae6":"# **2nd model : Decision Tree**","655c933c":"It seems that the different activity labels are located in disctinct clusters. This indicate that the activity could be efficiently classified. It also seems that there is some kind of symmetry in this map. It's due to the \"wrist\" parameter, as seen below :"}}