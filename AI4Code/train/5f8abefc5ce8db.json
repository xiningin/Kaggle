{"cell_type":{"6d90c790":"code","e5e610b1":"code","7a30b8e3":"code","baa3b495":"code","df6d8f84":"code","310f6854":"code","0a1b2a56":"code","d7e4302a":"code","299223f9":"code","b9b9a445":"code","397e8bc5":"code","7e9bdf87":"code","f3ba9758":"code","22752230":"code","ba090404":"code","f365fba4":"code","e57e2d91":"code","9a803270":"code","f859f6e4":"code","e1b3677d":"markdown","e5d69e53":"markdown","1aaf885b":"markdown","dee39181":"markdown","0603cc36":"markdown","cf0beec2":"markdown","a8e2d7e8":"markdown","d2524719":"markdown","8f1c5c26":"markdown","82827a7c":"markdown","92e0966f":"markdown","4918fcbb":"markdown","8f945972":"markdown","be4c6093":"markdown","9c345204":"markdown","8788c36b":"markdown","b4ebc40e":"markdown"},"source":{"6d90c790":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5e610b1":"#!pip install lazypredict","7a30b8e3":"#Adding all possible machine learning techniques, not going to use all.\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers.advanced_activations import ReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten,Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\nimport keras\nimport cv2\nimport tensorflow\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')","baa3b495":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","df6d8f84":"#import pandas_profiling\n#from pandas_profiling import ProfileReport\n\n#profile = ProfileReport(data, title='Pandas Profiling Report')\n#profile","310f6854":"#We can see that there is no null values inside \ndata.isnull().values.any()","0a1b2a56":"#Not entirely alot of data, but still an amount to create multiple graphs for data exploration.\ndata.shape","d7e4302a":"data.info()\n#Data is floats and ints which are fine for predictions.","299223f9":"#import pandas and seaborn to plot again to make sure, typically its useful to plot the \nimport pandas as pd\nimport seaborn as sb","b9b9a445":"#Create heatmap using seaborn style gradient.\nheat = sns.light_palette('green', as_cmap = True)\nstyle = data.style.background_gradient(cmap = heat)\nstyle","397e8bc5":"#####\nplt.subplots(figsize=(11, 11)) \nsns.heatmap(data.corr(),annot=True)","7e9bdf87":"fig=plt.figure(figsize=(20,20))\nax=fig.add_subplot(2,2,1, projection=\"3d\")\nax.scatter(data['DEATH_EVENT'],data['age'],data['serum_creatinine'],c=\"darkgreen\",alpha=.5)\nax.set(xlabel='DEATH_EVENT',ylabel='age',zlabel='serum_creatinine')\nax.set(ylim=[0,100])\n#ax.set(xlim=[0,10])","f3ba9758":"#####\ntrain_set, test_set = train_test_split(data, train_size=0.8, test_size=0.2, random_state = 42)\nprint(len(train_set), ' Training Set +', len(test_set), ' Testing Set')","22752230":"#####\n#scaling after splitting data to avoid leaks\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n#train\nscaler.fit(train_set)\ntrain_set = scaler.transform(train_set)\n#Convert to table format - MinMaxScaler\ntrain_set = pd.DataFrame(data=data, columns=[\"age\", \"serum_creatinine\",\"anaemia\",\"creatinine_phosphokinase\",\"diabetes\",\"ejection_fraction\",\"high_blood_pressure\",\"platelets\"\n                                                        ,\"serum_sodium\",\"sex\",\"smoking\",\"DEATH_EVENT\"])\n#test\nscaler.fit(test_set)\ntest_set = scaler.transform(test_set)\n#Convert to table format - MinMaxScaler\ntest_set = pd.DataFrame(data=data, columns=[\"age\", \"serum_creatinine\",\"anaemia\",\"creatinine_phosphokinase\",\"diabetes\",\"ejection_fraction\",\"high_blood_pressure\",\"platelets\"\n                                                        ,\"serum_sodium\",\"sex\",\"smoking\",\"DEATH_EVENT\"])","ba090404":"\n#Perform hyper parameter optimization\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score","f365fba4":"#######\n#Same for train set(Target is purchase)\nX_train = train_set.drop(\"DEATH_EVENT\", axis=1) # drop labels for training set\ny_train = train_set[\"DEATH_EVENT\"].copy()\n\n#same for test set\nX_test = test_set.drop(\"DEATH_EVENT\", axis=1)\ny_test = test_set[\"DEATH_EVENT\"].copy()","e57e2d91":"#------KNeighborsClassifier-----\npipelineone = Pipeline([('knn', KNeighborsClassifier())])\nparam_grid = {'knn__n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n              'knn__leaf_size': [10,20,30,40,50,60,70,80,90,100]\n             }\n\n#Gridsearch takes in param_grid, and pipeline.\ngridone = GridSearchCV(pipelineone, param_grid, cv =5, scoring = 'roc_auc')\ngridone.fit(X_train, y_train)\n\ntestscore = gridone.score(X_test, y_test)\ntrainscore = gridone.score(X_train, y_train)\nprint('----AUC Values from KNN train and test Set----')\nprint('Internal test score accuracy:', testscore)\nprint('Internal train score accuracy:', trainscore)\n\n\n#on the testing data\ny_testpred = gridone.predict(X_test)\n\n#on the training data\ny_trainpred = gridone.predict(X_train)\n\n# summarize and present ROC score\n#roc takes in y_true and y_score\nKNNtest = accuracy_score(y_test, y_testpred)\nKNNtrain = accuracy_score(y_train, y_trainpred)\nprint('Best parameters: ', gridone.best_params_)\nprint('KNN Accuracy score from test set: ', (KNNtest))\nprint('KNN Accuracy score  from train set: ', (KNNtrain))\n\n","9a803270":"#-------LogisticRegression-----\n#[0.001, 0.01, 0.1, 1, 10, 100, 1000]\npipelinetwo = Pipeline([('logisticregression', LogisticRegression())])\nparam_grid = {'logisticregression__penalty' : ['l2'],\n              'logisticregression__C' : [10],\n              'logisticregression__solver' : ['liblinear']}\n#Gridsearch takes in param_grid, and pipeline.\ngridtwo = GridSearchCV(pipelinetwo, param_grid, cv =5, scoring = 'roc_auc')\ngridtwo.fit(X_train, y_train)\n\ntestscore = gridtwo.score(X_test, y_test)\ntrainscore = gridtwo.score(X_train, y_train)\nprint('----AUC Values from Logistic Regression train and test Set----')\nprint('Internal test score accuracy: ', testscore)\nprint('Internal train score accuracy: ', trainscore)\n\n\n#on the testing data\ny_testpred = gridtwo.predict(X_test)\n\n#on the training data\ny_trainpred = gridtwo.predict(X_train)\n\n# summarize and present ROC score\n#roc takes in y_true and y_score\nlogtest = accuracy_score(y_test, y_testpred)\nlogtrain = accuracy_score(y_train, y_trainpred)\nprint('Best parameters: ', gridtwo.best_params_)\nprint('Logistic Regress Accuracy Score from test set: ', (logtest))\nprint('Logistic Regress Accuracy Score from train set: ', (logtrain))","f859f6e4":"#-----------svm.SVC------------\npipelinethree = Pipeline([('svc', SVC(kernel='rbf', probability = True))])\n\nparam_grid = {\n        'svc__C': [1],\n        'svc__gamma': ['scale']}\n\n#Gridsearch takes in param_grid, and pipeline.\ngridthree = GridSearchCV(pipelinethree, param_grid, cv =5, scoring = 'roc_auc')\ngridthree.fit(X_train, y_train)\n\ntestscore = gridthree.score(X_test, y_test)\ntrainscore = gridthree.score(X_train, y_train)\nprint('----AUC Values from SVM train and test Set----')\nprint('Internal test score accuracy:', testscore)\nprint('Internal train score accuracy:', trainscore)\n\n#on the testing data\ny_testpred = gridthree.predict(X_test)\n\n#on the training data\ny_trainpred = gridthree.predict(X_train)\n\n# summarize and present ROC score\n#roc takes in y_true and y_score\nSVMtest = accuracy_score(y_test, y_testpred)\nSVMtrain = accuracy_score(y_train, y_trainpred)\nprint('Best parameters: ', gridthree.best_params_)\nprint('SVC accuracy score from test set: ', (SVMtest))\nprint('SVC accuracy score from train set: ', (SVMtrain))","e1b3677d":"# Step 1:\nFirst check the values","e5d69e53":"**IF visualized, we can see that these three are somewhat correlated.**","1aaf885b":"**First Glance, there is no need for onehot encoding as everything is in floats\/ints **","dee39181":"![lazy%20c.jpg](attachment:lazy%20c.jpg)","0603cc36":"# Or simply we just check if there are no missing values","cf0beec2":"**Serum_creatinine, Age and Death(0 and 1)\n\nAge and Death_event are typically Weakly and positively correlated but not significantly.\nSerum and Death are Typically higher, but still a weak positive correlation.\n\nAre Death_event and age show any significance?\nDo Serum and death show any significance?**","a8e2d7e8":"# note this is regression analysis with pipes implemented.","d2524719":"# Step 5: PREPROCESS\n\nMax-Min Normalization will be used as we will obtain smaller standard deviations.","8f1c5c26":"# Regression analysis consists of a set of machine learning methods that allow us to predict a continuous outcome variable (y) based on the value of one or multiple predictor variables (x). ... Linear regression is the most simple and popular technique for predicting a continuous variable.\u00b6\n\n\nhttp:\/\/www.sthda.com\/english\/wiki\/regression-analysis-essentials-for-machine-learning","82827a7c":"# Step 4: SPLITTING THE DATA","92e0966f":"Logistic Regression, SVC and KNN with hyper parameter optimization can most likely stand out for accuracy\n\n# Conclusion:\n\nLabelPropagation, XGBClassifier, RandomForestClassifier, DecisionTreeClassifier, ExtraTreeClassifier, ExtraTreesClassifier, LabelSpreading\nare all also valid.","4918fcbb":"# Step 2:Check for missing Values ( we can easily do this by using pandas profiling from: https:\/\/github.com\/pandas-profiling\/pandas-profiling)\n\nThis will give us important uses: Such as what variables are significant. Checking for null values and finding more info about the variables themselves and their correlations.","8f945972":"# Step 9:\n# Quickly check other classifiers\nTo get a sense if Logistic Regression really is a great model.","be4c6093":"# Step 8: Conclusion using different classifier models:\n\n(from our test set to qualify performance)\n\nKNN Accuracy score: 0.6 = 60%\n\nLogistic Regression Accuracy score: 0.7833333333333333 = 78.3%\n\nSVC Accuracy score: 0.7 = 70%\n\n\nOverall it seems Logistic regression comes out as king which comes out as no surprise.","9c345204":"# ##**Step 3: Visualization and Correlation****","8788c36b":"# Step 7: Predicting and Accuracy Score\n\n(note: score method uses internal scoring(crossvlidation and gridsearch))\n(So refer to the accuracy_score for the actual score)","b4ebc40e":"# Step 6: BUILDING THE MODEL"}}