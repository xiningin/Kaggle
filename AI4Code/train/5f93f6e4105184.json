{"cell_type":{"be9dc80b":"code","c07b1585":"code","bf7a0710":"code","a486cd62":"code","0c0f5b5f":"code","02743375":"code","dc6c7ab7":"code","bfc89846":"code","e368e8ce":"code","cae5d472":"code","751e8308":"code","c2b86431":"code","5719c4b5":"code","c66a3839":"code","e65172b8":"code","2ed54d93":"code","e9768f19":"code","9a46de4f":"code","c48f0979":"code","d37b3364":"code","c2214389":"code","875a8186":"code","8ac9826c":"code","cdc744bc":"code","40d58afe":"code","409947ef":"markdown","3a4bb995":"markdown","1e0f994a":"markdown","ea635a70":"markdown","a85fef19":"markdown","fef9b70c":"markdown","095df4ce":"markdown","5764ec85":"markdown","284040c7":"markdown","054a970b":"markdown","512acb2a":"markdown","02e7a349":"markdown","396f53ed":"markdown","683ea73e":"markdown","6a0a8b23":"markdown","30a359be":"markdown"},"source":{"be9dc80b":"import re\nimport os\nimport numpy as np \nimport pandas as pd \nfrom sklearn.svm import SVC\nimport plotly.express as px\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.metrics import recall_score,precision_score,make_scorer\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier,EasyEnsembleClassifier,RUSBoostClassifier\n\npd.set_option('display.max_rows', 700)\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c07b1585":"train = pd.read_json('\/kaggle\/input\/outofscope-intent-classification-dataset\/is_train.json')\nval = pd.read_json('\/kaggle\/input\/outofscope-intent-classification-dataset\/is_val.json')\ntest = pd.read_json('\/kaggle\/input\/outofscope-intent-classification-dataset\/is_test.json')\noos_train = pd.read_json('\/kaggle\/input\/outofscope-intent-classification-dataset\/oos_train.json')\noos_val = pd.read_json('\/kaggle\/input\/outofscope-intent-classification-dataset\/oos_val.json')\noos_test = pd.read_json('\/kaggle\/input\/outofscope-intent-classification-dataset\/oos_test.json')\nfiles = [(train,'train'),(val,'val'),(test,'test'),(oos_train,'oos_train'),(oos_val,'oos_val'),(oos_test,'oos_test')]\nfor file,name in files:\n    file.columns = ['text','intent']\n    print(f'{name} shape:{file.shape}, {name} has {train.isna().sum().sum()} null values')\nin_train = train.copy()","bf7a0710":"in_train.intent.value_counts()","a486cd62":"def ngrams_top(corpus,ngram_range,n=10):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\n\ndef get_emails(x):\n    email = re.findall(r'[\\w\\.-]+@[\\w-]+\\.[\\w]+',str(x))\n    return \" \".join(email)\ndef get_urls(x):\n    url = re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\.[\\w]+',str(x))\n    return \" \".join(url)\ndef get_mentions(x):\n    mention = re.findall(r'(?<=@)\\w+',str(x))\n    return \" \".join(mention)\ndef get_hashtags(x):\n    hashtag = re.findall(r'(?<=#)\\w+',str(x))\n    return \" \".join(hashtag)\ndef text_at_a_glance(df):\n    res = df.apply(get_emails)\n    res = res[res.values!=\"\"]\n    print(\"Data has {} rows with emails\".format(len(res)))\n    res = df.apply(get_urls)\n    res = res[res.values!=\"\"]\n    print(\"Data has {} rows with urls\".format(len(res)))\n    res = df.apply(get_mentions)\n    res = res[res.values!=\"\"]\n    print(\"Data has {} rows with mentions\".format(len(res)))\n    res = df.apply(get_hashtags)\n    res = res[res.values!=\"\"]\n    print(\"Data has {} rows with hashtags\".format(len(res)))","0c0f5b5f":"text_at_a_glance(in_train.text)","02743375":"#check where hashtag has been found, punctuations will be taken care of by vectorizer()\ntemp = in_train.text.apply(get_hashtags)\nin_train.iloc[temp[temp.values!=\"\"].index].text","dc6c7ab7":"#top bigrams\nbigrams = ngrams_top(in_train.text,(2,2)).sort_values(by='count')\npx.bar(data_frame=bigrams,y='text',x='count',orientation='h',color_discrete_sequence=['#dc3912'],opacity=0.8,width=900,height=500)","bfc89846":"#top trigrams\ntrigrams = ngrams_top(in_train.text,(3,3)).sort_values(by='count')\npx.bar(data_frame=trigrams,y='text',x='count',orientation='h',color_discrete_sequence=['#f58518'],opacity=0.8,width=900,height=500)","e368e8ce":"def binarize(df):\n    df.intent = np.where(df.intent!='oos',0,1)\n    return df\n\ndef vectorizer(X):\n    cv = CountVectorizer(min_df=1,ngram_range=(1,2))\n    X_en = cv.fit_transform(X)\n    return cv,X_en\n\ndef labelencoder(y):\n    le = LabelEncoder()\n    le.fit(y)\n    y_enc = le.transform(y)\n    return le,y_enc\n\ndef preprocess(train):\n    X = train.text\n    y = train.intent\n    le,y = labelencoder(y)\n    cv,X = vectorizer(X)\n    return X,y,cv,le\n\ndef process_non_train(df,cv,le):\n    X = df.text\n    y = df.intent\n    X = cv.transform(X)\n    y = le.transform(y)\n    return X,y\n\ndef get_score(clf,binary=0):\n    clf.fit(X_train,y_train)\n    if binary==1:\n        y_pred = clf.predict(X_test)\n        return clf,clf.score(X_val,y_val),clf.score(X_test,y_test),recall_score(y_test,y_pred),precision_score(y_test,y_pred)\n    elif binary==0:\n        return clf,clf.score(X_val,y_val),clf.score(X_test,y_test)","cae5d472":"X_train,y_train,cv,le = preprocess(in_train)\nX_val,y_val = process_non_train(val,cv,le)\nX_test,y_test = process_non_train(test,cv,le)","751e8308":"val_scores = []\ntest_scores = []\nnames = []\n\nmodels = [(KNeighborsClassifier(n_neighbors=15),'KNN'),(SGDClassifier(),'SGD clf'),(MultinomialNB(),'MultinomialNB'),\n          (RandomForestClassifier(),'Random Forest'),(SVC(kernel='linear'),'Linear SVC')]\n\nfor model,name in models:\n    clf,score,test_score = get_score(model,0)\n    names.append(name)\n    val_scores.append(score*100)\n    test_scores.append(test_score*100)\npd.DataFrame(data=zip(val_scores,test_scores),index=names,columns=['val_score','test_score']).style.background_gradient()","c2b86431":"params = {\n    'loss':['squared_hinge','modified_huber'],\n    'alpha':[0.0001,0.001,0.01],\n    'max_iter':[250,500,1000],\n    'validation_fraction':[0.2]\n}\ncv = GridSearchCV(SGDClassifier(random_state=111),param_grid=params, cv=5,n_jobs=-1,verbose=2)","5719c4b5":"cv.fit(X_train,y_train)","c66a3839":"cv.best_params_","e65172b8":"cv.best_score_","2ed54d93":"oos_plus_train = binarize(pd.concat([in_train,oos_train],axis=0).reset_index(drop=True))\noos_plus_val = binarize(pd.concat([val,oos_val],axis=0).reset_index(drop=True))\noos_plus_test = binarize(pd.concat([test,oos_test],axis=0).reset_index(drop=True))","e9768f19":"oos_count = oos_plus_train.intent.value_counts()\noos_count","9a46de4f":"oos_count.plot(kind='bar')","c48f0979":"X_train,y_train,cv,le = preprocess(oos_plus_train)\nX_val,y_val = process_non_train(oos_plus_val,cv,le)\nX_test,y_test = process_non_train(oos_plus_test,cv,le)","d37b3364":"val_scores = []\ntest_scores = []\nrecall = []\nnames = []\nprecision = []\n\nmodels = [(BalancedRandomForestClassifier(sampling_strategy='not minority',random_state=111),'Balanced Random Forest'),\n          (RUSBoostClassifier(base_estimator=LogisticRegression(),sampling_strategy='not minority',random_state=111),'Random Undersampling + Adaboost'),\n          (EasyEnsembleClassifier(n_estimators=30,base_estimator=LogisticRegression(),replacement=True,sampling_strategy='not minority',random_state=111),'Easy Ensemble')]\n\nfor model,name in models:\n    _,score,test_score,recall_sc,precision_sc = get_score(model,1)\n    names.append(name)\n    val_scores.append(score*100)\n    test_scores.append(test_score*100)\n    recall.append(recall_sc*100)\n    precision.append(precision_sc*100)\npd.DataFrame(data=zip(val_scores,test_scores,recall,precision),index=names,\n             columns=['val_score','test_score','recall_score','precision_score']).style.background_gradient()","c2214389":"params = {\n    'n_estimators':[30,50,70],\n    'base_estimator':[AdaBoostClassifier(),LogisticRegression()],\n    'replacement':[False,True]\n}\ncv = GridSearchCV(EasyEnsembleClassifier(n_jobs=-1,sampling_strategy='not minority',random_state=111),\n                  verbose=2,\n                  scoring='recall',\n                  param_grid=params, cv=5,\n                  n_jobs=-1)\ncv.fit(X_train,y_train)","875a8186":"cv.best_score_","8ac9826c":"cv.best_params_","cdc744bc":"n_estimator = [25,50,100]\nlearning_rate=[0.001,0.0001]\nresult = []\nfor nest in n_estimator:\n    for lr in learning_rate:\n        eec = EasyEnsembleClassifier(base_estimator=AdaBoostClassifier(n_estimators=nest,learning_rate=lr)\n                                     ,n_estimators=100,\n                                     replacement=True,n_jobs=-1,\n                                     sampling_strategy='not minority',random_state=111)\n        eec.fit(X_train,y_train)\n        recall = recall_score(y_test,eec.predict(X_test))\n        result.append([nest,lr,recall])","40d58afe":"pd.DataFrame(result,columns=['n_estimators','learning_rate','recall_score']).style.background_gradient(subset=['recall_score'])","409947ef":"### Tune base estimator of EasyEnsemble","3a4bb995":"Easy ensemble has provided the best performance, next step is to explore easyensemble more\n\nLearn more about easyensemble:\n\nX. Y. Liu, J. Wu and Z. H. Zhou, \u201cExploratory Undersampling for Class-Imbalance Learning,\u201d in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 39, no. 2, pp. 539-550, April 2009.","1e0f994a":"# Import dependencies","ea635a70":"The imbalance ratio between 0 and 1 is 150\n\nThis imbalance can be dealt with by using sampling, here I will be using undersampling i.e the majority class will be undersampled to the size of minority class, coupling this with ensembling will give us best results.\n\nResources:\n\nhttps:\/\/machinelearningmastery.com\/random-oversampling-and-undersampling-for-imbalanced-classification\/\n\nhttps:\/\/www.kaggle.com\/residentmario\/undersampling-and-oversampling-imbalanced-data\/","a85fef19":"To perform in and out scope classification we have to encode all samples with intent features other than 'oos' to 0 and with 'oos' to 1 which will be done by binarize()","fef9b70c":"Balanced Random Forest:\n\nAccuracy \ud83d\ude2a\n\nrecall \ud83e\udd29\n\nprecision \ud83d\ude11","095df4ce":"# In-Scope and Out-Scope classification (binary)","5764ec85":"# Load and clean data","284040c7":"### Evaluate data over models","054a970b":"# Thank you for coming by, suggest modifications if any","512acb2a":"Check if any text sampe has email,url,mention and hashtags","02e7a349":"# Ngrams","396f53ed":"# Acknowledgement\nDataset:https:\/\/www.kaggle.com\/stefanlarson\/outofscope-intent-classification-dataset\n\nContext:\n\nMost supervised machine learning tasks assume a dataset with a set of well-defined target label set. But what happens when a trained model meets the real world, where inputs to the trained model might not be from the well-defined target label set? This dataset offers a way to evaluate intent classification models on \"out-of-scope\" inputs.\n\n\"Out-of-scope\" inputs are those that do not belong to the set of \"in-scope\" target labels. You may have heard other ways of referring to out-of-scope, including \"out-of-domain\" or \"out-of-distribution\".\nContent\n\n    is_*.json: these files house the train\/val\/test sets for the in-scope data. There are 150 in-scope \"intents\" (aka classes), which include samples such as \"what is my balance\" (which belongs to the balance class).\n    oos_*.json: these files house the train\/val\/test sets for the out-of-scope data. There is one out-of-scope intent: oos. Note that you don't have to use the oos_train.json data. In other words, an ML solution to the out-of-scope problem need not be trained on out-of-scope data, but it might help!\n\nEvaluation Metrics:\n\nThe task is intent classification, which generalizes to text classification (or categorization). This is a supervised ML problem. We use two metrics to evaluate:\n\n    In-scope accuracy is defined as #(correctly classified in-scope samples) \/ #(in-scope samples).\n    Out-of-scope recall is defined as #(correctly classified out-of-scope samples) \/ #(out-of-scope samples).\n\nAcknowledgements:\n\nThis dataset is from An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction by Larson et al., which was published in EMNLP in 2019. The GitHub page for this dataset is linked here.\nInspiration\n\nMost supervised machine learning tasks assume a dataset with a set of well-defined target label set. But what happens when a trained model meets the real world, where inputs to the trained model might not be from the well-defined target label set? This \"out-of-distribution\" problem has seen lots of recent development, as researchers and practitioners in both academia and industry are observing that many ML methods struggle on out-of-distribution data in a wide variety of tasks.","683ea73e":"All in-scope intents are balanced, no sampling required","6a0a8b23":"### Tuning EasyEnsemble","30a359be":"# In-scope prediction"}}