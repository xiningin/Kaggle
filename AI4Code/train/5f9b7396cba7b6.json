{"cell_type":{"52cd1a20":"code","d725f8de":"code","be7815cb":"code","3643b9f6":"code","1681bd26":"code","b0a1c89f":"code","b41e020d":"code","2ea61f9d":"code","9c0d5139":"code","5e6d7482":"code","a1ef2ff5":"code","b0a0f0d3":"code","f0f0eb06":"code","030ac168":"code","ffdf6aa2":"code","51cee354":"code","3e5a367e":"code","c3ca984d":"code","fd4d16ae":"code","46f5e2c6":"code","ab816c0b":"code","d2a9e096":"code","4f18fb98":"code","b1993df8":"code","38fac9a0":"code","e2ee4d2f":"code","22dece46":"code","e0e218a2":"code","d91716e8":"markdown"},"source":{"52cd1a20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d725f8de":"import pandas as pd\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","be7815cb":"Y_train = train['label']\nX_train = train.drop('label',axis=1)\n# X_train","3643b9f6":"X_train = X_train \/ 255.0\ntest = test \/ 255.0","1681bd26":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","b0a1c89f":"from keras.utils.np_utils import to_categorical\nY_train = to_categorical(Y_train, num_classes = 10)","b41e020d":"from sklearn.model_selection import train_test_split\nrandom_seed = 2\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)","2ea61f9d":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\ng = plt.imshow(X_train[0][:,:,0])","9c0d5139":"import keras\nfrom keras import models, layers, optimizers\nfrom keras.models import Sequential, model_from_json\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import MaxPooling2D,AveragePooling2D, GlobalAveragePooling2D,BatchNormalization\nfrom keras.models import Model","5e6d7482":"image_size = 28\nbatch_size = 32\nnum_classes = 10\n# epochs = 5\nepochs = 30","a1ef2ff5":"model = Sequential()\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(image_size, image_size,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_classes, activation='softmax'))\n\nprint(model.summary())\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])","b0a0f0d3":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)\n","f0f0eb06":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nfilepath=\"\/kaggle\/output\/5layered_best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3,verbose = 1,restore_best_weights = True)\ncallbacks_list = [earlystop,checkpoint]","030ac168":"history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=callbacks_list)","ffdf6aa2":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","51cee354":"import itertools\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, classification_report\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes)","3e5a367e":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n","c3ca984d":"plot_confusion_matrix(confusion_mtx, classes = range(10)) ","fd4d16ae":"print ('Train Accuracy', np.max(history.history['accuracy']))\nprint ('Train Loss', np.min(history.history['loss']))\nprint ('Validation Accuracy', np.max(history.history['val_accuracy']))\nprint ('Validation Loss', np.min(history.history['val_loss']))","46f5e2c6":"Y_pred = model.predict(test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\n","ab816c0b":"Y_pred_classes","d2a9e096":"df = pd.DataFrame(data = Y_pred_classes,columns=['Label'])","4f18fb98":"df.index = np.arange(1, len(df) + 1)","b1993df8":"df = df.reset_index()\ndf = df.rename(columns={'index':'ImageId','0':'Label'})","38fac9a0":"df","e2ee4d2f":"df.to_csv('predictions.csv',index=False)","22dece46":"test.shape","e0e218a2":"df.shape","d91716e8":"**If you found this notebook helpful, or any changes you would want to suggest, I would  be very happy to hear them.**"}}