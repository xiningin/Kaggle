{"cell_type":{"88be9473":"code","f5bcdfba":"code","191b6da9":"code","5ba2f75d":"code","545d92de":"code","9c78fe03":"code","a8b2f73e":"code","778317cb":"code","1612830c":"code","22cf999e":"code","f9ca60ed":"code","c819592a":"code","8cf48ce5":"code","a68a4c4d":"code","4fde9219":"code","fa45aa4a":"code","a84e3524":"code","127f5466":"code","0eda55ff":"code","066e4de7":"code","d6dcf452":"code","3623428a":"code","04372a44":"code","24905c69":"code","790ad548":"code","13646fde":"code","7f62900c":"code","56087799":"markdown"},"source":{"88be9473":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5bcdfba":"!pip install trax\nimport trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as fastnp\nimport random as rnd\n\nrnd.seed(11)","191b6da9":"!unzip '..\/input\/quora-question-pairs\/train.csv.zip'","5ba2f75d":"df = pd.read_csv('.\/train.csv',low_memory=False)\ndf.head()","545d92de":"len(df)","9c78fe03":"cut_df = int(len(df)*0.95)\n\ndf = df.sample(frac=1)\n\ntrain,test = df[:cut_df],df[cut_df:]","a8b2f73e":"len(train),len(test)","778317cb":"train = train[train.is_duplicate==1]\nlen(train)","1612830c":"Q1_train_words = np.array(train['question1'])\nQ2_train_words = np.array(train['question2'])\n\nQ1_test_words = np.array(test['question1'])\nQ2_test_words = np.array(test['question2'])\ny_test  = np.array(test['is_duplicate'])","22cf999e":"vocab_dir='gs:\/\/trax-ml\/vocabs\/'\nvocab_file='en_32k.subword'","f9ca60ed":"# sentence = 'It is nice to learn new things today!'\nQ1_train = list(map(list,list(trax.data.tokenize(iter(Q1_train_words),vocab_file=vocab_file))))\nQ2_train = list(map(list,list(trax.data.tokenize(iter(Q2_train_words),vocab_file=vocab_file))))\n\nQ1_test = list(map(list,list(trax.data.tokenize(iter(Q1_test_words),vocab_file=vocab_file))))\nQ2_test = list(map(list,list(trax.data.tokenize(iter(Q2_test_words),vocab_file=vocab_file))))","c819592a":"cut_off = int(len(Q1_train)*.8)\ntrain_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\nval_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]","8cf48ce5":"trax.data.text_encoder.PAD_ID","a68a4c4d":"def data_generator(Q1, Q2, batch_size, pad=0, shuffle=True):\n\n    input1 = []\n    input2 = []\n    idx = 0\n    len_q = len(Q1)\n    question_indexes = [*range(len_q)]\n    \n    if shuffle:\n        rnd.shuffle(question_indexes)\n    \n    while True:\n        if idx >= len_q:\n            idx = 0\n            if shuffle:\n                rnd.shuffle(question_indexes)\n#         print(len_q,idx)\n        q1 = Q1[question_indexes[idx]]\n        q2 = Q2[question_indexes[idx]]\n        \n        idx += 1\n        \n        input1.append(q1)\n        input2.append(q2)\n        \n        if len(input1) == batch_size:\n            max_len = len(max(max(input1,key=len),max(input2,key=len),key=len))\n            max_len =  2**int(np.ceil(np.log2(max_len)))\n#             print(max_len)\n            b1 = []\n            b2 = []\n            for q1, q2 in zip(input1, input2):  \n#                 print(q1.shape,q2.shape)\n                q1 = q1+[pad]*(max_len-len(q1))                \n                q2 = q2+[pad]*(max_len-len(q2))\n                \n                b1.append(q1)                \n                b2.append(q2)\n            yield np.array(b1), np.array(b2)\n\n            input1, input2 = [], [] ","4fde9219":"batch_size = 2\nres1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\nprint(\"First questions  : \",'\\n', res1, '\\n')\nprint(\"Second questions : \",'\\n', res2)","fa45aa4a":"def Siamese(vocab_size=33000, d_model=128, mode='train'):\n\n    def normalize(x):  # normalizes the vectors to have L2 norm 1\n        return x \/ fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n    \n    q_processor = tl.Serial(  # Processor will run on Q1 and Q2.\n        tl.Embedding(vocab_size,d_model), # Embedding layer\n        tl.LSTM(d_model), # LSTM layer\n        tl.Mean(axis=1), # Mean over columns\n        tl.Fn('Normalize', lambda x: normalize(x))  # Apply normalize function\n    )  # Returns one vector of shape [batch_size, d_model].\n        \n    # Run on Q1 and Q2 in parallel.\n    model = tl.Parallel(q_processor, q_processor)\n    return model","a84e3524":"model = Siamese();model","127f5466":"def TripletLossFn(v1, v2, margin=0.25):\n    \"\"\"Custom Loss function.\n\n    Args:\n        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.\n        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.\n        margin (float, optional): Desired margin. Defaults to 0.25.\n\n    Returns:\n        jax.interpreters.xla.DeviceArray: Triplet Loss.\n    \"\"\"\n    \n    scores = fastnp.dot(v1,v2.T) \n\n    batch_size = len(scores)\n    # use fastnp to grab all postive `diagonal` entries in `scores`\n    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n\n    # multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`\n    negative_without_positive = scores-fastnp.eye(batch_size)*2\n\n    # take the row by row `max` of `negative_without_positive`. \n    closest_negative = negative_without_positive.max(axis=[1])\n\n    # subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n    negative_zero_on_duplicate = (1-fastnp.eye(batch_size))*scores\n    \n    # use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` \n    mean_negative = fastnp.sum(negative_zero_on_duplicate,axis=1)\/(batch_size-1)\n    # compute `fastnp.maximum` among 0.0 and `A`\n    # A = subtract `positive` from `margin` and add `closest_negative` \n    triplet_loss1 = fastnp.maximum(margin-positive+closest_negative,0.0)\n    \n    # compute `fastnp.maximum` among 0.0 and `B`\n    # B = subtract `positive` from `margin` and add `mean_negative`\n    triplet_loss2 = fastnp.maximum(margin-positive+mean_negative,0)\n\n    # add the two losses together and take the `fastnp.mean` of it\n    triplet_loss = fastnp.mean(triplet_loss1+triplet_loss2)\n#     print(triplet_loss)\n    \n\n    \n    return triplet_loss","0eda55ff":"from functools import partial\ndef TripletLoss(margin=0.25):\n    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n    return tl.Fn('TripletLoss', triplet_loss_fn)","066e4de7":"batch_size = 256\ntrain_generator = data_generator(train_Q1, train_Q2, batch_size)\nval_generator = data_generator(val_Q1, val_Q2, batch_size)\n# print('train_Q1.shape ', train_Q1.shape)\n# print('val_Q1.shape   ', val_Q1.shape)","d6dcf452":"lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)","3623428a":"def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model\/'):\n    \"\"\"Training the Siamese Model\n\n    Args:\n        Siamese (function): Function that returns the Siamese model.\n        TripletLoss (function): Function that defines the TripletLoss loss function.\n        lr_schedule (function): Trax multifactor schedule function.\n        train_generator (generator, optional): Training generator. Defaults to train_generator.\n        val_generator (generator, optional): Validation generator. Defaults to val_generator.\n        output_dir (str, optional): Path to save model to. Defaults to 'model\/'.\n\n    Returns:\n        trax.supervised.training.Loop: Training loop for the model.\n    \"\"\"\n    output_dir = os.path.expanduser(output_dir)\n\n    train_task = training.TrainTask(\n        labeled_data=train_generator,       \n        loss_layer=TripletLoss(),         \n        optimizer=trax.optimizers.Adam(0.01),\n        lr_schedule=lr_schedule, \n    )\n\n    eval_task = training.EvalTask(\n        labeled_data=val_generator,      \n        metrics=[TripletLoss()],          \n    )\n    \n    training_loop = training.Loop(Siamese(),\n                                  train_task,\n                                  eval_tasks=eval_task,\n                                  output_dir=output_dir)\n\n    return training_loop","04372a44":"train_steps = 1000\ntraining_loop = train_model(Siamese, TripletLoss, lr_schedule)\ntraining_loop.run(n_steps = train_steps)","24905c69":"def classify(test_Q1, test_Q2, y, threshold, model, data_generator=data_generator, batch_size=64):\n    \"\"\"Function to test the accuracy of the model.\n\n    Args:\n        test_Q1 (numpy.ndarray): Array of Q1 questions.\n        test_Q2 (numpy.ndarray): Array of Q2 questions.\n        y (numpy.ndarray): Array of actual target.\n        threshold (float): Desired threshold.\n        model (trax.layers.combinators.Parallel): The Siamese model.\n        vocab (collections.defaultdict): The vocabulary used.\n        data_generator (function): Data generator function. Defaults to data_generator.\n        batch_size (int, optional): Size of the batches. Defaults to 64.\n\n    Returns:\n        float: Accuracy of the model.\n    \"\"\"\n    accuracy = 0\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    for i in range(0, len(test_Q1), batch_size):\n        # Call the data generator (built in Ex 01) with shuffle=False using next()\n        # use batch size chuncks of questions as Q1 & Q2 arguments of the data generator. e.g x[i:i + batch_size]\n        # Hint: use `vocab['<PAD>']` for the `pad` argument of the data generator\n        q1, q2 = next(data_generator(test_Q1[i:i+batch_size],test_Q2[i:i+batch_size],\n                                     batch_size,shuffle=False))\n        # use batch size chuncks of actual output targets (same syntax as example above)\n        \n        y_test = y[i:i+batch_size]\n#         print(y_test.shape)\n        # Call the model\n        v1, v2 = model([q1,q2])\n#         print(v1,v2)\n#         print(i)\n        for j in range(batch_size):\n            # take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]\n            # don't forget to transpose the second argument\n            d = fastnp.dot(v1[j],v2[j].T)\n            # is d greater than the threshold?\n            res = d>threshold\n#             print(j,res)\n            # increment accurancy if y_test is equal `res`\n            accuracy += float(y_test[j]==res)\n    # compute accuracy using accuracy and total length of test questions\n    accuracy = accuracy\/len(test_Q1)\n    ### END CODE HERE ###\n    \n    return accuracy","790ad548":"Q1_test = Q1_test[:512*39]\nQ2_test = Q2_test[:512*39]\ny_test = y_test[:512*39].tolist()","13646fde":"model = Siamese()\nmodel.init_from_file('.\/model\/model.pkl.gz')","7f62900c":"accuracy = classify(Q1_test,Q2_test, y_test, 0.7, model, batch_size = 512)\nprint(\"Accuracy\", accuracy)","56087799":"**This notebook is an assignment from the course: ['Natural Language Processing with Sequence Models'](https:\/\/www.coursera.org\/learn\/sequence-models-in-nlp)**"}}